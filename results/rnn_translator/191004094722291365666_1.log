Beginning trial 1 of 2
Gathering sys log on dss01
:::MLL 1570200541.876 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1570200541.876 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1570200541.877 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1570200541.877 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1570200541.878 submission_platform: {"value": "1xDSS8440", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1570200541.878 submission_entry: {"value": "{'hardware': 'DSS8440', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '5.0-0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz', 'num_cores': '40', 'num_vcpus': '40', 'accelerator': 'Tesla V100-PCIE-32GB', 'num_accelerators': '8', 'sys_mem_size': '754 GB', 'sys_storage_type': 'SATA SSD', 'sys_storage_size': '1x 447.1G + 1x 931.5G', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27700 Family [ConnectX-4]', 'num_network_cards': '1', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1570200541.878 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1570200541.879 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1570200547.267 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node dss01
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=4462' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=191004094722291365666 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191004094722291365666 ./run_and_time.sh
Run vars: id 191004094722291365666 gpus 8 mparams  --master_port=4462
NCCL_SOCKET_NTHREADS=16
NCCL_NSOCKS_PERTHREAD=4
STARTING TIMING RUN AT 2019-10-04 02:49:07 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=0
+ MATH=amp_fp16
+ [[ 0 -eq 1 ]]
+ LAUNCH_OPT='torch.distributed.launch --nproc_per_node 8  --master_port=4462'
+ echo 'running benchmark'
running benchmark
+ python -m torch.distributed.launch --nproc_per_node 8 --master_port=4462 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1570200549.964 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570200549.964 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570200549.965 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570200549.965 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570200549.967 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570200549.968 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570200549.970 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570200549.987 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 902653664
dss01:465:465 [0] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:465:465 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:465:465 [0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:465:465 [0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:465:465 [0] NCCL INFO NET/IB : No device found.
dss01:465:465 [0] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
NCCL version 2.4.8+cuda10.1
dss01:467:467 [2] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:467:467 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:466:466 [1] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:466:466 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:467:467 [2] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:467:467 [2] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:467:467 [2] NCCL INFO NET/IB : No device found.
dss01:471:471 [6] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:471:471 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:468:468 [3] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:468:468 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:470:470 [5] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:470:470 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:472:472 [7] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:472:472 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:469:469 [4] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:469:469 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:466:466 [1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:466:466 [1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:466:466 [1] NCCL INFO NET/IB : No device found.
dss01:467:467 [2] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>

dss01:470:470 [5] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:470:470 [5] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:470:470 [5] NCCL INFO NET/IB : No device found.

dss01:471:471 [6] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:471:471 [6] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:471:471 [6] NCCL INFO NET/IB : No device found.

dss01:468:468 [3] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:468:468 [3] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:468:468 [3] NCCL INFO NET/IB : No device found.

dss01:469:469 [4] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:469:469 [4] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:469:469 [4] NCCL INFO NET/IB : No device found.

dss01:472:472 [7] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:472:472 [7] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:472:472 [7] NCCL INFO NET/IB : No device found.
dss01:468:468 [3] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:472:472 [7] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:471:471 [6] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:466:466 [1] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:469:469 [4] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:470:470 [5] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:465:827 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555
dss01:467:828 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555
dss01:469:829 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa
dss01:470:831 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa
dss01:468:830 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555
dss01:471:832 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa
dss01:466:833 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555
dss01:472:834 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa
dss01:468:830 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to 5.
dss01:469:829 [4] NCCL INFO NCCL_P2P_LEVEL set by environment to 5.
dss01:470:831 [5] NCCL INFO NCCL_P2P_LEVEL set by environment to 5.
dss01:471:832 [6] NCCL INFO NCCL_P2P_LEVEL set by environment to 5.
dss01:467:828 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to 5.
dss01:472:834 [7] NCCL INFO NCCL_P2P_LEVEL set by environment to 5.
dss01:466:833 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to 5.
dss01:465:827 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to 5.
dss01:465:827 [0] NCCL INFO Channel 00 :    0   1   2   3   4   5   6   7
dss01:472:834 [7] NCCL INFO Ring 00 : 7[7] -> 0[0] via P2P/IPC
dss01:465:827 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/IPC
dss01:471:832 [6] NCCL INFO Ring 00 : 6[6] -> 7[7] via P2P/IPC
dss01:466:833 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via P2P/IPC
dss01:467:828 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/IPC
dss01:468:830 [3] NCCL INFO Ring 00 : 3[3] -> 4[4] via P2P/IPC
dss01:470:831 [5] NCCL INFO Ring 00 : 5[5] -> 6[6] via P2P/IPC
dss01:469:829 [4] NCCL INFO Ring 00 : 4[4] -> 5[5] via P2P/IPC
dss01:465:827 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
dss01:471:832 [6] NCCL INFO comm 0x7ffe98007590 rank 6 nranks 8 cudaDev 6 nvmlDev 6 - Init COMPLETE
dss01:465:827 [0] NCCL INFO comm 0x7ffe40007590 rank 0 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE
dss01:465:465 [0] NCCL INFO Launch mode Parallel
dss01:472:834 [7] NCCL INFO comm 0x7ffe98007590 rank 7 nranks 8 cudaDev 7 nvmlDev 7 - Init COMPLETE
dss01:466:833 [1] NCCL INFO comm 0x7fff40007590 rank 1 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE
0: Worker 0 is using worker seed: 345481642
0: Building vocabulary from /data/vocab.bpe.32000
dss01:467:828 [2] NCCL INFO comm 0x7fff4c007590 rank 2 nranks 8 cudaDev 2 nvmlDev 2 - Init COMPLETE
dss01:468:830 [3] NCCL INFO comm 0x7fff6c007590 rank 3 nranks 8 cudaDev 3 nvmlDev 3 - Init COMPLETE
dss01:470:831 [5] NCCL INFO comm 0x7ffe98007590 rank 5 nranks 8 cudaDev 5 nvmlDev 5 - Init COMPLETE
dss01:469:829 [4] NCCL INFO comm 0x7ffe98007590 rank 4 nranks 8 cudaDev 4 nvmlDev 4 - Init COMPLETE
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1570200575.207 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1570200578.731 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1570200578.731 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1570200578.732 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1570200579.767 global_batch_size: {"value": 2048, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1570200579.769 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1570200579.769 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1570200579.769 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1570200579.770 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1570200579.770 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1570200579.770 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1570200579.770 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1570200579.821 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1570200579.821 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 3452520939
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/1938]	Time 1.458 (1.458)	Data 7.80e-01 (7.80e-01)	Tok/s 7128 (7128)	Loss/tok 10.7213 (10.7213)	LR 2.000e-05
0: TRAIN [0][10/1938]	Time 0.658 (0.774)	Data 1.17e-04 (7.10e-02)	Tok/s 15829 (20313)	Loss/tok 9.7393 (10.2067)	LR 2.518e-05
0: TRAIN [0][20/1938]	Time 0.650 (0.741)	Data 1.24e-04 (3.73e-02)	Tok/s 16022 (20855)	Loss/tok 9.2828 (9.8736)	LR 3.170e-05
0: TRAIN [0][30/1938]	Time 0.660 (0.723)	Data 1.14e-04 (2.53e-02)	Tok/s 15582 (20369)	Loss/tok 8.9836 (9.6722)	LR 3.991e-05
0: TRAIN [0][40/1938]	Time 0.609 (0.717)	Data 1.50e-04 (1.92e-02)	Tok/s 8626 (20374)	Loss/tok 8.5355 (9.4975)	LR 5.024e-05
0: TRAIN [0][50/1938]	Time 0.780 (0.717)	Data 1.13e-04 (1.54e-02)	Tok/s 29858 (20908)	Loss/tok 8.8565 (9.3353)	LR 6.325e-05
0: TRAIN [0][60/1938]	Time 0.661 (0.709)	Data 1.30e-04 (1.29e-02)	Tok/s 15314 (20310)	Loss/tok 8.4327 (9.2210)	LR 7.962e-05
0: TRAIN [0][70/1938]	Time 0.783 (0.707)	Data 1.33e-04 (1.11e-02)	Tok/s 29392 (20414)	Loss/tok 8.4169 (9.1038)	LR 1.002e-04
0: TRAIN [0][80/1938]	Time 0.657 (0.702)	Data 1.07e-04 (9.76e-03)	Tok/s 15907 (20052)	Loss/tok 8.0057 (8.9988)	LR 1.262e-04
0: TRAIN [0][90/1938]	Time 0.651 (0.697)	Data 1.26e-04 (8.70e-03)	Tok/s 15986 (19697)	Loss/tok 7.9165 (8.9033)	LR 1.589e-04
0: TRAIN [0][100/1938]	Time 0.775 (0.698)	Data 1.01e-04 (7.85e-03)	Tok/s 30263 (19871)	Loss/tok 8.0929 (8.8052)	LR 2.000e-04
0: TRAIN [0][110/1938]	Time 0.788 (0.700)	Data 1.33e-04 (7.16e-03)	Tok/s 29760 (20164)	Loss/tok 8.0703 (8.7149)	LR 2.518e-04
0: TRAIN [0][120/1938]	Time 0.657 (0.697)	Data 1.18e-04 (6.58e-03)	Tok/s 15664 (19863)	Loss/tok 7.7356 (8.6557)	LR 3.170e-04
0: TRAIN [0][130/1938]	Time 0.658 (0.696)	Data 1.13e-04 (6.09e-03)	Tok/s 15727 (19842)	Loss/tok 7.6864 (8.5946)	LR 3.991e-04
0: TRAIN [0][140/1938]	Time 0.716 (0.697)	Data 1.14e-04 (5.66e-03)	Tok/s 23808 (19983)	Loss/tok 7.9485 (8.5363)	LR 5.024e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][150/1938]	Time 0.717 (0.698)	Data 1.02e-04 (5.30e-03)	Tok/s 23325 (20197)	Loss/tok 7.7851 (8.4782)	LR 6.181e-04
0: TRAIN [0][160/1938]	Time 0.649 (0.696)	Data 1.30e-04 (4.98e-03)	Tok/s 15744 (20014)	Loss/tok 7.4460 (8.4321)	LR 7.781e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][170/1938]	Time 0.705 (0.697)	Data 2.69e-04 (4.69e-03)	Tok/s 23826 (20152)	Loss/tok 7.5378 (8.3858)	LR 9.573e-04
0: TRAIN [0][180/1938]	Time 0.715 (0.696)	Data 1.51e-04 (4.44e-03)	Tok/s 23549 (20123)	Loss/tok 7.4367 (8.3347)	LR 1.205e-03
0: TRAIN [0][190/1938]	Time 0.596 (0.695)	Data 2.92e-04 (4.22e-03)	Tok/s 8815 (20004)	Loss/tok 6.2837 (8.2853)	LR 1.517e-03
0: TRAIN [0][200/1938]	Time 0.775 (0.696)	Data 1.15e-04 (4.01e-03)	Tok/s 30116 (20106)	Loss/tok 7.2602 (8.2235)	LR 1.910e-03
0: TRAIN [0][210/1938]	Time 0.661 (0.694)	Data 1.29e-04 (3.83e-03)	Tok/s 15612 (19941)	Loss/tok 6.7748 (8.1739)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.655 (0.694)	Data 1.08e-04 (3.66e-03)	Tok/s 15841 (19963)	Loss/tok 6.5287 (8.1089)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.652 (0.694)	Data 1.06e-04 (3.51e-03)	Tok/s 15775 (20013)	Loss/tok 6.3008 (8.0419)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.777 (0.694)	Data 1.05e-04 (3.37e-03)	Tok/s 30269 (20081)	Loss/tok 6.7307 (7.9737)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.710 (0.694)	Data 1.11e-04 (3.24e-03)	Tok/s 23622 (20050)	Loss/tok 6.3015 (7.9120)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.772 (0.695)	Data 1.26e-04 (3.12e-03)	Tok/s 30330 (20178)	Loss/tok 6.4782 (7.8407)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.774 (0.694)	Data 1.13e-04 (3.01e-03)	Tok/s 30062 (20048)	Loss/tok 6.3912 (7.7875)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.710 (0.694)	Data 1.23e-04 (2.91e-03)	Tok/s 23852 (20123)	Loss/tok 6.1165 (7.7204)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.719 (0.695)	Data 1.32e-04 (2.81e-03)	Tok/s 23569 (20247)	Loss/tok 6.0206 (7.6506)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.850 (0.695)	Data 1.05e-04 (2.72e-03)	Tok/s 35233 (20196)	Loss/tok 6.2116 (7.5951)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.710 (0.694)	Data 1.03e-04 (2.64e-03)	Tok/s 23795 (20144)	Loss/tok 5.7365 (7.5412)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.705 (0.695)	Data 1.70e-04 (2.57e-03)	Tok/s 23789 (20258)	Loss/tok 5.7510 (7.4744)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.777 (0.695)	Data 1.13e-04 (2.49e-03)	Tok/s 29782 (20258)	Loss/tok 5.8557 (7.4176)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.714 (0.696)	Data 1.02e-04 (2.42e-03)	Tok/s 23754 (20368)	Loss/tok 5.4773 (7.3513)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.600 (0.697)	Data 1.10e-04 (2.36e-03)	Tok/s 8836 (20417)	Loss/tok 4.2456 (7.2917)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.663 (0.696)	Data 1.20e-04 (2.30e-03)	Tok/s 15474 (20396)	Loss/tok 4.9853 (7.2391)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.711 (0.697)	Data 1.15e-04 (2.24e-03)	Tok/s 23731 (20455)	Loss/tok 5.3193 (7.1824)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.769 (0.697)	Data 1.19e-04 (2.18e-03)	Tok/s 30558 (20497)	Loss/tok 5.3951 (7.1282)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.601 (0.697)	Data 6.25e-04 (2.13e-03)	Tok/s 8841 (20490)	Loss/tok 3.9498 (7.0771)	LR 2.000e-03
0: TRAIN [0][400/1938]	Time 0.599 (0.697)	Data 1.26e-04 (2.08e-03)	Tok/s 8856 (20503)	Loss/tok 3.9166 (7.0238)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.712 (0.698)	Data 9.80e-05 (2.03e-03)	Tok/s 23473 (20562)	Loss/tok 4.8427 (6.9695)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.653 (0.697)	Data 1.22e-04 (1.99e-03)	Tok/s 15667 (20516)	Loss/tok 4.4736 (6.9242)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.655 (0.697)	Data 1.21e-04 (1.95e-03)	Tok/s 15519 (20473)	Loss/tok 4.3527 (6.8792)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.655 (0.697)	Data 1.09e-04 (1.90e-03)	Tok/s 15486 (20434)	Loss/tok 4.5119 (6.8344)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.775 (0.696)	Data 9.85e-05 (1.87e-03)	Tok/s 30134 (20400)	Loss/tok 4.9027 (6.7903)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.715 (0.697)	Data 1.34e-04 (1.83e-03)	Tok/s 24033 (20503)	Loss/tok 4.5937 (6.7349)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.658 (0.697)	Data 1.07e-04 (1.79e-03)	Tok/s 15619 (20515)	Loss/tok 4.1818 (6.6870)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.651 (0.696)	Data 1.01e-04 (1.76e-03)	Tok/s 15929 (20452)	Loss/tok 4.2063 (6.6491)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.598 (0.696)	Data 2.23e-04 (1.72e-03)	Tok/s 8663 (20430)	Loss/tok 3.5633 (6.6079)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.854 (0.697)	Data 1.28e-04 (1.69e-03)	Tok/s 35041 (20490)	Loss/tok 4.8997 (6.5603)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.711 (0.697)	Data 1.48e-04 (1.66e-03)	Tok/s 23554 (20508)	Loss/tok 4.3729 (6.5167)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.775 (0.697)	Data 1.33e-04 (1.63e-03)	Tok/s 30149 (20475)	Loss/tok 4.5548 (6.4787)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.652 (0.696)	Data 1.17e-04 (1.60e-03)	Tok/s 16245 (20417)	Loss/tok 4.1016 (6.4448)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.657 (0.696)	Data 1.20e-04 (1.58e-03)	Tok/s 15939 (20401)	Loss/tok 4.1035 (6.4083)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.665 (0.696)	Data 1.34e-04 (1.55e-03)	Tok/s 15235 (20357)	Loss/tok 4.1470 (6.3747)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.710 (0.696)	Data 1.05e-04 (1.53e-03)	Tok/s 23576 (20391)	Loss/tok 4.2548 (6.3344)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.663 (0.696)	Data 1.41e-04 (1.50e-03)	Tok/s 15530 (20412)	Loss/tok 3.8914 (6.2971)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.600 (0.696)	Data 1.19e-04 (1.48e-03)	Tok/s 9022 (20376)	Loss/tok 3.3844 (6.2650)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.711 (0.696)	Data 1.17e-04 (1.46e-03)	Tok/s 23601 (20368)	Loss/tok 4.2250 (6.2312)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.778 (0.696)	Data 1.33e-04 (1.43e-03)	Tok/s 29759 (20414)	Loss/tok 4.4118 (6.1930)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.711 (0.697)	Data 1.48e-04 (1.41e-03)	Tok/s 23602 (20484)	Loss/tok 4.2472 (6.1537)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][620/1938]	Time 0.601 (0.697)	Data 9.99e-05 (1.39e-03)	Tok/s 8688 (20472)	Loss/tok 3.3664 (6.1235)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.712 (0.697)	Data 1.05e-04 (1.37e-03)	Tok/s 23780 (20513)	Loss/tok 4.1792 (6.0892)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.709 (0.697)	Data 1.01e-04 (1.35e-03)	Tok/s 23849 (20533)	Loss/tok 4.1529 (6.0567)	LR 2.000e-03
0: TRAIN [0][650/1938]	Time 0.607 (0.696)	Data 1.49e-04 (1.33e-03)	Tok/s 8967 (20451)	Loss/tok 3.1553 (6.0344)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.708 (0.696)	Data 1.02e-04 (1.32e-03)	Tok/s 23643 (20471)	Loss/tok 4.0810 (6.0028)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.667 (0.697)	Data 1.28e-04 (1.30e-03)	Tok/s 15785 (20485)	Loss/tok 3.9927 (5.9744)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.656 (0.697)	Data 1.28e-04 (1.28e-03)	Tok/s 15862 (20495)	Loss/tok 3.7903 (5.9464)	LR 2.000e-03
0: TRAIN [0][690/1938]	Time 0.653 (0.697)	Data 1.02e-04 (1.26e-03)	Tok/s 15884 (20514)	Loss/tok 3.9028 (5.9185)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.667 (0.697)	Data 1.10e-04 (1.25e-03)	Tok/s 15780 (20547)	Loss/tok 3.8258 (5.8894)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.646 (0.697)	Data 1.03e-04 (1.23e-03)	Tok/s 16337 (20542)	Loss/tok 3.7633 (5.8639)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.652 (0.697)	Data 1.78e-04 (1.22e-03)	Tok/s 15958 (20587)	Loss/tok 3.7598 (5.8351)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.858 (0.698)	Data 1.48e-04 (1.20e-03)	Tok/s 34761 (20636)	Loss/tok 4.4959 (5.8074)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.715 (0.698)	Data 1.18e-04 (1.19e-03)	Tok/s 23354 (20617)	Loss/tok 4.0635 (5.7841)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.720 (0.698)	Data 1.07e-04 (1.17e-03)	Tok/s 23208 (20608)	Loss/tok 3.9294 (5.7608)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.601 (0.697)	Data 1.01e-04 (1.16e-03)	Tok/s 8754 (20572)	Loss/tok 3.1444 (5.7404)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.714 (0.697)	Data 1.16e-04 (1.15e-03)	Tok/s 23592 (20540)	Loss/tok 4.0012 (5.7199)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.643 (0.697)	Data 1.08e-04 (1.13e-03)	Tok/s 16009 (20525)	Loss/tok 3.6952 (5.6985)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.704 (0.697)	Data 1.24e-04 (1.12e-03)	Tok/s 23570 (20503)	Loss/tok 3.9771 (5.6777)	LR 2.000e-03
0: TRAIN [0][800/1938]	Time 0.722 (0.697)	Data 1.33e-04 (1.11e-03)	Tok/s 23478 (20503)	Loss/tok 3.9666 (5.6563)	LR 2.000e-03
0: TRAIN [0][810/1938]	Time 0.609 (0.697)	Data 1.27e-04 (1.10e-03)	Tok/s 8579 (20491)	Loss/tok 3.1667 (5.6362)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.657 (0.697)	Data 1.36e-04 (1.08e-03)	Tok/s 15664 (20499)	Loss/tok 3.7529 (5.6151)	LR 2.000e-03
0: TRAIN [0][830/1938]	Time 0.653 (0.697)	Data 1.26e-04 (1.07e-03)	Tok/s 15796 (20496)	Loss/tok 3.7405 (5.5955)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.600 (0.697)	Data 1.19e-04 (1.06e-03)	Tok/s 8823 (20484)	Loss/tok 3.0742 (5.5766)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.775 (0.697)	Data 1.03e-04 (1.05e-03)	Tok/s 30119 (20478)	Loss/tok 4.1684 (5.5576)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.849 (0.697)	Data 1.28e-04 (1.04e-03)	Tok/s 35512 (20468)	Loss/tok 4.2193 (5.5389)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.650 (0.697)	Data 1.03e-04 (1.03e-03)	Tok/s 15997 (20458)	Loss/tok 3.5445 (5.5208)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][880/1938]	Time 0.715 (0.696)	Data 1.01e-04 (1.02e-03)	Tok/s 23557 (20454)	Loss/tok 3.8601 (5.5028)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.660 (0.696)	Data 1.07e-04 (1.01e-03)	Tok/s 15624 (20446)	Loss/tok 3.6283 (5.4854)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.653 (0.696)	Data 1.21e-04 (9.99e-04)	Tok/s 15736 (20420)	Loss/tok 3.5075 (5.4691)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.601 (0.696)	Data 1.56e-04 (9.89e-04)	Tok/s 8907 (20389)	Loss/tok 3.0488 (5.4533)	LR 2.000e-03
0: TRAIN [0][920/1938]	Time 0.716 (0.696)	Data 1.15e-04 (9.80e-04)	Tok/s 23283 (20395)	Loss/tok 3.9324 (5.4355)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.648 (0.696)	Data 1.22e-04 (9.71e-04)	Tok/s 15870 (20378)	Loss/tok 3.6780 (5.4196)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.652 (0.696)	Data 1.10e-04 (9.62e-04)	Tok/s 16070 (20402)	Loss/tok 3.6032 (5.4014)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.707 (0.696)	Data 1.12e-04 (9.53e-04)	Tok/s 23737 (20408)	Loss/tok 3.8669 (5.3849)	LR 2.000e-03
0: TRAIN [0][960/1938]	Time 0.849 (0.697)	Data 1.29e-04 (9.44e-04)	Tok/s 35607 (20467)	Loss/tok 4.1992 (5.3648)	LR 2.000e-03
0: TRAIN [0][970/1938]	Time 0.717 (0.696)	Data 1.10e-04 (9.36e-04)	Tok/s 22997 (20443)	Loss/tok 3.8817 (5.3504)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.711 (0.697)	Data 1.26e-04 (9.27e-04)	Tok/s 23788 (20481)	Loss/tok 3.9419 (5.3327)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.600 (0.697)	Data 1.36e-04 (9.20e-04)	Tok/s 8688 (20473)	Loss/tok 3.0936 (5.3182)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.778 (0.697)	Data 1.17e-04 (9.13e-04)	Tok/s 30239 (20485)	Loss/tok 3.9547 (5.3022)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.778 (0.697)	Data 1.19e-04 (9.05e-04)	Tok/s 29898 (20490)	Loss/tok 3.9627 (5.2868)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.597 (0.696)	Data 1.16e-04 (8.98e-04)	Tok/s 8650 (20459)	Loss/tok 3.0411 (5.2740)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.658 (0.696)	Data 1.26e-04 (8.90e-04)	Tok/s 15912 (20471)	Loss/tok 3.5362 (5.2588)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.660 (0.696)	Data 1.18e-04 (8.83e-04)	Tok/s 15707 (20434)	Loss/tok 3.5245 (5.2474)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.650 (0.696)	Data 1.04e-04 (8.75e-04)	Tok/s 15914 (20456)	Loss/tok 3.4369 (5.2324)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.713 (0.696)	Data 1.21e-04 (8.68e-04)	Tok/s 23824 (20446)	Loss/tok 3.8189 (5.2195)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.650 (0.696)	Data 1.06e-04 (8.61e-04)	Tok/s 15938 (20464)	Loss/tok 3.5651 (5.2048)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.715 (0.696)	Data 1.26e-04 (8.55e-04)	Tok/s 23335 (20466)	Loss/tok 3.8878 (5.1917)	LR 2.000e-03
