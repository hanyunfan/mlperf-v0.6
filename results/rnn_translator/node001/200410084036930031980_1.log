Beginning trial 1 of 1
Gathering sys log on node001
:::MLL 1586526105.214 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1586526105.215 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1586526105.216 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1586526105.216 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1586526105.217 submission_platform: {"value": "1xPowerEdge R7525", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1586526105.218 submission_entry: {"value": "{'hardware': 'PowerEdge R7525', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': ' ', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.0-0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x AMD EPYC 7502 32-Core Processor', 'num_cores': '64', 'num_vcpus': '64', 'accelerator': 'Tesla V100-PCIE-32GB', 'num_accelerators': '3', 'sys_mem_size': '251 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '1x 931.5G', 'cpu_accel_interconnect': 'QPI', 'network_card': '', 'num_network_cards': '0', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1586526105.219 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1586526105.220 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1586526106.409 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node node001
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=5123' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=600 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=5 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=200410084036930031980 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_200410084036930031980 ./run_and_time.sh
Run vars: id 200410084036930031980 gpus 3 mparams  --master_port=5123
NCCL_SOCKET_NTHREADS=2
NCCL_NSOCKS_PERTHREAD=8
STARTING TIMING RUN AT 2020-04-10 01:41:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=600
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=5
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=0
+ MATH=amp_fp16
+ [[ 0 -eq 1 ]]
+ LAUNCH_OPT='torch.distributed.launch --nproc_per_node 3  --master_port=5123'
+ echo 'running benchmark'
running benchmark
+ python -m torch.distributed.launch --nproc_per_node 3 --master_port=5123 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 5 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 600 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1586526109.830 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1586526109.830 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1586526109.843 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=8, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=5, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=600)
0: L2 promotion: 128B
0: Using random master seed: 985267465
node001:588:588 [0] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.1<0>
node001:588:588 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node001:588:588 [0] NCCL INFO NET/IB : No device found.
node001:588:588 [0] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.1<0>
node001:588:588 [0] NCCL INFO Using network Socket
NCCL version 2.6.4+cuda10.2
node001:589:589 [1] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.1<0>
node001:589:589 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node001:589:589 [1] NCCL INFO NET/IB : No device found.
node001:589:589 [1] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.1<0>
node001:589:589 [1] NCCL INFO Using network Socket
node001:590:590 [2] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.1<0>
node001:590:590 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node001:590:590 [2] NCCL INFO NET/IB : No device found.
node001:590:590 [2] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.1<0>
node001:590:590 [2] NCCL INFO Using network Socket
node001:588:797 [0] NCCL INFO Channel 00/02 :    0   1   2
node001:590:799 [2] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 8/8/64
node001:589:798 [1] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 8/8/64
node001:588:797 [0] NCCL INFO Channel 01/02 :    0   1   2
node001:590:799 [2] NCCL INFO Trees [0] -1/-1/-1->2->1|1->2->-1/-1/-1 [1] -1/-1/-1->2->1|1->2->-1/-1/-1
node001:589:798 [1] NCCL INFO Trees [0] 2/-1/-1->1->0|0->1->2/-1/-1 [1] 2/-1/-1->1->0|0->1->2/-1/-1
node001:589:798 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000
node001:590:799 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000
node001:588:797 [0] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 8/8/64
node001:588:797 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 1/-1/-1->0->-1|-1->0->1/-1/-1
node001:588:797 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff
node001:590:799 [2] NCCL INFO Ring 00 : 2[e2000] -> 0[21000] via direct shared memory
node001:588:797 [0] NCCL INFO Ring 00 : 0[21000] -> 1[81000] via direct shared memory
node001:589:798 [1] NCCL INFO Ring 00 : 1[81000] -> 2[e2000] via P2P/IPC
node001:590:799 [2] NCCL INFO Ring 00 : 2[e2000] -> 1[81000] via P2P/IPC
node001:589:798 [1] NCCL INFO Ring 00 : 1[81000] -> 0[21000] via direct shared memory
node001:590:799 [2] NCCL INFO Ring 01 : 2[e2000] -> 0[21000] via direct shared memory
node001:588:797 [0] NCCL INFO Ring 01 : 0[21000] -> 1[81000] via direct shared memory
node001:589:798 [1] NCCL INFO Ring 01 : 1[81000] -> 2[e2000] via P2P/IPC
node001:590:799 [2] NCCL INFO Ring 01 : 2[e2000] -> 1[81000] via P2P/IPC
node001:589:798 [1] NCCL INFO Ring 01 : 1[81000] -> 0[21000] via direct shared memory
node001:590:799 [2] NCCL INFO comm 0x7fff0c006620 rank 2 nranks 3 cudaDev 2 busId e2000 - Init COMPLETE
node001:588:797 [0] NCCL INFO comm 0x7ffe54006620 rank 0 nranks 3 cudaDev 0 busId 21000 - Init COMPLETE
node001:588:588 [0] NCCL INFO Launch mode Parallel
node001:589:798 [1] NCCL INFO comm 0x7ffefc006620 rank 1 nranks 3 cudaDev 1 busId 81000 - Init COMPLETE
0: Worker 0 is using worker seed: 1378773584
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1586526119.539 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1586526121.110 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 407}}
:::MLL 1586526121.110 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 409}}
:::MLL 1586526121.111 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 414}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1586526121.975 global_batch_size: {"value": 768, "metadata": {"file": "train.py", "lineno": 459}}
0: Training LR schedule config: {'warmup_steps': 600, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 8, 'decay_factor': 0.5}
0: Scheduler warmup steps: 600
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 8
:::MLL 1586526121.977 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1586526121.977 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1586526121.978 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1586526121.978 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1586526121.978 opt_learning_rate_decay_steps: {"value": 8, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1586526121.979 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1586526121.979 opt_learning_rate_warmup_steps: {"value": 600, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1586526121.982 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1586526121.983 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 515}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 3489695884
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/5173]	Time 1.249 (1.249)	Data 5.76e-01 (5.76e-01)	Tok/s 13550 (13550)	Loss/tok 10.6513 (10.6513)	LR 2.000e-05
0: TRAIN [0][10/5173]	Time 0.689 (0.674)	Data 1.28e-04 (5.25e-02)	Tok/s 33674 (23690)	Loss/tok 9.8091 (10.1907)	LR 2.160e-05
0: TRAIN [0][20/5173]	Time 0.563 (0.630)	Data 1.20e-04 (2.76e-02)	Tok/s 18152 (22431)	Loss/tok 9.3227 (9.9043)	LR 2.332e-05
0: TRAIN [0][30/5173]	Time 0.560 (0.608)	Data 1.18e-04 (1.87e-02)	Tok/s 18431 (21133)	Loss/tok 9.0463 (9.7157)	LR 2.518e-05
0: TRAIN [0][40/5173]	Time 0.563 (0.613)	Data 1.32e-04 (1.42e-02)	Tok/s 18157 (22309)	Loss/tok 8.9708 (9.5409)	LR 2.719e-05
0: TRAIN [0][50/5173]	Time 0.617 (0.620)	Data 1.22e-04 (1.14e-02)	Tok/s 27165 (23690)	Loss/tok 8.8394 (9.3932)	LR 2.936e-05
0: TRAIN [0][60/5173]	Time 0.566 (0.618)	Data 1.23e-04 (9.57e-03)	Tok/s 18076 (23734)	Loss/tok 8.6419 (9.2951)	LR 3.170e-05
0: TRAIN [0][70/5173]	Time 0.623 (0.619)	Data 1.31e-04 (8.24e-03)	Tok/s 26716 (23974)	Loss/tok 8.6297 (9.2024)	LR 3.423e-05
0: TRAIN [0][80/5173]	Time 0.689 (0.617)	Data 1.17e-04 (7.24e-03)	Tok/s 33672 (23962)	Loss/tok 8.5896 (9.1198)	LR 3.696e-05
0: TRAIN [0][90/5173]	Time 0.624 (0.616)	Data 1.20e-04 (6.46e-03)	Tok/s 27306 (23965)	Loss/tok 8.3197 (9.0402)	LR 3.991e-05
0: TRAIN [0][100/5173]	Time 0.507 (0.612)	Data 1.10e-04 (5.83e-03)	Tok/s 10350 (23560)	Loss/tok 7.7450 (8.9774)	LR 4.309e-05
0: TRAIN [0][110/5173]	Time 0.623 (0.612)	Data 1.29e-04 (5.32e-03)	Tok/s 27232 (23594)	Loss/tok 8.2072 (8.9113)	LR 4.653e-05
0: TRAIN [0][120/5173]	Time 0.689 (0.613)	Data 1.44e-04 (4.89e-03)	Tok/s 34210 (23827)	Loss/tok 8.2184 (8.8415)	LR 5.024e-05
0: TRAIN [0][130/5173]	Time 0.628 (0.614)	Data 1.28e-04 (4.53e-03)	Tok/s 26643 (24003)	Loss/tok 8.0989 (8.7812)	LR 5.425e-05
0: TRAIN [0][140/5173]	Time 0.566 (0.611)	Data 1.34e-04 (4.22e-03)	Tok/s 18704 (23668)	Loss/tok 7.9252 (8.7380)	LR 5.857e-05
0: TRAIN [0][150/5173]	Time 0.567 (0.611)	Data 1.29e-04 (3.95e-03)	Tok/s 18235 (23630)	Loss/tok 7.8580 (8.6891)	LR 6.325e-05
0: TRAIN [0][160/5173]	Time 0.566 (0.609)	Data 1.31e-04 (3.71e-03)	Tok/s 18587 (23461)	Loss/tok 7.7827 (8.6484)	LR 6.829e-05
0: TRAIN [0][170/5173]	Time 0.565 (0.610)	Data 1.31e-04 (3.50e-03)	Tok/s 18314 (23635)	Loss/tok 7.7916 (8.6037)	LR 7.374e-05
0: TRAIN [0][180/5173]	Time 0.761 (0.610)	Data 1.29e-04 (3.32e-03)	Tok/s 39351 (23653)	Loss/tok 8.1646 (8.5662)	LR 7.962e-05
0: TRAIN [0][190/5173]	Time 0.567 (0.610)	Data 1.32e-04 (3.15e-03)	Tok/s 18528 (23567)	Loss/tok 7.7098 (8.5327)	LR 8.597e-05
0: TRAIN [0][200/5173]	Time 0.571 (0.609)	Data 1.29e-04 (3.00e-03)	Tok/s 17653 (23501)	Loss/tok 7.7375 (8.4999)	LR 9.283e-05
0: TRAIN [0][210/5173]	Time 0.624 (0.610)	Data 1.34e-04 (2.87e-03)	Tok/s 26994 (23689)	Loss/tok 7.7999 (8.4671)	LR 1.002e-04
0: TRAIN [0][220/5173]	Time 0.568 (0.610)	Data 1.22e-04 (2.74e-03)	Tok/s 18519 (23640)	Loss/tok 7.6532 (8.4392)	LR 1.082e-04
0: TRAIN [0][230/5173]	Time 0.690 (0.609)	Data 1.26e-04 (2.63e-03)	Tok/s 33720 (23596)	Loss/tok 8.0515 (8.4135)	LR 1.169e-04
0: TRAIN [0][240/5173]	Time 0.765 (0.611)	Data 1.32e-04 (2.53e-03)	Tok/s 39093 (23770)	Loss/tok 8.0288 (8.3860)	LR 1.262e-04
0: TRAIN [0][250/5173]	Time 0.692 (0.612)	Data 1.24e-04 (2.43e-03)	Tok/s 33214 (23859)	Loss/tok 7.9476 (8.3607)	LR 1.363e-04
0: TRAIN [0][260/5173]	Time 0.566 (0.611)	Data 1.22e-04 (2.35e-03)	Tok/s 18151 (23866)	Loss/tok 7.5003 (8.3378)	LR 1.471e-04
0: TRAIN [0][270/5173]	Time 0.562 (0.610)	Data 1.25e-04 (2.26e-03)	Tok/s 18160 (23725)	Loss/tok 7.6210 (8.3188)	LR 1.589e-04
0: TRAIN [0][280/5173]	Time 0.626 (0.609)	Data 1.20e-04 (2.19e-03)	Tok/s 26681 (23649)	Loss/tok 7.8628 (8.3015)	LR 1.715e-04
0: TRAIN [0][290/5173]	Time 0.623 (0.609)	Data 1.16e-04 (2.12e-03)	Tok/s 26958 (23648)	Loss/tok 7.7630 (8.2810)	LR 1.852e-04
0: TRAIN [0][300/5173]	Time 0.621 (0.610)	Data 1.37e-04 (2.05e-03)	Tok/s 27128 (23717)	Loss/tok 7.6580 (8.2591)	LR 2.000e-04
0: TRAIN [0][310/5173]	Time 0.474 (0.610)	Data 1.21e-04 (1.99e-03)	Tok/s 11171 (23720)	Loss/tok 6.7740 (8.2380)	LR 2.160e-04
0: TRAIN [0][320/5173]	Time 0.689 (0.610)	Data 1.21e-04 (1.93e-03)	Tok/s 33759 (23805)	Loss/tok 7.7648 (8.2178)	LR 2.332e-04
0: TRAIN [0][330/5173]	Time 0.625 (0.610)	Data 1.29e-04 (1.88e-03)	Tok/s 27186 (23811)	Loss/tok 7.5818 (8.1979)	LR 2.518e-04
0: TRAIN [0][340/5173]	Time 0.768 (0.611)	Data 1.28e-04 (1.83e-03)	Tok/s 38628 (23823)	Loss/tok 7.8071 (8.1783)	LR 2.719e-04
0: TRAIN [0][350/5173]	Time 0.687 (0.611)	Data 1.21e-04 (1.78e-03)	Tok/s 34040 (23824)	Loss/tok 7.6580 (8.1596)	LR 2.936e-04
0: TRAIN [0][360/5173]	Time 0.491 (0.610)	Data 1.29e-04 (1.73e-03)	Tok/s 10645 (23798)	Loss/tok 6.4873 (8.1404)	LR 3.170e-04
0: TRAIN [0][370/5173]	Time 0.627 (0.610)	Data 1.29e-04 (1.69e-03)	Tok/s 26623 (23707)	Loss/tok 7.4074 (8.1231)	LR 3.423e-04
0: TRAIN [0][380/5173]	Time 0.633 (0.610)	Data 1.22e-04 (1.65e-03)	Tok/s 26188 (23754)	Loss/tok 7.3522 (8.1027)	LR 3.696e-04
0: TRAIN [0][390/5173]	Time 0.555 (0.609)	Data 1.22e-04 (1.61e-03)	Tok/s 18861 (23661)	Loss/tok 7.0336 (8.0834)	LR 3.991e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][400/5173]	Time 0.568 (0.610)	Data 1.44e-04 (1.57e-03)	Tok/s 18102 (23726)	Loss/tok 6.9763 (8.0613)	LR 4.276e-04
0: TRAIN [0][410/5173]	Time 0.631 (0.611)	Data 1.21e-04 (1.54e-03)	Tok/s 26289 (23819)	Loss/tok 7.0886 (8.0362)	LR 4.617e-04
0: TRAIN [0][420/5173]	Time 0.693 (0.611)	Data 1.23e-04 (1.51e-03)	Tok/s 33639 (23879)	Loss/tok 7.2347 (8.0116)	LR 4.985e-04
0: TRAIN [0][430/5173]	Time 0.565 (0.610)	Data 1.29e-04 (1.47e-03)	Tok/s 18404 (23794)	Loss/tok 6.7649 (7.9910)	LR 5.383e-04
0: TRAIN [0][440/5173]	Time 0.566 (0.610)	Data 1.22e-04 (1.44e-03)	Tok/s 18211 (23803)	Loss/tok 6.7223 (7.9678)	LR 5.813e-04
0: TRAIN [0][450/5173]	Time 0.627 (0.610)	Data 1.24e-04 (1.42e-03)	Tok/s 26596 (23803)	Loss/tok 6.8913 (7.9440)	LR 6.276e-04
0: TRAIN [0][460/5173]	Time 0.565 (0.610)	Data 3.08e-04 (1.39e-03)	Tok/s 17958 (23746)	Loss/tok 6.5388 (7.9216)	LR 6.777e-04
0: TRAIN [0][470/5173]	Time 0.757 (0.611)	Data 1.64e-04 (1.36e-03)	Tok/s 39456 (23880)	Loss/tok 7.0709 (7.8936)	LR 7.318e-04
0: TRAIN [0][480/5173]	Time 0.627 (0.611)	Data 1.19e-04 (1.34e-03)	Tok/s 26407 (23906)	Loss/tok 6.8296 (7.8692)	LR 7.901e-04
0: TRAIN [0][490/5173]	Time 0.675 (0.611)	Data 1.29e-04 (1.31e-03)	Tok/s 34921 (23905)	Loss/tok 6.9039 (7.8463)	LR 8.532e-04
0: TRAIN [0][500/5173]	Time 0.627 (0.611)	Data 1.24e-04 (1.29e-03)	Tok/s 26394 (23874)	Loss/tok 6.5712 (7.8231)	LR 9.212e-04
0: TRAIN [0][510/5173]	Time 0.690 (0.611)	Data 1.21e-04 (1.27e-03)	Tok/s 33925 (23842)	Loss/tok 6.8384 (7.8009)	LR 9.947e-04
0: TRAIN [0][520/5173]	Time 0.691 (0.611)	Data 1.26e-04 (1.24e-03)	Tok/s 33677 (23861)	Loss/tok 6.7709 (7.7762)	LR 1.074e-03
0: TRAIN [0][530/5173]	Time 0.628 (0.611)	Data 1.24e-04 (1.22e-03)	Tok/s 26313 (23846)	Loss/tok 6.4786 (7.7532)	LR 1.160e-03
0: TRAIN [0][540/5173]	Time 0.568 (0.610)	Data 1.24e-04 (1.20e-03)	Tok/s 18284 (23814)	Loss/tok 6.2773 (7.7306)	LR 1.252e-03
0: TRAIN [0][550/5173]	Time 0.567 (0.611)	Data 1.25e-04 (1.18e-03)	Tok/s 18337 (23842)	Loss/tok 6.1947 (7.7055)	LR 1.352e-03
0: TRAIN [0][560/5173]	Time 0.692 (0.611)	Data 1.27e-04 (1.17e-03)	Tok/s 33728 (23856)	Loss/tok 6.4608 (7.6813)	LR 1.460e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][570/5173]	Time 0.625 (0.611)	Data 1.22e-04 (1.15e-03)	Tok/s 27517 (23863)	Loss/tok 6.3561 (7.6570)	LR 1.564e-03
0: TRAIN [0][580/5173]	Time 0.564 (0.611)	Data 1.30e-04 (1.13e-03)	Tok/s 18047 (23850)	Loss/tok 6.0249 (7.6344)	LR 1.689e-03
0: TRAIN [0][590/5173]	Time 0.624 (0.611)	Data 1.36e-04 (1.11e-03)	Tok/s 27086 (23908)	Loss/tok 6.2074 (7.6077)	LR 1.824e-03
0: TRAIN [0][600/5173]	Time 0.627 (0.611)	Data 1.23e-04 (1.10e-03)	Tok/s 26788 (23878)	Loss/tok 6.1764 (7.5853)	LR 1.970e-03
0: TRAIN [0][610/5173]	Time 0.629 (0.611)	Data 1.34e-04 (1.08e-03)	Tok/s 26626 (23927)	Loss/tok 6.1592 (7.5583)	LR 2.000e-03
0: TRAIN [0][620/5173]	Time 0.766 (0.612)	Data 1.32e-04 (1.07e-03)	Tok/s 39028 (24015)	Loss/tok 6.3445 (7.5307)	LR 2.000e-03
0: TRAIN [0][630/5173]	Time 0.622 (0.612)	Data 1.27e-04 (1.05e-03)	Tok/s 26959 (24011)	Loss/tok 6.0514 (7.5072)	LR 2.000e-03
0: TRAIN [0][640/5173]	Time 0.505 (0.611)	Data 1.24e-04 (1.04e-03)	Tok/s 10482 (23951)	Loss/tok 4.7410 (7.4854)	LR 2.000e-03
0: TRAIN [0][650/5173]	Time 0.567 (0.612)	Data 1.25e-04 (1.02e-03)	Tok/s 18296 (24008)	Loss/tok 5.5916 (7.4561)	LR 2.000e-03
0: TRAIN [0][660/5173]	Time 0.568 (0.612)	Data 1.21e-04 (1.01e-03)	Tok/s 18252 (23990)	Loss/tok 5.5953 (7.4327)	LR 2.000e-03
0: TRAIN [0][670/5173]	Time 0.628 (0.611)	Data 1.20e-04 (9.97e-04)	Tok/s 26592 (23931)	Loss/tok 5.7264 (7.4116)	LR 2.000e-03
0: TRAIN [0][680/5173]	Time 0.568 (0.610)	Data 1.33e-04 (9.85e-04)	Tok/s 18614 (23851)	Loss/tok 5.3730 (7.3909)	LR 2.000e-03
0: TRAIN [0][690/5173]	Time 0.629 (0.610)	Data 1.29e-04 (9.73e-04)	Tok/s 26815 (23824)	Loss/tok 5.6395 (7.3668)	LR 2.000e-03
0: TRAIN [0][700/5173]	Time 0.634 (0.610)	Data 1.19e-04 (9.61e-04)	Tok/s 26396 (23770)	Loss/tok 5.5840 (7.3445)	LR 2.000e-03
0: TRAIN [0][710/5173]	Time 0.504 (0.609)	Data 1.36e-04 (9.50e-04)	Tok/s 10504 (23716)	Loss/tok 4.3138 (7.3220)	LR 2.000e-03
0: TRAIN [0][720/5173]	Time 0.621 (0.609)	Data 1.27e-04 (9.38e-04)	Tok/s 27277 (23744)	Loss/tok 5.3399 (7.2939)	LR 2.000e-03
0: TRAIN [0][730/5173]	Time 0.763 (0.610)	Data 1.29e-04 (9.27e-04)	Tok/s 39011 (23757)	Loss/tok 5.7886 (7.2669)	LR 2.000e-03
0: TRAIN [0][740/5173]	Time 0.629 (0.610)	Data 1.24e-04 (9.17e-04)	Tok/s 26431 (23768)	Loss/tok 5.2536 (7.2387)	LR 2.000e-03
0: TRAIN [0][750/5173]	Time 0.568 (0.610)	Data 1.23e-04 (9.06e-04)	Tok/s 18038 (23783)	Loss/tok 4.7707 (7.2103)	LR 2.000e-03
0: TRAIN [0][760/5173]	Time 0.567 (0.610)	Data 1.20e-04 (8.96e-04)	Tok/s 17998 (23740)	Loss/tok 4.7521 (7.1869)	LR 2.000e-03
0: TRAIN [0][770/5173]	Time 0.568 (0.610)	Data 1.44e-04 (8.87e-04)	Tok/s 18426 (23809)	Loss/tok 4.7256 (7.1550)	LR 2.000e-03
0: TRAIN [0][780/5173]	Time 0.625 (0.610)	Data 1.23e-04 (8.77e-04)	Tok/s 26879 (23833)	Loss/tok 5.0642 (7.1254)	LR 2.000e-03
0: TRAIN [0][790/5173]	Time 0.675 (0.611)	Data 1.45e-04 (8.67e-04)	Tok/s 34508 (23884)	Loss/tok 5.1232 (7.0935)	LR 2.000e-03
0: TRAIN [0][800/5173]	Time 0.625 (0.611)	Data 1.23e-04 (8.58e-04)	Tok/s 26955 (23918)	Loss/tok 4.8442 (7.0632)	LR 2.000e-03
0: TRAIN [0][810/5173]	Time 0.633 (0.611)	Data 1.23e-04 (8.49e-04)	Tok/s 26252 (23934)	Loss/tok 4.9975 (7.0351)	LR 2.000e-03
0: TRAIN [0][820/5173]	Time 0.567 (0.612)	Data 1.34e-04 (8.41e-04)	Tok/s 17860 (23997)	Loss/tok 4.5173 (7.0028)	LR 2.000e-03
0: TRAIN [0][830/5173]	Time 0.567 (0.612)	Data 1.24e-04 (8.32e-04)	Tok/s 18321 (23977)	Loss/tok 4.5124 (6.9785)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][840/5173]	Time 0.686 (0.612)	Data 1.36e-04 (8.24e-04)	Tok/s 34216 (24041)	Loss/tok 4.9272 (6.9471)	LR 2.000e-03
0: TRAIN [0][850/5173]	Time 0.567 (0.612)	Data 1.28e-04 (8.16e-04)	Tok/s 18158 (24047)	Loss/tok 4.4412 (6.9209)	LR 2.000e-03
0: TRAIN [0][860/5173]	Time 0.564 (0.612)	Data 1.26e-04 (8.08e-04)	Tok/s 18232 (24055)	Loss/tok 4.4462 (6.8944)	LR 2.000e-03
0: TRAIN [0][870/5173]	Time 0.566 (0.612)	Data 1.25e-04 (8.01e-04)	Tok/s 18584 (24061)	Loss/tok 4.4641 (6.8691)	LR 2.000e-03
0: TRAIN [0][880/5173]	Time 0.567 (0.612)	Data 1.29e-04 (7.93e-04)	Tok/s 17984 (24068)	Loss/tok 4.3369 (6.8436)	LR 2.000e-03
0: TRAIN [0][890/5173]	Time 0.566 (0.613)	Data 1.37e-04 (7.86e-04)	Tok/s 18233 (24122)	Loss/tok 4.2861 (6.8142)	LR 2.000e-03
0: TRAIN [0][900/5173]	Time 0.628 (0.613)	Data 1.34e-04 (7.79e-04)	Tok/s 26802 (24116)	Loss/tok 4.6905 (6.7904)	LR 2.000e-03
0: TRAIN [0][910/5173]	Time 0.688 (0.613)	Data 1.26e-04 (7.72e-04)	Tok/s 34052 (24112)	Loss/tok 4.7083 (6.7664)	LR 2.000e-03
0: TRAIN [0][920/5173]	Time 0.570 (0.613)	Data 1.24e-04 (7.66e-04)	Tok/s 18347 (24134)	Loss/tok 4.1356 (6.7398)	LR 2.000e-03
0: TRAIN [0][930/5173]	Time 0.570 (0.613)	Data 1.37e-04 (7.59e-04)	Tok/s 18132 (24138)	Loss/tok 4.2228 (6.7164)	LR 2.000e-03
0: TRAIN [0][940/5173]	Time 0.631 (0.613)	Data 1.26e-04 (7.52e-04)	Tok/s 26527 (24146)	Loss/tok 4.5606 (6.6925)	LR 2.000e-03
0: TRAIN [0][950/5173]	Time 0.566 (0.613)	Data 1.23e-04 (7.46e-04)	Tok/s 18007 (24157)	Loss/tok 4.0732 (6.6683)	LR 2.000e-03
0: TRAIN [0][960/5173]	Time 0.556 (0.613)	Data 1.35e-04 (7.39e-04)	Tok/s 18489 (24133)	Loss/tok 4.1502 (6.6476)	LR 2.000e-03
0: TRAIN [0][970/5173]	Time 0.507 (0.613)	Data 1.31e-04 (7.33e-04)	Tok/s 10585 (24114)	Loss/tok 3.4608 (6.6267)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][980/5173]	Time 0.689 (0.613)	Data 1.26e-04 (7.27e-04)	Tok/s 33645 (24132)	Loss/tok 4.6502 (6.6033)	LR 2.000e-03
0: TRAIN [0][990/5173]	Time 0.694 (0.613)	Data 1.21e-04 (7.21e-04)	Tok/s 34113 (24159)	Loss/tok 4.5081 (6.5790)	LR 2.000e-03
0: TRAIN [0][1000/5173]	Time 0.569 (0.613)	Data 1.29e-04 (7.15e-04)	Tok/s 18186 (24148)	Loss/tok 4.1616 (6.5583)	LR 2.000e-03
0: TRAIN [0][1010/5173]	Time 0.567 (0.613)	Data 1.36e-04 (7.10e-04)	Tok/s 18538 (24163)	Loss/tok 4.0829 (6.5356)	LR 2.000e-03
0: TRAIN [0][1020/5173]	Time 0.565 (0.613)	Data 1.49e-04 (7.04e-04)	Tok/s 18266 (24144)	Loss/tok 3.9309 (6.5160)	LR 2.000e-03
0: TRAIN [0][1030/5173]	Time 0.568 (0.613)	Data 1.42e-04 (6.98e-04)	Tok/s 18179 (24150)	Loss/tok 4.0476 (6.4944)	LR 2.000e-03
0: TRAIN [0][1040/5173]	Time 0.567 (0.613)	Data 1.35e-04 (6.93e-04)	Tok/s 17984 (24122)	Loss/tok 4.0649 (6.4760)	LR 2.000e-03
0: TRAIN [0][1050/5173]	Time 0.631 (0.613)	Data 1.33e-04 (6.87e-04)	Tok/s 26716 (24154)	Loss/tok 4.3239 (6.4530)	LR 2.000e-03
0: TRAIN [0][1060/5173]	Time 0.567 (0.613)	Data 1.29e-04 (6.82e-04)	Tok/s 18406 (24114)	Loss/tok 3.9626 (6.4362)	LR 2.000e-03
0: TRAIN [0][1070/5173]	Time 0.571 (0.613)	Data 1.26e-04 (6.77e-04)	Tok/s 18558 (24128)	Loss/tok 3.9306 (6.4151)	LR 2.000e-03
0: TRAIN [0][1080/5173]	Time 0.505 (0.612)	Data 1.26e-04 (6.72e-04)	Tok/s 10221 (24085)	Loss/tok 3.4284 (6.3993)	LR 2.000e-03
0: TRAIN [0][1090/5173]	Time 0.625 (0.613)	Data 1.36e-04 (6.67e-04)	Tok/s 27051 (24108)	Loss/tok 4.2326 (6.3788)	LR 2.000e-03
0: TRAIN [0][1100/5173]	Time 0.566 (0.612)	Data 1.33e-04 (6.63e-04)	Tok/s 18119 (24063)	Loss/tok 3.9485 (6.3635)	LR 2.000e-03
0: TRAIN [0][1110/5173]	Time 0.631 (0.612)	Data 1.31e-04 (6.58e-04)	Tok/s 26446 (24053)	Loss/tok 4.3755 (6.3454)	LR 2.000e-03
0: TRAIN [0][1120/5173]	Time 0.692 (0.612)	Data 3.21e-04 (6.54e-04)	Tok/s 33792 (24065)	Loss/tok 4.5707 (6.3256)	LR 2.000e-03
0: TRAIN [0][1130/5173]	Time 0.564 (0.612)	Data 1.28e-04 (6.49e-04)	Tok/s 18677 (24039)	Loss/tok 3.9624 (6.3094)	LR 2.000e-03
0: TRAIN [0][1140/5173]	Time 0.505 (0.612)	Data 1.28e-04 (6.44e-04)	Tok/s 10151 (23997)	Loss/tok 3.2900 (6.2946)	LR 2.000e-03
0: TRAIN [0][1150/5173]	Time 0.632 (0.612)	Data 1.50e-04 (6.40e-04)	Tok/s 26440 (23976)	Loss/tok 4.1213 (6.2785)	LR 2.000e-03
0: TRAIN [0][1160/5173]	Time 0.631 (0.611)	Data 1.43e-04 (6.36e-04)	Tok/s 26633 (23955)	Loss/tok 4.1168 (6.2625)	LR 2.000e-03
0: TRAIN [0][1170/5173]	Time 0.563 (0.611)	Data 1.34e-04 (6.32e-04)	Tok/s 18608 (23921)	Loss/tok 3.9813 (6.2477)	LR 2.000e-03
0: TRAIN [0][1180/5173]	Time 0.567 (0.611)	Data 1.26e-04 (6.27e-04)	Tok/s 18115 (23888)	Loss/tok 3.8885 (6.2334)	LR 2.000e-03
0: TRAIN [0][1190/5173]	Time 0.567 (0.611)	Data 1.33e-04 (6.23e-04)	Tok/s 18804 (23910)	Loss/tok 3.9881 (6.2144)	LR 2.000e-03
0: TRAIN [0][1200/5173]	Time 0.687 (0.611)	Data 1.32e-04 (6.19e-04)	Tok/s 33713 (23916)	Loss/tok 4.3916 (6.1973)	LR 2.000e-03
0: TRAIN [0][1210/5173]	Time 0.566 (0.611)	Data 1.23e-04 (6.15e-04)	Tok/s 18235 (23923)	Loss/tok 3.8000 (6.1800)	LR 2.000e-03
0: TRAIN [0][1220/5173]	Time 0.568 (0.611)	Data 1.27e-04 (6.11e-04)	Tok/s 18515 (23937)	Loss/tok 3.9646 (6.1623)	LR 2.000e-03
0: TRAIN [0][1230/5173]	Time 0.691 (0.611)	Data 1.27e-04 (6.07e-04)	Tok/s 33465 (23944)	Loss/tok 4.3822 (6.1452)	LR 2.000e-03
0: TRAIN [0][1240/5173]	Time 0.567 (0.611)	Data 1.24e-04 (6.03e-04)	Tok/s 17946 (23924)	Loss/tok 3.9278 (6.1304)	LR 2.000e-03
0: TRAIN [0][1250/5173]	Time 0.633 (0.611)	Data 1.24e-04 (6.00e-04)	Tok/s 26214 (23899)	Loss/tok 4.0931 (6.1165)	LR 2.000e-03
0: TRAIN [0][1260/5173]	Time 0.628 (0.611)	Data 1.30e-04 (5.96e-04)	Tok/s 26746 (23900)	Loss/tok 4.1339 (6.1005)	LR 2.000e-03
0: TRAIN [0][1270/5173]	Time 0.625 (0.611)	Data 1.20e-04 (5.92e-04)	Tok/s 26803 (23896)	Loss/tok 4.2407 (6.0853)	LR 2.000e-03
0: TRAIN [0][1280/5173]	Time 0.566 (0.611)	Data 1.24e-04 (5.89e-04)	Tok/s 18032 (23865)	Loss/tok 3.7983 (6.0723)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1290/5173]	Time 0.625 (0.611)	Data 1.30e-04 (5.85e-04)	Tok/s 26938 (23851)	Loss/tok 4.2067 (6.0585)	LR 2.000e-03
0: TRAIN [0][1300/5173]	Time 0.567 (0.611)	Data 2.81e-04 (5.82e-04)	Tok/s 18625 (23864)	Loss/tok 3.7023 (6.0420)	LR 2.000e-03
0: TRAIN [0][1310/5173]	Time 0.687 (0.611)	Data 1.21e-04 (5.79e-04)	Tok/s 34107 (23881)	Loss/tok 4.2700 (6.0256)	LR 2.000e-03
0: TRAIN [0][1320/5173]	Time 0.568 (0.611)	Data 1.32e-04 (5.76e-04)	Tok/s 18216 (23885)	Loss/tok 3.6937 (6.0109)	LR 2.000e-03
0: TRAIN [0][1330/5173]	Time 0.627 (0.611)	Data 1.23e-04 (5.72e-04)	Tok/s 26807 (23875)	Loss/tok 4.0898 (5.9971)	LR 2.000e-03
0: TRAIN [0][1340/5173]	Time 0.568 (0.611)	Data 1.23e-04 (5.69e-04)	Tok/s 18041 (23858)	Loss/tok 3.9160 (5.9841)	LR 2.000e-03
0: TRAIN [0][1350/5173]	Time 0.691 (0.611)	Data 1.25e-04 (5.66e-04)	Tok/s 33825 (23858)	Loss/tok 4.2790 (5.9698)	LR 2.000e-03
0: TRAIN [0][1360/5173]	Time 0.632 (0.611)	Data 1.34e-04 (5.63e-04)	Tok/s 26609 (23886)	Loss/tok 4.0434 (5.9539)	LR 2.000e-03
0: TRAIN [0][1370/5173]	Time 0.568 (0.611)	Data 1.22e-04 (5.60e-04)	Tok/s 18520 (23855)	Loss/tok 3.6869 (5.9421)	LR 2.000e-03
0: TRAIN [0][1380/5173]	Time 0.630 (0.611)	Data 1.26e-04 (5.57e-04)	Tok/s 26657 (23833)	Loss/tok 4.0943 (5.9300)	LR 2.000e-03
0: TRAIN [0][1390/5173]	Time 0.568 (0.610)	Data 1.24e-04 (5.54e-04)	Tok/s 17748 (23806)	Loss/tok 3.7111 (5.9186)	LR 2.000e-03
0: TRAIN [0][1400/5173]	Time 0.565 (0.610)	Data 1.24e-04 (5.51e-04)	Tok/s 18101 (23804)	Loss/tok 3.7320 (5.9055)	LR 2.000e-03
0: TRAIN [0][1410/5173]	Time 0.625 (0.610)	Data 1.24e-04 (5.48e-04)	Tok/s 27012 (23794)	Loss/tok 4.0177 (5.8929)	LR 2.000e-03
0: TRAIN [0][1420/5173]	Time 0.568 (0.610)	Data 1.29e-04 (5.45e-04)	Tok/s 18368 (23770)	Loss/tok 3.7767 (5.8817)	LR 2.000e-03
0: TRAIN [0][1430/5173]	Time 0.569 (0.610)	Data 1.21e-04 (5.42e-04)	Tok/s 17849 (23760)	Loss/tok 3.6992 (5.8694)	LR 2.000e-03
0: TRAIN [0][1440/5173]	Time 0.625 (0.610)	Data 1.32e-04 (5.39e-04)	Tok/s 26613 (23765)	Loss/tok 4.0468 (5.8560)	LR 2.000e-03
0: TRAIN [0][1450/5173]	Time 0.631 (0.610)	Data 1.39e-04 (5.36e-04)	Tok/s 26361 (23786)	Loss/tok 4.0371 (5.8419)	LR 2.000e-03
0: TRAIN [0][1460/5173]	Time 0.565 (0.610)	Data 1.18e-04 (5.34e-04)	Tok/s 18376 (23776)	Loss/tok 3.8458 (5.8301)	LR 2.000e-03
0: TRAIN [0][1470/5173]	Time 0.566 (0.610)	Data 1.26e-04 (5.31e-04)	Tok/s 17873 (23751)	Loss/tok 3.7183 (5.8196)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1480/5173]	Time 0.688 (0.610)	Data 1.26e-04 (5.28e-04)	Tok/s 34088 (23740)	Loss/tok 4.1700 (5.8081)	LR 2.000e-03
0: TRAIN [0][1490/5173]	Time 0.570 (0.610)	Data 1.22e-04 (5.26e-04)	Tok/s 18137 (23742)	Loss/tok 3.6741 (5.7960)	LR 2.000e-03
0: TRAIN [0][1500/5173]	Time 0.623 (0.610)	Data 1.24e-04 (5.23e-04)	Tok/s 26555 (23742)	Loss/tok 4.0981 (5.7839)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1510/5173]	Time 0.690 (0.610)	Data 1.26e-04 (5.21e-04)	Tok/s 33699 (23752)	Loss/tok 4.1383 (5.7713)	LR 2.000e-03
0: TRAIN [0][1520/5173]	Time 0.693 (0.610)	Data 1.28e-04 (5.18e-04)	Tok/s 33348 (23742)	Loss/tok 4.2642 (5.7604)	LR 2.000e-03
0: TRAIN [0][1530/5173]	Time 0.688 (0.610)	Data 1.32e-04 (5.16e-04)	Tok/s 33322 (23756)	Loss/tok 4.2766 (5.7477)	LR 2.000e-03
0: TRAIN [0][1540/5173]	Time 0.566 (0.610)	Data 2.89e-04 (5.13e-04)	Tok/s 17926 (23762)	Loss/tok 3.7311 (5.7359)	LR 2.000e-03
0: TRAIN [0][1550/5173]	Time 0.677 (0.610)	Data 3.07e-04 (5.11e-04)	Tok/s 34270 (23745)	Loss/tok 4.0937 (5.7256)	LR 2.000e-03
0: TRAIN [0][1560/5173]	Time 0.568 (0.610)	Data 1.28e-04 (5.08e-04)	Tok/s 18308 (23737)	Loss/tok 3.6136 (5.7148)	LR 2.000e-03
0: TRAIN [0][1570/5173]	Time 0.641 (0.610)	Data 3.12e-04 (5.06e-04)	Tok/s 36511 (23730)	Loss/tok 4.0408 (5.7040)	LR 2.000e-03
0: TRAIN [0][1580/5173]	Time 0.765 (0.610)	Data 1.33e-04 (5.04e-04)	Tok/s 38882 (23741)	Loss/tok 4.3622 (5.6921)	LR 2.000e-03
0: TRAIN [0][1590/5173]	Time 0.568 (0.610)	Data 1.18e-04 (5.02e-04)	Tok/s 17987 (23737)	Loss/tok 3.6818 (5.6813)	LR 2.000e-03
0: TRAIN [0][1600/5173]	Time 0.629 (0.610)	Data 1.32e-04 (4.99e-04)	Tok/s 26524 (23716)	Loss/tok 3.8947 (5.6719)	LR 2.000e-03
0: TRAIN [0][1610/5173]	Time 0.568 (0.609)	Data 1.32e-04 (4.97e-04)	Tok/s 17760 (23698)	Loss/tok 3.6362 (5.6624)	LR 2.000e-03
0: TRAIN [0][1620/5173]	Time 0.689 (0.609)	Data 1.23e-04 (4.95e-04)	Tok/s 33816 (23709)	Loss/tok 4.1078 (5.6509)	LR 2.000e-03
0: TRAIN [0][1630/5173]	Time 0.632 (0.610)	Data 1.32e-04 (4.92e-04)	Tok/s 26646 (23708)	Loss/tok 3.8529 (5.6400)	LR 2.000e-03
0: TRAIN [0][1640/5173]	Time 0.767 (0.609)	Data 1.24e-04 (4.90e-04)	Tok/s 38639 (23698)	Loss/tok 4.4389 (5.6304)	LR 2.000e-03
0: TRAIN [0][1650/5173]	Time 0.689 (0.610)	Data 1.30e-04 (4.88e-04)	Tok/s 33968 (23712)	Loss/tok 4.1197 (5.6192)	LR 2.000e-03
0: TRAIN [0][1660/5173]	Time 0.692 (0.610)	Data 2.69e-04 (4.86e-04)	Tok/s 33772 (23714)	Loss/tok 4.1489 (5.6089)	LR 2.000e-03
0: TRAIN [0][1670/5173]	Time 0.630 (0.610)	Data 1.24e-04 (4.84e-04)	Tok/s 26363 (23709)	Loss/tok 3.9745 (5.5992)	LR 2.000e-03
0: TRAIN [0][1680/5173]	Time 0.566 (0.609)	Data 1.22e-04 (4.82e-04)	Tok/s 18107 (23673)	Loss/tok 3.5440 (5.5914)	LR 2.000e-03
0: TRAIN [0][1690/5173]	Time 0.564 (0.609)	Data 1.28e-04 (4.80e-04)	Tok/s 18212 (23660)	Loss/tok 3.6203 (5.5825)	LR 2.000e-03
0: TRAIN [0][1700/5173]	Time 0.692 (0.609)	Data 1.21e-04 (4.78e-04)	Tok/s 33928 (23671)	Loss/tok 4.1720 (5.5720)	LR 2.000e-03
0: TRAIN [0][1710/5173]	Time 0.569 (0.609)	Data 1.22e-04 (4.76e-04)	Tok/s 18231 (23653)	Loss/tok 3.5180 (5.5633)	LR 2.000e-03
0: TRAIN [0][1720/5173]	Time 0.568 (0.609)	Data 1.22e-04 (4.74e-04)	Tok/s 18080 (23627)	Loss/tok 3.6907 (5.5555)	LR 2.000e-03
0: TRAIN [0][1730/5173]	Time 0.567 (0.609)	Data 1.24e-04 (4.72e-04)	Tok/s 18324 (23614)	Loss/tok 3.6243 (5.5468)	LR 2.000e-03
0: TRAIN [0][1740/5173]	Time 0.567 (0.609)	Data 1.31e-04 (4.70e-04)	Tok/s 18193 (23616)	Loss/tok 3.6166 (5.5370)	LR 2.000e-03
0: TRAIN [0][1750/5173]	Time 0.629 (0.609)	Data 1.26e-04 (4.68e-04)	Tok/s 26886 (23615)	Loss/tok 3.8904 (5.5278)	LR 2.000e-03
0: TRAIN [0][1760/5173]	Time 0.565 (0.608)	Data 1.26e-04 (4.66e-04)	Tok/s 18526 (23590)	Loss/tok 3.6159 (5.5200)	LR 2.000e-03
0: TRAIN [0][1770/5173]	Time 0.568 (0.608)	Data 1.21e-04 (4.64e-04)	Tok/s 17978 (23585)	Loss/tok 3.7190 (5.5111)	LR 2.000e-03
0: TRAIN [0][1780/5173]	Time 0.567 (0.608)	Data 1.19e-04 (4.63e-04)	Tok/s 18341 (23559)	Loss/tok 3.5543 (5.5037)	LR 2.000e-03
0: TRAIN [0][1790/5173]	Time 0.552 (0.608)	Data 1.30e-04 (4.61e-04)	Tok/s 18342 (23548)	Loss/tok 3.5962 (5.4951)	LR 2.000e-03
0: TRAIN [0][1800/5173]	Time 0.687 (0.608)	Data 1.33e-04 (4.59e-04)	Tok/s 34038 (23558)	Loss/tok 4.0947 (5.4856)	LR 2.000e-03
0: TRAIN [0][1810/5173]	Time 0.567 (0.608)	Data 1.21e-04 (4.57e-04)	Tok/s 17782 (23547)	Loss/tok 3.6806 (5.4772)	LR 2.000e-03
0: TRAIN [0][1820/5173]	Time 0.689 (0.608)	Data 1.31e-04 (4.55e-04)	Tok/s 33775 (23548)	Loss/tok 4.0304 (5.4680)	LR 2.000e-03
0: TRAIN [0][1830/5173]	Time 0.628 (0.608)	Data 1.26e-04 (4.54e-04)	Tok/s 26662 (23562)	Loss/tok 3.9387 (5.4584)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1840/5173]	Time 0.689 (0.608)	Data 1.29e-04 (4.52e-04)	Tok/s 33981 (23578)	Loss/tok 4.0869 (5.4489)	LR 2.000e-03
0: TRAIN [0][1850/5173]	Time 0.567 (0.608)	Data 1.28e-04 (4.50e-04)	Tok/s 18064 (23570)	Loss/tok 3.5921 (5.4406)	LR 2.000e-03
0: TRAIN [0][1860/5173]	Time 0.506 (0.608)	Data 1.25e-04 (4.49e-04)	Tok/s 10369 (23551)	Loss/tok 2.9871 (5.4332)	LR 2.000e-03
0: TRAIN [0][1870/5173]	Time 0.568 (0.608)	Data 1.32e-04 (4.47e-04)	Tok/s 18172 (23572)	Loss/tok 3.6155 (5.4235)	LR 2.000e-03
0: TRAIN [0][1880/5173]	Time 0.568 (0.608)	Data 1.26e-04 (4.45e-04)	Tok/s 17907 (23552)	Loss/tok 3.6191 (5.4164)	LR 2.000e-03
0: TRAIN [0][1890/5173]	Time 0.690 (0.608)	Data 1.29e-04 (4.44e-04)	Tok/s 33877 (23533)	Loss/tok 4.1114 (5.4093)	LR 2.000e-03
0: TRAIN [0][1900/5173]	Time 0.568 (0.608)	Data 1.28e-04 (4.42e-04)	Tok/s 18280 (23518)	Loss/tok 3.5431 (5.4019)	LR 2.000e-03
0: TRAIN [0][1910/5173]	Time 0.566 (0.608)	Data 1.25e-04 (4.41e-04)	Tok/s 17815 (23499)	Loss/tok 3.5113 (5.3950)	LR 2.000e-03
0: TRAIN [0][1920/5173]	Time 0.633 (0.608)	Data 1.26e-04 (4.39e-04)	Tok/s 26683 (23505)	Loss/tok 3.8513 (5.3862)	LR 2.000e-03
0: TRAIN [0][1930/5173]	Time 0.631 (0.608)	Data 1.23e-04 (4.37e-04)	Tok/s 26467 (23499)	Loss/tok 3.8603 (5.3785)	LR 2.000e-03
0: TRAIN [0][1940/5173]	Time 0.629 (0.608)	Data 1.29e-04 (4.36e-04)	Tok/s 27072 (23495)	Loss/tok 3.7272 (5.3708)	LR 2.000e-03
0: TRAIN [0][1950/5173]	Time 0.568 (0.607)	Data 1.32e-04 (4.35e-04)	Tok/s 18221 (23465)	Loss/tok 3.5702 (5.3646)	LR 2.000e-03
0: TRAIN [0][1960/5173]	Time 0.568 (0.607)	Data 1.22e-04 (4.33e-04)	Tok/s 18140 (23443)	Loss/tok 3.6307 (5.3581)	LR 2.000e-03
0: TRAIN [0][1970/5173]	Time 0.571 (0.607)	Data 1.19e-04 (4.31e-04)	Tok/s 18488 (23437)	Loss/tok 3.6767 (5.3507)	LR 2.000e-03
0: TRAIN [0][1980/5173]	Time 0.634 (0.607)	Data 1.26e-04 (4.30e-04)	Tok/s 26740 (23432)	Loss/tok 3.7324 (5.3431)	LR 2.000e-03
0: TRAIN [0][1990/5173]	Time 0.567 (0.607)	Data 1.27e-04 (4.29e-04)	Tok/s 18418 (23448)	Loss/tok 3.5725 (5.3348)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2000/5173]	Time 0.506 (0.607)	Data 1.25e-04 (4.27e-04)	Tok/s 10472 (23452)	Loss/tok 3.0417 (5.3269)	LR 2.000e-03
0: TRAIN [0][2010/5173]	Time 0.626 (0.607)	Data 1.28e-04 (4.26e-04)	Tok/s 27044 (23457)	Loss/tok 3.7630 (5.3192)	LR 2.000e-03
0: TRAIN [0][2020/5173]	Time 0.571 (0.607)	Data 1.34e-04 (4.24e-04)	Tok/s 17948 (23459)	Loss/tok 3.5956 (5.3117)	LR 2.000e-03
0: TRAIN [0][2030/5173]	Time 0.768 (0.608)	Data 1.34e-04 (4.23e-04)	Tok/s 38465 (23476)	Loss/tok 4.3778 (5.3037)	LR 2.000e-03
0: TRAIN [0][2040/5173]	Time 0.635 (0.608)	Data 1.34e-04 (4.22e-04)	Tok/s 26423 (23467)	Loss/tok 3.7349 (5.2969)	LR 2.000e-03
0: TRAIN [0][2050/5173]	Time 0.505 (0.608)	Data 1.24e-04 (4.20e-04)	Tok/s 10552 (23457)	Loss/tok 3.0094 (5.2901)	LR 2.000e-03
0: TRAIN [0][2060/5173]	Time 0.509 (0.607)	Data 1.27e-04 (4.19e-04)	Tok/s 10361 (23440)	Loss/tok 2.9691 (5.2838)	LR 2.000e-03
0: TRAIN [0][2070/5173]	Time 0.506 (0.607)	Data 1.32e-04 (4.17e-04)	Tok/s 10581 (23412)	Loss/tok 2.9807 (5.2781)	LR 2.000e-03
0: TRAIN [0][2080/5173]	Time 0.507 (0.607)	Data 1.33e-04 (4.16e-04)	Tok/s 10442 (23392)	Loss/tok 2.9845 (5.2720)	LR 2.000e-03
0: TRAIN [0][2090/5173]	Time 0.630 (0.607)	Data 1.24e-04 (4.15e-04)	Tok/s 26833 (23391)	Loss/tok 3.6964 (5.2645)	LR 2.000e-03
0: TRAIN [0][2100/5173]	Time 0.632 (0.607)	Data 1.31e-04 (4.13e-04)	Tok/s 26383 (23394)	Loss/tok 3.6986 (5.2571)	LR 2.000e-03
0: TRAIN [0][2110/5173]	Time 0.568 (0.607)	Data 1.28e-04 (4.12e-04)	Tok/s 17982 (23370)	Loss/tok 3.6002 (5.2515)	LR 2.000e-03
0: TRAIN [0][2120/5173]	Time 0.489 (0.607)	Data 1.26e-04 (4.11e-04)	Tok/s 10829 (23389)	Loss/tok 3.0627 (5.2435)	LR 2.000e-03
0: TRAIN [0][2130/5173]	Time 0.570 (0.607)	Data 1.29e-04 (4.09e-04)	Tok/s 18232 (23380)	Loss/tok 3.4050 (5.2369)	LR 2.000e-03
0: TRAIN [0][2140/5173]	Time 0.568 (0.607)	Data 1.34e-04 (4.08e-04)	Tok/s 18359 (23368)	Loss/tok 3.5931 (5.2307)	LR 2.000e-03
0: TRAIN [0][2150/5173]	Time 0.631 (0.607)	Data 1.35e-04 (4.07e-04)	Tok/s 26287 (23366)	Loss/tok 3.8495 (5.2241)	LR 2.000e-03
0: TRAIN [0][2160/5173]	Time 0.567 (0.607)	Data 1.27e-04 (4.06e-04)	Tok/s 18407 (23376)	Loss/tok 3.4804 (5.2169)	LR 2.000e-03
0: TRAIN [0][2170/5173]	Time 0.567 (0.607)	Data 1.29e-04 (4.04e-04)	Tok/s 18103 (23377)	Loss/tok 3.4809 (5.2102)	LR 2.000e-03
0: TRAIN [0][2180/5173]	Time 0.628 (0.607)	Data 1.22e-04 (4.03e-04)	Tok/s 26604 (23386)	Loss/tok 3.8720 (5.2033)	LR 2.000e-03
0: TRAIN [0][2190/5173]	Time 0.692 (0.607)	Data 1.23e-04 (4.02e-04)	Tok/s 33809 (23392)	Loss/tok 3.9949 (5.1965)	LR 2.000e-03
0: TRAIN [0][2200/5173]	Time 0.572 (0.607)	Data 1.25e-04 (4.01e-04)	Tok/s 18136 (23384)	Loss/tok 3.5629 (5.1904)	LR 2.000e-03
0: TRAIN [0][2210/5173]	Time 0.682 (0.607)	Data 1.32e-04 (3.99e-04)	Tok/s 34405 (23391)	Loss/tok 3.9897 (5.1835)	LR 2.000e-03
0: TRAIN [0][2220/5173]	Time 0.505 (0.607)	Data 1.19e-04 (3.98e-04)	Tok/s 10313 (23386)	Loss/tok 2.8167 (5.1773)	LR 2.000e-03
0: TRAIN [0][2230/5173]	Time 0.633 (0.607)	Data 1.27e-04 (3.97e-04)	Tok/s 26435 (23389)	Loss/tok 3.8062 (5.1710)	LR 2.000e-03
0: TRAIN [0][2240/5173]	Time 0.632 (0.607)	Data 1.19e-04 (3.96e-04)	Tok/s 26795 (23370)	Loss/tok 3.7730 (5.1656)	LR 2.000e-03
0: TRAIN [0][2250/5173]	Time 0.567 (0.607)	Data 1.37e-04 (3.95e-04)	Tok/s 18210 (23362)	Loss/tok 3.4283 (5.1599)	LR 2.000e-03
0: TRAIN [0][2260/5173]	Time 0.569 (0.607)	Data 1.25e-04 (3.93e-04)	Tok/s 17955 (23359)	Loss/tok 3.4608 (5.1537)	LR 2.000e-03
0: TRAIN [0][2270/5173]	Time 0.623 (0.607)	Data 1.27e-04 (3.92e-04)	Tok/s 27140 (23358)	Loss/tok 3.7245 (5.1476)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][2280/5173]	Time 0.547 (0.607)	Data 1.24e-04 (3.91e-04)	Tok/s 18998 (23365)	Loss/tok 3.6047 (5.1409)	LR 2.000e-03
0: TRAIN [0][2290/5173]	Time 0.569 (0.607)	Data 1.37e-04 (3.90e-04)	Tok/s 17932 (23373)	Loss/tok 3.6028 (5.1345)	LR 2.000e-03
0: TRAIN [0][2300/5173]	Time 0.675 (0.607)	Data 1.27e-04 (3.89e-04)	Tok/s 34548 (23371)	Loss/tok 4.0144 (5.1286)	LR 2.000e-03
0: TRAIN [0][2310/5173]	Time 0.692 (0.607)	Data 1.25e-04 (3.88e-04)	Tok/s 33775 (23371)	Loss/tok 3.9353 (5.1228)	LR 2.000e-03
0: TRAIN [0][2320/5173]	Time 0.690 (0.607)	Data 1.21e-04 (3.87e-04)	Tok/s 33275 (23375)	Loss/tok 4.1200 (5.1167)	LR 2.000e-03
0: TRAIN [0][2330/5173]	Time 0.568 (0.607)	Data 1.23e-04 (3.86e-04)	Tok/s 17938 (23360)	Loss/tok 3.5874 (5.1115)	LR 2.000e-03
0: TRAIN [0][2340/5173]	Time 0.570 (0.607)	Data 1.23e-04 (3.85e-04)	Tok/s 17899 (23361)	Loss/tok 3.6201 (5.1057)	LR 2.000e-03
0: TRAIN [0][2350/5173]	Time 0.691 (0.607)	Data 1.30e-04 (3.84e-04)	Tok/s 33611 (23365)	Loss/tok 3.7698 (5.0994)	LR 2.000e-03
0: TRAIN [0][2360/5173]	Time 0.627 (0.607)	Data 1.24e-04 (3.83e-04)	Tok/s 26837 (23368)	Loss/tok 3.7007 (5.0935)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2370/5173]	Time 0.572 (0.607)	Data 1.36e-04 (3.82e-04)	Tok/s 18388 (23379)	Loss/tok 3.5029 (5.0871)	LR 2.000e-03
0: TRAIN [0][2380/5173]	Time 0.628 (0.607)	Data 1.22e-04 (3.81e-04)	Tok/s 26947 (23372)	Loss/tok 3.7754 (5.0818)	LR 2.000e-03
0: TRAIN [0][2390/5173]	Time 0.504 (0.607)	Data 2.97e-04 (3.80e-04)	Tok/s 10152 (23383)	Loss/tok 2.9122 (5.0756)	LR 2.000e-03
0: TRAIN [0][2400/5173]	Time 0.627 (0.607)	Data 1.29e-04 (3.79e-04)	Tok/s 26869 (23396)	Loss/tok 3.7947 (5.0693)	LR 2.000e-03
0: TRAIN [0][2410/5173]	Time 0.630 (0.607)	Data 1.24e-04 (3.78e-04)	Tok/s 26828 (23389)	Loss/tok 3.7210 (5.0640)	LR 2.000e-03
0: TRAIN [0][2420/5173]	Time 0.764 (0.607)	Data 1.27e-04 (3.76e-04)	Tok/s 39423 (23397)	Loss/tok 3.9547 (5.0580)	LR 2.000e-03
0: TRAIN [0][2430/5173]	Time 0.568 (0.607)	Data 1.23e-04 (3.76e-04)	Tok/s 18259 (23395)	Loss/tok 3.6088 (5.0525)	LR 2.000e-03
0: TRAIN [0][2440/5173]	Time 0.572 (0.607)	Data 1.22e-04 (3.75e-04)	Tok/s 18085 (23393)	Loss/tok 3.5460 (5.0471)	LR 2.000e-03
0: TRAIN [0][2450/5173]	Time 0.568 (0.607)	Data 1.29e-04 (3.74e-04)	Tok/s 18256 (23388)	Loss/tok 3.4527 (5.0418)	LR 2.000e-03
0: TRAIN [0][2460/5173]	Time 0.628 (0.607)	Data 1.23e-04 (3.73e-04)	Tok/s 26772 (23399)	Loss/tok 3.7832 (5.0359)	LR 2.000e-03
0: TRAIN [0][2470/5173]	Time 0.630 (0.607)	Data 1.22e-04 (3.72e-04)	Tok/s 26829 (23396)	Loss/tok 3.6514 (5.0305)	LR 2.000e-03
0: TRAIN [0][2480/5173]	Time 0.568 (0.607)	Data 1.31e-04 (3.71e-04)	Tok/s 17860 (23385)	Loss/tok 3.5005 (5.0257)	LR 2.000e-03
0: TRAIN [0][2490/5173]	Time 0.630 (0.607)	Data 1.27e-04 (3.70e-04)	Tok/s 26738 (23389)	Loss/tok 3.6350 (5.0204)	LR 2.000e-03
0: TRAIN [0][2500/5173]	Time 0.568 (0.607)	Data 1.23e-04 (3.69e-04)	Tok/s 18193 (23375)	Loss/tok 3.3845 (5.0156)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2510/5173]	Time 0.567 (0.607)	Data 1.23e-04 (3.68e-04)	Tok/s 17864 (23382)	Loss/tok 3.5601 (5.0099)	LR 2.000e-03
0: TRAIN [0][2520/5173]	Time 0.568 (0.607)	Data 1.28e-04 (3.67e-04)	Tok/s 18038 (23375)	Loss/tok 3.5042 (5.0053)	LR 2.000e-03
0: TRAIN [0][2530/5173]	Time 0.571 (0.607)	Data 1.22e-04 (3.66e-04)	Tok/s 18033 (23377)	Loss/tok 3.4272 (5.0001)	LR 2.000e-03
0: TRAIN [0][2540/5173]	Time 0.632 (0.607)	Data 1.25e-04 (3.65e-04)	Tok/s 26598 (23376)	Loss/tok 3.8747 (4.9950)	LR 2.000e-03
0: TRAIN [0][2550/5173]	Time 0.574 (0.607)	Data 1.36e-04 (3.64e-04)	Tok/s 18118 (23372)	Loss/tok 3.4143 (4.9901)	LR 2.000e-03
0: TRAIN [0][2560/5173]	Time 0.568 (0.607)	Data 1.15e-04 (3.63e-04)	Tok/s 18428 (23376)	Loss/tok 3.4668 (4.9847)	LR 2.000e-03
0: TRAIN [0][2570/5173]	Time 0.567 (0.607)	Data 1.22e-04 (3.63e-04)	Tok/s 18459 (23381)	Loss/tok 3.3251 (4.9797)	LR 2.000e-03
0: TRAIN [0][2580/5173]	Time 0.571 (0.607)	Data 2.91e-04 (3.62e-04)	Tok/s 18075 (23380)	Loss/tok 3.3627 (4.9747)	LR 2.000e-03
0: TRAIN [0][2590/5173]	Time 0.687 (0.607)	Data 1.22e-04 (3.61e-04)	Tok/s 34246 (23385)	Loss/tok 3.8508 (4.9695)	LR 2.000e-03
0: TRAIN [0][2600/5173]	Time 0.569 (0.607)	Data 1.23e-04 (3.60e-04)	Tok/s 18211 (23383)	Loss/tok 3.5159 (4.9646)	LR 2.000e-03
0: TRAIN [0][2610/5173]	Time 0.570 (0.607)	Data 1.24e-04 (3.59e-04)	Tok/s 17802 (23386)	Loss/tok 3.3626 (4.9595)	LR 2.000e-03
0: TRAIN [0][2620/5173]	Time 0.569 (0.607)	Data 1.45e-04 (3.58e-04)	Tok/s 17799 (23404)	Loss/tok 3.5098 (4.9540)	LR 2.000e-03
0: TRAIN [0][2630/5173]	Time 0.633 (0.607)	Data 1.20e-04 (3.57e-04)	Tok/s 26958 (23408)	Loss/tok 3.6329 (4.9491)	LR 2.000e-03
0: TRAIN [0][2640/5173]	Time 0.573 (0.607)	Data 1.27e-04 (3.57e-04)	Tok/s 17761 (23404)	Loss/tok 3.4685 (4.9445)	LR 2.000e-03
0: TRAIN [0][2650/5173]	Time 0.629 (0.607)	Data 1.35e-04 (3.56e-04)	Tok/s 27064 (23406)	Loss/tok 3.6479 (4.9397)	LR 2.000e-03
0: TRAIN [0][2660/5173]	Time 0.627 (0.607)	Data 1.22e-04 (3.55e-04)	Tok/s 27026 (23402)	Loss/tok 3.6902 (4.9352)	LR 2.000e-03
0: TRAIN [0][2670/5173]	Time 0.569 (0.607)	Data 1.26e-04 (3.54e-04)	Tok/s 17757 (23388)	Loss/tok 3.4994 (4.9311)	LR 2.000e-03
0: TRAIN [0][2680/5173]	Time 0.568 (0.607)	Data 1.28e-04 (3.53e-04)	Tok/s 18003 (23383)	Loss/tok 3.5881 (4.9267)	LR 2.000e-03
0: TRAIN [0][2690/5173]	Time 0.569 (0.607)	Data 1.27e-04 (3.52e-04)	Tok/s 18153 (23387)	Loss/tok 3.5238 (4.9218)	LR 2.000e-03
0: TRAIN [0][2700/5173]	Time 0.568 (0.607)	Data 1.54e-04 (3.52e-04)	Tok/s 18199 (23386)	Loss/tok 3.4409 (4.9173)	LR 2.000e-03
0: TRAIN [0][2710/5173]	Time 0.507 (0.607)	Data 1.47e-04 (3.51e-04)	Tok/s 10447 (23392)	Loss/tok 2.9432 (4.9124)	LR 2.000e-03
0: TRAIN [0][2720/5173]	Time 0.569 (0.607)	Data 1.70e-04 (3.50e-04)	Tok/s 17489 (23397)	Loss/tok 3.6122 (4.9079)	LR 2.000e-03
0: TRAIN [0][2730/5173]	Time 0.693 (0.607)	Data 1.30e-04 (3.49e-04)	Tok/s 33522 (23394)	Loss/tok 4.0031 (4.9037)	LR 2.000e-03
0: TRAIN [0][2740/5173]	Time 0.490 (0.607)	Data 1.31e-04 (3.49e-04)	Tok/s 10887 (23399)	Loss/tok 2.9605 (4.8991)	LR 2.000e-03
0: TRAIN [0][2750/5173]	Time 0.567 (0.607)	Data 2.95e-04 (3.48e-04)	Tok/s 17990 (23410)	Loss/tok 3.3476 (4.8942)	LR 2.000e-03
0: TRAIN [0][2760/5173]	Time 0.504 (0.607)	Data 1.22e-04 (3.47e-04)	Tok/s 10432 (23399)	Loss/tok 2.8789 (4.8902)	LR 2.000e-03
0: TRAIN [0][2770/5173]	Time 0.763 (0.608)	Data 1.30e-04 (3.46e-04)	Tok/s 38966 (23415)	Loss/tok 4.0810 (4.8852)	LR 2.000e-03
0: TRAIN [0][2780/5173]	Time 0.625 (0.607)	Data 1.19e-04 (3.46e-04)	Tok/s 26818 (23410)	Loss/tok 3.6241 (4.8808)	LR 2.000e-03
0: TRAIN [0][2790/5173]	Time 0.569 (0.608)	Data 1.24e-04 (3.45e-04)	Tok/s 18331 (23419)	Loss/tok 3.5344 (4.8761)	LR 2.000e-03
0: TRAIN [0][2800/5173]	Time 0.567 (0.607)	Data 2.84e-04 (3.44e-04)	Tok/s 18017 (23407)	Loss/tok 3.4907 (4.8723)	LR 2.000e-03
0: TRAIN [0][2810/5173]	Time 0.505 (0.607)	Data 1.24e-04 (3.43e-04)	Tok/s 10516 (23391)	Loss/tok 2.9620 (4.8686)	LR 2.000e-03
0: TRAIN [0][2820/5173]	Time 0.624 (0.607)	Data 1.25e-04 (3.43e-04)	Tok/s 26950 (23383)	Loss/tok 3.6678 (4.8646)	LR 2.000e-03
0: TRAIN [0][2830/5173]	Time 0.690 (0.607)	Data 1.26e-04 (3.42e-04)	Tok/s 33971 (23376)	Loss/tok 3.8542 (4.8605)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][2840/5173]	Time 0.494 (0.607)	Data 1.27e-04 (3.41e-04)	Tok/s 10927 (23377)	Loss/tok 3.0265 (4.8562)	LR 2.000e-03
0: TRAIN [0][2850/5173]	Time 0.633 (0.607)	Data 1.20e-04 (3.41e-04)	Tok/s 26596 (23372)	Loss/tok 3.6806 (4.8521)	LR 2.000e-03
0: TRAIN [0][2860/5173]	Time 0.568 (0.607)	Data 1.25e-04 (3.40e-04)	Tok/s 18355 (23359)	Loss/tok 3.3923 (4.8485)	LR 2.000e-03
0: TRAIN [0][2870/5173]	Time 0.691 (0.607)	Data 1.23e-04 (3.39e-04)	Tok/s 33234 (23358)	Loss/tok 3.9795 (4.8444)	LR 2.000e-03
0: TRAIN [0][2880/5173]	Time 0.636 (0.607)	Data 1.29e-04 (3.38e-04)	Tok/s 26116 (23366)	Loss/tok 3.7698 (4.8399)	LR 2.000e-03
0: TRAIN [0][2890/5173]	Time 0.627 (0.607)	Data 1.23e-04 (3.38e-04)	Tok/s 26915 (23363)	Loss/tok 3.7340 (4.8359)	LR 2.000e-03
0: TRAIN [0][2900/5173]	Time 0.568 (0.607)	Data 1.29e-04 (3.37e-04)	Tok/s 18126 (23362)	Loss/tok 3.4030 (4.8318)	LR 2.000e-03
0: TRAIN [0][2910/5173]	Time 0.506 (0.607)	Data 1.27e-04 (3.36e-04)	Tok/s 10144 (23368)	Loss/tok 2.9951 (4.8278)	LR 2.000e-03
0: TRAIN [0][2920/5173]	Time 0.760 (0.607)	Data 1.21e-04 (3.36e-04)	Tok/s 38932 (23375)	Loss/tok 4.0387 (4.8234)	LR 2.000e-03
0: TRAIN [0][2930/5173]	Time 0.506 (0.607)	Data 1.29e-04 (3.35e-04)	Tok/s 10356 (23371)	Loss/tok 2.8069 (4.8194)	LR 2.000e-03
0: TRAIN [0][2940/5173]	Time 0.572 (0.607)	Data 1.20e-04 (3.34e-04)	Tok/s 18141 (23373)	Loss/tok 3.4347 (4.8152)	LR 2.000e-03
0: TRAIN [0][2950/5173]	Time 0.630 (0.607)	Data 1.21e-04 (3.34e-04)	Tok/s 26836 (23382)	Loss/tok 3.6099 (4.8108)	LR 2.000e-03
0: TRAIN [0][2960/5173]	Time 0.695 (0.607)	Data 1.57e-04 (3.33e-04)	Tok/s 33300 (23389)	Loss/tok 3.8186 (4.8068)	LR 2.000e-03
0: TRAIN [0][2970/5173]	Time 0.508 (0.607)	Data 1.24e-04 (3.32e-04)	Tok/s 10481 (23394)	Loss/tok 2.9051 (4.8026)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][2980/5173]	Time 0.567 (0.607)	Data 1.22e-04 (3.32e-04)	Tok/s 18149 (23380)	Loss/tok 3.4749 (4.7993)	LR 2.000e-03
0: TRAIN [0][2990/5173]	Time 0.685 (0.607)	Data 1.19e-04 (3.31e-04)	Tok/s 34052 (23383)	Loss/tok 3.8001 (4.7954)	LR 2.000e-03
0: TRAIN [0][3000/5173]	Time 0.692 (0.607)	Data 1.24e-04 (3.30e-04)	Tok/s 33724 (23387)	Loss/tok 3.7370 (4.7914)	LR 2.000e-03
0: TRAIN [0][3010/5173]	Time 0.506 (0.607)	Data 1.20e-04 (3.30e-04)	Tok/s 10434 (23382)	Loss/tok 2.9039 (4.7879)	LR 2.000e-03
0: TRAIN [0][3020/5173]	Time 0.631 (0.607)	Data 1.18e-04 (3.29e-04)	Tok/s 27244 (23388)	Loss/tok 3.5758 (4.7840)	LR 2.000e-03
0: TRAIN [0][3030/5173]	Time 0.692 (0.607)	Data 1.26e-04 (3.28e-04)	Tok/s 33872 (23387)	Loss/tok 3.7686 (4.7802)	LR 2.000e-03
0: TRAIN [0][3040/5173]	Time 0.503 (0.607)	Data 1.24e-04 (3.28e-04)	Tok/s 10503 (23378)	Loss/tok 2.9642 (4.7768)	LR 2.000e-03
0: TRAIN [0][3050/5173]	Time 0.687 (0.607)	Data 1.28e-04 (3.27e-04)	Tok/s 34160 (23383)	Loss/tok 3.8160 (4.7727)	LR 2.000e-03
0: TRAIN [0][3060/5173]	Time 0.571 (0.607)	Data 1.35e-04 (3.27e-04)	Tok/s 17825 (23385)	Loss/tok 3.4030 (4.7689)	LR 2.000e-03
0: TRAIN [0][3070/5173]	Time 0.634 (0.607)	Data 1.29e-04 (3.26e-04)	Tok/s 26118 (23380)	Loss/tok 3.6458 (4.7653)	LR 2.000e-03
0: TRAIN [0][3080/5173]	Time 0.568 (0.607)	Data 1.26e-04 (3.25e-04)	Tok/s 18078 (23377)	Loss/tok 3.3649 (4.7617)	LR 2.000e-03
0: TRAIN [0][3090/5173]	Time 0.632 (0.607)	Data 1.25e-04 (3.25e-04)	Tok/s 26848 (23374)	Loss/tok 3.7118 (4.7582)	LR 2.000e-03
0: TRAIN [0][3100/5173]	Time 0.565 (0.607)	Data 1.13e-04 (3.24e-04)	Tok/s 18509 (23363)	Loss/tok 3.4314 (4.7549)	LR 2.000e-03
0: TRAIN [0][3110/5173]	Time 0.565 (0.607)	Data 1.20e-04 (3.23e-04)	Tok/s 18566 (23358)	Loss/tok 3.4119 (4.7515)	LR 2.000e-03
0: TRAIN [0][3120/5173]	Time 0.506 (0.607)	Data 1.24e-04 (3.23e-04)	Tok/s 10571 (23357)	Loss/tok 2.8139 (4.7477)	LR 2.000e-03
0: TRAIN [0][3130/5173]	Time 0.568 (0.607)	Data 1.24e-04 (3.22e-04)	Tok/s 18456 (23358)	Loss/tok 3.3460 (4.7442)	LR 2.000e-03
0: TRAIN [0][3140/5173]	Time 0.691 (0.607)	Data 1.23e-04 (3.22e-04)	Tok/s 34261 (23360)	Loss/tok 3.7778 (4.7406)	LR 2.000e-03
0: TRAIN [0][3150/5173]	Time 0.690 (0.607)	Data 1.30e-04 (3.21e-04)	Tok/s 34134 (23353)	Loss/tok 3.8766 (4.7373)	LR 2.000e-03
0: TRAIN [0][3160/5173]	Time 0.571 (0.607)	Data 3.15e-04 (3.20e-04)	Tok/s 18301 (23344)	Loss/tok 3.4129 (4.7342)	LR 2.000e-03
0: TRAIN [0][3170/5173]	Time 0.691 (0.607)	Data 1.21e-04 (3.20e-04)	Tok/s 33143 (23341)	Loss/tok 3.9580 (4.7308)	LR 2.000e-03
0: TRAIN [0][3180/5173]	Time 0.633 (0.607)	Data 1.33e-04 (3.19e-04)	Tok/s 26585 (23354)	Loss/tok 3.5452 (4.7269)	LR 2.000e-03
0: TRAIN [0][3190/5173]	Time 0.569 (0.607)	Data 1.29e-04 (3.19e-04)	Tok/s 18507 (23351)	Loss/tok 3.4887 (4.7233)	LR 2.000e-03
0: TRAIN [0][3200/5173]	Time 0.566 (0.607)	Data 1.22e-04 (3.18e-04)	Tok/s 18180 (23344)	Loss/tok 3.4290 (4.7202)	LR 2.000e-03
0: TRAIN [0][3210/5173]	Time 0.690 (0.607)	Data 1.29e-04 (3.18e-04)	Tok/s 34104 (23357)	Loss/tok 3.7692 (4.7162)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3220/5173]	Time 0.637 (0.607)	Data 1.25e-04 (3.17e-04)	Tok/s 26035 (23357)	Loss/tok 3.7138 (4.7128)	LR 2.000e-03
0: TRAIN [0][3230/5173]	Time 0.570 (0.607)	Data 1.21e-04 (3.16e-04)	Tok/s 18630 (23357)	Loss/tok 3.3662 (4.7094)	LR 2.000e-03
0: TRAIN [0][3240/5173]	Time 0.489 (0.607)	Data 1.29e-04 (3.16e-04)	Tok/s 11155 (23346)	Loss/tok 2.8297 (4.7063)	LR 2.000e-03
0: TRAIN [0][3250/5173]	Time 0.496 (0.607)	Data 1.29e-04 (3.15e-04)	Tok/s 10807 (23349)	Loss/tok 2.9784 (4.7030)	LR 2.000e-03
0: TRAIN [0][3260/5173]	Time 0.690 (0.607)	Data 1.26e-04 (3.15e-04)	Tok/s 34441 (23344)	Loss/tok 3.7415 (4.6997)	LR 2.000e-03
0: TRAIN [0][3270/5173]	Time 0.571 (0.607)	Data 1.23e-04 (3.14e-04)	Tok/s 18116 (23343)	Loss/tok 3.4261 (4.6964)	LR 2.000e-03
0: TRAIN [0][3280/5173]	Time 0.568 (0.607)	Data 1.31e-04 (3.14e-04)	Tok/s 18544 (23342)	Loss/tok 3.4181 (4.6930)	LR 2.000e-03
0: TRAIN [0][3290/5173]	Time 0.693 (0.607)	Data 3.31e-04 (3.13e-04)	Tok/s 33648 (23342)	Loss/tok 3.7222 (4.6897)	LR 2.000e-03
0: TRAIN [0][3300/5173]	Time 0.572 (0.607)	Data 1.24e-04 (3.13e-04)	Tok/s 18105 (23336)	Loss/tok 3.4075 (4.6866)	LR 2.000e-03
0: TRAIN [0][3310/5173]	Time 0.573 (0.607)	Data 1.26e-04 (3.12e-04)	Tok/s 17882 (23330)	Loss/tok 3.4984 (4.6834)	LR 2.000e-03
0: TRAIN [0][3320/5173]	Time 0.568 (0.607)	Data 1.22e-04 (3.12e-04)	Tok/s 18446 (23336)	Loss/tok 3.3425 (4.6800)	LR 2.000e-03
0: TRAIN [0][3330/5173]	Time 0.570 (0.607)	Data 1.19e-04 (3.11e-04)	Tok/s 18376 (23333)	Loss/tok 3.2573 (4.6768)	LR 2.000e-03
0: TRAIN [0][3340/5173]	Time 0.569 (0.607)	Data 1.27e-04 (3.11e-04)	Tok/s 18172 (23329)	Loss/tok 3.3535 (4.6737)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3350/5173]	Time 0.761 (0.607)	Data 1.24e-04 (3.10e-04)	Tok/s 39363 (23323)	Loss/tok 3.9225 (4.6707)	LR 2.000e-03
0: TRAIN [0][3360/5173]	Time 0.569 (0.607)	Data 1.21e-04 (3.09e-04)	Tok/s 17893 (23331)	Loss/tok 3.3768 (4.6672)	LR 2.000e-03
0: TRAIN [0][3370/5173]	Time 0.634 (0.607)	Data 1.26e-04 (3.09e-04)	Tok/s 26683 (23325)	Loss/tok 3.6068 (4.6642)	LR 2.000e-03
0: TRAIN [0][3380/5173]	Time 0.631 (0.607)	Data 2.95e-04 (3.09e-04)	Tok/s 26923 (23321)	Loss/tok 3.7052 (4.6611)	LR 2.000e-03
0: TRAIN [0][3390/5173]	Time 0.567 (0.607)	Data 1.18e-04 (3.08e-04)	Tok/s 18386 (23318)	Loss/tok 3.3972 (4.6582)	LR 2.000e-03
0: TRAIN [0][3400/5173]	Time 0.567 (0.607)	Data 1.23e-04 (3.08e-04)	Tok/s 18275 (23316)	Loss/tok 3.3355 (4.6551)	LR 2.000e-03
0: TRAIN [0][3410/5173]	Time 0.631 (0.607)	Data 1.38e-04 (3.07e-04)	Tok/s 26855 (23321)	Loss/tok 3.5249 (4.6519)	LR 2.000e-03
0: TRAIN [0][3420/5173]	Time 0.569 (0.607)	Data 1.21e-04 (3.07e-04)	Tok/s 18271 (23311)	Loss/tok 3.4453 (4.6492)	LR 2.000e-03
0: TRAIN [0][3430/5173]	Time 0.630 (0.607)	Data 3.11e-04 (3.06e-04)	Tok/s 26532 (23324)	Loss/tok 3.6199 (4.6457)	LR 2.000e-03
0: TRAIN [0][3440/5173]	Time 0.631 (0.607)	Data 1.28e-04 (3.06e-04)	Tok/s 26290 (23321)	Loss/tok 3.6459 (4.6426)	LR 2.000e-03
0: TRAIN [0][3450/5173]	Time 0.627 (0.607)	Data 3.16e-04 (3.05e-04)	Tok/s 26583 (23328)	Loss/tok 3.5551 (4.6394)	LR 2.000e-03
0: TRAIN [0][3460/5173]	Time 0.572 (0.607)	Data 1.27e-04 (3.05e-04)	Tok/s 17971 (23319)	Loss/tok 3.2276 (4.6366)	LR 2.000e-03
0: TRAIN [0][3470/5173]	Time 0.626 (0.607)	Data 1.29e-04 (3.04e-04)	Tok/s 26837 (23317)	Loss/tok 3.6389 (4.6336)	LR 2.000e-03
0: TRAIN [0][3480/5173]	Time 0.571 (0.607)	Data 1.22e-04 (3.04e-04)	Tok/s 17795 (23311)	Loss/tok 3.3687 (4.6308)	LR 2.000e-03
0: TRAIN [0][3490/5173]	Time 0.633 (0.607)	Data 1.34e-04 (3.03e-04)	Tok/s 27056 (23319)	Loss/tok 3.6162 (4.6274)	LR 2.000e-03
0: TRAIN [0][3500/5173]	Time 0.569 (0.607)	Data 1.32e-04 (3.03e-04)	Tok/s 18236 (23325)	Loss/tok 3.2976 (4.6242)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3510/5173]	Time 0.634 (0.607)	Data 1.31e-04 (3.02e-04)	Tok/s 26579 (23337)	Loss/tok 3.6186 (4.6208)	LR 2.000e-03
0: TRAIN [0][3520/5173]	Time 0.493 (0.607)	Data 1.31e-04 (3.02e-04)	Tok/s 10750 (23334)	Loss/tok 2.8387 (4.6181)	LR 2.000e-03
0: TRAIN [0][3530/5173]	Time 0.634 (0.607)	Data 1.23e-04 (3.01e-04)	Tok/s 26046 (23329)	Loss/tok 3.5043 (4.6152)	LR 2.000e-03
0: TRAIN [0][3540/5173]	Time 0.631 (0.607)	Data 1.24e-04 (3.01e-04)	Tok/s 26463 (23332)	Loss/tok 3.5810 (4.6122)	LR 2.000e-03
0: TRAIN [0][3550/5173]	Time 0.488 (0.607)	Data 1.17e-04 (3.00e-04)	Tok/s 10710 (23327)	Loss/tok 2.9279 (4.6094)	LR 2.000e-03
0: TRAIN [0][3560/5173]	Time 0.625 (0.607)	Data 1.29e-04 (3.00e-04)	Tok/s 27125 (23333)	Loss/tok 3.5103 (4.6063)	LR 2.000e-03
0: TRAIN [0][3570/5173]	Time 0.761 (0.607)	Data 1.16e-04 (2.99e-04)	Tok/s 39537 (23320)	Loss/tok 4.0147 (4.6039)	LR 2.000e-03
0: TRAIN [0][3580/5173]	Time 0.629 (0.607)	Data 1.22e-04 (2.99e-04)	Tok/s 26735 (23321)	Loss/tok 3.5653 (4.6010)	LR 2.000e-03
0: TRAIN [0][3590/5173]	Time 0.764 (0.607)	Data 1.25e-04 (2.98e-04)	Tok/s 38617 (23328)	Loss/tok 3.9383 (4.5979)	LR 2.000e-03
0: TRAIN [0][3600/5173]	Time 0.564 (0.607)	Data 1.25e-04 (2.98e-04)	Tok/s 18315 (23331)	Loss/tok 3.4866 (4.5950)	LR 2.000e-03
0: TRAIN [0][3610/5173]	Time 0.567 (0.607)	Data 3.07e-04 (2.98e-04)	Tok/s 18462 (23334)	Loss/tok 3.3385 (4.5923)	LR 2.000e-03
0: TRAIN [0][3620/5173]	Time 0.568 (0.607)	Data 1.25e-04 (2.97e-04)	Tok/s 18369 (23344)	Loss/tok 3.2249 (4.5893)	LR 2.000e-03
0: TRAIN [0][3630/5173]	Time 0.760 (0.607)	Data 1.24e-04 (2.97e-04)	Tok/s 39263 (23341)	Loss/tok 3.9984 (4.5865)	LR 2.000e-03
0: TRAIN [0][3640/5173]	Time 0.626 (0.607)	Data 1.24e-04 (2.96e-04)	Tok/s 27109 (23343)	Loss/tok 3.5036 (4.5837)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3650/5173]	Time 0.568 (0.607)	Data 1.24e-04 (2.96e-04)	Tok/s 18002 (23340)	Loss/tok 3.3451 (4.5810)	LR 2.000e-03
0: TRAIN [0][3660/5173]	Time 0.568 (0.607)	Data 1.26e-04 (2.95e-04)	Tok/s 17919 (23338)	Loss/tok 3.3610 (4.5783)	LR 2.000e-03
0: TRAIN [0][3670/5173]	Time 0.507 (0.607)	Data 1.31e-04 (2.95e-04)	Tok/s 10323 (23340)	Loss/tok 3.0076 (4.5757)	LR 2.000e-03
0: TRAIN [0][3680/5173]	Time 0.503 (0.607)	Data 1.32e-04 (2.95e-04)	Tok/s 10390 (23328)	Loss/tok 2.7831 (4.5732)	LR 2.000e-03
0: TRAIN [0][3690/5173]	Time 0.565 (0.607)	Data 1.17e-04 (2.94e-04)	Tok/s 18323 (23331)	Loss/tok 3.3930 (4.5705)	LR 2.000e-03
0: TRAIN [0][3700/5173]	Time 0.568 (0.607)	Data 1.22e-04 (2.94e-04)	Tok/s 18164 (23328)	Loss/tok 3.3134 (4.5680)	LR 2.000e-03
0: TRAIN [0][3710/5173]	Time 0.631 (0.607)	Data 1.25e-04 (2.93e-04)	Tok/s 26388 (23333)	Loss/tok 3.5825 (4.5653)	LR 2.000e-03
0: TRAIN [0][3720/5173]	Time 0.568 (0.607)	Data 1.31e-04 (2.93e-04)	Tok/s 18407 (23333)	Loss/tok 3.3714 (4.5625)	LR 2.000e-03
0: TRAIN [0][3730/5173]	Time 0.570 (0.607)	Data 1.50e-04 (2.92e-04)	Tok/s 17955 (23334)	Loss/tok 3.2617 (4.5598)	LR 2.000e-03
0: TRAIN [0][3740/5173]	Time 0.505 (0.607)	Data 1.39e-04 (2.92e-04)	Tok/s 10435 (23327)	Loss/tok 2.9428 (4.5573)	LR 2.000e-03
0: TRAIN [0][3750/5173]	Time 0.631 (0.607)	Data 1.31e-04 (2.92e-04)	Tok/s 26452 (23331)	Loss/tok 3.5296 (4.5544)	LR 2.000e-03
0: TRAIN [0][3760/5173]	Time 0.692 (0.607)	Data 1.33e-04 (2.91e-04)	Tok/s 34132 (23329)	Loss/tok 3.6394 (4.5518)	LR 2.000e-03
0: TRAIN [0][3770/5173]	Time 0.692 (0.607)	Data 1.30e-04 (2.91e-04)	Tok/s 33605 (23333)	Loss/tok 3.8035 (4.5491)	LR 2.000e-03
0: TRAIN [0][3780/5173]	Time 0.623 (0.607)	Data 1.31e-04 (2.90e-04)	Tok/s 26996 (23336)	Loss/tok 3.5972 (4.5464)	LR 2.000e-03
0: TRAIN [0][3790/5173]	Time 0.572 (0.607)	Data 1.31e-04 (2.90e-04)	Tok/s 17824 (23335)	Loss/tok 3.3730 (4.5438)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3800/5173]	Time 0.566 (0.607)	Data 3.05e-04 (2.90e-04)	Tok/s 18079 (23341)	Loss/tok 3.4189 (4.5411)	LR 2.000e-03
0: TRAIN [0][3810/5173]	Time 0.690 (0.607)	Data 1.28e-04 (2.89e-04)	Tok/s 33977 (23345)	Loss/tok 3.7366 (4.5385)	LR 2.000e-03
0: TRAIN [0][3820/5173]	Time 0.630 (0.607)	Data 1.17e-04 (2.89e-04)	Tok/s 26691 (23340)	Loss/tok 3.6666 (4.5360)	LR 2.000e-03
0: TRAIN [0][3830/5173]	Time 0.626 (0.607)	Data 1.23e-04 (2.88e-04)	Tok/s 26889 (23340)	Loss/tok 3.4609 (4.5334)	LR 2.000e-03
0: TRAIN [0][3840/5173]	Time 0.635 (0.607)	Data 1.26e-04 (2.88e-04)	Tok/s 26509 (23336)	Loss/tok 3.4896 (4.5310)	LR 2.000e-03
0: TRAIN [0][3850/5173]	Time 0.632 (0.607)	Data 1.15e-04 (2.87e-04)	Tok/s 26508 (23332)	Loss/tok 3.6328 (4.5286)	LR 2.000e-03
0: TRAIN [0][3860/5173]	Time 0.755 (0.607)	Data 1.36e-04 (2.87e-04)	Tok/s 39817 (23345)	Loss/tok 3.9007 (4.5258)	LR 2.000e-03
0: TRAIN [0][3870/5173]	Time 0.692 (0.607)	Data 1.27e-04 (2.87e-04)	Tok/s 33354 (23338)	Loss/tok 3.7451 (4.5235)	LR 2.000e-03
0: TRAIN [0][3880/5173]	Time 0.691 (0.607)	Data 1.25e-04 (2.86e-04)	Tok/s 33619 (23349)	Loss/tok 3.7700 (4.5208)	LR 2.000e-03
0: TRAIN [0][3890/5173]	Time 0.634 (0.608)	Data 1.29e-04 (2.86e-04)	Tok/s 26522 (23350)	Loss/tok 3.5321 (4.5181)	LR 2.000e-03
0: TRAIN [0][3900/5173]	Time 0.569 (0.608)	Data 1.24e-04 (2.85e-04)	Tok/s 18039 (23355)	Loss/tok 3.2198 (4.5156)	LR 2.000e-03
0: TRAIN [0][3910/5173]	Time 0.625 (0.608)	Data 1.21e-04 (2.85e-04)	Tok/s 27032 (23357)	Loss/tok 3.4400 (4.5130)	LR 2.000e-03
0: TRAIN [0][3920/5173]	Time 0.564 (0.607)	Data 1.23e-04 (2.85e-04)	Tok/s 18409 (23344)	Loss/tok 3.3503 (4.5109)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3930/5173]	Time 0.565 (0.607)	Data 1.33e-04 (2.84e-04)	Tok/s 18469 (23345)	Loss/tok 3.3239 (4.5084)	LR 2.000e-03
0: TRAIN [0][3940/5173]	Time 0.631 (0.607)	Data 1.28e-04 (2.84e-04)	Tok/s 26376 (23340)	Loss/tok 3.6316 (4.5060)	LR 2.000e-03
0: TRAIN [0][3950/5173]	Time 0.763 (0.608)	Data 1.21e-04 (2.84e-04)	Tok/s 39069 (23352)	Loss/tok 3.9433 (4.5033)	LR 2.000e-03
0: TRAIN [0][3960/5173]	Time 0.506 (0.607)	Data 1.14e-04 (2.83e-04)	Tok/s 10725 (23343)	Loss/tok 2.9260 (4.5011)	LR 2.000e-03
0: TRAIN [0][3970/5173]	Time 0.568 (0.607)	Data 1.28e-04 (2.83e-04)	Tok/s 18241 (23332)	Loss/tok 3.3414 (4.4990)	LR 2.000e-03
0: TRAIN [0][3980/5173]	Time 0.567 (0.607)	Data 1.25e-04 (2.82e-04)	Tok/s 18294 (23326)	Loss/tok 3.3883 (4.4967)	LR 2.000e-03
0: TRAIN [0][3990/5173]	Time 0.635 (0.607)	Data 1.21e-04 (2.82e-04)	Tok/s 26234 (23324)	Loss/tok 3.5623 (4.4942)	LR 2.000e-03
0: TRAIN [0][4000/5173]	Time 0.572 (0.607)	Data 1.26e-04 (2.82e-04)	Tok/s 18134 (23323)	Loss/tok 3.4240 (4.4919)	LR 2.000e-03
0: TRAIN [0][4010/5173]	Time 0.766 (0.607)	Data 1.27e-04 (2.81e-04)	Tok/s 38898 (23334)	Loss/tok 3.9012 (4.4892)	LR 2.000e-03
0: TRAIN [0][4020/5173]	Time 0.694 (0.607)	Data 1.26e-04 (2.81e-04)	Tok/s 33256 (23340)	Loss/tok 3.7429 (4.4867)	LR 2.000e-03
0: TRAIN [0][4030/5173]	Time 0.693 (0.607)	Data 1.25e-04 (2.80e-04)	Tok/s 33586 (23340)	Loss/tok 3.8152 (4.4844)	LR 2.000e-03
0: TRAIN [0][4040/5173]	Time 0.629 (0.607)	Data 1.29e-04 (2.80e-04)	Tok/s 26465 (23337)	Loss/tok 3.6066 (4.4820)	LR 2.000e-03
0: TRAIN [0][4050/5173]	Time 0.569 (0.607)	Data 1.26e-04 (2.80e-04)	Tok/s 17978 (23340)	Loss/tok 3.3375 (4.4795)	LR 2.000e-03
0: TRAIN [0][4060/5173]	Time 0.694 (0.608)	Data 1.27e-04 (2.79e-04)	Tok/s 33567 (23343)	Loss/tok 3.6319 (4.4771)	LR 2.000e-03
0: TRAIN [0][4070/5173]	Time 0.632 (0.608)	Data 1.26e-04 (2.79e-04)	Tok/s 26230 (23343)	Loss/tok 3.4714 (4.4747)	LR 2.000e-03
0: TRAIN [0][4080/5173]	Time 0.635 (0.608)	Data 1.24e-04 (2.79e-04)	Tok/s 26524 (23340)	Loss/tok 3.4650 (4.4724)	LR 2.000e-03
0: TRAIN [0][4090/5173]	Time 0.508 (0.607)	Data 1.22e-04 (2.78e-04)	Tok/s 10434 (23336)	Loss/tok 2.9314 (4.4702)	LR 2.000e-03
0: TRAIN [0][4100/5173]	Time 0.691 (0.607)	Data 1.18e-04 (2.78e-04)	Tok/s 33745 (23337)	Loss/tok 3.7107 (4.4679)	LR 2.000e-03
0: TRAIN [0][4110/5173]	Time 0.506 (0.608)	Data 1.28e-04 (2.78e-04)	Tok/s 10747 (23346)	Loss/tok 2.8025 (4.4652)	LR 2.000e-03
0: TRAIN [0][4120/5173]	Time 0.631 (0.608)	Data 1.22e-04 (2.77e-04)	Tok/s 26726 (23344)	Loss/tok 3.5755 (4.4630)	LR 2.000e-03
0: TRAIN [0][4130/5173]	Time 0.631 (0.608)	Data 1.24e-04 (2.77e-04)	Tok/s 26995 (23344)	Loss/tok 3.6599 (4.4607)	LR 2.000e-03
0: TRAIN [0][4140/5173]	Time 0.691 (0.607)	Data 1.27e-04 (2.77e-04)	Tok/s 33566 (23338)	Loss/tok 3.6739 (4.4587)	LR 2.000e-03
0: TRAIN [0][4150/5173]	Time 0.636 (0.607)	Data 1.23e-04 (2.76e-04)	Tok/s 26321 (23338)	Loss/tok 3.5040 (4.4564)	LR 2.000e-03
0: TRAIN [0][4160/5173]	Time 0.633 (0.607)	Data 1.23e-04 (2.76e-04)	Tok/s 26824 (23332)	Loss/tok 3.5442 (4.4543)	LR 2.000e-03
0: TRAIN [0][4170/5173]	Time 0.567 (0.607)	Data 1.25e-04 (2.76e-04)	Tok/s 17970 (23332)	Loss/tok 3.3412 (4.4520)	LR 2.000e-03
0: TRAIN [0][4180/5173]	Time 0.572 (0.607)	Data 1.19e-04 (2.75e-04)	Tok/s 18119 (23325)	Loss/tok 3.3775 (4.4500)	LR 2.000e-03
0: TRAIN [0][4190/5173]	Time 0.569 (0.607)	Data 1.26e-04 (2.75e-04)	Tok/s 18742 (23327)	Loss/tok 3.3178 (4.4478)	LR 2.000e-03
0: TRAIN [0][4200/5173]	Time 0.567 (0.607)	Data 1.32e-04 (2.75e-04)	Tok/s 18179 (23318)	Loss/tok 3.3982 (4.4458)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [0][4210/5173]	Time 0.571 (0.607)	Data 1.21e-04 (2.74e-04)	Tok/s 17645 (23326)	Loss/tok 3.3600 (4.4433)	LR 2.000e-03
0: TRAIN [0][4220/5173]	Time 0.567 (0.607)	Data 1.28e-04 (2.74e-04)	Tok/s 18456 (23316)	Loss/tok 3.2955 (4.4414)	LR 2.000e-03
0: TRAIN [0][4230/5173]	Time 0.761 (0.607)	Data 1.26e-04 (2.73e-04)	Tok/s 38297 (23313)	Loss/tok 4.0704 (4.4394)	LR 2.000e-03
0: TRAIN [0][4240/5173]	Time 0.695 (0.607)	Data 1.24e-04 (2.73e-04)	Tok/s 33474 (23308)	Loss/tok 3.6340 (4.4373)	LR 2.000e-03
0: TRAIN [0][4250/5173]	Time 0.564 (0.607)	Data 1.41e-04 (2.73e-04)	Tok/s 17961 (23305)	Loss/tok 3.3807 (4.4353)	LR 2.000e-03
0: TRAIN [0][4260/5173]	Time 0.569 (0.607)	Data 1.27e-04 (2.73e-04)	Tok/s 17989 (23299)	Loss/tok 3.3461 (4.4333)	LR 2.000e-03
0: TRAIN [0][4270/5173]	Time 0.570 (0.607)	Data 1.23e-04 (2.72e-04)	Tok/s 18201 (23295)	Loss/tok 3.3508 (4.4313)	LR 2.000e-03
0: TRAIN [0][4280/5173]	Time 0.629 (0.607)	Data 1.37e-04 (2.72e-04)	Tok/s 26915 (23306)	Loss/tok 3.4465 (4.4289)	LR 2.000e-03
0: TRAIN [0][4290/5173]	Time 0.507 (0.607)	Data 1.27e-04 (2.71e-04)	Tok/s 10545 (23304)	Loss/tok 2.8121 (4.4268)	LR 2.000e-03
0: TRAIN [0][4300/5173]	Time 0.618 (0.607)	Data 1.32e-04 (2.71e-04)	Tok/s 27302 (23299)	Loss/tok 3.4320 (4.4247)	LR 2.000e-03
0: TRAIN [0][4310/5173]	Time 0.567 (0.607)	Data 1.18e-04 (2.71e-04)	Tok/s 17984 (23297)	Loss/tok 3.2571 (4.4226)	LR 2.000e-03
0: TRAIN [0][4320/5173]	Time 0.506 (0.607)	Data 2.79e-04 (2.71e-04)	Tok/s 10494 (23291)	Loss/tok 2.9593 (4.4207)	LR 2.000e-03
0: TRAIN [0][4330/5173]	Time 0.568 (0.607)	Data 1.38e-04 (2.70e-04)	Tok/s 18025 (23288)	Loss/tok 3.2440 (4.4187)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [0][4340/5173]	Time 0.632 (0.607)	Data 1.22e-04 (2.70e-04)	Tok/s 26421 (23297)	Loss/tok 3.5040 (4.4163)	LR 2.000e-03
0: TRAIN [0][4350/5173]	Time 0.625 (0.607)	Data 1.31e-04 (2.70e-04)	Tok/s 26916 (23299)	Loss/tok 3.6683 (4.4142)	LR 2.000e-03
0: TRAIN [0][4360/5173]	Time 0.626 (0.607)	Data 1.25e-04 (2.69e-04)	Tok/s 27036 (23300)	Loss/tok 3.5610 (4.4120)	LR 2.000e-03
0: TRAIN [0][4370/5173]	Time 0.629 (0.607)	Data 1.23e-04 (2.69e-04)	Tok/s 26737 (23306)	Loss/tok 3.6264 (4.4097)	LR 2.000e-03
0: TRAIN [0][4380/5173]	Time 0.488 (0.607)	Data 1.28e-04 (2.69e-04)	Tok/s 10861 (23298)	Loss/tok 2.7946 (4.4078)	LR 2.000e-03
0: TRAIN [0][4390/5173]	Time 0.695 (0.607)	Data 1.35e-04 (2.68e-04)	Tok/s 33262 (23292)	Loss/tok 3.7428 (4.4059)	LR 2.000e-03
0: TRAIN [0][4400/5173]	Time 0.566 (0.607)	Data 1.20e-04 (2.68e-04)	Tok/s 18571 (23291)	Loss/tok 3.2391 (4.4039)	LR 2.000e-03
0: TRAIN [0][4410/5173]	Time 0.633 (0.607)	Data 3.13e-04 (2.68e-04)	Tok/s 26819 (23295)	Loss/tok 3.4822 (4.4019)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][4420/5173]	Time 0.567 (0.607)	Data 1.17e-04 (2.68e-04)	Tok/s 18236 (23304)	Loss/tok 3.4047 (4.3998)	LR 2.000e-03
0: TRAIN [0][4430/5173]	Time 0.690 (0.607)	Data 1.31e-04 (2.67e-04)	Tok/s 33939 (23316)	Loss/tok 3.8031 (4.3975)	LR 2.000e-03
0: TRAIN [0][4440/5173]	Time 0.628 (0.607)	Data 1.26e-04 (2.67e-04)	Tok/s 26890 (23319)	Loss/tok 3.5162 (4.3954)	LR 2.000e-03
0: TRAIN [0][4450/5173]	Time 0.764 (0.607)	Data 1.25e-04 (2.67e-04)	Tok/s 39503 (23320)	Loss/tok 3.7924 (4.3934)	LR 2.000e-03
0: TRAIN [0][4460/5173]	Time 0.496 (0.607)	Data 1.21e-04 (2.66e-04)	Tok/s 10590 (23317)	Loss/tok 2.8027 (4.3915)	LR 2.000e-03
0: TRAIN [0][4470/5173]	Time 0.483 (0.607)	Data 1.27e-04 (2.66e-04)	Tok/s 10756 (23319)	Loss/tok 2.7245 (4.3896)	LR 2.000e-03
0: TRAIN [0][4480/5173]	Time 0.569 (0.607)	Data 1.33e-04 (2.66e-04)	Tok/s 18406 (23320)	Loss/tok 3.3286 (4.3876)	LR 2.000e-03
0: TRAIN [0][4490/5173]	Time 0.506 (0.607)	Data 1.24e-04 (2.66e-04)	Tok/s 10299 (23306)	Loss/tok 2.8235 (4.3861)	LR 2.000e-03
0: TRAIN [0][4500/5173]	Time 0.634 (0.607)	Data 1.29e-04 (2.65e-04)	Tok/s 26425 (23309)	Loss/tok 3.5907 (4.3839)	LR 2.000e-03
0: TRAIN [0][4510/5173]	Time 0.567 (0.607)	Data 1.25e-04 (2.65e-04)	Tok/s 18026 (23314)	Loss/tok 3.2499 (4.3820)	LR 2.000e-03
0: TRAIN [0][4520/5173]	Time 0.567 (0.607)	Data 1.23e-04 (2.65e-04)	Tok/s 18232 (23309)	Loss/tok 3.3471 (4.3801)	LR 2.000e-03
0: TRAIN [0][4530/5173]	Time 0.634 (0.607)	Data 1.20e-04 (2.64e-04)	Tok/s 26505 (23308)	Loss/tok 3.5519 (4.3782)	LR 2.000e-03
0: TRAIN [0][4540/5173]	Time 0.609 (0.607)	Data 1.27e-04 (2.64e-04)	Tok/s 27555 (23316)	Loss/tok 3.4577 (4.3759)	LR 2.000e-03
0: TRAIN [0][4550/5173]	Time 0.566 (0.607)	Data 1.24e-04 (2.64e-04)	Tok/s 18076 (23307)	Loss/tok 3.3055 (4.3743)	LR 2.000e-03
0: TRAIN [0][4560/5173]	Time 0.572 (0.607)	Data 1.23e-04 (2.64e-04)	Tok/s 17808 (23297)	Loss/tok 3.3019 (4.3726)	LR 2.000e-03
0: TRAIN [0][4570/5173]	Time 0.695 (0.607)	Data 1.22e-04 (2.63e-04)	Tok/s 33494 (23297)	Loss/tok 3.6880 (4.3708)	LR 2.000e-03
0: TRAIN [0][4580/5173]	Time 0.568 (0.607)	Data 1.26e-04 (2.63e-04)	Tok/s 18317 (23293)	Loss/tok 3.2956 (4.3690)	LR 2.000e-03
0: TRAIN [0][4590/5173]	Time 0.626 (0.607)	Data 1.29e-04 (2.63e-04)	Tok/s 27013 (23297)	Loss/tok 3.5586 (4.3670)	LR 2.000e-03
0: TRAIN [0][4600/5173]	Time 0.508 (0.607)	Data 1.24e-04 (2.62e-04)	Tok/s 10197 (23294)	Loss/tok 2.8157 (4.3651)	LR 2.000e-03
0: TRAIN [0][4610/5173]	Time 0.626 (0.607)	Data 2.91e-04 (2.62e-04)	Tok/s 27025 (23292)	Loss/tok 3.4585 (4.3631)	LR 2.000e-03
0: TRAIN [0][4620/5173]	Time 0.569 (0.607)	Data 1.47e-04 (2.62e-04)	Tok/s 18001 (23298)	Loss/tok 3.2638 (4.3612)	LR 2.000e-03
0: TRAIN [0][4630/5173]	Time 0.628 (0.607)	Data 1.19e-04 (2.62e-04)	Tok/s 26746 (23303)	Loss/tok 3.6129 (4.3591)	LR 2.000e-03
0: TRAIN [0][4640/5173]	Time 0.568 (0.607)	Data 1.32e-04 (2.61e-04)	Tok/s 18378 (23299)	Loss/tok 3.2547 (4.3574)	LR 2.000e-03
0: TRAIN [0][4650/5173]	Time 0.506 (0.607)	Data 1.23e-04 (2.61e-04)	Tok/s 10654 (23294)	Loss/tok 2.8066 (4.3556)	LR 2.000e-03
0: TRAIN [0][4660/5173]	Time 0.572 (0.607)	Data 1.22e-04 (2.61e-04)	Tok/s 17985 (23298)	Loss/tok 3.2125 (4.3538)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [0][4670/5173]	Time 0.628 (0.607)	Data 1.35e-04 (2.61e-04)	Tok/s 26589 (23303)	Loss/tok 3.3887 (4.3518)	LR 2.000e-03
0: TRAIN [0][4680/5173]	Time 0.628 (0.607)	Data 3.14e-04 (2.60e-04)	Tok/s 26810 (23307)	Loss/tok 3.6453 (4.3499)	LR 2.000e-03
0: TRAIN [0][4690/5173]	Time 0.570 (0.607)	Data 1.12e-04 (2.60e-04)	Tok/s 18422 (23300)	Loss/tok 3.2966 (4.3482)	LR 2.000e-03
0: TRAIN [0][4700/5173]	Time 0.630 (0.607)	Data 1.12e-04 (2.60e-04)	Tok/s 26626 (23308)	Loss/tok 3.3997 (4.3462)	LR 2.000e-03
0: TRAIN [0][4710/5173]	Time 0.560 (0.607)	Data 1.29e-04 (2.60e-04)	Tok/s 18614 (23304)	Loss/tok 3.2425 (4.3444)	LR 2.000e-03
0: TRAIN [0][4720/5173]	Time 0.570 (0.607)	Data 1.18e-04 (2.59e-04)	Tok/s 18290 (23302)	Loss/tok 3.2981 (4.3426)	LR 2.000e-03
0: TRAIN [0][4730/5173]	Time 0.631 (0.607)	Data 1.21e-04 (2.59e-04)	Tok/s 26650 (23299)	Loss/tok 3.4110 (4.3408)	LR 2.000e-03
0: TRAIN [0][4740/5173]	Time 0.570 (0.607)	Data 1.54e-04 (2.59e-04)	Tok/s 18066 (23312)	Loss/tok 3.1297 (4.3389)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][4750/5173]	Time 0.694 (0.607)	Data 1.28e-04 (2.59e-04)	Tok/s 33353 (23318)	Loss/tok 3.6510 (4.3370)	LR 2.000e-03
0: TRAIN [0][4760/5173]	Time 0.620 (0.607)	Data 1.46e-04 (2.58e-04)	Tok/s 26987 (23321)	Loss/tok 3.4978 (4.3351)	LR 2.000e-03
0: TRAIN [0][4770/5173]	Time 0.571 (0.607)	Data 1.44e-04 (2.58e-04)	Tok/s 18322 (23322)	Loss/tok 3.2629 (4.3333)	LR 2.000e-03
0: TRAIN [0][4780/5173]	Time 0.569 (0.607)	Data 1.30e-04 (2.58e-04)	Tok/s 18156 (23318)	Loss/tok 3.3190 (4.3316)	LR 2.000e-03
0: TRAIN [0][4790/5173]	Time 0.495 (0.607)	Data 1.29e-04 (2.57e-04)	Tok/s 10742 (23316)	Loss/tok 2.7867 (4.3299)	LR 2.000e-03
0: TRAIN [0][4800/5173]	Time 0.629 (0.607)	Data 1.26e-04 (2.57e-04)	Tok/s 26977 (23319)	Loss/tok 3.5004 (4.3281)	LR 2.000e-03
0: TRAIN [0][4810/5173]	Time 0.632 (0.607)	Data 1.20e-04 (2.57e-04)	Tok/s 26523 (23320)	Loss/tok 3.4749 (4.3264)	LR 2.000e-03
0: TRAIN [0][4820/5173]	Time 0.565 (0.607)	Data 1.24e-04 (2.57e-04)	Tok/s 18186 (23320)	Loss/tok 3.1272 (4.3246)	LR 2.000e-03
0: TRAIN [0][4830/5173]	Time 0.688 (0.607)	Data 1.21e-04 (2.56e-04)	Tok/s 33566 (23321)	Loss/tok 3.7581 (4.3229)	LR 2.000e-03
0: TRAIN [0][4840/5173]	Time 0.494 (0.607)	Data 1.18e-04 (2.56e-04)	Tok/s 10811 (23323)	Loss/tok 2.7958 (4.3213)	LR 2.000e-03
0: TRAIN [0][4850/5173]	Time 0.505 (0.607)	Data 1.24e-04 (2.56e-04)	Tok/s 10534 (23317)	Loss/tok 2.8135 (4.3197)	LR 2.000e-03
0: TRAIN [0][4860/5173]	Time 0.757 (0.607)	Data 1.19e-04 (2.56e-04)	Tok/s 39111 (23324)	Loss/tok 3.8766 (4.3179)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][4870/5173]	Time 0.571 (0.607)	Data 1.16e-04 (2.55e-04)	Tok/s 18299 (23324)	Loss/tok 3.2291 (4.3162)	LR 2.000e-03
0: TRAIN [0][4880/5173]	Time 0.570 (0.607)	Data 1.16e-04 (2.55e-04)	Tok/s 18231 (23316)	Loss/tok 3.1806 (4.3147)	LR 2.000e-03
0: TRAIN [0][4890/5173]	Time 0.571 (0.607)	Data 1.24e-04 (2.55e-04)	Tok/s 17836 (23309)	Loss/tok 3.2145 (4.3131)	LR 2.000e-03
0: TRAIN [0][4900/5173]	Time 0.568 (0.607)	Data 1.19e-04 (2.55e-04)	Tok/s 18017 (23309)	Loss/tok 3.3261 (4.3114)	LR 2.000e-03
0: TRAIN [0][4910/5173]	Time 0.624 (0.607)	Data 1.13e-04 (2.54e-04)	Tok/s 27020 (23305)	Loss/tok 3.4625 (4.3097)	LR 2.000e-03
0: TRAIN [0][4920/5173]	Time 0.631 (0.607)	Data 1.28e-04 (2.54e-04)	Tok/s 26809 (23305)	Loss/tok 3.5437 (4.3080)	LR 2.000e-03
0: TRAIN [0][4930/5173]	Time 0.570 (0.607)	Data 1.18e-04 (2.54e-04)	Tok/s 18089 (23312)	Loss/tok 3.2684 (4.3063)	LR 2.000e-03
0: TRAIN [0][4940/5173]	Time 0.632 (0.607)	Data 1.17e-04 (2.53e-04)	Tok/s 26809 (23307)	Loss/tok 3.4555 (4.3048)	LR 2.000e-03
0: TRAIN [0][4950/5173]	Time 0.634 (0.607)	Data 1.20e-04 (2.53e-04)	Tok/s 26376 (23310)	Loss/tok 3.5581 (4.3031)	LR 2.000e-03
0: TRAIN [0][4960/5173]	Time 0.677 (0.607)	Data 1.17e-04 (2.53e-04)	Tok/s 34061 (23311)	Loss/tok 3.7518 (4.3014)	LR 2.000e-03
0: TRAIN [0][4970/5173]	Time 0.623 (0.607)	Data 1.17e-04 (2.53e-04)	Tok/s 27107 (23305)	Loss/tok 3.4605 (4.2999)	LR 2.000e-03
0: TRAIN [0][4980/5173]	Time 0.692 (0.607)	Data 1.18e-04 (2.52e-04)	Tok/s 33735 (23315)	Loss/tok 3.5776 (4.2981)	LR 2.000e-03
0: TRAIN [0][4990/5173]	Time 0.506 (0.607)	Data 1.20e-04 (2.52e-04)	Tok/s 10421 (23309)	Loss/tok 2.8968 (4.2966)	LR 2.000e-03
0: TRAIN [0][5000/5173]	Time 0.623 (0.607)	Data 1.13e-04 (2.52e-04)	Tok/s 26804 (23307)	Loss/tok 3.6057 (4.2950)	LR 2.000e-03
0: TRAIN [0][5010/5173]	Time 0.690 (0.607)	Data 1.20e-04 (2.52e-04)	Tok/s 34112 (23303)	Loss/tok 3.6880 (4.2934)	LR 2.000e-03
0: TRAIN [0][5020/5173]	Time 0.566 (0.607)	Data 1.19e-04 (2.51e-04)	Tok/s 18151 (23297)	Loss/tok 3.3507 (4.2919)	LR 2.000e-03
0: TRAIN [0][5030/5173]	Time 0.567 (0.607)	Data 1.14e-04 (2.51e-04)	Tok/s 18370 (23300)	Loss/tok 3.1824 (4.2903)	LR 2.000e-03
0: TRAIN [0][5040/5173]	Time 0.630 (0.607)	Data 1.17e-04 (2.51e-04)	Tok/s 26865 (23305)	Loss/tok 3.4994 (4.2886)	LR 2.000e-03
0: TRAIN [0][5050/5173]	Time 0.568 (0.607)	Data 1.27e-04 (2.51e-04)	Tok/s 18185 (23294)	Loss/tok 3.2284 (4.2872)	LR 2.000e-03
0: TRAIN [0][5060/5173]	Time 0.565 (0.607)	Data 1.19e-04 (2.50e-04)	Tok/s 18008 (23294)	Loss/tok 3.2823 (4.2856)	LR 2.000e-03
0: TRAIN [0][5070/5173]	Time 0.567 (0.607)	Data 1.11e-04 (2.50e-04)	Tok/s 18142 (23290)	Loss/tok 3.2932 (4.2841)	LR 2.000e-03
0: TRAIN [0][5080/5173]	Time 0.571 (0.607)	Data 1.25e-04 (2.50e-04)	Tok/s 18218 (23292)	Loss/tok 3.2509 (4.2825)	LR 2.000e-03
0: TRAIN [0][5090/5173]	Time 0.565 (0.607)	Data 3.12e-04 (2.50e-04)	Tok/s 18209 (23285)	Loss/tok 3.2557 (4.2810)	LR 2.000e-03
0: TRAIN [0][5100/5173]	Time 0.630 (0.607)	Data 1.17e-04 (2.49e-04)	Tok/s 26229 (23287)	Loss/tok 3.4453 (4.2794)	LR 2.000e-03
0: TRAIN [0][5110/5173]	Time 0.628 (0.607)	Data 1.16e-04 (2.49e-04)	Tok/s 26867 (23278)	Loss/tok 3.4519 (4.2780)	LR 2.000e-03
0: TRAIN [0][5120/5173]	Time 0.692 (0.607)	Data 1.28e-04 (2.49e-04)	Tok/s 33540 (23280)	Loss/tok 3.5674 (4.2764)	LR 2.000e-03
0: TRAIN [0][5130/5173]	Time 0.639 (0.607)	Data 1.14e-04 (2.49e-04)	Tok/s 26074 (23281)	Loss/tok 3.5021 (4.2748)	LR 2.000e-03
0: TRAIN [0][5140/5173]	Time 0.567 (0.607)	Data 1.23e-04 (2.48e-04)	Tok/s 18679 (23283)	Loss/tok 3.2943 (4.2731)	LR 2.000e-03
0: TRAIN [0][5150/5173]	Time 0.563 (0.607)	Data 1.19e-04 (2.48e-04)	Tok/s 18181 (23282)	Loss/tok 3.3561 (4.2716)	LR 2.000e-03
0: TRAIN [0][5160/5173]	Time 0.569 (0.607)	Data 1.15e-04 (2.48e-04)	Tok/s 17977 (23283)	Loss/tok 3.2549 (4.2700)	LR 2.000e-03
0: TRAIN [0][5170/5173]	Time 0.627 (0.607)	Data 1.18e-04 (2.48e-04)	Tok/s 27260 (23283)	Loss/tok 3.4682 (4.2684)	LR 2.000e-03
:::MLL 1586529263.501 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 525}}
:::MLL 1586529263.502 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [0][0/8]	Time 0.724 (0.724)	Decoder iters 142.0 (142.0)	Tok/s 22833 (22833)
0: Running moses detokenizer
0: BLEU(score=20.38335736404206, counts=[35290, 16485, 8853, 4944], totals=[66567, 63564, 60561, 57563], precisions=[53.01425631318822, 25.934491221446102, 14.618318719968297, 8.588850476868823], bp=1.0, sys_len=66567, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1586529269.464 eval_accuracy: {"value": 20.38, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 536}}
:::MLL 1586529269.465 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 0	Training Loss: 4.2696	Test BLEU: 20.38
0: Performance: Epoch: 0	Training: 69841 Tok/s
0: Finished epoch 0
:::MLL 1586529269.465 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 558}}
:::MLL 1586529269.465 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1586529269.466 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 515}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 3488389962
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][0/5173]	Time 1.196 (1.196)	Data 5.53e-01 (5.53e-01)	Tok/s 14030 (14030)	Loss/tok 3.4630 (3.4630)	LR 2.000e-03
0: TRAIN [1][10/5173]	Time 0.567 (0.649)	Data 1.21e-04 (5.04e-02)	Tok/s 18406 (21738)	Loss/tok 3.1916 (3.3601)	LR 2.000e-03
0: TRAIN [1][20/5173]	Time 0.567 (0.655)	Data 1.34e-04 (2.65e-02)	Tok/s 18228 (25531)	Loss/tok 3.1646 (3.4596)	LR 2.000e-03
0: TRAIN [1][30/5173]	Time 0.632 (0.647)	Data 1.31e-04 (1.80e-02)	Tok/s 27036 (25765)	Loss/tok 3.3989 (3.4539)	LR 2.000e-03
0: TRAIN [1][40/5173]	Time 0.691 (0.637)	Data 1.30e-04 (1.36e-02)	Tok/s 34167 (25359)	Loss/tok 3.4977 (3.4407)	LR 2.000e-03
0: TRAIN [1][50/5173]	Time 0.629 (0.629)	Data 1.24e-04 (1.10e-02)	Tok/s 26473 (24827)	Loss/tok 3.3837 (3.4190)	LR 2.000e-03
0: TRAIN [1][60/5173]	Time 0.696 (0.632)	Data 1.18e-04 (9.19e-03)	Tok/s 33450 (25236)	Loss/tok 3.6733 (3.4382)	LR 2.000e-03
0: TRAIN [1][70/5173]	Time 0.568 (0.627)	Data 1.25e-04 (7.91e-03)	Tok/s 18103 (24816)	Loss/tok 3.1640 (3.4265)	LR 2.000e-03
0: TRAIN [1][80/5173]	Time 0.691 (0.623)	Data 1.15e-04 (6.95e-03)	Tok/s 34094 (24505)	Loss/tok 3.4875 (3.4189)	LR 2.000e-03
0: TRAIN [1][90/5173]	Time 0.634 (0.624)	Data 1.34e-04 (6.20e-03)	Tok/s 26657 (24692)	Loss/tok 3.3582 (3.4197)	LR 2.000e-03
0: TRAIN [1][100/5173]	Time 0.506 (0.619)	Data 1.18e-04 (5.60e-03)	Tok/s 10030 (24146)	Loss/tok 2.6206 (3.4064)	LR 2.000e-03
0: TRAIN [1][110/5173]	Time 0.624 (0.618)	Data 1.22e-04 (5.11e-03)	Tok/s 27035 (24100)	Loss/tok 3.2761 (3.4055)	LR 2.000e-03
0: TRAIN [1][120/5173]	Time 0.623 (0.616)	Data 1.19e-04 (4.69e-03)	Tok/s 26983 (23950)	Loss/tok 3.3810 (3.4016)	LR 2.000e-03
0: TRAIN [1][130/5173]	Time 0.567 (0.614)	Data 1.12e-04 (4.35e-03)	Tok/s 18427 (23704)	Loss/tok 3.2873 (3.3955)	LR 2.000e-03
0: TRAIN [1][140/5173]	Time 0.694 (0.615)	Data 1.18e-04 (4.05e-03)	Tok/s 33730 (23839)	Loss/tok 3.5203 (3.4064)	LR 2.000e-03
0: TRAIN [1][150/5173]	Time 0.693 (0.615)	Data 1.13e-04 (3.78e-03)	Tok/s 33290 (23959)	Loss/tok 3.5411 (3.4101)	LR 2.000e-03
0: TRAIN [1][160/5173]	Time 0.625 (0.616)	Data 1.14e-04 (3.56e-03)	Tok/s 26624 (24037)	Loss/tok 3.3387 (3.4136)	LR 2.000e-03
0: TRAIN [1][170/5173]	Time 0.568 (0.615)	Data 1.12e-04 (3.36e-03)	Tok/s 18485 (24022)	Loss/tok 3.2711 (3.4139)	LR 2.000e-03
0: TRAIN [1][180/5173]	Time 0.508 (0.614)	Data 1.14e-04 (3.18e-03)	Tok/s 10369 (23897)	Loss/tok 2.8000 (3.4112)	LR 2.000e-03
0: TRAIN [1][190/5173]	Time 0.567 (0.614)	Data 1.25e-04 (3.02e-03)	Tok/s 18097 (23932)	Loss/tok 3.2182 (3.4120)	LR 2.000e-03
0: TRAIN [1][200/5173]	Time 0.569 (0.614)	Data 1.12e-04 (2.88e-03)	Tok/s 18169 (24013)	Loss/tok 3.2977 (3.4126)	LR 2.000e-03
0: TRAIN [1][210/5173]	Time 0.626 (0.615)	Data 1.22e-04 (2.75e-03)	Tok/s 26834 (24133)	Loss/tok 3.3841 (3.4185)	LR 2.000e-03
0: TRAIN [1][220/5173]	Time 0.566 (0.614)	Data 1.15e-04 (2.63e-03)	Tok/s 18472 (23931)	Loss/tok 3.1891 (3.4158)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][230/5173]	Time 0.687 (0.613)	Data 1.13e-04 (2.52e-03)	Tok/s 33620 (23900)	Loss/tok 3.5934 (3.4150)	LR 2.000e-03
0: TRAIN [1][240/5173]	Time 0.691 (0.613)	Data 1.11e-04 (2.42e-03)	Tok/s 33823 (23927)	Loss/tok 3.6706 (3.4166)	LR 2.000e-03
0: TRAIN [1][250/5173]	Time 0.568 (0.612)	Data 1.24e-04 (2.33e-03)	Tok/s 18180 (23836)	Loss/tok 3.2036 (3.4137)	LR 2.000e-03
0: TRAIN [1][260/5173]	Time 0.624 (0.613)	Data 1.18e-04 (2.25e-03)	Tok/s 27031 (23883)	Loss/tok 3.3766 (3.4169)	LR 2.000e-03
0: TRAIN [1][270/5173]	Time 0.566 (0.613)	Data 1.20e-04 (2.17e-03)	Tok/s 18187 (23913)	Loss/tok 3.1691 (3.4195)	LR 2.000e-03
0: TRAIN [1][280/5173]	Time 0.689 (0.614)	Data 1.22e-04 (2.10e-03)	Tok/s 33159 (23996)	Loss/tok 3.7631 (3.4214)	LR 2.000e-03
0: TRAIN [1][290/5173]	Time 0.506 (0.613)	Data 1.18e-04 (2.03e-03)	Tok/s 10547 (23870)	Loss/tok 2.7312 (3.4191)	LR 2.000e-03
0: TRAIN [1][300/5173]	Time 0.630 (0.613)	Data 1.59e-04 (1.97e-03)	Tok/s 27314 (23854)	Loss/tok 3.5071 (3.4173)	LR 2.000e-03
0: TRAIN [1][310/5173]	Time 0.627 (0.613)	Data 3.03e-04 (1.91e-03)	Tok/s 26654 (23880)	Loss/tok 3.4056 (3.4175)	LR 2.000e-03
0: TRAIN [1][320/5173]	Time 0.564 (0.613)	Data 1.24e-04 (1.85e-03)	Tok/s 18312 (23910)	Loss/tok 3.2626 (3.4169)	LR 2.000e-03
0: TRAIN [1][330/5173]	Time 0.568 (0.613)	Data 1.21e-04 (1.80e-03)	Tok/s 18210 (23875)	Loss/tok 3.2001 (3.4166)	LR 2.000e-03
0: TRAIN [1][340/5173]	Time 0.689 (0.613)	Data 1.20e-04 (1.75e-03)	Tok/s 33506 (23897)	Loss/tok 3.5914 (3.4169)	LR 2.000e-03
0: TRAIN [1][350/5173]	Time 0.570 (0.613)	Data 1.22e-04 (1.71e-03)	Tok/s 18005 (23893)	Loss/tok 3.1269 (3.4173)	LR 2.000e-03
0: TRAIN [1][360/5173]	Time 0.569 (0.612)	Data 1.21e-04 (1.66e-03)	Tok/s 18066 (23798)	Loss/tok 3.0727 (3.4143)	LR 2.000e-03
0: TRAIN [1][370/5173]	Time 0.568 (0.612)	Data 1.40e-04 (1.62e-03)	Tok/s 18280 (23777)	Loss/tok 3.1377 (3.4140)	LR 2.000e-03
0: TRAIN [1][380/5173]	Time 0.507 (0.611)	Data 1.27e-04 (1.58e-03)	Tok/s 10481 (23720)	Loss/tok 2.7914 (3.4111)	LR 2.000e-03
0: TRAIN [1][390/5173]	Time 0.567 (0.611)	Data 1.20e-04 (1.54e-03)	Tok/s 17967 (23737)	Loss/tok 3.1520 (3.4120)	LR 2.000e-03
0: TRAIN [1][400/5173]	Time 0.625 (0.611)	Data 1.21e-04 (1.51e-03)	Tok/s 26973 (23723)	Loss/tok 3.1677 (3.4112)	LR 2.000e-03
0: TRAIN [1][410/5173]	Time 0.507 (0.611)	Data 1.20e-04 (1.48e-03)	Tok/s 10627 (23670)	Loss/tok 2.8517 (3.4097)	LR 2.000e-03
0: TRAIN [1][420/5173]	Time 0.571 (0.611)	Data 1.49e-04 (1.44e-03)	Tok/s 17705 (23747)	Loss/tok 3.1310 (3.4147)	LR 2.000e-03
0: TRAIN [1][430/5173]	Time 0.506 (0.610)	Data 1.19e-04 (1.41e-03)	Tok/s 10311 (23585)	Loss/tok 2.8258 (3.4109)	LR 2.000e-03
0: TRAIN [1][440/5173]	Time 0.629 (0.610)	Data 1.18e-04 (1.38e-03)	Tok/s 26545 (23575)	Loss/tok 3.3890 (3.4098)	LR 2.000e-03
0: TRAIN [1][450/5173]	Time 0.692 (0.609)	Data 1.26e-04 (1.36e-03)	Tok/s 34040 (23510)	Loss/tok 3.4649 (3.4079)	LR 2.000e-03
0: TRAIN [1][460/5173]	Time 0.567 (0.609)	Data 1.20e-04 (1.33e-03)	Tok/s 18110 (23495)	Loss/tok 3.2341 (3.4076)	LR 2.000e-03
0: TRAIN [1][470/5173]	Time 0.678 (0.609)	Data 1.19e-04 (1.31e-03)	Tok/s 34817 (23486)	Loss/tok 3.5216 (3.4075)	LR 2.000e-03
0: TRAIN [1][480/5173]	Time 0.567 (0.609)	Data 1.26e-04 (1.28e-03)	Tok/s 18160 (23438)	Loss/tok 3.2826 (3.4069)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][490/5173]	Time 0.568 (0.609)	Data 1.25e-04 (1.26e-03)	Tok/s 17943 (23431)	Loss/tok 3.1278 (3.4061)	LR 2.000e-03
0: TRAIN [1][500/5173]	Time 0.690 (0.609)	Data 1.22e-04 (1.24e-03)	Tok/s 33777 (23456)	Loss/tok 3.5440 (3.4066)	LR 2.000e-03
0: TRAIN [1][510/5173]	Time 0.568 (0.610)	Data 1.24e-04 (1.22e-03)	Tok/s 18260 (23533)	Loss/tok 3.2659 (3.4101)	LR 2.000e-03
0: TRAIN [1][520/5173]	Time 0.497 (0.609)	Data 1.21e-04 (1.19e-03)	Tok/s 10305 (23504)	Loss/tok 2.8317 (3.4097)	LR 2.000e-03
0: TRAIN [1][530/5173]	Time 0.566 (0.609)	Data 1.20e-04 (1.17e-03)	Tok/s 18334 (23508)	Loss/tok 3.2497 (3.4103)	LR 2.000e-03
0: TRAIN [1][540/5173]	Time 0.507 (0.609)	Data 1.22e-04 (1.16e-03)	Tok/s 10444 (23489)	Loss/tok 2.6832 (3.4090)	LR 2.000e-03
0: TRAIN [1][550/5173]	Time 0.565 (0.608)	Data 1.23e-04 (1.14e-03)	Tok/s 18021 (23411)	Loss/tok 3.2103 (3.4066)	LR 2.000e-03
0: TRAIN [1][560/5173]	Time 0.631 (0.609)	Data 1.29e-04 (1.12e-03)	Tok/s 26456 (23471)	Loss/tok 3.4610 (3.4080)	LR 2.000e-03
0: TRAIN [1][570/5173]	Time 0.633 (0.608)	Data 1.34e-04 (1.10e-03)	Tok/s 26636 (23418)	Loss/tok 3.4069 (3.4074)	LR 2.000e-03
0: TRAIN [1][580/5173]	Time 0.568 (0.608)	Data 1.63e-04 (1.08e-03)	Tok/s 18477 (23380)	Loss/tok 3.1651 (3.4067)	LR 2.000e-03
0: TRAIN [1][590/5173]	Time 0.630 (0.608)	Data 1.22e-04 (1.07e-03)	Tok/s 26637 (23359)	Loss/tok 3.4139 (3.4057)	LR 2.000e-03
0: TRAIN [1][600/5173]	Time 0.506 (0.608)	Data 1.20e-04 (1.05e-03)	Tok/s 10659 (23314)	Loss/tok 2.6827 (3.4047)	LR 2.000e-03
0: TRAIN [1][610/5173]	Time 0.630 (0.607)	Data 1.24e-04 (1.04e-03)	Tok/s 26467 (23270)	Loss/tok 3.4293 (3.4039)	LR 2.000e-03
0: TRAIN [1][620/5173]	Time 0.566 (0.607)	Data 1.61e-04 (1.02e-03)	Tok/s 18367 (23221)	Loss/tok 3.1321 (3.4017)	LR 2.000e-03
0: TRAIN [1][630/5173]	Time 0.626 (0.606)	Data 1.21e-04 (1.01e-03)	Tok/s 27110 (23171)	Loss/tok 3.4635 (3.4003)	LR 2.000e-03
0: TRAIN [1][640/5173]	Time 0.628 (0.606)	Data 1.19e-04 (9.95e-04)	Tok/s 26772 (23185)	Loss/tok 3.4473 (3.3998)	LR 2.000e-03
0: TRAIN [1][650/5173]	Time 0.495 (0.606)	Data 1.20e-04 (9.82e-04)	Tok/s 10782 (23101)	Loss/tok 2.6854 (3.3977)	LR 2.000e-03
0: TRAIN [1][660/5173]	Time 0.625 (0.606)	Data 1.21e-04 (9.69e-04)	Tok/s 26740 (23111)	Loss/tok 3.4255 (3.3981)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][670/5173]	Time 0.566 (0.606)	Data 1.18e-04 (9.57e-04)	Tok/s 17835 (23150)	Loss/tok 3.1884 (3.4010)	LR 2.000e-03
0: TRAIN [1][680/5173]	Time 0.692 (0.606)	Data 1.21e-04 (9.44e-04)	Tok/s 34002 (23177)	Loss/tok 3.4602 (3.4007)	LR 2.000e-03
0: TRAIN [1][690/5173]	Time 0.569 (0.606)	Data 1.24e-04 (9.32e-04)	Tok/s 18041 (23144)	Loss/tok 3.1656 (3.3989)	LR 2.000e-03
0: TRAIN [1][700/5173]	Time 0.627 (0.606)	Data 1.19e-04 (9.21e-04)	Tok/s 26971 (23196)	Loss/tok 3.3171 (3.4008)	LR 2.000e-03
0: TRAIN [1][710/5173]	Time 0.490 (0.606)	Data 1.21e-04 (9.10e-04)	Tok/s 10747 (23185)	Loss/tok 2.8016 (3.4000)	LR 2.000e-03
0: TRAIN [1][720/5173]	Time 0.570 (0.606)	Data 1.30e-04 (8.99e-04)	Tok/s 18381 (23204)	Loss/tok 3.1283 (3.4008)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][730/5173]	Time 0.569 (0.607)	Data 1.19e-04 (8.89e-04)	Tok/s 18022 (23235)	Loss/tok 3.3061 (3.4016)	LR 2.000e-03
0: TRAIN [1][740/5173]	Time 0.619 (0.606)	Data 1.22e-04 (8.79e-04)	Tok/s 27017 (23192)	Loss/tok 3.4063 (3.3999)	LR 2.000e-03
0: TRAIN [1][750/5173]	Time 0.693 (0.607)	Data 1.24e-04 (8.69e-04)	Tok/s 33737 (23248)	Loss/tok 3.5687 (3.4018)	LR 2.000e-03
0: TRAIN [1][760/5173]	Time 0.566 (0.607)	Data 1.21e-04 (8.59e-04)	Tok/s 17963 (23233)	Loss/tok 3.2086 (3.4014)	LR 2.000e-03
0: TRAIN [1][770/5173]	Time 0.565 (0.607)	Data 1.23e-04 (8.50e-04)	Tok/s 17904 (23249)	Loss/tok 3.1741 (3.4015)	LR 2.000e-03
0: TRAIN [1][780/5173]	Time 0.631 (0.607)	Data 1.22e-04 (8.40e-04)	Tok/s 26463 (23279)	Loss/tok 3.4145 (3.4016)	LR 2.000e-03
0: TRAIN [1][790/5173]	Time 0.568 (0.607)	Data 1.30e-04 (8.32e-04)	Tok/s 18342 (23238)	Loss/tok 3.0239 (3.4001)	LR 2.000e-03
0: TRAIN [1][800/5173]	Time 0.568 (0.607)	Data 1.19e-04 (8.23e-04)	Tok/s 18093 (23241)	Loss/tok 3.2447 (3.4000)	LR 2.000e-03
0: TRAIN [1][810/5173]	Time 0.507 (0.607)	Data 1.23e-04 (8.14e-04)	Tok/s 10412 (23254)	Loss/tok 2.7216 (3.3997)	LR 2.000e-03
0: TRAIN [1][820/5173]	Time 0.687 (0.607)	Data 1.17e-04 (8.06e-04)	Tok/s 33705 (23281)	Loss/tok 3.5553 (3.4000)	LR 2.000e-03
0: TRAIN [1][830/5173]	Time 0.692 (0.607)	Data 1.29e-04 (7.98e-04)	Tok/s 34194 (23284)	Loss/tok 3.5517 (3.4002)	LR 2.000e-03
0: TRAIN [1][840/5173]	Time 0.628 (0.606)	Data 1.28e-04 (7.90e-04)	Tok/s 26728 (23247)	Loss/tok 3.4350 (3.3991)	LR 2.000e-03
0: TRAIN [1][850/5173]	Time 0.558 (0.606)	Data 1.23e-04 (7.82e-04)	Tok/s 18429 (23247)	Loss/tok 3.1681 (3.3982)	LR 2.000e-03
0: TRAIN [1][860/5173]	Time 0.567 (0.606)	Data 1.25e-04 (7.74e-04)	Tok/s 18079 (23222)	Loss/tok 3.1976 (3.3980)	LR 2.000e-03
0: TRAIN [1][870/5173]	Time 0.567 (0.606)	Data 1.20e-04 (7.67e-04)	Tok/s 18369 (23222)	Loss/tok 3.1807 (3.3976)	LR 2.000e-03
0: TRAIN [1][880/5173]	Time 0.505 (0.606)	Data 1.22e-04 (7.60e-04)	Tok/s 10783 (23207)	Loss/tok 2.7335 (3.3977)	LR 2.000e-03
0: TRAIN [1][890/5173]	Time 0.627 (0.607)	Data 1.29e-04 (7.53e-04)	Tok/s 27133 (23252)	Loss/tok 3.3504 (3.3993)	LR 2.000e-03
0: TRAIN [1][900/5173]	Time 0.623 (0.606)	Data 1.21e-04 (7.46e-04)	Tok/s 26635 (23206)	Loss/tok 3.3555 (3.3978)	LR 2.000e-03
0: TRAIN [1][910/5173]	Time 0.567 (0.606)	Data 1.22e-04 (7.40e-04)	Tok/s 18607 (23224)	Loss/tok 3.3130 (3.3976)	LR 2.000e-03
0: TRAIN [1][920/5173]	Time 0.629 (0.606)	Data 1.29e-04 (7.33e-04)	Tok/s 26717 (23208)	Loss/tok 3.4803 (3.3967)	LR 2.000e-03
0: TRAIN [1][930/5173]	Time 0.550 (0.606)	Data 1.22e-04 (7.26e-04)	Tok/s 18775 (23234)	Loss/tok 3.2499 (3.3981)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][940/5173]	Time 0.567 (0.606)	Data 1.20e-04 (7.20e-04)	Tok/s 18807 (23181)	Loss/tok 3.0639 (3.3971)	LR 2.000e-03
0: TRAIN [1][950/5173]	Time 0.567 (0.606)	Data 1.19e-04 (7.14e-04)	Tok/s 18277 (23197)	Loss/tok 3.1298 (3.3987)	LR 2.000e-03
0: TRAIN [1][960/5173]	Time 0.693 (0.606)	Data 1.36e-04 (7.08e-04)	Tok/s 33462 (23235)	Loss/tok 3.6477 (3.4003)	LR 2.000e-03
0: TRAIN [1][970/5173]	Time 0.566 (0.606)	Data 1.23e-04 (7.02e-04)	Tok/s 18212 (23234)	Loss/tok 3.1444 (3.3999)	LR 2.000e-03
0: TRAIN [1][980/5173]	Time 0.567 (0.606)	Data 1.23e-04 (6.96e-04)	Tok/s 17976 (23207)	Loss/tok 3.1414 (3.3989)	LR 2.000e-03
0: TRAIN [1][990/5173]	Time 0.565 (0.606)	Data 1.23e-04 (6.90e-04)	Tok/s 18241 (23180)	Loss/tok 3.0755 (3.3987)	LR 2.000e-03
0: TRAIN [1][1000/5173]	Time 0.504 (0.606)	Data 1.79e-04 (6.85e-04)	Tok/s 10254 (23177)	Loss/tok 2.7733 (3.3987)	LR 2.000e-03
0: TRAIN [1][1010/5173]	Time 0.567 (0.606)	Data 1.22e-04 (6.80e-04)	Tok/s 17822 (23177)	Loss/tok 3.1600 (3.3980)	LR 2.000e-03
0: TRAIN [1][1020/5173]	Time 0.506 (0.606)	Data 1.27e-04 (6.74e-04)	Tok/s 10415 (23199)	Loss/tok 2.7445 (3.3980)	LR 2.000e-03
0: TRAIN [1][1030/5173]	Time 0.568 (0.606)	Data 1.18e-04 (6.69e-04)	Tok/s 18290 (23159)	Loss/tok 3.2060 (3.3970)	LR 2.000e-03
0: TRAIN [1][1040/5173]	Time 0.769 (0.606)	Data 1.21e-04 (6.64e-04)	Tok/s 38781 (23212)	Loss/tok 3.7944 (3.3986)	LR 2.000e-03
0: TRAIN [1][1050/5173]	Time 0.504 (0.606)	Data 1.16e-04 (6.59e-04)	Tok/s 10269 (23196)	Loss/tok 2.8093 (3.3984)	LR 2.000e-03
0: TRAIN [1][1060/5173]	Time 0.770 (0.606)	Data 1.19e-04 (6.54e-04)	Tok/s 38895 (23220)	Loss/tok 3.7829 (3.3993)	LR 2.000e-03
0: TRAIN [1][1070/5173]	Time 0.565 (0.606)	Data 1.21e-04 (6.49e-04)	Tok/s 18382 (23190)	Loss/tok 2.9907 (3.3980)	LR 2.000e-03
0: TRAIN [1][1080/5173]	Time 0.506 (0.606)	Data 1.26e-04 (6.44e-04)	Tok/s 10336 (23179)	Loss/tok 2.6725 (3.3981)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1090/5173]	Time 0.692 (0.606)	Data 1.22e-04 (6.40e-04)	Tok/s 33545 (23211)	Loss/tok 3.6041 (3.3988)	LR 2.000e-03
0: TRAIN [1][1100/5173]	Time 0.569 (0.606)	Data 1.22e-04 (6.35e-04)	Tok/s 17988 (23225)	Loss/tok 3.1318 (3.3986)	LR 2.000e-03
0: TRAIN [1][1110/5173]	Time 0.691 (0.606)	Data 1.23e-04 (6.31e-04)	Tok/s 33750 (23230)	Loss/tok 3.4874 (3.3985)	LR 2.000e-03
0: TRAIN [1][1120/5173]	Time 0.626 (0.606)	Data 1.22e-04 (6.26e-04)	Tok/s 27079 (23255)	Loss/tok 3.3492 (3.3996)	LR 2.000e-03
0: TRAIN [1][1130/5173]	Time 0.629 (0.606)	Data 1.22e-04 (6.22e-04)	Tok/s 26942 (23228)	Loss/tok 3.2214 (3.3983)	LR 2.000e-03
0: TRAIN [1][1140/5173]	Time 0.622 (0.606)	Data 1.21e-04 (6.18e-04)	Tok/s 26928 (23245)	Loss/tok 3.3801 (3.3987)	LR 2.000e-03
0: TRAIN [1][1150/5173]	Time 0.690 (0.606)	Data 1.18e-04 (6.13e-04)	Tok/s 33895 (23230)	Loss/tok 3.4929 (3.3976)	LR 2.000e-03
0: TRAIN [1][1160/5173]	Time 0.568 (0.606)	Data 1.23e-04 (6.09e-04)	Tok/s 18084 (23209)	Loss/tok 3.2238 (3.3968)	LR 2.000e-03
0: TRAIN [1][1170/5173]	Time 0.688 (0.606)	Data 1.31e-04 (6.05e-04)	Tok/s 33776 (23220)	Loss/tok 3.5823 (3.3971)	LR 2.000e-03
0: TRAIN [1][1180/5173]	Time 0.507 (0.606)	Data 1.24e-04 (6.01e-04)	Tok/s 10286 (23203)	Loss/tok 2.7194 (3.3967)	LR 2.000e-03
0: TRAIN [1][1190/5173]	Time 0.630 (0.606)	Data 1.26e-04 (5.97e-04)	Tok/s 26807 (23246)	Loss/tok 3.4915 (3.3975)	LR 2.000e-03
0: TRAIN [1][1200/5173]	Time 0.692 (0.606)	Data 1.24e-04 (5.93e-04)	Tok/s 33770 (23257)	Loss/tok 3.5443 (3.3977)	LR 2.000e-03
0: TRAIN [1][1210/5173]	Time 0.631 (0.607)	Data 1.27e-04 (5.89e-04)	Tok/s 26526 (23263)	Loss/tok 3.3536 (3.3977)	LR 2.000e-03
0: TRAIN [1][1220/5173]	Time 0.568 (0.606)	Data 1.22e-04 (5.86e-04)	Tok/s 18302 (23264)	Loss/tok 3.1121 (3.3974)	LR 2.000e-03
0: TRAIN [1][1230/5173]	Time 0.565 (0.607)	Data 1.22e-04 (5.82e-04)	Tok/s 17995 (23270)	Loss/tok 3.1088 (3.3970)	LR 2.000e-03
0: TRAIN [1][1240/5173]	Time 0.627 (0.606)	Data 1.23e-04 (5.78e-04)	Tok/s 26462 (23263)	Loss/tok 3.3296 (3.3964)	LR 2.000e-03
0: TRAIN [1][1250/5173]	Time 0.760 (0.607)	Data 1.25e-04 (5.75e-04)	Tok/s 39355 (23304)	Loss/tok 3.7950 (3.3975)	LR 2.000e-03
0: TRAIN [1][1260/5173]	Time 0.566 (0.606)	Data 1.23e-04 (5.71e-04)	Tok/s 18045 (23271)	Loss/tok 3.2079 (3.3966)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1270/5173]	Time 0.627 (0.606)	Data 1.30e-04 (5.67e-04)	Tok/s 26642 (23279)	Loss/tok 3.4217 (3.3963)	LR 2.000e-03
0: TRAIN [1][1280/5173]	Time 0.690 (0.607)	Data 1.47e-04 (5.64e-04)	Tok/s 33512 (23313)	Loss/tok 3.6919 (3.3975)	LR 2.000e-03
0: TRAIN [1][1290/5173]	Time 0.626 (0.607)	Data 3.31e-04 (5.61e-04)	Tok/s 26952 (23308)	Loss/tok 3.4260 (3.3968)	LR 2.000e-03
0: TRAIN [1][1300/5173]	Time 0.569 (0.606)	Data 3.29e-04 (5.58e-04)	Tok/s 18149 (23288)	Loss/tok 3.2223 (3.3964)	LR 2.000e-03
0: TRAIN [1][1310/5173]	Time 0.564 (0.606)	Data 3.26e-04 (5.55e-04)	Tok/s 18455 (23268)	Loss/tok 3.1338 (3.3958)	LR 2.000e-03
0: TRAIN [1][1320/5173]	Time 0.503 (0.606)	Data 1.35e-04 (5.51e-04)	Tok/s 10319 (23269)	Loss/tok 2.6776 (3.3953)	LR 1.000e-03
0: TRAIN [1][1330/5173]	Time 0.630 (0.606)	Data 1.30e-04 (5.48e-04)	Tok/s 27035 (23287)	Loss/tok 3.4005 (3.3960)	LR 1.000e-03
0: TRAIN [1][1340/5173]	Time 0.551 (0.606)	Data 1.50e-04 (5.45e-04)	Tok/s 18472 (23281)	Loss/tok 3.2697 (3.3953)	LR 1.000e-03
0: TRAIN [1][1350/5173]	Time 0.567 (0.606)	Data 1.37e-04 (5.42e-04)	Tok/s 18179 (23263)	Loss/tok 3.1488 (3.3945)	LR 1.000e-03
0: TRAIN [1][1360/5173]	Time 0.565 (0.606)	Data 1.85e-04 (5.39e-04)	Tok/s 18599 (23245)	Loss/tok 3.0322 (3.3937)	LR 1.000e-03
0: TRAIN [1][1370/5173]	Time 0.568 (0.606)	Data 1.18e-04 (5.36e-04)	Tok/s 18400 (23232)	Loss/tok 3.1597 (3.3930)	LR 1.000e-03
0: TRAIN [1][1380/5173]	Time 0.694 (0.606)	Data 1.59e-04 (5.33e-04)	Tok/s 33433 (23231)	Loss/tok 3.4778 (3.3924)	LR 1.000e-03
0: TRAIN [1][1390/5173]	Time 0.489 (0.606)	Data 1.18e-04 (5.31e-04)	Tok/s 10924 (23225)	Loss/tok 2.7365 (3.3918)	LR 1.000e-03
0: TRAIN [1][1400/5173]	Time 0.566 (0.606)	Data 1.26e-04 (5.28e-04)	Tok/s 18427 (23216)	Loss/tok 3.0812 (3.3914)	LR 1.000e-03
0: TRAIN [1][1410/5173]	Time 0.629 (0.606)	Data 1.45e-04 (5.25e-04)	Tok/s 26397 (23219)	Loss/tok 3.3442 (3.3912)	LR 1.000e-03
0: TRAIN [1][1420/5173]	Time 0.569 (0.606)	Data 1.73e-04 (5.22e-04)	Tok/s 18117 (23236)	Loss/tok 3.2073 (3.3918)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1430/5173]	Time 0.567 (0.606)	Data 1.56e-04 (5.20e-04)	Tok/s 18149 (23248)	Loss/tok 3.1546 (3.3922)	LR 1.000e-03
0: TRAIN [1][1440/5173]	Time 0.764 (0.606)	Data 1.22e-04 (5.17e-04)	Tok/s 38579 (23288)	Loss/tok 3.7198 (3.3929)	LR 1.000e-03
0: TRAIN [1][1450/5173]	Time 0.568 (0.606)	Data 1.29e-04 (5.14e-04)	Tok/s 18480 (23289)	Loss/tok 3.1276 (3.3928)	LR 1.000e-03
0: TRAIN [1][1460/5173]	Time 0.628 (0.606)	Data 1.20e-04 (5.12e-04)	Tok/s 27044 (23290)	Loss/tok 3.2683 (3.3927)	LR 1.000e-03
0: TRAIN [1][1470/5173]	Time 0.631 (0.606)	Data 1.22e-04 (5.09e-04)	Tok/s 26802 (23284)	Loss/tok 3.4193 (3.3921)	LR 1.000e-03
0: TRAIN [1][1480/5173]	Time 0.490 (0.606)	Data 1.30e-04 (5.06e-04)	Tok/s 10887 (23260)	Loss/tok 2.7733 (3.3912)	LR 1.000e-03
0: TRAIN [1][1490/5173]	Time 0.565 (0.606)	Data 1.22e-04 (5.04e-04)	Tok/s 18384 (23248)	Loss/tok 3.1703 (3.3908)	LR 1.000e-03
0: TRAIN [1][1500/5173]	Time 0.571 (0.606)	Data 1.16e-04 (5.01e-04)	Tok/s 18121 (23209)	Loss/tok 3.0817 (3.3894)	LR 1.000e-03
0: TRAIN [1][1510/5173]	Time 0.567 (0.606)	Data 1.26e-04 (4.99e-04)	Tok/s 18635 (23208)	Loss/tok 3.2389 (3.3891)	LR 1.000e-03
0: TRAIN [1][1520/5173]	Time 0.567 (0.606)	Data 1.20e-04 (4.96e-04)	Tok/s 18413 (23197)	Loss/tok 3.1118 (3.3883)	LR 1.000e-03
0: TRAIN [1][1530/5173]	Time 0.695 (0.606)	Data 1.42e-04 (4.94e-04)	Tok/s 33748 (23226)	Loss/tok 3.5630 (3.3885)	LR 1.000e-03
0: TRAIN [1][1540/5173]	Time 0.558 (0.606)	Data 1.19e-04 (4.92e-04)	Tok/s 18625 (23220)	Loss/tok 3.0341 (3.3882)	LR 1.000e-03
0: TRAIN [1][1550/5173]	Time 0.623 (0.606)	Data 1.32e-04 (4.90e-04)	Tok/s 27367 (23242)	Loss/tok 3.2921 (3.3881)	LR 1.000e-03
0: TRAIN [1][1560/5173]	Time 0.625 (0.606)	Data 1.24e-04 (4.87e-04)	Tok/s 27296 (23227)	Loss/tok 3.4060 (3.3872)	LR 1.000e-03
0: TRAIN [1][1570/5173]	Time 0.627 (0.606)	Data 1.33e-04 (4.85e-04)	Tok/s 26631 (23254)	Loss/tok 3.2926 (3.3880)	LR 1.000e-03
0: TRAIN [1][1580/5173]	Time 0.572 (0.606)	Data 1.20e-04 (4.83e-04)	Tok/s 18333 (23248)	Loss/tok 3.1137 (3.3876)	LR 1.000e-03
0: TRAIN [1][1590/5173]	Time 0.691 (0.606)	Data 1.22e-04 (4.80e-04)	Tok/s 33647 (23252)	Loss/tok 3.5464 (3.3874)	LR 1.000e-03
0: TRAIN [1][1600/5173]	Time 0.626 (0.606)	Data 1.29e-04 (4.78e-04)	Tok/s 26615 (23261)	Loss/tok 3.4428 (3.3872)	LR 1.000e-03
0: TRAIN [1][1610/5173]	Time 0.627 (0.606)	Data 1.37e-04 (4.76e-04)	Tok/s 27191 (23285)	Loss/tok 3.3214 (3.3872)	LR 1.000e-03
0: TRAIN [1][1620/5173]	Time 0.634 (0.606)	Data 1.58e-04 (4.75e-04)	Tok/s 26191 (23291)	Loss/tok 3.4037 (3.3874)	LR 1.000e-03
0: TRAIN [1][1630/5173]	Time 0.677 (0.606)	Data 3.48e-04 (4.73e-04)	Tok/s 34150 (23281)	Loss/tok 3.4125 (3.3867)	LR 1.000e-03
0: TRAIN [1][1640/5173]	Time 0.567 (0.606)	Data 1.31e-04 (4.71e-04)	Tok/s 18025 (23293)	Loss/tok 3.0502 (3.3865)	LR 1.000e-03
0: TRAIN [1][1650/5173]	Time 0.569 (0.606)	Data 1.28e-04 (4.69e-04)	Tok/s 18027 (23297)	Loss/tok 3.1623 (3.3860)	LR 1.000e-03
0: TRAIN [1][1660/5173]	Time 0.691 (0.606)	Data 1.88e-04 (4.67e-04)	Tok/s 33726 (23291)	Loss/tok 3.4592 (3.3854)	LR 1.000e-03
0: TRAIN [1][1670/5173]	Time 0.565 (0.606)	Data 1.90e-04 (4.66e-04)	Tok/s 18481 (23266)	Loss/tok 3.1464 (3.3845)	LR 1.000e-03
0: TRAIN [1][1680/5173]	Time 0.768 (0.606)	Data 1.35e-04 (4.64e-04)	Tok/s 38607 (23281)	Loss/tok 3.7209 (3.3848)	LR 1.000e-03
0: TRAIN [1][1690/5173]	Time 0.569 (0.606)	Data 1.90e-04 (4.63e-04)	Tok/s 18082 (23292)	Loss/tok 3.0646 (3.3846)	LR 1.000e-03
0: TRAIN [1][1700/5173]	Time 0.628 (0.606)	Data 3.77e-04 (4.61e-04)	Tok/s 27172 (23306)	Loss/tok 3.2074 (3.3842)	LR 1.000e-03
0: TRAIN [1][1710/5173]	Time 0.624 (0.606)	Data 1.87e-04 (4.60e-04)	Tok/s 27355 (23287)	Loss/tok 3.4346 (3.3834)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][1720/5173]	Time 0.761 (0.606)	Data 1.35e-04 (4.58e-04)	Tok/s 39241 (23298)	Loss/tok 3.6220 (3.3834)	LR 1.000e-03
0: TRAIN [1][1730/5173]	Time 0.568 (0.606)	Data 1.54e-04 (4.57e-04)	Tok/s 17950 (23306)	Loss/tok 3.2239 (3.3833)	LR 1.000e-03
0: TRAIN [1][1740/5173]	Time 0.626 (0.606)	Data 1.44e-04 (4.55e-04)	Tok/s 26576 (23301)	Loss/tok 3.3336 (3.3827)	LR 1.000e-03
0: TRAIN [1][1750/5173]	Time 0.692 (0.607)	Data 1.76e-04 (4.53e-04)	Tok/s 33606 (23323)	Loss/tok 3.5694 (3.3834)	LR 1.000e-03
0: TRAIN [1][1760/5173]	Time 0.566 (0.607)	Data 1.41e-04 (4.52e-04)	Tok/s 18326 (23345)	Loss/tok 3.0968 (3.3834)	LR 1.000e-03
0: TRAIN [1][1770/5173]	Time 0.507 (0.607)	Data 1.43e-04 (4.50e-04)	Tok/s 10282 (23351)	Loss/tok 2.6456 (3.3829)	LR 1.000e-03
0: TRAIN [1][1780/5173]	Time 0.629 (0.607)	Data 1.31e-04 (4.49e-04)	Tok/s 26624 (23367)	Loss/tok 3.2700 (3.3826)	LR 1.000e-03
0: TRAIN [1][1790/5173]	Time 0.567 (0.607)	Data 3.15e-04 (4.47e-04)	Tok/s 18444 (23357)	Loss/tok 3.0309 (3.3819)	LR 1.000e-03
0: TRAIN [1][1800/5173]	Time 0.566 (0.607)	Data 1.33e-04 (4.46e-04)	Tok/s 18223 (23341)	Loss/tok 3.0544 (3.3812)	LR 1.000e-03
0: TRAIN [1][1810/5173]	Time 0.628 (0.607)	Data 2.04e-04 (4.44e-04)	Tok/s 26931 (23325)	Loss/tok 3.4971 (3.3810)	LR 1.000e-03
0: TRAIN [1][1820/5173]	Time 0.696 (0.607)	Data 1.34e-04 (4.43e-04)	Tok/s 33239 (23323)	Loss/tok 3.4681 (3.3805)	LR 1.000e-03
0: TRAIN [1][1830/5173]	Time 0.569 (0.607)	Data 3.93e-04 (4.42e-04)	Tok/s 18187 (23331)	Loss/tok 3.0351 (3.3806)	LR 1.000e-03
0: TRAIN [1][1840/5173]	Time 0.567 (0.607)	Data 1.87e-04 (4.41e-04)	Tok/s 18125 (23330)	Loss/tok 3.1262 (3.3803)	LR 1.000e-03
0: TRAIN [1][1850/5173]	Time 0.565 (0.606)	Data 1.83e-04 (4.39e-04)	Tok/s 18288 (23303)	Loss/tok 3.0684 (3.3792)	LR 1.000e-03
0: TRAIN [1][1860/5173]	Time 0.569 (0.606)	Data 2.04e-04 (4.38e-04)	Tok/s 18164 (23303)	Loss/tok 3.0457 (3.3788)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][1870/5173]	Time 0.693 (0.606)	Data 1.42e-04 (4.37e-04)	Tok/s 33606 (23314)	Loss/tok 3.5109 (3.3784)	LR 1.000e-03
0: TRAIN [1][1880/5173]	Time 0.566 (0.606)	Data 1.60e-04 (4.35e-04)	Tok/s 18094 (23304)	Loss/tok 3.0980 (3.3777)	LR 1.000e-03
0: TRAIN [1][1890/5173]	Time 0.754 (0.606)	Data 1.30e-04 (4.34e-04)	Tok/s 39760 (23299)	Loss/tok 3.5997 (3.3772)	LR 1.000e-03
0: TRAIN [1][1900/5173]	Time 0.567 (0.606)	Data 1.29e-04 (4.33e-04)	Tok/s 18125 (23298)	Loss/tok 3.1381 (3.3768)	LR 1.000e-03
0: TRAIN [1][1910/5173]	Time 0.564 (0.606)	Data 1.90e-04 (4.31e-04)	Tok/s 18160 (23300)	Loss/tok 3.1118 (3.3766)	LR 1.000e-03
0: TRAIN [1][1920/5173]	Time 0.568 (0.606)	Data 1.70e-04 (4.30e-04)	Tok/s 17875 (23295)	Loss/tok 3.1907 (3.3761)	LR 1.000e-03
0: TRAIN [1][1930/5173]	Time 0.626 (0.606)	Data 1.37e-04 (4.29e-04)	Tok/s 27132 (23294)	Loss/tok 3.3594 (3.3756)	LR 1.000e-03
0: TRAIN [1][1940/5173]	Time 0.566 (0.606)	Data 1.88e-04 (4.27e-04)	Tok/s 18084 (23281)	Loss/tok 3.1259 (3.3750)	LR 1.000e-03
0: TRAIN [1][1950/5173]	Time 0.626 (0.606)	Data 1.94e-04 (4.26e-04)	Tok/s 26876 (23300)	Loss/tok 3.3439 (3.3751)	LR 1.000e-03
0: TRAIN [1][1960/5173]	Time 0.758 (0.606)	Data 1.48e-04 (4.25e-04)	Tok/s 38717 (23324)	Loss/tok 3.7637 (3.3755)	LR 1.000e-03
0: TRAIN [1][1970/5173]	Time 0.625 (0.607)	Data 1.72e-04 (4.24e-04)	Tok/s 27261 (23337)	Loss/tok 3.3522 (3.3755)	LR 1.000e-03
0: TRAIN [1][1980/5173]	Time 0.564 (0.606)	Data 1.41e-04 (4.23e-04)	Tok/s 18556 (23329)	Loss/tok 3.0963 (3.3749)	LR 1.000e-03
0: TRAIN [1][1990/5173]	Time 0.569 (0.606)	Data 1.31e-04 (4.21e-04)	Tok/s 17885 (23336)	Loss/tok 3.1656 (3.3745)	LR 1.000e-03
0: TRAIN [1][2000/5173]	Time 0.568 (0.607)	Data 1.29e-04 (4.20e-04)	Tok/s 18422 (23344)	Loss/tok 3.1525 (3.3741)	LR 1.000e-03
0: TRAIN [1][2010/5173]	Time 0.633 (0.607)	Data 1.58e-04 (4.19e-04)	Tok/s 26569 (23358)	Loss/tok 3.3256 (3.3740)	LR 1.000e-03
0: TRAIN [1][2020/5173]	Time 0.570 (0.607)	Data 1.29e-04 (4.17e-04)	Tok/s 17938 (23341)	Loss/tok 3.1480 (3.3731)	LR 1.000e-03
0: TRAIN [1][2030/5173]	Time 0.488 (0.606)	Data 1.36e-04 (4.16e-04)	Tok/s 10905 (23339)	Loss/tok 2.6955 (3.3729)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][2040/5173]	Time 0.569 (0.606)	Data 1.96e-04 (4.15e-04)	Tok/s 17732 (23332)	Loss/tok 3.0706 (3.3728)	LR 1.000e-03
0: TRAIN [1][2050/5173]	Time 0.566 (0.606)	Data 1.96e-04 (4.14e-04)	Tok/s 18338 (23323)	Loss/tok 3.2447 (3.3725)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2060/5173]	Time 0.688 (0.607)	Data 1.83e-04 (4.13e-04)	Tok/s 34204 (23349)	Loss/tok 3.4759 (3.3733)	LR 1.000e-03
0: TRAIN [1][2070/5173]	Time 0.629 (0.607)	Data 1.38e-04 (4.12e-04)	Tok/s 26458 (23358)	Loss/tok 3.2745 (3.3734)	LR 1.000e-03
0: TRAIN [1][2080/5173]	Time 0.629 (0.607)	Data 3.32e-04 (4.11e-04)	Tok/s 26507 (23363)	Loss/tok 3.4664 (3.3736)	LR 1.000e-03
0: TRAIN [1][2090/5173]	Time 0.566 (0.607)	Data 1.87e-04 (4.10e-04)	Tok/s 18075 (23379)	Loss/tok 2.9975 (3.3742)	LR 1.000e-03
0: TRAIN [1][2100/5173]	Time 0.566 (0.607)	Data 3.77e-04 (4.10e-04)	Tok/s 18522 (23377)	Loss/tok 3.0370 (3.3739)	LR 1.000e-03
0: TRAIN [1][2110/5173]	Time 0.694 (0.607)	Data 1.36e-04 (4.08e-04)	Tok/s 34011 (23380)	Loss/tok 3.3839 (3.3734)	LR 1.000e-03
0: TRAIN [1][2120/5173]	Time 0.566 (0.607)	Data 1.46e-04 (4.07e-04)	Tok/s 18492 (23396)	Loss/tok 3.0189 (3.3735)	LR 1.000e-03
0: TRAIN [1][2130/5173]	Time 0.693 (0.607)	Data 1.80e-04 (4.06e-04)	Tok/s 33744 (23402)	Loss/tok 3.4342 (3.3732)	LR 5.000e-04
0: TRAIN [1][2140/5173]	Time 0.490 (0.607)	Data 1.33e-04 (4.05e-04)	Tok/s 10674 (23407)	Loss/tok 2.7402 (3.3728)	LR 5.000e-04
0: TRAIN [1][2150/5173]	Time 0.689 (0.607)	Data 1.36e-04 (4.04e-04)	Tok/s 33797 (23420)	Loss/tok 3.4640 (3.3725)	LR 5.000e-04
0: TRAIN [1][2160/5173]	Time 0.632 (0.607)	Data 1.91e-04 (4.03e-04)	Tok/s 26517 (23413)	Loss/tok 3.2706 (3.3723)	LR 5.000e-04
0: TRAIN [1][2170/5173]	Time 0.566 (0.607)	Data 2.01e-04 (4.03e-04)	Tok/s 18656 (23403)	Loss/tok 3.1954 (3.3721)	LR 5.000e-04
0: TRAIN [1][2180/5173]	Time 0.628 (0.607)	Data 1.31e-04 (4.02e-04)	Tok/s 26799 (23404)	Loss/tok 3.3386 (3.3719)	LR 5.000e-04
0: TRAIN [1][2190/5173]	Time 0.566 (0.607)	Data 3.30e-04 (4.01e-04)	Tok/s 18190 (23392)	Loss/tok 3.0465 (3.3712)	LR 5.000e-04
0: TRAIN [1][2200/5173]	Time 0.567 (0.607)	Data 1.25e-04 (4.00e-04)	Tok/s 18594 (23395)	Loss/tok 3.0703 (3.3707)	LR 5.000e-04
0: TRAIN [1][2210/5173]	Time 0.630 (0.607)	Data 1.55e-04 (3.99e-04)	Tok/s 26639 (23386)	Loss/tok 3.4258 (3.3700)	LR 5.000e-04
0: TRAIN [1][2220/5173]	Time 0.566 (0.607)	Data 1.34e-04 (3.97e-04)	Tok/s 17991 (23394)	Loss/tok 3.1267 (3.3699)	LR 5.000e-04
0: TRAIN [1][2230/5173]	Time 0.628 (0.607)	Data 1.35e-04 (3.96e-04)	Tok/s 26842 (23397)	Loss/tok 3.3311 (3.3695)	LR 5.000e-04
0: TRAIN [1][2240/5173]	Time 0.567 (0.607)	Data 3.47e-04 (3.96e-04)	Tok/s 18513 (23375)	Loss/tok 3.1225 (3.3686)	LR 5.000e-04
0: TRAIN [1][2250/5173]	Time 0.694 (0.607)	Data 1.35e-04 (3.95e-04)	Tok/s 33533 (23366)	Loss/tok 3.4948 (3.3682)	LR 5.000e-04
0: TRAIN [1][2260/5173]	Time 0.569 (0.607)	Data 1.32e-04 (3.94e-04)	Tok/s 17790 (23368)	Loss/tok 3.1324 (3.3679)	LR 5.000e-04
0: TRAIN [1][2270/5173]	Time 0.554 (0.607)	Data 1.49e-04 (3.93e-04)	Tok/s 19012 (23373)	Loss/tok 3.0427 (3.3678)	LR 5.000e-04
0: TRAIN [1][2280/5173]	Time 0.566 (0.607)	Data 1.33e-04 (3.92e-04)	Tok/s 18037 (23368)	Loss/tok 3.2674 (3.3676)	LR 5.000e-04
0: TRAIN [1][2290/5173]	Time 0.627 (0.607)	Data 1.94e-04 (3.91e-04)	Tok/s 27375 (23368)	Loss/tok 3.2144 (3.3671)	LR 5.000e-04
0: TRAIN [1][2300/5173]	Time 0.627 (0.607)	Data 1.33e-04 (3.90e-04)	Tok/s 27249 (23371)	Loss/tok 3.2870 (3.3666)	LR 5.000e-04
0: TRAIN [1][2310/5173]	Time 0.567 (0.607)	Data 1.27e-04 (3.89e-04)	Tok/s 18136 (23367)	Loss/tok 2.9822 (3.3660)	LR 5.000e-04
0: TRAIN [1][2320/5173]	Time 0.624 (0.607)	Data 1.46e-04 (3.88e-04)	Tok/s 26708 (23372)	Loss/tok 3.3083 (3.3658)	LR 5.000e-04
0: TRAIN [1][2330/5173]	Time 0.569 (0.607)	Data 1.34e-04 (3.87e-04)	Tok/s 18297 (23360)	Loss/tok 3.1125 (3.3652)	LR 5.000e-04
0: TRAIN [1][2340/5173]	Time 0.625 (0.607)	Data 1.80e-04 (3.86e-04)	Tok/s 26814 (23357)	Loss/tok 3.2057 (3.3647)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][2350/5173]	Time 0.625 (0.607)	Data 3.60e-04 (3.85e-04)	Tok/s 27073 (23351)	Loss/tok 3.2141 (3.3644)	LR 5.000e-04
0: TRAIN [1][2360/5173]	Time 0.568 (0.607)	Data 1.42e-04 (3.85e-04)	Tok/s 18071 (23351)	Loss/tok 3.2018 (3.3640)	LR 5.000e-04
0: TRAIN [1][2370/5173]	Time 0.568 (0.606)	Data 1.85e-04 (3.84e-04)	Tok/s 18347 (23347)	Loss/tok 3.0126 (3.3637)	LR 5.000e-04
0: TRAIN [1][2380/5173]	Time 0.624 (0.606)	Data 2.08e-04 (3.83e-04)	Tok/s 26858 (23326)	Loss/tok 3.3386 (3.3629)	LR 5.000e-04
0: TRAIN [1][2390/5173]	Time 0.692 (0.606)	Data 1.35e-04 (3.82e-04)	Tok/s 33465 (23338)	Loss/tok 3.5005 (3.3628)	LR 5.000e-04
0: TRAIN [1][2400/5173]	Time 0.692 (0.606)	Data 1.89e-04 (3.81e-04)	Tok/s 33698 (23350)	Loss/tok 3.4530 (3.3627)	LR 5.000e-04
0: TRAIN [1][2410/5173]	Time 0.506 (0.606)	Data 1.30e-04 (3.80e-04)	Tok/s 10585 (23347)	Loss/tok 2.7918 (3.3622)	LR 5.000e-04
0: TRAIN [1][2420/5173]	Time 0.762 (0.606)	Data 2.01e-04 (3.80e-04)	Tok/s 39308 (23341)	Loss/tok 3.7458 (3.3621)	LR 5.000e-04
0: TRAIN [1][2430/5173]	Time 0.569 (0.606)	Data 1.32e-04 (3.79e-04)	Tok/s 17910 (23333)	Loss/tok 3.1438 (3.3616)	LR 5.000e-04
0: TRAIN [1][2440/5173]	Time 0.569 (0.606)	Data 1.80e-04 (3.78e-04)	Tok/s 17836 (23332)	Loss/tok 3.0838 (3.3613)	LR 5.000e-04
0: TRAIN [1][2450/5173]	Time 0.628 (0.607)	Data 1.39e-04 (3.77e-04)	Tok/s 26822 (23352)	Loss/tok 3.3270 (3.3613)	LR 5.000e-04
0: TRAIN [1][2460/5173]	Time 0.689 (0.606)	Data 1.28e-04 (3.77e-04)	Tok/s 33733 (23345)	Loss/tok 3.4313 (3.3607)	LR 5.000e-04
0: TRAIN [1][2470/5173]	Time 0.626 (0.606)	Data 1.43e-04 (3.76e-04)	Tok/s 27011 (23353)	Loss/tok 3.3860 (3.3607)	LR 5.000e-04
0: TRAIN [1][2480/5173]	Time 0.627 (0.607)	Data 1.31e-04 (3.75e-04)	Tok/s 26532 (23360)	Loss/tok 3.3384 (3.3603)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][2490/5173]	Time 0.624 (0.607)	Data 1.34e-04 (3.74e-04)	Tok/s 27108 (23369)	Loss/tok 3.1938 (3.3604)	LR 5.000e-04
0: TRAIN [1][2500/5173]	Time 0.491 (0.607)	Data 1.34e-04 (3.73e-04)	Tok/s 10806 (23363)	Loss/tok 2.6056 (3.3598)	LR 5.000e-04
0: TRAIN [1][2510/5173]	Time 0.769 (0.607)	Data 1.38e-04 (3.72e-04)	Tok/s 38669 (23373)	Loss/tok 3.5162 (3.3595)	LR 5.000e-04
0: TRAIN [1][2520/5173]	Time 0.568 (0.607)	Data 1.88e-04 (3.72e-04)	Tok/s 18024 (23378)	Loss/tok 3.0046 (3.3594)	LR 5.000e-04
0: TRAIN [1][2530/5173]	Time 0.505 (0.607)	Data 1.35e-04 (3.71e-04)	Tok/s 10569 (23375)	Loss/tok 2.6866 (3.3593)	LR 5.000e-04
0: TRAIN [1][2540/5173]	Time 0.627 (0.607)	Data 1.87e-04 (3.70e-04)	Tok/s 26482 (23390)	Loss/tok 3.2735 (3.3594)	LR 5.000e-04
0: TRAIN [1][2550/5173]	Time 0.691 (0.607)	Data 1.37e-04 (3.70e-04)	Tok/s 33846 (23401)	Loss/tok 3.4660 (3.3592)	LR 5.000e-04
0: TRAIN [1][2560/5173]	Time 0.565 (0.607)	Data 2.01e-04 (3.69e-04)	Tok/s 17953 (23397)	Loss/tok 3.1878 (3.3588)	LR 5.000e-04
0: TRAIN [1][2570/5173]	Time 0.569 (0.607)	Data 1.58e-04 (3.68e-04)	Tok/s 18242 (23408)	Loss/tok 3.1324 (3.3591)	LR 5.000e-04
0: TRAIN [1][2580/5173]	Time 0.631 (0.607)	Data 1.99e-04 (3.68e-04)	Tok/s 26819 (23404)	Loss/tok 3.3150 (3.3587)	LR 5.000e-04
0: TRAIN [1][2590/5173]	Time 0.632 (0.607)	Data 1.52e-04 (3.67e-04)	Tok/s 26756 (23417)	Loss/tok 3.3476 (3.3586)	LR 5.000e-04
0: TRAIN [1][2600/5173]	Time 0.764 (0.607)	Data 1.26e-04 (3.66e-04)	Tok/s 38563 (23415)	Loss/tok 3.6940 (3.3585)	LR 5.000e-04
0: TRAIN [1][2610/5173]	Time 0.566 (0.607)	Data 1.58e-04 (3.66e-04)	Tok/s 18299 (23414)	Loss/tok 2.9943 (3.3581)	LR 5.000e-04
0: TRAIN [1][2620/5173]	Time 0.626 (0.607)	Data 1.28e-04 (3.65e-04)	Tok/s 26520 (23404)	Loss/tok 3.2755 (3.3576)	LR 5.000e-04
0: TRAIN [1][2630/5173]	Time 0.624 (0.607)	Data 1.27e-04 (3.64e-04)	Tok/s 26630 (23391)	Loss/tok 3.3522 (3.3571)	LR 5.000e-04
0: TRAIN [1][2640/5173]	Time 0.693 (0.607)	Data 1.48e-04 (3.63e-04)	Tok/s 33588 (23386)	Loss/tok 3.5479 (3.3568)	LR 5.000e-04
0: TRAIN [1][2650/5173]	Time 0.690 (0.607)	Data 1.20e-04 (3.62e-04)	Tok/s 34362 (23384)	Loss/tok 3.3279 (3.3563)	LR 5.000e-04
0: TRAIN [1][2660/5173]	Time 0.566 (0.607)	Data 1.39e-04 (3.62e-04)	Tok/s 18235 (23402)	Loss/tok 3.0954 (3.3564)	LR 5.000e-04
0: TRAIN [1][2670/5173]	Time 0.624 (0.607)	Data 1.18e-04 (3.61e-04)	Tok/s 26779 (23393)	Loss/tok 3.3653 (3.3558)	LR 5.000e-04
0: TRAIN [1][2680/5173]	Time 0.624 (0.607)	Data 1.24e-04 (3.60e-04)	Tok/s 27248 (23392)	Loss/tok 3.2313 (3.3554)	LR 5.000e-04
0: TRAIN [1][2690/5173]	Time 0.692 (0.607)	Data 1.29e-04 (3.59e-04)	Tok/s 33747 (23403)	Loss/tok 3.5424 (3.3553)	LR 5.000e-04
0: TRAIN [1][2700/5173]	Time 0.691 (0.607)	Data 1.25e-04 (3.58e-04)	Tok/s 34169 (23424)	Loss/tok 3.4673 (3.3557)	LR 5.000e-04
0: TRAIN [1][2710/5173]	Time 0.568 (0.607)	Data 1.26e-04 (3.57e-04)	Tok/s 18010 (23430)	Loss/tok 3.0763 (3.3557)	LR 5.000e-04
0: TRAIN [1][2720/5173]	Time 0.690 (0.607)	Data 1.27e-04 (3.57e-04)	Tok/s 33840 (23429)	Loss/tok 3.4365 (3.3553)	LR 5.000e-04
0: TRAIN [1][2730/5173]	Time 0.625 (0.607)	Data 1.24e-04 (3.56e-04)	Tok/s 26753 (23432)	Loss/tok 3.3676 (3.3548)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][2740/5173]	Time 0.625 (0.607)	Data 1.32e-04 (3.55e-04)	Tok/s 26917 (23444)	Loss/tok 3.2349 (3.3549)	LR 5.000e-04
0: TRAIN [1][2750/5173]	Time 0.753 (0.607)	Data 1.36e-04 (3.54e-04)	Tok/s 39607 (23450)	Loss/tok 3.7039 (3.3549)	LR 5.000e-04
0: TRAIN [1][2760/5173]	Time 0.569 (0.607)	Data 1.35e-04 (3.53e-04)	Tok/s 18405 (23437)	Loss/tok 3.0483 (3.3543)	LR 5.000e-04
0: TRAIN [1][2770/5173]	Time 0.566 (0.607)	Data 1.22e-04 (3.53e-04)	Tok/s 17725 (23436)	Loss/tok 3.0236 (3.3541)	LR 5.000e-04
0: TRAIN [1][2780/5173]	Time 0.564 (0.607)	Data 1.34e-04 (3.52e-04)	Tok/s 18115 (23417)	Loss/tok 3.0751 (3.3534)	LR 5.000e-04
0: TRAIN [1][2790/5173]	Time 0.692 (0.607)	Data 1.25e-04 (3.51e-04)	Tok/s 33800 (23429)	Loss/tok 3.5185 (3.3535)	LR 5.000e-04
0: TRAIN [1][2800/5173]	Time 0.506 (0.607)	Data 1.20e-04 (3.50e-04)	Tok/s 10841 (23426)	Loss/tok 2.6330 (3.3531)	LR 5.000e-04
0: TRAIN [1][2810/5173]	Time 0.626 (0.607)	Data 1.38e-04 (3.49e-04)	Tok/s 26519 (23440)	Loss/tok 3.3063 (3.3532)	LR 5.000e-04
0: TRAIN [1][2820/5173]	Time 0.628 (0.607)	Data 1.90e-04 (3.49e-04)	Tok/s 26898 (23436)	Loss/tok 3.3936 (3.3528)	LR 5.000e-04
0: TRAIN [1][2830/5173]	Time 0.567 (0.607)	Data 1.22e-04 (3.48e-04)	Tok/s 18230 (23415)	Loss/tok 3.0323 (3.3521)	LR 5.000e-04
0: TRAIN [1][2840/5173]	Time 0.690 (0.607)	Data 1.27e-04 (3.47e-04)	Tok/s 33872 (23415)	Loss/tok 3.3620 (3.3517)	LR 5.000e-04
0: TRAIN [1][2850/5173]	Time 0.626 (0.607)	Data 1.43e-04 (3.47e-04)	Tok/s 26535 (23426)	Loss/tok 3.3334 (3.3518)	LR 5.000e-04
0: TRAIN [1][2860/5173]	Time 0.629 (0.607)	Data 1.23e-04 (3.46e-04)	Tok/s 26712 (23419)	Loss/tok 3.3515 (3.3515)	LR 5.000e-04
0: TRAIN [1][2870/5173]	Time 0.627 (0.607)	Data 1.35e-04 (3.45e-04)	Tok/s 26822 (23429)	Loss/tok 3.2726 (3.3514)	LR 5.000e-04
0: TRAIN [1][2880/5173]	Time 0.629 (0.607)	Data 1.27e-04 (3.44e-04)	Tok/s 26511 (23430)	Loss/tok 3.3224 (3.3510)	LR 5.000e-04
0: TRAIN [1][2890/5173]	Time 0.628 (0.607)	Data 1.30e-04 (3.44e-04)	Tok/s 27160 (23427)	Loss/tok 3.2434 (3.3508)	LR 5.000e-04
0: TRAIN [1][2900/5173]	Time 0.691 (0.607)	Data 1.32e-04 (3.43e-04)	Tok/s 33815 (23420)	Loss/tok 3.5083 (3.3505)	LR 5.000e-04
0: TRAIN [1][2910/5173]	Time 0.507 (0.607)	Data 1.24e-04 (3.42e-04)	Tok/s 10347 (23414)	Loss/tok 2.7892 (3.3501)	LR 5.000e-04
0: TRAIN [1][2920/5173]	Time 0.569 (0.607)	Data 1.28e-04 (3.41e-04)	Tok/s 18139 (23414)	Loss/tok 3.1169 (3.3497)	LR 5.000e-04
0: TRAIN [1][2930/5173]	Time 0.566 (0.607)	Data 1.38e-04 (3.41e-04)	Tok/s 18360 (23424)	Loss/tok 3.0544 (3.3498)	LR 5.000e-04
0: TRAIN [1][2940/5173]	Time 0.621 (0.607)	Data 3.16e-04 (3.40e-04)	Tok/s 26815 (23421)	Loss/tok 3.3022 (3.3496)	LR 2.500e-04
0: TRAIN [1][2950/5173]	Time 0.767 (0.607)	Data 1.46e-04 (3.39e-04)	Tok/s 39119 (23429)	Loss/tok 3.6189 (3.3496)	LR 2.500e-04
0: TRAIN [1][2960/5173]	Time 0.571 (0.607)	Data 1.26e-04 (3.39e-04)	Tok/s 18070 (23435)	Loss/tok 3.1199 (3.3496)	LR 2.500e-04
0: TRAIN [1][2970/5173]	Time 0.630 (0.607)	Data 1.26e-04 (3.38e-04)	Tok/s 26683 (23432)	Loss/tok 3.1748 (3.3491)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][2980/5173]	Time 0.505 (0.607)	Data 1.80e-04 (3.38e-04)	Tok/s 10420 (23451)	Loss/tok 2.5353 (3.3496)	LR 2.500e-04
0: TRAIN [1][2990/5173]	Time 0.690 (0.607)	Data 1.23e-04 (3.37e-04)	Tok/s 33730 (23464)	Loss/tok 3.4498 (3.3497)	LR 2.500e-04
0: TRAIN [1][3000/5173]	Time 0.566 (0.607)	Data 1.29e-04 (3.36e-04)	Tok/s 18235 (23463)	Loss/tok 3.0844 (3.3494)	LR 2.500e-04
0: TRAIN [1][3010/5173]	Time 0.564 (0.607)	Data 1.30e-04 (3.36e-04)	Tok/s 18716 (23456)	Loss/tok 3.1056 (3.3491)	LR 2.500e-04
0: TRAIN [1][3020/5173]	Time 0.567 (0.607)	Data 1.23e-04 (3.35e-04)	Tok/s 18193 (23456)	Loss/tok 3.0989 (3.3489)	LR 2.500e-04
0: TRAIN [1][3030/5173]	Time 0.568 (0.607)	Data 1.25e-04 (3.34e-04)	Tok/s 18155 (23451)	Loss/tok 2.9275 (3.3483)	LR 2.500e-04
0: TRAIN [1][3040/5173]	Time 0.505 (0.607)	Data 1.27e-04 (3.34e-04)	Tok/s 10570 (23443)	Loss/tok 2.5931 (3.3478)	LR 2.500e-04
0: TRAIN [1][3050/5173]	Time 0.565 (0.607)	Data 1.25e-04 (3.33e-04)	Tok/s 18228 (23421)	Loss/tok 3.0026 (3.3472)	LR 2.500e-04
0: TRAIN [1][3060/5173]	Time 0.567 (0.607)	Data 1.28e-04 (3.32e-04)	Tok/s 18297 (23422)	Loss/tok 3.1193 (3.3469)	LR 2.500e-04
0: TRAIN [1][3070/5173]	Time 0.691 (0.607)	Data 1.31e-04 (3.32e-04)	Tok/s 33991 (23426)	Loss/tok 3.3614 (3.3467)	LR 2.500e-04
0: TRAIN [1][3080/5173]	Time 0.633 (0.607)	Data 1.39e-04 (3.31e-04)	Tok/s 26693 (23421)	Loss/tok 3.1710 (3.3463)	LR 2.500e-04
0: TRAIN [1][3090/5173]	Time 0.688 (0.607)	Data 1.26e-04 (3.30e-04)	Tok/s 33999 (23426)	Loss/tok 3.4504 (3.3461)	LR 2.500e-04
0: TRAIN [1][3100/5173]	Time 0.565 (0.607)	Data 1.30e-04 (3.30e-04)	Tok/s 18420 (23429)	Loss/tok 3.1495 (3.3459)	LR 2.500e-04
0: TRAIN [1][3110/5173]	Time 0.567 (0.607)	Data 1.25e-04 (3.29e-04)	Tok/s 17927 (23436)	Loss/tok 3.0188 (3.3458)	LR 2.500e-04
0: TRAIN [1][3120/5173]	Time 0.507 (0.607)	Data 1.31e-04 (3.28e-04)	Tok/s 10471 (23434)	Loss/tok 2.6034 (3.3455)	LR 2.500e-04
0: TRAIN [1][3130/5173]	Time 0.631 (0.607)	Data 1.24e-04 (3.28e-04)	Tok/s 26410 (23426)	Loss/tok 3.1400 (3.3449)	LR 2.500e-04
0: TRAIN [1][3140/5173]	Time 0.626 (0.607)	Data 1.28e-04 (3.27e-04)	Tok/s 26804 (23409)	Loss/tok 3.2385 (3.3443)	LR 2.500e-04
0: TRAIN [1][3150/5173]	Time 0.627 (0.607)	Data 1.26e-04 (3.27e-04)	Tok/s 26722 (23401)	Loss/tok 3.3106 (3.3437)	LR 2.500e-04
0: TRAIN [1][3160/5173]	Time 0.504 (0.607)	Data 1.29e-04 (3.26e-04)	Tok/s 10521 (23390)	Loss/tok 2.6935 (3.3433)	LR 2.500e-04
0: TRAIN [1][3170/5173]	Time 0.489 (0.607)	Data 1.79e-04 (3.25e-04)	Tok/s 10685 (23385)	Loss/tok 2.7233 (3.3430)	LR 2.500e-04
0: TRAIN [1][3180/5173]	Time 0.628 (0.607)	Data 1.27e-04 (3.25e-04)	Tok/s 26431 (23377)	Loss/tok 3.2699 (3.3425)	LR 2.500e-04
0: TRAIN [1][3190/5173]	Time 0.571 (0.607)	Data 1.31e-04 (3.24e-04)	Tok/s 18156 (23364)	Loss/tok 3.0525 (3.3419)	LR 2.500e-04
0: TRAIN [1][3200/5173]	Time 0.632 (0.607)	Data 1.29e-04 (3.24e-04)	Tok/s 26504 (23366)	Loss/tok 3.2088 (3.3416)	LR 2.500e-04
0: TRAIN [1][3210/5173]	Time 0.488 (0.607)	Data 1.26e-04 (3.23e-04)	Tok/s 10772 (23367)	Loss/tok 2.6170 (3.3414)	LR 2.500e-04
0: TRAIN [1][3220/5173]	Time 0.496 (0.607)	Data 1.30e-04 (3.23e-04)	Tok/s 10636 (23363)	Loss/tok 2.6657 (3.3411)	LR 2.500e-04
0: TRAIN [1][3230/5173]	Time 0.563 (0.607)	Data 1.27e-04 (3.22e-04)	Tok/s 18452 (23353)	Loss/tok 2.9880 (3.3405)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][3240/5173]	Time 0.687 (0.607)	Data 1.27e-04 (3.21e-04)	Tok/s 33677 (23358)	Loss/tok 3.5117 (3.3406)	LR 2.500e-04
0: TRAIN [1][3250/5173]	Time 0.631 (0.607)	Data 1.29e-04 (3.21e-04)	Tok/s 26591 (23360)	Loss/tok 3.3193 (3.3402)	LR 2.500e-04
0: TRAIN [1][3260/5173]	Time 0.568 (0.607)	Data 1.31e-04 (3.20e-04)	Tok/s 18522 (23359)	Loss/tok 3.1029 (3.3399)	LR 2.500e-04
0: TRAIN [1][3270/5173]	Time 0.566 (0.607)	Data 1.36e-04 (3.20e-04)	Tok/s 17985 (23355)	Loss/tok 3.0207 (3.3397)	LR 2.500e-04
0: TRAIN [1][3280/5173]	Time 0.629 (0.607)	Data 1.21e-04 (3.19e-04)	Tok/s 26814 (23352)	Loss/tok 3.2601 (3.3393)	LR 2.500e-04
0: TRAIN [1][3290/5173]	Time 0.693 (0.607)	Data 3.18e-04 (3.19e-04)	Tok/s 33512 (23361)	Loss/tok 3.4721 (3.3393)	LR 2.500e-04
0: TRAIN [1][3300/5173]	Time 0.629 (0.607)	Data 1.24e-04 (3.18e-04)	Tok/s 27435 (23359)	Loss/tok 3.1468 (3.3391)	LR 2.500e-04
0: TRAIN [1][3310/5173]	Time 0.692 (0.607)	Data 1.30e-04 (3.17e-04)	Tok/s 33662 (23363)	Loss/tok 3.3702 (3.3389)	LR 2.500e-04
0: TRAIN [1][3320/5173]	Time 0.565 (0.607)	Data 1.32e-04 (3.17e-04)	Tok/s 18161 (23357)	Loss/tok 3.0158 (3.3385)	LR 2.500e-04
0: TRAIN [1][3330/5173]	Time 0.625 (0.607)	Data 1.26e-04 (3.16e-04)	Tok/s 26683 (23361)	Loss/tok 3.2718 (3.3383)	LR 2.500e-04
0: TRAIN [1][3340/5173]	Time 0.568 (0.607)	Data 1.20e-04 (3.16e-04)	Tok/s 18267 (23362)	Loss/tok 3.0305 (3.3381)	LR 2.500e-04
0: TRAIN [1][3350/5173]	Time 0.626 (0.607)	Data 1.28e-04 (3.15e-04)	Tok/s 27010 (23359)	Loss/tok 3.2347 (3.3378)	LR 2.500e-04
0: TRAIN [1][3360/5173]	Time 0.692 (0.606)	Data 1.26e-04 (3.15e-04)	Tok/s 33881 (23352)	Loss/tok 3.4221 (3.3374)	LR 2.500e-04
0: TRAIN [1][3370/5173]	Time 0.504 (0.606)	Data 1.22e-04 (3.14e-04)	Tok/s 10352 (23337)	Loss/tok 2.7541 (3.3370)	LR 2.500e-04
0: TRAIN [1][3380/5173]	Time 0.631 (0.606)	Data 1.27e-04 (3.14e-04)	Tok/s 26525 (23337)	Loss/tok 3.2504 (3.3366)	LR 2.500e-04
0: TRAIN [1][3390/5173]	Time 0.510 (0.606)	Data 1.27e-04 (3.13e-04)	Tok/s 10118 (23329)	Loss/tok 2.5676 (3.3362)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][3400/5173]	Time 0.565 (0.606)	Data 1.30e-04 (3.13e-04)	Tok/s 18132 (23325)	Loss/tok 3.0091 (3.3358)	LR 2.500e-04
0: TRAIN [1][3410/5173]	Time 0.627 (0.606)	Data 1.26e-04 (3.12e-04)	Tok/s 26450 (23315)	Loss/tok 3.3766 (3.3354)	LR 2.500e-04
0: TRAIN [1][3420/5173]	Time 0.567 (0.606)	Data 1.26e-04 (3.12e-04)	Tok/s 17786 (23303)	Loss/tok 3.0738 (3.3349)	LR 2.500e-04
0: TRAIN [1][3430/5173]	Time 0.625 (0.606)	Data 1.27e-04 (3.11e-04)	Tok/s 26965 (23304)	Loss/tok 3.2010 (3.3346)	LR 2.500e-04
0: TRAIN [1][3440/5173]	Time 0.567 (0.606)	Data 1.28e-04 (3.11e-04)	Tok/s 18373 (23296)	Loss/tok 2.9145 (3.3342)	LR 2.500e-04
0: TRAIN [1][3450/5173]	Time 0.507 (0.606)	Data 1.34e-04 (3.10e-04)	Tok/s 10357 (23283)	Loss/tok 2.6406 (3.3339)	LR 2.500e-04
0: TRAIN [1][3460/5173]	Time 0.628 (0.606)	Data 1.47e-04 (3.10e-04)	Tok/s 27301 (23289)	Loss/tok 3.2631 (3.3338)	LR 2.500e-04
0: TRAIN [1][3470/5173]	Time 0.692 (0.606)	Data 1.31e-04 (3.09e-04)	Tok/s 33983 (23288)	Loss/tok 3.4290 (3.3335)	LR 2.500e-04
0: TRAIN [1][3480/5173]	Time 0.767 (0.606)	Data 1.24e-04 (3.09e-04)	Tok/s 39554 (23291)	Loss/tok 3.4187 (3.3332)	LR 2.500e-04
0: TRAIN [1][3490/5173]	Time 0.564 (0.606)	Data 1.26e-04 (3.08e-04)	Tok/s 18256 (23280)	Loss/tok 3.1350 (3.3329)	LR 2.500e-04
0: TRAIN [1][3500/5173]	Time 0.619 (0.606)	Data 1.31e-04 (3.08e-04)	Tok/s 26467 (23286)	Loss/tok 3.3830 (3.3329)	LR 2.500e-04
0: TRAIN [1][3510/5173]	Time 0.628 (0.606)	Data 1.32e-04 (3.07e-04)	Tok/s 26898 (23279)	Loss/tok 3.3262 (3.3324)	LR 2.500e-04
0: TRAIN [1][3520/5173]	Time 0.566 (0.606)	Data 1.27e-04 (3.07e-04)	Tok/s 18486 (23271)	Loss/tok 2.9716 (3.3320)	LR 2.500e-04
0: TRAIN [1][3530/5173]	Time 0.568 (0.606)	Data 1.23e-04 (3.06e-04)	Tok/s 17999 (23270)	Loss/tok 2.9248 (3.3317)	LR 2.500e-04
0: TRAIN [1][3540/5173]	Time 0.629 (0.606)	Data 1.29e-04 (3.06e-04)	Tok/s 26847 (23270)	Loss/tok 3.1782 (3.3314)	LR 2.500e-04
0: TRAIN [1][3550/5173]	Time 0.568 (0.606)	Data 1.29e-04 (3.05e-04)	Tok/s 18407 (23276)	Loss/tok 3.0403 (3.3313)	LR 2.500e-04
0: TRAIN [1][3560/5173]	Time 0.766 (0.606)	Data 1.24e-04 (3.05e-04)	Tok/s 38743 (23271)	Loss/tok 3.5406 (3.3310)	LR 2.500e-04
0: TRAIN [1][3570/5173]	Time 0.566 (0.606)	Data 1.38e-04 (3.04e-04)	Tok/s 18020 (23268)	Loss/tok 3.0662 (3.3305)	LR 2.500e-04
0: TRAIN [1][3580/5173]	Time 0.508 (0.606)	Data 1.25e-04 (3.04e-04)	Tok/s 10101 (23268)	Loss/tok 2.7413 (3.3303)	LR 2.500e-04
0: TRAIN [1][3590/5173]	Time 0.633 (0.606)	Data 1.27e-04 (3.03e-04)	Tok/s 26654 (23260)	Loss/tok 3.2671 (3.3299)	LR 2.500e-04
0: TRAIN [1][3600/5173]	Time 0.762 (0.606)	Data 1.31e-04 (3.03e-04)	Tok/s 38904 (23271)	Loss/tok 3.5932 (3.3303)	LR 2.500e-04
0: TRAIN [1][3610/5173]	Time 0.691 (0.606)	Data 1.30e-04 (3.03e-04)	Tok/s 33603 (23275)	Loss/tok 3.4761 (3.3301)	LR 2.500e-04
0: TRAIN [1][3620/5173]	Time 0.567 (0.606)	Data 1.24e-04 (3.02e-04)	Tok/s 18276 (23270)	Loss/tok 3.0689 (3.3300)	LR 2.500e-04
0: TRAIN [1][3630/5173]	Time 0.694 (0.606)	Data 1.33e-04 (3.02e-04)	Tok/s 33448 (23283)	Loss/tok 3.4519 (3.3300)	LR 2.500e-04
0: TRAIN [1][3640/5173]	Time 0.624 (0.606)	Data 1.26e-04 (3.01e-04)	Tok/s 27159 (23283)	Loss/tok 3.1186 (3.3297)	LR 2.500e-04
0: TRAIN [1][3650/5173]	Time 0.497 (0.606)	Data 1.33e-04 (3.01e-04)	Tok/s 10595 (23280)	Loss/tok 2.7692 (3.3294)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][3660/5173]	Time 0.565 (0.606)	Data 1.31e-04 (3.00e-04)	Tok/s 18413 (23270)	Loss/tok 3.1516 (3.3290)	LR 2.500e-04
0: TRAIN [1][3670/5173]	Time 0.628 (0.606)	Data 1.32e-04 (3.00e-04)	Tok/s 26614 (23263)	Loss/tok 3.3075 (3.3288)	LR 2.500e-04
0: TRAIN [1][3680/5173]	Time 0.695 (0.606)	Data 1.29e-04 (2.99e-04)	Tok/s 33639 (23259)	Loss/tok 3.3904 (3.3284)	LR 2.500e-04
0: TRAIN [1][3690/5173]	Time 0.686 (0.606)	Data 1.27e-04 (2.99e-04)	Tok/s 33706 (23258)	Loss/tok 3.4248 (3.3281)	LR 2.500e-04
0: TRAIN [1][3700/5173]	Time 0.569 (0.606)	Data 1.29e-04 (2.99e-04)	Tok/s 17940 (23258)	Loss/tok 3.0807 (3.3278)	LR 2.500e-04
0: TRAIN [1][3710/5173]	Time 0.494 (0.606)	Data 1.28e-04 (2.98e-04)	Tok/s 10586 (23260)	Loss/tok 2.6386 (3.3276)	LR 2.500e-04
0: TRAIN [1][3720/5173]	Time 0.508 (0.606)	Data 1.29e-04 (2.98e-04)	Tok/s 10493 (23252)	Loss/tok 2.6194 (3.3275)	LR 2.500e-04
0: TRAIN [1][3730/5173]	Time 0.567 (0.606)	Data 1.30e-04 (2.97e-04)	Tok/s 18299 (23244)	Loss/tok 2.9075 (3.3271)	LR 2.500e-04
0: TRAIN [1][3740/5173]	Time 0.566 (0.606)	Data 1.25e-04 (2.97e-04)	Tok/s 18241 (23245)	Loss/tok 3.0894 (3.3270)	LR 2.500e-04
0: TRAIN [1][3750/5173]	Time 0.692 (0.606)	Data 1.23e-04 (2.96e-04)	Tok/s 34171 (23252)	Loss/tok 3.3654 (3.3269)	LR 2.500e-04
0: TRAIN [1][3760/5173]	Time 0.570 (0.606)	Data 1.30e-04 (2.96e-04)	Tok/s 17897 (23249)	Loss/tok 3.1405 (3.3267)	LR 1.250e-04
0: TRAIN [1][3770/5173]	Time 0.767 (0.606)	Data 1.40e-04 (2.96e-04)	Tok/s 39044 (23251)	Loss/tok 3.5811 (3.3266)	LR 1.250e-04
0: TRAIN [1][3780/5173]	Time 0.504 (0.606)	Data 1.23e-04 (2.95e-04)	Tok/s 10723 (23246)	Loss/tok 2.7076 (3.3264)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][3790/5173]	Time 0.626 (0.606)	Data 1.34e-04 (2.95e-04)	Tok/s 26690 (23247)	Loss/tok 3.3874 (3.3263)	LR 1.250e-04
0: TRAIN [1][3800/5173]	Time 0.631 (0.606)	Data 1.33e-04 (2.94e-04)	Tok/s 26220 (23247)	Loss/tok 3.3168 (3.3259)	LR 1.250e-04
0: TRAIN [1][3810/5173]	Time 0.767 (0.606)	Data 1.57e-04 (2.94e-04)	Tok/s 38116 (23262)	Loss/tok 3.7447 (3.3262)	LR 1.250e-04
0: TRAIN [1][3820/5173]	Time 0.507 (0.606)	Data 1.27e-04 (2.94e-04)	Tok/s 10507 (23243)	Loss/tok 2.6081 (3.3258)	LR 1.250e-04
0: TRAIN [1][3830/5173]	Time 0.566 (0.605)	Data 1.25e-04 (2.93e-04)	Tok/s 18162 (23235)	Loss/tok 2.9569 (3.3253)	LR 1.250e-04
0: TRAIN [1][3840/5173]	Time 0.567 (0.605)	Data 1.20e-04 (2.93e-04)	Tok/s 17837 (23230)	Loss/tok 2.9948 (3.3250)	LR 1.250e-04
0: TRAIN [1][3850/5173]	Time 0.505 (0.605)	Data 1.26e-04 (2.93e-04)	Tok/s 10480 (23236)	Loss/tok 2.6200 (3.3251)	LR 1.250e-04
0: TRAIN [1][3860/5173]	Time 0.767 (0.605)	Data 1.28e-04 (2.92e-04)	Tok/s 38523 (23224)	Loss/tok 3.6007 (3.3248)	LR 1.250e-04
0: TRAIN [1][3870/5173]	Time 0.625 (0.605)	Data 1.23e-04 (2.92e-04)	Tok/s 27201 (23231)	Loss/tok 3.2853 (3.3248)	LR 1.250e-04
0: TRAIN [1][3880/5173]	Time 0.629 (0.605)	Data 1.24e-04 (2.91e-04)	Tok/s 26373 (23231)	Loss/tok 3.3508 (3.3247)	LR 1.250e-04
0: TRAIN [1][3890/5173]	Time 0.626 (0.605)	Data 1.22e-04 (2.91e-04)	Tok/s 26533 (23231)	Loss/tok 3.3077 (3.3245)	LR 1.250e-04
0: TRAIN [1][3900/5173]	Time 0.630 (0.606)	Data 1.28e-04 (2.91e-04)	Tok/s 26819 (23239)	Loss/tok 3.2296 (3.3246)	LR 1.250e-04
0: TRAIN [1][3910/5173]	Time 0.569 (0.606)	Data 1.44e-04 (2.90e-04)	Tok/s 18256 (23241)	Loss/tok 3.0531 (3.3245)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][3920/5173]	Time 0.688 (0.606)	Data 1.35e-04 (2.90e-04)	Tok/s 34325 (23254)	Loss/tok 3.2880 (3.3246)	LR 1.250e-04
0: TRAIN [1][3930/5173]	Time 0.566 (0.606)	Data 1.26e-04 (2.89e-04)	Tok/s 18577 (23256)	Loss/tok 3.0006 (3.3244)	LR 1.250e-04
0: TRAIN [1][3940/5173]	Time 0.566 (0.606)	Data 1.29e-04 (2.89e-04)	Tok/s 18120 (23255)	Loss/tok 3.1010 (3.3242)	LR 1.250e-04
0: TRAIN [1][3950/5173]	Time 0.624 (0.606)	Data 1.31e-04 (2.89e-04)	Tok/s 27031 (23258)	Loss/tok 3.3103 (3.3240)	LR 1.250e-04
0: TRAIN [1][3960/5173]	Time 0.569 (0.606)	Data 1.26e-04 (2.88e-04)	Tok/s 18559 (23260)	Loss/tok 3.0079 (3.3238)	LR 1.250e-04
0: TRAIN [1][3970/5173]	Time 0.682 (0.606)	Data 1.22e-04 (2.88e-04)	Tok/s 33844 (23250)	Loss/tok 3.3917 (3.3234)	LR 1.250e-04
0: TRAIN [1][3980/5173]	Time 0.565 (0.606)	Data 1.27e-04 (2.87e-04)	Tok/s 18366 (23253)	Loss/tok 3.1274 (3.3232)	LR 1.250e-04
0: TRAIN [1][3990/5173]	Time 0.567 (0.606)	Data 1.35e-04 (2.87e-04)	Tok/s 18332 (23246)	Loss/tok 3.1003 (3.3228)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][4000/5173]	Time 0.675 (0.606)	Data 1.24e-04 (2.87e-04)	Tok/s 34037 (23254)	Loss/tok 3.4065 (3.3228)	LR 1.250e-04
0: TRAIN [1][4010/5173]	Time 0.565 (0.606)	Data 1.26e-04 (2.86e-04)	Tok/s 18539 (23259)	Loss/tok 3.1499 (3.3227)	LR 1.250e-04
0: TRAIN [1][4020/5173]	Time 0.567 (0.606)	Data 1.36e-04 (2.86e-04)	Tok/s 18674 (23272)	Loss/tok 3.0451 (3.3229)	LR 1.250e-04
0: TRAIN [1][4030/5173]	Time 0.625 (0.606)	Data 1.24e-04 (2.86e-04)	Tok/s 26805 (23274)	Loss/tok 3.2436 (3.3227)	LR 1.250e-04
0: TRAIN [1][4040/5173]	Time 0.566 (0.606)	Data 2.83e-04 (2.85e-04)	Tok/s 17843 (23275)	Loss/tok 3.0870 (3.3224)	LR 1.250e-04
0: TRAIN [1][4050/5173]	Time 0.566 (0.606)	Data 1.23e-04 (2.85e-04)	Tok/s 18163 (23278)	Loss/tok 2.9984 (3.3222)	LR 1.250e-04
0: TRAIN [1][4060/5173]	Time 0.689 (0.606)	Data 1.22e-04 (2.84e-04)	Tok/s 33785 (23277)	Loss/tok 3.4931 (3.3221)	LR 1.250e-04
0: TRAIN [1][4070/5173]	Time 0.566 (0.606)	Data 1.28e-04 (2.84e-04)	Tok/s 17976 (23273)	Loss/tok 3.1327 (3.3218)	LR 1.250e-04
0: TRAIN [1][4080/5173]	Time 0.567 (0.606)	Data 1.44e-04 (2.84e-04)	Tok/s 18141 (23284)	Loss/tok 3.0703 (3.3221)	LR 1.250e-04
0: TRAIN [1][4090/5173]	Time 0.569 (0.606)	Data 1.23e-04 (2.83e-04)	Tok/s 18127 (23279)	Loss/tok 3.0160 (3.3218)	LR 1.250e-04
0: TRAIN [1][4100/5173]	Time 0.690 (0.606)	Data 1.36e-04 (2.83e-04)	Tok/s 34004 (23273)	Loss/tok 3.3784 (3.3215)	LR 1.250e-04
0: TRAIN [1][4110/5173]	Time 0.570 (0.606)	Data 1.18e-04 (2.83e-04)	Tok/s 18079 (23269)	Loss/tok 3.1314 (3.3213)	LR 1.250e-04
0: TRAIN [1][4120/5173]	Time 0.568 (0.606)	Data 1.25e-04 (2.82e-04)	Tok/s 18348 (23272)	Loss/tok 3.1421 (3.3213)	LR 1.250e-04
0: TRAIN [1][4130/5173]	Time 0.566 (0.606)	Data 1.42e-04 (2.82e-04)	Tok/s 18582 (23277)	Loss/tok 3.0675 (3.3212)	LR 1.250e-04
0: TRAIN [1][4140/5173]	Time 0.567 (0.606)	Data 1.42e-04 (2.82e-04)	Tok/s 18027 (23279)	Loss/tok 2.9694 (3.3209)	LR 1.250e-04
0: TRAIN [1][4150/5173]	Time 0.564 (0.606)	Data 1.39e-04 (2.81e-04)	Tok/s 18067 (23281)	Loss/tok 3.2097 (3.3209)	LR 1.250e-04
0: TRAIN [1][4160/5173]	Time 0.634 (0.606)	Data 1.20e-04 (2.81e-04)	Tok/s 26540 (23277)	Loss/tok 3.2333 (3.3206)	LR 1.250e-04
0: TRAIN [1][4170/5173]	Time 0.570 (0.606)	Data 1.31e-04 (2.81e-04)	Tok/s 18276 (23272)	Loss/tok 3.1360 (3.3204)	LR 1.250e-04
0: TRAIN [1][4180/5173]	Time 0.565 (0.606)	Data 3.17e-04 (2.80e-04)	Tok/s 18390 (23270)	Loss/tok 3.0940 (3.3201)	LR 1.250e-04
0: TRAIN [1][4190/5173]	Time 0.624 (0.606)	Data 1.21e-04 (2.80e-04)	Tok/s 26741 (23264)	Loss/tok 3.2303 (3.3198)	LR 1.250e-04
0: TRAIN [1][4200/5173]	Time 0.566 (0.606)	Data 1.41e-04 (2.80e-04)	Tok/s 18132 (23268)	Loss/tok 2.9937 (3.3196)	LR 1.250e-04
0: TRAIN [1][4210/5173]	Time 0.504 (0.606)	Data 1.25e-04 (2.79e-04)	Tok/s 10324 (23262)	Loss/tok 2.6195 (3.3193)	LR 1.250e-04
0: TRAIN [1][4220/5173]	Time 0.567 (0.606)	Data 1.34e-04 (2.79e-04)	Tok/s 18347 (23254)	Loss/tok 3.0683 (3.3191)	LR 1.250e-04
0: TRAIN [1][4230/5173]	Time 0.569 (0.606)	Data 1.28e-04 (2.79e-04)	Tok/s 17903 (23257)	Loss/tok 3.1636 (3.3190)	LR 1.250e-04
0: TRAIN [1][4240/5173]	Time 0.765 (0.606)	Data 1.37e-04 (2.78e-04)	Tok/s 38911 (23260)	Loss/tok 3.4111 (3.3189)	LR 1.250e-04
0: TRAIN [1][4250/5173]	Time 0.566 (0.606)	Data 1.49e-04 (2.78e-04)	Tok/s 18733 (23260)	Loss/tok 3.0395 (3.3187)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][4260/5173]	Time 0.627 (0.606)	Data 1.47e-04 (2.78e-04)	Tok/s 26608 (23262)	Loss/tok 3.2924 (3.3186)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][4270/5173]	Time 0.767 (0.606)	Data 1.38e-04 (2.77e-04)	Tok/s 38656 (23263)	Loss/tok 3.5017 (3.3184)	LR 1.250e-04
0: TRAIN [1][4280/5173]	Time 0.629 (0.606)	Data 1.30e-04 (2.77e-04)	Tok/s 26959 (23268)	Loss/tok 3.2267 (3.3182)	LR 1.250e-04
0: TRAIN [1][4290/5173]	Time 0.622 (0.606)	Data 1.25e-04 (2.76e-04)	Tok/s 27132 (23267)	Loss/tok 3.2693 (3.3179)	LR 1.250e-04
0: TRAIN [1][4300/5173]	Time 0.763 (0.606)	Data 1.28e-04 (2.76e-04)	Tok/s 38840 (23272)	Loss/tok 3.5789 (3.3179)	LR 1.250e-04
0: TRAIN [1][4310/5173]	Time 0.689 (0.606)	Data 2.90e-04 (2.76e-04)	Tok/s 33515 (23269)	Loss/tok 3.4950 (3.3177)	LR 1.250e-04
0: TRAIN [1][4320/5173]	Time 0.504 (0.606)	Data 1.38e-04 (2.75e-04)	Tok/s 10564 (23274)	Loss/tok 2.6319 (3.3176)	LR 1.250e-04
0: TRAIN [1][4330/5173]	Time 0.565 (0.606)	Data 1.43e-04 (2.75e-04)	Tok/s 18288 (23271)	Loss/tok 2.9741 (3.3172)	LR 1.250e-04
0: TRAIN [1][4340/5173]	Time 0.762 (0.606)	Data 1.30e-04 (2.75e-04)	Tok/s 39031 (23272)	Loss/tok 3.6617 (3.3173)	LR 1.250e-04
0: TRAIN [1][4350/5173]	Time 0.624 (0.606)	Data 1.25e-04 (2.75e-04)	Tok/s 26909 (23281)	Loss/tok 3.3056 (3.3175)	LR 1.250e-04
0: TRAIN [1][4360/5173]	Time 0.565 (0.606)	Data 1.35e-04 (2.74e-04)	Tok/s 17891 (23272)	Loss/tok 3.0459 (3.3170)	LR 1.250e-04
0: TRAIN [1][4370/5173]	Time 0.567 (0.606)	Data 1.35e-04 (2.74e-04)	Tok/s 18141 (23267)	Loss/tok 3.1064 (3.3169)	LR 1.250e-04
0: TRAIN [1][4380/5173]	Time 0.629 (0.606)	Data 1.24e-04 (2.74e-04)	Tok/s 26534 (23269)	Loss/tok 3.3143 (3.3166)	LR 1.250e-04
0: TRAIN [1][4390/5173]	Time 0.625 (0.606)	Data 1.32e-04 (2.73e-04)	Tok/s 26810 (23264)	Loss/tok 3.2482 (3.3163)	LR 1.250e-04
0: TRAIN [1][4400/5173]	Time 0.628 (0.606)	Data 1.29e-04 (2.73e-04)	Tok/s 26643 (23258)	Loss/tok 3.1824 (3.3159)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][4410/5173]	Time 0.626 (0.606)	Data 1.34e-04 (2.73e-04)	Tok/s 26692 (23255)	Loss/tok 3.2999 (3.3157)	LR 1.250e-04
0: TRAIN [1][4420/5173]	Time 0.631 (0.606)	Data 1.28e-04 (2.72e-04)	Tok/s 26434 (23263)	Loss/tok 3.1852 (3.3156)	LR 1.250e-04
0: TRAIN [1][4430/5173]	Time 0.566 (0.606)	Data 1.30e-04 (2.72e-04)	Tok/s 17852 (23269)	Loss/tok 3.1336 (3.3156)	LR 1.250e-04
0: TRAIN [1][4440/5173]	Time 0.634 (0.606)	Data 1.25e-04 (2.72e-04)	Tok/s 26214 (23266)	Loss/tok 3.2108 (3.3154)	LR 1.250e-04
0: TRAIN [1][4450/5173]	Time 0.623 (0.606)	Data 1.34e-04 (2.71e-04)	Tok/s 26961 (23266)	Loss/tok 3.2500 (3.3151)	LR 1.250e-04
0: TRAIN [1][4460/5173]	Time 0.572 (0.606)	Data 1.31e-04 (2.71e-04)	Tok/s 17832 (23264)	Loss/tok 3.1607 (3.3149)	LR 1.250e-04
0: TRAIN [1][4470/5173]	Time 0.764 (0.606)	Data 5.86e-04 (2.71e-04)	Tok/s 38649 (23272)	Loss/tok 3.6842 (3.3149)	LR 1.250e-04
0: TRAIN [1][4480/5173]	Time 0.630 (0.606)	Data 1.28e-04 (2.71e-04)	Tok/s 26440 (23275)	Loss/tok 3.1604 (3.3148)	LR 1.250e-04
0: TRAIN [1][4490/5173]	Time 0.570 (0.606)	Data 1.32e-04 (2.70e-04)	Tok/s 18042 (23272)	Loss/tok 2.9800 (3.3146)	LR 1.250e-04
0: TRAIN [1][4500/5173]	Time 0.567 (0.606)	Data 3.07e-04 (2.70e-04)	Tok/s 18112 (23267)	Loss/tok 3.0531 (3.3142)	LR 1.250e-04
0: TRAIN [1][4510/5173]	Time 0.566 (0.606)	Data 2.79e-04 (2.70e-04)	Tok/s 18211 (23274)	Loss/tok 2.9833 (3.3142)	LR 1.250e-04
0: TRAIN [1][4520/5173]	Time 0.691 (0.606)	Data 1.31e-04 (2.70e-04)	Tok/s 33328 (23272)	Loss/tok 3.4405 (3.3142)	LR 1.250e-04
0: TRAIN [1][4530/5173]	Time 0.505 (0.606)	Data 1.31e-04 (2.69e-04)	Tok/s 10470 (23268)	Loss/tok 2.6065 (3.3139)	LR 1.250e-04
0: TRAIN [1][4540/5173]	Time 0.626 (0.606)	Data 1.27e-04 (2.69e-04)	Tok/s 26595 (23267)	Loss/tok 3.2759 (3.3137)	LR 1.250e-04
0: TRAIN [1][4550/5173]	Time 0.570 (0.606)	Data 1.31e-04 (2.69e-04)	Tok/s 18379 (23269)	Loss/tok 3.0580 (3.3138)	LR 1.250e-04
0: TRAIN [1][4560/5173]	Time 0.566 (0.606)	Data 1.27e-04 (2.68e-04)	Tok/s 18365 (23271)	Loss/tok 3.0131 (3.3137)	LR 1.250e-04
0: TRAIN [1][4570/5173]	Time 0.563 (0.606)	Data 3.40e-04 (2.68e-04)	Tok/s 18256 (23278)	Loss/tok 3.0702 (3.3138)	LR 6.250e-05
0: TRAIN [1][4580/5173]	Time 0.568 (0.606)	Data 1.31e-04 (2.68e-04)	Tok/s 18508 (23281)	Loss/tok 3.0198 (3.3136)	LR 6.250e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][4590/5173]	Time 0.492 (0.606)	Data 1.27e-04 (2.68e-04)	Tok/s 10673 (23275)	Loss/tok 2.6356 (3.3134)	LR 6.250e-05
0: TRAIN [1][4600/5173]	Time 0.491 (0.606)	Data 1.29e-04 (2.67e-04)	Tok/s 10885 (23274)	Loss/tok 2.6190 (3.3134)	LR 6.250e-05
0: TRAIN [1][4610/5173]	Time 0.564 (0.606)	Data 1.31e-04 (2.67e-04)	Tok/s 18561 (23274)	Loss/tok 3.0569 (3.3132)	LR 6.250e-05
0: TRAIN [1][4620/5173]	Time 0.567 (0.606)	Data 1.26e-04 (2.67e-04)	Tok/s 17906 (23274)	Loss/tok 3.1134 (3.3130)	LR 6.250e-05
0: TRAIN [1][4630/5173]	Time 0.506 (0.606)	Data 1.24e-04 (2.66e-04)	Tok/s 10621 (23274)	Loss/tok 2.5931 (3.3130)	LR 6.250e-05
0: TRAIN [1][4640/5173]	Time 0.630 (0.606)	Data 1.27e-04 (2.66e-04)	Tok/s 26318 (23272)	Loss/tok 3.2592 (3.3128)	LR 6.250e-05
0: TRAIN [1][4650/5173]	Time 0.567 (0.606)	Data 1.31e-04 (2.66e-04)	Tok/s 18336 (23264)	Loss/tok 3.0293 (3.3125)	LR 6.250e-05
0: TRAIN [1][4660/5173]	Time 0.618 (0.606)	Data 1.26e-04 (2.66e-04)	Tok/s 27183 (23269)	Loss/tok 3.2431 (3.3126)	LR 6.250e-05
0: TRAIN [1][4670/5173]	Time 0.629 (0.606)	Data 1.27e-04 (2.65e-04)	Tok/s 26772 (23267)	Loss/tok 3.2545 (3.3124)	LR 6.250e-05
0: TRAIN [1][4680/5173]	Time 0.568 (0.606)	Data 1.29e-04 (2.65e-04)	Tok/s 17824 (23267)	Loss/tok 3.0196 (3.3121)	LR 6.250e-05
0: TRAIN [1][4690/5173]	Time 0.564 (0.606)	Data 1.34e-04 (2.65e-04)	Tok/s 18258 (23268)	Loss/tok 3.1242 (3.3120)	LR 6.250e-05
0: TRAIN [1][4700/5173]	Time 0.568 (0.606)	Data 1.28e-04 (2.65e-04)	Tok/s 18201 (23270)	Loss/tok 2.9580 (3.3120)	LR 6.250e-05
0: TRAIN [1][4710/5173]	Time 0.483 (0.606)	Data 1.34e-04 (2.64e-04)	Tok/s 10788 (23265)	Loss/tok 2.7233 (3.3117)	LR 6.250e-05
0: TRAIN [1][4720/5173]	Time 0.568 (0.606)	Data 1.30e-04 (2.64e-04)	Tok/s 18307 (23261)	Loss/tok 3.1209 (3.3115)	LR 6.250e-05
0: TRAIN [1][4730/5173]	Time 0.688 (0.606)	Data 1.33e-04 (2.64e-04)	Tok/s 33976 (23266)	Loss/tok 3.4690 (3.3115)	LR 6.250e-05
0: TRAIN [1][4740/5173]	Time 0.505 (0.606)	Data 1.31e-04 (2.63e-04)	Tok/s 10436 (23264)	Loss/tok 2.6532 (3.3114)	LR 6.250e-05
0: TRAIN [1][4750/5173]	Time 0.693 (0.606)	Data 1.23e-04 (2.63e-04)	Tok/s 33644 (23266)	Loss/tok 3.5037 (3.3113)	LR 6.250e-05
0: TRAIN [1][4760/5173]	Time 0.761 (0.606)	Data 1.24e-04 (2.63e-04)	Tok/s 39958 (23269)	Loss/tok 3.4418 (3.3112)	LR 6.250e-05
0: TRAIN [1][4770/5173]	Time 0.569 (0.606)	Data 1.28e-04 (2.63e-04)	Tok/s 18228 (23275)	Loss/tok 3.0558 (3.3112)	LR 6.250e-05
0: TRAIN [1][4780/5173]	Time 0.624 (0.606)	Data 1.29e-04 (2.63e-04)	Tok/s 26596 (23280)	Loss/tok 3.2762 (3.3110)	LR 6.250e-05
0: TRAIN [1][4790/5173]	Time 0.569 (0.606)	Data 1.21e-04 (2.62e-04)	Tok/s 18387 (23285)	Loss/tok 3.1152 (3.3110)	LR 6.250e-05
0: TRAIN [1][4800/5173]	Time 0.565 (0.606)	Data 1.30e-04 (2.62e-04)	Tok/s 18240 (23279)	Loss/tok 3.0359 (3.3107)	LR 6.250e-05
0: TRAIN [1][4810/5173]	Time 0.457 (0.606)	Data 3.10e-04 (2.62e-04)	Tok/s 11544 (23276)	Loss/tok 2.5837 (3.3105)	LR 6.250e-05
0: TRAIN [1][4820/5173]	Time 0.632 (0.606)	Data 1.32e-04 (2.62e-04)	Tok/s 26962 (23288)	Loss/tok 3.2423 (3.3105)	LR 6.250e-05
0: TRAIN [1][4830/5173]	Time 0.569 (0.606)	Data 1.28e-04 (2.61e-04)	Tok/s 18300 (23285)	Loss/tok 3.0817 (3.3103)	LR 6.250e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][4840/5173]	Time 0.494 (0.606)	Data 3.06e-04 (2.61e-04)	Tok/s 10762 (23285)	Loss/tok 2.6393 (3.3102)	LR 6.250e-05
0: TRAIN [1][4850/5173]	Time 0.690 (0.606)	Data 1.25e-04 (2.61e-04)	Tok/s 33364 (23283)	Loss/tok 3.4839 (3.3100)	LR 6.250e-05
0: TRAIN [1][4860/5173]	Time 0.762 (0.606)	Data 1.39e-04 (2.61e-04)	Tok/s 39302 (23280)	Loss/tok 3.6183 (3.3099)	LR 6.250e-05
0: TRAIN [1][4870/5173]	Time 0.691 (0.606)	Data 1.36e-04 (2.60e-04)	Tok/s 33763 (23278)	Loss/tok 3.4537 (3.3097)	LR 6.250e-05
0: TRAIN [1][4880/5173]	Time 0.692 (0.606)	Data 1.36e-04 (2.60e-04)	Tok/s 33322 (23279)	Loss/tok 3.4941 (3.3096)	LR 6.250e-05
0: TRAIN [1][4890/5173]	Time 0.628 (0.606)	Data 1.23e-04 (2.60e-04)	Tok/s 27074 (23287)	Loss/tok 3.2921 (3.3096)	LR 6.250e-05
0: TRAIN [1][4900/5173]	Time 0.765 (0.606)	Data 3.09e-04 (2.60e-04)	Tok/s 39640 (23287)	Loss/tok 3.4556 (3.3095)	LR 6.250e-05
0: TRAIN [1][4910/5173]	Time 0.569 (0.606)	Data 1.27e-04 (2.59e-04)	Tok/s 18138 (23283)	Loss/tok 2.9849 (3.3092)	LR 6.250e-05
0: TRAIN [1][4920/5173]	Time 0.630 (0.606)	Data 1.22e-04 (2.59e-04)	Tok/s 26145 (23276)	Loss/tok 3.3303 (3.3090)	LR 6.250e-05
0: TRAIN [1][4930/5173]	Time 0.631 (0.606)	Data 1.26e-04 (2.59e-04)	Tok/s 26827 (23281)	Loss/tok 3.2250 (3.3089)	LR 6.250e-05
0: TRAIN [1][4940/5173]	Time 0.636 (0.606)	Data 1.28e-04 (2.59e-04)	Tok/s 26367 (23280)	Loss/tok 3.3089 (3.3087)	LR 6.250e-05
0: TRAIN [1][4950/5173]	Time 0.630 (0.606)	Data 1.32e-04 (2.58e-04)	Tok/s 26464 (23287)	Loss/tok 3.1855 (3.3088)	LR 6.250e-05
0: TRAIN [1][4960/5173]	Time 0.565 (0.606)	Data 1.32e-04 (2.58e-04)	Tok/s 18300 (23285)	Loss/tok 3.0235 (3.3085)	LR 6.250e-05
0: TRAIN [1][4970/5173]	Time 0.569 (0.606)	Data 1.30e-04 (2.58e-04)	Tok/s 18260 (23282)	Loss/tok 3.0452 (3.3082)	LR 6.250e-05
0: TRAIN [1][4980/5173]	Time 0.569 (0.606)	Data 1.34e-04 (2.58e-04)	Tok/s 17863 (23284)	Loss/tok 3.0278 (3.3081)	LR 6.250e-05
0: TRAIN [1][4990/5173]	Time 0.767 (0.606)	Data 1.30e-04 (2.57e-04)	Tok/s 38915 (23285)	Loss/tok 3.5196 (3.3081)	LR 6.250e-05
0: TRAIN [1][5000/5173]	Time 0.772 (0.606)	Data 3.15e-04 (2.57e-04)	Tok/s 38619 (23279)	Loss/tok 3.6157 (3.3079)	LR 6.250e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][5010/5173]	Time 0.691 (0.606)	Data 1.25e-04 (2.57e-04)	Tok/s 33620 (23283)	Loss/tok 3.5044 (3.3078)	LR 6.250e-05
0: TRAIN [1][5020/5173]	Time 0.754 (0.606)	Data 1.32e-04 (2.57e-04)	Tok/s 39910 (23289)	Loss/tok 3.5379 (3.3077)	LR 6.250e-05
0: TRAIN [1][5030/5173]	Time 0.629 (0.606)	Data 1.30e-04 (2.57e-04)	Tok/s 26518 (23293)	Loss/tok 3.2044 (3.3077)	LR 6.250e-05
0: TRAIN [1][5040/5173]	Time 0.627 (0.606)	Data 1.41e-04 (2.56e-04)	Tok/s 27062 (23306)	Loss/tok 3.1029 (3.3079)	LR 6.250e-05
0: TRAIN [1][5050/5173]	Time 0.629 (0.606)	Data 1.30e-04 (2.56e-04)	Tok/s 26565 (23304)	Loss/tok 3.2937 (3.3076)	LR 6.250e-05
0: TRAIN [1][5060/5173]	Time 0.491 (0.606)	Data 1.34e-04 (2.56e-04)	Tok/s 10668 (23304)	Loss/tok 2.6382 (3.3075)	LR 6.250e-05
0: TRAIN [1][5070/5173]	Time 0.566 (0.606)	Data 1.40e-04 (2.56e-04)	Tok/s 18069 (23309)	Loss/tok 3.1064 (3.3075)	LR 6.250e-05
0: TRAIN [1][5080/5173]	Time 0.623 (0.606)	Data 1.23e-04 (2.56e-04)	Tok/s 26621 (23309)	Loss/tok 3.2980 (3.3074)	LR 6.250e-05
0: TRAIN [1][5090/5173]	Time 0.567 (0.606)	Data 1.36e-04 (2.55e-04)	Tok/s 18091 (23308)	Loss/tok 2.9908 (3.3072)	LR 6.250e-05
0: TRAIN [1][5100/5173]	Time 0.631 (0.606)	Data 1.47e-04 (2.55e-04)	Tok/s 26733 (23318)	Loss/tok 3.1477 (3.3074)	LR 6.250e-05
0: TRAIN [1][5110/5173]	Time 0.569 (0.606)	Data 1.40e-04 (2.55e-04)	Tok/s 17903 (23322)	Loss/tok 3.0484 (3.3074)	LR 6.250e-05
0: TRAIN [1][5120/5173]	Time 0.564 (0.606)	Data 1.31e-04 (2.55e-04)	Tok/s 18777 (23329)	Loss/tok 3.0585 (3.3074)	LR 6.250e-05
0: TRAIN [1][5130/5173]	Time 0.566 (0.606)	Data 1.29e-04 (2.54e-04)	Tok/s 18211 (23323)	Loss/tok 3.0039 (3.3072)	LR 6.250e-05
0: TRAIN [1][5140/5173]	Time 0.569 (0.606)	Data 1.27e-04 (2.54e-04)	Tok/s 18257 (23320)	Loss/tok 3.0585 (3.3070)	LR 6.250e-05
0: TRAIN [1][5150/5173]	Time 0.569 (0.606)	Data 1.31e-04 (2.54e-04)	Tok/s 18249 (23318)	Loss/tok 3.0623 (3.3068)	LR 6.250e-05
0: TRAIN [1][5160/5173]	Time 0.567 (0.606)	Data 1.25e-04 (2.54e-04)	Tok/s 18606 (23313)	Loss/tok 3.0693 (3.3066)	LR 6.250e-05
0: TRAIN [1][5170/5173]	Time 0.563 (0.606)	Data 1.31e-04 (2.53e-04)	Tok/s 18392 (23313)	Loss/tok 3.0066 (3.3063)	LR 6.250e-05
:::MLL 1586532406.378 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 525}}
:::MLL 1586532406.379 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [1][0/8]	Time 0.747 (0.747)	Decoder iters 149.0 (149.0)	Tok/s 22055 (22055)
0: Running moses detokenizer
0: BLEU(score=23.06265479257013, counts=[36620, 18009, 10083, 5878], totals=[65563, 62560, 59557, 56559], precisions=[55.85467413022589, 28.78676470588235, 16.929999832093625, 10.392687282306971], bp=1.0, sys_len=65563, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1586532412.455 eval_accuracy: {"value": 23.06, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 536}}
:::MLL 1586532412.455 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 1	Training Loss: 3.3077	Test BLEU: 23.06
0: Performance: Epoch: 1	Training: 69941 Tok/s
0: Finished epoch 1
:::MLL 1586532412.456 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 558}}
:::MLL 1586532412.456 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1586532412.456 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 515}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 174004288
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][0/5173]	Time 1.249 (1.249)	Data 5.29e-01 (5.29e-01)	Tok/s 18523 (18523)	Loss/tok 3.3845 (3.3845)	LR 6.250e-05
0: TRAIN [2][10/5173]	Time 0.630 (0.674)	Data 1.20e-04 (4.83e-02)	Tok/s 26587 (24202)	Loss/tok 3.2421 (3.2297)	LR 6.250e-05
0: TRAIN [2][20/5173]	Time 0.678 (0.634)	Data 1.35e-04 (2.53e-02)	Tok/s 34090 (22885)	Loss/tok 3.3703 (3.1867)	LR 6.250e-05
0: TRAIN [2][30/5173]	Time 0.768 (0.629)	Data 1.34e-04 (1.72e-02)	Tok/s 38718 (23403)	Loss/tok 3.5231 (3.1927)	LR 6.250e-05
0: TRAIN [2][40/5173]	Time 0.486 (0.632)	Data 1.51e-04 (1.30e-02)	Tok/s 10916 (24354)	Loss/tok 2.7319 (3.2204)	LR 6.250e-05
0: TRAIN [2][50/5173]	Time 0.627 (0.632)	Data 2.94e-04 (1.05e-02)	Tok/s 26902 (24767)	Loss/tok 3.1900 (3.2175)	LR 6.250e-05
0: TRAIN [2][60/5173]	Time 0.634 (0.633)	Data 1.31e-04 (8.82e-03)	Tok/s 26614 (25207)	Loss/tok 3.1554 (3.2315)	LR 6.250e-05
0: TRAIN [2][70/5173]	Time 0.691 (0.628)	Data 1.24e-04 (7.60e-03)	Tok/s 34117 (24830)	Loss/tok 3.3185 (3.2205)	LR 6.250e-05
0: TRAIN [2][80/5173]	Time 0.690 (0.624)	Data 1.35e-04 (6.68e-03)	Tok/s 34151 (24396)	Loss/tok 3.2885 (3.2119)	LR 6.250e-05
0: TRAIN [2][90/5173]	Time 0.693 (0.623)	Data 1.39e-04 (5.96e-03)	Tok/s 34276 (24412)	Loss/tok 3.3733 (3.2074)	LR 6.250e-05
0: TRAIN [2][100/5173]	Time 0.631 (0.622)	Data 1.31e-04 (5.39e-03)	Tok/s 26850 (24451)	Loss/tok 3.1845 (3.2058)	LR 6.250e-05
0: TRAIN [2][110/5173]	Time 0.503 (0.622)	Data 3.08e-04 (4.92e-03)	Tok/s 10538 (24522)	Loss/tok 2.4277 (3.2094)	LR 6.250e-05
0: TRAIN [2][120/5173]	Time 0.570 (0.621)	Data 1.37e-04 (4.52e-03)	Tok/s 17795 (24406)	Loss/tok 2.9135 (3.2013)	LR 6.250e-05
0: TRAIN [2][130/5173]	Time 0.567 (0.621)	Data 1.25e-04 (4.19e-03)	Tok/s 18341 (24517)	Loss/tok 2.9700 (3.2039)	LR 6.250e-05
0: TRAIN [2][140/5173]	Time 0.627 (0.620)	Data 1.24e-04 (3.90e-03)	Tok/s 27174 (24440)	Loss/tok 3.1288 (3.1996)	LR 6.250e-05
0: TRAIN [2][150/5173]	Time 0.568 (0.621)	Data 1.32e-04 (3.65e-03)	Tok/s 17669 (24668)	Loss/tok 2.9704 (3.2006)	LR 6.250e-05
0: TRAIN [2][160/5173]	Time 0.566 (0.620)	Data 1.24e-04 (3.43e-03)	Tok/s 18560 (24482)	Loss/tok 2.9177 (3.1960)	LR 6.250e-05
0: TRAIN [2][170/5173]	Time 0.568 (0.619)	Data 1.27e-04 (3.24e-03)	Tok/s 18167 (24483)	Loss/tok 2.9714 (3.1962)	LR 6.250e-05
0: TRAIN [2][180/5173]	Time 0.629 (0.618)	Data 1.29e-04 (3.07e-03)	Tok/s 26749 (24373)	Loss/tok 3.2896 (3.1938)	LR 6.250e-05
0: TRAIN [2][190/5173]	Time 0.767 (0.617)	Data 1.26e-04 (2.91e-03)	Tok/s 38138 (24207)	Loss/tok 3.6431 (3.1917)	LR 6.250e-05
0: TRAIN [2][200/5173]	Time 0.567 (0.617)	Data 1.27e-04 (2.78e-03)	Tok/s 18162 (24215)	Loss/tok 3.0160 (3.1925)	LR 6.250e-05
0: TRAIN [2][210/5173]	Time 0.569 (0.615)	Data 1.36e-04 (2.65e-03)	Tok/s 18041 (24094)	Loss/tok 2.8613 (3.1875)	LR 3.125e-05
0: TRAIN [2][220/5173]	Time 0.626 (0.615)	Data 1.26e-04 (2.54e-03)	Tok/s 26609 (24015)	Loss/tok 3.1437 (3.1836)	LR 3.125e-05
0: TRAIN [2][230/5173]	Time 0.557 (0.615)	Data 1.33e-04 (2.43e-03)	Tok/s 18769 (24047)	Loss/tok 3.0619 (3.1818)	LR 3.125e-05
0: TRAIN [2][240/5173]	Time 0.568 (0.613)	Data 1.26e-04 (2.34e-03)	Tok/s 18401 (23914)	Loss/tok 2.8622 (3.1785)	LR 3.125e-05
0: TRAIN [2][250/5173]	Time 0.692 (0.614)	Data 1.34e-04 (2.25e-03)	Tok/s 34046 (23970)	Loss/tok 3.2366 (3.1807)	LR 3.125e-05
0: TRAIN [2][260/5173]	Time 0.691 (0.614)	Data 1.35e-04 (2.17e-03)	Tok/s 33642 (24001)	Loss/tok 3.4515 (3.1850)	LR 3.125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][270/5173]	Time 0.566 (0.614)	Data 1.27e-04 (2.10e-03)	Tok/s 18194 (24005)	Loss/tok 2.9991 (3.1824)	LR 3.125e-05
0: TRAIN [2][280/5173]	Time 0.570 (0.614)	Data 1.41e-04 (2.03e-03)	Tok/s 18125 (23994)	Loss/tok 3.0702 (3.1816)	LR 3.125e-05
0: TRAIN [2][290/5173]	Time 0.568 (0.614)	Data 1.29e-04 (1.96e-03)	Tok/s 18326 (24066)	Loss/tok 2.9771 (3.1837)	LR 3.125e-05
0: TRAIN [2][300/5173]	Time 0.632 (0.614)	Data 1.34e-04 (1.90e-03)	Tok/s 26179 (23989)	Loss/tok 3.1970 (3.1806)	LR 3.125e-05
0: TRAIN [2][310/5173]	Time 0.558 (0.613)	Data 1.51e-04 (1.84e-03)	Tok/s 19019 (23949)	Loss/tok 3.0394 (3.1802)	LR 3.125e-05
0: TRAIN [2][320/5173]	Time 0.566 (0.613)	Data 1.36e-04 (1.79e-03)	Tok/s 18190 (23908)	Loss/tok 3.0731 (3.1817)	LR 3.125e-05
0: TRAIN [2][330/5173]	Time 0.568 (0.612)	Data 1.33e-04 (1.74e-03)	Tok/s 18277 (23837)	Loss/tok 2.9348 (3.1800)	LR 3.125e-05
0: TRAIN [2][340/5173]	Time 0.633 (0.613)	Data 1.34e-04 (1.69e-03)	Tok/s 26114 (23857)	Loss/tok 3.2183 (3.1801)	LR 3.125e-05
0: TRAIN [2][350/5173]	Time 0.567 (0.613)	Data 1.33e-04 (1.65e-03)	Tok/s 18564 (23873)	Loss/tok 2.9428 (3.1813)	LR 3.125e-05
0: TRAIN [2][360/5173]	Time 0.488 (0.612)	Data 1.27e-04 (1.61e-03)	Tok/s 11056 (23829)	Loss/tok 2.5452 (3.1809)	LR 3.125e-05
0: TRAIN [2][370/5173]	Time 0.632 (0.612)	Data 1.37e-04 (1.57e-03)	Tok/s 26583 (23835)	Loss/tok 3.2855 (3.1807)	LR 3.125e-05
0: TRAIN [2][380/5173]	Time 0.564 (0.612)	Data 1.32e-04 (1.53e-03)	Tok/s 18078 (23855)	Loss/tok 2.9284 (3.1807)	LR 3.125e-05
0: TRAIN [2][390/5173]	Time 0.692 (0.612)	Data 1.31e-04 (1.50e-03)	Tok/s 33785 (23859)	Loss/tok 3.3133 (3.1800)	LR 3.125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][400/5173]	Time 0.569 (0.612)	Data 1.38e-04 (1.46e-03)	Tok/s 17928 (23794)	Loss/tok 2.9480 (3.1784)	LR 3.125e-05
0: TRAIN [2][410/5173]	Time 0.626 (0.612)	Data 1.32e-04 (1.43e-03)	Tok/s 26535 (23768)	Loss/tok 3.1100 (3.1773)	LR 3.125e-05
0: TRAIN [2][420/5173]	Time 0.567 (0.611)	Data 1.25e-04 (1.40e-03)	Tok/s 18411 (23707)	Loss/tok 2.9383 (3.1778)	LR 3.125e-05
0: TRAIN [2][430/5173]	Time 0.566 (0.611)	Data 1.35e-04 (1.37e-03)	Tok/s 17847 (23702)	Loss/tok 3.0472 (3.1785)	LR 3.125e-05
0: TRAIN [2][440/5173]	Time 0.629 (0.612)	Data 1.37e-04 (1.34e-03)	Tok/s 27138 (23791)	Loss/tok 3.1839 (3.1814)	LR 3.125e-05
0: TRAIN [2][450/5173]	Time 0.631 (0.612)	Data 1.28e-04 (1.32e-03)	Tok/s 26963 (23835)	Loss/tok 3.1519 (3.1830)	LR 3.125e-05
0: TRAIN [2][460/5173]	Time 0.635 (0.611)	Data 1.29e-04 (1.29e-03)	Tok/s 26180 (23699)	Loss/tok 3.1750 (3.1800)	LR 3.125e-05
0: TRAIN [2][470/5173]	Time 0.568 (0.612)	Data 1.70e-04 (1.27e-03)	Tok/s 17835 (23762)	Loss/tok 2.9975 (3.1812)	LR 3.125e-05
0: TRAIN [2][480/5173]	Time 0.568 (0.612)	Data 1.28e-04 (1.25e-03)	Tok/s 17806 (23757)	Loss/tok 2.9239 (3.1822)	LR 3.125e-05
0: TRAIN [2][490/5173]	Time 0.625 (0.612)	Data 1.42e-04 (1.22e-03)	Tok/s 26602 (23752)	Loss/tok 3.1072 (3.1820)	LR 3.125e-05
0: TRAIN [2][500/5173]	Time 0.626 (0.612)	Data 1.28e-04 (1.20e-03)	Tok/s 26882 (23800)	Loss/tok 3.0835 (3.1829)	LR 3.125e-05
0: TRAIN [2][510/5173]	Time 0.565 (0.612)	Data 1.27e-04 (1.18e-03)	Tok/s 18367 (23784)	Loss/tok 2.9438 (3.1830)	LR 3.125e-05
0: TRAIN [2][520/5173]	Time 0.567 (0.612)	Data 1.31e-04 (1.16e-03)	Tok/s 17803 (23756)	Loss/tok 3.0268 (3.1818)	LR 3.125e-05
0: TRAIN [2][530/5173]	Time 0.504 (0.611)	Data 1.36e-04 (1.14e-03)	Tok/s 10381 (23703)	Loss/tok 2.6450 (3.1804)	LR 3.125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][540/5173]	Time 0.565 (0.611)	Data 1.30e-04 (1.12e-03)	Tok/s 18071 (23687)	Loss/tok 2.9200 (3.1805)	LR 3.125e-05
0: TRAIN [2][550/5173]	Time 0.570 (0.611)	Data 1.27e-04 (1.11e-03)	Tok/s 18016 (23739)	Loss/tok 2.8926 (3.1825)	LR 3.125e-05
0: TRAIN [2][560/5173]	Time 0.567 (0.611)	Data 1.30e-04 (1.09e-03)	Tok/s 18279 (23683)	Loss/tok 2.8818 (3.1810)	LR 3.125e-05
0: TRAIN [2][570/5173]	Time 0.624 (0.611)	Data 1.37e-04 (1.07e-03)	Tok/s 26734 (23698)	Loss/tok 3.1656 (3.1815)	LR 3.125e-05
0: TRAIN [2][580/5173]	Time 0.564 (0.611)	Data 1.27e-04 (1.06e-03)	Tok/s 18137 (23663)	Loss/tok 2.9051 (3.1802)	LR 3.125e-05
0: TRAIN [2][590/5173]	Time 0.567 (0.610)	Data 1.36e-04 (1.04e-03)	Tok/s 18446 (23639)	Loss/tok 3.0009 (3.1800)	LR 3.125e-05
0: TRAIN [2][600/5173]	Time 0.626 (0.611)	Data 1.35e-04 (1.02e-03)	Tok/s 26533 (23685)	Loss/tok 3.1486 (3.1801)	LR 3.125e-05
0: TRAIN [2][610/5173]	Time 0.566 (0.611)	Data 1.36e-04 (1.01e-03)	Tok/s 18403 (23733)	Loss/tok 3.0608 (3.1823)	LR 3.125e-05
0: TRAIN [2][620/5173]	Time 0.502 (0.610)	Data 1.41e-04 (9.96e-04)	Tok/s 10284 (23650)	Loss/tok 2.6747 (3.1801)	LR 3.125e-05
0: TRAIN [2][630/5173]	Time 0.690 (0.611)	Data 1.54e-04 (9.83e-04)	Tok/s 33971 (23688)	Loss/tok 3.3311 (3.1807)	LR 3.125e-05
0: TRAIN [2][640/5173]	Time 0.624 (0.610)	Data 1.47e-04 (9.70e-04)	Tok/s 26845 (23658)	Loss/tok 3.1638 (3.1794)	LR 3.125e-05
0: TRAIN [2][650/5173]	Time 0.628 (0.610)	Data 1.50e-04 (9.57e-04)	Tok/s 26769 (23650)	Loss/tok 3.1474 (3.1793)	LR 3.125e-05
0: TRAIN [2][660/5173]	Time 0.565 (0.610)	Data 1.38e-04 (9.45e-04)	Tok/s 18432 (23606)	Loss/tok 2.9867 (3.1784)	LR 3.125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][670/5173]	Time 0.629 (0.610)	Data 1.33e-04 (9.33e-04)	Tok/s 26705 (23618)	Loss/tok 3.2098 (3.1789)	LR 3.125e-05
0: TRAIN [2][680/5173]	Time 0.493 (0.609)	Data 2.81e-04 (9.22e-04)	Tok/s 10899 (23555)	Loss/tok 2.5946 (3.1773)	LR 3.125e-05
0: TRAIN [2][690/5173]	Time 0.564 (0.609)	Data 1.34e-04 (9.10e-04)	Tok/s 17845 (23512)	Loss/tok 2.9563 (3.1762)	LR 3.125e-05
0: TRAIN [2][700/5173]	Time 0.506 (0.609)	Data 1.30e-04 (8.99e-04)	Tok/s 10563 (23506)	Loss/tok 2.5772 (3.1760)	LR 3.125e-05
0: TRAIN [2][710/5173]	Time 0.569 (0.609)	Data 1.35e-04 (8.89e-04)	Tok/s 17950 (23510)	Loss/tok 2.9581 (3.1753)	LR 3.125e-05
0: TRAIN [2][720/5173]	Time 0.567 (0.609)	Data 1.34e-04 (8.78e-04)	Tok/s 18372 (23461)	Loss/tok 3.0302 (3.1739)	LR 3.125e-05
0: TRAIN [2][730/5173]	Time 0.656 (0.609)	Data 1.31e-04 (8.68e-04)	Tok/s 35344 (23478)	Loss/tok 3.3837 (3.1744)	LR 3.125e-05
0: TRAIN [2][740/5173]	Time 0.565 (0.608)	Data 1.27e-04 (8.58e-04)	Tok/s 18239 (23410)	Loss/tok 2.9987 (3.1728)	LR 3.125e-05
0: TRAIN [2][750/5173]	Time 0.492 (0.608)	Data 1.36e-04 (8.49e-04)	Tok/s 10996 (23365)	Loss/tok 2.6123 (3.1721)	LR 3.125e-05
0: TRAIN [2][760/5173]	Time 0.570 (0.607)	Data 1.30e-04 (8.40e-04)	Tok/s 17938 (23329)	Loss/tok 2.9857 (3.1706)	LR 3.125e-05
0: TRAIN [2][770/5173]	Time 0.569 (0.607)	Data 1.44e-04 (8.30e-04)	Tok/s 18132 (23346)	Loss/tok 3.0166 (3.1707)	LR 3.125e-05
0: TRAIN [2][780/5173]	Time 0.689 (0.607)	Data 1.34e-04 (8.22e-04)	Tok/s 33887 (23363)	Loss/tok 3.3254 (3.1709)	LR 3.125e-05
0: TRAIN [2][790/5173]	Time 0.628 (0.607)	Data 1.33e-04 (8.13e-04)	Tok/s 26992 (23347)	Loss/tok 3.1469 (3.1709)	LR 3.125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][800/5173]	Time 0.507 (0.607)	Data 1.30e-04 (8.05e-04)	Tok/s 10647 (23329)	Loss/tok 2.5906 (3.1717)	LR 3.125e-05
0: TRAIN [2][810/5173]	Time 0.690 (0.607)	Data 1.42e-04 (7.97e-04)	Tok/s 33997 (23346)	Loss/tok 3.3287 (3.1721)	LR 3.125e-05
0: TRAIN [2][820/5173]	Time 0.622 (0.607)	Data 1.39e-04 (7.89e-04)	Tok/s 27070 (23361)	Loss/tok 3.1308 (3.1720)	LR 3.125e-05
0: TRAIN [2][830/5173]	Time 0.630 (0.607)	Data 1.26e-04 (7.81e-04)	Tok/s 26929 (23340)	Loss/tok 3.1927 (3.1712)	LR 3.125e-05
0: TRAIN [2][840/5173]	Time 0.757 (0.607)	Data 3.39e-04 (7.74e-04)	Tok/s 39359 (23359)	Loss/tok 3.5332 (3.1721)	LR 3.125e-05
0: TRAIN [2][850/5173]	Time 0.634 (0.607)	Data 1.36e-04 (7.66e-04)	Tok/s 26600 (23359)	Loss/tok 3.1665 (3.1715)	LR 3.125e-05
0: TRAIN [2][860/5173]	Time 0.762 (0.607)	Data 1.27e-04 (7.59e-04)	Tok/s 38396 (23323)	Loss/tok 3.5212 (3.1708)	LR 3.125e-05
0: TRAIN [2][870/5173]	Time 0.568 (0.607)	Data 1.31e-04 (7.52e-04)	Tok/s 18275 (23336)	Loss/tok 2.9892 (3.1705)	LR 3.125e-05
0: TRAIN [2][880/5173]	Time 0.690 (0.607)	Data 1.33e-04 (7.45e-04)	Tok/s 33437 (23353)	Loss/tok 3.3324 (3.1707)	LR 3.125e-05
0: TRAIN [2][890/5173]	Time 0.690 (0.607)	Data 1.59e-04 (7.38e-04)	Tok/s 33796 (23359)	Loss/tok 3.4023 (3.1709)	LR 3.125e-05
0: TRAIN [2][900/5173]	Time 0.691 (0.608)	Data 1.64e-04 (7.32e-04)	Tok/s 33940 (23380)	Loss/tok 3.3584 (3.1713)	LR 3.125e-05
0: TRAIN [2][910/5173]	Time 0.690 (0.608)	Data 1.46e-04 (7.25e-04)	Tok/s 33852 (23419)	Loss/tok 3.4115 (3.1717)	LR 3.125e-05
0: TRAIN [2][920/5173]	Time 0.624 (0.608)	Data 1.38e-04 (7.19e-04)	Tok/s 26889 (23399)	Loss/tok 3.1750 (3.1709)	LR 3.125e-05
0: TRAIN [2][930/5173]	Time 0.693 (0.608)	Data 2.90e-04 (7.13e-04)	Tok/s 33464 (23423)	Loss/tok 3.3790 (3.1710)	LR 3.125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][940/5173]	Time 0.630 (0.608)	Data 1.32e-04 (7.07e-04)	Tok/s 26497 (23400)	Loss/tok 3.1513 (3.1706)	LR 3.125e-05
0: TRAIN [2][950/5173]	Time 0.628 (0.608)	Data 1.32e-04 (7.01e-04)	Tok/s 26978 (23390)	Loss/tok 3.1200 (3.1701)	LR 3.125e-05
0: TRAIN [2][960/5173]	Time 0.506 (0.607)	Data 1.39e-04 (6.95e-04)	Tok/s 10213 (23319)	Loss/tok 2.5015 (3.1686)	LR 3.125e-05
0: TRAIN [2][970/5173]	Time 0.565 (0.607)	Data 1.40e-04 (6.89e-04)	Tok/s 18208 (23310)	Loss/tok 3.0564 (3.1681)	LR 3.125e-05
0: TRAIN [2][980/5173]	Time 0.568 (0.606)	Data 1.34e-04 (6.84e-04)	Tok/s 18013 (23260)	Loss/tok 2.9157 (3.1671)	LR 3.125e-05
0: TRAIN [2][990/5173]	Time 0.568 (0.607)	Data 1.31e-04 (6.79e-04)	Tok/s 18035 (23277)	Loss/tok 2.9326 (3.1677)	LR 3.125e-05
0: TRAIN [2][1000/5173]	Time 0.567 (0.607)	Data 1.34e-04 (6.73e-04)	Tok/s 18192 (23285)	Loss/tok 2.9521 (3.1676)	LR 3.125e-05
0: TRAIN [2][1010/5173]	Time 0.631 (0.607)	Data 1.37e-04 (6.68e-04)	Tok/s 26853 (23281)	Loss/tok 3.1303 (3.1679)	LR 3.125e-05
0: TRAIN [2][1020/5173]	Time 0.566 (0.607)	Data 1.25e-04 (6.63e-04)	Tok/s 18128 (23310)	Loss/tok 3.0024 (3.1682)	LR 3.125e-05
0: TRAIN [2][1030/5173]	Time 0.626 (0.607)	Data 1.40e-04 (6.58e-04)	Tok/s 26768 (23328)	Loss/tok 3.1291 (3.1687)	LR 1.563e-05
0: TRAIN [2][1040/5173]	Time 0.629 (0.607)	Data 1.34e-04 (6.53e-04)	Tok/s 26419 (23325)	Loss/tok 3.1128 (3.1684)	LR 1.563e-05
0: TRAIN [2][1050/5173]	Time 0.556 (0.607)	Data 1.27e-04 (6.48e-04)	Tok/s 18981 (23336)	Loss/tok 3.0276 (3.1685)	LR 1.563e-05
0: TRAIN [2][1060/5173]	Time 0.568 (0.607)	Data 1.35e-04 (6.44e-04)	Tok/s 17894 (23336)	Loss/tok 3.0349 (3.1680)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1070/5173]	Time 0.567 (0.607)	Data 1.33e-04 (6.39e-04)	Tok/s 18406 (23334)	Loss/tok 2.9747 (3.1676)	LR 1.563e-05
0: TRAIN [2][1080/5173]	Time 0.625 (0.607)	Data 1.34e-04 (6.35e-04)	Tok/s 27184 (23345)	Loss/tok 3.0904 (3.1678)	LR 1.563e-05
0: TRAIN [2][1090/5173]	Time 0.633 (0.607)	Data 3.01e-04 (6.30e-04)	Tok/s 27039 (23387)	Loss/tok 3.1369 (3.1683)	LR 1.563e-05
0: TRAIN [2][1100/5173]	Time 0.630 (0.607)	Data 1.34e-04 (6.26e-04)	Tok/s 26595 (23371)	Loss/tok 3.0322 (3.1676)	LR 1.563e-05
0: TRAIN [2][1110/5173]	Time 0.569 (0.607)	Data 1.30e-04 (6.21e-04)	Tok/s 18010 (23344)	Loss/tok 2.9404 (3.1672)	LR 1.563e-05
0: TRAIN [2][1120/5173]	Time 0.565 (0.607)	Data 1.35e-04 (6.17e-04)	Tok/s 18587 (23334)	Loss/tok 3.0127 (3.1669)	LR 1.563e-05
0: TRAIN [2][1130/5173]	Time 0.566 (0.607)	Data 1.30e-04 (6.13e-04)	Tok/s 18256 (23330)	Loss/tok 3.0607 (3.1670)	LR 1.563e-05
0: TRAIN [2][1140/5173]	Time 0.504 (0.607)	Data 1.34e-04 (6.08e-04)	Tok/s 10441 (23312)	Loss/tok 2.5476 (3.1666)	LR 1.563e-05
0: TRAIN [2][1150/5173]	Time 0.566 (0.607)	Data 1.35e-04 (6.04e-04)	Tok/s 18338 (23323)	Loss/tok 2.9875 (3.1668)	LR 1.563e-05
0: TRAIN [2][1160/5173]	Time 0.569 (0.607)	Data 1.55e-04 (6.01e-04)	Tok/s 18438 (23336)	Loss/tok 2.9607 (3.1676)	LR 1.563e-05
0: TRAIN [2][1170/5173]	Time 0.691 (0.607)	Data 1.36e-04 (5.97e-04)	Tok/s 33597 (23375)	Loss/tok 3.3897 (3.1688)	LR 1.563e-05
0: TRAIN [2][1180/5173]	Time 0.569 (0.608)	Data 1.36e-04 (5.93e-04)	Tok/s 17887 (23399)	Loss/tok 3.0619 (3.1693)	LR 1.563e-05
0: TRAIN [2][1190/5173]	Time 0.568 (0.607)	Data 1.35e-04 (5.89e-04)	Tok/s 18404 (23356)	Loss/tok 3.1229 (3.1687)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1200/5173]	Time 0.564 (0.607)	Data 1.33e-04 (5.85e-04)	Tok/s 17975 (23365)	Loss/tok 2.8958 (3.1691)	LR 1.563e-05
0: TRAIN [2][1210/5173]	Time 0.631 (0.608)	Data 1.26e-04 (5.82e-04)	Tok/s 26495 (23382)	Loss/tok 3.2080 (3.1694)	LR 1.563e-05
0: TRAIN [2][1220/5173]	Time 0.505 (0.608)	Data 1.36e-04 (5.78e-04)	Tok/s 10560 (23382)	Loss/tok 2.5569 (3.1697)	LR 1.563e-05
0: TRAIN [2][1230/5173]	Time 0.568 (0.608)	Data 1.54e-04 (5.75e-04)	Tok/s 18876 (23418)	Loss/tok 2.9553 (3.1709)	LR 1.563e-05
0: TRAIN [2][1240/5173]	Time 0.567 (0.608)	Data 1.29e-04 (5.72e-04)	Tok/s 17979 (23397)	Loss/tok 2.9581 (3.1703)	LR 1.563e-05
0: TRAIN [2][1250/5173]	Time 0.694 (0.608)	Data 1.30e-04 (5.68e-04)	Tok/s 33602 (23427)	Loss/tok 3.2906 (3.1708)	LR 1.563e-05
0: TRAIN [2][1260/5173]	Time 0.687 (0.608)	Data 1.39e-04 (5.65e-04)	Tok/s 34086 (23437)	Loss/tok 3.2489 (3.1708)	LR 1.563e-05
0: TRAIN [2][1270/5173]	Time 0.624 (0.608)	Data 1.27e-04 (5.61e-04)	Tok/s 27044 (23435)	Loss/tok 3.2203 (3.1709)	LR 1.563e-05
0: TRAIN [2][1280/5173]	Time 0.628 (0.608)	Data 1.29e-04 (5.58e-04)	Tok/s 26645 (23428)	Loss/tok 3.0971 (3.1706)	LR 1.563e-05
0: TRAIN [2][1290/5173]	Time 0.567 (0.608)	Data 1.35e-04 (5.55e-04)	Tok/s 18314 (23455)	Loss/tok 3.0288 (3.1712)	LR 1.563e-05
0: TRAIN [2][1300/5173]	Time 0.626 (0.608)	Data 1.29e-04 (5.52e-04)	Tok/s 26737 (23448)	Loss/tok 3.1928 (3.1709)	LR 1.563e-05
0: TRAIN [2][1310/5173]	Time 0.566 (0.608)	Data 1.34e-04 (5.49e-04)	Tok/s 18475 (23426)	Loss/tok 2.9661 (3.1706)	LR 1.563e-05
0: TRAIN [2][1320/5173]	Time 0.568 (0.608)	Data 1.33e-04 (5.46e-04)	Tok/s 18010 (23419)	Loss/tok 3.0351 (3.1700)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1330/5173]	Time 0.568 (0.608)	Data 1.24e-04 (5.42e-04)	Tok/s 18302 (23417)	Loss/tok 2.9348 (3.1699)	LR 1.563e-05
0: TRAIN [2][1340/5173]	Time 0.566 (0.608)	Data 1.28e-04 (5.40e-04)	Tok/s 18277 (23411)	Loss/tok 2.9762 (3.1692)	LR 1.563e-05
0: TRAIN [2][1350/5173]	Time 0.690 (0.607)	Data 1.29e-04 (5.37e-04)	Tok/s 33574 (23403)	Loss/tok 3.2970 (3.1688)	LR 1.563e-05
0: TRAIN [2][1360/5173]	Time 0.569 (0.607)	Data 1.26e-04 (5.34e-04)	Tok/s 18201 (23377)	Loss/tok 2.9187 (3.1681)	LR 1.563e-05
0: TRAIN [2][1370/5173]	Time 0.693 (0.607)	Data 1.40e-04 (5.31e-04)	Tok/s 33526 (23371)	Loss/tok 3.3054 (3.1681)	LR 1.563e-05
0: TRAIN [2][1380/5173]	Time 0.691 (0.607)	Data 1.24e-04 (5.28e-04)	Tok/s 34171 (23357)	Loss/tok 3.2987 (3.1678)	LR 1.563e-05
0: TRAIN [2][1390/5173]	Time 0.692 (0.607)	Data 1.24e-04 (5.25e-04)	Tok/s 33342 (23333)	Loss/tok 3.3775 (3.1673)	LR 1.563e-05
0: TRAIN [2][1400/5173]	Time 0.624 (0.607)	Data 1.29e-04 (5.23e-04)	Tok/s 26948 (23336)	Loss/tok 3.2852 (3.1673)	LR 1.563e-05
0: TRAIN [2][1410/5173]	Time 0.691 (0.607)	Data 1.25e-04 (5.20e-04)	Tok/s 33573 (23337)	Loss/tok 3.3123 (3.1675)	LR 1.563e-05
0: TRAIN [2][1420/5173]	Time 0.566 (0.607)	Data 1.30e-04 (5.17e-04)	Tok/s 18597 (23336)	Loss/tok 2.9944 (3.1669)	LR 1.563e-05
0: TRAIN [2][1430/5173]	Time 0.571 (0.607)	Data 1.25e-04 (5.15e-04)	Tok/s 17870 (23340)	Loss/tok 3.0754 (3.1668)	LR 1.563e-05
0: TRAIN [2][1440/5173]	Time 0.691 (0.607)	Data 1.29e-04 (5.12e-04)	Tok/s 33842 (23345)	Loss/tok 3.2524 (3.1664)	LR 1.563e-05
0: TRAIN [2][1450/5173]	Time 0.566 (0.607)	Data 1.27e-04 (5.09e-04)	Tok/s 18173 (23352)	Loss/tok 2.9787 (3.1667)	LR 1.563e-05
0: TRAIN [2][1460/5173]	Time 0.628 (0.607)	Data 1.27e-04 (5.07e-04)	Tok/s 26871 (23323)	Loss/tok 3.1585 (3.1660)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1470/5173]	Time 0.635 (0.607)	Data 1.26e-04 (5.04e-04)	Tok/s 26258 (23299)	Loss/tok 3.1428 (3.1661)	LR 1.563e-05
0: TRAIN [2][1480/5173]	Time 0.569 (0.607)	Data 1.35e-04 (5.02e-04)	Tok/s 18313 (23311)	Loss/tok 2.9694 (3.1667)	LR 1.563e-05
0: TRAIN [2][1490/5173]	Time 0.629 (0.607)	Data 1.26e-04 (5.00e-04)	Tok/s 26456 (23300)	Loss/tok 3.1387 (3.1663)	LR 1.563e-05
0: TRAIN [2][1500/5173]	Time 0.691 (0.607)	Data 1.32e-04 (4.97e-04)	Tok/s 34034 (23314)	Loss/tok 3.3096 (3.1665)	LR 1.563e-05
0: TRAIN [2][1510/5173]	Time 0.690 (0.607)	Data 1.33e-04 (4.95e-04)	Tok/s 34182 (23337)	Loss/tok 3.2874 (3.1668)	LR 1.563e-05
0: TRAIN [2][1520/5173]	Time 0.568 (0.607)	Data 1.19e-04 (4.93e-04)	Tok/s 18291 (23343)	Loss/tok 2.9639 (3.1673)	LR 1.563e-05
0: TRAIN [2][1530/5173]	Time 0.567 (0.607)	Data 1.26e-04 (4.90e-04)	Tok/s 18458 (23322)	Loss/tok 2.9513 (3.1669)	LR 1.563e-05
0: TRAIN [2][1540/5173]	Time 0.554 (0.607)	Data 1.29e-04 (4.88e-04)	Tok/s 18841 (23328)	Loss/tok 2.8078 (3.1671)	LR 1.563e-05
0: TRAIN [2][1550/5173]	Time 0.633 (0.607)	Data 1.27e-04 (4.86e-04)	Tok/s 26516 (23326)	Loss/tok 3.0715 (3.1670)	LR 1.563e-05
0: TRAIN [2][1560/5173]	Time 0.566 (0.607)	Data 1.25e-04 (4.83e-04)	Tok/s 18173 (23330)	Loss/tok 2.8772 (3.1666)	LR 1.563e-05
0: TRAIN [2][1570/5173]	Time 0.562 (0.607)	Data 1.18e-04 (4.81e-04)	Tok/s 18312 (23341)	Loss/tok 3.0198 (3.1674)	LR 1.563e-05
0: TRAIN [2][1580/5173]	Time 0.491 (0.607)	Data 1.33e-04 (4.79e-04)	Tok/s 10808 (23324)	Loss/tok 2.5517 (3.1674)	LR 1.563e-05
0: TRAIN [2][1590/5173]	Time 0.567 (0.607)	Data 1.27e-04 (4.77e-04)	Tok/s 18214 (23353)	Loss/tok 2.9054 (3.1683)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1600/5173]	Time 0.566 (0.607)	Data 1.34e-04 (4.75e-04)	Tok/s 18241 (23372)	Loss/tok 2.9435 (3.1683)	LR 1.563e-05
0: TRAIN [2][1610/5173]	Time 0.566 (0.607)	Data 1.34e-04 (4.73e-04)	Tok/s 18070 (23388)	Loss/tok 3.0135 (3.1686)	LR 1.563e-05
0: TRAIN [2][1620/5173]	Time 0.566 (0.607)	Data 1.33e-04 (4.71e-04)	Tok/s 18473 (23373)	Loss/tok 3.0063 (3.1680)	LR 1.563e-05
0: TRAIN [2][1630/5173]	Time 0.565 (0.607)	Data 3.12e-04 (4.69e-04)	Tok/s 17948 (23360)	Loss/tok 2.9801 (3.1677)	LR 1.563e-05
0: TRAIN [2][1640/5173]	Time 0.629 (0.607)	Data 1.36e-04 (4.67e-04)	Tok/s 27128 (23362)	Loss/tok 3.0771 (3.1676)	LR 1.563e-05
0: TRAIN [2][1650/5173]	Time 0.633 (0.607)	Data 1.30e-04 (4.65e-04)	Tok/s 26549 (23387)	Loss/tok 3.2092 (3.1685)	LR 1.563e-05
0: TRAIN [2][1660/5173]	Time 0.688 (0.607)	Data 1.34e-04 (4.63e-04)	Tok/s 34093 (23394)	Loss/tok 3.3491 (3.1686)	LR 1.563e-05
0: TRAIN [2][1670/5173]	Time 0.628 (0.607)	Data 1.34e-04 (4.62e-04)	Tok/s 26409 (23400)	Loss/tok 3.1673 (3.1691)	LR 1.563e-05
0: TRAIN [2][1680/5173]	Time 0.692 (0.608)	Data 3.07e-04 (4.60e-04)	Tok/s 33713 (23417)	Loss/tok 3.2882 (3.1695)	LR 1.563e-05
0: TRAIN [2][1690/5173]	Time 0.624 (0.608)	Data 2.00e-04 (4.58e-04)	Tok/s 26860 (23438)	Loss/tok 3.3368 (3.1698)	LR 1.563e-05
0: TRAIN [2][1700/5173]	Time 0.570 (0.608)	Data 3.01e-04 (4.56e-04)	Tok/s 18203 (23460)	Loss/tok 3.0227 (3.1704)	LR 1.563e-05
0: TRAIN [2][1710/5173]	Time 0.565 (0.608)	Data 1.31e-04 (4.55e-04)	Tok/s 18217 (23453)	Loss/tok 2.8762 (3.1700)	LR 1.563e-05
0: TRAIN [2][1720/5173]	Time 0.609 (0.608)	Data 1.31e-04 (4.53e-04)	Tok/s 27349 (23444)	Loss/tok 3.2005 (3.1696)	LR 1.563e-05
0: TRAIN [2][1730/5173]	Time 0.566 (0.608)	Data 1.34e-04 (4.51e-04)	Tok/s 18209 (23440)	Loss/tok 2.8585 (3.1695)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1740/5173]	Time 0.485 (0.607)	Data 1.39e-04 (4.49e-04)	Tok/s 10857 (23414)	Loss/tok 2.6378 (3.1696)	LR 1.563e-05
0: TRAIN [2][1750/5173]	Time 0.488 (0.607)	Data 1.27e-04 (4.48e-04)	Tok/s 10540 (23407)	Loss/tok 2.5558 (3.1697)	LR 1.563e-05
0: TRAIN [2][1760/5173]	Time 0.624 (0.607)	Data 1.32e-04 (4.46e-04)	Tok/s 26889 (23399)	Loss/tok 3.1566 (3.1696)	LR 1.563e-05
0: TRAIN [2][1770/5173]	Time 0.565 (0.607)	Data 1.39e-04 (4.44e-04)	Tok/s 18605 (23409)	Loss/tok 3.0251 (3.1698)	LR 1.563e-05
0: TRAIN [2][1780/5173]	Time 0.688 (0.607)	Data 1.27e-04 (4.43e-04)	Tok/s 34206 (23399)	Loss/tok 3.2146 (3.1694)	LR 1.563e-05
0: TRAIN [2][1790/5173]	Time 0.624 (0.607)	Data 1.35e-04 (4.41e-04)	Tok/s 26843 (23398)	Loss/tok 3.1355 (3.1693)	LR 1.563e-05
0: TRAIN [2][1800/5173]	Time 0.628 (0.607)	Data 1.35e-04 (4.39e-04)	Tok/s 26525 (23389)	Loss/tok 3.0809 (3.1687)	LR 1.563e-05
0: TRAIN [2][1810/5173]	Time 0.567 (0.607)	Data 1.29e-04 (4.38e-04)	Tok/s 17975 (23379)	Loss/tok 3.0281 (3.1684)	LR 1.563e-05
0: TRAIN [2][1820/5173]	Time 0.626 (0.607)	Data 1.33e-04 (4.36e-04)	Tok/s 26637 (23385)	Loss/tok 3.2303 (3.1687)	LR 1.563e-05
0: TRAIN [2][1830/5173]	Time 0.627 (0.607)	Data 1.40e-04 (4.35e-04)	Tok/s 26814 (23387)	Loss/tok 3.0738 (3.1687)	LR 1.563e-05
0: TRAIN [2][1840/5173]	Time 0.567 (0.607)	Data 1.28e-04 (4.33e-04)	Tok/s 18367 (23390)	Loss/tok 3.0141 (3.1687)	LR 7.813e-06
0: TRAIN [2][1850/5173]	Time 0.765 (0.607)	Data 1.34e-04 (4.31e-04)	Tok/s 39079 (23404)	Loss/tok 3.5732 (3.1691)	LR 7.813e-06
0: TRAIN [2][1860/5173]	Time 0.567 (0.607)	Data 1.28e-04 (4.30e-04)	Tok/s 18238 (23397)	Loss/tok 2.9590 (3.1689)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1870/5173]	Time 0.566 (0.607)	Data 1.25e-04 (4.28e-04)	Tok/s 18466 (23396)	Loss/tok 2.9612 (3.1686)	LR 7.813e-06
0: TRAIN [2][1880/5173]	Time 0.568 (0.607)	Data 1.26e-04 (4.27e-04)	Tok/s 18127 (23394)	Loss/tok 3.0738 (3.1685)	LR 7.813e-06
0: TRAIN [2][1890/5173]	Time 0.637 (0.607)	Data 1.31e-04 (4.25e-04)	Tok/s 26483 (23399)	Loss/tok 3.0699 (3.1684)	LR 7.813e-06
0: TRAIN [2][1900/5173]	Time 0.766 (0.607)	Data 1.28e-04 (4.24e-04)	Tok/s 39018 (23404)	Loss/tok 3.4736 (3.1687)	LR 7.813e-06
0: TRAIN [2][1910/5173]	Time 0.508 (0.607)	Data 1.26e-04 (4.22e-04)	Tok/s 10287 (23385)	Loss/tok 2.6420 (3.1685)	LR 7.813e-06
0: TRAIN [2][1920/5173]	Time 0.506 (0.607)	Data 1.27e-04 (4.21e-04)	Tok/s 10429 (23386)	Loss/tok 2.5000 (3.1685)	LR 7.813e-06
0: TRAIN [2][1930/5173]	Time 0.568 (0.607)	Data 1.24e-04 (4.19e-04)	Tok/s 17926 (23368)	Loss/tok 2.9562 (3.1680)	LR 7.813e-06
0: TRAIN [2][1940/5173]	Time 0.567 (0.607)	Data 1.26e-04 (4.18e-04)	Tok/s 18367 (23371)	Loss/tok 2.9453 (3.1680)	LR 7.813e-06
0: TRAIN [2][1950/5173]	Time 0.567 (0.607)	Data 1.43e-04 (4.17e-04)	Tok/s 18743 (23381)	Loss/tok 3.0807 (3.1684)	LR 7.813e-06
0: TRAIN [2][1960/5173]	Time 0.633 (0.607)	Data 1.48e-04 (4.15e-04)	Tok/s 26441 (23393)	Loss/tok 3.2288 (3.1688)	LR 7.813e-06
0: TRAIN [2][1970/5173]	Time 0.628 (0.607)	Data 1.24e-04 (4.14e-04)	Tok/s 26948 (23387)	Loss/tok 3.0801 (3.1686)	LR 7.813e-06
0: TRAIN [2][1980/5173]	Time 0.478 (0.607)	Data 1.35e-04 (4.12e-04)	Tok/s 11111 (23391)	Loss/tok 2.5065 (3.1687)	LR 7.813e-06
0: TRAIN [2][1990/5173]	Time 0.567 (0.607)	Data 1.28e-04 (4.11e-04)	Tok/s 18468 (23366)	Loss/tok 2.9971 (3.1681)	LR 7.813e-06
0: TRAIN [2][2000/5173]	Time 0.631 (0.607)	Data 1.36e-04 (4.10e-04)	Tok/s 26384 (23370)	Loss/tok 3.2293 (3.1681)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][2010/5173]	Time 0.691 (0.607)	Data 1.35e-04 (4.08e-04)	Tok/s 34326 (23383)	Loss/tok 3.1616 (3.1683)	LR 7.813e-06
0: TRAIN [2][2020/5173]	Time 0.623 (0.607)	Data 2.98e-04 (4.07e-04)	Tok/s 26545 (23384)	Loss/tok 3.1879 (3.1685)	LR 7.813e-06
0: TRAIN [2][2030/5173]	Time 0.565 (0.607)	Data 1.25e-04 (4.06e-04)	Tok/s 18171 (23401)	Loss/tok 2.9020 (3.1690)	LR 7.813e-06
0: TRAIN [2][2040/5173]	Time 0.567 (0.607)	Data 1.31e-04 (4.04e-04)	Tok/s 18228 (23404)	Loss/tok 2.9270 (3.1690)	LR 7.813e-06
0: TRAIN [2][2050/5173]	Time 0.693 (0.607)	Data 1.29e-04 (4.03e-04)	Tok/s 33557 (23406)	Loss/tok 3.3524 (3.1690)	LR 7.813e-06
0: TRAIN [2][2060/5173]	Time 0.768 (0.607)	Data 1.24e-04 (4.02e-04)	Tok/s 38450 (23413)	Loss/tok 3.5266 (3.1692)	LR 7.813e-06
0: TRAIN [2][2070/5173]	Time 0.565 (0.607)	Data 1.25e-04 (4.01e-04)	Tok/s 18281 (23400)	Loss/tok 2.9741 (3.1688)	LR 7.813e-06
0: TRAIN [2][2080/5173]	Time 0.567 (0.607)	Data 2.83e-04 (3.99e-04)	Tok/s 18160 (23388)	Loss/tok 2.9943 (3.1684)	LR 7.813e-06
0: TRAIN [2][2090/5173]	Time 0.627 (0.607)	Data 1.67e-04 (3.98e-04)	Tok/s 26890 (23390)	Loss/tok 3.1831 (3.1684)	LR 7.813e-06
0: TRAIN [2][2100/5173]	Time 0.631 (0.607)	Data 1.08e-04 (3.97e-04)	Tok/s 26631 (23399)	Loss/tok 3.1083 (3.1686)	LR 7.813e-06
0: TRAIN [2][2110/5173]	Time 0.693 (0.608)	Data 1.41e-04 (3.96e-04)	Tok/s 34039 (23427)	Loss/tok 3.3482 (3.1695)	LR 7.813e-06
0: TRAIN [2][2120/5173]	Time 0.567 (0.607)	Data 1.30e-04 (3.94e-04)	Tok/s 18704 (23406)	Loss/tok 3.0762 (3.1691)	LR 7.813e-06
0: TRAIN [2][2130/5173]	Time 0.625 (0.608)	Data 1.44e-04 (3.93e-04)	Tok/s 26982 (23428)	Loss/tok 3.1477 (3.1696)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][2140/5173]	Time 0.752 (0.608)	Data 1.25e-04 (3.92e-04)	Tok/s 39601 (23440)	Loss/tok 3.5016 (3.1705)	LR 7.813e-06
0: TRAIN [2][2150/5173]	Time 0.567 (0.608)	Data 1.23e-04 (3.91e-04)	Tok/s 18119 (23455)	Loss/tok 2.9886 (3.1713)	LR 7.813e-06
0: TRAIN [2][2160/5173]	Time 0.568 (0.608)	Data 1.14e-04 (3.90e-04)	Tok/s 18552 (23437)	Loss/tok 3.0505 (3.1710)	LR 7.813e-06
0: TRAIN [2][2170/5173]	Time 0.571 (0.608)	Data 1.16e-04 (3.88e-04)	Tok/s 18361 (23436)	Loss/tok 2.9706 (3.1710)	LR 7.813e-06
0: TRAIN [2][2180/5173]	Time 0.760 (0.608)	Data 1.14e-04 (3.87e-04)	Tok/s 38553 (23450)	Loss/tok 3.6641 (3.1717)	LR 7.813e-06
0: TRAIN [2][2190/5173]	Time 0.770 (0.608)	Data 1.20e-04 (3.86e-04)	Tok/s 38298 (23449)	Loss/tok 3.6020 (3.1719)	LR 7.813e-06
0: TRAIN [2][2200/5173]	Time 0.629 (0.608)	Data 1.12e-04 (3.85e-04)	Tok/s 26457 (23429)	Loss/tok 3.0801 (3.1714)	LR 7.813e-06
0: TRAIN [2][2210/5173]	Time 0.690 (0.608)	Data 1.16e-04 (3.84e-04)	Tok/s 33959 (23434)	Loss/tok 3.2844 (3.1716)	LR 7.813e-06
0: TRAIN [2][2220/5173]	Time 0.564 (0.608)	Data 1.16e-04 (3.83e-04)	Tok/s 18304 (23449)	Loss/tok 3.0193 (3.1719)	LR 7.813e-06
0: TRAIN [2][2230/5173]	Time 0.627 (0.608)	Data 2.68e-04 (3.82e-04)	Tok/s 26872 (23441)	Loss/tok 3.1389 (3.1716)	LR 7.813e-06
0: TRAIN [2][2240/5173]	Time 0.567 (0.608)	Data 1.13e-04 (3.80e-04)	Tok/s 18294 (23436)	Loss/tok 3.0302 (3.1714)	LR 7.813e-06
0: TRAIN [2][2250/5173]	Time 0.567 (0.608)	Data 1.13e-04 (3.79e-04)	Tok/s 17902 (23445)	Loss/tok 2.9608 (3.1714)	LR 7.813e-06
0: TRAIN [2][2260/5173]	Time 0.628 (0.608)	Data 1.13e-04 (3.78e-04)	Tok/s 27038 (23436)	Loss/tok 3.1274 (3.1712)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][2270/5173]	Time 0.565 (0.608)	Data 1.11e-04 (3.77e-04)	Tok/s 18607 (23441)	Loss/tok 3.0260 (3.1713)	LR 7.813e-06
0: TRAIN [2][2280/5173]	Time 0.691 (0.608)	Data 1.16e-04 (3.76e-04)	Tok/s 34031 (23457)	Loss/tok 3.2928 (3.1717)	LR 7.813e-06
0: TRAIN [2][2290/5173]	Time 0.568 (0.608)	Data 1.16e-04 (3.75e-04)	Tok/s 17907 (23452)	Loss/tok 3.0499 (3.1713)	LR 7.813e-06
0: TRAIN [2][2300/5173]	Time 0.631 (0.608)	Data 1.23e-04 (3.74e-04)	Tok/s 26677 (23454)	Loss/tok 3.3253 (3.1718)	LR 7.813e-06
0: TRAIN [2][2310/5173]	Time 0.630 (0.608)	Data 1.13e-04 (3.72e-04)	Tok/s 26893 (23446)	Loss/tok 3.2153 (3.1716)	LR 7.813e-06
0: TRAIN [2][2320/5173]	Time 0.692 (0.608)	Data 1.16e-04 (3.71e-04)	Tok/s 33880 (23437)	Loss/tok 3.2692 (3.1714)	LR 7.813e-06
0: TRAIN [2][2330/5173]	Time 0.694 (0.608)	Data 1.12e-04 (3.70e-04)	Tok/s 33360 (23447)	Loss/tok 3.3918 (3.1713)	LR 7.813e-06
0: TRAIN [2][2340/5173]	Time 0.623 (0.608)	Data 1.13e-04 (3.69e-04)	Tok/s 26956 (23459)	Loss/tok 3.1459 (3.1715)	LR 7.813e-06
0: TRAIN [2][2350/5173]	Time 0.508 (0.608)	Data 1.13e-04 (3.68e-04)	Tok/s 10300 (23456)	Loss/tok 2.5597 (3.1713)	LR 7.813e-06
0: TRAIN [2][2360/5173]	Time 0.505 (0.608)	Data 1.24e-04 (3.67e-04)	Tok/s 10455 (23462)	Loss/tok 2.5286 (3.1718)	LR 7.813e-06
0: TRAIN [2][2370/5173]	Time 0.765 (0.608)	Data 1.14e-04 (3.66e-04)	Tok/s 38985 (23457)	Loss/tok 3.4347 (3.1718)	LR 7.813e-06
0: TRAIN [2][2380/5173]	Time 0.567 (0.608)	Data 1.14e-04 (3.65e-04)	Tok/s 18314 (23452)	Loss/tok 3.0216 (3.1715)	LR 7.813e-06
0: TRAIN [2][2390/5173]	Time 0.624 (0.608)	Data 1.09e-04 (3.64e-04)	Tok/s 26926 (23457)	Loss/tok 3.1455 (3.1715)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][2400/5173]	Time 0.565 (0.608)	Data 1.13e-04 (3.63e-04)	Tok/s 18230 (23443)	Loss/tok 3.0397 (3.1712)	LR 7.813e-06
0: TRAIN [2][2410/5173]	Time 0.569 (0.608)	Data 1.13e-04 (3.62e-04)	Tok/s 18419 (23447)	Loss/tok 2.8480 (3.1714)	LR 7.813e-06
0: TRAIN [2][2420/5173]	Time 0.567 (0.608)	Data 1.08e-04 (3.61e-04)	Tok/s 18059 (23432)	Loss/tok 2.9685 (3.1709)	LR 7.813e-06
0: TRAIN [2][2430/5173]	Time 0.566 (0.608)	Data 1.13e-04 (3.60e-04)	Tok/s 18085 (23434)	Loss/tok 2.9959 (3.1716)	LR 7.813e-06
0: TRAIN [2][2440/5173]	Time 0.559 (0.608)	Data 1.12e-04 (3.59e-04)	Tok/s 18678 (23434)	Loss/tok 2.9177 (3.1716)	LR 7.813e-06
0: TRAIN [2][2450/5173]	Time 0.568 (0.608)	Data 1.15e-04 (3.58e-04)	Tok/s 17937 (23429)	Loss/tok 2.9840 (3.1713)	LR 7.813e-06
0: TRAIN [2][2460/5173]	Time 0.570 (0.608)	Data 1.11e-04 (3.57e-04)	Tok/s 18062 (23434)	Loss/tok 2.9462 (3.1713)	LR 7.813e-06
0: TRAIN [2][2470/5173]	Time 0.690 (0.608)	Data 1.10e-04 (3.56e-04)	Tok/s 33572 (23426)	Loss/tok 3.3391 (3.1710)	LR 7.813e-06
0: TRAIN [2][2480/5173]	Time 0.570 (0.608)	Data 9.23e-05 (3.55e-04)	Tok/s 18551 (23426)	Loss/tok 3.0631 (3.1712)	LR 7.813e-06
0: TRAIN [2][2490/5173]	Time 0.568 (0.608)	Data 1.20e-04 (3.54e-04)	Tok/s 18291 (23416)	Loss/tok 2.9076 (3.1708)	LR 7.813e-06
0: TRAIN [2][2500/5173]	Time 0.568 (0.608)	Data 1.20e-04 (3.53e-04)	Tok/s 18498 (23418)	Loss/tok 3.0240 (3.1706)	LR 7.813e-06
0: TRAIN [2][2510/5173]	Time 0.769 (0.608)	Data 1.16e-04 (3.52e-04)	Tok/s 38985 (23433)	Loss/tok 3.4286 (3.1712)	LR 7.813e-06
0: TRAIN [2][2520/5173]	Time 0.626 (0.608)	Data 1.13e-04 (3.51e-04)	Tok/s 27013 (23439)	Loss/tok 3.1150 (3.1710)	LR 7.813e-06
0: TRAIN [2][2530/5173]	Time 0.632 (0.608)	Data 1.16e-04 (3.50e-04)	Tok/s 26528 (23445)	Loss/tok 3.1582 (3.1708)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][2540/5173]	Time 0.623 (0.608)	Data 1.11e-04 (3.49e-04)	Tok/s 26983 (23436)	Loss/tok 3.2677 (3.1708)	LR 7.813e-06
0: TRAIN [2][2550/5173]	Time 0.492 (0.608)	Data 1.13e-04 (3.49e-04)	Tok/s 10990 (23423)	Loss/tok 2.6397 (3.1705)	LR 7.813e-06
0: TRAIN [2][2560/5173]	Time 0.570 (0.608)	Data 1.17e-04 (3.48e-04)	Tok/s 18087 (23425)	Loss/tok 2.8806 (3.1705)	LR 7.813e-06
0: TRAIN [2][2570/5173]	Time 0.565 (0.608)	Data 1.13e-04 (3.47e-04)	Tok/s 18445 (23417)	Loss/tok 2.9774 (3.1702)	LR 7.813e-06
0: TRAIN [2][2580/5173]	Time 0.568 (0.608)	Data 1.60e-04 (3.46e-04)	Tok/s 18390 (23416)	Loss/tok 2.8774 (3.1701)	LR 7.813e-06
0: TRAIN [2][2590/5173]	Time 0.504 (0.607)	Data 1.09e-04 (3.45e-04)	Tok/s 10545 (23402)	Loss/tok 2.5450 (3.1697)	LR 7.813e-06
0: TRAIN [2][2600/5173]	Time 0.565 (0.607)	Data 1.07e-04 (3.44e-04)	Tok/s 18261 (23401)	Loss/tok 2.9306 (3.1697)	LR 7.813e-06
0: TRAIN [2][2610/5173]	Time 0.570 (0.607)	Data 1.08e-04 (3.44e-04)	Tok/s 17885 (23398)	Loss/tok 2.9511 (3.1693)	LR 7.813e-06
0: TRAIN [2][2620/5173]	Time 0.558 (0.607)	Data 1.14e-04 (3.43e-04)	Tok/s 18350 (23398)	Loss/tok 3.0085 (3.1695)	LR 7.813e-06
0: TRAIN [2][2630/5173]	Time 0.631 (0.607)	Data 1.09e-04 (3.42e-04)	Tok/s 26734 (23401)	Loss/tok 3.2354 (3.1696)	LR 7.813e-06
0: TRAIN [2][2640/5173]	Time 0.570 (0.608)	Data 1.18e-04 (3.41e-04)	Tok/s 18260 (23419)	Loss/tok 2.9688 (3.1701)	LR 7.813e-06
0: TRAIN [2][2650/5173]	Time 0.689 (0.608)	Data 1.11e-04 (3.40e-04)	Tok/s 34150 (23433)	Loss/tok 3.3160 (3.1703)	LR 7.813e-06
0: TRAIN [2][2660/5173]	Time 0.565 (0.608)	Data 1.06e-04 (3.39e-04)	Tok/s 18675 (23429)	Loss/tok 2.9571 (3.1700)	LR 7.813e-06
0: TRAIN [2][2670/5173]	Time 0.631 (0.608)	Data 1.08e-04 (3.38e-04)	Tok/s 26443 (23435)	Loss/tok 3.1914 (3.1700)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][2680/5173]	Time 0.568 (0.608)	Data 1.10e-04 (3.37e-04)	Tok/s 18143 (23435)	Loss/tok 2.9661 (3.1702)	LR 7.813e-06
0: TRAIN [2][2690/5173]	Time 0.628 (0.608)	Data 1.06e-04 (3.37e-04)	Tok/s 26971 (23445)	Loss/tok 3.2392 (3.1705)	LR 7.813e-06
0: TRAIN [2][2700/5173]	Time 0.628 (0.608)	Data 2.82e-04 (3.36e-04)	Tok/s 26777 (23442)	Loss/tok 3.1426 (3.1707)	LR 7.813e-06
0: TRAIN [2][2710/5173]	Time 0.564 (0.608)	Data 1.08e-04 (3.35e-04)	Tok/s 18396 (23433)	Loss/tok 2.9722 (3.1705)	LR 7.813e-06
0: TRAIN [2][2720/5173]	Time 0.568 (0.608)	Data 2.96e-04 (3.34e-04)	Tok/s 18526 (23447)	Loss/tok 2.9962 (3.1709)	LR 7.813e-06
0: TRAIN [2][2730/5173]	Time 0.682 (0.608)	Data 1.08e-04 (3.34e-04)	Tok/s 34085 (23446)	Loss/tok 3.1787 (3.1707)	LR 7.813e-06
0: TRAIN [2][2740/5173]	Time 0.506 (0.608)	Data 1.11e-04 (3.33e-04)	Tok/s 10463 (23452)	Loss/tok 2.5774 (3.1709)	LR 7.813e-06
0: TRAIN [2][2750/5173]	Time 0.629 (0.608)	Data 1.15e-04 (3.32e-04)	Tok/s 26604 (23455)	Loss/tok 3.2070 (3.1712)	LR 7.813e-06
0: TRAIN [2][2760/5173]	Time 0.694 (0.608)	Data 1.13e-04 (3.31e-04)	Tok/s 33633 (23456)	Loss/tok 3.3274 (3.1712)	LR 7.813e-06
0: TRAIN [2][2770/5173]	Time 0.566 (0.608)	Data 1.11e-04 (3.30e-04)	Tok/s 18393 (23458)	Loss/tok 3.0365 (3.1710)	LR 7.813e-06
0: TRAIN [2][2780/5173]	Time 0.568 (0.608)	Data 1.13e-04 (3.30e-04)	Tok/s 18111 (23461)	Loss/tok 2.9428 (3.1713)	LR 7.813e-06
0: TRAIN [2][2790/5173]	Time 0.565 (0.608)	Data 1.13e-04 (3.29e-04)	Tok/s 18120 (23465)	Loss/tok 2.9655 (3.1713)	LR 7.813e-06
0: TRAIN [2][2800/5173]	Time 0.568 (0.608)	Data 1.11e-04 (3.28e-04)	Tok/s 18131 (23461)	Loss/tok 2.9839 (3.1712)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][2810/5173]	Time 0.686 (0.608)	Data 1.10e-04 (3.27e-04)	Tok/s 34028 (23457)	Loss/tok 3.3222 (3.1710)	LR 7.813e-06
0: TRAIN [2][2820/5173]	Time 0.626 (0.608)	Data 1.09e-04 (3.27e-04)	Tok/s 26689 (23462)	Loss/tok 3.1728 (3.1713)	LR 7.813e-06
0: TRAIN [2][2830/5173]	Time 0.756 (0.608)	Data 1.16e-04 (3.26e-04)	Tok/s 39502 (23452)	Loss/tok 3.4519 (3.1711)	LR 7.813e-06
0: TRAIN [2][2840/5173]	Time 0.565 (0.608)	Data 1.16e-04 (3.25e-04)	Tok/s 18701 (23443)	Loss/tok 2.9034 (3.1708)	LR 7.813e-06
0: TRAIN [2][2850/5173]	Time 0.569 (0.608)	Data 1.19e-04 (3.24e-04)	Tok/s 18116 (23439)	Loss/tok 3.0783 (3.1707)	LR 7.813e-06
0: TRAIN [2][2860/5173]	Time 0.566 (0.608)	Data 1.23e-04 (3.24e-04)	Tok/s 18165 (23434)	Loss/tok 2.9506 (3.1705)	LR 7.813e-06
0: TRAIN [2][2870/5173]	Time 0.624 (0.608)	Data 1.22e-04 (3.23e-04)	Tok/s 27071 (23440)	Loss/tok 3.1711 (3.1707)	LR 7.813e-06
0: TRAIN [2][2880/5173]	Time 0.690 (0.608)	Data 1.17e-04 (3.22e-04)	Tok/s 33983 (23447)	Loss/tok 3.2710 (3.1708)	LR 7.813e-06
0: TRAIN [2][2890/5173]	Time 0.567 (0.608)	Data 1.27e-04 (3.22e-04)	Tok/s 18276 (23455)	Loss/tok 2.8687 (3.1712)	LR 7.813e-06
0: TRAIN [2][2900/5173]	Time 0.567 (0.608)	Data 1.25e-04 (3.21e-04)	Tok/s 18217 (23452)	Loss/tok 2.9656 (3.1710)	LR 7.813e-06
0: TRAIN [2][2910/5173]	Time 0.625 (0.608)	Data 1.13e-04 (3.20e-04)	Tok/s 26941 (23451)	Loss/tok 3.1371 (3.1710)	LR 7.813e-06
0: TRAIN [2][2920/5173]	Time 0.564 (0.608)	Data 2.79e-04 (3.20e-04)	Tok/s 18353 (23442)	Loss/tok 3.0334 (3.1708)	LR 7.813e-06
0: TRAIN [2][2930/5173]	Time 0.628 (0.608)	Data 1.17e-04 (3.19e-04)	Tok/s 26598 (23444)	Loss/tok 3.2188 (3.1708)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][2940/5173]	Time 0.568 (0.608)	Data 1.12e-04 (3.18e-04)	Tok/s 18409 (23446)	Loss/tok 3.0101 (3.1709)	LR 7.813e-06
0: TRAIN [2][2950/5173]	Time 0.760 (0.608)	Data 1.27e-04 (3.18e-04)	Tok/s 39112 (23454)	Loss/tok 3.4459 (3.1711)	LR 7.813e-06
0: TRAIN [2][2960/5173]	Time 0.626 (0.608)	Data 1.14e-04 (3.17e-04)	Tok/s 26775 (23453)	Loss/tok 3.2241 (3.1710)	LR 7.813e-06
0: TRAIN [2][2970/5173]	Time 0.569 (0.608)	Data 1.14e-04 (3.16e-04)	Tok/s 17856 (23459)	Loss/tok 3.0814 (3.1712)	LR 7.813e-06
0: TRAIN [2][2980/5173]	Time 0.632 (0.608)	Data 1.18e-04 (3.16e-04)	Tok/s 26386 (23467)	Loss/tok 3.0862 (3.1715)	LR 7.813e-06
0: TRAIN [2][2990/5173]	Time 0.570 (0.608)	Data 1.16e-04 (3.15e-04)	Tok/s 18006 (23455)	Loss/tok 2.9625 (3.1713)	LR 7.813e-06
0: TRAIN [2][3000/5173]	Time 0.625 (0.608)	Data 1.17e-04 (3.15e-04)	Tok/s 26809 (23458)	Loss/tok 3.2133 (3.1715)	LR 7.813e-06
0: TRAIN [2][3010/5173]	Time 0.566 (0.608)	Data 1.19e-04 (3.14e-04)	Tok/s 18414 (23450)	Loss/tok 2.9826 (3.1711)	LR 7.813e-06
0: TRAIN [2][3020/5173]	Time 0.569 (0.608)	Data 1.21e-04 (3.13e-04)	Tok/s 17774 (23449)	Loss/tok 3.0022 (3.1711)	LR 7.813e-06
0: TRAIN [2][3030/5173]	Time 0.690 (0.608)	Data 1.15e-04 (3.13e-04)	Tok/s 33384 (23443)	Loss/tok 3.3144 (3.1709)	LR 7.813e-06
0: TRAIN [2][3040/5173]	Time 0.628 (0.608)	Data 1.12e-04 (3.12e-04)	Tok/s 27333 (23445)	Loss/tok 3.1194 (3.1709)	LR 7.813e-06
0: TRAIN [2][3050/5173]	Time 0.564 (0.608)	Data 1.19e-04 (3.11e-04)	Tok/s 18575 (23446)	Loss/tok 2.9472 (3.1709)	LR 7.813e-06
0: TRAIN [2][3060/5173]	Time 0.563 (0.608)	Data 1.16e-04 (3.11e-04)	Tok/s 18556 (23451)	Loss/tok 2.9516 (3.1711)	LR 7.813e-06
0: TRAIN [2][3070/5173]	Time 0.633 (0.608)	Data 1.25e-04 (3.10e-04)	Tok/s 26519 (23450)	Loss/tok 3.1505 (3.1710)	LR 7.813e-06
0: TRAIN [2][3080/5173]	Time 0.570 (0.608)	Data 1.40e-04 (3.10e-04)	Tok/s 18505 (23444)	Loss/tok 2.9792 (3.1708)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3090/5173]	Time 0.631 (0.608)	Data 1.49e-04 (3.09e-04)	Tok/s 26754 (23441)	Loss/tok 3.1834 (3.1706)	LR 7.813e-06
0: TRAIN [2][3100/5173]	Time 0.493 (0.608)	Data 1.16e-04 (3.09e-04)	Tok/s 10553 (23425)	Loss/tok 2.5998 (3.1702)	LR 7.813e-06
0: TRAIN [2][3110/5173]	Time 0.762 (0.608)	Data 1.14e-04 (3.08e-04)	Tok/s 39195 (23420)	Loss/tok 3.5081 (3.1702)	LR 7.813e-06
0: TRAIN [2][3120/5173]	Time 0.566 (0.608)	Data 1.14e-04 (3.07e-04)	Tok/s 17964 (23418)	Loss/tok 3.0210 (3.1703)	LR 7.813e-06
0: TRAIN [2][3130/5173]	Time 0.506 (0.608)	Data 1.19e-04 (3.07e-04)	Tok/s 10608 (23414)	Loss/tok 2.6020 (3.1702)	LR 7.813e-06
0: TRAIN [2][3140/5173]	Time 0.630 (0.607)	Data 1.12e-04 (3.06e-04)	Tok/s 26549 (23401)	Loss/tok 3.1224 (3.1699)	LR 7.813e-06
0: TRAIN [2][3150/5173]	Time 0.569 (0.607)	Data 1.16e-04 (3.05e-04)	Tok/s 17817 (23400)	Loss/tok 2.9967 (3.1699)	LR 7.813e-06
0: TRAIN [2][3160/5173]	Time 0.560 (0.607)	Data 1.13e-04 (3.05e-04)	Tok/s 18429 (23389)	Loss/tok 3.0630 (3.1696)	LR 7.813e-06
0: TRAIN [2][3170/5173]	Time 0.566 (0.607)	Data 1.18e-04 (3.04e-04)	Tok/s 18454 (23392)	Loss/tok 2.9816 (3.1697)	LR 7.813e-06
0: TRAIN [2][3180/5173]	Time 0.569 (0.607)	Data 1.31e-04 (3.04e-04)	Tok/s 17819 (23393)	Loss/tok 2.9887 (3.1697)	LR 7.813e-06
0: TRAIN [2][3190/5173]	Time 0.567 (0.607)	Data 1.19e-04 (3.03e-04)	Tok/s 18283 (23386)	Loss/tok 3.0492 (3.1696)	LR 7.813e-06
0: TRAIN [2][3200/5173]	Time 0.690 (0.607)	Data 1.28e-04 (3.03e-04)	Tok/s 33934 (23391)	Loss/tok 3.3342 (3.1697)	LR 7.813e-06
0: TRAIN [2][3210/5173]	Time 0.564 (0.607)	Data 1.44e-04 (3.02e-04)	Tok/s 18531 (23388)	Loss/tok 2.9219 (3.1696)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3220/5173]	Time 0.628 (0.607)	Data 1.49e-04 (3.01e-04)	Tok/s 27039 (23393)	Loss/tok 3.1476 (3.1696)	LR 7.813e-06
0: TRAIN [2][3230/5173]	Time 0.623 (0.607)	Data 1.32e-04 (3.01e-04)	Tok/s 26611 (23392)	Loss/tok 3.1008 (3.1694)	LR 7.813e-06
0: TRAIN [2][3240/5173]	Time 0.635 (0.607)	Data 1.24e-04 (3.00e-04)	Tok/s 26628 (23391)	Loss/tok 3.2188 (3.1695)	LR 7.813e-06
0: TRAIN [2][3250/5173]	Time 0.488 (0.607)	Data 1.25e-04 (3.00e-04)	Tok/s 10906 (23380)	Loss/tok 2.5020 (3.1692)	LR 7.813e-06
0: TRAIN [2][3260/5173]	Time 0.691 (0.607)	Data 1.20e-04 (2.99e-04)	Tok/s 33385 (23385)	Loss/tok 3.4449 (3.1695)	LR 7.813e-06
0: TRAIN [2][3270/5173]	Time 0.566 (0.607)	Data 1.25e-04 (2.99e-04)	Tok/s 17969 (23388)	Loss/tok 3.0262 (3.1696)	LR 7.813e-06
0: TRAIN [2][3280/5173]	Time 0.630 (0.607)	Data 1.28e-04 (2.98e-04)	Tok/s 27151 (23387)	Loss/tok 3.1563 (3.1694)	LR 7.813e-06
0: TRAIN [2][3290/5173]	Time 0.627 (0.607)	Data 1.23e-04 (2.98e-04)	Tok/s 27009 (23395)	Loss/tok 3.1775 (3.1696)	LR 7.813e-06
0: TRAIN [2][3300/5173]	Time 0.688 (0.607)	Data 1.21e-04 (2.97e-04)	Tok/s 33749 (23404)	Loss/tok 3.3144 (3.1697)	LR 7.813e-06
0: TRAIN [2][3310/5173]	Time 0.692 (0.607)	Data 1.31e-04 (2.97e-04)	Tok/s 34030 (23414)	Loss/tok 3.3438 (3.1701)	LR 7.813e-06
0: TRAIN [2][3320/5173]	Time 0.633 (0.607)	Data 1.16e-04 (2.96e-04)	Tok/s 26434 (23413)	Loss/tok 3.1738 (3.1700)	LR 7.813e-06
0: TRAIN [2][3330/5173]	Time 0.566 (0.607)	Data 1.22e-04 (2.95e-04)	Tok/s 18578 (23413)	Loss/tok 2.8625 (3.1700)	LR 7.813e-06
0: TRAIN [2][3340/5173]	Time 0.693 (0.608)	Data 1.20e-04 (2.95e-04)	Tok/s 33548 (23416)	Loss/tok 3.2772 (3.1701)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3350/5173]	Time 0.567 (0.607)	Data 1.32e-04 (2.94e-04)	Tok/s 17771 (23413)	Loss/tok 2.9153 (3.1700)	LR 7.813e-06
0: TRAIN [2][3360/5173]	Time 0.626 (0.607)	Data 1.18e-04 (2.94e-04)	Tok/s 26836 (23413)	Loss/tok 3.2275 (3.1699)	LR 7.813e-06
0: TRAIN [2][3370/5173]	Time 0.568 (0.608)	Data 1.20e-04 (2.93e-04)	Tok/s 18384 (23423)	Loss/tok 2.9622 (3.1702)	LR 7.813e-06
0: TRAIN [2][3380/5173]	Time 0.690 (0.607)	Data 1.25e-04 (2.93e-04)	Tok/s 34165 (23417)	Loss/tok 3.4017 (3.1700)	LR 7.813e-06
0: TRAIN [2][3390/5173]	Time 0.566 (0.608)	Data 1.17e-04 (2.92e-04)	Tok/s 18421 (23417)	Loss/tok 2.9518 (3.1701)	LR 7.813e-06
0: TRAIN [2][3400/5173]	Time 0.626 (0.608)	Data 1.22e-04 (2.92e-04)	Tok/s 27014 (23416)	Loss/tok 3.2381 (3.1701)	LR 7.813e-06
0: TRAIN [2][3410/5173]	Time 0.569 (0.608)	Data 1.26e-04 (2.91e-04)	Tok/s 18152 (23422)	Loss/tok 2.8818 (3.1702)	LR 7.813e-06
0: TRAIN [2][3420/5173]	Time 0.566 (0.607)	Data 1.16e-04 (2.91e-04)	Tok/s 18039 (23416)	Loss/tok 2.9792 (3.1700)	LR 7.813e-06
0: TRAIN [2][3430/5173]	Time 0.567 (0.607)	Data 1.20e-04 (2.90e-04)	Tok/s 18358 (23413)	Loss/tok 2.9488 (3.1699)	LR 7.813e-06
0: TRAIN [2][3440/5173]	Time 0.625 (0.607)	Data 1.23e-04 (2.90e-04)	Tok/s 26997 (23412)	Loss/tok 3.2376 (3.1698)	LR 7.813e-06
0: TRAIN [2][3450/5173]	Time 0.560 (0.607)	Data 1.20e-04 (2.90e-04)	Tok/s 18834 (23411)	Loss/tok 2.9208 (3.1696)	LR 7.813e-06
0: TRAIN [2][3460/5173]	Time 0.693 (0.607)	Data 1.20e-04 (2.89e-04)	Tok/s 33921 (23405)	Loss/tok 3.3921 (3.1695)	LR 7.813e-06
0: TRAIN [2][3470/5173]	Time 0.565 (0.607)	Data 1.26e-04 (2.89e-04)	Tok/s 18132 (23400)	Loss/tok 2.9140 (3.1692)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3480/5173]	Time 0.765 (0.607)	Data 1.18e-04 (2.88e-04)	Tok/s 38850 (23403)	Loss/tok 3.5899 (3.1697)	LR 7.813e-06
0: TRAIN [2][3490/5173]	Time 0.762 (0.607)	Data 1.24e-04 (2.88e-04)	Tok/s 39679 (23410)	Loss/tok 3.5365 (3.1700)	LR 7.813e-06
0: TRAIN [2][3500/5173]	Time 0.489 (0.607)	Data 1.30e-04 (2.87e-04)	Tok/s 10624 (23393)	Loss/tok 2.5280 (3.1696)	LR 7.813e-06
0: TRAIN [2][3510/5173]	Time 0.694 (0.607)	Data 1.24e-04 (2.87e-04)	Tok/s 33287 (23399)	Loss/tok 3.2806 (3.1697)	LR 7.813e-06
0: TRAIN [2][3520/5173]	Time 0.502 (0.607)	Data 1.26e-04 (2.87e-04)	Tok/s 10500 (23407)	Loss/tok 2.6673 (3.1699)	LR 7.813e-06
0: TRAIN [2][3530/5173]	Time 0.690 (0.607)	Data 1.17e-04 (2.86e-04)	Tok/s 33556 (23415)	Loss/tok 3.3580 (3.1699)	LR 7.813e-06
0: TRAIN [2][3540/5173]	Time 0.571 (0.608)	Data 1.20e-04 (2.86e-04)	Tok/s 18564 (23417)	Loss/tok 2.9947 (3.1700)	LR 7.813e-06
0: TRAIN [2][3550/5173]	Time 0.690 (0.607)	Data 1.18e-04 (2.85e-04)	Tok/s 33711 (23411)	Loss/tok 3.2825 (3.1699)	LR 7.813e-06
0: TRAIN [2][3560/5173]	Time 0.621 (0.607)	Data 1.23e-04 (2.85e-04)	Tok/s 27128 (23395)	Loss/tok 3.2092 (3.1696)	LR 7.813e-06
0: TRAIN [2][3570/5173]	Time 0.628 (0.607)	Data 1.20e-04 (2.84e-04)	Tok/s 26613 (23405)	Loss/tok 3.1947 (3.1697)	LR 7.813e-06
0: TRAIN [2][3580/5173]	Time 0.506 (0.607)	Data 1.28e-04 (2.84e-04)	Tok/s 10259 (23406)	Loss/tok 2.5647 (3.1698)	LR 7.813e-06
0: TRAIN [2][3590/5173]	Time 0.566 (0.607)	Data 1.16e-04 (2.84e-04)	Tok/s 18275 (23397)	Loss/tok 2.9638 (3.1696)	LR 7.813e-06
0: TRAIN [2][3600/5173]	Time 0.693 (0.607)	Data 1.24e-04 (2.83e-04)	Tok/s 33814 (23402)	Loss/tok 3.3875 (3.1698)	LR 7.813e-06
0: TRAIN [2][3610/5173]	Time 0.568 (0.607)	Data 1.18e-04 (2.83e-04)	Tok/s 17887 (23403)	Loss/tok 2.9857 (3.1697)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3620/5173]	Time 0.628 (0.607)	Data 1.20e-04 (2.82e-04)	Tok/s 26811 (23409)	Loss/tok 3.0831 (3.1697)	LR 7.813e-06
0: TRAIN [2][3630/5173]	Time 0.689 (0.608)	Data 1.21e-04 (2.82e-04)	Tok/s 33829 (23420)	Loss/tok 3.3849 (3.1701)	LR 7.813e-06
0: TRAIN [2][3640/5173]	Time 0.569 (0.607)	Data 1.22e-04 (2.82e-04)	Tok/s 18052 (23409)	Loss/tok 2.9765 (3.1699)	LR 7.813e-06
0: TRAIN [2][3650/5173]	Time 0.569 (0.607)	Data 1.20e-04 (2.81e-04)	Tok/s 18418 (23396)	Loss/tok 2.8929 (3.1695)	LR 7.813e-06
0: TRAIN [2][3660/5173]	Time 0.568 (0.607)	Data 1.23e-04 (2.81e-04)	Tok/s 18392 (23399)	Loss/tok 2.9913 (3.1695)	LR 7.813e-06
0: TRAIN [2][3670/5173]	Time 0.567 (0.607)	Data 1.17e-04 (2.80e-04)	Tok/s 17956 (23402)	Loss/tok 2.8964 (3.1696)	LR 7.813e-06
0: TRAIN [2][3680/5173]	Time 0.570 (0.607)	Data 1.26e-04 (2.80e-04)	Tok/s 18229 (23410)	Loss/tok 2.9320 (3.1698)	LR 7.813e-06
0: TRAIN [2][3690/5173]	Time 0.569 (0.607)	Data 1.19e-04 (2.79e-04)	Tok/s 18498 (23407)	Loss/tok 2.9176 (3.1697)	LR 7.813e-06
0: TRAIN [2][3700/5173]	Time 0.632 (0.607)	Data 1.27e-04 (2.79e-04)	Tok/s 26467 (23403)	Loss/tok 3.0539 (3.1696)	LR 7.813e-06
0: TRAIN [2][3710/5173]	Time 0.564 (0.607)	Data 1.11e-04 (2.79e-04)	Tok/s 18907 (23404)	Loss/tok 2.9570 (3.1695)	LR 7.813e-06
0: TRAIN [2][3720/5173]	Time 0.507 (0.607)	Data 1.18e-04 (2.78e-04)	Tok/s 10225 (23402)	Loss/tok 2.5892 (3.1693)	LR 7.813e-06
0: TRAIN [2][3730/5173]	Time 0.626 (0.607)	Data 1.20e-04 (2.78e-04)	Tok/s 26715 (23405)	Loss/tok 3.1959 (3.1694)	LR 7.813e-06
0: TRAIN [2][3740/5173]	Time 0.568 (0.607)	Data 1.23e-04 (2.78e-04)	Tok/s 18156 (23403)	Loss/tok 2.9705 (3.1695)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3750/5173]	Time 0.567 (0.607)	Data 1.22e-04 (2.77e-04)	Tok/s 18391 (23404)	Loss/tok 2.9437 (3.1695)	LR 7.813e-06
0: TRAIN [2][3760/5173]	Time 0.692 (0.607)	Data 1.20e-04 (2.77e-04)	Tok/s 33359 (23407)	Loss/tok 3.3974 (3.1697)	LR 7.813e-06
0: TRAIN [2][3770/5173]	Time 0.497 (0.607)	Data 1.23e-04 (2.76e-04)	Tok/s 10578 (23407)	Loss/tok 2.5929 (3.1698)	LR 7.813e-06
0: TRAIN [2][3780/5173]	Time 0.507 (0.607)	Data 1.20e-04 (2.76e-04)	Tok/s 10061 (23401)	Loss/tok 2.4447 (3.1697)	LR 7.813e-06
0: TRAIN [2][3790/5173]	Time 0.624 (0.607)	Data 1.22e-04 (2.76e-04)	Tok/s 26695 (23401)	Loss/tok 3.2657 (3.1697)	LR 7.813e-06
0: TRAIN [2][3800/5173]	Time 0.570 (0.607)	Data 2.68e-04 (2.75e-04)	Tok/s 18106 (23406)	Loss/tok 2.9873 (3.1697)	LR 7.813e-06
0: TRAIN [2][3810/5173]	Time 0.695 (0.607)	Data 1.20e-04 (2.75e-04)	Tok/s 33164 (23407)	Loss/tok 3.4213 (3.1697)	LR 7.813e-06
0: TRAIN [2][3820/5173]	Time 0.630 (0.607)	Data 1.19e-04 (2.74e-04)	Tok/s 26852 (23402)	Loss/tok 3.1286 (3.1696)	LR 7.813e-06
0: TRAIN [2][3830/5173]	Time 0.565 (0.607)	Data 1.16e-04 (2.74e-04)	Tok/s 18372 (23400)	Loss/tok 2.9123 (3.1696)	LR 7.813e-06
0: TRAIN [2][3840/5173]	Time 0.567 (0.607)	Data 1.17e-04 (2.74e-04)	Tok/s 18488 (23395)	Loss/tok 2.9235 (3.1694)	LR 7.813e-06
0: TRAIN [2][3850/5173]	Time 0.768 (0.607)	Data 1.27e-04 (2.73e-04)	Tok/s 38799 (23399)	Loss/tok 3.4644 (3.1697)	LR 7.813e-06
0: TRAIN [2][3860/5173]	Time 0.569 (0.607)	Data 1.20e-04 (2.73e-04)	Tok/s 18130 (23402)	Loss/tok 3.0253 (3.1698)	LR 7.813e-06
0: TRAIN [2][3870/5173]	Time 0.569 (0.607)	Data 1.23e-04 (2.73e-04)	Tok/s 17797 (23405)	Loss/tok 2.9009 (3.1697)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3880/5173]	Time 0.761 (0.607)	Data 1.20e-04 (2.72e-04)	Tok/s 39407 (23407)	Loss/tok 3.4289 (3.1699)	LR 7.813e-06
0: TRAIN [2][3890/5173]	Time 0.630 (0.607)	Data 1.21e-04 (2.72e-04)	Tok/s 26724 (23405)	Loss/tok 3.1555 (3.1698)	LR 7.813e-06
0: TRAIN [2][3900/5173]	Time 0.505 (0.607)	Data 1.19e-04 (2.71e-04)	Tok/s 10302 (23396)	Loss/tok 2.5846 (3.1696)	LR 7.813e-06
0: TRAIN [2][3910/5173]	Time 0.631 (0.607)	Data 1.21e-04 (2.71e-04)	Tok/s 26582 (23385)	Loss/tok 3.1405 (3.1693)	LR 7.813e-06
0: TRAIN [2][3920/5173]	Time 0.568 (0.607)	Data 1.28e-04 (2.71e-04)	Tok/s 18403 (23385)	Loss/tok 2.8757 (3.1694)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3930/5173]	Time 0.633 (0.607)	Data 1.21e-04 (2.70e-04)	Tok/s 26507 (23383)	Loss/tok 3.1786 (3.1694)	LR 7.813e-06
0: TRAIN [2][3940/5173]	Time 0.569 (0.607)	Data 1.20e-04 (2.70e-04)	Tok/s 18122 (23374)	Loss/tok 3.0199 (3.1692)	LR 7.813e-06
0: TRAIN [2][3950/5173]	Time 0.568 (0.607)	Data 1.19e-04 (2.70e-04)	Tok/s 18409 (23369)	Loss/tok 2.9804 (3.1689)	LR 7.813e-06
0: TRAIN [2][3960/5173]	Time 0.624 (0.607)	Data 1.21e-04 (2.69e-04)	Tok/s 27079 (23353)	Loss/tok 3.2188 (3.1686)	LR 7.813e-06
0: TRAIN [2][3970/5173]	Time 0.569 (0.607)	Data 1.21e-04 (2.69e-04)	Tok/s 18268 (23348)	Loss/tok 2.9656 (3.1685)	LR 7.813e-06
0: TRAIN [2][3980/5173]	Time 0.628 (0.607)	Data 1.23e-04 (2.68e-04)	Tok/s 26823 (23341)	Loss/tok 3.1116 (3.1684)	LR 7.813e-06
0: TRAIN [2][3990/5173]	Time 0.630 (0.607)	Data 1.15e-04 (2.68e-04)	Tok/s 26791 (23345)	Loss/tok 3.1516 (3.1685)	LR 7.813e-06
0: TRAIN [2][4000/5173]	Time 0.568 (0.607)	Data 1.17e-04 (2.68e-04)	Tok/s 17946 (23341)	Loss/tok 2.9029 (3.1685)	LR 7.813e-06
0: TRAIN [2][4010/5173]	Time 0.568 (0.607)	Data 1.26e-04 (2.67e-04)	Tok/s 18051 (23352)	Loss/tok 3.0043 (3.1686)	LR 7.813e-06
0: TRAIN [2][4020/5173]	Time 0.567 (0.607)	Data 1.34e-04 (2.67e-04)	Tok/s 17915 (23356)	Loss/tok 3.0982 (3.1688)	LR 7.813e-06
0: TRAIN [2][4030/5173]	Time 0.625 (0.607)	Data 1.18e-04 (2.67e-04)	Tok/s 26954 (23360)	Loss/tok 3.1429 (3.1689)	LR 7.813e-06
0: TRAIN [2][4040/5173]	Time 0.564 (0.607)	Data 3.09e-04 (2.66e-04)	Tok/s 18508 (23352)	Loss/tok 3.0536 (3.1686)	LR 7.813e-06
0: TRAIN [2][4050/5173]	Time 0.566 (0.607)	Data 1.17e-04 (2.66e-04)	Tok/s 17981 (23337)	Loss/tok 3.0412 (3.1683)	LR 7.813e-06
0: TRAIN [2][4060/5173]	Time 0.691 (0.607)	Data 2.66e-04 (2.66e-04)	Tok/s 33623 (23337)	Loss/tok 3.2968 (3.1682)	LR 7.813e-06
0: TRAIN [2][4070/5173]	Time 0.568 (0.607)	Data 1.27e-04 (2.65e-04)	Tok/s 18481 (23331)	Loss/tok 2.8499 (3.1680)	LR 7.813e-06
0: TRAIN [2][4080/5173]	Time 0.565 (0.607)	Data 1.30e-04 (2.65e-04)	Tok/s 18470 (23331)	Loss/tok 2.9644 (3.1679)	LR 7.813e-06
0: TRAIN [2][4090/5173]	Time 0.567 (0.607)	Data 1.16e-04 (2.65e-04)	Tok/s 18235 (23334)	Loss/tok 2.9920 (3.1679)	LR 7.813e-06
0: TRAIN [2][4100/5173]	Time 0.565 (0.607)	Data 1.34e-04 (2.64e-04)	Tok/s 18854 (23331)	Loss/tok 2.9808 (3.1678)	LR 7.813e-06
0: TRAIN [2][4110/5173]	Time 0.566 (0.607)	Data 1.21e-04 (2.64e-04)	Tok/s 18383 (23329)	Loss/tok 3.0544 (3.1679)	LR 7.813e-06
0: TRAIN [2][4120/5173]	Time 0.630 (0.607)	Data 1.22e-04 (2.64e-04)	Tok/s 26551 (23335)	Loss/tok 3.0498 (3.1678)	LR 7.813e-06
0: TRAIN [2][4130/5173]	Time 0.625 (0.607)	Data 1.24e-04 (2.63e-04)	Tok/s 26739 (23331)	Loss/tok 3.1300 (3.1677)	LR 7.813e-06
0: TRAIN [2][4140/5173]	Time 0.506 (0.607)	Data 1.32e-04 (2.63e-04)	Tok/s 10552 (23327)	Loss/tok 2.6421 (3.1675)	LR 7.813e-06
0: TRAIN [2][4150/5173]	Time 0.628 (0.607)	Data 1.22e-04 (2.63e-04)	Tok/s 26642 (23329)	Loss/tok 3.1890 (3.1675)	LR 7.813e-06
0: TRAIN [2][4160/5173]	Time 0.568 (0.607)	Data 1.23e-04 (2.63e-04)	Tok/s 18081 (23331)	Loss/tok 2.9197 (3.1678)	LR 7.813e-06
0: TRAIN [2][4170/5173]	Time 0.621 (0.607)	Data 1.26e-04 (2.62e-04)	Tok/s 27043 (23331)	Loss/tok 3.1119 (3.1676)	LR 7.813e-06
0: TRAIN [2][4180/5173]	Time 0.568 (0.607)	Data 1.24e-04 (2.62e-04)	Tok/s 18175 (23330)	Loss/tok 2.9146 (3.1676)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][4190/5173]	Time 0.567 (0.607)	Data 1.28e-04 (2.62e-04)	Tok/s 18146 (23323)	Loss/tok 3.0236 (3.1674)	LR 7.813e-06
0: TRAIN [2][4200/5173]	Time 0.566 (0.607)	Data 1.29e-04 (2.61e-04)	Tok/s 18290 (23323)	Loss/tok 3.0450 (3.1673)	LR 7.813e-06
0: TRAIN [2][4210/5173]	Time 0.570 (0.607)	Data 1.21e-04 (2.61e-04)	Tok/s 18339 (23325)	Loss/tok 3.0504 (3.1672)	LR 7.813e-06
0: TRAIN [2][4220/5173]	Time 0.566 (0.607)	Data 1.18e-04 (2.61e-04)	Tok/s 18038 (23329)	Loss/tok 2.9614 (3.1673)	LR 7.813e-06
0: TRAIN [2][4230/5173]	Time 0.569 (0.607)	Data 1.25e-04 (2.60e-04)	Tok/s 18270 (23325)	Loss/tok 3.0907 (3.1672)	LR 7.813e-06
0: TRAIN [2][4240/5173]	Time 0.627 (0.607)	Data 1.23e-04 (2.60e-04)	Tok/s 26885 (23335)	Loss/tok 3.1326 (3.1675)	LR 7.813e-06
0: TRAIN [2][4250/5173]	Time 0.566 (0.607)	Data 1.22e-04 (2.60e-04)	Tok/s 17956 (23340)	Loss/tok 2.9553 (3.1675)	LR 7.813e-06
0: TRAIN [2][4260/5173]	Time 0.570 (0.607)	Data 1.32e-04 (2.59e-04)	Tok/s 18135 (23320)	Loss/tok 2.9841 (3.1672)	LR 7.813e-06
0: TRAIN [2][4270/5173]	Time 0.693 (0.607)	Data 1.27e-04 (2.59e-04)	Tok/s 33698 (23329)	Loss/tok 3.3048 (3.1672)	LR 7.813e-06
0: TRAIN [2][4280/5173]	Time 0.569 (0.607)	Data 1.19e-04 (2.59e-04)	Tok/s 18159 (23324)	Loss/tok 2.8802 (3.1671)	LR 7.813e-06
0: TRAIN [2][4290/5173]	Time 0.568 (0.607)	Data 1.27e-04 (2.58e-04)	Tok/s 17957 (23312)	Loss/tok 3.0091 (3.1668)	LR 7.813e-06
0: TRAIN [2][4300/5173]	Time 0.566 (0.607)	Data 1.26e-04 (2.58e-04)	Tok/s 18322 (23310)	Loss/tok 2.9200 (3.1667)	LR 7.813e-06
0: TRAIN [2][4310/5173]	Time 0.566 (0.606)	Data 1.17e-04 (2.58e-04)	Tok/s 18035 (23302)	Loss/tok 2.9765 (3.1665)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][4320/5173]	Time 0.628 (0.606)	Data 1.28e-04 (2.58e-04)	Tok/s 26690 (23303)	Loss/tok 3.1836 (3.1665)	LR 7.813e-06
0: TRAIN [2][4330/5173]	Time 0.568 (0.606)	Data 1.25e-04 (2.57e-04)	Tok/s 18459 (23307)	Loss/tok 2.9926 (3.1668)	LR 7.813e-06
0: TRAIN [2][4340/5173]	Time 0.572 (0.606)	Data 1.23e-04 (2.57e-04)	Tok/s 18077 (23298)	Loss/tok 3.0235 (3.1666)	LR 7.813e-06
0: TRAIN [2][4350/5173]	Time 0.567 (0.606)	Data 1.29e-04 (2.57e-04)	Tok/s 18186 (23297)	Loss/tok 2.9844 (3.1665)	LR 7.813e-06
0: TRAIN [2][4360/5173]	Time 0.490 (0.606)	Data 1.18e-04 (2.56e-04)	Tok/s 10641 (23297)	Loss/tok 2.5961 (3.1665)	LR 7.813e-06
0: TRAIN [2][4370/5173]	Time 0.623 (0.606)	Data 3.05e-04 (2.56e-04)	Tok/s 26890 (23301)	Loss/tok 3.2191 (3.1665)	LR 7.813e-06
0: TRAIN [2][4380/5173]	Time 0.567 (0.606)	Data 1.33e-04 (2.56e-04)	Tok/s 18092 (23306)	Loss/tok 2.9416 (3.1667)	LR 7.813e-06
0: TRAIN [2][4390/5173]	Time 0.628 (0.606)	Data 1.25e-04 (2.56e-04)	Tok/s 26798 (23306)	Loss/tok 3.1043 (3.1667)	LR 7.813e-06
0: TRAIN [2][4400/5173]	Time 0.689 (0.606)	Data 1.21e-04 (2.55e-04)	Tok/s 33717 (23307)	Loss/tok 3.4266 (3.1668)	LR 7.813e-06
0: TRAIN [2][4410/5173]	Time 0.759 (0.606)	Data 1.21e-04 (2.55e-04)	Tok/s 39671 (23299)	Loss/tok 3.5508 (3.1667)	LR 7.813e-06
0: TRAIN [2][4420/5173]	Time 0.752 (0.606)	Data 1.12e-04 (2.55e-04)	Tok/s 39793 (23300)	Loss/tok 3.4606 (3.1668)	LR 7.813e-06
0: TRAIN [2][4430/5173]	Time 0.567 (0.606)	Data 1.25e-04 (2.55e-04)	Tok/s 17760 (23309)	Loss/tok 2.9473 (3.1670)	LR 7.813e-06
0: TRAIN [2][4440/5173]	Time 0.630 (0.606)	Data 1.18e-04 (2.54e-04)	Tok/s 26798 (23301)	Loss/tok 3.2133 (3.1667)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][4450/5173]	Time 0.507 (0.606)	Data 1.20e-04 (2.54e-04)	Tok/s 10412 (23302)	Loss/tok 2.6026 (3.1668)	LR 7.813e-06
0: TRAIN [2][4460/5173]	Time 0.570 (0.606)	Data 1.29e-04 (2.54e-04)	Tok/s 17954 (23302)	Loss/tok 3.0305 (3.1668)	LR 7.813e-06
0: TRAIN [2][4470/5173]	Time 0.560 (0.606)	Data 1.23e-04 (2.54e-04)	Tok/s 18609 (23297)	Loss/tok 3.0574 (3.1667)	LR 7.813e-06
0: TRAIN [2][4480/5173]	Time 0.569 (0.606)	Data 1.18e-04 (2.53e-04)	Tok/s 18118 (23296)	Loss/tok 3.0241 (3.1668)	LR 7.813e-06
0: TRAIN [2][4490/5173]	Time 0.567 (0.606)	Data 1.19e-04 (2.53e-04)	Tok/s 17873 (23298)	Loss/tok 2.9997 (3.1669)	LR 7.813e-06
0: TRAIN [2][4500/5173]	Time 0.570 (0.606)	Data 1.17e-04 (2.53e-04)	Tok/s 18473 (23290)	Loss/tok 2.9228 (3.1666)	LR 7.813e-06
0: TRAIN [2][4510/5173]	Time 0.566 (0.606)	Data 1.22e-04 (2.53e-04)	Tok/s 18338 (23281)	Loss/tok 2.9439 (3.1665)	LR 7.813e-06
0: TRAIN [2][4520/5173]	Time 0.628 (0.606)	Data 1.17e-04 (2.52e-04)	Tok/s 27444 (23281)	Loss/tok 3.1468 (3.1664)	LR 7.813e-06
0: TRAIN [2][4530/5173]	Time 0.567 (0.606)	Data 1.18e-04 (2.52e-04)	Tok/s 18163 (23289)	Loss/tok 2.9288 (3.1665)	LR 7.813e-06
0: TRAIN [2][4540/5173]	Time 0.623 (0.606)	Data 1.20e-04 (2.52e-04)	Tok/s 26720 (23287)	Loss/tok 3.1735 (3.1663)	LR 7.813e-06
0: TRAIN [2][4550/5173]	Time 0.475 (0.606)	Data 1.27e-04 (2.52e-04)	Tok/s 11266 (23297)	Loss/tok 2.5324 (3.1667)	LR 7.813e-06
0: TRAIN [2][4560/5173]	Time 0.767 (0.606)	Data 1.30e-04 (2.51e-04)	Tok/s 39033 (23292)	Loss/tok 3.4545 (3.1666)	LR 7.813e-06
0: TRAIN [2][4570/5173]	Time 0.567 (0.606)	Data 1.21e-04 (2.51e-04)	Tok/s 18321 (23289)	Loss/tok 2.9285 (3.1664)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][4580/5173]	Time 0.626 (0.606)	Data 1.18e-04 (2.51e-04)	Tok/s 27238 (23287)	Loss/tok 3.1789 (3.1663)	LR 7.813e-06
0: TRAIN [2][4590/5173]	Time 0.694 (0.606)	Data 1.24e-04 (2.50e-04)	Tok/s 33312 (23281)	Loss/tok 3.3643 (3.1661)	LR 7.813e-06
0: TRAIN [2][4600/5173]	Time 0.628 (0.606)	Data 1.18e-04 (2.50e-04)	Tok/s 27011 (23280)	Loss/tok 3.1601 (3.1660)	LR 7.813e-06
0: TRAIN [2][4610/5173]	Time 0.631 (0.606)	Data 1.21e-04 (2.50e-04)	Tok/s 26648 (23281)	Loss/tok 3.1969 (3.1663)	LR 7.813e-06
0: TRAIN [2][4620/5173]	Time 0.693 (0.606)	Data 1.14e-04 (2.50e-04)	Tok/s 33635 (23285)	Loss/tok 3.3864 (3.1663)	LR 7.813e-06
0: TRAIN [2][4630/5173]	Time 0.508 (0.606)	Data 1.18e-04 (2.49e-04)	Tok/s 10326 (23287)	Loss/tok 2.5842 (3.1663)	LR 7.813e-06
0: TRAIN [2][4640/5173]	Time 0.506 (0.606)	Data 1.18e-04 (2.49e-04)	Tok/s 10354 (23273)	Loss/tok 2.5709 (3.1660)	LR 7.813e-06
0: TRAIN [2][4650/5173]	Time 0.569 (0.606)	Data 1.22e-04 (2.49e-04)	Tok/s 18217 (23283)	Loss/tok 3.0318 (3.1662)	LR 7.813e-06
0: TRAIN [2][4660/5173]	Time 0.624 (0.606)	Data 1.19e-04 (2.49e-04)	Tok/s 27097 (23278)	Loss/tok 3.1256 (3.1660)	LR 7.813e-06
0: TRAIN [2][4670/5173]	Time 0.688 (0.606)	Data 1.22e-04 (2.48e-04)	Tok/s 33482 (23278)	Loss/tok 3.4090 (3.1661)	LR 7.813e-06
0: TRAIN [2][4680/5173]	Time 0.631 (0.606)	Data 1.18e-04 (2.48e-04)	Tok/s 26560 (23273)	Loss/tok 3.1897 (3.1660)	LR 7.813e-06
0: TRAIN [2][4690/5173]	Time 0.567 (0.606)	Data 1.23e-04 (2.48e-04)	Tok/s 18693 (23273)	Loss/tok 2.9888 (3.1660)	LR 7.813e-06
0: TRAIN [2][4700/5173]	Time 0.625 (0.606)	Data 2.80e-04 (2.48e-04)	Tok/s 26962 (23275)	Loss/tok 3.1219 (3.1660)	LR 7.813e-06
0: TRAIN [2][4710/5173]	Time 0.624 (0.606)	Data 2.68e-04 (2.47e-04)	Tok/s 26484 (23273)	Loss/tok 3.2262 (3.1660)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][4720/5173]	Time 0.626 (0.606)	Data 1.20e-04 (2.47e-04)	Tok/s 26891 (23272)	Loss/tok 3.1655 (3.1661)	LR 7.813e-06
0: TRAIN [2][4730/5173]	Time 0.631 (0.606)	Data 1.21e-04 (2.47e-04)	Tok/s 27243 (23279)	Loss/tok 3.0784 (3.1663)	LR 7.813e-06
0: TRAIN [2][4740/5173]	Time 0.565 (0.606)	Data 1.20e-04 (2.47e-04)	Tok/s 18275 (23276)	Loss/tok 3.0232 (3.1661)	LR 7.813e-06
0: TRAIN [2][4750/5173]	Time 0.569 (0.606)	Data 1.18e-04 (2.46e-04)	Tok/s 18114 (23272)	Loss/tok 2.9681 (3.1660)	LR 7.813e-06
0: TRAIN [2][4760/5173]	Time 0.623 (0.606)	Data 1.20e-04 (2.46e-04)	Tok/s 26604 (23272)	Loss/tok 3.2206 (3.1660)	LR 7.813e-06
0: TRAIN [2][4770/5173]	Time 0.622 (0.606)	Data 2.84e-04 (2.46e-04)	Tok/s 26992 (23267)	Loss/tok 3.2198 (3.1659)	LR 7.813e-06
0: TRAIN [2][4780/5173]	Time 0.505 (0.606)	Data 1.17e-04 (2.46e-04)	Tok/s 10248 (23260)	Loss/tok 2.5808 (3.1658)	LR 7.813e-06
0: TRAIN [2][4790/5173]	Time 0.569 (0.606)	Data 1.20e-04 (2.45e-04)	Tok/s 17951 (23263)	Loss/tok 2.9058 (3.1658)	LR 7.813e-06
0: TRAIN [2][4800/5173]	Time 0.623 (0.606)	Data 1.31e-04 (2.45e-04)	Tok/s 26540 (23268)	Loss/tok 3.1267 (3.1660)	LR 7.813e-06
0: TRAIN [2][4810/5173]	Time 0.631 (0.606)	Data 2.66e-04 (2.45e-04)	Tok/s 26743 (23268)	Loss/tok 3.1913 (3.1659)	LR 7.813e-06
0: TRAIN [2][4820/5173]	Time 0.626 (0.606)	Data 1.16e-04 (2.45e-04)	Tok/s 26788 (23267)	Loss/tok 3.1779 (3.1659)	LR 7.813e-06
0: TRAIN [2][4830/5173]	Time 0.626 (0.606)	Data 1.17e-04 (2.45e-04)	Tok/s 26616 (23266)	Loss/tok 3.1837 (3.1659)	LR 7.813e-06
0: TRAIN [2][4840/5173]	Time 0.626 (0.606)	Data 1.19e-04 (2.44e-04)	Tok/s 26950 (23262)	Loss/tok 3.1156 (3.1658)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][4850/5173]	Time 0.627 (0.606)	Data 1.21e-04 (2.44e-04)	Tok/s 26934 (23259)	Loss/tok 3.1858 (3.1656)	LR 7.813e-06
0: TRAIN [2][4860/5173]	Time 0.570 (0.606)	Data 1.19e-04 (2.44e-04)	Tok/s 18283 (23255)	Loss/tok 2.9190 (3.1655)	LR 7.813e-06
0: TRAIN [2][4870/5173]	Time 0.570 (0.606)	Data 1.24e-04 (2.44e-04)	Tok/s 18306 (23267)	Loss/tok 2.8631 (3.1658)	LR 7.813e-06
0: TRAIN [2][4880/5173]	Time 0.622 (0.606)	Data 1.19e-04 (2.43e-04)	Tok/s 26925 (23272)	Loss/tok 3.0949 (3.1660)	LR 7.813e-06
0: TRAIN [2][4890/5173]	Time 0.622 (0.606)	Data 1.20e-04 (2.43e-04)	Tok/s 26969 (23270)	Loss/tok 3.1716 (3.1658)	LR 7.813e-06
0: TRAIN [2][4900/5173]	Time 0.624 (0.606)	Data 1.13e-04 (2.43e-04)	Tok/s 26641 (23271)	Loss/tok 3.0748 (3.1657)	LR 7.813e-06
0: TRAIN [2][4910/5173]	Time 0.765 (0.606)	Data 1.24e-04 (2.43e-04)	Tok/s 38730 (23282)	Loss/tok 3.5977 (3.1660)	LR 7.813e-06
0: TRAIN [2][4920/5173]	Time 0.567 (0.606)	Data 1.34e-04 (2.42e-04)	Tok/s 18279 (23285)	Loss/tok 2.9643 (3.1661)	LR 7.813e-06
0: TRAIN [2][4930/5173]	Time 0.566 (0.606)	Data 1.22e-04 (2.42e-04)	Tok/s 18141 (23286)	Loss/tok 2.9454 (3.1662)	LR 7.813e-06
0: TRAIN [2][4940/5173]	Time 0.565 (0.606)	Data 1.18e-04 (2.42e-04)	Tok/s 18384 (23291)	Loss/tok 2.8823 (3.1663)	LR 7.813e-06
0: TRAIN [2][4950/5173]	Time 0.631 (0.606)	Data 1.18e-04 (2.42e-04)	Tok/s 26259 (23294)	Loss/tok 3.1978 (3.1664)	LR 7.813e-06
0: TRAIN [2][4960/5173]	Time 0.506 (0.606)	Data 1.19e-04 (2.42e-04)	Tok/s 10461 (23291)	Loss/tok 2.6681 (3.1662)	LR 7.813e-06
0: TRAIN [2][4970/5173]	Time 0.694 (0.606)	Data 2.92e-04 (2.41e-04)	Tok/s 33292 (23290)	Loss/tok 3.3495 (3.1663)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][4980/5173]	Time 0.502 (0.606)	Data 1.18e-04 (2.41e-04)	Tok/s 10413 (23288)	Loss/tok 2.5861 (3.1662)	LR 7.813e-06
0: TRAIN [2][4990/5173]	Time 0.628 (0.606)	Data 1.21e-04 (2.41e-04)	Tok/s 26742 (23291)	Loss/tok 3.1038 (3.1663)	LR 7.813e-06
0: TRAIN [2][5000/5173]	Time 0.567 (0.606)	Data 1.22e-04 (2.41e-04)	Tok/s 18344 (23289)	Loss/tok 2.9795 (3.1662)	LR 7.813e-06
0: TRAIN [2][5010/5173]	Time 0.692 (0.606)	Data 1.25e-04 (2.40e-04)	Tok/s 33962 (23294)	Loss/tok 3.3314 (3.1663)	LR 7.813e-06
0: TRAIN [2][5020/5173]	Time 0.629 (0.606)	Data 1.51e-04 (2.40e-04)	Tok/s 26593 (23293)	Loss/tok 3.0864 (3.1662)	LR 7.813e-06
0: TRAIN [2][5030/5173]	Time 0.623 (0.606)	Data 1.45e-04 (2.40e-04)	Tok/s 27021 (23293)	Loss/tok 3.1197 (3.1661)	LR 7.813e-06
0: TRAIN [2][5040/5173]	Time 0.566 (0.606)	Data 1.31e-04 (2.40e-04)	Tok/s 18814 (23301)	Loss/tok 2.9672 (3.1664)	LR 7.813e-06
0: TRAIN [2][5050/5173]	Time 0.569 (0.606)	Data 1.26e-04 (2.39e-04)	Tok/s 18531 (23298)	Loss/tok 3.0202 (3.1663)	LR 7.813e-06
0: TRAIN [2][5060/5173]	Time 0.769 (0.606)	Data 1.20e-04 (2.39e-04)	Tok/s 38344 (23296)	Loss/tok 3.5341 (3.1664)	LR 7.813e-06
0: TRAIN [2][5070/5173]	Time 0.569 (0.606)	Data 2.90e-04 (2.39e-04)	Tok/s 18456 (23290)	Loss/tok 2.8594 (3.1662)	LR 7.813e-06
0: TRAIN [2][5080/5173]	Time 0.625 (0.606)	Data 1.23e-04 (2.39e-04)	Tok/s 27334 (23293)	Loss/tok 3.1717 (3.1662)	LR 7.813e-06
0: TRAIN [2][5090/5173]	Time 0.625 (0.606)	Data 1.14e-04 (2.39e-04)	Tok/s 26986 (23301)	Loss/tok 3.1651 (3.1664)	LR 7.813e-06
0: TRAIN [2][5100/5173]	Time 0.689 (0.606)	Data 1.09e-04 (2.38e-04)	Tok/s 33673 (23299)	Loss/tok 3.3548 (3.1663)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][5110/5173]	Time 0.504 (0.606)	Data 1.14e-04 (2.38e-04)	Tok/s 10667 (23300)	Loss/tok 2.5481 (3.1664)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][5120/5173]	Time 0.627 (0.606)	Data 1.17e-04 (2.38e-04)	Tok/s 26730 (23298)	Loss/tok 3.1721 (3.1663)	LR 7.813e-06
0: TRAIN [2][5130/5173]	Time 0.690 (0.606)	Data 1.11e-04 (2.38e-04)	Tok/s 33564 (23305)	Loss/tok 3.4662 (3.1666)	LR 7.813e-06
0: TRAIN [2][5140/5173]	Time 0.622 (0.606)	Data 2.91e-04 (2.38e-04)	Tok/s 26851 (23306)	Loss/tok 3.1009 (3.1665)	LR 7.813e-06
0: TRAIN [2][5150/5173]	Time 0.688 (0.606)	Data 1.13e-04 (2.37e-04)	Tok/s 34297 (23303)	Loss/tok 3.2786 (3.1664)	LR 7.813e-06
0: TRAIN [2][5160/5173]	Time 0.691 (0.606)	Data 1.14e-04 (2.37e-04)	Tok/s 33467 (23306)	Loss/tok 3.3776 (3.1665)	LR 7.813e-06
0: TRAIN [2][5170/5173]	Time 0.506 (0.606)	Data 1.13e-04 (2.37e-04)	Tok/s 10480 (23308)	Loss/tok 2.5183 (3.1664)	LR 7.813e-06
:::MLL 1586535550.454 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 525}}
:::MLL 1586535550.454 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [2][0/8]	Time 0.742 (0.742)	Decoder iters 149.0 (149.0)	Tok/s 22230 (22230)
0: Running moses detokenizer
0: BLEU(score=23.08133291878149, counts=[36675, 18009, 10116, 5886], totals=[65607, 62604, 59601, 56603], precisions=[55.901047144359595, 28.766532489936747, 16.97286958272512, 10.398742116142254], bp=1.0, sys_len=65607, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1586535556.532 eval_accuracy: {"value": 23.08, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 536}}
:::MLL 1586535556.532 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 2	Training Loss: 3.1682	Test BLEU: 23.08
0: Performance: Epoch: 2	Training: 69909 Tok/s
0: Finished epoch 2
:::MLL 1586535556.533 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 558}}
:::MLL 1586535556.533 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1586535556.533 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 515}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 2210508951
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][0/5173]	Time 1.266 (1.266)	Data 5.72e-01 (5.72e-01)	Tok/s 18530 (18530)	Loss/tok 3.2904 (3.2904)	LR 7.813e-06
0: TRAIN [3][10/5173]	Time 0.564 (0.655)	Data 1.24e-04 (5.22e-02)	Tok/s 18004 (21162)	Loss/tok 2.9140 (3.1168)	LR 7.813e-06
0: TRAIN [3][20/5173]	Time 0.571 (0.631)	Data 1.30e-04 (2.74e-02)	Tok/s 18178 (22141)	Loss/tok 2.8202 (3.1065)	LR 7.813e-06
0: TRAIN [3][30/5173]	Time 0.630 (0.619)	Data 1.29e-04 (1.86e-02)	Tok/s 26986 (21983)	Loss/tok 3.2381 (3.1002)	LR 7.813e-06
0: TRAIN [3][40/5173]	Time 0.568 (0.611)	Data 1.25e-04 (1.41e-02)	Tok/s 18150 (21663)	Loss/tok 2.8846 (3.0833)	LR 7.813e-06
0: TRAIN [3][50/5173]	Time 0.486 (0.609)	Data 1.29e-04 (1.14e-02)	Tok/s 10948 (21935)	Loss/tok 2.5816 (3.0956)	LR 7.813e-06
0: TRAIN [3][60/5173]	Time 0.629 (0.609)	Data 1.30e-04 (9.52e-03)	Tok/s 27245 (22264)	Loss/tok 3.2175 (3.1009)	LR 7.813e-06
0: TRAIN [3][70/5173]	Time 0.570 (0.606)	Data 3.08e-04 (8.21e-03)	Tok/s 17939 (22100)	Loss/tok 2.9194 (3.1052)	LR 7.813e-06
0: TRAIN [3][80/5173]	Time 0.689 (0.609)	Data 1.26e-04 (7.21e-03)	Tok/s 34136 (22491)	Loss/tok 3.2550 (3.1175)	LR 7.813e-06
0: TRAIN [3][90/5173]	Time 0.619 (0.608)	Data 1.26e-04 (6.43e-03)	Tok/s 27272 (22598)	Loss/tok 3.1450 (3.1149)	LR 7.813e-06
0: TRAIN [3][100/5173]	Time 0.569 (0.609)	Data 1.24e-04 (5.81e-03)	Tok/s 18512 (22769)	Loss/tok 3.0180 (3.1241)	LR 7.813e-06
0: TRAIN [3][110/5173]	Time 0.567 (0.611)	Data 1.29e-04 (5.30e-03)	Tok/s 18453 (23069)	Loss/tok 3.0642 (3.1332)	LR 7.813e-06
0: TRAIN [3][120/5173]	Time 0.691 (0.610)	Data 1.24e-04 (4.87e-03)	Tok/s 33631 (23092)	Loss/tok 3.4320 (3.1321)	LR 7.813e-06
0: TRAIN [3][130/5173]	Time 0.569 (0.610)	Data 1.21e-04 (4.51e-03)	Tok/s 17867 (23081)	Loss/tok 2.9773 (3.1334)	LR 7.813e-06
0: TRAIN [3][140/5173]	Time 0.566 (0.612)	Data 1.34e-04 (4.20e-03)	Tok/s 18530 (23325)	Loss/tok 2.9513 (3.1455)	LR 7.813e-06
0: TRAIN [3][150/5173]	Time 0.627 (0.613)	Data 1.26e-04 (3.93e-03)	Tok/s 27015 (23516)	Loss/tok 3.1478 (3.1521)	LR 7.813e-06
0: TRAIN [3][160/5173]	Time 0.569 (0.612)	Data 1.33e-04 (3.69e-03)	Tok/s 18018 (23437)	Loss/tok 3.0324 (3.1511)	LR 7.813e-06
0: TRAIN [3][170/5173]	Time 0.507 (0.612)	Data 1.22e-04 (3.49e-03)	Tok/s 10456 (23507)	Loss/tok 2.4228 (3.1541)	LR 7.813e-06
0: TRAIN [3][180/5173]	Time 0.556 (0.613)	Data 1.41e-04 (3.30e-03)	Tok/s 18886 (23675)	Loss/tok 2.9458 (3.1627)	LR 7.813e-06
0: TRAIN [3][190/5173]	Time 0.626 (0.612)	Data 1.28e-04 (3.14e-03)	Tok/s 26320 (23613)	Loss/tok 3.2777 (3.1593)	LR 7.813e-06
0: TRAIN [3][200/5173]	Time 0.566 (0.612)	Data 1.28e-04 (2.99e-03)	Tok/s 18241 (23585)	Loss/tok 2.8898 (3.1573)	LR 7.813e-06
0: TRAIN [3][210/5173]	Time 0.567 (0.612)	Data 1.29e-04 (2.85e-03)	Tok/s 18039 (23599)	Loss/tok 3.0784 (3.1581)	LR 7.813e-06
0: TRAIN [3][220/5173]	Time 0.569 (0.612)	Data 1.34e-04 (2.73e-03)	Tok/s 18572 (23565)	Loss/tok 2.9271 (3.1586)	LR 7.813e-06
0: TRAIN [3][230/5173]	Time 0.568 (0.611)	Data 1.24e-04 (2.62e-03)	Tok/s 17960 (23545)	Loss/tok 3.0189 (3.1575)	LR 7.813e-06
0: TRAIN [3][240/5173]	Time 0.696 (0.611)	Data 1.37e-04 (2.51e-03)	Tok/s 33493 (23600)	Loss/tok 3.4357 (3.1578)	LR 7.813e-06
0: TRAIN [3][250/5173]	Time 0.765 (0.611)	Data 1.25e-04 (2.42e-03)	Tok/s 39105 (23576)	Loss/tok 3.4349 (3.1576)	LR 7.813e-06
0: TRAIN [3][260/5173]	Time 0.626 (0.612)	Data 1.54e-04 (2.33e-03)	Tok/s 26828 (23714)	Loss/tok 3.1145 (3.1666)	LR 7.813e-06
0: TRAIN [3][270/5173]	Time 0.504 (0.612)	Data 1.34e-04 (2.25e-03)	Tok/s 10448 (23713)	Loss/tok 2.6169 (3.1668)	LR 7.813e-06
0: TRAIN [3][280/5173]	Time 0.624 (0.613)	Data 1.29e-04 (2.18e-03)	Tok/s 27264 (23783)	Loss/tok 3.1925 (3.1667)	LR 7.813e-06
0: TRAIN [3][290/5173]	Time 0.624 (0.612)	Data 1.32e-04 (2.11e-03)	Tok/s 26876 (23764)	Loss/tok 3.3278 (3.1674)	LR 7.813e-06
0: TRAIN [3][300/5173]	Time 0.633 (0.612)	Data 1.34e-04 (2.04e-03)	Tok/s 26460 (23684)	Loss/tok 3.2028 (3.1660)	LR 7.813e-06
0: TRAIN [3][310/5173]	Time 0.693 (0.612)	Data 1.30e-04 (1.98e-03)	Tok/s 33969 (23680)	Loss/tok 3.2631 (3.1665)	LR 7.813e-06
0: TRAIN [3][320/5173]	Time 0.767 (0.612)	Data 1.29e-04 (1.92e-03)	Tok/s 38437 (23739)	Loss/tok 3.4887 (3.1695)	LR 7.813e-06
0: TRAIN [3][330/5173]	Time 0.567 (0.613)	Data 1.34e-04 (1.87e-03)	Tok/s 17697 (23774)	Loss/tok 2.9913 (3.1729)	LR 7.813e-06
0: TRAIN [3][340/5173]	Time 0.505 (0.612)	Data 1.32e-04 (1.82e-03)	Tok/s 10398 (23714)	Loss/tok 2.6193 (3.1716)	LR 7.813e-06
0: TRAIN [3][350/5173]	Time 0.694 (0.612)	Data 1.47e-04 (1.77e-03)	Tok/s 33575 (23730)	Loss/tok 3.2340 (3.1729)	LR 7.813e-06
0: TRAIN [3][360/5173]	Time 0.628 (0.611)	Data 1.30e-04 (1.73e-03)	Tok/s 26803 (23606)	Loss/tok 2.9889 (3.1696)	LR 7.813e-06
0: TRAIN [3][370/5173]	Time 0.569 (0.610)	Data 1.28e-04 (1.68e-03)	Tok/s 18151 (23548)	Loss/tok 2.9649 (3.1695)	LR 7.813e-06
0: TRAIN [3][380/5173]	Time 0.496 (0.609)	Data 1.19e-04 (1.64e-03)	Tok/s 10785 (23397)	Loss/tok 2.5259 (3.1660)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][390/5173]	Time 0.628 (0.610)	Data 1.28e-04 (1.61e-03)	Tok/s 26867 (23486)	Loss/tok 3.2194 (3.1722)	LR 7.813e-06
0: TRAIN [3][400/5173]	Time 0.568 (0.610)	Data 1.30e-04 (1.57e-03)	Tok/s 18072 (23499)	Loss/tok 2.7534 (3.1718)	LR 7.813e-06
0: TRAIN [3][410/5173]	Time 0.564 (0.610)	Data 1.26e-04 (1.53e-03)	Tok/s 18597 (23470)	Loss/tok 3.0423 (3.1708)	LR 7.813e-06
0: TRAIN [3][420/5173]	Time 0.567 (0.609)	Data 1.28e-04 (1.50e-03)	Tok/s 18511 (23435)	Loss/tok 2.9676 (3.1707)	LR 7.813e-06
0: TRAIN [3][430/5173]	Time 0.693 (0.609)	Data 1.26e-04 (1.47e-03)	Tok/s 33490 (23447)	Loss/tok 3.3102 (3.1693)	LR 7.813e-06
0: TRAIN [3][440/5173]	Time 0.504 (0.609)	Data 1.26e-04 (1.44e-03)	Tok/s 10560 (23427)	Loss/tok 2.5658 (3.1701)	LR 7.813e-06
0: TRAIN [3][450/5173]	Time 0.689 (0.610)	Data 3.04e-04 (1.41e-03)	Tok/s 33833 (23472)	Loss/tok 3.3181 (3.1702)	LR 7.813e-06
0: TRAIN [3][460/5173]	Time 0.566 (0.609)	Data 1.26e-04 (1.38e-03)	Tok/s 18400 (23410)	Loss/tok 2.9231 (3.1682)	LR 7.813e-06
0: TRAIN [3][470/5173]	Time 0.634 (0.610)	Data 1.65e-04 (1.36e-03)	Tok/s 26835 (23508)	Loss/tok 3.1546 (3.1710)	LR 7.813e-06
0: TRAIN [3][480/5173]	Time 0.627 (0.610)	Data 1.24e-04 (1.33e-03)	Tok/s 26472 (23529)	Loss/tok 3.2222 (3.1726)	LR 7.813e-06
0: TRAIN [3][490/5173]	Time 0.564 (0.609)	Data 1.32e-04 (1.31e-03)	Tok/s 18455 (23472)	Loss/tok 2.8318 (3.1716)	LR 7.813e-06
0: TRAIN [3][500/5173]	Time 0.626 (0.609)	Data 1.26e-04 (1.29e-03)	Tok/s 26594 (23411)	Loss/tok 3.1795 (3.1710)	LR 7.813e-06
0: TRAIN [3][510/5173]	Time 0.688 (0.609)	Data 1.26e-04 (1.26e-03)	Tok/s 33541 (23461)	Loss/tok 3.3282 (3.1717)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][520/5173]	Time 0.567 (0.610)	Data 1.28e-04 (1.24e-03)	Tok/s 18084 (23479)	Loss/tok 3.0004 (3.1724)	LR 7.813e-06
0: TRAIN [3][530/5173]	Time 0.763 (0.610)	Data 1.37e-04 (1.22e-03)	Tok/s 38550 (23529)	Loss/tok 3.4946 (3.1741)	LR 7.813e-06
0: TRAIN [3][540/5173]	Time 0.630 (0.611)	Data 1.27e-04 (1.20e-03)	Tok/s 26154 (23611)	Loss/tok 3.1770 (3.1770)	LR 7.813e-06
0: TRAIN [3][550/5173]	Time 0.495 (0.611)	Data 1.28e-04 (1.18e-03)	Tok/s 10872 (23589)	Loss/tok 2.5152 (3.1763)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][560/5173]	Time 0.691 (0.611)	Data 1.36e-04 (1.17e-03)	Tok/s 33623 (23650)	Loss/tok 3.3080 (3.1784)	LR 7.813e-06
0: TRAIN [3][570/5173]	Time 0.565 (0.611)	Data 1.22e-04 (1.15e-03)	Tok/s 18356 (23581)	Loss/tok 2.9714 (3.1771)	LR 7.813e-06
0: TRAIN [3][580/5173]	Time 0.629 (0.610)	Data 1.29e-04 (1.13e-03)	Tok/s 26684 (23571)	Loss/tok 3.1505 (3.1764)	LR 7.813e-06
0: TRAIN [3][590/5173]	Time 0.627 (0.611)	Data 1.27e-04 (1.11e-03)	Tok/s 26543 (23613)	Loss/tok 3.0861 (3.1770)	LR 7.813e-06
0: TRAIN [3][600/5173]	Time 0.490 (0.610)	Data 1.24e-04 (1.10e-03)	Tok/s 10920 (23580)	Loss/tok 2.5671 (3.1755)	LR 7.813e-06
0: TRAIN [3][610/5173]	Time 0.690 (0.610)	Data 1.24e-04 (1.08e-03)	Tok/s 33648 (23586)	Loss/tok 3.3484 (3.1752)	LR 7.813e-06
0: TRAIN [3][620/5173]	Time 0.630 (0.610)	Data 1.28e-04 (1.07e-03)	Tok/s 26804 (23567)	Loss/tok 3.0973 (3.1759)	LR 7.813e-06
0: TRAIN [3][630/5173]	Time 0.692 (0.610)	Data 1.23e-04 (1.05e-03)	Tok/s 33440 (23597)	Loss/tok 3.3636 (3.1768)	LR 7.813e-06
0: TRAIN [3][640/5173]	Time 0.630 (0.611)	Data 1.21e-04 (1.04e-03)	Tok/s 26795 (23611)	Loss/tok 3.1600 (3.1768)	LR 7.813e-06
0: TRAIN [3][650/5173]	Time 0.566 (0.611)	Data 1.40e-04 (1.02e-03)	Tok/s 18178 (23680)	Loss/tok 3.0523 (3.1790)	LR 7.813e-06
0: TRAIN [3][660/5173]	Time 0.503 (0.611)	Data 1.21e-04 (1.01e-03)	Tok/s 10365 (23645)	Loss/tok 2.4726 (3.1785)	LR 7.813e-06
0: TRAIN [3][670/5173]	Time 0.505 (0.610)	Data 1.25e-04 (9.97e-04)	Tok/s 10607 (23589)	Loss/tok 2.4656 (3.1772)	LR 7.813e-06
0: TRAIN [3][680/5173]	Time 0.568 (0.611)	Data 1.27e-04 (9.85e-04)	Tok/s 18416 (23622)	Loss/tok 2.8631 (3.1783)	LR 7.813e-06
0: TRAIN [3][690/5173]	Time 0.570 (0.610)	Data 1.26e-04 (9.72e-04)	Tok/s 18001 (23590)	Loss/tok 3.0198 (3.1773)	LR 7.813e-06
0: TRAIN [3][700/5173]	Time 0.565 (0.611)	Data 1.20e-04 (9.60e-04)	Tok/s 18295 (23602)	Loss/tok 3.0107 (3.1779)	LR 7.813e-06
0: TRAIN [3][710/5173]	Time 0.561 (0.610)	Data 1.28e-04 (9.49e-04)	Tok/s 18530 (23593)	Loss/tok 2.9144 (3.1773)	LR 7.813e-06
0: TRAIN [3][720/5173]	Time 0.570 (0.610)	Data 1.26e-04 (9.38e-04)	Tok/s 18292 (23548)	Loss/tok 3.0473 (3.1770)	LR 7.813e-06
0: TRAIN [3][730/5173]	Time 0.626 (0.611)	Data 1.27e-04 (9.26e-04)	Tok/s 26716 (23605)	Loss/tok 3.1694 (3.1785)	LR 7.813e-06
0: TRAIN [3][740/5173]	Time 0.768 (0.611)	Data 1.37e-04 (9.16e-04)	Tok/s 39181 (23604)	Loss/tok 3.3974 (3.1784)	LR 7.813e-06
0: TRAIN [3][750/5173]	Time 0.623 (0.610)	Data 1.29e-04 (9.06e-04)	Tok/s 26808 (23547)	Loss/tok 3.0919 (3.1774)	LR 7.813e-06
0: TRAIN [3][760/5173]	Time 0.691 (0.610)	Data 1.22e-04 (8.96e-04)	Tok/s 33742 (23582)	Loss/tok 3.1864 (3.1781)	LR 7.813e-06
0: TRAIN [3][770/5173]	Time 0.763 (0.610)	Data 1.25e-04 (8.86e-04)	Tok/s 38741 (23561)	Loss/tok 3.4792 (3.1775)	LR 7.813e-06
0: TRAIN [3][780/5173]	Time 0.681 (0.610)	Data 1.29e-04 (8.76e-04)	Tok/s 34437 (23534)	Loss/tok 3.2569 (3.1764)	LR 7.813e-06
0: TRAIN [3][790/5173]	Time 0.625 (0.610)	Data 1.30e-04 (8.67e-04)	Tok/s 26505 (23537)	Loss/tok 3.1903 (3.1762)	LR 7.813e-06
0: TRAIN [3][800/5173]	Time 0.632 (0.610)	Data 1.22e-04 (8.58e-04)	Tok/s 26943 (23574)	Loss/tok 3.1297 (3.1760)	LR 7.813e-06
0: TRAIN [3][810/5173]	Time 0.625 (0.610)	Data 1.28e-04 (8.50e-04)	Tok/s 26824 (23556)	Loss/tok 3.0820 (3.1755)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][820/5173]	Time 0.629 (0.610)	Data 1.34e-04 (8.41e-04)	Tok/s 26656 (23544)	Loss/tok 3.2722 (3.1756)	LR 7.813e-06
0: TRAIN [3][830/5173]	Time 0.569 (0.610)	Data 1.22e-04 (8.33e-04)	Tok/s 18299 (23540)	Loss/tok 3.0053 (3.1750)	LR 7.813e-06
0: TRAIN [3][840/5173]	Time 0.570 (0.610)	Data 1.27e-04 (8.25e-04)	Tok/s 18602 (23523)	Loss/tok 2.8917 (3.1741)	LR 7.813e-06
0: TRAIN [3][850/5173]	Time 0.629 (0.610)	Data 1.24e-04 (8.17e-04)	Tok/s 26837 (23556)	Loss/tok 3.2461 (3.1744)	LR 7.813e-06
0: TRAIN [3][860/5173]	Time 0.693 (0.610)	Data 1.26e-04 (8.08e-04)	Tok/s 33759 (23540)	Loss/tok 3.3214 (3.1741)	LR 7.813e-06
0: TRAIN [3][870/5173]	Time 0.565 (0.610)	Data 1.36e-04 (8.01e-04)	Tok/s 18456 (23506)	Loss/tok 2.9520 (3.1729)	LR 7.813e-06
0: TRAIN [3][880/5173]	Time 0.568 (0.610)	Data 1.40e-04 (7.93e-04)	Tok/s 18314 (23518)	Loss/tok 2.9867 (3.1730)	LR 7.813e-06
0: TRAIN [3][890/5173]	Time 0.568 (0.609)	Data 1.29e-04 (7.86e-04)	Tok/s 18384 (23460)	Loss/tok 3.1688 (3.1721)	LR 7.813e-06
0: TRAIN [3][900/5173]	Time 0.568 (0.609)	Data 1.35e-04 (7.79e-04)	Tok/s 17984 (23434)	Loss/tok 2.9383 (3.1715)	LR 7.813e-06
0: TRAIN [3][910/5173]	Time 0.764 (0.609)	Data 1.28e-04 (7.72e-04)	Tok/s 38931 (23443)	Loss/tok 3.5137 (3.1720)	LR 7.813e-06
0: TRAIN [3][920/5173]	Time 0.565 (0.609)	Data 1.25e-04 (7.65e-04)	Tok/s 17994 (23440)	Loss/tok 2.8398 (3.1714)	LR 7.813e-06
0: TRAIN [3][930/5173]	Time 0.763 (0.609)	Data 1.36e-04 (7.58e-04)	Tok/s 38754 (23406)	Loss/tok 3.6743 (3.1710)	LR 7.813e-06
0: TRAIN [3][940/5173]	Time 0.624 (0.608)	Data 1.28e-04 (7.51e-04)	Tok/s 26734 (23337)	Loss/tok 3.1671 (3.1696)	LR 7.813e-06
0: TRAIN [3][950/5173]	Time 0.628 (0.608)	Data 1.22e-04 (7.45e-04)	Tok/s 26520 (23357)	Loss/tok 3.2150 (3.1701)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][960/5173]	Time 0.630 (0.608)	Data 1.29e-04 (7.39e-04)	Tok/s 26589 (23349)	Loss/tok 3.1912 (3.1695)	LR 7.813e-06
0: TRAIN [3][970/5173]	Time 0.507 (0.608)	Data 1.37e-04 (7.32e-04)	Tok/s 10397 (23344)	Loss/tok 2.5775 (3.1692)	LR 7.813e-06
0: TRAIN [3][980/5173]	Time 0.567 (0.608)	Data 1.22e-04 (7.26e-04)	Tok/s 17900 (23358)	Loss/tok 2.8947 (3.1689)	LR 7.813e-06
0: TRAIN [3][990/5173]	Time 0.687 (0.608)	Data 1.28e-04 (7.21e-04)	Tok/s 34087 (23341)	Loss/tok 3.3226 (3.1684)	LR 7.813e-06
0: TRAIN [3][1000/5173]	Time 0.688 (0.608)	Data 1.29e-04 (7.15e-04)	Tok/s 33739 (23361)	Loss/tok 3.3040 (3.1689)	LR 7.813e-06
0: TRAIN [3][1010/5173]	Time 0.568 (0.608)	Data 1.25e-04 (7.09e-04)	Tok/s 17915 (23334)	Loss/tok 3.0185 (3.1681)	LR 7.813e-06
0: TRAIN [3][1020/5173]	Time 0.569 (0.608)	Data 1.19e-04 (7.03e-04)	Tok/s 18571 (23334)	Loss/tok 2.9591 (3.1675)	LR 7.813e-06
0: TRAIN [3][1030/5173]	Time 0.626 (0.608)	Data 1.34e-04 (6.98e-04)	Tok/s 26658 (23310)	Loss/tok 3.4218 (3.1672)	LR 7.813e-06
0: TRAIN [3][1040/5173]	Time 0.635 (0.607)	Data 1.29e-04 (6.93e-04)	Tok/s 26390 (23278)	Loss/tok 3.2243 (3.1664)	LR 7.813e-06
0: TRAIN [3][1050/5173]	Time 0.637 (0.608)	Data 1.23e-04 (6.87e-04)	Tok/s 26642 (23289)	Loss/tok 3.1262 (3.1665)	LR 7.813e-06
0: TRAIN [3][1060/5173]	Time 0.498 (0.607)	Data 1.27e-04 (6.82e-04)	Tok/s 10735 (23244)	Loss/tok 2.5760 (3.1659)	LR 7.813e-06
0: TRAIN [3][1070/5173]	Time 0.627 (0.607)	Data 1.26e-04 (6.77e-04)	Tok/s 26723 (23274)	Loss/tok 3.1423 (3.1661)	LR 7.813e-06
0: TRAIN [3][1080/5173]	Time 0.569 (0.607)	Data 1.22e-04 (6.72e-04)	Tok/s 18596 (23251)	Loss/tok 2.9468 (3.1652)	LR 7.813e-06
0: TRAIN [3][1090/5173]	Time 0.570 (0.607)	Data 1.32e-04 (6.67e-04)	Tok/s 17916 (23229)	Loss/tok 2.9831 (3.1645)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][1100/5173]	Time 0.633 (0.607)	Data 1.36e-04 (6.63e-04)	Tok/s 26524 (23250)	Loss/tok 3.2940 (3.1654)	LR 7.813e-06
0: TRAIN [3][1110/5173]	Time 0.567 (0.607)	Data 1.24e-04 (6.58e-04)	Tok/s 18030 (23240)	Loss/tok 2.9076 (3.1652)	LR 7.813e-06
0: TRAIN [3][1120/5173]	Time 0.692 (0.607)	Data 1.22e-04 (6.53e-04)	Tok/s 34227 (23253)	Loss/tok 3.3918 (3.1653)	LR 7.813e-06
0: TRAIN [3][1130/5173]	Time 0.690 (0.607)	Data 1.27e-04 (6.48e-04)	Tok/s 33663 (23278)	Loss/tok 3.4603 (3.1661)	LR 7.813e-06
0: TRAIN [3][1140/5173]	Time 0.632 (0.607)	Data 1.25e-04 (6.44e-04)	Tok/s 26514 (23285)	Loss/tok 3.1698 (3.1657)	LR 7.813e-06
0: TRAIN [3][1150/5173]	Time 0.623 (0.607)	Data 1.26e-04 (6.40e-04)	Tok/s 26894 (23277)	Loss/tok 3.2059 (3.1651)	LR 7.813e-06
0: TRAIN [3][1160/5173]	Time 0.632 (0.607)	Data 1.31e-04 (6.35e-04)	Tok/s 26556 (23284)	Loss/tok 3.1967 (3.1647)	LR 7.813e-06
0: TRAIN [3][1170/5173]	Time 0.690 (0.607)	Data 1.37e-04 (6.31e-04)	Tok/s 33622 (23305)	Loss/tok 3.4276 (3.1658)	LR 7.813e-06
0: TRAIN [3][1180/5173]	Time 0.689 (0.607)	Data 1.29e-04 (6.27e-04)	Tok/s 33769 (23290)	Loss/tok 3.3477 (3.1653)	LR 7.813e-06
0: TRAIN [3][1190/5173]	Time 0.568 (0.607)	Data 1.28e-04 (6.23e-04)	Tok/s 18130 (23266)	Loss/tok 3.0418 (3.1649)	LR 7.813e-06
0: TRAIN [3][1200/5173]	Time 0.568 (0.607)	Data 1.23e-04 (6.19e-04)	Tok/s 17763 (23277)	Loss/tok 3.0691 (3.1651)	LR 7.813e-06
0: TRAIN [3][1210/5173]	Time 0.561 (0.607)	Data 1.25e-04 (6.15e-04)	Tok/s 18098 (23270)	Loss/tok 2.9330 (3.1648)	LR 7.813e-06
0: TRAIN [3][1220/5173]	Time 0.623 (0.607)	Data 1.27e-04 (6.11e-04)	Tok/s 26844 (23297)	Loss/tok 3.2608 (3.1658)	LR 7.813e-06
0: TRAIN [3][1230/5173]	Time 0.566 (0.607)	Data 1.24e-04 (6.07e-04)	Tok/s 18431 (23301)	Loss/tok 2.9952 (3.1660)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][1240/5173]	Time 0.505 (0.607)	Data 1.24e-04 (6.03e-04)	Tok/s 10155 (23301)	Loss/tok 2.6473 (3.1659)	LR 7.813e-06
0: TRAIN [3][1250/5173]	Time 0.628 (0.607)	Data 1.31e-04 (6.00e-04)	Tok/s 26443 (23304)	Loss/tok 3.1653 (3.1659)	LR 7.813e-06
0: TRAIN [3][1260/5173]	Time 0.624 (0.607)	Data 1.27e-04 (5.96e-04)	Tok/s 27004 (23317)	Loss/tok 3.1767 (3.1661)	LR 7.813e-06
0: TRAIN [3][1270/5173]	Time 0.688 (0.608)	Data 1.29e-04 (5.92e-04)	Tok/s 33843 (23332)	Loss/tok 3.2923 (3.1663)	LR 7.813e-06
0: TRAIN [3][1280/5173]	Time 0.565 (0.607)	Data 1.56e-04 (5.89e-04)	Tok/s 18610 (23325)	Loss/tok 3.0549 (3.1657)	LR 7.813e-06
0: TRAIN [3][1290/5173]	Time 0.627 (0.607)	Data 1.71e-04 (5.85e-04)	Tok/s 26864 (23286)	Loss/tok 3.1885 (3.1653)	LR 7.813e-06
0: TRAIN [3][1300/5173]	Time 0.692 (0.607)	Data 1.42e-04 (5.82e-04)	Tok/s 34217 (23282)	Loss/tok 3.2682 (3.1651)	LR 7.813e-06
0: TRAIN [3][1310/5173]	Time 0.629 (0.607)	Data 3.00e-04 (5.79e-04)	Tok/s 26551 (23269)	Loss/tok 3.1947 (3.1646)	LR 7.813e-06
0: TRAIN [3][1320/5173]	Time 0.566 (0.607)	Data 1.60e-04 (5.76e-04)	Tok/s 18306 (23312)	Loss/tok 2.9921 (3.1665)	LR 7.813e-06
0: TRAIN [3][1330/5173]	Time 0.566 (0.607)	Data 1.31e-04 (5.72e-04)	Tok/s 18115 (23306)	Loss/tok 3.0008 (3.1662)	LR 7.813e-06
0: TRAIN [3][1340/5173]	Time 0.689 (0.608)	Data 1.29e-04 (5.69e-04)	Tok/s 33796 (23341)	Loss/tok 3.4219 (3.1676)	LR 7.813e-06
0: TRAIN [3][1350/5173]	Time 0.566 (0.608)	Data 1.38e-04 (5.66e-04)	Tok/s 18078 (23353)	Loss/tok 3.0125 (3.1685)	LR 7.813e-06
0: TRAIN [3][1360/5173]	Time 0.567 (0.608)	Data 1.30e-04 (5.63e-04)	Tok/s 17975 (23370)	Loss/tok 3.0744 (3.1695)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][1370/5173]	Time 0.627 (0.608)	Data 1.27e-04 (5.60e-04)	Tok/s 27285 (23352)	Loss/tok 3.0580 (3.1689)	LR 7.813e-06
0: TRAIN [3][1380/5173]	Time 0.632 (0.608)	Data 1.35e-04 (5.57e-04)	Tok/s 26662 (23323)	Loss/tok 3.0674 (3.1680)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1390/5173]	Time 0.628 (0.607)	Data 1.24e-04 (5.54e-04)	Tok/s 26642 (23314)	Loss/tok 3.1470 (3.1680)	LR 7.813e-06
0: TRAIN [3][1400/5173]	Time 0.506 (0.607)	Data 1.32e-04 (5.51e-04)	Tok/s 10017 (23305)	Loss/tok 2.6497 (3.1678)	LR 7.813e-06
0: TRAIN [3][1410/5173]	Time 0.504 (0.607)	Data 1.29e-04 (5.48e-04)	Tok/s 10303 (23315)	Loss/tok 2.5692 (3.1679)	LR 7.813e-06
0: TRAIN [3][1420/5173]	Time 0.506 (0.607)	Data 1.28e-04 (5.45e-04)	Tok/s 10429 (23301)	Loss/tok 2.5136 (3.1675)	LR 7.813e-06
0: TRAIN [3][1430/5173]	Time 0.625 (0.608)	Data 1.24e-04 (5.42e-04)	Tok/s 26724 (23326)	Loss/tok 3.1994 (3.1682)	LR 7.813e-06
0: TRAIN [3][1440/5173]	Time 0.566 (0.608)	Data 1.42e-04 (5.39e-04)	Tok/s 18056 (23348)	Loss/tok 2.9715 (3.1686)	LR 7.813e-06
0: TRAIN [3][1450/5173]	Time 0.627 (0.608)	Data 1.34e-04 (5.37e-04)	Tok/s 26440 (23369)	Loss/tok 3.1667 (3.1688)	LR 7.813e-06
0: TRAIN [3][1460/5173]	Time 0.691 (0.608)	Data 1.38e-04 (5.34e-04)	Tok/s 33497 (23404)	Loss/tok 3.3216 (3.1694)	LR 7.813e-06
0: TRAIN [3][1470/5173]	Time 0.693 (0.608)	Data 1.29e-04 (5.31e-04)	Tok/s 33607 (23427)	Loss/tok 3.2105 (3.1698)	LR 7.813e-06
0: TRAIN [3][1480/5173]	Time 0.628 (0.609)	Data 1.36e-04 (5.29e-04)	Tok/s 27039 (23455)	Loss/tok 3.1182 (3.1704)	LR 7.813e-06
0: TRAIN [3][1490/5173]	Time 0.567 (0.608)	Data 1.28e-04 (5.26e-04)	Tok/s 17881 (23441)	Loss/tok 2.9624 (3.1698)	LR 7.813e-06
0: TRAIN [3][1500/5173]	Time 0.631 (0.608)	Data 1.33e-04 (5.23e-04)	Tok/s 26869 (23416)	Loss/tok 3.0931 (3.1694)	LR 7.813e-06
0: TRAIN [3][1510/5173]	Time 0.565 (0.608)	Data 1.31e-04 (5.21e-04)	Tok/s 18119 (23435)	Loss/tok 3.0358 (3.1695)	LR 7.813e-06
0: TRAIN [3][1520/5173]	Time 0.569 (0.608)	Data 1.29e-04 (5.18e-04)	Tok/s 18164 (23429)	Loss/tok 2.9507 (3.1690)	LR 7.813e-06
0: TRAIN [3][1530/5173]	Time 0.566 (0.608)	Data 1.35e-04 (5.16e-04)	Tok/s 18074 (23421)	Loss/tok 2.9876 (3.1688)	LR 7.813e-06
0: TRAIN [3][1540/5173]	Time 0.687 (0.608)	Data 1.35e-04 (5.14e-04)	Tok/s 34204 (23424)	Loss/tok 3.3454 (3.1684)	LR 7.813e-06
0: TRAIN [3][1550/5173]	Time 0.760 (0.608)	Data 1.31e-04 (5.11e-04)	Tok/s 39450 (23399)	Loss/tok 3.4537 (3.1681)	LR 7.813e-06
0: TRAIN [3][1560/5173]	Time 0.765 (0.608)	Data 1.35e-04 (5.09e-04)	Tok/s 39274 (23391)	Loss/tok 3.5365 (3.1682)	LR 7.813e-06
0: TRAIN [3][1570/5173]	Time 0.565 (0.608)	Data 3.11e-04 (5.07e-04)	Tok/s 18470 (23394)	Loss/tok 2.8455 (3.1679)	LR 7.813e-06
0: TRAIN [3][1580/5173]	Time 0.622 (0.608)	Data 1.29e-04 (5.04e-04)	Tok/s 27202 (23393)	Loss/tok 3.2043 (3.1677)	LR 7.813e-06
0: TRAIN [3][1590/5173]	Time 0.632 (0.608)	Data 1.31e-04 (5.02e-04)	Tok/s 26698 (23402)	Loss/tok 3.1638 (3.1675)	LR 7.813e-06
0: TRAIN [3][1600/5173]	Time 0.568 (0.608)	Data 1.33e-04 (5.00e-04)	Tok/s 18335 (23415)	Loss/tok 3.0521 (3.1678)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1610/5173]	Time 0.626 (0.608)	Data 1.32e-04 (4.98e-04)	Tok/s 26719 (23423)	Loss/tok 3.1071 (3.1680)	LR 7.813e-06
0: TRAIN [3][1620/5173]	Time 0.567 (0.608)	Data 1.30e-04 (4.96e-04)	Tok/s 18408 (23422)	Loss/tok 2.8813 (3.1679)	LR 7.813e-06
0: TRAIN [3][1630/5173]	Time 0.567 (0.608)	Data 1.34e-04 (4.93e-04)	Tok/s 18398 (23418)	Loss/tok 2.9718 (3.1680)	LR 7.813e-06
0: TRAIN [3][1640/5173]	Time 0.627 (0.608)	Data 1.40e-04 (4.91e-04)	Tok/s 26862 (23436)	Loss/tok 3.1328 (3.1682)	LR 7.813e-06
0: TRAIN [3][1650/5173]	Time 0.571 (0.608)	Data 1.42e-04 (4.89e-04)	Tok/s 18101 (23432)	Loss/tok 2.9634 (3.1681)	LR 7.813e-06
0: TRAIN [3][1660/5173]	Time 0.625 (0.608)	Data 1.35e-04 (4.87e-04)	Tok/s 27209 (23449)	Loss/tok 3.1641 (3.1682)	LR 7.813e-06
0: TRAIN [3][1670/5173]	Time 0.694 (0.608)	Data 1.31e-04 (4.85e-04)	Tok/s 33936 (23461)	Loss/tok 3.2552 (3.1683)	LR 7.813e-06
0: TRAIN [3][1680/5173]	Time 0.567 (0.608)	Data 1.29e-04 (4.83e-04)	Tok/s 18009 (23460)	Loss/tok 3.0541 (3.1682)	LR 7.813e-06
0: TRAIN [3][1690/5173]	Time 0.563 (0.608)	Data 1.34e-04 (4.81e-04)	Tok/s 18312 (23434)	Loss/tok 2.9723 (3.1677)	LR 7.813e-06
0: TRAIN [3][1700/5173]	Time 0.568 (0.608)	Data 2.78e-04 (4.79e-04)	Tok/s 18296 (23413)	Loss/tok 3.0226 (3.1672)	LR 7.813e-06
0: TRAIN [3][1710/5173]	Time 0.689 (0.608)	Data 1.29e-04 (4.77e-04)	Tok/s 33450 (23397)	Loss/tok 3.4335 (3.1666)	LR 7.813e-06
0: TRAIN [3][1720/5173]	Time 0.692 (0.608)	Data 1.34e-04 (4.75e-04)	Tok/s 33928 (23387)	Loss/tok 3.2228 (3.1662)	LR 7.813e-06
0: TRAIN [3][1730/5173]	Time 0.622 (0.608)	Data 1.32e-04 (4.73e-04)	Tok/s 26982 (23387)	Loss/tok 3.2346 (3.1660)	LR 7.813e-06
0: TRAIN [3][1740/5173]	Time 0.565 (0.608)	Data 2.78e-04 (4.72e-04)	Tok/s 18332 (23371)	Loss/tok 2.9625 (3.1655)	LR 7.813e-06
0: TRAIN [3][1750/5173]	Time 0.566 (0.607)	Data 1.29e-04 (4.70e-04)	Tok/s 18149 (23358)	Loss/tok 2.8498 (3.1650)	LR 7.813e-06
0: TRAIN [3][1760/5173]	Time 0.569 (0.608)	Data 1.39e-04 (4.68e-04)	Tok/s 18121 (23381)	Loss/tok 3.0234 (3.1656)	LR 7.813e-06
0: TRAIN [3][1770/5173]	Time 0.567 (0.608)	Data 1.34e-04 (4.66e-04)	Tok/s 18388 (23388)	Loss/tok 2.8777 (3.1658)	LR 7.813e-06
0: TRAIN [3][1780/5173]	Time 0.767 (0.608)	Data 1.28e-04 (4.64e-04)	Tok/s 39318 (23381)	Loss/tok 3.4794 (3.1656)	LR 7.813e-06
0: TRAIN [3][1790/5173]	Time 0.765 (0.608)	Data 3.03e-04 (4.62e-04)	Tok/s 38288 (23410)	Loss/tok 3.5807 (3.1669)	LR 7.813e-06
0: TRAIN [3][1800/5173]	Time 0.503 (0.608)	Data 1.29e-04 (4.61e-04)	Tok/s 10378 (23399)	Loss/tok 2.7059 (3.1667)	LR 7.813e-06
0: TRAIN [3][1810/5173]	Time 0.695 (0.608)	Data 1.26e-04 (4.59e-04)	Tok/s 33599 (23379)	Loss/tok 3.4002 (3.1665)	LR 7.813e-06
0: TRAIN [3][1820/5173]	Time 0.564 (0.608)	Data 1.42e-04 (4.57e-04)	Tok/s 18599 (23376)	Loss/tok 3.0405 (3.1662)	LR 7.813e-06
0: TRAIN [3][1830/5173]	Time 0.567 (0.608)	Data 1.33e-04 (4.55e-04)	Tok/s 18182 (23379)	Loss/tok 2.9694 (3.1659)	LR 7.813e-06
0: TRAIN [3][1840/5173]	Time 0.689 (0.608)	Data 1.27e-04 (4.54e-04)	Tok/s 34000 (23400)	Loss/tok 3.3350 (3.1665)	LR 7.813e-06
0: TRAIN [3][1850/5173]	Time 0.567 (0.608)	Data 1.38e-04 (4.52e-04)	Tok/s 18039 (23389)	Loss/tok 2.9835 (3.1664)	LR 7.813e-06
0: TRAIN [3][1860/5173]	Time 0.565 (0.608)	Data 3.09e-04 (4.50e-04)	Tok/s 18248 (23383)	Loss/tok 2.9014 (3.1663)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][1870/5173]	Time 0.567 (0.608)	Data 1.39e-04 (4.49e-04)	Tok/s 18268 (23404)	Loss/tok 2.9696 (3.1672)	LR 7.813e-06
0: TRAIN [3][1880/5173]	Time 0.624 (0.608)	Data 1.34e-04 (4.47e-04)	Tok/s 26656 (23411)	Loss/tok 3.2221 (3.1674)	LR 7.813e-06
0: TRAIN [3][1890/5173]	Time 0.625 (0.608)	Data 1.40e-04 (4.45e-04)	Tok/s 26956 (23431)	Loss/tok 3.2482 (3.1676)	LR 7.813e-06
0: TRAIN [3][1900/5173]	Time 0.692 (0.608)	Data 1.27e-04 (4.44e-04)	Tok/s 33645 (23422)	Loss/tok 3.2199 (3.1672)	LR 7.813e-06
0: TRAIN [3][1910/5173]	Time 0.626 (0.608)	Data 1.30e-04 (4.42e-04)	Tok/s 26847 (23420)	Loss/tok 3.1931 (3.1671)	LR 7.813e-06
0: TRAIN [3][1920/5173]	Time 0.619 (0.608)	Data 1.33e-04 (4.41e-04)	Tok/s 27140 (23428)	Loss/tok 3.1775 (3.1672)	LR 7.813e-06
0: TRAIN [3][1930/5173]	Time 0.630 (0.608)	Data 1.25e-04 (4.39e-04)	Tok/s 26939 (23426)	Loss/tok 3.1838 (3.1671)	LR 7.813e-06
0: TRAIN [3][1940/5173]	Time 0.614 (0.608)	Data 1.32e-04 (4.38e-04)	Tok/s 27157 (23415)	Loss/tok 3.1968 (3.1669)	LR 7.813e-06
0: TRAIN [3][1950/5173]	Time 0.628 (0.608)	Data 1.25e-04 (4.36e-04)	Tok/s 26526 (23403)	Loss/tok 3.1299 (3.1664)	LR 7.813e-06
0: TRAIN [3][1960/5173]	Time 0.487 (0.608)	Data 1.38e-04 (4.35e-04)	Tok/s 10872 (23402)	Loss/tok 2.6140 (3.1663)	LR 7.813e-06
0: TRAIN [3][1970/5173]	Time 0.627 (0.608)	Data 1.34e-04 (4.33e-04)	Tok/s 26934 (23396)	Loss/tok 3.0516 (3.1660)	LR 7.813e-06
0: TRAIN [3][1980/5173]	Time 0.567 (0.608)	Data 1.32e-04 (4.32e-04)	Tok/s 17751 (23387)	Loss/tok 2.9730 (3.1656)	LR 7.813e-06
0: TRAIN [3][1990/5173]	Time 0.631 (0.607)	Data 1.31e-04 (4.30e-04)	Tok/s 26811 (23375)	Loss/tok 3.0753 (3.1653)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][2000/5173]	Time 0.565 (0.607)	Data 1.39e-04 (4.29e-04)	Tok/s 17972 (23378)	Loss/tok 3.0134 (3.1657)	LR 7.813e-06
0: TRAIN [3][2010/5173]	Time 0.569 (0.607)	Data 1.33e-04 (4.27e-04)	Tok/s 18172 (23372)	Loss/tok 2.9937 (3.1654)	LR 7.813e-06
0: TRAIN [3][2020/5173]	Time 0.565 (0.607)	Data 1.33e-04 (4.26e-04)	Tok/s 18410 (23356)	Loss/tok 3.0256 (3.1651)	LR 7.813e-06
0: TRAIN [3][2030/5173]	Time 0.772 (0.607)	Data 1.42e-04 (4.25e-04)	Tok/s 38298 (23371)	Loss/tok 3.6382 (3.1658)	LR 7.813e-06
0: TRAIN [3][2040/5173]	Time 0.570 (0.607)	Data 1.32e-04 (4.23e-04)	Tok/s 17815 (23364)	Loss/tok 2.9461 (3.1657)	LR 7.813e-06
0: TRAIN [3][2050/5173]	Time 0.628 (0.607)	Data 1.33e-04 (4.22e-04)	Tok/s 26449 (23373)	Loss/tok 3.2475 (3.1659)	LR 7.813e-06
0: TRAIN [3][2060/5173]	Time 0.688 (0.607)	Data 1.31e-04 (4.20e-04)	Tok/s 33990 (23378)	Loss/tok 3.1441 (3.1660)	LR 7.813e-06
0: TRAIN [3][2070/5173]	Time 0.624 (0.607)	Data 1.33e-04 (4.19e-04)	Tok/s 26644 (23374)	Loss/tok 3.1830 (3.1659)	LR 7.813e-06
0: TRAIN [3][2080/5173]	Time 0.566 (0.607)	Data 1.32e-04 (4.18e-04)	Tok/s 18417 (23365)	Loss/tok 2.9656 (3.1654)	LR 7.813e-06
0: TRAIN [3][2090/5173]	Time 0.507 (0.607)	Data 1.28e-04 (4.16e-04)	Tok/s 10370 (23353)	Loss/tok 2.6156 (3.1652)	LR 7.813e-06
0: TRAIN [3][2100/5173]	Time 0.624 (0.607)	Data 1.34e-04 (4.15e-04)	Tok/s 26892 (23354)	Loss/tok 3.2183 (3.1651)	LR 7.813e-06
0: TRAIN [3][2110/5173]	Time 0.567 (0.607)	Data 1.39e-04 (4.14e-04)	Tok/s 17979 (23360)	Loss/tok 3.0879 (3.1652)	LR 7.813e-06
0: TRAIN [3][2120/5173]	Time 0.507 (0.607)	Data 1.35e-04 (4.12e-04)	Tok/s 10126 (23344)	Loss/tok 2.5261 (3.1648)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2130/5173]	Time 0.762 (0.607)	Data 1.63e-04 (4.11e-04)	Tok/s 39384 (23358)	Loss/tok 3.3855 (3.1654)	LR 7.813e-06
0: TRAIN [3][2140/5173]	Time 0.631 (0.607)	Data 1.41e-04 (4.10e-04)	Tok/s 26546 (23344)	Loss/tok 3.2266 (3.1653)	LR 7.813e-06
0: TRAIN [3][2150/5173]	Time 0.567 (0.607)	Data 1.30e-04 (4.09e-04)	Tok/s 18163 (23344)	Loss/tok 2.9456 (3.1653)	LR 7.813e-06
0: TRAIN [3][2160/5173]	Time 0.569 (0.607)	Data 1.36e-04 (4.07e-04)	Tok/s 18324 (23329)	Loss/tok 2.9329 (3.1648)	LR 7.813e-06
0: TRAIN [3][2170/5173]	Time 0.631 (0.607)	Data 1.49e-04 (4.06e-04)	Tok/s 26664 (23339)	Loss/tok 3.2740 (3.1651)	LR 7.813e-06
0: TRAIN [3][2180/5173]	Time 0.765 (0.607)	Data 1.32e-04 (4.05e-04)	Tok/s 39194 (23342)	Loss/tok 3.5427 (3.1651)	LR 7.813e-06
0: TRAIN [3][2190/5173]	Time 0.619 (0.607)	Data 1.33e-04 (4.04e-04)	Tok/s 26974 (23345)	Loss/tok 3.1865 (3.1652)	LR 7.813e-06
0: TRAIN [3][2200/5173]	Time 0.555 (0.607)	Data 1.32e-04 (4.03e-04)	Tok/s 18602 (23347)	Loss/tok 2.9575 (3.1651)	LR 7.813e-06
0: TRAIN [3][2210/5173]	Time 0.559 (0.607)	Data 1.34e-04 (4.01e-04)	Tok/s 18200 (23341)	Loss/tok 2.8780 (3.1650)	LR 7.813e-06
0: TRAIN [3][2220/5173]	Time 0.626 (0.607)	Data 1.26e-04 (4.00e-04)	Tok/s 26568 (23347)	Loss/tok 3.2322 (3.1651)	LR 7.813e-06
0: TRAIN [3][2230/5173]	Time 0.631 (0.607)	Data 1.34e-04 (3.99e-04)	Tok/s 26658 (23337)	Loss/tok 3.1124 (3.1647)	LR 7.813e-06
0: TRAIN [3][2240/5173]	Time 0.510 (0.607)	Data 1.32e-04 (3.98e-04)	Tok/s 10383 (23326)	Loss/tok 2.5407 (3.1642)	LR 7.813e-06
0: TRAIN [3][2250/5173]	Time 0.567 (0.607)	Data 1.33e-04 (3.97e-04)	Tok/s 18024 (23328)	Loss/tok 2.9999 (3.1641)	LR 7.813e-06
0: TRAIN [3][2260/5173]	Time 0.624 (0.607)	Data 1.36e-04 (3.96e-04)	Tok/s 26975 (23329)	Loss/tok 3.1220 (3.1637)	LR 7.813e-06
0: TRAIN [3][2270/5173]	Time 0.566 (0.607)	Data 1.29e-04 (3.95e-04)	Tok/s 18081 (23326)	Loss/tok 2.8980 (3.1635)	LR 7.813e-06
0: TRAIN [3][2280/5173]	Time 0.630 (0.607)	Data 1.27e-04 (3.94e-04)	Tok/s 26652 (23312)	Loss/tok 3.1694 (3.1630)	LR 7.813e-06
0: TRAIN [3][2290/5173]	Time 0.566 (0.607)	Data 1.36e-04 (3.92e-04)	Tok/s 18265 (23301)	Loss/tok 2.9853 (3.1627)	LR 7.813e-06
0: TRAIN [3][2300/5173]	Time 0.569 (0.607)	Data 1.31e-04 (3.91e-04)	Tok/s 18175 (23283)	Loss/tok 3.0682 (3.1622)	LR 7.813e-06
0: TRAIN [3][2310/5173]	Time 0.625 (0.607)	Data 1.32e-04 (3.90e-04)	Tok/s 26900 (23284)	Loss/tok 3.2234 (3.1624)	LR 7.813e-06
0: TRAIN [3][2320/5173]	Time 0.507 (0.606)	Data 1.36e-04 (3.89e-04)	Tok/s 10331 (23272)	Loss/tok 2.5633 (3.1622)	LR 7.813e-06
0: TRAIN [3][2330/5173]	Time 0.687 (0.606)	Data 1.31e-04 (3.88e-04)	Tok/s 34088 (23268)	Loss/tok 3.3644 (3.1623)	LR 7.813e-06
0: TRAIN [3][2340/5173]	Time 0.627 (0.606)	Data 1.30e-04 (3.87e-04)	Tok/s 26663 (23257)	Loss/tok 3.0916 (3.1619)	LR 7.813e-06
0: TRAIN [3][2350/5173]	Time 0.569 (0.606)	Data 1.29e-04 (3.86e-04)	Tok/s 18156 (23253)	Loss/tok 2.9741 (3.1617)	LR 7.813e-06
0: TRAIN [3][2360/5173]	Time 0.508 (0.606)	Data 1.36e-04 (3.85e-04)	Tok/s 10488 (23258)	Loss/tok 2.5136 (3.1619)	LR 7.813e-06
0: TRAIN [3][2370/5173]	Time 0.624 (0.606)	Data 1.27e-04 (3.84e-04)	Tok/s 27025 (23247)	Loss/tok 3.2361 (3.1617)	LR 7.813e-06
0: TRAIN [3][2380/5173]	Time 0.766 (0.606)	Data 1.32e-04 (3.83e-04)	Tok/s 38841 (23245)	Loss/tok 3.4132 (3.1617)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][2390/5173]	Time 0.761 (0.606)	Data 1.31e-04 (3.82e-04)	Tok/s 39535 (23257)	Loss/tok 3.4984 (3.1621)	LR 7.813e-06
0: TRAIN [3][2400/5173]	Time 0.568 (0.606)	Data 1.29e-04 (3.81e-04)	Tok/s 18314 (23246)	Loss/tok 2.8737 (3.1618)	LR 7.813e-06
0: TRAIN [3][2410/5173]	Time 0.566 (0.606)	Data 1.26e-04 (3.80e-04)	Tok/s 18149 (23232)	Loss/tok 2.9746 (3.1615)	LR 7.813e-06
0: TRAIN [3][2420/5173]	Time 0.569 (0.606)	Data 1.32e-04 (3.79e-04)	Tok/s 18034 (23228)	Loss/tok 2.9403 (3.1612)	LR 7.813e-06
0: TRAIN [3][2430/5173]	Time 0.630 (0.606)	Data 1.27e-04 (3.78e-04)	Tok/s 26502 (23231)	Loss/tok 3.1888 (3.1612)	LR 7.813e-06
0: TRAIN [3][2440/5173]	Time 0.691 (0.606)	Data 1.35e-04 (3.77e-04)	Tok/s 34025 (23233)	Loss/tok 3.2997 (3.1614)	LR 7.813e-06
0: TRAIN [3][2450/5173]	Time 0.569 (0.606)	Data 1.40e-04 (3.76e-04)	Tok/s 17954 (23238)	Loss/tok 3.0850 (3.1616)	LR 7.813e-06
0: TRAIN [3][2460/5173]	Time 0.630 (0.606)	Data 1.30e-04 (3.75e-04)	Tok/s 26846 (23231)	Loss/tok 3.0682 (3.1614)	LR 7.813e-06
0: TRAIN [3][2470/5173]	Time 0.691 (0.606)	Data 1.27e-04 (3.74e-04)	Tok/s 34090 (23242)	Loss/tok 3.4094 (3.1618)	LR 7.813e-06
0: TRAIN [3][2480/5173]	Time 0.568 (0.606)	Data 1.32e-04 (3.73e-04)	Tok/s 18079 (23234)	Loss/tok 2.8427 (3.1615)	LR 7.813e-06
0: TRAIN [3][2490/5173]	Time 0.566 (0.606)	Data 1.25e-04 (3.72e-04)	Tok/s 18027 (23239)	Loss/tok 2.9980 (3.1617)	LR 7.813e-06
0: TRAIN [3][2500/5173]	Time 0.566 (0.606)	Data 1.30e-04 (3.71e-04)	Tok/s 18043 (23237)	Loss/tok 2.8953 (3.1616)	LR 7.813e-06
0: TRAIN [3][2510/5173]	Time 0.629 (0.606)	Data 1.34e-04 (3.71e-04)	Tok/s 26863 (23238)	Loss/tok 3.1535 (3.1617)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][2520/5173]	Time 0.627 (0.606)	Data 2.79e-04 (3.70e-04)	Tok/s 27117 (23250)	Loss/tok 3.1041 (3.1618)	LR 7.813e-06
0: TRAIN [3][2530/5173]	Time 0.690 (0.606)	Data 1.40e-04 (3.69e-04)	Tok/s 34060 (23252)	Loss/tok 3.3129 (3.1618)	LR 7.813e-06
0: TRAIN [3][2540/5173]	Time 0.628 (0.606)	Data 1.39e-04 (3.68e-04)	Tok/s 26760 (23251)	Loss/tok 3.0555 (3.1617)	LR 7.813e-06
0: TRAIN [3][2550/5173]	Time 0.627 (0.606)	Data 1.34e-04 (3.67e-04)	Tok/s 27199 (23247)	Loss/tok 3.2391 (3.1616)	LR 7.813e-06
0: TRAIN [3][2560/5173]	Time 0.630 (0.606)	Data 1.28e-04 (3.66e-04)	Tok/s 26497 (23246)	Loss/tok 3.2132 (3.1614)	LR 7.813e-06
0: TRAIN [3][2570/5173]	Time 0.567 (0.606)	Data 1.32e-04 (3.65e-04)	Tok/s 18425 (23259)	Loss/tok 3.0008 (3.1618)	LR 7.813e-06
0: TRAIN [3][2580/5173]	Time 0.566 (0.606)	Data 1.28e-04 (3.64e-04)	Tok/s 18032 (23264)	Loss/tok 2.9493 (3.1621)	LR 7.813e-06
0: TRAIN [3][2590/5173]	Time 0.625 (0.606)	Data 1.25e-04 (3.64e-04)	Tok/s 26862 (23267)	Loss/tok 3.2489 (3.1623)	LR 7.813e-06
0: TRAIN [3][2600/5173]	Time 0.692 (0.606)	Data 1.25e-04 (3.63e-04)	Tok/s 33757 (23277)	Loss/tok 3.3383 (3.1624)	LR 7.813e-06
0: TRAIN [3][2610/5173]	Time 0.507 (0.606)	Data 1.32e-04 (3.62e-04)	Tok/s 10361 (23282)	Loss/tok 2.5311 (3.1626)	LR 7.813e-06
0: TRAIN [3][2620/5173]	Time 0.626 (0.606)	Data 1.29e-04 (3.61e-04)	Tok/s 26305 (23288)	Loss/tok 3.1616 (3.1626)	LR 7.813e-06
0: TRAIN [3][2630/5173]	Time 0.555 (0.606)	Data 2.90e-04 (3.60e-04)	Tok/s 18533 (23286)	Loss/tok 2.9123 (3.1624)	LR 7.813e-06
0: TRAIN [3][2640/5173]	Time 0.570 (0.606)	Data 1.33e-04 (3.59e-04)	Tok/s 17910 (23271)	Loss/tok 2.9612 (3.1620)	LR 7.813e-06
0: TRAIN [3][2650/5173]	Time 0.569 (0.606)	Data 1.30e-04 (3.59e-04)	Tok/s 18069 (23267)	Loss/tok 2.9183 (3.1619)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][2660/5173]	Time 0.564 (0.606)	Data 1.32e-04 (3.58e-04)	Tok/s 18236 (23286)	Loss/tok 2.9328 (3.1627)	LR 7.813e-06
0: TRAIN [3][2670/5173]	Time 0.623 (0.607)	Data 1.30e-04 (3.57e-04)	Tok/s 27148 (23292)	Loss/tok 3.1734 (3.1631)	LR 7.813e-06
0: TRAIN [3][2680/5173]	Time 0.565 (0.606)	Data 1.32e-04 (3.56e-04)	Tok/s 18110 (23287)	Loss/tok 2.9413 (3.1632)	LR 7.813e-06
0: TRAIN [3][2690/5173]	Time 0.694 (0.607)	Data 1.28e-04 (3.55e-04)	Tok/s 33395 (23303)	Loss/tok 3.3831 (3.1636)	LR 7.813e-06
0: TRAIN [3][2700/5173]	Time 0.692 (0.607)	Data 1.60e-04 (3.55e-04)	Tok/s 34018 (23301)	Loss/tok 3.2882 (3.1637)	LR 7.813e-06
0: TRAIN [3][2710/5173]	Time 0.688 (0.607)	Data 1.51e-04 (3.54e-04)	Tok/s 33848 (23301)	Loss/tok 3.2936 (3.1636)	LR 7.813e-06
0: TRAIN [3][2720/5173]	Time 0.567 (0.606)	Data 1.29e-04 (3.53e-04)	Tok/s 17672 (23285)	Loss/tok 3.0213 (3.1632)	LR 7.813e-06
0: TRAIN [3][2730/5173]	Time 0.567 (0.606)	Data 1.31e-04 (3.52e-04)	Tok/s 18357 (23282)	Loss/tok 3.0329 (3.1631)	LR 7.813e-06
0: TRAIN [3][2740/5173]	Time 0.567 (0.606)	Data 1.33e-04 (3.51e-04)	Tok/s 18232 (23282)	Loss/tok 2.9029 (3.1632)	LR 7.813e-06
0: TRAIN [3][2750/5173]	Time 0.507 (0.606)	Data 1.37e-04 (3.51e-04)	Tok/s 10502 (23284)	Loss/tok 2.6117 (3.1632)	LR 7.813e-06
0: TRAIN [3][2760/5173]	Time 0.504 (0.606)	Data 1.37e-04 (3.50e-04)	Tok/s 10366 (23271)	Loss/tok 2.4218 (3.1630)	LR 7.813e-06
0: TRAIN [3][2770/5173]	Time 0.758 (0.606)	Data 1.43e-04 (3.49e-04)	Tok/s 38795 (23280)	Loss/tok 3.6169 (3.1635)	LR 7.813e-06
0: TRAIN [3][2780/5173]	Time 0.567 (0.606)	Data 1.31e-04 (3.48e-04)	Tok/s 18281 (23256)	Loss/tok 3.0374 (3.1630)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][2790/5173]	Time 0.566 (0.606)	Data 1.40e-04 (3.48e-04)	Tok/s 18495 (23268)	Loss/tok 2.9860 (3.1632)	LR 7.813e-06
0: TRAIN [3][2800/5173]	Time 0.630 (0.606)	Data 1.33e-04 (3.47e-04)	Tok/s 26921 (23250)	Loss/tok 3.1111 (3.1628)	LR 7.813e-06
0: TRAIN [3][2810/5173]	Time 0.565 (0.606)	Data 1.31e-04 (3.46e-04)	Tok/s 18279 (23246)	Loss/tok 2.9662 (3.1627)	LR 7.813e-06
0: TRAIN [3][2820/5173]	Time 0.495 (0.606)	Data 1.43e-04 (3.46e-04)	Tok/s 10810 (23264)	Loss/tok 2.5871 (3.1630)	LR 7.813e-06
0: TRAIN [3][2830/5173]	Time 0.497 (0.606)	Data 1.68e-04 (3.45e-04)	Tok/s 10712 (23252)	Loss/tok 2.6031 (3.1628)	LR 7.813e-06
0: TRAIN [3][2840/5173]	Time 0.694 (0.606)	Data 3.40e-04 (3.44e-04)	Tok/s 33796 (23247)	Loss/tok 3.2338 (3.1627)	LR 7.813e-06
0: TRAIN [3][2850/5173]	Time 0.567 (0.606)	Data 2.08e-04 (3.44e-04)	Tok/s 18561 (23263)	Loss/tok 2.9661 (3.1632)	LR 7.813e-06
0: TRAIN [3][2860/5173]	Time 0.567 (0.606)	Data 1.58e-04 (3.43e-04)	Tok/s 18285 (23272)	Loss/tok 3.0284 (3.1634)	LR 7.813e-06
0: TRAIN [3][2870/5173]	Time 0.554 (0.606)	Data 1.40e-04 (3.42e-04)	Tok/s 18784 (23277)	Loss/tok 2.9354 (3.1635)	LR 7.813e-06
0: TRAIN [3][2880/5173]	Time 0.568 (0.606)	Data 1.36e-04 (3.42e-04)	Tok/s 18351 (23287)	Loss/tok 2.8175 (3.1636)	LR 7.813e-06
0: TRAIN [3][2890/5173]	Time 0.570 (0.606)	Data 1.31e-04 (3.41e-04)	Tok/s 18160 (23284)	Loss/tok 2.9139 (3.1635)	LR 7.813e-06
0: TRAIN [3][2900/5173]	Time 0.565 (0.606)	Data 1.21e-04 (3.40e-04)	Tok/s 18160 (23270)	Loss/tok 2.9649 (3.1632)	LR 7.813e-06
0: TRAIN [3][2910/5173]	Time 0.565 (0.606)	Data 1.18e-04 (3.39e-04)	Tok/s 18108 (23261)	Loss/tok 3.0047 (3.1628)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][2920/5173]	Time 0.692 (0.606)	Data 1.27e-04 (3.38e-04)	Tok/s 33716 (23268)	Loss/tok 3.2376 (3.1628)	LR 7.813e-06
0: TRAIN [3][2930/5173]	Time 0.692 (0.606)	Data 1.22e-04 (3.38e-04)	Tok/s 34332 (23270)	Loss/tok 3.2870 (3.1628)	LR 7.813e-06
0: TRAIN [3][2940/5173]	Time 0.566 (0.606)	Data 1.13e-04 (3.37e-04)	Tok/s 18162 (23267)	Loss/tok 2.9447 (3.1627)	LR 7.813e-06
0: TRAIN [3][2950/5173]	Time 0.566 (0.606)	Data 1.31e-04 (3.36e-04)	Tok/s 17980 (23273)	Loss/tok 2.9689 (3.1628)	LR 7.813e-06
0: TRAIN [3][2960/5173]	Time 0.566 (0.606)	Data 1.19e-04 (3.36e-04)	Tok/s 18321 (23264)	Loss/tok 2.9108 (3.1626)	LR 7.813e-06
0: TRAIN [3][2970/5173]	Time 0.689 (0.606)	Data 1.16e-04 (3.35e-04)	Tok/s 33921 (23272)	Loss/tok 3.4104 (3.1628)	LR 7.813e-06
0: TRAIN [3][2980/5173]	Time 0.567 (0.606)	Data 1.21e-04 (3.34e-04)	Tok/s 18421 (23266)	Loss/tok 3.0676 (3.1626)	LR 7.813e-06
0: TRAIN [3][2990/5173]	Time 0.627 (0.606)	Data 1.20e-04 (3.34e-04)	Tok/s 26727 (23261)	Loss/tok 3.0350 (3.1624)	LR 7.813e-06
0: TRAIN [3][3000/5173]	Time 0.765 (0.606)	Data 1.22e-04 (3.33e-04)	Tok/s 38440 (23267)	Loss/tok 3.4949 (3.1627)	LR 7.813e-06
0: TRAIN [3][3010/5173]	Time 0.566 (0.606)	Data 1.14e-04 (3.32e-04)	Tok/s 18449 (23270)	Loss/tok 2.9673 (3.1630)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3020/5173]	Time 0.568 (0.606)	Data 1.20e-04 (3.32e-04)	Tok/s 18315 (23276)	Loss/tok 2.9852 (3.1631)	LR 7.813e-06
0: TRAIN [3][3030/5173]	Time 0.569 (0.606)	Data 1.25e-04 (3.31e-04)	Tok/s 18151 (23278)	Loss/tok 2.9526 (3.1631)	LR 7.813e-06
0: TRAIN [3][3040/5173]	Time 0.626 (0.606)	Data 1.16e-04 (3.30e-04)	Tok/s 26961 (23268)	Loss/tok 3.0966 (3.1628)	LR 7.813e-06
0: TRAIN [3][3050/5173]	Time 0.632 (0.606)	Data 1.23e-04 (3.30e-04)	Tok/s 26469 (23271)	Loss/tok 3.0830 (3.1628)	LR 7.813e-06
0: TRAIN [3][3060/5173]	Time 0.510 (0.606)	Data 1.16e-04 (3.29e-04)	Tok/s 10436 (23271)	Loss/tok 2.5536 (3.1628)	LR 7.813e-06
0: TRAIN [3][3070/5173]	Time 0.629 (0.606)	Data 1.15e-04 (3.28e-04)	Tok/s 26556 (23276)	Loss/tok 3.1524 (3.1628)	LR 7.813e-06
0: TRAIN [3][3080/5173]	Time 0.628 (0.606)	Data 1.26e-04 (3.28e-04)	Tok/s 26675 (23290)	Loss/tok 3.0891 (3.1632)	LR 7.813e-06
0: TRAIN [3][3090/5173]	Time 0.566 (0.606)	Data 1.33e-04 (3.27e-04)	Tok/s 18301 (23283)	Loss/tok 3.0178 (3.1631)	LR 7.813e-06
0: TRAIN [3][3100/5173]	Time 0.569 (0.606)	Data 1.15e-04 (3.26e-04)	Tok/s 18428 (23282)	Loss/tok 3.0259 (3.1632)	LR 7.813e-06
0: TRAIN [3][3110/5173]	Time 0.568 (0.606)	Data 1.25e-04 (3.26e-04)	Tok/s 18227 (23280)	Loss/tok 2.8945 (3.1631)	LR 7.813e-06
0: TRAIN [3][3120/5173]	Time 0.569 (0.606)	Data 1.20e-04 (3.25e-04)	Tok/s 18312 (23274)	Loss/tok 3.0767 (3.1630)	LR 7.813e-06
0: TRAIN [3][3130/5173]	Time 0.687 (0.606)	Data 1.20e-04 (3.24e-04)	Tok/s 33718 (23267)	Loss/tok 3.3640 (3.1631)	LR 7.813e-06
0: TRAIN [3][3140/5173]	Time 0.504 (0.606)	Data 1.17e-04 (3.24e-04)	Tok/s 10386 (23262)	Loss/tok 2.6369 (3.1632)	LR 7.813e-06
0: TRAIN [3][3150/5173]	Time 0.568 (0.606)	Data 3.18e-04 (3.23e-04)	Tok/s 18152 (23266)	Loss/tok 2.8874 (3.1632)	LR 7.813e-06
0: TRAIN [3][3160/5173]	Time 0.567 (0.606)	Data 1.19e-04 (3.22e-04)	Tok/s 18063 (23259)	Loss/tok 3.0454 (3.1630)	LR 7.813e-06
0: TRAIN [3][3170/5173]	Time 0.628 (0.606)	Data 1.21e-04 (3.22e-04)	Tok/s 26639 (23256)	Loss/tok 3.2283 (3.1629)	LR 7.813e-06
0: TRAIN [3][3180/5173]	Time 0.635 (0.606)	Data 1.21e-04 (3.21e-04)	Tok/s 26779 (23260)	Loss/tok 3.0866 (3.1631)	LR 7.813e-06
0: TRAIN [3][3190/5173]	Time 0.567 (0.606)	Data 1.21e-04 (3.21e-04)	Tok/s 18466 (23252)	Loss/tok 2.9589 (3.1628)	LR 7.813e-06
0: TRAIN [3][3200/5173]	Time 0.692 (0.606)	Data 1.12e-04 (3.20e-04)	Tok/s 34159 (23251)	Loss/tok 3.4189 (3.1629)	LR 7.813e-06
0: TRAIN [3][3210/5173]	Time 0.630 (0.606)	Data 1.18e-04 (3.19e-04)	Tok/s 26195 (23259)	Loss/tok 3.1865 (3.1629)	LR 7.813e-06
0: TRAIN [3][3220/5173]	Time 0.564 (0.606)	Data 1.19e-04 (3.19e-04)	Tok/s 18377 (23249)	Loss/tok 2.9686 (3.1626)	LR 7.813e-06
0: TRAIN [3][3230/5173]	Time 0.631 (0.606)	Data 1.16e-04 (3.18e-04)	Tok/s 26682 (23244)	Loss/tok 3.1268 (3.1623)	LR 7.813e-06
0: TRAIN [3][3240/5173]	Time 0.567 (0.606)	Data 1.23e-04 (3.18e-04)	Tok/s 18282 (23244)	Loss/tok 2.9490 (3.1622)	LR 7.813e-06
0: TRAIN [3][3250/5173]	Time 0.566 (0.606)	Data 1.16e-04 (3.17e-04)	Tok/s 18382 (23236)	Loss/tok 2.9249 (3.1620)	LR 7.813e-06
0: TRAIN [3][3260/5173]	Time 0.566 (0.606)	Data 1.15e-04 (3.16e-04)	Tok/s 18146 (23228)	Loss/tok 3.0291 (3.1618)	LR 7.813e-06
0: TRAIN [3][3270/5173]	Time 0.624 (0.606)	Data 1.23e-04 (3.16e-04)	Tok/s 26885 (23235)	Loss/tok 3.1431 (3.1618)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][3280/5173]	Time 0.693 (0.606)	Data 1.22e-04 (3.15e-04)	Tok/s 33692 (23234)	Loss/tok 3.2742 (3.1617)	LR 7.813e-06
0: TRAIN [3][3290/5173]	Time 0.687 (0.606)	Data 1.18e-04 (3.15e-04)	Tok/s 34106 (23241)	Loss/tok 3.3354 (3.1621)	LR 7.813e-06
0: TRAIN [3][3300/5173]	Time 0.506 (0.606)	Data 1.29e-04 (3.14e-04)	Tok/s 10361 (23228)	Loss/tok 2.6227 (3.1618)	LR 7.813e-06
0: TRAIN [3][3310/5173]	Time 0.691 (0.606)	Data 3.18e-04 (3.14e-04)	Tok/s 33538 (23226)	Loss/tok 3.2864 (3.1617)	LR 7.813e-06
0: TRAIN [3][3320/5173]	Time 0.630 (0.606)	Data 1.20e-04 (3.13e-04)	Tok/s 26791 (23229)	Loss/tok 3.1971 (3.1616)	LR 7.813e-06
0: TRAIN [3][3330/5173]	Time 0.689 (0.606)	Data 1.23e-04 (3.12e-04)	Tok/s 33686 (23221)	Loss/tok 3.3825 (3.1616)	LR 7.813e-06
0: TRAIN [3][3340/5173]	Time 0.632 (0.606)	Data 1.20e-04 (3.12e-04)	Tok/s 26522 (23212)	Loss/tok 3.2603 (3.1613)	LR 7.813e-06
0: TRAIN [3][3350/5173]	Time 0.567 (0.606)	Data 1.29e-04 (3.11e-04)	Tok/s 18102 (23211)	Loss/tok 2.9928 (3.1612)	LR 7.813e-06
0: TRAIN [3][3360/5173]	Time 0.629 (0.606)	Data 1.22e-04 (3.11e-04)	Tok/s 27068 (23219)	Loss/tok 3.2618 (3.1613)	LR 7.813e-06
0: TRAIN [3][3370/5173]	Time 0.630 (0.606)	Data 1.26e-04 (3.10e-04)	Tok/s 26818 (23220)	Loss/tok 3.1275 (3.1614)	LR 7.813e-06
0: TRAIN [3][3380/5173]	Time 0.691 (0.606)	Data 1.18e-04 (3.10e-04)	Tok/s 34391 (23225)	Loss/tok 3.3061 (3.1613)	LR 7.813e-06
0: TRAIN [3][3390/5173]	Time 0.771 (0.606)	Data 1.24e-04 (3.09e-04)	Tok/s 38892 (23225)	Loss/tok 3.4570 (3.1614)	LR 7.813e-06
0: TRAIN [3][3400/5173]	Time 0.632 (0.606)	Data 1.67e-04 (3.09e-04)	Tok/s 26645 (23252)	Loss/tok 3.1564 (3.1624)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][3410/5173]	Time 0.627 (0.606)	Data 1.18e-04 (3.08e-04)	Tok/s 26841 (23259)	Loss/tok 3.1997 (3.1627)	LR 7.813e-06
0: TRAIN [3][3420/5173]	Time 0.688 (0.606)	Data 1.13e-04 (3.08e-04)	Tok/s 34063 (23251)	Loss/tok 3.3122 (3.1626)	LR 7.813e-06
0: TRAIN [3][3430/5173]	Time 0.571 (0.606)	Data 1.23e-04 (3.07e-04)	Tok/s 17806 (23237)	Loss/tok 2.9564 (3.1622)	LR 7.813e-06
0: TRAIN [3][3440/5173]	Time 0.697 (0.606)	Data 1.27e-04 (3.07e-04)	Tok/s 33750 (23250)	Loss/tok 3.3736 (3.1625)	LR 7.813e-06
0: TRAIN [3][3450/5173]	Time 0.567 (0.606)	Data 1.16e-04 (3.06e-04)	Tok/s 18299 (23256)	Loss/tok 2.9934 (3.1626)	LR 7.813e-06
0: TRAIN [3][3460/5173]	Time 0.623 (0.606)	Data 1.29e-04 (3.06e-04)	Tok/s 26916 (23265)	Loss/tok 3.1519 (3.1627)	LR 7.813e-06
0: TRAIN [3][3470/5173]	Time 0.567 (0.606)	Data 1.18e-04 (3.05e-04)	Tok/s 18256 (23267)	Loss/tok 3.0810 (3.1628)	LR 7.813e-06
0: TRAIN [3][3480/5173]	Time 0.569 (0.606)	Data 1.18e-04 (3.05e-04)	Tok/s 18092 (23268)	Loss/tok 3.1201 (3.1629)	LR 7.813e-06
0: TRAIN [3][3490/5173]	Time 0.566 (0.606)	Data 1.17e-04 (3.04e-04)	Tok/s 18362 (23261)	Loss/tok 2.9463 (3.1627)	LR 7.813e-06
0: TRAIN [3][3500/5173]	Time 0.623 (0.606)	Data 1.24e-04 (3.04e-04)	Tok/s 27415 (23263)	Loss/tok 3.1736 (3.1627)	LR 7.813e-06
0: TRAIN [3][3510/5173]	Time 0.690 (0.606)	Data 1.16e-04 (3.03e-04)	Tok/s 33796 (23260)	Loss/tok 3.4261 (3.1627)	LR 7.813e-06
0: TRAIN [3][3520/5173]	Time 0.626 (0.606)	Data 1.20e-04 (3.03e-04)	Tok/s 26495 (23261)	Loss/tok 3.0938 (3.1625)	LR 7.813e-06
0: TRAIN [3][3530/5173]	Time 0.627 (0.606)	Data 1.31e-04 (3.02e-04)	Tok/s 27331 (23270)	Loss/tok 3.0540 (3.1627)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][3540/5173]	Time 0.629 (0.606)	Data 1.20e-04 (3.02e-04)	Tok/s 26792 (23274)	Loss/tok 3.1413 (3.1627)	LR 7.813e-06
0: TRAIN [3][3550/5173]	Time 0.626 (0.606)	Data 1.21e-04 (3.01e-04)	Tok/s 26584 (23281)	Loss/tok 3.1088 (3.1626)	LR 7.813e-06
0: TRAIN [3][3560/5173]	Time 0.569 (0.606)	Data 1.21e-04 (3.01e-04)	Tok/s 17834 (23273)	Loss/tok 3.1613 (3.1624)	LR 7.813e-06
0: TRAIN [3][3570/5173]	Time 0.567 (0.606)	Data 1.22e-04 (3.00e-04)	Tok/s 17671 (23273)	Loss/tok 3.0432 (3.1626)	LR 7.813e-06
0: TRAIN [3][3580/5173]	Time 0.631 (0.606)	Data 1.21e-04 (3.00e-04)	Tok/s 26411 (23287)	Loss/tok 3.1203 (3.1627)	LR 7.813e-06
0: TRAIN [3][3590/5173]	Time 0.565 (0.606)	Data 1.22e-04 (2.99e-04)	Tok/s 18104 (23278)	Loss/tok 3.0574 (3.1624)	LR 7.813e-06
0: TRAIN [3][3600/5173]	Time 0.625 (0.606)	Data 1.20e-04 (2.99e-04)	Tok/s 27255 (23288)	Loss/tok 3.1059 (3.1625)	LR 7.813e-06
0: TRAIN [3][3610/5173]	Time 0.557 (0.606)	Data 1.18e-04 (2.98e-04)	Tok/s 18791 (23291)	Loss/tok 2.9755 (3.1625)	LR 7.813e-06
0: TRAIN [3][3620/5173]	Time 0.568 (0.606)	Data 1.17e-04 (2.98e-04)	Tok/s 18060 (23287)	Loss/tok 3.2200 (3.1626)	LR 7.813e-06
0: TRAIN [3][3630/5173]	Time 0.507 (0.606)	Data 1.19e-04 (2.97e-04)	Tok/s 10363 (23274)	Loss/tok 2.6736 (3.1624)	LR 7.813e-06
0: TRAIN [3][3640/5173]	Time 0.505 (0.606)	Data 1.14e-04 (2.97e-04)	Tok/s 10501 (23273)	Loss/tok 2.5796 (3.1624)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3650/5173]	Time 0.568 (0.606)	Data 1.37e-04 (2.96e-04)	Tok/s 18359 (23285)	Loss/tok 3.0085 (3.1627)	LR 7.813e-06
0: TRAIN [3][3660/5173]	Time 0.568 (0.606)	Data 1.19e-04 (2.96e-04)	Tok/s 17875 (23274)	Loss/tok 3.0805 (3.1624)	LR 7.813e-06
0: TRAIN [3][3670/5173]	Time 0.606 (0.606)	Data 1.20e-04 (2.95e-04)	Tok/s 27453 (23282)	Loss/tok 3.2048 (3.1627)	LR 7.813e-06
0: TRAIN [3][3680/5173]	Time 0.628 (0.606)	Data 1.24e-04 (2.95e-04)	Tok/s 26887 (23287)	Loss/tok 3.1076 (3.1628)	LR 7.813e-06
0: TRAIN [3][3690/5173]	Time 0.769 (0.606)	Data 1.22e-04 (2.94e-04)	Tok/s 38862 (23281)	Loss/tok 3.3997 (3.1628)	LR 7.813e-06
0: TRAIN [3][3700/5173]	Time 0.762 (0.606)	Data 1.23e-04 (2.94e-04)	Tok/s 38882 (23283)	Loss/tok 3.5093 (3.1630)	LR 7.813e-06
0: TRAIN [3][3710/5173]	Time 0.692 (0.606)	Data 1.20e-04 (2.94e-04)	Tok/s 34079 (23280)	Loss/tok 3.4290 (3.1630)	LR 7.813e-06
0: TRAIN [3][3720/5173]	Time 0.562 (0.606)	Data 1.23e-04 (2.93e-04)	Tok/s 18241 (23287)	Loss/tok 3.0324 (3.1632)	LR 7.813e-06
0: TRAIN [3][3730/5173]	Time 0.626 (0.606)	Data 1.28e-04 (2.93e-04)	Tok/s 26846 (23297)	Loss/tok 3.0909 (3.1634)	LR 7.813e-06
0: TRAIN [3][3740/5173]	Time 0.568 (0.606)	Data 1.18e-04 (2.92e-04)	Tok/s 18073 (23302)	Loss/tok 2.9278 (3.1635)	LR 7.813e-06
0: TRAIN [3][3750/5173]	Time 0.629 (0.606)	Data 1.21e-04 (2.92e-04)	Tok/s 26732 (23299)	Loss/tok 3.1043 (3.1633)	LR 7.813e-06
0: TRAIN [3][3760/5173]	Time 0.568 (0.606)	Data 1.18e-04 (2.91e-04)	Tok/s 18213 (23301)	Loss/tok 2.8919 (3.1632)	LR 7.813e-06
0: TRAIN [3][3770/5173]	Time 0.567 (0.607)	Data 1.12e-04 (2.91e-04)	Tok/s 18576 (23307)	Loss/tok 3.0673 (3.1633)	LR 7.813e-06
0: TRAIN [3][3780/5173]	Time 0.505 (0.606)	Data 1.21e-04 (2.90e-04)	Tok/s 10258 (23299)	Loss/tok 2.5972 (3.1629)	LR 7.813e-06
0: TRAIN [3][3790/5173]	Time 0.629 (0.606)	Data 1.12e-04 (2.90e-04)	Tok/s 27007 (23305)	Loss/tok 3.0342 (3.1629)	LR 7.813e-06
0: TRAIN [3][3800/5173]	Time 0.759 (0.606)	Data 1.31e-04 (2.90e-04)	Tok/s 39111 (23303)	Loss/tok 3.4722 (3.1630)	LR 7.813e-06
0: TRAIN [3][3810/5173]	Time 0.567 (0.607)	Data 1.49e-04 (2.89e-04)	Tok/s 18269 (23311)	Loss/tok 2.9511 (3.1632)	LR 7.813e-06
0: TRAIN [3][3820/5173]	Time 0.569 (0.606)	Data 1.18e-04 (2.89e-04)	Tok/s 18268 (23307)	Loss/tok 2.9291 (3.1630)	LR 7.813e-06
0: TRAIN [3][3830/5173]	Time 0.551 (0.606)	Data 1.16e-04 (2.88e-04)	Tok/s 18656 (23298)	Loss/tok 2.9310 (3.1629)	LR 7.813e-06
0: TRAIN [3][3840/5173]	Time 0.763 (0.606)	Data 1.27e-04 (2.88e-04)	Tok/s 39226 (23305)	Loss/tok 3.5404 (3.1630)	LR 7.813e-06
0: TRAIN [3][3850/5173]	Time 0.567 (0.607)	Data 1.28e-04 (2.87e-04)	Tok/s 18570 (23309)	Loss/tok 2.9327 (3.1629)	LR 7.813e-06
0: TRAIN [3][3860/5173]	Time 0.633 (0.607)	Data 1.41e-04 (2.87e-04)	Tok/s 26282 (23308)	Loss/tok 3.1720 (3.1627)	LR 7.813e-06
0: TRAIN [3][3870/5173]	Time 0.632 (0.607)	Data 1.59e-04 (2.87e-04)	Tok/s 26806 (23308)	Loss/tok 3.1760 (3.1626)	LR 7.813e-06
0: TRAIN [3][3880/5173]	Time 0.690 (0.607)	Data 1.50e-04 (2.86e-04)	Tok/s 33698 (23314)	Loss/tok 3.3619 (3.1629)	LR 7.813e-06
0: TRAIN [3][3890/5173]	Time 0.623 (0.607)	Data 1.48e-04 (2.86e-04)	Tok/s 27277 (23314)	Loss/tok 3.1840 (3.1629)	LR 7.813e-06
0: TRAIN [3][3900/5173]	Time 0.567 (0.607)	Data 1.31e-04 (2.85e-04)	Tok/s 17841 (23310)	Loss/tok 3.0096 (3.1626)	LR 7.813e-06
0: TRAIN [3][3910/5173]	Time 0.567 (0.606)	Data 1.36e-04 (2.85e-04)	Tok/s 18639 (23300)	Loss/tok 3.0183 (3.1624)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][3920/5173]	Time 0.624 (0.606)	Data 1.42e-04 (2.85e-04)	Tok/s 26830 (23305)	Loss/tok 3.1520 (3.1625)	LR 7.813e-06
0: TRAIN [3][3930/5173]	Time 0.626 (0.606)	Data 1.43e-04 (2.84e-04)	Tok/s 26878 (23306)	Loss/tok 3.2199 (3.1625)	LR 7.813e-06
0: TRAIN [3][3940/5173]	Time 0.567 (0.606)	Data 3.26e-04 (2.84e-04)	Tok/s 18236 (23300)	Loss/tok 2.9213 (3.1624)	LR 7.813e-06
0: TRAIN [3][3950/5173]	Time 0.566 (0.606)	Data 1.20e-04 (2.83e-04)	Tok/s 18048 (23296)	Loss/tok 2.9999 (3.1623)	LR 7.813e-06
0: TRAIN [3][3960/5173]	Time 0.567 (0.606)	Data 1.27e-04 (2.83e-04)	Tok/s 18438 (23290)	Loss/tok 2.9678 (3.1621)	LR 7.813e-06
0: TRAIN [3][3970/5173]	Time 0.755 (0.606)	Data 1.25e-04 (2.83e-04)	Tok/s 39467 (23291)	Loss/tok 3.5308 (3.1623)	LR 7.813e-06
0: TRAIN [3][3980/5173]	Time 0.565 (0.606)	Data 1.30e-04 (2.82e-04)	Tok/s 17858 (23288)	Loss/tok 2.9513 (3.1622)	LR 7.813e-06
0: TRAIN [3][3990/5173]	Time 0.568 (0.606)	Data 1.50e-04 (2.82e-04)	Tok/s 17663 (23290)	Loss/tok 3.0408 (3.1622)	LR 7.813e-06
0: TRAIN [3][4000/5173]	Time 0.689 (0.606)	Data 1.64e-04 (2.82e-04)	Tok/s 34263 (23290)	Loss/tok 3.2468 (3.1621)	LR 7.813e-06
0: TRAIN [3][4010/5173]	Time 0.505 (0.606)	Data 3.24e-04 (2.81e-04)	Tok/s 10520 (23282)	Loss/tok 2.5260 (3.1619)	LR 7.813e-06
0: TRAIN [3][4020/5173]	Time 0.689 (0.606)	Data 1.33e-04 (2.81e-04)	Tok/s 33833 (23283)	Loss/tok 3.2967 (3.1621)	LR 7.813e-06
0: TRAIN [3][4030/5173]	Time 0.689 (0.606)	Data 3.14e-04 (2.80e-04)	Tok/s 33825 (23282)	Loss/tok 3.4297 (3.1621)	LR 7.813e-06
0: TRAIN [3][4040/5173]	Time 0.506 (0.606)	Data 1.19e-04 (2.80e-04)	Tok/s 10518 (23274)	Loss/tok 2.5915 (3.1619)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][4050/5173]	Time 0.565 (0.606)	Data 1.38e-04 (2.80e-04)	Tok/s 18419 (23274)	Loss/tok 2.9743 (3.1621)	LR 7.813e-06
0: TRAIN [3][4060/5173]	Time 0.696 (0.606)	Data 1.34e-04 (2.79e-04)	Tok/s 33646 (23278)	Loss/tok 3.2332 (3.1620)	LR 7.813e-06
0: TRAIN [3][4070/5173]	Time 0.690 (0.606)	Data 1.22e-04 (2.79e-04)	Tok/s 34042 (23281)	Loss/tok 3.3267 (3.1621)	LR 7.813e-06
0: TRAIN [3][4080/5173]	Time 0.631 (0.606)	Data 1.22e-04 (2.79e-04)	Tok/s 26638 (23292)	Loss/tok 3.1912 (3.1624)	LR 7.813e-06
0: TRAIN [3][4090/5173]	Time 0.567 (0.606)	Data 1.20e-04 (2.78e-04)	Tok/s 18362 (23286)	Loss/tok 2.8980 (3.1623)	LR 7.813e-06
0: TRAIN [3][4100/5173]	Time 0.626 (0.606)	Data 1.23e-04 (2.78e-04)	Tok/s 26741 (23291)	Loss/tok 3.1528 (3.1625)	LR 7.813e-06
0: TRAIN [3][4110/5173]	Time 0.627 (0.606)	Data 1.19e-04 (2.77e-04)	Tok/s 26761 (23294)	Loss/tok 3.1557 (3.1625)	LR 7.813e-06
0: TRAIN [3][4120/5173]	Time 0.564 (0.606)	Data 1.16e-04 (2.77e-04)	Tok/s 18205 (23293)	Loss/tok 3.0723 (3.1626)	LR 7.813e-06
0: TRAIN [3][4130/5173]	Time 0.688 (0.606)	Data 2.89e-04 (2.77e-04)	Tok/s 34331 (23289)	Loss/tok 3.4117 (3.1624)	LR 7.813e-06
0: TRAIN [3][4140/5173]	Time 0.569 (0.606)	Data 1.78e-04 (2.76e-04)	Tok/s 18234 (23288)	Loss/tok 2.9580 (3.1625)	LR 7.813e-06
0: TRAIN [3][4150/5173]	Time 0.492 (0.606)	Data 1.14e-04 (2.76e-04)	Tok/s 10490 (23293)	Loss/tok 2.5635 (3.1626)	LR 7.813e-06
0: TRAIN [3][4160/5173]	Time 0.558 (0.606)	Data 1.24e-04 (2.76e-04)	Tok/s 18405 (23297)	Loss/tok 3.0425 (3.1628)	LR 7.813e-06
0: TRAIN [3][4170/5173]	Time 0.506 (0.606)	Data 1.19e-04 (2.75e-04)	Tok/s 10354 (23296)	Loss/tok 2.5545 (3.1629)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][4180/5173]	Time 0.692 (0.606)	Data 1.17e-04 (2.75e-04)	Tok/s 33985 (23289)	Loss/tok 3.3783 (3.1627)	LR 7.813e-06
0: TRAIN [3][4190/5173]	Time 0.569 (0.606)	Data 1.22e-04 (2.75e-04)	Tok/s 17774 (23289)	Loss/tok 3.0604 (3.1628)	LR 7.813e-06
0: TRAIN [3][4200/5173]	Time 0.630 (0.606)	Data 1.21e-04 (2.74e-04)	Tok/s 26696 (23290)	Loss/tok 3.2578 (3.1629)	LR 7.813e-06
0: TRAIN [3][4210/5173]	Time 0.630 (0.606)	Data 1.16e-04 (2.74e-04)	Tok/s 26552 (23290)	Loss/tok 3.1349 (3.1629)	LR 7.813e-06
0: TRAIN [3][4220/5173]	Time 0.568 (0.606)	Data 1.21e-04 (2.74e-04)	Tok/s 18122 (23284)	Loss/tok 2.9176 (3.1627)	LR 7.813e-06
0: TRAIN [3][4230/5173]	Time 0.746 (0.606)	Data 1.20e-04 (2.73e-04)	Tok/s 40064 (23291)	Loss/tok 3.3792 (3.1628)	LR 7.813e-06
0: TRAIN [3][4240/5173]	Time 0.568 (0.606)	Data 1.18e-04 (2.73e-04)	Tok/s 18275 (23292)	Loss/tok 3.0594 (3.1629)	LR 7.813e-06
0: TRAIN [3][4250/5173]	Time 0.566 (0.606)	Data 1.15e-04 (2.73e-04)	Tok/s 18266 (23293)	Loss/tok 2.9522 (3.1628)	LR 7.813e-06
0: TRAIN [3][4260/5173]	Time 0.614 (0.606)	Data 1.17e-04 (2.72e-04)	Tok/s 27395 (23286)	Loss/tok 3.1982 (3.1627)	LR 7.813e-06
0: TRAIN [3][4270/5173]	Time 0.693 (0.606)	Data 1.17e-04 (2.72e-04)	Tok/s 34557 (23284)	Loss/tok 3.2923 (3.1626)	LR 7.813e-06
0: TRAIN [3][4280/5173]	Time 0.631 (0.606)	Data 1.19e-04 (2.72e-04)	Tok/s 26890 (23275)	Loss/tok 3.1661 (3.1625)	LR 7.813e-06
0: TRAIN [3][4290/5173]	Time 0.508 (0.606)	Data 1.20e-04 (2.71e-04)	Tok/s 10476 (23277)	Loss/tok 2.5625 (3.1627)	LR 7.813e-06
0: TRAIN [3][4300/5173]	Time 0.625 (0.606)	Data 1.18e-04 (2.71e-04)	Tok/s 26612 (23275)	Loss/tok 3.2121 (3.1626)	LR 7.813e-06
0: TRAIN [3][4310/5173]	Time 0.629 (0.606)	Data 1.25e-04 (2.71e-04)	Tok/s 26777 (23276)	Loss/tok 3.2827 (3.1627)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][4320/5173]	Time 0.691 (0.606)	Data 1.22e-04 (2.70e-04)	Tok/s 34377 (23276)	Loss/tok 3.3333 (3.1627)	LR 7.813e-06
0: TRAIN [3][4330/5173]	Time 0.632 (0.606)	Data 1.28e-04 (2.70e-04)	Tok/s 26635 (23277)	Loss/tok 3.1320 (3.1626)	LR 7.813e-06
0: TRAIN [3][4340/5173]	Time 0.688 (0.606)	Data 1.13e-04 (2.70e-04)	Tok/s 34094 (23284)	Loss/tok 3.3548 (3.1627)	LR 7.813e-06
0: TRAIN [3][4350/5173]	Time 0.628 (0.606)	Data 1.26e-04 (2.70e-04)	Tok/s 27102 (23284)	Loss/tok 3.0614 (3.1628)	LR 7.813e-06
0: TRAIN [3][4360/5173]	Time 0.630 (0.606)	Data 1.09e-04 (2.69e-04)	Tok/s 26732 (23286)	Loss/tok 3.2140 (3.1628)	LR 7.813e-06
0: TRAIN [3][4370/5173]	Time 0.568 (0.606)	Data 1.15e-04 (2.69e-04)	Tok/s 18287 (23289)	Loss/tok 2.9800 (3.1629)	LR 7.813e-06
0: TRAIN [3][4380/5173]	Time 0.568 (0.606)	Data 1.17e-04 (2.69e-04)	Tok/s 18291 (23282)	Loss/tok 3.0013 (3.1627)	LR 7.813e-06
0: TRAIN [3][4390/5173]	Time 0.763 (0.606)	Data 1.27e-04 (2.68e-04)	Tok/s 39075 (23294)	Loss/tok 3.4648 (3.1630)	LR 7.813e-06
0: TRAIN [3][4400/5173]	Time 0.571 (0.606)	Data 1.30e-04 (2.68e-04)	Tok/s 17859 (23302)	Loss/tok 3.0469 (3.1632)	LR 7.813e-06
0: TRAIN [3][4410/5173]	Time 0.569 (0.606)	Data 1.30e-04 (2.68e-04)	Tok/s 18088 (23297)	Loss/tok 2.9655 (3.1631)	LR 7.813e-06
0: TRAIN [3][4420/5173]	Time 0.568 (0.606)	Data 1.28e-04 (2.67e-04)	Tok/s 18028 (23297)	Loss/tok 2.9344 (3.1631)	LR 7.813e-06
0: TRAIN [3][4430/5173]	Time 0.507 (0.606)	Data 1.22e-04 (2.67e-04)	Tok/s 10359 (23291)	Loss/tok 2.5768 (3.1629)	LR 7.813e-06
0: TRAIN [3][4440/5173]	Time 0.629 (0.606)	Data 1.25e-04 (2.67e-04)	Tok/s 26682 (23295)	Loss/tok 3.1084 (3.1629)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][4450/5173]	Time 0.688 (0.606)	Data 1.23e-04 (2.66e-04)	Tok/s 33753 (23295)	Loss/tok 3.3705 (3.1628)	LR 7.813e-06
0: TRAIN [3][4460/5173]	Time 0.763 (0.606)	Data 1.24e-04 (2.66e-04)	Tok/s 39235 (23300)	Loss/tok 3.4706 (3.1631)	LR 7.813e-06
0: TRAIN [3][4470/5173]	Time 0.567 (0.606)	Data 5.68e-04 (2.66e-04)	Tok/s 18180 (23292)	Loss/tok 2.9539 (3.1629)	LR 7.813e-06
0: TRAIN [3][4480/5173]	Time 0.568 (0.606)	Data 1.34e-04 (2.66e-04)	Tok/s 17948 (23279)	Loss/tok 3.0308 (3.1626)	LR 7.813e-06
0: TRAIN [3][4490/5173]	Time 0.568 (0.606)	Data 1.20e-04 (2.65e-04)	Tok/s 18628 (23279)	Loss/tok 2.8876 (3.1626)	LR 7.813e-06
0: TRAIN [3][4500/5173]	Time 0.767 (0.606)	Data 1.24e-04 (2.65e-04)	Tok/s 38382 (23276)	Loss/tok 3.5424 (3.1626)	LR 7.813e-06
0: TRAIN [3][4510/5173]	Time 0.694 (0.606)	Data 1.21e-04 (2.65e-04)	Tok/s 33831 (23281)	Loss/tok 3.3207 (3.1627)	LR 7.813e-06
0: TRAIN [3][4520/5173]	Time 0.631 (0.606)	Data 1.24e-04 (2.64e-04)	Tok/s 26196 (23270)	Loss/tok 3.1523 (3.1625)	LR 7.813e-06
0: TRAIN [3][4530/5173]	Time 0.692 (0.606)	Data 1.16e-04 (2.64e-04)	Tok/s 34246 (23269)	Loss/tok 3.2795 (3.1625)	LR 7.813e-06
0: TRAIN [3][4540/5173]	Time 0.569 (0.606)	Data 1.39e-04 (2.64e-04)	Tok/s 18284 (23277)	Loss/tok 2.8627 (3.1628)	LR 7.813e-06
0: TRAIN [3][4550/5173]	Time 0.634 (0.606)	Data 1.26e-04 (2.63e-04)	Tok/s 26612 (23280)	Loss/tok 3.1878 (3.1629)	LR 7.813e-06
0: TRAIN [3][4560/5173]	Time 0.568 (0.606)	Data 1.29e-04 (2.63e-04)	Tok/s 17983 (23280)	Loss/tok 2.8512 (3.1628)	LR 7.813e-06
0: TRAIN [3][4570/5173]	Time 0.692 (0.606)	Data 1.23e-04 (2.63e-04)	Tok/s 33546 (23281)	Loss/tok 3.3023 (3.1628)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][4580/5173]	Time 0.688 (0.606)	Data 1.24e-04 (2.63e-04)	Tok/s 33875 (23280)	Loss/tok 3.4364 (3.1628)	LR 7.813e-06
0: TRAIN [3][4590/5173]	Time 0.506 (0.606)	Data 1.23e-04 (2.62e-04)	Tok/s 10599 (23279)	Loss/tok 2.5121 (3.1628)	LR 7.813e-06
0: TRAIN [3][4600/5173]	Time 0.631 (0.606)	Data 1.24e-04 (2.62e-04)	Tok/s 26559 (23282)	Loss/tok 3.1737 (3.1628)	LR 7.813e-06
0: TRAIN [3][4610/5173]	Time 0.568 (0.606)	Data 1.21e-04 (2.62e-04)	Tok/s 18233 (23268)	Loss/tok 2.9957 (3.1625)	LR 7.813e-06
0: TRAIN [3][4620/5173]	Time 0.626 (0.606)	Data 1.37e-04 (2.61e-04)	Tok/s 26701 (23274)	Loss/tok 3.1952 (3.1625)	LR 7.813e-06
0: TRAIN [3][4630/5173]	Time 0.566 (0.606)	Data 1.26e-04 (2.61e-04)	Tok/s 17997 (23274)	Loss/tok 2.9549 (3.1625)	LR 7.813e-06
0: TRAIN [3][4640/5173]	Time 0.564 (0.606)	Data 1.22e-04 (2.61e-04)	Tok/s 18052 (23273)	Loss/tok 2.9244 (3.1624)	LR 7.813e-06
0: TRAIN [3][4650/5173]	Time 0.691 (0.606)	Data 1.19e-04 (2.61e-04)	Tok/s 33249 (23270)	Loss/tok 3.3595 (3.1623)	LR 7.813e-06
0: TRAIN [3][4660/5173]	Time 0.506 (0.606)	Data 1.21e-04 (2.60e-04)	Tok/s 10660 (23275)	Loss/tok 2.6645 (3.1625)	LR 7.813e-06
0: TRAIN [3][4670/5173]	Time 0.617 (0.606)	Data 1.28e-04 (2.60e-04)	Tok/s 26797 (23273)	Loss/tok 3.3160 (3.1624)	LR 7.813e-06
0: TRAIN [3][4680/5173]	Time 0.541 (0.606)	Data 1.20e-04 (2.60e-04)	Tok/s 19666 (23266)	Loss/tok 2.9379 (3.1621)	LR 7.813e-06
0: TRAIN [3][4690/5173]	Time 0.567 (0.606)	Data 1.23e-04 (2.59e-04)	Tok/s 18196 (23256)	Loss/tok 2.9348 (3.1619)	LR 7.813e-06
0: TRAIN [3][4700/5173]	Time 0.693 (0.606)	Data 1.25e-04 (2.59e-04)	Tok/s 33445 (23261)	Loss/tok 3.3709 (3.1621)	LR 7.813e-06
0: TRAIN [3][4710/5173]	Time 0.623 (0.606)	Data 1.36e-04 (2.59e-04)	Tok/s 27100 (23261)	Loss/tok 3.1438 (3.1620)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][4720/5173]	Time 0.566 (0.606)	Data 1.20e-04 (2.59e-04)	Tok/s 18020 (23260)	Loss/tok 2.9700 (3.1619)	LR 7.813e-06
0: TRAIN [3][4730/5173]	Time 0.568 (0.606)	Data 1.29e-04 (2.58e-04)	Tok/s 18053 (23261)	Loss/tok 2.9743 (3.1620)	LR 7.813e-06
0: TRAIN [3][4740/5173]	Time 0.626 (0.606)	Data 1.21e-04 (2.58e-04)	Tok/s 26886 (23263)	Loss/tok 3.2112 (3.1619)	LR 7.813e-06
0: TRAIN [3][4750/5173]	Time 0.627 (0.606)	Data 1.20e-04 (2.58e-04)	Tok/s 26525 (23257)	Loss/tok 3.1785 (3.1617)	LR 7.813e-06
0: TRAIN [3][4760/5173]	Time 0.633 (0.606)	Data 1.26e-04 (2.58e-04)	Tok/s 26408 (23258)	Loss/tok 3.1778 (3.1618)	LR 7.813e-06
0: TRAIN [3][4770/5173]	Time 0.633 (0.606)	Data 1.23e-04 (2.57e-04)	Tok/s 26629 (23263)	Loss/tok 3.2146 (3.1619)	LR 7.813e-06
0: TRAIN [3][4780/5173]	Time 0.629 (0.606)	Data 1.21e-04 (2.57e-04)	Tok/s 26729 (23265)	Loss/tok 3.1076 (3.1619)	LR 7.813e-06
0: TRAIN [3][4790/5173]	Time 0.567 (0.606)	Data 1.25e-04 (2.57e-04)	Tok/s 18176 (23265)	Loss/tok 2.9918 (3.1619)	LR 7.813e-06
0: TRAIN [3][4800/5173]	Time 0.629 (0.606)	Data 1.42e-04 (2.56e-04)	Tok/s 26343 (23272)	Loss/tok 3.2341 (3.1621)	LR 7.813e-06
0: TRAIN [3][4810/5173]	Time 0.689 (0.606)	Data 1.21e-04 (2.56e-04)	Tok/s 33863 (23270)	Loss/tok 3.3116 (3.1620)	LR 7.813e-06
0: TRAIN [3][4820/5173]	Time 0.568 (0.606)	Data 1.35e-04 (2.56e-04)	Tok/s 18528 (23275)	Loss/tok 2.9427 (3.1621)	LR 7.813e-06
0: TRAIN [3][4830/5173]	Time 0.693 (0.606)	Data 1.32e-04 (2.56e-04)	Tok/s 33810 (23280)	Loss/tok 3.3324 (3.1623)	LR 7.813e-06
0: TRAIN [3][4840/5173]	Time 0.565 (0.606)	Data 1.20e-04 (2.55e-04)	Tok/s 18256 (23285)	Loss/tok 2.8690 (3.1623)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][4850/5173]	Time 0.565 (0.606)	Data 1.23e-04 (2.55e-04)	Tok/s 18315 (23291)	Loss/tok 3.0587 (3.1625)	LR 7.813e-06
0: TRAIN [3][4860/5173]	Time 0.568 (0.606)	Data 1.25e-04 (2.55e-04)	Tok/s 18122 (23292)	Loss/tok 3.1328 (3.1625)	LR 7.813e-06
0: TRAIN [3][4870/5173]	Time 0.565 (0.606)	Data 1.22e-04 (2.55e-04)	Tok/s 18719 (23295)	Loss/tok 2.9700 (3.1625)	LR 7.813e-06
0: TRAIN [3][4880/5173]	Time 0.571 (0.606)	Data 1.34e-04 (2.54e-04)	Tok/s 18002 (23295)	Loss/tok 3.0611 (3.1625)	LR 7.813e-06
0: TRAIN [3][4890/5173]	Time 0.627 (0.607)	Data 1.24e-04 (2.54e-04)	Tok/s 26408 (23300)	Loss/tok 3.1245 (3.1626)	LR 7.813e-06
0: TRAIN [3][4900/5173]	Time 0.624 (0.607)	Data 1.31e-04 (2.54e-04)	Tok/s 27165 (23304)	Loss/tok 3.0734 (3.1626)	LR 7.813e-06
0: TRAIN [3][4910/5173]	Time 0.565 (0.607)	Data 1.21e-04 (2.54e-04)	Tok/s 18163 (23307)	Loss/tok 2.9288 (3.1627)	LR 7.813e-06
0: TRAIN [3][4920/5173]	Time 0.625 (0.607)	Data 2.88e-04 (2.53e-04)	Tok/s 26404 (23312)	Loss/tok 3.2438 (3.1630)	LR 7.813e-06
0: TRAIN [3][4930/5173]	Time 0.570 (0.607)	Data 1.20e-04 (2.53e-04)	Tok/s 17830 (23301)	Loss/tok 2.9395 (3.1628)	LR 7.813e-06
0: TRAIN [3][4940/5173]	Time 0.568 (0.606)	Data 1.22e-04 (2.53e-04)	Tok/s 17948 (23297)	Loss/tok 2.9210 (3.1626)	LR 7.813e-06
0: TRAIN [3][4950/5173]	Time 0.564 (0.606)	Data 1.26e-04 (2.53e-04)	Tok/s 18213 (23295)	Loss/tok 2.9196 (3.1626)	LR 7.813e-06
0: TRAIN [3][4960/5173]	Time 0.569 (0.606)	Data 1.24e-04 (2.52e-04)	Tok/s 18035 (23296)	Loss/tok 2.9255 (3.1626)	LR 7.813e-06
0: TRAIN [3][4970/5173]	Time 0.685 (0.606)	Data 1.30e-04 (2.52e-04)	Tok/s 34055 (23292)	Loss/tok 3.3205 (3.1626)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][4980/5173]	Time 0.567 (0.606)	Data 1.26e-04 (2.52e-04)	Tok/s 18357 (23291)	Loss/tok 3.0850 (3.1627)	LR 7.813e-06
0: TRAIN [3][4990/5173]	Time 0.632 (0.606)	Data 1.25e-04 (2.52e-04)	Tok/s 26689 (23295)	Loss/tok 3.0540 (3.1627)	LR 7.813e-06
0: TRAIN [3][5000/5173]	Time 0.630 (0.607)	Data 1.22e-04 (2.52e-04)	Tok/s 26347 (23306)	Loss/tok 3.1342 (3.1630)	LR 7.813e-06
0: TRAIN [3][5010/5173]	Time 0.626 (0.607)	Data 1.25e-04 (2.51e-04)	Tok/s 26659 (23305)	Loss/tok 3.2314 (3.1628)	LR 7.813e-06
0: TRAIN [3][5020/5173]	Time 0.630 (0.607)	Data 1.20e-04 (2.51e-04)	Tok/s 26453 (23302)	Loss/tok 3.0906 (3.1627)	LR 7.813e-06
0: TRAIN [3][5030/5173]	Time 0.507 (0.606)	Data 1.19e-04 (2.51e-04)	Tok/s 10580 (23293)	Loss/tok 2.5432 (3.1624)	LR 7.813e-06
0: TRAIN [3][5040/5173]	Time 0.567 (0.606)	Data 1.78e-04 (2.51e-04)	Tok/s 18054 (23295)	Loss/tok 2.9770 (3.1626)	LR 7.813e-06
0: TRAIN [3][5050/5173]	Time 0.625 (0.606)	Data 1.20e-04 (2.50e-04)	Tok/s 26910 (23294)	Loss/tok 3.1652 (3.1625)	LR 7.813e-06
0: TRAIN [3][5060/5173]	Time 0.505 (0.606)	Data 1.16e-04 (2.50e-04)	Tok/s 10425 (23291)	Loss/tok 2.6748 (3.1626)	LR 7.813e-06
0: TRAIN [3][5070/5173]	Time 0.623 (0.606)	Data 1.27e-04 (2.50e-04)	Tok/s 27199 (23294)	Loss/tok 3.1798 (3.1625)	LR 7.813e-06
0: TRAIN [3][5080/5173]	Time 0.764 (0.607)	Data 1.36e-04 (2.50e-04)	Tok/s 39160 (23304)	Loss/tok 3.5214 (3.1628)	LR 7.813e-06
0: TRAIN [3][5090/5173]	Time 0.566 (0.607)	Data 3.14e-04 (2.49e-04)	Tok/s 18124 (23298)	Loss/tok 2.9717 (3.1628)	LR 7.813e-06
0: TRAIN [3][5100/5173]	Time 0.567 (0.606)	Data 1.33e-04 (2.49e-04)	Tok/s 18216 (23293)	Loss/tok 2.9505 (3.1626)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][5110/5173]	Time 0.688 (0.607)	Data 1.32e-04 (2.49e-04)	Tok/s 33724 (23301)	Loss/tok 3.3173 (3.1628)	LR 7.813e-06
0: TRAIN [3][5120/5173]	Time 0.546 (0.606)	Data 1.24e-04 (2.49e-04)	Tok/s 18767 (23296)	Loss/tok 3.0019 (3.1627)	LR 7.813e-06
0: TRAIN [3][5130/5173]	Time 0.506 (0.607)	Data 1.18e-04 (2.49e-04)	Tok/s 10145 (23296)	Loss/tok 2.4882 (3.1628)	LR 7.813e-06
0: TRAIN [3][5140/5173]	Time 0.475 (0.607)	Data 1.37e-04 (2.48e-04)	Tok/s 11265 (23298)	Loss/tok 2.5867 (3.1630)	LR 7.813e-06
0: TRAIN [3][5150/5173]	Time 0.626 (0.607)	Data 1.45e-04 (2.48e-04)	Tok/s 26787 (23299)	Loss/tok 3.2077 (3.1630)	LR 7.813e-06
0: TRAIN [3][5160/5173]	Time 0.691 (0.607)	Data 1.50e-04 (2.48e-04)	Tok/s 33763 (23302)	Loss/tok 3.2838 (3.1629)	LR 7.813e-06
0: TRAIN [3][5170/5173]	Time 0.566 (0.607)	Data 1.43e-04 (2.48e-04)	Tok/s 18192 (23301)	Loss/tok 2.9657 (3.1630)	LR 7.813e-06
:::MLL 1586538695.089 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 525}}
:::MLL 1586538695.089 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [3][0/8]	Time 0.744 (0.744)	Decoder iters 149.0 (149.0)	Tok/s 22236 (22236)
0: Running moses detokenizer
0: BLEU(score=23.08441586892477, counts=[36686, 18042, 10117, 5888], totals=[65638, 62635, 59632, 56634], precisions=[55.89140436942015, 28.804981240520476, 16.96572310169037, 10.396581558780944], bp=1.0, sys_len=65638, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1586538700.952 eval_accuracy: {"value": 23.08, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 536}}
:::MLL 1586538700.953 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 3	Training Loss: 3.1630	Test BLEU: 23.08
0: Performance: Epoch: 3	Training: 69900 Tok/s
0: Finished epoch 3
:::MLL 1586538700.953 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 558}}
:::MLL 1586538700.953 block_start: {"value": null, "metadata": {"first_epoch_num": 5, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1586538700.954 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 515}}
0: Starting epoch 4
0: Executing preallocation
0: Sampler for epoch 4 uses seed 3418798183
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][0/5173]	Time 1.206 (1.206)	Data 5.18e-01 (5.18e-01)	Tok/s 13917 (13917)	Loss/tok 3.0423 (3.0423)	LR 7.813e-06
0: TRAIN [4][10/5173]	Time 0.766 (0.673)	Data 2.01e-04 (4.73e-02)	Tok/s 39190 (23080)	Loss/tok 3.3837 (3.2306)	LR 7.813e-06
0: TRAIN [4][20/5173]	Time 0.692 (0.646)	Data 1.80e-04 (2.49e-02)	Tok/s 33551 (23834)	Loss/tok 3.3479 (3.1922)	LR 7.813e-06
0: TRAIN [4][30/5173]	Time 0.568 (0.626)	Data 2.93e-04 (1.69e-02)	Tok/s 17857 (22786)	Loss/tok 3.0040 (3.1726)	LR 7.813e-06
0: TRAIN [4][40/5173]	Time 0.629 (0.619)	Data 1.13e-04 (1.28e-02)	Tok/s 27157 (22762)	Loss/tok 3.1731 (3.1535)	LR 7.813e-06
0: TRAIN [4][50/5173]	Time 0.566 (0.620)	Data 1.11e-04 (1.03e-02)	Tok/s 17764 (23152)	Loss/tok 3.0362 (3.1712)	LR 7.813e-06
0: TRAIN [4][60/5173]	Time 0.568 (0.618)	Data 1.27e-04 (8.65e-03)	Tok/s 17839 (23301)	Loss/tok 3.0367 (3.1687)	LR 7.813e-06
0: TRAIN [4][70/5173]	Time 0.687 (0.620)	Data 1.27e-04 (7.45e-03)	Tok/s 34041 (23670)	Loss/tok 3.2844 (3.1747)	LR 7.813e-06
0: TRAIN [4][80/5173]	Time 0.570 (0.619)	Data 1.17e-04 (6.54e-03)	Tok/s 18463 (23708)	Loss/tok 2.8885 (3.1702)	LR 7.813e-06
0: TRAIN [4][90/5173]	Time 0.691 (0.617)	Data 1.24e-04 (5.84e-03)	Tok/s 33820 (23636)	Loss/tok 3.2070 (3.1615)	LR 7.813e-06
0: TRAIN [4][100/5173]	Time 0.690 (0.617)	Data 1.19e-04 (5.27e-03)	Tok/s 33866 (23757)	Loss/tok 3.2349 (3.1596)	LR 7.813e-06
0: TRAIN [4][110/5173]	Time 0.627 (0.617)	Data 1.27e-04 (4.81e-03)	Tok/s 26992 (23759)	Loss/tok 3.1845 (3.1599)	LR 7.813e-06
0: TRAIN [4][120/5173]	Time 0.688 (0.616)	Data 1.17e-04 (4.42e-03)	Tok/s 33843 (23734)	Loss/tok 3.3933 (3.1671)	LR 7.813e-06
0: TRAIN [4][130/5173]	Time 0.565 (0.615)	Data 1.34e-04 (4.10e-03)	Tok/s 17741 (23650)	Loss/tok 3.0205 (3.1657)	LR 7.813e-06
0: TRAIN [4][140/5173]	Time 0.629 (0.616)	Data 2.95e-04 (3.82e-03)	Tok/s 26736 (23838)	Loss/tok 3.0833 (3.1703)	LR 7.813e-06
0: TRAIN [4][150/5173]	Time 0.569 (0.615)	Data 1.17e-04 (3.58e-03)	Tok/s 17922 (23704)	Loss/tok 3.0084 (3.1710)	LR 7.813e-06
0: TRAIN [4][160/5173]	Time 0.557 (0.613)	Data 1.20e-04 (3.36e-03)	Tok/s 18647 (23523)	Loss/tok 3.0604 (3.1648)	LR 7.813e-06
0: TRAIN [4][170/5173]	Time 0.567 (0.614)	Data 1.24e-04 (3.17e-03)	Tok/s 18387 (23681)	Loss/tok 2.9861 (3.1672)	LR 7.813e-06
0: TRAIN [4][180/5173]	Time 0.693 (0.613)	Data 1.15e-04 (3.01e-03)	Tok/s 33401 (23579)	Loss/tok 3.2389 (3.1672)	LR 7.813e-06
0: TRAIN [4][190/5173]	Time 0.691 (0.614)	Data 1.21e-04 (2.86e-03)	Tok/s 34048 (23620)	Loss/tok 3.2993 (3.1696)	LR 7.813e-06
0: TRAIN [4][200/5173]	Time 0.768 (0.615)	Data 1.23e-04 (2.72e-03)	Tok/s 38280 (23802)	Loss/tok 3.4713 (3.1737)	LR 7.813e-06
0: TRAIN [4][210/5173]	Time 0.565 (0.615)	Data 1.24e-04 (2.60e-03)	Tok/s 18499 (23845)	Loss/tok 2.9811 (3.1714)	LR 7.813e-06
0: TRAIN [4][220/5173]	Time 0.765 (0.616)	Data 1.23e-04 (2.49e-03)	Tok/s 38653 (23936)	Loss/tok 3.6535 (3.1764)	LR 7.813e-06
0: TRAIN [4][230/5173]	Time 0.566 (0.615)	Data 1.23e-04 (2.38e-03)	Tok/s 18001 (23828)	Loss/tok 2.9680 (3.1733)	LR 7.813e-06
0: TRAIN [4][240/5173]	Time 0.561 (0.614)	Data 1.16e-04 (2.29e-03)	Tok/s 18541 (23800)	Loss/tok 2.9313 (3.1701)	LR 7.813e-06
0: TRAIN [4][250/5173]	Time 0.565 (0.614)	Data 1.71e-04 (2.20e-03)	Tok/s 18235 (23801)	Loss/tok 2.9863 (3.1707)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][260/5173]	Time 0.568 (0.614)	Data 1.65e-04 (2.12e-03)	Tok/s 18041 (23807)	Loss/tok 2.8728 (3.1699)	LR 7.813e-06
0: TRAIN [4][270/5173]	Time 0.567 (0.614)	Data 1.61e-04 (2.05e-03)	Tok/s 18593 (23819)	Loss/tok 2.9925 (3.1686)	LR 7.813e-06
0: TRAIN [4][280/5173]	Time 0.569 (0.612)	Data 1.37e-04 (1.98e-03)	Tok/s 17700 (23621)	Loss/tok 2.9804 (3.1651)	LR 7.813e-06
0: TRAIN [4][290/5173]	Time 0.634 (0.611)	Data 1.22e-04 (1.92e-03)	Tok/s 26540 (23531)	Loss/tok 3.1979 (3.1622)	LR 7.813e-06
0: TRAIN [4][300/5173]	Time 0.567 (0.610)	Data 1.15e-04 (1.86e-03)	Tok/s 18010 (23462)	Loss/tok 2.9678 (3.1606)	LR 7.813e-06
0: TRAIN [4][310/5173]	Time 0.568 (0.611)	Data 1.36e-04 (1.80e-03)	Tok/s 18176 (23587)	Loss/tok 2.9999 (3.1662)	LR 7.813e-06
0: TRAIN [4][320/5173]	Time 0.627 (0.611)	Data 1.36e-04 (1.75e-03)	Tok/s 26794 (23599)	Loss/tok 3.0615 (3.1654)	LR 7.813e-06
0: TRAIN [4][330/5173]	Time 0.566 (0.611)	Data 1.24e-04 (1.70e-03)	Tok/s 18170 (23611)	Loss/tok 2.9199 (3.1657)	LR 7.813e-06
0: TRAIN [4][340/5173]	Time 0.566 (0.612)	Data 1.40e-04 (1.66e-03)	Tok/s 18115 (23725)	Loss/tok 2.9734 (3.1696)	LR 7.813e-06
0: TRAIN [4][350/5173]	Time 0.570 (0.613)	Data 1.25e-04 (1.61e-03)	Tok/s 18402 (23804)	Loss/tok 3.0046 (3.1733)	LR 7.813e-06
0: TRAIN [4][360/5173]	Time 0.567 (0.613)	Data 1.18e-04 (1.57e-03)	Tok/s 17941 (23806)	Loss/tok 3.1308 (3.1727)	LR 7.813e-06
0: TRAIN [4][370/5173]	Time 0.566 (0.613)	Data 1.14e-04 (1.53e-03)	Tok/s 18176 (23875)	Loss/tok 2.9116 (3.1753)	LR 7.813e-06
0: TRAIN [4][380/5173]	Time 0.568 (0.613)	Data 1.17e-04 (1.49e-03)	Tok/s 17782 (23815)	Loss/tok 2.9678 (3.1730)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][390/5173]	Time 0.558 (0.612)	Data 1.21e-04 (1.46e-03)	Tok/s 18833 (23774)	Loss/tok 2.9610 (3.1722)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][400/5173]	Time 0.559 (0.612)	Data 1.18e-04 (1.43e-03)	Tok/s 18339 (23787)	Loss/tok 3.0559 (3.1737)	LR 7.813e-06
0: TRAIN [4][410/5173]	Time 0.765 (0.612)	Data 1.22e-04 (1.39e-03)	Tok/s 38835 (23770)	Loss/tok 3.4368 (3.1744)	LR 7.813e-06
0: TRAIN [4][420/5173]	Time 0.497 (0.612)	Data 1.23e-04 (1.36e-03)	Tok/s 10713 (23768)	Loss/tok 2.5288 (3.1757)	LR 7.813e-06
0: TRAIN [4][430/5173]	Time 0.766 (0.613)	Data 1.25e-04 (1.34e-03)	Tok/s 38724 (23795)	Loss/tok 3.4908 (3.1774)	LR 7.813e-06
0: TRAIN [4][440/5173]	Time 0.570 (0.612)	Data 1.18e-04 (1.31e-03)	Tok/s 18132 (23777)	Loss/tok 2.9434 (3.1771)	LR 7.813e-06
0: TRAIN [4][450/5173]	Time 0.627 (0.612)	Data 1.28e-04 (1.28e-03)	Tok/s 26675 (23780)	Loss/tok 3.1212 (3.1770)	LR 7.813e-06
0: TRAIN [4][460/5173]	Time 0.693 (0.613)	Data 1.36e-04 (1.26e-03)	Tok/s 33903 (23879)	Loss/tok 3.3282 (3.1794)	LR 7.813e-06
0: TRAIN [4][470/5173]	Time 0.565 (0.613)	Data 1.14e-04 (1.23e-03)	Tok/s 18376 (23855)	Loss/tok 3.0088 (3.1795)	LR 7.813e-06
0: TRAIN [4][480/5173]	Time 0.635 (0.613)	Data 1.28e-04 (1.21e-03)	Tok/s 26801 (23830)	Loss/tok 3.1035 (3.1770)	LR 7.813e-06
0: TRAIN [4][490/5173]	Time 0.690 (0.612)	Data 1.20e-04 (1.19e-03)	Tok/s 34064 (23769)	Loss/tok 3.3231 (3.1755)	LR 7.813e-06
0: TRAIN [4][500/5173]	Time 0.568 (0.612)	Data 1.13e-04 (1.17e-03)	Tok/s 18603 (23786)	Loss/tok 2.9902 (3.1754)	LR 7.813e-06
0: TRAIN [4][510/5173]	Time 0.626 (0.612)	Data 1.16e-04 (1.15e-03)	Tok/s 27307 (23815)	Loss/tok 3.1298 (3.1756)	LR 7.813e-06
0: TRAIN [4][520/5173]	Time 0.633 (0.612)	Data 1.25e-04 (1.13e-03)	Tok/s 26680 (23781)	Loss/tok 3.2768 (3.1747)	LR 7.813e-06
0: TRAIN [4][530/5173]	Time 0.558 (0.612)	Data 1.18e-04 (1.11e-03)	Tok/s 18415 (23782)	Loss/tok 2.9167 (3.1742)	LR 7.813e-06
0: TRAIN [4][540/5173]	Time 0.567 (0.612)	Data 1.22e-04 (1.09e-03)	Tok/s 18028 (23745)	Loss/tok 3.0936 (3.1755)	LR 7.813e-06
0: TRAIN [4][550/5173]	Time 0.570 (0.611)	Data 1.23e-04 (1.07e-03)	Tok/s 18182 (23710)	Loss/tok 2.9277 (3.1747)	LR 7.813e-06
0: TRAIN [4][560/5173]	Time 0.507 (0.611)	Data 1.17e-04 (1.06e-03)	Tok/s 10475 (23671)	Loss/tok 2.5289 (3.1737)	LR 7.813e-06
0: TRAIN [4][570/5173]	Time 0.633 (0.611)	Data 1.27e-04 (1.04e-03)	Tok/s 26684 (23608)	Loss/tok 3.0899 (3.1718)	LR 7.813e-06
0: TRAIN [4][580/5173]	Time 0.756 (0.611)	Data 1.29e-04 (1.02e-03)	Tok/s 39797 (23687)	Loss/tok 3.4372 (3.1735)	LR 7.813e-06
0: TRAIN [4][590/5173]	Time 0.763 (0.611)	Data 1.19e-04 (1.01e-03)	Tok/s 39667 (23724)	Loss/tok 3.3812 (3.1744)	LR 7.813e-06
0: TRAIN [4][600/5173]	Time 0.568 (0.611)	Data 1.20e-04 (9.94e-04)	Tok/s 18141 (23710)	Loss/tok 2.8989 (3.1750)	LR 7.813e-06
0: TRAIN [4][610/5173]	Time 0.624 (0.611)	Data 1.20e-04 (9.80e-04)	Tok/s 26774 (23647)	Loss/tok 3.2949 (3.1741)	LR 7.813e-06
0: TRAIN [4][620/5173]	Time 0.623 (0.610)	Data 1.17e-04 (9.66e-04)	Tok/s 26649 (23601)	Loss/tok 3.1101 (3.1725)	LR 7.813e-06
0: TRAIN [4][630/5173]	Time 0.628 (0.610)	Data 1.25e-04 (9.53e-04)	Tok/s 26682 (23567)	Loss/tok 3.1079 (3.1718)	LR 7.813e-06
0: TRAIN [4][640/5173]	Time 0.628 (0.610)	Data 1.24e-04 (9.40e-04)	Tok/s 26792 (23614)	Loss/tok 3.1258 (3.1717)	LR 7.813e-06
0: TRAIN [4][650/5173]	Time 0.692 (0.610)	Data 1.24e-04 (9.27e-04)	Tok/s 33694 (23617)	Loss/tok 3.2638 (3.1716)	LR 7.813e-06
0: TRAIN [4][660/5173]	Time 0.624 (0.610)	Data 1.18e-04 (9.15e-04)	Tok/s 26754 (23602)	Loss/tok 3.1875 (3.1706)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][670/5173]	Time 0.566 (0.610)	Data 1.26e-04 (9.04e-04)	Tok/s 18520 (23594)	Loss/tok 2.9244 (3.1709)	LR 7.813e-06
0: TRAIN [4][680/5173]	Time 0.568 (0.610)	Data 1.19e-04 (8.92e-04)	Tok/s 18489 (23562)	Loss/tok 2.9884 (3.1699)	LR 7.813e-06
0: TRAIN [4][690/5173]	Time 0.628 (0.610)	Data 1.24e-04 (8.81e-04)	Tok/s 26994 (23568)	Loss/tok 3.2269 (3.1698)	LR 7.813e-06
0: TRAIN [4][700/5173]	Time 0.692 (0.610)	Data 1.27e-04 (8.70e-04)	Tok/s 33697 (23593)	Loss/tok 3.3400 (3.1704)	LR 7.813e-06
0: TRAIN [4][710/5173]	Time 0.763 (0.610)	Data 1.20e-04 (8.60e-04)	Tok/s 39383 (23592)	Loss/tok 3.4291 (3.1700)	LR 7.813e-06
0: TRAIN [4][720/5173]	Time 0.630 (0.610)	Data 1.21e-04 (8.50e-04)	Tok/s 26634 (23562)	Loss/tok 3.1159 (3.1691)	LR 7.813e-06
0: TRAIN [4][730/5173]	Time 0.569 (0.609)	Data 1.23e-04 (8.40e-04)	Tok/s 18615 (23513)	Loss/tok 2.9402 (3.1678)	LR 7.813e-06
0: TRAIN [4][740/5173]	Time 0.566 (0.610)	Data 1.23e-04 (8.30e-04)	Tok/s 18156 (23558)	Loss/tok 2.8878 (3.1684)	LR 7.813e-06
0: TRAIN [4][750/5173]	Time 0.566 (0.609)	Data 1.20e-04 (8.21e-04)	Tok/s 17704 (23537)	Loss/tok 3.0352 (3.1684)	LR 7.813e-06
0: TRAIN [4][760/5173]	Time 0.689 (0.610)	Data 1.25e-04 (8.12e-04)	Tok/s 33448 (23554)	Loss/tok 3.3848 (3.1685)	LR 7.813e-06
0: TRAIN [4][770/5173]	Time 0.565 (0.609)	Data 1.26e-04 (8.03e-04)	Tok/s 18513 (23533)	Loss/tok 2.9644 (3.1685)	LR 7.813e-06
0: TRAIN [4][780/5173]	Time 0.507 (0.609)	Data 1.25e-04 (7.95e-04)	Tok/s 10584 (23492)	Loss/tok 2.5356 (3.1673)	LR 7.813e-06
0: TRAIN [4][790/5173]	Time 0.628 (0.609)	Data 1.18e-04 (7.86e-04)	Tok/s 26672 (23489)	Loss/tok 3.2026 (3.1677)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][800/5173]	Time 0.569 (0.609)	Data 1.25e-04 (7.78e-04)	Tok/s 18257 (23464)	Loss/tok 2.9285 (3.1670)	LR 7.813e-06
0: TRAIN [4][810/5173]	Time 0.623 (0.609)	Data 1.22e-04 (7.70e-04)	Tok/s 26673 (23461)	Loss/tok 3.1178 (3.1666)	LR 7.813e-06
0: TRAIN [4][820/5173]	Time 0.689 (0.609)	Data 1.22e-04 (7.62e-04)	Tok/s 33424 (23486)	Loss/tok 3.3123 (3.1668)	LR 7.813e-06
0: TRAIN [4][830/5173]	Time 0.764 (0.609)	Data 1.25e-04 (7.55e-04)	Tok/s 38773 (23530)	Loss/tok 3.4702 (3.1686)	LR 7.813e-06
0: TRAIN [4][840/5173]	Time 0.564 (0.609)	Data 1.24e-04 (7.47e-04)	Tok/s 18555 (23554)	Loss/tok 3.0873 (3.1697)	LR 7.813e-06
0: TRAIN [4][850/5173]	Time 0.693 (0.609)	Data 1.22e-04 (7.40e-04)	Tok/s 33954 (23539)	Loss/tok 3.3699 (3.1694)	LR 7.813e-06
0: TRAIN [4][860/5173]	Time 0.568 (0.609)	Data 1.28e-04 (7.33e-04)	Tok/s 18564 (23524)	Loss/tok 2.8767 (3.1692)	LR 7.813e-06
0: TRAIN [4][870/5173]	Time 0.693 (0.609)	Data 1.24e-04 (7.26e-04)	Tok/s 33594 (23546)	Loss/tok 3.4356 (3.1695)	LR 7.813e-06
0: TRAIN [4][880/5173]	Time 0.760 (0.609)	Data 1.26e-04 (7.19e-04)	Tok/s 38804 (23538)	Loss/tok 3.4764 (3.1697)	LR 7.813e-06
0: TRAIN [4][890/5173]	Time 0.567 (0.609)	Data 1.28e-04 (7.13e-04)	Tok/s 18317 (23517)	Loss/tok 2.8143 (3.1685)	LR 7.813e-06
0: TRAIN [4][900/5173]	Time 0.623 (0.609)	Data 1.28e-04 (7.06e-04)	Tok/s 26906 (23554)	Loss/tok 3.2248 (3.1694)	LR 7.813e-06
0: TRAIN [4][910/5173]	Time 0.568 (0.609)	Data 1.23e-04 (7.00e-04)	Tok/s 18236 (23571)	Loss/tok 2.8717 (3.1698)	LR 7.813e-06
0: TRAIN [4][920/5173]	Time 0.568 (0.609)	Data 1.27e-04 (6.94e-04)	Tok/s 18047 (23541)	Loss/tok 3.0084 (3.1693)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][930/5173]	Time 0.627 (0.609)	Data 1.32e-04 (6.88e-04)	Tok/s 26537 (23557)	Loss/tok 3.1333 (3.1699)	LR 7.813e-06
0: TRAIN [4][940/5173]	Time 0.568 (0.609)	Data 1.22e-04 (6.82e-04)	Tok/s 18177 (23536)	Loss/tok 2.9357 (3.1697)	LR 7.813e-06
0: TRAIN [4][950/5173]	Time 0.691 (0.609)	Data 1.27e-04 (6.76e-04)	Tok/s 33532 (23554)	Loss/tok 3.4124 (3.1701)	LR 7.813e-06
0: TRAIN [4][960/5173]	Time 0.566 (0.609)	Data 1.29e-04 (6.70e-04)	Tok/s 18265 (23524)	Loss/tok 2.8691 (3.1692)	LR 7.813e-06
0: TRAIN [4][970/5173]	Time 0.570 (0.609)	Data 1.23e-04 (6.65e-04)	Tok/s 18344 (23520)	Loss/tok 2.9493 (3.1687)	LR 7.813e-06
0: TRAIN [4][980/5173]	Time 0.566 (0.609)	Data 1.38e-04 (6.59e-04)	Tok/s 18087 (23554)	Loss/tok 2.9879 (3.1694)	LR 7.813e-06
0: TRAIN [4][990/5173]	Time 0.564 (0.609)	Data 1.26e-04 (6.54e-04)	Tok/s 18172 (23550)	Loss/tok 2.8255 (3.1688)	LR 7.813e-06
0: TRAIN [4][1000/5173]	Time 0.566 (0.609)	Data 1.24e-04 (6.49e-04)	Tok/s 18029 (23515)	Loss/tok 2.9501 (3.1677)	LR 7.813e-06
0: TRAIN [4][1010/5173]	Time 0.489 (0.609)	Data 1.19e-04 (6.44e-04)	Tok/s 10884 (23480)	Loss/tok 2.5528 (3.1667)	LR 7.813e-06
0: TRAIN [4][1020/5173]	Time 0.569 (0.609)	Data 1.29e-04 (6.39e-04)	Tok/s 18399 (23487)	Loss/tok 3.0098 (3.1673)	LR 7.813e-06
0: TRAIN [4][1030/5173]	Time 0.627 (0.608)	Data 1.21e-04 (6.34e-04)	Tok/s 26636 (23438)	Loss/tok 3.1980 (3.1662)	LR 7.813e-06
0: TRAIN [4][1040/5173]	Time 0.628 (0.608)	Data 1.17e-04 (6.29e-04)	Tok/s 26983 (23459)	Loss/tok 3.1279 (3.1661)	LR 7.813e-06
0: TRAIN [4][1050/5173]	Time 0.618 (0.608)	Data 1.27e-04 (6.24e-04)	Tok/s 27145 (23420)	Loss/tok 3.1702 (3.1653)	LR 7.813e-06
0: TRAIN [4][1060/5173]	Time 0.625 (0.608)	Data 1.25e-04 (6.20e-04)	Tok/s 27153 (23412)	Loss/tok 2.9982 (3.1646)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][1070/5173]	Time 0.506 (0.608)	Data 2.93e-04 (6.15e-04)	Tok/s 10405 (23427)	Loss/tok 2.5317 (3.1652)	LR 7.813e-06
0: TRAIN [4][1080/5173]	Time 0.627 (0.608)	Data 1.28e-04 (6.11e-04)	Tok/s 27065 (23424)	Loss/tok 3.0831 (3.1646)	LR 7.813e-06
0: TRAIN [4][1090/5173]	Time 0.567 (0.608)	Data 1.26e-04 (6.06e-04)	Tok/s 18255 (23406)	Loss/tok 3.0023 (3.1641)	LR 7.813e-06
0: TRAIN [4][1100/5173]	Time 0.503 (0.608)	Data 1.24e-04 (6.02e-04)	Tok/s 10719 (23417)	Loss/tok 2.6919 (3.1644)	LR 7.813e-06
0: TRAIN [4][1110/5173]	Time 0.505 (0.608)	Data 1.28e-04 (5.98e-04)	Tok/s 10623 (23392)	Loss/tok 2.4991 (3.1646)	LR 7.813e-06
0: TRAIN [4][1120/5173]	Time 0.568 (0.608)	Data 1.26e-04 (5.94e-04)	Tok/s 18035 (23396)	Loss/tok 2.9473 (3.1646)	LR 7.813e-06
0: TRAIN [4][1130/5173]	Time 0.765 (0.608)	Data 1.31e-04 (5.89e-04)	Tok/s 38946 (23412)	Loss/tok 3.3979 (3.1646)	LR 7.813e-06
0: TRAIN [4][1140/5173]	Time 0.629 (0.608)	Data 1.30e-04 (5.85e-04)	Tok/s 26578 (23427)	Loss/tok 3.1238 (3.1649)	LR 7.813e-06
0: TRAIN [4][1150/5173]	Time 0.692 (0.608)	Data 1.31e-04 (5.81e-04)	Tok/s 34356 (23437)	Loss/tok 3.2934 (3.1653)	LR 7.813e-06
0: TRAIN [4][1160/5173]	Time 0.568 (0.608)	Data 1.22e-04 (5.78e-04)	Tok/s 17738 (23437)	Loss/tok 2.9941 (3.1652)	LR 7.813e-06
0: TRAIN [4][1170/5173]	Time 0.564 (0.608)	Data 1.29e-04 (5.74e-04)	Tok/s 18264 (23446)	Loss/tok 2.9308 (3.1654)	LR 7.813e-06
0: TRAIN [4][1180/5173]	Time 0.629 (0.608)	Data 1.26e-04 (5.70e-04)	Tok/s 26373 (23436)	Loss/tok 3.1810 (3.1650)	LR 7.813e-06
0: TRAIN [4][1190/5173]	Time 0.570 (0.608)	Data 1.28e-04 (5.66e-04)	Tok/s 18180 (23427)	Loss/tok 2.9782 (3.1644)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][1200/5173]	Time 0.565 (0.608)	Data 1.26e-04 (5.63e-04)	Tok/s 18624 (23411)	Loss/tok 3.0056 (3.1643)	LR 7.813e-06
0: TRAIN [4][1210/5173]	Time 0.568 (0.608)	Data 1.28e-04 (5.59e-04)	Tok/s 18099 (23381)	Loss/tok 2.9204 (3.1636)	LR 7.813e-06
0: TRAIN [4][1220/5173]	Time 0.757 (0.607)	Data 3.10e-04 (5.56e-04)	Tok/s 38929 (23350)	Loss/tok 3.5722 (3.1632)	LR 7.813e-06
0: TRAIN [4][1230/5173]	Time 0.628 (0.608)	Data 1.19e-04 (5.52e-04)	Tok/s 27207 (23373)	Loss/tok 3.1925 (3.1634)	LR 7.813e-06
0: TRAIN [4][1240/5173]	Time 0.569 (0.607)	Data 1.25e-04 (5.49e-04)	Tok/s 18487 (23346)	Loss/tok 2.9332 (3.1626)	LR 7.813e-06
0: TRAIN [4][1250/5173]	Time 0.628 (0.607)	Data 1.34e-04 (5.46e-04)	Tok/s 26849 (23349)	Loss/tok 3.1546 (3.1627)	LR 7.813e-06
0: TRAIN [4][1260/5173]	Time 0.625 (0.607)	Data 1.20e-04 (5.42e-04)	Tok/s 26961 (23362)	Loss/tok 2.9724 (3.1621)	LR 7.813e-06
0: TRAIN [4][1270/5173]	Time 0.627 (0.607)	Data 1.28e-04 (5.39e-04)	Tok/s 26602 (23349)	Loss/tok 3.2583 (3.1618)	LR 7.813e-06
0: TRAIN [4][1280/5173]	Time 0.567 (0.607)	Data 1.25e-04 (5.36e-04)	Tok/s 17942 (23341)	Loss/tok 2.9497 (3.1614)	LR 7.813e-06
0: TRAIN [4][1290/5173]	Time 0.688 (0.607)	Data 1.19e-04 (5.33e-04)	Tok/s 33569 (23328)	Loss/tok 3.4322 (3.1612)	LR 7.813e-06
0: TRAIN [4][1300/5173]	Time 0.627 (0.607)	Data 1.35e-04 (5.30e-04)	Tok/s 26919 (23350)	Loss/tok 3.1587 (3.1614)	LR 7.813e-06
0: TRAIN [4][1310/5173]	Time 0.631 (0.607)	Data 1.30e-04 (5.26e-04)	Tok/s 26661 (23339)	Loss/tok 3.2105 (3.1609)	LR 7.813e-06
0: TRAIN [4][1320/5173]	Time 0.623 (0.607)	Data 1.18e-04 (5.23e-04)	Tok/s 26698 (23341)	Loss/tok 3.2075 (3.1611)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][1330/5173]	Time 0.566 (0.607)	Data 1.41e-04 (5.20e-04)	Tok/s 18125 (23329)	Loss/tok 2.9927 (3.1609)	LR 7.813e-06
0: TRAIN [4][1340/5173]	Time 0.506 (0.607)	Data 1.26e-04 (5.18e-04)	Tok/s 10545 (23319)	Loss/tok 2.5740 (3.1609)	LR 7.813e-06
0: TRAIN [4][1350/5173]	Time 0.506 (0.607)	Data 1.26e-04 (5.15e-04)	Tok/s 10243 (23333)	Loss/tok 2.5503 (3.1615)	LR 7.813e-06
0: TRAIN [4][1360/5173]	Time 0.621 (0.607)	Data 1.25e-04 (5.12e-04)	Tok/s 26736 (23314)	Loss/tok 3.1494 (3.1611)	LR 7.813e-06
0: TRAIN [4][1370/5173]	Time 0.624 (0.607)	Data 1.24e-04 (5.09e-04)	Tok/s 26734 (23301)	Loss/tok 3.1727 (3.1606)	LR 7.813e-06
0: TRAIN [4][1380/5173]	Time 0.568 (0.606)	Data 1.24e-04 (5.06e-04)	Tok/s 18240 (23260)	Loss/tok 2.9615 (3.1597)	LR 7.813e-06
0: TRAIN [4][1390/5173]	Time 0.568 (0.606)	Data 1.24e-04 (5.04e-04)	Tok/s 18247 (23250)	Loss/tok 3.0726 (3.1597)	LR 7.813e-06
0: TRAIN [4][1400/5173]	Time 0.567 (0.606)	Data 1.31e-04 (5.01e-04)	Tok/s 17882 (23259)	Loss/tok 2.9018 (3.1599)	LR 7.813e-06
0: TRAIN [4][1410/5173]	Time 0.558 (0.606)	Data 1.20e-04 (4.98e-04)	Tok/s 18522 (23254)	Loss/tok 3.0359 (3.1604)	LR 7.813e-06
0: TRAIN [4][1420/5173]	Time 0.505 (0.606)	Data 1.19e-04 (4.96e-04)	Tok/s 10257 (23225)	Loss/tok 2.6956 (3.1598)	LR 7.813e-06
0: TRAIN [4][1430/5173]	Time 0.634 (0.606)	Data 1.44e-04 (4.93e-04)	Tok/s 26200 (23250)	Loss/tok 3.1555 (3.1603)	LR 7.813e-06
0: TRAIN [4][1440/5173]	Time 0.627 (0.606)	Data 1.22e-04 (4.91e-04)	Tok/s 26516 (23223)	Loss/tok 3.2389 (3.1595)	LR 7.813e-06
0: TRAIN [4][1450/5173]	Time 0.570 (0.606)	Data 1.19e-04 (4.88e-04)	Tok/s 17842 (23238)	Loss/tok 2.9480 (3.1596)	LR 7.813e-06
0: TRAIN [4][1460/5173]	Time 0.567 (0.606)	Data 1.27e-04 (4.86e-04)	Tok/s 17969 (23227)	Loss/tok 2.8982 (3.1588)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][1470/5173]	Time 0.681 (0.606)	Data 1.24e-04 (4.83e-04)	Tok/s 34467 (23226)	Loss/tok 3.3205 (3.1587)	LR 7.813e-06
0: TRAIN [4][1480/5173]	Time 0.498 (0.606)	Data 1.32e-04 (4.81e-04)	Tok/s 10567 (23225)	Loss/tok 2.5244 (3.1584)	LR 7.813e-06
0: TRAIN [4][1490/5173]	Time 0.629 (0.606)	Data 1.26e-04 (4.79e-04)	Tok/s 26869 (23204)	Loss/tok 3.2786 (3.1579)	LR 7.813e-06
0: TRAIN [4][1500/5173]	Time 0.688 (0.606)	Data 1.26e-04 (4.76e-04)	Tok/s 33615 (23213)	Loss/tok 3.3213 (3.1580)	LR 7.813e-06
0: TRAIN [4][1510/5173]	Time 0.570 (0.606)	Data 2.95e-04 (4.74e-04)	Tok/s 18288 (23192)	Loss/tok 3.0210 (3.1573)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1520/5173]	Time 0.770 (0.606)	Data 1.27e-04 (4.72e-04)	Tok/s 38764 (23208)	Loss/tok 3.5464 (3.1583)	LR 7.813e-06
0: TRAIN [4][1530/5173]	Time 0.627 (0.606)	Data 1.40e-04 (4.70e-04)	Tok/s 26801 (23234)	Loss/tok 3.2598 (3.1594)	LR 7.813e-06
0: TRAIN [4][1540/5173]	Time 0.627 (0.606)	Data 1.29e-04 (4.68e-04)	Tok/s 26563 (23250)	Loss/tok 3.1421 (3.1603)	LR 7.813e-06
0: TRAIN [4][1550/5173]	Time 0.566 (0.606)	Data 1.24e-04 (4.65e-04)	Tok/s 18661 (23249)	Loss/tok 2.9496 (3.1600)	LR 7.813e-06
0: TRAIN [4][1560/5173]	Time 0.628 (0.606)	Data 1.24e-04 (4.63e-04)	Tok/s 27059 (23264)	Loss/tok 3.0512 (3.1602)	LR 7.813e-06
0: TRAIN [4][1570/5173]	Time 0.493 (0.606)	Data 1.28e-04 (4.61e-04)	Tok/s 10661 (23256)	Loss/tok 2.5600 (3.1599)	LR 7.813e-06
0: TRAIN [4][1580/5173]	Time 0.564 (0.606)	Data 1.22e-04 (4.59e-04)	Tok/s 18031 (23235)	Loss/tok 3.0191 (3.1592)	LR 7.813e-06
0: TRAIN [4][1590/5173]	Time 0.571 (0.606)	Data 1.34e-04 (4.57e-04)	Tok/s 18349 (23233)	Loss/tok 2.9242 (3.1594)	LR 7.813e-06
0: TRAIN [4][1600/5173]	Time 0.566 (0.606)	Data 1.25e-04 (4.55e-04)	Tok/s 18032 (23250)	Loss/tok 3.0224 (3.1597)	LR 7.813e-06
0: TRAIN [4][1610/5173]	Time 0.629 (0.606)	Data 1.22e-04 (4.53e-04)	Tok/s 26617 (23264)	Loss/tok 3.2409 (3.1602)	LR 7.813e-06
0: TRAIN [4][1620/5173]	Time 0.508 (0.606)	Data 1.26e-04 (4.51e-04)	Tok/s 10451 (23271)	Loss/tok 2.5459 (3.1608)	LR 7.813e-06
0: TRAIN [4][1630/5173]	Time 0.567 (0.606)	Data 1.26e-04 (4.49e-04)	Tok/s 17970 (23270)	Loss/tok 3.0035 (3.1606)	LR 7.813e-06
0: TRAIN [4][1640/5173]	Time 0.692 (0.607)	Data 1.19e-04 (4.47e-04)	Tok/s 33643 (23280)	Loss/tok 3.4257 (3.1607)	LR 7.813e-06
0: TRAIN [4][1650/5173]	Time 0.609 (0.606)	Data 1.27e-04 (4.45e-04)	Tok/s 27262 (23273)	Loss/tok 3.1475 (3.1607)	LR 7.813e-06
0: TRAIN [4][1660/5173]	Time 0.565 (0.606)	Data 1.25e-04 (4.43e-04)	Tok/s 18254 (23276)	Loss/tok 3.0067 (3.1607)	LR 7.813e-06
0: TRAIN [4][1670/5173]	Time 0.568 (0.606)	Data 1.18e-04 (4.41e-04)	Tok/s 18707 (23274)	Loss/tok 3.0952 (3.1607)	LR 7.813e-06
0: TRAIN [4][1680/5173]	Time 0.690 (0.607)	Data 1.29e-04 (4.39e-04)	Tok/s 33704 (23294)	Loss/tok 3.3962 (3.1612)	LR 7.813e-06
0: TRAIN [4][1690/5173]	Time 0.564 (0.606)	Data 1.26e-04 (4.38e-04)	Tok/s 18275 (23274)	Loss/tok 2.9983 (3.1608)	LR 7.813e-06
0: TRAIN [4][1700/5173]	Time 0.628 (0.606)	Data 1.22e-04 (4.36e-04)	Tok/s 26863 (23274)	Loss/tok 3.1398 (3.1604)	LR 7.813e-06
0: TRAIN [4][1710/5173]	Time 0.628 (0.607)	Data 1.20e-04 (4.34e-04)	Tok/s 26819 (23282)	Loss/tok 3.2401 (3.1608)	LR 7.813e-06
0: TRAIN [4][1720/5173]	Time 0.627 (0.606)	Data 1.27e-04 (4.32e-04)	Tok/s 26374 (23276)	Loss/tok 3.2509 (3.1609)	LR 7.813e-06
0: TRAIN [4][1730/5173]	Time 0.624 (0.607)	Data 1.24e-04 (4.30e-04)	Tok/s 26774 (23296)	Loss/tok 3.2511 (3.1614)	LR 7.813e-06
0: TRAIN [4][1740/5173]	Time 0.693 (0.607)	Data 1.26e-04 (4.29e-04)	Tok/s 33892 (23316)	Loss/tok 3.2979 (3.1618)	LR 7.813e-06
0: TRAIN [4][1750/5173]	Time 0.621 (0.607)	Data 1.45e-04 (4.27e-04)	Tok/s 27396 (23339)	Loss/tok 3.0761 (3.1627)	LR 7.813e-06
0: TRAIN [4][1760/5173]	Time 0.566 (0.607)	Data 1.20e-04 (4.25e-04)	Tok/s 18573 (23322)	Loss/tok 3.0106 (3.1624)	LR 7.813e-06
0: TRAIN [4][1770/5173]	Time 0.761 (0.607)	Data 1.20e-04 (4.24e-04)	Tok/s 38762 (23324)	Loss/tok 3.5498 (3.1626)	LR 7.813e-06
0: TRAIN [4][1780/5173]	Time 0.566 (0.607)	Data 1.37e-04 (4.22e-04)	Tok/s 18437 (23318)	Loss/tok 3.0335 (3.1623)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][1790/5173]	Time 0.769 (0.607)	Data 1.31e-04 (4.20e-04)	Tok/s 38654 (23338)	Loss/tok 3.4991 (3.1627)	LR 7.813e-06
0: TRAIN [4][1800/5173]	Time 0.567 (0.607)	Data 1.19e-04 (4.19e-04)	Tok/s 18512 (23340)	Loss/tok 2.9982 (3.1627)	LR 7.813e-06
0: TRAIN [4][1810/5173]	Time 0.566 (0.607)	Data 1.39e-04 (4.17e-04)	Tok/s 18625 (23366)	Loss/tok 3.0480 (3.1631)	LR 7.813e-06
0: TRAIN [4][1820/5173]	Time 0.625 (0.607)	Data 1.23e-04 (4.15e-04)	Tok/s 26580 (23368)	Loss/tok 3.1318 (3.1633)	LR 7.813e-06
0: TRAIN [4][1830/5173]	Time 0.566 (0.607)	Data 1.16e-04 (4.14e-04)	Tok/s 18109 (23366)	Loss/tok 2.9992 (3.1632)	LR 7.813e-06
0: TRAIN [4][1840/5173]	Time 0.631 (0.607)	Data 1.23e-04 (4.12e-04)	Tok/s 26585 (23369)	Loss/tok 3.0938 (3.1630)	LR 7.813e-06
0: TRAIN [4][1850/5173]	Time 0.759 (0.608)	Data 3.10e-04 (4.11e-04)	Tok/s 38421 (23406)	Loss/tok 3.5709 (3.1649)	LR 7.813e-06
0: TRAIN [4][1860/5173]	Time 0.565 (0.607)	Data 1.22e-04 (4.10e-04)	Tok/s 18640 (23401)	Loss/tok 2.9345 (3.1645)	LR 7.813e-06
0: TRAIN [4][1870/5173]	Time 0.627 (0.608)	Data 1.38e-04 (4.08e-04)	Tok/s 26323 (23416)	Loss/tok 3.1143 (3.1648)	LR 7.813e-06
0: TRAIN [4][1880/5173]	Time 0.569 (0.608)	Data 1.24e-04 (4.07e-04)	Tok/s 18426 (23422)	Loss/tok 2.9535 (3.1650)	LR 7.813e-06
0: TRAIN [4][1890/5173]	Time 0.568 (0.608)	Data 1.18e-04 (4.05e-04)	Tok/s 18092 (23418)	Loss/tok 2.8867 (3.1650)	LR 7.813e-06
0: TRAIN [4][1900/5173]	Time 0.571 (0.608)	Data 2.81e-04 (4.04e-04)	Tok/s 18087 (23407)	Loss/tok 2.8832 (3.1646)	LR 7.813e-06
0: TRAIN [4][1910/5173]	Time 0.566 (0.608)	Data 1.24e-04 (4.02e-04)	Tok/s 18412 (23405)	Loss/tok 2.9570 (3.1645)	LR 7.813e-06
0: TRAIN [4][1920/5173]	Time 0.568 (0.607)	Data 1.21e-04 (4.01e-04)	Tok/s 18462 (23388)	Loss/tok 2.9332 (3.1638)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][1930/5173]	Time 0.566 (0.607)	Data 1.18e-04 (4.00e-04)	Tok/s 18256 (23394)	Loss/tok 3.0461 (3.1641)	LR 7.813e-06
0: TRAIN [4][1940/5173]	Time 0.565 (0.607)	Data 1.27e-04 (3.98e-04)	Tok/s 18281 (23400)	Loss/tok 3.0274 (3.1642)	LR 7.813e-06
0: TRAIN [4][1950/5173]	Time 0.505 (0.607)	Data 1.30e-04 (3.97e-04)	Tok/s 10532 (23396)	Loss/tok 2.5621 (3.1643)	LR 7.813e-06
0: TRAIN [4][1960/5173]	Time 0.566 (0.608)	Data 3.16e-04 (3.96e-04)	Tok/s 18192 (23408)	Loss/tok 2.9514 (3.1643)	LR 7.813e-06
0: TRAIN [4][1970/5173]	Time 0.694 (0.608)	Data 1.25e-04 (3.94e-04)	Tok/s 34037 (23418)	Loss/tok 3.3671 (3.1643)	LR 7.813e-06
0: TRAIN [4][1980/5173]	Time 0.628 (0.608)	Data 1.31e-04 (3.93e-04)	Tok/s 26981 (23424)	Loss/tok 3.1502 (3.1643)	LR 7.813e-06
0: TRAIN [4][1990/5173]	Time 0.691 (0.608)	Data 1.24e-04 (3.92e-04)	Tok/s 34149 (23434)	Loss/tok 3.2574 (3.1644)	LR 7.813e-06
0: TRAIN [4][2000/5173]	Time 0.631 (0.608)	Data 1.23e-04 (3.90e-04)	Tok/s 26786 (23429)	Loss/tok 3.1486 (3.1641)	LR 7.813e-06
0: TRAIN [4][2010/5173]	Time 0.568 (0.608)	Data 1.33e-04 (3.89e-04)	Tok/s 18320 (23444)	Loss/tok 2.9952 (3.1649)	LR 7.813e-06
0: TRAIN [4][2020/5173]	Time 0.567 (0.608)	Data 1.17e-04 (3.88e-04)	Tok/s 18277 (23432)	Loss/tok 3.0744 (3.1649)	LR 7.813e-06
0: TRAIN [4][2030/5173]	Time 0.566 (0.608)	Data 1.36e-04 (3.86e-04)	Tok/s 18649 (23450)	Loss/tok 2.9555 (3.1652)	LR 7.813e-06
0: TRAIN [4][2040/5173]	Time 0.633 (0.608)	Data 1.28e-04 (3.85e-04)	Tok/s 26792 (23468)	Loss/tok 3.1366 (3.1661)	LR 7.813e-06
0: TRAIN [4][2050/5173]	Time 0.563 (0.608)	Data 1.16e-04 (3.84e-04)	Tok/s 18270 (23457)	Loss/tok 2.9237 (3.1661)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][2060/5173]	Time 0.568 (0.608)	Data 1.24e-04 (3.83e-04)	Tok/s 18578 (23451)	Loss/tok 2.9112 (3.1660)	LR 7.813e-06
0: TRAIN [4][2070/5173]	Time 0.632 (0.608)	Data 1.26e-04 (3.82e-04)	Tok/s 26855 (23455)	Loss/tok 3.0052 (3.1657)	LR 7.813e-06
0: TRAIN [4][2080/5173]	Time 0.631 (0.608)	Data 1.18e-04 (3.80e-04)	Tok/s 26822 (23445)	Loss/tok 3.0262 (3.1653)	LR 7.813e-06
0: TRAIN [4][2090/5173]	Time 0.554 (0.608)	Data 1.26e-04 (3.79e-04)	Tok/s 18514 (23424)	Loss/tok 2.9987 (3.1648)	LR 7.813e-06
0: TRAIN [4][2100/5173]	Time 0.564 (0.608)	Data 1.26e-04 (3.78e-04)	Tok/s 17826 (23430)	Loss/tok 3.0197 (3.1649)	LR 7.813e-06
0: TRAIN [4][2110/5173]	Time 0.689 (0.608)	Data 1.26e-04 (3.77e-04)	Tok/s 33793 (23425)	Loss/tok 3.2764 (3.1646)	LR 7.813e-06
0: TRAIN [4][2120/5173]	Time 0.624 (0.608)	Data 1.20e-04 (3.76e-04)	Tok/s 26864 (23434)	Loss/tok 3.1196 (3.1650)	LR 7.813e-06
0: TRAIN [4][2130/5173]	Time 0.565 (0.608)	Data 1.23e-04 (3.74e-04)	Tok/s 18240 (23444)	Loss/tok 2.8834 (3.1650)	LR 7.813e-06
0: TRAIN [4][2140/5173]	Time 0.633 (0.608)	Data 1.20e-04 (3.73e-04)	Tok/s 26637 (23438)	Loss/tok 3.1203 (3.1649)	LR 7.813e-06
0: TRAIN [4][2150/5173]	Time 0.765 (0.608)	Data 1.23e-04 (3.72e-04)	Tok/s 38732 (23452)	Loss/tok 3.4959 (3.1657)	LR 7.813e-06
0: TRAIN [4][2160/5173]	Time 0.623 (0.608)	Data 1.25e-04 (3.71e-04)	Tok/s 27260 (23437)	Loss/tok 3.1140 (3.1654)	LR 7.813e-06
0: TRAIN [4][2170/5173]	Time 0.691 (0.608)	Data 1.22e-04 (3.70e-04)	Tok/s 33437 (23446)	Loss/tok 3.3389 (3.1655)	LR 7.813e-06
0: TRAIN [4][2180/5173]	Time 0.627 (0.608)	Data 1.20e-04 (3.69e-04)	Tok/s 26945 (23449)	Loss/tok 3.1104 (3.1654)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][2190/5173]	Time 0.508 (0.608)	Data 1.49e-04 (3.68e-04)	Tok/s 10379 (23477)	Loss/tok 2.6516 (3.1661)	LR 7.813e-06
0: TRAIN [4][2200/5173]	Time 0.544 (0.608)	Data 1.23e-04 (3.67e-04)	Tok/s 19333 (23487)	Loss/tok 2.8870 (3.1661)	LR 7.813e-06
0: TRAIN [4][2210/5173]	Time 0.629 (0.608)	Data 1.18e-04 (3.66e-04)	Tok/s 26817 (23491)	Loss/tok 3.0796 (3.1661)	LR 7.813e-06
0: TRAIN [4][2220/5173]	Time 0.569 (0.608)	Data 1.24e-04 (3.65e-04)	Tok/s 17871 (23490)	Loss/tok 2.9997 (3.1660)	LR 7.813e-06
0: TRAIN [4][2230/5173]	Time 0.568 (0.608)	Data 1.30e-04 (3.64e-04)	Tok/s 18231 (23484)	Loss/tok 2.9650 (3.1658)	LR 7.813e-06
0: TRAIN [4][2240/5173]	Time 0.633 (0.608)	Data 1.23e-04 (3.63e-04)	Tok/s 26573 (23488)	Loss/tok 3.1127 (3.1660)	LR 7.813e-06
0: TRAIN [4][2250/5173]	Time 0.563 (0.608)	Data 1.18e-04 (3.62e-04)	Tok/s 18521 (23481)	Loss/tok 2.9586 (3.1659)	LR 7.813e-06
0: TRAIN [4][2260/5173]	Time 0.566 (0.608)	Data 1.26e-04 (3.61e-04)	Tok/s 18605 (23478)	Loss/tok 2.9838 (3.1659)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2270/5173]	Time 0.566 (0.608)	Data 1.45e-04 (3.60e-04)	Tok/s 18234 (23497)	Loss/tok 2.9232 (3.1669)	LR 7.813e-06
0: TRAIN [4][2280/5173]	Time 0.764 (0.608)	Data 1.35e-04 (3.59e-04)	Tok/s 39203 (23489)	Loss/tok 3.5541 (3.1668)	LR 7.813e-06
0: TRAIN [4][2290/5173]	Time 0.692 (0.608)	Data 1.20e-04 (3.58e-04)	Tok/s 33706 (23486)	Loss/tok 3.2717 (3.1666)	LR 7.813e-06
0: TRAIN [4][2300/5173]	Time 0.631 (0.608)	Data 1.18e-04 (3.57e-04)	Tok/s 26830 (23474)	Loss/tok 3.1511 (3.1663)	LR 7.813e-06
0: TRAIN [4][2310/5173]	Time 0.626 (0.608)	Data 1.18e-04 (3.56e-04)	Tok/s 26927 (23469)	Loss/tok 3.1234 (3.1663)	LR 7.813e-06
0: TRAIN [4][2320/5173]	Time 0.632 (0.608)	Data 1.35e-04 (3.55e-04)	Tok/s 26430 (23457)	Loss/tok 3.0972 (3.1658)	LR 7.813e-06
0: TRAIN [4][2330/5173]	Time 0.507 (0.608)	Data 1.37e-04 (3.54e-04)	Tok/s 10246 (23465)	Loss/tok 2.6349 (3.1660)	LR 7.813e-06
0: TRAIN [4][2340/5173]	Time 0.625 (0.608)	Data 1.17e-04 (3.53e-04)	Tok/s 27152 (23470)	Loss/tok 3.1288 (3.1661)	LR 7.813e-06
0: TRAIN [4][2350/5173]	Time 0.566 (0.608)	Data 1.23e-04 (3.52e-04)	Tok/s 17861 (23478)	Loss/tok 2.8937 (3.1661)	LR 7.813e-06
0: TRAIN [4][2360/5173]	Time 0.567 (0.608)	Data 1.20e-04 (3.51e-04)	Tok/s 18496 (23454)	Loss/tok 3.0523 (3.1656)	LR 7.813e-06
0: TRAIN [4][2370/5173]	Time 0.692 (0.608)	Data 1.15e-04 (3.50e-04)	Tok/s 33606 (23457)	Loss/tok 3.3288 (3.1659)	LR 7.813e-06
0: TRAIN [4][2380/5173]	Time 0.627 (0.608)	Data 1.17e-04 (3.49e-04)	Tok/s 26691 (23452)	Loss/tok 3.1543 (3.1658)	LR 7.813e-06
0: TRAIN [4][2390/5173]	Time 0.688 (0.608)	Data 1.19e-04 (3.48e-04)	Tok/s 33457 (23461)	Loss/tok 3.3128 (3.1658)	LR 7.813e-06
0: TRAIN [4][2400/5173]	Time 0.690 (0.608)	Data 1.16e-04 (3.47e-04)	Tok/s 33572 (23470)	Loss/tok 3.3380 (3.1658)	LR 7.813e-06
0: TRAIN [4][2410/5173]	Time 0.566 (0.608)	Data 1.21e-04 (3.46e-04)	Tok/s 18496 (23466)	Loss/tok 2.9243 (3.1655)	LR 7.813e-06
0: TRAIN [4][2420/5173]	Time 0.626 (0.608)	Data 1.32e-04 (3.45e-04)	Tok/s 26779 (23484)	Loss/tok 3.0161 (3.1661)	LR 7.813e-06
0: TRAIN [4][2430/5173]	Time 0.567 (0.608)	Data 1.31e-04 (3.44e-04)	Tok/s 18331 (23494)	Loss/tok 2.8189 (3.1663)	LR 7.813e-06
0: TRAIN [4][2440/5173]	Time 0.504 (0.608)	Data 1.19e-04 (3.43e-04)	Tok/s 10523 (23477)	Loss/tok 2.5219 (3.1659)	LR 7.813e-06
0: TRAIN [4][2450/5173]	Time 0.630 (0.608)	Data 1.32e-04 (3.42e-04)	Tok/s 26775 (23478)	Loss/tok 3.1044 (3.1661)	LR 7.813e-06
0: TRAIN [4][2460/5173]	Time 0.629 (0.608)	Data 1.14e-04 (3.42e-04)	Tok/s 26875 (23468)	Loss/tok 3.0407 (3.1658)	LR 7.813e-06
0: TRAIN [4][2470/5173]	Time 0.566 (0.608)	Data 1.17e-04 (3.41e-04)	Tok/s 18336 (23461)	Loss/tok 3.0331 (3.1655)	LR 7.813e-06
0: TRAIN [4][2480/5173]	Time 0.505 (0.608)	Data 1.21e-04 (3.40e-04)	Tok/s 10416 (23450)	Loss/tok 2.5570 (3.1652)	LR 7.813e-06
0: TRAIN [4][2490/5173]	Time 0.488 (0.608)	Data 1.13e-04 (3.39e-04)	Tok/s 10654 (23449)	Loss/tok 2.6185 (3.1650)	LR 7.813e-06
0: TRAIN [4][2500/5173]	Time 0.633 (0.608)	Data 1.16e-04 (3.38e-04)	Tok/s 26513 (23435)	Loss/tok 3.2046 (3.1648)	LR 7.813e-06
0: TRAIN [4][2510/5173]	Time 0.634 (0.608)	Data 1.20e-04 (3.37e-04)	Tok/s 26327 (23430)	Loss/tok 3.1583 (3.1647)	LR 7.813e-06
0: TRAIN [4][2520/5173]	Time 0.569 (0.608)	Data 1.13e-04 (3.37e-04)	Tok/s 18063 (23422)	Loss/tok 2.9825 (3.1644)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][2530/5173]	Time 0.626 (0.608)	Data 1.20e-04 (3.36e-04)	Tok/s 27012 (23421)	Loss/tok 3.2575 (3.1644)	LR 7.813e-06
0: TRAIN [4][2540/5173]	Time 0.625 (0.608)	Data 1.20e-04 (3.35e-04)	Tok/s 26643 (23421)	Loss/tok 3.2318 (3.1644)	LR 7.813e-06
0: TRAIN [4][2550/5173]	Time 0.566 (0.607)	Data 1.26e-04 (3.34e-04)	Tok/s 18404 (23401)	Loss/tok 2.9553 (3.1639)	LR 7.813e-06
0: TRAIN [4][2560/5173]	Time 0.555 (0.607)	Data 1.14e-04 (3.33e-04)	Tok/s 18435 (23406)	Loss/tok 3.0125 (3.1641)	LR 7.813e-06
0: TRAIN [4][2570/5173]	Time 0.569 (0.607)	Data 1.25e-04 (3.33e-04)	Tok/s 17839 (23413)	Loss/tok 2.9625 (3.1643)	LR 7.813e-06
0: TRAIN [4][2580/5173]	Time 0.689 (0.607)	Data 1.21e-04 (3.32e-04)	Tok/s 34011 (23421)	Loss/tok 3.4091 (3.1645)	LR 7.813e-06
0: TRAIN [4][2590/5173]	Time 0.626 (0.608)	Data 1.30e-04 (3.31e-04)	Tok/s 26675 (23432)	Loss/tok 3.1977 (3.1646)	LR 7.813e-06
0: TRAIN [4][2600/5173]	Time 0.693 (0.608)	Data 1.23e-04 (3.30e-04)	Tok/s 33114 (23434)	Loss/tok 3.3982 (3.1646)	LR 7.813e-06
0: TRAIN [4][2610/5173]	Time 0.631 (0.608)	Data 1.28e-04 (3.30e-04)	Tok/s 26803 (23451)	Loss/tok 3.1434 (3.1649)	LR 7.813e-06
0: TRAIN [4][2620/5173]	Time 0.631 (0.608)	Data 1.20e-04 (3.29e-04)	Tok/s 26812 (23458)	Loss/tok 3.1186 (3.1652)	LR 7.813e-06
0: TRAIN [4][2630/5173]	Time 0.569 (0.608)	Data 1.24e-04 (3.28e-04)	Tok/s 18549 (23442)	Loss/tok 3.0262 (3.1648)	LR 7.813e-06
0: TRAIN [4][2640/5173]	Time 0.566 (0.608)	Data 1.26e-04 (3.27e-04)	Tok/s 18487 (23435)	Loss/tok 3.0264 (3.1646)	LR 7.813e-06
0: TRAIN [4][2650/5173]	Time 0.626 (0.607)	Data 1.19e-04 (3.27e-04)	Tok/s 26938 (23427)	Loss/tok 3.1620 (3.1643)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][2660/5173]	Time 0.632 (0.608)	Data 1.42e-04 (3.26e-04)	Tok/s 26988 (23441)	Loss/tok 3.2664 (3.1650)	LR 7.813e-06
0: TRAIN [4][2670/5173]	Time 0.630 (0.608)	Data 1.21e-04 (3.25e-04)	Tok/s 27314 (23444)	Loss/tok 3.0877 (3.1653)	LR 7.813e-06
0: TRAIN [4][2680/5173]	Time 0.495 (0.607)	Data 1.24e-04 (3.25e-04)	Tok/s 10594 (23420)	Loss/tok 2.6043 (3.1647)	LR 7.813e-06
0: TRAIN [4][2690/5173]	Time 0.626 (0.607)	Data 1.18e-04 (3.24e-04)	Tok/s 27426 (23405)	Loss/tok 3.2007 (3.1641)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2700/5173]	Time 0.768 (0.607)	Data 1.82e-04 (3.23e-04)	Tok/s 38788 (23421)	Loss/tok 3.4411 (3.1644)	LR 7.813e-06
0: TRAIN [4][2710/5173]	Time 0.568 (0.607)	Data 1.18e-04 (3.23e-04)	Tok/s 17695 (23416)	Loss/tok 2.9772 (3.1642)	LR 7.813e-06
0: TRAIN [4][2720/5173]	Time 0.689 (0.607)	Data 1.16e-04 (3.22e-04)	Tok/s 34118 (23426)	Loss/tok 3.3147 (3.1644)	LR 7.813e-06
0: TRAIN [4][2730/5173]	Time 0.567 (0.607)	Data 1.23e-04 (3.21e-04)	Tok/s 18503 (23423)	Loss/tok 3.0340 (3.1642)	LR 7.813e-06
0: TRAIN [4][2740/5173]	Time 0.568 (0.607)	Data 1.21e-04 (3.20e-04)	Tok/s 18067 (23416)	Loss/tok 2.9552 (3.1641)	LR 7.813e-06
0: TRAIN [4][2750/5173]	Time 0.568 (0.607)	Data 1.20e-04 (3.20e-04)	Tok/s 18030 (23402)	Loss/tok 2.9769 (3.1637)	LR 7.813e-06
0: TRAIN [4][2760/5173]	Time 0.564 (0.607)	Data 1.37e-04 (3.19e-04)	Tok/s 18593 (23418)	Loss/tok 2.9097 (3.1640)	LR 7.813e-06
0: TRAIN [4][2770/5173]	Time 0.690 (0.607)	Data 1.19e-04 (3.18e-04)	Tok/s 33765 (23422)	Loss/tok 3.3238 (3.1641)	LR 7.813e-06
0: TRAIN [4][2780/5173]	Time 0.558 (0.607)	Data 1.16e-04 (3.18e-04)	Tok/s 18265 (23415)	Loss/tok 2.9304 (3.1639)	LR 7.813e-06
0: TRAIN [4][2790/5173]	Time 0.567 (0.607)	Data 1.20e-04 (3.17e-04)	Tok/s 18215 (23406)	Loss/tok 2.8285 (3.1637)	LR 7.813e-06
0: TRAIN [4][2800/5173]	Time 0.566 (0.607)	Data 1.24e-04 (3.16e-04)	Tok/s 18127 (23403)	Loss/tok 3.0068 (3.1635)	LR 7.813e-06
0: TRAIN [4][2810/5173]	Time 0.755 (0.607)	Data 1.26e-04 (3.16e-04)	Tok/s 38965 (23425)	Loss/tok 3.5649 (3.1641)	LR 7.813e-06
0: TRAIN [4][2820/5173]	Time 0.567 (0.607)	Data 1.22e-04 (3.15e-04)	Tok/s 18348 (23410)	Loss/tok 2.8822 (3.1637)	LR 7.813e-06
0: TRAIN [4][2830/5173]	Time 0.563 (0.607)	Data 1.23e-04 (3.14e-04)	Tok/s 18176 (23399)	Loss/tok 3.0685 (3.1634)	LR 7.813e-06
0: TRAIN [4][2840/5173]	Time 0.565 (0.607)	Data 3.04e-04 (3.14e-04)	Tok/s 18738 (23396)	Loss/tok 2.8969 (3.1635)	LR 7.813e-06
0: TRAIN [4][2850/5173]	Time 0.761 (0.607)	Data 1.23e-04 (3.13e-04)	Tok/s 38270 (23392)	Loss/tok 3.5314 (3.1635)	LR 7.813e-06
0: TRAIN [4][2860/5173]	Time 0.633 (0.607)	Data 1.67e-04 (3.12e-04)	Tok/s 26928 (23393)	Loss/tok 3.0827 (3.1634)	LR 7.813e-06
0: TRAIN [4][2870/5173]	Time 0.567 (0.607)	Data 1.33e-04 (3.12e-04)	Tok/s 18207 (23405)	Loss/tok 3.0646 (3.1637)	LR 7.813e-06
0: TRAIN [4][2880/5173]	Time 0.494 (0.607)	Data 1.22e-04 (3.11e-04)	Tok/s 10627 (23385)	Loss/tok 2.5929 (3.1633)	LR 7.813e-06
0: TRAIN [4][2890/5173]	Time 0.627 (0.607)	Data 1.29e-04 (3.11e-04)	Tok/s 26958 (23387)	Loss/tok 3.3078 (3.1634)	LR 7.813e-06
0: TRAIN [4][2900/5173]	Time 0.565 (0.607)	Data 1.20e-04 (3.10e-04)	Tok/s 18358 (23384)	Loss/tok 2.9762 (3.1632)	LR 7.813e-06
0: TRAIN [4][2910/5173]	Time 0.568 (0.607)	Data 1.22e-04 (3.10e-04)	Tok/s 18396 (23393)	Loss/tok 3.0597 (3.1635)	LR 7.813e-06
0: TRAIN [4][2920/5173]	Time 0.568 (0.607)	Data 1.25e-04 (3.09e-04)	Tok/s 18273 (23402)	Loss/tok 2.9601 (3.1638)	LR 7.813e-06
0: TRAIN [4][2930/5173]	Time 0.632 (0.607)	Data 1.23e-04 (3.08e-04)	Tok/s 26431 (23413)	Loss/tok 3.2350 (3.1641)	LR 7.813e-06
0: TRAIN [4][2940/5173]	Time 0.631 (0.607)	Data 1.29e-04 (3.08e-04)	Tok/s 26517 (23413)	Loss/tok 3.1040 (3.1638)	LR 7.813e-06
0: TRAIN [4][2950/5173]	Time 0.567 (0.607)	Data 1.34e-04 (3.07e-04)	Tok/s 17784 (23424)	Loss/tok 2.8929 (3.1641)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][2960/5173]	Time 0.506 (0.607)	Data 1.90e-04 (3.07e-04)	Tok/s 9906 (23435)	Loss/tok 2.4839 (3.1646)	LR 7.813e-06
0: TRAIN [4][2970/5173]	Time 0.568 (0.607)	Data 1.72e-04 (3.06e-04)	Tok/s 18452 (23436)	Loss/tok 2.9352 (3.1647)	LR 7.813e-06
0: TRAIN [4][2980/5173]	Time 0.564 (0.607)	Data 1.53e-04 (3.06e-04)	Tok/s 18598 (23422)	Loss/tok 2.9432 (3.1642)	LR 7.813e-06
0: TRAIN [4][2990/5173]	Time 0.505 (0.607)	Data 1.46e-04 (3.05e-04)	Tok/s 10283 (23417)	Loss/tok 2.6444 (3.1643)	LR 7.813e-06
0: TRAIN [4][3000/5173]	Time 0.628 (0.607)	Data 1.30e-04 (3.04e-04)	Tok/s 26766 (23419)	Loss/tok 3.2043 (3.1643)	LR 7.813e-06
0: TRAIN [4][3010/5173]	Time 0.503 (0.607)	Data 1.27e-04 (3.04e-04)	Tok/s 10450 (23422)	Loss/tok 2.5896 (3.1646)	LR 7.813e-06
0: TRAIN [4][3020/5173]	Time 0.566 (0.607)	Data 1.47e-04 (3.03e-04)	Tok/s 18273 (23431)	Loss/tok 2.9107 (3.1648)	LR 7.813e-06
0: TRAIN [4][3030/5173]	Time 0.626 (0.607)	Data 1.46e-04 (3.03e-04)	Tok/s 26868 (23425)	Loss/tok 3.1457 (3.1646)	LR 7.813e-06
0: TRAIN [4][3040/5173]	Time 0.689 (0.607)	Data 1.32e-04 (3.02e-04)	Tok/s 33750 (23421)	Loss/tok 3.3127 (3.1645)	LR 7.813e-06
0: TRAIN [4][3050/5173]	Time 0.570 (0.607)	Data 1.20e-04 (3.02e-04)	Tok/s 18223 (23402)	Loss/tok 2.9726 (3.1642)	LR 7.813e-06
0: TRAIN [4][3060/5173]	Time 0.690 (0.607)	Data 1.18e-04 (3.01e-04)	Tok/s 34490 (23409)	Loss/tok 3.2254 (3.1645)	LR 7.813e-06
0: TRAIN [4][3070/5173]	Time 0.568 (0.607)	Data 1.18e-04 (3.00e-04)	Tok/s 18657 (23393)	Loss/tok 3.0356 (3.1641)	LR 7.813e-06
0: TRAIN [4][3080/5173]	Time 0.565 (0.607)	Data 1.26e-04 (3.00e-04)	Tok/s 18229 (23391)	Loss/tok 2.9926 (3.1640)	LR 7.813e-06
0: TRAIN [4][3090/5173]	Time 0.568 (0.607)	Data 1.43e-04 (2.99e-04)	Tok/s 18166 (23395)	Loss/tok 3.0154 (3.1640)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][3100/5173]	Time 0.626 (0.607)	Data 1.64e-04 (2.99e-04)	Tok/s 26999 (23395)	Loss/tok 3.1002 (3.1639)	LR 7.813e-06
0: TRAIN [4][3110/5173]	Time 0.685 (0.607)	Data 1.61e-04 (2.98e-04)	Tok/s 34043 (23403)	Loss/tok 3.2297 (3.1640)	LR 7.813e-06
0: TRAIN [4][3120/5173]	Time 0.689 (0.607)	Data 1.45e-04 (2.98e-04)	Tok/s 33660 (23408)	Loss/tok 3.3445 (3.1639)	LR 7.813e-06
0: TRAIN [4][3130/5173]	Time 0.765 (0.607)	Data 1.28e-04 (2.97e-04)	Tok/s 38588 (23410)	Loss/tok 3.5586 (3.1641)	LR 7.813e-06
0: TRAIN [4][3140/5173]	Time 0.507 (0.607)	Data 1.17e-04 (2.97e-04)	Tok/s 10683 (23404)	Loss/tok 2.5288 (3.1640)	LR 7.813e-06
0: TRAIN [4][3150/5173]	Time 0.568 (0.607)	Data 1.26e-04 (2.96e-04)	Tok/s 18022 (23417)	Loss/tok 3.0082 (3.1643)	LR 7.813e-06
0: TRAIN [4][3160/5173]	Time 0.628 (0.607)	Data 1.24e-04 (2.96e-04)	Tok/s 26705 (23424)	Loss/tok 3.1075 (3.1644)	LR 7.813e-06
0: TRAIN [4][3170/5173]	Time 0.566 (0.607)	Data 1.25e-04 (2.95e-04)	Tok/s 18170 (23423)	Loss/tok 2.9018 (3.1643)	LR 7.813e-06
0: TRAIN [4][3180/5173]	Time 0.507 (0.607)	Data 1.21e-04 (2.95e-04)	Tok/s 10279 (23410)	Loss/tok 2.5502 (3.1639)	LR 7.813e-06
0: TRAIN [4][3190/5173]	Time 0.505 (0.607)	Data 1.18e-04 (2.94e-04)	Tok/s 10212 (23410)	Loss/tok 2.5459 (3.1637)	LR 7.813e-06
0: TRAIN [4][3200/5173]	Time 0.566 (0.607)	Data 1.30e-04 (2.93e-04)	Tok/s 18136 (23420)	Loss/tok 2.9405 (3.1639)	LR 7.813e-06
0: TRAIN [4][3210/5173]	Time 0.633 (0.607)	Data 1.23e-04 (2.93e-04)	Tok/s 26384 (23417)	Loss/tok 3.1280 (3.1637)	LR 7.813e-06
0: TRAIN [4][3220/5173]	Time 0.631 (0.607)	Data 1.22e-04 (2.92e-04)	Tok/s 26872 (23418)	Loss/tok 3.1624 (3.1636)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][3230/5173]	Time 0.762 (0.607)	Data 1.21e-04 (2.92e-04)	Tok/s 39128 (23419)	Loss/tok 3.5799 (3.1639)	LR 7.813e-06
0: TRAIN [4][3240/5173]	Time 0.630 (0.607)	Data 1.24e-04 (2.92e-04)	Tok/s 26616 (23415)	Loss/tok 3.2073 (3.1638)	LR 7.813e-06
0: TRAIN [4][3250/5173]	Time 0.566 (0.607)	Data 1.25e-04 (2.91e-04)	Tok/s 17987 (23415)	Loss/tok 2.9597 (3.1637)	LR 7.813e-06
0: TRAIN [4][3260/5173]	Time 0.620 (0.607)	Data 1.37e-04 (2.91e-04)	Tok/s 27010 (23411)	Loss/tok 3.1297 (3.1637)	LR 7.813e-06
0: TRAIN [4][3270/5173]	Time 0.625 (0.607)	Data 1.26e-04 (2.90e-04)	Tok/s 26614 (23413)	Loss/tok 3.2242 (3.1638)	LR 7.813e-06
0: TRAIN [4][3280/5173]	Time 0.569 (0.607)	Data 1.19e-04 (2.90e-04)	Tok/s 18271 (23415)	Loss/tok 3.0423 (3.1638)	LR 7.813e-06
0: TRAIN [4][3290/5173]	Time 0.494 (0.607)	Data 1.30e-04 (2.89e-04)	Tok/s 10686 (23419)	Loss/tok 2.5666 (3.1639)	LR 7.813e-06
0: TRAIN [4][3300/5173]	Time 0.631 (0.607)	Data 1.24e-04 (2.89e-04)	Tok/s 26538 (23414)	Loss/tok 3.2451 (3.1637)	LR 7.813e-06
0: TRAIN [4][3310/5173]	Time 0.628 (0.607)	Data 1.26e-04 (2.88e-04)	Tok/s 26574 (23413)	Loss/tok 3.2446 (3.1638)	LR 7.813e-06
0: TRAIN [4][3320/5173]	Time 0.630 (0.607)	Data 1.24e-04 (2.88e-04)	Tok/s 26824 (23422)	Loss/tok 3.1622 (3.1638)	LR 7.813e-06
0: TRAIN [4][3330/5173]	Time 0.629 (0.607)	Data 1.23e-04 (2.87e-04)	Tok/s 26404 (23423)	Loss/tok 3.1055 (3.1638)	LR 7.813e-06
0: TRAIN [4][3340/5173]	Time 0.691 (0.607)	Data 1.25e-04 (2.87e-04)	Tok/s 33575 (23423)	Loss/tok 3.3533 (3.1638)	LR 7.813e-06
0: TRAIN [4][3350/5173]	Time 0.570 (0.607)	Data 1.24e-04 (2.86e-04)	Tok/s 18357 (23415)	Loss/tok 2.9096 (3.1636)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][3360/5173]	Time 0.688 (0.607)	Data 1.45e-04 (2.86e-04)	Tok/s 33141 (23416)	Loss/tok 3.5038 (3.1639)	LR 7.813e-06
0: TRAIN [4][3370/5173]	Time 0.625 (0.607)	Data 1.23e-04 (2.85e-04)	Tok/s 26643 (23411)	Loss/tok 3.0980 (3.1637)	LR 7.813e-06
0: TRAIN [4][3380/5173]	Time 0.625 (0.607)	Data 1.28e-04 (2.85e-04)	Tok/s 26940 (23415)	Loss/tok 3.1259 (3.1636)	LR 7.813e-06
0: TRAIN [4][3390/5173]	Time 0.508 (0.607)	Data 1.20e-04 (2.85e-04)	Tok/s 10298 (23421)	Loss/tok 2.5097 (3.1637)	LR 7.813e-06
0: TRAIN [4][3400/5173]	Time 0.691 (0.607)	Data 1.27e-04 (2.84e-04)	Tok/s 33590 (23435)	Loss/tok 3.4578 (3.1640)	LR 7.813e-06
0: TRAIN [4][3410/5173]	Time 0.568 (0.607)	Data 1.21e-04 (2.84e-04)	Tok/s 18495 (23441)	Loss/tok 3.0578 (3.1641)	LR 7.813e-06
0: TRAIN [4][3420/5173]	Time 0.505 (0.608)	Data 1.24e-04 (2.83e-04)	Tok/s 10595 (23450)	Loss/tok 2.6320 (3.1643)	LR 7.813e-06
0: TRAIN [4][3430/5173]	Time 0.568 (0.608)	Data 1.22e-04 (2.83e-04)	Tok/s 18042 (23446)	Loss/tok 2.9909 (3.1643)	LR 7.813e-06
0: TRAIN [4][3440/5173]	Time 0.694 (0.608)	Data 1.21e-04 (2.82e-04)	Tok/s 33553 (23461)	Loss/tok 3.4792 (3.1647)	LR 7.813e-06
0: TRAIN [4][3450/5173]	Time 0.692 (0.608)	Data 1.41e-04 (2.82e-04)	Tok/s 33544 (23464)	Loss/tok 3.3673 (3.1647)	LR 7.813e-06
0: TRAIN [4][3460/5173]	Time 0.626 (0.608)	Data 1.24e-04 (2.82e-04)	Tok/s 26778 (23462)	Loss/tok 3.0965 (3.1645)	LR 7.813e-06
0: TRAIN [4][3470/5173]	Time 0.565 (0.608)	Data 1.19e-04 (2.81e-04)	Tok/s 18257 (23457)	Loss/tok 2.9459 (3.1643)	LR 7.813e-06
0: TRAIN [4][3480/5173]	Time 0.569 (0.608)	Data 1.28e-04 (2.81e-04)	Tok/s 18179 (23452)	Loss/tok 2.8716 (3.1642)	LR 7.813e-06
0: TRAIN [4][3490/5173]	Time 0.568 (0.607)	Data 1.20e-04 (2.80e-04)	Tok/s 18314 (23433)	Loss/tok 2.8863 (3.1638)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][3500/5173]	Time 0.631 (0.607)	Data 1.24e-04 (2.80e-04)	Tok/s 26706 (23437)	Loss/tok 3.1737 (3.1637)	LR 7.813e-06
0: TRAIN [4][3510/5173]	Time 0.547 (0.607)	Data 1.19e-04 (2.79e-04)	Tok/s 18212 (23439)	Loss/tok 2.8848 (3.1637)	LR 7.813e-06
0: TRAIN [4][3520/5173]	Time 0.507 (0.607)	Data 1.82e-04 (2.79e-04)	Tok/s 10384 (23423)	Loss/tok 2.5935 (3.1635)	LR 7.813e-06
0: TRAIN [4][3530/5173]	Time 0.689 (0.607)	Data 1.23e-04 (2.79e-04)	Tok/s 33897 (23425)	Loss/tok 3.2276 (3.1634)	LR 7.813e-06
0: TRAIN [4][3540/5173]	Time 0.564 (0.607)	Data 1.40e-04 (2.78e-04)	Tok/s 17931 (23424)	Loss/tok 3.0692 (3.1634)	LR 7.813e-06
0: TRAIN [4][3550/5173]	Time 0.633 (0.607)	Data 1.24e-04 (2.78e-04)	Tok/s 26246 (23427)	Loss/tok 3.2455 (3.1634)	LR 7.813e-06
0: TRAIN [4][3560/5173]	Time 0.568 (0.607)	Data 1.28e-04 (2.77e-04)	Tok/s 18327 (23423)	Loss/tok 2.8826 (3.1633)	LR 7.813e-06
0: TRAIN [4][3570/5173]	Time 0.631 (0.607)	Data 1.31e-04 (2.77e-04)	Tok/s 26577 (23432)	Loss/tok 3.1366 (3.1636)	LR 7.813e-06
0: TRAIN [4][3580/5173]	Time 0.630 (0.607)	Data 1.19e-04 (2.77e-04)	Tok/s 26410 (23427)	Loss/tok 3.2423 (3.1635)	LR 7.813e-06
0: TRAIN [4][3590/5173]	Time 0.566 (0.607)	Data 1.26e-04 (2.76e-04)	Tok/s 18350 (23427)	Loss/tok 3.0035 (3.1636)	LR 7.813e-06
0: TRAIN [4][3600/5173]	Time 0.569 (0.607)	Data 1.28e-04 (2.76e-04)	Tok/s 17719 (23418)	Loss/tok 2.9385 (3.1633)	LR 7.813e-06
0: TRAIN [4][3610/5173]	Time 0.504 (0.607)	Data 1.20e-04 (2.75e-04)	Tok/s 10535 (23410)	Loss/tok 2.5166 (3.1632)	LR 7.813e-06
0: TRAIN [4][3620/5173]	Time 0.693 (0.607)	Data 1.22e-04 (2.75e-04)	Tok/s 33748 (23412)	Loss/tok 3.4177 (3.1633)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][3630/5173]	Time 0.627 (0.607)	Data 1.17e-04 (2.74e-04)	Tok/s 26407 (23409)	Loss/tok 3.1235 (3.1632)	LR 7.813e-06
0: TRAIN [4][3640/5173]	Time 0.566 (0.607)	Data 1.90e-04 (2.74e-04)	Tok/s 18288 (23399)	Loss/tok 2.9497 (3.1630)	LR 7.813e-06
0: TRAIN [4][3650/5173]	Time 0.633 (0.607)	Data 1.47e-04 (2.74e-04)	Tok/s 26766 (23408)	Loss/tok 3.3463 (3.1631)	LR 7.813e-06
0: TRAIN [4][3660/5173]	Time 0.567 (0.607)	Data 1.22e-04 (2.73e-04)	Tok/s 17926 (23404)	Loss/tok 2.9306 (3.1631)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3670/5173]	Time 0.567 (0.607)	Data 1.27e-04 (2.73e-04)	Tok/s 18058 (23399)	Loss/tok 2.9333 (3.1631)	LR 7.813e-06
0: TRAIN [4][3680/5173]	Time 0.690 (0.607)	Data 1.29e-04 (2.73e-04)	Tok/s 33947 (23398)	Loss/tok 3.3111 (3.1630)	LR 7.813e-06
0: TRAIN [4][3690/5173]	Time 0.570 (0.607)	Data 1.36e-04 (2.72e-04)	Tok/s 18006 (23412)	Loss/tok 2.8786 (3.1632)	LR 7.813e-06
0: TRAIN [4][3700/5173]	Time 0.630 (0.607)	Data 1.27e-04 (2.72e-04)	Tok/s 26716 (23410)	Loss/tok 3.1421 (3.1633)	LR 7.813e-06
0: TRAIN [4][3710/5173]	Time 0.506 (0.607)	Data 1.22e-04 (2.71e-04)	Tok/s 10429 (23410)	Loss/tok 2.6152 (3.1634)	LR 7.813e-06
0: TRAIN [4][3720/5173]	Time 0.691 (0.607)	Data 1.25e-04 (2.71e-04)	Tok/s 33802 (23403)	Loss/tok 3.4022 (3.1632)	LR 7.813e-06
0: TRAIN [4][3730/5173]	Time 0.569 (0.607)	Data 1.34e-04 (2.71e-04)	Tok/s 18361 (23408)	Loss/tok 2.9166 (3.1633)	LR 7.813e-06
0: TRAIN [4][3740/5173]	Time 0.567 (0.607)	Data 1.27e-04 (2.70e-04)	Tok/s 18370 (23405)	Loss/tok 2.8778 (3.1632)	LR 7.813e-06
0: TRAIN [4][3750/5173]	Time 0.568 (0.607)	Data 1.22e-04 (2.70e-04)	Tok/s 18353 (23389)	Loss/tok 2.9550 (3.1629)	LR 7.813e-06
0: TRAIN [4][3760/5173]	Time 0.567 (0.607)	Data 1.20e-04 (2.70e-04)	Tok/s 17994 (23389)	Loss/tok 2.9591 (3.1627)	LR 7.813e-06
0: TRAIN [4][3770/5173]	Time 0.568 (0.607)	Data 1.25e-04 (2.69e-04)	Tok/s 18030 (23392)	Loss/tok 3.1079 (3.1628)	LR 7.813e-06
0: TRAIN [4][3780/5173]	Time 0.566 (0.607)	Data 1.31e-04 (2.69e-04)	Tok/s 18362 (23390)	Loss/tok 2.9888 (3.1628)	LR 7.813e-06
0: TRAIN [4][3790/5173]	Time 0.691 (0.607)	Data 1.21e-04 (2.69e-04)	Tok/s 34030 (23385)	Loss/tok 3.3616 (3.1627)	LR 7.813e-06
0: TRAIN [4][3800/5173]	Time 0.504 (0.607)	Data 1.27e-04 (2.68e-04)	Tok/s 10478 (23373)	Loss/tok 2.5920 (3.1626)	LR 7.813e-06
0: TRAIN [4][3810/5173]	Time 0.566 (0.607)	Data 1.94e-04 (2.68e-04)	Tok/s 18156 (23375)	Loss/tok 2.9344 (3.1627)	LR 7.813e-06
0: TRAIN [4][3820/5173]	Time 0.629 (0.607)	Data 1.16e-04 (2.68e-04)	Tok/s 26698 (23376)	Loss/tok 3.1493 (3.1626)	LR 7.813e-06
0: TRAIN [4][3830/5173]	Time 0.631 (0.607)	Data 1.22e-04 (2.67e-04)	Tok/s 26347 (23372)	Loss/tok 3.1911 (3.1624)	LR 7.813e-06
0: TRAIN [4][3840/5173]	Time 0.632 (0.607)	Data 1.23e-04 (2.67e-04)	Tok/s 26393 (23367)	Loss/tok 3.1163 (3.1621)	LR 7.813e-06
0: TRAIN [4][3850/5173]	Time 0.688 (0.607)	Data 1.20e-04 (2.66e-04)	Tok/s 33694 (23366)	Loss/tok 3.3758 (3.1621)	LR 7.813e-06
0: TRAIN [4][3860/5173]	Time 0.633 (0.607)	Data 1.28e-04 (2.66e-04)	Tok/s 26524 (23359)	Loss/tok 3.0977 (3.1619)	LR 7.813e-06
0: TRAIN [4][3870/5173]	Time 0.569 (0.607)	Data 1.27e-04 (2.66e-04)	Tok/s 17867 (23352)	Loss/tok 3.0140 (3.1616)	LR 7.813e-06
0: TRAIN [4][3880/5173]	Time 0.626 (0.607)	Data 1.21e-04 (2.65e-04)	Tok/s 26841 (23350)	Loss/tok 3.1025 (3.1614)	LR 7.813e-06
0: TRAIN [4][3890/5173]	Time 0.568 (0.607)	Data 1.29e-04 (2.65e-04)	Tok/s 18552 (23355)	Loss/tok 2.8703 (3.1615)	LR 7.813e-06
0: TRAIN [4][3900/5173]	Time 0.627 (0.607)	Data 1.25e-04 (2.65e-04)	Tok/s 26906 (23356)	Loss/tok 3.1603 (3.1616)	LR 7.813e-06
0: TRAIN [4][3910/5173]	Time 0.630 (0.607)	Data 1.17e-04 (2.64e-04)	Tok/s 26991 (23355)	Loss/tok 3.1421 (3.1616)	LR 7.813e-06
0: TRAIN [4][3920/5173]	Time 0.691 (0.607)	Data 1.28e-04 (2.64e-04)	Tok/s 33884 (23354)	Loss/tok 3.3249 (3.1616)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][3930/5173]	Time 0.630 (0.607)	Data 1.29e-04 (2.64e-04)	Tok/s 26486 (23360)	Loss/tok 3.1716 (3.1615)	LR 7.813e-06
0: TRAIN [4][3940/5173]	Time 0.568 (0.607)	Data 1.27e-04 (2.63e-04)	Tok/s 18044 (23368)	Loss/tok 2.9150 (3.1615)	LR 7.813e-06
0: TRAIN [4][3950/5173]	Time 0.626 (0.607)	Data 1.15e-04 (2.63e-04)	Tok/s 27096 (23368)	Loss/tok 3.2384 (3.1616)	LR 7.813e-06
0: TRAIN [4][3960/5173]	Time 0.626 (0.607)	Data 1.23e-04 (2.63e-04)	Tok/s 27107 (23371)	Loss/tok 3.0445 (3.1617)	LR 7.813e-06
0: TRAIN [4][3970/5173]	Time 0.633 (0.607)	Data 1.23e-04 (2.62e-04)	Tok/s 26699 (23375)	Loss/tok 3.2555 (3.1617)	LR 7.813e-06
0: TRAIN [4][3980/5173]	Time 0.568 (0.607)	Data 1.19e-04 (2.62e-04)	Tok/s 18191 (23369)	Loss/tok 2.9708 (3.1618)	LR 7.813e-06
0: TRAIN [4][3990/5173]	Time 0.569 (0.607)	Data 1.28e-04 (2.62e-04)	Tok/s 18352 (23362)	Loss/tok 2.9620 (3.1617)	LR 7.813e-06
0: TRAIN [4][4000/5173]	Time 0.629 (0.607)	Data 1.23e-04 (2.62e-04)	Tok/s 26400 (23362)	Loss/tok 3.1132 (3.1615)	LR 7.813e-06
0: TRAIN [4][4010/5173]	Time 0.630 (0.607)	Data 1.17e-04 (2.61e-04)	Tok/s 26294 (23359)	Loss/tok 3.2037 (3.1615)	LR 7.813e-06
0: TRAIN [4][4020/5173]	Time 0.567 (0.607)	Data 1.20e-04 (2.61e-04)	Tok/s 18124 (23353)	Loss/tok 3.0241 (3.1613)	LR 7.813e-06
0: TRAIN [4][4030/5173]	Time 0.570 (0.607)	Data 1.24e-04 (2.61e-04)	Tok/s 18050 (23339)	Loss/tok 2.9776 (3.1609)	LR 7.813e-06
0: TRAIN [4][4040/5173]	Time 0.565 (0.607)	Data 1.19e-04 (2.60e-04)	Tok/s 18089 (23328)	Loss/tok 2.9693 (3.1607)	LR 7.813e-06
0: TRAIN [4][4050/5173]	Time 0.562 (0.607)	Data 1.29e-04 (2.60e-04)	Tok/s 18708 (23321)	Loss/tok 2.9441 (3.1604)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][4060/5173]	Time 0.564 (0.607)	Data 1.26e-04 (2.60e-04)	Tok/s 18337 (23325)	Loss/tok 2.9943 (3.1606)	LR 7.813e-06
0: TRAIN [4][4070/5173]	Time 0.690 (0.607)	Data 1.21e-04 (2.59e-04)	Tok/s 33638 (23325)	Loss/tok 3.4509 (3.1606)	LR 7.813e-06
0: TRAIN [4][4080/5173]	Time 0.568 (0.607)	Data 1.21e-04 (2.59e-04)	Tok/s 18374 (23326)	Loss/tok 2.9654 (3.1606)	LR 7.813e-06
0: TRAIN [4][4090/5173]	Time 0.629 (0.607)	Data 1.27e-04 (2.59e-04)	Tok/s 26538 (23322)	Loss/tok 3.2152 (3.1605)	LR 7.813e-06
0: TRAIN [4][4100/5173]	Time 0.693 (0.607)	Data 1.26e-04 (2.59e-04)	Tok/s 33559 (23328)	Loss/tok 3.3249 (3.1606)	LR 7.813e-06
0: TRAIN [4][4110/5173]	Time 0.629 (0.607)	Data 1.24e-04 (2.58e-04)	Tok/s 26697 (23326)	Loss/tok 3.1944 (3.1605)	LR 7.813e-06
0: TRAIN [4][4120/5173]	Time 0.636 (0.607)	Data 1.31e-04 (2.58e-04)	Tok/s 26618 (23325)	Loss/tok 3.2527 (3.1605)	LR 7.813e-06
0: TRAIN [4][4130/5173]	Time 0.569 (0.607)	Data 1.33e-04 (2.58e-04)	Tok/s 17896 (23321)	Loss/tok 2.9807 (3.1603)	LR 7.813e-06
0: TRAIN [4][4140/5173]	Time 0.627 (0.606)	Data 1.24e-04 (2.57e-04)	Tok/s 26658 (23318)	Loss/tok 3.1948 (3.1603)	LR 7.813e-06
0: TRAIN [4][4150/5173]	Time 0.507 (0.606)	Data 1.32e-04 (2.57e-04)	Tok/s 10805 (23313)	Loss/tok 2.6010 (3.1603)	LR 7.813e-06
0: TRAIN [4][4160/5173]	Time 0.568 (0.606)	Data 1.24e-04 (2.57e-04)	Tok/s 18291 (23313)	Loss/tok 3.0853 (3.1603)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][4170/5173]	Time 0.632 (0.606)	Data 1.21e-04 (2.56e-04)	Tok/s 26569 (23316)	Loss/tok 3.1521 (3.1606)	LR 7.813e-06
0: TRAIN [4][4180/5173]	Time 0.635 (0.606)	Data 1.25e-04 (2.56e-04)	Tok/s 26329 (23314)	Loss/tok 3.1642 (3.1605)	LR 7.813e-06
0: TRAIN [4][4190/5173]	Time 0.508 (0.607)	Data 1.20e-04 (2.56e-04)	Tok/s 10470 (23317)	Loss/tok 2.5717 (3.1607)	LR 7.813e-06
0: TRAIN [4][4200/5173]	Time 0.630 (0.606)	Data 1.19e-04 (2.56e-04)	Tok/s 26958 (23310)	Loss/tok 3.2022 (3.1605)	LR 7.813e-06
0: TRAIN [4][4210/5173]	Time 0.566 (0.607)	Data 1.33e-04 (2.55e-04)	Tok/s 18170 (23318)	Loss/tok 2.9614 (3.1608)	LR 7.813e-06
0: TRAIN [4][4220/5173]	Time 0.565 (0.607)	Data 1.30e-04 (2.55e-04)	Tok/s 18206 (23322)	Loss/tok 3.0320 (3.1610)	LR 7.813e-06
0: TRAIN [4][4230/5173]	Time 0.629 (0.607)	Data 1.17e-04 (2.55e-04)	Tok/s 26826 (23318)	Loss/tok 3.1239 (3.1609)	LR 7.813e-06
0: TRAIN [4][4240/5173]	Time 0.756 (0.607)	Data 1.23e-04 (2.54e-04)	Tok/s 39948 (23322)	Loss/tok 3.4607 (3.1610)	LR 7.813e-06
0: TRAIN [4][4250/5173]	Time 0.762 (0.607)	Data 1.27e-04 (2.54e-04)	Tok/s 38780 (23327)	Loss/tok 3.5630 (3.1612)	LR 7.813e-06
0: TRAIN [4][4260/5173]	Time 0.569 (0.607)	Data 1.34e-04 (2.54e-04)	Tok/s 18109 (23333)	Loss/tok 2.9648 (3.1614)	LR 7.813e-06
0: TRAIN [4][4270/5173]	Time 0.568 (0.607)	Data 1.22e-04 (2.53e-04)	Tok/s 18033 (23327)	Loss/tok 3.0483 (3.1612)	LR 7.813e-06
0: TRAIN [4][4280/5173]	Time 0.507 (0.607)	Data 1.21e-04 (2.53e-04)	Tok/s 10147 (23319)	Loss/tok 2.5374 (3.1610)	LR 7.813e-06
0: TRAIN [4][4290/5173]	Time 0.565 (0.607)	Data 1.40e-04 (2.53e-04)	Tok/s 18323 (23336)	Loss/tok 2.9050 (3.1613)	LR 7.813e-06
0: TRAIN [4][4300/5173]	Time 0.565 (0.607)	Data 1.25e-04 (2.53e-04)	Tok/s 18317 (23336)	Loss/tok 2.9736 (3.1613)	LR 7.813e-06
0: TRAIN [4][4310/5173]	Time 0.635 (0.607)	Data 1.36e-04 (2.52e-04)	Tok/s 26272 (23335)	Loss/tok 3.1359 (3.1613)	LR 7.813e-06
0: TRAIN [4][4320/5173]	Time 0.689 (0.607)	Data 1.44e-04 (2.52e-04)	Tok/s 33611 (23336)	Loss/tok 3.3660 (3.1613)	LR 7.813e-06
0: TRAIN [4][4330/5173]	Time 0.506 (0.607)	Data 1.28e-04 (2.52e-04)	Tok/s 10709 (23333)	Loss/tok 2.4896 (3.1613)	LR 7.813e-06
0: TRAIN [4][4340/5173]	Time 0.569 (0.607)	Data 1.23e-04 (2.51e-04)	Tok/s 18351 (23332)	Loss/tok 2.8180 (3.1612)	LR 7.813e-06
0: TRAIN [4][4350/5173]	Time 0.564 (0.607)	Data 2.98e-04 (2.51e-04)	Tok/s 18132 (23336)	Loss/tok 3.0293 (3.1614)	LR 7.813e-06
0: TRAIN [4][4360/5173]	Time 0.570 (0.607)	Data 1.20e-04 (2.51e-04)	Tok/s 18359 (23333)	Loss/tok 3.0529 (3.1613)	LR 7.813e-06
0: TRAIN [4][4370/5173]	Time 0.635 (0.607)	Data 1.24e-04 (2.51e-04)	Tok/s 26348 (23329)	Loss/tok 3.2257 (3.1612)	LR 7.813e-06
0: TRAIN [4][4380/5173]	Time 0.569 (0.607)	Data 1.20e-04 (2.50e-04)	Tok/s 17970 (23326)	Loss/tok 2.8164 (3.1612)	LR 7.813e-06
0: TRAIN [4][4390/5173]	Time 0.625 (0.607)	Data 1.21e-04 (2.50e-04)	Tok/s 27140 (23324)	Loss/tok 3.1544 (3.1612)	LR 7.813e-06
0: TRAIN [4][4400/5173]	Time 0.626 (0.607)	Data 1.24e-04 (2.50e-04)	Tok/s 26936 (23326)	Loss/tok 3.0631 (3.1614)	LR 7.813e-06
0: TRAIN [4][4410/5173]	Time 0.627 (0.607)	Data 1.34e-04 (2.50e-04)	Tok/s 26743 (23333)	Loss/tok 3.1536 (3.1617)	LR 7.813e-06
0: TRAIN [4][4420/5173]	Time 0.567 (0.607)	Data 1.16e-04 (2.49e-04)	Tok/s 18676 (23335)	Loss/tok 3.0368 (3.1618)	LR 7.813e-06
0: TRAIN [4][4430/5173]	Time 0.566 (0.607)	Data 1.24e-04 (2.49e-04)	Tok/s 18327 (23324)	Loss/tok 2.9950 (3.1615)	LR 7.813e-06
0: TRAIN [4][4440/5173]	Time 0.564 (0.607)	Data 1.32e-04 (2.49e-04)	Tok/s 18102 (23326)	Loss/tok 2.8885 (3.1614)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][4450/5173]	Time 0.570 (0.607)	Data 1.20e-04 (2.49e-04)	Tok/s 18186 (23327)	Loss/tok 2.8981 (3.1613)	LR 7.813e-06
0: TRAIN [4][4460/5173]	Time 0.485 (0.607)	Data 1.22e-04 (2.48e-04)	Tok/s 10696 (23316)	Loss/tok 2.4973 (3.1611)	LR 7.813e-06
0: TRAIN [4][4470/5173]	Time 0.491 (0.607)	Data 1.26e-04 (2.48e-04)	Tok/s 10844 (23316)	Loss/tok 2.6565 (3.1610)	LR 7.813e-06
0: TRAIN [4][4480/5173]	Time 0.505 (0.607)	Data 1.17e-04 (2.48e-04)	Tok/s 10543 (23311)	Loss/tok 2.5848 (3.1610)	LR 7.813e-06
0: TRAIN [4][4490/5173]	Time 0.567 (0.607)	Data 1.24e-04 (2.48e-04)	Tok/s 18343 (23311)	Loss/tok 3.0016 (3.1610)	LR 7.813e-06
0: TRAIN [4][4500/5173]	Time 0.568 (0.606)	Data 1.24e-04 (2.47e-04)	Tok/s 17912 (23307)	Loss/tok 2.8912 (3.1609)	LR 7.813e-06
0: TRAIN [4][4510/5173]	Time 0.631 (0.606)	Data 1.23e-04 (2.47e-04)	Tok/s 26719 (23312)	Loss/tok 3.2347 (3.1610)	LR 7.813e-06
0: TRAIN [4][4520/5173]	Time 0.630 (0.607)	Data 1.25e-04 (2.47e-04)	Tok/s 26638 (23316)	Loss/tok 3.1716 (3.1611)	LR 7.813e-06
0: TRAIN [4][4530/5173]	Time 0.563 (0.607)	Data 1.24e-04 (2.47e-04)	Tok/s 18359 (23315)	Loss/tok 3.0407 (3.1612)	LR 7.813e-06
0: TRAIN [4][4540/5173]	Time 0.625 (0.606)	Data 1.21e-04 (2.46e-04)	Tok/s 26988 (23307)	Loss/tok 3.1021 (3.1611)	LR 7.813e-06
0: TRAIN [4][4550/5173]	Time 0.507 (0.606)	Data 1.19e-04 (2.46e-04)	Tok/s 10502 (23296)	Loss/tok 2.5512 (3.1608)	LR 7.813e-06
0: TRAIN [4][4560/5173]	Time 0.629 (0.606)	Data 1.26e-04 (2.46e-04)	Tok/s 26655 (23289)	Loss/tok 3.0518 (3.1606)	LR 7.813e-06
0: TRAIN [4][4570/5173]	Time 0.624 (0.606)	Data 1.20e-04 (2.46e-04)	Tok/s 27141 (23289)	Loss/tok 3.2671 (3.1606)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][4580/5173]	Time 0.629 (0.606)	Data 1.26e-04 (2.45e-04)	Tok/s 26860 (23295)	Loss/tok 3.1508 (3.1606)	LR 7.813e-06
0: TRAIN [4][4590/5173]	Time 0.569 (0.606)	Data 1.24e-04 (2.45e-04)	Tok/s 18181 (23303)	Loss/tok 2.9698 (3.1609)	LR 7.813e-06
0: TRAIN [4][4600/5173]	Time 0.629 (0.606)	Data 1.30e-04 (2.45e-04)	Tok/s 26743 (23304)	Loss/tok 3.0292 (3.1608)	LR 7.813e-06
0: TRAIN [4][4610/5173]	Time 0.569 (0.606)	Data 1.30e-04 (2.45e-04)	Tok/s 18283 (23312)	Loss/tok 3.0225 (3.1611)	LR 7.813e-06
0: TRAIN [4][4620/5173]	Time 0.570 (0.606)	Data 1.24e-04 (2.44e-04)	Tok/s 18663 (23310)	Loss/tok 2.9282 (3.1609)	LR 7.813e-06
0: TRAIN [4][4630/5173]	Time 0.691 (0.606)	Data 1.31e-04 (2.44e-04)	Tok/s 34077 (23306)	Loss/tok 3.2343 (3.1608)	LR 7.813e-06
0: TRAIN [4][4640/5173]	Time 0.627 (0.606)	Data 1.26e-04 (2.44e-04)	Tok/s 26492 (23311)	Loss/tok 3.2388 (3.1608)	LR 7.813e-06
0: TRAIN [4][4650/5173]	Time 0.565 (0.606)	Data 1.27e-04 (2.44e-04)	Tok/s 18529 (23304)	Loss/tok 2.9890 (3.1606)	LR 7.813e-06
0: TRAIN [4][4660/5173]	Time 0.565 (0.606)	Data 1.23e-04 (2.44e-04)	Tok/s 18236 (23295)	Loss/tok 2.9384 (3.1604)	LR 7.813e-06
0: TRAIN [4][4670/5173]	Time 0.566 (0.606)	Data 1.23e-04 (2.43e-04)	Tok/s 18463 (23300)	Loss/tok 2.9641 (3.1605)	LR 7.813e-06
0: TRAIN [4][4680/5173]	Time 0.570 (0.606)	Data 1.22e-04 (2.43e-04)	Tok/s 17954 (23304)	Loss/tok 3.0129 (3.1606)	LR 7.813e-06
0: TRAIN [4][4690/5173]	Time 0.626 (0.606)	Data 1.47e-04 (2.43e-04)	Tok/s 26818 (23305)	Loss/tok 3.1263 (3.1606)	LR 7.813e-06
0: TRAIN [4][4700/5173]	Time 0.625 (0.606)	Data 1.19e-04 (2.43e-04)	Tok/s 26754 (23310)	Loss/tok 3.2085 (3.1606)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][4710/5173]	Time 0.509 (0.607)	Data 1.37e-04 (2.42e-04)	Tok/s 10528 (23318)	Loss/tok 2.5974 (3.1609)	LR 7.813e-06
0: TRAIN [4][4720/5173]	Time 0.627 (0.607)	Data 1.27e-04 (2.42e-04)	Tok/s 26681 (23329)	Loss/tok 3.1123 (3.1610)	LR 7.813e-06
0: TRAIN [4][4730/5173]	Time 0.690 (0.607)	Data 1.21e-04 (2.42e-04)	Tok/s 34044 (23330)	Loss/tok 3.3435 (3.1610)	LR 7.813e-06
0: TRAIN [4][4740/5173]	Time 0.564 (0.607)	Data 1.16e-04 (2.42e-04)	Tok/s 18611 (23324)	Loss/tok 2.8777 (3.1609)	LR 7.813e-06
0: TRAIN [4][4750/5173]	Time 0.565 (0.607)	Data 1.27e-04 (2.41e-04)	Tok/s 18194 (23320)	Loss/tok 2.9081 (3.1607)	LR 7.813e-06
0: TRAIN [4][4760/5173]	Time 0.567 (0.606)	Data 1.20e-04 (2.41e-04)	Tok/s 18080 (23316)	Loss/tok 2.9983 (3.1605)	LR 7.813e-06
0: TRAIN [4][4770/5173]	Time 0.693 (0.606)	Data 1.26e-04 (2.41e-04)	Tok/s 33665 (23314)	Loss/tok 3.3180 (3.1605)	LR 7.813e-06
0: TRAIN [4][4780/5173]	Time 0.630 (0.606)	Data 1.22e-04 (2.41e-04)	Tok/s 27230 (23310)	Loss/tok 3.2194 (3.1605)	LR 7.813e-06
0: TRAIN [4][4790/5173]	Time 0.628 (0.606)	Data 1.21e-04 (2.40e-04)	Tok/s 27043 (23310)	Loss/tok 3.0869 (3.1604)	LR 7.813e-06
0: TRAIN [4][4800/5173]	Time 0.761 (0.606)	Data 1.17e-04 (2.40e-04)	Tok/s 39236 (23306)	Loss/tok 3.4653 (3.1604)	LR 7.813e-06
0: TRAIN [4][4810/5173]	Time 0.567 (0.606)	Data 1.24e-04 (2.40e-04)	Tok/s 18564 (23313)	Loss/tok 3.0587 (3.1606)	LR 7.813e-06
0: TRAIN [4][4820/5173]	Time 0.625 (0.606)	Data 1.22e-04 (2.40e-04)	Tok/s 27007 (23313)	Loss/tok 3.1096 (3.1605)	LR 7.813e-06
0: TRAIN [4][4830/5173]	Time 0.622 (0.606)	Data 1.21e-04 (2.40e-04)	Tok/s 27351 (23317)	Loss/tok 3.2826 (3.1606)	LR 7.813e-06
0: TRAIN [4][4840/5173]	Time 0.626 (0.606)	Data 1.32e-04 (2.39e-04)	Tok/s 26452 (23304)	Loss/tok 3.0461 (3.1604)	LR 7.813e-06
0: TRAIN [4][4850/5173]	Time 0.568 (0.606)	Data 1.25e-04 (2.39e-04)	Tok/s 18023 (23296)	Loss/tok 2.8762 (3.1601)	LR 7.813e-06
0: TRAIN [4][4860/5173]	Time 0.631 (0.606)	Data 1.29e-04 (2.39e-04)	Tok/s 26051 (23296)	Loss/tok 3.1312 (3.1600)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][4870/5173]	Time 0.691 (0.606)	Data 1.20e-04 (2.39e-04)	Tok/s 33376 (23295)	Loss/tok 3.2706 (3.1600)	LR 7.813e-06
0: TRAIN [4][4880/5173]	Time 0.693 (0.606)	Data 1.20e-04 (2.39e-04)	Tok/s 33595 (23293)	Loss/tok 3.2566 (3.1599)	LR 7.813e-06
0: TRAIN [4][4890/5173]	Time 0.568 (0.606)	Data 1.25e-04 (2.38e-04)	Tok/s 18238 (23292)	Loss/tok 3.0455 (3.1600)	LR 7.813e-06
0: TRAIN [4][4900/5173]	Time 0.568 (0.606)	Data 1.32e-04 (2.38e-04)	Tok/s 18311 (23288)	Loss/tok 2.9386 (3.1599)	LR 7.813e-06
0: TRAIN [4][4910/5173]	Time 0.567 (0.606)	Data 1.23e-04 (2.38e-04)	Tok/s 18112 (23287)	Loss/tok 3.0889 (3.1600)	LR 7.813e-06
0: TRAIN [4][4920/5173]	Time 0.565 (0.606)	Data 1.24e-04 (2.38e-04)	Tok/s 18239 (23291)	Loss/tok 2.9583 (3.1601)	LR 7.813e-06
0: TRAIN [4][4930/5173]	Time 0.561 (0.606)	Data 1.30e-04 (2.37e-04)	Tok/s 18145 (23292)	Loss/tok 2.8855 (3.1600)	LR 7.813e-06
0: TRAIN [4][4940/5173]	Time 0.566 (0.606)	Data 1.42e-04 (2.37e-04)	Tok/s 18358 (23295)	Loss/tok 2.9738 (3.1600)	LR 7.813e-06
0: TRAIN [4][4950/5173]	Time 0.627 (0.606)	Data 1.28e-04 (2.37e-04)	Tok/s 26792 (23290)	Loss/tok 3.0461 (3.1599)	LR 7.813e-06
0: TRAIN [4][4960/5173]	Time 0.629 (0.606)	Data 1.25e-04 (2.37e-04)	Tok/s 26871 (23295)	Loss/tok 3.2067 (3.1599)	LR 7.813e-06
0: TRAIN [4][4970/5173]	Time 0.565 (0.606)	Data 1.32e-04 (2.37e-04)	Tok/s 18415 (23296)	Loss/tok 2.9476 (3.1600)	LR 7.813e-06
0: TRAIN [4][4980/5173]	Time 0.631 (0.606)	Data 1.27e-04 (2.37e-04)	Tok/s 26514 (23293)	Loss/tok 3.0786 (3.1599)	LR 7.813e-06
0: TRAIN [4][4990/5173]	Time 0.692 (0.606)	Data 1.24e-04 (2.36e-04)	Tok/s 33508 (23293)	Loss/tok 3.3788 (3.1599)	LR 7.813e-06
0: TRAIN [4][5000/5173]	Time 0.621 (0.606)	Data 1.30e-04 (2.36e-04)	Tok/s 26771 (23297)	Loss/tok 3.1968 (3.1600)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][5010/5173]	Time 0.690 (0.606)	Data 1.36e-04 (2.36e-04)	Tok/s 34039 (23295)	Loss/tok 3.3870 (3.1599)	LR 7.813e-06
0: TRAIN [4][5020/5173]	Time 0.566 (0.606)	Data 1.23e-04 (2.36e-04)	Tok/s 18462 (23292)	Loss/tok 3.0359 (3.1599)	LR 7.813e-06
0: TRAIN [4][5030/5173]	Time 0.754 (0.606)	Data 2.74e-04 (2.36e-04)	Tok/s 39955 (23300)	Loss/tok 3.4781 (3.1603)	LR 7.813e-06
0: TRAIN [4][5040/5173]	Time 0.566 (0.606)	Data 1.28e-04 (2.35e-04)	Tok/s 18282 (23301)	Loss/tok 2.9115 (3.1603)	LR 7.813e-06
0: TRAIN [4][5050/5173]	Time 0.630 (0.606)	Data 1.41e-04 (2.35e-04)	Tok/s 27008 (23306)	Loss/tok 3.1580 (3.1604)	LR 7.813e-06
0: TRAIN [4][5060/5173]	Time 0.630 (0.606)	Data 1.29e-04 (2.35e-04)	Tok/s 27190 (23305)	Loss/tok 3.0398 (3.1602)	LR 7.813e-06
0: TRAIN [4][5070/5173]	Time 0.625 (0.606)	Data 1.29e-04 (2.35e-04)	Tok/s 27202 (23301)	Loss/tok 3.1476 (3.1601)	LR 7.813e-06
0: TRAIN [4][5080/5173]	Time 0.566 (0.606)	Data 1.26e-04 (2.35e-04)	Tok/s 18027 (23295)	Loss/tok 2.9361 (3.1598)	LR 7.813e-06
0: TRAIN [4][5090/5173]	Time 0.504 (0.606)	Data 1.26e-04 (2.34e-04)	Tok/s 10417 (23294)	Loss/tok 2.5926 (3.1598)	LR 7.813e-06
0: TRAIN [4][5100/5173]	Time 0.627 (0.606)	Data 1.33e-04 (2.34e-04)	Tok/s 26523 (23300)	Loss/tok 3.2283 (3.1599)	LR 7.813e-06
0: TRAIN [4][5110/5173]	Time 0.756 (0.606)	Data 1.28e-04 (2.34e-04)	Tok/s 39795 (23300)	Loss/tok 3.4936 (3.1599)	LR 7.813e-06
0: TRAIN [4][5120/5173]	Time 0.625 (0.606)	Data 1.24e-04 (2.34e-04)	Tok/s 26682 (23298)	Loss/tok 3.0923 (3.1598)	LR 7.813e-06
0: TRAIN [4][5130/5173]	Time 0.569 (0.606)	Data 1.30e-04 (2.34e-04)	Tok/s 17914 (23295)	Loss/tok 2.9062 (3.1597)	LR 7.813e-06
0: TRAIN [4][5140/5173]	Time 0.629 (0.606)	Data 1.26e-04 (2.34e-04)	Tok/s 26471 (23298)	Loss/tok 3.0156 (3.1599)	LR 7.813e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][5150/5173]	Time 0.624 (0.606)	Data 1.21e-04 (2.33e-04)	Tok/s 26824 (23298)	Loss/tok 3.2785 (3.1598)	LR 7.813e-06
0: TRAIN [4][5160/5173]	Time 0.763 (0.606)	Data 1.31e-04 (2.33e-04)	Tok/s 38694 (23298)	Loss/tok 3.5252 (3.1599)	LR 7.813e-06
0: TRAIN [4][5170/5173]	Time 0.630 (0.606)	Data 2.69e-04 (2.33e-04)	Tok/s 26397 (23307)	Loss/tok 3.2539 (3.1602)	LR 7.813e-06
:::MLL 1586541838.645 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 525}}
:::MLL 1586541838.646 eval_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [4][0/8]	Time 0.748 (0.748)	Decoder iters 149.0 (149.0)	Tok/s 22114 (22114)
0: Running moses detokenizer
0: BLEU(score=23.112991964142836, counts=[36684, 18049, 10124, 5891], totals=[65586, 62583, 59580, 56582], precisions=[55.93266855731406, 28.84010034673953, 16.992279288351796, 10.411438266586547], bp=1.0, sys_len=65586, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1586541844.504 eval_accuracy: {"value": 23.11, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 536}}
:::MLL 1586541844.504 eval_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 4	Training Loss: 3.1603	Test BLEU: 23.11
0: Performance: Epoch: 4	Training: 69923 Tok/s
0: Finished epoch 4
:::MLL 1586541844.505 block_stop: {"value": null, "metadata": {"first_epoch_num": 5, "file": "train.py", "lineno": 558}}
0: Closing preprocessed data file
:::MLL 1586541844.505 run_stop: {"value": null, "metadata": {"status": "aborted", "file": "train.py", "lineno": 569}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-04-10 06:04:08 PM
RESULT,RNN_TRANSLATOR,,15742,nvidia,2020-04-10 01:41:46 PM
