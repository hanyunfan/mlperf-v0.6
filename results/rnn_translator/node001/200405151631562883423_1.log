Beginning trial 1 of 1
Gathering sys log on node001
:::MLL 1586117856.628 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1586117856.629 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1586117856.630 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1586117856.630 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1586117856.631 submission_platform: {"value": "1xPowerEdge R7525", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1586117856.632 submission_entry: {"value": "{'hardware': 'PowerEdge R7525', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': ' ', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.0-0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x AMD EPYC 7502 32-Core Processor', 'num_cores': '64', 'num_vcpus': '64', 'accelerator': 'Tesla V100-PCIE-32GB', 'num_accelerators': '3', 'sys_mem_size': '251 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '1x 931.5G', 'cpu_accel_interconnect': 'QPI', 'network_card': '', 'num_network_cards': '0', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1586117856.633 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1586117856.634 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1586117858.947 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node node001
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=4278' -e LR=4.0e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=600 -e REMAIN_STEPS=4300 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=5 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=200405151631562883423 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_200405151631562883423 ./run_and_time.sh
Run vars: id 200405151631562883423 gpus 3 mparams  --master_port=4278
NCCL_SOCKET_NTHREADS=2
NCCL_NSOCKS_PERTHREAD=8
STARTING TIMING RUN AT 2020-04-05 08:17:39 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=600
+ REMAIN_STEPS=4300
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=5
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=0
+ MATH=amp_fp16
+ [[ 0 -eq 1 ]]
+ LAUNCH_OPT='torch.distributed.launch --nproc_per_node 3  --master_port=4278'
+ echo 'running benchmark'
running benchmark
+ python -m torch.distributed.launch --nproc_per_node 3 --master_port=4278 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 5 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 600 --remain-steps 4300 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1586117862.259 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1586117862.270 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1586117862.283 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=8, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=5, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.004, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=4300, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=600)
0: L2 promotion: 128B
0: Using random master seed: 4014319874
node001:588:588 [0] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.1<0>
node001:588:588 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node001:588:588 [0] NCCL INFO NET/IB : No device found.
node001:588:588 [0] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.1<0>
node001:588:588 [0] NCCL INFO Using network Socket
NCCL version 2.6.4+cuda10.2
node001:590:590 [2] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.1<0>
node001:590:590 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node001:590:590 [2] NCCL INFO NET/IB : No device found.
node001:590:590 [2] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.1<0>
node001:590:590 [2] NCCL INFO Using network Socket
node001:589:589 [1] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.1<0>
node001:589:589 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node001:589:589 [1] NCCL INFO NET/IB : No device found.
node001:589:589 [1] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.1<0>
node001:589:589 [1] NCCL INFO Using network Socket
node001:588:798 [0] NCCL INFO Channel 00/02 :    0   1   2
node001:590:797 [2] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 8/8/64
node001:589:799 [1] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 8/8/64
node001:588:798 [0] NCCL INFO Channel 01/02 :    0   1   2
node001:589:799 [1] NCCL INFO Trees [0] 2/-1/-1->1->0|0->1->2/-1/-1 [1] 2/-1/-1->1->0|0->1->2/-1/-1
node001:590:797 [2] NCCL INFO Trees [0] -1/-1/-1->2->1|1->2->-1/-1/-1 [1] -1/-1/-1->2->1|1->2->-1/-1/-1
node001:589:799 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000
node001:590:797 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000
node001:588:798 [0] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 8/8/64
node001:588:798 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 1/-1/-1->0->-1|-1->0->1/-1/-1
node001:588:798 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff
node001:590:797 [2] NCCL INFO Ring 00 : 2[e2000] -> 0[21000] via direct shared memory
node001:589:799 [1] NCCL INFO Ring 00 : 1[81000] -> 2[e2000] via P2P/IPC
node001:588:798 [0] NCCL INFO Ring 00 : 0[21000] -> 1[81000] via direct shared memory
node001:590:797 [2] NCCL INFO Ring 00 : 2[e2000] -> 1[81000] via P2P/IPC
node001:589:799 [1] NCCL INFO Ring 00 : 1[81000] -> 0[21000] via direct shared memory
node001:590:797 [2] NCCL INFO Ring 01 : 2[e2000] -> 0[21000] via direct shared memory
node001:588:798 [0] NCCL INFO Ring 01 : 0[21000] -> 1[81000] via direct shared memory
node001:589:799 [1] NCCL INFO Ring 01 : 1[81000] -> 2[e2000] via P2P/IPC
node001:590:797 [2] NCCL INFO Ring 01 : 2[e2000] -> 1[81000] via P2P/IPC
node001:589:799 [1] NCCL INFO Ring 01 : 1[81000] -> 0[21000] via direct shared memory
node001:590:797 [2] NCCL INFO comm 0x7fff8c006620 rank 2 nranks 3 cudaDev 2 busId e2000 - Init COMPLETE
node001:588:798 [0] NCCL INFO comm 0x7ffe30006620 rank 0 nranks 3 cudaDev 0 busId 21000 - Init COMPLETE
node001:588:588 [0] NCCL INFO Launch mode Parallel
node001:589:799 [1] NCCL INFO comm 0x7ffebc006620 rank 1 nranks 3 cudaDev 1 busId 81000 - Init COMPLETE
0: Worker 0 is using worker seed: 3442395529
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.004}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.004
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1586117871.377 opt_base_learning_rate: {"value": 0.004, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1586117872.893 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 407}}
:::MLL 1586117872.893 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 409}}
:::MLL 1586117872.894 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 414}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1586117873.660 global_batch_size: {"value": 768, "metadata": {"file": "train.py", "lineno": 459}}
0: Training LR schedule config: {'warmup_steps': 600, 'remain_steps': 4300, 'decay_interval': 809, 'decay_steps': 8, 'decay_factor': 0.5}
0: Scheduler warmup steps: 600
0: Scheduler remain steps: 4300
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 8
:::MLL 1586117873.674 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1586117873.675 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1586117873.675 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1586117873.675 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1586117873.676 opt_learning_rate_decay_steps: {"value": 8, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1586117873.676 opt_learning_rate_remain_steps: {"value": 4300, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1586117873.677 opt_learning_rate_warmup_steps: {"value": 600, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1586117873.740 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1586117873.740 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 515}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 3988099207
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/5173]	Time 1.242 (1.242)	Data 6.01e-01 (6.01e-01)	Tok/s 8269 (8269)	Loss/tok 10.5800 (10.5800)	LR 4.000e-05
0: TRAIN [0][10/5173]	Time 0.579 (0.681)	Data 1.50e-04 (5.49e-02)	Tok/s 17662 (21064)	Loss/tok 9.3922 (9.9307)	LR 4.319e-05
0: TRAIN [0][20/5173]	Time 0.520 (0.657)	Data 9.78e-05 (2.88e-02)	Tok/s 10105 (22388)	Loss/tok 8.8850 (9.6200)	LR 4.664e-05
0: TRAIN [0][30/5173]	Time 0.710 (0.653)	Data 1.07e-04 (1.96e-02)	Tok/s 33142 (23332)	Loss/tok 8.9811 (9.4339)	LR 5.036e-05
0: TRAIN [0][40/5173]	Time 0.710 (0.640)	Data 1.16e-04 (1.48e-02)	Tok/s 32946 (22571)	Loss/tok 8.7734 (9.2954)	LR 5.437e-05
0: TRAIN [0][50/5173]	Time 0.707 (0.636)	Data 1.13e-04 (1.19e-02)	Tok/s 32932 (22589)	Loss/tok 8.6335 (9.1623)	LR 5.871e-05
0: TRAIN [0][60/5173]	Time 0.581 (0.631)	Data 1.07e-04 (1.00e-02)	Tok/s 18339 (22327)	Loss/tok 8.3258 (9.0560)	LR 6.340e-05
0: TRAIN [0][70/5173]	Time 0.581 (0.631)	Data 9.78e-05 (8.61e-03)	Tok/s 17726 (22538)	Loss/tok 8.1227 (8.9484)	LR 6.845e-05
0: TRAIN [0][80/5173]	Time 0.641 (0.632)	Data 9.85e-05 (7.56e-03)	Tok/s 26376 (22939)	Loss/tok 8.1771 (8.8468)	LR 7.391e-05
0: TRAIN [0][90/5173]	Time 0.581 (0.631)	Data 1.69e-04 (6.75e-03)	Tok/s 17915 (22984)	Loss/tok 7.9464 (8.7633)	LR 7.981e-05
0: TRAIN [0][100/5173]	Time 0.644 (0.634)	Data 1.11e-04 (6.09e-03)	Tok/s 26287 (23388)	Loss/tok 7.9729 (8.6847)	LR 8.618e-05
0: TRAIN [0][110/5173]	Time 0.772 (0.635)	Data 1.15e-04 (5.56e-03)	Tok/s 38894 (23560)	Loss/tok 8.1887 (8.6216)	LR 9.305e-05
0: TRAIN [0][120/5173]	Time 0.581 (0.633)	Data 1.23e-04 (5.11e-03)	Tok/s 17911 (23426)	Loss/tok 7.7803 (8.5700)	LR 1.005e-04
0: TRAIN [0][130/5173]	Time 0.708 (0.634)	Data 1.10e-04 (4.73e-03)	Tok/s 33137 (23553)	Loss/tok 7.9834 (8.5201)	LR 1.085e-04
0: TRAIN [0][140/5173]	Time 0.637 (0.634)	Data 1.18e-04 (4.41e-03)	Tok/s 26187 (23614)	Loss/tok 7.9298 (8.4792)	LR 1.171e-04
0: TRAIN [0][150/5173]	Time 0.582 (0.631)	Data 1.17e-04 (4.12e-03)	Tok/s 17740 (23332)	Loss/tok 7.7332 (8.4436)	LR 1.265e-04
0: TRAIN [0][160/5173]	Time 0.643 (0.629)	Data 1.11e-04 (3.87e-03)	Tok/s 26459 (23156)	Loss/tok 7.9081 (8.4107)	LR 1.366e-04
0: TRAIN [0][170/5173]	Time 0.781 (0.632)	Data 1.16e-04 (3.65e-03)	Tok/s 38167 (23450)	Loss/tok 8.0773 (8.3754)	LR 1.475e-04
0: TRAIN [0][180/5173]	Time 0.582 (0.632)	Data 1.16e-04 (3.46e-03)	Tok/s 17880 (23568)	Loss/tok 7.6691 (8.3444)	LR 1.592e-04
0: TRAIN [0][190/5173]	Time 0.644 (0.631)	Data 1.15e-04 (3.29e-03)	Tok/s 26125 (23393)	Loss/tok 7.8358 (8.3186)	LR 1.719e-04
0: TRAIN [0][200/5173]	Time 0.637 (0.633)	Data 1.34e-04 (3.13e-03)	Tok/s 26740 (23636)	Loss/tok 7.7505 (8.2888)	LR 1.857e-04
0: TRAIN [0][210/5173]	Time 0.583 (0.632)	Data 1.14e-04 (2.99e-03)	Tok/s 17604 (23623)	Loss/tok 7.5440 (8.2617)	LR 2.005e-04
0: TRAIN [0][220/5173]	Time 0.639 (0.633)	Data 1.12e-04 (2.86e-03)	Tok/s 26239 (23752)	Loss/tok 7.7181 (8.2359)	LR 2.165e-04
0: TRAIN [0][230/5173]	Time 0.520 (0.632)	Data 1.10e-04 (2.74e-03)	Tok/s 10253 (23595)	Loss/tok 6.7845 (8.2131)	LR 2.337e-04
0: TRAIN [0][240/5173]	Time 0.778 (0.631)	Data 1.18e-04 (2.63e-03)	Tok/s 37592 (23545)	Loss/tok 7.8471 (8.1889)	LR 2.524e-04
0: TRAIN [0][250/5173]	Time 0.582 (0.631)	Data 1.11e-04 (2.53e-03)	Tok/s 18047 (23542)	Loss/tok 7.3560 (8.1642)	LR 2.725e-04
0: TRAIN [0][260/5173]	Time 0.641 (0.630)	Data 1.16e-04 (2.44e-03)	Tok/s 26013 (23498)	Loss/tok 7.6242 (8.1403)	LR 2.943e-04
0: TRAIN [0][270/5173]	Time 0.640 (0.630)	Data 1.19e-04 (2.35e-03)	Tok/s 26071 (23532)	Loss/tok 7.5513 (8.1155)	LR 3.177e-04
0: TRAIN [0][280/5173]	Time 0.640 (0.630)	Data 1.14e-04 (2.27e-03)	Tok/s 26217 (23500)	Loss/tok 7.3446 (8.0918)	LR 3.431e-04
0: TRAIN [0][290/5173]	Time 0.579 (0.629)	Data 1.12e-04 (2.20e-03)	Tok/s 17574 (23478)	Loss/tok 7.1418 (8.0671)	LR 3.704e-04
0: TRAIN [0][300/5173]	Time 0.640 (0.629)	Data 1.18e-04 (2.13e-03)	Tok/s 26383 (23451)	Loss/tok 7.3455 (8.0421)	LR 4.000e-04
0: TRAIN [0][310/5173]	Time 0.580 (0.628)	Data 1.07e-04 (2.06e-03)	Tok/s 17608 (23322)	Loss/tok 6.9512 (8.0185)	LR 4.319e-04
0: TRAIN [0][320/5173]	Time 0.638 (0.629)	Data 1.14e-04 (2.00e-03)	Tok/s 26778 (23433)	Loss/tok 7.0909 (7.9892)	LR 4.664e-04
0: TRAIN [0][330/5173]	Time 0.702 (0.628)	Data 1.17e-04 (1.95e-03)	Tok/s 33087 (23380)	Loss/tok 7.2712 (7.9639)	LR 5.036e-04
0: TRAIN [0][340/5173]	Time 0.582 (0.628)	Data 1.17e-04 (1.89e-03)	Tok/s 17490 (23355)	Loss/tok 6.8035 (7.9374)	LR 5.437e-04
0: TRAIN [0][350/5173]	Time 0.580 (0.627)	Data 1.10e-04 (1.84e-03)	Tok/s 18065 (23225)	Loss/tok 6.8315 (7.9139)	LR 5.871e-04
0: TRAIN [0][360/5173]	Time 0.641 (0.627)	Data 1.43e-04 (1.80e-03)	Tok/s 26470 (23228)	Loss/tok 7.0148 (7.8876)	LR 6.340e-04
0: TRAIN [0][370/5173]	Time 0.584 (0.626)	Data 1.39e-04 (1.75e-03)	Tok/s 17684 (23193)	Loss/tok 6.7898 (7.8627)	LR 6.845e-04
0: TRAIN [0][380/5173]	Time 0.587 (0.626)	Data 3.04e-04 (1.71e-03)	Tok/s 17886 (23215)	Loss/tok 6.6496 (7.8370)	LR 7.391e-04
0: TRAIN [0][390/5173]	Time 0.707 (0.626)	Data 1.18e-04 (1.67e-03)	Tok/s 32846 (23171)	Loss/tok 6.9171 (7.8111)	LR 7.981e-04
0: TRAIN [0][400/5173]	Time 0.583 (0.626)	Data 1.15e-04 (1.63e-03)	Tok/s 17692 (23148)	Loss/tok 6.5213 (7.7864)	LR 8.618e-04
0: TRAIN [0][410/5173]	Time 0.580 (0.626)	Data 1.14e-04 (1.59e-03)	Tok/s 17518 (23130)	Loss/tok 6.3666 (7.7598)	LR 9.305e-04
0: TRAIN [0][420/5173]	Time 0.583 (0.626)	Data 1.14e-04 (1.56e-03)	Tok/s 17863 (23152)	Loss/tok 6.4395 (7.7317)	LR 1.005e-03
0: TRAIN [0][430/5173]	Time 0.521 (0.626)	Data 1.12e-04 (1.53e-03)	Tok/s 10094 (23118)	Loss/tok 5.5014 (7.7057)	LR 1.085e-03
0: TRAIN [0][440/5173]	Time 0.517 (0.625)	Data 1.12e-04 (1.49e-03)	Tok/s 10178 (23090)	Loss/tok 5.3257 (7.6800)	LR 1.171e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][450/5173]	Time 0.635 (0.626)	Data 1.21e-04 (1.46e-03)	Tok/s 26519 (23163)	Loss/tok 6.5635 (7.6551)	LR 1.255e-03
0: TRAIN [0][460/5173]	Time 0.521 (0.626)	Data 1.13e-04 (1.43e-03)	Tok/s 10232 (23186)	Loss/tok 5.3861 (7.6286)	LR 1.355e-03
0: TRAIN [0][470/5173]	Time 0.583 (0.626)	Data 2.79e-04 (1.41e-03)	Tok/s 17791 (23194)	Loss/tok 6.1051 (7.6024)	LR 1.464e-03
0: TRAIN [0][480/5173]	Time 0.704 (0.626)	Data 1.11e-04 (1.38e-03)	Tok/s 32920 (23180)	Loss/tok 6.5352 (7.5780)	LR 1.580e-03
0: TRAIN [0][490/5173]	Time 0.583 (0.625)	Data 1.16e-04 (1.36e-03)	Tok/s 17897 (23111)	Loss/tok 6.0822 (7.5554)	LR 1.706e-03
0: TRAIN [0][500/5173]	Time 0.637 (0.625)	Data 1.17e-04 (1.33e-03)	Tok/s 26415 (23057)	Loss/tok 6.5450 (7.5326)	LR 1.842e-03
0: TRAIN [0][510/5173]	Time 0.706 (0.625)	Data 1.11e-04 (1.31e-03)	Tok/s 33505 (23096)	Loss/tok 6.3678 (7.5061)	LR 1.989e-03
0: TRAIN [0][520/5173]	Time 0.582 (0.625)	Data 1.16e-04 (1.28e-03)	Tok/s 17698 (23090)	Loss/tok 5.8346 (7.4802)	LR 2.148e-03
0: TRAIN [0][530/5173]	Time 0.581 (0.625)	Data 1.13e-04 (1.26e-03)	Tok/s 17904 (23091)	Loss/tok 5.7907 (7.4533)	LR 2.319e-03
0: TRAIN [0][540/5173]	Time 0.581 (0.625)	Data 1.14e-04 (1.24e-03)	Tok/s 17778 (23147)	Loss/tok 5.6269 (7.4241)	LR 2.505e-03
0: TRAIN [0][550/5173]	Time 0.638 (0.625)	Data 1.19e-04 (1.22e-03)	Tok/s 26055 (23139)	Loss/tok 5.9700 (7.3972)	LR 2.704e-03
0: TRAIN [0][560/5173]	Time 0.581 (0.625)	Data 1.13e-04 (1.20e-03)	Tok/s 17628 (23142)	Loss/tok 5.6189 (7.3700)	LR 2.920e-03
0: TRAIN [0][570/5173]	Time 0.581 (0.625)	Data 1.12e-04 (1.18e-03)	Tok/s 17274 (23147)	Loss/tok 5.4884 (7.3426)	LR 3.153e-03
0: TRAIN [0][580/5173]	Time 0.581 (0.625)	Data 1.09e-04 (1.17e-03)	Tok/s 17782 (23127)	Loss/tok 5.4650 (7.3164)	LR 3.405e-03
0: TRAIN [0][590/5173]	Time 0.648 (0.625)	Data 1.30e-04 (1.15e-03)	Tok/s 25959 (23153)	Loss/tok 5.6334 (7.2878)	LR 3.676e-03
0: TRAIN [0][600/5173]	Time 0.580 (0.625)	Data 1.22e-04 (1.13e-03)	Tok/s 17747 (23167)	Loss/tok 5.4212 (7.2592)	LR 3.969e-03
0: TRAIN [0][610/5173]	Time 0.581 (0.625)	Data 1.45e-04 (1.11e-03)	Tok/s 17213 (23127)	Loss/tok 5.3011 (7.2337)	LR 4.000e-03
0: TRAIN [0][620/5173]	Time 0.583 (0.625)	Data 1.30e-04 (1.10e-03)	Tok/s 17836 (23185)	Loss/tok 5.1438 (7.2021)	LR 4.000e-03
0: TRAIN [0][630/5173]	Time 0.582 (0.625)	Data 1.21e-04 (1.08e-03)	Tok/s 17698 (23161)	Loss/tok 5.0042 (7.1751)	LR 4.000e-03
0: TRAIN [0][640/5173]	Time 0.644 (0.626)	Data 1.35e-04 (1.07e-03)	Tok/s 26388 (23231)	Loss/tok 5.2598 (7.1419)	LR 4.000e-03
0: TRAIN [0][650/5173]	Time 0.520 (0.625)	Data 1.15e-04 (1.05e-03)	Tok/s 10008 (23150)	Loss/tok 4.0920 (7.1181)	LR 4.000e-03
0: TRAIN [0][660/5173]	Time 0.582 (0.625)	Data 2.84e-04 (1.04e-03)	Tok/s 17353 (23110)	Loss/tok 4.7448 (7.0920)	LR 4.000e-03
0: TRAIN [0][670/5173]	Time 0.642 (0.625)	Data 1.07e-04 (1.03e-03)	Tok/s 26021 (23069)	Loss/tok 5.2138 (7.0655)	LR 4.000e-03
0: TRAIN [0][680/5173]	Time 0.642 (0.624)	Data 1.15e-04 (1.01e-03)	Tok/s 26556 (23042)	Loss/tok 5.0976 (7.0377)	LR 4.000e-03
0: TRAIN [0][690/5173]	Time 0.705 (0.625)	Data 1.23e-04 (1.00e-03)	Tok/s 32641 (23058)	Loss/tok 5.2552 (7.0068)	LR 4.000e-03
0: TRAIN [0][700/5173]	Time 0.582 (0.624)	Data 1.12e-04 (9.88e-04)	Tok/s 17974 (23050)	Loss/tok 4.7378 (6.9782)	LR 4.000e-03
0: TRAIN [0][710/5173]	Time 0.643 (0.625)	Data 1.24e-04 (9.76e-04)	Tok/s 26366 (23067)	Loss/tok 4.9243 (6.9481)	LR 4.000e-03
0: TRAIN [0][720/5173]	Time 0.580 (0.625)	Data 1.13e-04 (9.64e-04)	Tok/s 18270 (23075)	Loss/tok 4.5887 (6.9186)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][730/5173]	Time 0.643 (0.624)	Data 2.90e-04 (9.53e-04)	Tok/s 25959 (23054)	Loss/tok 4.8180 (6.8919)	LR 4.000e-03
0: TRAIN [0][740/5173]	Time 0.582 (0.624)	Data 1.21e-04 (9.41e-04)	Tok/s 17681 (23054)	Loss/tok 4.5250 (6.8631)	LR 4.000e-03
0: TRAIN [0][750/5173]	Time 0.521 (0.624)	Data 2.65e-04 (9.31e-04)	Tok/s 10118 (23008)	Loss/tok 3.7419 (6.8386)	LR 4.000e-03
0: TRAIN [0][760/5173]	Time 0.580 (0.624)	Data 1.13e-04 (9.20e-04)	Tok/s 17690 (22970)	Loss/tok 4.4510 (6.8142)	LR 4.000e-03
0: TRAIN [0][770/5173]	Time 0.646 (0.624)	Data 1.15e-04 (9.10e-04)	Tok/s 25944 (23029)	Loss/tok 4.7362 (6.7826)	LR 4.000e-03
0: TRAIN [0][780/5173]	Time 0.582 (0.624)	Data 1.16e-04 (9.00e-04)	Tok/s 17938 (23012)	Loss/tok 4.4783 (6.7571)	LR 4.000e-03
0: TRAIN [0][790/5173]	Time 0.639 (0.624)	Data 1.11e-04 (8.90e-04)	Tok/s 26408 (22988)	Loss/tok 4.5014 (6.7324)	LR 4.000e-03
0: TRAIN [0][800/5173]	Time 0.645 (0.624)	Data 1.18e-04 (8.80e-04)	Tok/s 25596 (22979)	Loss/tok 4.6620 (6.7068)	LR 4.000e-03
0: TRAIN [0][810/5173]	Time 0.581 (0.623)	Data 1.17e-04 (8.71e-04)	Tok/s 17965 (22918)	Loss/tok 4.3858 (6.6860)	LR 4.000e-03
0: TRAIN [0][820/5173]	Time 0.582 (0.623)	Data 1.18e-04 (8.62e-04)	Tok/s 17470 (22876)	Loss/tok 4.1604 (6.6637)	LR 4.000e-03
0: TRAIN [0][830/5173]	Time 0.704 (0.623)	Data 1.18e-04 (8.53e-04)	Tok/s 32843 (22917)	Loss/tok 4.7295 (6.6350)	LR 4.000e-03
0: TRAIN [0][840/5173]	Time 0.645 (0.623)	Data 2.92e-04 (8.45e-04)	Tok/s 25850 (22950)	Loss/tok 4.6733 (6.6073)	LR 4.000e-03
0: TRAIN [0][850/5173]	Time 0.582 (0.624)	Data 1.27e-04 (8.37e-04)	Tok/s 17916 (22965)	Loss/tok 4.2556 (6.5816)	LR 4.000e-03
0: TRAIN [0][860/5173]	Time 0.640 (0.623)	Data 1.18e-04 (8.28e-04)	Tok/s 26004 (22942)	Loss/tok 4.5322 (6.5596)	LR 4.000e-03
0: TRAIN [0][870/5173]	Time 0.523 (0.623)	Data 1.23e-04 (8.20e-04)	Tok/s 10130 (22936)	Loss/tok 3.4630 (6.5356)	LR 4.000e-03
0: TRAIN [0][880/5173]	Time 0.581 (0.623)	Data 1.21e-04 (8.13e-04)	Tok/s 17653 (22936)	Loss/tok 4.1724 (6.5125)	LR 4.000e-03
0: TRAIN [0][890/5173]	Time 0.582 (0.624)	Data 1.16e-04 (8.05e-04)	Tok/s 17908 (22974)	Loss/tok 4.1127 (6.4856)	LR 4.000e-03
0: TRAIN [0][900/5173]	Time 0.583 (0.624)	Data 1.23e-04 (7.98e-04)	Tok/s 18077 (23002)	Loss/tok 4.1460 (6.4601)	LR 4.000e-03
0: TRAIN [0][910/5173]	Time 0.640 (0.624)	Data 1.24e-04 (7.91e-04)	Tok/s 26287 (22999)	Loss/tok 4.3531 (6.4376)	LR 4.000e-03
0: TRAIN [0][920/5173]	Time 0.579 (0.624)	Data 1.20e-04 (7.84e-04)	Tok/s 18015 (23017)	Loss/tok 4.1246 (6.4141)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][930/5173]	Time 0.521 (0.624)	Data 1.24e-04 (7.77e-04)	Tok/s 10095 (23076)	Loss/tok 3.4295 (6.3873)	LR 4.000e-03
0: TRAIN [0][940/5173]	Time 0.645 (0.624)	Data 1.18e-04 (7.70e-04)	Tok/s 26163 (23065)	Loss/tok 4.2209 (6.3667)	LR 4.000e-03
0: TRAIN [0][950/5173]	Time 0.582 (0.625)	Data 1.71e-04 (7.64e-04)	Tok/s 17674 (23129)	Loss/tok 4.0549 (6.3402)	LR 4.000e-03
0: TRAIN [0][960/5173]	Time 0.627 (0.625)	Data 1.20e-04 (7.58e-04)	Tok/s 26842 (23122)	Loss/tok 4.3085 (6.3203)	LR 4.000e-03
0: TRAIN [0][970/5173]	Time 0.705 (0.625)	Data 1.18e-04 (7.51e-04)	Tok/s 33414 (23139)	Loss/tok 4.5470 (6.2987)	LR 4.000e-03
0: TRAIN [0][980/5173]	Time 0.643 (0.625)	Data 1.33e-04 (7.45e-04)	Tok/s 26163 (23189)	Loss/tok 4.3490 (6.2743)	LR 4.000e-03
0: TRAIN [0][990/5173]	Time 0.645 (0.625)	Data 1.26e-04 (7.39e-04)	Tok/s 26123 (23160)	Loss/tok 4.3449 (6.2566)	LR 4.000e-03
0: TRAIN [0][1000/5173]	Time 0.640 (0.625)	Data 1.26e-04 (7.33e-04)	Tok/s 26550 (23146)	Loss/tok 4.2703 (6.2381)	LR 4.000e-03
0: TRAIN [0][1010/5173]	Time 0.520 (0.625)	Data 1.23e-04 (7.27e-04)	Tok/s 9759 (23127)	Loss/tok 3.3955 (6.2200)	LR 4.000e-03
0: TRAIN [0][1020/5173]	Time 0.640 (0.625)	Data 2.79e-04 (7.22e-04)	Tok/s 26233 (23121)	Loss/tok 4.2598 (6.2012)	LR 4.000e-03
0: TRAIN [0][1030/5173]	Time 0.636 (0.625)	Data 1.26e-04 (7.16e-04)	Tok/s 26307 (23101)	Loss/tok 4.2683 (6.1836)	LR 4.000e-03
0: TRAIN [0][1040/5173]	Time 0.706 (0.625)	Data 1.31e-04 (7.10e-04)	Tok/s 32777 (23147)	Loss/tok 4.4715 (6.1619)	LR 4.000e-03
0: TRAIN [0][1050/5173]	Time 0.642 (0.625)	Data 1.27e-04 (7.05e-04)	Tok/s 26062 (23142)	Loss/tok 4.3123 (6.1441)	LR 4.000e-03
0: TRAIN [0][1060/5173]	Time 0.644 (0.625)	Data 2.94e-04 (7.00e-04)	Tok/s 25838 (23150)	Loss/tok 4.2914 (6.1260)	LR 4.000e-03
0: TRAIN [0][1070/5173]	Time 0.639 (0.625)	Data 1.25e-04 (6.94e-04)	Tok/s 26268 (23141)	Loss/tok 4.1526 (6.1088)	LR 4.000e-03
0: TRAIN [0][1080/5173]	Time 0.706 (0.625)	Data 1.33e-04 (6.89e-04)	Tok/s 32973 (23141)	Loss/tok 4.3987 (6.0913)	LR 4.000e-03
0: TRAIN [0][1090/5173]	Time 0.581 (0.625)	Data 1.23e-04 (6.84e-04)	Tok/s 17717 (23108)	Loss/tok 3.9105 (6.0763)	LR 4.000e-03
0: TRAIN [0][1100/5173]	Time 0.584 (0.624)	Data 1.20e-04 (6.79e-04)	Tok/s 17445 (23053)	Loss/tok 3.9554 (6.0632)	LR 4.000e-03
0: TRAIN [0][1110/5173]	Time 0.582 (0.624)	Data 1.18e-04 (6.74e-04)	Tok/s 17511 (23055)	Loss/tok 3.9763 (6.0464)	LR 4.000e-03
0: TRAIN [0][1120/5173]	Time 0.647 (0.624)	Data 3.16e-04 (6.70e-04)	Tok/s 25961 (23057)	Loss/tok 4.0792 (6.0298)	LR 4.000e-03
0: TRAIN [0][1130/5173]	Time 0.644 (0.624)	Data 1.25e-04 (6.65e-04)	Tok/s 25943 (23058)	Loss/tok 4.2859 (6.0137)	LR 4.000e-03
0: TRAIN [0][1140/5173]	Time 0.583 (0.624)	Data 1.19e-04 (6.60e-04)	Tok/s 17613 (23027)	Loss/tok 3.9668 (5.9998)	LR 4.000e-03
0: TRAIN [0][1150/5173]	Time 0.583 (0.624)	Data 1.22e-04 (6.56e-04)	Tok/s 17870 (23054)	Loss/tok 3.9614 (5.9822)	LR 4.000e-03
0: TRAIN [0][1160/5173]	Time 0.521 (0.625)	Data 1.39e-04 (6.52e-04)	Tok/s 9963 (23081)	Loss/tok 3.3051 (5.9643)	LR 4.000e-03
0: TRAIN [0][1170/5173]	Time 0.581 (0.625)	Data 1.15e-04 (6.47e-04)	Tok/s 17730 (23094)	Loss/tok 3.9292 (5.9479)	LR 4.000e-03
0: TRAIN [0][1180/5173]	Time 0.583 (0.625)	Data 3.15e-04 (6.43e-04)	Tok/s 17806 (23114)	Loss/tok 3.8964 (5.9317)	LR 4.000e-03
0: TRAIN [0][1190/5173]	Time 0.709 (0.625)	Data 1.23e-04 (6.39e-04)	Tok/s 32426 (23141)	Loss/tok 4.3951 (5.9146)	LR 4.000e-03
0: TRAIN [0][1200/5173]	Time 0.704 (0.626)	Data 1.22e-04 (6.35e-04)	Tok/s 33244 (23184)	Loss/tok 4.2491 (5.8970)	LR 4.000e-03
0: TRAIN [0][1210/5173]	Time 0.639 (0.626)	Data 1.16e-04 (6.31e-04)	Tok/s 25859 (23194)	Loss/tok 4.1280 (5.8819)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1220/5173]	Time 0.579 (0.626)	Data 1.27e-04 (6.27e-04)	Tok/s 17693 (23191)	Loss/tok 3.7863 (5.8678)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1230/5173]	Time 0.585 (0.626)	Data 1.20e-04 (6.23e-04)	Tok/s 17965 (23187)	Loss/tok 3.9700 (5.8539)	LR 4.000e-03
0: TRAIN [0][1240/5173]	Time 0.643 (0.626)	Data 1.19e-04 (6.19e-04)	Tok/s 25825 (23191)	Loss/tok 4.1677 (5.8397)	LR 4.000e-03
0: TRAIN [0][1250/5173]	Time 0.579 (0.625)	Data 1.33e-04 (6.15e-04)	Tok/s 17417 (23147)	Loss/tok 3.8305 (5.8288)	LR 4.000e-03
0: TRAIN [0][1260/5173]	Time 0.643 (0.625)	Data 1.29e-04 (6.11e-04)	Tok/s 25547 (23125)	Loss/tok 4.1984 (5.8170)	LR 4.000e-03
0: TRAIN [0][1270/5173]	Time 0.643 (0.625)	Data 1.18e-04 (6.07e-04)	Tok/s 25879 (23134)	Loss/tok 3.9841 (5.8026)	LR 4.000e-03
0: TRAIN [0][1280/5173]	Time 0.705 (0.625)	Data 1.59e-04 (6.04e-04)	Tok/s 33106 (23158)	Loss/tok 4.2904 (5.7878)	LR 4.000e-03
0: TRAIN [0][1290/5173]	Time 0.518 (0.625)	Data 1.50e-04 (6.00e-04)	Tok/s 10147 (23123)	Loss/tok 3.1831 (5.7769)	LR 4.000e-03
0: TRAIN [0][1300/5173]	Time 0.704 (0.625)	Data 1.47e-04 (5.96e-04)	Tok/s 33315 (23153)	Loss/tok 4.2985 (5.7618)	LR 4.000e-03
0: TRAIN [0][1310/5173]	Time 0.709 (0.625)	Data 1.26e-04 (5.93e-04)	Tok/s 32361 (23175)	Loss/tok 4.3645 (5.7482)	LR 4.000e-03
0: TRAIN [0][1320/5173]	Time 0.521 (0.625)	Data 1.27e-04 (5.89e-04)	Tok/s 10181 (23147)	Loss/tok 3.1334 (5.7373)	LR 4.000e-03
0: TRAIN [0][1330/5173]	Time 0.581 (0.625)	Data 1.19e-04 (5.86e-04)	Tok/s 17588 (23125)	Loss/tok 3.8011 (5.7260)	LR 4.000e-03
0: TRAIN [0][1340/5173]	Time 0.522 (0.625)	Data 1.20e-04 (5.82e-04)	Tok/s 10053 (23131)	Loss/tok 3.2136 (5.7133)	LR 4.000e-03
0: TRAIN [0][1350/5173]	Time 0.644 (0.625)	Data 2.77e-04 (5.79e-04)	Tok/s 26096 (23121)	Loss/tok 4.0291 (5.7014)	LR 4.000e-03
0: TRAIN [0][1360/5173]	Time 0.581 (0.625)	Data 1.60e-04 (5.76e-04)	Tok/s 17837 (23124)	Loss/tok 3.7591 (5.6891)	LR 4.000e-03
0: TRAIN [0][1370/5173]	Time 0.710 (0.625)	Data 1.70e-04 (5.73e-04)	Tok/s 33136 (23153)	Loss/tok 4.2645 (5.6759)	LR 4.000e-03
0: TRAIN [0][1380/5173]	Time 0.639 (0.625)	Data 1.26e-04 (5.70e-04)	Tok/s 25998 (23162)	Loss/tok 4.0678 (5.6640)	LR 4.000e-03
0: TRAIN [0][1390/5173]	Time 0.709 (0.625)	Data 1.21e-04 (5.67e-04)	Tok/s 33057 (23169)	Loss/tok 4.2153 (5.6518)	LR 4.000e-03
0: TRAIN [0][1400/5173]	Time 0.582 (0.625)	Data 1.18e-04 (5.64e-04)	Tok/s 17719 (23159)	Loss/tok 3.7408 (5.6411)	LR 4.000e-03
0: TRAIN [0][1410/5173]	Time 0.519 (0.625)	Data 1.34e-04 (5.61e-04)	Tok/s 10237 (23172)	Loss/tok 3.1653 (5.6291)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1420/5173]	Time 0.637 (0.625)	Data 1.22e-04 (5.58e-04)	Tok/s 26240 (23146)	Loss/tok 4.0504 (5.6196)	LR 4.000e-03
0: TRAIN [0][1430/5173]	Time 0.582 (0.625)	Data 1.23e-04 (5.56e-04)	Tok/s 17484 (23141)	Loss/tok 3.7438 (5.6087)	LR 4.000e-03
0: TRAIN [0][1440/5173]	Time 0.641 (0.625)	Data 3.10e-04 (5.53e-04)	Tok/s 25978 (23152)	Loss/tok 4.0075 (5.5973)	LR 4.000e-03
0: TRAIN [0][1450/5173]	Time 0.581 (0.625)	Data 1.24e-04 (5.50e-04)	Tok/s 18009 (23132)	Loss/tok 3.6563 (5.5875)	LR 4.000e-03
0: TRAIN [0][1460/5173]	Time 0.647 (0.625)	Data 1.13e-04 (5.47e-04)	Tok/s 26034 (23139)	Loss/tok 3.8916 (5.5760)	LR 4.000e-03
0: TRAIN [0][1470/5173]	Time 0.581 (0.625)	Data 1.24e-04 (5.44e-04)	Tok/s 17766 (23136)	Loss/tok 3.8300 (5.5655)	LR 4.000e-03
0: TRAIN [0][1480/5173]	Time 0.642 (0.625)	Data 1.29e-04 (5.42e-04)	Tok/s 25989 (23150)	Loss/tok 3.9577 (5.5543)	LR 4.000e-03
0: TRAIN [0][1490/5173]	Time 0.707 (0.625)	Data 1.20e-04 (5.39e-04)	Tok/s 32626 (23161)	Loss/tok 4.2451 (5.5433)	LR 4.000e-03
0: TRAIN [0][1500/5173]	Time 0.644 (0.625)	Data 1.26e-04 (5.36e-04)	Tok/s 25948 (23165)	Loss/tok 3.9501 (5.5328)	LR 4.000e-03
0: TRAIN [0][1510/5173]	Time 0.521 (0.625)	Data 1.29e-04 (5.34e-04)	Tok/s 10098 (23136)	Loss/tok 3.1554 (5.5243)	LR 4.000e-03
0: TRAIN [0][1520/5173]	Time 0.703 (0.625)	Data 1.24e-04 (5.32e-04)	Tok/s 32916 (23141)	Loss/tok 4.1445 (5.5141)	LR 4.000e-03
0: TRAIN [0][1530/5173]	Time 0.579 (0.625)	Data 1.32e-04 (5.29e-04)	Tok/s 17730 (23136)	Loss/tok 3.8251 (5.5044)	LR 4.000e-03
0: TRAIN [0][1540/5173]	Time 0.645 (0.625)	Data 3.09e-04 (5.26e-04)	Tok/s 25910 (23141)	Loss/tok 3.9175 (5.4942)	LR 4.000e-03
0: TRAIN [0][1550/5173]	Time 0.580 (0.625)	Data 1.29e-04 (5.24e-04)	Tok/s 17596 (23135)	Loss/tok 3.8307 (5.4847)	LR 4.000e-03
0: TRAIN [0][1560/5173]	Time 0.638 (0.625)	Data 1.30e-04 (5.22e-04)	Tok/s 26055 (23140)	Loss/tok 4.0508 (5.4748)	LR 4.000e-03
0: TRAIN [0][1570/5173]	Time 0.582 (0.625)	Data 1.26e-04 (5.19e-04)	Tok/s 17906 (23159)	Loss/tok 3.6913 (5.4644)	LR 4.000e-03
0: TRAIN [0][1580/5173]	Time 0.643 (0.625)	Data 1.24e-04 (5.17e-04)	Tok/s 26170 (23135)	Loss/tok 4.0689 (5.4562)	LR 4.000e-03
0: TRAIN [0][1590/5173]	Time 0.583 (0.625)	Data 1.18e-04 (5.14e-04)	Tok/s 17815 (23136)	Loss/tok 3.6022 (5.4465)	LR 4.000e-03
0: TRAIN [0][1600/5173]	Time 0.644 (0.625)	Data 3.01e-04 (5.12e-04)	Tok/s 26057 (23133)	Loss/tok 3.9688 (5.4378)	LR 4.000e-03
0: TRAIN [0][1610/5173]	Time 0.705 (0.625)	Data 1.32e-04 (5.10e-04)	Tok/s 32652 (23131)	Loss/tok 4.1936 (5.4292)	LR 4.000e-03
0: TRAIN [0][1620/5173]	Time 0.580 (0.625)	Data 1.18e-04 (5.08e-04)	Tok/s 18195 (23128)	Loss/tok 3.6924 (5.4205)	LR 4.000e-03
0: TRAIN [0][1630/5173]	Time 0.580 (0.625)	Data 1.27e-04 (5.06e-04)	Tok/s 17816 (23122)	Loss/tok 3.7116 (5.4118)	LR 4.000e-03
0: TRAIN [0][1640/5173]	Time 0.648 (0.625)	Data 1.23e-04 (5.04e-04)	Tok/s 26025 (23111)	Loss/tok 4.0226 (5.4035)	LR 4.000e-03
0: TRAIN [0][1650/5173]	Time 0.578 (0.625)	Data 2.89e-04 (5.01e-04)	Tok/s 18039 (23114)	Loss/tok 3.6467 (5.3947)	LR 4.000e-03
0: TRAIN [0][1660/5173]	Time 0.647 (0.625)	Data 1.26e-04 (4.99e-04)	Tok/s 25440 (23119)	Loss/tok 3.8886 (5.3857)	LR 4.000e-03
0: TRAIN [0][1670/5173]	Time 0.585 (0.625)	Data 1.27e-04 (4.97e-04)	Tok/s 17574 (23112)	Loss/tok 3.6577 (5.3773)	LR 4.000e-03
0: TRAIN [0][1680/5173]	Time 0.584 (0.625)	Data 1.21e-04 (4.95e-04)	Tok/s 17651 (23107)	Loss/tok 3.6987 (5.3693)	LR 4.000e-03
0: TRAIN [0][1690/5173]	Time 0.585 (0.625)	Data 1.26e-04 (4.93e-04)	Tok/s 17337 (23090)	Loss/tok 3.7626 (5.3618)	LR 4.000e-03
0: TRAIN [0][1700/5173]	Time 0.581 (0.625)	Data 1.18e-04 (4.91e-04)	Tok/s 17988 (23065)	Loss/tok 3.7438 (5.3546)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1710/5173]	Time 0.582 (0.625)	Data 1.19e-04 (4.89e-04)	Tok/s 18069 (23061)	Loss/tok 3.6641 (5.3468)	LR 4.000e-03
0: TRAIN [0][1720/5173]	Time 0.585 (0.625)	Data 1.25e-04 (4.87e-04)	Tok/s 17846 (23072)	Loss/tok 3.7124 (5.3377)	LR 4.000e-03
0: TRAIN [0][1730/5173]	Time 0.583 (0.625)	Data 1.29e-04 (4.85e-04)	Tok/s 17922 (23053)	Loss/tok 3.5567 (5.3307)	LR 4.000e-03
0: TRAIN [0][1740/5173]	Time 0.582 (0.624)	Data 1.24e-04 (4.83e-04)	Tok/s 17767 (23032)	Loss/tok 3.6677 (5.3237)	LR 4.000e-03
0: TRAIN [0][1750/5173]	Time 0.582 (0.624)	Data 1.23e-04 (4.81e-04)	Tok/s 17803 (23025)	Loss/tok 3.6962 (5.3158)	LR 4.000e-03
0: TRAIN [0][1760/5173]	Time 0.643 (0.624)	Data 1.30e-04 (4.79e-04)	Tok/s 26126 (23014)	Loss/tok 3.9213 (5.3084)	LR 4.000e-03
0: TRAIN [0][1770/5173]	Time 0.707 (0.624)	Data 1.26e-04 (4.78e-04)	Tok/s 32663 (23013)	Loss/tok 4.2509 (5.3009)	LR 4.000e-03
0: TRAIN [0][1780/5173]	Time 0.643 (0.624)	Data 3.00e-04 (4.76e-04)	Tok/s 25872 (23010)	Loss/tok 3.9574 (5.2931)	LR 4.000e-03
0: TRAIN [0][1790/5173]	Time 0.774 (0.624)	Data 1.29e-04 (4.74e-04)	Tok/s 37893 (23006)	Loss/tok 4.4308 (5.2859)	LR 4.000e-03
0: TRAIN [0][1800/5173]	Time 0.644 (0.624)	Data 1.26e-04 (4.72e-04)	Tok/s 26108 (23000)	Loss/tok 3.8809 (5.2786)	LR 4.000e-03
0: TRAIN [0][1810/5173]	Time 0.583 (0.624)	Data 1.19e-04 (4.70e-04)	Tok/s 17969 (23000)	Loss/tok 3.6788 (5.2711)	LR 4.000e-03
0: TRAIN [0][1820/5173]	Time 0.584 (0.624)	Data 1.26e-04 (4.69e-04)	Tok/s 17607 (23009)	Loss/tok 3.6864 (5.2632)	LR 4.000e-03
0: TRAIN [0][1830/5173]	Time 0.639 (0.624)	Data 2.81e-04 (4.67e-04)	Tok/s 26367 (23024)	Loss/tok 3.8520 (5.2549)	LR 4.000e-03
0: TRAIN [0][1840/5173]	Time 0.783 (0.624)	Data 1.25e-04 (4.65e-04)	Tok/s 38149 (23027)	Loss/tok 4.2256 (5.2474)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1850/5173]	Time 0.580 (0.624)	Data 1.34e-04 (4.64e-04)	Tok/s 17777 (23020)	Loss/tok 3.5972 (5.2407)	LR 4.000e-03
0: TRAIN [0][1860/5173]	Time 0.708 (0.624)	Data 1.32e-04 (4.62e-04)	Tok/s 33290 (23009)	Loss/tok 4.1396 (5.2341)	LR 4.000e-03
0: TRAIN [0][1870/5173]	Time 0.578 (0.624)	Data 3.03e-04 (4.60e-04)	Tok/s 18159 (23007)	Loss/tok 3.5410 (5.2270)	LR 4.000e-03
0: TRAIN [0][1880/5173]	Time 0.646 (0.624)	Data 1.27e-04 (4.59e-04)	Tok/s 26134 (22999)	Loss/tok 3.9116 (5.2206)	LR 4.000e-03
0: TRAIN [0][1890/5173]	Time 0.522 (0.624)	Data 1.29e-04 (4.57e-04)	Tok/s 10026 (22993)	Loss/tok 3.0104 (5.2136)	LR 4.000e-03
0: TRAIN [0][1900/5173]	Time 0.641 (0.624)	Data 1.28e-04 (4.56e-04)	Tok/s 26434 (22991)	Loss/tok 3.9375 (5.2067)	LR 4.000e-03
0: TRAIN [0][1910/5173]	Time 0.640 (0.624)	Data 1.23e-04 (4.54e-04)	Tok/s 25746 (22999)	Loss/tok 3.9224 (5.1993)	LR 4.000e-03
0: TRAIN [0][1920/5173]	Time 0.583 (0.624)	Data 1.25e-04 (4.52e-04)	Tok/s 17819 (22996)	Loss/tok 3.7136 (5.1927)	LR 4.000e-03
0: TRAIN [0][1930/5173]	Time 0.582 (0.624)	Data 1.24e-04 (4.51e-04)	Tok/s 18103 (22990)	Loss/tok 3.6519 (5.1864)	LR 4.000e-03
0: TRAIN [0][1940/5173]	Time 0.584 (0.624)	Data 2.72e-04 (4.49e-04)	Tok/s 17800 (22994)	Loss/tok 3.6138 (5.1796)	LR 4.000e-03
0: TRAIN [0][1950/5173]	Time 0.581 (0.624)	Data 3.24e-04 (4.48e-04)	Tok/s 17827 (22999)	Loss/tok 3.5687 (5.1729)	LR 4.000e-03
0: TRAIN [0][1960/5173]	Time 0.520 (0.624)	Data 2.84e-04 (4.46e-04)	Tok/s 10127 (22983)	Loss/tok 3.1295 (5.1672)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1970/5173]	Time 0.642 (0.624)	Data 1.22e-04 (4.45e-04)	Tok/s 26268 (22983)	Loss/tok 3.9201 (5.1607)	LR 4.000e-03
0: TRAIN [0][1980/5173]	Time 0.582 (0.624)	Data 1.23e-04 (4.43e-04)	Tok/s 17466 (22966)	Loss/tok 3.7482 (5.1550)	LR 4.000e-03
0: TRAIN [0][1990/5173]	Time 0.639 (0.624)	Data 1.28e-04 (4.42e-04)	Tok/s 26440 (22973)	Loss/tok 3.8506 (5.1483)	LR 4.000e-03
0: TRAIN [0][2000/5173]	Time 0.581 (0.624)	Data 1.34e-04 (4.40e-04)	Tok/s 17567 (22990)	Loss/tok 3.6123 (5.1413)	LR 4.000e-03
0: TRAIN [0][2010/5173]	Time 0.585 (0.624)	Data 1.20e-04 (4.39e-04)	Tok/s 17680 (22981)	Loss/tok 3.7240 (5.1352)	LR 4.000e-03
0: TRAIN [0][2020/5173]	Time 0.584 (0.624)	Data 1.25e-04 (4.37e-04)	Tok/s 17303 (22972)	Loss/tok 3.5949 (5.1293)	LR 4.000e-03
0: TRAIN [0][2030/5173]	Time 0.582 (0.624)	Data 1.26e-04 (4.36e-04)	Tok/s 17525 (22966)	Loss/tok 3.6178 (5.1235)	LR 4.000e-03
0: TRAIN [0][2040/5173]	Time 0.581 (0.624)	Data 1.30e-04 (4.34e-04)	Tok/s 17465 (22963)	Loss/tok 3.5810 (5.1177)	LR 4.000e-03
0: TRAIN [0][2050/5173]	Time 0.581 (0.624)	Data 1.47e-04 (4.33e-04)	Tok/s 17519 (22961)	Loss/tok 3.6240 (5.1116)	LR 4.000e-03
0: TRAIN [0][2060/5173]	Time 0.643 (0.624)	Data 3.03e-04 (4.31e-04)	Tok/s 26283 (22965)	Loss/tok 3.8159 (5.1054)	LR 4.000e-03
0: TRAIN [0][2070/5173]	Time 0.582 (0.624)	Data 1.26e-04 (4.30e-04)	Tok/s 17892 (22964)	Loss/tok 3.6434 (5.0992)	LR 4.000e-03
0: TRAIN [0][2080/5173]	Time 0.643 (0.624)	Data 1.18e-04 (4.29e-04)	Tok/s 25998 (22963)	Loss/tok 3.7953 (5.0930)	LR 4.000e-03
0: TRAIN [0][2090/5173]	Time 0.584 (0.624)	Data 1.19e-04 (4.27e-04)	Tok/s 17869 (22953)	Loss/tok 3.5617 (5.0876)	LR 4.000e-03
0: TRAIN [0][2100/5173]	Time 0.640 (0.623)	Data 1.16e-04 (4.26e-04)	Tok/s 26762 (22949)	Loss/tok 4.0028 (5.0819)	LR 4.000e-03
0: TRAIN [0][2110/5173]	Time 0.645 (0.623)	Data 1.19e-04 (4.25e-04)	Tok/s 26098 (22947)	Loss/tok 3.8087 (5.0760)	LR 4.000e-03
0: TRAIN [0][2120/5173]	Time 0.705 (0.624)	Data 1.24e-04 (4.23e-04)	Tok/s 32981 (22969)	Loss/tok 4.1097 (5.0697)	LR 4.000e-03
0: TRAIN [0][2130/5173]	Time 0.639 (0.624)	Data 1.24e-04 (4.22e-04)	Tok/s 26354 (22961)	Loss/tok 3.8754 (5.0641)	LR 4.000e-03
0: TRAIN [0][2140/5173]	Time 0.520 (0.623)	Data 1.24e-04 (4.21e-04)	Tok/s 10166 (22937)	Loss/tok 3.0400 (5.0595)	LR 4.000e-03
0: TRAIN [0][2150/5173]	Time 0.642 (0.623)	Data 1.30e-04 (4.19e-04)	Tok/s 26342 (22942)	Loss/tok 3.8664 (5.0535)	LR 4.000e-03
0: TRAIN [0][2160/5173]	Time 0.644 (0.623)	Data 1.21e-04 (4.18e-04)	Tok/s 25920 (22932)	Loss/tok 3.8365 (5.0484)	LR 4.000e-03
0: TRAIN [0][2170/5173]	Time 0.644 (0.623)	Data 1.25e-04 (4.17e-04)	Tok/s 26346 (22924)	Loss/tok 3.8718 (5.0432)	LR 4.000e-03
0: TRAIN [0][2180/5173]	Time 0.583 (0.623)	Data 1.24e-04 (4.16e-04)	Tok/s 17691 (22900)	Loss/tok 3.5816 (5.0385)	LR 4.000e-03
0: TRAIN [0][2190/5173]	Time 0.584 (0.623)	Data 1.23e-04 (4.14e-04)	Tok/s 17958 (22914)	Loss/tok 3.5297 (5.0327)	LR 4.000e-03
0: TRAIN [0][2200/5173]	Time 0.581 (0.623)	Data 1.28e-04 (4.13e-04)	Tok/s 17645 (22895)	Loss/tok 3.6603 (5.0280)	LR 4.000e-03
0: TRAIN [0][2210/5173]	Time 0.521 (0.623)	Data 1.24e-04 (4.12e-04)	Tok/s 10269 (22872)	Loss/tok 3.1121 (5.0234)	LR 4.000e-03
0: TRAIN [0][2220/5173]	Time 0.641 (0.623)	Data 1.19e-04 (4.10e-04)	Tok/s 26015 (22867)	Loss/tok 3.7477 (5.0181)	LR 4.000e-03
0: TRAIN [0][2230/5173]	Time 0.521 (0.623)	Data 1.31e-04 (4.09e-04)	Tok/s 10166 (22876)	Loss/tok 3.0486 (5.0123)	LR 4.000e-03
0: TRAIN [0][2240/5173]	Time 0.783 (0.623)	Data 1.28e-04 (4.08e-04)	Tok/s 37602 (22896)	Loss/tok 4.2267 (5.0062)	LR 4.000e-03
0: TRAIN [0][2250/5173]	Time 0.644 (0.623)	Data 2.99e-04 (4.07e-04)	Tok/s 26272 (22914)	Loss/tok 3.6905 (5.0002)	LR 4.000e-03
0: TRAIN [0][2260/5173]	Time 0.705 (0.623)	Data 1.29e-04 (4.06e-04)	Tok/s 33195 (22909)	Loss/tok 4.0033 (4.9952)	LR 4.000e-03
0: TRAIN [0][2270/5173]	Time 0.644 (0.623)	Data 1.25e-04 (4.04e-04)	Tok/s 25853 (22915)	Loss/tok 3.9511 (4.9899)	LR 4.000e-03
0: TRAIN [0][2280/5173]	Time 0.584 (0.623)	Data 1.25e-04 (4.03e-04)	Tok/s 17772 (22900)	Loss/tok 3.6303 (4.9853)	LR 4.000e-03
0: TRAIN [0][2290/5173]	Time 0.521 (0.623)	Data 1.23e-04 (4.02e-04)	Tok/s 9917 (22912)	Loss/tok 3.0234 (4.9798)	LR 4.000e-03
0: TRAIN [0][2300/5173]	Time 0.521 (0.623)	Data 1.18e-04 (4.01e-04)	Tok/s 10062 (22924)	Loss/tok 3.1206 (4.9744)	LR 4.000e-03
0: TRAIN [0][2310/5173]	Time 0.778 (0.623)	Data 1.25e-04 (4.00e-04)	Tok/s 38637 (22934)	Loss/tok 4.1616 (4.9690)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2320/5173]	Time 0.584 (0.623)	Data 1.89e-04 (3.99e-04)	Tok/s 17678 (22926)	Loss/tok 3.4210 (4.9642)	LR 4.000e-03
0: TRAIN [0][2330/5173]	Time 0.646 (0.623)	Data 1.28e-04 (3.97e-04)	Tok/s 26129 (22933)	Loss/tok 3.7434 (4.9592)	LR 4.000e-03
0: TRAIN [0][2340/5173]	Time 0.579 (0.623)	Data 1.26e-04 (3.96e-04)	Tok/s 17808 (22918)	Loss/tok 3.5753 (4.9548)	LR 4.000e-03
0: TRAIN [0][2350/5173]	Time 0.585 (0.623)	Data 1.20e-04 (3.95e-04)	Tok/s 17352 (22910)	Loss/tok 3.7641 (4.9503)	LR 4.000e-03
0: TRAIN [0][2360/5173]	Time 0.581 (0.623)	Data 1.27e-04 (3.94e-04)	Tok/s 18026 (22911)	Loss/tok 3.6769 (4.9457)	LR 4.000e-03
0: TRAIN [0][2370/5173]	Time 0.584 (0.623)	Data 1.26e-04 (3.93e-04)	Tok/s 17396 (22910)	Loss/tok 3.5988 (4.9410)	LR 4.000e-03
0: TRAIN [0][2380/5173]	Time 0.582 (0.623)	Data 1.22e-04 (3.92e-04)	Tok/s 17802 (22896)	Loss/tok 3.5261 (4.9367)	LR 4.000e-03
0: TRAIN [0][2390/5173]	Time 0.583 (0.623)	Data 1.25e-04 (3.91e-04)	Tok/s 17899 (22894)	Loss/tok 3.4835 (4.9321)	LR 4.000e-03
0: TRAIN [0][2400/5173]	Time 0.522 (0.623)	Data 1.29e-04 (3.90e-04)	Tok/s 10187 (22873)	Loss/tok 3.1250 (4.9282)	LR 4.000e-03
0: TRAIN [0][2410/5173]	Time 0.644 (0.623)	Data 1.20e-04 (3.89e-04)	Tok/s 26121 (22868)	Loss/tok 3.7108 (4.9236)	LR 4.000e-03
0: TRAIN [0][2420/5173]	Time 0.582 (0.623)	Data 1.17e-04 (3.88e-04)	Tok/s 17641 (22872)	Loss/tok 3.6220 (4.9188)	LR 4.000e-03
0: TRAIN [0][2430/5173]	Time 0.520 (0.623)	Data 1.25e-04 (3.87e-04)	Tok/s 10025 (22858)	Loss/tok 2.9324 (4.9146)	LR 4.000e-03
0: TRAIN [0][2440/5173]	Time 0.579 (0.623)	Data 1.24e-04 (3.86e-04)	Tok/s 18030 (22846)	Loss/tok 3.4961 (4.9104)	LR 4.000e-03
0: TRAIN [0][2450/5173]	Time 0.638 (0.623)	Data 1.46e-04 (3.86e-04)	Tok/s 26197 (22860)	Loss/tok 3.7421 (4.9053)	LR 4.000e-03
0: TRAIN [0][2460/5173]	Time 0.705 (0.623)	Data 1.28e-04 (3.85e-04)	Tok/s 33021 (22864)	Loss/tok 3.9284 (4.9006)	LR 4.000e-03
0: TRAIN [0][2470/5173]	Time 0.523 (0.623)	Data 1.22e-04 (3.84e-04)	Tok/s 10065 (22866)	Loss/tok 3.1113 (4.8959)	LR 4.000e-03
0: TRAIN [0][2480/5173]	Time 0.642 (0.623)	Data 1.24e-04 (3.83e-04)	Tok/s 26150 (22873)	Loss/tok 3.6835 (4.8913)	LR 4.000e-03
0: TRAIN [0][2490/5173]	Time 0.642 (0.623)	Data 2.93e-04 (3.82e-04)	Tok/s 26068 (22866)	Loss/tok 3.9291 (4.8872)	LR 4.000e-03
0: TRAIN [0][2500/5173]	Time 0.584 (0.623)	Data 1.27e-04 (3.81e-04)	Tok/s 17553 (22873)	Loss/tok 3.5783 (4.8826)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2510/5173]	Time 0.639 (0.623)	Data 1.39e-04 (3.80e-04)	Tok/s 26255 (22886)	Loss/tok 3.8265 (4.8778)	LR 4.000e-03
0: TRAIN [0][2520/5173]	Time 0.639 (0.623)	Data 1.49e-04 (3.79e-04)	Tok/s 26064 (22892)	Loss/tok 3.7755 (4.8733)	LR 4.000e-03
0: TRAIN [0][2530/5173]	Time 0.640 (0.623)	Data 1.21e-04 (3.78e-04)	Tok/s 26436 (22884)	Loss/tok 3.7825 (4.8692)	LR 4.000e-03
0: TRAIN [0][2540/5173]	Time 0.583 (0.623)	Data 1.13e-04 (3.77e-04)	Tok/s 17780 (22891)	Loss/tok 3.4705 (4.8649)	LR 4.000e-03
0: TRAIN [0][2550/5173]	Time 0.708 (0.623)	Data 1.38e-04 (3.76e-04)	Tok/s 33094 (22904)	Loss/tok 4.0446 (4.8601)	LR 4.000e-03
0: TRAIN [0][2560/5173]	Time 0.518 (0.623)	Data 1.47e-04 (3.75e-04)	Tok/s 10040 (22888)	Loss/tok 3.0510 (4.8564)	LR 4.000e-03
0: TRAIN [0][2570/5173]	Time 0.581 (0.623)	Data 1.16e-04 (3.74e-04)	Tok/s 17838 (22880)	Loss/tok 3.5640 (4.8524)	LR 4.000e-03
0: TRAIN [0][2580/5173]	Time 0.580 (0.623)	Data 1.63e-04 (3.74e-04)	Tok/s 17670 (22906)	Loss/tok 3.5437 (4.8477)	LR 4.000e-03
0: TRAIN [0][2590/5173]	Time 0.581 (0.623)	Data 3.39e-04 (3.73e-04)	Tok/s 18033 (22913)	Loss/tok 3.4945 (4.8434)	LR 4.000e-03
0: TRAIN [0][2600/5173]	Time 0.705 (0.623)	Data 1.24e-04 (3.72e-04)	Tok/s 32919 (22906)	Loss/tok 3.9915 (4.8394)	LR 4.000e-03
0: TRAIN [0][2610/5173]	Time 0.580 (0.623)	Data 1.24e-04 (3.71e-04)	Tok/s 17940 (22893)	Loss/tok 3.4739 (4.8357)	LR 4.000e-03
0: TRAIN [0][2620/5173]	Time 0.584 (0.623)	Data 1.26e-04 (3.70e-04)	Tok/s 17701 (22882)	Loss/tok 3.5573 (4.8321)	LR 4.000e-03
0: TRAIN [0][2630/5173]	Time 0.583 (0.623)	Data 1.31e-04 (3.69e-04)	Tok/s 17365 (22869)	Loss/tok 3.5601 (4.8286)	LR 4.000e-03
0: TRAIN [0][2640/5173]	Time 0.647 (0.623)	Data 1.35e-04 (3.68e-04)	Tok/s 26205 (22873)	Loss/tok 3.7409 (4.8246)	LR 4.000e-03
0: TRAIN [0][2650/5173]	Time 0.643 (0.623)	Data 1.26e-04 (3.67e-04)	Tok/s 26356 (22877)	Loss/tok 3.7545 (4.8206)	LR 4.000e-03
0: TRAIN [0][2660/5173]	Time 0.584 (0.623)	Data 1.20e-04 (3.67e-04)	Tok/s 17811 (22880)	Loss/tok 3.5224 (4.8165)	LR 4.000e-03
0: TRAIN [0][2670/5173]	Time 0.644 (0.623)	Data 1.24e-04 (3.66e-04)	Tok/s 25842 (22890)	Loss/tok 3.7447 (4.8124)	LR 4.000e-03
0: TRAIN [0][2680/5173]	Time 0.519 (0.623)	Data 1.25e-04 (3.65e-04)	Tok/s 10317 (22900)	Loss/tok 3.0625 (4.8083)	LR 4.000e-03
0: TRAIN [0][2690/5173]	Time 0.639 (0.623)	Data 1.26e-04 (3.64e-04)	Tok/s 26733 (22899)	Loss/tok 3.6230 (4.8044)	LR 4.000e-03
0: TRAIN [0][2700/5173]	Time 0.583 (0.623)	Data 1.27e-04 (3.63e-04)	Tok/s 17417 (22892)	Loss/tok 3.4650 (4.8007)	LR 4.000e-03
0: TRAIN [0][2710/5173]	Time 0.704 (0.623)	Data 1.36e-04 (3.62e-04)	Tok/s 32495 (22882)	Loss/tok 4.1161 (4.7973)	LR 4.000e-03
0: TRAIN [0][2720/5173]	Time 0.646 (0.623)	Data 1.28e-04 (3.61e-04)	Tok/s 25378 (22878)	Loss/tok 3.7704 (4.7937)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2730/5173]	Time 0.581 (0.623)	Data 1.21e-04 (3.61e-04)	Tok/s 17512 (22867)	Loss/tok 3.5337 (4.7904)	LR 4.000e-03
0: TRAIN [0][2740/5173]	Time 0.582 (0.623)	Data 3.08e-04 (3.60e-04)	Tok/s 17948 (22873)	Loss/tok 3.4809 (4.7867)	LR 4.000e-03
0: TRAIN [0][2750/5173]	Time 0.642 (0.623)	Data 1.32e-04 (3.59e-04)	Tok/s 26210 (22876)	Loss/tok 3.6417 (4.7829)	LR 4.000e-03
0: TRAIN [0][2760/5173]	Time 0.522 (0.623)	Data 1.23e-04 (3.58e-04)	Tok/s 10039 (22877)	Loss/tok 3.0608 (4.7790)	LR 4.000e-03
0: TRAIN [0][2770/5173]	Time 0.645 (0.623)	Data 1.27e-04 (3.58e-04)	Tok/s 25731 (22878)	Loss/tok 3.6799 (4.7755)	LR 4.000e-03
0: TRAIN [0][2780/5173]	Time 0.645 (0.623)	Data 1.25e-04 (3.57e-04)	Tok/s 26137 (22866)	Loss/tok 3.8695 (4.7724)	LR 4.000e-03
0: TRAIN [0][2790/5173]	Time 0.638 (0.623)	Data 1.20e-04 (3.56e-04)	Tok/s 26055 (22865)	Loss/tok 3.7992 (4.7687)	LR 4.000e-03
0: TRAIN [0][2800/5173]	Time 0.639 (0.623)	Data 1.31e-04 (3.55e-04)	Tok/s 26115 (22885)	Loss/tok 3.8068 (4.7645)	LR 4.000e-03
0: TRAIN [0][2810/5173]	Time 0.584 (0.623)	Data 1.27e-04 (3.54e-04)	Tok/s 17614 (22882)	Loss/tok 3.4607 (4.7610)	LR 4.000e-03
0: TRAIN [0][2820/5173]	Time 0.645 (0.623)	Data 1.38e-04 (3.54e-04)	Tok/s 26013 (22899)	Loss/tok 3.8903 (4.7571)	LR 4.000e-03
0: TRAIN [0][2830/5173]	Time 0.702 (0.623)	Data 1.31e-04 (3.53e-04)	Tok/s 33394 (22903)	Loss/tok 3.9001 (4.7534)	LR 4.000e-03
0: TRAIN [0][2840/5173]	Time 0.641 (0.623)	Data 1.21e-04 (3.52e-04)	Tok/s 25939 (22904)	Loss/tok 3.8865 (4.7498)	LR 4.000e-03
0: TRAIN [0][2850/5173]	Time 0.517 (0.623)	Data 1.21e-04 (3.51e-04)	Tok/s 10141 (22907)	Loss/tok 2.9618 (4.7462)	LR 4.000e-03
0: TRAIN [0][2860/5173]	Time 0.583 (0.623)	Data 1.22e-04 (3.51e-04)	Tok/s 17882 (22894)	Loss/tok 3.4757 (4.7432)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2870/5173]	Time 0.583 (0.623)	Data 1.24e-04 (3.50e-04)	Tok/s 17754 (22900)	Loss/tok 3.5344 (4.7397)	LR 4.000e-03
0: TRAIN [0][2880/5173]	Time 0.778 (0.623)	Data 1.28e-04 (3.49e-04)	Tok/s 38317 (22912)	Loss/tok 4.1476 (4.7361)	LR 4.000e-03
0: TRAIN [0][2890/5173]	Time 0.641 (0.623)	Data 1.29e-04 (3.48e-04)	Tok/s 25826 (22903)	Loss/tok 3.8407 (4.7329)	LR 4.000e-03
0: TRAIN [0][2900/5173]	Time 0.520 (0.623)	Data 1.24e-04 (3.48e-04)	Tok/s 9869 (22891)	Loss/tok 2.9338 (4.7297)	LR 4.000e-03
0: TRAIN [0][2910/5173]	Time 0.705 (0.623)	Data 1.25e-04 (3.47e-04)	Tok/s 33326 (22881)	Loss/tok 3.8777 (4.7267)	LR 4.000e-03
0: TRAIN [0][2920/5173]	Time 0.520 (0.623)	Data 1.20e-04 (3.46e-04)	Tok/s 10150 (22877)	Loss/tok 3.0156 (4.7234)	LR 4.000e-03
0: TRAIN [0][2930/5173]	Time 0.583 (0.623)	Data 1.27e-04 (3.46e-04)	Tok/s 17567 (22862)	Loss/tok 3.3852 (4.7206)	LR 4.000e-03
0: TRAIN [0][2940/5173]	Time 0.583 (0.623)	Data 1.19e-04 (3.45e-04)	Tok/s 17348 (22868)	Loss/tok 3.5555 (4.7172)	LR 4.000e-03
0: TRAIN [0][2950/5173]	Time 0.581 (0.623)	Data 1.29e-04 (3.44e-04)	Tok/s 17984 (22873)	Loss/tok 3.5448 (4.7137)	LR 4.000e-03
0: TRAIN [0][2960/5173]	Time 0.582 (0.623)	Data 1.25e-04 (3.44e-04)	Tok/s 17821 (22863)	Loss/tok 3.6418 (4.7108)	LR 4.000e-03
0: TRAIN [0][2970/5173]	Time 0.645 (0.623)	Data 2.89e-04 (3.43e-04)	Tok/s 26161 (22872)	Loss/tok 3.7798 (4.7075)	LR 4.000e-03
0: TRAIN [0][2980/5173]	Time 0.641 (0.623)	Data 1.21e-04 (3.42e-04)	Tok/s 26171 (22865)	Loss/tok 3.8674 (4.7045)	LR 4.000e-03
0: TRAIN [0][2990/5173]	Time 0.642 (0.623)	Data 2.85e-04 (3.42e-04)	Tok/s 26092 (22846)	Loss/tok 3.6885 (4.7016)	LR 4.000e-03
0: TRAIN [0][3000/5173]	Time 0.641 (0.623)	Data 1.33e-04 (3.41e-04)	Tok/s 26400 (22847)	Loss/tok 3.6173 (4.6984)	LR 4.000e-03
0: TRAIN [0][3010/5173]	Time 0.778 (0.623)	Data 1.29e-04 (3.40e-04)	Tok/s 37685 (22848)	Loss/tok 4.2088 (4.6954)	LR 4.000e-03
0: TRAIN [0][3020/5173]	Time 0.641 (0.623)	Data 2.75e-04 (3.40e-04)	Tok/s 26418 (22849)	Loss/tok 3.7425 (4.6921)	LR 4.000e-03
0: TRAIN [0][3030/5173]	Time 0.521 (0.623)	Data 1.65e-04 (3.39e-04)	Tok/s 9964 (22847)	Loss/tok 3.0800 (4.6889)	LR 4.000e-03
0: TRAIN [0][3040/5173]	Time 0.643 (0.623)	Data 1.48e-04 (3.39e-04)	Tok/s 26385 (22847)	Loss/tok 3.5785 (4.6857)	LR 4.000e-03
0: TRAIN [0][3050/5173]	Time 0.584 (0.623)	Data 1.59e-04 (3.38e-04)	Tok/s 17704 (22845)	Loss/tok 3.5154 (4.6825)	LR 4.000e-03
0: TRAIN [0][3060/5173]	Time 0.643 (0.623)	Data 1.46e-04 (3.37e-04)	Tok/s 26233 (22844)	Loss/tok 3.4823 (4.6793)	LR 4.000e-03
0: TRAIN [0][3070/5173]	Time 0.642 (0.623)	Data 1.22e-04 (3.37e-04)	Tok/s 26050 (22844)	Loss/tok 3.7304 (4.6761)	LR 4.000e-03
0: TRAIN [0][3080/5173]	Time 0.522 (0.623)	Data 1.21e-04 (3.36e-04)	Tok/s 10393 (22842)	Loss/tok 2.9528 (4.6730)	LR 4.000e-03
0: TRAIN [0][3090/5173]	Time 0.640 (0.623)	Data 2.97e-04 (3.35e-04)	Tok/s 26301 (22834)	Loss/tok 3.8498 (4.6701)	LR 4.000e-03
0: TRAIN [0][3100/5173]	Time 0.580 (0.623)	Data 1.24e-04 (3.35e-04)	Tok/s 17765 (22826)	Loss/tok 3.6621 (4.6672)	LR 4.000e-03
0: TRAIN [0][3110/5173]	Time 0.582 (0.623)	Data 1.29e-04 (3.34e-04)	Tok/s 17929 (22832)	Loss/tok 3.4243 (4.6640)	LR 4.000e-03
0: TRAIN [0][3120/5173]	Time 0.582 (0.623)	Data 1.32e-04 (3.34e-04)	Tok/s 17467 (22825)	Loss/tok 3.5269 (4.6611)	LR 4.000e-03
0: TRAIN [0][3130/5173]	Time 0.585 (0.622)	Data 1.32e-04 (3.33e-04)	Tok/s 17728 (22819)	Loss/tok 3.5811 (4.6582)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3140/5173]	Time 0.638 (0.623)	Data 1.22e-04 (3.32e-04)	Tok/s 26437 (22824)	Loss/tok 3.7493 (4.6552)	LR 4.000e-03
0: TRAIN [0][3150/5173]	Time 0.778 (0.623)	Data 1.40e-04 (3.32e-04)	Tok/s 38358 (22841)	Loss/tok 4.0135 (4.6517)	LR 4.000e-03
0: TRAIN [0][3160/5173]	Time 0.703 (0.623)	Data 1.35e-04 (3.31e-04)	Tok/s 33112 (22849)	Loss/tok 3.8936 (4.6487)	LR 4.000e-03
0: TRAIN [0][3170/5173]	Time 0.583 (0.623)	Data 1.19e-04 (3.31e-04)	Tok/s 18197 (22844)	Loss/tok 3.4754 (4.6458)	LR 4.000e-03
0: TRAIN [0][3180/5173]	Time 0.583 (0.623)	Data 1.20e-04 (3.30e-04)	Tok/s 17636 (22838)	Loss/tok 3.5204 (4.6431)	LR 4.000e-03
0: TRAIN [0][3190/5173]	Time 0.706 (0.623)	Data 1.29e-04 (3.30e-04)	Tok/s 33223 (22844)	Loss/tok 3.8768 (4.6401)	LR 4.000e-03
0: TRAIN [0][3200/5173]	Time 0.584 (0.623)	Data 3.09e-04 (3.29e-04)	Tok/s 17788 (22838)	Loss/tok 3.4823 (4.6372)	LR 4.000e-03
0: TRAIN [0][3210/5173]	Time 0.580 (0.623)	Data 1.26e-04 (3.28e-04)	Tok/s 18184 (22844)	Loss/tok 3.5319 (4.6343)	LR 4.000e-03
0: TRAIN [0][3220/5173]	Time 0.580 (0.623)	Data 1.23e-04 (3.28e-04)	Tok/s 17913 (22833)	Loss/tok 3.5967 (4.6317)	LR 4.000e-03
0: TRAIN [0][3230/5173]	Time 0.703 (0.623)	Data 1.29e-04 (3.27e-04)	Tok/s 33138 (22835)	Loss/tok 3.9560 (4.6288)	LR 4.000e-03
0: TRAIN [0][3240/5173]	Time 0.644 (0.623)	Data 1.23e-04 (3.27e-04)	Tok/s 25815 (22834)	Loss/tok 3.7552 (4.6260)	LR 4.000e-03
0: TRAIN [0][3250/5173]	Time 0.523 (0.623)	Data 1.31e-04 (3.26e-04)	Tok/s 10246 (22827)	Loss/tok 2.8943 (4.6234)	LR 4.000e-03
0: TRAIN [0][3260/5173]	Time 0.583 (0.622)	Data 1.19e-04 (3.25e-04)	Tok/s 17870 (22816)	Loss/tok 3.5410 (4.6209)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3270/5173]	Time 0.639 (0.623)	Data 1.18e-04 (3.25e-04)	Tok/s 26569 (22828)	Loss/tok 3.7084 (4.6178)	LR 4.000e-03
0: TRAIN [0][3280/5173]	Time 0.581 (0.623)	Data 1.25e-04 (3.24e-04)	Tok/s 17449 (22834)	Loss/tok 3.4685 (4.6149)	LR 4.000e-03
0: TRAIN [0][3290/5173]	Time 0.641 (0.623)	Data 3.15e-04 (3.24e-04)	Tok/s 26385 (22841)	Loss/tok 3.7525 (4.6123)	LR 4.000e-03
0: TRAIN [0][3300/5173]	Time 0.583 (0.623)	Data 1.50e-04 (3.23e-04)	Tok/s 17807 (22840)	Loss/tok 3.3992 (4.6095)	LR 4.000e-03
0: TRAIN [0][3310/5173]	Time 0.646 (0.623)	Data 1.30e-04 (3.23e-04)	Tok/s 25854 (22849)	Loss/tok 3.6659 (4.6067)	LR 4.000e-03
0: TRAIN [0][3320/5173]	Time 0.585 (0.623)	Data 1.20e-04 (3.22e-04)	Tok/s 17934 (22834)	Loss/tok 3.4992 (4.6044)	LR 4.000e-03
0: TRAIN [0][3330/5173]	Time 0.704 (0.623)	Data 1.23e-04 (3.21e-04)	Tok/s 33078 (22836)	Loss/tok 3.9193 (4.6016)	LR 4.000e-03
0: TRAIN [0][3340/5173]	Time 0.582 (0.623)	Data 1.39e-04 (3.21e-04)	Tok/s 17599 (22841)	Loss/tok 3.4847 (4.5989)	LR 4.000e-03
0: TRAIN [0][3350/5173]	Time 0.520 (0.623)	Data 1.14e-04 (3.21e-04)	Tok/s 10133 (22833)	Loss/tok 3.0523 (4.5965)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][3360/5173]	Time 0.705 (0.623)	Data 1.19e-04 (3.20e-04)	Tok/s 33367 (22834)	Loss/tok 3.9049 (4.5939)	LR 4.000e-03
0: TRAIN [0][3370/5173]	Time 0.581 (0.623)	Data 1.14e-04 (3.19e-04)	Tok/s 17810 (22828)	Loss/tok 3.3395 (4.5913)	LR 4.000e-03
0: TRAIN [0][3380/5173]	Time 0.585 (0.623)	Data 1.16e-04 (3.19e-04)	Tok/s 17268 (22824)	Loss/tok 3.4903 (4.5889)	LR 4.000e-03
0: TRAIN [0][3390/5173]	Time 0.705 (0.623)	Data 1.22e-04 (3.18e-04)	Tok/s 33245 (22835)	Loss/tok 3.9170 (4.5859)	LR 4.000e-03
0: TRAIN [0][3400/5173]	Time 0.703 (0.623)	Data 2.84e-04 (3.18e-04)	Tok/s 33390 (22847)	Loss/tok 3.8669 (4.5831)	LR 4.000e-03
0: TRAIN [0][3410/5173]	Time 0.648 (0.623)	Data 1.13e-04 (3.17e-04)	Tok/s 25674 (22849)	Loss/tok 3.8009 (4.5804)	LR 4.000e-03
0: TRAIN [0][3420/5173]	Time 0.581 (0.623)	Data 1.13e-04 (3.17e-04)	Tok/s 18079 (22848)	Loss/tok 3.5537 (4.5778)	LR 4.000e-03
0: TRAIN [0][3430/5173]	Time 0.581 (0.623)	Data 2.93e-04 (3.16e-04)	Tok/s 17763 (22864)	Loss/tok 3.5524 (4.5749)	LR 4.000e-03
0: TRAIN [0][3440/5173]	Time 0.639 (0.623)	Data 1.18e-04 (3.16e-04)	Tok/s 26093 (22861)	Loss/tok 3.8847 (4.5726)	LR 4.000e-03
0: TRAIN [0][3450/5173]	Time 0.642 (0.623)	Data 1.28e-04 (3.15e-04)	Tok/s 26343 (22866)	Loss/tok 3.6596 (4.5699)	LR 4.000e-03
0: TRAIN [0][3460/5173]	Time 0.580 (0.623)	Data 2.78e-04 (3.15e-04)	Tok/s 17796 (22870)	Loss/tok 3.4856 (4.5672)	LR 4.000e-03
0: TRAIN [0][3470/5173]	Time 0.580 (0.623)	Data 1.23e-04 (3.14e-04)	Tok/s 17757 (22875)	Loss/tok 3.5083 (4.5646)	LR 4.000e-03
0: TRAIN [0][3480/5173]	Time 0.643 (0.623)	Data 1.80e-04 (3.14e-04)	Tok/s 25631 (22896)	Loss/tok 3.7404 (4.5617)	LR 4.000e-03
0: TRAIN [0][3490/5173]	Time 0.523 (0.623)	Data 1.53e-04 (3.13e-04)	Tok/s 10046 (22907)	Loss/tok 2.9922 (4.5592)	LR 4.000e-03
0: TRAIN [0][3500/5173]	Time 0.518 (0.623)	Data 1.44e-04 (3.13e-04)	Tok/s 10295 (22899)	Loss/tok 2.8204 (4.5570)	LR 4.000e-03
0: TRAIN [0][3510/5173]	Time 0.583 (0.623)	Data 1.48e-04 (3.12e-04)	Tok/s 17508 (22898)	Loss/tok 3.4106 (4.5545)	LR 4.000e-03
0: TRAIN [0][3520/5173]	Time 0.581 (0.623)	Data 3.04e-04 (3.12e-04)	Tok/s 18009 (22891)	Loss/tok 3.4461 (4.5521)	LR 4.000e-03
0: TRAIN [0][3530/5173]	Time 0.582 (0.623)	Data 2.73e-04 (3.11e-04)	Tok/s 17946 (22896)	Loss/tok 3.4312 (4.5496)	LR 4.000e-03
0: TRAIN [0][3540/5173]	Time 0.640 (0.623)	Data 1.26e-04 (3.11e-04)	Tok/s 25975 (22892)	Loss/tok 3.7560 (4.5473)	LR 4.000e-03
0: TRAIN [0][3550/5173]	Time 0.707 (0.623)	Data 1.25e-04 (3.10e-04)	Tok/s 33071 (22889)	Loss/tok 3.8415 (4.5449)	LR 4.000e-03
0: TRAIN [0][3560/5173]	Time 0.708 (0.623)	Data 1.19e-04 (3.10e-04)	Tok/s 33019 (22884)	Loss/tok 3.9608 (4.5427)	LR 4.000e-03
0: TRAIN [0][3570/5173]	Time 0.642 (0.623)	Data 1.33e-04 (3.10e-04)	Tok/s 26028 (22884)	Loss/tok 3.6811 (4.5401)	LR 4.000e-03
0: TRAIN [0][3580/5173]	Time 0.708 (0.623)	Data 1.39e-04 (3.09e-04)	Tok/s 33074 (22884)	Loss/tok 3.8802 (4.5377)	LR 4.000e-03
0: TRAIN [0][3590/5173]	Time 0.583 (0.623)	Data 1.23e-04 (3.09e-04)	Tok/s 17836 (22880)	Loss/tok 3.3807 (4.5353)	LR 4.000e-03
0: TRAIN [0][3600/5173]	Time 0.703 (0.623)	Data 1.29e-04 (3.08e-04)	Tok/s 32887 (22888)	Loss/tok 3.9142 (4.5329)	LR 4.000e-03
0: TRAIN [0][3610/5173]	Time 0.778 (0.623)	Data 1.26e-04 (3.08e-04)	Tok/s 38492 (22895)	Loss/tok 4.0152 (4.5305)	LR 4.000e-03
0: TRAIN [0][3620/5173]	Time 0.582 (0.623)	Data 2.75e-04 (3.07e-04)	Tok/s 17882 (22897)	Loss/tok 3.4785 (4.5280)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3630/5173]	Time 0.584 (0.623)	Data 1.30e-04 (3.07e-04)	Tok/s 17858 (22901)	Loss/tok 3.4250 (4.5256)	LR 4.000e-03
0: TRAIN [0][3640/5173]	Time 0.581 (0.623)	Data 1.27e-04 (3.06e-04)	Tok/s 17554 (22904)	Loss/tok 3.5498 (4.5233)	LR 4.000e-03
0: TRAIN [0][3650/5173]	Time 0.582 (0.623)	Data 1.23e-04 (3.06e-04)	Tok/s 17928 (22901)	Loss/tok 3.3086 (4.5210)	LR 4.000e-03
0: TRAIN [0][3660/5173]	Time 0.581 (0.623)	Data 2.98e-04 (3.05e-04)	Tok/s 17651 (22898)	Loss/tok 3.4382 (4.5187)	LR 4.000e-03
0: TRAIN [0][3670/5173]	Time 0.584 (0.623)	Data 1.25e-04 (3.05e-04)	Tok/s 17731 (22893)	Loss/tok 3.4869 (4.5165)	LR 4.000e-03
0: TRAIN [0][3680/5173]	Time 0.581 (0.623)	Data 1.25e-04 (3.05e-04)	Tok/s 17808 (22901)	Loss/tok 3.3867 (4.5140)	LR 4.000e-03
0: TRAIN [0][3690/5173]	Time 0.704 (0.623)	Data 1.28e-04 (3.04e-04)	Tok/s 32884 (22891)	Loss/tok 3.8619 (4.5119)	LR 4.000e-03
0: TRAIN [0][3700/5173]	Time 0.583 (0.623)	Data 1.23e-04 (3.04e-04)	Tok/s 17510 (22889)	Loss/tok 3.4101 (4.5097)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][3710/5173]	Time 0.585 (0.623)	Data 1.15e-04 (3.03e-04)	Tok/s 17559 (22889)	Loss/tok 3.4775 (4.5076)	LR 4.000e-03
0: TRAIN [0][3720/5173]	Time 0.780 (0.623)	Data 1.23e-04 (3.03e-04)	Tok/s 38260 (22886)	Loss/tok 4.0574 (4.5055)	LR 4.000e-03
0: TRAIN [0][3730/5173]	Time 0.779 (0.623)	Data 1.24e-04 (3.02e-04)	Tok/s 38410 (22889)	Loss/tok 4.0658 (4.5033)	LR 4.000e-03
0: TRAIN [0][3740/5173]	Time 0.582 (0.623)	Data 1.15e-04 (3.02e-04)	Tok/s 17548 (22886)	Loss/tok 3.3709 (4.5012)	LR 4.000e-03
0: TRAIN [0][3750/5173]	Time 0.581 (0.623)	Data 1.29e-04 (3.01e-04)	Tok/s 17710 (22886)	Loss/tok 3.3680 (4.4989)	LR 4.000e-03
0: TRAIN [0][3760/5173]	Time 0.580 (0.623)	Data 1.31e-04 (3.01e-04)	Tok/s 17802 (22875)	Loss/tok 3.4631 (4.4971)	LR 4.000e-03
0: TRAIN [0][3770/5173]	Time 0.647 (0.623)	Data 1.37e-04 (3.01e-04)	Tok/s 25969 (22872)	Loss/tok 3.7880 (4.4950)	LR 4.000e-03
0: TRAIN [0][3780/5173]	Time 0.642 (0.623)	Data 2.90e-04 (3.00e-04)	Tok/s 26499 (22870)	Loss/tok 3.6682 (4.4929)	LR 4.000e-03
0: TRAIN [0][3790/5173]	Time 0.639 (0.623)	Data 1.23e-04 (3.00e-04)	Tok/s 26208 (22874)	Loss/tok 3.6149 (4.4906)	LR 4.000e-03
0: TRAIN [0][3800/5173]	Time 0.637 (0.623)	Data 1.31e-04 (2.99e-04)	Tok/s 26738 (22877)	Loss/tok 3.5727 (4.4883)	LR 4.000e-03
0: TRAIN [0][3810/5173]	Time 0.583 (0.623)	Data 1.16e-04 (2.99e-04)	Tok/s 17567 (22879)	Loss/tok 3.4638 (4.4861)	LR 4.000e-03
0: TRAIN [0][3820/5173]	Time 0.581 (0.623)	Data 1.25e-04 (2.99e-04)	Tok/s 17548 (22878)	Loss/tok 3.6010 (4.4839)	LR 4.000e-03
0: TRAIN [0][3830/5173]	Time 0.582 (0.623)	Data 1.22e-04 (2.98e-04)	Tok/s 17546 (22884)	Loss/tok 3.5648 (4.4818)	LR 4.000e-03
0: TRAIN [0][3840/5173]	Time 0.704 (0.623)	Data 1.22e-04 (2.98e-04)	Tok/s 32486 (22894)	Loss/tok 3.8692 (4.4795)	LR 4.000e-03
0: TRAIN [0][3850/5173]	Time 0.522 (0.623)	Data 1.13e-04 (2.97e-04)	Tok/s 10266 (22891)	Loss/tok 2.8911 (4.4775)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][3860/5173]	Time 0.644 (0.623)	Data 1.24e-04 (2.97e-04)	Tok/s 26009 (22902)	Loss/tok 3.6624 (4.4753)	LR 4.000e-03
0: TRAIN [0][3870/5173]	Time 0.581 (0.623)	Data 1.17e-04 (2.97e-04)	Tok/s 17830 (22893)	Loss/tok 3.4570 (4.4734)	LR 4.000e-03
0: TRAIN [0][3880/5173]	Time 0.778 (0.623)	Data 1.29e-04 (2.96e-04)	Tok/s 38153 (22885)	Loss/tok 4.1920 (4.4716)	LR 4.000e-03
0: TRAIN [0][3890/5173]	Time 0.583 (0.623)	Data 2.95e-04 (2.96e-04)	Tok/s 18107 (22876)	Loss/tok 3.5238 (4.4698)	LR 4.000e-03
0: TRAIN [0][3900/5173]	Time 0.707 (0.623)	Data 1.21e-04 (2.96e-04)	Tok/s 32606 (22885)	Loss/tok 3.9590 (4.4676)	LR 4.000e-03
0: TRAIN [0][3910/5173]	Time 0.643 (0.623)	Data 2.94e-04 (2.95e-04)	Tok/s 25964 (22884)	Loss/tok 3.5924 (4.4655)	LR 4.000e-03
0: TRAIN [0][3920/5173]	Time 0.643 (0.623)	Data 1.23e-04 (2.95e-04)	Tok/s 26088 (22887)	Loss/tok 3.7760 (4.4634)	LR 4.000e-03
0: TRAIN [0][3930/5173]	Time 0.708 (0.623)	Data 1.26e-04 (2.94e-04)	Tok/s 32898 (22888)	Loss/tok 3.8917 (4.4616)	LR 4.000e-03
0: TRAIN [0][3940/5173]	Time 0.582 (0.623)	Data 1.19e-04 (2.94e-04)	Tok/s 17942 (22877)	Loss/tok 3.5422 (4.4597)	LR 4.000e-03
0: TRAIN [0][3950/5173]	Time 0.708 (0.623)	Data 1.31e-04 (2.94e-04)	Tok/s 32941 (22893)	Loss/tok 3.8569 (4.4574)	LR 4.000e-03
0: TRAIN [0][3960/5173]	Time 0.521 (0.623)	Data 1.23e-04 (2.93e-04)	Tok/s 10026 (22898)	Loss/tok 2.9178 (4.4554)	LR 4.000e-03
0: TRAIN [0][3970/5173]	Time 0.707 (0.623)	Data 2.91e-04 (2.93e-04)	Tok/s 33451 (22899)	Loss/tok 3.7543 (4.4533)	LR 4.000e-03
0: TRAIN [0][3980/5173]	Time 0.639 (0.623)	Data 1.23e-04 (2.93e-04)	Tok/s 26522 (22896)	Loss/tok 3.6635 (4.4515)	LR 4.000e-03
0: TRAIN [0][3990/5173]	Time 0.709 (0.623)	Data 1.52e-04 (2.92e-04)	Tok/s 32636 (22891)	Loss/tok 3.9466 (4.4496)	LR 4.000e-03
0: TRAIN [0][4000/5173]	Time 0.584 (0.623)	Data 3.12e-04 (2.92e-04)	Tok/s 17699 (22878)	Loss/tok 3.3651 (4.4478)	LR 4.000e-03
0: TRAIN [0][4010/5173]	Time 0.581 (0.623)	Data 1.37e-04 (2.91e-04)	Tok/s 17725 (22870)	Loss/tok 3.3708 (4.4460)	LR 4.000e-03
0: TRAIN [0][4020/5173]	Time 0.582 (0.623)	Data 1.26e-04 (2.91e-04)	Tok/s 17851 (22867)	Loss/tok 3.3514 (4.4441)	LR 4.000e-03
0: TRAIN [0][4030/5173]	Time 0.581 (0.623)	Data 1.31e-04 (2.91e-04)	Tok/s 17851 (22857)	Loss/tok 3.5059 (4.4424)	LR 4.000e-03
0: TRAIN [0][4040/5173]	Time 0.581 (0.623)	Data 1.26e-04 (2.90e-04)	Tok/s 18171 (22862)	Loss/tok 3.4358 (4.4403)	LR 4.000e-03
0: TRAIN [0][4050/5173]	Time 0.644 (0.623)	Data 1.16e-04 (2.90e-04)	Tok/s 25896 (22867)	Loss/tok 3.6675 (4.4382)	LR 4.000e-03
0: TRAIN [0][4060/5173]	Time 0.641 (0.623)	Data 1.25e-04 (2.90e-04)	Tok/s 26121 (22867)	Loss/tok 3.7152 (4.4363)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][4070/5173]	Time 0.641 (0.623)	Data 1.25e-04 (2.89e-04)	Tok/s 25965 (22870)	Loss/tok 3.6272 (4.4345)	LR 4.000e-03
0: TRAIN [0][4080/5173]	Time 0.643 (0.623)	Data 1.24e-04 (2.89e-04)	Tok/s 25442 (22867)	Loss/tok 3.6614 (4.4328)	LR 4.000e-03
0: TRAIN [0][4090/5173]	Time 0.582 (0.623)	Data 1.21e-04 (2.88e-04)	Tok/s 17886 (22860)	Loss/tok 3.3270 (4.4309)	LR 4.000e-03
0: TRAIN [0][4100/5173]	Time 0.707 (0.623)	Data 1.20e-04 (2.88e-04)	Tok/s 32872 (22863)	Loss/tok 3.8950 (4.4290)	LR 4.000e-03
0: TRAIN [0][4110/5173]	Time 0.642 (0.623)	Data 1.26e-04 (2.88e-04)	Tok/s 25981 (22860)	Loss/tok 3.6673 (4.4271)	LR 4.000e-03
0: TRAIN [0][4120/5173]	Time 0.582 (0.623)	Data 3.08e-04 (2.87e-04)	Tok/s 17633 (22863)	Loss/tok 3.3542 (4.4251)	LR 4.000e-03
0: TRAIN [0][4130/5173]	Time 0.581 (0.623)	Data 1.21e-04 (2.87e-04)	Tok/s 17891 (22851)	Loss/tok 3.4162 (4.4234)	LR 4.000e-03
0: TRAIN [0][4140/5173]	Time 0.778 (0.623)	Data 1.22e-04 (2.87e-04)	Tok/s 38456 (22852)	Loss/tok 3.9225 (4.4217)	LR 4.000e-03
0: TRAIN [0][4150/5173]	Time 0.578 (0.623)	Data 1.19e-04 (2.86e-04)	Tok/s 17710 (22856)	Loss/tok 3.3766 (4.4199)	LR 4.000e-03
0: TRAIN [0][4160/5173]	Time 0.781 (0.623)	Data 1.24e-04 (2.86e-04)	Tok/s 37525 (22860)	Loss/tok 4.1829 (4.4181)	LR 4.000e-03
0: TRAIN [0][4170/5173]	Time 0.582 (0.623)	Data 1.35e-04 (2.86e-04)	Tok/s 17761 (22861)	Loss/tok 3.2859 (4.4163)	LR 4.000e-03
0: TRAIN [0][4180/5173]	Time 0.584 (0.623)	Data 1.20e-04 (2.85e-04)	Tok/s 17230 (22854)	Loss/tok 3.4073 (4.4145)	LR 4.000e-03
0: TRAIN [0][4190/5173]	Time 0.520 (0.623)	Data 1.16e-04 (2.85e-04)	Tok/s 10249 (22850)	Loss/tok 2.9217 (4.4127)	LR 4.000e-03
0: TRAIN [0][4200/5173]	Time 0.582 (0.623)	Data 1.26e-04 (2.84e-04)	Tok/s 17951 (22844)	Loss/tok 3.3979 (4.4110)	LR 4.000e-03
0: TRAIN [0][4210/5173]	Time 0.582 (0.623)	Data 1.16e-04 (2.84e-04)	Tok/s 17602 (22837)	Loss/tok 3.4751 (4.4093)	LR 4.000e-03
0: TRAIN [0][4220/5173]	Time 0.703 (0.623)	Data 1.20e-04 (2.84e-04)	Tok/s 33006 (22837)	Loss/tok 3.8495 (4.4076)	LR 4.000e-03
0: TRAIN [0][4230/5173]	Time 0.703 (0.623)	Data 1.29e-04 (2.83e-04)	Tok/s 33598 (22840)	Loss/tok 3.8808 (4.4057)	LR 4.000e-03
0: TRAIN [0][4240/5173]	Time 0.582 (0.623)	Data 1.23e-04 (2.83e-04)	Tok/s 17240 (22832)	Loss/tok 3.3945 (4.4041)	LR 4.000e-03
0: TRAIN [0][4250/5173]	Time 0.581 (0.623)	Data 1.74e-04 (2.83e-04)	Tok/s 17594 (22826)	Loss/tok 3.4936 (4.4024)	LR 4.000e-03
0: TRAIN [0][4260/5173]	Time 0.640 (0.623)	Data 1.24e-04 (2.82e-04)	Tok/s 26299 (22829)	Loss/tok 3.6472 (4.4005)	LR 4.000e-03
0: TRAIN [0][4270/5173]	Time 0.581 (0.623)	Data 1.49e-04 (2.82e-04)	Tok/s 18129 (22829)	Loss/tok 3.4480 (4.3987)	LR 4.000e-03
0: TRAIN [0][4280/5173]	Time 0.644 (0.623)	Data 1.20e-04 (2.82e-04)	Tok/s 25561 (22828)	Loss/tok 3.6452 (4.3969)	LR 4.000e-03
0: TRAIN [0][4290/5173]	Time 0.703 (0.623)	Data 1.18e-04 (2.81e-04)	Tok/s 32955 (22831)	Loss/tok 3.8546 (4.3952)	LR 4.000e-03
0: TRAIN [0][4300/5173]	Time 0.709 (0.623)	Data 1.19e-04 (2.81e-04)	Tok/s 32786 (22830)	Loss/tok 3.8183 (4.3935)	LR 4.000e-03
0: TRAIN [0][4310/5173]	Time 0.638 (0.623)	Data 1.16e-04 (2.81e-04)	Tok/s 26561 (22845)	Loss/tok 3.6313 (4.3916)	LR 4.000e-03
0: TRAIN [0][4320/5173]	Time 0.582 (0.623)	Data 1.17e-04 (2.80e-04)	Tok/s 17535 (22846)	Loss/tok 3.4922 (4.3898)	LR 2.000e-03
0: TRAIN [0][4330/5173]	Time 0.580 (0.623)	Data 1.15e-04 (2.80e-04)	Tok/s 18068 (22852)	Loss/tok 3.3915 (4.3879)	LR 2.000e-03
0: TRAIN [0][4340/5173]	Time 0.638 (0.623)	Data 1.15e-04 (2.80e-04)	Tok/s 26640 (22851)	Loss/tok 3.5698 (4.3861)	LR 2.000e-03
0: TRAIN [0][4350/5173]	Time 0.585 (0.623)	Data 1.13e-04 (2.79e-04)	Tok/s 17629 (22849)	Loss/tok 3.4275 (4.3844)	LR 2.000e-03
0: TRAIN [0][4360/5173]	Time 0.643 (0.623)	Data 1.34e-04 (2.79e-04)	Tok/s 26274 (22863)	Loss/tok 3.7123 (4.3824)	LR 2.000e-03
0: TRAIN [0][4370/5173]	Time 0.584 (0.623)	Data 1.21e-04 (2.78e-04)	Tok/s 17715 (22862)	Loss/tok 3.3610 (4.3806)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][4380/5173]	Time 0.706 (0.623)	Data 1.09e-04 (2.78e-04)	Tok/s 32879 (22865)	Loss/tok 3.8056 (4.3789)	LR 2.000e-03
0: TRAIN [0][4390/5173]	Time 0.644 (0.623)	Data 1.18e-04 (2.78e-04)	Tok/s 25948 (22862)	Loss/tok 3.4340 (4.3771)	LR 2.000e-03
0: TRAIN [0][4400/5173]	Time 0.521 (0.623)	Data 1.18e-04 (2.78e-04)	Tok/s 10207 (22865)	Loss/tok 2.8689 (4.3753)	LR 2.000e-03
0: TRAIN [0][4410/5173]	Time 0.778 (0.623)	Data 1.21e-04 (2.77e-04)	Tok/s 38031 (22875)	Loss/tok 3.9548 (4.3734)	LR 2.000e-03
0: TRAIN [0][4420/5173]	Time 0.585 (0.623)	Data 1.22e-04 (2.77e-04)	Tok/s 17699 (22885)	Loss/tok 3.4388 (4.3715)	LR 2.000e-03
0: TRAIN [0][4430/5173]	Time 0.585 (0.623)	Data 2.96e-04 (2.77e-04)	Tok/s 17573 (22881)	Loss/tok 3.3140 (4.3698)	LR 2.000e-03
0: TRAIN [0][4440/5173]	Time 0.583 (0.623)	Data 1.13e-04 (2.76e-04)	Tok/s 17580 (22876)	Loss/tok 3.3913 (4.3682)	LR 2.000e-03
0: TRAIN [0][4450/5173]	Time 0.521 (0.623)	Data 1.17e-04 (2.76e-04)	Tok/s 10124 (22872)	Loss/tok 2.8510 (4.3666)	LR 2.000e-03
0: TRAIN [0][4460/5173]	Time 0.780 (0.623)	Data 1.18e-04 (2.76e-04)	Tok/s 37997 (22873)	Loss/tok 4.0847 (4.3650)	LR 2.000e-03
0: TRAIN [0][4470/5173]	Time 0.522 (0.623)	Data 1.11e-04 (2.75e-04)	Tok/s 10279 (22866)	Loss/tok 2.8638 (4.3634)	LR 2.000e-03
0: TRAIN [0][4480/5173]	Time 0.580 (0.623)	Data 1.14e-04 (2.75e-04)	Tok/s 18054 (22864)	Loss/tok 3.3386 (4.3617)	LR 2.000e-03
0: TRAIN [0][4490/5173]	Time 0.643 (0.623)	Data 1.19e-04 (2.75e-04)	Tok/s 26541 (22859)	Loss/tok 3.5593 (4.3600)	LR 2.000e-03
0: TRAIN [0][4500/5173]	Time 0.707 (0.623)	Data 1.24e-04 (2.74e-04)	Tok/s 32623 (22866)	Loss/tok 3.7884 (4.3582)	LR 2.000e-03
0: TRAIN [0][4510/5173]	Time 0.521 (0.623)	Data 1.14e-04 (2.74e-04)	Tok/s 10006 (22854)	Loss/tok 2.8106 (4.3567)	LR 2.000e-03
0: TRAIN [0][4520/5173]	Time 0.648 (0.623)	Data 1.19e-04 (2.74e-04)	Tok/s 26123 (22850)	Loss/tok 3.5492 (4.3550)	LR 2.000e-03
0: TRAIN [0][4530/5173]	Time 0.643 (0.623)	Data 2.87e-04 (2.73e-04)	Tok/s 25856 (22853)	Loss/tok 3.7341 (4.3531)	LR 2.000e-03
0: TRAIN [0][4540/5173]	Time 0.707 (0.623)	Data 1.23e-04 (2.73e-04)	Tok/s 33003 (22853)	Loss/tok 3.8217 (4.3514)	LR 2.000e-03
0: TRAIN [0][4550/5173]	Time 0.581 (0.623)	Data 1.16e-04 (2.73e-04)	Tok/s 17397 (22839)	Loss/tok 3.4726 (4.3501)	LR 2.000e-03
0: TRAIN [0][4560/5173]	Time 0.582 (0.623)	Data 1.12e-04 (2.73e-04)	Tok/s 17668 (22842)	Loss/tok 3.3973 (4.3482)	LR 2.000e-03
0: TRAIN [0][4570/5173]	Time 0.707 (0.623)	Data 1.08e-04 (2.72e-04)	Tok/s 32919 (22839)	Loss/tok 3.6527 (4.3466)	LR 2.000e-03
0: TRAIN [0][4580/5173]	Time 0.583 (0.623)	Data 1.17e-04 (2.72e-04)	Tok/s 17923 (22840)	Loss/tok 3.3316 (4.3448)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][4590/5173]	Time 0.584 (0.623)	Data 1.15e-04 (2.72e-04)	Tok/s 17240 (22838)	Loss/tok 3.3243 (4.3433)	LR 2.000e-03
0: TRAIN [0][4600/5173]	Time 0.581 (0.623)	Data 1.11e-04 (2.71e-04)	Tok/s 17766 (22834)	Loss/tok 3.4892 (4.3417)	LR 2.000e-03
0: TRAIN [0][4610/5173]	Time 0.583 (0.623)	Data 1.20e-04 (2.71e-04)	Tok/s 17705 (22834)	Loss/tok 3.3532 (4.3400)	LR 2.000e-03
0: TRAIN [0][4620/5173]	Time 0.647 (0.623)	Data 1.14e-04 (2.71e-04)	Tok/s 26141 (22833)	Loss/tok 3.6009 (4.3383)	LR 2.000e-03
0: TRAIN [0][4630/5173]	Time 0.583 (0.623)	Data 1.08e-04 (2.70e-04)	Tok/s 17695 (22834)	Loss/tok 3.2752 (4.3367)	LR 2.000e-03
0: TRAIN [0][4640/5173]	Time 0.579 (0.623)	Data 1.18e-04 (2.70e-04)	Tok/s 17806 (22828)	Loss/tok 3.3072 (4.3351)	LR 2.000e-03
0: TRAIN [0][4650/5173]	Time 0.582 (0.623)	Data 1.17e-04 (2.70e-04)	Tok/s 17795 (22825)	Loss/tok 3.2569 (4.3335)	LR 2.000e-03
0: TRAIN [0][4660/5173]	Time 0.708 (0.623)	Data 1.13e-04 (2.69e-04)	Tok/s 32825 (22823)	Loss/tok 3.8564 (4.3319)	LR 2.000e-03
0: TRAIN [0][4670/5173]	Time 0.645 (0.623)	Data 2.85e-04 (2.69e-04)	Tok/s 26268 (22828)	Loss/tok 3.5048 (4.3301)	LR 2.000e-03
0: TRAIN [0][4680/5173]	Time 0.640 (0.623)	Data 1.13e-04 (2.69e-04)	Tok/s 26369 (22830)	Loss/tok 3.4871 (4.3285)	LR 2.000e-03
0: TRAIN [0][4690/5173]	Time 0.647 (0.623)	Data 1.13e-04 (2.68e-04)	Tok/s 26069 (22832)	Loss/tok 3.6070 (4.3268)	LR 2.000e-03
0: TRAIN [0][4700/5173]	Time 0.584 (0.623)	Data 2.67e-04 (2.68e-04)	Tok/s 17706 (22828)	Loss/tok 3.4335 (4.3253)	LR 2.000e-03
0: TRAIN [0][4710/5173]	Time 0.581 (0.623)	Data 1.34e-04 (2.68e-04)	Tok/s 17785 (22825)	Loss/tok 3.3507 (4.3237)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][4720/5173]	Time 0.641 (0.623)	Data 1.13e-04 (2.68e-04)	Tok/s 26175 (22821)	Loss/tok 3.5877 (4.3222)	LR 2.000e-03
0: TRAIN [0][4730/5173]	Time 0.647 (0.623)	Data 1.15e-04 (2.67e-04)	Tok/s 25579 (22821)	Loss/tok 3.6028 (4.3205)	LR 2.000e-03
0: TRAIN [0][4740/5173]	Time 0.581 (0.623)	Data 1.18e-04 (2.67e-04)	Tok/s 17784 (22818)	Loss/tok 3.3758 (4.3190)	LR 2.000e-03
0: TRAIN [0][4750/5173]	Time 0.583 (0.623)	Data 1.18e-04 (2.67e-04)	Tok/s 17503 (22819)	Loss/tok 3.3026 (4.3174)	LR 2.000e-03
0: TRAIN [0][4760/5173]	Time 0.580 (0.623)	Data 1.11e-04 (2.66e-04)	Tok/s 18140 (22814)	Loss/tok 3.3691 (4.3159)	LR 2.000e-03
0: TRAIN [0][4770/5173]	Time 0.645 (0.622)	Data 1.26e-04 (2.66e-04)	Tok/s 26387 (22810)	Loss/tok 3.5868 (4.3143)	LR 2.000e-03
0: TRAIN [0][4780/5173]	Time 0.585 (0.622)	Data 1.46e-04 (2.66e-04)	Tok/s 17969 (22811)	Loss/tok 3.3553 (4.3127)	LR 2.000e-03
0: TRAIN [0][4790/5173]	Time 0.521 (0.622)	Data 1.16e-04 (2.66e-04)	Tok/s 10005 (22801)	Loss/tok 2.8494 (4.3113)	LR 2.000e-03
0: TRAIN [0][4800/5173]	Time 0.582 (0.622)	Data 1.19e-04 (2.65e-04)	Tok/s 17765 (22799)	Loss/tok 3.3855 (4.3097)	LR 2.000e-03
0: TRAIN [0][4810/5173]	Time 0.642 (0.622)	Data 1.16e-04 (2.65e-04)	Tok/s 26214 (22793)	Loss/tok 3.5763 (4.3082)	LR 2.000e-03
0: TRAIN [0][4820/5173]	Time 0.706 (0.622)	Data 1.13e-04 (2.65e-04)	Tok/s 32450 (22783)	Loss/tok 3.7707 (4.3068)	LR 2.000e-03
0: TRAIN [0][4830/5173]	Time 0.642 (0.622)	Data 1.25e-04 (2.65e-04)	Tok/s 25911 (22788)	Loss/tok 3.5241 (4.3052)	LR 2.000e-03
0: TRAIN [0][4840/5173]	Time 0.644 (0.622)	Data 1.21e-04 (2.64e-04)	Tok/s 26312 (22785)	Loss/tok 3.5936 (4.3036)	LR 2.000e-03
0: TRAIN [0][4850/5173]	Time 0.579 (0.622)	Data 1.16e-04 (2.64e-04)	Tok/s 18030 (22787)	Loss/tok 3.3175 (4.3019)	LR 2.000e-03
0: TRAIN [0][4860/5173]	Time 0.583 (0.622)	Data 1.19e-04 (2.64e-04)	Tok/s 17406 (22789)	Loss/tok 3.3387 (4.3005)	LR 2.000e-03
0: TRAIN [0][4870/5173]	Time 0.781 (0.622)	Data 1.22e-04 (2.63e-04)	Tok/s 38383 (22783)	Loss/tok 3.9675 (4.2991)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][4880/5173]	Time 0.580 (0.622)	Data 1.13e-04 (2.63e-04)	Tok/s 17849 (22787)	Loss/tok 3.1894 (4.2976)	LR 2.000e-03
0: TRAIN [0][4890/5173]	Time 0.583 (0.622)	Data 1.18e-04 (2.63e-04)	Tok/s 17701 (22789)	Loss/tok 3.2218 (4.2960)	LR 2.000e-03
0: TRAIN [0][4900/5173]	Time 0.646 (0.622)	Data 1.20e-04 (2.63e-04)	Tok/s 26209 (22787)	Loss/tok 3.5522 (4.2944)	LR 2.000e-03
0: TRAIN [0][4910/5173]	Time 0.584 (0.622)	Data 1.17e-04 (2.62e-04)	Tok/s 17283 (22791)	Loss/tok 3.3708 (4.2928)	LR 2.000e-03
0: TRAIN [0][4920/5173]	Time 0.582 (0.622)	Data 1.08e-04 (2.62e-04)	Tok/s 18136 (22792)	Loss/tok 3.2262 (4.2914)	LR 2.000e-03
0: TRAIN [0][4930/5173]	Time 0.583 (0.622)	Data 1.18e-04 (2.62e-04)	Tok/s 17744 (22782)	Loss/tok 3.2241 (4.2900)	LR 2.000e-03
0: TRAIN [0][4940/5173]	Time 0.706 (0.622)	Data 1.15e-04 (2.61e-04)	Tok/s 33082 (22781)	Loss/tok 3.7114 (4.2885)	LR 2.000e-03
0: TRAIN [0][4950/5173]	Time 0.640 (0.622)	Data 1.20e-04 (2.61e-04)	Tok/s 26413 (22783)	Loss/tok 3.4930 (4.2870)	LR 2.000e-03
0: TRAIN [0][4960/5173]	Time 0.650 (0.622)	Data 1.34e-04 (2.61e-04)	Tok/s 26131 (22785)	Loss/tok 3.5658 (4.2855)	LR 2.000e-03
0: TRAIN [0][4970/5173]	Time 0.581 (0.622)	Data 1.22e-04 (2.61e-04)	Tok/s 17630 (22783)	Loss/tok 3.3613 (4.2841)	LR 2.000e-03
0: TRAIN [0][4980/5173]	Time 0.582 (0.622)	Data 1.20e-04 (2.61e-04)	Tok/s 17612 (22781)	Loss/tok 3.3125 (4.2826)	LR 2.000e-03
0: TRAIN [0][4990/5173]	Time 0.637 (0.622)	Data 1.32e-04 (2.60e-04)	Tok/s 26162 (22776)	Loss/tok 3.4707 (4.2812)	LR 2.000e-03
0: TRAIN [0][5000/5173]	Time 0.520 (0.622)	Data 1.24e-04 (2.60e-04)	Tok/s 10105 (22774)	Loss/tok 2.9075 (4.2798)	LR 2.000e-03
0: TRAIN [0][5010/5173]	Time 0.581 (0.622)	Data 1.26e-04 (2.60e-04)	Tok/s 17819 (22770)	Loss/tok 3.3324 (4.2783)	LR 2.000e-03
0: TRAIN [0][5020/5173]	Time 0.649 (0.622)	Data 1.26e-04 (2.60e-04)	Tok/s 25832 (22772)	Loss/tok 3.5540 (4.2766)	LR 2.000e-03
0: TRAIN [0][5030/5173]	Time 0.652 (0.622)	Data 1.25e-04 (2.59e-04)	Tok/s 26038 (22773)	Loss/tok 3.4872 (4.2752)	LR 2.000e-03
0: TRAIN [0][5040/5173]	Time 0.520 (0.622)	Data 1.29e-04 (2.59e-04)	Tok/s 10159 (22768)	Loss/tok 2.8109 (4.2737)	LR 2.000e-03
0: TRAIN [0][5050/5173]	Time 0.580 (0.622)	Data 1.66e-04 (2.59e-04)	Tok/s 17575 (22768)	Loss/tok 3.2768 (4.2722)	LR 2.000e-03
0: TRAIN [0][5060/5173]	Time 0.651 (0.622)	Data 1.24e-04 (2.59e-04)	Tok/s 25988 (22766)	Loss/tok 3.5364 (4.2707)	LR 2.000e-03
0: TRAIN [0][5070/5173]	Time 0.585 (0.622)	Data 1.22e-04 (2.59e-04)	Tok/s 17704 (22767)	Loss/tok 3.3032 (4.2692)	LR 2.000e-03
0: TRAIN [0][5080/5173]	Time 0.518 (0.622)	Data 1.26e-04 (2.58e-04)	Tok/s 10367 (22755)	Loss/tok 2.9523 (4.2681)	LR 2.000e-03
0: TRAIN [0][5090/5173]	Time 0.583 (0.622)	Data 1.25e-04 (2.58e-04)	Tok/s 17273 (22761)	Loss/tok 3.3649 (4.2665)	LR 2.000e-03
0: TRAIN [0][5100/5173]	Time 0.581 (0.622)	Data 1.20e-04 (2.58e-04)	Tok/s 17474 (22755)	Loss/tok 3.3588 (4.2651)	LR 2.000e-03
0: TRAIN [0][5110/5173]	Time 0.648 (0.622)	Data 1.31e-04 (2.58e-04)	Tok/s 25973 (22754)	Loss/tok 3.4734 (4.2636)	LR 2.000e-03
0: TRAIN [0][5120/5173]	Time 0.581 (0.622)	Data 2.78e-04 (2.58e-04)	Tok/s 17532 (22742)	Loss/tok 3.4166 (4.2624)	LR 2.000e-03
0: TRAIN [0][5130/5173]	Time 0.580 (0.622)	Data 1.28e-04 (2.57e-04)	Tok/s 17892 (22747)	Loss/tok 3.3075 (4.2610)	LR 2.000e-03
0: TRAIN [0][5140/5173]	Time 0.646 (0.622)	Data 1.19e-04 (2.57e-04)	Tok/s 26090 (22749)	Loss/tok 3.5952 (4.2595)	LR 1.000e-03
0: TRAIN [0][5150/5173]	Time 0.646 (0.622)	Data 1.26e-04 (2.57e-04)	Tok/s 26128 (22744)	Loss/tok 3.5962 (4.2581)	LR 1.000e-03
0: TRAIN [0][5160/5173]	Time 0.520 (0.622)	Data 1.21e-04 (2.57e-04)	Tok/s 10020 (22745)	Loss/tok 2.8296 (4.2567)	LR 1.000e-03
0: TRAIN [0][5170/5173]	Time 0.580 (0.622)	Data 1.21e-04 (2.57e-04)	Tok/s 17900 (22743)	Loss/tok 3.1633 (4.2552)	LR 1.000e-03
:::MLL 1586121092.080 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 525}}
:::MLL 1586121092.081 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [0][0/8]	Time 0.730 (0.730)	Decoder iters 149.0 (149.0)	Tok/s 22342 (22342)
0: Running moses detokenizer
0: BLEU(score=20.032280568150423, counts=[34236, 15700, 8376, 4644], totals=[63904, 60901, 57899, 54901], precisions=[53.57411116675013, 25.779543849854683, 14.466571097946424, 8.458862315804812], bp=0.9879920568111682, sys_len=63904, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1586121098.523 eval_accuracy: {"value": 20.03, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 536}}
:::MLL 1586121098.524 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 0	Training Loss: 4.2554	Test BLEU: 20.03
0: Performance: Epoch: 0	Training: 68230 Tok/s
0: Finished epoch 0
:::MLL 1586121098.525 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 558}}
:::MLL 1586121098.525 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1586121098.526 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 515}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 1018588959
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][0/5173]	Time 1.216 (1.216)	Data 5.64e-01 (5.64e-01)	Tok/s 14046 (14046)	Loss/tok 3.4982 (3.4982)	LR 1.000e-03
0: TRAIN [1][10/5173]	Time 0.520 (0.640)	Data 1.13e-04 (5.15e-02)	Tok/s 10051 (17525)	Loss/tok 2.8056 (3.3002)	LR 1.000e-03
0: TRAIN [1][20/5173]	Time 0.580 (0.625)	Data 1.16e-04 (2.70e-02)	Tok/s 17797 (19119)	Loss/tok 3.0960 (3.3333)	LR 1.000e-03
0: TRAIN [1][30/5173]	Time 0.702 (0.615)	Data 1.17e-04 (1.83e-02)	Tok/s 33054 (19206)	Loss/tok 3.6401 (3.3356)	LR 1.000e-03
0: TRAIN [1][40/5173]	Time 0.644 (0.620)	Data 1.14e-04 (1.39e-02)	Tok/s 26371 (20329)	Loss/tok 3.3778 (3.3750)	LR 1.000e-03
0: TRAIN [1][50/5173]	Time 0.644 (0.616)	Data 1.14e-04 (1.12e-02)	Tok/s 25945 (20315)	Loss/tok 3.3724 (3.3694)	LR 1.000e-03
0: TRAIN [1][60/5173]	Time 0.583 (0.615)	Data 1.14e-04 (9.39e-03)	Tok/s 17628 (20566)	Loss/tok 3.1884 (3.3734)	LR 1.000e-03
0: TRAIN [1][70/5173]	Time 0.646 (0.615)	Data 1.14e-04 (8.10e-03)	Tok/s 26337 (20758)	Loss/tok 3.4576 (3.3713)	LR 1.000e-03
0: TRAIN [1][80/5173]	Time 0.642 (0.612)	Data 1.09e-04 (7.11e-03)	Tok/s 25994 (20463)	Loss/tok 3.3814 (3.3683)	LR 1.000e-03
0: TRAIN [1][90/5173]	Time 0.706 (0.616)	Data 1.24e-04 (6.34e-03)	Tok/s 33172 (21095)	Loss/tok 3.6829 (3.3951)	LR 1.000e-03
0: TRAIN [1][100/5173]	Time 0.585 (0.621)	Data 1.26e-04 (5.73e-03)	Tok/s 17231 (21713)	Loss/tok 3.2687 (3.4229)	LR 1.000e-03
0: TRAIN [1][110/5173]	Time 0.648 (0.620)	Data 1.16e-04 (5.23e-03)	Tok/s 25910 (21706)	Loss/tok 3.4182 (3.4217)	LR 1.000e-03
0: TRAIN [1][120/5173]	Time 0.580 (0.623)	Data 1.23e-04 (4.81e-03)	Tok/s 17778 (22059)	Loss/tok 3.3814 (3.4322)	LR 1.000e-03
0: TRAIN [1][130/5173]	Time 0.521 (0.621)	Data 1.10e-04 (4.45e-03)	Tok/s 10105 (21917)	Loss/tok 2.7836 (3.4317)	LR 1.000e-03
0: TRAIN [1][140/5173]	Time 0.583 (0.621)	Data 1.18e-04 (4.14e-03)	Tok/s 17261 (21959)	Loss/tok 3.2739 (3.4310)	LR 1.000e-03
0: TRAIN [1][150/5173]	Time 0.643 (0.624)	Data 2.72e-04 (3.88e-03)	Tok/s 26324 (22382)	Loss/tok 3.4091 (3.4411)	LR 1.000e-03
0: TRAIN [1][160/5173]	Time 0.705 (0.624)	Data 2.96e-04 (3.65e-03)	Tok/s 32777 (22434)	Loss/tok 3.7717 (3.4417)	LR 1.000e-03
0: TRAIN [1][170/5173]	Time 0.584 (0.624)	Data 1.20e-04 (3.44e-03)	Tok/s 17570 (22495)	Loss/tok 3.0717 (3.4413)	LR 1.000e-03
0: TRAIN [1][180/5173]	Time 0.582 (0.622)	Data 1.17e-04 (3.26e-03)	Tok/s 17946 (22286)	Loss/tok 3.2437 (3.4349)	LR 1.000e-03
0: TRAIN [1][190/5173]	Time 0.640 (0.624)	Data 1.13e-04 (3.09e-03)	Tok/s 26026 (22467)	Loss/tok 3.5381 (3.4379)	LR 1.000e-03
0: TRAIN [1][200/5173]	Time 0.520 (0.623)	Data 1.16e-04 (2.95e-03)	Tok/s 10003 (22437)	Loss/tok 2.8539 (3.4348)	LR 1.000e-03
0: TRAIN [1][210/5173]	Time 0.649 (0.622)	Data 1.18e-04 (2.81e-03)	Tok/s 26160 (22290)	Loss/tok 3.5216 (3.4310)	LR 1.000e-03
0: TRAIN [1][220/5173]	Time 0.584 (0.622)	Data 1.19e-04 (2.69e-03)	Tok/s 17457 (22369)	Loss/tok 3.1923 (3.4342)	LR 1.000e-03
0: TRAIN [1][230/5173]	Time 0.585 (0.623)	Data 1.13e-04 (2.58e-03)	Tok/s 17650 (22479)	Loss/tok 3.2215 (3.4352)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][240/5173]	Time 0.579 (0.623)	Data 1.20e-04 (2.48e-03)	Tok/s 17896 (22524)	Loss/tok 3.2950 (3.4364)	LR 1.000e-03
0: TRAIN [1][250/5173]	Time 0.639 (0.625)	Data 1.28e-04 (2.39e-03)	Tok/s 26640 (22732)	Loss/tok 3.4385 (3.4420)	LR 1.000e-03
0: TRAIN [1][260/5173]	Time 0.649 (0.625)	Data 1.15e-04 (2.30e-03)	Tok/s 25855 (22751)	Loss/tok 3.4509 (3.4426)	LR 1.000e-03
0: TRAIN [1][270/5173]	Time 0.582 (0.625)	Data 1.24e-04 (2.22e-03)	Tok/s 18581 (22734)	Loss/tok 3.1171 (3.4432)	LR 1.000e-03
0: TRAIN [1][280/5173]	Time 0.583 (0.624)	Data 1.16e-04 (2.14e-03)	Tok/s 17736 (22702)	Loss/tok 3.1710 (3.4421)	LR 1.000e-03
0: TRAIN [1][290/5173]	Time 0.582 (0.625)	Data 1.25e-04 (2.08e-03)	Tok/s 17870 (22791)	Loss/tok 3.2274 (3.4458)	LR 1.000e-03
0: TRAIN [1][300/5173]	Time 0.581 (0.624)	Data 1.21e-04 (2.01e-03)	Tok/s 17453 (22717)	Loss/tok 3.2319 (3.4440)	LR 1.000e-03
0: TRAIN [1][310/5173]	Time 0.582 (0.625)	Data 1.33e-04 (1.95e-03)	Tok/s 17780 (22756)	Loss/tok 3.1874 (3.4487)	LR 1.000e-03
0: TRAIN [1][320/5173]	Time 0.646 (0.625)	Data 1.26e-04 (1.90e-03)	Tok/s 25847 (22786)	Loss/tok 3.4403 (3.4489)	LR 1.000e-03
0: TRAIN [1][330/5173]	Time 0.584 (0.625)	Data 1.20e-04 (1.84e-03)	Tok/s 17885 (22780)	Loss/tok 3.2811 (3.4478)	LR 1.000e-03
0: TRAIN [1][340/5173]	Time 0.650 (0.625)	Data 1.24e-04 (1.79e-03)	Tok/s 25644 (22829)	Loss/tok 3.5098 (3.4489)	LR 1.000e-03
0: TRAIN [1][350/5173]	Time 0.581 (0.625)	Data 1.26e-04 (1.74e-03)	Tok/s 17557 (22778)	Loss/tok 3.2155 (3.4465)	LR 1.000e-03
0: TRAIN [1][360/5173]	Time 0.582 (0.624)	Data 1.28e-04 (1.70e-03)	Tok/s 17754 (22662)	Loss/tok 3.1648 (3.4425)	LR 1.000e-03
0: TRAIN [1][370/5173]	Time 0.645 (0.624)	Data 3.00e-04 (1.66e-03)	Tok/s 25906 (22743)	Loss/tok 3.3068 (3.4428)	LR 1.000e-03
0: TRAIN [1][380/5173]	Time 0.583 (0.625)	Data 1.20e-04 (1.62e-03)	Tok/s 17976 (22788)	Loss/tok 3.3229 (3.4452)	LR 1.000e-03
0: TRAIN [1][390/5173]	Time 0.706 (0.625)	Data 1.26e-04 (1.58e-03)	Tok/s 32902 (22762)	Loss/tok 3.5981 (3.4430)	LR 1.000e-03
0: TRAIN [1][400/5173]	Time 0.646 (0.624)	Data 1.17e-04 (1.54e-03)	Tok/s 25991 (22731)	Loss/tok 3.4296 (3.4425)	LR 1.000e-03
0: TRAIN [1][410/5173]	Time 0.708 (0.624)	Data 1.24e-04 (1.51e-03)	Tok/s 32690 (22720)	Loss/tok 3.6811 (3.4422)	LR 1.000e-03
0: TRAIN [1][420/5173]	Time 0.582 (0.624)	Data 1.56e-04 (1.48e-03)	Tok/s 17749 (22748)	Loss/tok 3.2501 (3.4427)	LR 1.000e-03
0: TRAIN [1][430/5173]	Time 0.702 (0.625)	Data 1.48e-04 (1.44e-03)	Tok/s 33465 (22764)	Loss/tok 3.5778 (3.4442)	LR 1.000e-03
0: TRAIN [1][440/5173]	Time 0.647 (0.624)	Data 1.44e-04 (1.41e-03)	Tok/s 25932 (22726)	Loss/tok 3.3978 (3.4424)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][450/5173]	Time 0.520 (0.624)	Data 1.24e-04 (1.39e-03)	Tok/s 10330 (22699)	Loss/tok 2.7806 (3.4411)	LR 1.000e-03
0: TRAIN [1][460/5173]	Time 0.585 (0.624)	Data 1.18e-04 (1.36e-03)	Tok/s 17248 (22645)	Loss/tok 3.2095 (3.4387)	LR 1.000e-03
0: TRAIN [1][470/5173]	Time 0.777 (0.623)	Data 2.82e-04 (1.33e-03)	Tok/s 37967 (22636)	Loss/tok 3.8422 (3.4389)	LR 1.000e-03
0: TRAIN [1][480/5173]	Time 0.645 (0.624)	Data 1.24e-04 (1.31e-03)	Tok/s 26271 (22675)	Loss/tok 3.3787 (3.4395)	LR 1.000e-03
0: TRAIN [1][490/5173]	Time 0.581 (0.624)	Data 1.23e-04 (1.29e-03)	Tok/s 17666 (22708)	Loss/tok 3.1794 (3.4412)	LR 1.000e-03
0: TRAIN [1][500/5173]	Time 0.643 (0.624)	Data 2.75e-04 (1.26e-03)	Tok/s 26263 (22691)	Loss/tok 3.3724 (3.4392)	LR 1.000e-03
0: TRAIN [1][510/5173]	Time 0.642 (0.625)	Data 1.16e-04 (1.24e-03)	Tok/s 25804 (22817)	Loss/tok 3.3989 (3.4427)	LR 1.000e-03
0: TRAIN [1][520/5173]	Time 0.651 (0.625)	Data 1.24e-04 (1.22e-03)	Tok/s 26037 (22829)	Loss/tok 3.3619 (3.4434)	LR 1.000e-03
0: TRAIN [1][530/5173]	Time 0.519 (0.625)	Data 1.31e-04 (1.20e-03)	Tok/s 10283 (22851)	Loss/tok 2.7744 (3.4459)	LR 1.000e-03
0: TRAIN [1][540/5173]	Time 0.645 (0.626)	Data 1.29e-04 (1.18e-03)	Tok/s 26182 (22887)	Loss/tok 3.4283 (3.4461)	LR 1.000e-03
0: TRAIN [1][550/5173]	Time 0.585 (0.625)	Data 1.20e-04 (1.16e-03)	Tok/s 17971 (22883)	Loss/tok 3.2408 (3.4450)	LR 1.000e-03
0: TRAIN [1][560/5173]	Time 0.583 (0.625)	Data 1.25e-04 (1.14e-03)	Tok/s 17738 (22849)	Loss/tok 3.2836 (3.4431)	LR 1.000e-03
0: TRAIN [1][570/5173]	Time 0.646 (0.625)	Data 1.19e-04 (1.13e-03)	Tok/s 25803 (22814)	Loss/tok 3.4682 (3.4419)	LR 1.000e-03
0: TRAIN [1][580/5173]	Time 0.645 (0.624)	Data 1.26e-04 (1.11e-03)	Tok/s 25672 (22770)	Loss/tok 3.4460 (3.4400)	LR 1.000e-03
0: TRAIN [1][590/5173]	Time 0.584 (0.624)	Data 1.25e-04 (1.09e-03)	Tok/s 17867 (22781)	Loss/tok 3.1863 (3.4394)	LR 1.000e-03
0: TRAIN [1][600/5173]	Time 0.704 (0.624)	Data 1.25e-04 (1.08e-03)	Tok/s 33134 (22789)	Loss/tok 3.5393 (3.4399)	LR 1.000e-03
0: TRAIN [1][610/5173]	Time 0.646 (0.624)	Data 1.24e-04 (1.06e-03)	Tok/s 25940 (22761)	Loss/tok 3.5145 (3.4384)	LR 1.000e-03
0: TRAIN [1][620/5173]	Time 0.581 (0.624)	Data 1.17e-04 (1.05e-03)	Tok/s 17797 (22719)	Loss/tok 3.2084 (3.4372)	LR 1.000e-03
0: TRAIN [1][630/5173]	Time 0.584 (0.624)	Data 1.23e-04 (1.03e-03)	Tok/s 17504 (22762)	Loss/tok 3.2229 (3.4379)	LR 1.000e-03
0: TRAIN [1][640/5173]	Time 0.584 (0.624)	Data 1.16e-04 (1.02e-03)	Tok/s 17811 (22747)	Loss/tok 3.2037 (3.4372)	LR 1.000e-03
0: TRAIN [1][650/5173]	Time 0.648 (0.624)	Data 1.16e-04 (1.00e-03)	Tok/s 25990 (22719)	Loss/tok 3.3687 (3.4359)	LR 1.000e-03
0: TRAIN [1][660/5173]	Time 0.523 (0.624)	Data 1.25e-04 (9.92e-04)	Tok/s 10025 (22718)	Loss/tok 2.7857 (3.4355)	LR 1.000e-03
0: TRAIN [1][670/5173]	Time 0.649 (0.624)	Data 1.21e-04 (9.79e-04)	Tok/s 25883 (22705)	Loss/tok 3.5440 (3.4346)	LR 1.000e-03
0: TRAIN [1][680/5173]	Time 0.649 (0.623)	Data 1.27e-04 (9.66e-04)	Tok/s 25768 (22684)	Loss/tok 3.5460 (3.4334)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][690/5173]	Time 0.778 (0.623)	Data 1.51e-04 (9.54e-04)	Tok/s 38602 (22689)	Loss/tok 3.6275 (3.4338)	LR 1.000e-03
0: TRAIN [1][700/5173]	Time 0.581 (0.623)	Data 1.59e-04 (9.43e-04)	Tok/s 17830 (22700)	Loss/tok 3.1825 (3.4354)	LR 1.000e-03
0: TRAIN [1][710/5173]	Time 0.649 (0.623)	Data 1.28e-04 (9.31e-04)	Tok/s 25945 (22699)	Loss/tok 3.4726 (3.4351)	LR 1.000e-03
0: TRAIN [1][720/5173]	Time 0.642 (0.623)	Data 1.26e-04 (9.20e-04)	Tok/s 25971 (22690)	Loss/tok 3.4138 (3.4337)	LR 1.000e-03
0: TRAIN [1][730/5173]	Time 0.645 (0.623)	Data 1.25e-04 (9.09e-04)	Tok/s 26071 (22679)	Loss/tok 3.4538 (3.4323)	LR 1.000e-03
0: TRAIN [1][740/5173]	Time 0.580 (0.623)	Data 1.20e-04 (8.98e-04)	Tok/s 17808 (22708)	Loss/tok 3.2132 (3.4323)	LR 1.000e-03
0: TRAIN [1][750/5173]	Time 0.647 (0.623)	Data 1.20e-04 (8.88e-04)	Tok/s 26137 (22707)	Loss/tok 3.4167 (3.4318)	LR 1.000e-03
0: TRAIN [1][760/5173]	Time 0.644 (0.623)	Data 1.22e-04 (8.78e-04)	Tok/s 26117 (22718)	Loss/tok 3.5505 (3.4308)	LR 1.000e-03
0: TRAIN [1][770/5173]	Time 0.649 (0.623)	Data 1.21e-04 (8.69e-04)	Tok/s 25943 (22736)	Loss/tok 3.4040 (3.4306)	LR 1.000e-03
0: TRAIN [1][780/5173]	Time 0.639 (0.624)	Data 1.14e-04 (8.59e-04)	Tok/s 26046 (22754)	Loss/tok 3.4018 (3.4306)	LR 5.000e-04
0: TRAIN [1][790/5173]	Time 0.647 (0.623)	Data 1.23e-04 (8.50e-04)	Tok/s 25628 (22754)	Loss/tok 3.4267 (3.4298)	LR 5.000e-04
0: TRAIN [1][800/5173]	Time 0.582 (0.624)	Data 1.19e-04 (8.41e-04)	Tok/s 17667 (22755)	Loss/tok 3.1011 (3.4298)	LR 5.000e-04
0: TRAIN [1][810/5173]	Time 0.581 (0.623)	Data 1.17e-04 (8.33e-04)	Tok/s 18069 (22737)	Loss/tok 3.1459 (3.4301)	LR 5.000e-04
0: TRAIN [1][820/5173]	Time 0.581 (0.623)	Data 1.29e-04 (8.24e-04)	Tok/s 17557 (22743)	Loss/tok 3.2296 (3.4300)	LR 5.000e-04
0: TRAIN [1][830/5173]	Time 0.648 (0.624)	Data 1.18e-04 (8.16e-04)	Tok/s 25790 (22760)	Loss/tok 3.5044 (3.4295)	LR 5.000e-04
0: TRAIN [1][840/5173]	Time 0.582 (0.624)	Data 1.16e-04 (8.08e-04)	Tok/s 17707 (22763)	Loss/tok 3.1975 (3.4294)	LR 5.000e-04
0: TRAIN [1][850/5173]	Time 0.784 (0.624)	Data 1.27e-04 (8.00e-04)	Tok/s 38348 (22765)	Loss/tok 3.7706 (3.4297)	LR 5.000e-04
0: TRAIN [1][860/5173]	Time 0.782 (0.623)	Data 1.21e-04 (7.92e-04)	Tok/s 37681 (22732)	Loss/tok 3.8435 (3.4289)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][870/5173]	Time 0.704 (0.624)	Data 1.35e-04 (7.85e-04)	Tok/s 33158 (22789)	Loss/tok 3.6895 (3.4306)	LR 5.000e-04
0: TRAIN [1][880/5173]	Time 0.646 (0.624)	Data 1.20e-04 (7.78e-04)	Tok/s 25852 (22795)	Loss/tok 3.4520 (3.4305)	LR 5.000e-04
0: TRAIN [1][890/5173]	Time 0.642 (0.624)	Data 2.74e-04 (7.71e-04)	Tok/s 26427 (22798)	Loss/tok 3.4019 (3.4306)	LR 5.000e-04
0: TRAIN [1][900/5173]	Time 0.787 (0.624)	Data 1.21e-04 (7.64e-04)	Tok/s 38012 (22809)	Loss/tok 3.7057 (3.4305)	LR 5.000e-04
0: TRAIN [1][910/5173]	Time 0.646 (0.624)	Data 1.33e-04 (7.58e-04)	Tok/s 26151 (22816)	Loss/tok 3.3659 (3.4299)	LR 5.000e-04
0: TRAIN [1][920/5173]	Time 0.583 (0.624)	Data 1.31e-04 (7.51e-04)	Tok/s 18198 (22779)	Loss/tok 3.1124 (3.4287)	LR 5.000e-04
0: TRAIN [1][930/5173]	Time 0.581 (0.623)	Data 1.18e-04 (7.44e-04)	Tok/s 17865 (22727)	Loss/tok 3.1443 (3.4272)	LR 5.000e-04
0: TRAIN [1][940/5173]	Time 0.584 (0.623)	Data 1.25e-04 (7.38e-04)	Tok/s 18005 (22761)	Loss/tok 3.2821 (3.4283)	LR 5.000e-04
0: TRAIN [1][950/5173]	Time 0.641 (0.624)	Data 1.27e-04 (7.32e-04)	Tok/s 26020 (22774)	Loss/tok 3.5014 (3.4285)	LR 5.000e-04
0: TRAIN [1][960/5173]	Time 0.779 (0.624)	Data 1.22e-04 (7.26e-04)	Tok/s 37916 (22776)	Loss/tok 3.8918 (3.4289)	LR 5.000e-04
0: TRAIN [1][970/5173]	Time 0.581 (0.623)	Data 1.18e-04 (7.19e-04)	Tok/s 17643 (22769)	Loss/tok 3.2059 (3.4280)	LR 5.000e-04
0: TRAIN [1][980/5173]	Time 0.582 (0.623)	Data 1.23e-04 (7.13e-04)	Tok/s 17910 (22753)	Loss/tok 3.2463 (3.4272)	LR 5.000e-04
0: TRAIN [1][990/5173]	Time 0.585 (0.623)	Data 3.07e-04 (7.08e-04)	Tok/s 17406 (22741)	Loss/tok 3.0864 (3.4269)	LR 5.000e-04
0: TRAIN [1][1000/5173]	Time 0.583 (0.623)	Data 1.17e-04 (7.02e-04)	Tok/s 17612 (22724)	Loss/tok 3.1503 (3.4256)	LR 5.000e-04
0: TRAIN [1][1010/5173]	Time 0.779 (0.623)	Data 1.23e-04 (6.96e-04)	Tok/s 37919 (22724)	Loss/tok 3.9088 (3.4262)	LR 5.000e-04
0: TRAIN [1][1020/5173]	Time 0.773 (0.623)	Data 1.39e-04 (6.91e-04)	Tok/s 38143 (22713)	Loss/tok 3.7977 (3.4258)	LR 5.000e-04
0: TRAIN [1][1030/5173]	Time 0.582 (0.623)	Data 1.19e-04 (6.85e-04)	Tok/s 17820 (22731)	Loss/tok 3.1288 (3.4265)	LR 5.000e-04
0: TRAIN [1][1040/5173]	Time 0.581 (0.623)	Data 2.88e-04 (6.80e-04)	Tok/s 17992 (22710)	Loss/tok 3.1959 (3.4260)	LR 5.000e-04
0: TRAIN [1][1050/5173]	Time 0.581 (0.623)	Data 1.29e-04 (6.75e-04)	Tok/s 18166 (22686)	Loss/tok 3.1709 (3.4248)	LR 5.000e-04
0: TRAIN [1][1060/5173]	Time 0.639 (0.623)	Data 1.16e-04 (6.70e-04)	Tok/s 26456 (22690)	Loss/tok 3.4837 (3.4246)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1070/5173]	Time 0.583 (0.623)	Data 1.23e-04 (6.65e-04)	Tok/s 18157 (22714)	Loss/tok 3.2253 (3.4251)	LR 5.000e-04
0: TRAIN [1][1080/5173]	Time 0.774 (0.623)	Data 1.25e-04 (6.60e-04)	Tok/s 38338 (22717)	Loss/tok 3.7713 (3.4249)	LR 5.000e-04
0: TRAIN [1][1090/5173]	Time 0.581 (0.623)	Data 1.18e-04 (6.55e-04)	Tok/s 17374 (22741)	Loss/tok 3.1462 (3.4252)	LR 5.000e-04
0: TRAIN [1][1100/5173]	Time 0.640 (0.623)	Data 1.22e-04 (6.51e-04)	Tok/s 25918 (22748)	Loss/tok 3.3896 (3.4251)	LR 5.000e-04
0: TRAIN [1][1110/5173]	Time 0.778 (0.624)	Data 1.25e-04 (6.46e-04)	Tok/s 38841 (22787)	Loss/tok 3.7545 (3.4262)	LR 5.000e-04
0: TRAIN [1][1120/5173]	Time 0.585 (0.624)	Data 3.02e-04 (6.42e-04)	Tok/s 17697 (22807)	Loss/tok 3.2370 (3.4261)	LR 5.000e-04
0: TRAIN [1][1130/5173]	Time 0.579 (0.624)	Data 1.22e-04 (6.37e-04)	Tok/s 17658 (22789)	Loss/tok 3.1566 (3.4258)	LR 5.000e-04
0: TRAIN [1][1140/5173]	Time 0.708 (0.624)	Data 1.23e-04 (6.33e-04)	Tok/s 32994 (22807)	Loss/tok 3.6204 (3.4261)	LR 5.000e-04
0: TRAIN [1][1150/5173]	Time 0.642 (0.623)	Data 1.34e-04 (6.28e-04)	Tok/s 26222 (22778)	Loss/tok 3.3176 (3.4251)	LR 5.000e-04
0: TRAIN [1][1160/5173]	Time 0.521 (0.623)	Data 3.02e-04 (6.24e-04)	Tok/s 10091 (22762)	Loss/tok 2.6906 (3.4244)	LR 5.000e-04
0: TRAIN [1][1170/5173]	Time 0.638 (0.624)	Data 1.24e-04 (6.20e-04)	Tok/s 26542 (22813)	Loss/tok 3.3400 (3.4252)	LR 5.000e-04
0: TRAIN [1][1180/5173]	Time 0.585 (0.623)	Data 1.22e-04 (6.16e-04)	Tok/s 17413 (22778)	Loss/tok 3.2417 (3.4243)	LR 5.000e-04
0: TRAIN [1][1190/5173]	Time 0.785 (0.624)	Data 1.16e-04 (6.12e-04)	Tok/s 38164 (22804)	Loss/tok 3.7225 (3.4247)	LR 5.000e-04
0: TRAIN [1][1200/5173]	Time 0.643 (0.623)	Data 1.27e-04 (6.08e-04)	Tok/s 26126 (22782)	Loss/tok 3.4344 (3.4239)	LR 5.000e-04
0: TRAIN [1][1210/5173]	Time 0.519 (0.623)	Data 1.17e-04 (6.04e-04)	Tok/s 10298 (22795)	Loss/tok 2.7352 (3.4238)	LR 5.000e-04
0: TRAIN [1][1220/5173]	Time 0.634 (0.624)	Data 1.29e-04 (6.00e-04)	Tok/s 26825 (22808)	Loss/tok 3.3983 (3.4238)	LR 5.000e-04
0: TRAIN [1][1230/5173]	Time 0.583 (0.624)	Data 1.14e-04 (5.96e-04)	Tok/s 17751 (22814)	Loss/tok 3.1291 (3.4238)	LR 5.000e-04
0: TRAIN [1][1240/5173]	Time 0.584 (0.624)	Data 2.94e-04 (5.93e-04)	Tok/s 17577 (22851)	Loss/tok 3.1504 (3.4250)	LR 5.000e-04
0: TRAIN [1][1250/5173]	Time 0.707 (0.624)	Data 1.24e-04 (5.89e-04)	Tok/s 33196 (22868)	Loss/tok 3.5104 (3.4247)	LR 5.000e-04
0: TRAIN [1][1260/5173]	Time 0.639 (0.624)	Data 1.20e-04 (5.86e-04)	Tok/s 26336 (22876)	Loss/tok 3.4790 (3.4257)	LR 5.000e-04
0: TRAIN [1][1270/5173]	Time 0.583 (0.624)	Data 1.24e-04 (5.82e-04)	Tok/s 17671 (22890)	Loss/tok 3.1734 (3.4258)	LR 5.000e-04
0: TRAIN [1][1280/5173]	Time 0.581 (0.624)	Data 1.17e-04 (5.79e-04)	Tok/s 17746 (22884)	Loss/tok 3.2387 (3.4255)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1290/5173]	Time 0.522 (0.624)	Data 1.31e-04 (5.75e-04)	Tok/s 10224 (22868)	Loss/tok 2.7153 (3.4250)	LR 5.000e-04
0: TRAIN [1][1300/5173]	Time 0.642 (0.624)	Data 1.23e-04 (5.72e-04)	Tok/s 25729 (22889)	Loss/tok 3.5316 (3.4252)	LR 5.000e-04
0: TRAIN [1][1310/5173]	Time 0.705 (0.624)	Data 1.18e-04 (5.69e-04)	Tok/s 33231 (22914)	Loss/tok 3.6216 (3.4256)	LR 5.000e-04
0: TRAIN [1][1320/5173]	Time 0.585 (0.624)	Data 1.25e-04 (5.65e-04)	Tok/s 17764 (22913)	Loss/tok 3.2366 (3.4256)	LR 5.000e-04
0: TRAIN [1][1330/5173]	Time 0.582 (0.625)	Data 1.34e-04 (5.62e-04)	Tok/s 17610 (22914)	Loss/tok 3.2741 (3.4254)	LR 5.000e-04
0: TRAIN [1][1340/5173]	Time 0.707 (0.625)	Data 1.20e-04 (5.59e-04)	Tok/s 33087 (22923)	Loss/tok 3.5977 (3.4252)	LR 5.000e-04
0: TRAIN [1][1350/5173]	Time 0.644 (0.625)	Data 1.18e-04 (5.56e-04)	Tok/s 25883 (22947)	Loss/tok 3.5250 (3.4254)	LR 5.000e-04
0: TRAIN [1][1360/5173]	Time 0.641 (0.625)	Data 1.46e-04 (5.52e-04)	Tok/s 26143 (22972)	Loss/tok 3.4871 (3.4257)	LR 5.000e-04
0: TRAIN [1][1370/5173]	Time 0.644 (0.625)	Data 1.24e-04 (5.50e-04)	Tok/s 25960 (22978)	Loss/tok 3.4546 (3.4254)	LR 5.000e-04
0: TRAIN [1][1380/5173]	Time 0.644 (0.625)	Data 1.37e-04 (5.47e-04)	Tok/s 25542 (23007)	Loss/tok 3.4621 (3.4265)	LR 5.000e-04
0: TRAIN [1][1390/5173]	Time 0.581 (0.625)	Data 1.26e-04 (5.44e-04)	Tok/s 18083 (22986)	Loss/tok 3.1843 (3.4263)	LR 5.000e-04
0: TRAIN [1][1400/5173]	Time 0.585 (0.625)	Data 1.22e-04 (5.41e-04)	Tok/s 17639 (22966)	Loss/tok 3.0796 (3.4253)	LR 5.000e-04
0: TRAIN [1][1410/5173]	Time 0.585 (0.625)	Data 1.13e-04 (5.38e-04)	Tok/s 17489 (22957)	Loss/tok 3.2298 (3.4247)	LR 5.000e-04
0: TRAIN [1][1420/5173]	Time 0.707 (0.625)	Data 3.18e-04 (5.36e-04)	Tok/s 32777 (22973)	Loss/tok 3.5347 (3.4249)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1430/5173]	Time 0.581 (0.625)	Data 1.18e-04 (5.33e-04)	Tok/s 17454 (22991)	Loss/tok 3.2387 (3.4256)	LR 5.000e-04
0: TRAIN [1][1440/5173]	Time 0.520 (0.625)	Data 1.18e-04 (5.30e-04)	Tok/s 10211 (22962)	Loss/tok 2.6852 (3.4249)	LR 5.000e-04
0: TRAIN [1][1450/5173]	Time 0.781 (0.625)	Data 1.32e-04 (5.28e-04)	Tok/s 37962 (22984)	Loss/tok 3.7951 (3.4256)	LR 5.000e-04
0: TRAIN [1][1460/5173]	Time 0.582 (0.625)	Data 1.22e-04 (5.25e-04)	Tok/s 17946 (22971)	Loss/tok 3.2743 (3.4248)	LR 5.000e-04
0: TRAIN [1][1470/5173]	Time 0.585 (0.625)	Data 1.15e-04 (5.22e-04)	Tok/s 17339 (22958)	Loss/tok 3.2455 (3.4248)	LR 5.000e-04
0: TRAIN [1][1480/5173]	Time 0.583 (0.625)	Data 1.21e-04 (5.20e-04)	Tok/s 17671 (22973)	Loss/tok 3.3059 (3.4248)	LR 5.000e-04
0: TRAIN [1][1490/5173]	Time 0.580 (0.625)	Data 1.23e-04 (5.17e-04)	Tok/s 18078 (22989)	Loss/tok 3.1962 (3.4251)	LR 5.000e-04
0: TRAIN [1][1500/5173]	Time 0.583 (0.625)	Data 1.15e-04 (5.15e-04)	Tok/s 17756 (22981)	Loss/tok 3.0967 (3.4245)	LR 5.000e-04
0: TRAIN [1][1510/5173]	Time 0.581 (0.625)	Data 1.19e-04 (5.12e-04)	Tok/s 17644 (22959)	Loss/tok 3.2184 (3.4236)	LR 5.000e-04
0: TRAIN [1][1520/5173]	Time 0.582 (0.625)	Data 1.27e-04 (5.10e-04)	Tok/s 17796 (22965)	Loss/tok 3.1907 (3.4233)	LR 5.000e-04
0: TRAIN [1][1530/5173]	Time 0.641 (0.625)	Data 1.18e-04 (5.07e-04)	Tok/s 25843 (22948)	Loss/tok 3.4092 (3.4232)	LR 5.000e-04
0: TRAIN [1][1540/5173]	Time 0.583 (0.625)	Data 1.30e-04 (5.05e-04)	Tok/s 17567 (22975)	Loss/tok 3.1453 (3.4237)	LR 5.000e-04
0: TRAIN [1][1550/5173]	Time 0.581 (0.625)	Data 1.23e-04 (5.02e-04)	Tok/s 18095 (22977)	Loss/tok 3.1779 (3.4239)	LR 5.000e-04
0: TRAIN [1][1560/5173]	Time 0.646 (0.625)	Data 1.28e-04 (5.00e-04)	Tok/s 25691 (22988)	Loss/tok 3.4088 (3.4240)	LR 5.000e-04
0: TRAIN [1][1570/5173]	Time 0.585 (0.625)	Data 1.24e-04 (4.98e-04)	Tok/s 17429 (23000)	Loss/tok 3.1129 (3.4241)	LR 5.000e-04
0: TRAIN [1][1580/5173]	Time 0.582 (0.625)	Data 1.28e-04 (4.95e-04)	Tok/s 18003 (23004)	Loss/tok 3.3255 (3.4243)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1590/5173]	Time 0.641 (0.625)	Data 3.09e-04 (4.93e-04)	Tok/s 26294 (23001)	Loss/tok 3.4030 (3.4242)	LR 2.500e-04
0: TRAIN [1][1600/5173]	Time 0.579 (0.625)	Data 1.19e-04 (4.91e-04)	Tok/s 17834 (23005)	Loss/tok 3.1623 (3.4243)	LR 2.500e-04
0: TRAIN [1][1610/5173]	Time 0.704 (0.625)	Data 1.26e-04 (4.89e-04)	Tok/s 33199 (22997)	Loss/tok 3.5555 (3.4239)	LR 2.500e-04
0: TRAIN [1][1620/5173]	Time 0.584 (0.625)	Data 1.22e-04 (4.86e-04)	Tok/s 17931 (23002)	Loss/tok 3.2003 (3.4241)	LR 2.500e-04
0: TRAIN [1][1630/5173]	Time 0.641 (0.625)	Data 1.16e-04 (4.84e-04)	Tok/s 25987 (23010)	Loss/tok 3.4033 (3.4237)	LR 2.500e-04
0: TRAIN [1][1640/5173]	Time 0.583 (0.625)	Data 1.39e-04 (4.82e-04)	Tok/s 17689 (23021)	Loss/tok 3.1425 (3.4237)	LR 2.500e-04
0: TRAIN [1][1650/5173]	Time 0.581 (0.625)	Data 1.22e-04 (4.80e-04)	Tok/s 18000 (23026)	Loss/tok 3.1031 (3.4239)	LR 2.500e-04
0: TRAIN [1][1660/5173]	Time 0.640 (0.626)	Data 1.18e-04 (4.78e-04)	Tok/s 26182 (23043)	Loss/tok 3.4569 (3.4241)	LR 2.500e-04
0: TRAIN [1][1670/5173]	Time 0.646 (0.625)	Data 1.22e-04 (4.76e-04)	Tok/s 26263 (23030)	Loss/tok 3.3798 (3.4236)	LR 2.500e-04
0: TRAIN [1][1680/5173]	Time 0.520 (0.625)	Data 2.83e-04 (4.74e-04)	Tok/s 10263 (23027)	Loss/tok 2.6662 (3.4232)	LR 2.500e-04
0: TRAIN [1][1690/5173]	Time 0.641 (0.625)	Data 1.24e-04 (4.72e-04)	Tok/s 26363 (23022)	Loss/tok 3.3716 (3.4227)	LR 2.500e-04
0: TRAIN [1][1700/5173]	Time 0.649 (0.625)	Data 1.23e-04 (4.70e-04)	Tok/s 25621 (23005)	Loss/tok 3.3967 (3.4220)	LR 2.500e-04
0: TRAIN [1][1710/5173]	Time 0.582 (0.625)	Data 1.20e-04 (4.68e-04)	Tok/s 17398 (22998)	Loss/tok 3.1808 (3.4220)	LR 2.500e-04
0: TRAIN [1][1720/5173]	Time 0.645 (0.625)	Data 1.34e-04 (4.66e-04)	Tok/s 25832 (23019)	Loss/tok 3.4114 (3.4228)	LR 2.500e-04
0: TRAIN [1][1730/5173]	Time 0.582 (0.625)	Data 1.20e-04 (4.64e-04)	Tok/s 17719 (22994)	Loss/tok 3.2199 (3.4220)	LR 2.500e-04
0: TRAIN [1][1740/5173]	Time 0.779 (0.625)	Data 1.22e-04 (4.62e-04)	Tok/s 38378 (22995)	Loss/tok 3.8029 (3.4219)	LR 2.500e-04
0: TRAIN [1][1750/5173]	Time 0.582 (0.625)	Data 2.81e-04 (4.61e-04)	Tok/s 17551 (22970)	Loss/tok 3.1225 (3.4212)	LR 2.500e-04
0: TRAIN [1][1760/5173]	Time 0.582 (0.625)	Data 1.20e-04 (4.59e-04)	Tok/s 17676 (22967)	Loss/tok 3.2020 (3.4209)	LR 2.500e-04
0: TRAIN [1][1770/5173]	Time 0.518 (0.625)	Data 1.22e-04 (4.57e-04)	Tok/s 10071 (22946)	Loss/tok 2.7308 (3.4201)	LR 2.500e-04
0: TRAIN [1][1780/5173]	Time 0.643 (0.625)	Data 1.19e-04 (4.55e-04)	Tok/s 26097 (22949)	Loss/tok 3.4543 (3.4201)	LR 2.500e-04
0: TRAIN [1][1790/5173]	Time 0.582 (0.625)	Data 1.14e-04 (4.53e-04)	Tok/s 17676 (22946)	Loss/tok 3.2336 (3.4199)	LR 2.500e-04
0: TRAIN [1][1800/5173]	Time 0.582 (0.625)	Data 1.21e-04 (4.52e-04)	Tok/s 17797 (22940)	Loss/tok 3.1869 (3.4197)	LR 2.500e-04
0: TRAIN [1][1810/5173]	Time 0.582 (0.625)	Data 1.20e-04 (4.50e-04)	Tok/s 17637 (22950)	Loss/tok 3.3320 (3.4196)	LR 2.500e-04
0: TRAIN [1][1820/5173]	Time 0.642 (0.625)	Data 1.16e-04 (4.48e-04)	Tok/s 26307 (22932)	Loss/tok 3.5034 (3.4191)	LR 2.500e-04
0: TRAIN [1][1830/5173]	Time 0.639 (0.625)	Data 1.75e-04 (4.47e-04)	Tok/s 26256 (22967)	Loss/tok 3.3330 (3.4204)	LR 2.500e-04
0: TRAIN [1][1840/5173]	Time 0.581 (0.625)	Data 1.17e-04 (4.45e-04)	Tok/s 17780 (22969)	Loss/tok 3.2099 (3.4201)	LR 2.500e-04
0: TRAIN [1][1850/5173]	Time 0.646 (0.625)	Data 1.19e-04 (4.43e-04)	Tok/s 26314 (22958)	Loss/tok 3.3414 (3.4196)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][1860/5173]	Time 0.650 (0.625)	Data 1.23e-04 (4.42e-04)	Tok/s 25750 (22983)	Loss/tok 3.3826 (3.4202)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1870/5173]	Time 0.643 (0.625)	Data 3.18e-04 (4.40e-04)	Tok/s 25974 (22999)	Loss/tok 3.3957 (3.4204)	LR 2.500e-04
0: TRAIN [1][1880/5173]	Time 0.581 (0.625)	Data 1.23e-04 (4.39e-04)	Tok/s 17846 (23025)	Loss/tok 3.1991 (3.4208)	LR 2.500e-04
0: TRAIN [1][1890/5173]	Time 0.521 (0.625)	Data 1.26e-04 (4.37e-04)	Tok/s 10109 (23014)	Loss/tok 2.8409 (3.4204)	LR 2.500e-04
0: TRAIN [1][1900/5173]	Time 0.707 (0.625)	Data 3.11e-04 (4.35e-04)	Tok/s 32934 (23024)	Loss/tok 3.5677 (3.4204)	LR 2.500e-04
0: TRAIN [1][1910/5173]	Time 0.583 (0.625)	Data 1.17e-04 (4.34e-04)	Tok/s 17996 (23024)	Loss/tok 3.1560 (3.4203)	LR 2.500e-04
0: TRAIN [1][1920/5173]	Time 0.582 (0.625)	Data 1.29e-04 (4.32e-04)	Tok/s 17686 (23018)	Loss/tok 3.1727 (3.4200)	LR 2.500e-04
0: TRAIN [1][1930/5173]	Time 0.584 (0.625)	Data 1.28e-04 (4.31e-04)	Tok/s 17450 (23007)	Loss/tok 3.2703 (3.4195)	LR 2.500e-04
0: TRAIN [1][1940/5173]	Time 0.642 (0.625)	Data 1.20e-04 (4.29e-04)	Tok/s 26021 (23018)	Loss/tok 3.4444 (3.4194)	LR 2.500e-04
0: TRAIN [1][1950/5173]	Time 0.645 (0.625)	Data 1.26e-04 (4.28e-04)	Tok/s 25764 (23023)	Loss/tok 3.4276 (3.4192)	LR 2.500e-04
0: TRAIN [1][1960/5173]	Time 0.648 (0.625)	Data 1.27e-04 (4.26e-04)	Tok/s 25762 (23035)	Loss/tok 3.3978 (3.4195)	LR 2.500e-04
0: TRAIN [1][1970/5173]	Time 0.582 (0.625)	Data 1.22e-04 (4.25e-04)	Tok/s 17976 (23043)	Loss/tok 3.2318 (3.4195)	LR 2.500e-04
0: TRAIN [1][1980/5173]	Time 0.581 (0.625)	Data 1.16e-04 (4.23e-04)	Tok/s 17782 (23029)	Loss/tok 3.1760 (3.4188)	LR 2.500e-04
0: TRAIN [1][1990/5173]	Time 0.582 (0.625)	Data 1.22e-04 (4.22e-04)	Tok/s 17920 (23021)	Loss/tok 3.1455 (3.4187)	LR 2.500e-04
0: TRAIN [1][2000/5173]	Time 0.582 (0.625)	Data 1.21e-04 (4.20e-04)	Tok/s 18248 (23010)	Loss/tok 3.1779 (3.4184)	LR 2.500e-04
0: TRAIN [1][2010/5173]	Time 0.642 (0.625)	Data 1.12e-04 (4.19e-04)	Tok/s 26578 (23007)	Loss/tok 3.4009 (3.4183)	LR 2.500e-04
0: TRAIN [1][2020/5173]	Time 0.645 (0.625)	Data 1.21e-04 (4.18e-04)	Tok/s 25923 (23008)	Loss/tok 3.3583 (3.4181)	LR 2.500e-04
0: TRAIN [1][2030/5173]	Time 0.644 (0.625)	Data 1.20e-04 (4.16e-04)	Tok/s 26141 (23003)	Loss/tok 3.4992 (3.4176)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2040/5173]	Time 0.646 (0.625)	Data 1.21e-04 (4.15e-04)	Tok/s 25673 (23023)	Loss/tok 3.3727 (3.4185)	LR 2.500e-04
0: TRAIN [1][2050/5173]	Time 0.583 (0.625)	Data 1.52e-04 (4.14e-04)	Tok/s 17529 (23005)	Loss/tok 3.1582 (3.4179)	LR 2.500e-04
0: TRAIN [1][2060/5173]	Time 0.648 (0.625)	Data 1.46e-04 (4.12e-04)	Tok/s 25912 (22999)	Loss/tok 3.4011 (3.4176)	LR 2.500e-04
0: TRAIN [1][2070/5173]	Time 0.585 (0.625)	Data 1.32e-04 (4.11e-04)	Tok/s 17737 (22995)	Loss/tok 3.2289 (3.4175)	LR 2.500e-04
0: TRAIN [1][2080/5173]	Time 0.704 (0.625)	Data 1.33e-04 (4.10e-04)	Tok/s 33444 (23011)	Loss/tok 3.5038 (3.4176)	LR 2.500e-04
0: TRAIN [1][2090/5173]	Time 0.646 (0.625)	Data 1.24e-04 (4.08e-04)	Tok/s 26165 (23013)	Loss/tok 3.4373 (3.4173)	LR 2.500e-04
0: TRAIN [1][2100/5173]	Time 0.644 (0.625)	Data 1.19e-04 (4.07e-04)	Tok/s 25995 (23021)	Loss/tok 3.5130 (3.4175)	LR 2.500e-04
0: TRAIN [1][2110/5173]	Time 0.520 (0.625)	Data 1.24e-04 (4.06e-04)	Tok/s 10193 (23025)	Loss/tok 2.7295 (3.4175)	LR 2.500e-04
0: TRAIN [1][2120/5173]	Time 0.706 (0.625)	Data 1.25e-04 (4.04e-04)	Tok/s 32762 (23026)	Loss/tok 3.5582 (3.4171)	LR 2.500e-04
0: TRAIN [1][2130/5173]	Time 0.709 (0.625)	Data 1.20e-04 (4.03e-04)	Tok/s 32691 (23043)	Loss/tok 3.4647 (3.4175)	LR 2.500e-04
0: TRAIN [1][2140/5173]	Time 0.779 (0.625)	Data 1.25e-04 (4.02e-04)	Tok/s 37833 (23035)	Loss/tok 3.8303 (3.4174)	LR 2.500e-04
0: TRAIN [1][2150/5173]	Time 0.705 (0.625)	Data 1.22e-04 (4.01e-04)	Tok/s 33062 (23045)	Loss/tok 3.5018 (3.4177)	LR 2.500e-04
0: TRAIN [1][2160/5173]	Time 0.646 (0.625)	Data 1.17e-04 (3.99e-04)	Tok/s 25956 (23032)	Loss/tok 3.3379 (3.4171)	LR 2.500e-04
0: TRAIN [1][2170/5173]	Time 0.642 (0.625)	Data 1.21e-04 (3.98e-04)	Tok/s 26261 (23041)	Loss/tok 3.3569 (3.4171)	LR 2.500e-04
0: TRAIN [1][2180/5173]	Time 0.522 (0.625)	Data 1.20e-04 (3.97e-04)	Tok/s 9811 (23025)	Loss/tok 2.6499 (3.4167)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2190/5173]	Time 0.583 (0.625)	Data 1.18e-04 (3.96e-04)	Tok/s 17692 (23021)	Loss/tok 3.3458 (3.4165)	LR 2.500e-04
0: TRAIN [1][2200/5173]	Time 0.703 (0.625)	Data 2.98e-04 (3.95e-04)	Tok/s 33145 (23016)	Loss/tok 3.6701 (3.4163)	LR 2.500e-04
0: TRAIN [1][2210/5173]	Time 0.705 (0.625)	Data 2.77e-04 (3.93e-04)	Tok/s 33082 (23011)	Loss/tok 3.5223 (3.4160)	LR 2.500e-04
0: TRAIN [1][2220/5173]	Time 0.583 (0.625)	Data 1.21e-04 (3.92e-04)	Tok/s 18387 (23006)	Loss/tok 3.1166 (3.4156)	LR 2.500e-04
0: TRAIN [1][2230/5173]	Time 0.643 (0.625)	Data 1.19e-04 (3.91e-04)	Tok/s 26388 (22997)	Loss/tok 3.3303 (3.4152)	LR 2.500e-04
0: TRAIN [1][2240/5173]	Time 0.779 (0.625)	Data 1.23e-04 (3.90e-04)	Tok/s 38042 (23002)	Loss/tok 3.7032 (3.4154)	LR 2.500e-04
0: TRAIN [1][2250/5173]	Time 0.584 (0.625)	Data 1.22e-04 (3.89e-04)	Tok/s 17473 (22992)	Loss/tok 3.1767 (3.4149)	LR 2.500e-04
0: TRAIN [1][2260/5173]	Time 0.580 (0.625)	Data 1.18e-04 (3.88e-04)	Tok/s 17465 (22977)	Loss/tok 3.1608 (3.4144)	LR 2.500e-04
0: TRAIN [1][2270/5173]	Time 0.639 (0.625)	Data 1.19e-04 (3.87e-04)	Tok/s 26242 (22951)	Loss/tok 3.4048 (3.4137)	LR 2.500e-04
0: TRAIN [1][2280/5173]	Time 0.704 (0.625)	Data 1.30e-04 (3.86e-04)	Tok/s 32857 (22956)	Loss/tok 3.6452 (3.4138)	LR 2.500e-04
0: TRAIN [1][2290/5173]	Time 0.648 (0.625)	Data 1.20e-04 (3.85e-04)	Tok/s 26114 (22969)	Loss/tok 3.3980 (3.4137)	LR 2.500e-04
0: TRAIN [1][2300/5173]	Time 0.639 (0.625)	Data 1.48e-04 (3.83e-04)	Tok/s 26419 (22961)	Loss/tok 3.4966 (3.4132)	LR 2.500e-04
0: TRAIN [1][2310/5173]	Time 0.708 (0.625)	Data 1.49e-04 (3.82e-04)	Tok/s 32769 (22959)	Loss/tok 3.4192 (3.4128)	LR 2.500e-04
0: TRAIN [1][2320/5173]	Time 0.643 (0.625)	Data 1.49e-04 (3.81e-04)	Tok/s 25786 (22963)	Loss/tok 3.3444 (3.4126)	LR 2.500e-04
0: TRAIN [1][2330/5173]	Time 0.585 (0.625)	Data 3.05e-04 (3.80e-04)	Tok/s 18034 (22955)	Loss/tok 3.2211 (3.4123)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2340/5173]	Time 0.641 (0.625)	Data 1.25e-04 (3.79e-04)	Tok/s 26195 (22969)	Loss/tok 3.3528 (3.4124)	LR 2.500e-04
0: TRAIN [1][2350/5173]	Time 0.581 (0.625)	Data 1.20e-04 (3.78e-04)	Tok/s 17787 (22967)	Loss/tok 3.1853 (3.4122)	LR 2.500e-04
0: TRAIN [1][2360/5173]	Time 0.583 (0.625)	Data 1.15e-04 (3.77e-04)	Tok/s 17789 (22974)	Loss/tok 3.0402 (3.4122)	LR 2.500e-04
0: TRAIN [1][2370/5173]	Time 0.520 (0.625)	Data 1.24e-04 (3.76e-04)	Tok/s 9892 (22965)	Loss/tok 2.6185 (3.4119)	LR 2.500e-04
0: TRAIN [1][2380/5173]	Time 0.782 (0.625)	Data 3.14e-04 (3.75e-04)	Tok/s 38919 (22971)	Loss/tok 3.7487 (3.4121)	LR 2.500e-04
0: TRAIN [1][2390/5173]	Time 0.705 (0.625)	Data 1.18e-04 (3.74e-04)	Tok/s 33013 (22994)	Loss/tok 3.5878 (3.4126)	LR 2.500e-04
0: TRAIN [1][2400/5173]	Time 0.709 (0.625)	Data 1.24e-04 (3.74e-04)	Tok/s 33168 (22987)	Loss/tok 3.5757 (3.4124)	LR 1.250e-04
0: TRAIN [1][2410/5173]	Time 0.645 (0.625)	Data 1.23e-04 (3.73e-04)	Tok/s 25752 (22982)	Loss/tok 3.4683 (3.4120)	LR 1.250e-04
0: TRAIN [1][2420/5173]	Time 0.581 (0.625)	Data 1.18e-04 (3.71e-04)	Tok/s 17647 (22991)	Loss/tok 3.2252 (3.4122)	LR 1.250e-04
0: TRAIN [1][2430/5173]	Time 0.584 (0.625)	Data 1.25e-04 (3.71e-04)	Tok/s 17517 (22984)	Loss/tok 3.1100 (3.4118)	LR 1.250e-04
0: TRAIN [1][2440/5173]	Time 0.520 (0.625)	Data 1.69e-04 (3.70e-04)	Tok/s 10426 (23016)	Loss/tok 2.7659 (3.4129)	LR 1.250e-04
0: TRAIN [1][2450/5173]	Time 0.702 (0.625)	Data 1.18e-04 (3.69e-04)	Tok/s 33283 (23001)	Loss/tok 3.5676 (3.4124)	LR 1.250e-04
0: TRAIN [1][2460/5173]	Time 0.579 (0.625)	Data 1.28e-04 (3.68e-04)	Tok/s 17397 (23011)	Loss/tok 3.1676 (3.4124)	LR 1.250e-04
0: TRAIN [1][2470/5173]	Time 0.582 (0.625)	Data 1.23e-04 (3.67e-04)	Tok/s 17633 (23019)	Loss/tok 3.1541 (3.4123)	LR 1.250e-04
0: TRAIN [1][2480/5173]	Time 0.641 (0.625)	Data 1.27e-04 (3.66e-04)	Tok/s 26361 (23014)	Loss/tok 3.3613 (3.4119)	LR 1.250e-04
0: TRAIN [1][2490/5173]	Time 0.707 (0.625)	Data 1.25e-04 (3.65e-04)	Tok/s 33154 (23026)	Loss/tok 3.5627 (3.4122)	LR 1.250e-04
0: TRAIN [1][2500/5173]	Time 0.522 (0.625)	Data 2.88e-04 (3.64e-04)	Tok/s 10009 (23015)	Loss/tok 2.6325 (3.4116)	LR 1.250e-04
0: TRAIN [1][2510/5173]	Time 0.581 (0.625)	Data 1.15e-04 (3.63e-04)	Tok/s 17586 (23006)	Loss/tok 3.1318 (3.4113)	LR 1.250e-04
0: TRAIN [1][2520/5173]	Time 0.640 (0.625)	Data 1.18e-04 (3.62e-04)	Tok/s 26375 (22993)	Loss/tok 3.3920 (3.4108)	LR 1.250e-04
0: TRAIN [1][2530/5173]	Time 0.584 (0.625)	Data 1.32e-04 (3.61e-04)	Tok/s 17879 (22979)	Loss/tok 3.1537 (3.4104)	LR 1.250e-04
0: TRAIN [1][2540/5173]	Time 0.782 (0.625)	Data 1.18e-04 (3.60e-04)	Tok/s 38223 (22990)	Loss/tok 3.7209 (3.4107)	LR 1.250e-04
0: TRAIN [1][2550/5173]	Time 0.583 (0.625)	Data 3.14e-04 (3.60e-04)	Tok/s 17487 (22983)	Loss/tok 3.2190 (3.4103)	LR 1.250e-04
0: TRAIN [1][2560/5173]	Time 0.580 (0.625)	Data 1.16e-04 (3.59e-04)	Tok/s 18069 (22982)	Loss/tok 3.2960 (3.4101)	LR 1.250e-04
0: TRAIN [1][2570/5173]	Time 0.778 (0.625)	Data 1.33e-04 (3.58e-04)	Tok/s 38775 (22983)	Loss/tok 3.7449 (3.4101)	LR 1.250e-04
0: TRAIN [1][2580/5173]	Time 0.581 (0.625)	Data 1.17e-04 (3.57e-04)	Tok/s 18252 (22978)	Loss/tok 3.1884 (3.4098)	LR 1.250e-04
0: TRAIN [1][2590/5173]	Time 0.585 (0.625)	Data 1.16e-04 (3.56e-04)	Tok/s 17594 (22987)	Loss/tok 3.0925 (3.4099)	LR 1.250e-04
0: TRAIN [1][2600/5173]	Time 0.579 (0.625)	Data 1.21e-04 (3.55e-04)	Tok/s 17850 (22979)	Loss/tok 3.2309 (3.4096)	LR 1.250e-04
0: TRAIN [1][2610/5173]	Time 0.648 (0.625)	Data 1.18e-04 (3.54e-04)	Tok/s 25828 (22984)	Loss/tok 3.4279 (3.4094)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][2620/5173]	Time 0.583 (0.625)	Data 1.24e-04 (3.53e-04)	Tok/s 18094 (22970)	Loss/tok 3.1351 (3.4091)	LR 1.250e-04
0: TRAIN [1][2630/5173]	Time 0.583 (0.624)	Data 1.21e-04 (3.53e-04)	Tok/s 18088 (22945)	Loss/tok 3.1418 (3.4084)	LR 1.250e-04
0: TRAIN [1][2640/5173]	Time 0.706 (0.625)	Data 1.50e-04 (3.52e-04)	Tok/s 33083 (22954)	Loss/tok 3.5478 (3.4087)	LR 1.250e-04
0: TRAIN [1][2650/5173]	Time 0.582 (0.625)	Data 1.18e-04 (3.51e-04)	Tok/s 17780 (22956)	Loss/tok 3.2243 (3.4085)	LR 1.250e-04
0: TRAIN [1][2660/5173]	Time 0.584 (0.625)	Data 1.18e-04 (3.50e-04)	Tok/s 17044 (22956)	Loss/tok 3.2740 (3.4083)	LR 1.250e-04
0: TRAIN [1][2670/5173]	Time 0.644 (0.625)	Data 1.55e-04 (3.50e-04)	Tok/s 26220 (22982)	Loss/tok 3.4050 (3.4090)	LR 1.250e-04
0: TRAIN [1][2680/5173]	Time 0.582 (0.625)	Data 1.25e-04 (3.49e-04)	Tok/s 18080 (22978)	Loss/tok 3.1166 (3.4087)	LR 1.250e-04
0: TRAIN [1][2690/5173]	Time 0.709 (0.625)	Data 1.25e-04 (3.48e-04)	Tok/s 33044 (22968)	Loss/tok 3.4523 (3.4083)	LR 1.250e-04
0: TRAIN [1][2700/5173]	Time 0.522 (0.625)	Data 1.20e-04 (3.47e-04)	Tok/s 10211 (22949)	Loss/tok 2.7156 (3.4078)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2710/5173]	Time 0.775 (0.625)	Data 1.22e-04 (3.46e-04)	Tok/s 38568 (22963)	Loss/tok 3.7141 (3.4081)	LR 1.250e-04
0: TRAIN [1][2720/5173]	Time 0.519 (0.625)	Data 1.20e-04 (3.46e-04)	Tok/s 10232 (22954)	Loss/tok 2.5676 (3.4078)	LR 1.250e-04
0: TRAIN [1][2730/5173]	Time 0.704 (0.624)	Data 1.25e-04 (3.45e-04)	Tok/s 33173 (22948)	Loss/tok 3.7147 (3.4076)	LR 1.250e-04
0: TRAIN [1][2740/5173]	Time 0.584 (0.624)	Data 1.19e-04 (3.44e-04)	Tok/s 17837 (22944)	Loss/tok 3.2087 (3.4072)	LR 1.250e-04
0: TRAIN [1][2750/5173]	Time 0.642 (0.624)	Data 1.19e-04 (3.43e-04)	Tok/s 26267 (22947)	Loss/tok 3.3803 (3.4072)	LR 1.250e-04
0: TRAIN [1][2760/5173]	Time 0.579 (0.624)	Data 1.33e-04 (3.43e-04)	Tok/s 17830 (22937)	Loss/tok 3.1824 (3.4070)	LR 1.250e-04
0: TRAIN [1][2770/5173]	Time 0.646 (0.624)	Data 1.18e-04 (3.42e-04)	Tok/s 25776 (22940)	Loss/tok 3.3326 (3.4068)	LR 1.250e-04
0: TRAIN [1][2780/5173]	Time 0.647 (0.624)	Data 1.17e-04 (3.41e-04)	Tok/s 26268 (22939)	Loss/tok 3.4530 (3.4067)	LR 1.250e-04
0: TRAIN [1][2790/5173]	Time 0.583 (0.624)	Data 1.16e-04 (3.40e-04)	Tok/s 18069 (22941)	Loss/tok 3.1514 (3.4065)	LR 1.250e-04
0: TRAIN [1][2800/5173]	Time 0.702 (0.624)	Data 1.35e-04 (3.40e-04)	Tok/s 33327 (22950)	Loss/tok 3.5415 (3.4068)	LR 1.250e-04
0: TRAIN [1][2810/5173]	Time 0.584 (0.625)	Data 1.18e-04 (3.39e-04)	Tok/s 17908 (22958)	Loss/tok 3.1820 (3.4067)	LR 1.250e-04
0: TRAIN [1][2820/5173]	Time 0.645 (0.624)	Data 2.85e-04 (3.38e-04)	Tok/s 26019 (22954)	Loss/tok 3.4411 (3.4064)	LR 1.250e-04
0: TRAIN [1][2830/5173]	Time 0.583 (0.624)	Data 1.22e-04 (3.37e-04)	Tok/s 18010 (22953)	Loss/tok 3.2507 (3.4062)	LR 1.250e-04
0: TRAIN [1][2840/5173]	Time 0.582 (0.625)	Data 1.14e-04 (3.37e-04)	Tok/s 17677 (22957)	Loss/tok 3.1794 (3.4064)	LR 1.250e-04
0: TRAIN [1][2850/5173]	Time 0.644 (0.625)	Data 1.21e-04 (3.36e-04)	Tok/s 26365 (22961)	Loss/tok 3.3086 (3.4063)	LR 1.250e-04
0: TRAIN [1][2860/5173]	Time 0.521 (0.625)	Data 1.18e-04 (3.35e-04)	Tok/s 10273 (22969)	Loss/tok 2.8248 (3.4065)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2870/5173]	Time 0.639 (0.625)	Data 1.22e-04 (3.35e-04)	Tok/s 26146 (22968)	Loss/tok 3.3881 (3.4067)	LR 1.250e-04
0: TRAIN [1][2880/5173]	Time 0.645 (0.625)	Data 1.24e-04 (3.34e-04)	Tok/s 26064 (22988)	Loss/tok 3.4672 (3.4071)	LR 1.250e-04
0: TRAIN [1][2890/5173]	Time 0.637 (0.625)	Data 1.22e-04 (3.33e-04)	Tok/s 26292 (23006)	Loss/tok 3.3207 (3.4073)	LR 1.250e-04
0: TRAIN [1][2900/5173]	Time 0.520 (0.625)	Data 1.16e-04 (3.33e-04)	Tok/s 10165 (23004)	Loss/tok 2.7795 (3.4072)	LR 1.250e-04
0: TRAIN [1][2910/5173]	Time 0.702 (0.625)	Data 3.13e-04 (3.32e-04)	Tok/s 32783 (22999)	Loss/tok 3.4570 (3.4069)	LR 1.250e-04
0: TRAIN [1][2920/5173]	Time 0.584 (0.625)	Data 1.32e-04 (3.31e-04)	Tok/s 17612 (23002)	Loss/tok 3.1594 (3.4074)	LR 1.250e-04
0: TRAIN [1][2930/5173]	Time 0.581 (0.625)	Data 1.29e-04 (3.31e-04)	Tok/s 17806 (23006)	Loss/tok 3.1134 (3.4074)	LR 1.250e-04
0: TRAIN [1][2940/5173]	Time 0.580 (0.625)	Data 1.24e-04 (3.30e-04)	Tok/s 17610 (23002)	Loss/tok 3.1071 (3.4074)	LR 1.250e-04
0: TRAIN [1][2950/5173]	Time 0.585 (0.625)	Data 1.18e-04 (3.29e-04)	Tok/s 17522 (23004)	Loss/tok 3.2536 (3.4072)	LR 1.250e-04
0: TRAIN [1][2960/5173]	Time 0.779 (0.625)	Data 1.35e-04 (3.29e-04)	Tok/s 37615 (23011)	Loss/tok 3.7284 (3.4075)	LR 1.250e-04
0: TRAIN [1][2970/5173]	Time 0.583 (0.625)	Data 1.20e-04 (3.28e-04)	Tok/s 18049 (23002)	Loss/tok 3.1345 (3.4072)	LR 1.250e-04
0: TRAIN [1][2980/5173]	Time 0.645 (0.625)	Data 1.20e-04 (3.27e-04)	Tok/s 26203 (23006)	Loss/tok 3.3977 (3.4070)	LR 1.250e-04
0: TRAIN [1][2990/5173]	Time 0.780 (0.625)	Data 1.58e-04 (3.27e-04)	Tok/s 38732 (23013)	Loss/tok 3.6720 (3.4072)	LR 1.250e-04
0: TRAIN [1][3000/5173]	Time 0.582 (0.625)	Data 1.50e-04 (3.26e-04)	Tok/s 17925 (23009)	Loss/tok 3.1008 (3.4069)	LR 1.250e-04
0: TRAIN [1][3010/5173]	Time 0.585 (0.625)	Data 1.32e-04 (3.25e-04)	Tok/s 17554 (22992)	Loss/tok 3.1359 (3.4064)	LR 1.250e-04
0: TRAIN [1][3020/5173]	Time 0.704 (0.625)	Data 1.28e-04 (3.25e-04)	Tok/s 32901 (22998)	Loss/tok 3.5829 (3.4063)	LR 1.250e-04
0: TRAIN [1][3030/5173]	Time 0.706 (0.625)	Data 1.22e-04 (3.24e-04)	Tok/s 33365 (23002)	Loss/tok 3.5089 (3.4061)	LR 1.250e-04
0: TRAIN [1][3040/5173]	Time 0.646 (0.625)	Data 3.41e-04 (3.23e-04)	Tok/s 25774 (22993)	Loss/tok 3.3592 (3.4057)	LR 1.250e-04
0: TRAIN [1][3050/5173]	Time 0.642 (0.625)	Data 1.21e-04 (3.23e-04)	Tok/s 26150 (23002)	Loss/tok 3.4990 (3.4057)	LR 1.250e-04
0: TRAIN [1][3060/5173]	Time 0.642 (0.625)	Data 1.26e-04 (3.22e-04)	Tok/s 25965 (22994)	Loss/tok 3.3783 (3.4054)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][3070/5173]	Time 0.706 (0.625)	Data 1.23e-04 (3.22e-04)	Tok/s 32889 (23006)	Loss/tok 3.6600 (3.4058)	LR 1.250e-04
0: TRAIN [1][3080/5173]	Time 0.584 (0.625)	Data 1.21e-04 (3.21e-04)	Tok/s 17581 (23009)	Loss/tok 3.2512 (3.4057)	LR 1.250e-04
0: TRAIN [1][3090/5173]	Time 0.583 (0.625)	Data 1.24e-04 (3.21e-04)	Tok/s 17612 (22998)	Loss/tok 3.1945 (3.4053)	LR 1.250e-04
0: TRAIN [1][3100/5173]	Time 0.579 (0.625)	Data 1.18e-04 (3.20e-04)	Tok/s 17545 (22986)	Loss/tok 3.1929 (3.4050)	LR 1.250e-04
0: TRAIN [1][3110/5173]	Time 0.647 (0.625)	Data 1.22e-04 (3.19e-04)	Tok/s 25993 (22987)	Loss/tok 3.3845 (3.4048)	LR 1.250e-04
0: TRAIN [1][3120/5173]	Time 0.639 (0.625)	Data 1.23e-04 (3.19e-04)	Tok/s 26101 (22983)	Loss/tok 3.4319 (3.4046)	LR 1.250e-04
0: TRAIN [1][3130/5173]	Time 0.648 (0.625)	Data 1.22e-04 (3.18e-04)	Tok/s 26010 (22982)	Loss/tok 3.4574 (3.4043)	LR 1.250e-04
0: TRAIN [1][3140/5173]	Time 0.581 (0.625)	Data 1.23e-04 (3.18e-04)	Tok/s 18003 (22975)	Loss/tok 3.2961 (3.4041)	LR 1.250e-04
0: TRAIN [1][3150/5173]	Time 0.581 (0.624)	Data 1.21e-04 (3.17e-04)	Tok/s 17874 (22964)	Loss/tok 3.1097 (3.4037)	LR 1.250e-04
0: TRAIN [1][3160/5173]	Time 0.583 (0.624)	Data 3.03e-04 (3.17e-04)	Tok/s 17568 (22960)	Loss/tok 3.1529 (3.4035)	LR 1.250e-04
0: TRAIN [1][3170/5173]	Time 0.581 (0.624)	Data 3.60e-04 (3.16e-04)	Tok/s 17914 (22946)	Loss/tok 3.2067 (3.4031)	LR 1.250e-04
0: TRAIN [1][3180/5173]	Time 0.583 (0.624)	Data 1.24e-04 (3.16e-04)	Tok/s 17818 (22945)	Loss/tok 3.1227 (3.4032)	LR 1.250e-04
0: TRAIN [1][3190/5173]	Time 0.582 (0.624)	Data 1.26e-04 (3.15e-04)	Tok/s 18125 (22935)	Loss/tok 3.1845 (3.4028)	LR 1.250e-04
0: TRAIN [1][3200/5173]	Time 0.580 (0.624)	Data 1.29e-04 (3.15e-04)	Tok/s 18020 (22926)	Loss/tok 3.0985 (3.4024)	LR 1.250e-04
0: TRAIN [1][3210/5173]	Time 0.579 (0.624)	Data 1.22e-04 (3.14e-04)	Tok/s 17672 (22925)	Loss/tok 3.2204 (3.4022)	LR 1.250e-04
0: TRAIN [1][3220/5173]	Time 0.520 (0.624)	Data 1.22e-04 (3.13e-04)	Tok/s 10175 (22921)	Loss/tok 2.6767 (3.4020)	LR 6.250e-05
0: TRAIN [1][3230/5173]	Time 0.520 (0.624)	Data 1.21e-04 (3.13e-04)	Tok/s 10207 (22918)	Loss/tok 2.7236 (3.4018)	LR 6.250e-05
0: TRAIN [1][3240/5173]	Time 0.581 (0.624)	Data 1.23e-04 (3.12e-04)	Tok/s 17412 (22919)	Loss/tok 3.1583 (3.4021)	LR 6.250e-05
0: TRAIN [1][3250/5173]	Time 0.582 (0.624)	Data 1.26e-04 (3.12e-04)	Tok/s 17749 (22917)	Loss/tok 3.2333 (3.4020)	LR 6.250e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][3260/5173]	Time 0.782 (0.624)	Data 1.24e-04 (3.11e-04)	Tok/s 38221 (22920)	Loss/tok 3.6522 (3.4021)	LR 6.250e-05
0: TRAIN [1][3270/5173]	Time 0.784 (0.624)	Data 1.26e-04 (3.11e-04)	Tok/s 38003 (22920)	Loss/tok 3.6441 (3.4020)	LR 6.250e-05
0: TRAIN [1][3280/5173]	Time 0.523 (0.624)	Data 1.27e-04 (3.10e-04)	Tok/s 10128 (22912)	Loss/tok 2.7986 (3.4016)	LR 6.250e-05
0: TRAIN [1][3290/5173]	Time 0.581 (0.624)	Data 1.25e-04 (3.10e-04)	Tok/s 17398 (22901)	Loss/tok 3.1867 (3.4012)	LR 6.250e-05
0: TRAIN [1][3300/5173]	Time 0.582 (0.624)	Data 1.42e-04 (3.09e-04)	Tok/s 17608 (22892)	Loss/tok 3.0990 (3.4010)	LR 6.250e-05
0: TRAIN [1][3310/5173]	Time 0.581 (0.624)	Data 1.24e-04 (3.09e-04)	Tok/s 18044 (22894)	Loss/tok 3.1945 (3.4010)	LR 6.250e-05
0: TRAIN [1][3320/5173]	Time 0.581 (0.624)	Data 1.33e-04 (3.08e-04)	Tok/s 17481 (22903)	Loss/tok 3.1000 (3.4012)	LR 6.250e-05
0: TRAIN [1][3330/5173]	Time 0.581 (0.624)	Data 1.28e-04 (3.08e-04)	Tok/s 17521 (22909)	Loss/tok 3.0428 (3.4017)	LR 6.250e-05
0: TRAIN [1][3340/5173]	Time 0.582 (0.624)	Data 1.26e-04 (3.07e-04)	Tok/s 17661 (22906)	Loss/tok 3.2317 (3.4015)	LR 6.250e-05
0: TRAIN [1][3350/5173]	Time 0.584 (0.624)	Data 1.66e-04 (3.07e-04)	Tok/s 17482 (22894)	Loss/tok 3.0435 (3.4010)	LR 6.250e-05
0: TRAIN [1][3360/5173]	Time 0.580 (0.624)	Data 1.30e-04 (3.07e-04)	Tok/s 18202 (22884)	Loss/tok 3.0961 (3.4006)	LR 6.250e-05
0: TRAIN [1][3370/5173]	Time 0.582 (0.624)	Data 2.89e-04 (3.06e-04)	Tok/s 17967 (22873)	Loss/tok 3.1398 (3.4002)	LR 6.250e-05
0: TRAIN [1][3380/5173]	Time 0.583 (0.624)	Data 1.27e-04 (3.06e-04)	Tok/s 17755 (22870)	Loss/tok 3.0610 (3.4001)	LR 6.250e-05
0: TRAIN [1][3390/5173]	Time 0.520 (0.624)	Data 1.24e-04 (3.05e-04)	Tok/s 10154 (22862)	Loss/tok 2.6817 (3.3998)	LR 6.250e-05
0: TRAIN [1][3400/5173]	Time 0.643 (0.624)	Data 1.22e-04 (3.05e-04)	Tok/s 25936 (22857)	Loss/tok 3.2004 (3.3995)	LR 6.250e-05
0: TRAIN [1][3410/5173]	Time 0.584 (0.624)	Data 1.26e-04 (3.04e-04)	Tok/s 17604 (22851)	Loss/tok 3.1635 (3.3993)	LR 6.250e-05
0: TRAIN [1][3420/5173]	Time 0.585 (0.624)	Data 1.22e-04 (3.04e-04)	Tok/s 17563 (22848)	Loss/tok 3.0904 (3.3992)	LR 6.250e-05
0: TRAIN [1][3430/5173]	Time 0.581 (0.623)	Data 1.22e-04 (3.03e-04)	Tok/s 17804 (22842)	Loss/tok 3.1483 (3.3990)	LR 6.250e-05
0: TRAIN [1][3440/5173]	Time 0.581 (0.623)	Data 2.86e-04 (3.03e-04)	Tok/s 17829 (22835)	Loss/tok 3.0399 (3.3988)	LR 6.250e-05
0: TRAIN [1][3450/5173]	Time 0.648 (0.623)	Data 2.78e-04 (3.02e-04)	Tok/s 26225 (22836)	Loss/tok 3.4448 (3.3987)	LR 6.250e-05
0: TRAIN [1][3460/5173]	Time 0.584 (0.623)	Data 1.17e-04 (3.02e-04)	Tok/s 17639 (22832)	Loss/tok 3.1763 (3.3986)	LR 6.250e-05
0: TRAIN [1][3470/5173]	Time 0.706 (0.623)	Data 1.28e-04 (3.01e-04)	Tok/s 32930 (22836)	Loss/tok 3.5049 (3.3986)	LR 6.250e-05
0: TRAIN [1][3480/5173]	Time 0.579 (0.623)	Data 1.30e-04 (3.01e-04)	Tok/s 18013 (22841)	Loss/tok 3.1718 (3.3984)	LR 6.250e-05
0: TRAIN [1][3490/5173]	Time 0.523 (0.623)	Data 1.22e-04 (3.01e-04)	Tok/s 10057 (22829)	Loss/tok 2.7484 (3.3980)	LR 6.250e-05
0: TRAIN [1][3500/5173]	Time 0.582 (0.623)	Data 1.31e-04 (3.00e-04)	Tok/s 17722 (22822)	Loss/tok 3.1452 (3.3978)	LR 6.250e-05
0: TRAIN [1][3510/5173]	Time 0.706 (0.623)	Data 1.30e-04 (3.00e-04)	Tok/s 32959 (22821)	Loss/tok 3.6005 (3.3976)	LR 6.250e-05
0: TRAIN [1][3520/5173]	Time 0.640 (0.623)	Data 1.31e-04 (2.99e-04)	Tok/s 26041 (22829)	Loss/tok 3.4082 (3.3977)	LR 6.250e-05
0: TRAIN [1][3530/5173]	Time 0.708 (0.623)	Data 1.26e-04 (2.99e-04)	Tok/s 32902 (22819)	Loss/tok 3.5434 (3.3974)	LR 6.250e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][3540/5173]	Time 0.645 (0.623)	Data 1.22e-04 (2.98e-04)	Tok/s 26087 (22817)	Loss/tok 3.3844 (3.3976)	LR 6.250e-05
0: TRAIN [1][3550/5173]	Time 0.581 (0.623)	Data 1.50e-04 (2.98e-04)	Tok/s 17977 (22811)	Loss/tok 3.1257 (3.3975)	LR 6.250e-05
0: TRAIN [1][3560/5173]	Time 0.650 (0.623)	Data 1.13e-04 (2.97e-04)	Tok/s 25910 (22819)	Loss/tok 3.3337 (3.3976)	LR 6.250e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][3570/5173]	Time 0.643 (0.623)	Data 1.33e-04 (2.97e-04)	Tok/s 26265 (22813)	Loss/tok 3.4439 (3.3975)	LR 6.250e-05
0: TRAIN [1][3580/5173]	Time 0.644 (0.623)	Data 1.20e-04 (2.97e-04)	Tok/s 26122 (22812)	Loss/tok 3.3815 (3.3974)	LR 6.250e-05
0: TRAIN [1][3590/5173]	Time 0.780 (0.623)	Data 1.28e-04 (2.96e-04)	Tok/s 38141 (22818)	Loss/tok 3.7743 (3.3976)	LR 6.250e-05
0: TRAIN [1][3600/5173]	Time 0.584 (0.623)	Data 1.25e-04 (2.96e-04)	Tok/s 17942 (22819)	Loss/tok 3.1321 (3.3976)	LR 6.250e-05
0: TRAIN [1][3610/5173]	Time 0.648 (0.623)	Data 1.16e-04 (2.95e-04)	Tok/s 25905 (22811)	Loss/tok 3.2925 (3.3974)	LR 6.250e-05
0: TRAIN [1][3620/5173]	Time 0.582 (0.623)	Data 1.12e-04 (2.95e-04)	Tok/s 17676 (22804)	Loss/tok 3.2062 (3.3971)	LR 6.250e-05
0: TRAIN [1][3630/5173]	Time 0.645 (0.623)	Data 1.25e-04 (2.94e-04)	Tok/s 26372 (22800)	Loss/tok 3.3058 (3.3969)	LR 6.250e-05
0: TRAIN [1][3640/5173]	Time 0.644 (0.623)	Data 1.16e-04 (2.94e-04)	Tok/s 25920 (22786)	Loss/tok 3.3437 (3.3964)	LR 6.250e-05
0: TRAIN [1][3650/5173]	Time 0.646 (0.623)	Data 1.16e-04 (2.93e-04)	Tok/s 25895 (22786)	Loss/tok 3.4834 (3.3962)	LR 6.250e-05
0: TRAIN [1][3660/5173]	Time 0.641 (0.623)	Data 1.19e-04 (2.93e-04)	Tok/s 26010 (22785)	Loss/tok 3.4397 (3.3964)	LR 6.250e-05
0: TRAIN [1][3670/5173]	Time 0.582 (0.623)	Data 1.38e-04 (2.93e-04)	Tok/s 17777 (22793)	Loss/tok 3.2382 (3.3968)	LR 6.250e-05
0: TRAIN [1][3680/5173]	Time 0.702 (0.623)	Data 1.14e-04 (2.92e-04)	Tok/s 32422 (22792)	Loss/tok 3.5545 (3.3966)	LR 6.250e-05
0: TRAIN [1][3690/5173]	Time 0.644 (0.623)	Data 1.16e-04 (2.92e-04)	Tok/s 26403 (22788)	Loss/tok 3.4024 (3.3965)	LR 6.250e-05
0: TRAIN [1][3700/5173]	Time 0.582 (0.623)	Data 1.48e-04 (2.91e-04)	Tok/s 17453 (22792)	Loss/tok 3.1949 (3.3964)	LR 6.250e-05
0: TRAIN [1][3710/5173]	Time 0.581 (0.623)	Data 1.31e-04 (2.91e-04)	Tok/s 17718 (22785)	Loss/tok 3.1535 (3.3961)	LR 6.250e-05
0: TRAIN [1][3720/5173]	Time 0.520 (0.623)	Data 1.31e-04 (2.91e-04)	Tok/s 10299 (22780)	Loss/tok 2.7338 (3.3959)	LR 6.250e-05
0: TRAIN [1][3730/5173]	Time 0.706 (0.623)	Data 1.33e-04 (2.90e-04)	Tok/s 32766 (22788)	Loss/tok 3.5112 (3.3960)	LR 6.250e-05
0: TRAIN [1][3740/5173]	Time 0.708 (0.623)	Data 1.38e-04 (2.90e-04)	Tok/s 32774 (22800)	Loss/tok 3.7555 (3.3962)	LR 6.250e-05
0: TRAIN [1][3750/5173]	Time 0.649 (0.623)	Data 1.25e-04 (2.90e-04)	Tok/s 25884 (22797)	Loss/tok 3.3812 (3.3960)	LR 6.250e-05
0: TRAIN [1][3760/5173]	Time 0.582 (0.623)	Data 1.36e-04 (2.89e-04)	Tok/s 17617 (22804)	Loss/tok 3.0188 (3.3962)	LR 6.250e-05
0: TRAIN [1][3770/5173]	Time 0.648 (0.623)	Data 1.30e-04 (2.89e-04)	Tok/s 26127 (22803)	Loss/tok 3.2652 (3.3959)	LR 6.250e-05
0: TRAIN [1][3780/5173]	Time 0.645 (0.623)	Data 1.23e-04 (2.88e-04)	Tok/s 25949 (22795)	Loss/tok 3.3374 (3.3956)	LR 6.250e-05
0: TRAIN [1][3790/5173]	Time 0.583 (0.623)	Data 1.30e-04 (2.88e-04)	Tok/s 18317 (22788)	Loss/tok 3.1912 (3.3954)	LR 6.250e-05
0: TRAIN [1][3800/5173]	Time 0.640 (0.623)	Data 1.30e-04 (2.88e-04)	Tok/s 26326 (22789)	Loss/tok 3.4449 (3.3954)	LR 6.250e-05
0: TRAIN [1][3810/5173]	Time 0.640 (0.623)	Data 1.24e-04 (2.87e-04)	Tok/s 25975 (22792)	Loss/tok 3.3846 (3.3954)	LR 6.250e-05
0: TRAIN [1][3820/5173]	Time 0.647 (0.623)	Data 1.27e-04 (2.87e-04)	Tok/s 26264 (22794)	Loss/tok 3.2975 (3.3953)	LR 6.250e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][3830/5173]	Time 0.580 (0.623)	Data 1.31e-04 (2.86e-04)	Tok/s 17894 (22792)	Loss/tok 3.1130 (3.3951)	LR 6.250e-05
0: TRAIN [1][3840/5173]	Time 0.583 (0.623)	Data 1.37e-04 (2.86e-04)	Tok/s 17800 (22784)	Loss/tok 3.1262 (3.3949)	LR 6.250e-05
0: TRAIN [1][3850/5173]	Time 0.580 (0.623)	Data 1.38e-04 (2.86e-04)	Tok/s 17506 (22781)	Loss/tok 3.2233 (3.3946)	LR 6.250e-05
0: TRAIN [1][3860/5173]	Time 0.707 (0.623)	Data 1.46e-04 (2.85e-04)	Tok/s 33423 (22787)	Loss/tok 3.5656 (3.3948)	LR 6.250e-05
0: TRAIN [1][3870/5173]	Time 0.710 (0.623)	Data 1.24e-04 (2.85e-04)	Tok/s 32997 (22789)	Loss/tok 3.4822 (3.3947)	LR 6.250e-05
0: TRAIN [1][3880/5173]	Time 0.708 (0.623)	Data 1.29e-04 (2.85e-04)	Tok/s 33089 (22794)	Loss/tok 3.4506 (3.3947)	LR 6.250e-05
0: TRAIN [1][3890/5173]	Time 0.647 (0.623)	Data 1.27e-04 (2.84e-04)	Tok/s 25883 (22791)	Loss/tok 3.3216 (3.3944)	LR 6.250e-05
0: TRAIN [1][3900/5173]	Time 0.581 (0.623)	Data 1.23e-04 (2.84e-04)	Tok/s 17620 (22783)	Loss/tok 3.1637 (3.3941)	LR 6.250e-05
0: TRAIN [1][3910/5173]	Time 0.582 (0.623)	Data 1.28e-04 (2.83e-04)	Tok/s 17255 (22788)	Loss/tok 3.2182 (3.3941)	LR 6.250e-05
0: TRAIN [1][3920/5173]	Time 0.583 (0.623)	Data 1.26e-04 (2.83e-04)	Tok/s 17814 (22780)	Loss/tok 3.2516 (3.3938)	LR 6.250e-05
0: TRAIN [1][3930/5173]	Time 0.641 (0.623)	Data 1.27e-04 (2.83e-04)	Tok/s 26241 (22778)	Loss/tok 3.2261 (3.3936)	LR 6.250e-05
0: TRAIN [1][3940/5173]	Time 0.645 (0.623)	Data 1.24e-04 (2.82e-04)	Tok/s 25886 (22773)	Loss/tok 3.3855 (3.3932)	LR 6.250e-05
0: TRAIN [1][3950/5173]	Time 0.585 (0.623)	Data 1.39e-04 (2.82e-04)	Tok/s 17866 (22766)	Loss/tok 3.1222 (3.3931)	LR 6.250e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][3960/5173]	Time 0.520 (0.623)	Data 1.28e-04 (2.82e-04)	Tok/s 10351 (22765)	Loss/tok 2.6911 (3.3931)	LR 6.250e-05
0: TRAIN [1][3970/5173]	Time 0.782 (0.623)	Data 1.27e-04 (2.81e-04)	Tok/s 37850 (22775)	Loss/tok 3.6610 (3.3932)	LR 6.250e-05
0: TRAIN [1][3980/5173]	Time 0.641 (0.623)	Data 1.28e-04 (2.81e-04)	Tok/s 26067 (22774)	Loss/tok 3.3401 (3.3930)	LR 6.250e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][3990/5173]	Time 0.707 (0.623)	Data 1.28e-04 (2.81e-04)	Tok/s 32539 (22780)	Loss/tok 3.5082 (3.3931)	LR 6.250e-05
0: TRAIN [1][4000/5173]	Time 0.645 (0.623)	Data 1.29e-04 (2.80e-04)	Tok/s 26286 (22781)	Loss/tok 3.3460 (3.3933)	LR 6.250e-05
0: TRAIN [1][4010/5173]	Time 0.584 (0.623)	Data 3.18e-04 (2.80e-04)	Tok/s 17696 (22783)	Loss/tok 3.1351 (3.3931)	LR 6.250e-05
0: TRAIN [1][4020/5173]	Time 0.648 (0.623)	Data 1.29e-04 (2.80e-04)	Tok/s 25819 (22782)	Loss/tok 3.3740 (3.3930)	LR 6.250e-05
0: TRAIN [1][4030/5173]	Time 0.583 (0.623)	Data 1.24e-04 (2.79e-04)	Tok/s 17827 (22783)	Loss/tok 3.1222 (3.3929)	LR 3.125e-05
0: TRAIN [1][4040/5173]	Time 0.581 (0.623)	Data 1.24e-04 (2.79e-04)	Tok/s 18014 (22782)	Loss/tok 3.3005 (3.3929)	LR 3.125e-05
0: TRAIN [1][4050/5173]	Time 0.579 (0.623)	Data 1.40e-04 (2.79e-04)	Tok/s 18240 (22794)	Loss/tok 3.0947 (3.3932)	LR 3.125e-05
0: TRAIN [1][4060/5173]	Time 0.581 (0.623)	Data 1.24e-04 (2.78e-04)	Tok/s 17569 (22788)	Loss/tok 3.1335 (3.3930)	LR 3.125e-05
0: TRAIN [1][4070/5173]	Time 0.647 (0.623)	Data 1.82e-04 (2.78e-04)	Tok/s 25989 (22798)	Loss/tok 3.3840 (3.3933)	LR 3.125e-05
0: TRAIN [1][4080/5173]	Time 0.520 (0.623)	Data 1.66e-04 (2.78e-04)	Tok/s 9946 (22802)	Loss/tok 2.7458 (3.3936)	LR 3.125e-05
0: TRAIN [1][4090/5173]	Time 0.643 (0.623)	Data 1.26e-04 (2.77e-04)	Tok/s 26419 (22799)	Loss/tok 3.3782 (3.3934)	LR 3.125e-05
0: TRAIN [1][4100/5173]	Time 0.521 (0.623)	Data 1.27e-04 (2.77e-04)	Tok/s 10096 (22800)	Loss/tok 2.6879 (3.3934)	LR 3.125e-05
0: TRAIN [1][4110/5173]	Time 0.580 (0.623)	Data 1.30e-04 (2.77e-04)	Tok/s 17735 (22795)	Loss/tok 3.2028 (3.3932)	LR 3.125e-05
0: TRAIN [1][4120/5173]	Time 0.780 (0.623)	Data 1.31e-04 (2.77e-04)	Tok/s 38266 (22791)	Loss/tok 3.7106 (3.3930)	LR 3.125e-05
0: TRAIN [1][4130/5173]	Time 0.583 (0.623)	Data 1.21e-04 (2.76e-04)	Tok/s 17899 (22786)	Loss/tok 3.2022 (3.3929)	LR 3.125e-05
0: TRAIN [1][4140/5173]	Time 0.638 (0.623)	Data 1.23e-04 (2.76e-04)	Tok/s 26276 (22784)	Loss/tok 3.2949 (3.3929)	LR 3.125e-05
0: TRAIN [1][4150/5173]	Time 0.584 (0.623)	Data 1.25e-04 (2.76e-04)	Tok/s 17889 (22781)	Loss/tok 3.2009 (3.3928)	LR 3.125e-05
0: TRAIN [1][4160/5173]	Time 0.581 (0.623)	Data 1.19e-04 (2.75e-04)	Tok/s 17950 (22788)	Loss/tok 3.1994 (3.3930)	LR 3.125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][4170/5173]	Time 0.582 (0.623)	Data 1.26e-04 (2.75e-04)	Tok/s 17847 (22796)	Loss/tok 3.2066 (3.3931)	LR 3.125e-05
0: TRAIN [1][4180/5173]	Time 0.580 (0.623)	Data 1.24e-04 (2.75e-04)	Tok/s 17930 (22786)	Loss/tok 3.1467 (3.3929)	LR 3.125e-05
0: TRAIN [1][4190/5173]	Time 0.643 (0.623)	Data 1.25e-04 (2.74e-04)	Tok/s 25955 (22778)	Loss/tok 3.3851 (3.3926)	LR 3.125e-05
0: TRAIN [1][4200/5173]	Time 0.712 (0.623)	Data 1.26e-04 (2.74e-04)	Tok/s 33187 (22783)	Loss/tok 3.5081 (3.3926)	LR 3.125e-05
0: TRAIN [1][4210/5173]	Time 0.584 (0.623)	Data 1.23e-04 (2.74e-04)	Tok/s 17649 (22775)	Loss/tok 3.1662 (3.3924)	LR 3.125e-05
0: TRAIN [1][4220/5173]	Time 0.646 (0.623)	Data 1.19e-04 (2.74e-04)	Tok/s 26039 (22773)	Loss/tok 3.4272 (3.3922)	LR 3.125e-05
0: TRAIN [1][4230/5173]	Time 0.706 (0.623)	Data 1.31e-04 (2.73e-04)	Tok/s 32832 (22770)	Loss/tok 3.5503 (3.3920)	LR 3.125e-05
0: TRAIN [1][4240/5173]	Time 0.781 (0.623)	Data 1.29e-04 (2.73e-04)	Tok/s 38042 (22773)	Loss/tok 3.6758 (3.3919)	LR 3.125e-05
0: TRAIN [1][4250/5173]	Time 0.641 (0.623)	Data 1.22e-04 (2.73e-04)	Tok/s 26507 (22770)	Loss/tok 3.3563 (3.3918)	LR 3.125e-05
0: TRAIN [1][4260/5173]	Time 0.705 (0.623)	Data 1.36e-04 (2.72e-04)	Tok/s 33033 (22780)	Loss/tok 3.5660 (3.3920)	LR 3.125e-05
0: TRAIN [1][4270/5173]	Time 0.710 (0.623)	Data 1.41e-04 (2.72e-04)	Tok/s 32938 (22789)	Loss/tok 3.5264 (3.3921)	LR 3.125e-05
0: TRAIN [1][4280/5173]	Time 0.643 (0.623)	Data 1.21e-04 (2.72e-04)	Tok/s 26136 (22789)	Loss/tok 3.3265 (3.3922)	LR 3.125e-05
0: TRAIN [1][4290/5173]	Time 0.648 (0.623)	Data 1.26e-04 (2.71e-04)	Tok/s 26057 (22790)	Loss/tok 3.3151 (3.3922)	LR 3.125e-05
0: TRAIN [1][4300/5173]	Time 0.583 (0.623)	Data 1.25e-04 (2.71e-04)	Tok/s 17416 (22788)	Loss/tok 3.1433 (3.3922)	LR 3.125e-05
0: TRAIN [1][4310/5173]	Time 0.640 (0.623)	Data 1.24e-04 (2.71e-04)	Tok/s 26898 (22784)	Loss/tok 3.3183 (3.3920)	LR 3.125e-05
0: TRAIN [1][4320/5173]	Time 0.644 (0.623)	Data 1.53e-04 (2.70e-04)	Tok/s 25858 (22782)	Loss/tok 3.2711 (3.3918)	LR 3.125e-05
0: TRAIN [1][4330/5173]	Time 0.519 (0.623)	Data 1.57e-04 (2.70e-04)	Tok/s 10007 (22780)	Loss/tok 2.6725 (3.3916)	LR 3.125e-05
0: TRAIN [1][4340/5173]	Time 0.582 (0.623)	Data 1.49e-04 (2.70e-04)	Tok/s 17837 (22776)	Loss/tok 3.2671 (3.3914)	LR 3.125e-05
0: TRAIN [1][4350/5173]	Time 0.708 (0.623)	Data 1.38e-04 (2.70e-04)	Tok/s 32495 (22773)	Loss/tok 3.6459 (3.3913)	LR 3.125e-05
0: TRAIN [1][4360/5173]	Time 0.780 (0.623)	Data 1.36e-04 (2.69e-04)	Tok/s 38292 (22776)	Loss/tok 3.7926 (3.3913)	LR 3.125e-05
0: TRAIN [1][4370/5173]	Time 0.709 (0.623)	Data 1.47e-04 (2.69e-04)	Tok/s 33156 (22783)	Loss/tok 3.5138 (3.3915)	LR 3.125e-05
0: TRAIN [1][4380/5173]	Time 0.580 (0.623)	Data 1.20e-04 (2.69e-04)	Tok/s 18061 (22776)	Loss/tok 3.1596 (3.3912)	LR 3.125e-05
0: TRAIN [1][4390/5173]	Time 0.583 (0.623)	Data 1.28e-04 (2.68e-04)	Tok/s 17275 (22782)	Loss/tok 3.1927 (3.3912)	LR 3.125e-05
0: TRAIN [1][4400/5173]	Time 0.581 (0.623)	Data 1.29e-04 (2.68e-04)	Tok/s 17751 (22780)	Loss/tok 3.1314 (3.3911)	LR 3.125e-05
0: TRAIN [1][4410/5173]	Time 0.703 (0.623)	Data 1.24e-04 (2.68e-04)	Tok/s 33284 (22774)	Loss/tok 3.5683 (3.3909)	LR 3.125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][4420/5173]	Time 0.702 (0.623)	Data 1.29e-04 (2.68e-04)	Tok/s 33368 (22769)	Loss/tok 3.4908 (3.3908)	LR 3.125e-05
0: TRAIN [1][4430/5173]	Time 0.704 (0.623)	Data 1.27e-04 (2.67e-04)	Tok/s 33233 (22776)	Loss/tok 3.5543 (3.3909)	LR 3.125e-05
0: TRAIN [1][4440/5173]	Time 0.645 (0.623)	Data 7.87e-04 (2.67e-04)	Tok/s 26577 (22772)	Loss/tok 3.3796 (3.3908)	LR 3.125e-05
0: TRAIN [1][4450/5173]	Time 0.708 (0.623)	Data 1.31e-04 (2.67e-04)	Tok/s 32879 (22773)	Loss/tok 3.5963 (3.3907)	LR 3.125e-05
0: TRAIN [1][4460/5173]	Time 0.645 (0.623)	Data 1.28e-04 (2.67e-04)	Tok/s 26132 (22770)	Loss/tok 3.3933 (3.3906)	LR 3.125e-05
0: TRAIN [1][4470/5173]	Time 0.579 (0.623)	Data 1.32e-04 (2.66e-04)	Tok/s 17878 (22771)	Loss/tok 3.1241 (3.3906)	LR 3.125e-05
0: TRAIN [1][4480/5173]	Time 0.706 (0.623)	Data 1.33e-04 (2.66e-04)	Tok/s 32824 (22770)	Loss/tok 3.6189 (3.3905)	LR 3.125e-05
0: TRAIN [1][4490/5173]	Time 0.520 (0.623)	Data 2.95e-04 (2.66e-04)	Tok/s 10392 (22768)	Loss/tok 2.7248 (3.3904)	LR 3.125e-05
0: TRAIN [1][4500/5173]	Time 0.644 (0.623)	Data 1.24e-04 (2.66e-04)	Tok/s 26211 (22776)	Loss/tok 3.3205 (3.3908)	LR 3.125e-05
0: TRAIN [1][4510/5173]	Time 0.521 (0.623)	Data 1.20e-04 (2.65e-04)	Tok/s 9985 (22767)	Loss/tok 2.6737 (3.3905)	LR 3.125e-05
0: TRAIN [1][4520/5173]	Time 0.585 (0.623)	Data 1.25e-04 (2.65e-04)	Tok/s 17775 (22767)	Loss/tok 3.1344 (3.3906)	LR 3.125e-05
0: TRAIN [1][4530/5173]	Time 0.585 (0.623)	Data 3.09e-04 (2.65e-04)	Tok/s 17912 (22765)	Loss/tok 3.0836 (3.3903)	LR 3.125e-05
0: TRAIN [1][4540/5173]	Time 0.520 (0.623)	Data 1.20e-04 (2.65e-04)	Tok/s 10413 (22760)	Loss/tok 2.6138 (3.3902)	LR 3.125e-05
0: TRAIN [1][4550/5173]	Time 0.520 (0.623)	Data 1.33e-04 (2.64e-04)	Tok/s 9987 (22758)	Loss/tok 2.6858 (3.3900)	LR 3.125e-05
0: TRAIN [1][4560/5173]	Time 0.709 (0.623)	Data 1.27e-04 (2.64e-04)	Tok/s 32570 (22764)	Loss/tok 3.5245 (3.3901)	LR 3.125e-05
0: TRAIN [1][4570/5173]	Time 0.581 (0.623)	Data 1.19e-04 (2.64e-04)	Tok/s 17833 (22762)	Loss/tok 3.0485 (3.3898)	LR 3.125e-05
0: TRAIN [1][4580/5173]	Time 0.580 (0.623)	Data 1.30e-04 (2.64e-04)	Tok/s 17805 (22762)	Loss/tok 3.1401 (3.3897)	LR 3.125e-05
0: TRAIN [1][4590/5173]	Time 0.644 (0.623)	Data 1.54e-04 (2.63e-04)	Tok/s 26510 (22768)	Loss/tok 3.5080 (3.3899)	LR 3.125e-05
0: TRAIN [1][4600/5173]	Time 0.582 (0.623)	Data 1.30e-04 (2.63e-04)	Tok/s 17283 (22767)	Loss/tok 3.1194 (3.3899)	LR 3.125e-05
0: TRAIN [1][4610/5173]	Time 0.579 (0.623)	Data 1.29e-04 (2.63e-04)	Tok/s 17676 (22768)	Loss/tok 3.0934 (3.3898)	LR 3.125e-05
0: TRAIN [1][4620/5173]	Time 0.583 (0.623)	Data 1.16e-04 (2.63e-04)	Tok/s 17274 (22757)	Loss/tok 3.1758 (3.3895)	LR 3.125e-05
0: TRAIN [1][4630/5173]	Time 0.581 (0.623)	Data 2.85e-04 (2.62e-04)	Tok/s 17250 (22751)	Loss/tok 3.1318 (3.3892)	LR 3.125e-05
0: TRAIN [1][4640/5173]	Time 0.642 (0.623)	Data 1.25e-04 (2.62e-04)	Tok/s 26578 (22754)	Loss/tok 3.4428 (3.3892)	LR 3.125e-05
0: TRAIN [1][4650/5173]	Time 0.522 (0.623)	Data 1.32e-04 (2.62e-04)	Tok/s 10133 (22747)	Loss/tok 2.6454 (3.3890)	LR 3.125e-05
0: TRAIN [1][4660/5173]	Time 0.649 (0.623)	Data 1.29e-04 (2.62e-04)	Tok/s 25833 (22747)	Loss/tok 3.3512 (3.3888)	LR 3.125e-05
0: TRAIN [1][4670/5173]	Time 0.642 (0.623)	Data 1.17e-04 (2.61e-04)	Tok/s 25877 (22748)	Loss/tok 3.4286 (3.3887)	LR 3.125e-05
0: TRAIN [1][4680/5173]	Time 0.709 (0.623)	Data 3.07e-04 (2.61e-04)	Tok/s 33247 (22753)	Loss/tok 3.5346 (3.3887)	LR 3.125e-05
0: TRAIN [1][4690/5173]	Time 0.580 (0.623)	Data 1.22e-04 (2.61e-04)	Tok/s 17645 (22750)	Loss/tok 3.1421 (3.3886)	LR 3.125e-05
0: TRAIN [1][4700/5173]	Time 0.521 (0.623)	Data 1.18e-04 (2.61e-04)	Tok/s 10146 (22750)	Loss/tok 2.7920 (3.3884)	LR 3.125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][4710/5173]	Time 0.644 (0.623)	Data 1.26e-04 (2.60e-04)	Tok/s 26337 (22755)	Loss/tok 3.3549 (3.3884)	LR 3.125e-05
0: TRAIN [1][4720/5173]	Time 0.585 (0.623)	Data 2.66e-04 (2.60e-04)	Tok/s 17704 (22758)	Loss/tok 3.1129 (3.3884)	LR 3.125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][4730/5173]	Time 0.704 (0.623)	Data 1.18e-04 (2.60e-04)	Tok/s 33263 (22768)	Loss/tok 3.5346 (3.3886)	LR 3.125e-05
0: TRAIN [1][4740/5173]	Time 0.646 (0.623)	Data 1.30e-04 (2.60e-04)	Tok/s 25730 (22774)	Loss/tok 3.3325 (3.3887)	LR 3.125e-05
0: TRAIN [1][4750/5173]	Time 0.580 (0.623)	Data 1.22e-04 (2.59e-04)	Tok/s 17510 (22771)	Loss/tok 3.1740 (3.3886)	LR 3.125e-05
0: TRAIN [1][4760/5173]	Time 0.583 (0.623)	Data 1.19e-04 (2.59e-04)	Tok/s 18257 (22770)	Loss/tok 3.1454 (3.3886)	LR 3.125e-05
0: TRAIN [1][4770/5173]	Time 0.777 (0.623)	Data 1.29e-04 (2.59e-04)	Tok/s 37941 (22769)	Loss/tok 3.7920 (3.3886)	LR 3.125e-05
0: TRAIN [1][4780/5173]	Time 0.645 (0.623)	Data 1.23e-04 (2.59e-04)	Tok/s 25829 (22772)	Loss/tok 3.3697 (3.3886)	LR 3.125e-05
0: TRAIN [1][4790/5173]	Time 0.583 (0.623)	Data 1.22e-04 (2.58e-04)	Tok/s 17728 (22776)	Loss/tok 3.1684 (3.3886)	LR 3.125e-05
0: TRAIN [1][4800/5173]	Time 0.786 (0.623)	Data 2.86e-04 (2.58e-04)	Tok/s 37429 (22779)	Loss/tok 3.7412 (3.3886)	LR 3.125e-05
0: TRAIN [1][4810/5173]	Time 0.703 (0.623)	Data 1.25e-04 (2.58e-04)	Tok/s 33255 (22779)	Loss/tok 3.5323 (3.3887)	LR 3.125e-05
0: TRAIN [1][4820/5173]	Time 0.643 (0.623)	Data 1.30e-04 (2.58e-04)	Tok/s 26373 (22775)	Loss/tok 3.3257 (3.3885)	LR 3.125e-05
0: TRAIN [1][4830/5173]	Time 0.583 (0.623)	Data 1.22e-04 (2.58e-04)	Tok/s 17476 (22765)	Loss/tok 3.1867 (3.3882)	LR 3.125e-05
0: TRAIN [1][4840/5173]	Time 0.582 (0.623)	Data 1.27e-04 (2.57e-04)	Tok/s 17709 (22763)	Loss/tok 3.1119 (3.3880)	LR 3.125e-05
0: TRAIN [1][4850/5173]	Time 0.704 (0.623)	Data 3.06e-04 (2.57e-04)	Tok/s 33039 (22764)	Loss/tok 3.4165 (3.3880)	LR 1.563e-05
0: TRAIN [1][4860/5173]	Time 0.703 (0.623)	Data 1.22e-04 (2.57e-04)	Tok/s 33116 (22766)	Loss/tok 3.6181 (3.3881)	LR 1.563e-05
0: TRAIN [1][4870/5173]	Time 0.639 (0.623)	Data 1.28e-04 (2.57e-04)	Tok/s 26025 (22764)	Loss/tok 3.3051 (3.3879)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][4880/5173]	Time 0.584 (0.623)	Data 1.23e-04 (2.57e-04)	Tok/s 17595 (22772)	Loss/tok 3.1848 (3.3880)	LR 1.563e-05
0: TRAIN [1][4890/5173]	Time 0.524 (0.623)	Data 1.25e-04 (2.56e-04)	Tok/s 10232 (22776)	Loss/tok 2.7252 (3.3881)	LR 1.563e-05
0: TRAIN [1][4900/5173]	Time 0.521 (0.623)	Data 1.23e-04 (2.56e-04)	Tok/s 10295 (22772)	Loss/tok 2.8054 (3.3880)	LR 1.563e-05
0: TRAIN [1][4910/5173]	Time 0.583 (0.623)	Data 1.19e-04 (2.56e-04)	Tok/s 17476 (22767)	Loss/tok 3.0711 (3.3878)	LR 1.563e-05
0: TRAIN [1][4920/5173]	Time 0.584 (0.623)	Data 3.01e-04 (2.56e-04)	Tok/s 17512 (22767)	Loss/tok 3.2755 (3.3879)	LR 1.563e-05
0: TRAIN [1][4930/5173]	Time 0.640 (0.623)	Data 1.34e-04 (2.56e-04)	Tok/s 25797 (22766)	Loss/tok 3.4545 (3.3877)	LR 1.563e-05
0: TRAIN [1][4940/5173]	Time 0.641 (0.623)	Data 1.24e-04 (2.55e-04)	Tok/s 26133 (22759)	Loss/tok 3.3181 (3.3875)	LR 1.563e-05
0: TRAIN [1][4950/5173]	Time 0.583 (0.623)	Data 1.22e-04 (2.55e-04)	Tok/s 17630 (22758)	Loss/tok 3.3200 (3.3876)	LR 1.563e-05
0: TRAIN [1][4960/5173]	Time 0.580 (0.623)	Data 1.28e-04 (2.55e-04)	Tok/s 17506 (22753)	Loss/tok 3.2165 (3.3874)	LR 1.563e-05
0: TRAIN [1][4970/5173]	Time 0.644 (0.623)	Data 1.21e-04 (2.55e-04)	Tok/s 26593 (22756)	Loss/tok 3.2754 (3.3874)	LR 1.563e-05
0: TRAIN [1][4980/5173]	Time 0.580 (0.623)	Data 1.20e-04 (2.54e-04)	Tok/s 17699 (22758)	Loss/tok 3.1302 (3.3875)	LR 1.563e-05
0: TRAIN [1][4990/5173]	Time 0.518 (0.623)	Data 1.23e-04 (2.54e-04)	Tok/s 10239 (22755)	Loss/tok 2.7468 (3.3876)	LR 1.563e-05
0: TRAIN [1][5000/5173]	Time 0.583 (0.623)	Data 3.11e-04 (2.54e-04)	Tok/s 17555 (22749)	Loss/tok 3.1974 (3.3874)	LR 1.563e-05
0: TRAIN [1][5010/5173]	Time 0.584 (0.623)	Data 2.86e-04 (2.54e-04)	Tok/s 17621 (22749)	Loss/tok 3.1242 (3.3874)	LR 1.563e-05
0: TRAIN [1][5020/5173]	Time 0.582 (0.623)	Data 1.29e-04 (2.53e-04)	Tok/s 17468 (22749)	Loss/tok 3.0062 (3.3872)	LR 1.563e-05
0: TRAIN [1][5030/5173]	Time 0.579 (0.623)	Data 1.24e-04 (2.53e-04)	Tok/s 17806 (22746)	Loss/tok 3.2243 (3.3871)	LR 1.563e-05
0: TRAIN [1][5040/5173]	Time 0.643 (0.623)	Data 1.18e-04 (2.53e-04)	Tok/s 25985 (22746)	Loss/tok 3.2872 (3.3871)	LR 1.563e-05
0: TRAIN [1][5050/5173]	Time 0.521 (0.623)	Data 1.20e-04 (2.53e-04)	Tok/s 10278 (22746)	Loss/tok 2.7079 (3.3871)	LR 1.563e-05
0: TRAIN [1][5060/5173]	Time 0.707 (0.623)	Data 1.20e-04 (2.53e-04)	Tok/s 32849 (22746)	Loss/tok 3.5758 (3.3870)	LR 1.563e-05
0: TRAIN [1][5070/5173]	Time 0.705 (0.623)	Data 1.22e-04 (2.52e-04)	Tok/s 33064 (22748)	Loss/tok 3.5391 (3.3871)	LR 1.563e-05
0: TRAIN [1][5080/5173]	Time 0.584 (0.623)	Data 1.27e-04 (2.52e-04)	Tok/s 17596 (22743)	Loss/tok 3.0647 (3.3869)	LR 1.563e-05
0: TRAIN [1][5090/5173]	Time 0.647 (0.623)	Data 1.35e-04 (2.52e-04)	Tok/s 26077 (22746)	Loss/tok 3.2589 (3.3868)	LR 1.563e-05
0: TRAIN [1][5100/5173]	Time 0.644 (0.623)	Data 1.31e-04 (2.52e-04)	Tok/s 25878 (22740)	Loss/tok 3.4144 (3.3866)	LR 1.563e-05
0: TRAIN [1][5110/5173]	Time 0.706 (0.623)	Data 1.24e-04 (2.52e-04)	Tok/s 33067 (22745)	Loss/tok 3.5405 (3.3867)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][5120/5173]	Time 0.643 (0.623)	Data 1.26e-04 (2.51e-04)	Tok/s 25973 (22747)	Loss/tok 3.3640 (3.3868)	LR 1.563e-05
0: TRAIN [1][5130/5173]	Time 0.582 (0.623)	Data 1.25e-04 (2.51e-04)	Tok/s 18038 (22741)	Loss/tok 3.1241 (3.3866)	LR 1.563e-05
0: TRAIN [1][5140/5173]	Time 0.704 (0.623)	Data 1.22e-04 (2.51e-04)	Tok/s 33109 (22742)	Loss/tok 3.5853 (3.3865)	LR 1.563e-05
0: TRAIN [1][5150/5173]	Time 0.521 (0.623)	Data 1.27e-04 (2.51e-04)	Tok/s 10178 (22734)	Loss/tok 2.6000 (3.3862)	LR 1.563e-05
0: TRAIN [1][5160/5173]	Time 0.582 (0.622)	Data 1.29e-04 (2.51e-04)	Tok/s 17557 (22731)	Loss/tok 3.1760 (3.3860)	LR 1.563e-05
0: TRAIN [1][5170/5173]	Time 0.640 (0.622)	Data 2.93e-04 (2.50e-04)	Tok/s 26338 (22722)	Loss/tok 3.3397 (3.3858)	LR 1.563e-05
:::MLL 1586124319.259 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 525}}
:::MLL 1586124319.260 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [1][0/8]	Time 0.698 (0.698)	Decoder iters 132.0 (132.0)	Tok/s 23503 (23503)
0: Running moses detokenizer
0: BLEU(score=21.214438077602296, counts=[35234, 16578, 8993, 5075], totals=[64829, 61826, 58823, 55824], precisions=[54.34913387527187, 26.8139617636593, 15.288237594138348, 9.091071940384063], bp=1.0, sys_len=64829, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1586124325.035 eval_accuracy: {"value": 21.21, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 536}}
:::MLL 1586124325.036 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 1	Training Loss: 3.3864	Test BLEU: 21.21
0: Performance: Epoch: 1	Training: 68167 Tok/s
0: Finished epoch 1
:::MLL 1586124325.036 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 558}}
:::MLL 1586124325.036 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1586124325.037 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 515}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 3138780563
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][0/5173]	Time 1.191 (1.191)	Data 5.08e-01 (5.08e-01)	Tok/s 14153 (14153)	Loss/tok 3.3732 (3.3732)	LR 1.563e-05
0: TRAIN [2][10/5173]	Time 0.583 (0.696)	Data 1.72e-04 (4.64e-02)	Tok/s 17638 (24326)	Loss/tok 2.9461 (3.3862)	LR 1.563e-05
0: TRAIN [2][20/5173]	Time 0.579 (0.653)	Data 1.80e-04 (2.44e-02)	Tok/s 17717 (22811)	Loss/tok 3.2736 (3.3449)	LR 1.563e-05
0: TRAIN [2][30/5173]	Time 0.643 (0.648)	Data 2.04e-04 (1.66e-02)	Tok/s 26238 (23485)	Loss/tok 3.2775 (3.3443)	LR 1.563e-05
0: TRAIN [2][40/5173]	Time 0.585 (0.646)	Data 3.11e-04 (1.26e-02)	Tok/s 17801 (23822)	Loss/tok 3.0098 (3.3574)	LR 1.563e-05
0: TRAIN [2][50/5173]	Time 0.582 (0.636)	Data 1.36e-04 (1.02e-02)	Tok/s 17731 (22953)	Loss/tok 3.2610 (3.3352)	LR 1.563e-05
0: TRAIN [2][60/5173]	Time 0.641 (0.637)	Data 1.43e-04 (8.51e-03)	Tok/s 26522 (23409)	Loss/tok 3.3941 (3.3432)	LR 1.563e-05
0: TRAIN [2][70/5173]	Time 0.584 (0.633)	Data 3.11e-04 (7.34e-03)	Tok/s 18029 (23056)	Loss/tok 3.2033 (3.3382)	LR 1.563e-05
0: TRAIN [2][80/5173]	Time 0.785 (0.637)	Data 1.39e-04 (6.45e-03)	Tok/s 38140 (23675)	Loss/tok 3.5999 (3.3602)	LR 1.563e-05
0: TRAIN [2][90/5173]	Time 0.583 (0.633)	Data 1.54e-04 (5.76e-03)	Tok/s 17618 (23215)	Loss/tok 3.1445 (3.3515)	LR 1.563e-05
0: TRAIN [2][100/5173]	Time 0.639 (0.629)	Data 1.42e-04 (5.20e-03)	Tok/s 26358 (22915)	Loss/tok 3.3150 (3.3417)	LR 1.563e-05
0: TRAIN [2][110/5173]	Time 0.644 (0.631)	Data 1.33e-04 (4.75e-03)	Tok/s 25920 (23115)	Loss/tok 3.3441 (3.3472)	LR 1.563e-05
0: TRAIN [2][120/5173]	Time 0.646 (0.631)	Data 1.31e-04 (4.37e-03)	Tok/s 25631 (23143)	Loss/tok 3.3715 (3.3438)	LR 1.563e-05
0: TRAIN [2][130/5173]	Time 0.584 (0.630)	Data 3.20e-04 (4.05e-03)	Tok/s 17780 (23091)	Loss/tok 3.0836 (3.3399)	LR 1.563e-05
0: TRAIN [2][140/5173]	Time 0.644 (0.630)	Data 1.53e-04 (3.77e-03)	Tok/s 25653 (23199)	Loss/tok 3.3480 (3.3437)	LR 1.563e-05
0: TRAIN [2][150/5173]	Time 0.587 (0.628)	Data 1.36e-04 (3.53e-03)	Tok/s 17482 (22956)	Loss/tok 3.2501 (3.3376)	LR 1.563e-05
0: TRAIN [2][160/5173]	Time 0.707 (0.629)	Data 1.39e-04 (3.32e-03)	Tok/s 32941 (23063)	Loss/tok 3.5063 (3.3414)	LR 1.563e-05
0: TRAIN [2][170/5173]	Time 0.582 (0.629)	Data 3.17e-04 (3.14e-03)	Tok/s 17748 (23119)	Loss/tok 3.2605 (3.3435)	LR 1.563e-05
0: TRAIN [2][180/5173]	Time 0.582 (0.629)	Data 1.60e-04 (2.97e-03)	Tok/s 18167 (23185)	Loss/tok 3.1309 (3.3474)	LR 1.563e-05
0: TRAIN [2][190/5173]	Time 0.706 (0.628)	Data 1.44e-04 (2.83e-03)	Tok/s 32772 (22985)	Loss/tok 3.3624 (3.3423)	LR 1.563e-05
0: TRAIN [2][200/5173]	Time 0.584 (0.628)	Data 1.27e-04 (2.69e-03)	Tok/s 17950 (23048)	Loss/tok 3.1339 (3.3413)	LR 1.563e-05
0: TRAIN [2][210/5173]	Time 0.648 (0.627)	Data 1.22e-04 (2.57e-03)	Tok/s 25811 (22994)	Loss/tok 3.4013 (3.3386)	LR 1.563e-05
0: TRAIN [2][220/5173]	Time 0.583 (0.626)	Data 1.37e-04 (2.46e-03)	Tok/s 17679 (22837)	Loss/tok 3.2289 (3.3354)	LR 1.563e-05
0: TRAIN [2][230/5173]	Time 0.778 (0.628)	Data 1.33e-04 (2.36e-03)	Tok/s 38073 (23076)	Loss/tok 3.8323 (3.3445)	LR 1.563e-05
0: TRAIN [2][240/5173]	Time 0.782 (0.629)	Data 1.40e-04 (2.27e-03)	Tok/s 38165 (23195)	Loss/tok 3.7347 (3.3499)	LR 1.563e-05
0: TRAIN [2][250/5173]	Time 0.582 (0.628)	Data 1.33e-04 (2.19e-03)	Tok/s 17895 (23072)	Loss/tok 3.0561 (3.3453)	LR 1.563e-05
0: TRAIN [2][260/5173]	Time 0.582 (0.627)	Data 1.27e-04 (2.11e-03)	Tok/s 17461 (23050)	Loss/tok 3.2387 (3.3451)	LR 1.563e-05
0: TRAIN [2][270/5173]	Time 0.649 (0.627)	Data 1.29e-04 (2.04e-03)	Tok/s 26145 (23080)	Loss/tok 3.3782 (3.3459)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][280/5173]	Time 0.644 (0.627)	Data 1.36e-04 (1.97e-03)	Tok/s 26003 (23048)	Loss/tok 3.4047 (3.3469)	LR 1.563e-05
0: TRAIN [2][290/5173]	Time 0.584 (0.626)	Data 1.31e-04 (1.91e-03)	Tok/s 17819 (22924)	Loss/tok 3.0834 (3.3439)	LR 1.563e-05
0: TRAIN [2][300/5173]	Time 0.643 (0.625)	Data 1.33e-04 (1.85e-03)	Tok/s 25891 (22861)	Loss/tok 3.2953 (3.3427)	LR 1.563e-05
0: TRAIN [2][310/5173]	Time 0.780 (0.627)	Data 1.51e-04 (1.80e-03)	Tok/s 37601 (23077)	Loss/tok 3.7243 (3.3504)	LR 1.563e-05
0: TRAIN [2][320/5173]	Time 0.520 (0.627)	Data 1.34e-04 (1.74e-03)	Tok/s 10305 (22997)	Loss/tok 2.7038 (3.3503)	LR 1.563e-05
0: TRAIN [2][330/5173]	Time 0.579 (0.626)	Data 1.24e-04 (1.69e-03)	Tok/s 17779 (22952)	Loss/tok 3.0976 (3.3486)	LR 1.563e-05
0: TRAIN [2][340/5173]	Time 0.583 (0.626)	Data 1.28e-04 (1.65e-03)	Tok/s 18103 (22922)	Loss/tok 3.1803 (3.3473)	LR 1.563e-05
0: TRAIN [2][350/5173]	Time 0.582 (0.625)	Data 1.79e-04 (1.61e-03)	Tok/s 17870 (22798)	Loss/tok 3.0181 (3.3431)	LR 1.563e-05
0: TRAIN [2][360/5173]	Time 0.642 (0.624)	Data 1.61e-04 (1.57e-03)	Tok/s 26217 (22749)	Loss/tok 3.3930 (3.3414)	LR 1.563e-05
0: TRAIN [2][370/5173]	Time 0.644 (0.624)	Data 1.79e-04 (1.53e-03)	Tok/s 26100 (22737)	Loss/tok 3.2916 (3.3414)	LR 1.563e-05
0: TRAIN [2][380/5173]	Time 0.708 (0.625)	Data 1.70e-04 (1.50e-03)	Tok/s 32829 (22870)	Loss/tok 3.4742 (3.3448)	LR 1.563e-05
0: TRAIN [2][390/5173]	Time 0.582 (0.626)	Data 1.63e-04 (1.46e-03)	Tok/s 17772 (22931)	Loss/tok 3.1515 (3.3466)	LR 1.563e-05
0: TRAIN [2][400/5173]	Time 0.582 (0.625)	Data 1.67e-04 (1.43e-03)	Tok/s 17501 (22902)	Loss/tok 3.2259 (3.3442)	LR 1.563e-05
0: TRAIN [2][410/5173]	Time 0.643 (0.625)	Data 3.64e-04 (1.40e-03)	Tok/s 26273 (22822)	Loss/tok 3.3275 (3.3407)	LR 1.563e-05
0: TRAIN [2][420/5173]	Time 0.643 (0.625)	Data 1.65e-04 (1.37e-03)	Tok/s 26340 (22890)	Loss/tok 3.3654 (3.3415)	LR 1.563e-05
0: TRAIN [2][430/5173]	Time 0.583 (0.626)	Data 1.70e-04 (1.34e-03)	Tok/s 17587 (22963)	Loss/tok 3.1506 (3.3437)	LR 1.563e-05
0: TRAIN [2][440/5173]	Time 0.581 (0.625)	Data 1.76e-04 (1.32e-03)	Tok/s 17971 (22879)	Loss/tok 3.1524 (3.3413)	LR 1.563e-05
0: TRAIN [2][450/5173]	Time 0.584 (0.625)	Data 1.72e-04 (1.29e-03)	Tok/s 17925 (22866)	Loss/tok 3.0624 (3.3409)	LR 1.563e-05
0: TRAIN [2][460/5173]	Time 0.583 (0.624)	Data 1.65e-04 (1.27e-03)	Tok/s 17693 (22789)	Loss/tok 3.1429 (3.3384)	LR 1.563e-05
0: TRAIN [2][470/5173]	Time 0.646 (0.624)	Data 1.67e-04 (1.25e-03)	Tok/s 25690 (22699)	Loss/tok 3.3555 (3.3358)	LR 1.563e-05
0: TRAIN [2][480/5173]	Time 0.779 (0.624)	Data 1.68e-04 (1.23e-03)	Tok/s 38024 (22753)	Loss/tok 3.6726 (3.3412)	LR 1.563e-05
0: TRAIN [2][490/5173]	Time 0.582 (0.624)	Data 1.68e-04 (1.20e-03)	Tok/s 17624 (22682)	Loss/tok 3.0867 (3.3393)	LR 1.563e-05
0: TRAIN [2][500/5173]	Time 0.587 (0.623)	Data 1.67e-04 (1.18e-03)	Tok/s 17721 (22636)	Loss/tok 3.1609 (3.3370)	LR 1.563e-05
0: TRAIN [2][510/5173]	Time 0.581 (0.623)	Data 1.62e-04 (1.16e-03)	Tok/s 17695 (22658)	Loss/tok 3.1067 (3.3382)	LR 1.563e-05
0: TRAIN [2][520/5173]	Time 0.520 (0.623)	Data 1.65e-04 (1.15e-03)	Tok/s 10245 (22634)	Loss/tok 2.6428 (3.3380)	LR 1.563e-05
0: TRAIN [2][530/5173]	Time 0.579 (0.623)	Data 1.73e-04 (1.13e-03)	Tok/s 17480 (22584)	Loss/tok 3.2160 (3.3372)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][540/5173]	Time 0.640 (0.623)	Data 3.65e-04 (1.11e-03)	Tok/s 25818 (22614)	Loss/tok 3.4298 (3.3388)	LR 1.563e-05
0: TRAIN [2][550/5173]	Time 0.641 (0.623)	Data 3.40e-04 (1.09e-03)	Tok/s 26266 (22571)	Loss/tok 3.3900 (3.3380)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][560/5173]	Time 0.641 (0.623)	Data 1.67e-04 (1.08e-03)	Tok/s 26422 (22605)	Loss/tok 3.2564 (3.3389)	LR 1.563e-05
0: TRAIN [2][570/5173]	Time 0.778 (0.623)	Data 1.23e-04 (1.06e-03)	Tok/s 39299 (22668)	Loss/tok 3.6198 (3.3396)	LR 1.563e-05
0: TRAIN [2][580/5173]	Time 0.525 (0.623)	Data 1.33e-04 (1.05e-03)	Tok/s 10146 (22654)	Loss/tok 2.6636 (3.3386)	LR 1.563e-05
0: TRAIN [2][590/5173]	Time 0.584 (0.623)	Data 1.36e-04 (1.03e-03)	Tok/s 17989 (22683)	Loss/tok 3.1819 (3.3389)	LR 1.563e-05
0: TRAIN [2][600/5173]	Time 0.581 (0.623)	Data 1.22e-04 (1.02e-03)	Tok/s 17916 (22608)	Loss/tok 3.1488 (3.3376)	LR 1.563e-05
0: TRAIN [2][610/5173]	Time 0.584 (0.622)	Data 1.22e-04 (1.00e-03)	Tok/s 17575 (22543)	Loss/tok 3.1362 (3.3357)	LR 1.563e-05
0: TRAIN [2][620/5173]	Time 0.707 (0.623)	Data 1.35e-04 (9.88e-04)	Tok/s 33130 (22587)	Loss/tok 3.5068 (3.3362)	LR 1.563e-05
0: TRAIN [2][630/5173]	Time 0.648 (0.623)	Data 1.31e-04 (9.75e-04)	Tok/s 26047 (22679)	Loss/tok 3.4079 (3.3390)	LR 1.563e-05
0: TRAIN [2][640/5173]	Time 0.648 (0.623)	Data 1.21e-04 (9.62e-04)	Tok/s 25811 (22667)	Loss/tok 3.2591 (3.3380)	LR 1.563e-05
0: TRAIN [2][650/5173]	Time 0.523 (0.623)	Data 1.25e-04 (9.50e-04)	Tok/s 9965 (22677)	Loss/tok 2.7391 (3.3374)	LR 1.563e-05
0: TRAIN [2][660/5173]	Time 0.581 (0.623)	Data 1.22e-04 (9.37e-04)	Tok/s 17793 (22683)	Loss/tok 3.0687 (3.3377)	LR 1.563e-05
0: TRAIN [2][670/5173]	Time 0.519 (0.623)	Data 1.27e-04 (9.25e-04)	Tok/s 10277 (22682)	Loss/tok 2.7297 (3.3372)	LR 1.563e-05
0: TRAIN [2][680/5173]	Time 0.649 (0.623)	Data 1.24e-04 (9.14e-04)	Tok/s 26095 (22700)	Loss/tok 3.2320 (3.3373)	LR 1.563e-05
0: TRAIN [2][690/5173]	Time 0.640 (0.624)	Data 1.39e-04 (9.03e-04)	Tok/s 26336 (22766)	Loss/tok 3.4540 (3.3402)	LR 1.563e-05
0: TRAIN [2][700/5173]	Time 0.709 (0.624)	Data 1.19e-04 (8.92e-04)	Tok/s 32780 (22776)	Loss/tok 3.4801 (3.3400)	LR 1.563e-05
0: TRAIN [2][710/5173]	Time 0.644 (0.624)	Data 1.17e-04 (8.81e-04)	Tok/s 25693 (22767)	Loss/tok 3.3848 (3.3404)	LR 1.563e-05
0: TRAIN [2][720/5173]	Time 0.776 (0.625)	Data 1.24e-04 (8.71e-04)	Tok/s 38371 (22845)	Loss/tok 3.5944 (3.3425)	LR 1.563e-05
0: TRAIN [2][730/5173]	Time 0.584 (0.625)	Data 2.67e-04 (8.61e-04)	Tok/s 17991 (22831)	Loss/tok 3.0276 (3.3413)	LR 1.563e-05
0: TRAIN [2][740/5173]	Time 0.639 (0.625)	Data 1.20e-04 (8.51e-04)	Tok/s 26179 (22852)	Loss/tok 3.4289 (3.3414)	LR 1.563e-05
0: TRAIN [2][750/5173]	Time 0.645 (0.625)	Data 1.24e-04 (8.41e-04)	Tok/s 25929 (22875)	Loss/tok 3.4549 (3.3426)	LR 1.563e-05
0: TRAIN [2][760/5173]	Time 0.584 (0.624)	Data 1.21e-04 (8.32e-04)	Tok/s 17673 (22839)	Loss/tok 3.1546 (3.3415)	LR 1.563e-05
0: TRAIN [2][770/5173]	Time 0.643 (0.625)	Data 1.21e-04 (8.24e-04)	Tok/s 25889 (22854)	Loss/tok 3.4018 (3.3416)	LR 1.563e-05
0: TRAIN [2][780/5173]	Time 0.583 (0.625)	Data 1.27e-04 (8.15e-04)	Tok/s 17943 (22907)	Loss/tok 3.1366 (3.3432)	LR 1.563e-05
0: TRAIN [2][790/5173]	Time 0.584 (0.625)	Data 1.28e-04 (8.07e-04)	Tok/s 17719 (22939)	Loss/tok 3.1403 (3.3442)	LR 1.563e-05
0: TRAIN [2][800/5173]	Time 0.648 (0.625)	Data 1.27e-04 (7.98e-04)	Tok/s 26051 (22935)	Loss/tok 3.2795 (3.3436)	LR 1.563e-05
0: TRAIN [2][810/5173]	Time 0.582 (0.625)	Data 1.37e-04 (7.90e-04)	Tok/s 17479 (22921)	Loss/tok 3.1017 (3.3430)	LR 1.563e-05
0: TRAIN [2][820/5173]	Time 0.520 (0.624)	Data 1.23e-04 (7.83e-04)	Tok/s 10048 (22848)	Loss/tok 2.6601 (3.3412)	LR 1.563e-05
0: TRAIN [2][830/5173]	Time 0.580 (0.624)	Data 1.65e-04 (7.76e-04)	Tok/s 17881 (22847)	Loss/tok 3.1638 (3.3409)	LR 1.563e-05
0: TRAIN [2][840/5173]	Time 0.645 (0.624)	Data 1.33e-04 (7.68e-04)	Tok/s 26472 (22836)	Loss/tok 3.3413 (3.3399)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][850/5173]	Time 0.642 (0.625)	Data 1.37e-04 (7.61e-04)	Tok/s 26130 (22896)	Loss/tok 3.4507 (3.3422)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][860/5173]	Time 0.582 (0.625)	Data 1.24e-04 (7.54e-04)	Tok/s 18111 (22879)	Loss/tok 3.0807 (3.3416)	LR 1.563e-05
0: TRAIN [2][870/5173]	Time 0.648 (0.625)	Data 1.36e-04 (7.46e-04)	Tok/s 25895 (22895)	Loss/tok 3.2961 (3.3420)	LR 1.563e-05
0: TRAIN [2][880/5173]	Time 0.583 (0.625)	Data 1.20e-04 (7.39e-04)	Tok/s 17915 (22861)	Loss/tok 3.1283 (3.3414)	LR 1.563e-05
0: TRAIN [2][890/5173]	Time 0.581 (0.625)	Data 1.20e-04 (7.33e-04)	Tok/s 17750 (22874)	Loss/tok 3.1418 (3.3415)	LR 1.563e-05
0: TRAIN [2][900/5173]	Time 0.710 (0.625)	Data 2.67e-04 (7.26e-04)	Tok/s 33157 (22888)	Loss/tok 3.4187 (3.3414)	LR 1.563e-05
0: TRAIN [2][910/5173]	Time 0.706 (0.625)	Data 1.25e-04 (7.20e-04)	Tok/s 33210 (22866)	Loss/tok 3.4167 (3.3407)	LR 1.563e-05
0: TRAIN [2][920/5173]	Time 0.520 (0.625)	Data 1.20e-04 (7.13e-04)	Tok/s 9952 (22873)	Loss/tok 2.6591 (3.3417)	LR 1.563e-05
0: TRAIN [2][930/5173]	Time 0.580 (0.625)	Data 1.39e-04 (7.07e-04)	Tok/s 18367 (22909)	Loss/tok 3.1217 (3.3424)	LR 1.563e-05
0: TRAIN [2][940/5173]	Time 0.644 (0.625)	Data 1.22e-04 (7.01e-04)	Tok/s 25980 (22923)	Loss/tok 3.3489 (3.3422)	LR 1.563e-05
0: TRAIN [2][950/5173]	Time 0.583 (0.625)	Data 1.19e-04 (6.95e-04)	Tok/s 17398 (22871)	Loss/tok 3.1374 (3.3406)	LR 1.563e-05
0: TRAIN [2][960/5173]	Time 0.581 (0.625)	Data 1.25e-04 (6.89e-04)	Tok/s 17635 (22868)	Loss/tok 3.2877 (3.3406)	LR 1.563e-05
0: TRAIN [2][970/5173]	Time 0.521 (0.624)	Data 2.84e-04 (6.84e-04)	Tok/s 10431 (22809)	Loss/tok 2.6410 (3.3388)	LR 1.563e-05
0: TRAIN [2][980/5173]	Time 0.642 (0.624)	Data 1.25e-04 (6.79e-04)	Tok/s 26296 (22791)	Loss/tok 3.3892 (3.3381)	LR 1.563e-05
0: TRAIN [2][990/5173]	Time 0.583 (0.624)	Data 1.23e-04 (6.73e-04)	Tok/s 17956 (22818)	Loss/tok 3.0963 (3.3391)	LR 1.563e-05
0: TRAIN [2][1000/5173]	Time 0.584 (0.624)	Data 1.24e-04 (6.68e-04)	Tok/s 17896 (22813)	Loss/tok 3.0947 (3.3391)	LR 1.563e-05
0: TRAIN [2][1010/5173]	Time 0.639 (0.624)	Data 1.27e-04 (6.63e-04)	Tok/s 26565 (22788)	Loss/tok 3.3820 (3.3380)	LR 1.563e-05
0: TRAIN [2][1020/5173]	Time 0.704 (0.624)	Data 1.24e-04 (6.58e-04)	Tok/s 32844 (22791)	Loss/tok 3.4541 (3.3376)	LR 1.563e-05
0: TRAIN [2][1030/5173]	Time 0.583 (0.624)	Data 1.22e-04 (6.53e-04)	Tok/s 17769 (22775)	Loss/tok 3.1696 (3.3370)	LR 1.563e-05
0: TRAIN [2][1040/5173]	Time 0.619 (0.623)	Data 1.22e-04 (6.47e-04)	Tok/s 26732 (22756)	Loss/tok 3.4287 (3.3370)	LR 1.563e-05
0: TRAIN [2][1050/5173]	Time 0.649 (0.624)	Data 1.37e-04 (6.43e-04)	Tok/s 25829 (22787)	Loss/tok 3.4750 (3.3384)	LR 1.563e-05
0: TRAIN [2][1060/5173]	Time 0.521 (0.624)	Data 1.29e-04 (6.38e-04)	Tok/s 10216 (22775)	Loss/tok 2.7250 (3.3390)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1070/5173]	Time 0.580 (0.624)	Data 1.36e-04 (6.34e-04)	Tok/s 18085 (22785)	Loss/tok 3.0715 (3.3392)	LR 1.563e-05
0: TRAIN [2][1080/5173]	Time 0.649 (0.624)	Data 1.23e-04 (6.29e-04)	Tok/s 25762 (22780)	Loss/tok 3.3909 (3.3390)	LR 1.563e-05
0: TRAIN [2][1090/5173]	Time 0.584 (0.624)	Data 1.24e-04 (6.25e-04)	Tok/s 17488 (22756)	Loss/tok 3.1678 (3.3383)	LR 1.563e-05
0: TRAIN [2][1100/5173]	Time 0.582 (0.624)	Data 1.27e-04 (6.21e-04)	Tok/s 17951 (22773)	Loss/tok 3.2082 (3.3384)	LR 1.563e-05
0: TRAIN [2][1110/5173]	Time 0.580 (0.624)	Data 1.24e-04 (6.16e-04)	Tok/s 17770 (22779)	Loss/tok 3.0707 (3.3380)	LR 1.563e-05
0: TRAIN [2][1120/5173]	Time 0.584 (0.624)	Data 1.28e-04 (6.12e-04)	Tok/s 17648 (22785)	Loss/tok 3.1607 (3.3380)	LR 1.563e-05
0: TRAIN [2][1130/5173]	Time 0.704 (0.624)	Data 1.21e-04 (6.08e-04)	Tok/s 33299 (22807)	Loss/tok 3.6575 (3.3386)	LR 1.563e-05
0: TRAIN [2][1140/5173]	Time 0.643 (0.624)	Data 3.01e-04 (6.04e-04)	Tok/s 26317 (22812)	Loss/tok 3.2214 (3.3385)	LR 1.563e-05
0: TRAIN [2][1150/5173]	Time 0.644 (0.624)	Data 1.22e-04 (5.99e-04)	Tok/s 26271 (22808)	Loss/tok 3.3297 (3.3384)	LR 1.563e-05
0: TRAIN [2][1160/5173]	Time 0.646 (0.624)	Data 1.24e-04 (5.95e-04)	Tok/s 25854 (22821)	Loss/tok 3.3732 (3.3386)	LR 1.563e-05
0: TRAIN [2][1170/5173]	Time 0.646 (0.624)	Data 1.22e-04 (5.91e-04)	Tok/s 25968 (22819)	Loss/tok 3.3532 (3.3386)	LR 1.563e-05
0: TRAIN [2][1180/5173]	Time 0.582 (0.624)	Data 1.24e-04 (5.87e-04)	Tok/s 17427 (22822)	Loss/tok 3.1516 (3.3389)	LR 1.563e-05
0: TRAIN [2][1190/5173]	Time 0.647 (0.624)	Data 1.31e-04 (5.84e-04)	Tok/s 26252 (22832)	Loss/tok 3.2695 (3.3388)	LR 1.563e-05
0: TRAIN [2][1200/5173]	Time 0.585 (0.624)	Data 1.25e-04 (5.80e-04)	Tok/s 17117 (22823)	Loss/tok 3.0850 (3.3377)	LR 1.563e-05
0: TRAIN [2][1210/5173]	Time 0.644 (0.624)	Data 3.47e-04 (5.76e-04)	Tok/s 26315 (22822)	Loss/tok 3.3404 (3.3375)	LR 1.563e-05
0: TRAIN [2][1220/5173]	Time 0.709 (0.624)	Data 1.29e-04 (5.73e-04)	Tok/s 33310 (22832)	Loss/tok 3.4685 (3.3377)	LR 1.563e-05
0: TRAIN [2][1230/5173]	Time 0.642 (0.624)	Data 1.28e-04 (5.69e-04)	Tok/s 25676 (22835)	Loss/tok 3.4654 (3.3378)	LR 1.563e-05
0: TRAIN [2][1240/5173]	Time 0.580 (0.624)	Data 2.94e-04 (5.66e-04)	Tok/s 17734 (22799)	Loss/tok 3.1508 (3.3376)	LR 1.563e-05
0: TRAIN [2][1250/5173]	Time 0.581 (0.624)	Data 1.28e-04 (5.63e-04)	Tok/s 17875 (22796)	Loss/tok 3.1243 (3.3373)	LR 1.563e-05
0: TRAIN [2][1260/5173]	Time 0.583 (0.623)	Data 1.18e-04 (5.60e-04)	Tok/s 17660 (22776)	Loss/tok 3.1526 (3.3368)	LR 1.563e-05
0: TRAIN [2][1270/5173]	Time 0.709 (0.623)	Data 1.21e-04 (5.57e-04)	Tok/s 32601 (22779)	Loss/tok 3.5587 (3.3369)	LR 1.563e-05
0: TRAIN [2][1280/5173]	Time 0.644 (0.624)	Data 3.01e-04 (5.54e-04)	Tok/s 25798 (22802)	Loss/tok 3.2978 (3.3381)	LR 1.563e-05
0: TRAIN [2][1290/5173]	Time 0.585 (0.624)	Data 1.70e-04 (5.51e-04)	Tok/s 17791 (22795)	Loss/tok 3.2160 (3.3378)	LR 1.563e-05
0: TRAIN [2][1300/5173]	Time 0.643 (0.624)	Data 1.52e-04 (5.48e-04)	Tok/s 25614 (22781)	Loss/tok 3.3400 (3.3375)	LR 1.563e-05
0: TRAIN [2][1310/5173]	Time 0.579 (0.624)	Data 1.23e-04 (5.45e-04)	Tok/s 17608 (22792)	Loss/tok 3.1356 (3.3372)	LR 1.563e-05
0: TRAIN [2][1320/5173]	Time 0.581 (0.623)	Data 1.23e-04 (5.41e-04)	Tok/s 17453 (22760)	Loss/tok 3.0297 (3.3362)	LR 1.563e-05
0: TRAIN [2][1330/5173]	Time 0.583 (0.623)	Data 1.25e-04 (5.39e-04)	Tok/s 17607 (22754)	Loss/tok 3.2655 (3.3362)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1340/5173]	Time 0.645 (0.623)	Data 1.21e-04 (5.36e-04)	Tok/s 25772 (22764)	Loss/tok 3.3093 (3.3363)	LR 1.563e-05
0: TRAIN [2][1350/5173]	Time 0.581 (0.623)	Data 1.27e-04 (5.33e-04)	Tok/s 17658 (22750)	Loss/tok 3.0730 (3.3361)	LR 1.563e-05
0: TRAIN [2][1360/5173]	Time 0.646 (0.623)	Data 2.70e-04 (5.30e-04)	Tok/s 26142 (22742)	Loss/tok 3.5052 (3.3360)	LR 1.563e-05
0: TRAIN [2][1370/5173]	Time 0.522 (0.623)	Data 1.25e-04 (5.27e-04)	Tok/s 10073 (22722)	Loss/tok 2.8282 (3.3360)	LR 1.563e-05
0: TRAIN [2][1380/5173]	Time 0.778 (0.623)	Data 1.26e-04 (5.25e-04)	Tok/s 38007 (22712)	Loss/tok 3.5934 (3.3359)	LR 1.563e-05
0: TRAIN [2][1390/5173]	Time 0.582 (0.623)	Data 1.20e-04 (5.22e-04)	Tok/s 17513 (22711)	Loss/tok 3.1374 (3.3359)	LR 1.563e-05
0: TRAIN [2][1400/5173]	Time 0.582 (0.623)	Data 1.22e-04 (5.19e-04)	Tok/s 17407 (22705)	Loss/tok 3.1011 (3.3354)	LR 1.563e-05
0: TRAIN [2][1410/5173]	Time 0.706 (0.623)	Data 1.26e-04 (5.16e-04)	Tok/s 32790 (22715)	Loss/tok 3.5538 (3.3354)	LR 1.563e-05
0: TRAIN [2][1420/5173]	Time 0.585 (0.623)	Data 2.69e-04 (5.13e-04)	Tok/s 17768 (22690)	Loss/tok 3.0454 (3.3349)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1430/5173]	Time 0.583 (0.623)	Data 1.16e-04 (5.11e-04)	Tok/s 17752 (22681)	Loss/tok 3.1025 (3.3347)	LR 1.563e-05
0: TRAIN [2][1440/5173]	Time 0.643 (0.623)	Data 2.70e-04 (5.08e-04)	Tok/s 26372 (22659)	Loss/tok 3.3099 (3.3342)	LR 1.563e-05
0: TRAIN [2][1450/5173]	Time 0.583 (0.622)	Data 1.21e-04 (5.06e-04)	Tok/s 17542 (22648)	Loss/tok 3.0720 (3.3334)	LR 1.563e-05
0: TRAIN [2][1460/5173]	Time 0.581 (0.623)	Data 1.22e-04 (5.03e-04)	Tok/s 17493 (22667)	Loss/tok 3.0772 (3.3339)	LR 1.563e-05
0: TRAIN [2][1470/5173]	Time 0.781 (0.623)	Data 1.26e-04 (5.01e-04)	Tok/s 38180 (22669)	Loss/tok 3.6476 (3.3342)	LR 1.563e-05
0: TRAIN [2][1480/5173]	Time 0.705 (0.623)	Data 1.25e-04 (4.98e-04)	Tok/s 33213 (22684)	Loss/tok 3.4645 (3.3345)	LR 1.563e-05
0: TRAIN [2][1490/5173]	Time 0.647 (0.623)	Data 1.21e-04 (4.96e-04)	Tok/s 26262 (22684)	Loss/tok 3.3693 (3.3345)	LR 1.563e-05
0: TRAIN [2][1500/5173]	Time 0.520 (0.622)	Data 1.20e-04 (4.94e-04)	Tok/s 10138 (22651)	Loss/tok 2.6783 (3.3339)	LR 1.563e-05
0: TRAIN [2][1510/5173]	Time 0.582 (0.623)	Data 1.23e-04 (4.91e-04)	Tok/s 17834 (22674)	Loss/tok 3.1893 (3.3346)	LR 1.563e-05
0: TRAIN [2][1520/5173]	Time 0.581 (0.623)	Data 1.21e-04 (4.89e-04)	Tok/s 17970 (22672)	Loss/tok 3.1513 (3.3347)	LR 1.563e-05
0: TRAIN [2][1530/5173]	Time 0.779 (0.623)	Data 1.32e-04 (4.86e-04)	Tok/s 37899 (22669)	Loss/tok 3.7235 (3.3347)	LR 1.563e-05
0: TRAIN [2][1540/5173]	Time 0.520 (0.623)	Data 1.21e-04 (4.84e-04)	Tok/s 10125 (22664)	Loss/tok 2.6710 (3.3354)	LR 1.563e-05
0: TRAIN [2][1550/5173]	Time 0.779 (0.623)	Data 1.39e-04 (4.82e-04)	Tok/s 37927 (22680)	Loss/tok 3.7435 (3.3361)	LR 1.563e-05
0: TRAIN [2][1560/5173]	Time 0.521 (0.623)	Data 1.37e-04 (4.80e-04)	Tok/s 10335 (22679)	Loss/tok 2.6172 (3.3361)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1570/5173]	Time 0.522 (0.623)	Data 1.26e-04 (4.78e-04)	Tok/s 9994 (22671)	Loss/tok 2.7281 (3.3361)	LR 1.563e-05
0: TRAIN [2][1580/5173]	Time 0.582 (0.623)	Data 1.19e-04 (4.76e-04)	Tok/s 17813 (22675)	Loss/tok 3.2122 (3.3360)	LR 1.563e-05
0: TRAIN [2][1590/5173]	Time 0.645 (0.623)	Data 1.18e-04 (4.73e-04)	Tok/s 25786 (22660)	Loss/tok 3.2617 (3.3356)	LR 1.563e-05
0: TRAIN [2][1600/5173]	Time 0.650 (0.622)	Data 1.23e-04 (4.71e-04)	Tok/s 25895 (22636)	Loss/tok 3.4160 (3.3348)	LR 1.563e-05
0: TRAIN [2][1610/5173]	Time 0.579 (0.622)	Data 1.21e-04 (4.69e-04)	Tok/s 17701 (22636)	Loss/tok 3.1915 (3.3346)	LR 1.563e-05
0: TRAIN [2][1620/5173]	Time 0.584 (0.622)	Data 1.19e-04 (4.68e-04)	Tok/s 17779 (22616)	Loss/tok 3.1761 (3.3337)	LR 1.563e-05
0: TRAIN [2][1630/5173]	Time 0.582 (0.622)	Data 1.22e-04 (4.66e-04)	Tok/s 17706 (22621)	Loss/tok 3.0023 (3.3341)	LR 1.563e-05
0: TRAIN [2][1640/5173]	Time 0.581 (0.622)	Data 2.77e-04 (4.64e-04)	Tok/s 17588 (22587)	Loss/tok 3.1747 (3.3332)	LR 1.563e-05
0: TRAIN [2][1650/5173]	Time 0.643 (0.622)	Data 1.26e-04 (4.62e-04)	Tok/s 26032 (22600)	Loss/tok 3.2940 (3.3340)	LR 1.563e-05
0: TRAIN [2][1660/5173]	Time 0.582 (0.622)	Data 1.21e-04 (4.60e-04)	Tok/s 17635 (22574)	Loss/tok 3.1766 (3.3334)	LR 1.563e-05
0: TRAIN [2][1670/5173]	Time 0.647 (0.622)	Data 1.26e-04 (4.58e-04)	Tok/s 25943 (22560)	Loss/tok 3.3196 (3.3330)	LR 1.563e-05
0: TRAIN [2][1680/5173]	Time 0.521 (0.622)	Data 1.19e-04 (4.56e-04)	Tok/s 10145 (22578)	Loss/tok 2.6679 (3.3331)	LR 1.563e-05
0: TRAIN [2][1690/5173]	Time 0.520 (0.622)	Data 1.25e-04 (4.54e-04)	Tok/s 10205 (22607)	Loss/tok 2.6441 (3.3340)	LR 1.563e-05
0: TRAIN [2][1700/5173]	Time 0.645 (0.622)	Data 1.21e-04 (4.52e-04)	Tok/s 25949 (22608)	Loss/tok 3.4273 (3.3339)	LR 1.563e-05
0: TRAIN [2][1710/5173]	Time 0.582 (0.622)	Data 1.21e-04 (4.51e-04)	Tok/s 17976 (22601)	Loss/tok 3.1247 (3.3340)	LR 1.563e-05
0: TRAIN [2][1720/5173]	Time 0.642 (0.622)	Data 1.20e-04 (4.49e-04)	Tok/s 26324 (22615)	Loss/tok 3.3403 (3.3342)	LR 1.563e-05
0: TRAIN [2][1730/5173]	Time 0.583 (0.622)	Data 1.23e-04 (4.47e-04)	Tok/s 17036 (22626)	Loss/tok 3.1410 (3.3349)	LR 1.563e-05
0: TRAIN [2][1740/5173]	Time 0.580 (0.622)	Data 1.23e-04 (4.45e-04)	Tok/s 18032 (22616)	Loss/tok 3.1591 (3.3344)	LR 1.563e-05
0: TRAIN [2][1750/5173]	Time 0.641 (0.622)	Data 1.23e-04 (4.43e-04)	Tok/s 26191 (22589)	Loss/tok 3.4308 (3.3339)	LR 1.563e-05
0: TRAIN [2][1760/5173]	Time 0.645 (0.622)	Data 1.21e-04 (4.42e-04)	Tok/s 25989 (22594)	Loss/tok 3.4382 (3.3341)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1770/5173]	Time 0.701 (0.622)	Data 1.34e-04 (4.40e-04)	Tok/s 33292 (22619)	Loss/tok 3.4999 (3.3352)	LR 1.563e-05
0: TRAIN [2][1780/5173]	Time 0.522 (0.622)	Data 1.21e-04 (4.38e-04)	Tok/s 10181 (22619)	Loss/tok 2.7423 (3.3350)	LR 1.563e-05
0: TRAIN [2][1790/5173]	Time 0.582 (0.622)	Data 1.24e-04 (4.37e-04)	Tok/s 17395 (22610)	Loss/tok 3.1466 (3.3349)	LR 1.563e-05
0: TRAIN [2][1800/5173]	Time 0.644 (0.622)	Data 1.25e-04 (4.35e-04)	Tok/s 25738 (22629)	Loss/tok 3.3926 (3.3357)	LR 1.563e-05
0: TRAIN [2][1810/5173]	Time 0.642 (0.622)	Data 1.24e-04 (4.33e-04)	Tok/s 26677 (22621)	Loss/tok 3.4164 (3.3353)	LR 1.563e-05
0: TRAIN [2][1820/5173]	Time 0.779 (0.622)	Data 1.33e-04 (4.32e-04)	Tok/s 38616 (22612)	Loss/tok 3.5901 (3.3349)	LR 1.563e-05
0: TRAIN [2][1830/5173]	Time 0.584 (0.622)	Data 1.24e-04 (4.31e-04)	Tok/s 17364 (22613)	Loss/tok 3.1115 (3.3352)	LR 1.563e-05
0: TRAIN [2][1840/5173]	Time 0.706 (0.622)	Data 1.26e-04 (4.29e-04)	Tok/s 33252 (22629)	Loss/tok 3.4452 (3.3352)	LR 1.563e-05
0: TRAIN [2][1850/5173]	Time 0.521 (0.622)	Data 1.25e-04 (4.27e-04)	Tok/s 9774 (22613)	Loss/tok 2.6546 (3.3351)	LR 1.563e-05
0: TRAIN [2][1860/5173]	Time 0.581 (0.622)	Data 1.32e-04 (4.26e-04)	Tok/s 17733 (22599)	Loss/tok 3.1156 (3.3349)	LR 1.563e-05
0: TRAIN [2][1870/5173]	Time 0.582 (0.622)	Data 1.19e-04 (4.24e-04)	Tok/s 17490 (22590)	Loss/tok 3.1410 (3.3347)	LR 1.563e-05
0: TRAIN [2][1880/5173]	Time 0.645 (0.622)	Data 1.23e-04 (4.23e-04)	Tok/s 25970 (22597)	Loss/tok 3.4494 (3.3348)	LR 1.563e-05
0: TRAIN [2][1890/5173]	Time 0.703 (0.622)	Data 1.26e-04 (4.21e-04)	Tok/s 33051 (22616)	Loss/tok 3.4886 (3.3358)	LR 1.563e-05
0: TRAIN [2][1900/5173]	Time 0.521 (0.622)	Data 1.28e-04 (4.20e-04)	Tok/s 10019 (22634)	Loss/tok 2.6036 (3.3365)	LR 1.563e-05
0: TRAIN [2][1910/5173]	Time 0.644 (0.622)	Data 1.29e-04 (4.19e-04)	Tok/s 26076 (22644)	Loss/tok 3.2611 (3.3370)	LR 1.563e-05
0: TRAIN [2][1920/5173]	Time 0.519 (0.622)	Data 1.25e-04 (4.17e-04)	Tok/s 9977 (22640)	Loss/tok 2.6266 (3.3366)	LR 1.563e-05
0: TRAIN [2][1930/5173]	Time 0.579 (0.622)	Data 1.23e-04 (4.16e-04)	Tok/s 18156 (22639)	Loss/tok 3.0378 (3.3366)	LR 1.563e-05
0: TRAIN [2][1940/5173]	Time 0.581 (0.622)	Data 1.21e-04 (4.14e-04)	Tok/s 17822 (22619)	Loss/tok 3.2542 (3.3361)	LR 1.563e-05
0: TRAIN [2][1950/5173]	Time 0.648 (0.622)	Data 1.23e-04 (4.13e-04)	Tok/s 25973 (22610)	Loss/tok 3.3605 (3.3360)	LR 1.563e-05
0: TRAIN [2][1960/5173]	Time 0.778 (0.622)	Data 1.24e-04 (4.12e-04)	Tok/s 38137 (22613)	Loss/tok 3.6518 (3.3359)	LR 1.563e-05
0: TRAIN [2][1970/5173]	Time 0.643 (0.622)	Data 1.17e-04 (4.10e-04)	Tok/s 26413 (22612)	Loss/tok 3.3579 (3.3357)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1980/5173]	Time 0.641 (0.622)	Data 2.89e-04 (4.09e-04)	Tok/s 26086 (22621)	Loss/tok 3.2880 (3.3362)	LR 1.563e-05
0: TRAIN [2][1990/5173]	Time 0.584 (0.622)	Data 1.24e-04 (4.08e-04)	Tok/s 17722 (22620)	Loss/tok 3.0454 (3.3363)	LR 1.563e-05
0: TRAIN [2][2000/5173]	Time 0.647 (0.622)	Data 1.24e-04 (4.06e-04)	Tok/s 25956 (22619)	Loss/tok 3.2322 (3.3362)	LR 1.563e-05
0: TRAIN [2][2010/5173]	Time 0.645 (0.622)	Data 1.23e-04 (4.05e-04)	Tok/s 26686 (22631)	Loss/tok 3.1658 (3.3362)	LR 1.563e-05
0: TRAIN [2][2020/5173]	Time 0.521 (0.622)	Data 1.22e-04 (4.03e-04)	Tok/s 10306 (22627)	Loss/tok 2.6937 (3.3361)	LR 1.563e-05
0: TRAIN [2][2030/5173]	Time 0.581 (0.622)	Data 1.20e-04 (4.02e-04)	Tok/s 17723 (22617)	Loss/tok 3.2124 (3.3362)	LR 1.563e-05
0: TRAIN [2][2040/5173]	Time 0.522 (0.622)	Data 1.26e-04 (4.01e-04)	Tok/s 10150 (22629)	Loss/tok 2.6257 (3.3367)	LR 1.563e-05
0: TRAIN [2][2050/5173]	Time 0.645 (0.622)	Data 1.31e-04 (3.99e-04)	Tok/s 25992 (22623)	Loss/tok 3.3990 (3.3368)	LR 1.563e-05
0: TRAIN [2][2060/5173]	Time 0.582 (0.622)	Data 1.18e-04 (3.98e-04)	Tok/s 17652 (22626)	Loss/tok 3.2047 (3.3365)	LR 1.563e-05
0: TRAIN [2][2070/5173]	Time 0.583 (0.622)	Data 2.71e-04 (3.97e-04)	Tok/s 17751 (22618)	Loss/tok 3.1691 (3.3363)	LR 1.563e-05
0: TRAIN [2][2080/5173]	Time 0.582 (0.622)	Data 1.16e-04 (3.96e-04)	Tok/s 17917 (22603)	Loss/tok 3.1196 (3.3359)	LR 1.563e-05
0: TRAIN [2][2090/5173]	Time 0.582 (0.622)	Data 1.23e-04 (3.95e-04)	Tok/s 18063 (22602)	Loss/tok 3.1626 (3.3361)	LR 1.563e-05
0: TRAIN [2][2100/5173]	Time 0.647 (0.622)	Data 1.21e-04 (3.94e-04)	Tok/s 25739 (22604)	Loss/tok 3.4462 (3.3363)	LR 1.563e-05
0: TRAIN [2][2110/5173]	Time 0.704 (0.622)	Data 2.73e-04 (3.93e-04)	Tok/s 33628 (22614)	Loss/tok 3.4448 (3.3364)	LR 1.563e-05
0: TRAIN [2][2120/5173]	Time 0.641 (0.622)	Data 1.21e-04 (3.91e-04)	Tok/s 26061 (22614)	Loss/tok 3.3867 (3.3361)	LR 1.563e-05
0: TRAIN [2][2130/5173]	Time 0.781 (0.622)	Data 3.12e-04 (3.90e-04)	Tok/s 38311 (22614)	Loss/tok 3.7227 (3.3362)	LR 1.563e-05
0: TRAIN [2][2140/5173]	Time 0.580 (0.622)	Data 3.07e-04 (3.89e-04)	Tok/s 17725 (22622)	Loss/tok 3.1661 (3.3367)	LR 1.563e-05
0: TRAIN [2][2150/5173]	Time 0.645 (0.622)	Data 1.21e-04 (3.88e-04)	Tok/s 26266 (22641)	Loss/tok 3.3022 (3.3370)	LR 1.563e-05
0: TRAIN [2][2160/5173]	Time 0.707 (0.622)	Data 1.27e-04 (3.87e-04)	Tok/s 33314 (22641)	Loss/tok 3.4890 (3.3369)	LR 1.563e-05
0: TRAIN [2][2170/5173]	Time 0.586 (0.622)	Data 1.26e-04 (3.86e-04)	Tok/s 17680 (22644)	Loss/tok 3.1324 (3.3367)	LR 1.563e-05
0: TRAIN [2][2180/5173]	Time 0.649 (0.622)	Data 1.24e-04 (3.85e-04)	Tok/s 25913 (22661)	Loss/tok 3.3180 (3.3370)	LR 1.563e-05
0: TRAIN [2][2190/5173]	Time 0.582 (0.622)	Data 1.27e-04 (3.84e-04)	Tok/s 17760 (22670)	Loss/tok 3.2474 (3.3371)	LR 1.563e-05
0: TRAIN [2][2200/5173]	Time 0.585 (0.623)	Data 1.25e-04 (3.83e-04)	Tok/s 17966 (22673)	Loss/tok 3.0385 (3.3369)	LR 1.563e-05
0: TRAIN [2][2210/5173]	Time 0.583 (0.623)	Data 1.22e-04 (3.81e-04)	Tok/s 17238 (22677)	Loss/tok 3.0780 (3.3372)	LR 1.563e-05
0: TRAIN [2][2220/5173]	Time 0.584 (0.622)	Data 1.19e-04 (3.80e-04)	Tok/s 17710 (22663)	Loss/tok 3.1557 (3.3368)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][2230/5173]	Time 0.582 (0.622)	Data 1.22e-04 (3.79e-04)	Tok/s 17372 (22661)	Loss/tok 3.1069 (3.3368)	LR 1.563e-05
0: TRAIN [2][2240/5173]	Time 0.650 (0.623)	Data 1.23e-04 (3.78e-04)	Tok/s 25420 (22676)	Loss/tok 3.4165 (3.3372)	LR 1.563e-05
0: TRAIN [2][2250/5173]	Time 0.582 (0.623)	Data 1.17e-04 (3.77e-04)	Tok/s 17833 (22686)	Loss/tok 3.2032 (3.3378)	LR 1.563e-05
0: TRAIN [2][2260/5173]	Time 0.579 (0.623)	Data 1.46e-04 (3.76e-04)	Tok/s 17927 (22696)	Loss/tok 3.2129 (3.3378)	LR 1.563e-05
0: TRAIN [2][2270/5173]	Time 0.647 (0.623)	Data 1.29e-04 (3.75e-04)	Tok/s 26166 (22692)	Loss/tok 3.4607 (3.3378)	LR 1.563e-05
0: TRAIN [2][2280/5173]	Time 0.707 (0.623)	Data 1.28e-04 (3.74e-04)	Tok/s 33474 (22704)	Loss/tok 3.3978 (3.3380)	LR 1.563e-05
0: TRAIN [2][2290/5173]	Time 0.646 (0.623)	Data 1.41e-04 (3.73e-04)	Tok/s 25669 (22720)	Loss/tok 3.3725 (3.3384)	LR 1.563e-05
0: TRAIN [2][2300/5173]	Time 0.643 (0.623)	Data 3.11e-04 (3.72e-04)	Tok/s 26304 (22718)	Loss/tok 3.3511 (3.3383)	LR 1.563e-05
0: TRAIN [2][2310/5173]	Time 0.582 (0.623)	Data 1.27e-04 (3.71e-04)	Tok/s 18008 (22710)	Loss/tok 3.1233 (3.3382)	LR 1.563e-05
0: TRAIN [2][2320/5173]	Time 0.581 (0.623)	Data 2.71e-04 (3.70e-04)	Tok/s 17516 (22719)	Loss/tok 3.1213 (3.3383)	LR 1.563e-05
0: TRAIN [2][2330/5173]	Time 0.705 (0.623)	Data 1.34e-04 (3.69e-04)	Tok/s 33032 (22712)	Loss/tok 3.5688 (3.3382)	LR 1.563e-05
0: TRAIN [2][2340/5173]	Time 0.649 (0.623)	Data 1.23e-04 (3.68e-04)	Tok/s 25644 (22714)	Loss/tok 3.3787 (3.3383)	LR 1.563e-05
0: TRAIN [2][2350/5173]	Time 0.638 (0.623)	Data 1.19e-04 (3.67e-04)	Tok/s 26294 (22713)	Loss/tok 3.3387 (3.3381)	LR 1.563e-05
0: TRAIN [2][2360/5173]	Time 0.580 (0.623)	Data 1.23e-04 (3.66e-04)	Tok/s 17994 (22699)	Loss/tok 3.0700 (3.3377)	LR 1.563e-05
0: TRAIN [2][2370/5173]	Time 0.647 (0.623)	Data 1.31e-04 (3.65e-04)	Tok/s 25916 (22699)	Loss/tok 3.2153 (3.3375)	LR 1.563e-05
0: TRAIN [2][2380/5173]	Time 0.703 (0.623)	Data 1.25e-04 (3.65e-04)	Tok/s 33012 (22701)	Loss/tok 3.6541 (3.3376)	LR 1.563e-05
0: TRAIN [2][2390/5173]	Time 0.580 (0.623)	Data 1.23e-04 (3.64e-04)	Tok/s 17893 (22681)	Loss/tok 3.0766 (3.3372)	LR 1.563e-05
0: TRAIN [2][2400/5173]	Time 0.583 (0.623)	Data 1.22e-04 (3.63e-04)	Tok/s 17581 (22687)	Loss/tok 3.0790 (3.3372)	LR 1.563e-05
0: TRAIN [2][2410/5173]	Time 0.640 (0.623)	Data 1.21e-04 (3.62e-04)	Tok/s 25956 (22693)	Loss/tok 3.3634 (3.3371)	LR 1.563e-05
0: TRAIN [2][2420/5173]	Time 0.708 (0.623)	Data 1.25e-04 (3.61e-04)	Tok/s 32692 (22695)	Loss/tok 3.5480 (3.3371)	LR 1.563e-05
0: TRAIN [2][2430/5173]	Time 0.584 (0.623)	Data 2.70e-04 (3.60e-04)	Tok/s 17811 (22682)	Loss/tok 3.1392 (3.3368)	LR 1.563e-05
0: TRAIN [2][2440/5173]	Time 0.584 (0.623)	Data 1.20e-04 (3.59e-04)	Tok/s 17616 (22684)	Loss/tok 3.2168 (3.3367)	LR 1.563e-05
0: TRAIN [2][2450/5173]	Time 0.642 (0.622)	Data 1.24e-04 (3.58e-04)	Tok/s 26167 (22680)	Loss/tok 3.3049 (3.3366)	LR 1.563e-05
0: TRAIN [2][2460/5173]	Time 0.585 (0.622)	Data 1.23e-04 (3.57e-04)	Tok/s 17820 (22663)	Loss/tok 3.1789 (3.3360)	LR 1.563e-05
0: TRAIN [2][2470/5173]	Time 0.584 (0.622)	Data 1.20e-04 (3.56e-04)	Tok/s 17863 (22673)	Loss/tok 3.1185 (3.3360)	LR 1.563e-05
0: TRAIN [2][2480/5173]	Time 0.645 (0.622)	Data 2.70e-04 (3.55e-04)	Tok/s 26126 (22675)	Loss/tok 3.3697 (3.3360)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2490/5173]	Time 0.580 (0.622)	Data 1.28e-04 (3.54e-04)	Tok/s 17677 (22674)	Loss/tok 3.2597 (3.3359)	LR 1.563e-05
0: TRAIN [2][2500/5173]	Time 0.582 (0.622)	Data 2.68e-04 (3.54e-04)	Tok/s 17765 (22665)	Loss/tok 3.1576 (3.3356)	LR 1.563e-05
0: TRAIN [2][2510/5173]	Time 0.642 (0.622)	Data 1.50e-04 (3.53e-04)	Tok/s 26278 (22676)	Loss/tok 3.2955 (3.3360)	LR 1.563e-05
0: TRAIN [2][2520/5173]	Time 0.584 (0.622)	Data 1.48e-04 (3.52e-04)	Tok/s 17115 (22681)	Loss/tok 3.1765 (3.3361)	LR 1.563e-05
0: TRAIN [2][2530/5173]	Time 0.648 (0.622)	Data 1.23e-04 (3.51e-04)	Tok/s 25818 (22674)	Loss/tok 3.3742 (3.3357)	LR 1.563e-05
0: TRAIN [2][2540/5173]	Time 0.641 (0.623)	Data 1.38e-04 (3.50e-04)	Tok/s 25977 (22688)	Loss/tok 3.3598 (3.3362)	LR 1.563e-05
0: TRAIN [2][2550/5173]	Time 0.582 (0.623)	Data 1.24e-04 (3.49e-04)	Tok/s 17432 (22690)	Loss/tok 3.0920 (3.3360)	LR 1.563e-05
0: TRAIN [2][2560/5173]	Time 0.647 (0.623)	Data 1.24e-04 (3.48e-04)	Tok/s 26212 (22694)	Loss/tok 3.2742 (3.3360)	LR 1.563e-05
0: TRAIN [2][2570/5173]	Time 0.583 (0.623)	Data 1.23e-04 (3.48e-04)	Tok/s 17621 (22714)	Loss/tok 3.1799 (3.3365)	LR 1.563e-05
0: TRAIN [2][2580/5173]	Time 0.582 (0.623)	Data 1.22e-04 (3.47e-04)	Tok/s 17802 (22724)	Loss/tok 3.1585 (3.3369)	LR 1.563e-05
0: TRAIN [2][2590/5173]	Time 0.647 (0.623)	Data 2.69e-04 (3.46e-04)	Tok/s 26194 (22718)	Loss/tok 3.2218 (3.3365)	LR 1.563e-05
0: TRAIN [2][2600/5173]	Time 0.645 (0.623)	Data 1.22e-04 (3.45e-04)	Tok/s 26151 (22711)	Loss/tok 3.2738 (3.3362)	LR 1.563e-05
0: TRAIN [2][2610/5173]	Time 0.645 (0.623)	Data 1.23e-04 (3.45e-04)	Tok/s 25777 (22710)	Loss/tok 3.2819 (3.3361)	LR 1.563e-05
0: TRAIN [2][2620/5173]	Time 0.645 (0.623)	Data 1.28e-04 (3.44e-04)	Tok/s 26079 (22695)	Loss/tok 3.3131 (3.3357)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2630/5173]	Time 0.647 (0.623)	Data 1.17e-04 (3.43e-04)	Tok/s 25878 (22707)	Loss/tok 3.4479 (3.3361)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][2640/5173]	Time 0.581 (0.623)	Data 1.85e-04 (3.42e-04)	Tok/s 17412 (22731)	Loss/tok 3.1363 (3.3374)	LR 1.563e-05
0: TRAIN [2][2650/5173]	Time 0.643 (0.623)	Data 2.78e-04 (3.42e-04)	Tok/s 26004 (22716)	Loss/tok 3.2414 (3.3369)	LR 1.563e-05
0: TRAIN [2][2660/5173]	Time 0.643 (0.623)	Data 1.41e-04 (3.41e-04)	Tok/s 25588 (22733)	Loss/tok 3.3791 (3.3374)	LR 1.563e-05
0: TRAIN [2][2670/5173]	Time 0.702 (0.623)	Data 1.25e-04 (3.40e-04)	Tok/s 32917 (22720)	Loss/tok 3.6062 (3.3372)	LR 1.563e-05
0: TRAIN [2][2680/5173]	Time 0.584 (0.623)	Data 1.25e-04 (3.40e-04)	Tok/s 17682 (22712)	Loss/tok 3.0511 (3.3372)	LR 1.563e-05
0: TRAIN [2][2690/5173]	Time 0.584 (0.623)	Data 2.69e-04 (3.39e-04)	Tok/s 17495 (22717)	Loss/tok 3.1657 (3.3372)	LR 1.563e-05
0: TRAIN [2][2700/5173]	Time 0.645 (0.623)	Data 2.71e-04 (3.38e-04)	Tok/s 25876 (22710)	Loss/tok 3.2831 (3.3371)	LR 1.563e-05
0: TRAIN [2][2710/5173]	Time 0.583 (0.623)	Data 1.22e-04 (3.37e-04)	Tok/s 18015 (22695)	Loss/tok 3.0149 (3.3366)	LR 1.563e-05
0: TRAIN [2][2720/5173]	Time 0.779 (0.623)	Data 1.35e-04 (3.37e-04)	Tok/s 38050 (22718)	Loss/tok 3.6765 (3.3375)	LR 1.563e-05
0: TRAIN [2][2730/5173]	Time 0.585 (0.623)	Data 1.41e-04 (3.36e-04)	Tok/s 17580 (22716)	Loss/tok 3.1303 (3.3374)	LR 1.563e-05
0: TRAIN [2][2740/5173]	Time 0.521 (0.623)	Data 1.20e-04 (3.35e-04)	Tok/s 10026 (22710)	Loss/tok 2.5711 (3.3373)	LR 1.563e-05
0: TRAIN [2][2750/5173]	Time 0.706 (0.623)	Data 1.21e-04 (3.35e-04)	Tok/s 33056 (22721)	Loss/tok 3.6594 (3.3379)	LR 1.563e-05
0: TRAIN [2][2760/5173]	Time 0.708 (0.623)	Data 1.22e-04 (3.34e-04)	Tok/s 33071 (22714)	Loss/tok 3.5321 (3.3378)	LR 1.563e-05
0: TRAIN [2][2770/5173]	Time 0.521 (0.623)	Data 1.23e-04 (3.33e-04)	Tok/s 10307 (22708)	Loss/tok 2.7582 (3.3375)	LR 1.563e-05
0: TRAIN [2][2780/5173]	Time 0.644 (0.623)	Data 1.21e-04 (3.33e-04)	Tok/s 26258 (22718)	Loss/tok 3.2773 (3.3378)	LR 1.563e-05
0: TRAIN [2][2790/5173]	Time 0.646 (0.623)	Data 1.25e-04 (3.32e-04)	Tok/s 26007 (22714)	Loss/tok 3.4537 (3.3378)	LR 1.563e-05
0: TRAIN [2][2800/5173]	Time 0.642 (0.623)	Data 1.32e-04 (3.31e-04)	Tok/s 26181 (22705)	Loss/tok 3.3584 (3.3375)	LR 1.563e-05
0: TRAIN [2][2810/5173]	Time 0.704 (0.623)	Data 1.44e-04 (3.31e-04)	Tok/s 33374 (22701)	Loss/tok 3.4428 (3.3375)	LR 1.563e-05
0: TRAIN [2][2820/5173]	Time 0.519 (0.623)	Data 1.16e-04 (3.30e-04)	Tok/s 9984 (22692)	Loss/tok 2.7536 (3.3372)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][2830/5173]	Time 0.581 (0.623)	Data 1.22e-04 (3.29e-04)	Tok/s 17907 (22690)	Loss/tok 3.1173 (3.3372)	LR 1.563e-05
0: TRAIN [2][2840/5173]	Time 0.521 (0.623)	Data 2.69e-04 (3.29e-04)	Tok/s 10265 (22681)	Loss/tok 2.7351 (3.3370)	LR 1.563e-05
0: TRAIN [2][2850/5173]	Time 0.702 (0.623)	Data 2.67e-04 (3.28e-04)	Tok/s 32778 (22678)	Loss/tok 3.6192 (3.3370)	LR 1.563e-05
0: TRAIN [2][2860/5173]	Time 0.581 (0.622)	Data 1.19e-04 (3.28e-04)	Tok/s 17778 (22666)	Loss/tok 3.0384 (3.3366)	LR 1.563e-05
0: TRAIN [2][2870/5173]	Time 0.581 (0.622)	Data 1.12e-04 (3.27e-04)	Tok/s 17686 (22670)	Loss/tok 3.0838 (3.3367)	LR 1.563e-05
0: TRAIN [2][2880/5173]	Time 0.642 (0.622)	Data 1.17e-04 (3.26e-04)	Tok/s 25725 (22661)	Loss/tok 3.3899 (3.3364)	LR 1.563e-05
0: TRAIN [2][2890/5173]	Time 0.581 (0.622)	Data 1.18e-04 (3.26e-04)	Tok/s 17765 (22658)	Loss/tok 3.1012 (3.3364)	LR 1.563e-05
0: TRAIN [2][2900/5173]	Time 0.521 (0.622)	Data 1.21e-04 (3.25e-04)	Tok/s 10237 (22658)	Loss/tok 2.6257 (3.3363)	LR 1.563e-05
0: TRAIN [2][2910/5173]	Time 0.583 (0.622)	Data 1.20e-04 (3.24e-04)	Tok/s 17613 (22660)	Loss/tok 3.1896 (3.3366)	LR 1.563e-05
0: TRAIN [2][2920/5173]	Time 0.583 (0.622)	Data 1.19e-04 (3.24e-04)	Tok/s 17851 (22652)	Loss/tok 3.1563 (3.3363)	LR 1.563e-05
0: TRAIN [2][2930/5173]	Time 0.645 (0.622)	Data 1.16e-04 (3.23e-04)	Tok/s 25925 (22640)	Loss/tok 3.4403 (3.3363)	LR 1.563e-05
0: TRAIN [2][2940/5173]	Time 0.704 (0.622)	Data 1.20e-04 (3.22e-04)	Tok/s 33129 (22649)	Loss/tok 3.5943 (3.3366)	LR 1.563e-05
0: TRAIN [2][2950/5173]	Time 0.644 (0.622)	Data 1.80e-04 (3.22e-04)	Tok/s 26205 (22669)	Loss/tok 3.3201 (3.3370)	LR 1.563e-05
0: TRAIN [2][2960/5173]	Time 0.709 (0.622)	Data 1.18e-04 (3.21e-04)	Tok/s 32951 (22669)	Loss/tok 3.4993 (3.3370)	LR 1.563e-05
0: TRAIN [2][2970/5173]	Time 0.520 (0.622)	Data 1.17e-04 (3.21e-04)	Tok/s 10142 (22655)	Loss/tok 2.5970 (3.3367)	LR 1.563e-05
0: TRAIN [2][2980/5173]	Time 0.582 (0.622)	Data 1.11e-04 (3.20e-04)	Tok/s 17875 (22660)	Loss/tok 3.0362 (3.3367)	LR 1.563e-05
0: TRAIN [2][2990/5173]	Time 0.707 (0.622)	Data 1.21e-04 (3.19e-04)	Tok/s 32933 (22669)	Loss/tok 3.6503 (3.3369)	LR 1.563e-05
0: TRAIN [2][3000/5173]	Time 0.641 (0.622)	Data 1.31e-04 (3.19e-04)	Tok/s 26121 (22673)	Loss/tok 3.4482 (3.3372)	LR 1.563e-05
0: TRAIN [2][3010/5173]	Time 0.584 (0.622)	Data 1.14e-04 (3.18e-04)	Tok/s 17466 (22669)	Loss/tok 3.1152 (3.3374)	LR 1.563e-05
0: TRAIN [2][3020/5173]	Time 0.704 (0.622)	Data 1.18e-04 (3.17e-04)	Tok/s 33421 (22670)	Loss/tok 3.5188 (3.3375)	LR 1.563e-05
0: TRAIN [2][3030/5173]	Time 0.647 (0.622)	Data 1.21e-04 (3.17e-04)	Tok/s 25849 (22670)	Loss/tok 3.3617 (3.3374)	LR 1.563e-05
0: TRAIN [2][3040/5173]	Time 0.641 (0.622)	Data 1.16e-04 (3.16e-04)	Tok/s 26424 (22657)	Loss/tok 3.3278 (3.3370)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][3050/5173]	Time 0.576 (0.622)	Data 1.25e-04 (3.16e-04)	Tok/s 18015 (22661)	Loss/tok 3.2427 (3.3371)	LR 1.563e-05
0: TRAIN [2][3060/5173]	Time 0.580 (0.622)	Data 3.08e-04 (3.15e-04)	Tok/s 17906 (22661)	Loss/tok 3.0913 (3.3370)	LR 1.563e-05
0: TRAIN [2][3070/5173]	Time 0.582 (0.622)	Data 1.13e-04 (3.15e-04)	Tok/s 17559 (22665)	Loss/tok 3.1577 (3.3371)	LR 1.563e-05
0: TRAIN [2][3080/5173]	Time 0.583 (0.622)	Data 3.17e-04 (3.14e-04)	Tok/s 17928 (22668)	Loss/tok 3.0695 (3.3372)	LR 1.563e-05
0: TRAIN [2][3090/5173]	Time 0.706 (0.622)	Data 1.56e-04 (3.14e-04)	Tok/s 33215 (22673)	Loss/tok 3.4543 (3.3372)	LR 1.563e-05
0: TRAIN [2][3100/5173]	Time 0.640 (0.622)	Data 1.61e-04 (3.13e-04)	Tok/s 26240 (22666)	Loss/tok 3.2942 (3.3369)	LR 1.563e-05
0: TRAIN [2][3110/5173]	Time 0.646 (0.622)	Data 1.64e-04 (3.13e-04)	Tok/s 26353 (22666)	Loss/tok 3.2664 (3.3368)	LR 1.563e-05
0: TRAIN [2][3120/5173]	Time 0.580 (0.622)	Data 1.70e-04 (3.13e-04)	Tok/s 17825 (22656)	Loss/tok 3.1762 (3.3366)	LR 1.563e-05
0: TRAIN [2][3130/5173]	Time 0.647 (0.622)	Data 1.59e-04 (3.12e-04)	Tok/s 25506 (22653)	Loss/tok 3.4393 (3.3365)	LR 1.563e-05
0: TRAIN [2][3140/5173]	Time 0.581 (0.622)	Data 1.54e-04 (3.12e-04)	Tok/s 17776 (22660)	Loss/tok 3.1502 (3.3368)	LR 1.563e-05
0: TRAIN [2][3150/5173]	Time 0.706 (0.622)	Data 1.63e-04 (3.12e-04)	Tok/s 32838 (22667)	Loss/tok 3.4419 (3.3368)	LR 1.563e-05
0: TRAIN [2][3160/5173]	Time 0.582 (0.622)	Data 1.62e-04 (3.11e-04)	Tok/s 17855 (22659)	Loss/tok 3.0972 (3.3365)	LR 1.563e-05
0: TRAIN [2][3170/5173]	Time 0.707 (0.622)	Data 1.54e-04 (3.11e-04)	Tok/s 32794 (22667)	Loss/tok 3.5234 (3.3370)	LR 1.563e-05
0: TRAIN [2][3180/5173]	Time 0.584 (0.622)	Data 1.60e-04 (3.10e-04)	Tok/s 17544 (22659)	Loss/tok 3.0881 (3.3366)	LR 1.563e-05
0: TRAIN [2][3190/5173]	Time 0.709 (0.622)	Data 1.64e-04 (3.10e-04)	Tok/s 33078 (22664)	Loss/tok 3.5661 (3.3367)	LR 1.563e-05
0: TRAIN [2][3200/5173]	Time 0.579 (0.622)	Data 1.60e-04 (3.09e-04)	Tok/s 18174 (22671)	Loss/tok 3.1471 (3.3368)	LR 1.563e-05
0: TRAIN [2][3210/5173]	Time 0.650 (0.622)	Data 3.50e-04 (3.09e-04)	Tok/s 25803 (22667)	Loss/tok 3.4679 (3.3367)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][3220/5173]	Time 0.704 (0.622)	Data 1.59e-04 (3.09e-04)	Tok/s 33070 (22680)	Loss/tok 3.5240 (3.3370)	LR 1.563e-05
0: TRAIN [2][3230/5173]	Time 0.705 (0.623)	Data 3.60e-04 (3.08e-04)	Tok/s 33016 (22694)	Loss/tok 3.4374 (3.3375)	LR 1.563e-05
0: TRAIN [2][3240/5173]	Time 0.521 (0.623)	Data 1.76e-04 (3.08e-04)	Tok/s 10377 (22706)	Loss/tok 2.5974 (3.3381)	LR 1.563e-05
0: TRAIN [2][3250/5173]	Time 0.583 (0.623)	Data 1.65e-04 (3.07e-04)	Tok/s 17547 (22704)	Loss/tok 3.0822 (3.3382)	LR 1.563e-05
0: TRAIN [2][3260/5173]	Time 0.705 (0.623)	Data 1.56e-04 (3.07e-04)	Tok/s 33182 (22705)	Loss/tok 3.5390 (3.3382)	LR 1.563e-05
0: TRAIN [2][3270/5173]	Time 0.583 (0.623)	Data 1.52e-04 (3.07e-04)	Tok/s 17784 (22714)	Loss/tok 3.1845 (3.3383)	LR 1.563e-05
0: TRAIN [2][3280/5173]	Time 0.645 (0.623)	Data 1.59e-04 (3.06e-04)	Tok/s 25934 (22715)	Loss/tok 3.3505 (3.3385)	LR 1.563e-05
0: TRAIN [2][3290/5173]	Time 0.586 (0.623)	Data 1.60e-04 (3.06e-04)	Tok/s 17496 (22713)	Loss/tok 3.0753 (3.3386)	LR 1.563e-05
0: TRAIN [2][3300/5173]	Time 0.584 (0.623)	Data 1.66e-04 (3.06e-04)	Tok/s 17958 (22717)	Loss/tok 3.0771 (3.3387)	LR 1.563e-05
0: TRAIN [2][3310/5173]	Time 0.582 (0.623)	Data 1.18e-04 (3.05e-04)	Tok/s 17667 (22711)	Loss/tok 3.1506 (3.3385)	LR 1.563e-05
0: TRAIN [2][3320/5173]	Time 0.707 (0.623)	Data 1.64e-04 (3.05e-04)	Tok/s 33008 (22719)	Loss/tok 3.4568 (3.3388)	LR 1.563e-05
0: TRAIN [2][3330/5173]	Time 0.638 (0.623)	Data 1.62e-04 (3.05e-04)	Tok/s 26645 (22722)	Loss/tok 3.2878 (3.3389)	LR 1.563e-05
0: TRAIN [2][3340/5173]	Time 0.578 (0.623)	Data 1.20e-04 (3.04e-04)	Tok/s 18254 (22739)	Loss/tok 3.0660 (3.3393)	LR 1.563e-05
0: TRAIN [2][3350/5173]	Time 0.648 (0.623)	Data 1.19e-04 (3.04e-04)	Tok/s 26048 (22735)	Loss/tok 3.2570 (3.3390)	LR 1.563e-05
0: TRAIN [2][3360/5173]	Time 0.585 (0.623)	Data 1.16e-04 (3.03e-04)	Tok/s 17139 (22723)	Loss/tok 3.1464 (3.3388)	LR 1.563e-05
0: TRAIN [2][3370/5173]	Time 0.581 (0.623)	Data 1.16e-04 (3.03e-04)	Tok/s 17405 (22723)	Loss/tok 3.0941 (3.3387)	LR 1.563e-05
0: TRAIN [2][3380/5173]	Time 0.582 (0.623)	Data 1.17e-04 (3.02e-04)	Tok/s 17799 (22707)	Loss/tok 3.0914 (3.3383)	LR 1.563e-05
0: TRAIN [2][3390/5173]	Time 0.645 (0.623)	Data 1.13e-04 (3.02e-04)	Tok/s 26664 (22709)	Loss/tok 3.2858 (3.3384)	LR 1.563e-05
0: TRAIN [2][3400/5173]	Time 0.584 (0.623)	Data 1.16e-04 (3.01e-04)	Tok/s 17491 (22709)	Loss/tok 3.3092 (3.3385)	LR 1.563e-05
0: TRAIN [2][3410/5173]	Time 0.783 (0.623)	Data 1.18e-04 (3.01e-04)	Tok/s 37987 (22710)	Loss/tok 3.6654 (3.3387)	LR 1.563e-05
0: TRAIN [2][3420/5173]	Time 0.641 (0.623)	Data 1.11e-04 (3.00e-04)	Tok/s 25935 (22705)	Loss/tok 3.3335 (3.3385)	LR 1.563e-05
0: TRAIN [2][3430/5173]	Time 0.582 (0.623)	Data 2.78e-04 (3.00e-04)	Tok/s 17667 (22703)	Loss/tok 3.1875 (3.3384)	LR 1.563e-05
0: TRAIN [2][3440/5173]	Time 0.708 (0.623)	Data 2.68e-04 (2.99e-04)	Tok/s 33074 (22712)	Loss/tok 3.5612 (3.3386)	LR 1.563e-05
0: TRAIN [2][3450/5173]	Time 0.642 (0.623)	Data 1.54e-04 (2.99e-04)	Tok/s 26289 (22716)	Loss/tok 3.2261 (3.3386)	LR 1.563e-05
0: TRAIN [2][3460/5173]	Time 0.639 (0.623)	Data 1.42e-04 (2.99e-04)	Tok/s 26441 (22719)	Loss/tok 3.3631 (3.3383)	LR 1.563e-05
0: TRAIN [2][3470/5173]	Time 0.583 (0.623)	Data 1.18e-04 (2.98e-04)	Tok/s 17492 (22718)	Loss/tok 3.1406 (3.3383)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3480/5173]	Time 0.584 (0.623)	Data 1.15e-04 (2.98e-04)	Tok/s 17696 (22718)	Loss/tok 3.0754 (3.3385)	LR 1.563e-05
0: TRAIN [2][3490/5173]	Time 0.702 (0.623)	Data 1.14e-04 (2.97e-04)	Tok/s 33370 (22725)	Loss/tok 3.3914 (3.3385)	LR 1.563e-05
0: TRAIN [2][3500/5173]	Time 0.581 (0.623)	Data 1.18e-04 (2.97e-04)	Tok/s 17539 (22729)	Loss/tok 3.1045 (3.3385)	LR 1.563e-05
0: TRAIN [2][3510/5173]	Time 0.581 (0.623)	Data 1.18e-04 (2.96e-04)	Tok/s 17977 (22724)	Loss/tok 3.1412 (3.3382)	LR 1.563e-05
0: TRAIN [2][3520/5173]	Time 0.581 (0.623)	Data 1.15e-04 (2.96e-04)	Tok/s 17806 (22717)	Loss/tok 3.2686 (3.3379)	LR 1.563e-05
0: TRAIN [2][3530/5173]	Time 0.703 (0.623)	Data 1.14e-04 (2.95e-04)	Tok/s 32820 (22710)	Loss/tok 3.5716 (3.3377)	LR 1.563e-05
0: TRAIN [2][3540/5173]	Time 0.704 (0.623)	Data 1.18e-04 (2.95e-04)	Tok/s 32869 (22714)	Loss/tok 3.5663 (3.3378)	LR 1.563e-05
0: TRAIN [2][3550/5173]	Time 0.649 (0.623)	Data 1.17e-04 (2.94e-04)	Tok/s 26385 (22718)	Loss/tok 3.3094 (3.3377)	LR 1.563e-05
0: TRAIN [2][3560/5173]	Time 0.704 (0.623)	Data 1.11e-04 (2.94e-04)	Tok/s 33478 (22729)	Loss/tok 3.4218 (3.3380)	LR 1.563e-05
0: TRAIN [2][3570/5173]	Time 0.582 (0.623)	Data 1.21e-04 (2.93e-04)	Tok/s 17653 (22730)	Loss/tok 3.1115 (3.3381)	LR 1.563e-05
0: TRAIN [2][3580/5173]	Time 0.582 (0.623)	Data 1.16e-04 (2.93e-04)	Tok/s 18142 (22726)	Loss/tok 3.0212 (3.3379)	LR 1.563e-05
0: TRAIN [2][3590/5173]	Time 0.581 (0.623)	Data 1.16e-04 (2.92e-04)	Tok/s 17726 (22731)	Loss/tok 3.2474 (3.3380)	LR 1.563e-05
0: TRAIN [2][3600/5173]	Time 0.700 (0.623)	Data 1.17e-04 (2.92e-04)	Tok/s 33378 (22731)	Loss/tok 3.4380 (3.3379)	LR 1.563e-05
0: TRAIN [2][3610/5173]	Time 0.582 (0.623)	Data 1.14e-04 (2.91e-04)	Tok/s 17635 (22728)	Loss/tok 3.1730 (3.3378)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3620/5173]	Time 0.706 (0.623)	Data 1.13e-04 (2.91e-04)	Tok/s 33608 (22725)	Loss/tok 3.4643 (3.3377)	LR 1.563e-05
0: TRAIN [2][3630/5173]	Time 0.581 (0.623)	Data 1.18e-04 (2.91e-04)	Tok/s 17696 (22718)	Loss/tok 3.1442 (3.3375)	LR 1.563e-05
0: TRAIN [2][3640/5173]	Time 0.707 (0.623)	Data 1.18e-04 (2.90e-04)	Tok/s 33112 (22717)	Loss/tok 3.5641 (3.3376)	LR 1.563e-05
0: TRAIN [2][3650/5173]	Time 0.643 (0.623)	Data 1.24e-04 (2.90e-04)	Tok/s 25846 (22727)	Loss/tok 3.4019 (3.3377)	LR 1.563e-05
0: TRAIN [2][3660/5173]	Time 0.644 (0.623)	Data 1.17e-04 (2.89e-04)	Tok/s 25970 (22732)	Loss/tok 3.3183 (3.3378)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][3670/5173]	Time 0.578 (0.623)	Data 1.29e-04 (2.89e-04)	Tok/s 18135 (22735)	Loss/tok 3.0809 (3.3379)	LR 1.563e-05
0: TRAIN [2][3680/5173]	Time 0.582 (0.623)	Data 1.17e-04 (2.88e-04)	Tok/s 17427 (22738)	Loss/tok 3.0868 (3.3378)	LR 1.563e-05
0: TRAIN [2][3690/5173]	Time 0.582 (0.623)	Data 3.08e-04 (2.88e-04)	Tok/s 17637 (22725)	Loss/tok 3.0118 (3.3376)	LR 1.563e-05
0: TRAIN [2][3700/5173]	Time 0.584 (0.623)	Data 1.32e-04 (2.88e-04)	Tok/s 17691 (22740)	Loss/tok 3.0311 (3.3381)	LR 1.563e-05
0: TRAIN [2][3710/5173]	Time 0.707 (0.623)	Data 1.16e-04 (2.87e-04)	Tok/s 32723 (22748)	Loss/tok 3.5764 (3.3384)	LR 1.563e-05
0: TRAIN [2][3720/5173]	Time 0.581 (0.623)	Data 1.15e-04 (2.87e-04)	Tok/s 17828 (22739)	Loss/tok 3.1848 (3.3380)	LR 1.563e-05
0: TRAIN [2][3730/5173]	Time 0.642 (0.623)	Data 1.15e-04 (2.86e-04)	Tok/s 26369 (22730)	Loss/tok 3.3495 (3.3378)	LR 1.563e-05
0: TRAIN [2][3740/5173]	Time 0.643 (0.623)	Data 1.17e-04 (2.86e-04)	Tok/s 25979 (22723)	Loss/tok 3.3712 (3.3376)	LR 1.563e-05
0: TRAIN [2][3750/5173]	Time 0.701 (0.623)	Data 1.15e-04 (2.85e-04)	Tok/s 32718 (22729)	Loss/tok 3.6287 (3.3377)	LR 1.563e-05
0: TRAIN [2][3760/5173]	Time 0.784 (0.623)	Data 1.19e-04 (2.85e-04)	Tok/s 37631 (22738)	Loss/tok 3.7726 (3.3380)	LR 1.563e-05
0: TRAIN [2][3770/5173]	Time 0.584 (0.623)	Data 1.15e-04 (2.85e-04)	Tok/s 17538 (22734)	Loss/tok 3.0553 (3.3380)	LR 1.563e-05
0: TRAIN [2][3780/5173]	Time 0.585 (0.623)	Data 1.23e-04 (2.84e-04)	Tok/s 17543 (22732)	Loss/tok 3.1698 (3.3381)	LR 1.563e-05
0: TRAIN [2][3790/5173]	Time 0.645 (0.623)	Data 1.16e-04 (2.84e-04)	Tok/s 26152 (22734)	Loss/tok 3.3271 (3.3382)	LR 1.563e-05
0: TRAIN [2][3800/5173]	Time 0.647 (0.623)	Data 1.15e-04 (2.83e-04)	Tok/s 26007 (22733)	Loss/tok 3.3418 (3.3382)	LR 1.563e-05
0: TRAIN [2][3810/5173]	Time 0.582 (0.623)	Data 1.19e-04 (2.83e-04)	Tok/s 17472 (22720)	Loss/tok 3.0936 (3.3378)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][3820/5173]	Time 0.779 (0.623)	Data 1.17e-04 (2.83e-04)	Tok/s 38830 (22724)	Loss/tok 3.5823 (3.3380)	LR 1.563e-05
0: TRAIN [2][3830/5173]	Time 0.648 (0.623)	Data 1.16e-04 (2.82e-04)	Tok/s 26010 (22718)	Loss/tok 3.2124 (3.3378)	LR 1.563e-05
0: TRAIN [2][3840/5173]	Time 0.643 (0.623)	Data 1.12e-04 (2.82e-04)	Tok/s 26104 (22711)	Loss/tok 3.3526 (3.3377)	LR 1.563e-05
0: TRAIN [2][3850/5173]	Time 0.582 (0.623)	Data 1.12e-04 (2.82e-04)	Tok/s 17239 (22709)	Loss/tok 3.0788 (3.3376)	LR 1.563e-05
0: TRAIN [2][3860/5173]	Time 0.647 (0.623)	Data 1.11e-04 (2.81e-04)	Tok/s 26229 (22714)	Loss/tok 3.3063 (3.3378)	LR 1.563e-05
0: TRAIN [2][3870/5173]	Time 0.644 (0.623)	Data 1.15e-04 (2.81e-04)	Tok/s 25563 (22708)	Loss/tok 3.3104 (3.3376)	LR 1.563e-05
0: TRAIN [2][3880/5173]	Time 0.643 (0.623)	Data 1.13e-04 (2.80e-04)	Tok/s 26304 (22720)	Loss/tok 3.3562 (3.3377)	LR 1.563e-05
0: TRAIN [2][3890/5173]	Time 0.777 (0.623)	Data 1.17e-04 (2.80e-04)	Tok/s 38016 (22730)	Loss/tok 3.7224 (3.3380)	LR 1.563e-05
0: TRAIN [2][3900/5173]	Time 0.647 (0.623)	Data 1.14e-04 (2.80e-04)	Tok/s 25990 (22724)	Loss/tok 3.4085 (3.3378)	LR 1.563e-05
0: TRAIN [2][3910/5173]	Time 0.584 (0.623)	Data 1.15e-04 (2.79e-04)	Tok/s 17703 (22726)	Loss/tok 3.1979 (3.3378)	LR 1.563e-05
0: TRAIN [2][3920/5173]	Time 0.649 (0.623)	Data 1.19e-04 (2.79e-04)	Tok/s 25952 (22731)	Loss/tok 3.3889 (3.3378)	LR 1.563e-05
0: TRAIN [2][3930/5173]	Time 0.641 (0.623)	Data 1.16e-04 (2.78e-04)	Tok/s 26375 (22738)	Loss/tok 3.3787 (3.3380)	LR 1.563e-05
0: TRAIN [2][3940/5173]	Time 0.646 (0.623)	Data 1.18e-04 (2.78e-04)	Tok/s 26079 (22739)	Loss/tok 3.2350 (3.3380)	LR 1.563e-05
0: TRAIN [2][3950/5173]	Time 0.642 (0.623)	Data 1.16e-04 (2.78e-04)	Tok/s 26288 (22738)	Loss/tok 3.2921 (3.3380)	LR 1.563e-05
0: TRAIN [2][3960/5173]	Time 0.580 (0.623)	Data 1.10e-04 (2.77e-04)	Tok/s 17621 (22740)	Loss/tok 3.1555 (3.3380)	LR 1.563e-05
0: TRAIN [2][3970/5173]	Time 0.584 (0.623)	Data 1.13e-04 (2.77e-04)	Tok/s 17566 (22751)	Loss/tok 3.0413 (3.3381)	LR 1.563e-05
0: TRAIN [2][3980/5173]	Time 0.707 (0.623)	Data 1.18e-04 (2.76e-04)	Tok/s 32934 (22756)	Loss/tok 3.4586 (3.3382)	LR 1.563e-05
0: TRAIN [2][3990/5173]	Time 0.518 (0.623)	Data 1.11e-04 (2.76e-04)	Tok/s 10210 (22753)	Loss/tok 2.6965 (3.3381)	LR 1.563e-05
0: TRAIN [2][4000/5173]	Time 0.646 (0.623)	Data 1.13e-04 (2.76e-04)	Tok/s 26185 (22765)	Loss/tok 3.3731 (3.3386)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][4010/5173]	Time 0.651 (0.623)	Data 1.25e-04 (2.75e-04)	Tok/s 26017 (22762)	Loss/tok 3.3731 (3.3385)	LR 1.563e-05
0: TRAIN [2][4020/5173]	Time 0.583 (0.623)	Data 1.12e-04 (2.75e-04)	Tok/s 17543 (22752)	Loss/tok 3.1042 (3.3382)	LR 1.563e-05
0: TRAIN [2][4030/5173]	Time 0.639 (0.623)	Data 1.12e-04 (2.75e-04)	Tok/s 26235 (22754)	Loss/tok 3.3046 (3.3382)	LR 1.563e-05
0: TRAIN [2][4040/5173]	Time 0.579 (0.623)	Data 1.15e-04 (2.74e-04)	Tok/s 17781 (22753)	Loss/tok 3.1459 (3.3381)	LR 1.563e-05
0: TRAIN [2][4050/5173]	Time 0.521 (0.623)	Data 1.15e-04 (2.74e-04)	Tok/s 10016 (22750)	Loss/tok 2.6333 (3.3381)	LR 1.563e-05
0: TRAIN [2][4060/5173]	Time 0.647 (0.623)	Data 1.14e-04 (2.74e-04)	Tok/s 25754 (22749)	Loss/tok 3.3158 (3.3380)	LR 1.563e-05
0: TRAIN [2][4070/5173]	Time 0.643 (0.623)	Data 1.65e-04 (2.73e-04)	Tok/s 26426 (22747)	Loss/tok 3.3474 (3.3379)	LR 1.563e-05
0: TRAIN [2][4080/5173]	Time 0.584 (0.623)	Data 3.07e-04 (2.73e-04)	Tok/s 17537 (22742)	Loss/tok 3.0514 (3.3379)	LR 1.563e-05
0: TRAIN [2][4090/5173]	Time 0.585 (0.623)	Data 1.15e-04 (2.73e-04)	Tok/s 17719 (22747)	Loss/tok 3.1888 (3.3381)	LR 1.563e-05
0: TRAIN [2][4100/5173]	Time 0.644 (0.623)	Data 1.58e-04 (2.72e-04)	Tok/s 26549 (22735)	Loss/tok 3.2565 (3.3377)	LR 1.563e-05
0: TRAIN [2][4110/5173]	Time 0.640 (0.623)	Data 1.81e-04 (2.72e-04)	Tok/s 26084 (22748)	Loss/tok 3.4054 (3.3381)	LR 1.563e-05
0: TRAIN [2][4120/5173]	Time 0.639 (0.623)	Data 1.59e-04 (2.72e-04)	Tok/s 26345 (22745)	Loss/tok 3.2944 (3.3379)	LR 1.563e-05
0: TRAIN [2][4130/5173]	Time 0.583 (0.623)	Data 1.62e-04 (2.72e-04)	Tok/s 17664 (22750)	Loss/tok 3.0666 (3.3380)	LR 1.563e-05
0: TRAIN [2][4140/5173]	Time 0.580 (0.623)	Data 1.59e-04 (2.71e-04)	Tok/s 17677 (22738)	Loss/tok 3.1652 (3.3378)	LR 1.563e-05
0: TRAIN [2][4150/5173]	Time 0.704 (0.623)	Data 1.64e-04 (2.71e-04)	Tok/s 33055 (22740)	Loss/tok 3.5598 (3.3378)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][4160/5173]	Time 0.706 (0.623)	Data 2.42e-04 (2.71e-04)	Tok/s 33238 (22740)	Loss/tok 3.4538 (3.3379)	LR 1.563e-05
0: TRAIN [2][4170/5173]	Time 0.584 (0.623)	Data 2.32e-04 (2.71e-04)	Tok/s 17525 (22742)	Loss/tok 3.1631 (3.3380)	LR 1.563e-05
0: TRAIN [2][4180/5173]	Time 0.642 (0.623)	Data 1.61e-04 (2.71e-04)	Tok/s 26270 (22743)	Loss/tok 3.4333 (3.3381)	LR 1.563e-05
0: TRAIN [2][4190/5173]	Time 0.519 (0.623)	Data 4.35e-04 (2.71e-04)	Tok/s 10382 (22743)	Loss/tok 2.6105 (3.3382)	LR 1.563e-05
0: TRAIN [2][4200/5173]	Time 0.580 (0.623)	Data 1.60e-04 (2.71e-04)	Tok/s 18060 (22744)	Loss/tok 3.1386 (3.3382)	LR 1.563e-05
0: TRAIN [2][4210/5173]	Time 0.582 (0.623)	Data 1.58e-04 (2.70e-04)	Tok/s 17827 (22749)	Loss/tok 3.0047 (3.3382)	LR 1.563e-05
0: TRAIN [2][4220/5173]	Time 0.520 (0.623)	Data 1.57e-04 (2.70e-04)	Tok/s 10050 (22742)	Loss/tok 2.6403 (3.3382)	LR 1.563e-05
0: TRAIN [2][4230/5173]	Time 0.582 (0.623)	Data 1.58e-04 (2.70e-04)	Tok/s 17658 (22746)	Loss/tok 3.2155 (3.3383)	LR 1.563e-05
0: TRAIN [2][4240/5173]	Time 0.581 (0.623)	Data 1.56e-04 (2.70e-04)	Tok/s 17712 (22746)	Loss/tok 3.0907 (3.3382)	LR 1.563e-05
0: TRAIN [2][4250/5173]	Time 0.581 (0.623)	Data 1.58e-04 (2.69e-04)	Tok/s 17976 (22745)	Loss/tok 3.0136 (3.3382)	LR 1.563e-05
0: TRAIN [2][4260/5173]	Time 0.645 (0.623)	Data 1.63e-04 (2.69e-04)	Tok/s 26074 (22734)	Loss/tok 3.4681 (3.3379)	LR 1.563e-05
0: TRAIN [2][4270/5173]	Time 0.524 (0.623)	Data 3.39e-04 (2.69e-04)	Tok/s 10041 (22734)	Loss/tok 2.6967 (3.3377)	LR 1.563e-05
0: TRAIN [2][4280/5173]	Time 0.647 (0.623)	Data 1.62e-04 (2.69e-04)	Tok/s 26036 (22725)	Loss/tok 3.2156 (3.3374)	LR 1.563e-05
0: TRAIN [2][4290/5173]	Time 0.585 (0.623)	Data 3.44e-04 (2.69e-04)	Tok/s 17637 (22717)	Loss/tok 3.1835 (3.3372)	LR 1.563e-05
0: TRAIN [2][4300/5173]	Time 0.520 (0.623)	Data 1.65e-04 (2.68e-04)	Tok/s 10008 (22715)	Loss/tok 2.6165 (3.3370)	LR 1.563e-05
0: TRAIN [2][4310/5173]	Time 0.706 (0.623)	Data 1.61e-04 (2.68e-04)	Tok/s 33161 (22711)	Loss/tok 3.4980 (3.3368)	LR 1.563e-05
0: TRAIN [2][4320/5173]	Time 0.582 (0.623)	Data 1.36e-04 (2.68e-04)	Tok/s 17786 (22719)	Loss/tok 3.1587 (3.3370)	LR 1.563e-05
0: TRAIN [2][4330/5173]	Time 0.706 (0.623)	Data 1.19e-04 (2.68e-04)	Tok/s 33100 (22720)	Loss/tok 3.5445 (3.3371)	LR 1.563e-05
0: TRAIN [2][4340/5173]	Time 0.580 (0.623)	Data 1.13e-04 (2.67e-04)	Tok/s 17857 (22720)	Loss/tok 3.0997 (3.3370)	LR 1.563e-05
0: TRAIN [2][4350/5173]	Time 0.583 (0.623)	Data 2.68e-04 (2.67e-04)	Tok/s 17783 (22721)	Loss/tok 3.1919 (3.3372)	LR 1.563e-05
0: TRAIN [2][4360/5173]	Time 0.784 (0.623)	Data 1.17e-04 (2.67e-04)	Tok/s 37644 (22724)	Loss/tok 3.7082 (3.3374)	LR 1.563e-05
0: TRAIN [2][4370/5173]	Time 0.581 (0.623)	Data 1.14e-04 (2.67e-04)	Tok/s 17827 (22720)	Loss/tok 3.2331 (3.3373)	LR 1.563e-05
0: TRAIN [2][4380/5173]	Time 0.581 (0.623)	Data 1.15e-04 (2.66e-04)	Tok/s 17852 (22705)	Loss/tok 3.0921 (3.3369)	LR 1.563e-05
0: TRAIN [2][4390/5173]	Time 0.646 (0.623)	Data 1.16e-04 (2.66e-04)	Tok/s 25908 (22702)	Loss/tok 3.3559 (3.3368)	LR 1.563e-05
0: TRAIN [2][4400/5173]	Time 0.705 (0.623)	Data 1.18e-04 (2.66e-04)	Tok/s 32884 (22701)	Loss/tok 3.5233 (3.3366)	LR 1.563e-05
0: TRAIN [2][4410/5173]	Time 0.581 (0.623)	Data 3.07e-04 (2.65e-04)	Tok/s 17706 (22701)	Loss/tok 3.1832 (3.3365)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][4420/5173]	Time 0.579 (0.623)	Data 1.28e-04 (2.65e-04)	Tok/s 17815 (22711)	Loss/tok 3.2501 (3.3370)	LR 1.563e-05
0: TRAIN [2][4430/5173]	Time 0.641 (0.623)	Data 1.24e-04 (2.65e-04)	Tok/s 26139 (22716)	Loss/tok 3.3934 (3.3371)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][4440/5173]	Time 0.582 (0.623)	Data 5.61e-04 (2.65e-04)	Tok/s 17622 (22718)	Loss/tok 3.1823 (3.3372)	LR 1.563e-05
0: TRAIN [2][4450/5173]	Time 0.520 (0.623)	Data 1.21e-04 (2.64e-04)	Tok/s 10210 (22717)	Loss/tok 2.7367 (3.3371)	LR 1.563e-05
0: TRAIN [2][4460/5173]	Time 0.518 (0.623)	Data 1.16e-04 (2.64e-04)	Tok/s 10084 (22722)	Loss/tok 2.7513 (3.3373)	LR 1.563e-05
0: TRAIN [2][4470/5173]	Time 0.645 (0.623)	Data 1.16e-04 (2.64e-04)	Tok/s 26477 (22718)	Loss/tok 3.3055 (3.3371)	LR 1.563e-05
0: TRAIN [2][4480/5173]	Time 0.709 (0.623)	Data 1.15e-04 (2.63e-04)	Tok/s 32799 (22723)	Loss/tok 3.5464 (3.3372)	LR 1.563e-05
0: TRAIN [2][4490/5173]	Time 0.642 (0.623)	Data 1.13e-04 (2.63e-04)	Tok/s 26441 (22717)	Loss/tok 3.3859 (3.3369)	LR 1.563e-05
0: TRAIN [2][4500/5173]	Time 0.582 (0.623)	Data 2.95e-04 (2.63e-04)	Tok/s 17850 (22714)	Loss/tok 2.9918 (3.3368)	LR 1.563e-05
0: TRAIN [2][4510/5173]	Time 0.780 (0.623)	Data 1.19e-04 (2.62e-04)	Tok/s 37811 (22717)	Loss/tok 3.7307 (3.3370)	LR 1.563e-05
0: TRAIN [2][4520/5173]	Time 0.583 (0.623)	Data 1.22e-04 (2.62e-04)	Tok/s 17993 (22714)	Loss/tok 3.0947 (3.3368)	LR 1.563e-05
0: TRAIN [2][4530/5173]	Time 0.520 (0.623)	Data 1.16e-04 (2.62e-04)	Tok/s 9822 (22714)	Loss/tok 2.6818 (3.3368)	LR 1.563e-05
0: TRAIN [2][4540/5173]	Time 0.645 (0.623)	Data 1.20e-04 (2.62e-04)	Tok/s 25991 (22712)	Loss/tok 3.3617 (3.3366)	LR 1.563e-05
0: TRAIN [2][4550/5173]	Time 0.586 (0.623)	Data 1.13e-04 (2.61e-04)	Tok/s 17583 (22721)	Loss/tok 3.2313 (3.3370)	LR 1.563e-05
0: TRAIN [2][4560/5173]	Time 0.583 (0.623)	Data 1.13e-04 (2.61e-04)	Tok/s 17972 (22729)	Loss/tok 3.1526 (3.3372)	LR 1.563e-05
0: TRAIN [2][4570/5173]	Time 0.579 (0.623)	Data 1.19e-04 (2.61e-04)	Tok/s 17404 (22723)	Loss/tok 3.0970 (3.3370)	LR 1.563e-05
0: TRAIN [2][4580/5173]	Time 0.521 (0.623)	Data 1.17e-04 (2.61e-04)	Tok/s 10278 (22722)	Loss/tok 2.7195 (3.3369)	LR 1.563e-05
0: TRAIN [2][4590/5173]	Time 0.581 (0.623)	Data 1.12e-04 (2.60e-04)	Tok/s 17947 (22725)	Loss/tok 3.0045 (3.3369)	LR 1.563e-05
0: TRAIN [2][4600/5173]	Time 0.648 (0.623)	Data 2.99e-04 (2.60e-04)	Tok/s 25876 (22734)	Loss/tok 3.4472 (3.3371)	LR 1.563e-05
0: TRAIN [2][4610/5173]	Time 0.641 (0.623)	Data 1.16e-04 (2.60e-04)	Tok/s 26295 (22737)	Loss/tok 3.3504 (3.3372)	LR 1.563e-05
0: TRAIN [2][4620/5173]	Time 0.706 (0.623)	Data 1.16e-04 (2.59e-04)	Tok/s 33042 (22742)	Loss/tok 3.3515 (3.3371)	LR 1.563e-05
0: TRAIN [2][4630/5173]	Time 0.706 (0.623)	Data 1.08e-04 (2.59e-04)	Tok/s 32971 (22744)	Loss/tok 3.4256 (3.3372)	LR 1.563e-05
0: TRAIN [2][4640/5173]	Time 0.580 (0.623)	Data 1.31e-04 (2.59e-04)	Tok/s 17867 (22744)	Loss/tok 3.1488 (3.3373)	LR 1.563e-05
0: TRAIN [2][4650/5173]	Time 0.582 (0.623)	Data 1.51e-04 (2.59e-04)	Tok/s 17654 (22744)	Loss/tok 3.2019 (3.3373)	LR 1.563e-05
0: TRAIN [2][4660/5173]	Time 0.642 (0.623)	Data 1.22e-04 (2.58e-04)	Tok/s 26363 (22744)	Loss/tok 3.4074 (3.3373)	LR 1.563e-05
0: TRAIN [2][4670/5173]	Time 0.644 (0.623)	Data 1.22e-04 (2.58e-04)	Tok/s 26207 (22751)	Loss/tok 3.4745 (3.3374)	LR 1.563e-05
0: TRAIN [2][4680/5173]	Time 0.649 (0.623)	Data 1.25e-04 (2.58e-04)	Tok/s 25865 (22753)	Loss/tok 3.4929 (3.3374)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][4690/5173]	Time 0.702 (0.623)	Data 2.66e-04 (2.57e-04)	Tok/s 33309 (22757)	Loss/tok 3.5137 (3.3376)	LR 1.563e-05
0: TRAIN [2][4700/5173]	Time 0.581 (0.623)	Data 1.26e-04 (2.57e-04)	Tok/s 17599 (22754)	Loss/tok 3.0780 (3.3376)	LR 1.563e-05
0: TRAIN [2][4710/5173]	Time 0.702 (0.623)	Data 3.05e-04 (2.57e-04)	Tok/s 33311 (22756)	Loss/tok 3.5350 (3.3378)	LR 1.563e-05
0: TRAIN [2][4720/5173]	Time 0.644 (0.623)	Data 1.27e-04 (2.57e-04)	Tok/s 25715 (22764)	Loss/tok 3.3825 (3.3380)	LR 1.563e-05
0: TRAIN [2][4730/5173]	Time 0.521 (0.623)	Data 1.14e-04 (2.57e-04)	Tok/s 10163 (22757)	Loss/tok 2.5682 (3.3378)	LR 1.563e-05
0: TRAIN [2][4740/5173]	Time 0.644 (0.623)	Data 1.17e-04 (2.56e-04)	Tok/s 26377 (22759)	Loss/tok 3.3730 (3.3378)	LR 1.563e-05
0: TRAIN [2][4750/5173]	Time 0.523 (0.623)	Data 1.14e-04 (2.56e-04)	Tok/s 9773 (22756)	Loss/tok 2.6542 (3.3377)	LR 1.563e-05
0: TRAIN [2][4760/5173]	Time 0.708 (0.623)	Data 1.16e-04 (2.56e-04)	Tok/s 32517 (22750)	Loss/tok 3.5658 (3.3376)	LR 1.563e-05
0: TRAIN [2][4770/5173]	Time 0.582 (0.623)	Data 1.13e-04 (2.55e-04)	Tok/s 17905 (22744)	Loss/tok 3.0695 (3.3374)	LR 1.563e-05
0: TRAIN [2][4780/5173]	Time 0.581 (0.623)	Data 1.13e-04 (2.55e-04)	Tok/s 17947 (22741)	Loss/tok 3.1458 (3.3373)	LR 1.563e-05
0: TRAIN [2][4790/5173]	Time 0.777 (0.623)	Data 1.14e-04 (2.55e-04)	Tok/s 38772 (22749)	Loss/tok 3.6091 (3.3374)	LR 1.563e-05
0: TRAIN [2][4800/5173]	Time 0.583 (0.623)	Data 1.19e-04 (2.55e-04)	Tok/s 17899 (22754)	Loss/tok 3.0229 (3.3374)	LR 1.563e-05
0: TRAIN [2][4810/5173]	Time 0.583 (0.623)	Data 1.20e-04 (2.54e-04)	Tok/s 17994 (22751)	Loss/tok 3.1440 (3.3373)	LR 1.563e-05
0: TRAIN [2][4820/5173]	Time 0.581 (0.623)	Data 7.72e-04 (2.54e-04)	Tok/s 17839 (22745)	Loss/tok 3.1874 (3.3372)	LR 1.563e-05
0: TRAIN [2][4830/5173]	Time 0.703 (0.623)	Data 1.18e-04 (2.54e-04)	Tok/s 33350 (22743)	Loss/tok 3.4723 (3.3372)	LR 1.563e-05
0: TRAIN [2][4840/5173]	Time 0.582 (0.623)	Data 1.15e-04 (2.54e-04)	Tok/s 17640 (22739)	Loss/tok 3.1165 (3.3372)	LR 1.563e-05
0: TRAIN [2][4850/5173]	Time 0.700 (0.623)	Data 1.13e-04 (2.53e-04)	Tok/s 33328 (22737)	Loss/tok 3.4599 (3.3371)	LR 1.563e-05
0: TRAIN [2][4860/5173]	Time 0.707 (0.623)	Data 1.16e-04 (2.53e-04)	Tok/s 33067 (22739)	Loss/tok 3.5799 (3.3372)	LR 1.563e-05
0: TRAIN [2][4870/5173]	Time 0.583 (0.623)	Data 1.17e-04 (2.53e-04)	Tok/s 17833 (22740)	Loss/tok 3.2158 (3.3372)	LR 1.563e-05
0: TRAIN [2][4880/5173]	Time 0.641 (0.623)	Data 1.14e-04 (2.53e-04)	Tok/s 25846 (22740)	Loss/tok 3.3897 (3.3372)	LR 1.563e-05
0: TRAIN [2][4890/5173]	Time 0.646 (0.623)	Data 1.18e-04 (2.52e-04)	Tok/s 26628 (22732)	Loss/tok 3.2484 (3.3371)	LR 1.563e-05
0: TRAIN [2][4900/5173]	Time 0.519 (0.623)	Data 1.29e-04 (2.52e-04)	Tok/s 10040 (22732)	Loss/tok 2.6509 (3.3371)	LR 1.563e-05
0: TRAIN [2][4910/5173]	Time 0.649 (0.623)	Data 1.14e-04 (2.52e-04)	Tok/s 25914 (22731)	Loss/tok 3.2965 (3.3371)	LR 1.563e-05
0: TRAIN [2][4920/5173]	Time 0.583 (0.623)	Data 1.14e-04 (2.52e-04)	Tok/s 17853 (22724)	Loss/tok 3.0791 (3.3369)	LR 1.563e-05
0: TRAIN [2][4930/5173]	Time 0.583 (0.623)	Data 1.17e-04 (2.51e-04)	Tok/s 17658 (22722)	Loss/tok 3.1293 (3.3369)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][4940/5173]	Time 0.581 (0.623)	Data 1.14e-04 (2.51e-04)	Tok/s 17921 (22723)	Loss/tok 3.1605 (3.3370)	LR 1.563e-05
0: TRAIN [2][4950/5173]	Time 0.707 (0.623)	Data 1.22e-04 (2.51e-04)	Tok/s 32764 (22722)	Loss/tok 3.5153 (3.3369)	LR 1.563e-05
0: TRAIN [2][4960/5173]	Time 0.520 (0.623)	Data 1.29e-04 (2.51e-04)	Tok/s 10182 (22731)	Loss/tok 2.6624 (3.3372)	LR 1.563e-05
0: TRAIN [2][4970/5173]	Time 0.641 (0.623)	Data 1.18e-04 (2.50e-04)	Tok/s 26535 (22726)	Loss/tok 3.3125 (3.3371)	LR 1.563e-05
0: TRAIN [2][4980/5173]	Time 0.707 (0.623)	Data 1.22e-04 (2.50e-04)	Tok/s 33045 (22736)	Loss/tok 3.5722 (3.3375)	LR 1.563e-05
0: TRAIN [2][4990/5173]	Time 0.522 (0.623)	Data 1.18e-04 (2.50e-04)	Tok/s 9967 (22730)	Loss/tok 2.6251 (3.3373)	LR 1.563e-05
0: TRAIN [2][5000/5173]	Time 0.582 (0.623)	Data 1.16e-04 (2.50e-04)	Tok/s 17811 (22722)	Loss/tok 3.1011 (3.3370)	LR 1.563e-05
0: TRAIN [2][5010/5173]	Time 0.646 (0.623)	Data 2.94e-04 (2.49e-04)	Tok/s 26197 (22721)	Loss/tok 3.3092 (3.3370)	LR 1.563e-05
0: TRAIN [2][5020/5173]	Time 0.709 (0.623)	Data 1.20e-04 (2.49e-04)	Tok/s 32951 (22721)	Loss/tok 3.3505 (3.3371)	LR 1.563e-05
0: TRAIN [2][5030/5173]	Time 0.642 (0.623)	Data 2.67e-04 (2.49e-04)	Tok/s 26082 (22720)	Loss/tok 3.2925 (3.3372)	LR 1.563e-05
0: TRAIN [2][5040/5173]	Time 0.711 (0.623)	Data 1.19e-04 (2.49e-04)	Tok/s 33040 (22719)	Loss/tok 3.4961 (3.3372)	LR 1.563e-05
0: TRAIN [2][5050/5173]	Time 0.783 (0.623)	Data 1.29e-04 (2.49e-04)	Tok/s 38427 (22722)	Loss/tok 3.6155 (3.3374)	LR 1.563e-05
0: TRAIN [2][5060/5173]	Time 0.704 (0.623)	Data 1.22e-04 (2.48e-04)	Tok/s 33316 (22722)	Loss/tok 3.5467 (3.3373)	LR 1.563e-05
0: TRAIN [2][5070/5173]	Time 0.583 (0.623)	Data 1.28e-04 (2.48e-04)	Tok/s 18003 (22720)	Loss/tok 3.0911 (3.3373)	LR 1.563e-05
0: TRAIN [2][5080/5173]	Time 0.584 (0.623)	Data 1.63e-04 (2.48e-04)	Tok/s 17691 (22725)	Loss/tok 3.0495 (3.3374)	LR 1.563e-05
0: TRAIN [2][5090/5173]	Time 0.583 (0.623)	Data 1.66e-04 (2.48e-04)	Tok/s 17845 (22728)	Loss/tok 3.0223 (3.3373)	LR 1.563e-05
0: TRAIN [2][5100/5173]	Time 0.583 (0.623)	Data 1.65e-04 (2.48e-04)	Tok/s 17502 (22727)	Loss/tok 3.2281 (3.3373)	LR 1.563e-05
0: TRAIN [2][5110/5173]	Time 0.581 (0.623)	Data 1.69e-04 (2.48e-04)	Tok/s 17739 (22729)	Loss/tok 3.0552 (3.3374)	LR 1.563e-05
0: TRAIN [2][5120/5173]	Time 0.640 (0.623)	Data 1.59e-04 (2.48e-04)	Tok/s 26220 (22726)	Loss/tok 3.3256 (3.3372)	LR 1.563e-05
0: TRAIN [2][5130/5173]	Time 0.520 (0.623)	Data 1.61e-04 (2.48e-04)	Tok/s 10076 (22731)	Loss/tok 2.7124 (3.3374)	LR 1.563e-05
0: TRAIN [2][5140/5173]	Time 0.706 (0.623)	Data 1.66e-04 (2.47e-04)	Tok/s 33030 (22727)	Loss/tok 3.4676 (3.3372)	LR 1.563e-05
0: TRAIN [2][5150/5173]	Time 0.520 (0.623)	Data 1.78e-04 (2.47e-04)	Tok/s 9823 (22723)	Loss/tok 2.6643 (3.3371)	LR 1.563e-05
0: TRAIN [2][5160/5173]	Time 0.581 (0.623)	Data 1.60e-04 (2.47e-04)	Tok/s 17739 (22720)	Loss/tok 3.2262 (3.3370)	LR 1.563e-05
0: TRAIN [2][5170/5173]	Time 0.583 (0.623)	Data 1.59e-04 (2.47e-04)	Tok/s 17858 (22717)	Loss/tok 3.1131 (3.3369)	LR 1.563e-05
:::MLL 1586127546.679 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 525}}
:::MLL 1586127546.679 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [2][0/8]	Time 0.686 (0.686)	Decoder iters 125.0 (125.0)	Tok/s 23916 (23916)
0: Running moses detokenizer
0: BLEU(score=21.315806037546782, counts=[35265, 16652, 9072, 5132], totals=[64922, 61919, 58916, 55917], precisions=[54.31902898863251, 26.893199179573312, 15.398194038970738, 9.177888656401452], bp=1.0, sys_len=64922, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1586127552.447 eval_accuracy: {"value": 21.32, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 536}}
:::MLL 1586127552.447 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 2	Training Loss: 3.3361	Test BLEU: 21.32
0: Performance: Epoch: 2	Training: 68147 Tok/s
0: Finished epoch 2
:::MLL 1586127552.448 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 558}}
:::MLL 1586127552.449 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1586127552.449 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 515}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 890872127
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][0/5173]	Time 1.216 (1.216)	Data 5.73e-01 (5.73e-01)	Tok/s 13975 (13975)	Loss/tok 3.3089 (3.3089)	LR 1.563e-05
0: TRAIN [3][10/5173]	Time 0.522 (0.654)	Data 1.21e-04 (5.22e-02)	Tok/s 9968 (18581)	Loss/tok 2.7315 (3.2626)	LR 1.563e-05
0: TRAIN [3][20/5173]	Time 0.708 (0.628)	Data 1.22e-04 (2.74e-02)	Tok/s 32962 (19397)	Loss/tok 3.4966 (3.2564)	LR 1.563e-05
0: TRAIN [3][30/5173]	Time 0.704 (0.627)	Data 2.73e-04 (1.86e-02)	Tok/s 32913 (20625)	Loss/tok 3.6005 (3.3053)	LR 1.563e-05
0: TRAIN [3][40/5173]	Time 0.583 (0.630)	Data 1.22e-04 (1.41e-02)	Tok/s 17913 (21568)	Loss/tok 3.2242 (3.3321)	LR 1.563e-05
0: TRAIN [3][50/5173]	Time 0.582 (0.631)	Data 1.44e-04 (1.14e-02)	Tok/s 17651 (21988)	Loss/tok 3.0069 (3.3391)	LR 1.563e-05
0: TRAIN [3][60/5173]	Time 0.581 (0.632)	Data 1.39e-04 (9.55e-03)	Tok/s 17907 (22348)	Loss/tok 3.1005 (3.3490)	LR 1.563e-05
0: TRAIN [3][70/5173]	Time 0.640 (0.628)	Data 1.18e-04 (8.23e-03)	Tok/s 26391 (22056)	Loss/tok 3.2581 (3.3320)	LR 1.563e-05
0: TRAIN [3][80/5173]	Time 0.585 (0.627)	Data 1.30e-04 (7.23e-03)	Tok/s 17539 (22069)	Loss/tok 3.2130 (3.3306)	LR 1.563e-05
0: TRAIN [3][90/5173]	Time 0.583 (0.627)	Data 1.29e-04 (6.45e-03)	Tok/s 17829 (22142)	Loss/tok 3.0098 (3.3381)	LR 1.563e-05
0: TRAIN [3][100/5173]	Time 0.641 (0.626)	Data 1.17e-04 (5.82e-03)	Tok/s 26214 (22079)	Loss/tok 3.4035 (3.3344)	LR 1.563e-05
0: TRAIN [3][110/5173]	Time 0.643 (0.626)	Data 1.30e-04 (5.31e-03)	Tok/s 25967 (22270)	Loss/tok 3.4355 (3.3346)	LR 1.563e-05
0: TRAIN [3][120/5173]	Time 0.782 (0.626)	Data 1.24e-04 (4.89e-03)	Tok/s 38079 (22350)	Loss/tok 3.8067 (3.3380)	LR 1.563e-05
0: TRAIN [3][130/5173]	Time 0.582 (0.626)	Data 1.18e-04 (4.53e-03)	Tok/s 17603 (22370)	Loss/tok 3.0590 (3.3341)	LR 1.563e-05
0: TRAIN [3][140/5173]	Time 0.583 (0.626)	Data 2.89e-04 (4.21e-03)	Tok/s 17596 (22401)	Loss/tok 3.1325 (3.3366)	LR 1.563e-05
0: TRAIN [3][150/5173]	Time 0.640 (0.628)	Data 1.22e-04 (3.95e-03)	Tok/s 26629 (22706)	Loss/tok 3.3502 (3.3447)	LR 1.563e-05
0: TRAIN [3][160/5173]	Time 0.583 (0.627)	Data 1.24e-04 (3.71e-03)	Tok/s 17876 (22691)	Loss/tok 3.1571 (3.3446)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][170/5173]	Time 0.581 (0.626)	Data 1.33e-04 (3.50e-03)	Tok/s 17724 (22533)	Loss/tok 3.1868 (3.3425)	LR 1.563e-05
0: TRAIN [3][180/5173]	Time 0.583 (0.625)	Data 1.22e-04 (3.32e-03)	Tok/s 17986 (22537)	Loss/tok 3.1011 (3.3406)	LR 1.563e-05
0: TRAIN [3][190/5173]	Time 0.579 (0.625)	Data 1.23e-04 (3.15e-03)	Tok/s 17874 (22501)	Loss/tok 3.1150 (3.3375)	LR 1.563e-05
0: TRAIN [3][200/5173]	Time 0.703 (0.626)	Data 1.22e-04 (3.00e-03)	Tok/s 33262 (22600)	Loss/tok 3.4815 (3.3396)	LR 1.563e-05
0: TRAIN [3][210/5173]	Time 0.647 (0.626)	Data 1.29e-04 (2.86e-03)	Tok/s 26218 (22681)	Loss/tok 3.4039 (3.3440)	LR 1.563e-05
0: TRAIN [3][220/5173]	Time 0.649 (0.627)	Data 1.25e-04 (2.74e-03)	Tok/s 25753 (22805)	Loss/tok 3.2568 (3.3436)	LR 1.563e-05
0: TRAIN [3][230/5173]	Time 0.639 (0.625)	Data 1.21e-04 (2.63e-03)	Tok/s 26183 (22620)	Loss/tok 3.3168 (3.3392)	LR 1.563e-05
0: TRAIN [3][240/5173]	Time 0.778 (0.626)	Data 1.26e-04 (2.52e-03)	Tok/s 38299 (22688)	Loss/tok 3.7159 (3.3420)	LR 1.563e-05
0: TRAIN [3][250/5173]	Time 0.706 (0.625)	Data 2.74e-04 (2.43e-03)	Tok/s 32995 (22618)	Loss/tok 3.4467 (3.3381)	LR 1.563e-05
0: TRAIN [3][260/5173]	Time 0.645 (0.625)	Data 1.14e-04 (2.34e-03)	Tok/s 26157 (22552)	Loss/tok 3.3319 (3.3360)	LR 1.563e-05
0: TRAIN [3][270/5173]	Time 0.644 (0.625)	Data 1.97e-04 (2.26e-03)	Tok/s 25965 (22672)	Loss/tok 3.3688 (3.3364)	LR 1.563e-05
0: TRAIN [3][280/5173]	Time 0.583 (0.624)	Data 1.76e-04 (2.19e-03)	Tok/s 17989 (22588)	Loss/tok 3.1594 (3.3336)	LR 1.563e-05
0: TRAIN [3][290/5173]	Time 0.580 (0.624)	Data 1.14e-04 (2.12e-03)	Tok/s 17856 (22566)	Loss/tok 3.1015 (3.3308)	LR 1.563e-05
0: TRAIN [3][300/5173]	Time 0.649 (0.624)	Data 1.23e-04 (2.05e-03)	Tok/s 25972 (22596)	Loss/tok 3.2626 (3.3288)	LR 1.563e-05
0: TRAIN [3][310/5173]	Time 0.583 (0.624)	Data 1.51e-04 (1.99e-03)	Tok/s 18017 (22619)	Loss/tok 3.1872 (3.3281)	LR 1.563e-05
0: TRAIN [3][320/5173]	Time 0.581 (0.624)	Data 1.38e-04 (1.93e-03)	Tok/s 17395 (22670)	Loss/tok 3.1871 (3.3289)	LR 1.563e-05
0: TRAIN [3][330/5173]	Time 0.647 (0.625)	Data 1.27e-04 (1.88e-03)	Tok/s 26182 (22710)	Loss/tok 3.4127 (3.3293)	LR 1.563e-05
0: TRAIN [3][340/5173]	Time 0.643 (0.624)	Data 1.25e-04 (1.83e-03)	Tok/s 25575 (22678)	Loss/tok 3.3696 (3.3283)	LR 1.563e-05
0: TRAIN [3][350/5173]	Time 0.644 (0.624)	Data 1.18e-04 (1.78e-03)	Tok/s 26002 (22632)	Loss/tok 3.4118 (3.3256)	LR 1.563e-05
0: TRAIN [3][360/5173]	Time 0.579 (0.624)	Data 1.19e-04 (1.73e-03)	Tok/s 17899 (22653)	Loss/tok 3.0997 (3.3264)	LR 1.563e-05
0: TRAIN [3][370/5173]	Time 0.583 (0.623)	Data 1.21e-04 (1.69e-03)	Tok/s 17512 (22634)	Loss/tok 3.0408 (3.3248)	LR 1.563e-05
0: TRAIN [3][380/5173]	Time 0.581 (0.623)	Data 1.24e-04 (1.65e-03)	Tok/s 17522 (22629)	Loss/tok 3.1345 (3.3237)	LR 1.563e-05
0: TRAIN [3][390/5173]	Time 0.583 (0.623)	Data 1.16e-04 (1.61e-03)	Tok/s 17591 (22606)	Loss/tok 3.0972 (3.3223)	LR 1.563e-05
0: TRAIN [3][400/5173]	Time 0.582 (0.623)	Data 1.19e-04 (1.57e-03)	Tok/s 17276 (22615)	Loss/tok 3.1073 (3.3233)	LR 1.563e-05
0: TRAIN [3][410/5173]	Time 0.704 (0.623)	Data 3.11e-04 (1.54e-03)	Tok/s 33143 (22649)	Loss/tok 3.5603 (3.3245)	LR 1.563e-05
0: TRAIN [3][420/5173]	Time 0.585 (0.623)	Data 1.22e-04 (1.51e-03)	Tok/s 17802 (22665)	Loss/tok 3.2021 (3.3248)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][430/5173]	Time 0.583 (0.624)	Data 1.21e-04 (1.47e-03)	Tok/s 17763 (22676)	Loss/tok 3.1278 (3.3253)	LR 1.563e-05
0: TRAIN [3][440/5173]	Time 0.584 (0.624)	Data 3.18e-04 (1.44e-03)	Tok/s 17633 (22680)	Loss/tok 3.0525 (3.3258)	LR 1.563e-05
0: TRAIN [3][450/5173]	Time 0.642 (0.624)	Data 1.19e-04 (1.42e-03)	Tok/s 26211 (22693)	Loss/tok 3.3577 (3.3263)	LR 1.563e-05
0: TRAIN [3][460/5173]	Time 0.583 (0.623)	Data 1.18e-04 (1.39e-03)	Tok/s 18007 (22673)	Loss/tok 3.0848 (3.3254)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][470/5173]	Time 0.583 (0.623)	Data 3.08e-04 (1.36e-03)	Tok/s 17953 (22647)	Loss/tok 3.0597 (3.3253)	LR 1.563e-05
0: TRAIN [3][480/5173]	Time 0.581 (0.623)	Data 1.17e-04 (1.34e-03)	Tok/s 17842 (22624)	Loss/tok 3.1270 (3.3255)	LR 1.563e-05
0: TRAIN [3][490/5173]	Time 0.775 (0.624)	Data 1.24e-04 (1.31e-03)	Tok/s 38462 (22705)	Loss/tok 3.7012 (3.3292)	LR 1.563e-05
0: TRAIN [3][500/5173]	Time 0.651 (0.623)	Data 1.20e-04 (1.29e-03)	Tok/s 25699 (22610)	Loss/tok 3.2363 (3.3269)	LR 1.563e-05
0: TRAIN [3][510/5173]	Time 0.584 (0.623)	Data 1.24e-04 (1.27e-03)	Tok/s 17467 (22668)	Loss/tok 3.0652 (3.3279)	LR 1.563e-05
0: TRAIN [3][520/5173]	Time 0.643 (0.623)	Data 1.24e-04 (1.24e-03)	Tok/s 26148 (22701)	Loss/tok 3.3939 (3.3280)	LR 1.563e-05
0: TRAIN [3][530/5173]	Time 0.585 (0.623)	Data 1.24e-04 (1.22e-03)	Tok/s 17481 (22701)	Loss/tok 3.1422 (3.3274)	LR 1.563e-05
0: TRAIN [3][540/5173]	Time 0.583 (0.624)	Data 1.28e-04 (1.20e-03)	Tok/s 17972 (22749)	Loss/tok 3.0822 (3.3286)	LR 1.563e-05
0: TRAIN [3][550/5173]	Time 0.647 (0.624)	Data 1.29e-04 (1.18e-03)	Tok/s 26055 (22716)	Loss/tok 3.2447 (3.3268)	LR 1.563e-05
0: TRAIN [3][560/5173]	Time 0.583 (0.623)	Data 1.46e-04 (1.17e-03)	Tok/s 17766 (22672)	Loss/tok 3.1014 (3.3249)	LR 1.563e-05
0: TRAIN [3][570/5173]	Time 0.581 (0.623)	Data 1.49e-04 (1.15e-03)	Tok/s 17660 (22678)	Loss/tok 3.1279 (3.3259)	LR 1.563e-05
0: TRAIN [3][580/5173]	Time 0.647 (0.623)	Data 1.14e-04 (1.13e-03)	Tok/s 25865 (22672)	Loss/tok 3.3740 (3.3257)	LR 1.563e-05
0: TRAIN [3][590/5173]	Time 0.644 (0.623)	Data 1.24e-04 (1.11e-03)	Tok/s 26202 (22605)	Loss/tok 3.2923 (3.3237)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][600/5173]	Time 0.780 (0.623)	Data 2.91e-04 (1.10e-03)	Tok/s 38750 (22624)	Loss/tok 3.5691 (3.3244)	LR 1.563e-05
0: TRAIN [3][610/5173]	Time 0.584 (0.622)	Data 1.28e-04 (1.08e-03)	Tok/s 17863 (22581)	Loss/tok 3.0518 (3.3236)	LR 1.563e-05
0: TRAIN [3][620/5173]	Time 0.584 (0.623)	Data 1.30e-04 (1.07e-03)	Tok/s 17768 (22679)	Loss/tok 3.0112 (3.3267)	LR 1.563e-05
0: TRAIN [3][630/5173]	Time 0.582 (0.623)	Data 1.20e-04 (1.05e-03)	Tok/s 17464 (22720)	Loss/tok 3.1200 (3.3283)	LR 1.563e-05
0: TRAIN [3][640/5173]	Time 0.584 (0.623)	Data 1.20e-04 (1.04e-03)	Tok/s 17447 (22724)	Loss/tok 3.1582 (3.3288)	LR 1.563e-05
0: TRAIN [3][650/5173]	Time 0.709 (0.624)	Data 1.25e-04 (1.02e-03)	Tok/s 33138 (22741)	Loss/tok 3.4995 (3.3293)	LR 1.563e-05
0: TRAIN [3][660/5173]	Time 0.640 (0.623)	Data 1.16e-04 (1.01e-03)	Tok/s 25902 (22680)	Loss/tok 3.3080 (3.3274)	LR 1.563e-05
0: TRAIN [3][670/5173]	Time 0.582 (0.622)	Data 1.26e-04 (9.97e-04)	Tok/s 17841 (22573)	Loss/tok 3.0612 (3.3248)	LR 1.563e-05
0: TRAIN [3][680/5173]	Time 0.779 (0.622)	Data 1.22e-04 (9.84e-04)	Tok/s 38403 (22557)	Loss/tok 3.7168 (3.3251)	LR 1.563e-05
0: TRAIN [3][690/5173]	Time 0.647 (0.622)	Data 1.19e-04 (9.72e-04)	Tok/s 25921 (22556)	Loss/tok 3.4321 (3.3251)	LR 1.563e-05
0: TRAIN [3][700/5173]	Time 0.584 (0.622)	Data 1.26e-04 (9.60e-04)	Tok/s 17592 (22582)	Loss/tok 3.2760 (3.3264)	LR 1.563e-05
0: TRAIN [3][710/5173]	Time 0.781 (0.622)	Data 1.22e-04 (9.48e-04)	Tok/s 37682 (22612)	Loss/tok 3.6454 (3.3268)	LR 1.563e-05
0: TRAIN [3][720/5173]	Time 0.581 (0.622)	Data 1.18e-04 (9.37e-04)	Tok/s 17737 (22566)	Loss/tok 3.1545 (3.3255)	LR 1.563e-05
0: TRAIN [3][730/5173]	Time 0.644 (0.622)	Data 1.18e-04 (9.26e-04)	Tok/s 25779 (22579)	Loss/tok 3.3616 (3.3253)	LR 1.563e-05
0: TRAIN [3][740/5173]	Time 0.581 (0.622)	Data 1.23e-04 (9.16e-04)	Tok/s 17647 (22597)	Loss/tok 3.0336 (3.3265)	LR 1.563e-05
0: TRAIN [3][750/5173]	Time 0.582 (0.622)	Data 3.26e-04 (9.06e-04)	Tok/s 17678 (22617)	Loss/tok 3.1342 (3.3266)	LR 1.563e-05
0: TRAIN [3][760/5173]	Time 0.581 (0.622)	Data 1.15e-04 (8.95e-04)	Tok/s 18258 (22597)	Loss/tok 3.1153 (3.3260)	LR 1.563e-05
0: TRAIN [3][770/5173]	Time 0.709 (0.622)	Data 1.25e-04 (8.85e-04)	Tok/s 32874 (22585)	Loss/tok 3.5185 (3.3258)	LR 1.563e-05
0: TRAIN [3][780/5173]	Time 0.583 (0.622)	Data 1.26e-04 (8.75e-04)	Tok/s 17648 (22575)	Loss/tok 3.1806 (3.3253)	LR 1.563e-05
0: TRAIN [3][790/5173]	Time 0.705 (0.622)	Data 1.26e-04 (8.66e-04)	Tok/s 32993 (22566)	Loss/tok 3.5038 (3.3246)	LR 1.563e-05
0: TRAIN [3][800/5173]	Time 0.585 (0.622)	Data 1.21e-04 (8.58e-04)	Tok/s 17778 (22534)	Loss/tok 3.1431 (3.3238)	LR 1.563e-05
0: TRAIN [3][810/5173]	Time 0.523 (0.621)	Data 1.26e-04 (8.49e-04)	Tok/s 10241 (22467)	Loss/tok 2.6523 (3.3222)	LR 1.563e-05
0: TRAIN [3][820/5173]	Time 0.646 (0.621)	Data 1.23e-04 (8.40e-04)	Tok/s 25770 (22517)	Loss/tok 3.3543 (3.3232)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][830/5173]	Time 0.521 (0.622)	Data 1.52e-04 (8.32e-04)	Tok/s 10073 (22533)	Loss/tok 2.6971 (3.3237)	LR 1.563e-05
0: TRAIN [3][840/5173]	Time 0.650 (0.622)	Data 1.25e-04 (8.23e-04)	Tok/s 26010 (22552)	Loss/tok 3.3503 (3.3236)	LR 1.563e-05
0: TRAIN [3][850/5173]	Time 0.649 (0.622)	Data 1.21e-04 (8.15e-04)	Tok/s 26219 (22571)	Loss/tok 3.2439 (3.3238)	LR 1.563e-05
0: TRAIN [3][860/5173]	Time 0.642 (0.622)	Data 1.22e-04 (8.08e-04)	Tok/s 25685 (22567)	Loss/tok 3.4086 (3.3242)	LR 1.563e-05
0: TRAIN [3][870/5173]	Time 0.582 (0.622)	Data 1.20e-04 (8.00e-04)	Tok/s 17585 (22604)	Loss/tok 3.0232 (3.3253)	LR 1.563e-05
0: TRAIN [3][880/5173]	Time 0.705 (0.622)	Data 1.18e-04 (7.92e-04)	Tok/s 32991 (22640)	Loss/tok 3.4822 (3.3262)	LR 1.563e-05
0: TRAIN [3][890/5173]	Time 0.644 (0.622)	Data 1.21e-04 (7.85e-04)	Tok/s 26402 (22654)	Loss/tok 3.3919 (3.3270)	LR 1.563e-05
0: TRAIN [3][900/5173]	Time 0.711 (0.622)	Data 1.31e-04 (7.78e-04)	Tok/s 32390 (22643)	Loss/tok 3.6110 (3.3269)	LR 1.563e-05
0: TRAIN [3][910/5173]	Time 0.582 (0.622)	Data 1.21e-04 (7.71e-04)	Tok/s 17750 (22643)	Loss/tok 3.0851 (3.3262)	LR 1.563e-05
0: TRAIN [3][920/5173]	Time 0.522 (0.622)	Data 1.16e-04 (7.64e-04)	Tok/s 10094 (22601)	Loss/tok 2.6131 (3.3251)	LR 1.563e-05
0: TRAIN [3][930/5173]	Time 0.581 (0.622)	Data 3.09e-04 (7.57e-04)	Tok/s 17860 (22621)	Loss/tok 3.1907 (3.3257)	LR 1.563e-05
0: TRAIN [3][940/5173]	Time 0.582 (0.622)	Data 1.28e-04 (7.51e-04)	Tok/s 18231 (22614)	Loss/tok 3.1637 (3.3252)	LR 1.563e-05
0: TRAIN [3][950/5173]	Time 0.581 (0.622)	Data 1.16e-04 (7.44e-04)	Tok/s 18217 (22633)	Loss/tok 3.1715 (3.3266)	LR 1.563e-05
0: TRAIN [3][960/5173]	Time 0.583 (0.622)	Data 1.22e-04 (7.38e-04)	Tok/s 17998 (22656)	Loss/tok 3.1769 (3.3270)	LR 1.563e-05
0: TRAIN [3][970/5173]	Time 0.581 (0.622)	Data 1.19e-04 (7.31e-04)	Tok/s 18076 (22661)	Loss/tok 3.1474 (3.3275)	LR 1.563e-05
0: TRAIN [3][980/5173]	Time 0.583 (0.623)	Data 3.03e-04 (7.26e-04)	Tok/s 17884 (22698)	Loss/tok 3.1416 (3.3283)	LR 1.563e-05
0: TRAIN [3][990/5173]	Time 0.579 (0.623)	Data 1.15e-04 (7.20e-04)	Tok/s 17775 (22693)	Loss/tok 3.2109 (3.3281)	LR 1.563e-05
0: TRAIN [3][1000/5173]	Time 0.585 (0.622)	Data 1.26e-04 (7.14e-04)	Tok/s 18205 (22650)	Loss/tok 3.0559 (3.3275)	LR 1.563e-05
0: TRAIN [3][1010/5173]	Time 0.583 (0.622)	Data 1.21e-04 (7.08e-04)	Tok/s 17704 (22632)	Loss/tok 3.2807 (3.3276)	LR 1.563e-05
0: TRAIN [3][1020/5173]	Time 0.644 (0.622)	Data 1.19e-04 (7.02e-04)	Tok/s 26083 (22639)	Loss/tok 3.3787 (3.3276)	LR 1.563e-05
0: TRAIN [3][1030/5173]	Time 0.520 (0.622)	Data 1.21e-04 (6.97e-04)	Tok/s 10093 (22609)	Loss/tok 2.6267 (3.3268)	LR 1.563e-05
0: TRAIN [3][1040/5173]	Time 0.708 (0.622)	Data 1.63e-04 (6.91e-04)	Tok/s 33507 (22615)	Loss/tok 3.4185 (3.3265)	LR 1.563e-05
0: TRAIN [3][1050/5173]	Time 0.582 (0.622)	Data 2.87e-04 (6.87e-04)	Tok/s 17896 (22627)	Loss/tok 3.1161 (3.3270)	LR 1.563e-05
0: TRAIN [3][1060/5173]	Time 0.583 (0.622)	Data 1.38e-04 (6.81e-04)	Tok/s 17971 (22617)	Loss/tok 3.1860 (3.3267)	LR 1.563e-05
0: TRAIN [3][1070/5173]	Time 0.519 (0.622)	Data 2.77e-04 (6.77e-04)	Tok/s 10243 (22595)	Loss/tok 2.6382 (3.3261)	LR 1.563e-05
0: TRAIN [3][1080/5173]	Time 0.643 (0.622)	Data 1.21e-04 (6.72e-04)	Tok/s 26303 (22577)	Loss/tok 3.2871 (3.3258)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1090/5173]	Time 0.582 (0.622)	Data 1.25e-04 (6.67e-04)	Tok/s 17416 (22581)	Loss/tok 3.1454 (3.3255)	LR 1.563e-05
0: TRAIN [3][1100/5173]	Time 0.520 (0.621)	Data 1.24e-04 (6.62e-04)	Tok/s 10232 (22538)	Loss/tok 2.6966 (3.3248)	LR 1.563e-05
0: TRAIN [3][1110/5173]	Time 0.648 (0.622)	Data 1.18e-04 (6.57e-04)	Tok/s 25746 (22559)	Loss/tok 3.3698 (3.3248)	LR 1.563e-05
0: TRAIN [3][1120/5173]	Time 0.783 (0.622)	Data 1.28e-04 (6.53e-04)	Tok/s 37957 (22550)	Loss/tok 3.6187 (3.3246)	LR 1.563e-05
0: TRAIN [3][1130/5173]	Time 0.648 (0.621)	Data 1.23e-04 (6.48e-04)	Tok/s 26070 (22528)	Loss/tok 3.2717 (3.3237)	LR 1.563e-05
0: TRAIN [3][1140/5173]	Time 0.583 (0.621)	Data 1.21e-04 (6.44e-04)	Tok/s 18010 (22538)	Loss/tok 3.1852 (3.3234)	LR 1.563e-05
0: TRAIN [3][1150/5173]	Time 0.641 (0.621)	Data 1.26e-04 (6.39e-04)	Tok/s 26028 (22548)	Loss/tok 3.3513 (3.3241)	LR 1.563e-05
0: TRAIN [3][1160/5173]	Time 0.521 (0.622)	Data 1.37e-04 (6.35e-04)	Tok/s 10237 (22595)	Loss/tok 2.6837 (3.3265)	LR 1.563e-05
0: TRAIN [3][1170/5173]	Time 0.705 (0.622)	Data 1.24e-04 (6.31e-04)	Tok/s 32912 (22595)	Loss/tok 3.6273 (3.3266)	LR 1.563e-05
0: TRAIN [3][1180/5173]	Time 0.520 (0.622)	Data 1.29e-04 (6.27e-04)	Tok/s 10058 (22569)	Loss/tok 2.6900 (3.3259)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1190/5173]	Time 0.776 (0.622)	Data 1.28e-04 (6.23e-04)	Tok/s 38022 (22581)	Loss/tok 3.6374 (3.3269)	LR 1.563e-05
0: TRAIN [3][1200/5173]	Time 0.584 (0.622)	Data 1.29e-04 (6.19e-04)	Tok/s 17852 (22612)	Loss/tok 3.1882 (3.3277)	LR 1.563e-05
0: TRAIN [3][1210/5173]	Time 0.580 (0.622)	Data 1.37e-04 (6.15e-04)	Tok/s 17923 (22649)	Loss/tok 3.1258 (3.3288)	LR 1.563e-05
0: TRAIN [3][1220/5173]	Time 0.581 (0.622)	Data 1.36e-04 (6.11e-04)	Tok/s 17827 (22604)	Loss/tok 3.0839 (3.3275)	LR 1.563e-05
0: TRAIN [3][1230/5173]	Time 0.585 (0.622)	Data 1.25e-04 (6.08e-04)	Tok/s 17272 (22610)	Loss/tok 3.0605 (3.3274)	LR 1.563e-05
0: TRAIN [3][1240/5173]	Time 0.644 (0.622)	Data 1.25e-04 (6.04e-04)	Tok/s 25899 (22597)	Loss/tok 3.3603 (3.3269)	LR 1.563e-05
0: TRAIN [3][1250/5173]	Time 0.582 (0.622)	Data 1.39e-04 (6.00e-04)	Tok/s 17971 (22603)	Loss/tok 3.2341 (3.3272)	LR 1.563e-05
0: TRAIN [3][1260/5173]	Time 0.779 (0.622)	Data 1.32e-04 (5.97e-04)	Tok/s 37925 (22623)	Loss/tok 3.7286 (3.3281)	LR 1.563e-05
0: TRAIN [3][1270/5173]	Time 0.705 (0.622)	Data 1.20e-04 (5.93e-04)	Tok/s 32970 (22633)	Loss/tok 3.4625 (3.3283)	LR 1.563e-05
0: TRAIN [3][1280/5173]	Time 0.520 (0.622)	Data 1.39e-04 (5.90e-04)	Tok/s 10161 (22644)	Loss/tok 2.7501 (3.3296)	LR 1.563e-05
0: TRAIN [3][1290/5173]	Time 0.581 (0.622)	Data 3.14e-04 (5.87e-04)	Tok/s 17489 (22666)	Loss/tok 3.1953 (3.3300)	LR 1.563e-05
0: TRAIN [3][1300/5173]	Time 0.585 (0.623)	Data 1.33e-04 (5.84e-04)	Tok/s 17630 (22686)	Loss/tok 3.1041 (3.3308)	LR 1.563e-05
0: TRAIN [3][1310/5173]	Time 0.580 (0.623)	Data 1.32e-04 (5.81e-04)	Tok/s 17883 (22697)	Loss/tok 3.1100 (3.3309)	LR 1.563e-05
0: TRAIN [3][1320/5173]	Time 0.580 (0.623)	Data 1.28e-04 (5.77e-04)	Tok/s 17869 (22705)	Loss/tok 3.2004 (3.3304)	LR 1.563e-05
0: TRAIN [3][1330/5173]	Time 0.584 (0.623)	Data 1.25e-04 (5.74e-04)	Tok/s 17610 (22704)	Loss/tok 3.1413 (3.3300)	LR 1.563e-05
0: TRAIN [3][1340/5173]	Time 0.636 (0.623)	Data 1.32e-04 (5.71e-04)	Tok/s 26321 (22735)	Loss/tok 3.3400 (3.3309)	LR 1.563e-05
0: TRAIN [3][1350/5173]	Time 0.580 (0.623)	Data 1.38e-04 (5.68e-04)	Tok/s 17898 (22739)	Loss/tok 3.1785 (3.3311)	LR 1.563e-05
0: TRAIN [3][1360/5173]	Time 0.645 (0.623)	Data 2.88e-04 (5.65e-04)	Tok/s 25984 (22737)	Loss/tok 3.3227 (3.3308)	LR 1.563e-05
0: TRAIN [3][1370/5173]	Time 0.703 (0.623)	Data 1.23e-04 (5.62e-04)	Tok/s 32989 (22719)	Loss/tok 3.4908 (3.3302)	LR 1.563e-05
0: TRAIN [3][1380/5173]	Time 0.778 (0.623)	Data 1.35e-04 (5.59e-04)	Tok/s 38342 (22727)	Loss/tok 3.6376 (3.3303)	LR 1.563e-05
0: TRAIN [3][1390/5173]	Time 0.647 (0.623)	Data 1.39e-04 (5.56e-04)	Tok/s 26091 (22763)	Loss/tok 3.4208 (3.3314)	LR 1.563e-05
0: TRAIN [3][1400/5173]	Time 0.784 (0.623)	Data 1.28e-04 (5.53e-04)	Tok/s 37798 (22753)	Loss/tok 3.6437 (3.3315)	LR 1.563e-05
0: TRAIN [3][1410/5173]	Time 0.645 (0.623)	Data 1.29e-04 (5.50e-04)	Tok/s 25936 (22741)	Loss/tok 3.2778 (3.3308)	LR 1.563e-05
0: TRAIN [3][1420/5173]	Time 0.581 (0.623)	Data 1.33e-04 (5.47e-04)	Tok/s 17517 (22726)	Loss/tok 3.0667 (3.3306)	LR 1.563e-05
0: TRAIN [3][1430/5173]	Time 0.520 (0.623)	Data 1.67e-04 (5.45e-04)	Tok/s 10123 (22730)	Loss/tok 2.7483 (3.3311)	LR 1.563e-05
0: TRAIN [3][1440/5173]	Time 0.522 (0.623)	Data 1.52e-04 (5.43e-04)	Tok/s 9925 (22722)	Loss/tok 2.6406 (3.3311)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1450/5173]	Time 0.581 (0.623)	Data 1.24e-04 (5.40e-04)	Tok/s 18068 (22705)	Loss/tok 3.1678 (3.3305)	LR 1.563e-05
0: TRAIN [3][1460/5173]	Time 0.641 (0.623)	Data 1.20e-04 (5.37e-04)	Tok/s 26214 (22708)	Loss/tok 3.2426 (3.3305)	LR 1.563e-05
0: TRAIN [3][1470/5173]	Time 0.583 (0.623)	Data 1.25e-04 (5.34e-04)	Tok/s 17318 (22705)	Loss/tok 3.1333 (3.3306)	LR 1.563e-05
0: TRAIN [3][1480/5173]	Time 0.581 (0.623)	Data 2.83e-04 (5.32e-04)	Tok/s 17997 (22724)	Loss/tok 3.1286 (3.3310)	LR 1.563e-05
0: TRAIN [3][1490/5173]	Time 0.702 (0.623)	Data 1.45e-04 (5.29e-04)	Tok/s 33062 (22727)	Loss/tok 3.5493 (3.3313)	LR 1.563e-05
0: TRAIN [3][1500/5173]	Time 0.583 (0.623)	Data 1.88e-04 (5.27e-04)	Tok/s 17505 (22738)	Loss/tok 3.1338 (3.3318)	LR 1.563e-05
0: TRAIN [3][1510/5173]	Time 0.706 (0.623)	Data 1.26e-04 (5.24e-04)	Tok/s 32446 (22744)	Loss/tok 3.5262 (3.3321)	LR 1.563e-05
0: TRAIN [3][1520/5173]	Time 0.648 (0.623)	Data 1.53e-04 (5.22e-04)	Tok/s 26034 (22749)	Loss/tok 3.1577 (3.3319)	LR 1.563e-05
0: TRAIN [3][1530/5173]	Time 0.583 (0.623)	Data 1.43e-04 (5.19e-04)	Tok/s 17650 (22758)	Loss/tok 3.0864 (3.3320)	LR 1.563e-05
0: TRAIN [3][1540/5173]	Time 0.580 (0.623)	Data 1.28e-04 (5.17e-04)	Tok/s 17850 (22746)	Loss/tok 3.1835 (3.3317)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1550/5173]	Time 0.708 (0.623)	Data 1.24e-04 (5.15e-04)	Tok/s 32810 (22755)	Loss/tok 3.3963 (3.3321)	LR 1.563e-05
0: TRAIN [3][1560/5173]	Time 0.645 (0.623)	Data 1.29e-04 (5.12e-04)	Tok/s 25663 (22768)	Loss/tok 3.3511 (3.3325)	LR 1.563e-05
0: TRAIN [3][1570/5173]	Time 0.708 (0.623)	Data 1.35e-04 (5.10e-04)	Tok/s 32768 (22774)	Loss/tok 3.5863 (3.3331)	LR 1.563e-05
0: TRAIN [3][1580/5173]	Time 0.584 (0.623)	Data 1.26e-04 (5.07e-04)	Tok/s 17428 (22757)	Loss/tok 3.0521 (3.3326)	LR 1.563e-05
0: TRAIN [3][1590/5173]	Time 0.584 (0.623)	Data 1.28e-04 (5.05e-04)	Tok/s 17847 (22736)	Loss/tok 3.0490 (3.3319)	LR 1.563e-05
0: TRAIN [3][1600/5173]	Time 0.705 (0.623)	Data 1.27e-04 (5.03e-04)	Tok/s 32892 (22726)	Loss/tok 3.5419 (3.3316)	LR 1.563e-05
0: TRAIN [3][1610/5173]	Time 0.583 (0.623)	Data 2.71e-04 (5.01e-04)	Tok/s 17694 (22724)	Loss/tok 3.1254 (3.3316)	LR 1.563e-05
0: TRAIN [3][1620/5173]	Time 0.583 (0.623)	Data 1.29e-04 (4.98e-04)	Tok/s 17756 (22747)	Loss/tok 3.1085 (3.3322)	LR 1.563e-05
0: TRAIN [3][1630/5173]	Time 0.649 (0.623)	Data 1.28e-04 (4.96e-04)	Tok/s 25896 (22754)	Loss/tok 3.3816 (3.3323)	LR 1.563e-05
0: TRAIN [3][1640/5173]	Time 0.643 (0.623)	Data 1.28e-04 (4.94e-04)	Tok/s 26484 (22739)	Loss/tok 3.2387 (3.3316)	LR 1.563e-05
0: TRAIN [3][1650/5173]	Time 0.580 (0.623)	Data 1.04e-04 (4.92e-04)	Tok/s 17861 (22731)	Loss/tok 3.0154 (3.3314)	LR 1.563e-05
0: TRAIN [3][1660/5173]	Time 0.524 (0.623)	Data 9.92e-05 (4.90e-04)	Tok/s 10107 (22712)	Loss/tok 2.7715 (3.3307)	LR 1.563e-05
0: TRAIN [3][1670/5173]	Time 0.647 (0.623)	Data 1.02e-04 (4.87e-04)	Tok/s 25829 (22703)	Loss/tok 3.3328 (3.3304)	LR 1.563e-05
0: TRAIN [3][1680/5173]	Time 0.643 (0.622)	Data 1.06e-04 (4.85e-04)	Tok/s 26255 (22697)	Loss/tok 3.3128 (3.3303)	LR 1.563e-05
0: TRAIN [3][1690/5173]	Time 0.646 (0.623)	Data 9.58e-05 (4.83e-04)	Tok/s 25912 (22720)	Loss/tok 3.2950 (3.3306)	LR 1.563e-05
0: TRAIN [3][1700/5173]	Time 0.582 (0.622)	Data 9.56e-05 (4.81e-04)	Tok/s 17850 (22696)	Loss/tok 3.1362 (3.3300)	LR 1.563e-05
0: TRAIN [3][1710/5173]	Time 0.642 (0.623)	Data 9.97e-05 (4.79e-04)	Tok/s 26182 (22705)	Loss/tok 3.3527 (3.3307)	LR 1.563e-05
0: TRAIN [3][1720/5173]	Time 0.518 (0.622)	Data 9.89e-05 (4.76e-04)	Tok/s 10049 (22692)	Loss/tok 2.6941 (3.3302)	LR 1.563e-05
0: TRAIN [3][1730/5173]	Time 0.581 (0.622)	Data 9.94e-05 (4.74e-04)	Tok/s 17694 (22687)	Loss/tok 3.0565 (3.3307)	LR 1.563e-05
0: TRAIN [3][1740/5173]	Time 0.581 (0.622)	Data 9.32e-05 (4.72e-04)	Tok/s 17913 (22659)	Loss/tok 3.0879 (3.3302)	LR 1.563e-05
0: TRAIN [3][1750/5173]	Time 0.644 (0.622)	Data 9.47e-05 (4.70e-04)	Tok/s 26256 (22646)	Loss/tok 3.3784 (3.3298)	LR 1.563e-05
0: TRAIN [3][1760/5173]	Time 0.579 (0.622)	Data 9.58e-05 (4.68e-04)	Tok/s 18029 (22653)	Loss/tok 3.1595 (3.3302)	LR 1.563e-05
0: TRAIN [3][1770/5173]	Time 0.573 (0.622)	Data 9.18e-05 (4.66e-04)	Tok/s 18045 (22642)	Loss/tok 3.1040 (3.3299)	LR 1.563e-05
0: TRAIN [3][1780/5173]	Time 0.586 (0.622)	Data 1.28e-04 (4.64e-04)	Tok/s 17485 (22648)	Loss/tok 3.1189 (3.3300)	LR 1.563e-05
0: TRAIN [3][1790/5173]	Time 0.521 (0.622)	Data 1.29e-04 (4.62e-04)	Tok/s 9798 (22638)	Loss/tok 2.6652 (3.3298)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1800/5173]	Time 0.584 (0.622)	Data 1.18e-04 (4.60e-04)	Tok/s 17724 (22635)	Loss/tok 3.0868 (3.3299)	LR 1.563e-05
0: TRAIN [3][1810/5173]	Time 0.584 (0.622)	Data 9.16e-05 (4.58e-04)	Tok/s 18009 (22631)	Loss/tok 3.1338 (3.3297)	LR 1.563e-05
0: TRAIN [3][1820/5173]	Time 0.648 (0.622)	Data 9.44e-05 (4.56e-04)	Tok/s 25502 (22624)	Loss/tok 3.4782 (3.3297)	LR 1.563e-05
0: TRAIN [3][1830/5173]	Time 0.580 (0.622)	Data 1.00e-04 (4.54e-04)	Tok/s 17884 (22623)	Loss/tok 3.1050 (3.3298)	LR 1.563e-05
0: TRAIN [3][1840/5173]	Time 0.642 (0.622)	Data 1.05e-04 (4.52e-04)	Tok/s 26022 (22620)	Loss/tok 3.3823 (3.3297)	LR 1.563e-05
0: TRAIN [3][1850/5173]	Time 0.646 (0.622)	Data 2.62e-04 (4.50e-04)	Tok/s 26414 (22644)	Loss/tok 3.3129 (3.3301)	LR 1.563e-05
0: TRAIN [3][1860/5173]	Time 0.583 (0.622)	Data 9.70e-05 (4.48e-04)	Tok/s 18191 (22639)	Loss/tok 3.0849 (3.3300)	LR 1.563e-05
0: TRAIN [3][1870/5173]	Time 0.517 (0.622)	Data 1.24e-04 (4.47e-04)	Tok/s 10116 (22633)	Loss/tok 2.6199 (3.3298)	LR 1.563e-05
0: TRAIN [3][1880/5173]	Time 0.648 (0.622)	Data 1.04e-04 (4.45e-04)	Tok/s 26045 (22618)	Loss/tok 3.2079 (3.3291)	LR 1.563e-05
0: TRAIN [3][1890/5173]	Time 0.583 (0.622)	Data 9.80e-05 (4.43e-04)	Tok/s 17735 (22625)	Loss/tok 3.0570 (3.3293)	LR 1.563e-05
0: TRAIN [3][1900/5173]	Time 0.581 (0.622)	Data 1.61e-04 (4.42e-04)	Tok/s 17974 (22612)	Loss/tok 3.1329 (3.3288)	LR 1.563e-05
0: TRAIN [3][1910/5173]	Time 0.583 (0.622)	Data 3.02e-04 (4.40e-04)	Tok/s 17616 (22634)	Loss/tok 3.2889 (3.3295)	LR 1.563e-05
0: TRAIN [3][1920/5173]	Time 0.642 (0.622)	Data 1.60e-04 (4.39e-04)	Tok/s 26197 (22634)	Loss/tok 3.3180 (3.3294)	LR 1.563e-05
0: TRAIN [3][1930/5173]	Time 0.585 (0.622)	Data 1.13e-04 (4.37e-04)	Tok/s 17882 (22650)	Loss/tok 3.2470 (3.3296)	LR 1.563e-05
0: TRAIN [3][1940/5173]	Time 0.521 (0.622)	Data 9.39e-05 (4.36e-04)	Tok/s 10151 (22654)	Loss/tok 2.7544 (3.3295)	LR 1.563e-05
0: TRAIN [3][1950/5173]	Time 0.584 (0.622)	Data 1.05e-04 (4.34e-04)	Tok/s 17667 (22664)	Loss/tok 3.2708 (3.3297)	LR 1.563e-05
0: TRAIN [3][1960/5173]	Time 0.707 (0.622)	Data 4.29e-04 (4.33e-04)	Tok/s 32998 (22677)	Loss/tok 3.4853 (3.3302)	LR 1.563e-05
0: TRAIN [3][1970/5173]	Time 0.585 (0.622)	Data 3.64e-04 (4.32e-04)	Tok/s 17702 (22682)	Loss/tok 3.1246 (3.3301)	LR 1.563e-05
0: TRAIN [3][1980/5173]	Time 0.637 (0.622)	Data 1.21e-04 (4.30e-04)	Tok/s 26089 (22689)	Loss/tok 3.2505 (3.3305)	LR 1.563e-05
0: TRAIN [3][1990/5173]	Time 0.707 (0.622)	Data 1.22e-04 (4.29e-04)	Tok/s 33295 (22694)	Loss/tok 3.4360 (3.3308)	LR 1.563e-05
0: TRAIN [3][2000/5173]	Time 0.647 (0.622)	Data 1.19e-04 (4.27e-04)	Tok/s 25987 (22694)	Loss/tok 3.3038 (3.3307)	LR 1.563e-05
0: TRAIN [3][2010/5173]	Time 0.780 (0.623)	Data 1.91e-04 (4.26e-04)	Tok/s 37942 (22702)	Loss/tok 3.7241 (3.3312)	LR 1.563e-05
0: TRAIN [3][2020/5173]	Time 0.647 (0.623)	Data 1.24e-04 (4.25e-04)	Tok/s 26252 (22705)	Loss/tok 3.4101 (3.3313)	LR 1.563e-05
0: TRAIN [3][2030/5173]	Time 0.581 (0.622)	Data 1.23e-04 (4.24e-04)	Tok/s 17862 (22695)	Loss/tok 3.1117 (3.3309)	LR 1.563e-05
0: TRAIN [3][2040/5173]	Time 0.706 (0.622)	Data 1.32e-04 (4.22e-04)	Tok/s 32649 (22686)	Loss/tok 3.6074 (3.3309)	LR 1.563e-05
0: TRAIN [3][2050/5173]	Time 0.644 (0.622)	Data 3.12e-04 (4.21e-04)	Tok/s 25776 (22688)	Loss/tok 3.3343 (3.3310)	LR 1.563e-05
0: TRAIN [3][2060/5173]	Time 0.642 (0.622)	Data 1.24e-04 (4.20e-04)	Tok/s 26160 (22669)	Loss/tok 3.3820 (3.3304)	LR 1.563e-05
0: TRAIN [3][2070/5173]	Time 0.709 (0.622)	Data 1.26e-04 (4.18e-04)	Tok/s 32512 (22657)	Loss/tok 3.5726 (3.3302)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2080/5173]	Time 0.582 (0.622)	Data 1.24e-04 (4.17e-04)	Tok/s 17840 (22670)	Loss/tok 3.0100 (3.3307)	LR 1.563e-05
0: TRAIN [3][2090/5173]	Time 0.709 (0.622)	Data 1.17e-04 (4.16e-04)	Tok/s 32538 (22674)	Loss/tok 3.5077 (3.3307)	LR 1.563e-05
0: TRAIN [3][2100/5173]	Time 0.582 (0.622)	Data 1.28e-04 (4.14e-04)	Tok/s 17719 (22676)	Loss/tok 3.0027 (3.3309)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][2110/5173]	Time 0.782 (0.623)	Data 2.87e-04 (4.13e-04)	Tok/s 37677 (22705)	Loss/tok 3.7094 (3.3320)	LR 1.563e-05
0: TRAIN [3][2120/5173]	Time 0.582 (0.622)	Data 1.21e-04 (4.12e-04)	Tok/s 17751 (22697)	Loss/tok 3.1194 (3.3317)	LR 1.563e-05
0: TRAIN [3][2130/5173]	Time 0.582 (0.623)	Data 1.70e-04 (4.11e-04)	Tok/s 17940 (22715)	Loss/tok 3.1950 (3.3324)	LR 1.563e-05
0: TRAIN [3][2140/5173]	Time 0.640 (0.623)	Data 1.28e-04 (4.10e-04)	Tok/s 26036 (22723)	Loss/tok 3.2359 (3.3329)	LR 1.563e-05
0: TRAIN [3][2150/5173]	Time 0.644 (0.623)	Data 1.20e-04 (4.08e-04)	Tok/s 25880 (22708)	Loss/tok 3.3846 (3.3325)	LR 1.563e-05
0: TRAIN [3][2160/5173]	Time 0.644 (0.623)	Data 1.30e-04 (4.07e-04)	Tok/s 26026 (22705)	Loss/tok 3.2917 (3.3327)	LR 1.563e-05
0: TRAIN [3][2170/5173]	Time 0.519 (0.622)	Data 1.31e-04 (4.06e-04)	Tok/s 10284 (22690)	Loss/tok 2.6273 (3.3323)	LR 1.563e-05
0: TRAIN [3][2180/5173]	Time 0.582 (0.622)	Data 1.21e-04 (4.05e-04)	Tok/s 17381 (22685)	Loss/tok 3.1389 (3.3321)	LR 1.563e-05
0: TRAIN [3][2190/5173]	Time 0.583 (0.622)	Data 1.27e-04 (4.04e-04)	Tok/s 17901 (22677)	Loss/tok 3.0737 (3.3317)	LR 1.563e-05
0: TRAIN [3][2200/5173]	Time 0.641 (0.622)	Data 3.15e-04 (4.03e-04)	Tok/s 26058 (22681)	Loss/tok 3.3282 (3.3321)	LR 1.563e-05
0: TRAIN [3][2210/5173]	Time 0.782 (0.622)	Data 1.29e-04 (4.01e-04)	Tok/s 37662 (22688)	Loss/tok 3.8136 (3.3327)	LR 1.563e-05
0: TRAIN [3][2220/5173]	Time 0.580 (0.622)	Data 1.20e-04 (4.00e-04)	Tok/s 18103 (22678)	Loss/tok 3.1650 (3.3329)	LR 1.563e-05
0: TRAIN [3][2230/5173]	Time 0.640 (0.622)	Data 3.00e-04 (4.00e-04)	Tok/s 26149 (22682)	Loss/tok 3.4116 (3.3327)	LR 1.563e-05
0: TRAIN [3][2240/5173]	Time 0.584 (0.622)	Data 1.20e-04 (3.98e-04)	Tok/s 18091 (22667)	Loss/tok 3.0552 (3.3322)	LR 1.563e-05
0: TRAIN [3][2250/5173]	Time 0.707 (0.622)	Data 1.22e-04 (3.97e-04)	Tok/s 32893 (22677)	Loss/tok 3.4113 (3.3324)	LR 1.563e-05
0: TRAIN [3][2260/5173]	Time 0.645 (0.622)	Data 1.30e-04 (3.96e-04)	Tok/s 25927 (22672)	Loss/tok 3.3848 (3.3321)	LR 1.563e-05
0: TRAIN [3][2270/5173]	Time 0.581 (0.622)	Data 1.27e-04 (3.95e-04)	Tok/s 17912 (22683)	Loss/tok 3.0954 (3.3326)	LR 1.563e-05
0: TRAIN [3][2280/5173]	Time 0.585 (0.622)	Data 1.18e-04 (3.94e-04)	Tok/s 17509 (22676)	Loss/tok 3.1220 (3.3324)	LR 1.563e-05
0: TRAIN [3][2290/5173]	Time 0.647 (0.622)	Data 1.28e-04 (3.93e-04)	Tok/s 26108 (22671)	Loss/tok 3.2670 (3.3323)	LR 1.563e-05
0: TRAIN [3][2300/5173]	Time 0.649 (0.622)	Data 1.24e-04 (3.92e-04)	Tok/s 26180 (22668)	Loss/tok 3.3130 (3.3321)	LR 1.563e-05
0: TRAIN [3][2310/5173]	Time 0.584 (0.622)	Data 1.18e-04 (3.91e-04)	Tok/s 17531 (22677)	Loss/tok 3.0758 (3.3324)	LR 1.563e-05
0: TRAIN [3][2320/5173]	Time 0.520 (0.622)	Data 1.25e-04 (3.90e-04)	Tok/s 10242 (22685)	Loss/tok 2.7040 (3.3326)	LR 1.563e-05
0: TRAIN [3][2330/5173]	Time 0.582 (0.622)	Data 1.24e-04 (3.89e-04)	Tok/s 17509 (22684)	Loss/tok 3.0117 (3.3327)	LR 1.563e-05
0: TRAIN [3][2340/5173]	Time 0.582 (0.623)	Data 1.35e-04 (3.88e-04)	Tok/s 17920 (22697)	Loss/tok 3.1210 (3.3333)	LR 1.563e-05
0: TRAIN [3][2350/5173]	Time 0.581 (0.622)	Data 1.30e-04 (3.87e-04)	Tok/s 17452 (22683)	Loss/tok 3.2108 (3.3327)	LR 1.563e-05
0: TRAIN [3][2360/5173]	Time 0.643 (0.622)	Data 3.22e-04 (3.86e-04)	Tok/s 26043 (22677)	Loss/tok 3.3948 (3.3324)	LR 1.563e-05
0: TRAIN [3][2370/5173]	Time 0.647 (0.622)	Data 1.24e-04 (3.85e-04)	Tok/s 26144 (22682)	Loss/tok 3.3641 (3.3325)	LR 1.563e-05
0: TRAIN [3][2380/5173]	Time 0.646 (0.622)	Data 1.28e-04 (3.84e-04)	Tok/s 26142 (22679)	Loss/tok 3.3066 (3.3324)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2390/5173]	Time 0.650 (0.622)	Data 3.58e-04 (3.83e-04)	Tok/s 25566 (22693)	Loss/tok 3.4151 (3.3331)	LR 1.563e-05
0: TRAIN [3][2400/5173]	Time 0.704 (0.623)	Data 1.26e-04 (3.82e-04)	Tok/s 32919 (22701)	Loss/tok 3.5301 (3.3335)	LR 1.563e-05
0: TRAIN [3][2410/5173]	Time 0.582 (0.622)	Data 1.24e-04 (3.81e-04)	Tok/s 17653 (22691)	Loss/tok 3.0716 (3.3331)	LR 1.563e-05
0: TRAIN [3][2420/5173]	Time 0.581 (0.622)	Data 2.97e-04 (3.80e-04)	Tok/s 17868 (22693)	Loss/tok 3.2403 (3.3332)	LR 1.563e-05
0: TRAIN [3][2430/5173]	Time 0.779 (0.622)	Data 3.02e-04 (3.79e-04)	Tok/s 37792 (22692)	Loss/tok 3.8391 (3.3333)	LR 1.563e-05
0: TRAIN [3][2440/5173]	Time 0.648 (0.623)	Data 1.23e-04 (3.78e-04)	Tok/s 25878 (22699)	Loss/tok 3.4372 (3.3333)	LR 1.563e-05
0: TRAIN [3][2450/5173]	Time 0.708 (0.623)	Data 3.01e-04 (3.77e-04)	Tok/s 33036 (22718)	Loss/tok 3.5450 (3.3340)	LR 1.563e-05
0: TRAIN [3][2460/5173]	Time 0.644 (0.623)	Data 1.21e-04 (3.76e-04)	Tok/s 26176 (22702)	Loss/tok 3.3156 (3.3335)	LR 1.563e-05
0: TRAIN [3][2470/5173]	Time 0.583 (0.622)	Data 1.26e-04 (3.75e-04)	Tok/s 17831 (22688)	Loss/tok 3.0628 (3.3331)	LR 1.563e-05
0: TRAIN [3][2480/5173]	Time 0.581 (0.622)	Data 1.26e-04 (3.74e-04)	Tok/s 17643 (22687)	Loss/tok 3.0708 (3.3331)	LR 1.563e-05
0: TRAIN [3][2490/5173]	Time 0.583 (0.622)	Data 1.29e-04 (3.74e-04)	Tok/s 17809 (22691)	Loss/tok 3.0956 (3.3329)	LR 1.563e-05
0: TRAIN [3][2500/5173]	Time 0.581 (0.622)	Data 1.40e-04 (3.73e-04)	Tok/s 17585 (22683)	Loss/tok 3.1276 (3.3326)	LR 1.563e-05
0: TRAIN [3][2510/5173]	Time 0.583 (0.622)	Data 1.40e-04 (3.72e-04)	Tok/s 17788 (22670)	Loss/tok 2.9791 (3.3322)	LR 1.563e-05
0: TRAIN [3][2520/5173]	Time 0.584 (0.622)	Data 1.31e-04 (3.71e-04)	Tok/s 18026 (22670)	Loss/tok 3.1669 (3.3322)	LR 1.563e-05
0: TRAIN [3][2530/5173]	Time 0.578 (0.622)	Data 1.22e-04 (3.70e-04)	Tok/s 18040 (22676)	Loss/tok 3.0707 (3.3322)	LR 1.563e-05
0: TRAIN [3][2540/5173]	Time 0.647 (0.622)	Data 1.30e-04 (3.69e-04)	Tok/s 26093 (22664)	Loss/tok 3.2222 (3.3317)	LR 1.563e-05
0: TRAIN [3][2550/5173]	Time 0.581 (0.622)	Data 1.25e-04 (3.68e-04)	Tok/s 17392 (22651)	Loss/tok 3.1864 (3.3313)	LR 1.563e-05
0: TRAIN [3][2560/5173]	Time 0.520 (0.622)	Data 1.20e-04 (3.67e-04)	Tok/s 10255 (22656)	Loss/tok 2.7569 (3.3316)	LR 1.563e-05
0: TRAIN [3][2570/5173]	Time 0.583 (0.622)	Data 1.28e-04 (3.66e-04)	Tok/s 17750 (22656)	Loss/tok 3.0981 (3.3315)	LR 1.563e-05
0: TRAIN [3][2580/5173]	Time 0.579 (0.622)	Data 1.30e-04 (3.65e-04)	Tok/s 17491 (22643)	Loss/tok 3.2264 (3.3312)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2590/5173]	Time 0.646 (0.622)	Data 1.75e-04 (3.65e-04)	Tok/s 25930 (22656)	Loss/tok 3.3063 (3.3312)	LR 1.563e-05
0: TRAIN [3][2600/5173]	Time 0.582 (0.622)	Data 3.73e-04 (3.64e-04)	Tok/s 17625 (22661)	Loss/tok 3.0687 (3.3314)	LR 1.563e-05
0: TRAIN [3][2610/5173]	Time 0.581 (0.622)	Data 1.23e-04 (3.63e-04)	Tok/s 17720 (22654)	Loss/tok 3.0982 (3.3312)	LR 1.563e-05
0: TRAIN [3][2620/5173]	Time 0.582 (0.622)	Data 1.12e-04 (3.62e-04)	Tok/s 17718 (22663)	Loss/tok 3.1433 (3.3314)	LR 1.563e-05
0: TRAIN [3][2630/5173]	Time 0.650 (0.622)	Data 1.14e-04 (3.62e-04)	Tok/s 26184 (22662)	Loss/tok 3.3465 (3.3314)	LR 1.563e-05
0: TRAIN [3][2640/5173]	Time 0.781 (0.622)	Data 2.76e-04 (3.61e-04)	Tok/s 37614 (22649)	Loss/tok 3.7504 (3.3314)	LR 1.563e-05
0: TRAIN [3][2650/5173]	Time 0.586 (0.622)	Data 1.20e-04 (3.60e-04)	Tok/s 17700 (22655)	Loss/tok 3.0882 (3.3317)	LR 1.563e-05
0: TRAIN [3][2660/5173]	Time 0.580 (0.622)	Data 1.13e-04 (3.59e-04)	Tok/s 17945 (22655)	Loss/tok 3.1216 (3.3316)	LR 1.563e-05
0: TRAIN [3][2670/5173]	Time 0.583 (0.622)	Data 1.16e-04 (3.58e-04)	Tok/s 17840 (22649)	Loss/tok 3.0576 (3.3314)	LR 1.563e-05
0: TRAIN [3][2680/5173]	Time 0.519 (0.622)	Data 1.13e-04 (3.57e-04)	Tok/s 10216 (22642)	Loss/tok 2.6183 (3.3314)	LR 1.563e-05
0: TRAIN [3][2690/5173]	Time 0.706 (0.622)	Data 1.15e-04 (3.57e-04)	Tok/s 33010 (22632)	Loss/tok 3.4347 (3.3311)	LR 1.563e-05
0: TRAIN [3][2700/5173]	Time 0.522 (0.622)	Data 1.34e-04 (3.56e-04)	Tok/s 10155 (22634)	Loss/tok 2.6936 (3.3311)	LR 1.563e-05
0: TRAIN [3][2710/5173]	Time 0.518 (0.622)	Data 1.13e-04 (3.55e-04)	Tok/s 10362 (22642)	Loss/tok 2.5870 (3.3313)	LR 1.563e-05
0: TRAIN [3][2720/5173]	Time 0.583 (0.622)	Data 1.10e-04 (3.54e-04)	Tok/s 17494 (22639)	Loss/tok 3.1643 (3.3311)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2730/5173]	Time 0.648 (0.622)	Data 1.18e-04 (3.53e-04)	Tok/s 25764 (22633)	Loss/tok 3.3951 (3.3310)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][2740/5173]	Time 0.647 (0.622)	Data 1.17e-04 (3.53e-04)	Tok/s 26087 (22636)	Loss/tok 3.3527 (3.3312)	LR 1.563e-05
0: TRAIN [3][2750/5173]	Time 0.708 (0.622)	Data 1.22e-04 (3.52e-04)	Tok/s 33451 (22640)	Loss/tok 3.5468 (3.3313)	LR 1.563e-05
0: TRAIN [3][2760/5173]	Time 0.581 (0.622)	Data 1.23e-04 (3.51e-04)	Tok/s 17360 (22630)	Loss/tok 3.1435 (3.3311)	LR 1.563e-05
0: TRAIN [3][2770/5173]	Time 0.584 (0.622)	Data 1.18e-04 (3.50e-04)	Tok/s 17827 (22627)	Loss/tok 3.1291 (3.3310)	LR 1.563e-05
0: TRAIN [3][2780/5173]	Time 0.579 (0.622)	Data 1.11e-04 (3.49e-04)	Tok/s 17927 (22624)	Loss/tok 3.0460 (3.3309)	LR 1.563e-05
0: TRAIN [3][2790/5173]	Time 0.640 (0.622)	Data 2.86e-04 (3.49e-04)	Tok/s 26453 (22624)	Loss/tok 3.2701 (3.3308)	LR 1.563e-05
0: TRAIN [3][2800/5173]	Time 0.582 (0.622)	Data 1.15e-04 (3.48e-04)	Tok/s 18107 (22619)	Loss/tok 3.0481 (3.3305)	LR 1.563e-05
0: TRAIN [3][2810/5173]	Time 0.641 (0.622)	Data 1.23e-04 (3.47e-04)	Tok/s 26464 (22636)	Loss/tok 3.3407 (3.3313)	LR 1.563e-05
0: TRAIN [3][2820/5173]	Time 0.582 (0.622)	Data 1.15e-04 (3.46e-04)	Tok/s 17712 (22641)	Loss/tok 3.0012 (3.3312)	LR 1.563e-05
0: TRAIN [3][2830/5173]	Time 0.644 (0.622)	Data 1.18e-04 (3.45e-04)	Tok/s 26106 (22647)	Loss/tok 3.2722 (3.3313)	LR 1.563e-05
0: TRAIN [3][2840/5173]	Time 0.639 (0.622)	Data 1.13e-04 (3.45e-04)	Tok/s 26306 (22649)	Loss/tok 3.3496 (3.3313)	LR 1.563e-05
0: TRAIN [3][2850/5173]	Time 0.708 (0.622)	Data 1.16e-04 (3.44e-04)	Tok/s 33202 (22653)	Loss/tok 3.4385 (3.3316)	LR 1.563e-05
0: TRAIN [3][2860/5173]	Time 0.580 (0.622)	Data 1.19e-04 (3.43e-04)	Tok/s 18003 (22655)	Loss/tok 2.9779 (3.3314)	LR 1.563e-05
0: TRAIN [3][2870/5173]	Time 0.646 (0.622)	Data 1.23e-04 (3.43e-04)	Tok/s 25418 (22655)	Loss/tok 3.3939 (3.3314)	LR 1.563e-05
0: TRAIN [3][2880/5173]	Time 0.521 (0.622)	Data 1.13e-04 (3.42e-04)	Tok/s 10034 (22639)	Loss/tok 2.6752 (3.3310)	LR 1.563e-05
0: TRAIN [3][2890/5173]	Time 0.584 (0.622)	Data 1.19e-04 (3.41e-04)	Tok/s 17806 (22643)	Loss/tok 3.1393 (3.3312)	LR 1.563e-05
0: TRAIN [3][2900/5173]	Time 0.520 (0.622)	Data 1.14e-04 (3.40e-04)	Tok/s 10137 (22632)	Loss/tok 2.6853 (3.3308)	LR 1.563e-05
0: TRAIN [3][2910/5173]	Time 0.582 (0.622)	Data 1.13e-04 (3.40e-04)	Tok/s 17776 (22629)	Loss/tok 3.0393 (3.3307)	LR 1.563e-05
0: TRAIN [3][2920/5173]	Time 0.643 (0.622)	Data 1.18e-04 (3.39e-04)	Tok/s 25819 (22637)	Loss/tok 3.3297 (3.3308)	LR 1.563e-05
0: TRAIN [3][2930/5173]	Time 0.708 (0.622)	Data 1.20e-04 (3.38e-04)	Tok/s 32890 (22643)	Loss/tok 3.4796 (3.3310)	LR 1.563e-05
0: TRAIN [3][2940/5173]	Time 0.582 (0.622)	Data 1.15e-04 (3.38e-04)	Tok/s 17715 (22655)	Loss/tok 3.1954 (3.3312)	LR 1.563e-05
0: TRAIN [3][2950/5173]	Time 0.584 (0.622)	Data 1.31e-04 (3.37e-04)	Tok/s 17711 (22658)	Loss/tok 3.1832 (3.3315)	LR 1.563e-05
0: TRAIN [3][2960/5173]	Time 0.583 (0.622)	Data 1.14e-04 (3.36e-04)	Tok/s 17802 (22657)	Loss/tok 3.0672 (3.3315)	LR 1.563e-05
0: TRAIN [3][2970/5173]	Time 0.650 (0.622)	Data 3.26e-04 (3.35e-04)	Tok/s 25759 (22647)	Loss/tok 3.3563 (3.3311)	LR 1.563e-05
0: TRAIN [3][2980/5173]	Time 0.582 (0.622)	Data 1.47e-04 (3.35e-04)	Tok/s 17745 (22642)	Loss/tok 3.1149 (3.3309)	LR 1.563e-05
0: TRAIN [3][2990/5173]	Time 0.646 (0.622)	Data 1.58e-04 (3.34e-04)	Tok/s 25959 (22637)	Loss/tok 3.2292 (3.3306)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3000/5173]	Time 0.647 (0.622)	Data 1.42e-04 (3.33e-04)	Tok/s 25582 (22639)	Loss/tok 3.3332 (3.3305)	LR 1.563e-05
0: TRAIN [3][3010/5173]	Time 0.581 (0.622)	Data 1.20e-04 (3.33e-04)	Tok/s 17809 (22636)	Loss/tok 2.9727 (3.3303)	LR 1.563e-05
0: TRAIN [3][3020/5173]	Time 0.784 (0.622)	Data 2.99e-04 (3.32e-04)	Tok/s 38480 (22640)	Loss/tok 3.6131 (3.3309)	LR 1.563e-05
0: TRAIN [3][3030/5173]	Time 0.649 (0.622)	Data 1.11e-04 (3.32e-04)	Tok/s 26152 (22657)	Loss/tok 3.2833 (3.3312)	LR 1.563e-05
0: TRAIN [3][3040/5173]	Time 0.642 (0.622)	Data 1.11e-04 (3.31e-04)	Tok/s 26092 (22677)	Loss/tok 3.4107 (3.3318)	LR 1.563e-05
0: TRAIN [3][3050/5173]	Time 0.646 (0.622)	Data 1.16e-04 (3.30e-04)	Tok/s 26170 (22679)	Loss/tok 3.3800 (3.3319)	LR 1.563e-05
0: TRAIN [3][3060/5173]	Time 0.583 (0.622)	Data 1.08e-04 (3.30e-04)	Tok/s 17586 (22676)	Loss/tok 3.1970 (3.3317)	LR 1.563e-05
0: TRAIN [3][3070/5173]	Time 0.645 (0.622)	Data 1.09e-04 (3.29e-04)	Tok/s 25950 (22676)	Loss/tok 3.2544 (3.3316)	LR 1.563e-05
0: TRAIN [3][3080/5173]	Time 0.578 (0.622)	Data 1.14e-04 (3.28e-04)	Tok/s 17761 (22680)	Loss/tok 3.0726 (3.3317)	LR 1.563e-05
0: TRAIN [3][3090/5173]	Time 0.517 (0.622)	Data 1.15e-04 (3.28e-04)	Tok/s 10260 (22667)	Loss/tok 2.6457 (3.3313)	LR 1.563e-05
0: TRAIN [3][3100/5173]	Time 0.645 (0.622)	Data 1.17e-04 (3.27e-04)	Tok/s 26095 (22651)	Loss/tok 3.2382 (3.3308)	LR 1.563e-05
0: TRAIN [3][3110/5173]	Time 0.638 (0.622)	Data 1.21e-04 (3.26e-04)	Tok/s 25887 (22663)	Loss/tok 3.4699 (3.3312)	LR 1.563e-05
0: TRAIN [3][3120/5173]	Time 0.582 (0.622)	Data 1.21e-04 (3.26e-04)	Tok/s 17835 (22661)	Loss/tok 3.1341 (3.3311)	LR 1.563e-05
0: TRAIN [3][3130/5173]	Time 0.584 (0.622)	Data 1.14e-04 (3.25e-04)	Tok/s 17260 (22657)	Loss/tok 3.1919 (3.3311)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3140/5173]	Time 0.584 (0.622)	Data 1.12e-04 (3.24e-04)	Tok/s 17949 (22672)	Loss/tok 3.0190 (3.3317)	LR 1.563e-05
0: TRAIN [3][3150/5173]	Time 0.584 (0.622)	Data 1.17e-04 (3.24e-04)	Tok/s 17587 (22657)	Loss/tok 3.2254 (3.3314)	LR 1.563e-05
0: TRAIN [3][3160/5173]	Time 0.702 (0.622)	Data 1.13e-04 (3.23e-04)	Tok/s 33051 (22669)	Loss/tok 3.5445 (3.3319)	LR 1.563e-05
0: TRAIN [3][3170/5173]	Time 0.521 (0.622)	Data 1.65e-04 (3.23e-04)	Tok/s 9929 (22680)	Loss/tok 2.7486 (3.3326)	LR 1.563e-05
0: TRAIN [3][3180/5173]	Time 0.646 (0.622)	Data 1.24e-04 (3.22e-04)	Tok/s 25889 (22679)	Loss/tok 3.3626 (3.3325)	LR 1.563e-05
0: TRAIN [3][3190/5173]	Time 0.583 (0.622)	Data 1.22e-04 (3.22e-04)	Tok/s 17907 (22672)	Loss/tok 3.0692 (3.3322)	LR 1.563e-05
0: TRAIN [3][3200/5173]	Time 0.708 (0.622)	Data 1.14e-04 (3.21e-04)	Tok/s 32886 (22676)	Loss/tok 3.4892 (3.3323)	LR 1.563e-05
0: TRAIN [3][3210/5173]	Time 0.647 (0.622)	Data 3.12e-04 (3.21e-04)	Tok/s 25696 (22675)	Loss/tok 3.3421 (3.3322)	LR 1.563e-05
0: TRAIN [3][3220/5173]	Time 0.703 (0.622)	Data 1.21e-04 (3.20e-04)	Tok/s 33092 (22682)	Loss/tok 3.5223 (3.3323)	LR 1.563e-05
0: TRAIN [3][3230/5173]	Time 0.582 (0.622)	Data 1.20e-04 (3.19e-04)	Tok/s 17683 (22684)	Loss/tok 3.2205 (3.3322)	LR 1.563e-05
0: TRAIN [3][3240/5173]	Time 0.641 (0.622)	Data 1.21e-04 (3.19e-04)	Tok/s 26510 (22692)	Loss/tok 3.3157 (3.3323)	LR 1.563e-05
0: TRAIN [3][3250/5173]	Time 0.708 (0.622)	Data 1.23e-04 (3.18e-04)	Tok/s 33146 (22692)	Loss/tok 3.5469 (3.3323)	LR 1.563e-05
0: TRAIN [3][3260/5173]	Time 0.583 (0.622)	Data 1.17e-04 (3.18e-04)	Tok/s 17275 (22693)	Loss/tok 3.0734 (3.3323)	LR 1.563e-05
0: TRAIN [3][3270/5173]	Time 0.583 (0.622)	Data 1.27e-04 (3.17e-04)	Tok/s 17564 (22690)	Loss/tok 3.1412 (3.3322)	LR 1.563e-05
0: TRAIN [3][3280/5173]	Time 0.642 (0.622)	Data 1.22e-04 (3.17e-04)	Tok/s 26003 (22690)	Loss/tok 3.2469 (3.3319)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3290/5173]	Time 0.641 (0.622)	Data 1.18e-04 (3.16e-04)	Tok/s 25838 (22702)	Loss/tok 3.4364 (3.3323)	LR 1.563e-05
0: TRAIN [3][3300/5173]	Time 0.638 (0.622)	Data 1.21e-04 (3.15e-04)	Tok/s 25764 (22707)	Loss/tok 3.4229 (3.3322)	LR 1.563e-05
0: TRAIN [3][3310/5173]	Time 0.585 (0.622)	Data 1.22e-04 (3.15e-04)	Tok/s 17534 (22702)	Loss/tok 3.1197 (3.3319)	LR 1.563e-05
0: TRAIN [3][3320/5173]	Time 0.583 (0.622)	Data 1.24e-04 (3.14e-04)	Tok/s 17882 (22695)	Loss/tok 2.9596 (3.3316)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][3330/5173]	Time 0.584 (0.622)	Data 1.38e-04 (3.14e-04)	Tok/s 17760 (22697)	Loss/tok 3.1613 (3.3318)	LR 1.563e-05
0: TRAIN [3][3340/5173]	Time 0.584 (0.622)	Data 1.23e-04 (3.13e-04)	Tok/s 17800 (22693)	Loss/tok 3.1514 (3.3318)	LR 1.563e-05
0: TRAIN [3][3350/5173]	Time 0.703 (0.622)	Data 1.24e-04 (3.13e-04)	Tok/s 32985 (22699)	Loss/tok 3.3668 (3.3322)	LR 1.563e-05
0: TRAIN [3][3360/5173]	Time 0.648 (0.622)	Data 1.25e-04 (3.12e-04)	Tok/s 26066 (22696)	Loss/tok 3.3883 (3.3320)	LR 1.563e-05
0: TRAIN [3][3370/5173]	Time 0.582 (0.622)	Data 1.22e-04 (3.12e-04)	Tok/s 17805 (22698)	Loss/tok 3.0745 (3.3319)	LR 1.563e-05
0: TRAIN [3][3380/5173]	Time 0.582 (0.622)	Data 1.13e-04 (3.11e-04)	Tok/s 17564 (22694)	Loss/tok 3.1238 (3.3317)	LR 1.563e-05
0: TRAIN [3][3390/5173]	Time 0.580 (0.622)	Data 1.18e-04 (3.11e-04)	Tok/s 17535 (22705)	Loss/tok 3.1892 (3.3320)	LR 1.563e-05
0: TRAIN [3][3400/5173]	Time 0.522 (0.622)	Data 1.21e-04 (3.10e-04)	Tok/s 10110 (22697)	Loss/tok 2.6665 (3.3317)	LR 1.563e-05
0: TRAIN [3][3410/5173]	Time 0.583 (0.622)	Data 1.24e-04 (3.10e-04)	Tok/s 17483 (22693)	Loss/tok 3.1307 (3.3315)	LR 1.563e-05
0: TRAIN [3][3420/5173]	Time 0.581 (0.622)	Data 1.18e-04 (3.09e-04)	Tok/s 17229 (22690)	Loss/tok 3.1675 (3.3313)	LR 1.563e-05
0: TRAIN [3][3430/5173]	Time 0.707 (0.622)	Data 1.27e-04 (3.09e-04)	Tok/s 33268 (22682)	Loss/tok 3.4318 (3.3310)	LR 1.563e-05
0: TRAIN [3][3440/5173]	Time 0.643 (0.622)	Data 1.18e-04 (3.08e-04)	Tok/s 26152 (22680)	Loss/tok 3.3422 (3.3308)	LR 1.563e-05
0: TRAIN [3][3450/5173]	Time 0.582 (0.622)	Data 1.25e-04 (3.08e-04)	Tok/s 17339 (22677)	Loss/tok 3.1608 (3.3306)	LR 1.563e-05
0: TRAIN [3][3460/5173]	Time 0.584 (0.622)	Data 1.29e-04 (3.07e-04)	Tok/s 17505 (22665)	Loss/tok 3.1648 (3.3303)	LR 1.563e-05
0: TRAIN [3][3470/5173]	Time 0.581 (0.622)	Data 1.31e-04 (3.07e-04)	Tok/s 17527 (22661)	Loss/tok 3.1065 (3.3301)	LR 1.563e-05
0: TRAIN [3][3480/5173]	Time 0.581 (0.622)	Data 1.29e-04 (3.06e-04)	Tok/s 17838 (22655)	Loss/tok 3.0951 (3.3301)	LR 1.563e-05
0: TRAIN [3][3490/5173]	Time 0.779 (0.622)	Data 1.30e-04 (3.06e-04)	Tok/s 37652 (22643)	Loss/tok 3.6518 (3.3298)	LR 1.563e-05
0: TRAIN [3][3500/5173]	Time 0.778 (0.622)	Data 1.23e-04 (3.05e-04)	Tok/s 38044 (22642)	Loss/tok 3.7031 (3.3298)	LR 1.563e-05
0: TRAIN [3][3510/5173]	Time 0.579 (0.622)	Data 1.15e-04 (3.05e-04)	Tok/s 17960 (22641)	Loss/tok 3.1162 (3.3297)	LR 1.563e-05
0: TRAIN [3][3520/5173]	Time 0.519 (0.622)	Data 3.12e-04 (3.04e-04)	Tok/s 10087 (22639)	Loss/tok 2.7147 (3.3295)	LR 1.563e-05
0: TRAIN [3][3530/5173]	Time 0.642 (0.622)	Data 1.20e-04 (3.04e-04)	Tok/s 26235 (22639)	Loss/tok 3.4459 (3.3295)	LR 1.563e-05
0: TRAIN [3][3540/5173]	Time 0.646 (0.622)	Data 1.25e-04 (3.04e-04)	Tok/s 25897 (22642)	Loss/tok 3.3676 (3.3295)	LR 1.563e-05
0: TRAIN [3][3550/5173]	Time 0.520 (0.622)	Data 3.58e-04 (3.03e-04)	Tok/s 10174 (22634)	Loss/tok 2.7256 (3.3294)	LR 1.563e-05
0: TRAIN [3][3560/5173]	Time 0.703 (0.622)	Data 3.67e-04 (3.03e-04)	Tok/s 33134 (22641)	Loss/tok 3.5303 (3.3297)	LR 1.563e-05
0: TRAIN [3][3570/5173]	Time 0.646 (0.622)	Data 1.21e-04 (3.03e-04)	Tok/s 25825 (22654)	Loss/tok 3.3117 (3.3301)	LR 1.563e-05
0: TRAIN [3][3580/5173]	Time 0.521 (0.622)	Data 1.25e-04 (3.02e-04)	Tok/s 10276 (22634)	Loss/tok 2.7453 (3.3297)	LR 1.563e-05
0: TRAIN [3][3590/5173]	Time 0.582 (0.622)	Data 1.23e-04 (3.02e-04)	Tok/s 18029 (22631)	Loss/tok 3.0653 (3.3297)	LR 1.563e-05
0: TRAIN [3][3600/5173]	Time 0.706 (0.622)	Data 1.30e-04 (3.01e-04)	Tok/s 33628 (22641)	Loss/tok 3.5895 (3.3300)	LR 1.563e-05
0: TRAIN [3][3610/5173]	Time 0.584 (0.622)	Data 1.21e-04 (3.01e-04)	Tok/s 17956 (22630)	Loss/tok 3.0816 (3.3296)	LR 1.563e-05
0: TRAIN [3][3620/5173]	Time 0.519 (0.622)	Data 1.27e-04 (3.00e-04)	Tok/s 10058 (22628)	Loss/tok 2.6528 (3.3295)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3630/5173]	Time 0.643 (0.622)	Data 1.20e-04 (3.00e-04)	Tok/s 26825 (22621)	Loss/tok 3.2715 (3.3293)	LR 1.563e-05
0: TRAIN [3][3640/5173]	Time 0.580 (0.622)	Data 1.15e-04 (2.99e-04)	Tok/s 17682 (22627)	Loss/tok 3.0723 (3.3293)	LR 1.563e-05
0: TRAIN [3][3650/5173]	Time 0.517 (0.622)	Data 1.28e-04 (2.99e-04)	Tok/s 10202 (22622)	Loss/tok 2.6539 (3.3293)	LR 1.563e-05
0: TRAIN [3][3660/5173]	Time 0.643 (0.622)	Data 3.02e-04 (2.98e-04)	Tok/s 25933 (22617)	Loss/tok 3.4016 (3.3292)	LR 1.563e-05
0: TRAIN [3][3670/5173]	Time 0.779 (0.622)	Data 1.19e-04 (2.98e-04)	Tok/s 38111 (22619)	Loss/tok 3.6326 (3.3292)	LR 1.563e-05
0: TRAIN [3][3680/5173]	Time 0.709 (0.622)	Data 1.19e-04 (2.98e-04)	Tok/s 32642 (22621)	Loss/tok 3.5031 (3.3291)	LR 1.563e-05
0: TRAIN [3][3690/5173]	Time 0.584 (0.622)	Data 1.24e-04 (2.97e-04)	Tok/s 17904 (22614)	Loss/tok 3.1440 (3.3289)	LR 1.563e-05
0: TRAIN [3][3700/5173]	Time 0.779 (0.622)	Data 1.20e-04 (2.97e-04)	Tok/s 38401 (22622)	Loss/tok 3.6149 (3.3292)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][3710/5173]	Time 0.779 (0.622)	Data 1.32e-04 (2.96e-04)	Tok/s 38090 (22636)	Loss/tok 3.6163 (3.3296)	LR 1.563e-05
0: TRAIN [3][3720/5173]	Time 0.645 (0.622)	Data 1.33e-04 (2.96e-04)	Tok/s 26048 (22642)	Loss/tok 3.3857 (3.3297)	LR 1.563e-05
0: TRAIN [3][3730/5173]	Time 0.641 (0.622)	Data 2.97e-04 (2.96e-04)	Tok/s 26476 (22643)	Loss/tok 3.2223 (3.3296)	LR 1.563e-05
0: TRAIN [3][3740/5173]	Time 0.647 (0.622)	Data 1.48e-04 (2.95e-04)	Tok/s 25619 (22656)	Loss/tok 3.3721 (3.3301)	LR 1.563e-05
0: TRAIN [3][3750/5173]	Time 0.646 (0.622)	Data 1.22e-04 (2.95e-04)	Tok/s 25666 (22663)	Loss/tok 3.3129 (3.3301)	LR 1.563e-05
0: TRAIN [3][3760/5173]	Time 0.648 (0.622)	Data 1.24e-04 (2.94e-04)	Tok/s 26355 (22658)	Loss/tok 3.3913 (3.3300)	LR 1.563e-05
0: TRAIN [3][3770/5173]	Time 0.645 (0.622)	Data 2.90e-04 (2.94e-04)	Tok/s 26397 (22659)	Loss/tok 3.2535 (3.3299)	LR 1.563e-05
0: TRAIN [3][3780/5173]	Time 0.639 (0.622)	Data 3.13e-04 (2.94e-04)	Tok/s 26501 (22653)	Loss/tok 3.2783 (3.3297)	LR 1.563e-05
0: TRAIN [3][3790/5173]	Time 0.644 (0.622)	Data 1.16e-04 (2.93e-04)	Tok/s 25919 (22654)	Loss/tok 3.3517 (3.3297)	LR 1.563e-05
0: TRAIN [3][3800/5173]	Time 0.641 (0.622)	Data 1.24e-04 (2.93e-04)	Tok/s 25893 (22654)	Loss/tok 3.3556 (3.3297)	LR 1.563e-05
0: TRAIN [3][3810/5173]	Time 0.581 (0.622)	Data 1.36e-04 (2.92e-04)	Tok/s 17476 (22659)	Loss/tok 3.2166 (3.3298)	LR 1.563e-05
0: TRAIN [3][3820/5173]	Time 0.583 (0.622)	Data 1.21e-04 (2.92e-04)	Tok/s 17585 (22654)	Loss/tok 3.1231 (3.3297)	LR 1.563e-05
0: TRAIN [3][3830/5173]	Time 0.780 (0.622)	Data 1.22e-04 (2.92e-04)	Tok/s 37682 (22653)	Loss/tok 3.7102 (3.3297)	LR 1.563e-05
0: TRAIN [3][3840/5173]	Time 0.581 (0.622)	Data 1.24e-04 (2.91e-04)	Tok/s 18169 (22654)	Loss/tok 3.0920 (3.3297)	LR 1.563e-05
0: TRAIN [3][3850/5173]	Time 0.648 (0.622)	Data 1.21e-04 (2.91e-04)	Tok/s 25538 (22650)	Loss/tok 3.2437 (3.3295)	LR 1.563e-05
0: TRAIN [3][3860/5173]	Time 0.581 (0.622)	Data 1.21e-04 (2.90e-04)	Tok/s 17764 (22639)	Loss/tok 3.0130 (3.3292)	LR 1.563e-05
0: TRAIN [3][3870/5173]	Time 0.581 (0.622)	Data 1.22e-04 (2.90e-04)	Tok/s 17637 (22637)	Loss/tok 3.1156 (3.3292)	LR 1.563e-05
0: TRAIN [3][3880/5173]	Time 0.583 (0.622)	Data 1.22e-04 (2.90e-04)	Tok/s 17625 (22646)	Loss/tok 3.1282 (3.3294)	LR 1.563e-05
0: TRAIN [3][3890/5173]	Time 0.707 (0.622)	Data 1.20e-04 (2.89e-04)	Tok/s 33206 (22648)	Loss/tok 3.4397 (3.3293)	LR 1.563e-05
0: TRAIN [3][3900/5173]	Time 0.640 (0.622)	Data 3.06e-04 (2.89e-04)	Tok/s 25900 (22631)	Loss/tok 3.2830 (3.3289)	LR 1.563e-05
0: TRAIN [3][3910/5173]	Time 0.649 (0.622)	Data 1.33e-04 (2.89e-04)	Tok/s 25873 (22627)	Loss/tok 3.2785 (3.3287)	LR 1.563e-05
0: TRAIN [3][3920/5173]	Time 0.649 (0.622)	Data 1.18e-04 (2.88e-04)	Tok/s 25684 (22631)	Loss/tok 3.3148 (3.3287)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][3930/5173]	Time 0.582 (0.622)	Data 1.19e-04 (2.88e-04)	Tok/s 17847 (22630)	Loss/tok 3.1034 (3.3286)	LR 1.563e-05
0: TRAIN [3][3940/5173]	Time 0.583 (0.622)	Data 1.24e-04 (2.87e-04)	Tok/s 18068 (22632)	Loss/tok 3.0250 (3.3288)	LR 1.563e-05
0: TRAIN [3][3950/5173]	Time 0.648 (0.622)	Data 1.18e-04 (2.87e-04)	Tok/s 25975 (22633)	Loss/tok 3.2726 (3.3288)	LR 1.563e-05
0: TRAIN [3][3960/5173]	Time 0.520 (0.622)	Data 1.25e-04 (2.86e-04)	Tok/s 10185 (22639)	Loss/tok 2.6470 (3.3291)	LR 1.563e-05
0: TRAIN [3][3970/5173]	Time 0.702 (0.622)	Data 3.01e-04 (2.86e-04)	Tok/s 33141 (22646)	Loss/tok 3.5408 (3.3293)	LR 1.563e-05
0: TRAIN [3][3980/5173]	Time 0.582 (0.622)	Data 1.19e-04 (2.86e-04)	Tok/s 17638 (22654)	Loss/tok 2.9965 (3.3296)	LR 1.563e-05
0: TRAIN [3][3990/5173]	Time 0.582 (0.622)	Data 1.20e-04 (2.85e-04)	Tok/s 17853 (22656)	Loss/tok 3.1610 (3.3296)	LR 1.563e-05
0: TRAIN [3][4000/5173]	Time 0.583 (0.622)	Data 1.27e-04 (2.85e-04)	Tok/s 17741 (22646)	Loss/tok 3.1024 (3.3294)	LR 1.563e-05
0: TRAIN [3][4010/5173]	Time 0.647 (0.622)	Data 1.22e-04 (2.85e-04)	Tok/s 25801 (22656)	Loss/tok 3.3283 (3.3295)	LR 1.563e-05
0: TRAIN [3][4020/5173]	Time 0.642 (0.622)	Data 1.19e-04 (2.84e-04)	Tok/s 26189 (22657)	Loss/tok 3.3247 (3.3296)	LR 1.563e-05
0: TRAIN [3][4030/5173]	Time 0.581 (0.622)	Data 1.24e-04 (2.84e-04)	Tok/s 17900 (22664)	Loss/tok 3.1630 (3.3297)	LR 1.563e-05
0: TRAIN [3][4040/5173]	Time 0.583 (0.622)	Data 1.41e-04 (2.83e-04)	Tok/s 17954 (22667)	Loss/tok 3.1217 (3.3298)	LR 1.563e-05
0: TRAIN [3][4050/5173]	Time 0.521 (0.622)	Data 1.19e-04 (2.83e-04)	Tok/s 10072 (22663)	Loss/tok 2.6917 (3.3297)	LR 1.563e-05
0: TRAIN [3][4060/5173]	Time 0.522 (0.622)	Data 1.33e-04 (2.83e-04)	Tok/s 10030 (22669)	Loss/tok 2.7096 (3.3298)	LR 1.563e-05
0: TRAIN [3][4070/5173]	Time 0.779 (0.622)	Data 1.25e-04 (2.82e-04)	Tok/s 38316 (22675)	Loss/tok 3.6982 (3.3300)	LR 1.563e-05
0: TRAIN [3][4080/5173]	Time 0.648 (0.622)	Data 1.22e-04 (2.82e-04)	Tok/s 26176 (22679)	Loss/tok 3.3582 (3.3302)	LR 1.563e-05
0: TRAIN [3][4090/5173]	Time 0.642 (0.622)	Data 1.26e-04 (2.82e-04)	Tok/s 26002 (22682)	Loss/tok 3.3089 (3.3302)	LR 1.563e-05
0: TRAIN [3][4100/5173]	Time 0.643 (0.622)	Data 1.30e-04 (2.81e-04)	Tok/s 26179 (22680)	Loss/tok 3.2789 (3.3301)	LR 1.563e-05
0: TRAIN [3][4110/5173]	Time 0.706 (0.622)	Data 1.18e-04 (2.81e-04)	Tok/s 33227 (22687)	Loss/tok 3.5880 (3.3303)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][4120/5173]	Time 0.581 (0.622)	Data 1.25e-04 (2.81e-04)	Tok/s 17908 (22695)	Loss/tok 3.1801 (3.3306)	LR 1.563e-05
0: TRAIN [3][4130/5173]	Time 0.582 (0.622)	Data 1.24e-04 (2.80e-04)	Tok/s 17491 (22696)	Loss/tok 3.1039 (3.3306)	LR 1.563e-05
0: TRAIN [3][4140/5173]	Time 0.708 (0.622)	Data 1.21e-04 (2.80e-04)	Tok/s 32893 (22694)	Loss/tok 3.4219 (3.3304)	LR 1.563e-05
0: TRAIN [3][4150/5173]	Time 0.648 (0.622)	Data 1.53e-04 (2.80e-04)	Tok/s 26100 (22698)	Loss/tok 3.2952 (3.3306)	LR 1.563e-05
0: TRAIN [3][4160/5173]	Time 0.583 (0.622)	Data 1.32e-04 (2.79e-04)	Tok/s 17395 (22695)	Loss/tok 3.0816 (3.3306)	LR 1.563e-05
0: TRAIN [3][4170/5173]	Time 0.583 (0.622)	Data 1.20e-04 (2.79e-04)	Tok/s 17506 (22693)	Loss/tok 3.0711 (3.3305)	LR 1.563e-05
0: TRAIN [3][4180/5173]	Time 0.782 (0.622)	Data 1.43e-04 (2.79e-04)	Tok/s 38035 (22702)	Loss/tok 3.6532 (3.3309)	LR 1.563e-05
0: TRAIN [3][4190/5173]	Time 0.780 (0.622)	Data 1.30e-04 (2.78e-04)	Tok/s 37913 (22702)	Loss/tok 3.6986 (3.3310)	LR 1.563e-05
0: TRAIN [3][4200/5173]	Time 0.584 (0.622)	Data 1.29e-04 (2.78e-04)	Tok/s 17748 (22702)	Loss/tok 3.1683 (3.3308)	LR 1.563e-05
0: TRAIN [3][4210/5173]	Time 0.580 (0.622)	Data 1.18e-04 (2.78e-04)	Tok/s 17790 (22705)	Loss/tok 3.1048 (3.3307)	LR 1.563e-05
0: TRAIN [3][4220/5173]	Time 0.521 (0.622)	Data 1.48e-04 (2.77e-04)	Tok/s 10382 (22711)	Loss/tok 2.7682 (3.3309)	LR 1.563e-05
0: TRAIN [3][4230/5173]	Time 0.583 (0.622)	Data 1.26e-04 (2.77e-04)	Tok/s 17888 (22709)	Loss/tok 3.0884 (3.3309)	LR 1.563e-05
0: TRAIN [3][4240/5173]	Time 0.647 (0.622)	Data 1.20e-04 (2.77e-04)	Tok/s 25907 (22710)	Loss/tok 3.2431 (3.3309)	LR 1.563e-05
0: TRAIN [3][4250/5173]	Time 0.639 (0.623)	Data 1.56e-04 (2.76e-04)	Tok/s 25863 (22721)	Loss/tok 3.3994 (3.3312)	LR 1.563e-05
0: TRAIN [3][4260/5173]	Time 0.582 (0.622)	Data 1.23e-04 (2.76e-04)	Tok/s 17521 (22715)	Loss/tok 3.1522 (3.3312)	LR 1.563e-05
0: TRAIN [3][4270/5173]	Time 0.581 (0.623)	Data 1.22e-04 (2.76e-04)	Tok/s 17836 (22722)	Loss/tok 3.1182 (3.3314)	LR 1.563e-05
0: TRAIN [3][4280/5173]	Time 0.647 (0.622)	Data 1.23e-04 (2.75e-04)	Tok/s 26104 (22713)	Loss/tok 3.3082 (3.3311)	LR 1.563e-05
0: TRAIN [3][4290/5173]	Time 0.706 (0.623)	Data 1.22e-04 (2.75e-04)	Tok/s 32990 (22724)	Loss/tok 3.4900 (3.3312)	LR 1.563e-05
0: TRAIN [3][4300/5173]	Time 0.584 (0.623)	Data 1.18e-04 (2.75e-04)	Tok/s 17833 (22731)	Loss/tok 3.2958 (3.3315)	LR 1.563e-05
0: TRAIN [3][4310/5173]	Time 0.778 (0.623)	Data 2.93e-04 (2.75e-04)	Tok/s 38390 (22728)	Loss/tok 3.5631 (3.3314)	LR 1.563e-05
0: TRAIN [3][4320/5173]	Time 0.584 (0.623)	Data 1.21e-04 (2.74e-04)	Tok/s 17728 (22718)	Loss/tok 3.0605 (3.3312)	LR 1.563e-05
0: TRAIN [3][4330/5173]	Time 0.522 (0.622)	Data 1.21e-04 (2.74e-04)	Tok/s 10335 (22700)	Loss/tok 2.7386 (3.3309)	LR 1.563e-05
0: TRAIN [3][4340/5173]	Time 0.683 (0.622)	Data 1.22e-04 (2.74e-04)	Tok/s 34060 (22699)	Loss/tok 3.4849 (3.3308)	LR 1.563e-05
0: TRAIN [3][4350/5173]	Time 0.782 (0.622)	Data 1.25e-04 (2.73e-04)	Tok/s 37960 (22707)	Loss/tok 3.6204 (3.3312)	LR 1.563e-05
0: TRAIN [3][4360/5173]	Time 0.518 (0.622)	Data 1.19e-04 (2.73e-04)	Tok/s 10153 (22704)	Loss/tok 2.6600 (3.3311)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][4370/5173]	Time 0.648 (0.622)	Data 1.25e-04 (2.73e-04)	Tok/s 25869 (22710)	Loss/tok 3.3273 (3.3312)	LR 1.563e-05
0: TRAIN [3][4380/5173]	Time 0.702 (0.623)	Data 1.31e-04 (2.72e-04)	Tok/s 32964 (22717)	Loss/tok 3.5187 (3.3313)	LR 1.563e-05
0: TRAIN [3][4390/5173]	Time 0.581 (0.623)	Data 3.27e-04 (2.72e-04)	Tok/s 17782 (22718)	Loss/tok 3.0188 (3.3314)	LR 1.563e-05
0: TRAIN [3][4400/5173]	Time 0.521 (0.623)	Data 1.16e-04 (2.72e-04)	Tok/s 10142 (22722)	Loss/tok 2.7412 (3.3315)	LR 1.563e-05
0: TRAIN [3][4410/5173]	Time 0.704 (0.623)	Data 1.22e-04 (2.72e-04)	Tok/s 33516 (22723)	Loss/tok 3.5186 (3.3317)	LR 1.563e-05
0: TRAIN [3][4420/5173]	Time 0.581 (0.623)	Data 1.26e-04 (2.71e-04)	Tok/s 17650 (22725)	Loss/tok 3.1942 (3.3318)	LR 1.563e-05
0: TRAIN [3][4430/5173]	Time 0.642 (0.623)	Data 1.19e-04 (2.71e-04)	Tok/s 26459 (22721)	Loss/tok 3.2343 (3.3315)	LR 1.563e-05
0: TRAIN [3][4440/5173]	Time 0.705 (0.623)	Data 7.73e-04 (2.71e-04)	Tok/s 33446 (22720)	Loss/tok 3.5349 (3.3317)	LR 1.563e-05
0: TRAIN [3][4450/5173]	Time 0.641 (0.623)	Data 1.22e-04 (2.71e-04)	Tok/s 26577 (22718)	Loss/tok 3.3918 (3.3315)	LR 1.563e-05
0: TRAIN [3][4460/5173]	Time 0.640 (0.622)	Data 2.87e-04 (2.70e-04)	Tok/s 26056 (22709)	Loss/tok 3.3629 (3.3313)	LR 1.563e-05
0: TRAIN [3][4470/5173]	Time 0.582 (0.623)	Data 1.28e-04 (2.70e-04)	Tok/s 17599 (22716)	Loss/tok 3.1731 (3.3317)	LR 1.563e-05
0: TRAIN [3][4480/5173]	Time 0.704 (0.623)	Data 1.29e-04 (2.70e-04)	Tok/s 32501 (22714)	Loss/tok 3.5325 (3.3316)	LR 1.563e-05
0: TRAIN [3][4490/5173]	Time 0.639 (0.622)	Data 1.28e-04 (2.69e-04)	Tok/s 26592 (22710)	Loss/tok 3.3304 (3.3314)	LR 1.563e-05
0: TRAIN [3][4500/5173]	Time 0.644 (0.622)	Data 1.22e-04 (2.69e-04)	Tok/s 26516 (22711)	Loss/tok 3.2793 (3.3316)	LR 1.563e-05
0: TRAIN [3][4510/5173]	Time 0.643 (0.622)	Data 3.15e-04 (2.69e-04)	Tok/s 25866 (22713)	Loss/tok 3.3280 (3.3316)	LR 1.563e-05
0: TRAIN [3][4520/5173]	Time 0.647 (0.622)	Data 1.18e-04 (2.69e-04)	Tok/s 25938 (22713)	Loss/tok 3.2762 (3.3316)	LR 1.563e-05
0: TRAIN [3][4530/5173]	Time 0.584 (0.622)	Data 1.25e-04 (2.68e-04)	Tok/s 17725 (22713)	Loss/tok 3.1807 (3.3316)	LR 1.563e-05
0: TRAIN [3][4540/5173]	Time 0.582 (0.622)	Data 1.26e-04 (2.68e-04)	Tok/s 17864 (22707)	Loss/tok 3.2033 (3.3314)	LR 1.563e-05
0: TRAIN [3][4550/5173]	Time 0.641 (0.622)	Data 2.91e-04 (2.68e-04)	Tok/s 26220 (22713)	Loss/tok 3.3031 (3.3316)	LR 1.563e-05
0: TRAIN [3][4560/5173]	Time 0.704 (0.622)	Data 1.32e-04 (2.67e-04)	Tok/s 33393 (22712)	Loss/tok 3.4856 (3.3315)	LR 1.563e-05
0: TRAIN [3][4570/5173]	Time 0.581 (0.623)	Data 1.26e-04 (2.67e-04)	Tok/s 17779 (22717)	Loss/tok 3.0669 (3.3316)	LR 1.563e-05
0: TRAIN [3][4580/5173]	Time 0.645 (0.623)	Data 1.20e-04 (2.67e-04)	Tok/s 26076 (22727)	Loss/tok 3.3414 (3.3320)	LR 1.563e-05
0: TRAIN [3][4590/5173]	Time 0.643 (0.623)	Data 1.14e-04 (2.67e-04)	Tok/s 26013 (22724)	Loss/tok 3.3311 (3.3320)	LR 1.563e-05
0: TRAIN [3][4600/5173]	Time 0.521 (0.623)	Data 1.31e-04 (2.66e-04)	Tok/s 10028 (22724)	Loss/tok 2.6806 (3.3320)	LR 1.563e-05
0: TRAIN [3][4610/5173]	Time 0.521 (0.623)	Data 3.09e-04 (2.66e-04)	Tok/s 10089 (22725)	Loss/tok 2.6935 (3.3322)	LR 1.563e-05
0: TRAIN [3][4620/5173]	Time 0.584 (0.623)	Data 1.17e-04 (2.66e-04)	Tok/s 17700 (22720)	Loss/tok 3.1410 (3.3320)	LR 1.563e-05
0: TRAIN [3][4630/5173]	Time 0.648 (0.623)	Data 1.29e-04 (2.66e-04)	Tok/s 25974 (22722)	Loss/tok 3.3121 (3.3322)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][4640/5173]	Time 0.582 (0.623)	Data 1.26e-04 (2.65e-04)	Tok/s 17699 (22729)	Loss/tok 3.0055 (3.3324)	LR 1.563e-05
0: TRAIN [3][4650/5173]	Time 0.581 (0.623)	Data 2.77e-04 (2.65e-04)	Tok/s 17432 (22722)	Loss/tok 3.0990 (3.3321)	LR 1.563e-05
0: TRAIN [3][4660/5173]	Time 0.708 (0.623)	Data 1.41e-04 (2.65e-04)	Tok/s 32622 (22721)	Loss/tok 3.5724 (3.3323)	LR 1.563e-05
0: TRAIN [3][4670/5173]	Time 0.581 (0.623)	Data 1.22e-04 (2.64e-04)	Tok/s 17432 (22722)	Loss/tok 3.2076 (3.3323)	LR 1.563e-05
0: TRAIN [3][4680/5173]	Time 0.648 (0.623)	Data 1.23e-04 (2.64e-04)	Tok/s 25688 (22723)	Loss/tok 3.3114 (3.3323)	LR 1.563e-05
0: TRAIN [3][4690/5173]	Time 0.581 (0.623)	Data 1.25e-04 (2.64e-04)	Tok/s 17570 (22722)	Loss/tok 3.1898 (3.3322)	LR 1.563e-05
0: TRAIN [3][4700/5173]	Time 0.584 (0.623)	Data 1.27e-04 (2.64e-04)	Tok/s 17841 (22723)	Loss/tok 3.1640 (3.3323)	LR 1.563e-05
0: TRAIN [3][4710/5173]	Time 0.582 (0.623)	Data 1.22e-04 (2.63e-04)	Tok/s 17575 (22717)	Loss/tok 3.1930 (3.3320)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][4720/5173]	Time 0.647 (0.623)	Data 1.26e-04 (2.63e-04)	Tok/s 25696 (22722)	Loss/tok 3.4561 (3.3323)	LR 1.563e-05
0: TRAIN [3][4730/5173]	Time 0.580 (0.623)	Data 1.25e-04 (2.63e-04)	Tok/s 17967 (22719)	Loss/tok 3.0734 (3.3321)	LR 1.563e-05
0: TRAIN [3][4740/5173]	Time 0.583 (0.623)	Data 1.29e-04 (2.63e-04)	Tok/s 17899 (22719)	Loss/tok 3.2111 (3.3320)	LR 1.563e-05
0: TRAIN [3][4750/5173]	Time 0.641 (0.623)	Data 1.24e-04 (2.63e-04)	Tok/s 26367 (22721)	Loss/tok 3.4299 (3.3321)	LR 1.563e-05
0: TRAIN [3][4760/5173]	Time 0.644 (0.623)	Data 1.23e-04 (2.62e-04)	Tok/s 25923 (22724)	Loss/tok 3.3678 (3.3321)	LR 1.563e-05
0: TRAIN [3][4770/5173]	Time 0.582 (0.623)	Data 1.19e-04 (2.62e-04)	Tok/s 17881 (22722)	Loss/tok 3.1827 (3.3321)	LR 1.563e-05
0: TRAIN [3][4780/5173]	Time 0.519 (0.623)	Data 1.18e-04 (2.62e-04)	Tok/s 9963 (22720)	Loss/tok 2.6962 (3.3320)	LR 1.563e-05
0: TRAIN [3][4790/5173]	Time 0.520 (0.623)	Data 3.12e-04 (2.62e-04)	Tok/s 10203 (22713)	Loss/tok 2.6994 (3.3318)	LR 1.563e-05
0: TRAIN [3][4800/5173]	Time 0.644 (0.623)	Data 1.23e-04 (2.61e-04)	Tok/s 25860 (22714)	Loss/tok 3.4606 (3.3318)	LR 1.563e-05
0: TRAIN [3][4810/5173]	Time 0.582 (0.623)	Data 1.18e-04 (2.61e-04)	Tok/s 17772 (22717)	Loss/tok 3.2840 (3.3322)	LR 1.563e-05
0: TRAIN [3][4820/5173]	Time 0.644 (0.623)	Data 1.27e-04 (2.61e-04)	Tok/s 26029 (22717)	Loss/tok 3.3549 (3.3321)	LR 1.563e-05
0: TRAIN [3][4830/5173]	Time 0.582 (0.623)	Data 1.24e-04 (2.61e-04)	Tok/s 17764 (22724)	Loss/tok 2.9810 (3.3323)	LR 1.563e-05
0: TRAIN [3][4840/5173]	Time 0.638 (0.623)	Data 1.19e-04 (2.61e-04)	Tok/s 26332 (22725)	Loss/tok 3.3910 (3.3323)	LR 1.563e-05
0: TRAIN [3][4850/5173]	Time 0.582 (0.623)	Data 1.32e-04 (2.60e-04)	Tok/s 17699 (22730)	Loss/tok 3.1158 (3.3325)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][4860/5173]	Time 0.584 (0.623)	Data 1.57e-04 (2.60e-04)	Tok/s 17365 (22734)	Loss/tok 3.0569 (3.3328)	LR 1.563e-05
0: TRAIN [3][4870/5173]	Time 0.709 (0.623)	Data 1.22e-04 (2.60e-04)	Tok/s 32513 (22736)	Loss/tok 3.5099 (3.3329)	LR 1.563e-05
0: TRAIN [3][4880/5173]	Time 0.520 (0.623)	Data 1.30e-04 (2.60e-04)	Tok/s 10283 (22733)	Loss/tok 2.6524 (3.3328)	LR 1.563e-05
0: TRAIN [3][4890/5173]	Time 0.702 (0.623)	Data 1.27e-04 (2.59e-04)	Tok/s 33241 (22743)	Loss/tok 3.4810 (3.3331)	LR 1.563e-05
0: TRAIN [3][4900/5173]	Time 0.582 (0.623)	Data 1.20e-04 (2.59e-04)	Tok/s 17931 (22745)	Loss/tok 3.0485 (3.3330)	LR 1.563e-05
0: TRAIN [3][4910/5173]	Time 0.581 (0.623)	Data 1.21e-04 (2.59e-04)	Tok/s 18191 (22749)	Loss/tok 3.0999 (3.3331)	LR 1.563e-05
0: TRAIN [3][4920/5173]	Time 0.650 (0.623)	Data 1.34e-04 (2.59e-04)	Tok/s 25717 (22749)	Loss/tok 3.3363 (3.3332)	LR 1.563e-05
0: TRAIN [3][4930/5173]	Time 0.584 (0.623)	Data 1.24e-04 (2.58e-04)	Tok/s 17660 (22748)	Loss/tok 3.2386 (3.3332)	LR 1.563e-05
0: TRAIN [3][4940/5173]	Time 0.580 (0.623)	Data 1.22e-04 (2.58e-04)	Tok/s 17755 (22747)	Loss/tok 3.1838 (3.3331)	LR 1.563e-05
0: TRAIN [3][4950/5173]	Time 0.708 (0.623)	Data 1.25e-04 (2.58e-04)	Tok/s 33139 (22751)	Loss/tok 3.5280 (3.3331)	LR 1.563e-05
0: TRAIN [3][4960/5173]	Time 0.640 (0.623)	Data 1.29e-04 (2.58e-04)	Tok/s 26348 (22751)	Loss/tok 3.3250 (3.3332)	LR 1.563e-05
0: TRAIN [3][4970/5173]	Time 0.581 (0.623)	Data 1.30e-04 (2.57e-04)	Tok/s 17470 (22757)	Loss/tok 3.0278 (3.3333)	LR 1.563e-05
0: TRAIN [3][4980/5173]	Time 0.581 (0.623)	Data 1.24e-04 (2.57e-04)	Tok/s 17564 (22751)	Loss/tok 3.1637 (3.3332)	LR 1.563e-05
0: TRAIN [3][4990/5173]	Time 0.707 (0.623)	Data 1.22e-04 (2.57e-04)	Tok/s 33432 (22751)	Loss/tok 3.4558 (3.3332)	LR 1.563e-05
0: TRAIN [3][5000/5173]	Time 0.581 (0.623)	Data 1.20e-04 (2.57e-04)	Tok/s 18177 (22747)	Loss/tok 3.1217 (3.3331)	LR 1.563e-05
0: TRAIN [3][5010/5173]	Time 0.583 (0.623)	Data 1.30e-04 (2.57e-04)	Tok/s 17758 (22734)	Loss/tok 3.1553 (3.3328)	LR 1.563e-05
0: TRAIN [3][5020/5173]	Time 0.584 (0.623)	Data 1.23e-04 (2.56e-04)	Tok/s 18126 (22736)	Loss/tok 3.0992 (3.3327)	LR 1.563e-05
0: TRAIN [3][5030/5173]	Time 0.643 (0.623)	Data 1.18e-04 (2.56e-04)	Tok/s 25725 (22733)	Loss/tok 3.3717 (3.3326)	LR 1.563e-05
0: TRAIN [3][5040/5173]	Time 0.647 (0.623)	Data 1.21e-04 (2.56e-04)	Tok/s 25822 (22728)	Loss/tok 3.4086 (3.3324)	LR 1.563e-05
0: TRAIN [3][5050/5173]	Time 0.583 (0.623)	Data 1.25e-04 (2.56e-04)	Tok/s 17678 (22725)	Loss/tok 3.0103 (3.3323)	LR 1.563e-05
0: TRAIN [3][5060/5173]	Time 0.584 (0.623)	Data 1.19e-04 (2.55e-04)	Tok/s 18052 (22720)	Loss/tok 3.1705 (3.3321)	LR 1.563e-05
0: TRAIN [3][5070/5173]	Time 0.645 (0.623)	Data 1.26e-04 (2.55e-04)	Tok/s 26406 (22720)	Loss/tok 3.3672 (3.3320)	LR 1.563e-05
0: TRAIN [3][5080/5173]	Time 0.643 (0.623)	Data 1.24e-04 (2.55e-04)	Tok/s 25948 (22721)	Loss/tok 3.2876 (3.3320)	LR 1.563e-05
0: TRAIN [3][5090/5173]	Time 0.637 (0.623)	Data 1.28e-04 (2.55e-04)	Tok/s 26374 (22724)	Loss/tok 3.2920 (3.3320)	LR 1.563e-05
0: TRAIN [3][5100/5173]	Time 0.579 (0.623)	Data 1.23e-04 (2.54e-04)	Tok/s 17857 (22722)	Loss/tok 3.1324 (3.3319)	LR 1.563e-05
0: TRAIN [3][5110/5173]	Time 0.583 (0.623)	Data 1.24e-04 (2.54e-04)	Tok/s 17680 (22720)	Loss/tok 2.9474 (3.3319)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][5120/5173]	Time 0.581 (0.623)	Data 1.20e-04 (2.54e-04)	Tok/s 17469 (22719)	Loss/tok 3.1340 (3.3319)	LR 1.563e-05
0: TRAIN [3][5130/5173]	Time 0.582 (0.623)	Data 1.22e-04 (2.54e-04)	Tok/s 17559 (22714)	Loss/tok 3.0992 (3.3317)	LR 1.563e-05
0: TRAIN [3][5140/5173]	Time 0.645 (0.623)	Data 1.27e-04 (2.53e-04)	Tok/s 25765 (22717)	Loss/tok 3.3198 (3.3318)	LR 1.563e-05
0: TRAIN [3][5150/5173]	Time 0.705 (0.623)	Data 2.88e-04 (2.53e-04)	Tok/s 33063 (22718)	Loss/tok 3.5162 (3.3318)	LR 1.563e-05
0: TRAIN [3][5160/5173]	Time 0.583 (0.623)	Data 2.71e-04 (2.53e-04)	Tok/s 17318 (22720)	Loss/tok 3.0476 (3.3318)	LR 1.563e-05
0: TRAIN [3][5170/5173]	Time 0.582 (0.623)	Data 1.21e-04 (2.53e-04)	Tok/s 18176 (22717)	Loss/tok 3.0182 (3.3317)	LR 1.563e-05
:::MLL 1586130773.907 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 525}}
:::MLL 1586130773.908 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [3][0/8]	Time 0.668 (0.668)	Decoder iters 118.0 (118.0)	Tok/s 24687 (24687)
0: Running moses detokenizer
0: BLEU(score=21.292762710514367, counts=[35339, 16685, 9085, 5132], totals=[65070, 62067, 59064, 56066], precisions=[54.3092054710312, 26.882240159827283, 15.381619937694705, 9.153497663468055], bp=1.0, sys_len=65070, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1586130780.606 eval_accuracy: {"value": 21.29, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 536}}
:::MLL 1586130780.606 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 3	Training Loss: 3.3314	Test BLEU: 21.29
0: Performance: Epoch: 3	Training: 68150 Tok/s
0: Finished epoch 3
:::MLL 1586130780.607 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 558}}
:::MLL 1586130780.607 block_start: {"value": null, "metadata": {"first_epoch_num": 5, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1586130780.608 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 515}}
0: Starting epoch 4
0: Executing preallocation
0: Sampler for epoch 4 uses seed 282975562
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][0/5173]	Time 1.255 (1.255)	Data 5.08e-01 (5.08e-01)	Tok/s 18277 (18277)	Loss/tok 3.5101 (3.5101)	LR 1.563e-05
0: TRAIN [4][10/5173]	Time 0.582 (0.683)	Data 1.20e-04 (4.64e-02)	Tok/s 17551 (23207)	Loss/tok 3.2313 (3.2812)	LR 1.563e-05
0: TRAIN [4][20/5173]	Time 0.585 (0.644)	Data 1.26e-04 (2.43e-02)	Tok/s 17410 (21641)	Loss/tok 3.0769 (3.2720)	LR 1.563e-05
0: TRAIN [4][30/5173]	Time 0.582 (0.626)	Data 1.23e-04 (1.65e-02)	Tok/s 17929 (20667)	Loss/tok 3.2364 (3.2502)	LR 1.563e-05
0: TRAIN [4][40/5173]	Time 0.580 (0.628)	Data 1.36e-04 (1.25e-02)	Tok/s 18064 (21386)	Loss/tok 3.2074 (3.2884)	LR 1.563e-05
0: TRAIN [4][50/5173]	Time 0.581 (0.621)	Data 1.17e-04 (1.01e-02)	Tok/s 17405 (21016)	Loss/tok 3.1835 (3.2772)	LR 1.563e-05
0: TRAIN [4][60/5173]	Time 0.644 (0.624)	Data 1.23e-04 (8.46e-03)	Tok/s 26191 (21615)	Loss/tok 3.4114 (3.3019)	LR 1.563e-05
0: TRAIN [4][70/5173]	Time 0.639 (0.624)	Data 1.25e-04 (7.29e-03)	Tok/s 25927 (21840)	Loss/tok 3.4628 (3.3073)	LR 1.563e-05
0: TRAIN [4][80/5173]	Time 0.519 (0.621)	Data 1.24e-04 (6.41e-03)	Tok/s 10042 (21642)	Loss/tok 2.6820 (3.2992)	LR 1.563e-05
0: TRAIN [4][90/5173]	Time 0.641 (0.623)	Data 1.20e-04 (5.72e-03)	Tok/s 26137 (21985)	Loss/tok 3.3476 (3.3099)	LR 1.563e-05
0: TRAIN [4][100/5173]	Time 0.642 (0.623)	Data 1.25e-04 (5.17e-03)	Tok/s 26156 (22045)	Loss/tok 3.4513 (3.3135)	LR 1.563e-05
0: TRAIN [4][110/5173]	Time 0.585 (0.623)	Data 1.23e-04 (4.72e-03)	Tok/s 17367 (22157)	Loss/tok 3.0970 (3.3145)	LR 1.563e-05
0: TRAIN [4][120/5173]	Time 0.639 (0.623)	Data 1.27e-04 (4.34e-03)	Tok/s 26006 (22269)	Loss/tok 3.2785 (3.3114)	LR 1.563e-05
0: TRAIN [4][130/5173]	Time 0.708 (0.623)	Data 1.20e-04 (4.02e-03)	Tok/s 33122 (22283)	Loss/tok 3.5162 (3.3141)	LR 1.563e-05
0: TRAIN [4][140/5173]	Time 0.644 (0.624)	Data 1.18e-04 (3.74e-03)	Tok/s 26031 (22398)	Loss/tok 3.3646 (3.3188)	LR 1.563e-05
0: TRAIN [4][150/5173]	Time 0.581 (0.623)	Data 1.19e-04 (3.50e-03)	Tok/s 18033 (22406)	Loss/tok 3.0167 (3.3177)	LR 1.563e-05
0: TRAIN [4][160/5173]	Time 0.646 (0.626)	Data 1.26e-04 (3.29e-03)	Tok/s 26505 (22728)	Loss/tok 3.3221 (3.3279)	LR 1.563e-05
0: TRAIN [4][170/5173]	Time 0.520 (0.626)	Data 1.25e-04 (3.11e-03)	Tok/s 10123 (22762)	Loss/tok 2.6950 (3.3293)	LR 1.563e-05
0: TRAIN [4][180/5173]	Time 0.646 (0.626)	Data 1.20e-04 (2.95e-03)	Tok/s 25874 (22835)	Loss/tok 3.3797 (3.3311)	LR 1.563e-05
0: TRAIN [4][190/5173]	Time 0.705 (0.627)	Data 1.31e-04 (2.80e-03)	Tok/s 32567 (23015)	Loss/tok 3.5224 (3.3371)	LR 1.563e-05
0: TRAIN [4][200/5173]	Time 0.644 (0.627)	Data 1.22e-04 (2.67e-03)	Tok/s 26045 (22997)	Loss/tok 3.2456 (3.3354)	LR 1.563e-05
0: TRAIN [4][210/5173]	Time 0.644 (0.627)	Data 1.26e-04 (2.55e-03)	Tok/s 26178 (23030)	Loss/tok 3.3053 (3.3363)	LR 1.563e-05
0: TRAIN [4][220/5173]	Time 0.582 (0.627)	Data 1.24e-04 (2.44e-03)	Tok/s 17452 (23087)	Loss/tok 3.1288 (3.3356)	LR 1.563e-05
0: TRAIN [4][230/5173]	Time 0.584 (0.627)	Data 2.85e-04 (2.34e-03)	Tok/s 17692 (23087)	Loss/tok 3.1414 (3.3377)	LR 1.563e-05
0: TRAIN [4][240/5173]	Time 0.640 (0.627)	Data 1.23e-04 (2.25e-03)	Tok/s 26494 (23085)	Loss/tok 3.2793 (3.3380)	LR 1.563e-05
0: TRAIN [4][250/5173]	Time 0.581 (0.626)	Data 1.22e-04 (2.16e-03)	Tok/s 17929 (23035)	Loss/tok 3.0683 (3.3341)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][260/5173]	Time 0.703 (0.626)	Data 1.23e-04 (2.09e-03)	Tok/s 32967 (23045)	Loss/tok 3.5621 (3.3326)	LR 1.563e-05
0: TRAIN [4][270/5173]	Time 0.642 (0.628)	Data 1.35e-04 (2.01e-03)	Tok/s 26097 (23190)	Loss/tok 3.2915 (3.3353)	LR 1.563e-05
0: TRAIN [4][280/5173]	Time 0.582 (0.626)	Data 1.24e-04 (1.95e-03)	Tok/s 17693 (23060)	Loss/tok 3.0908 (3.3338)	LR 1.563e-05
0: TRAIN [4][290/5173]	Time 0.779 (0.626)	Data 1.27e-04 (1.89e-03)	Tok/s 38348 (23007)	Loss/tok 3.6765 (3.3319)	LR 1.563e-05
0: TRAIN [4][300/5173]	Time 0.582 (0.625)	Data 1.29e-04 (1.83e-03)	Tok/s 17825 (22891)	Loss/tok 3.1003 (3.3276)	LR 1.563e-05
0: TRAIN [4][310/5173]	Time 0.705 (0.626)	Data 1.24e-04 (1.77e-03)	Tok/s 33257 (23031)	Loss/tok 3.3886 (3.3304)	LR 1.563e-05
0: TRAIN [4][320/5173]	Time 0.582 (0.626)	Data 1.29e-04 (1.72e-03)	Tok/s 17511 (23016)	Loss/tok 3.1096 (3.3290)	LR 1.563e-05
0: TRAIN [4][330/5173]	Time 0.522 (0.625)	Data 1.25e-04 (1.67e-03)	Tok/s 10216 (22917)	Loss/tok 2.6613 (3.3256)	LR 1.563e-05
0: TRAIN [4][340/5173]	Time 0.640 (0.624)	Data 1.21e-04 (1.63e-03)	Tok/s 26005 (22891)	Loss/tok 3.4315 (3.3247)	LR 1.563e-05
0: TRAIN [4][350/5173]	Time 0.708 (0.624)	Data 3.10e-04 (1.59e-03)	Tok/s 32578 (22854)	Loss/tok 3.5459 (3.3238)	LR 1.563e-05
0: TRAIN [4][360/5173]	Time 0.583 (0.624)	Data 1.27e-04 (1.55e-03)	Tok/s 17939 (22931)	Loss/tok 3.0307 (3.3242)	LR 1.563e-05
0: TRAIN [4][370/5173]	Time 0.579 (0.625)	Data 1.24e-04 (1.51e-03)	Tok/s 17603 (22929)	Loss/tok 3.0410 (3.3243)	LR 1.563e-05
0: TRAIN [4][380/5173]	Time 0.583 (0.625)	Data 1.31e-04 (1.47e-03)	Tok/s 17787 (22995)	Loss/tok 3.0312 (3.3262)	LR 1.563e-05
0: TRAIN [4][390/5173]	Time 0.706 (0.625)	Data 1.20e-04 (1.44e-03)	Tok/s 33017 (22965)	Loss/tok 3.4980 (3.3253)	LR 1.563e-05
0: TRAIN [4][400/5173]	Time 0.708 (0.626)	Data 1.26e-04 (1.41e-03)	Tok/s 33339 (23106)	Loss/tok 3.5896 (3.3300)	LR 1.563e-05
0: TRAIN [4][410/5173]	Time 0.580 (0.626)	Data 1.27e-04 (1.37e-03)	Tok/s 18023 (23098)	Loss/tok 3.2423 (3.3306)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][420/5173]	Time 0.782 (0.626)	Data 1.36e-04 (1.35e-03)	Tok/s 37546 (23135)	Loss/tok 3.6955 (3.3326)	LR 1.563e-05
0: TRAIN [4][430/5173]	Time 0.645 (0.626)	Data 1.25e-04 (1.32e-03)	Tok/s 25944 (23139)	Loss/tok 3.4124 (3.3325)	LR 1.563e-05
0: TRAIN [4][440/5173]	Time 0.581 (0.626)	Data 1.32e-04 (1.29e-03)	Tok/s 17968 (23120)	Loss/tok 3.1418 (3.3317)	LR 1.563e-05
0: TRAIN [4][450/5173]	Time 0.582 (0.626)	Data 1.29e-04 (1.27e-03)	Tok/s 17523 (23109)	Loss/tok 3.1473 (3.3308)	LR 1.563e-05
0: TRAIN [4][460/5173]	Time 0.584 (0.626)	Data 1.24e-04 (1.24e-03)	Tok/s 17910 (23092)	Loss/tok 3.1056 (3.3297)	LR 1.563e-05
0: TRAIN [4][470/5173]	Time 0.583 (0.625)	Data 1.32e-04 (1.22e-03)	Tok/s 17622 (23041)	Loss/tok 3.1262 (3.3288)	LR 1.563e-05
0: TRAIN [4][480/5173]	Time 0.644 (0.626)	Data 1.24e-04 (1.20e-03)	Tok/s 26321 (23077)	Loss/tok 3.3317 (3.3295)	LR 1.563e-05
0: TRAIN [4][490/5173]	Time 0.580 (0.625)	Data 2.86e-04 (1.17e-03)	Tok/s 17754 (22985)	Loss/tok 3.2839 (3.3276)	LR 1.563e-05
0: TRAIN [4][500/5173]	Time 0.645 (0.626)	Data 1.24e-04 (1.16e-03)	Tok/s 26018 (23072)	Loss/tok 3.3854 (3.3313)	LR 1.563e-05
0: TRAIN [4][510/5173]	Time 0.642 (0.625)	Data 1.27e-04 (1.13e-03)	Tok/s 26061 (23002)	Loss/tok 3.3614 (3.3291)	LR 1.563e-05
0: TRAIN [4][520/5173]	Time 0.641 (0.625)	Data 1.25e-04 (1.12e-03)	Tok/s 26230 (23057)	Loss/tok 3.3897 (3.3293)	LR 1.563e-05
0: TRAIN [4][530/5173]	Time 0.638 (0.626)	Data 1.25e-04 (1.10e-03)	Tok/s 26194 (23116)	Loss/tok 3.3843 (3.3311)	LR 1.563e-05
0: TRAIN [4][540/5173]	Time 0.583 (0.626)	Data 1.53e-04 (1.08e-03)	Tok/s 17288 (23163)	Loss/tok 3.1091 (3.3344)	LR 1.563e-05
0: TRAIN [4][550/5173]	Time 0.585 (0.626)	Data 1.26e-04 (1.06e-03)	Tok/s 17785 (23127)	Loss/tok 2.9630 (3.3328)	LR 1.563e-05
0: TRAIN [4][560/5173]	Time 0.780 (0.626)	Data 1.28e-04 (1.05e-03)	Tok/s 38829 (23156)	Loss/tok 3.6466 (3.3334)	LR 1.563e-05
0: TRAIN [4][570/5173]	Time 0.775 (0.627)	Data 3.15e-04 (1.03e-03)	Tok/s 38115 (23195)	Loss/tok 3.6053 (3.3360)	LR 1.563e-05
0: TRAIN [4][580/5173]	Time 0.583 (0.627)	Data 1.31e-04 (1.02e-03)	Tok/s 17554 (23176)	Loss/tok 3.0543 (3.3363)	LR 1.563e-05
0: TRAIN [4][590/5173]	Time 0.584 (0.626)	Data 1.27e-04 (1.00e-03)	Tok/s 17313 (23142)	Loss/tok 3.1128 (3.3350)	LR 1.563e-05
0: TRAIN [4][600/5173]	Time 0.706 (0.626)	Data 1.28e-04 (9.86e-04)	Tok/s 33388 (23162)	Loss/tok 3.3999 (3.3346)	LR 1.563e-05
0: TRAIN [4][610/5173]	Time 0.583 (0.626)	Data 1.24e-04 (9.73e-04)	Tok/s 17926 (23088)	Loss/tok 3.1219 (3.3326)	LR 1.563e-05
0: TRAIN [4][620/5173]	Time 0.519 (0.626)	Data 2.80e-04 (9.60e-04)	Tok/s 10399 (23086)	Loss/tok 2.6979 (3.3329)	LR 1.563e-05
0: TRAIN [4][630/5173]	Time 0.703 (0.625)	Data 1.19e-04 (9.47e-04)	Tok/s 32915 (23040)	Loss/tok 3.5241 (3.3318)	LR 1.563e-05
0: TRAIN [4][640/5173]	Time 0.521 (0.625)	Data 1.27e-04 (9.34e-04)	Tok/s 9985 (22982)	Loss/tok 2.6768 (3.3305)	LR 1.563e-05
0: TRAIN [4][650/5173]	Time 0.708 (0.626)	Data 1.34e-04 (9.22e-04)	Tok/s 32737 (23052)	Loss/tok 3.5759 (3.3328)	LR 1.563e-05
0: TRAIN [4][660/5173]	Time 0.581 (0.626)	Data 1.20e-04 (9.10e-04)	Tok/s 17509 (23081)	Loss/tok 3.2560 (3.3333)	LR 1.563e-05
0: TRAIN [4][670/5173]	Time 0.643 (0.626)	Data 1.17e-04 (8.99e-04)	Tok/s 26247 (23098)	Loss/tok 3.3592 (3.3333)	LR 1.563e-05
0: TRAIN [4][680/5173]	Time 0.641 (0.626)	Data 1.26e-04 (8.88e-04)	Tok/s 26265 (23069)	Loss/tok 3.3337 (3.3320)	LR 1.563e-05
0: TRAIN [4][690/5173]	Time 0.584 (0.625)	Data 1.23e-04 (8.77e-04)	Tok/s 17530 (23040)	Loss/tok 3.0933 (3.3311)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][700/5173]	Time 0.584 (0.625)	Data 1.36e-04 (8.67e-04)	Tok/s 17758 (23007)	Loss/tok 3.1529 (3.3306)	LR 1.563e-05
0: TRAIN [4][710/5173]	Time 0.581 (0.625)	Data 1.31e-04 (8.57e-04)	Tok/s 17796 (23002)	Loss/tok 3.1268 (3.3300)	LR 1.563e-05
0: TRAIN [4][720/5173]	Time 0.638 (0.625)	Data 1.28e-04 (8.47e-04)	Tok/s 26559 (23018)	Loss/tok 3.2904 (3.3297)	LR 1.563e-05
0: TRAIN [4][730/5173]	Time 0.584 (0.625)	Data 2.70e-04 (8.38e-04)	Tok/s 17713 (23049)	Loss/tok 3.1058 (3.3307)	LR 1.563e-05
0: TRAIN [4][740/5173]	Time 0.582 (0.625)	Data 1.26e-04 (8.29e-04)	Tok/s 17254 (23023)	Loss/tok 2.9755 (3.3296)	LR 1.563e-05
0: TRAIN [4][750/5173]	Time 0.639 (0.625)	Data 1.24e-04 (8.19e-04)	Tok/s 26293 (23036)	Loss/tok 3.1974 (3.3297)	LR 1.563e-05
0: TRAIN [4][760/5173]	Time 0.583 (0.625)	Data 1.27e-04 (8.11e-04)	Tok/s 17699 (22966)	Loss/tok 3.0648 (3.3278)	LR 1.563e-05
0: TRAIN [4][770/5173]	Time 0.643 (0.625)	Data 1.29e-04 (8.02e-04)	Tok/s 26236 (22972)	Loss/tok 3.3072 (3.3281)	LR 1.563e-05
0: TRAIN [4][780/5173]	Time 0.642 (0.624)	Data 1.25e-04 (7.94e-04)	Tok/s 26248 (22942)	Loss/tok 3.3253 (3.3286)	LR 1.563e-05
0: TRAIN [4][790/5173]	Time 0.645 (0.624)	Data 1.26e-04 (7.85e-04)	Tok/s 26217 (22879)	Loss/tok 3.4470 (3.3273)	LR 1.563e-05
0: TRAIN [4][800/5173]	Time 0.646 (0.624)	Data 3.15e-04 (7.77e-04)	Tok/s 26128 (22915)	Loss/tok 3.2635 (3.3276)	LR 1.563e-05
0: TRAIN [4][810/5173]	Time 0.636 (0.624)	Data 1.23e-04 (7.70e-04)	Tok/s 26566 (22932)	Loss/tok 3.2733 (3.3271)	LR 1.563e-05
0: TRAIN [4][820/5173]	Time 0.581 (0.624)	Data 1.19e-04 (7.62e-04)	Tok/s 17994 (22924)	Loss/tok 3.0327 (3.3273)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][830/5173]	Time 0.780 (0.624)	Data 1.37e-04 (7.55e-04)	Tok/s 38051 (22931)	Loss/tok 3.6950 (3.3281)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][840/5173]	Time 0.583 (0.625)	Data 1.28e-04 (7.47e-04)	Tok/s 17710 (22961)	Loss/tok 3.0511 (3.3291)	LR 1.563e-05
0: TRAIN [4][850/5173]	Time 0.581 (0.625)	Data 1.24e-04 (7.40e-04)	Tok/s 17937 (23004)	Loss/tok 3.1485 (3.3306)	LR 1.563e-05
0: TRAIN [4][860/5173]	Time 0.583 (0.625)	Data 1.28e-04 (7.33e-04)	Tok/s 17727 (22991)	Loss/tok 3.1953 (3.3300)	LR 1.563e-05
0: TRAIN [4][870/5173]	Time 0.522 (0.624)	Data 2.95e-04 (7.27e-04)	Tok/s 9970 (22921)	Loss/tok 2.6834 (3.3283)	LR 1.563e-05
0: TRAIN [4][880/5173]	Time 0.648 (0.624)	Data 1.35e-04 (7.20e-04)	Tok/s 26193 (22936)	Loss/tok 3.4510 (3.3283)	LR 1.563e-05
0: TRAIN [4][890/5173]	Time 0.583 (0.624)	Data 1.39e-04 (7.14e-04)	Tok/s 17700 (22916)	Loss/tok 3.0798 (3.3271)	LR 1.563e-05
0: TRAIN [4][900/5173]	Time 0.637 (0.624)	Data 1.31e-04 (7.07e-04)	Tok/s 26197 (22913)	Loss/tok 3.4046 (3.3271)	LR 1.563e-05
0: TRAIN [4][910/5173]	Time 0.781 (0.624)	Data 1.31e-04 (7.01e-04)	Tok/s 38904 (22950)	Loss/tok 3.6654 (3.3277)	LR 1.563e-05
0: TRAIN [4][920/5173]	Time 0.581 (0.624)	Data 1.33e-04 (6.95e-04)	Tok/s 17737 (22946)	Loss/tok 3.1472 (3.3268)	LR 1.563e-05
0: TRAIN [4][930/5173]	Time 0.703 (0.624)	Data 1.36e-04 (6.89e-04)	Tok/s 33154 (22918)	Loss/tok 3.5026 (3.3261)	LR 1.563e-05
0: TRAIN [4][940/5173]	Time 0.520 (0.624)	Data 1.29e-04 (6.84e-04)	Tok/s 10127 (22915)	Loss/tok 2.7257 (3.3263)	LR 1.563e-05
0: TRAIN [4][950/5173]	Time 0.584 (0.624)	Data 1.35e-04 (6.78e-04)	Tok/s 17553 (22913)	Loss/tok 3.3434 (3.3269)	LR 1.563e-05
0: TRAIN [4][960/5173]	Time 0.643 (0.624)	Data 1.47e-04 (6.73e-04)	Tok/s 26191 (22915)	Loss/tok 3.2898 (3.3272)	LR 1.563e-05
0: TRAIN [4][970/5173]	Time 0.582 (0.624)	Data 1.38e-04 (6.68e-04)	Tok/s 17865 (22879)	Loss/tok 2.9857 (3.3263)	LR 1.563e-05
0: TRAIN [4][980/5173]	Time 0.639 (0.624)	Data 1.44e-04 (6.62e-04)	Tok/s 26136 (22869)	Loss/tok 3.4463 (3.3254)	LR 1.563e-05
0: TRAIN [4][990/5173]	Time 0.583 (0.623)	Data 1.39e-04 (6.57e-04)	Tok/s 17351 (22833)	Loss/tok 3.1838 (3.3244)	LR 1.563e-05
0: TRAIN [4][1000/5173]	Time 0.523 (0.623)	Data 3.09e-04 (6.52e-04)	Tok/s 10047 (22795)	Loss/tok 2.6969 (3.3236)	LR 1.563e-05
0: TRAIN [4][1010/5173]	Time 0.639 (0.623)	Data 1.38e-04 (6.47e-04)	Tok/s 26237 (22823)	Loss/tok 3.2548 (3.3239)	LR 1.563e-05
0: TRAIN [4][1020/5173]	Time 0.584 (0.623)	Data 1.32e-04 (6.42e-04)	Tok/s 17523 (22811)	Loss/tok 2.9785 (3.3234)	LR 1.563e-05
0: TRAIN [4][1030/5173]	Time 0.709 (0.623)	Data 1.36e-04 (6.37e-04)	Tok/s 32720 (22794)	Loss/tok 3.5019 (3.3230)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][1040/5173]	Time 0.645 (0.623)	Data 1.29e-04 (6.33e-04)	Tok/s 26055 (22829)	Loss/tok 3.2598 (3.3237)	LR 1.563e-05
0: TRAIN [4][1050/5173]	Time 0.582 (0.623)	Data 1.38e-04 (6.28e-04)	Tok/s 17570 (22804)	Loss/tok 3.1438 (3.3229)	LR 1.563e-05
0: TRAIN [4][1060/5173]	Time 0.583 (0.623)	Data 1.90e-04 (6.24e-04)	Tok/s 17886 (22780)	Loss/tok 3.0757 (3.3220)	LR 1.563e-05
0: TRAIN [4][1070/5173]	Time 0.639 (0.623)	Data 9.97e-05 (6.19e-04)	Tok/s 26127 (22818)	Loss/tok 3.3184 (3.3235)	LR 1.563e-05
0: TRAIN [4][1080/5173]	Time 0.644 (0.623)	Data 1.08e-04 (6.15e-04)	Tok/s 26065 (22838)	Loss/tok 3.4335 (3.3242)	LR 1.563e-05
0: TRAIN [4][1090/5173]	Time 0.643 (0.623)	Data 1.08e-04 (6.10e-04)	Tok/s 26361 (22850)	Loss/tok 3.3129 (3.3243)	LR 1.563e-05
0: TRAIN [4][1100/5173]	Time 0.641 (0.623)	Data 1.03e-04 (6.06e-04)	Tok/s 26018 (22870)	Loss/tok 3.3285 (3.3247)	LR 1.563e-05
0: TRAIN [4][1110/5173]	Time 0.583 (0.623)	Data 1.16e-04 (6.01e-04)	Tok/s 17726 (22849)	Loss/tok 3.1699 (3.3239)	LR 1.563e-05
0: TRAIN [4][1120/5173]	Time 0.582 (0.623)	Data 1.04e-04 (5.97e-04)	Tok/s 17776 (22856)	Loss/tok 3.0972 (3.3244)	LR 1.563e-05
0: TRAIN [4][1130/5173]	Time 0.643 (0.623)	Data 9.92e-05 (5.92e-04)	Tok/s 26721 (22861)	Loss/tok 3.1385 (3.3241)	LR 1.563e-05
0: TRAIN [4][1140/5173]	Time 0.642 (0.623)	Data 1.15e-04 (5.88e-04)	Tok/s 26224 (22866)	Loss/tok 3.3124 (3.3240)	LR 1.563e-05
0: TRAIN [4][1150/5173]	Time 0.642 (0.623)	Data 1.08e-04 (5.84e-04)	Tok/s 26446 (22878)	Loss/tok 3.3898 (3.3241)	LR 1.563e-05
0: TRAIN [4][1160/5173]	Time 0.581 (0.623)	Data 1.01e-04 (5.80e-04)	Tok/s 17926 (22880)	Loss/tok 3.1241 (3.3245)	LR 1.563e-05
0: TRAIN [4][1170/5173]	Time 0.642 (0.624)	Data 1.10e-04 (5.76e-04)	Tok/s 26511 (22899)	Loss/tok 3.4417 (3.3246)	LR 1.563e-05
0: TRAIN [4][1180/5173]	Time 0.641 (0.624)	Data 1.05e-04 (5.72e-04)	Tok/s 26382 (22906)	Loss/tok 3.4642 (3.3253)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][1190/5173]	Time 0.580 (0.624)	Data 1.03e-04 (5.68e-04)	Tok/s 17796 (22915)	Loss/tok 3.2011 (3.3256)	LR 1.563e-05
0: TRAIN [4][1200/5173]	Time 0.520 (0.624)	Data 1.03e-04 (5.65e-04)	Tok/s 10349 (22892)	Loss/tok 2.7158 (3.3247)	LR 1.563e-05
0: TRAIN [4][1210/5173]	Time 0.647 (0.624)	Data 1.43e-04 (5.61e-04)	Tok/s 25758 (22901)	Loss/tok 3.4174 (3.3249)	LR 1.563e-05
0: TRAIN [4][1220/5173]	Time 0.580 (0.623)	Data 1.10e-04 (5.58e-04)	Tok/s 17985 (22869)	Loss/tok 3.0249 (3.3243)	LR 1.563e-05
0: TRAIN [4][1230/5173]	Time 0.707 (0.624)	Data 1.07e-04 (5.54e-04)	Tok/s 33094 (22885)	Loss/tok 3.5203 (3.3248)	LR 1.563e-05
0: TRAIN [4][1240/5173]	Time 0.582 (0.623)	Data 1.13e-04 (5.51e-04)	Tok/s 17420 (22850)	Loss/tok 3.1004 (3.3237)	LR 1.563e-05
0: TRAIN [4][1250/5173]	Time 0.639 (0.623)	Data 1.48e-04 (5.48e-04)	Tok/s 26606 (22852)	Loss/tok 3.3855 (3.3237)	LR 1.563e-05
0: TRAIN [4][1260/5173]	Time 0.581 (0.623)	Data 9.99e-05 (5.44e-04)	Tok/s 17578 (22867)	Loss/tok 3.0563 (3.3242)	LR 1.563e-05
0: TRAIN [4][1270/5173]	Time 0.581 (0.623)	Data 1.05e-04 (5.41e-04)	Tok/s 17908 (22852)	Loss/tok 3.1581 (3.3238)	LR 1.563e-05
0: TRAIN [4][1280/5173]	Time 0.703 (0.623)	Data 1.03e-04 (5.38e-04)	Tok/s 33065 (22860)	Loss/tok 3.4869 (3.3239)	LR 1.563e-05
0: TRAIN [4][1290/5173]	Time 0.645 (0.623)	Data 9.80e-05 (5.35e-04)	Tok/s 25982 (22868)	Loss/tok 3.4142 (3.3242)	LR 1.563e-05
0: TRAIN [4][1300/5173]	Time 0.583 (0.623)	Data 1.06e-04 (5.32e-04)	Tok/s 17530 (22847)	Loss/tok 3.2544 (3.3237)	LR 1.563e-05
0: TRAIN [4][1310/5173]	Time 0.641 (0.623)	Data 1.04e-04 (5.28e-04)	Tok/s 26300 (22827)	Loss/tok 3.3191 (3.3231)	LR 1.563e-05
0: TRAIN [4][1320/5173]	Time 0.582 (0.623)	Data 9.97e-05 (5.25e-04)	Tok/s 17962 (22823)	Loss/tok 3.1139 (3.3231)	LR 1.563e-05
0: TRAIN [4][1330/5173]	Time 0.579 (0.623)	Data 1.06e-04 (5.22e-04)	Tok/s 17480 (22817)	Loss/tok 3.1530 (3.3232)	LR 1.563e-05
0: TRAIN [4][1340/5173]	Time 0.579 (0.623)	Data 1.07e-04 (5.19e-04)	Tok/s 17946 (22820)	Loss/tok 3.2314 (3.3237)	LR 1.563e-05
0: TRAIN [4][1350/5173]	Time 0.641 (0.623)	Data 1.04e-04 (5.16e-04)	Tok/s 26498 (22826)	Loss/tok 3.3539 (3.3239)	LR 1.563e-05
0: TRAIN [4][1360/5173]	Time 0.651 (0.623)	Data 2.92e-04 (5.13e-04)	Tok/s 25357 (22830)	Loss/tok 3.3095 (3.3238)	LR 1.563e-05
0: TRAIN [4][1370/5173]	Time 0.636 (0.623)	Data 1.14e-04 (5.10e-04)	Tok/s 26661 (22832)	Loss/tok 3.3008 (3.3236)	LR 1.563e-05
0: TRAIN [4][1380/5173]	Time 0.648 (0.623)	Data 1.02e-04 (5.08e-04)	Tok/s 25948 (22839)	Loss/tok 3.3989 (3.3239)	LR 1.563e-05
0: TRAIN [4][1390/5173]	Time 0.650 (0.623)	Data 1.04e-04 (5.05e-04)	Tok/s 25692 (22823)	Loss/tok 3.2286 (3.3239)	LR 1.563e-05
0: TRAIN [4][1400/5173]	Time 0.647 (0.623)	Data 1.07e-04 (5.02e-04)	Tok/s 25714 (22815)	Loss/tok 3.2993 (3.3236)	LR 1.563e-05
0: TRAIN [4][1410/5173]	Time 0.705 (0.623)	Data 1.03e-04 (5.00e-04)	Tok/s 32738 (22838)	Loss/tok 3.6342 (3.3245)	LR 1.563e-05
0: TRAIN [4][1420/5173]	Time 0.706 (0.623)	Data 1.11e-04 (4.97e-04)	Tok/s 32975 (22837)	Loss/tok 3.4823 (3.3243)	LR 1.563e-05
0: TRAIN [4][1430/5173]	Time 0.584 (0.623)	Data 1.06e-04 (4.94e-04)	Tok/s 17222 (22839)	Loss/tok 3.1904 (3.3246)	LR 1.563e-05
0: TRAIN [4][1440/5173]	Time 0.708 (0.623)	Data 1.06e-04 (4.92e-04)	Tok/s 32406 (22826)	Loss/tok 3.5106 (3.3242)	LR 1.563e-05
0: TRAIN [4][1450/5173]	Time 0.581 (0.623)	Data 1.03e-04 (4.89e-04)	Tok/s 18289 (22811)	Loss/tok 3.1211 (3.3242)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1460/5173]	Time 0.583 (0.623)	Data 1.39e-04 (4.87e-04)	Tok/s 17968 (22826)	Loss/tok 3.1256 (3.3246)	LR 1.563e-05
0: TRAIN [4][1470/5173]	Time 0.582 (0.623)	Data 3.57e-04 (4.85e-04)	Tok/s 17851 (22782)	Loss/tok 3.0744 (3.3237)	LR 1.563e-05
0: TRAIN [4][1480/5173]	Time 0.582 (0.623)	Data 1.43e-04 (4.82e-04)	Tok/s 17740 (22794)	Loss/tok 3.0533 (3.3244)	LR 1.563e-05
0: TRAIN [4][1490/5173]	Time 0.578 (0.623)	Data 1.35e-04 (4.80e-04)	Tok/s 18149 (22793)	Loss/tok 3.1068 (3.3243)	LR 1.563e-05
0: TRAIN [4][1500/5173]	Time 0.580 (0.623)	Data 1.22e-04 (4.78e-04)	Tok/s 18173 (22780)	Loss/tok 2.9839 (3.3238)	LR 1.563e-05
0: TRAIN [4][1510/5173]	Time 0.707 (0.622)	Data 1.21e-04 (4.76e-04)	Tok/s 33337 (22757)	Loss/tok 3.4807 (3.3233)	LR 1.563e-05
0: TRAIN [4][1520/5173]	Time 0.586 (0.623)	Data 3.05e-04 (4.74e-04)	Tok/s 17600 (22776)	Loss/tok 3.0690 (3.3237)	LR 1.563e-05
0: TRAIN [4][1530/5173]	Time 0.641 (0.623)	Data 1.23e-04 (4.72e-04)	Tok/s 26209 (22770)	Loss/tok 3.4800 (3.3236)	LR 1.563e-05
0: TRAIN [4][1540/5173]	Time 0.520 (0.623)	Data 1.18e-04 (4.69e-04)	Tok/s 10346 (22775)	Loss/tok 2.7148 (3.3237)	LR 1.563e-05
0: TRAIN [4][1550/5173]	Time 0.521 (0.622)	Data 1.22e-04 (4.67e-04)	Tok/s 10154 (22766)	Loss/tok 2.6565 (3.3236)	LR 1.563e-05
0: TRAIN [4][1560/5173]	Time 0.706 (0.622)	Data 1.23e-04 (4.65e-04)	Tok/s 33328 (22739)	Loss/tok 3.5480 (3.3233)	LR 1.563e-05
0: TRAIN [4][1570/5173]	Time 0.706 (0.622)	Data 1.15e-04 (4.63e-04)	Tok/s 32695 (22746)	Loss/tok 3.6092 (3.3239)	LR 1.563e-05
0: TRAIN [4][1580/5173]	Time 0.650 (0.622)	Data 1.22e-04 (4.61e-04)	Tok/s 25422 (22740)	Loss/tok 3.2498 (3.3236)	LR 1.563e-05
0: TRAIN [4][1590/5173]	Time 0.647 (0.622)	Data 1.25e-04 (4.59e-04)	Tok/s 25810 (22735)	Loss/tok 3.3615 (3.3232)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1600/5173]	Time 0.582 (0.622)	Data 1.21e-04 (4.57e-04)	Tok/s 18000 (22741)	Loss/tok 3.0687 (3.3238)	LR 1.563e-05
0: TRAIN [4][1610/5173]	Time 0.584 (0.622)	Data 1.20e-04 (4.55e-04)	Tok/s 17775 (22742)	Loss/tok 3.0985 (3.3239)	LR 1.563e-05
0: TRAIN [4][1620/5173]	Time 0.777 (0.622)	Data 1.26e-04 (4.53e-04)	Tok/s 39216 (22753)	Loss/tok 3.5831 (3.3243)	LR 1.563e-05
0: TRAIN [4][1630/5173]	Time 0.583 (0.622)	Data 1.23e-04 (4.51e-04)	Tok/s 17720 (22738)	Loss/tok 2.9860 (3.3238)	LR 1.563e-05
0: TRAIN [4][1640/5173]	Time 0.582 (0.622)	Data 1.13e-04 (4.49e-04)	Tok/s 17871 (22751)	Loss/tok 3.1679 (3.3240)	LR 1.563e-05
0: TRAIN [4][1650/5173]	Time 0.646 (0.623)	Data 1.25e-04 (4.48e-04)	Tok/s 25539 (22773)	Loss/tok 3.4129 (3.3246)	LR 1.563e-05
0: TRAIN [4][1660/5173]	Time 0.522 (0.623)	Data 1.25e-04 (4.46e-04)	Tok/s 10133 (22779)	Loss/tok 2.6760 (3.3255)	LR 1.563e-05
0: TRAIN [4][1670/5173]	Time 0.585 (0.622)	Data 1.24e-04 (4.44e-04)	Tok/s 17774 (22763)	Loss/tok 3.0862 (3.3251)	LR 1.563e-05
0: TRAIN [4][1680/5173]	Time 0.584 (0.622)	Data 1.18e-04 (4.42e-04)	Tok/s 17750 (22759)	Loss/tok 3.0931 (3.3257)	LR 1.563e-05
0: TRAIN [4][1690/5173]	Time 0.583 (0.622)	Data 1.32e-04 (4.40e-04)	Tok/s 17464 (22761)	Loss/tok 3.1311 (3.3259)	LR 1.563e-05
0: TRAIN [4][1700/5173]	Time 0.644 (0.623)	Data 1.24e-04 (4.38e-04)	Tok/s 26521 (22764)	Loss/tok 3.3918 (3.3261)	LR 1.563e-05
0: TRAIN [4][1710/5173]	Time 0.580 (0.623)	Data 1.30e-04 (4.37e-04)	Tok/s 18006 (22771)	Loss/tok 3.2427 (3.3263)	LR 1.563e-05
0: TRAIN [4][1720/5173]	Time 0.706 (0.623)	Data 1.54e-04 (4.35e-04)	Tok/s 33108 (22764)	Loss/tok 3.4340 (3.3263)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1730/5173]	Time 0.580 (0.622)	Data 1.50e-04 (4.33e-04)	Tok/s 17508 (22763)	Loss/tok 3.0576 (3.3260)	LR 1.563e-05
0: TRAIN [4][1740/5173]	Time 0.772 (0.622)	Data 1.54e-04 (4.31e-04)	Tok/s 39127 (22756)	Loss/tok 3.5908 (3.3258)	LR 1.563e-05
0: TRAIN [4][1750/5173]	Time 0.647 (0.622)	Data 1.26e-04 (4.30e-04)	Tok/s 26017 (22759)	Loss/tok 3.4619 (3.3258)	LR 1.563e-05
0: TRAIN [4][1760/5173]	Time 0.581 (0.622)	Data 1.22e-04 (4.28e-04)	Tok/s 17740 (22758)	Loss/tok 3.1450 (3.3256)	LR 1.563e-05
0: TRAIN [4][1770/5173]	Time 0.580 (0.622)	Data 2.75e-04 (4.27e-04)	Tok/s 17671 (22752)	Loss/tok 3.1323 (3.3254)	LR 1.563e-05
0: TRAIN [4][1780/5173]	Time 0.520 (0.622)	Data 1.22e-04 (4.25e-04)	Tok/s 9957 (22742)	Loss/tok 2.6587 (3.3250)	LR 1.563e-05
0: TRAIN [4][1790/5173]	Time 0.645 (0.622)	Data 1.22e-04 (4.23e-04)	Tok/s 26037 (22749)	Loss/tok 3.3391 (3.3248)	LR 1.563e-05
0: TRAIN [4][1800/5173]	Time 0.646 (0.622)	Data 1.19e-04 (4.22e-04)	Tok/s 26314 (22741)	Loss/tok 3.2560 (3.3245)	LR 1.563e-05
0: TRAIN [4][1810/5173]	Time 0.704 (0.622)	Data 1.27e-04 (4.20e-04)	Tok/s 33242 (22742)	Loss/tok 3.5789 (3.3248)	LR 1.563e-05
0: TRAIN [4][1820/5173]	Time 0.646 (0.622)	Data 1.19e-04 (4.19e-04)	Tok/s 25984 (22745)	Loss/tok 3.3485 (3.3247)	LR 1.563e-05
0: TRAIN [4][1830/5173]	Time 0.704 (0.622)	Data 1.15e-04 (4.17e-04)	Tok/s 33193 (22751)	Loss/tok 3.5374 (3.3249)	LR 1.563e-05
0: TRAIN [4][1840/5173]	Time 0.522 (0.622)	Data 1.29e-04 (4.16e-04)	Tok/s 9990 (22750)	Loss/tok 2.6469 (3.3250)	LR 1.563e-05
0: TRAIN [4][1850/5173]	Time 0.582 (0.622)	Data 1.23e-04 (4.14e-04)	Tok/s 17574 (22748)	Loss/tok 3.0519 (3.3247)	LR 1.563e-05
0: TRAIN [4][1860/5173]	Time 0.583 (0.622)	Data 1.15e-04 (4.13e-04)	Tok/s 17873 (22738)	Loss/tok 3.1987 (3.3245)	LR 1.563e-05
0: TRAIN [4][1870/5173]	Time 0.703 (0.622)	Data 1.31e-04 (4.11e-04)	Tok/s 33660 (22748)	Loss/tok 3.3505 (3.3245)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1880/5173]	Time 0.583 (0.622)	Data 1.18e-04 (4.10e-04)	Tok/s 17782 (22751)	Loss/tok 3.0950 (3.3246)	LR 1.563e-05
0: TRAIN [4][1890/5173]	Time 0.641 (0.622)	Data 1.16e-04 (4.08e-04)	Tok/s 26484 (22747)	Loss/tok 3.2741 (3.3241)	LR 1.563e-05
0: TRAIN [4][1900/5173]	Time 0.583 (0.622)	Data 1.20e-04 (4.07e-04)	Tok/s 17514 (22753)	Loss/tok 3.1264 (3.3245)	LR 1.563e-05
0: TRAIN [4][1910/5173]	Time 0.584 (0.622)	Data 1.23e-04 (4.05e-04)	Tok/s 17629 (22767)	Loss/tok 3.1509 (3.3248)	LR 1.563e-05
0: TRAIN [4][1920/5173]	Time 0.585 (0.623)	Data 1.25e-04 (4.04e-04)	Tok/s 17540 (22784)	Loss/tok 3.2429 (3.3253)	LR 1.563e-05
0: TRAIN [4][1930/5173]	Time 0.586 (0.623)	Data 1.22e-04 (4.03e-04)	Tok/s 17622 (22801)	Loss/tok 3.1003 (3.3258)	LR 1.563e-05
0: TRAIN [4][1940/5173]	Time 0.583 (0.623)	Data 1.29e-04 (4.01e-04)	Tok/s 18051 (22792)	Loss/tok 3.0209 (3.3256)	LR 1.563e-05
0: TRAIN [4][1950/5173]	Time 0.581 (0.623)	Data 1.21e-04 (4.00e-04)	Tok/s 17676 (22800)	Loss/tok 3.0594 (3.3254)	LR 1.563e-05
0: TRAIN [4][1960/5173]	Time 0.779 (0.623)	Data 1.22e-04 (3.99e-04)	Tok/s 38357 (22812)	Loss/tok 3.6110 (3.3259)	LR 1.563e-05
0: TRAIN [4][1970/5173]	Time 0.638 (0.623)	Data 1.24e-04 (3.97e-04)	Tok/s 26785 (22814)	Loss/tok 3.3055 (3.3260)	LR 1.563e-05
0: TRAIN [4][1980/5173]	Time 0.584 (0.623)	Data 1.20e-04 (3.96e-04)	Tok/s 17981 (22809)	Loss/tok 3.1208 (3.3258)	LR 1.563e-05
0: TRAIN [4][1990/5173]	Time 0.585 (0.623)	Data 1.15e-04 (3.95e-04)	Tok/s 17474 (22798)	Loss/tok 3.1203 (3.3256)	LR 1.563e-05
0: TRAIN [4][2000/5173]	Time 0.585 (0.622)	Data 1.27e-04 (3.93e-04)	Tok/s 17254 (22769)	Loss/tok 3.1581 (3.3250)	LR 1.563e-05
0: TRAIN [4][2010/5173]	Time 0.650 (0.622)	Data 2.92e-04 (3.92e-04)	Tok/s 25922 (22771)	Loss/tok 3.3582 (3.3249)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2020/5173]	Time 0.580 (0.623)	Data 1.16e-04 (3.91e-04)	Tok/s 17687 (22776)	Loss/tok 3.1333 (3.3252)	LR 1.563e-05
0: TRAIN [4][2030/5173]	Time 0.640 (0.622)	Data 1.33e-04 (3.90e-04)	Tok/s 26331 (22759)	Loss/tok 3.3091 (3.3247)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][2040/5173]	Time 0.584 (0.623)	Data 1.36e-04 (3.88e-04)	Tok/s 17743 (22776)	Loss/tok 3.0324 (3.3252)	LR 1.563e-05
0: TRAIN [4][2050/5173]	Time 0.640 (0.623)	Data 1.24e-04 (3.87e-04)	Tok/s 26340 (22790)	Loss/tok 3.3979 (3.3257)	LR 1.563e-05
0: TRAIN [4][2060/5173]	Time 0.580 (0.623)	Data 1.26e-04 (3.86e-04)	Tok/s 17744 (22784)	Loss/tok 3.1424 (3.3254)	LR 1.563e-05
0: TRAIN [4][2070/5173]	Time 0.640 (0.623)	Data 1.20e-04 (3.85e-04)	Tok/s 26494 (22780)	Loss/tok 3.3843 (3.3252)	LR 1.563e-05
0: TRAIN [4][2080/5173]	Time 0.646 (0.623)	Data 1.15e-04 (3.83e-04)	Tok/s 25487 (22774)	Loss/tok 3.5153 (3.3252)	LR 1.563e-05
0: TRAIN [4][2090/5173]	Time 0.585 (0.622)	Data 1.20e-04 (3.82e-04)	Tok/s 17216 (22767)	Loss/tok 3.1892 (3.3252)	LR 1.563e-05
0: TRAIN [4][2100/5173]	Time 0.583 (0.622)	Data 1.29e-04 (3.81e-04)	Tok/s 17694 (22767)	Loss/tok 3.1577 (3.3251)	LR 1.563e-05
0: TRAIN [4][2110/5173]	Time 0.582 (0.622)	Data 1.17e-04 (3.80e-04)	Tok/s 17312 (22762)	Loss/tok 3.2115 (3.3248)	LR 1.563e-05
0: TRAIN [4][2120/5173]	Time 0.522 (0.622)	Data 1.29e-04 (3.79e-04)	Tok/s 10360 (22750)	Loss/tok 2.6606 (3.3245)	LR 1.563e-05
0: TRAIN [4][2130/5173]	Time 0.579 (0.622)	Data 1.27e-04 (3.78e-04)	Tok/s 17537 (22726)	Loss/tok 3.0592 (3.3239)	LR 1.563e-05
0: TRAIN [4][2140/5173]	Time 0.640 (0.622)	Data 1.17e-04 (3.76e-04)	Tok/s 25818 (22723)	Loss/tok 3.3679 (3.3237)	LR 1.563e-05
0: TRAIN [4][2150/5173]	Time 0.583 (0.622)	Data 2.80e-04 (3.75e-04)	Tok/s 17720 (22737)	Loss/tok 3.0251 (3.3242)	LR 1.563e-05
0: TRAIN [4][2160/5173]	Time 0.705 (0.622)	Data 1.45e-04 (3.74e-04)	Tok/s 33230 (22767)	Loss/tok 3.4357 (3.3248)	LR 1.563e-05
0: TRAIN [4][2170/5173]	Time 0.583 (0.622)	Data 3.00e-04 (3.73e-04)	Tok/s 17282 (22758)	Loss/tok 3.1379 (3.3248)	LR 1.563e-05
0: TRAIN [4][2180/5173]	Time 0.646 (0.622)	Data 1.14e-04 (3.72e-04)	Tok/s 26011 (22751)	Loss/tok 3.2581 (3.3246)	LR 1.563e-05
0: TRAIN [4][2190/5173]	Time 0.777 (0.622)	Data 1.25e-04 (3.71e-04)	Tok/s 38219 (22750)	Loss/tok 3.7661 (3.3246)	LR 1.563e-05
0: TRAIN [4][2200/5173]	Time 0.582 (0.622)	Data 1.16e-04 (3.70e-04)	Tok/s 17701 (22742)	Loss/tok 3.1024 (3.3241)	LR 1.563e-05
0: TRAIN [4][2210/5173]	Time 0.646 (0.622)	Data 1.15e-04 (3.69e-04)	Tok/s 25938 (22724)	Loss/tok 3.3556 (3.3236)	LR 1.563e-05
0: TRAIN [4][2220/5173]	Time 0.776 (0.622)	Data 1.34e-04 (3.68e-04)	Tok/s 38524 (22730)	Loss/tok 3.6677 (3.3239)	LR 1.563e-05
0: TRAIN [4][2230/5173]	Time 0.642 (0.622)	Data 1.12e-04 (3.67e-04)	Tok/s 26319 (22743)	Loss/tok 3.2546 (3.3241)	LR 1.563e-05
0: TRAIN [4][2240/5173]	Time 0.643 (0.622)	Data 1.14e-04 (3.66e-04)	Tok/s 26299 (22739)	Loss/tok 3.4030 (3.3239)	LR 1.563e-05
0: TRAIN [4][2250/5173]	Time 0.638 (0.622)	Data 1.17e-04 (3.65e-04)	Tok/s 26093 (22727)	Loss/tok 3.4135 (3.3236)	LR 1.563e-05
0: TRAIN [4][2260/5173]	Time 0.705 (0.622)	Data 1.21e-04 (3.64e-04)	Tok/s 32871 (22739)	Loss/tok 3.5469 (3.3239)	LR 1.563e-05
0: TRAIN [4][2270/5173]	Time 0.581 (0.622)	Data 1.14e-04 (3.63e-04)	Tok/s 17669 (22761)	Loss/tok 3.1119 (3.3247)	LR 1.563e-05
0: TRAIN [4][2280/5173]	Time 0.582 (0.622)	Data 1.25e-04 (3.62e-04)	Tok/s 17968 (22751)	Loss/tok 3.1147 (3.3243)	LR 1.563e-05
0: TRAIN [4][2290/5173]	Time 0.582 (0.622)	Data 1.22e-04 (3.61e-04)	Tok/s 17625 (22757)	Loss/tok 3.2594 (3.3244)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2300/5173]	Time 0.579 (0.622)	Data 1.29e-04 (3.60e-04)	Tok/s 17593 (22765)	Loss/tok 3.1153 (3.3246)	LR 1.563e-05
0: TRAIN [4][2310/5173]	Time 0.706 (0.622)	Data 1.16e-04 (3.59e-04)	Tok/s 32971 (22757)	Loss/tok 3.5729 (3.3244)	LR 1.563e-05
0: TRAIN [4][2320/5173]	Time 0.581 (0.622)	Data 1.22e-04 (3.58e-04)	Tok/s 17646 (22743)	Loss/tok 3.0960 (3.3240)	LR 1.563e-05
0: TRAIN [4][2330/5173]	Time 0.584 (0.622)	Data 1.18e-04 (3.57e-04)	Tok/s 17842 (22725)	Loss/tok 3.1095 (3.3234)	LR 1.563e-05
0: TRAIN [4][2340/5173]	Time 0.581 (0.622)	Data 1.18e-04 (3.56e-04)	Tok/s 17321 (22719)	Loss/tok 3.1373 (3.3229)	LR 1.563e-05
0: TRAIN [4][2350/5173]	Time 0.584 (0.622)	Data 1.19e-04 (3.55e-04)	Tok/s 17823 (22716)	Loss/tok 3.1279 (3.3227)	LR 1.563e-05
0: TRAIN [4][2360/5173]	Time 0.582 (0.622)	Data 1.19e-04 (3.54e-04)	Tok/s 17455 (22720)	Loss/tok 3.2109 (3.3229)	LR 1.563e-05
0: TRAIN [4][2370/5173]	Time 0.586 (0.622)	Data 1.16e-04 (3.53e-04)	Tok/s 17470 (22703)	Loss/tok 3.0216 (3.3226)	LR 1.563e-05
0: TRAIN [4][2380/5173]	Time 0.704 (0.622)	Data 1.16e-04 (3.53e-04)	Tok/s 32897 (22701)	Loss/tok 3.5782 (3.3226)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][2390/5173]	Time 0.781 (0.622)	Data 1.22e-04 (3.52e-04)	Tok/s 37814 (22714)	Loss/tok 3.6027 (3.3231)	LR 1.563e-05
0: TRAIN [4][2400/5173]	Time 0.583 (0.622)	Data 1.17e-04 (3.51e-04)	Tok/s 17847 (22714)	Loss/tok 3.1765 (3.3230)	LR 1.563e-05
0: TRAIN [4][2410/5173]	Time 0.640 (0.622)	Data 1.14e-04 (3.50e-04)	Tok/s 26218 (22725)	Loss/tok 3.3744 (3.3234)	LR 1.563e-05
0: TRAIN [4][2420/5173]	Time 0.646 (0.622)	Data 1.19e-04 (3.49e-04)	Tok/s 26398 (22733)	Loss/tok 3.3099 (3.3235)	LR 1.563e-05
0: TRAIN [4][2430/5173]	Time 0.584 (0.622)	Data 2.98e-04 (3.48e-04)	Tok/s 18188 (22723)	Loss/tok 3.0870 (3.3231)	LR 1.563e-05
0: TRAIN [4][2440/5173]	Time 0.580 (0.622)	Data 1.20e-04 (3.47e-04)	Tok/s 17916 (22716)	Loss/tok 3.1071 (3.3229)	LR 1.563e-05
0: TRAIN [4][2450/5173]	Time 0.777 (0.622)	Data 1.23e-04 (3.46e-04)	Tok/s 38528 (22711)	Loss/tok 3.5777 (3.3228)	LR 1.563e-05
0: TRAIN [4][2460/5173]	Time 0.643 (0.622)	Data 3.03e-04 (3.46e-04)	Tok/s 26071 (22707)	Loss/tok 3.2581 (3.3228)	LR 1.563e-05
0: TRAIN [4][2470/5173]	Time 0.584 (0.622)	Data 1.29e-04 (3.45e-04)	Tok/s 17508 (22697)	Loss/tok 3.2385 (3.3224)	LR 1.563e-05
0: TRAIN [4][2480/5173]	Time 0.579 (0.622)	Data 1.27e-04 (3.44e-04)	Tok/s 17755 (22692)	Loss/tok 3.0308 (3.3223)	LR 1.563e-05
0: TRAIN [4][2490/5173]	Time 0.583 (0.622)	Data 3.02e-04 (3.43e-04)	Tok/s 17930 (22692)	Loss/tok 3.0992 (3.3222)	LR 1.563e-05
0: TRAIN [4][2500/5173]	Time 0.581 (0.622)	Data 1.21e-04 (3.43e-04)	Tok/s 17533 (22690)	Loss/tok 3.1332 (3.3222)	LR 1.563e-05
0: TRAIN [4][2510/5173]	Time 0.708 (0.622)	Data 1.27e-04 (3.42e-04)	Tok/s 33208 (22694)	Loss/tok 3.5066 (3.3225)	LR 1.563e-05
0: TRAIN [4][2520/5173]	Time 0.640 (0.622)	Data 1.18e-04 (3.41e-04)	Tok/s 25988 (22697)	Loss/tok 3.3023 (3.3225)	LR 1.563e-05
0: TRAIN [4][2530/5173]	Time 0.520 (0.622)	Data 1.13e-04 (3.40e-04)	Tok/s 10011 (22695)	Loss/tok 2.6646 (3.3225)	LR 1.563e-05
0: TRAIN [4][2540/5173]	Time 0.521 (0.622)	Data 1.22e-04 (3.39e-04)	Tok/s 10276 (22688)	Loss/tok 2.7371 (3.3224)	LR 1.563e-05
0: TRAIN [4][2550/5173]	Time 0.583 (0.622)	Data 1.19e-04 (3.38e-04)	Tok/s 17521 (22683)	Loss/tok 3.1839 (3.3223)	LR 1.563e-05
0: TRAIN [4][2560/5173]	Time 0.776 (0.622)	Data 3.06e-04 (3.38e-04)	Tok/s 37983 (22682)	Loss/tok 3.6205 (3.3226)	LR 1.563e-05
0: TRAIN [4][2570/5173]	Time 0.584 (0.622)	Data 1.19e-04 (3.37e-04)	Tok/s 17357 (22684)	Loss/tok 3.1484 (3.3227)	LR 1.563e-05
0: TRAIN [4][2580/5173]	Time 0.582 (0.622)	Data 1.25e-04 (3.36e-04)	Tok/s 17666 (22659)	Loss/tok 3.1476 (3.3221)	LR 1.563e-05
0: TRAIN [4][2590/5173]	Time 0.640 (0.622)	Data 1.16e-04 (3.36e-04)	Tok/s 26498 (22662)	Loss/tok 3.3696 (3.3221)	LR 1.563e-05
0: TRAIN [4][2600/5173]	Time 0.703 (0.622)	Data 1.25e-04 (3.35e-04)	Tok/s 32905 (22663)	Loss/tok 3.5734 (3.3222)	LR 1.563e-05
0: TRAIN [4][2610/5173]	Time 0.707 (0.622)	Data 1.21e-04 (3.34e-04)	Tok/s 32521 (22680)	Loss/tok 3.4926 (3.3226)	LR 1.563e-05
0: TRAIN [4][2620/5173]	Time 0.582 (0.622)	Data 1.37e-04 (3.33e-04)	Tok/s 17968 (22696)	Loss/tok 3.2170 (3.3231)	LR 1.563e-05
0: TRAIN [4][2630/5173]	Time 0.643 (0.622)	Data 1.88e-04 (3.33e-04)	Tok/s 26125 (22680)	Loss/tok 3.2453 (3.3226)	LR 1.563e-05
0: TRAIN [4][2640/5173]	Time 0.705 (0.622)	Data 1.85e-04 (3.32e-04)	Tok/s 33425 (22687)	Loss/tok 3.5064 (3.3229)	LR 1.563e-05
0: TRAIN [4][2650/5173]	Time 0.703 (0.622)	Data 1.85e-04 (3.32e-04)	Tok/s 33297 (22713)	Loss/tok 3.6067 (3.3242)	LR 1.563e-05
0: TRAIN [4][2660/5173]	Time 0.584 (0.622)	Data 1.82e-04 (3.31e-04)	Tok/s 17800 (22698)	Loss/tok 3.0338 (3.3237)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2670/5173]	Time 0.637 (0.622)	Data 1.22e-04 (3.31e-04)	Tok/s 26050 (22712)	Loss/tok 3.3918 (3.3243)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][2680/5173]	Time 0.580 (0.622)	Data 1.80e-04 (3.30e-04)	Tok/s 18134 (22721)	Loss/tok 3.1447 (3.3248)	LR 1.563e-05
0: TRAIN [4][2690/5173]	Time 0.583 (0.622)	Data 1.16e-04 (3.29e-04)	Tok/s 18192 (22709)	Loss/tok 3.0943 (3.3243)	LR 1.563e-05
0: TRAIN [4][2700/5173]	Time 0.645 (0.622)	Data 1.23e-04 (3.29e-04)	Tok/s 26260 (22711)	Loss/tok 3.4405 (3.3244)	LR 1.563e-05
0: TRAIN [4][2710/5173]	Time 0.580 (0.622)	Data 1.83e-04 (3.28e-04)	Tok/s 17811 (22701)	Loss/tok 3.1919 (3.3242)	LR 1.563e-05
0: TRAIN [4][2720/5173]	Time 0.582 (0.622)	Data 1.39e-04 (3.27e-04)	Tok/s 17728 (22687)	Loss/tok 3.0367 (3.3238)	LR 1.563e-05
0: TRAIN [4][2730/5173]	Time 0.645 (0.622)	Data 1.44e-04 (3.27e-04)	Tok/s 25628 (22682)	Loss/tok 3.3673 (3.3238)	LR 1.563e-05
0: TRAIN [4][2740/5173]	Time 0.584 (0.622)	Data 1.22e-04 (3.26e-04)	Tok/s 17460 (22681)	Loss/tok 3.1854 (3.3238)	LR 1.563e-05
0: TRAIN [4][2750/5173]	Time 0.581 (0.622)	Data 3.10e-04 (3.25e-04)	Tok/s 18084 (22660)	Loss/tok 3.0702 (3.3232)	LR 1.563e-05
0: TRAIN [4][2760/5173]	Time 0.644 (0.622)	Data 3.07e-04 (3.25e-04)	Tok/s 25963 (22663)	Loss/tok 3.3246 (3.3235)	LR 1.563e-05
0: TRAIN [4][2770/5173]	Time 0.518 (0.622)	Data 1.18e-04 (3.24e-04)	Tok/s 9876 (22649)	Loss/tok 2.6647 (3.3230)	LR 1.563e-05
0: TRAIN [4][2780/5173]	Time 0.584 (0.622)	Data 1.22e-04 (3.23e-04)	Tok/s 17747 (22649)	Loss/tok 3.1299 (3.3229)	LR 1.563e-05
0: TRAIN [4][2790/5173]	Time 0.708 (0.622)	Data 1.22e-04 (3.23e-04)	Tok/s 32978 (22649)	Loss/tok 3.4823 (3.3229)	LR 1.563e-05
0: TRAIN [4][2800/5173]	Time 0.581 (0.622)	Data 1.24e-04 (3.22e-04)	Tok/s 17739 (22650)	Loss/tok 3.0828 (3.3229)	LR 1.563e-05
0: TRAIN [4][2810/5173]	Time 0.776 (0.622)	Data 1.20e-04 (3.21e-04)	Tok/s 38023 (22654)	Loss/tok 3.6536 (3.3232)	LR 1.563e-05
0: TRAIN [4][2820/5173]	Time 0.580 (0.622)	Data 1.17e-04 (3.21e-04)	Tok/s 17946 (22653)	Loss/tok 3.0304 (3.3231)	LR 1.563e-05
0: TRAIN [4][2830/5173]	Time 0.582 (0.622)	Data 1.21e-04 (3.20e-04)	Tok/s 17616 (22648)	Loss/tok 3.1200 (3.3229)	LR 1.563e-05
0: TRAIN [4][2840/5173]	Time 0.584 (0.622)	Data 1.19e-04 (3.19e-04)	Tok/s 17935 (22653)	Loss/tok 3.0705 (3.3230)	LR 1.563e-05
0: TRAIN [4][2850/5173]	Time 0.709 (0.622)	Data 1.23e-04 (3.19e-04)	Tok/s 32966 (22654)	Loss/tok 3.5371 (3.3231)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][2860/5173]	Time 0.709 (0.622)	Data 1.23e-04 (3.18e-04)	Tok/s 33238 (22664)	Loss/tok 3.4795 (3.3234)	LR 1.563e-05
0: TRAIN [4][2870/5173]	Time 0.704 (0.622)	Data 1.15e-04 (3.17e-04)	Tok/s 33048 (22664)	Loss/tok 3.5248 (3.3234)	LR 1.563e-05
0: TRAIN [4][2880/5173]	Time 0.644 (0.622)	Data 1.20e-04 (3.17e-04)	Tok/s 26005 (22669)	Loss/tok 3.3166 (3.3235)	LR 1.563e-05
0: TRAIN [4][2890/5173]	Time 0.583 (0.622)	Data 1.21e-04 (3.16e-04)	Tok/s 17854 (22673)	Loss/tok 3.0999 (3.3234)	LR 1.563e-05
0: TRAIN [4][2900/5173]	Time 0.705 (0.622)	Data 1.14e-04 (3.16e-04)	Tok/s 33584 (22670)	Loss/tok 3.5448 (3.3233)	LR 1.563e-05
0: TRAIN [4][2910/5173]	Time 0.582 (0.622)	Data 1.28e-04 (3.15e-04)	Tok/s 17627 (22667)	Loss/tok 3.0699 (3.3232)	LR 1.563e-05
0: TRAIN [4][2920/5173]	Time 0.708 (0.622)	Data 1.25e-04 (3.14e-04)	Tok/s 32819 (22669)	Loss/tok 3.5780 (3.3233)	LR 1.563e-05
0: TRAIN [4][2930/5173]	Time 0.581 (0.622)	Data 1.21e-04 (3.14e-04)	Tok/s 17566 (22662)	Loss/tok 3.0771 (3.3232)	LR 1.563e-05
0: TRAIN [4][2940/5173]	Time 0.642 (0.622)	Data 1.16e-04 (3.13e-04)	Tok/s 26379 (22660)	Loss/tok 3.3078 (3.3229)	LR 1.563e-05
0: TRAIN [4][2950/5173]	Time 0.647 (0.622)	Data 1.21e-04 (3.13e-04)	Tok/s 26020 (22678)	Loss/tok 3.2230 (3.3236)	LR 1.563e-05
0: TRAIN [4][2960/5173]	Time 0.646 (0.622)	Data 1.19e-04 (3.12e-04)	Tok/s 25890 (22677)	Loss/tok 3.3874 (3.3238)	LR 1.563e-05
0: TRAIN [4][2970/5173]	Time 0.582 (0.622)	Data 1.29e-04 (3.11e-04)	Tok/s 17603 (22680)	Loss/tok 3.0516 (3.3240)	LR 1.563e-05
0: TRAIN [4][2980/5173]	Time 0.584 (0.622)	Data 1.45e-04 (3.11e-04)	Tok/s 17766 (22688)	Loss/tok 3.2093 (3.3244)	LR 1.563e-05
0: TRAIN [4][2990/5173]	Time 0.710 (0.622)	Data 1.22e-04 (3.10e-04)	Tok/s 32519 (22694)	Loss/tok 3.5159 (3.3245)	LR 1.563e-05
0: TRAIN [4][3000/5173]	Time 0.521 (0.622)	Data 1.30e-04 (3.10e-04)	Tok/s 9948 (22691)	Loss/tok 2.8079 (3.3244)	LR 1.563e-05
0: TRAIN [4][3010/5173]	Time 0.520 (0.622)	Data 1.35e-04 (3.09e-04)	Tok/s 10167 (22687)	Loss/tok 2.7120 (3.3245)	LR 1.563e-05
0: TRAIN [4][3020/5173]	Time 0.645 (0.622)	Data 1.25e-04 (3.09e-04)	Tok/s 25710 (22687)	Loss/tok 3.2659 (3.3244)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][3030/5173]	Time 0.777 (0.622)	Data 1.25e-04 (3.08e-04)	Tok/s 38674 (22695)	Loss/tok 3.6366 (3.3245)	LR 1.563e-05
0: TRAIN [4][3040/5173]	Time 0.707 (0.622)	Data 1.29e-04 (3.08e-04)	Tok/s 32993 (22689)	Loss/tok 3.5068 (3.3244)	LR 1.563e-05
0: TRAIN [4][3050/5173]	Time 0.519 (0.622)	Data 1.26e-04 (3.07e-04)	Tok/s 10206 (22687)	Loss/tok 2.6583 (3.3242)	LR 1.563e-05
0: TRAIN [4][3060/5173]	Time 0.582 (0.622)	Data 1.29e-04 (3.07e-04)	Tok/s 18004 (22676)	Loss/tok 3.0483 (3.3239)	LR 1.563e-05
0: TRAIN [4][3070/5173]	Time 0.585 (0.622)	Data 1.24e-04 (3.06e-04)	Tok/s 17465 (22672)	Loss/tok 3.1506 (3.3241)	LR 1.563e-05
0: TRAIN [4][3080/5173]	Time 0.581 (0.622)	Data 1.33e-04 (3.05e-04)	Tok/s 18166 (22673)	Loss/tok 3.2389 (3.3244)	LR 1.563e-05
0: TRAIN [4][3090/5173]	Time 0.644 (0.622)	Data 1.29e-04 (3.05e-04)	Tok/s 26023 (22670)	Loss/tok 3.2990 (3.3241)	LR 1.563e-05
0: TRAIN [4][3100/5173]	Time 0.784 (0.622)	Data 1.32e-04 (3.04e-04)	Tok/s 38583 (22661)	Loss/tok 3.5521 (3.3239)	LR 1.563e-05
0: TRAIN [4][3110/5173]	Time 0.706 (0.622)	Data 1.26e-04 (3.04e-04)	Tok/s 32702 (22659)	Loss/tok 3.5973 (3.3239)	LR 1.563e-05
0: TRAIN [4][3120/5173]	Time 0.579 (0.622)	Data 1.25e-04 (3.03e-04)	Tok/s 18116 (22650)	Loss/tok 3.0108 (3.3238)	LR 1.563e-05
0: TRAIN [4][3130/5173]	Time 0.708 (0.622)	Data 2.96e-04 (3.03e-04)	Tok/s 32866 (22665)	Loss/tok 3.5890 (3.3244)	LR 1.563e-05
0: TRAIN [4][3140/5173]	Time 0.584 (0.622)	Data 1.29e-04 (3.02e-04)	Tok/s 17903 (22653)	Loss/tok 3.0555 (3.3240)	LR 1.563e-05
0: TRAIN [4][3150/5173]	Time 0.640 (0.622)	Data 1.54e-04 (3.02e-04)	Tok/s 26091 (22668)	Loss/tok 3.2658 (3.3243)	LR 1.563e-05
0: TRAIN [4][3160/5173]	Time 0.582 (0.622)	Data 1.21e-04 (3.01e-04)	Tok/s 17755 (22658)	Loss/tok 3.1725 (3.3241)	LR 1.563e-05
0: TRAIN [4][3170/5173]	Time 0.520 (0.622)	Data 1.41e-04 (3.01e-04)	Tok/s 10306 (22666)	Loss/tok 2.7095 (3.3244)	LR 1.563e-05
0: TRAIN [4][3180/5173]	Time 0.704 (0.622)	Data 1.30e-04 (3.00e-04)	Tok/s 33124 (22671)	Loss/tok 3.5346 (3.3246)	LR 1.563e-05
0: TRAIN [4][3190/5173]	Time 0.708 (0.622)	Data 1.18e-04 (3.00e-04)	Tok/s 32936 (22681)	Loss/tok 3.5212 (3.3248)	LR 1.563e-05
0: TRAIN [4][3200/5173]	Time 0.582 (0.622)	Data 3.54e-04 (3.00e-04)	Tok/s 18043 (22688)	Loss/tok 3.0060 (3.3251)	LR 1.563e-05
0: TRAIN [4][3210/5173]	Time 0.646 (0.622)	Data 1.48e-04 (2.99e-04)	Tok/s 25990 (22691)	Loss/tok 3.3122 (3.3250)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][3220/5173]	Time 0.779 (0.622)	Data 3.41e-04 (2.99e-04)	Tok/s 38523 (22694)	Loss/tok 3.6591 (3.3252)	LR 1.563e-05
0: TRAIN [4][3230/5173]	Time 0.581 (0.622)	Data 1.34e-04 (2.98e-04)	Tok/s 17950 (22695)	Loss/tok 3.1699 (3.3254)	LR 1.563e-05
0: TRAIN [4][3240/5173]	Time 0.518 (0.622)	Data 1.19e-04 (2.98e-04)	Tok/s 10094 (22693)	Loss/tok 2.7172 (3.3254)	LR 1.563e-05
0: TRAIN [4][3250/5173]	Time 0.639 (0.622)	Data 3.09e-04 (2.97e-04)	Tok/s 26377 (22712)	Loss/tok 3.2449 (3.3259)	LR 1.563e-05
0: TRAIN [4][3260/5173]	Time 0.642 (0.622)	Data 1.22e-04 (2.97e-04)	Tok/s 26216 (22716)	Loss/tok 3.2603 (3.3258)	LR 1.563e-05
0: TRAIN [4][3270/5173]	Time 0.704 (0.622)	Data 1.29e-04 (2.96e-04)	Tok/s 32963 (22722)	Loss/tok 3.5324 (3.3260)	LR 1.563e-05
0: TRAIN [4][3280/5173]	Time 0.581 (0.622)	Data 1.23e-04 (2.96e-04)	Tok/s 17686 (22719)	Loss/tok 3.2620 (3.3258)	LR 1.563e-05
0: TRAIN [4][3290/5173]	Time 0.779 (0.622)	Data 2.85e-04 (2.95e-04)	Tok/s 38510 (22725)	Loss/tok 3.6921 (3.3260)	LR 1.563e-05
0: TRAIN [4][3300/5173]	Time 0.644 (0.622)	Data 1.37e-04 (2.95e-04)	Tok/s 26163 (22732)	Loss/tok 3.3273 (3.3262)	LR 1.563e-05
0: TRAIN [4][3310/5173]	Time 0.584 (0.622)	Data 1.21e-04 (2.94e-04)	Tok/s 17686 (22729)	Loss/tok 2.9799 (3.3259)	LR 1.563e-05
0: TRAIN [4][3320/5173]	Time 0.705 (0.622)	Data 1.23e-04 (2.94e-04)	Tok/s 33101 (22726)	Loss/tok 3.4943 (3.3257)	LR 1.563e-05
0: TRAIN [4][3330/5173]	Time 0.582 (0.622)	Data 1.29e-04 (2.93e-04)	Tok/s 17743 (22734)	Loss/tok 3.1177 (3.3261)	LR 1.563e-05
0: TRAIN [4][3340/5173]	Time 0.647 (0.622)	Data 1.21e-04 (2.93e-04)	Tok/s 25551 (22727)	Loss/tok 3.3435 (3.3259)	LR 1.563e-05
0: TRAIN [4][3350/5173]	Time 0.519 (0.622)	Data 1.20e-04 (2.93e-04)	Tok/s 9998 (22724)	Loss/tok 2.7964 (3.3258)	LR 1.563e-05
0: TRAIN [4][3360/5173]	Time 0.582 (0.622)	Data 1.28e-04 (2.92e-04)	Tok/s 17526 (22712)	Loss/tok 3.1986 (3.3254)	LR 1.563e-05
0: TRAIN [4][3370/5173]	Time 0.649 (0.622)	Data 1.25e-04 (2.92e-04)	Tok/s 25894 (22720)	Loss/tok 3.3865 (3.3259)	LR 1.563e-05
0: TRAIN [4][3380/5173]	Time 0.521 (0.622)	Data 1.21e-04 (2.91e-04)	Tok/s 10178 (22717)	Loss/tok 2.7439 (3.3259)	LR 1.563e-05
0: TRAIN [4][3390/5173]	Time 0.584 (0.622)	Data 1.27e-04 (2.91e-04)	Tok/s 17593 (22716)	Loss/tok 3.0802 (3.3258)	LR 1.563e-05
0: TRAIN [4][3400/5173]	Time 0.644 (0.622)	Data 1.35e-04 (2.90e-04)	Tok/s 26097 (22723)	Loss/tok 3.3066 (3.3260)	LR 1.563e-05
0: TRAIN [4][3410/5173]	Time 0.520 (0.622)	Data 1.20e-04 (2.90e-04)	Tok/s 9938 (22711)	Loss/tok 2.8382 (3.3258)	LR 1.563e-05
0: TRAIN [4][3420/5173]	Time 0.646 (0.622)	Data 1.24e-04 (2.90e-04)	Tok/s 25647 (22712)	Loss/tok 3.3471 (3.3258)	LR 1.563e-05
0: TRAIN [4][3430/5173]	Time 0.583 (0.622)	Data 2.86e-04 (2.89e-04)	Tok/s 17993 (22704)	Loss/tok 3.1227 (3.3255)	LR 1.563e-05
0: TRAIN [4][3440/5173]	Time 0.644 (0.622)	Data 1.24e-04 (2.89e-04)	Tok/s 26402 (22705)	Loss/tok 3.4282 (3.3257)	LR 1.563e-05
0: TRAIN [4][3450/5173]	Time 0.523 (0.622)	Data 1.32e-04 (2.88e-04)	Tok/s 10200 (22713)	Loss/tok 2.6657 (3.3260)	LR 1.563e-05
0: TRAIN [4][3460/5173]	Time 0.640 (0.622)	Data 1.27e-04 (2.88e-04)	Tok/s 26260 (22712)	Loss/tok 3.2376 (3.3260)	LR 1.563e-05
0: TRAIN [4][3470/5173]	Time 0.780 (0.622)	Data 1.26e-04 (2.88e-04)	Tok/s 38240 (22718)	Loss/tok 3.7021 (3.3261)	LR 1.563e-05
0: TRAIN [4][3480/5173]	Time 0.582 (0.622)	Data 1.33e-04 (2.87e-04)	Tok/s 17850 (22708)	Loss/tok 3.1060 (3.3258)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3490/5173]	Time 0.643 (0.622)	Data 1.28e-04 (2.87e-04)	Tok/s 26575 (22714)	Loss/tok 3.3042 (3.3261)	LR 1.563e-05
0: TRAIN [4][3500/5173]	Time 0.579 (0.622)	Data 1.31e-04 (2.86e-04)	Tok/s 18128 (22723)	Loss/tok 3.1964 (3.3264)	LR 1.563e-05
0: TRAIN [4][3510/5173]	Time 0.641 (0.622)	Data 1.20e-04 (2.86e-04)	Tok/s 26157 (22721)	Loss/tok 3.3307 (3.3262)	LR 1.563e-05
0: TRAIN [4][3520/5173]	Time 0.581 (0.622)	Data 1.31e-04 (2.85e-04)	Tok/s 17841 (22729)	Loss/tok 3.1730 (3.3265)	LR 1.563e-05
0: TRAIN [4][3530/5173]	Time 0.518 (0.622)	Data 1.24e-04 (2.85e-04)	Tok/s 10054 (22727)	Loss/tok 2.6800 (3.3263)	LR 1.563e-05
0: TRAIN [4][3540/5173]	Time 0.642 (0.622)	Data 1.22e-04 (2.85e-04)	Tok/s 26266 (22720)	Loss/tok 3.3099 (3.3260)	LR 1.563e-05
0: TRAIN [4][3550/5173]	Time 0.704 (0.622)	Data 1.26e-04 (2.84e-04)	Tok/s 33604 (22713)	Loss/tok 3.4654 (3.3258)	LR 1.563e-05
0: TRAIN [4][3560/5173]	Time 0.707 (0.622)	Data 1.29e-04 (2.84e-04)	Tok/s 33176 (22727)	Loss/tok 3.5499 (3.3262)	LR 1.563e-05
0: TRAIN [4][3570/5173]	Time 0.582 (0.622)	Data 1.27e-04 (2.83e-04)	Tok/s 17986 (22721)	Loss/tok 3.0211 (3.3262)	LR 1.563e-05
0: TRAIN [4][3580/5173]	Time 0.706 (0.622)	Data 1.35e-04 (2.83e-04)	Tok/s 33259 (22734)	Loss/tok 3.5104 (3.3266)	LR 1.563e-05
0: TRAIN [4][3590/5173]	Time 0.517 (0.622)	Data 1.24e-04 (2.83e-04)	Tok/s 10203 (22731)	Loss/tok 2.7035 (3.3265)	LR 1.563e-05
0: TRAIN [4][3600/5173]	Time 0.523 (0.622)	Data 1.25e-04 (2.82e-04)	Tok/s 10124 (22735)	Loss/tok 2.7551 (3.3266)	LR 1.563e-05
0: TRAIN [4][3610/5173]	Time 0.701 (0.622)	Data 1.26e-04 (2.82e-04)	Tok/s 33659 (22737)	Loss/tok 3.5496 (3.3266)	LR 1.563e-05
0: TRAIN [4][3620/5173]	Time 0.583 (0.622)	Data 2.79e-04 (2.81e-04)	Tok/s 17885 (22726)	Loss/tok 3.0682 (3.3263)	LR 1.563e-05
0: TRAIN [4][3630/5173]	Time 0.583 (0.622)	Data 1.17e-04 (2.81e-04)	Tok/s 17639 (22727)	Loss/tok 3.1338 (3.3263)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3640/5173]	Time 0.643 (0.622)	Data 1.27e-04 (2.81e-04)	Tok/s 26000 (22728)	Loss/tok 3.3930 (3.3264)	LR 1.563e-05
0: TRAIN [4][3650/5173]	Time 0.644 (0.622)	Data 3.14e-04 (2.80e-04)	Tok/s 26218 (22735)	Loss/tok 3.2947 (3.3266)	LR 1.563e-05
0: TRAIN [4][3660/5173]	Time 0.581 (0.622)	Data 1.27e-04 (2.80e-04)	Tok/s 17647 (22731)	Loss/tok 3.2136 (3.3264)	LR 1.563e-05
0: TRAIN [4][3670/5173]	Time 0.778 (0.622)	Data 1.38e-04 (2.79e-04)	Tok/s 37503 (22729)	Loss/tok 3.8150 (3.3265)	LR 1.563e-05
0: TRAIN [4][3680/5173]	Time 0.641 (0.622)	Data 1.29e-04 (2.79e-04)	Tok/s 26416 (22724)	Loss/tok 3.3308 (3.3264)	LR 1.563e-05
0: TRAIN [4][3690/5173]	Time 0.638 (0.622)	Data 1.36e-04 (2.79e-04)	Tok/s 26047 (22723)	Loss/tok 3.1893 (3.3264)	LR 1.563e-05
0: TRAIN [4][3700/5173]	Time 0.583 (0.622)	Data 1.25e-04 (2.78e-04)	Tok/s 18125 (22729)	Loss/tok 3.0751 (3.3265)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][3710/5173]	Time 0.642 (0.622)	Data 1.45e-04 (2.78e-04)	Tok/s 25835 (22737)	Loss/tok 3.3804 (3.3266)	LR 1.563e-05
0: TRAIN [4][3720/5173]	Time 0.581 (0.622)	Data 1.24e-04 (2.78e-04)	Tok/s 17871 (22742)	Loss/tok 3.0778 (3.3267)	LR 1.563e-05
0: TRAIN [4][3730/5173]	Time 0.583 (0.622)	Data 1.23e-04 (2.77e-04)	Tok/s 17859 (22744)	Loss/tok 3.1149 (3.3269)	LR 1.563e-05
0: TRAIN [4][3740/5173]	Time 0.644 (0.622)	Data 1.29e-04 (2.77e-04)	Tok/s 25796 (22742)	Loss/tok 3.3961 (3.3269)	LR 1.563e-05
0: TRAIN [4][3750/5173]	Time 0.642 (0.622)	Data 1.24e-04 (2.76e-04)	Tok/s 25996 (22743)	Loss/tok 3.3514 (3.3268)	LR 1.563e-05
0: TRAIN [4][3760/5173]	Time 0.639 (0.622)	Data 1.19e-04 (2.76e-04)	Tok/s 26560 (22742)	Loss/tok 3.2999 (3.3266)	LR 1.563e-05
0: TRAIN [4][3770/5173]	Time 0.706 (0.622)	Data 1.35e-04 (2.76e-04)	Tok/s 33161 (22748)	Loss/tok 3.5674 (3.3270)	LR 1.563e-05
0: TRAIN [4][3780/5173]	Time 0.584 (0.622)	Data 1.28e-04 (2.75e-04)	Tok/s 17234 (22739)	Loss/tok 3.2364 (3.3267)	LR 1.563e-05
0: TRAIN [4][3790/5173]	Time 0.520 (0.622)	Data 1.23e-04 (2.75e-04)	Tok/s 10175 (22733)	Loss/tok 2.6900 (3.3266)	LR 1.563e-05
0: TRAIN [4][3800/5173]	Time 0.584 (0.622)	Data 1.27e-04 (2.75e-04)	Tok/s 17600 (22727)	Loss/tok 3.1224 (3.3266)	LR 1.563e-05
0: TRAIN [4][3810/5173]	Time 0.581 (0.622)	Data 1.29e-04 (2.74e-04)	Tok/s 17835 (22724)	Loss/tok 3.1029 (3.3264)	LR 1.563e-05
0: TRAIN [4][3820/5173]	Time 0.585 (0.622)	Data 1.23e-04 (2.74e-04)	Tok/s 17421 (22717)	Loss/tok 3.2095 (3.3262)	LR 1.563e-05
0: TRAIN [4][3830/5173]	Time 0.522 (0.622)	Data 1.26e-04 (2.74e-04)	Tok/s 10001 (22714)	Loss/tok 2.5961 (3.3262)	LR 1.563e-05
0: TRAIN [4][3840/5173]	Time 0.521 (0.622)	Data 1.28e-04 (2.73e-04)	Tok/s 10112 (22716)	Loss/tok 2.7181 (3.3262)	LR 1.563e-05
0: TRAIN [4][3850/5173]	Time 0.583 (0.622)	Data 1.53e-04 (2.73e-04)	Tok/s 17352 (22724)	Loss/tok 3.1477 (3.3266)	LR 1.563e-05
0: TRAIN [4][3860/5173]	Time 0.646 (0.622)	Data 1.48e-04 (2.73e-04)	Tok/s 25963 (22727)	Loss/tok 3.4455 (3.3267)	LR 1.563e-05
0: TRAIN [4][3870/5173]	Time 0.583 (0.622)	Data 1.34e-04 (2.72e-04)	Tok/s 17863 (22732)	Loss/tok 3.0997 (3.3268)	LR 1.563e-05
0: TRAIN [4][3880/5173]	Time 0.778 (0.622)	Data 1.30e-04 (2.72e-04)	Tok/s 38082 (22737)	Loss/tok 3.7707 (3.3271)	LR 1.563e-05
0: TRAIN [4][3890/5173]	Time 0.582 (0.622)	Data 1.23e-04 (2.72e-04)	Tok/s 17984 (22735)	Loss/tok 3.0785 (3.3271)	LR 1.563e-05
0: TRAIN [4][3900/5173]	Time 0.777 (0.622)	Data 3.16e-04 (2.71e-04)	Tok/s 38336 (22740)	Loss/tok 3.7824 (3.3273)	LR 1.563e-05
0: TRAIN [4][3910/5173]	Time 0.580 (0.622)	Data 1.42e-04 (2.71e-04)	Tok/s 17422 (22738)	Loss/tok 3.1546 (3.3271)	LR 1.563e-05
0: TRAIN [4][3920/5173]	Time 0.582 (0.622)	Data 1.34e-04 (2.71e-04)	Tok/s 17838 (22732)	Loss/tok 3.1105 (3.3269)	LR 1.563e-05
0: TRAIN [4][3930/5173]	Time 0.641 (0.622)	Data 2.93e-04 (2.70e-04)	Tok/s 26497 (22733)	Loss/tok 3.4324 (3.3269)	LR 1.563e-05
0: TRAIN [4][3940/5173]	Time 0.640 (0.622)	Data 3.23e-04 (2.70e-04)	Tok/s 26246 (22738)	Loss/tok 3.4108 (3.3271)	LR 1.563e-05
0: TRAIN [4][3950/5173]	Time 0.584 (0.622)	Data 1.20e-04 (2.70e-04)	Tok/s 17861 (22737)	Loss/tok 3.1279 (3.3271)	LR 1.563e-05
0: TRAIN [4][3960/5173]	Time 0.582 (0.622)	Data 3.30e-04 (2.69e-04)	Tok/s 17724 (22747)	Loss/tok 3.0756 (3.3274)	LR 1.563e-05
0: TRAIN [4][3970/5173]	Time 0.522 (0.622)	Data 1.23e-04 (2.69e-04)	Tok/s 10008 (22741)	Loss/tok 2.6442 (3.3272)	LR 1.563e-05
0: TRAIN [4][3980/5173]	Time 0.584 (0.622)	Data 1.29e-04 (2.69e-04)	Tok/s 17442 (22736)	Loss/tok 3.1631 (3.3271)	LR 1.563e-05
0: TRAIN [4][3990/5173]	Time 0.645 (0.622)	Data 1.28e-04 (2.68e-04)	Tok/s 25765 (22724)	Loss/tok 3.3347 (3.3269)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][4000/5173]	Time 0.639 (0.622)	Data 1.26e-04 (2.68e-04)	Tok/s 26079 (22728)	Loss/tok 3.3908 (3.3271)	LR 1.563e-05
0: TRAIN [4][4010/5173]	Time 0.707 (0.622)	Data 1.20e-04 (2.68e-04)	Tok/s 33248 (22727)	Loss/tok 3.4183 (3.3270)	LR 1.563e-05
0: TRAIN [4][4020/5173]	Time 0.642 (0.622)	Data 1.32e-04 (2.68e-04)	Tok/s 26163 (22729)	Loss/tok 3.3118 (3.3269)	LR 1.563e-05
0: TRAIN [4][4030/5173]	Time 0.641 (0.622)	Data 1.27e-04 (2.67e-04)	Tok/s 26119 (22721)	Loss/tok 3.3589 (3.3268)	LR 1.563e-05
0: TRAIN [4][4040/5173]	Time 0.582 (0.622)	Data 1.23e-04 (2.67e-04)	Tok/s 17739 (22729)	Loss/tok 3.1110 (3.3270)	LR 1.563e-05
0: TRAIN [4][4050/5173]	Time 0.708 (0.622)	Data 1.24e-04 (2.67e-04)	Tok/s 33531 (22722)	Loss/tok 3.5854 (3.3270)	LR 1.563e-05
0: TRAIN [4][4060/5173]	Time 0.648 (0.622)	Data 1.25e-04 (2.66e-04)	Tok/s 25792 (22723)	Loss/tok 3.3081 (3.3270)	LR 1.563e-05
0: TRAIN [4][4070/5173]	Time 0.641 (0.622)	Data 1.22e-04 (2.66e-04)	Tok/s 25996 (22722)	Loss/tok 3.4199 (3.3270)	LR 1.563e-05
0: TRAIN [4][4080/5173]	Time 0.709 (0.622)	Data 1.25e-04 (2.66e-04)	Tok/s 32926 (22727)	Loss/tok 3.5913 (3.3271)	LR 1.563e-05
0: TRAIN [4][4090/5173]	Time 0.519 (0.622)	Data 1.26e-04 (2.65e-04)	Tok/s 10134 (22724)	Loss/tok 2.6738 (3.3270)	LR 1.563e-05
0: TRAIN [4][4100/5173]	Time 0.579 (0.622)	Data 1.26e-04 (2.65e-04)	Tok/s 17629 (22727)	Loss/tok 3.0980 (3.3271)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][4110/5173]	Time 0.647 (0.622)	Data 1.32e-04 (2.65e-04)	Tok/s 26134 (22733)	Loss/tok 3.2746 (3.3274)	LR 1.563e-05
0: TRAIN [4][4120/5173]	Time 0.643 (0.622)	Data 1.29e-04 (2.65e-04)	Tok/s 25848 (22735)	Loss/tok 3.2945 (3.3274)	LR 1.563e-05
0: TRAIN [4][4130/5173]	Time 0.520 (0.622)	Data 1.27e-04 (2.64e-04)	Tok/s 10181 (22723)	Loss/tok 2.7813 (3.3271)	LR 1.563e-05
0: TRAIN [4][4140/5173]	Time 0.578 (0.622)	Data 1.29e-04 (2.64e-04)	Tok/s 18074 (22723)	Loss/tok 3.1721 (3.3271)	LR 1.563e-05
0: TRAIN [4][4150/5173]	Time 0.707 (0.622)	Data 1.29e-04 (2.64e-04)	Tok/s 33203 (22721)	Loss/tok 3.5534 (3.3271)	LR 1.563e-05
0: TRAIN [4][4160/5173]	Time 0.582 (0.622)	Data 1.55e-04 (2.64e-04)	Tok/s 18077 (22718)	Loss/tok 3.0772 (3.3270)	LR 1.563e-05
0: TRAIN [4][4170/5173]	Time 0.522 (0.622)	Data 3.40e-04 (2.63e-04)	Tok/s 10131 (22715)	Loss/tok 2.7480 (3.3271)	LR 1.563e-05
0: TRAIN [4][4180/5173]	Time 0.776 (0.622)	Data 1.22e-04 (2.63e-04)	Tok/s 38482 (22730)	Loss/tok 3.6726 (3.3275)	LR 1.563e-05
0: TRAIN [4][4190/5173]	Time 0.582 (0.622)	Data 1.32e-04 (2.63e-04)	Tok/s 17764 (22723)	Loss/tok 3.1009 (3.3275)	LR 1.563e-05
0: TRAIN [4][4200/5173]	Time 0.580 (0.622)	Data 1.25e-04 (2.62e-04)	Tok/s 17326 (22727)	Loss/tok 3.2102 (3.3276)	LR 1.563e-05
0: TRAIN [4][4210/5173]	Time 0.521 (0.622)	Data 1.24e-04 (2.62e-04)	Tok/s 10041 (22717)	Loss/tok 2.6830 (3.3273)	LR 1.563e-05
0: TRAIN [4][4220/5173]	Time 0.704 (0.622)	Data 2.86e-04 (2.62e-04)	Tok/s 32792 (22713)	Loss/tok 3.5722 (3.3271)	LR 1.563e-05
0: TRAIN [4][4230/5173]	Time 0.643 (0.622)	Data 1.18e-04 (2.62e-04)	Tok/s 26644 (22718)	Loss/tok 3.2934 (3.3272)	LR 1.563e-05
0: TRAIN [4][4240/5173]	Time 0.579 (0.622)	Data 1.27e-04 (2.61e-04)	Tok/s 17595 (22723)	Loss/tok 3.0752 (3.3272)	LR 1.563e-05
0: TRAIN [4][4250/5173]	Time 0.582 (0.622)	Data 1.26e-04 (2.61e-04)	Tok/s 17693 (22721)	Loss/tok 3.0078 (3.3271)	LR 1.563e-05
0: TRAIN [4][4260/5173]	Time 0.584 (0.622)	Data 1.20e-04 (2.61e-04)	Tok/s 17518 (22719)	Loss/tok 3.1795 (3.3270)	LR 1.563e-05
0: TRAIN [4][4270/5173]	Time 0.580 (0.622)	Data 1.26e-04 (2.60e-04)	Tok/s 17800 (22717)	Loss/tok 3.0492 (3.3269)	LR 1.563e-05
0: TRAIN [4][4280/5173]	Time 0.583 (0.622)	Data 1.34e-04 (2.60e-04)	Tok/s 17978 (22723)	Loss/tok 3.1715 (3.3270)	LR 1.563e-05
0: TRAIN [4][4290/5173]	Time 0.642 (0.622)	Data 1.26e-04 (2.60e-04)	Tok/s 26262 (22724)	Loss/tok 3.3919 (3.3269)	LR 1.563e-05
0: TRAIN [4][4300/5173]	Time 0.709 (0.622)	Data 1.21e-04 (2.60e-04)	Tok/s 32898 (22721)	Loss/tok 3.4035 (3.3268)	LR 1.563e-05
0: TRAIN [4][4310/5173]	Time 0.583 (0.622)	Data 1.24e-04 (2.59e-04)	Tok/s 17447 (22724)	Loss/tok 3.2272 (3.3268)	LR 1.563e-05
0: TRAIN [4][4320/5173]	Time 0.642 (0.622)	Data 1.25e-04 (2.59e-04)	Tok/s 26545 (22717)	Loss/tok 3.1856 (3.3265)	LR 1.563e-05
0: TRAIN [4][4330/5173]	Time 0.580 (0.622)	Data 1.20e-04 (2.59e-04)	Tok/s 17794 (22713)	Loss/tok 3.1678 (3.3263)	LR 1.563e-05
0: TRAIN [4][4340/5173]	Time 0.641 (0.622)	Data 1.26e-04 (2.59e-04)	Tok/s 25870 (22714)	Loss/tok 3.3757 (3.3263)	LR 1.563e-05
0: TRAIN [4][4350/5173]	Time 0.647 (0.622)	Data 1.23e-04 (2.59e-04)	Tok/s 25888 (22714)	Loss/tok 3.4533 (3.3263)	LR 1.563e-05
0: TRAIN [4][4360/5173]	Time 0.584 (0.622)	Data 1.23e-04 (2.58e-04)	Tok/s 18042 (22714)	Loss/tok 3.1759 (3.3263)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][4370/5173]	Time 0.640 (0.622)	Data 1.33e-04 (2.58e-04)	Tok/s 26880 (22723)	Loss/tok 3.2845 (3.3266)	LR 1.563e-05
0: TRAIN [4][4380/5173]	Time 0.578 (0.622)	Data 1.27e-04 (2.58e-04)	Tok/s 17797 (22720)	Loss/tok 3.1251 (3.3264)	LR 1.563e-05
0: TRAIN [4][4390/5173]	Time 0.705 (0.622)	Data 1.25e-04 (2.58e-04)	Tok/s 33147 (22716)	Loss/tok 3.4710 (3.3263)	LR 1.563e-05
0: TRAIN [4][4400/5173]	Time 0.581 (0.622)	Data 1.25e-04 (2.57e-04)	Tok/s 17821 (22716)	Loss/tok 3.1669 (3.3264)	LR 1.563e-05
0: TRAIN [4][4410/5173]	Time 0.645 (0.622)	Data 1.35e-04 (2.57e-04)	Tok/s 25708 (22714)	Loss/tok 3.4040 (3.3263)	LR 1.563e-05
0: TRAIN [4][4420/5173]	Time 0.646 (0.622)	Data 1.55e-04 (2.57e-04)	Tok/s 26541 (22721)	Loss/tok 3.3334 (3.3265)	LR 1.563e-05
0: TRAIN [4][4430/5173]	Time 0.644 (0.622)	Data 1.32e-04 (2.57e-04)	Tok/s 25890 (22731)	Loss/tok 3.2859 (3.3269)	LR 1.563e-05
0: TRAIN [4][4440/5173]	Time 0.705 (0.622)	Data 3.20e-04 (2.56e-04)	Tok/s 32992 (22731)	Loss/tok 3.5008 (3.3268)	LR 1.563e-05
0: TRAIN [4][4450/5173]	Time 0.704 (0.622)	Data 1.27e-04 (2.57e-04)	Tok/s 32888 (22737)	Loss/tok 3.4976 (3.3271)	LR 1.563e-05
0: TRAIN [4][4460/5173]	Time 0.580 (0.622)	Data 1.22e-04 (2.56e-04)	Tok/s 17879 (22732)	Loss/tok 3.1141 (3.3269)	LR 1.563e-05
0: TRAIN [4][4470/5173]	Time 0.582 (0.622)	Data 3.15e-04 (2.56e-04)	Tok/s 17867 (22727)	Loss/tok 3.0836 (3.3268)	LR 1.563e-05
0: TRAIN [4][4480/5173]	Time 0.580 (0.622)	Data 1.18e-04 (2.56e-04)	Tok/s 17434 (22730)	Loss/tok 3.1589 (3.3268)	LR 1.563e-05
0: TRAIN [4][4490/5173]	Time 0.580 (0.622)	Data 1.22e-04 (2.56e-04)	Tok/s 17768 (22728)	Loss/tok 3.1981 (3.3269)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][4500/5173]	Time 0.582 (0.622)	Data 1.29e-04 (2.56e-04)	Tok/s 17515 (22730)	Loss/tok 3.0546 (3.3272)	LR 1.563e-05
0: TRAIN [4][4510/5173]	Time 0.646 (0.622)	Data 1.20e-04 (2.55e-04)	Tok/s 25712 (22735)	Loss/tok 3.4218 (3.3272)	LR 1.563e-05
0: TRAIN [4][4520/5173]	Time 0.708 (0.622)	Data 1.24e-04 (2.55e-04)	Tok/s 32667 (22734)	Loss/tok 3.6248 (3.3271)	LR 1.563e-05
0: TRAIN [4][4530/5173]	Time 0.583 (0.622)	Data 1.80e-04 (2.55e-04)	Tok/s 17551 (22735)	Loss/tok 3.0864 (3.3272)	LR 1.563e-05
0: TRAIN [4][4540/5173]	Time 0.519 (0.622)	Data 1.22e-04 (2.54e-04)	Tok/s 10070 (22733)	Loss/tok 2.6795 (3.3272)	LR 1.563e-05
0: TRAIN [4][4550/5173]	Time 0.643 (0.622)	Data 1.22e-04 (2.54e-04)	Tok/s 26589 (22724)	Loss/tok 3.3513 (3.3269)	LR 1.563e-05
0: TRAIN [4][4560/5173]	Time 0.581 (0.622)	Data 1.26e-04 (2.54e-04)	Tok/s 17468 (22725)	Loss/tok 3.2296 (3.3269)	LR 1.563e-05
0: TRAIN [4][4570/5173]	Time 0.707 (0.622)	Data 1.26e-04 (2.54e-04)	Tok/s 32918 (22729)	Loss/tok 3.4309 (3.3270)	LR 1.563e-05
0: TRAIN [4][4580/5173]	Time 0.521 (0.622)	Data 3.00e-04 (2.54e-04)	Tok/s 10040 (22728)	Loss/tok 2.5459 (3.3268)	LR 1.563e-05
0: TRAIN [4][4590/5173]	Time 0.584 (0.622)	Data 1.22e-04 (2.53e-04)	Tok/s 17463 (22717)	Loss/tok 3.1154 (3.3265)	LR 1.563e-05
0: TRAIN [4][4600/5173]	Time 0.779 (0.622)	Data 1.32e-04 (2.53e-04)	Tok/s 38812 (22716)	Loss/tok 3.6253 (3.3265)	LR 1.563e-05
0: TRAIN [4][4610/5173]	Time 0.583 (0.622)	Data 1.19e-04 (2.53e-04)	Tok/s 17632 (22711)	Loss/tok 3.2210 (3.3265)	LR 1.563e-05
0: TRAIN [4][4620/5173]	Time 0.779 (0.622)	Data 2.97e-04 (2.53e-04)	Tok/s 38539 (22716)	Loss/tok 3.6331 (3.3267)	LR 1.563e-05
0: TRAIN [4][4630/5173]	Time 0.642 (0.622)	Data 1.25e-04 (2.52e-04)	Tok/s 26272 (22713)	Loss/tok 3.3442 (3.3267)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][4640/5173]	Time 0.639 (0.622)	Data 1.27e-04 (2.52e-04)	Tok/s 26114 (22713)	Loss/tok 3.3981 (3.3268)	LR 1.563e-05
0: TRAIN [4][4650/5173]	Time 0.580 (0.622)	Data 1.33e-04 (2.52e-04)	Tok/s 18203 (22714)	Loss/tok 3.0030 (3.3271)	LR 1.563e-05
0: TRAIN [4][4660/5173]	Time 0.646 (0.622)	Data 1.27e-04 (2.52e-04)	Tok/s 25997 (22714)	Loss/tok 3.3474 (3.3270)	LR 1.563e-05
0: TRAIN [4][4670/5173]	Time 0.582 (0.622)	Data 1.24e-04 (2.51e-04)	Tok/s 17738 (22720)	Loss/tok 3.1374 (3.3271)	LR 1.563e-05
0: TRAIN [4][4680/5173]	Time 0.521 (0.622)	Data 3.06e-04 (2.51e-04)	Tok/s 10076 (22717)	Loss/tok 2.6019 (3.3271)	LR 1.563e-05
0: TRAIN [4][4690/5173]	Time 0.584 (0.622)	Data 1.22e-04 (2.51e-04)	Tok/s 17686 (22717)	Loss/tok 3.1867 (3.3270)	LR 1.563e-05
0: TRAIN [4][4700/5173]	Time 0.775 (0.622)	Data 1.24e-04 (2.51e-04)	Tok/s 38789 (22718)	Loss/tok 3.6503 (3.3271)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][4710/5173]	Time 0.702 (0.622)	Data 1.24e-04 (2.51e-04)	Tok/s 33513 (22718)	Loss/tok 3.4806 (3.3271)	LR 1.563e-05
0: TRAIN [4][4720/5173]	Time 0.649 (0.622)	Data 1.25e-04 (2.50e-04)	Tok/s 25891 (22718)	Loss/tok 3.2916 (3.3271)	LR 1.563e-05
0: TRAIN [4][4730/5173]	Time 0.523 (0.622)	Data 1.23e-04 (2.50e-04)	Tok/s 10520 (22706)	Loss/tok 2.7174 (3.3268)	LR 1.563e-05
0: TRAIN [4][4740/5173]	Time 0.640 (0.622)	Data 1.29e-04 (2.50e-04)	Tok/s 26707 (22704)	Loss/tok 3.3276 (3.3267)	LR 1.563e-05
0: TRAIN [4][4750/5173]	Time 0.583 (0.622)	Data 3.06e-04 (2.50e-04)	Tok/s 17688 (22706)	Loss/tok 3.1793 (3.3269)	LR 1.563e-05
0: TRAIN [4][4760/5173]	Time 0.580 (0.622)	Data 1.28e-04 (2.50e-04)	Tok/s 17815 (22705)	Loss/tok 3.1199 (3.3269)	LR 1.563e-05
0: TRAIN [4][4770/5173]	Time 0.708 (0.622)	Data 1.24e-04 (2.49e-04)	Tok/s 33036 (22710)	Loss/tok 3.3869 (3.3269)	LR 1.563e-05
0: TRAIN [4][4780/5173]	Time 0.641 (0.622)	Data 1.26e-04 (2.49e-04)	Tok/s 26387 (22706)	Loss/tok 3.2155 (3.3267)	LR 1.563e-05
0: TRAIN [4][4790/5173]	Time 0.583 (0.622)	Data 1.26e-04 (2.49e-04)	Tok/s 17673 (22708)	Loss/tok 3.1164 (3.3268)	LR 1.563e-05
0: TRAIN [4][4800/5173]	Time 0.644 (0.622)	Data 1.37e-04 (2.49e-04)	Tok/s 26294 (22718)	Loss/tok 3.3414 (3.3270)	LR 1.563e-05
0: TRAIN [4][4810/5173]	Time 0.645 (0.622)	Data 1.29e-04 (2.49e-04)	Tok/s 25653 (22714)	Loss/tok 3.3522 (3.3268)	LR 1.563e-05
0: TRAIN [4][4820/5173]	Time 0.649 (0.622)	Data 1.30e-04 (2.48e-04)	Tok/s 25893 (22719)	Loss/tok 3.2792 (3.3271)	LR 1.563e-05
0: TRAIN [4][4830/5173]	Time 0.644 (0.622)	Data 1.24e-04 (2.48e-04)	Tok/s 26355 (22721)	Loss/tok 3.2019 (3.3271)	LR 1.563e-05
0: TRAIN [4][4840/5173]	Time 0.645 (0.622)	Data 1.27e-04 (2.48e-04)	Tok/s 25325 (22719)	Loss/tok 3.3484 (3.3272)	LR 1.563e-05
0: TRAIN [4][4850/5173]	Time 0.584 (0.622)	Data 1.28e-04 (2.48e-04)	Tok/s 17485 (22715)	Loss/tok 3.1089 (3.3270)	LR 1.563e-05
0: TRAIN [4][4860/5173]	Time 0.583 (0.622)	Data 2.72e-04 (2.48e-04)	Tok/s 17947 (22714)	Loss/tok 2.9278 (3.3270)	LR 1.563e-05
0: TRAIN [4][4870/5173]	Time 0.641 (0.622)	Data 1.24e-04 (2.47e-04)	Tok/s 26681 (22708)	Loss/tok 3.2423 (3.3268)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][4880/5173]	Time 0.640 (0.622)	Data 1.25e-04 (2.47e-04)	Tok/s 26025 (22712)	Loss/tok 3.4346 (3.3268)	LR 1.563e-05
0: TRAIN [4][4890/5173]	Time 0.520 (0.622)	Data 1.23e-04 (2.47e-04)	Tok/s 10241 (22705)	Loss/tok 2.5805 (3.3267)	LR 1.563e-05
0: TRAIN [4][4900/5173]	Time 0.648 (0.622)	Data 1.19e-04 (2.47e-04)	Tok/s 26046 (22700)	Loss/tok 3.3477 (3.3266)	LR 1.563e-05
0: TRAIN [4][4910/5173]	Time 0.775 (0.622)	Data 1.31e-04 (2.47e-04)	Tok/s 38400 (22703)	Loss/tok 3.6942 (3.3268)	LR 1.563e-05
0: TRAIN [4][4920/5173]	Time 0.641 (0.622)	Data 1.38e-04 (2.46e-04)	Tok/s 25893 (22706)	Loss/tok 3.4223 (3.3269)	LR 1.563e-05
0: TRAIN [4][4930/5173]	Time 0.583 (0.622)	Data 1.23e-04 (2.46e-04)	Tok/s 18005 (22710)	Loss/tok 3.1358 (3.3270)	LR 1.563e-05
0: TRAIN [4][4940/5173]	Time 0.647 (0.622)	Data 1.41e-04 (2.46e-04)	Tok/s 26024 (22715)	Loss/tok 3.3281 (3.3271)	LR 1.563e-05
0: TRAIN [4][4950/5173]	Time 0.644 (0.622)	Data 1.26e-04 (2.46e-04)	Tok/s 26151 (22718)	Loss/tok 3.4053 (3.3271)	LR 1.563e-05
0: TRAIN [4][4960/5173]	Time 0.645 (0.622)	Data 2.95e-04 (2.46e-04)	Tok/s 26321 (22725)	Loss/tok 3.3161 (3.3273)	LR 1.563e-05
0: TRAIN [4][4970/5173]	Time 0.581 (0.622)	Data 1.24e-04 (2.45e-04)	Tok/s 18005 (22718)	Loss/tok 3.1622 (3.3272)	LR 1.563e-05
0: TRAIN [4][4980/5173]	Time 0.583 (0.622)	Data 1.27e-04 (2.45e-04)	Tok/s 17804 (22722)	Loss/tok 3.0666 (3.3275)	LR 1.563e-05
0: TRAIN [4][4990/5173]	Time 0.646 (0.622)	Data 3.18e-04 (2.45e-04)	Tok/s 26186 (22715)	Loss/tok 3.2272 (3.3272)	LR 1.563e-05
0: TRAIN [4][5000/5173]	Time 0.707 (0.622)	Data 1.24e-04 (2.45e-04)	Tok/s 32451 (22721)	Loss/tok 3.5202 (3.3273)	LR 1.563e-05
0: TRAIN [4][5010/5173]	Time 0.583 (0.622)	Data 3.00e-04 (2.45e-04)	Tok/s 17948 (22715)	Loss/tok 3.0851 (3.3270)	LR 1.563e-05
0: TRAIN [4][5020/5173]	Time 0.641 (0.622)	Data 2.77e-04 (2.44e-04)	Tok/s 26451 (22716)	Loss/tok 3.3162 (3.3272)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][5030/5173]	Time 0.582 (0.622)	Data 1.24e-04 (2.44e-04)	Tok/s 17881 (22716)	Loss/tok 3.1854 (3.3272)	LR 1.563e-05
0: TRAIN [4][5040/5173]	Time 0.582 (0.622)	Data 1.27e-04 (2.44e-04)	Tok/s 17474 (22711)	Loss/tok 3.1935 (3.3270)	LR 1.563e-05
0: TRAIN [4][5050/5173]	Time 0.581 (0.622)	Data 1.25e-04 (2.44e-04)	Tok/s 18103 (22712)	Loss/tok 3.1041 (3.3270)	LR 1.563e-05
0: TRAIN [4][5060/5173]	Time 0.520 (0.622)	Data 1.19e-04 (2.44e-04)	Tok/s 10421 (22714)	Loss/tok 2.7196 (3.3270)	LR 1.563e-05
0: TRAIN [4][5070/5173]	Time 0.521 (0.622)	Data 1.26e-04 (2.43e-04)	Tok/s 10105 (22713)	Loss/tok 2.7334 (3.3269)	LR 1.563e-05
0: TRAIN [4][5080/5173]	Time 0.708 (0.622)	Data 1.28e-04 (2.43e-04)	Tok/s 32934 (22721)	Loss/tok 3.3842 (3.3270)	LR 1.563e-05
0: TRAIN [4][5090/5173]	Time 0.582 (0.622)	Data 1.19e-04 (2.43e-04)	Tok/s 17738 (22719)	Loss/tok 3.0453 (3.3271)	LR 1.563e-05
0: TRAIN [4][5100/5173]	Time 0.702 (0.622)	Data 1.24e-04 (2.43e-04)	Tok/s 33248 (22719)	Loss/tok 3.4999 (3.3271)	LR 1.563e-05
0: TRAIN [4][5110/5173]	Time 0.521 (0.622)	Data 1.25e-04 (2.43e-04)	Tok/s 10431 (22714)	Loss/tok 2.6993 (3.3269)	LR 1.563e-05
0: TRAIN [4][5120/5173]	Time 0.647 (0.622)	Data 1.35e-04 (2.42e-04)	Tok/s 26071 (22706)	Loss/tok 3.2665 (3.3268)	LR 1.563e-05
0: TRAIN [4][5130/5173]	Time 0.708 (0.622)	Data 1.30e-04 (2.42e-04)	Tok/s 32727 (22710)	Loss/tok 3.5388 (3.3269)	LR 1.563e-05
0: TRAIN [4][5140/5173]	Time 0.709 (0.622)	Data 1.27e-04 (2.42e-04)	Tok/s 32530 (22710)	Loss/tok 3.5775 (3.3269)	LR 1.563e-05
0: TRAIN [4][5150/5173]	Time 0.519 (0.622)	Data 1.39e-04 (2.42e-04)	Tok/s 10166 (22716)	Loss/tok 2.6249 (3.3272)	LR 1.563e-05
0: TRAIN [4][5160/5173]	Time 0.580 (0.622)	Data 1.37e-04 (2.42e-04)	Tok/s 17563 (22721)	Loss/tok 3.1160 (3.3275)	LR 1.563e-05
0: TRAIN [4][5170/5173]	Time 0.582 (0.622)	Data 1.46e-04 (2.41e-04)	Tok/s 17782 (22720)	Loss/tok 3.1587 (3.3275)	LR 1.563e-05
:::MLL 1586134000.995 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 525}}
:::MLL 1586134000.995 eval_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [4][0/8]	Time 0.709 (0.709)	Decoder iters 134.0 (134.0)	Tok/s 23346 (23346)
0: Running moses detokenizer
0: BLEU(score=21.365925900667765, counts=[35344, 16716, 9135, 5182], totals=[65122, 62119, 59116, 56119], precisions=[54.27351739811431, 26.909641172588096, 15.452669328100683, 9.23394928633796], bp=1.0, sys_len=65122, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1586134006.768 eval_accuracy: {"value": 21.37, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 536}}
:::MLL 1586134006.768 eval_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 4	Training Loss: 3.3270	Test BLEU: 21.37
0: Performance: Epoch: 4	Training: 68174 Tok/s
0: Finished epoch 4
:::MLL 1586134006.769 block_stop: {"value": null, "metadata": {"first_epoch_num": 5, "file": "train.py", "lineno": 558}}
0: Closing preprocessed data file
:::MLL 1586134006.769 run_stop: {"value": null, "metadata": {"status": "aborted", "file": "train.py", "lineno": 569}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-04-06 12:46:50 AM
RESULT,RNN_TRANSLATOR,,16151,nvidia,2020-04-05 08:17:39 PM
