Beginning trial 1 of 5
Gathering sys log on node001
:::MLL 1585765331.551 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1585765331.552 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1585765331.553 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1585765331.554 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1585765331.555 submission_platform: {"value": "1xPowerEdge R7525", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1585765331.556 submission_entry: {"value": "{'hardware': 'PowerEdge R7525', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': ' ', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.0-0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x AMD EPYC 7502 32-Core Processor', 'num_cores': '64', 'num_vcpus': '64', 'accelerator': 'Tesla V100-PCIE-32GB', 'num_accelerators': '3', 'sys_mem_size': '251 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '1x 931.5G', 'cpu_accel_interconnect': 'QPI', 'network_card': '', 'num_network_cards': '0', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1585765331.556 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1585765331.557 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1585765332.991 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node node001
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=4812' -e LR=4.0e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=4300 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=200401132104230766830 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_200401132104230766830 ./run_and_time.sh
Run vars: id 200401132104230766830 gpus 3 mparams  --master_port=4812
NCCL_SOCKET_NTHREADS=2
NCCL_NSOCKS_PERTHREAD=8
STARTING TIMING RUN AT 2020-04-01 06:22:13 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4300
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=0
+ MATH=amp_fp16
+ [[ 0 -eq 1 ]]
+ LAUNCH_OPT='torch.distributed.launch --nproc_per_node 3  --master_port=4812'
+ echo 'running benchmark'
running benchmark
+ python -m torch.distributed.launch --nproc_per_node 3 --master_port=4812 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 4300 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1585765336.174 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1585765336.217 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1585765336.218 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=8, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.004, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=4300, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 1841566426
node001:588:588 [0] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.1<0>
node001:588:588 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node001:588:588 [0] NCCL INFO NET/IB : No device found.
node001:588:588 [0] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.1<0>
node001:588:588 [0] NCCL INFO Using network Socket
NCCL version 2.6.4+cuda10.2
node001:589:589 [1] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.1<0>
node001:589:589 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node001:589:589 [1] NCCL INFO NET/IB : No device found.
node001:589:589 [1] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.1<0>
node001:589:589 [1] NCCL INFO Using network Socket
node001:590:590 [2] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.1<0>
node001:590:590 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node001:590:590 [2] NCCL INFO NET/IB : No device found.
node001:590:590 [2] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.1<0>
node001:590:590 [2] NCCL INFO Using network Socket
node001:589:798 [1] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 8/8/64
node001:588:797 [0] NCCL INFO Channel 00/02 :    0   1   2
node001:589:798 [1] NCCL INFO Trees [0] 2/-1/-1->1->0|0->1->2/-1/-1 [1] 2/-1/-1->1->0|0->1->2/-1/-1
node001:588:797 [0] NCCL INFO Channel 01/02 :    0   1   2
node001:590:799 [2] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 8/8/64
node001:589:798 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000
node001:590:799 [2] NCCL INFO Trees [0] -1/-1/-1->2->1|1->2->-1/-1/-1 [1] -1/-1/-1->2->1|1->2->-1/-1/-1
node001:590:799 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000
node001:588:797 [0] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 8/8/64
node001:588:797 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 1/-1/-1->0->-1|-1->0->1/-1/-1
node001:588:797 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff
node001:590:799 [2] NCCL INFO Ring 00 : 2[e2000] -> 0[21000] via direct shared memory
node001:588:797 [0] NCCL INFO Ring 00 : 0[21000] -> 1[81000] via direct shared memory
node001:589:798 [1] NCCL INFO Ring 00 : 1[81000] -> 2[e2000] via P2P/IPC
node001:590:799 [2] NCCL INFO Ring 00 : 2[e2000] -> 1[81000] via P2P/IPC
node001:589:798 [1] NCCL INFO Ring 00 : 1[81000] -> 0[21000] via direct shared memory
node001:590:799 [2] NCCL INFO Ring 01 : 2[e2000] -> 0[21000] via direct shared memory
node001:588:797 [0] NCCL INFO Ring 01 : 0[21000] -> 1[81000] via direct shared memory
node001:589:798 [1] NCCL INFO Ring 01 : 1[81000] -> 2[e2000] via P2P/IPC
node001:590:799 [2] NCCL INFO Ring 01 : 2[e2000] -> 1[81000] via P2P/IPC
node001:589:798 [1] NCCL INFO Ring 01 : 1[81000] -> 0[21000] via direct shared memory
node001:590:799 [2] NCCL INFO comm 0x7ffee8006620 rank 2 nranks 3 cudaDev 2 busId e2000 - Init COMPLETE
node001:588:797 [0] NCCL INFO comm 0x7ffe18006620 rank 0 nranks 3 cudaDev 0 busId 21000 - Init COMPLETE
node001:588:588 [0] NCCL INFO Launch mode Parallel
node001:589:798 [1] NCCL INFO comm 0x7ffe74006620 rank 1 nranks 3 cudaDev 1 busId 81000 - Init COMPLETE
0: Worker 0 is using worker seed: 688974375
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.004}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.004
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1585765345.410 opt_base_learning_rate: {"value": 0.004, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1585765346.917 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 407}}
:::MLL 1585765346.918 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 409}}
:::MLL 1585765346.918 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 414}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1585765347.723 global_batch_size: {"value": 768, "metadata": {"file": "train.py", "lineno": 459}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4300, 'decay_interval': 809, 'decay_steps': 8, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4300
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 8
:::MLL 1585765347.725 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1585765347.725 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1585765347.726 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1585765347.726 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1585765347.727 opt_learning_rate_decay_steps: {"value": 8, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1585765347.727 opt_learning_rate_remain_steps: {"value": 4300, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1585765347.728 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1585765347.729 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1585765347.730 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 515}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 1982079505
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/5173]	Time 1.291 (1.291)	Data 6.53e-01 (6.53e-01)	Tok/s 12919 (12919)	Loss/tok 10.7538 (10.7538)	LR 4.000e-05
0: TRAIN [0][10/5173]	Time 0.777 (0.683)	Data 3.67e-04 (5.96e-02)	Tok/s 38315 (23310)	Loss/tok 9.5645 (9.9848)	LR 5.036e-05
0: TRAIN [0][20/5173]	Time 0.567 (0.632)	Data 1.65e-04 (3.13e-02)	Tok/s 18100 (21792)	Loss/tok 9.0185 (9.6602)	LR 6.340e-05
0: TRAIN [0][30/5173]	Time 0.701 (0.618)	Data 1.56e-04 (2.13e-02)	Tok/s 32787 (21656)	Loss/tok 8.9746 (9.4276)	LR 7.981e-05
0: TRAIN [0][40/5173]	Time 0.558 (0.618)	Data 3.30e-04 (1.61e-02)	Tok/s 18403 (22053)	Loss/tok 8.3742 (9.2141)	LR 1.005e-04
0: TRAIN [0][50/5173]	Time 0.635 (0.620)	Data 1.47e-04 (1.30e-02)	Tok/s 26337 (22695)	Loss/tok 8.2369 (9.0250)	LR 1.265e-04
0: TRAIN [0][60/5173]	Time 0.569 (0.615)	Data 1.38e-04 (1.09e-02)	Tok/s 17639 (22487)	Loss/tok 7.9874 (8.9170)	LR 1.592e-04
0: TRAIN [0][70/5173]	Time 0.634 (0.615)	Data 3.19e-04 (9.38e-03)	Tok/s 26469 (22678)	Loss/tok 8.0284 (8.7878)	LR 2.005e-04
0: TRAIN [0][80/5173]	Time 0.569 (0.614)	Data 1.47e-04 (8.24e-03)	Tok/s 18265 (22680)	Loss/tok 7.9690 (8.7190)	LR 2.524e-04
0: TRAIN [0][90/5173]	Time 0.565 (0.611)	Data 1.54e-04 (7.35e-03)	Tok/s 17847 (22474)	Loss/tok 7.8576 (8.6406)	LR 3.177e-04
0: TRAIN [0][100/5173]	Time 0.699 (0.611)	Data 1.47e-04 (6.64e-03)	Tok/s 33662 (22651)	Loss/tok 8.0808 (8.5764)	LR 4.000e-04
0: TRAIN [0][110/5173]	Time 0.635 (0.608)	Data 1.46e-04 (6.05e-03)	Tok/s 26842 (22417)	Loss/tok 7.8617 (8.5137)	LR 5.036e-04
0: TRAIN [0][120/5173]	Time 0.634 (0.609)	Data 1.43e-04 (5.57e-03)	Tok/s 26078 (22581)	Loss/tok 8.1190 (8.4866)	LR 6.340e-04
0: TRAIN [0][130/5173]	Time 0.629 (0.608)	Data 1.63e-04 (5.15e-03)	Tok/s 26692 (22500)	Loss/tok 7.8238 (8.4394)	LR 7.981e-04
0: TRAIN [0][140/5173]	Time 0.695 (0.608)	Data 1.57e-04 (4.80e-03)	Tok/s 33931 (22605)	Loss/tok 7.9110 (8.3876)	LR 1.005e-03
0: TRAIN [0][150/5173]	Time 0.634 (0.611)	Data 1.50e-04 (4.49e-03)	Tok/s 26834 (22905)	Loss/tok 7.6050 (8.3352)	LR 1.265e-03
0: TRAIN [0][160/5173]	Time 0.690 (0.610)	Data 1.52e-04 (4.23e-03)	Tok/s 34059 (22898)	Loss/tok 7.5571 (8.2849)	LR 1.592e-03
0: TRAIN [0][170/5173]	Time 0.500 (0.611)	Data 1.54e-04 (3.99e-03)	Tok/s 10499 (22983)	Loss/tok 6.4810 (8.2306)	LR 2.005e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][180/5173]	Time 0.702 (0.610)	Data 1.59e-04 (3.78e-03)	Tok/s 33155 (22973)	Loss/tok 7.4268 (8.1850)	LR 2.410e-03
0: TRAIN [0][190/5173]	Time 0.644 (0.612)	Data 1.48e-04 (3.59e-03)	Tok/s 26068 (23193)	Loss/tok 7.2282 (8.1269)	LR 3.034e-03
0: TRAIN [0][200/5173]	Time 0.562 (0.612)	Data 1.37e-04 (3.42e-03)	Tok/s 18239 (23140)	Loss/tok 6.9276 (8.0797)	LR 3.820e-03
0: TRAIN [0][210/5173]	Time 0.501 (0.612)	Data 1.46e-04 (3.26e-03)	Tok/s 10558 (23148)	Loss/tok 6.2173 (8.0304)	LR 4.000e-03
0: TRAIN [0][220/5173]	Time 0.636 (0.613)	Data 1.37e-04 (3.13e-03)	Tok/s 25972 (23304)	Loss/tok 6.8496 (7.9735)	LR 4.000e-03
0: TRAIN [0][230/5173]	Time 0.568 (0.613)	Data 1.37e-04 (3.00e-03)	Tok/s 18120 (23313)	Loss/tok 6.4368 (7.9207)	LR 4.000e-03
0: TRAIN [0][240/5173]	Time 0.698 (0.612)	Data 1.37e-04 (2.88e-03)	Tok/s 33214 (23216)	Loss/tok 6.7496 (7.8714)	LR 4.000e-03
0: TRAIN [0][250/5173]	Time 0.504 (0.611)	Data 1.44e-04 (2.77e-03)	Tok/s 10525 (23080)	Loss/tok 5.5839 (7.8248)	LR 4.000e-03
0: TRAIN [0][260/5173]	Time 0.634 (0.611)	Data 1.47e-04 (2.67e-03)	Tok/s 26058 (23102)	Loss/tok 6.4737 (7.7705)	LR 4.000e-03
0: TRAIN [0][270/5173]	Time 0.696 (0.611)	Data 1.43e-04 (2.58e-03)	Tok/s 33064 (23157)	Loss/tok 6.4627 (7.7134)	LR 4.000e-03
0: TRAIN [0][280/5173]	Time 0.568 (0.611)	Data 1.53e-04 (2.49e-03)	Tok/s 18057 (23066)	Loss/tok 5.7575 (7.6653)	LR 4.000e-03
0: TRAIN [0][290/5173]	Time 0.635 (0.612)	Data 1.40e-04 (2.41e-03)	Tok/s 26843 (23184)	Loss/tok 6.0456 (7.6045)	LR 4.000e-03
0: TRAIN [0][300/5173]	Time 0.700 (0.613)	Data 1.42e-04 (2.33e-03)	Tok/s 33933 (23294)	Loss/tok 6.2129 (7.5449)	LR 4.000e-03
0: TRAIN [0][310/5173]	Time 0.705 (0.613)	Data 1.34e-04 (2.26e-03)	Tok/s 33355 (23281)	Loss/tok 6.1147 (7.4949)	LR 4.000e-03
0: TRAIN [0][320/5173]	Time 0.498 (0.612)	Data 1.53e-04 (2.20e-03)	Tok/s 10663 (23256)	Loss/tok 4.7691 (7.4458)	LR 4.000e-03
0: TRAIN [0][330/5173]	Time 0.568 (0.611)	Data 1.55e-04 (2.14e-03)	Tok/s 18002 (23123)	Loss/tok 5.4179 (7.4043)	LR 4.000e-03
0: TRAIN [0][340/5173]	Time 0.703 (0.611)	Data 1.60e-04 (2.08e-03)	Tok/s 32646 (23086)	Loss/tok 6.0183 (7.3570)	LR 4.000e-03
0: TRAIN [0][350/5173]	Time 0.631 (0.610)	Data 1.37e-04 (2.02e-03)	Tok/s 27063 (22953)	Loss/tok 5.6818 (7.3164)	LR 4.000e-03
0: TRAIN [0][360/5173]	Time 0.643 (0.611)	Data 1.39e-04 (1.97e-03)	Tok/s 26477 (23075)	Loss/tok 5.6421 (7.2606)	LR 4.000e-03
0: TRAIN [0][370/5173]	Time 0.570 (0.610)	Data 1.35e-04 (1.92e-03)	Tok/s 17931 (23032)	Loss/tok 5.3240 (7.2167)	LR 4.000e-03
0: TRAIN [0][380/5173]	Time 0.564 (0.610)	Data 1.28e-04 (1.87e-03)	Tok/s 18127 (23049)	Loss/tok 5.0651 (7.1682)	LR 4.000e-03
0: TRAIN [0][390/5173]	Time 0.566 (0.610)	Data 1.29e-04 (1.83e-03)	Tok/s 18054 (23066)	Loss/tok 5.0252 (7.1211)	LR 4.000e-03
0: TRAIN [0][400/5173]	Time 0.502 (0.610)	Data 1.30e-04 (1.79e-03)	Tok/s 10538 (23047)	Loss/tok 4.2912 (7.0765)	LR 4.000e-03
0: TRAIN [0][410/5173]	Time 0.559 (0.610)	Data 1.42e-04 (1.75e-03)	Tok/s 17815 (23067)	Loss/tok 4.8641 (7.0313)	LR 4.000e-03
0: TRAIN [0][420/5173]	Time 0.643 (0.611)	Data 1.25e-04 (1.71e-03)	Tok/s 26024 (23096)	Loss/tok 5.2213 (6.9857)	LR 4.000e-03
0: TRAIN [0][430/5173]	Time 0.622 (0.610)	Data 1.42e-04 (1.67e-03)	Tok/s 27000 (23066)	Loss/tok 5.0705 (6.9450)	LR 4.000e-03
0: TRAIN [0][440/5173]	Time 0.567 (0.610)	Data 3.01e-04 (1.64e-03)	Tok/s 18380 (23062)	Loss/tok 4.7366 (6.9018)	LR 4.000e-03
0: TRAIN [0][450/5173]	Time 0.501 (0.610)	Data 1.28e-04 (1.60e-03)	Tok/s 10541 (22987)	Loss/tok 4.0556 (6.8647)	LR 4.000e-03
0: TRAIN [0][460/5173]	Time 0.702 (0.610)	Data 1.38e-04 (1.57e-03)	Tok/s 33245 (23020)	Loss/tok 5.2820 (6.8209)	LR 4.000e-03
0: TRAIN [0][470/5173]	Time 0.704 (0.610)	Data 1.33e-04 (1.54e-03)	Tok/s 33328 (23055)	Loss/tok 5.1485 (6.7781)	LR 4.000e-03
0: TRAIN [0][480/5173]	Time 0.767 (0.610)	Data 1.29e-04 (1.51e-03)	Tok/s 38705 (23032)	Loss/tok 5.4098 (6.7412)	LR 4.000e-03
0: TRAIN [0][490/5173]	Time 0.568 (0.610)	Data 1.43e-04 (1.49e-03)	Tok/s 18456 (23026)	Loss/tok 4.7155 (6.7027)	LR 4.000e-03
0: TRAIN [0][500/5173]	Time 0.560 (0.610)	Data 1.33e-04 (1.46e-03)	Tok/s 18551 (23046)	Loss/tok 4.5843 (6.6650)	LR 4.000e-03
0: TRAIN [0][510/5173]	Time 0.571 (0.610)	Data 1.27e-04 (1.43e-03)	Tok/s 18238 (23032)	Loss/tok 4.5179 (6.6288)	LR 4.000e-03
0: TRAIN [0][520/5173]	Time 0.500 (0.610)	Data 1.32e-04 (1.41e-03)	Tok/s 10471 (23069)	Loss/tok 3.6568 (6.5908)	LR 4.000e-03
0: TRAIN [0][530/5173]	Time 0.561 (0.610)	Data 3.27e-04 (1.38e-03)	Tok/s 18245 (23081)	Loss/tok 4.4170 (6.5534)	LR 4.000e-03
0: TRAIN [0][540/5173]	Time 0.637 (0.610)	Data 1.28e-04 (1.36e-03)	Tok/s 26362 (23102)	Loss/tok 4.5648 (6.5178)	LR 4.000e-03
0: TRAIN [0][550/5173]	Time 0.703 (0.611)	Data 1.34e-04 (1.34e-03)	Tok/s 33225 (23122)	Loss/tok 4.9130 (6.4840)	LR 4.000e-03
0: TRAIN [0][560/5173]	Time 0.499 (0.610)	Data 1.30e-04 (1.32e-03)	Tok/s 10625 (23079)	Loss/tok 3.5963 (6.4536)	LR 4.000e-03
0: TRAIN [0][570/5173]	Time 0.567 (0.610)	Data 1.23e-04 (1.30e-03)	Tok/s 18483 (23112)	Loss/tok 4.3481 (6.4191)	LR 4.000e-03
0: TRAIN [0][580/5173]	Time 0.570 (0.610)	Data 1.30e-04 (1.28e-03)	Tok/s 18130 (23086)	Loss/tok 4.3313 (6.3901)	LR 4.000e-03
0: TRAIN [0][590/5173]	Time 0.627 (0.610)	Data 1.47e-04 (1.26e-03)	Tok/s 26671 (23045)	Loss/tok 4.6221 (6.3623)	LR 4.000e-03
0: TRAIN [0][600/5173]	Time 0.704 (0.610)	Data 3.05e-04 (1.24e-03)	Tok/s 33011 (23053)	Loss/tok 4.8145 (6.3320)	LR 4.000e-03
0: TRAIN [0][610/5173]	Time 0.639 (0.610)	Data 1.32e-04 (1.22e-03)	Tok/s 25878 (23082)	Loss/tok 4.5966 (6.3006)	LR 4.000e-03
0: TRAIN [0][620/5173]	Time 0.641 (0.610)	Data 1.29e-04 (1.20e-03)	Tok/s 26308 (23111)	Loss/tok 4.5484 (6.2707)	LR 4.000e-03
0: TRAIN [0][630/5173]	Time 0.691 (0.611)	Data 1.31e-04 (1.19e-03)	Tok/s 33185 (23151)	Loss/tok 4.6763 (6.2403)	LR 4.000e-03
0: TRAIN [0][640/5173]	Time 0.565 (0.611)	Data 1.25e-04 (1.17e-03)	Tok/s 18385 (23184)	Loss/tok 4.1984 (6.2101)	LR 4.000e-03
0: TRAIN [0][650/5173]	Time 0.572 (0.610)	Data 1.32e-04 (1.15e-03)	Tok/s 17470 (23121)	Loss/tok 4.1688 (6.1873)	LR 4.000e-03
0: TRAIN [0][660/5173]	Time 0.701 (0.611)	Data 1.39e-04 (1.14e-03)	Tok/s 33207 (23142)	Loss/tok 4.7175 (6.1601)	LR 4.000e-03
0: TRAIN [0][670/5173]	Time 0.774 (0.611)	Data 1.28e-04 (1.12e-03)	Tok/s 38448 (23153)	Loss/tok 4.9718 (6.1345)	LR 4.000e-03
0: TRAIN [0][680/5173]	Time 0.562 (0.611)	Data 1.25e-04 (1.11e-03)	Tok/s 18304 (23128)	Loss/tok 4.2749 (6.1122)	LR 4.000e-03
0: TRAIN [0][690/5173]	Time 0.499 (0.611)	Data 1.34e-04 (1.10e-03)	Tok/s 10501 (23147)	Loss/tok 3.5031 (6.0857)	LR 4.000e-03
0: TRAIN [0][700/5173]	Time 0.568 (0.610)	Data 1.38e-04 (1.08e-03)	Tok/s 18169 (23089)	Loss/tok 4.2482 (6.0654)	LR 4.000e-03
0: TRAIN [0][710/5173]	Time 0.641 (0.610)	Data 1.29e-04 (1.07e-03)	Tok/s 26433 (23085)	Loss/tok 4.2806 (6.0426)	LR 4.000e-03
0: TRAIN [0][720/5173]	Time 0.639 (0.610)	Data 1.31e-04 (1.06e-03)	Tok/s 25920 (23076)	Loss/tok 4.5789 (6.0201)	LR 4.000e-03
0: TRAIN [0][730/5173]	Time 0.565 (0.610)	Data 3.05e-04 (1.04e-03)	Tok/s 18481 (23121)	Loss/tok 4.2050 (5.9950)	LR 4.000e-03
0: TRAIN [0][740/5173]	Time 0.569 (0.610)	Data 1.46e-04 (1.03e-03)	Tok/s 18213 (23082)	Loss/tok 4.1172 (5.9760)	LR 4.000e-03
0: TRAIN [0][750/5173]	Time 0.702 (0.610)	Data 1.29e-04 (1.02e-03)	Tok/s 32823 (23121)	Loss/tok 4.5617 (5.9519)	LR 4.000e-03
0: TRAIN [0][760/5173]	Time 0.635 (0.610)	Data 1.30e-04 (1.01e-03)	Tok/s 26308 (23108)	Loss/tok 4.3746 (5.9316)	LR 4.000e-03
0: TRAIN [0][770/5173]	Time 0.693 (0.611)	Data 1.43e-04 (9.97e-04)	Tok/s 33881 (23171)	Loss/tok 4.5196 (5.9081)	LR 4.000e-03
0: TRAIN [0][780/5173]	Time 0.567 (0.611)	Data 1.30e-04 (9.87e-04)	Tok/s 18169 (23189)	Loss/tok 4.0313 (5.8863)	LR 4.000e-03
0: TRAIN [0][790/5173]	Time 0.568 (0.611)	Data 1.29e-04 (9.76e-04)	Tok/s 18066 (23155)	Loss/tok 4.0836 (5.8684)	LR 4.000e-03
0: TRAIN [0][800/5173]	Time 0.568 (0.611)	Data 1.29e-04 (9.66e-04)	Tok/s 18403 (23149)	Loss/tok 4.0485 (5.8492)	LR 4.000e-03
0: TRAIN [0][810/5173]	Time 0.708 (0.611)	Data 1.39e-04 (9.56e-04)	Tok/s 32756 (23179)	Loss/tok 4.7690 (5.8284)	LR 4.000e-03
0: TRAIN [0][820/5173]	Time 0.704 (0.611)	Data 1.37e-04 (9.46e-04)	Tok/s 33495 (23187)	Loss/tok 4.5943 (5.8090)	LR 4.000e-03
0: TRAIN [0][830/5173]	Time 0.641 (0.611)	Data 1.26e-04 (9.36e-04)	Tok/s 25956 (23239)	Loss/tok 4.3930 (5.7874)	LR 4.000e-03
0: TRAIN [0][840/5173]	Time 0.559 (0.611)	Data 1.32e-04 (9.27e-04)	Tok/s 18492 (23238)	Loss/tok 4.0055 (5.7694)	LR 4.000e-03
0: TRAIN [0][850/5173]	Time 0.642 (0.611)	Data 1.39e-04 (9.17e-04)	Tok/s 26040 (23244)	Loss/tok 4.1709 (5.7511)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][860/5173]	Time 0.566 (0.612)	Data 1.38e-04 (9.08e-04)	Tok/s 18194 (23265)	Loss/tok 3.8894 (5.7329)	LR 4.000e-03
0: TRAIN [0][870/5173]	Time 0.635 (0.612)	Data 1.32e-04 (8.99e-04)	Tok/s 26000 (23264)	Loss/tok 4.3148 (5.7158)	LR 4.000e-03
0: TRAIN [0][880/5173]	Time 0.566 (0.611)	Data 1.38e-04 (8.91e-04)	Tok/s 18122 (23233)	Loss/tok 3.9373 (5.7003)	LR 4.000e-03
0: TRAIN [0][890/5173]	Time 0.561 (0.611)	Data 1.30e-04 (8.82e-04)	Tok/s 18345 (23228)	Loss/tok 3.8391 (5.6840)	LR 4.000e-03
0: TRAIN [0][900/5173]	Time 0.566 (0.611)	Data 1.31e-04 (8.74e-04)	Tok/s 18211 (23226)	Loss/tok 4.0719 (5.6682)	LR 4.000e-03
0: TRAIN [0][910/5173]	Time 0.642 (0.612)	Data 1.44e-04 (8.66e-04)	Tok/s 25843 (23268)	Loss/tok 4.3151 (5.6505)	LR 4.000e-03
0: TRAIN [0][920/5173]	Time 0.641 (0.612)	Data 1.33e-04 (8.58e-04)	Tok/s 26150 (23261)	Loss/tok 4.1826 (5.6354)	LR 4.000e-03
0: TRAIN [0][930/5173]	Time 0.565 (0.612)	Data 1.35e-04 (8.51e-04)	Tok/s 18026 (23280)	Loss/tok 4.0045 (5.6192)	LR 4.000e-03
0: TRAIN [0][940/5173]	Time 0.569 (0.611)	Data 1.32e-04 (8.43e-04)	Tok/s 18338 (23236)	Loss/tok 3.9265 (5.6060)	LR 4.000e-03
0: TRAIN [0][950/5173]	Time 0.561 (0.612)	Data 1.38e-04 (8.36e-04)	Tok/s 18349 (23261)	Loss/tok 4.0156 (5.5907)	LR 4.000e-03
0: TRAIN [0][960/5173]	Time 0.631 (0.611)	Data 1.27e-04 (8.29e-04)	Tok/s 26554 (23250)	Loss/tok 4.1358 (5.5764)	LR 4.000e-03
0: TRAIN [0][970/5173]	Time 0.571 (0.611)	Data 3.19e-04 (8.22e-04)	Tok/s 17820 (23216)	Loss/tok 3.8824 (5.5633)	LR 4.000e-03
0: TRAIN [0][980/5173]	Time 0.568 (0.611)	Data 1.30e-04 (8.15e-04)	Tok/s 18282 (23210)	Loss/tok 4.0098 (5.5498)	LR 4.000e-03
0: TRAIN [0][990/5173]	Time 0.569 (0.611)	Data 1.19e-04 (8.08e-04)	Tok/s 17885 (23229)	Loss/tok 3.9425 (5.5356)	LR 4.000e-03
0: TRAIN [0][1000/5173]	Time 0.509 (0.611)	Data 1.30e-04 (8.01e-04)	Tok/s 10386 (23214)	Loss/tok 3.4434 (5.5232)	LR 4.000e-03
0: TRAIN [0][1010/5173]	Time 0.500 (0.611)	Data 1.29e-04 (7.95e-04)	Tok/s 10564 (23217)	Loss/tok 3.2653 (5.5096)	LR 4.000e-03
0: TRAIN [0][1020/5173]	Time 0.505 (0.611)	Data 3.07e-04 (7.88e-04)	Tok/s 10379 (23192)	Loss/tok 3.3491 (5.4974)	LR 4.000e-03
0: TRAIN [0][1030/5173]	Time 0.568 (0.611)	Data 1.28e-04 (7.82e-04)	Tok/s 17976 (23181)	Loss/tok 3.9630 (5.4849)	LR 4.000e-03
0: TRAIN [0][1040/5173]	Time 0.559 (0.611)	Data 1.34e-04 (7.76e-04)	Tok/s 18443 (23230)	Loss/tok 3.8041 (5.4695)	LR 4.000e-03
0: TRAIN [0][1050/5173]	Time 0.766 (0.611)	Data 1.21e-04 (7.70e-04)	Tok/s 39018 (23231)	Loss/tok 4.6086 (5.4570)	LR 4.000e-03
0: TRAIN [0][1060/5173]	Time 0.568 (0.611)	Data 1.32e-04 (7.64e-04)	Tok/s 17939 (23229)	Loss/tok 3.9573 (5.4443)	LR 4.000e-03
0: TRAIN [0][1070/5173]	Time 0.638 (0.611)	Data 1.31e-04 (7.58e-04)	Tok/s 26433 (23225)	Loss/tok 4.1326 (5.4324)	LR 4.000e-03
0: TRAIN [0][1080/5173]	Time 0.619 (0.612)	Data 1.51e-04 (7.52e-04)	Tok/s 27019 (23262)	Loss/tok 4.1161 (5.4192)	LR 4.000e-03
0: TRAIN [0][1090/5173]	Time 0.567 (0.612)	Data 1.38e-04 (7.47e-04)	Tok/s 18186 (23273)	Loss/tok 3.8887 (5.4071)	LR 4.000e-03
0: TRAIN [0][1100/5173]	Time 0.614 (0.612)	Data 2.79e-04 (7.41e-04)	Tok/s 26848 (23263)	Loss/tok 4.2948 (5.3960)	LR 4.000e-03
0: TRAIN [0][1110/5173]	Time 0.766 (0.611)	Data 1.28e-04 (7.36e-04)	Tok/s 39449 (23253)	Loss/tok 4.4162 (5.3849)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1120/5173]	Time 0.639 (0.611)	Data 1.34e-04 (7.30e-04)	Tok/s 26383 (23263)	Loss/tok 4.1294 (5.3732)	LR 4.000e-03
0: TRAIN [0][1130/5173]	Time 0.565 (0.611)	Data 1.33e-04 (7.25e-04)	Tok/s 18171 (23260)	Loss/tok 3.8230 (5.3626)	LR 4.000e-03
0: TRAIN [0][1140/5173]	Time 0.509 (0.611)	Data 1.30e-04 (7.20e-04)	Tok/s 10436 (23229)	Loss/tok 3.1949 (5.3526)	LR 4.000e-03
0: TRAIN [0][1150/5173]	Time 0.574 (0.611)	Data 1.27e-04 (7.15e-04)	Tok/s 18172 (23221)	Loss/tok 3.8774 (5.3419)	LR 4.000e-03
0: TRAIN [0][1160/5173]	Time 0.645 (0.611)	Data 1.30e-04 (7.10e-04)	Tok/s 25863 (23225)	Loss/tok 4.2076 (5.3312)	LR 4.000e-03
0: TRAIN [0][1170/5173]	Time 0.703 (0.611)	Data 1.46e-04 (7.05e-04)	Tok/s 32932 (23190)	Loss/tok 4.3664 (5.3221)	LR 4.000e-03
0: TRAIN [0][1180/5173]	Time 0.563 (0.611)	Data 1.26e-04 (7.00e-04)	Tok/s 18872 (23182)	Loss/tok 3.7098 (5.3118)	LR 4.000e-03
0: TRAIN [0][1190/5173]	Time 0.568 (0.611)	Data 1.31e-04 (6.95e-04)	Tok/s 17819 (23166)	Loss/tok 3.8518 (5.3020)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1200/5173]	Time 0.638 (0.611)	Data 1.32e-04 (6.91e-04)	Tok/s 26320 (23180)	Loss/tok 4.1317 (5.2916)	LR 4.000e-03
0: TRAIN [0][1210/5173]	Time 0.695 (0.611)	Data 1.26e-04 (6.86e-04)	Tok/s 33575 (23200)	Loss/tok 4.2826 (5.2810)	LR 4.000e-03
0: TRAIN [0][1220/5173]	Time 0.631 (0.611)	Data 1.30e-04 (6.82e-04)	Tok/s 26527 (23217)	Loss/tok 4.0834 (5.2703)	LR 4.000e-03
0: TRAIN [0][1230/5173]	Time 0.618 (0.611)	Data 1.28e-04 (6.77e-04)	Tok/s 27394 (23238)	Loss/tok 4.0750 (5.2597)	LR 4.000e-03
0: TRAIN [0][1240/5173]	Time 0.699 (0.612)	Data 1.37e-04 (6.73e-04)	Tok/s 33522 (23269)	Loss/tok 4.2637 (5.2488)	LR 4.000e-03
0: TRAIN [0][1250/5173]	Time 0.631 (0.611)	Data 1.32e-04 (6.69e-04)	Tok/s 27007 (23261)	Loss/tok 3.9433 (5.2396)	LR 4.000e-03
0: TRAIN [0][1260/5173]	Time 0.566 (0.611)	Data 3.05e-04 (6.65e-04)	Tok/s 18246 (23264)	Loss/tok 3.7389 (5.2302)	LR 4.000e-03
0: TRAIN [0][1270/5173]	Time 0.632 (0.612)	Data 1.26e-04 (6.61e-04)	Tok/s 26187 (23305)	Loss/tok 4.0875 (5.2192)	LR 4.000e-03
0: TRAIN [0][1280/5173]	Time 0.565 (0.612)	Data 1.29e-04 (6.57e-04)	Tok/s 18006 (23287)	Loss/tok 3.7815 (5.2101)	LR 4.000e-03
0: TRAIN [0][1290/5173]	Time 0.559 (0.612)	Data 1.54e-04 (6.53e-04)	Tok/s 18532 (23313)	Loss/tok 3.7933 (5.2005)	LR 4.000e-03
0: TRAIN [0][1300/5173]	Time 0.686 (0.612)	Data 1.32e-04 (6.49e-04)	Tok/s 34067 (23320)	Loss/tok 4.3795 (5.1917)	LR 4.000e-03
0: TRAIN [0][1310/5173]	Time 0.578 (0.612)	Data 1.22e-04 (6.45e-04)	Tok/s 17778 (23299)	Loss/tok 3.8695 (5.1836)	LR 4.000e-03
0: TRAIN [0][1320/5173]	Time 0.502 (0.611)	Data 1.31e-04 (6.41e-04)	Tok/s 10346 (23268)	Loss/tok 3.1737 (5.1763)	LR 4.000e-03
0: TRAIN [0][1330/5173]	Time 0.503 (0.611)	Data 1.26e-04 (6.37e-04)	Tok/s 10353 (23263)	Loss/tok 3.2292 (5.1679)	LR 4.000e-03
0: TRAIN [0][1340/5173]	Time 0.760 (0.611)	Data 1.31e-04 (6.34e-04)	Tok/s 39045 (23268)	Loss/tok 4.3980 (5.1594)	LR 4.000e-03
0: TRAIN [0][1350/5173]	Time 0.631 (0.611)	Data 1.33e-04 (6.30e-04)	Tok/s 26410 (23283)	Loss/tok 4.0028 (5.1505)	LR 4.000e-03
0: TRAIN [0][1360/5173]	Time 0.564 (0.611)	Data 1.30e-04 (6.26e-04)	Tok/s 18377 (23270)	Loss/tok 3.7572 (5.1427)	LR 4.000e-03
0: TRAIN [0][1370/5173]	Time 0.702 (0.611)	Data 3.17e-04 (6.23e-04)	Tok/s 33006 (23256)	Loss/tok 4.2900 (5.1349)	LR 4.000e-03
0: TRAIN [0][1380/5173]	Time 0.703 (0.611)	Data 1.31e-04 (6.19e-04)	Tok/s 32829 (23231)	Loss/tok 4.3753 (5.1277)	LR 4.000e-03
0: TRAIN [0][1390/5173]	Time 0.624 (0.611)	Data 1.30e-04 (6.16e-04)	Tok/s 26654 (23224)	Loss/tok 4.1164 (5.1199)	LR 4.000e-03
0: TRAIN [0][1400/5173]	Time 0.692 (0.611)	Data 1.28e-04 (6.13e-04)	Tok/s 34088 (23234)	Loss/tok 4.1474 (5.1115)	LR 4.000e-03
0: TRAIN [0][1410/5173]	Time 0.567 (0.611)	Data 3.22e-04 (6.09e-04)	Tok/s 18268 (23205)	Loss/tok 3.7680 (5.1046)	LR 4.000e-03
0: TRAIN [0][1420/5173]	Time 0.632 (0.611)	Data 1.30e-04 (6.06e-04)	Tok/s 26848 (23209)	Loss/tok 3.9394 (5.0967)	LR 4.000e-03
0: TRAIN [0][1430/5173]	Time 0.629 (0.611)	Data 1.30e-04 (6.03e-04)	Tok/s 26664 (23197)	Loss/tok 4.0866 (5.0895)	LR 4.000e-03
0: TRAIN [0][1440/5173]	Time 0.640 (0.610)	Data 1.33e-04 (5.99e-04)	Tok/s 25920 (23183)	Loss/tok 4.0682 (5.0824)	LR 4.000e-03
0: TRAIN [0][1450/5173]	Time 0.500 (0.610)	Data 1.33e-04 (5.96e-04)	Tok/s 10620 (23172)	Loss/tok 3.2361 (5.0755)	LR 4.000e-03
0: TRAIN [0][1460/5173]	Time 0.500 (0.610)	Data 1.30e-04 (5.93e-04)	Tok/s 10692 (23179)	Loss/tok 3.1839 (5.0677)	LR 4.000e-03
0: TRAIN [0][1470/5173]	Time 0.568 (0.610)	Data 1.34e-04 (5.90e-04)	Tok/s 18427 (23168)	Loss/tok 3.8215 (5.0607)	LR 4.000e-03
0: TRAIN [0][1480/5173]	Time 0.639 (0.610)	Data 1.32e-04 (5.87e-04)	Tok/s 26541 (23187)	Loss/tok 3.9214 (5.0525)	LR 4.000e-03
0: TRAIN [0][1490/5173]	Time 0.693 (0.610)	Data 1.32e-04 (5.84e-04)	Tok/s 33654 (23185)	Loss/tok 4.1922 (5.0453)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1500/5173]	Time 0.695 (0.611)	Data 1.44e-04 (5.81e-04)	Tok/s 33846 (23217)	Loss/tok 4.1150 (5.0373)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1510/5173]	Time 0.699 (0.611)	Data 1.29e-04 (5.78e-04)	Tok/s 33270 (23214)	Loss/tok 4.1660 (5.0305)	LR 4.000e-03
0: TRAIN [0][1520/5173]	Time 0.569 (0.611)	Data 1.29e-04 (5.76e-04)	Tok/s 18146 (23211)	Loss/tok 3.6575 (5.0237)	LR 4.000e-03
0: TRAIN [0][1530/5173]	Time 0.627 (0.611)	Data 1.29e-04 (5.73e-04)	Tok/s 26792 (23210)	Loss/tok 3.8650 (5.0171)	LR 4.000e-03
0: TRAIN [0][1540/5173]	Time 0.501 (0.610)	Data 1.35e-04 (5.70e-04)	Tok/s 10647 (23192)	Loss/tok 3.2253 (5.0107)	LR 4.000e-03
0: TRAIN [0][1550/5173]	Time 0.634 (0.610)	Data 1.24e-04 (5.67e-04)	Tok/s 26445 (23192)	Loss/tok 3.9118 (5.0038)	LR 4.000e-03
0: TRAIN [0][1560/5173]	Time 0.625 (0.610)	Data 1.34e-04 (5.64e-04)	Tok/s 26969 (23201)	Loss/tok 3.9287 (4.9971)	LR 4.000e-03
0: TRAIN [0][1570/5173]	Time 0.642 (0.610)	Data 1.32e-04 (5.62e-04)	Tok/s 25964 (23208)	Loss/tok 3.9019 (4.9902)	LR 4.000e-03
0: TRAIN [0][1580/5173]	Time 0.574 (0.610)	Data 1.31e-04 (5.59e-04)	Tok/s 17777 (23209)	Loss/tok 3.6581 (4.9843)	LR 4.000e-03
0: TRAIN [0][1590/5173]	Time 0.632 (0.610)	Data 1.39e-04 (5.56e-04)	Tok/s 26604 (23208)	Loss/tok 3.8882 (4.9776)	LR 4.000e-03
0: TRAIN [0][1600/5173]	Time 0.705 (0.611)	Data 1.46e-04 (5.54e-04)	Tok/s 33072 (23218)	Loss/tok 4.3013 (4.9709)	LR 4.000e-03
0: TRAIN [0][1610/5173]	Time 0.569 (0.610)	Data 1.60e-04 (5.51e-04)	Tok/s 18608 (23199)	Loss/tok 3.8159 (4.9651)	LR 4.000e-03
0: TRAIN [0][1620/5173]	Time 0.760 (0.610)	Data 1.43e-04 (5.49e-04)	Tok/s 39907 (23191)	Loss/tok 4.3886 (4.9590)	LR 4.000e-03
0: TRAIN [0][1630/5173]	Time 0.571 (0.610)	Data 1.54e-04 (5.46e-04)	Tok/s 18016 (23187)	Loss/tok 3.6355 (4.9531)	LR 4.000e-03
0: TRAIN [0][1640/5173]	Time 0.614 (0.610)	Data 1.61e-04 (5.44e-04)	Tok/s 27046 (23197)	Loss/tok 3.9621 (4.9470)	LR 4.000e-03
0: TRAIN [0][1650/5173]	Time 0.644 (0.610)	Data 1.52e-04 (5.42e-04)	Tok/s 26068 (23190)	Loss/tok 3.8971 (4.9412)	LR 4.000e-03
0: TRAIN [0][1660/5173]	Time 0.560 (0.610)	Data 1.40e-04 (5.39e-04)	Tok/s 18329 (23189)	Loss/tok 3.5985 (4.9350)	LR 4.000e-03
0: TRAIN [0][1670/5173]	Time 0.574 (0.610)	Data 1.42e-04 (5.37e-04)	Tok/s 18137 (23197)	Loss/tok 3.7280 (4.9292)	LR 4.000e-03
0: TRAIN [0][1680/5173]	Time 0.495 (0.610)	Data 1.47e-04 (5.35e-04)	Tok/s 10882 (23191)	Loss/tok 3.2266 (4.9232)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1690/5173]	Time 0.767 (0.610)	Data 3.43e-04 (5.32e-04)	Tok/s 38741 (23197)	Loss/tok 4.4176 (4.9176)	LR 4.000e-03
0: TRAIN [0][1700/5173]	Time 0.699 (0.611)	Data 1.48e-04 (5.30e-04)	Tok/s 33253 (23213)	Loss/tok 4.1364 (4.9112)	LR 4.000e-03
0: TRAIN [0][1710/5173]	Time 0.570 (0.611)	Data 1.36e-04 (5.28e-04)	Tok/s 17905 (23222)	Loss/tok 3.6466 (4.9056)	LR 4.000e-03
0: TRAIN [0][1720/5173]	Time 0.638 (0.611)	Data 1.27e-04 (5.26e-04)	Tok/s 26313 (23214)	Loss/tok 3.8937 (4.9001)	LR 4.000e-03
0: TRAIN [0][1730/5173]	Time 0.498 (0.611)	Data 1.28e-04 (5.24e-04)	Tok/s 10738 (23201)	Loss/tok 3.0860 (4.8950)	LR 4.000e-03
0: TRAIN [0][1740/5173]	Time 0.560 (0.611)	Data 1.26e-04 (5.22e-04)	Tok/s 18657 (23207)	Loss/tok 3.5992 (4.8893)	LR 4.000e-03
0: TRAIN [0][1750/5173]	Time 0.563 (0.611)	Data 1.22e-04 (5.19e-04)	Tok/s 18281 (23201)	Loss/tok 3.6197 (4.8837)	LR 4.000e-03
0: TRAIN [0][1760/5173]	Time 0.563 (0.611)	Data 1.27e-04 (5.17e-04)	Tok/s 18885 (23210)	Loss/tok 3.6294 (4.8779)	LR 4.000e-03
0: TRAIN [0][1770/5173]	Time 0.771 (0.611)	Data 1.34e-04 (5.15e-04)	Tok/s 39309 (23216)	Loss/tok 4.3919 (4.8725)	LR 4.000e-03
0: TRAIN [0][1780/5173]	Time 0.635 (0.611)	Data 1.19e-04 (5.13e-04)	Tok/s 26601 (23201)	Loss/tok 3.9112 (4.8674)	LR 4.000e-03
0: TRAIN [0][1790/5173]	Time 0.690 (0.611)	Data 3.13e-04 (5.11e-04)	Tok/s 33780 (23203)	Loss/tok 4.1286 (4.8620)	LR 4.000e-03
0: TRAIN [0][1800/5173]	Time 0.507 (0.610)	Data 1.26e-04 (5.09e-04)	Tok/s 10460 (23197)	Loss/tok 3.1533 (4.8570)	LR 4.000e-03
0: TRAIN [0][1810/5173]	Time 0.562 (0.611)	Data 1.54e-04 (5.07e-04)	Tok/s 18367 (23202)	Loss/tok 3.7483 (4.8520)	LR 4.000e-03
0: TRAIN [0][1820/5173]	Time 0.626 (0.611)	Data 1.49e-04 (5.05e-04)	Tok/s 26955 (23228)	Loss/tok 3.8786 (4.8467)	LR 4.000e-03
0: TRAIN [0][1830/5173]	Time 0.564 (0.610)	Data 1.33e-04 (5.03e-04)	Tok/s 18229 (23196)	Loss/tok 3.5942 (4.8424)	LR 4.000e-03
0: TRAIN [0][1840/5173]	Time 0.568 (0.610)	Data 1.19e-04 (5.01e-04)	Tok/s 17748 (23190)	Loss/tok 3.6293 (4.8375)	LR 4.000e-03
0: TRAIN [0][1850/5173]	Time 0.701 (0.611)	Data 1.31e-04 (4.99e-04)	Tok/s 33290 (23203)	Loss/tok 4.1073 (4.8321)	LR 4.000e-03
0: TRAIN [0][1860/5173]	Time 0.636 (0.610)	Data 1.21e-04 (4.97e-04)	Tok/s 26158 (23201)	Loss/tok 3.8837 (4.8273)	LR 4.000e-03
0: TRAIN [0][1870/5173]	Time 0.562 (0.610)	Data 1.34e-04 (4.95e-04)	Tok/s 18262 (23203)	Loss/tok 3.6884 (4.8227)	LR 4.000e-03
0: TRAIN [0][1880/5173]	Time 0.639 (0.611)	Data 1.25e-04 (4.93e-04)	Tok/s 26341 (23217)	Loss/tok 3.9069 (4.8177)	LR 4.000e-03
0: TRAIN [0][1890/5173]	Time 0.509 (0.611)	Data 1.24e-04 (4.91e-04)	Tok/s 10198 (23211)	Loss/tok 3.0901 (4.8127)	LR 4.000e-03
0: TRAIN [0][1900/5173]	Time 0.766 (0.611)	Data 1.25e-04 (4.89e-04)	Tok/s 38914 (23228)	Loss/tok 4.3448 (4.8077)	LR 4.000e-03
0: TRAIN [0][1910/5173]	Time 0.565 (0.611)	Data 1.18e-04 (4.87e-04)	Tok/s 17904 (23215)	Loss/tok 3.6075 (4.8035)	LR 4.000e-03
0: TRAIN [0][1920/5173]	Time 0.569 (0.611)	Data 1.33e-04 (4.85e-04)	Tok/s 17951 (23213)	Loss/tok 3.7758 (4.7987)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1930/5173]	Time 0.559 (0.610)	Data 1.23e-04 (4.83e-04)	Tok/s 18271 (23207)	Loss/tok 3.5575 (4.7942)	LR 4.000e-03
0: TRAIN [0][1940/5173]	Time 0.565 (0.610)	Data 1.23e-04 (4.82e-04)	Tok/s 18500 (23211)	Loss/tok 3.5933 (4.7897)	LR 4.000e-03
0: TRAIN [0][1950/5173]	Time 0.498 (0.610)	Data 1.25e-04 (4.80e-04)	Tok/s 10471 (23202)	Loss/tok 3.0079 (4.7851)	LR 4.000e-03
0: TRAIN [0][1960/5173]	Time 0.627 (0.610)	Data 1.30e-04 (4.78e-04)	Tok/s 26599 (23216)	Loss/tok 3.9112 (4.7804)	LR 4.000e-03
0: TRAIN [0][1970/5173]	Time 0.629 (0.610)	Data 1.22e-04 (4.76e-04)	Tok/s 26613 (23215)	Loss/tok 3.8523 (4.7759)	LR 4.000e-03
0: TRAIN [0][1980/5173]	Time 0.639 (0.610)	Data 1.33e-04 (4.75e-04)	Tok/s 26614 (23217)	Loss/tok 3.9601 (4.7713)	LR 4.000e-03
0: TRAIN [0][1990/5173]	Time 0.630 (0.610)	Data 1.32e-04 (4.73e-04)	Tok/s 26624 (23208)	Loss/tok 3.8688 (4.7670)	LR 4.000e-03
0: TRAIN [0][2000/5173]	Time 0.564 (0.611)	Data 1.21e-04 (4.71e-04)	Tok/s 18478 (23227)	Loss/tok 3.6263 (4.7618)	LR 4.000e-03
0: TRAIN [0][2010/5173]	Time 0.704 (0.611)	Data 1.28e-04 (4.69e-04)	Tok/s 33139 (23231)	Loss/tok 4.0752 (4.7575)	LR 4.000e-03
0: TRAIN [0][2020/5173]	Time 0.562 (0.611)	Data 1.32e-04 (4.68e-04)	Tok/s 18086 (23227)	Loss/tok 3.6019 (4.7531)	LR 4.000e-03
0: TRAIN [0][2030/5173]	Time 0.690 (0.610)	Data 1.31e-04 (4.66e-04)	Tok/s 33693 (23222)	Loss/tok 4.1661 (4.7490)	LR 4.000e-03
0: TRAIN [0][2040/5173]	Time 0.566 (0.611)	Data 1.29e-04 (4.64e-04)	Tok/s 18350 (23230)	Loss/tok 3.6150 (4.7446)	LR 4.000e-03
0: TRAIN [0][2050/5173]	Time 0.565 (0.610)	Data 1.32e-04 (4.63e-04)	Tok/s 18556 (23224)	Loss/tok 3.6913 (4.7406)	LR 4.000e-03
0: TRAIN [0][2060/5173]	Time 0.701 (0.611)	Data 1.34e-04 (4.61e-04)	Tok/s 33256 (23230)	Loss/tok 4.0972 (4.7363)	LR 4.000e-03
0: TRAIN [0][2070/5173]	Time 0.507 (0.611)	Data 1.31e-04 (4.60e-04)	Tok/s 10293 (23232)	Loss/tok 3.0432 (4.7321)	LR 4.000e-03
0: TRAIN [0][2080/5173]	Time 0.704 (0.610)	Data 1.28e-04 (4.58e-04)	Tok/s 33304 (23222)	Loss/tok 4.1124 (4.7280)	LR 4.000e-03
0: TRAIN [0][2090/5173]	Time 0.563 (0.611)	Data 1.30e-04 (4.57e-04)	Tok/s 18480 (23227)	Loss/tok 3.5371 (4.7238)	LR 4.000e-03
0: TRAIN [0][2100/5173]	Time 0.704 (0.611)	Data 1.23e-04 (4.55e-04)	Tok/s 33261 (23235)	Loss/tok 4.1010 (4.7197)	LR 4.000e-03
0: TRAIN [0][2110/5173]	Time 0.629 (0.611)	Data 1.27e-04 (4.53e-04)	Tok/s 26335 (23254)	Loss/tok 3.9658 (4.7154)	LR 4.000e-03
0: TRAIN [0][2120/5173]	Time 0.694 (0.611)	Data 1.48e-04 (4.52e-04)	Tok/s 34012 (23274)	Loss/tok 4.1081 (4.7111)	LR 4.000e-03
0: TRAIN [0][2130/5173]	Time 0.506 (0.611)	Data 1.29e-04 (4.51e-04)	Tok/s 10395 (23279)	Loss/tok 3.2101 (4.7071)	LR 4.000e-03
0: TRAIN [0][2140/5173]	Time 0.639 (0.611)	Data 2.86e-04 (4.49e-04)	Tok/s 26049 (23286)	Loss/tok 3.9551 (4.7036)	LR 4.000e-03
0: TRAIN [0][2150/5173]	Time 0.561 (0.611)	Data 1.25e-04 (4.48e-04)	Tok/s 18267 (23291)	Loss/tok 3.5750 (4.6996)	LR 4.000e-03
0: TRAIN [0][2160/5173]	Time 0.568 (0.611)	Data 3.12e-04 (4.46e-04)	Tok/s 17904 (23285)	Loss/tok 3.7450 (4.6958)	LR 4.000e-03
0: TRAIN [0][2170/5173]	Time 0.573 (0.611)	Data 1.25e-04 (4.45e-04)	Tok/s 18030 (23283)	Loss/tok 3.5682 (4.6921)	LR 4.000e-03
0: TRAIN [0][2180/5173]	Time 0.627 (0.611)	Data 1.28e-04 (4.43e-04)	Tok/s 26638 (23298)	Loss/tok 3.8553 (4.6880)	LR 4.000e-03
0: TRAIN [0][2190/5173]	Time 0.502 (0.611)	Data 1.22e-04 (4.42e-04)	Tok/s 10619 (23288)	Loss/tok 3.0968 (4.6847)	LR 4.000e-03
0: TRAIN [0][2200/5173]	Time 0.566 (0.611)	Data 1.35e-04 (4.41e-04)	Tok/s 18236 (23288)	Loss/tok 3.4931 (4.6808)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2210/5173]	Time 0.565 (0.611)	Data 1.30e-04 (4.39e-04)	Tok/s 18192 (23292)	Loss/tok 3.5715 (4.6771)	LR 4.000e-03
0: TRAIN [0][2220/5173]	Time 0.638 (0.611)	Data 1.16e-04 (4.38e-04)	Tok/s 26253 (23284)	Loss/tok 3.8826 (4.6733)	LR 4.000e-03
0: TRAIN [0][2230/5173]	Time 0.640 (0.611)	Data 1.24e-04 (4.36e-04)	Tok/s 26087 (23282)	Loss/tok 3.9741 (4.6699)	LR 4.000e-03
0: TRAIN [0][2240/5173]	Time 0.562 (0.611)	Data 1.26e-04 (4.35e-04)	Tok/s 18849 (23275)	Loss/tok 3.6670 (4.6664)	LR 4.000e-03
0: TRAIN [0][2250/5173]	Time 0.561 (0.611)	Data 1.18e-04 (4.34e-04)	Tok/s 18648 (23270)	Loss/tok 3.5036 (4.6627)	LR 4.000e-03
0: TRAIN [0][2260/5173]	Time 0.507 (0.611)	Data 1.39e-04 (4.32e-04)	Tok/s 10266 (23272)	Loss/tok 3.0038 (4.6594)	LR 4.000e-03
0: TRAIN [0][2270/5173]	Time 0.647 (0.611)	Data 1.30e-04 (4.31e-04)	Tok/s 26146 (23277)	Loss/tok 3.7616 (4.6556)	LR 4.000e-03
0: TRAIN [0][2280/5173]	Time 0.633 (0.611)	Data 3.18e-04 (4.30e-04)	Tok/s 26457 (23282)	Loss/tok 3.8419 (4.6519)	LR 4.000e-03
0: TRAIN [0][2290/5173]	Time 0.564 (0.611)	Data 1.27e-04 (4.28e-04)	Tok/s 18085 (23270)	Loss/tok 3.6429 (4.6487)	LR 4.000e-03
0: TRAIN [0][2300/5173]	Time 0.641 (0.611)	Data 1.27e-04 (4.27e-04)	Tok/s 26406 (23271)	Loss/tok 3.7550 (4.6451)	LR 4.000e-03
0: TRAIN [0][2310/5173]	Time 0.646 (0.611)	Data 1.27e-04 (4.26e-04)	Tok/s 25639 (23266)	Loss/tok 3.8717 (4.6417)	LR 4.000e-03
0: TRAIN [0][2320/5173]	Time 0.575 (0.611)	Data 1.18e-04 (4.25e-04)	Tok/s 17676 (23261)	Loss/tok 3.5670 (4.6383)	LR 4.000e-03
0: TRAIN [0][2330/5173]	Time 0.500 (0.611)	Data 1.30e-04 (4.23e-04)	Tok/s 10581 (23240)	Loss/tok 3.1511 (4.6354)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2340/5173]	Time 0.570 (0.611)	Data 1.23e-04 (4.22e-04)	Tok/s 18379 (23234)	Loss/tok 3.4403 (4.6320)	LR 4.000e-03
0: TRAIN [0][2350/5173]	Time 0.570 (0.611)	Data 1.36e-04 (4.21e-04)	Tok/s 18489 (23240)	Loss/tok 3.5822 (4.6287)	LR 4.000e-03
0: TRAIN [0][2360/5173]	Time 0.571 (0.611)	Data 1.24e-04 (4.20e-04)	Tok/s 17941 (23234)	Loss/tok 3.5696 (4.6255)	LR 4.000e-03
0: TRAIN [0][2370/5173]	Time 0.616 (0.611)	Data 1.28e-04 (4.18e-04)	Tok/s 27254 (23226)	Loss/tok 3.8273 (4.6222)	LR 4.000e-03
0: TRAIN [0][2380/5173]	Time 0.636 (0.611)	Data 1.24e-04 (4.17e-04)	Tok/s 26140 (23224)	Loss/tok 3.7972 (4.6189)	LR 4.000e-03
0: TRAIN [0][2390/5173]	Time 0.646 (0.611)	Data 1.27e-04 (4.16e-04)	Tok/s 25710 (23231)	Loss/tok 3.7939 (4.6154)	LR 4.000e-03
0: TRAIN [0][2400/5173]	Time 0.568 (0.611)	Data 1.23e-04 (4.15e-04)	Tok/s 18042 (23230)	Loss/tok 3.5371 (4.6119)	LR 4.000e-03
0: TRAIN [0][2410/5173]	Time 0.561 (0.611)	Data 1.20e-04 (4.14e-04)	Tok/s 18295 (23229)	Loss/tok 3.4857 (4.6085)	LR 4.000e-03
0: TRAIN [0][2420/5173]	Time 0.567 (0.611)	Data 1.23e-04 (4.12e-04)	Tok/s 18250 (23243)	Loss/tok 3.5688 (4.6053)	LR 4.000e-03
0: TRAIN [0][2430/5173]	Time 0.570 (0.611)	Data 1.29e-04 (4.11e-04)	Tok/s 18094 (23242)	Loss/tok 3.6761 (4.6021)	LR 4.000e-03
0: TRAIN [0][2440/5173]	Time 0.637 (0.611)	Data 1.20e-04 (4.10e-04)	Tok/s 26491 (23236)	Loss/tok 3.6881 (4.5992)	LR 4.000e-03
0: TRAIN [0][2450/5173]	Time 0.566 (0.611)	Data 1.35e-04 (4.09e-04)	Tok/s 17975 (23238)	Loss/tok 3.5711 (4.5961)	LR 4.000e-03
0: TRAIN [0][2460/5173]	Time 0.763 (0.611)	Data 1.30e-04 (4.08e-04)	Tok/s 38661 (23246)	Loss/tok 4.3250 (4.5930)	LR 4.000e-03
0: TRAIN [0][2470/5173]	Time 0.567 (0.611)	Data 1.22e-04 (4.07e-04)	Tok/s 18090 (23229)	Loss/tok 3.6606 (4.5902)	LR 4.000e-03
0: TRAIN [0][2480/5173]	Time 0.498 (0.611)	Data 1.19e-04 (4.05e-04)	Tok/s 10471 (23228)	Loss/tok 3.0622 (4.5870)	LR 4.000e-03
0: TRAIN [0][2490/5173]	Time 0.570 (0.611)	Data 1.27e-04 (4.04e-04)	Tok/s 17957 (23226)	Loss/tok 3.6025 (4.5841)	LR 4.000e-03
0: TRAIN [0][2500/5173]	Time 0.637 (0.611)	Data 1.23e-04 (4.03e-04)	Tok/s 25955 (23218)	Loss/tok 3.8196 (4.5812)	LR 4.000e-03
0: TRAIN [0][2510/5173]	Time 0.563 (0.611)	Data 3.15e-04 (4.02e-04)	Tok/s 18455 (23229)	Loss/tok 3.4843 (4.5778)	LR 4.000e-03
0: TRAIN [0][2520/5173]	Time 0.644 (0.611)	Data 1.28e-04 (4.01e-04)	Tok/s 25934 (23239)	Loss/tok 3.7986 (4.5746)	LR 4.000e-03
0: TRAIN [0][2530/5173]	Time 0.706 (0.611)	Data 1.35e-04 (4.00e-04)	Tok/s 32996 (23244)	Loss/tok 4.0077 (4.5714)	LR 4.000e-03
0: TRAIN [0][2540/5173]	Time 0.500 (0.611)	Data 1.18e-04 (3.99e-04)	Tok/s 10223 (23242)	Loss/tok 3.0343 (4.5685)	LR 4.000e-03
0: TRAIN [0][2550/5173]	Time 0.503 (0.611)	Data 1.28e-04 (3.98e-04)	Tok/s 10688 (23237)	Loss/tok 3.0380 (4.5656)	LR 4.000e-03
0: TRAIN [0][2560/5173]	Time 0.648 (0.611)	Data 1.24e-04 (3.97e-04)	Tok/s 26122 (23235)	Loss/tok 3.7122 (4.5625)	LR 4.000e-03
0: TRAIN [0][2570/5173]	Time 0.631 (0.611)	Data 1.21e-04 (3.96e-04)	Tok/s 26851 (23233)	Loss/tok 3.8602 (4.5596)	LR 4.000e-03
0: TRAIN [0][2580/5173]	Time 0.574 (0.611)	Data 1.24e-04 (3.95e-04)	Tok/s 17837 (23225)	Loss/tok 3.4791 (4.5569)	LR 4.000e-03
0: TRAIN [0][2590/5173]	Time 0.630 (0.611)	Data 1.24e-04 (3.94e-04)	Tok/s 26368 (23226)	Loss/tok 3.8376 (4.5543)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2600/5173]	Time 0.562 (0.611)	Data 1.21e-04 (3.93e-04)	Tok/s 18718 (23236)	Loss/tok 3.5119 (4.5515)	LR 4.000e-03
0: TRAIN [0][2610/5173]	Time 0.706 (0.611)	Data 1.42e-04 (3.92e-04)	Tok/s 33071 (23244)	Loss/tok 3.9573 (4.5487)	LR 4.000e-03
0: TRAIN [0][2620/5173]	Time 0.625 (0.611)	Data 1.39e-04 (3.91e-04)	Tok/s 26679 (23245)	Loss/tok 3.8133 (4.5457)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][2630/5173]	Time 0.574 (0.611)	Data 1.21e-04 (3.90e-04)	Tok/s 18197 (23248)	Loss/tok 3.5762 (4.5431)	LR 4.000e-03
0: TRAIN [0][2640/5173]	Time 0.569 (0.611)	Data 1.34e-04 (3.89e-04)	Tok/s 18296 (23252)	Loss/tok 3.6029 (4.5404)	LR 4.000e-03
0: TRAIN [0][2650/5173]	Time 0.566 (0.611)	Data 1.27e-04 (3.88e-04)	Tok/s 18350 (23261)	Loss/tok 3.5077 (4.5374)	LR 4.000e-03
0: TRAIN [0][2660/5173]	Time 0.554 (0.611)	Data 1.24e-04 (3.87e-04)	Tok/s 18850 (23259)	Loss/tok 3.4928 (4.5349)	LR 4.000e-03
0: TRAIN [0][2670/5173]	Time 0.570 (0.611)	Data 1.24e-04 (3.86e-04)	Tok/s 18862 (23255)	Loss/tok 3.5039 (4.5322)	LR 4.000e-03
0: TRAIN [0][2680/5173]	Time 0.570 (0.611)	Data 1.25e-04 (3.85e-04)	Tok/s 18310 (23237)	Loss/tok 3.6709 (4.5297)	LR 4.000e-03
0: TRAIN [0][2690/5173]	Time 0.573 (0.611)	Data 1.30e-04 (3.84e-04)	Tok/s 17434 (23228)	Loss/tok 3.5938 (4.5273)	LR 4.000e-03
0: TRAIN [0][2700/5173]	Time 0.693 (0.611)	Data 1.22e-04 (3.83e-04)	Tok/s 33682 (23219)	Loss/tok 3.9774 (4.5247)	LR 4.000e-03
0: TRAIN [0][2710/5173]	Time 0.572 (0.611)	Data 1.25e-04 (3.82e-04)	Tok/s 18086 (23224)	Loss/tok 3.4480 (4.5217)	LR 4.000e-03
0: TRAIN [0][2720/5173]	Time 0.561 (0.611)	Data 1.22e-04 (3.81e-04)	Tok/s 18096 (23202)	Loss/tok 3.5713 (4.5194)	LR 4.000e-03
0: TRAIN [0][2730/5173]	Time 0.503 (0.610)	Data 1.40e-04 (3.80e-04)	Tok/s 10483 (23185)	Loss/tok 3.0868 (4.5171)	LR 4.000e-03
0: TRAIN [0][2740/5173]	Time 0.568 (0.610)	Data 1.27e-04 (3.79e-04)	Tok/s 18412 (23169)	Loss/tok 3.3766 (4.5145)	LR 4.000e-03
0: TRAIN [0][2750/5173]	Time 0.499 (0.610)	Data 1.42e-04 (3.78e-04)	Tok/s 10446 (23153)	Loss/tok 3.0137 (4.5122)	LR 4.000e-03
0: TRAIN [0][2760/5173]	Time 0.557 (0.610)	Data 1.22e-04 (3.78e-04)	Tok/s 18523 (23155)	Loss/tok 3.4683 (4.5096)	LR 4.000e-03
0: TRAIN [0][2770/5173]	Time 0.572 (0.610)	Data 1.22e-04 (3.77e-04)	Tok/s 17968 (23154)	Loss/tok 3.5141 (4.5070)	LR 4.000e-03
0: TRAIN [0][2780/5173]	Time 0.575 (0.610)	Data 1.21e-04 (3.76e-04)	Tok/s 17509 (23147)	Loss/tok 3.5161 (4.5047)	LR 4.000e-03
0: TRAIN [0][2790/5173]	Time 0.693 (0.610)	Data 1.21e-04 (3.75e-04)	Tok/s 33532 (23155)	Loss/tok 4.0638 (4.5022)	LR 4.000e-03
0: TRAIN [0][2800/5173]	Time 0.561 (0.610)	Data 1.22e-04 (3.74e-04)	Tok/s 18458 (23146)	Loss/tok 3.5332 (4.4998)	LR 4.000e-03
0: TRAIN [0][2810/5173]	Time 0.563 (0.610)	Data 1.22e-04 (3.73e-04)	Tok/s 17997 (23134)	Loss/tok 3.5753 (4.4974)	LR 4.000e-03
0: TRAIN [0][2820/5173]	Time 0.568 (0.610)	Data 1.28e-04 (3.72e-04)	Tok/s 18146 (23124)	Loss/tok 3.5181 (4.4952)	LR 4.000e-03
0: TRAIN [0][2830/5173]	Time 0.636 (0.610)	Data 1.21e-04 (3.71e-04)	Tok/s 26833 (23130)	Loss/tok 3.7977 (4.4925)	LR 4.000e-03
0: TRAIN [0][2840/5173]	Time 0.636 (0.610)	Data 1.30e-04 (3.71e-04)	Tok/s 26229 (23124)	Loss/tok 3.7882 (4.4899)	LR 4.000e-03
0: TRAIN [0][2850/5173]	Time 0.572 (0.610)	Data 1.24e-04 (3.70e-04)	Tok/s 18220 (23131)	Loss/tok 3.5697 (4.4875)	LR 4.000e-03
0: TRAIN [0][2860/5173]	Time 0.638 (0.610)	Data 1.20e-04 (3.69e-04)	Tok/s 26591 (23122)	Loss/tok 3.8957 (4.4851)	LR 4.000e-03
0: TRAIN [0][2870/5173]	Time 0.568 (0.610)	Data 1.33e-04 (3.68e-04)	Tok/s 18392 (23119)	Loss/tok 3.6027 (4.4827)	LR 4.000e-03
0: TRAIN [0][2880/5173]	Time 0.695 (0.610)	Data 1.28e-04 (3.67e-04)	Tok/s 33878 (23132)	Loss/tok 3.9878 (4.4801)	LR 4.000e-03
0: TRAIN [0][2890/5173]	Time 0.565 (0.610)	Data 1.28e-04 (3.66e-04)	Tok/s 18088 (23115)	Loss/tok 3.5041 (4.4779)	LR 4.000e-03
0: TRAIN [0][2900/5173]	Time 0.561 (0.610)	Data 1.37e-04 (3.66e-04)	Tok/s 18437 (23111)	Loss/tok 3.5208 (4.4755)	LR 4.000e-03
0: TRAIN [0][2910/5173]	Time 0.570 (0.610)	Data 1.25e-04 (3.65e-04)	Tok/s 18585 (23112)	Loss/tok 3.4513 (4.4732)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2920/5173]	Time 0.768 (0.610)	Data 1.37e-04 (3.64e-04)	Tok/s 38460 (23116)	Loss/tok 4.1429 (4.4712)	LR 4.000e-03
0: TRAIN [0][2930/5173]	Time 0.554 (0.610)	Data 1.24e-04 (3.63e-04)	Tok/s 18528 (23115)	Loss/tok 3.6454 (4.4689)	LR 4.000e-03
0: TRAIN [0][2940/5173]	Time 0.626 (0.610)	Data 1.36e-04 (3.62e-04)	Tok/s 27291 (23119)	Loss/tok 3.8307 (4.4665)	LR 4.000e-03
0: TRAIN [0][2950/5173]	Time 0.562 (0.610)	Data 1.19e-04 (3.62e-04)	Tok/s 18639 (23121)	Loss/tok 3.4188 (4.4643)	LR 4.000e-03
0: TRAIN [0][2960/5173]	Time 0.641 (0.610)	Data 1.33e-04 (3.61e-04)	Tok/s 26417 (23119)	Loss/tok 3.9143 (4.4620)	LR 4.000e-03
0: TRAIN [0][2970/5173]	Time 0.563 (0.610)	Data 1.34e-04 (3.60e-04)	Tok/s 18843 (23128)	Loss/tok 3.4809 (4.4597)	LR 4.000e-03
0: TRAIN [0][2980/5173]	Time 0.774 (0.610)	Data 1.23e-04 (3.59e-04)	Tok/s 38617 (23127)	Loss/tok 4.1232 (4.4573)	LR 4.000e-03
0: TRAIN [0][2990/5173]	Time 0.704 (0.610)	Data 1.23e-04 (3.58e-04)	Tok/s 32863 (23133)	Loss/tok 4.0957 (4.4551)	LR 4.000e-03
0: TRAIN [0][3000/5173]	Time 0.561 (0.610)	Data 1.25e-04 (3.58e-04)	Tok/s 18649 (23130)	Loss/tok 3.5039 (4.4527)	LR 4.000e-03
0: TRAIN [0][3010/5173]	Time 0.567 (0.610)	Data 1.24e-04 (3.57e-04)	Tok/s 18397 (23119)	Loss/tok 3.5550 (4.4505)	LR 4.000e-03
0: TRAIN [0][3020/5173]	Time 0.641 (0.610)	Data 1.20e-04 (3.56e-04)	Tok/s 26080 (23123)	Loss/tok 3.7865 (4.4483)	LR 4.000e-03
0: TRAIN [0][3030/5173]	Time 0.761 (0.610)	Data 3.11e-04 (3.55e-04)	Tok/s 39204 (23138)	Loss/tok 4.3177 (4.4462)	LR 4.000e-03
0: TRAIN [0][3040/5173]	Time 0.764 (0.610)	Data 1.23e-04 (3.55e-04)	Tok/s 39442 (23137)	Loss/tok 4.1159 (4.4440)	LR 4.000e-03
0: TRAIN [0][3050/5173]	Time 0.572 (0.610)	Data 1.27e-04 (3.54e-04)	Tok/s 17762 (23138)	Loss/tok 3.4479 (4.4417)	LR 4.000e-03
0: TRAIN [0][3060/5173]	Time 0.500 (0.610)	Data 1.29e-04 (3.53e-04)	Tok/s 10525 (23140)	Loss/tok 2.9673 (4.4394)	LR 4.000e-03
0: TRAIN [0][3070/5173]	Time 0.702 (0.610)	Data 1.24e-04 (3.53e-04)	Tok/s 33158 (23142)	Loss/tok 3.9362 (4.4371)	LR 4.000e-03
0: TRAIN [0][3080/5173]	Time 0.500 (0.610)	Data 1.18e-04 (3.52e-04)	Tok/s 10700 (23133)	Loss/tok 3.0889 (4.4351)	LR 4.000e-03
0: TRAIN [0][3090/5173]	Time 0.572 (0.610)	Data 1.24e-04 (3.51e-04)	Tok/s 18378 (23122)	Loss/tok 3.5107 (4.4330)	LR 4.000e-03
0: TRAIN [0][3100/5173]	Time 0.702 (0.610)	Data 1.28e-04 (3.50e-04)	Tok/s 33665 (23133)	Loss/tok 3.9373 (4.4307)	LR 4.000e-03
0: TRAIN [0][3110/5173]	Time 0.566 (0.610)	Data 1.20e-04 (3.50e-04)	Tok/s 18309 (23118)	Loss/tok 3.5396 (4.4287)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][3120/5173]	Time 0.632 (0.610)	Data 1.31e-04 (3.49e-04)	Tok/s 26771 (23131)	Loss/tok 3.7908 (4.4266)	LR 4.000e-03
0: TRAIN [0][3130/5173]	Time 0.567 (0.610)	Data 1.29e-04 (3.48e-04)	Tok/s 18295 (23139)	Loss/tok 3.4947 (4.4244)	LR 4.000e-03
0: TRAIN [0][3140/5173]	Time 0.573 (0.610)	Data 1.24e-04 (3.48e-04)	Tok/s 17875 (23142)	Loss/tok 3.3926 (4.4222)	LR 4.000e-03
0: TRAIN [0][3150/5173]	Time 0.702 (0.610)	Data 3.09e-04 (3.47e-04)	Tok/s 33278 (23148)	Loss/tok 4.0100 (4.4201)	LR 4.000e-03
0: TRAIN [0][3160/5173]	Time 0.576 (0.610)	Data 1.22e-04 (3.46e-04)	Tok/s 17843 (23145)	Loss/tok 3.5228 (4.4180)	LR 4.000e-03
0: TRAIN [0][3170/5173]	Time 0.575 (0.610)	Data 1.22e-04 (3.46e-04)	Tok/s 18058 (23130)	Loss/tok 3.6114 (4.4161)	LR 4.000e-03
0: TRAIN [0][3180/5173]	Time 0.569 (0.610)	Data 1.23e-04 (3.45e-04)	Tok/s 18154 (23119)	Loss/tok 3.5709 (4.4142)	LR 4.000e-03
0: TRAIN [0][3190/5173]	Time 0.564 (0.610)	Data 1.26e-04 (3.44e-04)	Tok/s 18306 (23118)	Loss/tok 3.5468 (4.4121)	LR 4.000e-03
0: TRAIN [0][3200/5173]	Time 0.631 (0.610)	Data 1.19e-04 (3.44e-04)	Tok/s 26918 (23113)	Loss/tok 3.8726 (4.4100)	LR 4.000e-03
0: TRAIN [0][3210/5173]	Time 0.702 (0.610)	Data 4.21e-04 (3.43e-04)	Tok/s 33482 (23127)	Loss/tok 3.9636 (4.4080)	LR 4.000e-03
0: TRAIN [0][3220/5173]	Time 0.573 (0.610)	Data 1.44e-04 (3.42e-04)	Tok/s 18225 (23125)	Loss/tok 3.5938 (4.4061)	LR 4.000e-03
0: TRAIN [0][3230/5173]	Time 0.564 (0.610)	Data 3.09e-04 (3.42e-04)	Tok/s 18627 (23128)	Loss/tok 3.5441 (4.4042)	LR 4.000e-03
0: TRAIN [0][3240/5173]	Time 0.561 (0.610)	Data 1.20e-04 (3.41e-04)	Tok/s 17925 (23125)	Loss/tok 3.5901 (4.4021)	LR 4.000e-03
0: TRAIN [0][3250/5173]	Time 0.564 (0.610)	Data 1.24e-04 (3.41e-04)	Tok/s 18219 (23115)	Loss/tok 3.4107 (4.4002)	LR 4.000e-03
0: TRAIN [0][3260/5173]	Time 0.560 (0.610)	Data 1.19e-04 (3.40e-04)	Tok/s 18391 (23115)	Loss/tok 3.4893 (4.3980)	LR 4.000e-03
0: TRAIN [0][3270/5173]	Time 0.564 (0.610)	Data 1.23e-04 (3.39e-04)	Tok/s 18438 (23119)	Loss/tok 3.4026 (4.3961)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][3280/5173]	Time 0.767 (0.610)	Data 1.30e-04 (3.39e-04)	Tok/s 38823 (23119)	Loss/tok 4.1233 (4.3941)	LR 4.000e-03
0: TRAIN [0][3290/5173]	Time 0.704 (0.610)	Data 1.31e-04 (3.38e-04)	Tok/s 32830 (23126)	Loss/tok 3.9142 (4.3921)	LR 4.000e-03
0: TRAIN [0][3300/5173]	Time 0.771 (0.610)	Data 1.28e-04 (3.37e-04)	Tok/s 38374 (23125)	Loss/tok 4.2640 (4.3904)	LR 4.000e-03
0: TRAIN [0][3310/5173]	Time 0.623 (0.610)	Data 1.28e-04 (3.37e-04)	Tok/s 27022 (23124)	Loss/tok 3.8108 (4.3884)	LR 4.000e-03
0: TRAIN [0][3320/5173]	Time 0.707 (0.610)	Data 1.33e-04 (3.36e-04)	Tok/s 33202 (23128)	Loss/tok 4.0179 (4.3866)	LR 4.000e-03
0: TRAIN [0][3330/5173]	Time 0.641 (0.610)	Data 1.29e-04 (3.36e-04)	Tok/s 26483 (23142)	Loss/tok 3.6585 (4.3846)	LR 4.000e-03
0: TRAIN [0][3340/5173]	Time 0.639 (0.610)	Data 1.28e-04 (3.35e-04)	Tok/s 25891 (23148)	Loss/tok 3.8766 (4.3827)	LR 4.000e-03
0: TRAIN [0][3350/5173]	Time 0.626 (0.610)	Data 1.26e-04 (3.35e-04)	Tok/s 26741 (23141)	Loss/tok 3.9113 (4.3809)	LR 4.000e-03
0: TRAIN [0][3360/5173]	Time 0.646 (0.610)	Data 1.20e-04 (3.34e-04)	Tok/s 26467 (23141)	Loss/tok 3.7609 (4.3787)	LR 4.000e-03
0: TRAIN [0][3370/5173]	Time 0.571 (0.610)	Data 1.31e-04 (3.33e-04)	Tok/s 18148 (23134)	Loss/tok 3.4912 (4.3769)	LR 4.000e-03
0: TRAIN [0][3380/5173]	Time 0.562 (0.610)	Data 1.14e-04 (3.33e-04)	Tok/s 18339 (23127)	Loss/tok 3.5372 (4.3751)	LR 4.000e-03
0: TRAIN [0][3390/5173]	Time 0.772 (0.610)	Data 1.24e-04 (3.32e-04)	Tok/s 38832 (23139)	Loss/tok 4.0810 (4.3731)	LR 4.000e-03
0: TRAIN [0][3400/5173]	Time 0.646 (0.610)	Data 1.08e-04 (3.31e-04)	Tok/s 26138 (23140)	Loss/tok 3.7140 (4.3713)	LR 4.000e-03
0: TRAIN [0][3410/5173]	Time 0.570 (0.610)	Data 1.18e-04 (3.31e-04)	Tok/s 17855 (23133)	Loss/tok 3.4337 (4.3694)	LR 4.000e-03
0: TRAIN [0][3420/5173]	Time 0.692 (0.610)	Data 1.17e-04 (3.30e-04)	Tok/s 33626 (23125)	Loss/tok 4.0150 (4.3676)	LR 4.000e-03
0: TRAIN [0][3430/5173]	Time 0.704 (0.610)	Data 1.22e-04 (3.30e-04)	Tok/s 33609 (23131)	Loss/tok 3.8672 (4.3657)	LR 4.000e-03
0: TRAIN [0][3440/5173]	Time 0.566 (0.610)	Data 1.18e-04 (3.29e-04)	Tok/s 18295 (23137)	Loss/tok 3.4615 (4.3638)	LR 4.000e-03
0: TRAIN [0][3450/5173]	Time 0.551 (0.610)	Data 1.18e-04 (3.29e-04)	Tok/s 18547 (23129)	Loss/tok 3.4451 (4.3619)	LR 4.000e-03
0: TRAIN [0][3460/5173]	Time 0.565 (0.610)	Data 3.02e-04 (3.28e-04)	Tok/s 18344 (23131)	Loss/tok 3.4756 (4.3600)	LR 4.000e-03
0: TRAIN [0][3470/5173]	Time 0.563 (0.610)	Data 1.14e-04 (3.27e-04)	Tok/s 18387 (23127)	Loss/tok 3.5743 (4.3583)	LR 4.000e-03
0: TRAIN [0][3480/5173]	Time 0.499 (0.610)	Data 1.13e-04 (3.27e-04)	Tok/s 10607 (23128)	Loss/tok 2.9651 (4.3564)	LR 4.000e-03
0: TRAIN [0][3490/5173]	Time 0.768 (0.610)	Data 1.20e-04 (3.26e-04)	Tok/s 39275 (23127)	Loss/tok 4.0976 (4.3547)	LR 4.000e-03
0: TRAIN [0][3500/5173]	Time 0.766 (0.610)	Data 1.18e-04 (3.26e-04)	Tok/s 38554 (23135)	Loss/tok 4.1028 (4.3529)	LR 4.000e-03
0: TRAIN [0][3510/5173]	Time 0.572 (0.610)	Data 1.17e-04 (3.25e-04)	Tok/s 17756 (23124)	Loss/tok 3.4198 (4.3512)	LR 4.000e-03
0: TRAIN [0][3520/5173]	Time 0.579 (0.610)	Data 1.20e-04 (3.24e-04)	Tok/s 17896 (23125)	Loss/tok 3.4113 (4.3494)	LR 4.000e-03
0: TRAIN [0][3530/5173]	Time 0.640 (0.610)	Data 1.10e-04 (3.24e-04)	Tok/s 25564 (23133)	Loss/tok 3.9096 (4.3477)	LR 4.000e-03
0: TRAIN [0][3540/5173]	Time 0.640 (0.610)	Data 1.18e-04 (3.23e-04)	Tok/s 26113 (23134)	Loss/tok 3.8308 (4.3459)	LR 4.000e-03
0: TRAIN [0][3550/5173]	Time 0.566 (0.610)	Data 1.14e-04 (3.23e-04)	Tok/s 17651 (23127)	Loss/tok 3.4907 (4.3444)	LR 4.000e-03
0: TRAIN [0][3560/5173]	Time 0.564 (0.610)	Data 1.33e-04 (3.22e-04)	Tok/s 18109 (23122)	Loss/tok 3.5022 (4.3426)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3570/5173]	Time 0.690 (0.610)	Data 1.23e-04 (3.22e-04)	Tok/s 33661 (23120)	Loss/tok 3.9259 (4.3410)	LR 4.000e-03
0: TRAIN [0][3580/5173]	Time 0.701 (0.610)	Data 1.38e-04 (3.21e-04)	Tok/s 33347 (23126)	Loss/tok 3.9800 (4.3393)	LR 4.000e-03
0: TRAIN [0][3590/5173]	Time 0.775 (0.610)	Data 1.39e-04 (3.20e-04)	Tok/s 37989 (23135)	Loss/tok 4.2073 (4.3377)	LR 4.000e-03
0: TRAIN [0][3600/5173]	Time 0.696 (0.610)	Data 1.30e-04 (3.20e-04)	Tok/s 33282 (23140)	Loss/tok 3.8425 (4.3358)	LR 4.000e-03
0: TRAIN [0][3610/5173]	Time 0.642 (0.610)	Data 1.22e-04 (3.19e-04)	Tok/s 26029 (23136)	Loss/tok 3.7611 (4.3341)	LR 4.000e-03
0: TRAIN [0][3620/5173]	Time 0.559 (0.610)	Data 1.19e-04 (3.19e-04)	Tok/s 17762 (23131)	Loss/tok 3.4493 (4.3324)	LR 4.000e-03
0: TRAIN [0][3630/5173]	Time 0.568 (0.610)	Data 1.22e-04 (3.18e-04)	Tok/s 18256 (23132)	Loss/tok 3.5104 (4.3308)	LR 4.000e-03
0: TRAIN [0][3640/5173]	Time 0.646 (0.610)	Data 1.22e-04 (3.18e-04)	Tok/s 26101 (23123)	Loss/tok 3.7417 (4.3292)	LR 4.000e-03
0: TRAIN [0][3650/5173]	Time 0.632 (0.610)	Data 1.19e-04 (3.17e-04)	Tok/s 26414 (23114)	Loss/tok 3.6733 (4.3276)	LR 4.000e-03
0: TRAIN [0][3660/5173]	Time 0.637 (0.610)	Data 1.19e-04 (3.17e-04)	Tok/s 26536 (23109)	Loss/tok 3.7160 (4.3261)	LR 4.000e-03
0: TRAIN [0][3670/5173]	Time 0.566 (0.610)	Data 1.27e-04 (3.16e-04)	Tok/s 18312 (23124)	Loss/tok 3.4631 (4.3246)	LR 4.000e-03
0: TRAIN [0][3680/5173]	Time 0.761 (0.610)	Data 1.24e-04 (3.16e-04)	Tok/s 39465 (23139)	Loss/tok 4.0767 (4.3229)	LR 4.000e-03
0: TRAIN [0][3690/5173]	Time 0.633 (0.610)	Data 1.17e-04 (3.15e-04)	Tok/s 26475 (23138)	Loss/tok 3.7604 (4.3213)	LR 4.000e-03
0: TRAIN [0][3700/5173]	Time 0.564 (0.610)	Data 1.19e-04 (3.15e-04)	Tok/s 18110 (23138)	Loss/tok 3.5078 (4.3197)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3710/5173]	Time 0.638 (0.610)	Data 1.29e-04 (3.14e-04)	Tok/s 26268 (23143)	Loss/tok 3.7398 (4.3181)	LR 4.000e-03
0: TRAIN [0][3720/5173]	Time 0.572 (0.610)	Data 1.22e-04 (3.14e-04)	Tok/s 18034 (23133)	Loss/tok 3.4470 (4.3166)	LR 4.000e-03
0: TRAIN [0][3730/5173]	Time 0.568 (0.610)	Data 1.20e-04 (3.13e-04)	Tok/s 18314 (23135)	Loss/tok 3.5750 (4.3150)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][3740/5173]	Time 0.643 (0.610)	Data 1.19e-04 (3.13e-04)	Tok/s 25945 (23142)	Loss/tok 3.6364 (4.3135)	LR 4.000e-03
0: TRAIN [0][3750/5173]	Time 0.571 (0.610)	Data 1.18e-04 (3.12e-04)	Tok/s 18071 (23146)	Loss/tok 3.6164 (4.3119)	LR 4.000e-03
0: TRAIN [0][3760/5173]	Time 0.506 (0.610)	Data 3.02e-04 (3.12e-04)	Tok/s 10192 (23143)	Loss/tok 2.8917 (4.3105)	LR 4.000e-03
0: TRAIN [0][3770/5173]	Time 0.635 (0.610)	Data 1.25e-04 (3.11e-04)	Tok/s 26417 (23151)	Loss/tok 3.7259 (4.3089)	LR 4.000e-03
0: TRAIN [0][3780/5173]	Time 0.631 (0.610)	Data 1.26e-04 (3.11e-04)	Tok/s 26827 (23155)	Loss/tok 3.8005 (4.3073)	LR 4.000e-03
0: TRAIN [0][3790/5173]	Time 0.567 (0.610)	Data 1.21e-04 (3.10e-04)	Tok/s 17941 (23149)	Loss/tok 3.7000 (4.3058)	LR 4.000e-03
0: TRAIN [0][3800/5173]	Time 0.704 (0.610)	Data 1.25e-04 (3.10e-04)	Tok/s 33267 (23144)	Loss/tok 3.8211 (4.3043)	LR 4.000e-03
0: TRAIN [0][3810/5173]	Time 0.638 (0.610)	Data 1.29e-04 (3.09e-04)	Tok/s 26206 (23155)	Loss/tok 3.6648 (4.3028)	LR 4.000e-03
0: TRAIN [0][3820/5173]	Time 0.569 (0.610)	Data 2.75e-04 (3.09e-04)	Tok/s 18434 (23144)	Loss/tok 3.5291 (4.3014)	LR 4.000e-03
0: TRAIN [0][3830/5173]	Time 0.706 (0.610)	Data 1.23e-04 (3.09e-04)	Tok/s 33238 (23137)	Loss/tok 3.9102 (4.3000)	LR 4.000e-03
0: TRAIN [0][3840/5173]	Time 0.572 (0.610)	Data 1.23e-04 (3.08e-04)	Tok/s 17923 (23132)	Loss/tok 3.5124 (4.2985)	LR 4.000e-03
0: TRAIN [0][3850/5173]	Time 0.562 (0.610)	Data 1.17e-04 (3.08e-04)	Tok/s 18188 (23129)	Loss/tok 3.4899 (4.2971)	LR 4.000e-03
0: TRAIN [0][3860/5173]	Time 0.569 (0.610)	Data 1.22e-04 (3.07e-04)	Tok/s 17871 (23140)	Loss/tok 3.5498 (4.2955)	LR 4.000e-03
0: TRAIN [0][3870/5173]	Time 0.571 (0.610)	Data 1.17e-04 (3.07e-04)	Tok/s 18060 (23133)	Loss/tok 3.5099 (4.2941)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][3880/5173]	Time 0.501 (0.610)	Data 1.25e-04 (3.06e-04)	Tok/s 10744 (23133)	Loss/tok 2.8635 (4.2927)	LR 4.000e-03
0: TRAIN [0][3890/5173]	Time 0.570 (0.610)	Data 1.22e-04 (3.06e-04)	Tok/s 17452 (23141)	Loss/tok 3.5074 (4.2914)	LR 4.000e-03
0: TRAIN [0][3900/5173]	Time 0.572 (0.610)	Data 1.54e-04 (3.05e-04)	Tok/s 17872 (23155)	Loss/tok 3.5316 (4.2900)	LR 4.000e-03
0: TRAIN [0][3910/5173]	Time 0.629 (0.610)	Data 1.22e-04 (3.05e-04)	Tok/s 26592 (23157)	Loss/tok 3.8139 (4.2885)	LR 4.000e-03
0: TRAIN [0][3920/5173]	Time 0.625 (0.610)	Data 1.21e-04 (3.04e-04)	Tok/s 26956 (23153)	Loss/tok 3.8363 (4.2870)	LR 4.000e-03
0: TRAIN [0][3930/5173]	Time 0.498 (0.610)	Data 1.20e-04 (3.04e-04)	Tok/s 10719 (23141)	Loss/tok 2.9034 (4.2855)	LR 4.000e-03
0: TRAIN [0][3940/5173]	Time 0.576 (0.610)	Data 1.19e-04 (3.04e-04)	Tok/s 18269 (23143)	Loss/tok 3.5259 (4.2839)	LR 4.000e-03
0: TRAIN [0][3950/5173]	Time 0.506 (0.610)	Data 1.19e-04 (3.03e-04)	Tok/s 10423 (23136)	Loss/tok 2.9111 (4.2825)	LR 4.000e-03
0: TRAIN [0][3960/5173]	Time 0.642 (0.610)	Data 1.22e-04 (3.03e-04)	Tok/s 26377 (23125)	Loss/tok 3.7231 (4.2812)	LR 4.000e-03
0: TRAIN [0][3970/5173]	Time 0.571 (0.610)	Data 1.22e-04 (3.02e-04)	Tok/s 18354 (23113)	Loss/tok 3.4837 (4.2798)	LR 4.000e-03
0: TRAIN [0][3980/5173]	Time 0.763 (0.610)	Data 1.17e-04 (3.02e-04)	Tok/s 39181 (23115)	Loss/tok 4.1677 (4.2784)	LR 4.000e-03
0: TRAIN [0][3990/5173]	Time 0.646 (0.610)	Data 1.28e-04 (3.01e-04)	Tok/s 25955 (23126)	Loss/tok 3.5458 (4.2767)	LR 4.000e-03
0: TRAIN [0][4000/5173]	Time 0.769 (0.610)	Data 1.23e-04 (3.01e-04)	Tok/s 38459 (23126)	Loss/tok 4.1380 (4.2753)	LR 4.000e-03
0: TRAIN [0][4010/5173]	Time 0.633 (0.610)	Data 1.25e-04 (3.00e-04)	Tok/s 26924 (23118)	Loss/tok 3.7326 (4.2739)	LR 4.000e-03
0: TRAIN [0][4020/5173]	Time 0.566 (0.610)	Data 1.24e-04 (3.00e-04)	Tok/s 18245 (23116)	Loss/tok 3.5206 (4.2724)	LR 4.000e-03
0: TRAIN [0][4030/5173]	Time 0.574 (0.610)	Data 1.18e-04 (3.00e-04)	Tok/s 17772 (23111)	Loss/tok 3.3501 (4.2710)	LR 4.000e-03
0: TRAIN [0][4040/5173]	Time 0.643 (0.610)	Data 1.17e-04 (2.99e-04)	Tok/s 26552 (23114)	Loss/tok 3.7280 (4.2697)	LR 4.000e-03
0: TRAIN [0][4050/5173]	Time 0.647 (0.610)	Data 1.29e-04 (2.99e-04)	Tok/s 26241 (23109)	Loss/tok 3.6930 (4.2683)	LR 4.000e-03
0: TRAIN [0][4060/5173]	Time 0.565 (0.610)	Data 1.16e-04 (2.98e-04)	Tok/s 18197 (23121)	Loss/tok 3.4520 (4.2667)	LR 4.000e-03
0: TRAIN [0][4070/5173]	Time 0.642 (0.610)	Data 1.21e-04 (2.98e-04)	Tok/s 25725 (23122)	Loss/tok 3.7579 (4.2653)	LR 4.000e-03
0: TRAIN [0][4080/5173]	Time 0.572 (0.610)	Data 1.17e-04 (2.97e-04)	Tok/s 17940 (23116)	Loss/tok 3.4294 (4.2640)	LR 4.000e-03
0: TRAIN [0][4090/5173]	Time 0.643 (0.610)	Data 1.21e-04 (2.97e-04)	Tok/s 25930 (23123)	Loss/tok 3.6030 (4.2626)	LR 4.000e-03
0: TRAIN [0][4100/5173]	Time 0.642 (0.610)	Data 1.15e-04 (2.97e-04)	Tok/s 26130 (23131)	Loss/tok 3.8341 (4.2612)	LR 4.000e-03
0: TRAIN [0][4110/5173]	Time 0.569 (0.610)	Data 1.21e-04 (2.96e-04)	Tok/s 17862 (23133)	Loss/tok 3.4959 (4.2598)	LR 4.000e-03
0: TRAIN [0][4120/5173]	Time 0.695 (0.610)	Data 1.21e-04 (2.96e-04)	Tok/s 33297 (23134)	Loss/tok 3.9547 (4.2585)	LR 4.000e-03
0: TRAIN [0][4130/5173]	Time 0.568 (0.610)	Data 1.29e-04 (2.96e-04)	Tok/s 18143 (23138)	Loss/tok 3.4906 (4.2570)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][4140/5173]	Time 0.632 (0.610)	Data 1.31e-04 (2.95e-04)	Tok/s 26511 (23148)	Loss/tok 3.7222 (4.2556)	LR 4.000e-03
0: TRAIN [0][4150/5173]	Time 0.561 (0.610)	Data 2.88e-04 (2.95e-04)	Tok/s 18438 (23142)	Loss/tok 3.4084 (4.2542)	LR 4.000e-03
0: TRAIN [0][4160/5173]	Time 0.566 (0.610)	Data 1.10e-04 (2.94e-04)	Tok/s 18274 (23143)	Loss/tok 3.4714 (4.2529)	LR 4.000e-03
0: TRAIN [0][4170/5173]	Time 0.633 (0.610)	Data 1.18e-04 (2.94e-04)	Tok/s 26405 (23139)	Loss/tok 3.7724 (4.2515)	LR 4.000e-03
0: TRAIN [0][4180/5173]	Time 0.565 (0.610)	Data 1.22e-04 (2.94e-04)	Tok/s 18463 (23135)	Loss/tok 3.5049 (4.2503)	LR 4.000e-03
0: TRAIN [0][4190/5173]	Time 0.560 (0.610)	Data 1.20e-04 (2.93e-04)	Tok/s 18318 (23125)	Loss/tok 3.4067 (4.2489)	LR 4.000e-03
0: TRAIN [0][4200/5173]	Time 0.633 (0.610)	Data 1.26e-04 (2.93e-04)	Tok/s 26648 (23129)	Loss/tok 3.6787 (4.2475)	LR 4.000e-03
0: TRAIN [0][4210/5173]	Time 0.504 (0.610)	Data 1.19e-04 (2.93e-04)	Tok/s 10402 (23124)	Loss/tok 2.9341 (4.2463)	LR 4.000e-03
0: TRAIN [0][4220/5173]	Time 0.565 (0.610)	Data 1.15e-04 (2.92e-04)	Tok/s 18444 (23121)	Loss/tok 3.4629 (4.2449)	LR 4.000e-03
0: TRAIN [0][4230/5173]	Time 0.703 (0.610)	Data 1.21e-04 (2.92e-04)	Tok/s 33059 (23123)	Loss/tok 3.8196 (4.2435)	LR 4.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][4240/5173]	Time 0.637 (0.610)	Data 1.38e-04 (2.91e-04)	Tok/s 26288 (23138)	Loss/tok 3.7297 (4.2422)	LR 4.000e-03
0: TRAIN [0][4250/5173]	Time 0.563 (0.610)	Data 1.16e-04 (2.91e-04)	Tok/s 18794 (23132)	Loss/tok 3.3603 (4.2409)	LR 4.000e-03
0: TRAIN [0][4260/5173]	Time 0.763 (0.610)	Data 1.15e-04 (2.91e-04)	Tok/s 39091 (23148)	Loss/tok 4.0910 (4.2397)	LR 4.000e-03
0: TRAIN [0][4270/5173]	Time 0.640 (0.610)	Data 1.18e-04 (2.90e-04)	Tok/s 26244 (23145)	Loss/tok 3.6708 (4.2384)	LR 4.000e-03
0: TRAIN [0][4280/5173]	Time 0.638 (0.610)	Data 1.22e-04 (2.90e-04)	Tok/s 25967 (23139)	Loss/tok 3.6676 (4.2371)	LR 4.000e-03
0: TRAIN [0][4290/5173]	Time 0.566 (0.610)	Data 1.22e-04 (2.89e-04)	Tok/s 18313 (23138)	Loss/tok 3.6099 (4.2358)	LR 4.000e-03
0: TRAIN [0][4300/5173]	Time 0.568 (0.610)	Data 1.20e-04 (2.89e-04)	Tok/s 18240 (23125)	Loss/tok 3.5575 (4.2346)	LR 4.000e-03
0: TRAIN [0][4310/5173]	Time 0.506 (0.610)	Data 1.20e-04 (2.89e-04)	Tok/s 10334 (23121)	Loss/tok 3.0186 (4.2333)	LR 4.000e-03
0: TRAIN [0][4320/5173]	Time 0.702 (0.610)	Data 1.26e-04 (2.88e-04)	Tok/s 33136 (23130)	Loss/tok 3.8467 (4.2320)	LR 4.000e-03
0: TRAIN [0][4330/5173]	Time 0.571 (0.610)	Data 1.21e-04 (2.88e-04)	Tok/s 17886 (23128)	Loss/tok 3.4526 (4.2308)	LR 2.000e-03
0: TRAIN [0][4340/5173]	Time 0.636 (0.610)	Data 3.03e-04 (2.88e-04)	Tok/s 26449 (23127)	Loss/tok 3.5557 (4.2295)	LR 2.000e-03
0: TRAIN [0][4350/5173]	Time 0.568 (0.610)	Data 1.23e-04 (2.87e-04)	Tok/s 18632 (23120)	Loss/tok 3.6200 (4.2284)	LR 2.000e-03
0: TRAIN [0][4360/5173]	Time 0.635 (0.610)	Data 1.19e-04 (2.87e-04)	Tok/s 26397 (23122)	Loss/tok 3.6675 (4.2270)	LR 2.000e-03
0: TRAIN [0][4370/5173]	Time 0.758 (0.610)	Data 1.22e-04 (2.87e-04)	Tok/s 39335 (23124)	Loss/tok 3.9149 (4.2257)	LR 2.000e-03
0: TRAIN [0][4380/5173]	Time 0.571 (0.610)	Data 1.20e-04 (2.86e-04)	Tok/s 18222 (23125)	Loss/tok 3.4304 (4.2244)	LR 2.000e-03
0: TRAIN [0][4390/5173]	Time 0.568 (0.610)	Data 1.23e-04 (2.86e-04)	Tok/s 18121 (23120)	Loss/tok 3.4146 (4.2230)	LR 2.000e-03
0: TRAIN [0][4400/5173]	Time 0.504 (0.610)	Data 1.21e-04 (2.86e-04)	Tok/s 10418 (23115)	Loss/tok 2.8703 (4.2218)	LR 2.000e-03
0: TRAIN [0][4410/5173]	Time 0.567 (0.610)	Data 1.22e-04 (2.85e-04)	Tok/s 18278 (23114)	Loss/tok 3.4196 (4.2205)	LR 2.000e-03
0: TRAIN [0][4420/5173]	Time 0.640 (0.610)	Data 1.17e-04 (2.85e-04)	Tok/s 26685 (23117)	Loss/tok 3.7375 (4.2191)	LR 2.000e-03
0: TRAIN [0][4430/5173]	Time 0.769 (0.610)	Data 1.21e-04 (2.84e-04)	Tok/s 38357 (23119)	Loss/tok 4.0935 (4.2178)	LR 2.000e-03
0: TRAIN [0][4440/5173]	Time 0.571 (0.610)	Data 1.28e-04 (2.84e-04)	Tok/s 18025 (23125)	Loss/tok 3.5593 (4.2166)	LR 2.000e-03
0: TRAIN [0][4450/5173]	Time 0.511 (0.610)	Data 1.22e-04 (2.84e-04)	Tok/s 10205 (23118)	Loss/tok 2.9946 (4.2152)	LR 2.000e-03
0: TRAIN [0][4460/5173]	Time 0.700 (0.610)	Data 1.22e-04 (2.83e-04)	Tok/s 33510 (23128)	Loss/tok 3.8186 (4.2138)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][4470/5173]	Time 0.705 (0.610)	Data 1.24e-04 (2.83e-04)	Tok/s 33033 (23136)	Loss/tok 3.8196 (4.2126)	LR 2.000e-03
0: TRAIN [0][4480/5173]	Time 0.565 (0.610)	Data 1.24e-04 (2.83e-04)	Tok/s 18292 (23139)	Loss/tok 3.4283 (4.2112)	LR 2.000e-03
0: TRAIN [0][4490/5173]	Time 0.770 (0.610)	Data 1.17e-04 (2.82e-04)	Tok/s 39160 (23140)	Loss/tok 3.9604 (4.2100)	LR 2.000e-03
0: TRAIN [0][4500/5173]	Time 0.636 (0.610)	Data 1.13e-04 (2.82e-04)	Tok/s 26774 (23142)	Loss/tok 3.4942 (4.2086)	LR 2.000e-03
0: TRAIN [0][4510/5173]	Time 0.768 (0.610)	Data 1.24e-04 (2.82e-04)	Tok/s 38733 (23149)	Loss/tok 4.0319 (4.2073)	LR 2.000e-03
0: TRAIN [0][4520/5173]	Time 0.504 (0.610)	Data 1.25e-04 (2.81e-04)	Tok/s 10717 (23148)	Loss/tok 2.8632 (4.2060)	LR 2.000e-03
0: TRAIN [0][4530/5173]	Time 0.500 (0.610)	Data 1.23e-04 (2.81e-04)	Tok/s 10465 (23147)	Loss/tok 2.9505 (4.2048)	LR 2.000e-03
0: TRAIN [0][4540/5173]	Time 0.564 (0.610)	Data 1.22e-04 (2.81e-04)	Tok/s 17857 (23155)	Loss/tok 3.2858 (4.2034)	LR 2.000e-03
0: TRAIN [0][4550/5173]	Time 0.770 (0.610)	Data 1.19e-04 (2.80e-04)	Tok/s 38423 (23152)	Loss/tok 3.9923 (4.2021)	LR 2.000e-03
0: TRAIN [0][4560/5173]	Time 0.637 (0.611)	Data 1.36e-04 (2.80e-04)	Tok/s 26301 (23161)	Loss/tok 3.6490 (4.2010)	LR 2.000e-03
0: TRAIN [0][4570/5173]	Time 0.561 (0.611)	Data 1.22e-04 (2.80e-04)	Tok/s 18602 (23162)	Loss/tok 3.5153 (4.1997)	LR 2.000e-03
0: TRAIN [0][4580/5173]	Time 0.701 (0.611)	Data 1.24e-04 (2.79e-04)	Tok/s 33341 (23162)	Loss/tok 3.8209 (4.1984)	LR 2.000e-03
0: TRAIN [0][4590/5173]	Time 0.704 (0.610)	Data 1.13e-04 (2.79e-04)	Tok/s 32908 (23154)	Loss/tok 3.8573 (4.1972)	LR 2.000e-03
0: TRAIN [0][4600/5173]	Time 0.501 (0.610)	Data 1.21e-04 (2.79e-04)	Tok/s 10453 (23155)	Loss/tok 2.8309 (4.1960)	LR 2.000e-03
0: TRAIN [0][4610/5173]	Time 0.566 (0.610)	Data 3.12e-04 (2.78e-04)	Tok/s 18063 (23145)	Loss/tok 3.4249 (4.1950)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][4620/5173]	Time 0.695 (0.610)	Data 1.27e-04 (2.78e-04)	Tok/s 33799 (23156)	Loss/tok 3.8353 (4.1937)	LR 2.000e-03
0: TRAIN [0][4630/5173]	Time 0.645 (0.611)	Data 1.20e-04 (2.78e-04)	Tok/s 25959 (23159)	Loss/tok 3.6111 (4.1925)	LR 2.000e-03
0: TRAIN [0][4640/5173]	Time 0.765 (0.610)	Data 1.22e-04 (2.78e-04)	Tok/s 39288 (23155)	Loss/tok 4.1246 (4.1914)	LR 2.000e-03
0: TRAIN [0][4650/5173]	Time 0.571 (0.611)	Data 1.31e-04 (2.77e-04)	Tok/s 18291 (23157)	Loss/tok 3.2666 (4.1902)	LR 2.000e-03
0: TRAIN [0][4660/5173]	Time 0.764 (0.610)	Data 1.27e-04 (2.77e-04)	Tok/s 38681 (23156)	Loss/tok 4.0483 (4.1891)	LR 2.000e-03
0: TRAIN [0][4670/5173]	Time 0.569 (0.610)	Data 1.24e-04 (2.77e-04)	Tok/s 17587 (23154)	Loss/tok 3.3611 (4.1878)	LR 2.000e-03
0: TRAIN [0][4680/5173]	Time 0.581 (0.611)	Data 1.21e-04 (2.76e-04)	Tok/s 17376 (23157)	Loss/tok 3.3519 (4.1866)	LR 2.000e-03
0: TRAIN [0][4690/5173]	Time 0.622 (0.610)	Data 1.22e-04 (2.76e-04)	Tok/s 27106 (23154)	Loss/tok 3.6359 (4.1854)	LR 2.000e-03
0: TRAIN [0][4700/5173]	Time 0.560 (0.611)	Data 1.21e-04 (2.76e-04)	Tok/s 18661 (23162)	Loss/tok 3.4043 (4.1842)	LR 2.000e-03
0: TRAIN [0][4710/5173]	Time 0.496 (0.610)	Data 1.25e-04 (2.75e-04)	Tok/s 10452 (23152)	Loss/tok 2.9237 (4.1830)	LR 2.000e-03
0: TRAIN [0][4720/5173]	Time 0.631 (0.610)	Data 1.19e-04 (2.75e-04)	Tok/s 26491 (23155)	Loss/tok 3.6284 (4.1817)	LR 2.000e-03
0: TRAIN [0][4730/5173]	Time 0.633 (0.610)	Data 1.23e-04 (2.75e-04)	Tok/s 26411 (23155)	Loss/tok 3.6463 (4.1804)	LR 2.000e-03
0: TRAIN [0][4740/5173]	Time 0.621 (0.610)	Data 1.14e-04 (2.74e-04)	Tok/s 27518 (23154)	Loss/tok 3.5229 (4.1793)	LR 2.000e-03
0: TRAIN [0][4750/5173]	Time 0.640 (0.610)	Data 1.20e-04 (2.74e-04)	Tok/s 26195 (23151)	Loss/tok 3.7301 (4.1780)	LR 2.000e-03
0: TRAIN [0][4760/5173]	Time 0.571 (0.610)	Data 1.22e-04 (2.74e-04)	Tok/s 17992 (23146)	Loss/tok 3.5281 (4.1768)	LR 2.000e-03
0: TRAIN [0][4770/5173]	Time 0.762 (0.610)	Data 1.22e-04 (2.74e-04)	Tok/s 38985 (23146)	Loss/tok 3.9702 (4.1757)	LR 2.000e-03
0: TRAIN [0][4780/5173]	Time 0.567 (0.610)	Data 1.32e-04 (2.73e-04)	Tok/s 18468 (23138)	Loss/tok 3.3429 (4.1746)	LR 2.000e-03
0: TRAIN [0][4790/5173]	Time 0.704 (0.610)	Data 1.24e-04 (2.73e-04)	Tok/s 32869 (23132)	Loss/tok 3.9508 (4.1735)	LR 2.000e-03
0: TRAIN [0][4800/5173]	Time 0.626 (0.610)	Data 1.25e-04 (2.73e-04)	Tok/s 26439 (23134)	Loss/tok 3.5954 (4.1721)	LR 2.000e-03
0: TRAIN [0][4810/5173]	Time 0.644 (0.610)	Data 1.20e-04 (2.72e-04)	Tok/s 26196 (23133)	Loss/tok 3.6605 (4.1709)	LR 2.000e-03
0: TRAIN [0][4820/5173]	Time 0.499 (0.610)	Data 1.37e-04 (2.72e-04)	Tok/s 10457 (23131)	Loss/tok 2.7931 (4.1698)	LR 2.000e-03
0: TRAIN [0][4830/5173]	Time 0.637 (0.610)	Data 1.19e-04 (2.72e-04)	Tok/s 26433 (23134)	Loss/tok 3.6108 (4.1686)	LR 2.000e-03
0: TRAIN [0][4840/5173]	Time 0.502 (0.610)	Data 1.24e-04 (2.71e-04)	Tok/s 10544 (23123)	Loss/tok 2.7880 (4.1674)	LR 2.000e-03
0: TRAIN [0][4850/5173]	Time 0.700 (0.610)	Data 1.29e-04 (2.71e-04)	Tok/s 33104 (23117)	Loss/tok 3.8596 (4.1662)	LR 2.000e-03
0: TRAIN [0][4860/5173]	Time 0.703 (0.610)	Data 1.16e-04 (2.71e-04)	Tok/s 33723 (23118)	Loss/tok 3.6403 (4.1650)	LR 2.000e-03
0: TRAIN [0][4870/5173]	Time 0.567 (0.610)	Data 1.20e-04 (2.70e-04)	Tok/s 18078 (23115)	Loss/tok 3.3946 (4.1638)	LR 2.000e-03
0: TRAIN [0][4880/5173]	Time 0.499 (0.610)	Data 1.25e-04 (2.70e-04)	Tok/s 10499 (23114)	Loss/tok 2.8787 (4.1626)	LR 2.000e-03
0: TRAIN [0][4890/5173]	Time 0.694 (0.610)	Data 1.21e-04 (2.70e-04)	Tok/s 33648 (23119)	Loss/tok 3.7461 (4.1615)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][4900/5173]	Time 0.637 (0.610)	Data 1.20e-04 (2.70e-04)	Tok/s 26295 (23123)	Loss/tok 3.7564 (4.1603)	LR 2.000e-03
0: TRAIN [0][4910/5173]	Time 0.562 (0.610)	Data 1.18e-04 (2.69e-04)	Tok/s 18014 (23116)	Loss/tok 3.4075 (4.1592)	LR 2.000e-03
0: TRAIN [0][4920/5173]	Time 0.631 (0.610)	Data 1.11e-04 (2.69e-04)	Tok/s 26749 (23114)	Loss/tok 3.4945 (4.1580)	LR 2.000e-03
0: TRAIN [0][4930/5173]	Time 0.642 (0.610)	Data 1.30e-04 (2.69e-04)	Tok/s 26095 (23115)	Loss/tok 3.5015 (4.1568)	LR 2.000e-03
0: TRAIN [0][4940/5173]	Time 0.770 (0.610)	Data 1.21e-04 (2.69e-04)	Tok/s 39121 (23118)	Loss/tok 4.0484 (4.1558)	LR 2.000e-03
0: TRAIN [0][4950/5173]	Time 0.635 (0.610)	Data 1.22e-04 (2.68e-04)	Tok/s 26663 (23117)	Loss/tok 3.5526 (4.1545)	LR 2.000e-03
0: TRAIN [0][4960/5173]	Time 0.583 (0.610)	Data 1.21e-04 (2.68e-04)	Tok/s 17986 (23115)	Loss/tok 3.3590 (4.1535)	LR 2.000e-03
0: TRAIN [0][4970/5173]	Time 0.629 (0.610)	Data 1.20e-04 (2.68e-04)	Tok/s 26431 (23119)	Loss/tok 3.5743 (4.1522)	LR 2.000e-03
0: TRAIN [0][4980/5173]	Time 0.558 (0.610)	Data 1.19e-04 (2.68e-04)	Tok/s 18648 (23120)	Loss/tok 3.2933 (4.1509)	LR 2.000e-03
0: TRAIN [0][4990/5173]	Time 0.562 (0.610)	Data 1.10e-04 (2.67e-04)	Tok/s 17963 (23117)	Loss/tok 3.3752 (4.1497)	LR 2.000e-03
0: TRAIN [0][5000/5173]	Time 0.777 (0.610)	Data 1.21e-04 (2.67e-04)	Tok/s 38377 (23123)	Loss/tok 3.9735 (4.1487)	LR 2.000e-03
0: TRAIN [0][5010/5173]	Time 0.567 (0.610)	Data 1.22e-04 (2.67e-04)	Tok/s 18545 (23115)	Loss/tok 3.3958 (4.1476)	LR 2.000e-03
0: TRAIN [0][5020/5173]	Time 0.572 (0.610)	Data 1.21e-04 (2.66e-04)	Tok/s 18268 (23119)	Loss/tok 3.4117 (4.1465)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][5030/5173]	Time 0.767 (0.610)	Data 1.23e-04 (2.66e-04)	Tok/s 38784 (23124)	Loss/tok 3.9521 (4.1454)	LR 2.000e-03
0: TRAIN [0][5040/5173]	Time 0.570 (0.610)	Data 1.23e-04 (2.66e-04)	Tok/s 17702 (23128)	Loss/tok 3.2692 (4.1441)	LR 2.000e-03
0: TRAIN [0][5050/5173]	Time 0.774 (0.610)	Data 1.29e-04 (2.66e-04)	Tok/s 38870 (23125)	Loss/tok 4.0046 (4.1431)	LR 2.000e-03
0: TRAIN [0][5060/5173]	Time 0.500 (0.610)	Data 1.23e-04 (2.65e-04)	Tok/s 10706 (23123)	Loss/tok 2.8781 (4.1419)	LR 2.000e-03
0: TRAIN [0][5070/5173]	Time 0.570 (0.610)	Data 1.21e-04 (2.65e-04)	Tok/s 18356 (23128)	Loss/tok 3.3077 (4.1408)	LR 2.000e-03
0: TRAIN [0][5080/5173]	Time 0.636 (0.610)	Data 1.20e-04 (2.65e-04)	Tok/s 26639 (23131)	Loss/tok 3.6149 (4.1397)	LR 2.000e-03
0: TRAIN [0][5090/5173]	Time 0.564 (0.610)	Data 1.20e-04 (2.65e-04)	Tok/s 18225 (23126)	Loss/tok 3.3452 (4.1386)	LR 2.000e-03
0: TRAIN [0][5100/5173]	Time 0.566 (0.610)	Data 1.18e-04 (2.64e-04)	Tok/s 18616 (23125)	Loss/tok 3.3169 (4.1375)	LR 2.000e-03
0: TRAIN [0][5110/5173]	Time 0.762 (0.610)	Data 1.24e-04 (2.64e-04)	Tok/s 38584 (23126)	Loss/tok 4.0783 (4.1365)	LR 2.000e-03
0: TRAIN [0][5120/5173]	Time 0.501 (0.610)	Data 1.23e-04 (2.64e-04)	Tok/s 10503 (23117)	Loss/tok 2.8099 (4.1354)	LR 2.000e-03
0: TRAIN [0][5130/5173]	Time 0.642 (0.610)	Data 1.17e-04 (2.63e-04)	Tok/s 26160 (23122)	Loss/tok 3.5869 (4.1343)	LR 2.000e-03
0: TRAIN [0][5140/5173]	Time 0.641 (0.610)	Data 1.24e-04 (2.63e-04)	Tok/s 26414 (23125)	Loss/tok 3.5416 (4.1331)	LR 1.000e-03
0: TRAIN [0][5150/5173]	Time 0.771 (0.610)	Data 1.24e-04 (2.63e-04)	Tok/s 38662 (23128)	Loss/tok 3.9562 (4.1320)	LR 1.000e-03
0: TRAIN [0][5160/5173]	Time 0.562 (0.610)	Data 1.17e-04 (2.63e-04)	Tok/s 18594 (23134)	Loss/tok 3.3872 (4.1308)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][5170/5173]	Time 0.571 (0.610)	Data 1.24e-04 (2.62e-04)	Tok/s 18111 (23133)	Loss/tok 3.2843 (4.1297)	LR 1.000e-03
:::MLL 1585768505.619 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 525}}
:::MLL 1585768505.620 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [0][0/8]	Time 0.748 (0.748)	Decoder iters 149.0 (149.0)	Tok/s 21709 (21709)
0: Running moses detokenizer
0: BLEU(score=19.215596476307773, counts=[33639, 15161, 7962, 4369], totals=[64402, 61399, 58397, 55401], precisions=[52.23284991149343, 24.692584569781268, 13.634262034008596, 7.88613923936391], bp=0.9957545114183353, sys_len=64402, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1585768511.715 eval_accuracy: {"value": 19.22, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 536}}
:::MLL 1585768511.715 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 0	Training Loss: 4.1296	Test BLEU: 19.22
0: Performance: Epoch: 0	Training: 69405 Tok/s
0: Finished epoch 0
:::MLL 1585768511.716 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 558}}
:::MLL 1585768511.716 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1585768511.716 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 515}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 2643407227
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][0/5173]	Time 1.134 (1.134)	Data 5.54e-01 (5.54e-01)	Tok/s 8989 (8989)	Loss/tok 3.3322 (3.3322)	LR 1.000e-03
0: TRAIN [1][10/5173]	Time 0.695 (0.685)	Data 1.40e-04 (5.06e-02)	Tok/s 33390 (24827)	Loss/tok 3.7629 (3.6045)	LR 1.000e-03
0: TRAIN [1][20/5173]	Time 0.634 (0.638)	Data 1.26e-04 (2.66e-02)	Tok/s 26618 (22882)	Loss/tok 3.6032 (3.5259)	LR 1.000e-03
0: TRAIN [1][30/5173]	Time 0.647 (0.635)	Data 1.29e-04 (1.80e-02)	Tok/s 25963 (23655)	Loss/tok 3.5018 (3.5388)	LR 1.000e-03
0: TRAIN [1][40/5173]	Time 0.765 (0.631)	Data 1.23e-04 (1.37e-02)	Tok/s 39026 (23847)	Loss/tok 3.8931 (3.5431)	LR 1.000e-03
0: TRAIN [1][50/5173]	Time 0.641 (0.637)	Data 2.99e-04 (1.10e-02)	Tok/s 26275 (24832)	Loss/tok 3.5386 (3.5684)	LR 1.000e-03
0: TRAIN [1][60/5173]	Time 0.567 (0.635)	Data 1.32e-04 (9.24e-03)	Tok/s 18541 (24913)	Loss/tok 3.3168 (3.5604)	LR 1.000e-03
0: TRAIN [1][70/5173]	Time 0.635 (0.634)	Data 1.39e-04 (7.96e-03)	Tok/s 26412 (25048)	Loss/tok 3.4745 (3.5621)	LR 1.000e-03
0: TRAIN [1][80/5173]	Time 0.563 (0.632)	Data 1.24e-04 (6.99e-03)	Tok/s 18589 (24869)	Loss/tok 3.3101 (3.5581)	LR 1.000e-03
0: TRAIN [1][90/5173]	Time 0.633 (0.629)	Data 1.25e-04 (6.24e-03)	Tok/s 26479 (24650)	Loss/tok 3.4303 (3.5484)	LR 1.000e-03
0: TRAIN [1][100/5173]	Time 0.565 (0.629)	Data 1.38e-04 (5.63e-03)	Tok/s 18162 (24772)	Loss/tok 3.3297 (3.5469)	LR 1.000e-03
0: TRAIN [1][110/5173]	Time 0.561 (0.629)	Data 1.20e-04 (5.14e-03)	Tok/s 18939 (24778)	Loss/tok 3.3506 (3.5505)	LR 1.000e-03
0: TRAIN [1][120/5173]	Time 0.568 (0.629)	Data 3.22e-04 (4.73e-03)	Tok/s 18392 (24852)	Loss/tok 3.2494 (3.5506)	LR 1.000e-03
0: TRAIN [1][130/5173]	Time 0.563 (0.627)	Data 1.28e-04 (4.38e-03)	Tok/s 18318 (24656)	Loss/tok 3.2100 (3.5391)	LR 1.000e-03
0: TRAIN [1][140/5173]	Time 0.569 (0.625)	Data 1.28e-04 (4.08e-03)	Tok/s 18212 (24472)	Loss/tok 3.2525 (3.5350)	LR 1.000e-03
0: TRAIN [1][150/5173]	Time 0.630 (0.623)	Data 1.28e-04 (3.82e-03)	Tok/s 26578 (24339)	Loss/tok 3.6453 (3.5291)	LR 1.000e-03
0: TRAIN [1][160/5173]	Time 0.702 (0.622)	Data 1.27e-04 (3.59e-03)	Tok/s 33532 (24254)	Loss/tok 3.7112 (3.5240)	LR 1.000e-03
0: TRAIN [1][170/5173]	Time 0.569 (0.621)	Data 1.26e-04 (3.39e-03)	Tok/s 18525 (24146)	Loss/tok 3.2204 (3.5187)	LR 1.000e-03
0: TRAIN [1][180/5173]	Time 0.564 (0.621)	Data 1.22e-04 (3.21e-03)	Tok/s 18491 (24177)	Loss/tok 3.3096 (3.5176)	LR 1.000e-03
0: TRAIN [1][190/5173]	Time 0.560 (0.619)	Data 1.28e-04 (3.05e-03)	Tok/s 18669 (24036)	Loss/tok 3.3147 (3.5133)	LR 1.000e-03
0: TRAIN [1][200/5173]	Time 0.562 (0.618)	Data 1.21e-04 (2.90e-03)	Tok/s 18856 (23938)	Loss/tok 3.2472 (3.5111)	LR 1.000e-03
0: TRAIN [1][210/5173]	Time 0.695 (0.617)	Data 1.45e-04 (2.77e-03)	Tok/s 33483 (23779)	Loss/tok 3.6977 (3.5053)	LR 1.000e-03
0: TRAIN [1][220/5173]	Time 0.640 (0.617)	Data 1.33e-04 (2.65e-03)	Tok/s 25806 (23802)	Loss/tok 3.5365 (3.5075)	LR 1.000e-03
0: TRAIN [1][230/5173]	Time 0.568 (0.615)	Data 1.35e-04 (2.54e-03)	Tok/s 18412 (23558)	Loss/tok 3.1926 (3.5000)	LR 1.000e-03
0: TRAIN [1][240/5173]	Time 0.568 (0.615)	Data 1.35e-04 (2.44e-03)	Tok/s 17970 (23604)	Loss/tok 3.2053 (3.4992)	LR 1.000e-03
0: TRAIN [1][250/5173]	Time 0.644 (0.615)	Data 1.38e-04 (2.35e-03)	Tok/s 25900 (23615)	Loss/tok 3.5038 (3.5016)	LR 1.000e-03
0: TRAIN [1][260/5173]	Time 0.636 (0.614)	Data 1.28e-04 (2.27e-03)	Tok/s 26224 (23545)	Loss/tok 3.5591 (3.4987)	LR 1.000e-03
0: TRAIN [1][270/5173]	Time 0.567 (0.614)	Data 1.25e-04 (2.19e-03)	Tok/s 18276 (23498)	Loss/tok 3.2748 (3.4965)	LR 1.000e-03
0: TRAIN [1][280/5173]	Time 0.562 (0.614)	Data 1.41e-04 (2.12e-03)	Tok/s 18256 (23509)	Loss/tok 3.2501 (3.4957)	LR 1.000e-03
0: TRAIN [1][290/5173]	Time 0.700 (0.613)	Data 1.36e-04 (2.05e-03)	Tok/s 33198 (23487)	Loss/tok 3.7602 (3.4961)	LR 1.000e-03
0: TRAIN [1][300/5173]	Time 0.499 (0.613)	Data 1.35e-04 (1.99e-03)	Tok/s 10601 (23421)	Loss/tok 2.7813 (3.4932)	LR 1.000e-03
0: TRAIN [1][310/5173]	Time 0.573 (0.613)	Data 1.38e-04 (1.93e-03)	Tok/s 18015 (23422)	Loss/tok 3.3199 (3.4938)	LR 1.000e-03
0: TRAIN [1][320/5173]	Time 0.558 (0.612)	Data 1.29e-04 (1.87e-03)	Tok/s 18341 (23338)	Loss/tok 3.2362 (3.4909)	LR 1.000e-03
0: TRAIN [1][330/5173]	Time 0.634 (0.612)	Data 1.27e-04 (1.82e-03)	Tok/s 26511 (23306)	Loss/tok 3.3374 (3.4886)	LR 1.000e-03
0: TRAIN [1][340/5173]	Time 0.642 (0.612)	Data 1.32e-04 (1.77e-03)	Tok/s 25327 (23321)	Loss/tok 3.5912 (3.4886)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][350/5173]	Time 0.565 (0.613)	Data 1.43e-04 (1.72e-03)	Tok/s 18123 (23390)	Loss/tok 3.2402 (3.4905)	LR 1.000e-03
0: TRAIN [1][360/5173]	Time 0.629 (0.613)	Data 1.44e-04 (1.68e-03)	Tok/s 26899 (23470)	Loss/tok 3.4248 (3.4947)	LR 1.000e-03
0: TRAIN [1][370/5173]	Time 0.570 (0.613)	Data 1.34e-04 (1.64e-03)	Tok/s 17671 (23416)	Loss/tok 3.3773 (3.4933)	LR 1.000e-03
0: TRAIN [1][380/5173]	Time 0.639 (0.612)	Data 1.31e-04 (1.60e-03)	Tok/s 26732 (23347)	Loss/tok 3.4712 (3.4913)	LR 1.000e-03
0: TRAIN [1][390/5173]	Time 0.749 (0.612)	Data 1.61e-04 (1.56e-03)	Tok/s 39365 (23341)	Loss/tok 4.0643 (3.4938)	LR 1.000e-03
0: TRAIN [1][400/5173]	Time 0.499 (0.612)	Data 1.26e-04 (1.52e-03)	Tok/s 10839 (23275)	Loss/tok 2.7632 (3.4907)	LR 1.000e-03
0: TRAIN [1][410/5173]	Time 0.762 (0.612)	Data 1.24e-04 (1.49e-03)	Tok/s 38894 (23280)	Loss/tok 3.8727 (3.4924)	LR 1.000e-03
0: TRAIN [1][420/5173]	Time 0.564 (0.611)	Data 1.24e-04 (1.46e-03)	Tok/s 18186 (23253)	Loss/tok 3.1928 (3.4923)	LR 1.000e-03
0: TRAIN [1][430/5173]	Time 0.569 (0.611)	Data 1.18e-04 (1.43e-03)	Tok/s 17978 (23223)	Loss/tok 3.2829 (3.4909)	LR 1.000e-03
0: TRAIN [1][440/5173]	Time 0.632 (0.611)	Data 3.01e-04 (1.40e-03)	Tok/s 26640 (23182)	Loss/tok 3.4837 (3.4890)	LR 1.000e-03
0: TRAIN [1][450/5173]	Time 0.632 (0.610)	Data 1.24e-04 (1.37e-03)	Tok/s 26418 (23144)	Loss/tok 3.4325 (3.4877)	LR 1.000e-03
0: TRAIN [1][460/5173]	Time 0.636 (0.611)	Data 1.30e-04 (1.34e-03)	Tok/s 26266 (23199)	Loss/tok 3.5177 (3.4907)	LR 1.000e-03
0: TRAIN [1][470/5173]	Time 0.648 (0.611)	Data 1.30e-04 (1.32e-03)	Tok/s 25652 (23198)	Loss/tok 3.4303 (3.4904)	LR 1.000e-03
0: TRAIN [1][480/5173]	Time 0.560 (0.611)	Data 3.14e-04 (1.29e-03)	Tok/s 18211 (23265)	Loss/tok 3.2385 (3.4925)	LR 1.000e-03
0: TRAIN [1][490/5173]	Time 0.622 (0.611)	Data 1.22e-04 (1.27e-03)	Tok/s 26751 (23216)	Loss/tok 3.4105 (3.4899)	LR 1.000e-03
0: TRAIN [1][500/5173]	Time 0.702 (0.611)	Data 1.30e-04 (1.25e-03)	Tok/s 33439 (23231)	Loss/tok 3.6053 (3.4892)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][510/5173]	Time 0.646 (0.611)	Data 1.28e-04 (1.23e-03)	Tok/s 26026 (23214)	Loss/tok 3.4533 (3.4895)	LR 1.000e-03
0: TRAIN [1][520/5173]	Time 0.567 (0.611)	Data 1.22e-04 (1.20e-03)	Tok/s 18016 (23214)	Loss/tok 3.2049 (3.4905)	LR 1.000e-03
0: TRAIN [1][530/5173]	Time 0.630 (0.611)	Data 1.19e-04 (1.18e-03)	Tok/s 26535 (23218)	Loss/tok 3.5864 (3.4918)	LR 1.000e-03
0: TRAIN [1][540/5173]	Time 0.505 (0.610)	Data 1.23e-04 (1.16e-03)	Tok/s 10471 (23138)	Loss/tok 2.8077 (3.4891)	LR 1.000e-03
0: TRAIN [1][550/5173]	Time 0.637 (0.610)	Data 1.16e-04 (1.15e-03)	Tok/s 26442 (23136)	Loss/tok 3.4544 (3.4882)	LR 1.000e-03
0: TRAIN [1][560/5173]	Time 0.703 (0.611)	Data 1.21e-04 (1.13e-03)	Tok/s 32762 (23194)	Loss/tok 3.7263 (3.4897)	LR 1.000e-03
0: TRAIN [1][570/5173]	Time 0.625 (0.610)	Data 1.17e-04 (1.11e-03)	Tok/s 26617 (23136)	Loss/tok 3.5467 (3.4876)	LR 1.000e-03
0: TRAIN [1][580/5173]	Time 0.498 (0.610)	Data 1.14e-04 (1.09e-03)	Tok/s 10666 (23080)	Loss/tok 2.7356 (3.4858)	LR 1.000e-03
0: TRAIN [1][590/5173]	Time 0.569 (0.610)	Data 1.27e-04 (1.08e-03)	Tok/s 18546 (23073)	Loss/tok 3.2834 (3.4863)	LR 1.000e-03
0: TRAIN [1][600/5173]	Time 0.761 (0.610)	Data 1.27e-04 (1.06e-03)	Tok/s 39374 (23117)	Loss/tok 3.9026 (3.4874)	LR 1.000e-03
0: TRAIN [1][610/5173]	Time 0.565 (0.610)	Data 1.24e-04 (1.05e-03)	Tok/s 18131 (23122)	Loss/tok 3.0946 (3.4869)	LR 1.000e-03
0: TRAIN [1][620/5173]	Time 0.647 (0.610)	Data 1.17e-04 (1.03e-03)	Tok/s 25874 (23120)	Loss/tok 3.5417 (3.4866)	LR 1.000e-03
0: TRAIN [1][630/5173]	Time 0.702 (0.610)	Data 3.03e-04 (1.02e-03)	Tok/s 33687 (23129)	Loss/tok 3.5452 (3.4863)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][640/5173]	Time 0.566 (0.610)	Data 1.28e-04 (1.00e-03)	Tok/s 18338 (23108)	Loss/tok 3.3129 (3.4860)	LR 1.000e-03
0: TRAIN [1][650/5173]	Time 0.635 (0.610)	Data 1.22e-04 (9.90e-04)	Tok/s 26082 (23069)	Loss/tok 3.5743 (3.4850)	LR 1.000e-03
0: TRAIN [1][660/5173]	Time 0.633 (0.610)	Data 1.42e-04 (9.77e-04)	Tok/s 26737 (23145)	Loss/tok 3.3858 (3.4867)	LR 1.000e-03
0: TRAIN [1][670/5173]	Time 0.700 (0.610)	Data 1.29e-04 (9.64e-04)	Tok/s 33476 (23141)	Loss/tok 3.6913 (3.4863)	LR 1.000e-03
0: TRAIN [1][680/5173]	Time 0.562 (0.611)	Data 1.14e-04 (9.52e-04)	Tok/s 18671 (23160)	Loss/tok 3.2454 (3.4855)	LR 1.000e-03
0: TRAIN [1][690/5173]	Time 0.569 (0.610)	Data 1.25e-04 (9.40e-04)	Tok/s 17736 (23140)	Loss/tok 3.3201 (3.4855)	LR 1.000e-03
0: TRAIN [1][700/5173]	Time 0.564 (0.610)	Data 1.23e-04 (9.29e-04)	Tok/s 18249 (23113)	Loss/tok 3.3311 (3.4845)	LR 1.000e-03
0: TRAIN [1][710/5173]	Time 0.688 (0.611)	Data 1.22e-04 (9.17e-04)	Tok/s 34233 (23168)	Loss/tok 3.6439 (3.4857)	LR 1.000e-03
0: TRAIN [1][720/5173]	Time 0.638 (0.611)	Data 1.24e-04 (9.06e-04)	Tok/s 26352 (23189)	Loss/tok 3.5523 (3.4854)	LR 1.000e-03
0: TRAIN [1][730/5173]	Time 0.629 (0.611)	Data 1.24e-04 (8.96e-04)	Tok/s 26547 (23164)	Loss/tok 3.5209 (3.4839)	LR 1.000e-03
0: TRAIN [1][740/5173]	Time 0.509 (0.610)	Data 1.20e-04 (8.85e-04)	Tok/s 10367 (23129)	Loss/tok 2.8517 (3.4826)	LR 1.000e-03
0: TRAIN [1][750/5173]	Time 0.579 (0.610)	Data 1.24e-04 (8.75e-04)	Tok/s 17769 (23096)	Loss/tok 3.3061 (3.4817)	LR 1.000e-03
0: TRAIN [1][760/5173]	Time 0.568 (0.610)	Data 1.46e-04 (8.65e-04)	Tok/s 18122 (23097)	Loss/tok 3.1756 (3.4806)	LR 1.000e-03
0: TRAIN [1][770/5173]	Time 0.628 (0.610)	Data 1.27e-04 (8.56e-04)	Tok/s 26398 (23137)	Loss/tok 3.3768 (3.4816)	LR 1.000e-03
0: TRAIN [1][780/5173]	Time 0.690 (0.610)	Data 1.24e-04 (8.47e-04)	Tok/s 33751 (23126)	Loss/tok 3.7172 (3.4813)	LR 5.000e-04
0: TRAIN [1][790/5173]	Time 0.634 (0.610)	Data 1.27e-04 (8.37e-04)	Tok/s 26495 (23145)	Loss/tok 3.3962 (3.4809)	LR 5.000e-04
0: TRAIN [1][800/5173]	Time 0.641 (0.610)	Data 1.28e-04 (8.29e-04)	Tok/s 26050 (23164)	Loss/tok 3.5508 (3.4809)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][810/5173]	Time 0.640 (0.610)	Data 1.32e-04 (8.20e-04)	Tok/s 26546 (23166)	Loss/tok 3.5412 (3.4816)	LR 5.000e-04
0: TRAIN [1][820/5173]	Time 0.701 (0.611)	Data 1.34e-04 (8.11e-04)	Tok/s 32983 (23192)	Loss/tok 3.6995 (3.4819)	LR 5.000e-04
0: TRAIN [1][830/5173]	Time 0.501 (0.610)	Data 1.31e-04 (8.03e-04)	Tok/s 10667 (23152)	Loss/tok 2.7355 (3.4803)	LR 5.000e-04
0: TRAIN [1][840/5173]	Time 0.566 (0.610)	Data 1.20e-04 (7.96e-04)	Tok/s 18056 (23149)	Loss/tok 3.1368 (3.4803)	LR 5.000e-04
0: TRAIN [1][850/5173]	Time 0.501 (0.610)	Data 1.26e-04 (7.88e-04)	Tok/s 10552 (23132)	Loss/tok 2.6109 (3.4790)	LR 5.000e-04
0: TRAIN [1][860/5173]	Time 0.697 (0.610)	Data 1.22e-04 (7.80e-04)	Tok/s 33139 (23121)	Loss/tok 3.7008 (3.4783)	LR 5.000e-04
0: TRAIN [1][870/5173]	Time 0.565 (0.610)	Data 1.17e-04 (7.72e-04)	Tok/s 18476 (23096)	Loss/tok 3.1186 (3.4769)	LR 5.000e-04
0: TRAIN [1][880/5173]	Time 0.639 (0.609)	Data 1.32e-04 (7.65e-04)	Tok/s 26331 (23077)	Loss/tok 3.4449 (3.4759)	LR 5.000e-04
0: TRAIN [1][890/5173]	Time 0.564 (0.609)	Data 3.04e-04 (7.58e-04)	Tok/s 18735 (23087)	Loss/tok 3.3146 (3.4753)	LR 5.000e-04
0: TRAIN [1][900/5173]	Time 0.569 (0.609)	Data 1.27e-04 (7.51e-04)	Tok/s 18037 (23089)	Loss/tok 3.2043 (3.4752)	LR 5.000e-04
0: TRAIN [1][910/5173]	Time 0.689 (0.609)	Data 1.25e-04 (7.44e-04)	Tok/s 34230 (23066)	Loss/tok 3.5951 (3.4747)	LR 5.000e-04
0: TRAIN [1][920/5173]	Time 0.639 (0.610)	Data 1.31e-04 (7.38e-04)	Tok/s 26334 (23115)	Loss/tok 3.4023 (3.4768)	LR 5.000e-04
0: TRAIN [1][930/5173]	Time 0.633 (0.610)	Data 1.20e-04 (7.31e-04)	Tok/s 26214 (23131)	Loss/tok 3.4166 (3.4764)	LR 5.000e-04
0: TRAIN [1][940/5173]	Time 0.571 (0.610)	Data 1.24e-04 (7.25e-04)	Tok/s 17992 (23150)	Loss/tok 3.1976 (3.4771)	LR 5.000e-04
0: TRAIN [1][950/5173]	Time 0.621 (0.610)	Data 1.26e-04 (7.19e-04)	Tok/s 27252 (23146)	Loss/tok 3.5435 (3.4770)	LR 5.000e-04
0: TRAIN [1][960/5173]	Time 0.569 (0.610)	Data 1.21e-04 (7.13e-04)	Tok/s 17980 (23114)	Loss/tok 3.3804 (3.4759)	LR 5.000e-04
0: TRAIN [1][970/5173]	Time 0.693 (0.610)	Data 1.21e-04 (7.07e-04)	Tok/s 33638 (23104)	Loss/tok 3.5316 (3.4748)	LR 5.000e-04
0: TRAIN [1][980/5173]	Time 0.565 (0.610)	Data 1.24e-04 (7.01e-04)	Tok/s 17845 (23119)	Loss/tok 3.2635 (3.4749)	LR 5.000e-04
0: TRAIN [1][990/5173]	Time 0.644 (0.610)	Data 1.21e-04 (6.95e-04)	Tok/s 26123 (23126)	Loss/tok 3.3490 (3.4742)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1000/5173]	Time 0.619 (0.610)	Data 1.22e-04 (6.89e-04)	Tok/s 27014 (23128)	Loss/tok 3.4900 (3.4747)	LR 5.000e-04
0: TRAIN [1][1010/5173]	Time 0.565 (0.610)	Data 1.26e-04 (6.84e-04)	Tok/s 18539 (23108)	Loss/tok 3.2124 (3.4743)	LR 5.000e-04
0: TRAIN [1][1020/5173]	Time 0.701 (0.610)	Data 1.23e-04 (6.78e-04)	Tok/s 33510 (23182)	Loss/tok 3.6327 (3.4758)	LR 5.000e-04
0: TRAIN [1][1030/5173]	Time 0.558 (0.610)	Data 1.18e-04 (6.73e-04)	Tok/s 18421 (23157)	Loss/tok 3.2307 (3.4746)	LR 5.000e-04
0: TRAIN [1][1040/5173]	Time 0.565 (0.610)	Data 1.23e-04 (6.68e-04)	Tok/s 18365 (23176)	Loss/tok 3.2397 (3.4753)	LR 5.000e-04
0: TRAIN [1][1050/5173]	Time 0.636 (0.610)	Data 1.25e-04 (6.63e-04)	Tok/s 26807 (23195)	Loss/tok 3.3721 (3.4754)	LR 5.000e-04
0: TRAIN [1][1060/5173]	Time 0.563 (0.610)	Data 1.19e-04 (6.58e-04)	Tok/s 18086 (23185)	Loss/tok 3.1493 (3.4751)	LR 5.000e-04
0: TRAIN [1][1070/5173]	Time 0.565 (0.610)	Data 1.20e-04 (6.53e-04)	Tok/s 17965 (23160)	Loss/tok 3.2458 (3.4745)	LR 5.000e-04
0: TRAIN [1][1080/5173]	Time 0.565 (0.610)	Data 1.23e-04 (6.48e-04)	Tok/s 17846 (23165)	Loss/tok 3.3450 (3.4745)	LR 5.000e-04
0: TRAIN [1][1090/5173]	Time 0.562 (0.610)	Data 1.68e-04 (6.43e-04)	Tok/s 18504 (23214)	Loss/tok 3.1415 (3.4761)	LR 5.000e-04
0: TRAIN [1][1100/5173]	Time 0.698 (0.611)	Data 1.27e-04 (6.39e-04)	Tok/s 32871 (23223)	Loss/tok 3.7516 (3.4768)	LR 5.000e-04
0: TRAIN [1][1110/5173]	Time 0.562 (0.611)	Data 1.42e-04 (6.34e-04)	Tok/s 18256 (23235)	Loss/tok 3.2552 (3.4770)	LR 5.000e-04
0: TRAIN [1][1120/5173]	Time 0.508 (0.610)	Data 1.23e-04 (6.30e-04)	Tok/s 10245 (23184)	Loss/tok 2.7799 (3.4753)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1130/5173]	Time 0.566 (0.611)	Data 1.33e-04 (6.25e-04)	Tok/s 18651 (23232)	Loss/tok 3.2934 (3.4767)	LR 5.000e-04
0: TRAIN [1][1140/5173]	Time 0.560 (0.611)	Data 1.29e-04 (6.21e-04)	Tok/s 18377 (23265)	Loss/tok 3.2539 (3.4778)	LR 5.000e-04
0: TRAIN [1][1150/5173]	Time 0.560 (0.611)	Data 1.21e-04 (6.17e-04)	Tok/s 18201 (23249)	Loss/tok 3.1678 (3.4770)	LR 5.000e-04
0: TRAIN [1][1160/5173]	Time 0.575 (0.610)	Data 1.19e-04 (6.13e-04)	Tok/s 17977 (23199)	Loss/tok 3.3886 (3.4755)	LR 5.000e-04
0: TRAIN [1][1170/5173]	Time 0.506 (0.610)	Data 1.25e-04 (6.08e-04)	Tok/s 10259 (23186)	Loss/tok 2.9295 (3.4760)	LR 5.000e-04
0: TRAIN [1][1180/5173]	Time 0.703 (0.610)	Data 1.21e-04 (6.04e-04)	Tok/s 33489 (23213)	Loss/tok 3.5310 (3.4767)	LR 5.000e-04
0: TRAIN [1][1190/5173]	Time 0.566 (0.610)	Data 1.27e-04 (6.00e-04)	Tok/s 18157 (23186)	Loss/tok 3.2993 (3.4759)	LR 5.000e-04
0: TRAIN [1][1200/5173]	Time 0.702 (0.610)	Data 1.28e-04 (5.96e-04)	Tok/s 33161 (23181)	Loss/tok 3.6911 (3.4758)	LR 5.000e-04
0: TRAIN [1][1210/5173]	Time 0.558 (0.610)	Data 1.27e-04 (5.93e-04)	Tok/s 18223 (23159)	Loss/tok 3.3240 (3.4754)	LR 5.000e-04
0: TRAIN [1][1220/5173]	Time 0.503 (0.610)	Data 1.18e-04 (5.89e-04)	Tok/s 10360 (23175)	Loss/tok 2.7239 (3.4759)	LR 5.000e-04
0: TRAIN [1][1230/5173]	Time 0.565 (0.610)	Data 1.27e-04 (5.85e-04)	Tok/s 18393 (23183)	Loss/tok 3.2747 (3.4758)	LR 5.000e-04
0: TRAIN [1][1240/5173]	Time 0.566 (0.610)	Data 1.24e-04 (5.82e-04)	Tok/s 18454 (23157)	Loss/tok 3.1593 (3.4750)	LR 5.000e-04
0: TRAIN [1][1250/5173]	Time 0.570 (0.610)	Data 1.20e-04 (5.78e-04)	Tok/s 18056 (23160)	Loss/tok 3.1702 (3.4747)	LR 5.000e-04
0: TRAIN [1][1260/5173]	Time 0.568 (0.610)	Data 1.32e-04 (5.74e-04)	Tok/s 17983 (23162)	Loss/tok 3.1359 (3.4744)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1270/5173]	Time 0.643 (0.610)	Data 1.26e-04 (5.71e-04)	Tok/s 26099 (23165)	Loss/tok 3.4399 (3.4741)	LR 5.000e-04
0: TRAIN [1][1280/5173]	Time 0.564 (0.610)	Data 1.18e-04 (5.67e-04)	Tok/s 18063 (23151)	Loss/tok 3.1825 (3.4734)	LR 5.000e-04
0: TRAIN [1][1290/5173]	Time 0.564 (0.610)	Data 1.18e-04 (5.64e-04)	Tok/s 18091 (23165)	Loss/tok 3.2152 (3.4737)	LR 5.000e-04
0: TRAIN [1][1300/5173]	Time 0.560 (0.610)	Data 1.30e-04 (5.61e-04)	Tok/s 18376 (23140)	Loss/tok 3.2525 (3.4728)	LR 5.000e-04
0: TRAIN [1][1310/5173]	Time 0.634 (0.610)	Data 1.24e-04 (5.58e-04)	Tok/s 26711 (23138)	Loss/tok 3.4175 (3.4725)	LR 5.000e-04
0: TRAIN [1][1320/5173]	Time 0.561 (0.610)	Data 1.36e-04 (5.54e-04)	Tok/s 18154 (23117)	Loss/tok 3.2138 (3.4720)	LR 5.000e-04
0: TRAIN [1][1330/5173]	Time 0.497 (0.610)	Data 1.43e-04 (5.51e-04)	Tok/s 10405 (23099)	Loss/tok 2.8559 (3.4715)	LR 5.000e-04
0: TRAIN [1][1340/5173]	Time 0.764 (0.610)	Data 1.29e-04 (5.48e-04)	Tok/s 39178 (23102)	Loss/tok 3.8388 (3.4717)	LR 5.000e-04
0: TRAIN [1][1350/5173]	Time 0.642 (0.609)	Data 1.30e-04 (5.45e-04)	Tok/s 25936 (23100)	Loss/tok 3.4410 (3.4714)	LR 5.000e-04
0: TRAIN [1][1360/5173]	Time 0.565 (0.609)	Data 1.26e-04 (5.42e-04)	Tok/s 18368 (23088)	Loss/tok 3.1127 (3.4704)	LR 5.000e-04
0: TRAIN [1][1370/5173]	Time 0.559 (0.609)	Data 1.28e-04 (5.39e-04)	Tok/s 18791 (23087)	Loss/tok 3.2360 (3.4700)	LR 5.000e-04
0: TRAIN [1][1380/5173]	Time 0.571 (0.609)	Data 1.22e-04 (5.36e-04)	Tok/s 17867 (23097)	Loss/tok 3.2358 (3.4701)	LR 5.000e-04
0: TRAIN [1][1390/5173]	Time 0.691 (0.609)	Data 1.34e-04 (5.33e-04)	Tok/s 33852 (23108)	Loss/tok 3.6086 (3.4701)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1400/5173]	Time 0.561 (0.609)	Data 1.38e-04 (5.30e-04)	Tok/s 18114 (23087)	Loss/tok 3.1980 (3.4700)	LR 5.000e-04
0: TRAIN [1][1410/5173]	Time 0.610 (0.610)	Data 1.30e-04 (5.28e-04)	Tok/s 27360 (23114)	Loss/tok 3.4022 (3.4710)	LR 5.000e-04
0: TRAIN [1][1420/5173]	Time 0.764 (0.610)	Data 1.52e-04 (5.25e-04)	Tok/s 38993 (23144)	Loss/tok 3.8690 (3.4717)	LR 5.000e-04
0: TRAIN [1][1430/5173]	Time 0.563 (0.610)	Data 1.52e-04 (5.22e-04)	Tok/s 18285 (23143)	Loss/tok 3.0877 (3.4715)	LR 5.000e-04
0: TRAIN [1][1440/5173]	Time 0.696 (0.610)	Data 1.41e-04 (5.19e-04)	Tok/s 33728 (23125)	Loss/tok 3.5606 (3.4706)	LR 5.000e-04
0: TRAIN [1][1450/5173]	Time 0.644 (0.610)	Data 1.40e-04 (5.17e-04)	Tok/s 26220 (23155)	Loss/tok 3.4196 (3.4713)	LR 5.000e-04
0: TRAIN [1][1460/5173]	Time 0.566 (0.610)	Data 1.33e-04 (5.15e-04)	Tok/s 18056 (23157)	Loss/tok 3.2134 (3.4714)	LR 5.000e-04
0: TRAIN [1][1470/5173]	Time 0.560 (0.610)	Data 1.34e-04 (5.12e-04)	Tok/s 18441 (23168)	Loss/tok 3.1710 (3.4720)	LR 5.000e-04
0: TRAIN [1][1480/5173]	Time 0.641 (0.610)	Data 1.27e-04 (5.09e-04)	Tok/s 26142 (23168)	Loss/tok 3.4312 (3.4713)	LR 5.000e-04
0: TRAIN [1][1490/5173]	Time 0.568 (0.610)	Data 1.33e-04 (5.07e-04)	Tok/s 18426 (23187)	Loss/tok 3.2815 (3.4728)	LR 5.000e-04
0: TRAIN [1][1500/5173]	Time 0.499 (0.610)	Data 1.34e-04 (5.05e-04)	Tok/s 10459 (23195)	Loss/tok 2.5811 (3.4726)	LR 5.000e-04
0: TRAIN [1][1510/5173]	Time 0.562 (0.610)	Data 1.33e-04 (5.02e-04)	Tok/s 18339 (23201)	Loss/tok 3.2594 (3.4728)	LR 5.000e-04
0: TRAIN [1][1520/5173]	Time 0.568 (0.610)	Data 1.31e-04 (5.00e-04)	Tok/s 17806 (23204)	Loss/tok 3.1758 (3.4725)	LR 5.000e-04
0: TRAIN [1][1530/5173]	Time 0.562 (0.611)	Data 1.37e-04 (4.98e-04)	Tok/s 18546 (23213)	Loss/tok 3.3624 (3.4729)	LR 5.000e-04
0: TRAIN [1][1540/5173]	Time 0.562 (0.611)	Data 1.29e-04 (4.96e-04)	Tok/s 18154 (23216)	Loss/tok 3.4094 (3.4729)	LR 5.000e-04
0: TRAIN [1][1550/5173]	Time 0.703 (0.611)	Data 1.33e-04 (4.93e-04)	Tok/s 33033 (23249)	Loss/tok 3.8241 (3.4736)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1560/5173]	Time 0.761 (0.611)	Data 1.29e-04 (4.91e-04)	Tok/s 39705 (23275)	Loss/tok 3.6934 (3.4744)	LR 5.000e-04
0: TRAIN [1][1570/5173]	Time 0.566 (0.611)	Data 1.29e-04 (4.89e-04)	Tok/s 18068 (23258)	Loss/tok 3.1812 (3.4736)	LR 5.000e-04
0: TRAIN [1][1580/5173]	Time 0.500 (0.611)	Data 1.33e-04 (4.87e-04)	Tok/s 10594 (23247)	Loss/tok 2.7634 (3.4730)	LR 5.000e-04
0: TRAIN [1][1590/5173]	Time 0.567 (0.611)	Data 3.09e-04 (4.85e-04)	Tok/s 18584 (23236)	Loss/tok 3.2088 (3.4723)	LR 2.500e-04
0: TRAIN [1][1600/5173]	Time 0.703 (0.611)	Data 1.27e-04 (4.83e-04)	Tok/s 32884 (23230)	Loss/tok 3.7114 (3.4718)	LR 2.500e-04
0: TRAIN [1][1610/5173]	Time 0.629 (0.611)	Data 1.32e-04 (4.80e-04)	Tok/s 26621 (23237)	Loss/tok 3.5008 (3.4717)	LR 2.500e-04
0: TRAIN [1][1620/5173]	Time 0.567 (0.611)	Data 1.28e-04 (4.78e-04)	Tok/s 18067 (23246)	Loss/tok 3.0956 (3.4711)	LR 2.500e-04
0: TRAIN [1][1630/5173]	Time 0.577 (0.611)	Data 1.27e-04 (4.76e-04)	Tok/s 17847 (23230)	Loss/tok 3.1517 (3.4704)	LR 2.500e-04
0: TRAIN [1][1640/5173]	Time 0.564 (0.611)	Data 1.45e-04 (4.74e-04)	Tok/s 18474 (23250)	Loss/tok 3.2491 (3.4708)	LR 2.500e-04
0: TRAIN [1][1650/5173]	Time 0.703 (0.611)	Data 1.25e-04 (4.72e-04)	Tok/s 33481 (23258)	Loss/tok 3.6114 (3.4708)	LR 2.500e-04
0: TRAIN [1][1660/5173]	Time 0.567 (0.611)	Data 1.26e-04 (4.70e-04)	Tok/s 18499 (23256)	Loss/tok 3.3221 (3.4704)	LR 2.500e-04
0: TRAIN [1][1670/5173]	Time 0.569 (0.611)	Data 1.32e-04 (4.68e-04)	Tok/s 17802 (23239)	Loss/tok 3.1998 (3.4698)	LR 2.500e-04
0: TRAIN [1][1680/5173]	Time 0.645 (0.611)	Data 1.31e-04 (4.66e-04)	Tok/s 25837 (23227)	Loss/tok 3.4429 (3.4695)	LR 2.500e-04
0: TRAIN [1][1690/5173]	Time 0.639 (0.611)	Data 1.26e-04 (4.64e-04)	Tok/s 25905 (23225)	Loss/tok 3.4115 (3.4691)	LR 2.500e-04
0: TRAIN [1][1700/5173]	Time 0.707 (0.610)	Data 1.32e-04 (4.62e-04)	Tok/s 33295 (23219)	Loss/tok 3.5938 (3.4687)	LR 2.500e-04
0: TRAIN [1][1710/5173]	Time 0.628 (0.610)	Data 1.31e-04 (4.60e-04)	Tok/s 26885 (23205)	Loss/tok 3.3693 (3.4678)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1720/5173]	Time 0.570 (0.611)	Data 1.45e-04 (4.59e-04)	Tok/s 18199 (23232)	Loss/tok 3.2030 (3.4686)	LR 2.500e-04
0: TRAIN [1][1730/5173]	Time 0.498 (0.610)	Data 1.31e-04 (4.57e-04)	Tok/s 10358 (23212)	Loss/tok 2.7569 (3.4678)	LR 2.500e-04
0: TRAIN [1][1740/5173]	Time 0.508 (0.610)	Data 1.40e-04 (4.55e-04)	Tok/s 10450 (23211)	Loss/tok 2.7657 (3.4674)	LR 2.500e-04
0: TRAIN [1][1750/5173]	Time 0.637 (0.610)	Data 1.32e-04 (4.53e-04)	Tok/s 26072 (23216)	Loss/tok 3.4480 (3.4674)	LR 2.500e-04
0: TRAIN [1][1760/5173]	Time 0.568 (0.610)	Data 1.39e-04 (4.52e-04)	Tok/s 18008 (23215)	Loss/tok 3.1577 (3.4669)	LR 2.500e-04
0: TRAIN [1][1770/5173]	Time 0.704 (0.611)	Data 1.43e-04 (4.50e-04)	Tok/s 33066 (23229)	Loss/tok 3.6984 (3.4672)	LR 2.500e-04
0: TRAIN [1][1780/5173]	Time 0.703 (0.610)	Data 3.04e-04 (4.48e-04)	Tok/s 33198 (23219)	Loss/tok 3.6449 (3.4669)	LR 2.500e-04
0: TRAIN [1][1790/5173]	Time 0.564 (0.610)	Data 1.23e-04 (4.46e-04)	Tok/s 18149 (23200)	Loss/tok 3.2798 (3.4663)	LR 2.500e-04
0: TRAIN [1][1800/5173]	Time 0.696 (0.610)	Data 1.33e-04 (4.45e-04)	Tok/s 33077 (23219)	Loss/tok 3.7058 (3.4666)	LR 2.500e-04
0: TRAIN [1][1810/5173]	Time 0.629 (0.611)	Data 1.35e-04 (4.43e-04)	Tok/s 26738 (23240)	Loss/tok 3.5170 (3.4676)	LR 2.500e-04
0: TRAIN [1][1820/5173]	Time 0.567 (0.611)	Data 1.28e-04 (4.41e-04)	Tok/s 17738 (23237)	Loss/tok 3.2239 (3.4675)	LR 2.500e-04
0: TRAIN [1][1830/5173]	Time 0.565 (0.611)	Data 1.29e-04 (4.40e-04)	Tok/s 18187 (23227)	Loss/tok 3.0874 (3.4669)	LR 2.500e-04
0: TRAIN [1][1840/5173]	Time 0.565 (0.611)	Data 1.29e-04 (4.38e-04)	Tok/s 18350 (23226)	Loss/tok 3.1943 (3.4665)	LR 2.500e-04
0: TRAIN [1][1850/5173]	Time 0.566 (0.611)	Data 1.31e-04 (4.37e-04)	Tok/s 18005 (23235)	Loss/tok 3.3135 (3.4667)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1860/5173]	Time 0.497 (0.611)	Data 1.52e-04 (4.35e-04)	Tok/s 10560 (23255)	Loss/tok 2.7251 (3.4674)	LR 2.500e-04
0: TRAIN [1][1870/5173]	Time 0.564 (0.611)	Data 1.33e-04 (4.33e-04)	Tok/s 18154 (23253)	Loss/tok 3.2345 (3.4673)	LR 2.500e-04
0: TRAIN [1][1880/5173]	Time 0.566 (0.611)	Data 1.29e-04 (4.32e-04)	Tok/s 17982 (23251)	Loss/tok 3.1329 (3.4669)	LR 2.500e-04
0: TRAIN [1][1890/5173]	Time 0.693 (0.611)	Data 1.30e-04 (4.30e-04)	Tok/s 33934 (23254)	Loss/tok 3.6434 (3.4668)	LR 2.500e-04
0: TRAIN [1][1900/5173]	Time 0.643 (0.611)	Data 1.30e-04 (4.29e-04)	Tok/s 26587 (23261)	Loss/tok 3.4667 (3.4665)	LR 2.500e-04
0: TRAIN [1][1910/5173]	Time 0.764 (0.611)	Data 1.29e-04 (4.27e-04)	Tok/s 38935 (23269)	Loss/tok 3.7680 (3.4668)	LR 2.500e-04
0: TRAIN [1][1920/5173]	Time 0.639 (0.611)	Data 1.32e-04 (4.26e-04)	Tok/s 26277 (23273)	Loss/tok 3.4097 (3.4669)	LR 2.500e-04
0: TRAIN [1][1930/5173]	Time 0.583 (0.611)	Data 1.44e-04 (4.24e-04)	Tok/s 18170 (23286)	Loss/tok 3.1735 (3.4672)	LR 2.500e-04
0: TRAIN [1][1940/5173]	Time 0.686 (0.611)	Data 1.37e-04 (4.23e-04)	Tok/s 34414 (23272)	Loss/tok 3.5139 (3.4666)	LR 2.500e-04
0: TRAIN [1][1950/5173]	Time 0.563 (0.611)	Data 1.29e-04 (4.21e-04)	Tok/s 18621 (23262)	Loss/tok 3.2541 (3.4665)	LR 2.500e-04
0: TRAIN [1][1960/5173]	Time 0.570 (0.611)	Data 1.34e-04 (4.20e-04)	Tok/s 17735 (23252)	Loss/tok 3.0981 (3.4659)	LR 2.500e-04
0: TRAIN [1][1970/5173]	Time 0.631 (0.611)	Data 1.36e-04 (4.18e-04)	Tok/s 26999 (23247)	Loss/tok 3.4001 (3.4656)	LR 2.500e-04
0: TRAIN [1][1980/5173]	Time 0.706 (0.611)	Data 1.23e-04 (4.17e-04)	Tok/s 33180 (23253)	Loss/tok 3.5562 (3.4655)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1990/5173]	Time 0.760 (0.611)	Data 3.15e-04 (4.16e-04)	Tok/s 38452 (23256)	Loss/tok 3.8372 (3.4658)	LR 2.500e-04
0: TRAIN [1][2000/5173]	Time 0.560 (0.611)	Data 1.32e-04 (4.14e-04)	Tok/s 18835 (23266)	Loss/tok 3.3329 (3.4657)	LR 2.500e-04
0: TRAIN [1][2010/5173]	Time 0.630 (0.611)	Data 1.33e-04 (4.13e-04)	Tok/s 26785 (23274)	Loss/tok 3.4967 (3.4656)	LR 2.500e-04
0: TRAIN [1][2020/5173]	Time 0.701 (0.611)	Data 1.31e-04 (4.12e-04)	Tok/s 33162 (23256)	Loss/tok 3.5970 (3.4650)	LR 2.500e-04
0: TRAIN [1][2030/5173]	Time 0.634 (0.611)	Data 3.23e-04 (4.10e-04)	Tok/s 26501 (23236)	Loss/tok 3.4052 (3.4644)	LR 2.500e-04
0: TRAIN [1][2040/5173]	Time 0.702 (0.611)	Data 1.28e-04 (4.09e-04)	Tok/s 33745 (23243)	Loss/tok 3.5933 (3.4642)	LR 2.500e-04
0: TRAIN [1][2050/5173]	Time 0.565 (0.611)	Data 1.30e-04 (4.08e-04)	Tok/s 18276 (23226)	Loss/tok 3.1307 (3.4635)	LR 2.500e-04
0: TRAIN [1][2060/5173]	Time 0.701 (0.611)	Data 1.39e-04 (4.06e-04)	Tok/s 33491 (23242)	Loss/tok 3.6234 (3.4640)	LR 2.500e-04
0: TRAIN [1][2070/5173]	Time 0.642 (0.611)	Data 1.21e-04 (4.05e-04)	Tok/s 26247 (23241)	Loss/tok 3.3742 (3.4635)	LR 2.500e-04
0: TRAIN [1][2080/5173]	Time 0.768 (0.611)	Data 1.35e-04 (4.04e-04)	Tok/s 38562 (23257)	Loss/tok 3.7745 (3.4638)	LR 2.500e-04
0: TRAIN [1][2090/5173]	Time 0.569 (0.611)	Data 1.28e-04 (4.02e-04)	Tok/s 18421 (23258)	Loss/tok 3.2156 (3.4638)	LR 2.500e-04
0: TRAIN [1][2100/5173]	Time 0.567 (0.611)	Data 1.54e-04 (4.01e-04)	Tok/s 18251 (23289)	Loss/tok 3.1740 (3.4647)	LR 2.500e-04
0: TRAIN [1][2110/5173]	Time 0.632 (0.611)	Data 1.27e-04 (4.00e-04)	Tok/s 26369 (23289)	Loss/tok 3.3998 (3.4647)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2120/5173]	Time 0.764 (0.611)	Data 1.33e-04 (3.99e-04)	Tok/s 38749 (23290)	Loss/tok 3.8315 (3.4647)	LR 2.500e-04
0: TRAIN [1][2130/5173]	Time 0.566 (0.611)	Data 1.25e-04 (3.98e-04)	Tok/s 18614 (23291)	Loss/tok 3.1466 (3.4645)	LR 2.500e-04
0: TRAIN [1][2140/5173]	Time 0.500 (0.611)	Data 1.29e-04 (3.96e-04)	Tok/s 10320 (23264)	Loss/tok 2.7380 (3.4636)	LR 2.500e-04
0: TRAIN [1][2150/5173]	Time 0.566 (0.611)	Data 1.33e-04 (3.95e-04)	Tok/s 17907 (23248)	Loss/tok 3.2078 (3.4632)	LR 2.500e-04
0: TRAIN [1][2160/5173]	Time 0.564 (0.611)	Data 1.30e-04 (3.94e-04)	Tok/s 18177 (23253)	Loss/tok 3.2260 (3.4631)	LR 2.500e-04
0: TRAIN [1][2170/5173]	Time 0.638 (0.611)	Data 1.26e-04 (3.93e-04)	Tok/s 26119 (23256)	Loss/tok 3.4568 (3.4628)	LR 2.500e-04
0: TRAIN [1][2180/5173]	Time 0.563 (0.611)	Data 1.33e-04 (3.92e-04)	Tok/s 18162 (23230)	Loss/tok 3.2261 (3.4621)	LR 2.500e-04
0: TRAIN [1][2190/5173]	Time 0.499 (0.610)	Data 1.39e-04 (3.90e-04)	Tok/s 10488 (23215)	Loss/tok 2.6724 (3.4618)	LR 2.500e-04
0: TRAIN [1][2200/5173]	Time 0.563 (0.611)	Data 1.23e-04 (3.89e-04)	Tok/s 18174 (23215)	Loss/tok 3.1945 (3.4618)	LR 2.500e-04
0: TRAIN [1][2210/5173]	Time 0.632 (0.610)	Data 1.40e-04 (3.88e-04)	Tok/s 26703 (23213)	Loss/tok 3.3025 (3.4616)	LR 2.500e-04
0: TRAIN [1][2220/5173]	Time 0.565 (0.611)	Data 1.30e-04 (3.87e-04)	Tok/s 17937 (23216)	Loss/tok 3.1843 (3.4616)	LR 2.500e-04
0: TRAIN [1][2230/5173]	Time 0.571 (0.611)	Data 1.28e-04 (3.86e-04)	Tok/s 18085 (23218)	Loss/tok 3.2497 (3.4612)	LR 2.500e-04
0: TRAIN [1][2240/5173]	Time 0.498 (0.610)	Data 1.43e-04 (3.85e-04)	Tok/s 10469 (23211)	Loss/tok 2.7426 (3.4609)	LR 2.500e-04
0: TRAIN [1][2250/5173]	Time 0.564 (0.610)	Data 1.28e-04 (3.84e-04)	Tok/s 18432 (23202)	Loss/tok 3.2948 (3.4606)	LR 2.500e-04
0: TRAIN [1][2260/5173]	Time 0.628 (0.610)	Data 1.29e-04 (3.83e-04)	Tok/s 26537 (23186)	Loss/tok 3.3974 (3.4598)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2270/5173]	Time 0.556 (0.610)	Data 1.31e-04 (3.82e-04)	Tok/s 18689 (23201)	Loss/tok 3.1801 (3.4601)	LR 2.500e-04
0: TRAIN [1][2280/5173]	Time 0.573 (0.610)	Data 1.31e-04 (3.81e-04)	Tok/s 17802 (23205)	Loss/tok 3.2747 (3.4599)	LR 2.500e-04
0: TRAIN [1][2290/5173]	Time 0.702 (0.610)	Data 3.09e-04 (3.80e-04)	Tok/s 33115 (23210)	Loss/tok 3.6731 (3.4598)	LR 2.500e-04
0: TRAIN [1][2300/5173]	Time 0.496 (0.610)	Data 1.31e-04 (3.79e-04)	Tok/s 10442 (23213)	Loss/tok 2.8061 (3.4601)	LR 2.500e-04
0: TRAIN [1][2310/5173]	Time 0.703 (0.611)	Data 1.36e-04 (3.78e-04)	Tok/s 33307 (23218)	Loss/tok 3.5016 (3.4599)	LR 2.500e-04
0: TRAIN [1][2320/5173]	Time 0.704 (0.611)	Data 1.37e-04 (3.77e-04)	Tok/s 33023 (23229)	Loss/tok 3.6303 (3.4601)	LR 2.500e-04
0: TRAIN [1][2330/5173]	Time 0.566 (0.611)	Data 1.32e-04 (3.76e-04)	Tok/s 18275 (23230)	Loss/tok 3.2298 (3.4600)	LR 2.500e-04
0: TRAIN [1][2340/5173]	Time 0.630 (0.611)	Data 1.38e-04 (3.75e-04)	Tok/s 26267 (23227)	Loss/tok 3.3929 (3.4599)	LR 2.500e-04
0: TRAIN [1][2350/5173]	Time 0.703 (0.611)	Data 1.41e-04 (3.74e-04)	Tok/s 33413 (23241)	Loss/tok 3.5632 (3.4601)	LR 2.500e-04
0: TRAIN [1][2360/5173]	Time 0.637 (0.611)	Data 2.88e-04 (3.73e-04)	Tok/s 26716 (23239)	Loss/tok 3.4254 (3.4599)	LR 2.500e-04
0: TRAIN [1][2370/5173]	Time 0.643 (0.611)	Data 1.45e-04 (3.72e-04)	Tok/s 26296 (23230)	Loss/tok 3.3837 (3.4596)	LR 2.500e-04
0: TRAIN [1][2380/5173]	Time 0.565 (0.611)	Data 1.29e-04 (3.71e-04)	Tok/s 17892 (23233)	Loss/tok 3.2655 (3.4601)	LR 2.500e-04
0: TRAIN [1][2390/5173]	Time 0.637 (0.611)	Data 1.24e-04 (3.70e-04)	Tok/s 26609 (23234)	Loss/tok 3.3736 (3.4600)	LR 2.500e-04
0: TRAIN [1][2400/5173]	Time 0.567 (0.611)	Data 1.22e-04 (3.69e-04)	Tok/s 18162 (23220)	Loss/tok 3.2539 (3.4595)	LR 2.500e-04
0: TRAIN [1][2410/5173]	Time 0.641 (0.611)	Data 1.30e-04 (3.68e-04)	Tok/s 26479 (23223)	Loss/tok 3.4188 (3.4596)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2420/5173]	Time 0.646 (0.611)	Data 1.25e-04 (3.67e-04)	Tok/s 25905 (23222)	Loss/tok 3.3460 (3.4594)	LR 1.250e-04
0: TRAIN [1][2430/5173]	Time 0.501 (0.611)	Data 1.34e-04 (3.66e-04)	Tok/s 10597 (23217)	Loss/tok 2.6892 (3.4593)	LR 1.250e-04
0: TRAIN [1][2440/5173]	Time 0.767 (0.611)	Data 1.33e-04 (3.65e-04)	Tok/s 38809 (23225)	Loss/tok 3.8486 (3.4594)	LR 1.250e-04
0: TRAIN [1][2450/5173]	Time 0.500 (0.611)	Data 1.28e-04 (3.64e-04)	Tok/s 10806 (23214)	Loss/tok 2.7489 (3.4591)	LR 1.250e-04
0: TRAIN [1][2460/5173]	Time 0.503 (0.610)	Data 1.31e-04 (3.63e-04)	Tok/s 10647 (23204)	Loss/tok 2.7475 (3.4587)	LR 1.250e-04
0: TRAIN [1][2470/5173]	Time 0.510 (0.610)	Data 1.41e-04 (3.62e-04)	Tok/s 10511 (23191)	Loss/tok 2.6938 (3.4582)	LR 1.250e-04
0: TRAIN [1][2480/5173]	Time 0.561 (0.610)	Data 1.27e-04 (3.61e-04)	Tok/s 18247 (23183)	Loss/tok 3.2282 (3.4579)	LR 1.250e-04
0: TRAIN [1][2490/5173]	Time 0.636 (0.610)	Data 1.36e-04 (3.60e-04)	Tok/s 26522 (23189)	Loss/tok 3.4438 (3.4580)	LR 1.250e-04
0: TRAIN [1][2500/5173]	Time 0.570 (0.610)	Data 1.31e-04 (3.60e-04)	Tok/s 18261 (23187)	Loss/tok 3.3058 (3.4580)	LR 1.250e-04
0: TRAIN [1][2510/5173]	Time 0.565 (0.610)	Data 3.17e-04 (3.59e-04)	Tok/s 18327 (23197)	Loss/tok 3.2050 (3.4583)	LR 1.250e-04
0: TRAIN [1][2520/5173]	Time 0.639 (0.610)	Data 1.29e-04 (3.58e-04)	Tok/s 26319 (23195)	Loss/tok 3.4633 (3.4583)	LR 1.250e-04
0: TRAIN [1][2530/5173]	Time 0.562 (0.610)	Data 1.31e-04 (3.57e-04)	Tok/s 18541 (23184)	Loss/tok 3.1869 (3.4579)	LR 1.250e-04
0: TRAIN [1][2540/5173]	Time 0.638 (0.610)	Data 1.37e-04 (3.56e-04)	Tok/s 26206 (23189)	Loss/tok 3.5223 (3.4579)	LR 1.250e-04
0: TRAIN [1][2550/5173]	Time 0.562 (0.610)	Data 1.48e-04 (3.55e-04)	Tok/s 18020 (23192)	Loss/tok 3.2714 (3.4578)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2560/5173]	Time 0.635 (0.610)	Data 1.46e-04 (3.55e-04)	Tok/s 25960 (23198)	Loss/tok 3.4374 (3.4580)	LR 1.250e-04
0: TRAIN [1][2570/5173]	Time 0.641 (0.610)	Data 1.37e-04 (3.54e-04)	Tok/s 26128 (23205)	Loss/tok 3.4636 (3.4583)	LR 1.250e-04
0: TRAIN [1][2580/5173]	Time 0.640 (0.611)	Data 1.26e-04 (3.53e-04)	Tok/s 26446 (23215)	Loss/tok 3.3608 (3.4584)	LR 1.250e-04
0: TRAIN [1][2590/5173]	Time 0.695 (0.611)	Data 1.35e-04 (3.52e-04)	Tok/s 33193 (23222)	Loss/tok 3.7233 (3.4584)	LR 1.250e-04
0: TRAIN [1][2600/5173]	Time 0.635 (0.611)	Data 1.41e-04 (3.51e-04)	Tok/s 26420 (23238)	Loss/tok 3.3862 (3.4588)	LR 1.250e-04
0: TRAIN [1][2610/5173]	Time 0.699 (0.611)	Data 1.29e-04 (3.51e-04)	Tok/s 33441 (23236)	Loss/tok 3.6172 (3.4587)	LR 1.250e-04
0: TRAIN [1][2620/5173]	Time 0.690 (0.611)	Data 1.32e-04 (3.50e-04)	Tok/s 33945 (23235)	Loss/tok 3.5297 (3.4583)	LR 1.250e-04
0: TRAIN [1][2630/5173]	Time 0.766 (0.611)	Data 1.33e-04 (3.49e-04)	Tok/s 38356 (23248)	Loss/tok 3.9298 (3.4588)	LR 1.250e-04
0: TRAIN [1][2640/5173]	Time 0.560 (0.611)	Data 1.26e-04 (3.48e-04)	Tok/s 18340 (23236)	Loss/tok 3.1547 (3.4583)	LR 1.250e-04
0: TRAIN [1][2650/5173]	Time 0.641 (0.611)	Data 1.25e-04 (3.47e-04)	Tok/s 26194 (23237)	Loss/tok 3.4900 (3.4582)	LR 1.250e-04
0: TRAIN [1][2660/5173]	Time 0.641 (0.611)	Data 1.41e-04 (3.47e-04)	Tok/s 26229 (23240)	Loss/tok 3.5047 (3.4585)	LR 1.250e-04
0: TRAIN [1][2670/5173]	Time 0.559 (0.611)	Data 1.36e-04 (3.46e-04)	Tok/s 18378 (23238)	Loss/tok 3.2874 (3.4584)	LR 1.250e-04
0: TRAIN [1][2680/5173]	Time 0.573 (0.611)	Data 1.30e-04 (3.45e-04)	Tok/s 18167 (23236)	Loss/tok 3.0848 (3.4585)	LR 1.250e-04
0: TRAIN [1][2690/5173]	Time 0.707 (0.611)	Data 1.35e-04 (3.44e-04)	Tok/s 33110 (23238)	Loss/tok 3.6396 (3.4583)	LR 1.250e-04
0: TRAIN [1][2700/5173]	Time 0.678 (0.611)	Data 1.28e-04 (3.44e-04)	Tok/s 34273 (23244)	Loss/tok 3.5264 (3.4581)	LR 1.250e-04
0: TRAIN [1][2710/5173]	Time 0.703 (0.611)	Data 1.31e-04 (3.43e-04)	Tok/s 33196 (23260)	Loss/tok 3.5591 (3.4581)	LR 1.250e-04
0: TRAIN [1][2720/5173]	Time 0.633 (0.611)	Data 1.29e-04 (3.42e-04)	Tok/s 26662 (23265)	Loss/tok 3.4367 (3.4580)	LR 1.250e-04
0: TRAIN [1][2730/5173]	Time 0.643 (0.611)	Data 1.36e-04 (3.41e-04)	Tok/s 25994 (23275)	Loss/tok 3.3890 (3.4581)	LR 1.250e-04
0: TRAIN [1][2740/5173]	Time 0.637 (0.611)	Data 1.49e-04 (3.41e-04)	Tok/s 26215 (23289)	Loss/tok 3.4860 (3.4583)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2750/5173]	Time 0.762 (0.611)	Data 1.40e-04 (3.40e-04)	Tok/s 38864 (23288)	Loss/tok 3.9463 (3.4584)	LR 1.250e-04
0: TRAIN [1][2760/5173]	Time 0.623 (0.611)	Data 1.30e-04 (3.39e-04)	Tok/s 26693 (23285)	Loss/tok 3.5448 (3.4586)	LR 1.250e-04
0: TRAIN [1][2770/5173]	Time 0.648 (0.611)	Data 1.27e-04 (3.38e-04)	Tok/s 25986 (23292)	Loss/tok 3.4581 (3.4586)	LR 1.250e-04
0: TRAIN [1][2780/5173]	Time 0.501 (0.611)	Data 1.35e-04 (3.38e-04)	Tok/s 10575 (23281)	Loss/tok 2.7708 (3.4582)	LR 1.250e-04
0: TRAIN [1][2790/5173]	Time 0.571 (0.611)	Data 1.30e-04 (3.37e-04)	Tok/s 17882 (23275)	Loss/tok 3.2053 (3.4578)	LR 1.250e-04
0: TRAIN [1][2800/5173]	Time 0.627 (0.611)	Data 1.28e-04 (3.36e-04)	Tok/s 26652 (23269)	Loss/tok 3.4468 (3.4575)	LR 1.250e-04
0: TRAIN [1][2810/5173]	Time 0.642 (0.611)	Data 1.39e-04 (3.35e-04)	Tok/s 26277 (23276)	Loss/tok 3.4580 (3.4576)	LR 1.250e-04
0: TRAIN [1][2820/5173]	Time 0.557 (0.611)	Data 1.25e-04 (3.35e-04)	Tok/s 18755 (23275)	Loss/tok 3.2050 (3.4573)	LR 1.250e-04
0: TRAIN [1][2830/5173]	Time 0.567 (0.611)	Data 2.87e-04 (3.34e-04)	Tok/s 18341 (23264)	Loss/tok 3.1652 (3.4568)	LR 1.250e-04
0: TRAIN [1][2840/5173]	Time 0.571 (0.611)	Data 1.30e-04 (3.33e-04)	Tok/s 18069 (23249)	Loss/tok 3.0878 (3.4561)	LR 1.250e-04
0: TRAIN [1][2850/5173]	Time 0.628 (0.611)	Data 1.32e-04 (3.33e-04)	Tok/s 27120 (23247)	Loss/tok 3.2945 (3.4560)	LR 1.250e-04
0: TRAIN [1][2860/5173]	Time 0.570 (0.611)	Data 1.28e-04 (3.32e-04)	Tok/s 17946 (23255)	Loss/tok 3.2435 (3.4559)	LR 1.250e-04
0: TRAIN [1][2870/5173]	Time 0.562 (0.611)	Data 1.33e-04 (3.31e-04)	Tok/s 18235 (23248)	Loss/tok 3.2635 (3.4557)	LR 1.250e-04
0: TRAIN [1][2880/5173]	Time 0.505 (0.611)	Data 1.34e-04 (3.31e-04)	Tok/s 10429 (23242)	Loss/tok 2.5888 (3.4553)	LR 1.250e-04
0: TRAIN [1][2890/5173]	Time 0.503 (0.611)	Data 1.37e-04 (3.30e-04)	Tok/s 10383 (23246)	Loss/tok 2.6844 (3.4554)	LR 1.250e-04
0: TRAIN [1][2900/5173]	Time 0.558 (0.611)	Data 1.29e-04 (3.29e-04)	Tok/s 18417 (23251)	Loss/tok 3.1983 (3.4552)	LR 1.250e-04
0: TRAIN [1][2910/5173]	Time 0.560 (0.611)	Data 3.16e-04 (3.29e-04)	Tok/s 18440 (23258)	Loss/tok 3.2005 (3.4552)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2920/5173]	Time 0.772 (0.611)	Data 1.35e-04 (3.28e-04)	Tok/s 38588 (23269)	Loss/tok 3.7829 (3.4555)	LR 1.250e-04
0: TRAIN [1][2930/5173]	Time 0.561 (0.611)	Data 1.36e-04 (3.27e-04)	Tok/s 18757 (23270)	Loss/tok 3.1225 (3.4553)	LR 1.250e-04
0: TRAIN [1][2940/5173]	Time 0.565 (0.611)	Data 1.30e-04 (3.27e-04)	Tok/s 18315 (23270)	Loss/tok 3.1974 (3.4550)	LR 1.250e-04
0: TRAIN [1][2950/5173]	Time 0.620 (0.611)	Data 1.24e-04 (3.26e-04)	Tok/s 27441 (23264)	Loss/tok 3.3370 (3.4546)	LR 1.250e-04
0: TRAIN [1][2960/5173]	Time 0.626 (0.611)	Data 1.31e-04 (3.25e-04)	Tok/s 26873 (23260)	Loss/tok 3.4259 (3.4544)	LR 1.250e-04
0: TRAIN [1][2970/5173]	Time 0.632 (0.611)	Data 1.32e-04 (3.25e-04)	Tok/s 26371 (23254)	Loss/tok 3.4103 (3.4541)	LR 1.250e-04
0: TRAIN [1][2980/5173]	Time 0.643 (0.611)	Data 1.30e-04 (3.24e-04)	Tok/s 26135 (23250)	Loss/tok 3.3869 (3.4539)	LR 1.250e-04
0: TRAIN [1][2990/5173]	Time 0.637 (0.611)	Data 1.35e-04 (3.24e-04)	Tok/s 26367 (23250)	Loss/tok 3.5304 (3.4541)	LR 1.250e-04
0: TRAIN [1][3000/5173]	Time 0.503 (0.611)	Data 1.40e-04 (3.23e-04)	Tok/s 10739 (23241)	Loss/tok 2.8122 (3.4539)	LR 1.250e-04
0: TRAIN [1][3010/5173]	Time 0.694 (0.611)	Data 1.39e-04 (3.23e-04)	Tok/s 33620 (23245)	Loss/tok 3.6056 (3.4538)	LR 1.250e-04
0: TRAIN [1][3020/5173]	Time 0.704 (0.611)	Data 3.05e-04 (3.22e-04)	Tok/s 33319 (23251)	Loss/tok 3.6532 (3.4539)	LR 1.250e-04
0: TRAIN [1][3030/5173]	Time 0.562 (0.611)	Data 1.41e-04 (3.22e-04)	Tok/s 18200 (23255)	Loss/tok 3.2754 (3.4539)	LR 1.250e-04
0: TRAIN [1][3040/5173]	Time 0.702 (0.611)	Data 1.40e-04 (3.21e-04)	Tok/s 33453 (23263)	Loss/tok 3.4401 (3.4540)	LR 1.250e-04
0: TRAIN [1][3050/5173]	Time 0.644 (0.611)	Data 1.26e-04 (3.20e-04)	Tok/s 25806 (23265)	Loss/tok 3.3973 (3.4538)	LR 1.250e-04
0: TRAIN [1][3060/5173]	Time 0.705 (0.611)	Data 1.38e-04 (3.20e-04)	Tok/s 33467 (23257)	Loss/tok 3.5016 (3.4534)	LR 1.250e-04
0: TRAIN [1][3070/5173]	Time 0.505 (0.611)	Data 3.22e-04 (3.19e-04)	Tok/s 10368 (23241)	Loss/tok 2.8026 (3.4530)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][3080/5173]	Time 0.567 (0.611)	Data 1.31e-04 (3.19e-04)	Tok/s 17890 (23247)	Loss/tok 3.2148 (3.4532)	LR 1.250e-04
0: TRAIN [1][3090/5173]	Time 0.567 (0.611)	Data 1.30e-04 (3.18e-04)	Tok/s 18160 (23236)	Loss/tok 3.1330 (3.4528)	LR 1.250e-04
0: TRAIN [1][3100/5173]	Time 0.562 (0.611)	Data 1.38e-04 (3.18e-04)	Tok/s 18487 (23239)	Loss/tok 3.2062 (3.4529)	LR 1.250e-04
0: TRAIN [1][3110/5173]	Time 0.645 (0.611)	Data 1.32e-04 (3.17e-04)	Tok/s 26087 (23243)	Loss/tok 3.4426 (3.4527)	LR 1.250e-04
0: TRAIN [1][3120/5173]	Time 0.643 (0.610)	Data 1.29e-04 (3.17e-04)	Tok/s 26218 (23227)	Loss/tok 3.4808 (3.4523)	LR 1.250e-04
0: TRAIN [1][3130/5173]	Time 0.566 (0.610)	Data 1.43e-04 (3.16e-04)	Tok/s 18566 (23230)	Loss/tok 3.2106 (3.4525)	LR 1.250e-04
0: TRAIN [1][3140/5173]	Time 0.496 (0.610)	Data 1.32e-04 (3.16e-04)	Tok/s 10591 (23221)	Loss/tok 2.7720 (3.4522)	LR 1.250e-04
0: TRAIN [1][3150/5173]	Time 0.638 (0.610)	Data 1.26e-04 (3.15e-04)	Tok/s 26491 (23221)	Loss/tok 3.5352 (3.4521)	LR 1.250e-04
0: TRAIN [1][3160/5173]	Time 0.633 (0.610)	Data 1.31e-04 (3.14e-04)	Tok/s 26548 (23227)	Loss/tok 3.5003 (3.4521)	LR 1.250e-04
0: TRAIN [1][3170/5173]	Time 0.561 (0.610)	Data 1.32e-04 (3.14e-04)	Tok/s 18439 (23232)	Loss/tok 3.1948 (3.4519)	LR 1.250e-04
0: TRAIN [1][3180/5173]	Time 0.563 (0.611)	Data 1.32e-04 (3.13e-04)	Tok/s 18341 (23235)	Loss/tok 3.1280 (3.4517)	LR 1.250e-04
0: TRAIN [1][3190/5173]	Time 0.570 (0.610)	Data 1.43e-04 (3.13e-04)	Tok/s 18035 (23232)	Loss/tok 3.2310 (3.4514)	LR 1.250e-04
0: TRAIN [1][3200/5173]	Time 0.564 (0.610)	Data 1.26e-04 (3.12e-04)	Tok/s 18271 (23224)	Loss/tok 3.2396 (3.4511)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][3210/5173]	Time 0.641 (0.610)	Data 1.23e-04 (3.12e-04)	Tok/s 26325 (23228)	Loss/tok 3.4504 (3.4512)	LR 1.250e-04
0: TRAIN [1][3220/5173]	Time 0.704 (0.611)	Data 1.42e-04 (3.11e-04)	Tok/s 32763 (23244)	Loss/tok 3.6266 (3.4517)	LR 6.250e-05
0: TRAIN [1][3230/5173]	Time 0.566 (0.611)	Data 1.42e-04 (3.10e-04)	Tok/s 18136 (23240)	Loss/tok 3.2684 (3.4515)	LR 6.250e-05
0: TRAIN [1][3240/5173]	Time 0.498 (0.610)	Data 1.26e-04 (3.10e-04)	Tok/s 10540 (23225)	Loss/tok 2.8155 (3.4511)	LR 6.250e-05
0: TRAIN [1][3250/5173]	Time 0.498 (0.610)	Data 1.38e-04 (3.09e-04)	Tok/s 10628 (23228)	Loss/tok 2.8113 (3.4512)	LR 6.250e-05
0: TRAIN [1][3260/5173]	Time 0.576 (0.610)	Data 1.31e-04 (3.09e-04)	Tok/s 18317 (23224)	Loss/tok 3.1505 (3.4510)	LR 6.250e-05
0: TRAIN [1][3270/5173]	Time 0.631 (0.610)	Data 1.26e-04 (3.08e-04)	Tok/s 26408 (23229)	Loss/tok 3.4638 (3.4510)	LR 6.250e-05
0: TRAIN [1][3280/5173]	Time 0.621 (0.610)	Data 3.07e-04 (3.08e-04)	Tok/s 27028 (23232)	Loss/tok 3.4591 (3.4509)	LR 6.250e-05
0: TRAIN [1][3290/5173]	Time 0.704 (0.610)	Data 1.25e-04 (3.07e-04)	Tok/s 33295 (23233)	Loss/tok 3.6010 (3.4509)	LR 6.250e-05
0: TRAIN [1][3300/5173]	Time 0.567 (0.611)	Data 1.25e-04 (3.07e-04)	Tok/s 18328 (23235)	Loss/tok 3.1333 (3.4510)	LR 6.250e-05
0: TRAIN [1][3310/5173]	Time 0.497 (0.610)	Data 1.42e-04 (3.06e-04)	Tok/s 10486 (23225)	Loss/tok 2.7711 (3.4507)	LR 6.250e-05
0: TRAIN [1][3320/5173]	Time 0.560 (0.610)	Data 1.34e-04 (3.06e-04)	Tok/s 18335 (23223)	Loss/tok 3.2116 (3.4503)	LR 6.250e-05
0: TRAIN [1][3330/5173]	Time 0.564 (0.610)	Data 1.25e-04 (3.05e-04)	Tok/s 18443 (23216)	Loss/tok 3.2600 (3.4500)	LR 6.250e-05
0: TRAIN [1][3340/5173]	Time 0.568 (0.610)	Data 1.26e-04 (3.05e-04)	Tok/s 18525 (23209)	Loss/tok 3.1553 (3.4498)	LR 6.250e-05
0: TRAIN [1][3350/5173]	Time 0.567 (0.610)	Data 1.71e-04 (3.05e-04)	Tok/s 17875 (23223)	Loss/tok 3.1924 (3.4503)	LR 6.250e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][3360/5173]	Time 0.558 (0.610)	Data 1.31e-04 (3.04e-04)	Tok/s 18886 (23226)	Loss/tok 3.1248 (3.4502)	LR 6.250e-05
0: TRAIN [1][3370/5173]	Time 0.631 (0.610)	Data 1.28e-04 (3.04e-04)	Tok/s 26937 (23229)	Loss/tok 3.4025 (3.4502)	LR 6.250e-05
0: TRAIN [1][3380/5173]	Time 0.645 (0.610)	Data 1.32e-04 (3.03e-04)	Tok/s 26238 (23215)	Loss/tok 3.2859 (3.4497)	LR 6.250e-05
0: TRAIN [1][3390/5173]	Time 0.766 (0.610)	Data 1.35e-04 (3.03e-04)	Tok/s 39253 (23215)	Loss/tok 3.8132 (3.4496)	LR 6.250e-05
0: TRAIN [1][3400/5173]	Time 0.630 (0.610)	Data 1.25e-04 (3.02e-04)	Tok/s 26476 (23215)	Loss/tok 3.4104 (3.4495)	LR 6.250e-05
0: TRAIN [1][3410/5173]	Time 0.703 (0.610)	Data 1.30e-04 (3.02e-04)	Tok/s 33691 (23208)	Loss/tok 3.6404 (3.4492)	LR 6.250e-05
0: TRAIN [1][3420/5173]	Time 0.681 (0.610)	Data 1.32e-04 (3.01e-04)	Tok/s 34627 (23212)	Loss/tok 3.5019 (3.4491)	LR 6.250e-05
0: TRAIN [1][3430/5173]	Time 0.562 (0.610)	Data 1.27e-04 (3.01e-04)	Tok/s 17956 (23199)	Loss/tok 3.0963 (3.4487)	LR 6.250e-05
0: TRAIN [1][3440/5173]	Time 0.566 (0.610)	Data 1.33e-04 (3.00e-04)	Tok/s 18160 (23200)	Loss/tok 3.2134 (3.4487)	LR 6.250e-05
0: TRAIN [1][3450/5173]	Time 0.636 (0.610)	Data 1.30e-04 (3.00e-04)	Tok/s 26496 (23200)	Loss/tok 3.4083 (3.4484)	LR 6.250e-05
0: TRAIN [1][3460/5173]	Time 0.640 (0.610)	Data 1.30e-04 (2.99e-04)	Tok/s 26415 (23202)	Loss/tok 3.4465 (3.4483)	LR 6.250e-05
0: TRAIN [1][3470/5173]	Time 0.768 (0.610)	Data 1.38e-04 (2.99e-04)	Tok/s 38290 (23206)	Loss/tok 3.8976 (3.4484)	LR 6.250e-05
0: TRAIN [1][3480/5173]	Time 0.705 (0.610)	Data 1.32e-04 (2.98e-04)	Tok/s 32544 (23203)	Loss/tok 3.6878 (3.4483)	LR 6.250e-05
0: TRAIN [1][3490/5173]	Time 0.575 (0.610)	Data 1.26e-04 (2.98e-04)	Tok/s 18154 (23196)	Loss/tok 3.2367 (3.4482)	LR 6.250e-05
0: TRAIN [1][3500/5173]	Time 0.764 (0.610)	Data 1.25e-04 (2.98e-04)	Tok/s 39263 (23190)	Loss/tok 3.7964 (3.4481)	LR 6.250e-05
0: TRAIN [1][3510/5173]	Time 0.561 (0.610)	Data 1.31e-04 (2.97e-04)	Tok/s 18379 (23185)	Loss/tok 3.1391 (3.4477)	LR 6.250e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][3520/5173]	Time 0.767 (0.610)	Data 1.30e-04 (2.97e-04)	Tok/s 39336 (23183)	Loss/tok 3.8026 (3.4479)	LR 6.250e-05
0: TRAIN [1][3530/5173]	Time 0.695 (0.610)	Data 1.26e-04 (2.96e-04)	Tok/s 33167 (23185)	Loss/tok 3.6378 (3.4478)	LR 6.250e-05
0: TRAIN [1][3540/5173]	Time 0.626 (0.610)	Data 1.33e-04 (2.96e-04)	Tok/s 26645 (23182)	Loss/tok 3.3449 (3.4475)	LR 6.250e-05
0: TRAIN [1][3550/5173]	Time 0.502 (0.610)	Data 1.28e-04 (2.95e-04)	Tok/s 10484 (23181)	Loss/tok 2.7172 (3.4474)	LR 6.250e-05
0: TRAIN [1][3560/5173]	Time 0.764 (0.610)	Data 1.30e-04 (2.95e-04)	Tok/s 39158 (23186)	Loss/tok 3.6818 (3.4475)	LR 6.250e-05
0: TRAIN [1][3570/5173]	Time 0.566 (0.610)	Data 1.33e-04 (2.95e-04)	Tok/s 18318 (23187)	Loss/tok 3.1693 (3.4474)	LR 6.250e-05
0: TRAIN [1][3580/5173]	Time 0.611 (0.610)	Data 1.28e-04 (2.94e-04)	Tok/s 27167 (23183)	Loss/tok 3.5787 (3.4473)	LR 6.250e-05
0: TRAIN [1][3590/5173]	Time 0.637 (0.610)	Data 1.26e-04 (2.94e-04)	Tok/s 26334 (23176)	Loss/tok 3.3242 (3.4469)	LR 6.250e-05
0: TRAIN [1][3600/5173]	Time 0.629 (0.610)	Data 1.34e-04 (2.93e-04)	Tok/s 26817 (23181)	Loss/tok 3.3196 (3.4470)	LR 6.250e-05
0: TRAIN [1][3610/5173]	Time 0.775 (0.610)	Data 1.44e-04 (2.93e-04)	Tok/s 38729 (23186)	Loss/tok 3.7350 (3.4470)	LR 6.250e-05
0: TRAIN [1][3620/5173]	Time 0.769 (0.610)	Data 1.67e-04 (2.92e-04)	Tok/s 39197 (23197)	Loss/tok 3.8445 (3.4474)	LR 6.250e-05
0: TRAIN [1][3630/5173]	Time 0.637 (0.610)	Data 2.80e-04 (2.92e-04)	Tok/s 26303 (23193)	Loss/tok 3.4653 (3.4472)	LR 6.250e-05
0: TRAIN [1][3640/5173]	Time 0.569 (0.610)	Data 1.31e-04 (2.92e-04)	Tok/s 18318 (23186)	Loss/tok 3.1533 (3.4469)	LR 6.250e-05
0: TRAIN [1][3650/5173]	Time 0.640 (0.610)	Data 1.25e-04 (2.91e-04)	Tok/s 26318 (23183)	Loss/tok 3.4921 (3.4467)	LR 6.250e-05
0: TRAIN [1][3660/5173]	Time 0.624 (0.610)	Data 1.28e-04 (2.91e-04)	Tok/s 27022 (23186)	Loss/tok 3.4905 (3.4466)	LR 6.250e-05
0: TRAIN [1][3670/5173]	Time 0.566 (0.610)	Data 1.34e-04 (2.90e-04)	Tok/s 18394 (23177)	Loss/tok 3.1401 (3.4462)	LR 6.250e-05
0: TRAIN [1][3680/5173]	Time 0.694 (0.610)	Data 1.37e-04 (2.90e-04)	Tok/s 33998 (23170)	Loss/tok 3.5952 (3.4460)	LR 6.250e-05
0: TRAIN [1][3690/5173]	Time 0.624 (0.610)	Data 1.31e-04 (2.90e-04)	Tok/s 26867 (23176)	Loss/tok 3.3591 (3.4460)	LR 6.250e-05
0: TRAIN [1][3700/5173]	Time 0.561 (0.610)	Data 1.34e-04 (2.89e-04)	Tok/s 18318 (23172)	Loss/tok 3.2877 (3.4461)	LR 6.250e-05
0: TRAIN [1][3710/5173]	Time 0.567 (0.610)	Data 1.32e-04 (2.89e-04)	Tok/s 17974 (23165)	Loss/tok 3.1947 (3.4459)	LR 6.250e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][3720/5173]	Time 0.503 (0.610)	Data 1.30e-04 (2.88e-04)	Tok/s 10466 (23159)	Loss/tok 2.7841 (3.4457)	LR 6.250e-05
0: TRAIN [1][3730/5173]	Time 0.624 (0.610)	Data 1.34e-04 (2.88e-04)	Tok/s 26939 (23156)	Loss/tok 3.2854 (3.4455)	LR 6.250e-05
0: TRAIN [1][3740/5173]	Time 0.637 (0.610)	Data 1.31e-04 (2.87e-04)	Tok/s 25847 (23157)	Loss/tok 3.4210 (3.4454)	LR 6.250e-05
0: TRAIN [1][3750/5173]	Time 0.772 (0.610)	Data 1.28e-04 (2.87e-04)	Tok/s 38441 (23159)	Loss/tok 3.8173 (3.4456)	LR 6.250e-05
0: TRAIN [1][3760/5173]	Time 0.623 (0.610)	Data 1.31e-04 (2.87e-04)	Tok/s 26975 (23171)	Loss/tok 3.4605 (3.4457)	LR 6.250e-05
0: TRAIN [1][3770/5173]	Time 0.567 (0.610)	Data 1.26e-04 (2.86e-04)	Tok/s 17687 (23163)	Loss/tok 3.1849 (3.4453)	LR 6.250e-05
0: TRAIN [1][3780/5173]	Time 0.568 (0.610)	Data 1.22e-04 (2.86e-04)	Tok/s 18552 (23161)	Loss/tok 3.1643 (3.4451)	LR 6.250e-05
0: TRAIN [1][3790/5173]	Time 0.498 (0.610)	Data 1.28e-04 (2.85e-04)	Tok/s 10859 (23167)	Loss/tok 2.7365 (3.4454)	LR 6.250e-05
0: TRAIN [1][3800/5173]	Time 0.772 (0.610)	Data 1.26e-04 (2.85e-04)	Tok/s 38559 (23168)	Loss/tok 3.7142 (3.4454)	LR 6.250e-05
0: TRAIN [1][3810/5173]	Time 0.700 (0.610)	Data 1.24e-04 (2.85e-04)	Tok/s 33396 (23165)	Loss/tok 3.6163 (3.4452)	LR 6.250e-05
0: TRAIN [1][3820/5173]	Time 0.635 (0.610)	Data 1.27e-04 (2.84e-04)	Tok/s 26539 (23159)	Loss/tok 3.5292 (3.4452)	LR 6.250e-05
0: TRAIN [1][3830/5173]	Time 0.699 (0.610)	Data 1.37e-04 (2.84e-04)	Tok/s 33289 (23156)	Loss/tok 3.6530 (3.4449)	LR 6.250e-05
0: TRAIN [1][3840/5173]	Time 0.642 (0.610)	Data 1.30e-04 (2.84e-04)	Tok/s 26551 (23152)	Loss/tok 3.4001 (3.4448)	LR 6.250e-05
0: TRAIN [1][3850/5173]	Time 0.703 (0.609)	Data 1.33e-04 (2.83e-04)	Tok/s 32807 (23139)	Loss/tok 3.6617 (3.4444)	LR 6.250e-05
0: TRAIN [1][3860/5173]	Time 0.624 (0.609)	Data 1.38e-04 (2.83e-04)	Tok/s 27045 (23141)	Loss/tok 3.4337 (3.4444)	LR 6.250e-05
0: TRAIN [1][3870/5173]	Time 0.571 (0.609)	Data 1.27e-04 (2.82e-04)	Tok/s 18121 (23135)	Loss/tok 3.1982 (3.4441)	LR 6.250e-05
0: TRAIN [1][3880/5173]	Time 0.560 (0.609)	Data 1.34e-04 (2.82e-04)	Tok/s 18761 (23127)	Loss/tok 3.0977 (3.4437)	LR 6.250e-05
0: TRAIN [1][3890/5173]	Time 0.548 (0.609)	Data 1.27e-04 (2.82e-04)	Tok/s 18765 (23120)	Loss/tok 3.2343 (3.4436)	LR 6.250e-05
0: TRAIN [1][3900/5173]	Time 0.566 (0.609)	Data 1.29e-04 (2.81e-04)	Tok/s 18117 (23116)	Loss/tok 3.2417 (3.4434)	LR 6.250e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][3910/5173]	Time 0.642 (0.609)	Data 1.27e-04 (2.81e-04)	Tok/s 26466 (23118)	Loss/tok 3.4454 (3.4434)	LR 6.250e-05
0: TRAIN [1][3920/5173]	Time 0.562 (0.609)	Data 1.22e-04 (2.80e-04)	Tok/s 18405 (23118)	Loss/tok 3.1738 (3.4434)	LR 6.250e-05
0: TRAIN [1][3930/5173]	Time 0.643 (0.609)	Data 1.27e-04 (2.80e-04)	Tok/s 26097 (23123)	Loss/tok 3.5327 (3.4436)	LR 6.250e-05
0: TRAIN [1][3940/5173]	Time 0.704 (0.609)	Data 1.29e-04 (2.80e-04)	Tok/s 32860 (23124)	Loss/tok 3.7135 (3.4436)	LR 6.250e-05
0: TRAIN [1][3950/5173]	Time 0.566 (0.609)	Data 3.21e-04 (2.79e-04)	Tok/s 18353 (23130)	Loss/tok 3.1471 (3.4435)	LR 6.250e-05
0: TRAIN [1][3960/5173]	Time 0.567 (0.609)	Data 1.38e-04 (2.79e-04)	Tok/s 18120 (23126)	Loss/tok 3.1416 (3.4433)	LR 6.250e-05
0: TRAIN [1][3970/5173]	Time 0.696 (0.609)	Data 1.25e-04 (2.79e-04)	Tok/s 33508 (23126)	Loss/tok 3.6283 (3.4433)	LR 6.250e-05
0: TRAIN [1][3980/5173]	Time 0.628 (0.609)	Data 1.36e-04 (2.78e-04)	Tok/s 26638 (23136)	Loss/tok 3.4273 (3.4435)	LR 6.250e-05
0: TRAIN [1][3990/5173]	Time 0.569 (0.609)	Data 1.36e-04 (2.78e-04)	Tok/s 17916 (23132)	Loss/tok 3.0398 (3.4433)	LR 6.250e-05
0: TRAIN [1][4000/5173]	Time 0.567 (0.609)	Data 1.26e-04 (2.78e-04)	Tok/s 18110 (23137)	Loss/tok 3.1531 (3.4433)	LR 6.250e-05
0: TRAIN [1][4010/5173]	Time 0.630 (0.609)	Data 1.39e-04 (2.77e-04)	Tok/s 26963 (23131)	Loss/tok 3.4399 (3.4431)	LR 6.250e-05
0: TRAIN [1][4020/5173]	Time 0.631 (0.609)	Data 1.33e-04 (2.77e-04)	Tok/s 26708 (23139)	Loss/tok 3.3646 (3.4434)	LR 6.250e-05
0: TRAIN [1][4030/5173]	Time 0.563 (0.609)	Data 1.25e-04 (2.77e-04)	Tok/s 18562 (23137)	Loss/tok 3.2298 (3.4432)	LR 6.250e-05
0: TRAIN [1][4040/5173]	Time 0.504 (0.609)	Data 1.27e-04 (2.76e-04)	Tok/s 10533 (23139)	Loss/tok 2.8112 (3.4433)	LR 3.125e-05
0: TRAIN [1][4050/5173]	Time 0.642 (0.609)	Data 1.36e-04 (2.76e-04)	Tok/s 25965 (23134)	Loss/tok 3.4532 (3.4431)	LR 3.125e-05
0: TRAIN [1][4060/5173]	Time 0.567 (0.609)	Data 1.27e-04 (2.76e-04)	Tok/s 18528 (23139)	Loss/tok 3.0726 (3.4432)	LR 3.125e-05
0: TRAIN [1][4070/5173]	Time 0.630 (0.609)	Data 1.32e-04 (2.75e-04)	Tok/s 26596 (23127)	Loss/tok 3.3323 (3.4428)	LR 3.125e-05
0: TRAIN [1][4080/5173]	Time 0.560 (0.609)	Data 1.46e-04 (2.75e-04)	Tok/s 18020 (23133)	Loss/tok 3.1688 (3.4431)	LR 3.125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][4090/5173]	Time 0.569 (0.609)	Data 1.30e-04 (2.75e-04)	Tok/s 17859 (23138)	Loss/tok 3.1393 (3.4430)	LR 3.125e-05
0: TRAIN [1][4100/5173]	Time 0.693 (0.610)	Data 3.19e-04 (2.74e-04)	Tok/s 33663 (23152)	Loss/tok 3.6731 (3.4433)	LR 3.125e-05
0: TRAIN [1][4110/5173]	Time 0.632 (0.610)	Data 1.29e-04 (2.74e-04)	Tok/s 26914 (23150)	Loss/tok 3.4223 (3.4431)	LR 3.125e-05
0: TRAIN [1][4120/5173]	Time 0.505 (0.610)	Data 1.40e-04 (2.74e-04)	Tok/s 10305 (23150)	Loss/tok 2.6921 (3.4430)	LR 3.125e-05
0: TRAIN [1][4130/5173]	Time 0.573 (0.609)	Data 1.23e-04 (2.73e-04)	Tok/s 18196 (23147)	Loss/tok 3.2349 (3.4429)	LR 3.125e-05
0: TRAIN [1][4140/5173]	Time 0.565 (0.609)	Data 1.24e-04 (2.73e-04)	Tok/s 18326 (23148)	Loss/tok 3.2597 (3.4428)	LR 3.125e-05
0: TRAIN [1][4150/5173]	Time 0.704 (0.609)	Data 1.30e-04 (2.73e-04)	Tok/s 32936 (23148)	Loss/tok 3.5217 (3.4427)	LR 3.125e-05
0: TRAIN [1][4160/5173]	Time 0.566 (0.609)	Data 1.24e-04 (2.72e-04)	Tok/s 18116 (23143)	Loss/tok 3.1294 (3.4425)	LR 3.125e-05
0: TRAIN [1][4170/5173]	Time 0.624 (0.609)	Data 1.32e-04 (2.72e-04)	Tok/s 26669 (23141)	Loss/tok 3.4546 (3.4424)	LR 3.125e-05
0: TRAIN [1][4180/5173]	Time 0.572 (0.609)	Data 1.30e-04 (2.72e-04)	Tok/s 17601 (23136)	Loss/tok 3.1900 (3.4423)	LR 3.125e-05
0: TRAIN [1][4190/5173]	Time 0.564 (0.609)	Data 1.28e-04 (2.72e-04)	Tok/s 18334 (23130)	Loss/tok 3.0990 (3.4421)	LR 3.125e-05
0: TRAIN [1][4200/5173]	Time 0.764 (0.609)	Data 1.34e-04 (2.71e-04)	Tok/s 39350 (23129)	Loss/tok 3.7785 (3.4420)	LR 3.125e-05
0: TRAIN [1][4210/5173]	Time 0.638 (0.609)	Data 1.32e-04 (2.71e-04)	Tok/s 26032 (23127)	Loss/tok 3.3895 (3.4419)	LR 3.125e-05
0: TRAIN [1][4220/5173]	Time 0.702 (0.609)	Data 1.29e-04 (2.71e-04)	Tok/s 33515 (23123)	Loss/tok 3.6013 (3.4416)	LR 3.125e-05
0: TRAIN [1][4230/5173]	Time 0.639 (0.609)	Data 1.34e-04 (2.70e-04)	Tok/s 26350 (23123)	Loss/tok 3.3373 (3.4414)	LR 3.125e-05
0: TRAIN [1][4240/5173]	Time 0.569 (0.609)	Data 1.39e-04 (2.70e-04)	Tok/s 18157 (23121)	Loss/tok 3.3359 (3.4413)	LR 3.125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][4250/5173]	Time 0.561 (0.609)	Data 1.29e-04 (2.70e-04)	Tok/s 18267 (23115)	Loss/tok 3.1752 (3.4411)	LR 3.125e-05
0: TRAIN [1][4260/5173]	Time 0.500 (0.609)	Data 1.35e-04 (2.69e-04)	Tok/s 10757 (23108)	Loss/tok 2.8019 (3.4409)	LR 3.125e-05
0: TRAIN [1][4270/5173]	Time 0.567 (0.609)	Data 1.32e-04 (2.69e-04)	Tok/s 18178 (23112)	Loss/tok 3.2362 (3.4411)	LR 3.125e-05
0: TRAIN [1][4280/5173]	Time 0.623 (0.609)	Data 2.98e-04 (2.69e-04)	Tok/s 27483 (23120)	Loss/tok 3.3604 (3.4413)	LR 3.125e-05
0: TRAIN [1][4290/5173]	Time 0.574 (0.609)	Data 3.10e-04 (2.68e-04)	Tok/s 17693 (23122)	Loss/tok 3.1122 (3.4413)	LR 3.125e-05
0: TRAIN [1][4300/5173]	Time 0.617 (0.609)	Data 1.34e-04 (2.68e-04)	Tok/s 27356 (23119)	Loss/tok 3.3887 (3.4413)	LR 3.125e-05
0: TRAIN [1][4310/5173]	Time 0.568 (0.609)	Data 1.27e-04 (2.68e-04)	Tok/s 18105 (23120)	Loss/tok 3.1169 (3.4412)	LR 3.125e-05
0: TRAIN [1][4320/5173]	Time 0.564 (0.609)	Data 1.29e-04 (2.68e-04)	Tok/s 17948 (23128)	Loss/tok 3.2200 (3.4412)	LR 3.125e-05
0: TRAIN [1][4330/5173]	Time 0.497 (0.609)	Data 1.32e-04 (2.67e-04)	Tok/s 10516 (23126)	Loss/tok 2.6396 (3.4411)	LR 3.125e-05
0: TRAIN [1][4340/5173]	Time 0.569 (0.609)	Data 1.32e-04 (2.67e-04)	Tok/s 18354 (23123)	Loss/tok 3.1737 (3.4411)	LR 3.125e-05
0: TRAIN [1][4350/5173]	Time 0.567 (0.609)	Data 1.25e-04 (2.67e-04)	Tok/s 17953 (23121)	Loss/tok 3.0551 (3.4410)	LR 3.125e-05
0: TRAIN [1][4360/5173]	Time 0.505 (0.609)	Data 1.31e-04 (2.66e-04)	Tok/s 10262 (23122)	Loss/tok 2.8103 (3.4410)	LR 3.125e-05
0: TRAIN [1][4370/5173]	Time 0.703 (0.609)	Data 1.36e-04 (2.66e-04)	Tok/s 32954 (23125)	Loss/tok 3.6407 (3.4409)	LR 3.125e-05
0: TRAIN [1][4380/5173]	Time 0.562 (0.609)	Data 1.34e-04 (2.66e-04)	Tok/s 18464 (23135)	Loss/tok 3.3073 (3.4412)	LR 3.125e-05
0: TRAIN [1][4390/5173]	Time 0.576 (0.609)	Data 1.29e-04 (2.66e-04)	Tok/s 18495 (23131)	Loss/tok 3.1456 (3.4410)	LR 3.125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][4400/5173]	Time 0.559 (0.609)	Data 1.29e-04 (2.65e-04)	Tok/s 18715 (23140)	Loss/tok 3.2836 (3.4412)	LR 3.125e-05
0: TRAIN [1][4410/5173]	Time 0.701 (0.609)	Data 1.18e-04 (2.65e-04)	Tok/s 33564 (23142)	Loss/tok 3.5110 (3.4411)	LR 3.125e-05
0: TRAIN [1][4420/5173]	Time 0.636 (0.610)	Data 1.33e-04 (2.65e-04)	Tok/s 26628 (23148)	Loss/tok 3.3695 (3.4412)	LR 3.125e-05
0: TRAIN [1][4430/5173]	Time 0.627 (0.610)	Data 1.30e-04 (2.64e-04)	Tok/s 26547 (23147)	Loss/tok 3.4179 (3.4410)	LR 3.125e-05
0: TRAIN [1][4440/5173]	Time 0.566 (0.610)	Data 1.24e-04 (2.64e-04)	Tok/s 18669 (23154)	Loss/tok 3.2512 (3.4410)	LR 3.125e-05
0: TRAIN [1][4450/5173]	Time 0.498 (0.610)	Data 1.23e-04 (2.64e-04)	Tok/s 10465 (23148)	Loss/tok 2.7244 (3.4408)	LR 3.125e-05
0: TRAIN [1][4460/5173]	Time 0.561 (0.609)	Data 1.19e-04 (2.64e-04)	Tok/s 18635 (23137)	Loss/tok 3.1588 (3.4406)	LR 3.125e-05
0: TRAIN [1][4470/5173]	Time 0.703 (0.609)	Data 1.20e-04 (2.63e-04)	Tok/s 32654 (23140)	Loss/tok 3.6822 (3.4405)	LR 3.125e-05
0: TRAIN [1][4480/5173]	Time 0.645 (0.609)	Data 1.21e-04 (2.63e-04)	Tok/s 25912 (23142)	Loss/tok 3.3815 (3.4405)	LR 3.125e-05
0: TRAIN [1][4490/5173]	Time 0.770 (0.609)	Data 1.18e-04 (2.63e-04)	Tok/s 38497 (23140)	Loss/tok 3.7342 (3.4404)	LR 3.125e-05
0: TRAIN [1][4500/5173]	Time 0.563 (0.610)	Data 1.19e-04 (2.63e-04)	Tok/s 18056 (23148)	Loss/tok 3.2043 (3.4405)	LR 3.125e-05
0: TRAIN [1][4510/5173]	Time 0.568 (0.609)	Data 1.25e-04 (2.62e-04)	Tok/s 18146 (23139)	Loss/tok 3.1967 (3.4402)	LR 3.125e-05
0: TRAIN [1][4520/5173]	Time 0.564 (0.609)	Data 1.19e-04 (2.62e-04)	Tok/s 18305 (23134)	Loss/tok 3.0449 (3.4399)	LR 3.125e-05
0: TRAIN [1][4530/5173]	Time 0.500 (0.609)	Data 1.23e-04 (2.62e-04)	Tok/s 10689 (23137)	Loss/tok 2.7345 (3.4399)	LR 3.125e-05
0: TRAIN [1][4540/5173]	Time 0.502 (0.609)	Data 1.22e-04 (2.61e-04)	Tok/s 10702 (23136)	Loss/tok 2.7554 (3.4397)	LR 3.125e-05
0: TRAIN [1][4550/5173]	Time 0.624 (0.609)	Data 1.22e-04 (2.61e-04)	Tok/s 26732 (23134)	Loss/tok 3.4033 (3.4395)	LR 3.125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][4560/5173]	Time 0.568 (0.609)	Data 1.26e-04 (2.61e-04)	Tok/s 18607 (23133)	Loss/tok 3.0944 (3.4394)	LR 3.125e-05
0: TRAIN [1][4570/5173]	Time 0.502 (0.609)	Data 1.35e-04 (2.61e-04)	Tok/s 10382 (23133)	Loss/tok 2.7132 (3.4394)	LR 3.125e-05
0: TRAIN [1][4580/5173]	Time 0.636 (0.609)	Data 1.24e-04 (2.60e-04)	Tok/s 26229 (23140)	Loss/tok 3.4485 (3.4394)	LR 3.125e-05
0: TRAIN [1][4590/5173]	Time 0.631 (0.609)	Data 1.18e-04 (2.60e-04)	Tok/s 27021 (23141)	Loss/tok 3.4252 (3.4394)	LR 3.125e-05
0: TRAIN [1][4600/5173]	Time 0.571 (0.609)	Data 1.17e-04 (2.60e-04)	Tok/s 18047 (23142)	Loss/tok 3.0638 (3.4393)	LR 3.125e-05
0: TRAIN [1][4610/5173]	Time 0.633 (0.609)	Data 1.20e-04 (2.59e-04)	Tok/s 26526 (23144)	Loss/tok 3.4054 (3.4392)	LR 3.125e-05
0: TRAIN [1][4620/5173]	Time 0.569 (0.609)	Data 1.27e-04 (2.59e-04)	Tok/s 18047 (23146)	Loss/tok 3.1860 (3.4393)	LR 3.125e-05
0: TRAIN [1][4630/5173]	Time 0.568 (0.609)	Data 1.21e-04 (2.59e-04)	Tok/s 18517 (23138)	Loss/tok 3.1481 (3.4390)	LR 3.125e-05
0: TRAIN [1][4640/5173]	Time 0.563 (0.609)	Data 1.21e-04 (2.59e-04)	Tok/s 18251 (23129)	Loss/tok 3.1844 (3.4387)	LR 3.125e-05
0: TRAIN [1][4650/5173]	Time 0.560 (0.609)	Data 1.27e-04 (2.58e-04)	Tok/s 18391 (23124)	Loss/tok 3.1642 (3.4384)	LR 3.125e-05
0: TRAIN [1][4660/5173]	Time 0.765 (0.609)	Data 1.24e-04 (2.58e-04)	Tok/s 38960 (23121)	Loss/tok 3.8613 (3.4384)	LR 3.125e-05
0: TRAIN [1][4670/5173]	Time 0.625 (0.609)	Data 1.21e-04 (2.58e-04)	Tok/s 26841 (23125)	Loss/tok 3.4499 (3.4384)	LR 3.125e-05
0: TRAIN [1][4680/5173]	Time 0.568 (0.609)	Data 1.23e-04 (2.57e-04)	Tok/s 18269 (23124)	Loss/tok 3.1498 (3.4382)	LR 3.125e-05
0: TRAIN [1][4690/5173]	Time 0.691 (0.609)	Data 1.19e-04 (2.57e-04)	Tok/s 33736 (23128)	Loss/tok 3.5805 (3.4381)	LR 3.125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][4700/5173]	Time 0.501 (0.609)	Data 1.22e-04 (2.57e-04)	Tok/s 10779 (23128)	Loss/tok 2.7265 (3.4382)	LR 3.125e-05
0: TRAIN [1][4710/5173]	Time 0.565 (0.609)	Data 1.30e-04 (2.57e-04)	Tok/s 18138 (23119)	Loss/tok 3.1137 (3.4379)	LR 3.125e-05
0: TRAIN [1][4720/5173]	Time 0.562 (0.609)	Data 1.20e-04 (2.56e-04)	Tok/s 18229 (23109)	Loss/tok 3.1806 (3.4377)	LR 3.125e-05
0: TRAIN [1][4730/5173]	Time 0.570 (0.609)	Data 1.21e-04 (2.56e-04)	Tok/s 17940 (23112)	Loss/tok 3.1774 (3.4379)	LR 3.125e-05
0: TRAIN [1][4740/5173]	Time 0.571 (0.609)	Data 1.24e-04 (2.56e-04)	Tok/s 18230 (23113)	Loss/tok 3.1322 (3.4379)	LR 3.125e-05
0: TRAIN [1][4750/5173]	Time 0.501 (0.609)	Data 1.21e-04 (2.56e-04)	Tok/s 10479 (23112)	Loss/tok 2.7491 (3.4377)	LR 3.125e-05
0: TRAIN [1][4760/5173]	Time 0.625 (0.609)	Data 1.24e-04 (2.55e-04)	Tok/s 26442 (23107)	Loss/tok 3.3928 (3.4375)	LR 3.125e-05
0: TRAIN [1][4770/5173]	Time 0.764 (0.609)	Data 1.32e-04 (2.55e-04)	Tok/s 38506 (23105)	Loss/tok 3.8248 (3.4376)	LR 3.125e-05
0: TRAIN [1][4780/5173]	Time 0.567 (0.609)	Data 1.23e-04 (2.55e-04)	Tok/s 17925 (23106)	Loss/tok 3.2178 (3.4375)	LR 3.125e-05
0: TRAIN [1][4790/5173]	Time 0.635 (0.609)	Data 1.18e-04 (2.55e-04)	Tok/s 26219 (23102)	Loss/tok 3.5131 (3.4373)	LR 3.125e-05
0: TRAIN [1][4800/5173]	Time 0.640 (0.609)	Data 1.21e-04 (2.54e-04)	Tok/s 26575 (23104)	Loss/tok 3.4184 (3.4374)	LR 3.125e-05
0: TRAIN [1][4810/5173]	Time 0.502 (0.609)	Data 1.21e-04 (2.54e-04)	Tok/s 10581 (23105)	Loss/tok 2.7420 (3.4375)	LR 3.125e-05
0: TRAIN [1][4820/5173]	Time 0.703 (0.609)	Data 1.20e-04 (2.54e-04)	Tok/s 32956 (23118)	Loss/tok 3.6282 (3.4379)	LR 3.125e-05
0: TRAIN [1][4830/5173]	Time 0.642 (0.609)	Data 1.24e-04 (2.54e-04)	Tok/s 26238 (23119)	Loss/tok 3.3615 (3.4379)	LR 3.125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][4840/5173]	Time 0.765 (0.609)	Data 1.21e-04 (2.53e-04)	Tok/s 38814 (23123)	Loss/tok 3.6763 (3.4379)	LR 3.125e-05
0: TRAIN [1][4850/5173]	Time 0.570 (0.609)	Data 1.17e-04 (2.53e-04)	Tok/s 18325 (23123)	Loss/tok 3.1449 (3.4378)	LR 1.563e-05
0: TRAIN [1][4860/5173]	Time 0.629 (0.609)	Data 1.22e-04 (2.53e-04)	Tok/s 26702 (23120)	Loss/tok 3.3805 (3.4375)	LR 1.563e-05
0: TRAIN [1][4870/5173]	Time 0.699 (0.609)	Data 1.18e-04 (2.53e-04)	Tok/s 33122 (23126)	Loss/tok 3.6530 (3.4377)	LR 1.563e-05
0: TRAIN [1][4880/5173]	Time 0.640 (0.609)	Data 1.27e-04 (2.52e-04)	Tok/s 26240 (23127)	Loss/tok 3.5553 (3.4376)	LR 1.563e-05
0: TRAIN [1][4890/5173]	Time 0.635 (0.609)	Data 1.24e-04 (2.52e-04)	Tok/s 26376 (23126)	Loss/tok 3.3751 (3.4375)	LR 1.563e-05
0: TRAIN [1][4900/5173]	Time 0.640 (0.609)	Data 1.22e-04 (2.52e-04)	Tok/s 26414 (23126)	Loss/tok 3.3901 (3.4374)	LR 1.563e-05
0: TRAIN [1][4910/5173]	Time 0.638 (0.609)	Data 1.18e-04 (2.52e-04)	Tok/s 26623 (23125)	Loss/tok 3.3564 (3.4374)	LR 1.563e-05
0: TRAIN [1][4920/5173]	Time 0.569 (0.609)	Data 1.17e-04 (2.51e-04)	Tok/s 18245 (23125)	Loss/tok 3.1612 (3.4372)	LR 1.563e-05
0: TRAIN [1][4930/5173]	Time 0.702 (0.609)	Data 1.16e-04 (2.51e-04)	Tok/s 33575 (23128)	Loss/tok 3.5731 (3.4371)	LR 1.563e-05
0: TRAIN [1][4940/5173]	Time 0.570 (0.609)	Data 1.21e-04 (2.51e-04)	Tok/s 17969 (23128)	Loss/tok 3.2455 (3.4370)	LR 1.563e-05
0: TRAIN [1][4950/5173]	Time 0.564 (0.609)	Data 1.23e-04 (2.51e-04)	Tok/s 17727 (23128)	Loss/tok 3.2467 (3.4369)	LR 1.563e-05
0: TRAIN [1][4960/5173]	Time 0.643 (0.609)	Data 1.19e-04 (2.50e-04)	Tok/s 25879 (23124)	Loss/tok 3.2965 (3.4369)	LR 1.563e-05
0: TRAIN [1][4970/5173]	Time 0.701 (0.609)	Data 1.21e-04 (2.50e-04)	Tok/s 33119 (23127)	Loss/tok 3.7440 (3.4370)	LR 1.563e-05
0: TRAIN [1][4980/5173]	Time 0.572 (0.609)	Data 1.23e-04 (2.50e-04)	Tok/s 17657 (23130)	Loss/tok 3.1923 (3.4370)	LR 1.563e-05
0: TRAIN [1][4990/5173]	Time 0.568 (0.609)	Data 1.19e-04 (2.50e-04)	Tok/s 18348 (23129)	Loss/tok 3.1961 (3.4369)	LR 1.563e-05
0: TRAIN [1][5000/5173]	Time 0.637 (0.609)	Data 1.21e-04 (2.49e-04)	Tok/s 26476 (23127)	Loss/tok 3.3874 (3.4368)	LR 1.563e-05
0: TRAIN [1][5010/5173]	Time 0.566 (0.609)	Data 1.18e-04 (2.49e-04)	Tok/s 18185 (23125)	Loss/tok 3.2766 (3.4369)	LR 1.563e-05
0: TRAIN [1][5020/5173]	Time 0.631 (0.609)	Data 1.21e-04 (2.49e-04)	Tok/s 26801 (23128)	Loss/tok 3.2848 (3.4368)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][5030/5173]	Time 0.559 (0.609)	Data 1.23e-04 (2.49e-04)	Tok/s 18566 (23121)	Loss/tok 3.1450 (3.4367)	LR 1.563e-05
0: TRAIN [1][5040/5173]	Time 0.691 (0.609)	Data 1.22e-04 (2.48e-04)	Tok/s 33878 (23121)	Loss/tok 3.6382 (3.4366)	LR 1.563e-05
0: TRAIN [1][5050/5173]	Time 0.561 (0.609)	Data 3.07e-04 (2.48e-04)	Tok/s 18255 (23124)	Loss/tok 3.1620 (3.4368)	LR 1.563e-05
0: TRAIN [1][5060/5173]	Time 0.568 (0.609)	Data 1.23e-04 (2.48e-04)	Tok/s 18285 (23126)	Loss/tok 3.1881 (3.4366)	LR 1.563e-05
0: TRAIN [1][5070/5173]	Time 0.705 (0.609)	Data 1.15e-04 (2.48e-04)	Tok/s 33185 (23127)	Loss/tok 3.6097 (3.4366)	LR 1.563e-05
0: TRAIN [1][5080/5173]	Time 0.566 (0.609)	Data 1.22e-04 (2.47e-04)	Tok/s 18327 (23126)	Loss/tok 3.2167 (3.4367)	LR 1.563e-05
0: TRAIN [1][5090/5173]	Time 0.564 (0.609)	Data 1.22e-04 (2.47e-04)	Tok/s 18443 (23126)	Loss/tok 3.1723 (3.4366)	LR 1.563e-05
0: TRAIN [1][5100/5173]	Time 0.644 (0.609)	Data 1.30e-04 (2.47e-04)	Tok/s 26511 (23121)	Loss/tok 3.4177 (3.4364)	LR 1.563e-05
0: TRAIN [1][5110/5173]	Time 0.498 (0.609)	Data 1.53e-04 (2.47e-04)	Tok/s 10719 (23129)	Loss/tok 2.7156 (3.4367)	LR 1.563e-05
0: TRAIN [1][5120/5173]	Time 0.640 (0.609)	Data 1.14e-04 (2.47e-04)	Tok/s 26725 (23131)	Loss/tok 3.3898 (3.4367)	LR 1.563e-05
0: TRAIN [1][5130/5173]	Time 0.627 (0.609)	Data 1.35e-04 (2.47e-04)	Tok/s 26922 (23138)	Loss/tok 3.5058 (3.4369)	LR 1.563e-05
0: TRAIN [1][5140/5173]	Time 0.776 (0.609)	Data 1.18e-04 (2.47e-04)	Tok/s 37489 (23143)	Loss/tok 3.9442 (3.4371)	LR 1.563e-05
0: TRAIN [1][5150/5173]	Time 0.636 (0.610)	Data 1.26e-04 (2.46e-04)	Tok/s 26076 (23148)	Loss/tok 3.5516 (3.4373)	LR 1.563e-05
0: TRAIN [1][5160/5173]	Time 0.568 (0.610)	Data 1.16e-04 (2.46e-04)	Tok/s 18205 (23153)	Loss/tok 3.2910 (3.4373)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][5170/5173]	Time 0.632 (0.610)	Data 1.22e-04 (2.46e-04)	Tok/s 26696 (23158)	Loss/tok 3.4246 (3.4374)	LR 1.563e-05
:::MLL 1585771666.309 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 525}}
:::MLL 1585771666.309 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [1][0/8]	Time 0.652 (0.652)	Decoder iters 107.0 (107.0)	Tok/s 25113 (25113)
0: Running moses detokenizer
0: BLEU(score=20.535515390022304, counts=[34667, 16094, 8651, 4841], totals=[64802, 61799, 58797, 55800], precisions=[53.49680565414648, 26.042492596967588, 14.713335714407197, 8.675627240143369], bp=1.0, sys_len=64802, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1585771672.862 eval_accuracy: {"value": 20.54, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 536}}
:::MLL 1585771672.862 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 1	Training Loss: 3.4388	Test BLEU: 20.54
0: Performance: Epoch: 1	Training: 69475 Tok/s
0: Finished epoch 1
:::MLL 1585771672.863 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 558}}
:::MLL 1585771672.863 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1585771672.863 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 515}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 514977025
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][0/5173]	Time 1.144 (1.144)	Data 5.36e-01 (5.36e-01)	Tok/s 8943 (8943)	Loss/tok 3.2501 (3.2501)	LR 1.563e-05
0: TRAIN [2][10/5173]	Time 0.633 (0.649)	Data 1.80e-04 (4.90e-02)	Tok/s 26755 (21160)	Loss/tok 3.3981 (3.3716)	LR 1.563e-05
0: TRAIN [2][20/5173]	Time 0.571 (0.622)	Data 1.78e-04 (2.58e-02)	Tok/s 18195 (21442)	Loss/tok 3.2940 (3.3438)	LR 1.563e-05
0: TRAIN [2][30/5173]	Time 0.572 (0.616)	Data 1.94e-04 (1.75e-02)	Tok/s 18243 (21703)	Loss/tok 3.2821 (3.3497)	LR 1.563e-05
0: TRAIN [2][40/5173]	Time 0.569 (0.607)	Data 1.89e-04 (1.33e-02)	Tok/s 18334 (21273)	Loss/tok 3.0142 (3.3320)	LR 1.563e-05
0: TRAIN [2][50/5173]	Time 0.564 (0.609)	Data 1.80e-04 (1.07e-02)	Tok/s 18428 (21866)	Loss/tok 3.1826 (3.3600)	LR 1.563e-05
0: TRAIN [2][60/5173]	Time 0.636 (0.615)	Data 2.08e-04 (9.01e-03)	Tok/s 26352 (22762)	Loss/tok 3.3697 (3.3922)	LR 1.563e-05
0: TRAIN [2][70/5173]	Time 0.557 (0.611)	Data 1.72e-04 (7.76e-03)	Tok/s 18711 (22497)	Loss/tok 3.3019 (3.3771)	LR 1.563e-05
0: TRAIN [2][80/5173]	Time 0.688 (0.614)	Data 1.78e-04 (6.83e-03)	Tok/s 33479 (22924)	Loss/tok 3.6091 (3.3960)	LR 1.563e-05
0: TRAIN [2][90/5173]	Time 0.560 (0.610)	Data 1.52e-04 (6.10e-03)	Tok/s 18736 (22574)	Loss/tok 3.3422 (3.3872)	LR 1.563e-05
0: TRAIN [2][100/5173]	Time 0.765 (0.608)	Data 1.74e-04 (5.52e-03)	Tok/s 39007 (22438)	Loss/tok 3.7524 (3.3811)	LR 1.563e-05
0: TRAIN [2][110/5173]	Time 0.566 (0.607)	Data 1.35e-04 (5.04e-03)	Tok/s 18137 (22356)	Loss/tok 3.2966 (3.3790)	LR 1.563e-05
0: TRAIN [2][120/5173]	Time 0.565 (0.604)	Data 1.62e-04 (4.63e-03)	Tok/s 18071 (22042)	Loss/tok 3.2117 (3.3686)	LR 1.563e-05
0: TRAIN [2][130/5173]	Time 0.640 (0.607)	Data 1.70e-04 (4.29e-03)	Tok/s 26523 (22512)	Loss/tok 3.3344 (3.3807)	LR 1.563e-05
0: TRAIN [2][140/5173]	Time 0.560 (0.606)	Data 1.73e-04 (4.00e-03)	Tok/s 18468 (22302)	Loss/tok 3.2350 (3.3788)	LR 1.563e-05
0: TRAIN [2][150/5173]	Time 0.623 (0.606)	Data 1.83e-04 (3.75e-03)	Tok/s 26699 (22363)	Loss/tok 3.3501 (3.3777)	LR 1.563e-05
0: TRAIN [2][160/5173]	Time 0.645 (0.606)	Data 1.70e-04 (3.53e-03)	Tok/s 26013 (22352)	Loss/tok 3.3806 (3.3730)	LR 1.563e-05
0: TRAIN [2][170/5173]	Time 0.624 (0.604)	Data 1.79e-04 (3.33e-03)	Tok/s 27247 (22219)	Loss/tok 3.4792 (3.3673)	LR 1.563e-05
0: TRAIN [2][180/5173]	Time 0.564 (0.604)	Data 1.85e-04 (3.16e-03)	Tok/s 18469 (22206)	Loss/tok 3.2317 (3.3687)	LR 1.563e-05
0: TRAIN [2][190/5173]	Time 0.638 (0.606)	Data 1.73e-04 (3.01e-03)	Tok/s 26617 (22394)	Loss/tok 3.3835 (3.3758)	LR 1.563e-05
0: TRAIN [2][200/5173]	Time 0.701 (0.608)	Data 1.69e-04 (2.87e-03)	Tok/s 33159 (22651)	Loss/tok 3.6021 (3.3821)	LR 1.563e-05
0: TRAIN [2][210/5173]	Time 0.561 (0.607)	Data 1.77e-04 (2.74e-03)	Tok/s 18520 (22619)	Loss/tok 3.1766 (3.3810)	LR 1.563e-05
0: TRAIN [2][220/5173]	Time 0.701 (0.608)	Data 1.74e-04 (2.62e-03)	Tok/s 33274 (22697)	Loss/tok 3.6156 (3.3846)	LR 1.563e-05
0: TRAIN [2][230/5173]	Time 0.650 (0.607)	Data 1.67e-04 (2.52e-03)	Tok/s 25926 (22569)	Loss/tok 3.3743 (3.3784)	LR 1.563e-05
0: TRAIN [2][240/5173]	Time 0.702 (0.606)	Data 3.51e-04 (2.42e-03)	Tok/s 32799 (22507)	Loss/tok 3.5792 (3.3791)	LR 1.563e-05
0: TRAIN [2][250/5173]	Time 0.636 (0.606)	Data 1.71e-04 (2.33e-03)	Tok/s 26089 (22459)	Loss/tok 3.4051 (3.3786)	LR 1.563e-05
0: TRAIN [2][260/5173]	Time 0.634 (0.605)	Data 1.62e-04 (2.25e-03)	Tok/s 26603 (22354)	Loss/tok 3.4121 (3.3760)	LR 1.563e-05
0: TRAIN [2][270/5173]	Time 0.564 (0.605)	Data 1.76e-04 (2.17e-03)	Tok/s 18266 (22409)	Loss/tok 3.2036 (3.3756)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][280/5173]	Time 0.565 (0.606)	Data 1.75e-04 (2.10e-03)	Tok/s 18073 (22484)	Loss/tok 3.1631 (3.3807)	LR 1.563e-05
0: TRAIN [2][290/5173]	Time 0.645 (0.606)	Data 1.66e-04 (2.04e-03)	Tok/s 25758 (22565)	Loss/tok 3.2629 (3.3800)	LR 1.563e-05
0: TRAIN [2][300/5173]	Time 0.706 (0.607)	Data 1.70e-04 (1.97e-03)	Tok/s 32992 (22597)	Loss/tok 3.6011 (3.3818)	LR 1.563e-05
0: TRAIN [2][310/5173]	Time 0.567 (0.606)	Data 1.66e-04 (1.92e-03)	Tok/s 18447 (22563)	Loss/tok 3.1943 (3.3807)	LR 1.563e-05
0: TRAIN [2][320/5173]	Time 0.566 (0.606)	Data 1.68e-04 (1.86e-03)	Tok/s 18406 (22574)	Loss/tok 3.3185 (3.3788)	LR 1.563e-05
0: TRAIN [2][330/5173]	Time 0.576 (0.606)	Data 1.63e-04 (1.81e-03)	Tok/s 17725 (22535)	Loss/tok 3.0977 (3.3774)	LR 1.563e-05
0: TRAIN [2][340/5173]	Time 0.642 (0.606)	Data 1.71e-04 (1.76e-03)	Tok/s 26480 (22576)	Loss/tok 3.4059 (3.3768)	LR 1.563e-05
0: TRAIN [2][350/5173]	Time 0.639 (0.607)	Data 1.66e-04 (1.72e-03)	Tok/s 26060 (22611)	Loss/tok 3.4448 (3.3770)	LR 1.563e-05
0: TRAIN [2][360/5173]	Time 0.568 (0.607)	Data 1.71e-04 (1.68e-03)	Tok/s 18110 (22632)	Loss/tok 3.1276 (3.3770)	LR 1.563e-05
0: TRAIN [2][370/5173]	Time 0.639 (0.607)	Data 1.85e-04 (1.64e-03)	Tok/s 26335 (22635)	Loss/tok 3.4151 (3.3781)	LR 1.563e-05
0: TRAIN [2][380/5173]	Time 0.568 (0.607)	Data 1.70e-04 (1.60e-03)	Tok/s 18193 (22605)	Loss/tok 3.2131 (3.3762)	LR 1.563e-05
0: TRAIN [2][390/5173]	Time 0.570 (0.607)	Data 1.66e-04 (1.56e-03)	Tok/s 18077 (22605)	Loss/tok 3.1921 (3.3761)	LR 1.563e-05
0: TRAIN [2][400/5173]	Time 0.643 (0.606)	Data 1.78e-04 (1.53e-03)	Tok/s 26212 (22582)	Loss/tok 3.4308 (3.3750)	LR 1.563e-05
0: TRAIN [2][410/5173]	Time 0.566 (0.606)	Data 1.65e-04 (1.49e-03)	Tok/s 18007 (22575)	Loss/tok 3.1250 (3.3732)	LR 1.563e-05
0: TRAIN [2][420/5173]	Time 0.567 (0.606)	Data 1.63e-04 (1.46e-03)	Tok/s 18250 (22547)	Loss/tok 3.1599 (3.3728)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][430/5173]	Time 0.623 (0.607)	Data 1.80e-04 (1.44e-03)	Tok/s 27031 (22690)	Loss/tok 3.2867 (3.3777)	LR 1.563e-05
0: TRAIN [2][440/5173]	Time 0.562 (0.607)	Data 1.66e-04 (1.41e-03)	Tok/s 18485 (22644)	Loss/tok 3.1891 (3.3770)	LR 1.563e-05
0: TRAIN [2][450/5173]	Time 0.639 (0.607)	Data 1.60e-04 (1.38e-03)	Tok/s 26640 (22677)	Loss/tok 3.3705 (3.3783)	LR 1.563e-05
0: TRAIN [2][460/5173]	Time 0.578 (0.607)	Data 1.72e-04 (1.35e-03)	Tok/s 18074 (22717)	Loss/tok 3.0653 (3.3789)	LR 1.563e-05
0: TRAIN [2][470/5173]	Time 0.699 (0.608)	Data 1.74e-04 (1.33e-03)	Tok/s 33126 (22768)	Loss/tok 3.5308 (3.3802)	LR 1.563e-05
0: TRAIN [2][480/5173]	Time 0.691 (0.608)	Data 3.53e-04 (1.31e-03)	Tok/s 33631 (22834)	Loss/tok 3.6476 (3.3813)	LR 1.563e-05
0: TRAIN [2][490/5173]	Time 0.705 (0.610)	Data 2.11e-04 (1.28e-03)	Tok/s 33213 (22994)	Loss/tok 3.5823 (3.3872)	LR 1.563e-05
0: TRAIN [2][500/5173]	Time 0.570 (0.609)	Data 1.74e-04 (1.26e-03)	Tok/s 18345 (22958)	Loss/tok 3.1660 (3.3868)	LR 1.563e-05
0: TRAIN [2][510/5173]	Time 0.641 (0.609)	Data 1.71e-04 (1.24e-03)	Tok/s 26284 (22958)	Loss/tok 3.4400 (3.3863)	LR 1.563e-05
0: TRAIN [2][520/5173]	Time 0.700 (0.610)	Data 1.86e-04 (1.22e-03)	Tok/s 33035 (22976)	Loss/tok 3.6352 (3.3876)	LR 1.563e-05
0: TRAIN [2][530/5173]	Time 0.567 (0.609)	Data 1.67e-04 (1.20e-03)	Tok/s 18035 (22890)	Loss/tok 3.1258 (3.3854)	LR 1.563e-05
0: TRAIN [2][540/5173]	Time 0.503 (0.608)	Data 1.65e-04 (1.18e-03)	Tok/s 10343 (22836)	Loss/tok 2.7298 (3.3833)	LR 1.563e-05
0: TRAIN [2][550/5173]	Time 0.502 (0.607)	Data 1.79e-04 (1.16e-03)	Tok/s 10340 (22722)	Loss/tok 2.6629 (3.3810)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][560/5173]	Time 0.569 (0.607)	Data 1.66e-04 (1.15e-03)	Tok/s 17965 (22707)	Loss/tok 3.1348 (3.3808)	LR 1.563e-05
0: TRAIN [2][570/5173]	Time 0.561 (0.607)	Data 1.72e-04 (1.13e-03)	Tok/s 18375 (22704)	Loss/tok 3.1440 (3.3809)	LR 1.563e-05
0: TRAIN [2][580/5173]	Time 0.646 (0.607)	Data 1.60e-04 (1.11e-03)	Tok/s 26009 (22709)	Loss/tok 3.4509 (3.3804)	LR 1.563e-05
0: TRAIN [2][590/5173]	Time 0.622 (0.607)	Data 1.64e-04 (1.10e-03)	Tok/s 27113 (22675)	Loss/tok 3.4181 (3.3795)	LR 1.563e-05
0: TRAIN [2][600/5173]	Time 0.640 (0.607)	Data 1.70e-04 (1.08e-03)	Tok/s 26818 (22716)	Loss/tok 3.4354 (3.3803)	LR 1.563e-05
0: TRAIN [2][610/5173]	Time 0.561 (0.607)	Data 1.63e-04 (1.07e-03)	Tok/s 18321 (22736)	Loss/tok 3.1960 (3.3803)	LR 1.563e-05
0: TRAIN [2][620/5173]	Time 0.575 (0.607)	Data 1.73e-04 (1.05e-03)	Tok/s 17811 (22766)	Loss/tok 3.2467 (3.3812)	LR 1.563e-05
0: TRAIN [2][630/5173]	Time 0.695 (0.607)	Data 1.73e-04 (1.04e-03)	Tok/s 33753 (22772)	Loss/tok 3.4957 (3.3808)	LR 1.563e-05
0: TRAIN [2][640/5173]	Time 0.643 (0.607)	Data 1.65e-04 (1.02e-03)	Tok/s 26043 (22727)	Loss/tok 3.4208 (3.3801)	LR 1.563e-05
0: TRAIN [2][650/5173]	Time 0.624 (0.608)	Data 1.67e-04 (1.01e-03)	Tok/s 26978 (22806)	Loss/tok 3.3577 (3.3826)	LR 1.563e-05
0: TRAIN [2][660/5173]	Time 0.550 (0.608)	Data 1.67e-04 (9.99e-04)	Tok/s 18729 (22818)	Loss/tok 3.0665 (3.3831)	LR 1.563e-05
0: TRAIN [2][670/5173]	Time 0.501 (0.608)	Data 1.69e-04 (9.87e-04)	Tok/s 10286 (22837)	Loss/tok 2.6875 (3.3842)	LR 1.563e-05
0: TRAIN [2][680/5173]	Time 0.546 (0.608)	Data 3.77e-04 (9.76e-04)	Tok/s 18688 (22834)	Loss/tok 3.1187 (3.3840)	LR 1.563e-05
0: TRAIN [2][690/5173]	Time 0.645 (0.607)	Data 1.69e-04 (9.65e-04)	Tok/s 25639 (22781)	Loss/tok 3.4320 (3.3826)	LR 1.563e-05
0: TRAIN [2][700/5173]	Time 0.566 (0.608)	Data 1.61e-04 (9.53e-04)	Tok/s 18080 (22806)	Loss/tok 3.3626 (3.3832)	LR 1.563e-05
0: TRAIN [2][710/5173]	Time 0.565 (0.607)	Data 1.64e-04 (9.42e-04)	Tok/s 18647 (22743)	Loss/tok 3.1477 (3.3816)	LR 1.563e-05
0: TRAIN [2][720/5173]	Time 0.568 (0.607)	Data 1.63e-04 (9.32e-04)	Tok/s 18136 (22770)	Loss/tok 3.1967 (3.3816)	LR 1.563e-05
0: TRAIN [2][730/5173]	Time 0.567 (0.607)	Data 3.58e-04 (9.22e-04)	Tok/s 18354 (22733)	Loss/tok 3.2411 (3.3803)	LR 1.563e-05
0: TRAIN [2][740/5173]	Time 0.569 (0.607)	Data 1.69e-04 (9.12e-04)	Tok/s 18160 (22732)	Loss/tok 3.1824 (3.3807)	LR 1.563e-05
0: TRAIN [2][750/5173]	Time 0.581 (0.607)	Data 1.69e-04 (9.02e-04)	Tok/s 17577 (22721)	Loss/tok 3.2088 (3.3802)	LR 1.563e-05
0: TRAIN [2][760/5173]	Time 0.559 (0.607)	Data 1.62e-04 (8.93e-04)	Tok/s 18555 (22736)	Loss/tok 3.2628 (3.3809)	LR 1.563e-05
0: TRAIN [2][770/5173]	Time 0.631 (0.607)	Data 1.64e-04 (8.83e-04)	Tok/s 27240 (22724)	Loss/tok 3.2660 (3.3795)	LR 1.563e-05
0: TRAIN [2][780/5173]	Time 0.569 (0.607)	Data 1.65e-04 (8.74e-04)	Tok/s 17869 (22747)	Loss/tok 3.0971 (3.3801)	LR 1.563e-05
0: TRAIN [2][790/5173]	Time 0.637 (0.607)	Data 1.66e-04 (8.65e-04)	Tok/s 26371 (22795)	Loss/tok 3.5053 (3.3817)	LR 1.563e-05
0: TRAIN [2][800/5173]	Time 0.569 (0.607)	Data 1.81e-04 (8.56e-04)	Tok/s 18353 (22823)	Loss/tok 3.2224 (3.3827)	LR 1.563e-05
0: TRAIN [2][810/5173]	Time 0.559 (0.607)	Data 1.65e-04 (8.49e-04)	Tok/s 18217 (22768)	Loss/tok 3.1739 (3.3813)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][820/5173]	Time 0.570 (0.607)	Data 3.32e-04 (8.41e-04)	Tok/s 18238 (22796)	Loss/tok 3.2957 (3.3824)	LR 1.563e-05
0: TRAIN [2][830/5173]	Time 0.571 (0.607)	Data 1.55e-04 (8.33e-04)	Tok/s 18292 (22803)	Loss/tok 3.1636 (3.3827)	LR 1.563e-05
0: TRAIN [2][840/5173]	Time 0.499 (0.607)	Data 1.65e-04 (8.25e-04)	Tok/s 10608 (22816)	Loss/tok 2.7317 (3.3841)	LR 1.563e-05
0: TRAIN [2][850/5173]	Time 0.635 (0.607)	Data 1.59e-04 (8.17e-04)	Tok/s 26157 (22790)	Loss/tok 3.4371 (3.3834)	LR 1.563e-05
0: TRAIN [2][860/5173]	Time 0.567 (0.607)	Data 1.68e-04 (8.10e-04)	Tok/s 17875 (22786)	Loss/tok 3.2654 (3.3843)	LR 1.563e-05
0: TRAIN [2][870/5173]	Time 0.548 (0.607)	Data 1.67e-04 (8.03e-04)	Tok/s 19064 (22775)	Loss/tok 3.1408 (3.3840)	LR 1.563e-05
0: TRAIN [2][880/5173]	Time 0.572 (0.607)	Data 1.67e-04 (7.95e-04)	Tok/s 18179 (22758)	Loss/tok 3.1071 (3.3836)	LR 1.563e-05
0: TRAIN [2][890/5173]	Time 0.702 (0.607)	Data 1.95e-04 (7.89e-04)	Tok/s 33388 (22790)	Loss/tok 3.5873 (3.3854)	LR 1.563e-05
0: TRAIN [2][900/5173]	Time 0.572 (0.607)	Data 1.62e-04 (7.82e-04)	Tok/s 18184 (22765)	Loss/tok 3.2237 (3.3847)	LR 1.563e-05
0: TRAIN [2][910/5173]	Time 0.569 (0.607)	Data 1.64e-04 (7.75e-04)	Tok/s 18325 (22790)	Loss/tok 3.3158 (3.3859)	LR 1.563e-05
0: TRAIN [2][920/5173]	Time 0.637 (0.607)	Data 1.70e-04 (7.69e-04)	Tok/s 26113 (22800)	Loss/tok 3.4349 (3.3856)	LR 1.563e-05
0: TRAIN [2][930/5173]	Time 0.638 (0.607)	Data 1.58e-04 (7.62e-04)	Tok/s 26194 (22794)	Loss/tok 3.4323 (3.3851)	LR 1.563e-05
0: TRAIN [2][940/5173]	Time 0.571 (0.607)	Data 1.63e-04 (7.56e-04)	Tok/s 18163 (22770)	Loss/tok 3.1667 (3.3842)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][950/5173]	Time 0.767 (0.608)	Data 1.68e-04 (7.50e-04)	Tok/s 39066 (22829)	Loss/tok 3.7359 (3.3864)	LR 1.563e-05
0: TRAIN [2][960/5173]	Time 0.638 (0.608)	Data 1.66e-04 (7.44e-04)	Tok/s 26567 (22850)	Loss/tok 3.3249 (3.3872)	LR 1.563e-05
0: TRAIN [2][970/5173]	Time 0.563 (0.608)	Data 1.76e-04 (7.38e-04)	Tok/s 18364 (22885)	Loss/tok 3.2584 (3.3891)	LR 1.563e-05
0: TRAIN [2][980/5173]	Time 0.642 (0.608)	Data 1.68e-04 (7.32e-04)	Tok/s 26061 (22908)	Loss/tok 3.4121 (3.3895)	LR 1.563e-05
0: TRAIN [2][990/5173]	Time 0.646 (0.609)	Data 1.65e-04 (7.27e-04)	Tok/s 25924 (22948)	Loss/tok 3.4079 (3.3903)	LR 1.563e-05
0: TRAIN [2][1000/5173]	Time 0.564 (0.609)	Data 1.77e-04 (7.21e-04)	Tok/s 18335 (22965)	Loss/tok 3.1151 (3.3914)	LR 1.563e-05
0: TRAIN [2][1010/5173]	Time 0.569 (0.609)	Data 1.69e-04 (7.16e-04)	Tok/s 17399 (22947)	Loss/tok 3.2265 (3.3911)	LR 1.563e-05
0: TRAIN [2][1020/5173]	Time 0.569 (0.609)	Data 1.35e-04 (7.11e-04)	Tok/s 17890 (22948)	Loss/tok 3.0896 (3.3905)	LR 1.563e-05
0: TRAIN [2][1030/5173]	Time 0.691 (0.609)	Data 1.77e-04 (7.06e-04)	Tok/s 34814 (22965)	Loss/tok 3.4098 (3.3905)	LR 1.563e-05
0: TRAIN [2][1040/5173]	Time 0.770 (0.609)	Data 1.80e-04 (7.01e-04)	Tok/s 38434 (22977)	Loss/tok 3.8161 (3.3910)	LR 1.563e-05
0: TRAIN [2][1050/5173]	Time 0.567 (0.609)	Data 1.65e-04 (6.96e-04)	Tok/s 18045 (23010)	Loss/tok 3.1787 (3.3920)	LR 1.563e-05
0: TRAIN [2][1060/5173]	Time 0.564 (0.609)	Data 1.69e-04 (6.91e-04)	Tok/s 18575 (23014)	Loss/tok 3.2872 (3.3918)	LR 1.563e-05
0: TRAIN [2][1070/5173]	Time 0.765 (0.610)	Data 1.87e-04 (6.87e-04)	Tok/s 39236 (23067)	Loss/tok 3.6331 (3.3937)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1080/5173]	Time 0.643 (0.610)	Data 1.84e-04 (6.82e-04)	Tok/s 26068 (23090)	Loss/tok 3.3638 (3.3948)	LR 1.563e-05
0: TRAIN [2][1090/5173]	Time 0.638 (0.611)	Data 3.47e-04 (6.78e-04)	Tok/s 25887 (23143)	Loss/tok 3.4639 (3.3967)	LR 1.563e-05
0: TRAIN [2][1100/5173]	Time 0.564 (0.611)	Data 1.68e-04 (6.73e-04)	Tok/s 18470 (23141)	Loss/tok 3.2104 (3.3961)	LR 1.563e-05
0: TRAIN [2][1110/5173]	Time 0.565 (0.611)	Data 1.94e-04 (6.69e-04)	Tok/s 18031 (23148)	Loss/tok 3.1618 (3.3968)	LR 1.563e-05
0: TRAIN [2][1120/5173]	Time 0.570 (0.610)	Data 1.75e-04 (6.65e-04)	Tok/s 18376 (23103)	Loss/tok 3.2305 (3.3954)	LR 1.563e-05
0: TRAIN [2][1130/5173]	Time 0.642 (0.610)	Data 1.72e-04 (6.60e-04)	Tok/s 25949 (23120)	Loss/tok 3.3398 (3.3956)	LR 1.563e-05
0: TRAIN [2][1140/5173]	Time 0.702 (0.611)	Data 1.71e-04 (6.56e-04)	Tok/s 33185 (23149)	Loss/tok 3.4657 (3.3964)	LR 1.563e-05
0: TRAIN [2][1150/5173]	Time 0.564 (0.611)	Data 1.64e-04 (6.52e-04)	Tok/s 18019 (23158)	Loss/tok 3.2065 (3.3967)	LR 1.563e-05
0: TRAIN [2][1160/5173]	Time 0.502 (0.611)	Data 1.65e-04 (6.48e-04)	Tok/s 10452 (23133)	Loss/tok 2.6846 (3.3963)	LR 1.563e-05
0: TRAIN [2][1170/5173]	Time 0.501 (0.611)	Data 1.71e-04 (6.44e-04)	Tok/s 10249 (23130)	Loss/tok 2.7677 (3.3963)	LR 1.563e-05
0: TRAIN [2][1180/5173]	Time 0.768 (0.611)	Data 1.65e-04 (6.40e-04)	Tok/s 39004 (23175)	Loss/tok 3.6956 (3.3979)	LR 1.563e-05
0: TRAIN [2][1190/5173]	Time 0.640 (0.611)	Data 1.72e-04 (6.37e-04)	Tok/s 26672 (23201)	Loss/tok 3.3704 (3.3987)	LR 1.563e-05
0: TRAIN [2][1200/5173]	Time 0.563 (0.611)	Data 1.74e-04 (6.33e-04)	Tok/s 18483 (23173)	Loss/tok 3.1398 (3.3980)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1210/5173]	Time 0.768 (0.611)	Data 1.73e-04 (6.29e-04)	Tok/s 37972 (23190)	Loss/tok 3.9241 (3.3994)	LR 1.563e-05
0: TRAIN [2][1220/5173]	Time 0.631 (0.611)	Data 1.75e-04 (6.25e-04)	Tok/s 26621 (23188)	Loss/tok 3.4203 (3.3992)	LR 1.563e-05
0: TRAIN [2][1230/5173]	Time 0.625 (0.611)	Data 1.66e-04 (6.22e-04)	Tok/s 27065 (23199)	Loss/tok 3.3168 (3.3988)	LR 1.563e-05
0: TRAIN [2][1240/5173]	Time 0.700 (0.611)	Data 1.70e-04 (6.19e-04)	Tok/s 33359 (23198)	Loss/tok 3.5915 (3.3986)	LR 1.563e-05
0: TRAIN [2][1250/5173]	Time 0.563 (0.611)	Data 1.71e-04 (6.15e-04)	Tok/s 18590 (23164)	Loss/tok 3.1347 (3.3976)	LR 1.563e-05
0: TRAIN [2][1260/5173]	Time 0.624 (0.611)	Data 1.66e-04 (6.12e-04)	Tok/s 27046 (23157)	Loss/tok 3.3479 (3.3973)	LR 1.563e-05
0: TRAIN [2][1270/5173]	Time 0.559 (0.611)	Data 1.63e-04 (6.08e-04)	Tok/s 18571 (23142)	Loss/tok 3.1796 (3.3969)	LR 1.563e-05
0: TRAIN [2][1280/5173]	Time 0.504 (0.611)	Data 1.74e-04 (6.05e-04)	Tok/s 10507 (23142)	Loss/tok 2.7356 (3.3972)	LR 1.563e-05
0: TRAIN [2][1290/5173]	Time 0.706 (0.611)	Data 1.27e-04 (6.02e-04)	Tok/s 33145 (23151)	Loss/tok 3.6055 (3.3978)	LR 1.563e-05
0: TRAIN [2][1300/5173]	Time 0.496 (0.611)	Data 1.21e-04 (5.98e-04)	Tok/s 10821 (23128)	Loss/tok 2.7471 (3.3972)	LR 1.563e-05
0: TRAIN [2][1310/5173]	Time 0.639 (0.610)	Data 1.14e-04 (5.94e-04)	Tok/s 26385 (23110)	Loss/tok 3.4379 (3.3966)	LR 1.563e-05
0: TRAIN [2][1320/5173]	Time 0.564 (0.611)	Data 1.12e-04 (5.91e-04)	Tok/s 18498 (23123)	Loss/tok 3.1622 (3.3971)	LR 1.563e-05
0: TRAIN [2][1330/5173]	Time 0.548 (0.610)	Data 2.68e-04 (5.88e-04)	Tok/s 18552 (23114)	Loss/tok 3.1932 (3.3967)	LR 1.563e-05
0: TRAIN [2][1340/5173]	Time 0.500 (0.611)	Data 1.21e-04 (5.84e-04)	Tok/s 10649 (23115)	Loss/tok 2.7933 (3.3971)	LR 1.563e-05
0: TRAIN [2][1350/5173]	Time 0.638 (0.611)	Data 1.16e-04 (5.81e-04)	Tok/s 26127 (23134)	Loss/tok 3.4801 (3.3979)	LR 1.563e-05
0: TRAIN [2][1360/5173]	Time 0.629 (0.611)	Data 1.13e-04 (5.77e-04)	Tok/s 26274 (23171)	Loss/tok 3.5028 (3.3988)	LR 1.563e-05
0: TRAIN [2][1370/5173]	Time 0.571 (0.611)	Data 1.11e-04 (5.74e-04)	Tok/s 18206 (23143)	Loss/tok 3.3373 (3.3982)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1380/5173]	Time 0.500 (0.611)	Data 1.17e-04 (5.71e-04)	Tok/s 10363 (23139)	Loss/tok 2.6770 (3.3979)	LR 1.563e-05
0: TRAIN [2][1390/5173]	Time 0.499 (0.610)	Data 1.14e-04 (5.67e-04)	Tok/s 10794 (23093)	Loss/tok 2.7365 (3.3967)	LR 1.563e-05
0: TRAIN [2][1400/5173]	Time 0.647 (0.611)	Data 1.19e-04 (5.64e-04)	Tok/s 25998 (23114)	Loss/tok 3.3289 (3.3979)	LR 1.563e-05
0: TRAIN [2][1410/5173]	Time 0.638 (0.610)	Data 1.13e-04 (5.61e-04)	Tok/s 25721 (23097)	Loss/tok 3.4712 (3.3974)	LR 1.563e-05
0: TRAIN [2][1420/5173]	Time 0.643 (0.610)	Data 1.18e-04 (5.58e-04)	Tok/s 26003 (23084)	Loss/tok 3.3370 (3.3969)	LR 1.563e-05
0: TRAIN [2][1430/5173]	Time 0.640 (0.610)	Data 1.12e-04 (5.55e-04)	Tok/s 26575 (23076)	Loss/tok 3.3828 (3.3967)	LR 1.563e-05
0: TRAIN [2][1440/5173]	Time 0.563 (0.610)	Data 1.13e-04 (5.52e-04)	Tok/s 18437 (23079)	Loss/tok 3.1511 (3.3968)	LR 1.563e-05
0: TRAIN [2][1450/5173]	Time 0.638 (0.610)	Data 1.12e-04 (5.49e-04)	Tok/s 26341 (23071)	Loss/tok 3.3512 (3.3966)	LR 1.563e-05
0: TRAIN [2][1460/5173]	Time 0.567 (0.610)	Data 1.09e-04 (5.46e-04)	Tok/s 18092 (23054)	Loss/tok 3.2456 (3.3960)	LR 1.563e-05
0: TRAIN [2][1470/5173]	Time 0.560 (0.610)	Data 1.12e-04 (5.43e-04)	Tok/s 18434 (23046)	Loss/tok 3.1279 (3.3960)	LR 1.563e-05
0: TRAIN [2][1480/5173]	Time 0.566 (0.610)	Data 1.17e-04 (5.41e-04)	Tok/s 18614 (23039)	Loss/tok 3.2360 (3.3959)	LR 1.563e-05
0: TRAIN [2][1490/5173]	Time 0.499 (0.610)	Data 1.36e-04 (5.38e-04)	Tok/s 10748 (23056)	Loss/tok 2.7649 (3.3967)	LR 1.563e-05
0: TRAIN [2][1500/5173]	Time 0.508 (0.610)	Data 1.15e-04 (5.35e-04)	Tok/s 10425 (23034)	Loss/tok 2.7306 (3.3958)	LR 1.563e-05
0: TRAIN [2][1510/5173]	Time 0.642 (0.610)	Data 3.00e-04 (5.33e-04)	Tok/s 26254 (23042)	Loss/tok 3.3694 (3.3963)	LR 1.563e-05
0: TRAIN [2][1520/5173]	Time 0.640 (0.610)	Data 1.27e-04 (5.30e-04)	Tok/s 26477 (23042)	Loss/tok 3.4951 (3.3963)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1530/5173]	Time 0.763 (0.610)	Data 1.26e-04 (5.27e-04)	Tok/s 38680 (23049)	Loss/tok 3.7860 (3.3964)	LR 1.563e-05
0: TRAIN [2][1540/5173]	Time 0.699 (0.610)	Data 1.21e-04 (5.25e-04)	Tok/s 33404 (23036)	Loss/tok 3.5548 (3.3963)	LR 1.563e-05
0: TRAIN [2][1550/5173]	Time 0.568 (0.610)	Data 1.15e-04 (5.22e-04)	Tok/s 18121 (23051)	Loss/tok 3.1572 (3.3963)	LR 1.563e-05
0: TRAIN [2][1560/5173]	Time 0.565 (0.610)	Data 1.11e-04 (5.20e-04)	Tok/s 17982 (23044)	Loss/tok 3.2927 (3.3959)	LR 1.563e-05
0: TRAIN [2][1570/5173]	Time 0.627 (0.610)	Data 1.10e-04 (5.17e-04)	Tok/s 26427 (23039)	Loss/tok 3.3960 (3.3956)	LR 1.563e-05
0: TRAIN [2][1580/5173]	Time 0.631 (0.610)	Data 1.14e-04 (5.15e-04)	Tok/s 26591 (23036)	Loss/tok 3.4406 (3.3956)	LR 1.563e-05
0: TRAIN [2][1590/5173]	Time 0.564 (0.610)	Data 1.15e-04 (5.13e-04)	Tok/s 18221 (23043)	Loss/tok 3.1393 (3.3959)	LR 1.563e-05
0: TRAIN [2][1600/5173]	Time 0.632 (0.610)	Data 1.13e-04 (5.10e-04)	Tok/s 26890 (23048)	Loss/tok 3.3920 (3.3956)	LR 1.563e-05
0: TRAIN [2][1610/5173]	Time 0.636 (0.610)	Data 1.15e-04 (5.08e-04)	Tok/s 26527 (23060)	Loss/tok 3.3995 (3.3960)	LR 1.563e-05
0: TRAIN [2][1620/5173]	Time 0.558 (0.610)	Data 1.11e-04 (5.05e-04)	Tok/s 18656 (23055)	Loss/tok 3.2268 (3.3956)	LR 1.563e-05
0: TRAIN [2][1630/5173]	Time 0.570 (0.610)	Data 1.13e-04 (5.03e-04)	Tok/s 18134 (23048)	Loss/tok 3.2660 (3.3953)	LR 1.563e-05
0: TRAIN [2][1640/5173]	Time 0.623 (0.610)	Data 1.20e-04 (5.01e-04)	Tok/s 26844 (23062)	Loss/tok 3.2722 (3.3952)	LR 1.563e-05
0: TRAIN [2][1650/5173]	Time 0.639 (0.610)	Data 1.14e-04 (4.98e-04)	Tok/s 26282 (23047)	Loss/tok 3.3604 (3.3947)	LR 1.563e-05
0: TRAIN [2][1660/5173]	Time 0.570 (0.610)	Data 1.13e-04 (4.96e-04)	Tok/s 18117 (23008)	Loss/tok 3.1053 (3.3937)	LR 1.563e-05
0: TRAIN [2][1670/5173]	Time 0.560 (0.610)	Data 1.17e-04 (4.94e-04)	Tok/s 18693 (23008)	Loss/tok 3.1706 (3.3937)	LR 1.563e-05
0: TRAIN [2][1680/5173]	Time 0.628 (0.610)	Data 1.12e-04 (4.92e-04)	Tok/s 26752 (23030)	Loss/tok 3.4324 (3.3944)	LR 1.563e-05
0: TRAIN [2][1690/5173]	Time 0.646 (0.610)	Data 1.08e-04 (4.89e-04)	Tok/s 26202 (23025)	Loss/tok 3.3994 (3.3941)	LR 1.563e-05
0: TRAIN [2][1700/5173]	Time 0.562 (0.610)	Data 1.14e-04 (4.87e-04)	Tok/s 17964 (22997)	Loss/tok 3.1499 (3.3933)	LR 1.563e-05
0: TRAIN [2][1710/5173]	Time 0.506 (0.610)	Data 1.14e-04 (4.85e-04)	Tok/s 10381 (22988)	Loss/tok 2.6819 (3.3928)	LR 1.563e-05
0: TRAIN [2][1720/5173]	Time 0.501 (0.609)	Data 1.15e-04 (4.83e-04)	Tok/s 10587 (22957)	Loss/tok 2.7286 (3.3920)	LR 1.563e-05
0: TRAIN [2][1730/5173]	Time 0.702 (0.609)	Data 1.24e-04 (4.81e-04)	Tok/s 33255 (22960)	Loss/tok 3.5545 (3.3921)	LR 1.563e-05
0: TRAIN [2][1740/5173]	Time 0.569 (0.609)	Data 1.11e-04 (4.79e-04)	Tok/s 17949 (22947)	Loss/tok 3.1433 (3.3916)	LR 1.563e-05
0: TRAIN [2][1750/5173]	Time 0.635 (0.609)	Data 1.17e-04 (4.77e-04)	Tok/s 26308 (22938)	Loss/tok 3.4116 (3.3914)	LR 1.563e-05
0: TRAIN [2][1760/5173]	Time 0.690 (0.609)	Data 1.15e-04 (4.75e-04)	Tok/s 33861 (22950)	Loss/tok 3.6072 (3.3918)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1770/5173]	Time 0.638 (0.609)	Data 1.14e-04 (4.73e-04)	Tok/s 26155 (22966)	Loss/tok 3.3591 (3.3921)	LR 1.563e-05
0: TRAIN [2][1780/5173]	Time 0.569 (0.609)	Data 1.11e-04 (4.71e-04)	Tok/s 17848 (22953)	Loss/tok 3.2645 (3.3916)	LR 1.563e-05
0: TRAIN [2][1790/5173]	Time 0.695 (0.609)	Data 1.13e-04 (4.69e-04)	Tok/s 33883 (22957)	Loss/tok 3.5213 (3.3916)	LR 1.563e-05
0: TRAIN [2][1800/5173]	Time 0.495 (0.609)	Data 1.21e-04 (4.67e-04)	Tok/s 10487 (22973)	Loss/tok 2.6705 (3.3921)	LR 1.563e-05
0: TRAIN [2][1810/5173]	Time 0.702 (0.609)	Data 1.15e-04 (4.65e-04)	Tok/s 32795 (22982)	Loss/tok 3.5651 (3.3925)	LR 1.563e-05
0: TRAIN [2][1820/5173]	Time 0.567 (0.609)	Data 1.15e-04 (4.63e-04)	Tok/s 18672 (22966)	Loss/tok 3.0879 (3.3918)	LR 1.563e-05
0: TRAIN [2][1830/5173]	Time 0.703 (0.609)	Data 1.16e-04 (4.62e-04)	Tok/s 32998 (22986)	Loss/tok 3.6480 (3.3924)	LR 1.563e-05
0: TRAIN [2][1840/5173]	Time 0.570 (0.609)	Data 1.12e-04 (4.60e-04)	Tok/s 18272 (22961)	Loss/tok 3.1692 (3.3917)	LR 1.563e-05
0: TRAIN [2][1850/5173]	Time 0.770 (0.609)	Data 1.19e-04 (4.58e-04)	Tok/s 38820 (22966)	Loss/tok 3.6853 (3.3920)	LR 1.563e-05
0: TRAIN [2][1860/5173]	Time 0.694 (0.609)	Data 1.15e-04 (4.56e-04)	Tok/s 33723 (22979)	Loss/tok 3.4986 (3.3920)	LR 1.563e-05
0: TRAIN [2][1870/5173]	Time 0.773 (0.610)	Data 1.11e-04 (4.55e-04)	Tok/s 38570 (22994)	Loss/tok 3.7510 (3.3924)	LR 1.563e-05
0: TRAIN [2][1880/5173]	Time 0.610 (0.610)	Data 1.11e-04 (4.53e-04)	Tok/s 27526 (23007)	Loss/tok 3.4010 (3.3927)	LR 1.563e-05
0: TRAIN [2][1890/5173]	Time 0.635 (0.610)	Data 2.75e-04 (4.51e-04)	Tok/s 25981 (23015)	Loss/tok 3.5250 (3.3928)	LR 1.563e-05
0: TRAIN [2][1900/5173]	Time 0.567 (0.609)	Data 1.12e-04 (4.49e-04)	Tok/s 18335 (22998)	Loss/tok 3.1689 (3.3922)	LR 1.563e-05
0: TRAIN [2][1910/5173]	Time 0.695 (0.609)	Data 1.13e-04 (4.48e-04)	Tok/s 33691 (22998)	Loss/tok 3.5730 (3.3923)	LR 1.563e-05
0: TRAIN [2][1920/5173]	Time 0.570 (0.609)	Data 1.09e-04 (4.46e-04)	Tok/s 18205 (22977)	Loss/tok 3.1565 (3.3917)	LR 1.563e-05
0: TRAIN [2][1930/5173]	Time 0.695 (0.609)	Data 1.15e-04 (4.44e-04)	Tok/s 33089 (22991)	Loss/tok 3.6555 (3.3922)	LR 1.563e-05
0: TRAIN [2][1940/5173]	Time 0.770 (0.610)	Data 1.16e-04 (4.43e-04)	Tok/s 38926 (23001)	Loss/tok 3.7077 (3.3924)	LR 1.563e-05
0: TRAIN [2][1950/5173]	Time 0.639 (0.610)	Data 1.17e-04 (4.41e-04)	Tok/s 26497 (23007)	Loss/tok 3.4457 (3.3926)	LR 1.563e-05
0: TRAIN [2][1960/5173]	Time 0.629 (0.610)	Data 1.07e-04 (4.39e-04)	Tok/s 26866 (23012)	Loss/tok 3.4157 (3.3923)	LR 1.563e-05
0: TRAIN [2][1970/5173]	Time 0.568 (0.610)	Data 1.07e-04 (4.38e-04)	Tok/s 17970 (23005)	Loss/tok 3.2262 (3.3920)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1980/5173]	Time 0.562 (0.610)	Data 1.23e-04 (4.36e-04)	Tok/s 18474 (23020)	Loss/tok 3.1548 (3.3929)	LR 1.563e-05
0: TRAIN [2][1990/5173]	Time 0.769 (0.609)	Data 2.88e-04 (4.35e-04)	Tok/s 38552 (22990)	Loss/tok 3.8242 (3.3926)	LR 1.563e-05
0: TRAIN [2][2000/5173]	Time 0.566 (0.609)	Data 1.17e-04 (4.33e-04)	Tok/s 18588 (22985)	Loss/tok 3.1213 (3.3925)	LR 1.563e-05
0: TRAIN [2][2010/5173]	Time 0.501 (0.609)	Data 1.21e-04 (4.32e-04)	Tok/s 10560 (22982)	Loss/tok 2.6708 (3.3922)	LR 1.563e-05
0: TRAIN [2][2020/5173]	Time 0.640 (0.609)	Data 1.20e-04 (4.30e-04)	Tok/s 25960 (22997)	Loss/tok 3.3924 (3.3926)	LR 1.563e-05
0: TRAIN [2][2030/5173]	Time 0.555 (0.609)	Data 1.12e-04 (4.29e-04)	Tok/s 18376 (22992)	Loss/tok 3.2291 (3.3924)	LR 1.563e-05
0: TRAIN [2][2040/5173]	Time 0.564 (0.610)	Data 1.53e-04 (4.27e-04)	Tok/s 18207 (23006)	Loss/tok 3.2385 (3.3927)	LR 1.563e-05
0: TRAIN [2][2050/5173]	Time 0.624 (0.610)	Data 1.51e-04 (4.26e-04)	Tok/s 26685 (23023)	Loss/tok 3.3062 (3.3933)	LR 1.563e-05
0: TRAIN [2][2060/5173]	Time 0.562 (0.610)	Data 1.52e-04 (4.25e-04)	Tok/s 18337 (23033)	Loss/tok 3.0946 (3.3932)	LR 1.563e-05
0: TRAIN [2][2070/5173]	Time 0.564 (0.610)	Data 1.32e-04 (4.23e-04)	Tok/s 18533 (23041)	Loss/tok 3.1235 (3.3934)	LR 1.563e-05
0: TRAIN [2][2080/5173]	Time 0.568 (0.610)	Data 1.29e-04 (4.22e-04)	Tok/s 18300 (23026)	Loss/tok 3.3148 (3.3929)	LR 1.563e-05
0: TRAIN [2][2090/5173]	Time 0.764 (0.610)	Data 1.24e-04 (4.20e-04)	Tok/s 39000 (23025)	Loss/tok 3.7597 (3.3931)	LR 1.563e-05
0: TRAIN [2][2100/5173]	Time 0.566 (0.610)	Data 1.18e-04 (4.19e-04)	Tok/s 18113 (23018)	Loss/tok 3.1383 (3.3926)	LR 1.563e-05
0: TRAIN [2][2110/5173]	Time 0.628 (0.610)	Data 1.23e-04 (4.17e-04)	Tok/s 26691 (23013)	Loss/tok 3.3474 (3.3923)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][2120/5173]	Time 0.507 (0.610)	Data 2.99e-04 (4.16e-04)	Tok/s 10245 (23023)	Loss/tok 2.7523 (3.3926)	LR 1.563e-05
0: TRAIN [2][2130/5173]	Time 0.503 (0.610)	Data 1.28e-04 (4.15e-04)	Tok/s 10419 (23039)	Loss/tok 2.7202 (3.3930)	LR 1.563e-05
0: TRAIN [2][2140/5173]	Time 0.567 (0.610)	Data 1.25e-04 (4.13e-04)	Tok/s 18504 (23034)	Loss/tok 3.1917 (3.3930)	LR 1.563e-05
0: TRAIN [2][2150/5173]	Time 0.573 (0.610)	Data 1.16e-04 (4.12e-04)	Tok/s 18081 (23037)	Loss/tok 3.2356 (3.3928)	LR 1.563e-05
0: TRAIN [2][2160/5173]	Time 0.633 (0.610)	Data 1.20e-04 (4.11e-04)	Tok/s 26949 (23037)	Loss/tok 3.4267 (3.3928)	LR 1.563e-05
0: TRAIN [2][2170/5173]	Time 0.567 (0.610)	Data 2.65e-04 (4.10e-04)	Tok/s 18203 (23033)	Loss/tok 3.2303 (3.3926)	LR 1.563e-05
0: TRAIN [2][2180/5173]	Time 0.495 (0.610)	Data 1.21e-04 (4.08e-04)	Tok/s 10523 (23024)	Loss/tok 2.7467 (3.3925)	LR 1.563e-05
0: TRAIN [2][2190/5173]	Time 0.567 (0.610)	Data 1.15e-04 (4.07e-04)	Tok/s 18041 (23016)	Loss/tok 3.2014 (3.3922)	LR 1.563e-05
0: TRAIN [2][2200/5173]	Time 0.641 (0.610)	Data 1.19e-04 (4.06e-04)	Tok/s 26637 (23010)	Loss/tok 3.3536 (3.3919)	LR 1.563e-05
0: TRAIN [2][2210/5173]	Time 0.554 (0.609)	Data 1.18e-04 (4.04e-04)	Tok/s 18870 (22999)	Loss/tok 3.2064 (3.3915)	LR 1.563e-05
0: TRAIN [2][2220/5173]	Time 0.632 (0.610)	Data 1.21e-04 (4.03e-04)	Tok/s 27144 (23006)	Loss/tok 3.4334 (3.3921)	LR 1.563e-05
0: TRAIN [2][2230/5173]	Time 0.564 (0.610)	Data 1.23e-04 (4.02e-04)	Tok/s 17882 (23014)	Loss/tok 3.2282 (3.3925)	LR 1.563e-05
0: TRAIN [2][2240/5173]	Time 0.701 (0.610)	Data 1.21e-04 (4.01e-04)	Tok/s 33488 (23016)	Loss/tok 3.7327 (3.3925)	LR 1.563e-05
0: TRAIN [2][2250/5173]	Time 0.644 (0.610)	Data 1.20e-04 (4.00e-04)	Tok/s 26364 (23012)	Loss/tok 3.3841 (3.3923)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][2260/5173]	Time 0.568 (0.610)	Data 1.19e-04 (3.98e-04)	Tok/s 18270 (23017)	Loss/tok 3.1778 (3.3925)	LR 1.563e-05
0: TRAIN [2][2270/5173]	Time 0.700 (0.610)	Data 1.28e-04 (3.97e-04)	Tok/s 33340 (23013)	Loss/tok 3.6263 (3.3923)	LR 1.563e-05
0: TRAIN [2][2280/5173]	Time 0.565 (0.610)	Data 1.14e-04 (3.96e-04)	Tok/s 18320 (23007)	Loss/tok 3.1709 (3.3920)	LR 1.563e-05
0: TRAIN [2][2290/5173]	Time 0.638 (0.610)	Data 1.15e-04 (3.95e-04)	Tok/s 26376 (23010)	Loss/tok 3.4992 (3.3919)	LR 1.563e-05
0: TRAIN [2][2300/5173]	Time 0.563 (0.610)	Data 1.21e-04 (3.94e-04)	Tok/s 18426 (23010)	Loss/tok 3.2582 (3.3919)	LR 1.563e-05
0: TRAIN [2][2310/5173]	Time 0.632 (0.610)	Data 2.78e-04 (3.93e-04)	Tok/s 26476 (23034)	Loss/tok 3.5150 (3.3928)	LR 1.563e-05
0: TRAIN [2][2320/5173]	Time 0.702 (0.610)	Data 1.18e-04 (3.92e-04)	Tok/s 33527 (23047)	Loss/tok 3.5644 (3.3931)	LR 1.563e-05
0: TRAIN [2][2330/5173]	Time 0.628 (0.610)	Data 1.35e-04 (3.91e-04)	Tok/s 26470 (23057)	Loss/tok 3.2764 (3.3932)	LR 1.563e-05
0: TRAIN [2][2340/5173]	Time 0.636 (0.610)	Data 3.05e-04 (3.90e-04)	Tok/s 26003 (23067)	Loss/tok 3.3380 (3.3934)	LR 1.563e-05
0: TRAIN [2][2350/5173]	Time 0.570 (0.610)	Data 1.18e-04 (3.89e-04)	Tok/s 17854 (23063)	Loss/tok 3.2342 (3.3934)	LR 1.563e-05
0: TRAIN [2][2360/5173]	Time 0.565 (0.610)	Data 1.21e-04 (3.87e-04)	Tok/s 18267 (23064)	Loss/tok 3.2025 (3.3933)	LR 1.563e-05
0: TRAIN [2][2370/5173]	Time 0.634 (0.610)	Data 1.29e-04 (3.86e-04)	Tok/s 26738 (23063)	Loss/tok 3.3323 (3.3934)	LR 1.563e-05
0: TRAIN [2][2380/5173]	Time 0.644 (0.610)	Data 1.22e-04 (3.85e-04)	Tok/s 25811 (23048)	Loss/tok 3.4496 (3.3930)	LR 1.563e-05
0: TRAIN [2][2390/5173]	Time 0.567 (0.610)	Data 1.23e-04 (3.84e-04)	Tok/s 17873 (23049)	Loss/tok 3.2283 (3.3928)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][2400/5173]	Time 0.566 (0.610)	Data 1.28e-04 (3.83e-04)	Tok/s 17976 (23060)	Loss/tok 3.2348 (3.3931)	LR 1.563e-05
0: TRAIN [2][2410/5173]	Time 0.559 (0.610)	Data 1.21e-04 (3.82e-04)	Tok/s 18529 (23039)	Loss/tok 3.2032 (3.3928)	LR 1.563e-05
0: TRAIN [2][2420/5173]	Time 0.569 (0.610)	Data 1.21e-04 (3.81e-04)	Tok/s 18700 (23051)	Loss/tok 3.3069 (3.3931)	LR 1.563e-05
0: TRAIN [2][2430/5173]	Time 0.776 (0.610)	Data 1.40e-04 (3.80e-04)	Tok/s 38726 (23064)	Loss/tok 3.7230 (3.3937)	LR 1.563e-05
0: TRAIN [2][2440/5173]	Time 0.567 (0.610)	Data 1.27e-04 (3.79e-04)	Tok/s 18445 (23067)	Loss/tok 3.0297 (3.3937)	LR 1.563e-05
0: TRAIN [2][2450/5173]	Time 0.567 (0.610)	Data 1.30e-04 (3.78e-04)	Tok/s 18001 (23059)	Loss/tok 3.1637 (3.3934)	LR 1.563e-05
0: TRAIN [2][2460/5173]	Time 0.576 (0.610)	Data 1.30e-04 (3.77e-04)	Tok/s 17698 (23066)	Loss/tok 3.2017 (3.3937)	LR 1.563e-05
0: TRAIN [2][2470/5173]	Time 0.629 (0.610)	Data 1.18e-04 (3.76e-04)	Tok/s 26672 (23079)	Loss/tok 3.3734 (3.3937)	LR 1.563e-05
0: TRAIN [2][2480/5173]	Time 0.566 (0.610)	Data 1.22e-04 (3.75e-04)	Tok/s 17902 (23068)	Loss/tok 3.2455 (3.3935)	LR 1.563e-05
0: TRAIN [2][2490/5173]	Time 0.567 (0.610)	Data 1.25e-04 (3.74e-04)	Tok/s 18163 (23071)	Loss/tok 3.2550 (3.3935)	LR 1.563e-05
0: TRAIN [2][2500/5173]	Time 0.562 (0.610)	Data 1.21e-04 (3.73e-04)	Tok/s 18476 (23074)	Loss/tok 3.2707 (3.3935)	LR 1.563e-05
0: TRAIN [2][2510/5173]	Time 0.565 (0.610)	Data 1.28e-04 (3.72e-04)	Tok/s 17891 (23073)	Loss/tok 3.0733 (3.3932)	LR 1.563e-05
0: TRAIN [2][2520/5173]	Time 0.559 (0.610)	Data 1.29e-04 (3.71e-04)	Tok/s 18393 (23088)	Loss/tok 3.0852 (3.3941)	LR 1.563e-05
0: TRAIN [2][2530/5173]	Time 0.569 (0.610)	Data 1.19e-04 (3.70e-04)	Tok/s 17866 (23084)	Loss/tok 3.2146 (3.3939)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][2540/5173]	Time 0.563 (0.610)	Data 1.22e-04 (3.69e-04)	Tok/s 18543 (23092)	Loss/tok 3.2212 (3.3940)	LR 1.563e-05
0: TRAIN [2][2550/5173]	Time 0.500 (0.610)	Data 2.90e-04 (3.69e-04)	Tok/s 10783 (23086)	Loss/tok 2.7823 (3.3936)	LR 1.563e-05
0: TRAIN [2][2560/5173]	Time 0.762 (0.610)	Data 1.28e-04 (3.68e-04)	Tok/s 38829 (23098)	Loss/tok 3.8693 (3.3939)	LR 1.563e-05
0: TRAIN [2][2570/5173]	Time 0.628 (0.610)	Data 1.23e-04 (3.67e-04)	Tok/s 26795 (23105)	Loss/tok 3.3678 (3.3942)	LR 1.563e-05
0: TRAIN [2][2580/5173]	Time 0.771 (0.610)	Data 1.34e-04 (3.66e-04)	Tok/s 39110 (23120)	Loss/tok 3.7828 (3.3948)	LR 1.563e-05
0: TRAIN [2][2590/5173]	Time 0.772 (0.610)	Data 1.34e-04 (3.65e-04)	Tok/s 38472 (23128)	Loss/tok 3.8566 (3.3950)	LR 1.563e-05
0: TRAIN [2][2600/5173]	Time 0.565 (0.610)	Data 3.09e-04 (3.65e-04)	Tok/s 18124 (23126)	Loss/tok 3.2326 (3.3949)	LR 1.563e-05
0: TRAIN [2][2610/5173]	Time 0.565 (0.610)	Data 1.35e-04 (3.64e-04)	Tok/s 18680 (23127)	Loss/tok 3.1435 (3.3951)	LR 1.563e-05
0: TRAIN [2][2620/5173]	Time 0.565 (0.611)	Data 1.26e-04 (3.63e-04)	Tok/s 18822 (23136)	Loss/tok 3.1511 (3.3950)	LR 1.563e-05
0: TRAIN [2][2630/5173]	Time 0.566 (0.611)	Data 1.71e-04 (3.62e-04)	Tok/s 18467 (23159)	Loss/tok 3.2772 (3.3960)	LR 1.563e-05
0: TRAIN [2][2640/5173]	Time 0.636 (0.611)	Data 1.26e-04 (3.61e-04)	Tok/s 26036 (23147)	Loss/tok 3.2878 (3.3956)	LR 1.563e-05
0: TRAIN [2][2650/5173]	Time 0.690 (0.611)	Data 1.31e-04 (3.60e-04)	Tok/s 33124 (23155)	Loss/tok 3.6522 (3.3957)	LR 1.563e-05
0: TRAIN [2][2660/5173]	Time 0.506 (0.611)	Data 1.23e-04 (3.59e-04)	Tok/s 10631 (23154)	Loss/tok 2.7779 (3.3959)	LR 1.563e-05
0: TRAIN [2][2670/5173]	Time 0.567 (0.611)	Data 1.25e-04 (3.58e-04)	Tok/s 17835 (23156)	Loss/tok 3.1631 (3.3960)	LR 1.563e-05
0: TRAIN [2][2680/5173]	Time 0.570 (0.611)	Data 1.24e-04 (3.57e-04)	Tok/s 18187 (23149)	Loss/tok 3.1777 (3.3958)	LR 1.563e-05
0: TRAIN [2][2690/5173]	Time 0.706 (0.611)	Data 1.21e-04 (3.57e-04)	Tok/s 32814 (23148)	Loss/tok 3.5952 (3.3959)	LR 1.563e-05
0: TRAIN [2][2700/5173]	Time 0.710 (0.611)	Data 1.23e-04 (3.56e-04)	Tok/s 32996 (23142)	Loss/tok 3.4995 (3.3956)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][2710/5173]	Time 0.560 (0.611)	Data 1.38e-04 (3.55e-04)	Tok/s 18234 (23165)	Loss/tok 3.1600 (3.3964)	LR 1.563e-05
0: TRAIN [2][2720/5173]	Time 0.708 (0.611)	Data 1.22e-04 (3.54e-04)	Tok/s 32794 (23164)	Loss/tok 3.6625 (3.3964)	LR 1.563e-05
0: TRAIN [2][2730/5173]	Time 0.564 (0.611)	Data 1.23e-04 (3.53e-04)	Tok/s 18239 (23155)	Loss/tok 3.2136 (3.3961)	LR 1.563e-05
0: TRAIN [2][2740/5173]	Time 0.569 (0.611)	Data 1.28e-04 (3.53e-04)	Tok/s 17985 (23157)	Loss/tok 3.1059 (3.3962)	LR 1.563e-05
0: TRAIN [2][2750/5173]	Time 0.573 (0.611)	Data 1.29e-04 (3.52e-04)	Tok/s 18314 (23160)	Loss/tok 3.0920 (3.3963)	LR 1.563e-05
0: TRAIN [2][2760/5173]	Time 0.567 (0.611)	Data 1.21e-04 (3.51e-04)	Tok/s 18230 (23165)	Loss/tok 3.1879 (3.3963)	LR 1.563e-05
0: TRAIN [2][2770/5173]	Time 0.559 (0.611)	Data 1.24e-04 (3.50e-04)	Tok/s 18652 (23158)	Loss/tok 3.2103 (3.3962)	LR 1.563e-05
0: TRAIN [2][2780/5173]	Time 0.640 (0.611)	Data 1.21e-04 (3.49e-04)	Tok/s 26437 (23159)	Loss/tok 3.3489 (3.3962)	LR 1.563e-05
0: TRAIN [2][2790/5173]	Time 0.646 (0.611)	Data 1.19e-04 (3.49e-04)	Tok/s 26318 (23157)	Loss/tok 3.4680 (3.3959)	LR 1.563e-05
0: TRAIN [2][2800/5173]	Time 0.769 (0.611)	Data 1.24e-04 (3.48e-04)	Tok/s 39088 (23161)	Loss/tok 3.7747 (3.3961)	LR 1.563e-05
0: TRAIN [2][2810/5173]	Time 0.704 (0.611)	Data 1.22e-04 (3.47e-04)	Tok/s 33404 (23164)	Loss/tok 3.4671 (3.3964)	LR 1.563e-05
0: TRAIN [2][2820/5173]	Time 0.571 (0.611)	Data 1.24e-04 (3.46e-04)	Tok/s 17967 (23172)	Loss/tok 3.1845 (3.3969)	LR 1.563e-05
0: TRAIN [2][2830/5173]	Time 0.573 (0.611)	Data 1.19e-04 (3.46e-04)	Tok/s 18328 (23158)	Loss/tok 3.1674 (3.3966)	LR 1.563e-05
0: TRAIN [2][2840/5173]	Time 0.635 (0.611)	Data 1.29e-04 (3.45e-04)	Tok/s 26351 (23169)	Loss/tok 3.4513 (3.3968)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][2850/5173]	Time 0.699 (0.611)	Data 1.29e-04 (3.44e-04)	Tok/s 33479 (23180)	Loss/tok 3.5406 (3.3970)	LR 1.563e-05
0: TRAIN [2][2860/5173]	Time 0.562 (0.611)	Data 1.23e-04 (3.44e-04)	Tok/s 18593 (23171)	Loss/tok 3.1804 (3.3967)	LR 1.563e-05
0: TRAIN [2][2870/5173]	Time 0.565 (0.611)	Data 1.23e-04 (3.43e-04)	Tok/s 18264 (23167)	Loss/tok 3.1801 (3.3967)	LR 1.563e-05
0: TRAIN [2][2880/5173]	Time 0.699 (0.611)	Data 1.19e-04 (3.42e-04)	Tok/s 33314 (23169)	Loss/tok 3.5326 (3.3968)	LR 1.563e-05
0: TRAIN [2][2890/5173]	Time 0.562 (0.611)	Data 1.26e-04 (3.41e-04)	Tok/s 18041 (23158)	Loss/tok 3.3157 (3.3965)	LR 1.563e-05
0: TRAIN [2][2900/5173]	Time 0.573 (0.611)	Data 1.45e-04 (3.41e-04)	Tok/s 18336 (23176)	Loss/tok 3.1166 (3.3970)	LR 1.563e-05
0: TRAIN [2][2910/5173]	Time 0.642 (0.611)	Data 1.23e-04 (3.40e-04)	Tok/s 26268 (23169)	Loss/tok 3.4215 (3.3969)	LR 1.563e-05
0: TRAIN [2][2920/5173]	Time 0.692 (0.611)	Data 1.24e-04 (3.39e-04)	Tok/s 33606 (23178)	Loss/tok 3.7046 (3.3971)	LR 1.563e-05
0: TRAIN [2][2930/5173]	Time 0.570 (0.611)	Data 1.30e-04 (3.38e-04)	Tok/s 18305 (23179)	Loss/tok 3.1929 (3.3973)	LR 1.563e-05
0: TRAIN [2][2940/5173]	Time 0.565 (0.611)	Data 1.22e-04 (3.38e-04)	Tok/s 18175 (23176)	Loss/tok 3.1623 (3.3972)	LR 1.563e-05
0: TRAIN [2][2950/5173]	Time 0.692 (0.611)	Data 1.29e-04 (3.37e-04)	Tok/s 33848 (23186)	Loss/tok 3.6451 (3.3977)	LR 1.563e-05
0: TRAIN [2][2960/5173]	Time 0.700 (0.611)	Data 1.25e-04 (3.36e-04)	Tok/s 33204 (23178)	Loss/tok 3.5161 (3.3975)	LR 1.563e-05
0: TRAIN [2][2970/5173]	Time 0.763 (0.611)	Data 1.20e-04 (3.36e-04)	Tok/s 39459 (23180)	Loss/tok 3.7843 (3.3978)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][2980/5173]	Time 0.498 (0.611)	Data 1.22e-04 (3.35e-04)	Tok/s 10796 (23186)	Loss/tok 2.7148 (3.3980)	LR 1.563e-05
0: TRAIN [2][2990/5173]	Time 0.566 (0.611)	Data 1.24e-04 (3.34e-04)	Tok/s 17854 (23180)	Loss/tok 3.1741 (3.3977)	LR 1.563e-05
0: TRAIN [2][3000/5173]	Time 0.564 (0.611)	Data 1.23e-04 (3.34e-04)	Tok/s 18214 (23182)	Loss/tok 3.1543 (3.3975)	LR 1.563e-05
0: TRAIN [2][3010/5173]	Time 0.770 (0.611)	Data 1.44e-04 (3.33e-04)	Tok/s 38590 (23188)	Loss/tok 3.7175 (3.3977)	LR 1.563e-05
0: TRAIN [2][3020/5173]	Time 0.638 (0.611)	Data 1.41e-04 (3.32e-04)	Tok/s 26108 (23188)	Loss/tok 3.4328 (3.3976)	LR 1.563e-05
0: TRAIN [2][3030/5173]	Time 0.558 (0.611)	Data 1.37e-04 (3.32e-04)	Tok/s 18702 (23194)	Loss/tok 3.2903 (3.3979)	LR 1.563e-05
0: TRAIN [2][3040/5173]	Time 0.636 (0.611)	Data 1.37e-04 (3.31e-04)	Tok/s 26562 (23207)	Loss/tok 3.4459 (3.3984)	LR 1.563e-05
0: TRAIN [2][3050/5173]	Time 0.561 (0.611)	Data 1.34e-04 (3.31e-04)	Tok/s 18450 (23224)	Loss/tok 3.3413 (3.3991)	LR 1.563e-05
0: TRAIN [2][3060/5173]	Time 0.622 (0.611)	Data 1.23e-04 (3.30e-04)	Tok/s 27215 (23216)	Loss/tok 3.3071 (3.3988)	LR 1.563e-05
0: TRAIN [2][3070/5173]	Time 0.621 (0.611)	Data 1.19e-04 (3.29e-04)	Tok/s 27194 (23222)	Loss/tok 3.3658 (3.3991)	LR 1.563e-05
0: TRAIN [2][3080/5173]	Time 0.639 (0.611)	Data 1.47e-04 (3.29e-04)	Tok/s 25834 (23221)	Loss/tok 3.2847 (3.3991)	LR 1.563e-05
0: TRAIN [2][3090/5173]	Time 0.628 (0.611)	Data 1.25e-04 (3.28e-04)	Tok/s 26527 (23224)	Loss/tok 3.3894 (3.3990)	LR 1.563e-05
0: TRAIN [2][3100/5173]	Time 0.567 (0.611)	Data 1.34e-04 (3.27e-04)	Tok/s 18537 (23232)	Loss/tok 3.2143 (3.3991)	LR 1.563e-05
0: TRAIN [2][3110/5173]	Time 0.569 (0.611)	Data 1.21e-04 (3.27e-04)	Tok/s 18213 (23230)	Loss/tok 3.1198 (3.3991)	LR 1.563e-05
0: TRAIN [2][3120/5173]	Time 0.643 (0.611)	Data 1.49e-04 (3.26e-04)	Tok/s 25678 (23241)	Loss/tok 3.4392 (3.3993)	LR 1.563e-05
0: TRAIN [2][3130/5173]	Time 0.579 (0.611)	Data 1.19e-04 (3.26e-04)	Tok/s 17742 (23228)	Loss/tok 3.2544 (3.3989)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][3140/5173]	Time 0.634 (0.611)	Data 1.21e-04 (3.25e-04)	Tok/s 26814 (23221)	Loss/tok 3.3649 (3.3987)	LR 1.563e-05
0: TRAIN [2][3150/5173]	Time 0.565 (0.611)	Data 1.21e-04 (3.24e-04)	Tok/s 17976 (23235)	Loss/tok 3.1802 (3.3990)	LR 1.563e-05
0: TRAIN [2][3160/5173]	Time 0.561 (0.611)	Data 1.21e-04 (3.24e-04)	Tok/s 18522 (23227)	Loss/tok 3.1578 (3.3987)	LR 1.563e-05
0: TRAIN [2][3170/5173]	Time 0.627 (0.611)	Data 1.23e-04 (3.23e-04)	Tok/s 26242 (23222)	Loss/tok 3.4463 (3.3985)	LR 1.563e-05
0: TRAIN [2][3180/5173]	Time 0.562 (0.611)	Data 1.28e-04 (3.23e-04)	Tok/s 18263 (23212)	Loss/tok 3.1799 (3.3981)	LR 1.563e-05
0: TRAIN [2][3190/5173]	Time 0.640 (0.611)	Data 1.24e-04 (3.22e-04)	Tok/s 26365 (23217)	Loss/tok 3.3600 (3.3982)	LR 1.563e-05
0: TRAIN [2][3200/5173]	Time 0.576 (0.611)	Data 1.32e-04 (3.21e-04)	Tok/s 17537 (23217)	Loss/tok 3.2415 (3.3982)	LR 1.563e-05
0: TRAIN [2][3210/5173]	Time 0.502 (0.611)	Data 1.20e-04 (3.21e-04)	Tok/s 10495 (23212)	Loss/tok 2.6304 (3.3980)	LR 1.563e-05
0: TRAIN [2][3220/5173]	Time 0.695 (0.611)	Data 1.26e-04 (3.20e-04)	Tok/s 33444 (23202)	Loss/tok 3.5192 (3.3976)	LR 1.563e-05
0: TRAIN [2][3230/5173]	Time 0.640 (0.611)	Data 1.20e-04 (3.20e-04)	Tok/s 26506 (23212)	Loss/tok 3.4201 (3.3978)	LR 1.563e-05
0: TRAIN [2][3240/5173]	Time 0.633 (0.611)	Data 1.22e-04 (3.19e-04)	Tok/s 26597 (23204)	Loss/tok 3.2809 (3.3974)	LR 1.563e-05
0: TRAIN [2][3250/5173]	Time 0.563 (0.611)	Data 1.21e-04 (3.19e-04)	Tok/s 18356 (23197)	Loss/tok 3.0899 (3.3972)	LR 1.563e-05
0: TRAIN [2][3260/5173]	Time 0.502 (0.611)	Data 1.18e-04 (3.18e-04)	Tok/s 10782 (23187)	Loss/tok 2.7676 (3.3968)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][3270/5173]	Time 0.496 (0.611)	Data 1.29e-04 (3.17e-04)	Tok/s 10481 (23194)	Loss/tok 2.7282 (3.3973)	LR 1.563e-05
0: TRAIN [2][3280/5173]	Time 0.645 (0.611)	Data 1.25e-04 (3.17e-04)	Tok/s 25780 (23185)	Loss/tok 3.5771 (3.3971)	LR 1.563e-05
0: TRAIN [2][3290/5173]	Time 0.562 (0.611)	Data 1.17e-04 (3.16e-04)	Tok/s 18711 (23174)	Loss/tok 3.1318 (3.3967)	LR 1.563e-05
0: TRAIN [2][3300/5173]	Time 0.566 (0.611)	Data 1.23e-04 (3.16e-04)	Tok/s 17966 (23169)	Loss/tok 3.1308 (3.3964)	LR 1.563e-05
0: TRAIN [2][3310/5173]	Time 0.561 (0.611)	Data 1.33e-04 (3.15e-04)	Tok/s 18444 (23164)	Loss/tok 3.1369 (3.3961)	LR 1.563e-05
0: TRAIN [2][3320/5173]	Time 0.702 (0.611)	Data 1.21e-04 (3.15e-04)	Tok/s 33304 (23172)	Loss/tok 3.5689 (3.3962)	LR 1.563e-05
0: TRAIN [2][3330/5173]	Time 0.560 (0.611)	Data 1.20e-04 (3.14e-04)	Tok/s 18549 (23169)	Loss/tok 3.1960 (3.3961)	LR 1.563e-05
0: TRAIN [2][3340/5173]	Time 0.772 (0.611)	Data 1.34e-04 (3.14e-04)	Tok/s 38189 (23169)	Loss/tok 3.7166 (3.3961)	LR 1.563e-05
0: TRAIN [2][3350/5173]	Time 0.628 (0.611)	Data 1.19e-04 (3.13e-04)	Tok/s 27130 (23171)	Loss/tok 3.3849 (3.3961)	LR 1.563e-05
0: TRAIN [2][3360/5173]	Time 0.630 (0.611)	Data 1.19e-04 (3.13e-04)	Tok/s 26462 (23173)	Loss/tok 3.3107 (3.3959)	LR 1.563e-05
0: TRAIN [2][3370/5173]	Time 0.502 (0.611)	Data 1.28e-04 (3.12e-04)	Tok/s 10492 (23168)	Loss/tok 2.7623 (3.3958)	LR 1.563e-05
0: TRAIN [2][3380/5173]	Time 0.563 (0.611)	Data 1.21e-04 (3.12e-04)	Tok/s 18275 (23161)	Loss/tok 3.2309 (3.3956)	LR 1.563e-05
0: TRAIN [2][3390/5173]	Time 0.643 (0.611)	Data 1.27e-04 (3.11e-04)	Tok/s 25839 (23161)	Loss/tok 3.4272 (3.3955)	LR 1.563e-05
0: TRAIN [2][3400/5173]	Time 0.569 (0.611)	Data 1.18e-04 (3.10e-04)	Tok/s 18362 (23155)	Loss/tok 3.2387 (3.3953)	LR 1.563e-05
0: TRAIN [2][3410/5173]	Time 0.702 (0.611)	Data 1.32e-04 (3.10e-04)	Tok/s 32820 (23163)	Loss/tok 3.6996 (3.3955)	LR 1.563e-05
0: TRAIN [2][3420/5173]	Time 0.630 (0.611)	Data 1.35e-04 (3.09e-04)	Tok/s 26434 (23159)	Loss/tok 3.4268 (3.3955)	LR 1.563e-05
0: TRAIN [2][3430/5173]	Time 0.566 (0.611)	Data 1.36e-04 (3.09e-04)	Tok/s 18305 (23155)	Loss/tok 3.2303 (3.3955)	LR 1.563e-05
0: TRAIN [2][3440/5173]	Time 0.570 (0.610)	Data 1.29e-04 (3.09e-04)	Tok/s 17987 (23145)	Loss/tok 3.2261 (3.3953)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][3450/5173]	Time 0.638 (0.611)	Data 1.20e-04 (3.08e-04)	Tok/s 26163 (23158)	Loss/tok 3.3505 (3.3956)	LR 1.563e-05
0: TRAIN [2][3460/5173]	Time 0.568 (0.611)	Data 1.26e-04 (3.07e-04)	Tok/s 17975 (23164)	Loss/tok 3.1565 (3.3959)	LR 1.563e-05
0: TRAIN [2][3470/5173]	Time 0.636 (0.611)	Data 1.23e-04 (3.07e-04)	Tok/s 26912 (23154)	Loss/tok 3.3231 (3.3956)	LR 1.563e-05
0: TRAIN [2][3480/5173]	Time 0.635 (0.610)	Data 1.30e-04 (3.06e-04)	Tok/s 26322 (23152)	Loss/tok 3.3696 (3.3954)	LR 1.563e-05
0: TRAIN [2][3490/5173]	Time 0.629 (0.610)	Data 1.24e-04 (3.06e-04)	Tok/s 27299 (23154)	Loss/tok 3.3341 (3.3953)	LR 1.563e-05
0: TRAIN [2][3500/5173]	Time 0.559 (0.611)	Data 1.23e-04 (3.05e-04)	Tok/s 18826 (23155)	Loss/tok 3.2012 (3.3955)	LR 1.563e-05
0: TRAIN [2][3510/5173]	Time 0.703 (0.610)	Data 2.79e-04 (3.05e-04)	Tok/s 32796 (23149)	Loss/tok 3.7448 (3.3954)	LR 1.563e-05
0: TRAIN [2][3520/5173]	Time 0.641 (0.610)	Data 1.23e-04 (3.05e-04)	Tok/s 26078 (23142)	Loss/tok 3.3989 (3.3952)	LR 1.563e-05
0: TRAIN [2][3530/5173]	Time 0.628 (0.610)	Data 1.23e-04 (3.04e-04)	Tok/s 26484 (23139)	Loss/tok 3.3860 (3.3950)	LR 1.563e-05
0: TRAIN [2][3540/5173]	Time 0.560 (0.610)	Data 1.27e-04 (3.04e-04)	Tok/s 18549 (23135)	Loss/tok 3.1633 (3.3948)	LR 1.563e-05
0: TRAIN [2][3550/5173]	Time 0.503 (0.610)	Data 1.24e-04 (3.03e-04)	Tok/s 10467 (23138)	Loss/tok 2.7612 (3.3949)	LR 1.563e-05
0: TRAIN [2][3560/5173]	Time 0.567 (0.610)	Data 1.23e-04 (3.03e-04)	Tok/s 18141 (23129)	Loss/tok 3.1843 (3.3947)	LR 1.563e-05
0: TRAIN [2][3570/5173]	Time 0.693 (0.610)	Data 1.26e-04 (3.02e-04)	Tok/s 33805 (23137)	Loss/tok 3.5743 (3.3948)	LR 1.563e-05
0: TRAIN [2][3580/5173]	Time 0.706 (0.610)	Data 1.19e-04 (3.02e-04)	Tok/s 32903 (23135)	Loss/tok 3.6398 (3.3947)	LR 1.563e-05
0: TRAIN [2][3590/5173]	Time 0.638 (0.610)	Data 1.24e-04 (3.01e-04)	Tok/s 25920 (23127)	Loss/tok 3.3625 (3.3945)	LR 1.563e-05
0: TRAIN [2][3600/5173]	Time 0.570 (0.610)	Data 1.17e-04 (3.01e-04)	Tok/s 18312 (23124)	Loss/tok 3.1376 (3.3943)	LR 1.563e-05
0: TRAIN [2][3610/5173]	Time 0.570 (0.610)	Data 1.19e-04 (3.00e-04)	Tok/s 18123 (23118)	Loss/tok 3.2682 (3.3941)	LR 1.563e-05
0: TRAIN [2][3620/5173]	Time 0.768 (0.610)	Data 1.28e-04 (3.00e-04)	Tok/s 39062 (23125)	Loss/tok 3.6681 (3.3942)	LR 1.563e-05
0: TRAIN [2][3630/5173]	Time 0.571 (0.610)	Data 1.27e-04 (2.99e-04)	Tok/s 18164 (23110)	Loss/tok 3.1118 (3.3938)	LR 1.563e-05
0: TRAIN [2][3640/5173]	Time 0.649 (0.610)	Data 1.15e-04 (2.99e-04)	Tok/s 25511 (23096)	Loss/tok 3.4285 (3.3934)	LR 1.563e-05
0: TRAIN [2][3650/5173]	Time 0.646 (0.610)	Data 1.33e-04 (2.99e-04)	Tok/s 25717 (23097)	Loss/tok 3.4690 (3.3935)	LR 1.563e-05
0: TRAIN [2][3660/5173]	Time 0.688 (0.610)	Data 1.22e-04 (2.98e-04)	Tok/s 33901 (23099)	Loss/tok 3.5029 (3.3935)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][3670/5173]	Time 0.702 (0.610)	Data 1.20e-04 (2.98e-04)	Tok/s 33005 (23104)	Loss/tok 3.5715 (3.3936)	LR 1.563e-05
0: TRAIN [2][3680/5173]	Time 0.632 (0.610)	Data 1.16e-04 (2.97e-04)	Tok/s 26644 (23105)	Loss/tok 3.5035 (3.3935)	LR 1.563e-05
0: TRAIN [2][3690/5173]	Time 0.569 (0.610)	Data 1.32e-04 (2.97e-04)	Tok/s 18362 (23101)	Loss/tok 3.1944 (3.3934)	LR 1.563e-05
0: TRAIN [2][3700/5173]	Time 0.627 (0.610)	Data 1.37e-04 (2.96e-04)	Tok/s 26870 (23097)	Loss/tok 3.2925 (3.3931)	LR 1.563e-05
0: TRAIN [2][3710/5173]	Time 0.702 (0.610)	Data 1.41e-04 (2.96e-04)	Tok/s 33347 (23098)	Loss/tok 3.6027 (3.3932)	LR 1.563e-05
0: TRAIN [2][3720/5173]	Time 0.567 (0.610)	Data 1.40e-04 (2.95e-04)	Tok/s 18626 (23106)	Loss/tok 3.2112 (3.3934)	LR 1.563e-05
0: TRAIN [2][3730/5173]	Time 0.637 (0.610)	Data 1.25e-04 (2.95e-04)	Tok/s 26448 (23109)	Loss/tok 3.3082 (3.3934)	LR 1.563e-05
0: TRAIN [2][3740/5173]	Time 0.640 (0.610)	Data 1.22e-04 (2.94e-04)	Tok/s 26113 (23114)	Loss/tok 3.4174 (3.3937)	LR 1.563e-05
0: TRAIN [2][3750/5173]	Time 0.499 (0.610)	Data 1.24e-04 (2.94e-04)	Tok/s 10588 (23111)	Loss/tok 2.6405 (3.3935)	LR 1.563e-05
0: TRAIN [2][3760/5173]	Time 0.764 (0.610)	Data 1.85e-04 (2.93e-04)	Tok/s 39069 (23114)	Loss/tok 3.7915 (3.3938)	LR 1.563e-05
0: TRAIN [2][3770/5173]	Time 0.561 (0.610)	Data 1.13e-04 (2.93e-04)	Tok/s 18795 (23109)	Loss/tok 3.1597 (3.3937)	LR 1.563e-05
0: TRAIN [2][3780/5173]	Time 0.497 (0.610)	Data 1.18e-04 (2.93e-04)	Tok/s 10740 (23104)	Loss/tok 2.7729 (3.3936)	LR 1.563e-05
0: TRAIN [2][3790/5173]	Time 0.501 (0.610)	Data 1.14e-04 (2.92e-04)	Tok/s 10691 (23087)	Loss/tok 2.6297 (3.3932)	LR 1.563e-05
0: TRAIN [2][3800/5173]	Time 0.698 (0.610)	Data 1.20e-04 (2.92e-04)	Tok/s 33617 (23089)	Loss/tok 3.4211 (3.3932)	LR 1.563e-05
0: TRAIN [2][3810/5173]	Time 0.567 (0.610)	Data 1.22e-04 (2.91e-04)	Tok/s 17854 (23079)	Loss/tok 3.1089 (3.3930)	LR 1.563e-05
0: TRAIN [2][3820/5173]	Time 0.570 (0.610)	Data 1.18e-04 (2.91e-04)	Tok/s 18171 (23080)	Loss/tok 3.2407 (3.3929)	LR 1.563e-05
0: TRAIN [2][3830/5173]	Time 0.636 (0.610)	Data 1.23e-04 (2.90e-04)	Tok/s 26737 (23084)	Loss/tok 3.4178 (3.3929)	LR 1.563e-05
0: TRAIN [2][3840/5173]	Time 0.641 (0.610)	Data 1.19e-04 (2.90e-04)	Tok/s 26546 (23077)	Loss/tok 3.3770 (3.3927)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][3850/5173]	Time 0.642 (0.610)	Data 1.22e-04 (2.90e-04)	Tok/s 26027 (23081)	Loss/tok 3.4863 (3.3929)	LR 1.563e-05
0: TRAIN [2][3860/5173]	Time 0.570 (0.610)	Data 1.23e-04 (2.89e-04)	Tok/s 18360 (23078)	Loss/tok 3.2763 (3.3929)	LR 1.563e-05
0: TRAIN [2][3870/5173]	Time 0.702 (0.610)	Data 1.29e-04 (2.89e-04)	Tok/s 32945 (23082)	Loss/tok 3.5589 (3.3929)	LR 1.563e-05
0: TRAIN [2][3880/5173]	Time 0.624 (0.610)	Data 1.23e-04 (2.88e-04)	Tok/s 27260 (23076)	Loss/tok 3.3937 (3.3926)	LR 1.563e-05
0: TRAIN [2][3890/5173]	Time 0.626 (0.610)	Data 3.07e-04 (2.88e-04)	Tok/s 26704 (23072)	Loss/tok 3.4081 (3.3925)	LR 1.563e-05
0: TRAIN [2][3900/5173]	Time 0.497 (0.610)	Data 1.22e-04 (2.88e-04)	Tok/s 10611 (23075)	Loss/tok 2.8275 (3.3926)	LR 1.563e-05
0: TRAIN [2][3910/5173]	Time 0.702 (0.610)	Data 1.25e-04 (2.87e-04)	Tok/s 33037 (23080)	Loss/tok 3.6006 (3.3928)	LR 1.563e-05
0: TRAIN [2][3920/5173]	Time 0.629 (0.610)	Data 1.25e-04 (2.87e-04)	Tok/s 26972 (23079)	Loss/tok 3.4849 (3.3927)	LR 1.563e-05
0: TRAIN [2][3930/5173]	Time 0.621 (0.610)	Data 1.20e-04 (2.87e-04)	Tok/s 26943 (23076)	Loss/tok 3.5296 (3.3926)	LR 1.563e-05
0: TRAIN [2][3940/5173]	Time 0.574 (0.610)	Data 1.22e-04 (2.86e-04)	Tok/s 18112 (23074)	Loss/tok 3.2131 (3.3924)	LR 1.563e-05
0: TRAIN [2][3950/5173]	Time 0.563 (0.610)	Data 1.22e-04 (2.86e-04)	Tok/s 18631 (23077)	Loss/tok 3.2155 (3.3926)	LR 1.563e-05
0: TRAIN [2][3960/5173]	Time 0.705 (0.610)	Data 1.21e-04 (2.85e-04)	Tok/s 33118 (23082)	Loss/tok 3.4795 (3.3927)	LR 1.563e-05
0: TRAIN [2][3970/5173]	Time 0.567 (0.610)	Data 1.19e-04 (2.85e-04)	Tok/s 17773 (23076)	Loss/tok 3.1663 (3.3925)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][3980/5173]	Time 0.568 (0.610)	Data 1.26e-04 (2.85e-04)	Tok/s 17292 (23084)	Loss/tok 3.2430 (3.3930)	LR 1.563e-05
0: TRAIN [2][3990/5173]	Time 0.566 (0.610)	Data 1.19e-04 (2.84e-04)	Tok/s 18070 (23073)	Loss/tok 3.2184 (3.3927)	LR 1.563e-05
0: TRAIN [2][4000/5173]	Time 0.701 (0.610)	Data 1.18e-04 (2.84e-04)	Tok/s 33138 (23077)	Loss/tok 3.6676 (3.3926)	LR 1.563e-05
0: TRAIN [2][4010/5173]	Time 0.561 (0.610)	Data 1.23e-04 (2.83e-04)	Tok/s 18334 (23073)	Loss/tok 3.1284 (3.3925)	LR 1.563e-05
0: TRAIN [2][4020/5173]	Time 0.564 (0.610)	Data 1.17e-04 (2.83e-04)	Tok/s 18271 (23063)	Loss/tok 3.1531 (3.3922)	LR 1.563e-05
0: TRAIN [2][4030/5173]	Time 0.644 (0.610)	Data 1.13e-04 (2.83e-04)	Tok/s 26048 (23061)	Loss/tok 3.3704 (3.3920)	LR 1.563e-05
0: TRAIN [2][4040/5173]	Time 0.706 (0.610)	Data 1.36e-04 (2.82e-04)	Tok/s 33027 (23069)	Loss/tok 3.5945 (3.3923)	LR 1.563e-05
0: TRAIN [2][4050/5173]	Time 0.699 (0.610)	Data 1.19e-04 (2.82e-04)	Tok/s 33324 (23065)	Loss/tok 3.7175 (3.3923)	LR 1.563e-05
0: TRAIN [2][4060/5173]	Time 0.565 (0.610)	Data 1.69e-04 (2.82e-04)	Tok/s 18163 (23063)	Loss/tok 3.1998 (3.3920)	LR 1.563e-05
0: TRAIN [2][4070/5173]	Time 0.704 (0.610)	Data 1.70e-04 (2.81e-04)	Tok/s 33265 (23059)	Loss/tok 3.4803 (3.3918)	LR 1.563e-05
0: TRAIN [2][4080/5173]	Time 0.633 (0.610)	Data 1.71e-04 (2.81e-04)	Tok/s 26502 (23058)	Loss/tok 3.4684 (3.3918)	LR 1.563e-05
0: TRAIN [2][4090/5173]	Time 0.643 (0.610)	Data 1.55e-04 (2.81e-04)	Tok/s 26062 (23055)	Loss/tok 3.3498 (3.3916)	LR 1.563e-05
0: TRAIN [2][4100/5173]	Time 0.704 (0.610)	Data 1.66e-04 (2.81e-04)	Tok/s 32838 (23059)	Loss/tok 3.5682 (3.3918)	LR 1.563e-05
0: TRAIN [2][4110/5173]	Time 0.642 (0.610)	Data 1.80e-04 (2.81e-04)	Tok/s 25837 (23061)	Loss/tok 3.4203 (3.3917)	LR 1.563e-05
0: TRAIN [2][4120/5173]	Time 0.567 (0.610)	Data 3.44e-04 (2.80e-04)	Tok/s 18330 (23060)	Loss/tok 3.1175 (3.3917)	LR 1.563e-05
0: TRAIN [2][4130/5173]	Time 0.643 (0.610)	Data 1.70e-04 (2.80e-04)	Tok/s 26126 (23058)	Loss/tok 3.3311 (3.3915)	LR 1.563e-05
0: TRAIN [2][4140/5173]	Time 0.634 (0.610)	Data 1.71e-04 (2.80e-04)	Tok/s 26743 (23057)	Loss/tok 3.4310 (3.3916)	LR 1.563e-05
0: TRAIN [2][4150/5173]	Time 0.505 (0.610)	Data 1.72e-04 (2.80e-04)	Tok/s 10502 (23057)	Loss/tok 2.7837 (3.3915)	LR 1.563e-05
0: TRAIN [2][4160/5173]	Time 0.567 (0.609)	Data 1.65e-04 (2.79e-04)	Tok/s 18132 (23047)	Loss/tok 3.1241 (3.3913)	LR 1.563e-05
0: TRAIN [2][4170/5173]	Time 0.632 (0.609)	Data 1.67e-04 (2.79e-04)	Tok/s 26607 (23046)	Loss/tok 3.2450 (3.3912)	LR 1.563e-05
0: TRAIN [2][4180/5173]	Time 0.701 (0.609)	Data 1.61e-04 (2.79e-04)	Tok/s 33595 (23043)	Loss/tok 3.5041 (3.3910)	LR 1.563e-05
0: TRAIN [2][4190/5173]	Time 0.644 (0.609)	Data 1.66e-04 (2.79e-04)	Tok/s 25935 (23042)	Loss/tok 3.4104 (3.3910)	LR 1.563e-05
0: TRAIN [2][4200/5173]	Time 0.566 (0.609)	Data 1.67e-04 (2.79e-04)	Tok/s 18569 (23039)	Loss/tok 3.1557 (3.3907)	LR 1.563e-05
0: TRAIN [2][4210/5173]	Time 0.571 (0.609)	Data 1.62e-04 (2.78e-04)	Tok/s 18270 (23040)	Loss/tok 3.1666 (3.3908)	LR 1.563e-05
0: TRAIN [2][4220/5173]	Time 0.566 (0.609)	Data 1.80e-04 (2.78e-04)	Tok/s 18279 (23027)	Loss/tok 3.1815 (3.3904)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][4230/5173]	Time 0.564 (0.609)	Data 1.67e-04 (2.78e-04)	Tok/s 18424 (23028)	Loss/tok 3.1997 (3.3905)	LR 1.563e-05
0: TRAIN [2][4240/5173]	Time 0.701 (0.609)	Data 1.67e-04 (2.78e-04)	Tok/s 33594 (23025)	Loss/tok 3.4525 (3.3904)	LR 1.563e-05
0: TRAIN [2][4250/5173]	Time 0.569 (0.609)	Data 1.77e-04 (2.77e-04)	Tok/s 18172 (23026)	Loss/tok 3.1849 (3.3905)	LR 1.563e-05
0: TRAIN [2][4260/5173]	Time 0.563 (0.609)	Data 1.81e-04 (2.77e-04)	Tok/s 18516 (23033)	Loss/tok 3.2045 (3.3907)	LR 1.563e-05
0: TRAIN [2][4270/5173]	Time 0.562 (0.609)	Data 1.65e-04 (2.77e-04)	Tok/s 18469 (23030)	Loss/tok 3.1793 (3.3906)	LR 1.563e-05
0: TRAIN [2][4280/5173]	Time 0.569 (0.609)	Data 1.20e-04 (2.77e-04)	Tok/s 18038 (23038)	Loss/tok 3.0210 (3.3909)	LR 1.563e-05
0: TRAIN [2][4290/5173]	Time 0.564 (0.609)	Data 1.76e-04 (2.76e-04)	Tok/s 18257 (23041)	Loss/tok 3.2533 (3.3909)	LR 1.563e-05
0: TRAIN [2][4300/5173]	Time 0.564 (0.609)	Data 1.65e-04 (2.76e-04)	Tok/s 18499 (23035)	Loss/tok 3.1084 (3.3907)	LR 1.563e-05
0: TRAIN [2][4310/5173]	Time 0.572 (0.609)	Data 1.25e-04 (2.76e-04)	Tok/s 18271 (23039)	Loss/tok 3.1937 (3.3907)	LR 1.563e-05
0: TRAIN [2][4320/5173]	Time 0.637 (0.609)	Data 1.23e-04 (2.76e-04)	Tok/s 26486 (23044)	Loss/tok 3.3858 (3.3908)	LR 1.563e-05
0: TRAIN [2][4330/5173]	Time 0.642 (0.609)	Data 1.19e-04 (2.75e-04)	Tok/s 26330 (23035)	Loss/tok 3.3527 (3.3905)	LR 1.563e-05
0: TRAIN [2][4340/5173]	Time 0.762 (0.609)	Data 1.23e-04 (2.75e-04)	Tok/s 38765 (23040)	Loss/tok 3.7937 (3.3908)	LR 1.563e-05
0: TRAIN [2][4350/5173]	Time 0.569 (0.609)	Data 1.24e-04 (2.75e-04)	Tok/s 17931 (23031)	Loss/tok 3.1573 (3.3906)	LR 1.563e-05
0: TRAIN [2][4360/5173]	Time 0.633 (0.609)	Data 1.44e-04 (2.74e-04)	Tok/s 26538 (23036)	Loss/tok 3.3668 (3.3907)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][4370/5173]	Time 0.763 (0.609)	Data 1.28e-04 (2.74e-04)	Tok/s 38887 (23042)	Loss/tok 3.8006 (3.3909)	LR 1.563e-05
0: TRAIN [2][4380/5173]	Time 0.635 (0.609)	Data 1.25e-04 (2.74e-04)	Tok/s 26604 (23042)	Loss/tok 3.3120 (3.3908)	LR 1.563e-05
0: TRAIN [2][4390/5173]	Time 0.567 (0.609)	Data 1.24e-04 (2.73e-04)	Tok/s 18376 (23029)	Loss/tok 3.2543 (3.3906)	LR 1.563e-05
0: TRAIN [2][4400/5173]	Time 0.563 (0.609)	Data 1.25e-04 (2.73e-04)	Tok/s 18526 (23035)	Loss/tok 3.1473 (3.3907)	LR 1.563e-05
0: TRAIN [2][4410/5173]	Time 0.563 (0.609)	Data 1.35e-04 (2.73e-04)	Tok/s 18381 (23043)	Loss/tok 3.1803 (3.3909)	LR 1.563e-05
0: TRAIN [2][4420/5173]	Time 0.569 (0.609)	Data 1.23e-04 (2.72e-04)	Tok/s 18040 (23036)	Loss/tok 3.1132 (3.3907)	LR 1.563e-05
0: TRAIN [2][4430/5173]	Time 0.570 (0.609)	Data 1.35e-04 (2.72e-04)	Tok/s 18124 (23041)	Loss/tok 3.1629 (3.3909)	LR 1.563e-05
0: TRAIN [2][4440/5173]	Time 0.702 (0.610)	Data 1.22e-04 (2.72e-04)	Tok/s 33404 (23052)	Loss/tok 3.5383 (3.3913)	LR 1.563e-05
0: TRAIN [2][4450/5173]	Time 0.563 (0.609)	Data 1.22e-04 (2.71e-04)	Tok/s 18613 (23045)	Loss/tok 3.2090 (3.3911)	LR 1.563e-05
0: TRAIN [2][4460/5173]	Time 0.645 (0.609)	Data 1.21e-04 (2.71e-04)	Tok/s 26030 (23043)	Loss/tok 3.3950 (3.3910)	LR 1.563e-05
0: TRAIN [2][4470/5173]	Time 0.640 (0.609)	Data 1.22e-04 (2.71e-04)	Tok/s 26347 (23041)	Loss/tok 3.4182 (3.3908)	LR 1.563e-05
0: TRAIN [2][4480/5173]	Time 0.689 (0.609)	Data 1.24e-04 (2.70e-04)	Tok/s 33692 (23045)	Loss/tok 3.6368 (3.3910)	LR 1.563e-05
0: TRAIN [2][4490/5173]	Time 0.630 (0.609)	Data 1.20e-04 (2.70e-04)	Tok/s 26081 (23045)	Loss/tok 3.3599 (3.3910)	LR 1.563e-05
0: TRAIN [2][4500/5173]	Time 0.642 (0.609)	Data 1.20e-04 (2.70e-04)	Tok/s 26315 (23042)	Loss/tok 3.4143 (3.3908)	LR 1.563e-05
0: TRAIN [2][4510/5173]	Time 0.643 (0.609)	Data 1.25e-04 (2.70e-04)	Tok/s 26376 (23043)	Loss/tok 3.3518 (3.3908)	LR 1.563e-05
0: TRAIN [2][4520/5173]	Time 0.564 (0.609)	Data 1.13e-04 (2.69e-04)	Tok/s 18463 (23036)	Loss/tok 3.1070 (3.3906)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][4530/5173]	Time 0.565 (0.609)	Data 5.81e-04 (2.69e-04)	Tok/s 18556 (23042)	Loss/tok 3.1733 (3.3908)	LR 1.563e-05
0: TRAIN [2][4540/5173]	Time 0.502 (0.609)	Data 1.26e-04 (2.69e-04)	Tok/s 10730 (23038)	Loss/tok 2.7477 (3.3908)	LR 1.563e-05
0: TRAIN [2][4550/5173]	Time 0.570 (0.609)	Data 1.22e-04 (2.68e-04)	Tok/s 18210 (23036)	Loss/tok 3.0984 (3.3907)	LR 1.563e-05
0: TRAIN [2][4560/5173]	Time 0.567 (0.609)	Data 1.23e-04 (2.68e-04)	Tok/s 18014 (23042)	Loss/tok 3.1830 (3.3909)	LR 1.563e-05
0: TRAIN [2][4570/5173]	Time 0.644 (0.609)	Data 1.32e-04 (2.68e-04)	Tok/s 26047 (23042)	Loss/tok 3.4314 (3.3909)	LR 1.563e-05
0: TRAIN [2][4580/5173]	Time 0.771 (0.609)	Data 1.26e-04 (2.68e-04)	Tok/s 38730 (23042)	Loss/tok 3.7464 (3.3910)	LR 1.563e-05
0: TRAIN [2][4590/5173]	Time 0.769 (0.610)	Data 1.28e-04 (2.67e-04)	Tok/s 38360 (23050)	Loss/tok 3.7415 (3.3912)	LR 1.563e-05
0: TRAIN [2][4600/5173]	Time 0.627 (0.610)	Data 1.17e-04 (2.67e-04)	Tok/s 27044 (23051)	Loss/tok 3.3879 (3.3913)	LR 1.563e-05
0: TRAIN [2][4610/5173]	Time 0.567 (0.610)	Data 1.31e-04 (2.67e-04)	Tok/s 18080 (23060)	Loss/tok 3.1096 (3.3916)	LR 1.563e-05
0: TRAIN [2][4620/5173]	Time 0.563 (0.610)	Data 1.21e-04 (2.66e-04)	Tok/s 18523 (23065)	Loss/tok 3.1861 (3.3916)	LR 1.563e-05
0: TRAIN [2][4630/5173]	Time 0.705 (0.610)	Data 3.12e-04 (2.66e-04)	Tok/s 33214 (23069)	Loss/tok 3.5943 (3.3918)	LR 1.563e-05
0: TRAIN [2][4640/5173]	Time 0.687 (0.610)	Data 1.22e-04 (2.66e-04)	Tok/s 34153 (23071)	Loss/tok 3.6111 (3.3918)	LR 1.563e-05
0: TRAIN [2][4650/5173]	Time 0.570 (0.610)	Data 1.32e-04 (2.66e-04)	Tok/s 18325 (23080)	Loss/tok 3.2133 (3.3922)	LR 1.563e-05
0: TRAIN [2][4660/5173]	Time 0.567 (0.610)	Data 1.18e-04 (2.65e-04)	Tok/s 18365 (23077)	Loss/tok 3.1269 (3.3921)	LR 1.563e-05
0: TRAIN [2][4670/5173]	Time 0.570 (0.610)	Data 1.24e-04 (2.65e-04)	Tok/s 18394 (23080)	Loss/tok 3.1729 (3.3921)	LR 1.563e-05
0: TRAIN [2][4680/5173]	Time 0.559 (0.610)	Data 1.20e-04 (2.65e-04)	Tok/s 17990 (23079)	Loss/tok 3.2129 (3.3920)	LR 1.563e-05
0: TRAIN [2][4690/5173]	Time 0.562 (0.610)	Data 1.18e-04 (2.64e-04)	Tok/s 18419 (23070)	Loss/tok 3.1704 (3.3917)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][4700/5173]	Time 0.637 (0.610)	Data 1.20e-04 (2.64e-04)	Tok/s 26667 (23077)	Loss/tok 3.2538 (3.3920)	LR 1.563e-05
0: TRAIN [2][4710/5173]	Time 0.766 (0.610)	Data 1.26e-04 (2.64e-04)	Tok/s 38424 (23083)	Loss/tok 3.7949 (3.3922)	LR 1.563e-05
0: TRAIN [2][4720/5173]	Time 0.640 (0.610)	Data 1.20e-04 (2.64e-04)	Tok/s 26194 (23078)	Loss/tok 3.3664 (3.3920)	LR 1.563e-05
0: TRAIN [2][4730/5173]	Time 0.702 (0.610)	Data 1.13e-04 (2.63e-04)	Tok/s 33269 (23077)	Loss/tok 3.6549 (3.3919)	LR 1.563e-05
0: TRAIN [2][4740/5173]	Time 0.768 (0.610)	Data 1.33e-04 (2.63e-04)	Tok/s 38792 (23083)	Loss/tok 3.7840 (3.3921)	LR 1.563e-05
0: TRAIN [2][4750/5173]	Time 0.641 (0.610)	Data 1.23e-04 (2.63e-04)	Tok/s 26086 (23078)	Loss/tok 3.3832 (3.3919)	LR 1.563e-05
0: TRAIN [2][4760/5173]	Time 0.640 (0.610)	Data 1.19e-04 (2.62e-04)	Tok/s 26622 (23073)	Loss/tok 3.4198 (3.3917)	LR 1.563e-05
0: TRAIN [2][4770/5173]	Time 0.767 (0.610)	Data 1.27e-04 (2.62e-04)	Tok/s 39196 (23085)	Loss/tok 3.6461 (3.3920)	LR 1.563e-05
0: TRAIN [2][4780/5173]	Time 0.641 (0.610)	Data 1.19e-04 (2.62e-04)	Tok/s 25919 (23096)	Loss/tok 3.4740 (3.3922)	LR 1.563e-05
0: TRAIN [2][4790/5173]	Time 0.624 (0.610)	Data 1.20e-04 (2.62e-04)	Tok/s 26918 (23096)	Loss/tok 3.4585 (3.3921)	LR 1.563e-05
0: TRAIN [2][4800/5173]	Time 0.707 (0.610)	Data 1.32e-04 (2.61e-04)	Tok/s 32922 (23092)	Loss/tok 3.5758 (3.3920)	LR 1.563e-05
0: TRAIN [2][4810/5173]	Time 0.569 (0.610)	Data 1.20e-04 (2.61e-04)	Tok/s 18418 (23085)	Loss/tok 3.1620 (3.3918)	LR 1.563e-05
0: TRAIN [2][4820/5173]	Time 0.626 (0.610)	Data 1.40e-04 (2.61e-04)	Tok/s 26791 (23094)	Loss/tok 3.3405 (3.3921)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][4830/5173]	Time 0.707 (0.610)	Data 1.74e-04 (2.61e-04)	Tok/s 32797 (23108)	Loss/tok 3.5950 (3.3926)	LR 1.563e-05
0: TRAIN [2][4840/5173]	Time 0.634 (0.610)	Data 1.24e-04 (2.60e-04)	Tok/s 26139 (23110)	Loss/tok 3.4385 (3.3926)	LR 1.563e-05
0: TRAIN [2][4850/5173]	Time 0.686 (0.610)	Data 1.22e-04 (2.60e-04)	Tok/s 33673 (23106)	Loss/tok 3.6381 (3.3925)	LR 1.563e-05
0: TRAIN [2][4860/5173]	Time 0.552 (0.610)	Data 1.21e-04 (2.60e-04)	Tok/s 18574 (23097)	Loss/tok 3.2634 (3.3923)	LR 1.563e-05
0: TRAIN [2][4870/5173]	Time 0.562 (0.610)	Data 1.26e-04 (2.60e-04)	Tok/s 18121 (23094)	Loss/tok 3.2068 (3.3921)	LR 1.563e-05
0: TRAIN [2][4880/5173]	Time 0.699 (0.610)	Data 1.33e-04 (2.59e-04)	Tok/s 33932 (23097)	Loss/tok 3.5250 (3.3923)	LR 1.563e-05
0: TRAIN [2][4890/5173]	Time 0.644 (0.610)	Data 1.28e-04 (2.59e-04)	Tok/s 25911 (23095)	Loss/tok 3.4463 (3.3922)	LR 1.563e-05
0: TRAIN [2][4900/5173]	Time 0.770 (0.610)	Data 1.23e-04 (2.59e-04)	Tok/s 38277 (23105)	Loss/tok 3.8043 (3.3926)	LR 1.563e-05
0: TRAIN [2][4910/5173]	Time 0.559 (0.610)	Data 1.24e-04 (2.59e-04)	Tok/s 18336 (23100)	Loss/tok 3.1164 (3.3924)	LR 1.563e-05
0: TRAIN [2][4920/5173]	Time 0.562 (0.610)	Data 1.24e-04 (2.58e-04)	Tok/s 18123 (23092)	Loss/tok 3.1685 (3.3922)	LR 1.563e-05
0: TRAIN [2][4930/5173]	Time 0.568 (0.610)	Data 1.27e-04 (2.58e-04)	Tok/s 18329 (23094)	Loss/tok 3.1108 (3.3922)	LR 1.563e-05
0: TRAIN [2][4940/5173]	Time 0.626 (0.610)	Data 1.20e-04 (2.58e-04)	Tok/s 27106 (23092)	Loss/tok 3.3377 (3.3922)	LR 1.563e-05
0: TRAIN [2][4950/5173]	Time 0.697 (0.610)	Data 1.29e-04 (2.58e-04)	Tok/s 33889 (23104)	Loss/tok 3.4416 (3.3926)	LR 1.563e-05
0: TRAIN [2][4960/5173]	Time 0.637 (0.610)	Data 1.23e-04 (2.57e-04)	Tok/s 26681 (23104)	Loss/tok 3.3635 (3.3924)	LR 1.563e-05
0: TRAIN [2][4970/5173]	Time 0.698 (0.610)	Data 1.23e-04 (2.57e-04)	Tok/s 33105 (23105)	Loss/tok 3.6404 (3.3925)	LR 1.563e-05
0: TRAIN [2][4980/5173]	Time 0.639 (0.610)	Data 1.22e-04 (2.57e-04)	Tok/s 26000 (23109)	Loss/tok 3.4393 (3.3926)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][4990/5173]	Time 0.629 (0.610)	Data 1.34e-04 (2.57e-04)	Tok/s 26459 (23113)	Loss/tok 3.3871 (3.3927)	LR 1.563e-05
0: TRAIN [2][5000/5173]	Time 0.703 (0.610)	Data 1.17e-04 (2.56e-04)	Tok/s 33366 (23114)	Loss/tok 3.5470 (3.3927)	LR 1.563e-05
0: TRAIN [2][5010/5173]	Time 0.630 (0.610)	Data 1.20e-04 (2.56e-04)	Tok/s 26900 (23106)	Loss/tok 3.3993 (3.3925)	LR 1.563e-05
0: TRAIN [2][5020/5173]	Time 0.633 (0.610)	Data 1.26e-04 (2.56e-04)	Tok/s 26823 (23110)	Loss/tok 3.3874 (3.3924)	LR 1.563e-05
0: TRAIN [2][5030/5173]	Time 0.569 (0.610)	Data 1.25e-04 (2.56e-04)	Tok/s 18416 (23112)	Loss/tok 3.1318 (3.3925)	LR 1.563e-05
0: TRAIN [2][5040/5173]	Time 0.569 (0.610)	Data 1.63e-04 (2.55e-04)	Tok/s 18260 (23117)	Loss/tok 3.1851 (3.3925)	LR 1.563e-05
0: TRAIN [2][5050/5173]	Time 0.638 (0.610)	Data 1.24e-04 (2.55e-04)	Tok/s 26109 (23118)	Loss/tok 3.5059 (3.3926)	LR 1.563e-05
0: TRAIN [2][5060/5173]	Time 0.641 (0.610)	Data 3.18e-04 (2.55e-04)	Tok/s 26678 (23123)	Loss/tok 3.2585 (3.3929)	LR 1.563e-05
0: TRAIN [2][5070/5173]	Time 0.635 (0.610)	Data 1.63e-04 (2.55e-04)	Tok/s 26400 (23130)	Loss/tok 3.2780 (3.3930)	LR 1.563e-05
0: TRAIN [2][5080/5173]	Time 0.691 (0.610)	Data 1.75e-04 (2.55e-04)	Tok/s 33339 (23135)	Loss/tok 3.5976 (3.3932)	LR 1.563e-05
0: TRAIN [2][5090/5173]	Time 0.768 (0.610)	Data 1.72e-04 (2.54e-04)	Tok/s 38830 (23132)	Loss/tok 3.7847 (3.3931)	LR 1.563e-05
0: TRAIN [2][5100/5173]	Time 0.627 (0.610)	Data 1.82e-04 (2.54e-04)	Tok/s 26627 (23139)	Loss/tok 3.3818 (3.3933)	LR 1.563e-05
0: TRAIN [2][5110/5173]	Time 0.635 (0.610)	Data 1.76e-04 (2.54e-04)	Tok/s 26491 (23145)	Loss/tok 3.3485 (3.3935)	LR 1.563e-05
0: TRAIN [2][5120/5173]	Time 0.571 (0.610)	Data 1.65e-04 (2.54e-04)	Tok/s 18424 (23150)	Loss/tok 3.1011 (3.3937)	LR 1.563e-05
0: TRAIN [2][5130/5173]	Time 0.565 (0.610)	Data 1.64e-04 (2.54e-04)	Tok/s 18235 (23148)	Loss/tok 3.1384 (3.3935)	LR 1.563e-05
0: TRAIN [2][5140/5173]	Time 0.699 (0.610)	Data 3.50e-04 (2.54e-04)	Tok/s 33595 (23143)	Loss/tok 3.4859 (3.3933)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][5150/5173]	Time 0.630 (0.610)	Data 1.70e-04 (2.54e-04)	Tok/s 26570 (23153)	Loss/tok 3.4205 (3.3935)	LR 1.563e-05
0: TRAIN [2][5160/5173]	Time 0.574 (0.610)	Data 1.66e-04 (2.54e-04)	Tok/s 18212 (23142)	Loss/tok 3.1989 (3.3932)	LR 1.563e-05
0: TRAIN [2][5170/5173]	Time 0.566 (0.610)	Data 1.75e-04 (2.54e-04)	Tok/s 18271 (23135)	Loss/tok 3.1925 (3.3930)	LR 1.563e-05
:::MLL 1585774830.671 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 525}}
:::MLL 1585774830.671 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [2][0/8]	Time 0.651 (0.651)	Decoder iters 107.0 (107.0)	Tok/s 25178 (25178)
0: Running moses detokenizer
0: BLEU(score=20.612974472654585, counts=[34713, 16136, 8666, 4863], totals=[64729, 61726, 58724, 55727], precisions=[53.628203741754085, 26.14133428377021, 14.757169130168245, 8.72647011323057], bp=1.0, sys_len=64729, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1585774836.803 eval_accuracy: {"value": 20.61, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 536}}
:::MLL 1585774836.803 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 2	Training Loss: 3.3914	Test BLEU: 20.61
0: Performance: Epoch: 2	Training: 69403 Tok/s
0: Finished epoch 2
:::MLL 1585774836.803 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 558}}
:::MLL 1585774836.804 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1585774836.804 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 515}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 3026772632
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][0/5173]	Time 1.122 (1.122)	Data 5.52e-01 (5.52e-01)	Tok/s 9226 (9226)	Loss/tok 3.2184 (3.2184)	LR 1.563e-05
0: TRAIN [3][10/5173]	Time 0.636 (0.656)	Data 1.19e-04 (5.03e-02)	Tok/s 26353 (21626)	Loss/tok 3.3884 (3.3719)	LR 1.563e-05
0: TRAIN [3][20/5173]	Time 0.703 (0.642)	Data 1.19e-04 (2.64e-02)	Tok/s 33268 (23188)	Loss/tok 3.6006 (3.4210)	LR 1.563e-05
0: TRAIN [3][30/5173]	Time 0.648 (0.638)	Data 1.24e-04 (1.79e-02)	Tok/s 26183 (23886)	Loss/tok 3.3300 (3.4182)	LR 1.563e-05
0: TRAIN [3][40/5173]	Time 0.706 (0.633)	Data 1.19e-04 (1.36e-02)	Tok/s 32747 (23963)	Loss/tok 3.5889 (3.4102)	LR 1.563e-05
0: TRAIN [3][50/5173]	Time 0.570 (0.626)	Data 1.18e-04 (1.10e-02)	Tok/s 18214 (23466)	Loss/tok 3.1164 (3.3886)	LR 1.563e-05
0: TRAIN [3][60/5173]	Time 0.639 (0.621)	Data 1.19e-04 (9.18e-03)	Tok/s 26217 (23111)	Loss/tok 3.3144 (3.3788)	LR 1.563e-05
0: TRAIN [3][70/5173]	Time 0.637 (0.613)	Data 1.14e-04 (7.90e-03)	Tok/s 26676 (22445)	Loss/tok 3.3617 (3.3590)	LR 1.563e-05
0: TRAIN [3][80/5173]	Time 0.637 (0.611)	Data 1.18e-04 (6.94e-03)	Tok/s 26224 (22293)	Loss/tok 3.4623 (3.3551)	LR 1.563e-05
0: TRAIN [3][90/5173]	Time 0.566 (0.612)	Data 1.18e-04 (6.19e-03)	Tok/s 18239 (22497)	Loss/tok 3.0955 (3.3610)	LR 1.563e-05
0: TRAIN [3][100/5173]	Time 0.561 (0.611)	Data 1.14e-04 (5.59e-03)	Tok/s 18250 (22451)	Loss/tok 3.1425 (3.3618)	LR 1.563e-05
0: TRAIN [3][110/5173]	Time 0.568 (0.609)	Data 1.18e-04 (5.10e-03)	Tok/s 18160 (22343)	Loss/tok 3.1034 (3.3586)	LR 1.563e-05
0: TRAIN [3][120/5173]	Time 0.640 (0.609)	Data 1.23e-04 (4.69e-03)	Tok/s 26403 (22342)	Loss/tok 3.3771 (3.3566)	LR 1.563e-05
0: TRAIN [3][130/5173]	Time 0.566 (0.607)	Data 1.12e-04 (4.34e-03)	Tok/s 17980 (22195)	Loss/tok 3.0641 (3.3543)	LR 1.563e-05
0: TRAIN [3][140/5173]	Time 0.642 (0.608)	Data 1.18e-04 (4.04e-03)	Tok/s 25963 (22325)	Loss/tok 3.4284 (3.3621)	LR 1.563e-05
0: TRAIN [3][150/5173]	Time 0.640 (0.606)	Data 1.17e-04 (3.78e-03)	Tok/s 26302 (22180)	Loss/tok 3.3634 (3.3572)	LR 1.563e-05
0: TRAIN [3][160/5173]	Time 0.571 (0.606)	Data 1.17e-04 (3.55e-03)	Tok/s 17956 (22175)	Loss/tok 3.2858 (3.3561)	LR 1.563e-05
0: TRAIN [3][170/5173]	Time 0.630 (0.605)	Data 1.15e-04 (3.35e-03)	Tok/s 26813 (22097)	Loss/tok 3.2564 (3.3505)	LR 1.563e-05
0: TRAIN [3][180/5173]	Time 0.505 (0.606)	Data 1.24e-04 (3.17e-03)	Tok/s 10473 (22248)	Loss/tok 2.7459 (3.3601)	LR 1.563e-05
0: TRAIN [3][190/5173]	Time 0.557 (0.608)	Data 1.25e-04 (3.02e-03)	Tok/s 18316 (22491)	Loss/tok 3.1884 (3.3711)	LR 1.563e-05
0: TRAIN [3][200/5173]	Time 0.569 (0.609)	Data 1.41e-04 (2.87e-03)	Tok/s 18070 (22610)	Loss/tok 3.2150 (3.3751)	LR 1.563e-05
0: TRAIN [3][210/5173]	Time 0.625 (0.610)	Data 1.28e-04 (2.74e-03)	Tok/s 26987 (22732)	Loss/tok 3.3211 (3.3759)	LR 1.563e-05
0: TRAIN [3][220/5173]	Time 0.705 (0.610)	Data 1.20e-04 (2.62e-03)	Tok/s 33288 (22710)	Loss/tok 3.4701 (3.3725)	LR 1.563e-05
0: TRAIN [3][230/5173]	Time 0.565 (0.609)	Data 1.23e-04 (2.52e-03)	Tok/s 18008 (22625)	Loss/tok 3.1778 (3.3691)	LR 1.563e-05
0: TRAIN [3][240/5173]	Time 0.562 (0.609)	Data 1.31e-04 (2.42e-03)	Tok/s 18047 (22721)	Loss/tok 3.1656 (3.3715)	LR 1.563e-05
0: TRAIN [3][250/5173]	Time 0.572 (0.610)	Data 1.22e-04 (2.33e-03)	Tok/s 18063 (22800)	Loss/tok 3.1190 (3.3760)	LR 1.563e-05
0: TRAIN [3][260/5173]	Time 0.566 (0.609)	Data 1.22e-04 (2.24e-03)	Tok/s 18266 (22749)	Loss/tok 3.0374 (3.3738)	LR 1.563e-05
0: TRAIN [3][270/5173]	Time 0.703 (0.610)	Data 1.22e-04 (2.16e-03)	Tok/s 33446 (22817)	Loss/tok 3.4757 (3.3757)	LR 1.563e-05
0: TRAIN [3][280/5173]	Time 0.767 (0.611)	Data 1.25e-04 (2.09e-03)	Tok/s 38722 (22942)	Loss/tok 3.8201 (3.3808)	LR 1.563e-05
0: TRAIN [3][290/5173]	Time 0.641 (0.611)	Data 1.21e-04 (2.02e-03)	Tok/s 25965 (22988)	Loss/tok 3.4358 (3.3816)	LR 1.563e-05
0: TRAIN [3][300/5173]	Time 0.566 (0.611)	Data 1.26e-04 (1.96e-03)	Tok/s 17828 (22934)	Loss/tok 3.1383 (3.3804)	LR 1.563e-05
0: TRAIN [3][310/5173]	Time 0.641 (0.611)	Data 1.22e-04 (1.90e-03)	Tok/s 26255 (22973)	Loss/tok 3.3232 (3.3794)	LR 1.563e-05
0: TRAIN [3][320/5173]	Time 0.701 (0.612)	Data 1.35e-04 (1.85e-03)	Tok/s 33011 (23083)	Loss/tok 3.5430 (3.3821)	LR 1.563e-05
0: TRAIN [3][330/5173]	Time 0.570 (0.611)	Data 1.23e-04 (1.79e-03)	Tok/s 18354 (22989)	Loss/tok 3.0639 (3.3791)	LR 1.563e-05
0: TRAIN [3][340/5173]	Time 0.647 (0.611)	Data 1.24e-04 (1.75e-03)	Tok/s 25883 (22995)	Loss/tok 3.3554 (3.3770)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][350/5173]	Time 0.627 (0.612)	Data 1.67e-04 (1.70e-03)	Tok/s 26440 (23156)	Loss/tok 3.3957 (3.3836)	LR 1.563e-05
0: TRAIN [3][360/5173]	Time 0.568 (0.611)	Data 1.20e-04 (1.66e-03)	Tok/s 17764 (23084)	Loss/tok 3.1896 (3.3824)	LR 1.563e-05
0: TRAIN [3][370/5173]	Time 0.626 (0.611)	Data 1.41e-04 (1.61e-03)	Tok/s 27151 (22991)	Loss/tok 3.3156 (3.3805)	LR 1.563e-05
0: TRAIN [3][380/5173]	Time 0.640 (0.611)	Data 1.22e-04 (1.58e-03)	Tok/s 26463 (23035)	Loss/tok 3.3270 (3.3802)	LR 1.563e-05
0: TRAIN [3][390/5173]	Time 0.701 (0.611)	Data 1.23e-04 (1.54e-03)	Tok/s 33812 (23091)	Loss/tok 3.5335 (3.3813)	LR 1.563e-05
0: TRAIN [3][400/5173]	Time 0.696 (0.611)	Data 1.19e-04 (1.50e-03)	Tok/s 33692 (23100)	Loss/tok 3.5458 (3.3828)	LR 1.563e-05
0: TRAIN [3][410/5173]	Time 0.576 (0.612)	Data 1.19e-04 (1.47e-03)	Tok/s 17808 (23181)	Loss/tok 3.1142 (3.3853)	LR 1.563e-05
0: TRAIN [3][420/5173]	Time 0.574 (0.611)	Data 1.24e-04 (1.44e-03)	Tok/s 17979 (23086)	Loss/tok 3.2615 (3.3836)	LR 1.563e-05
0: TRAIN [3][430/5173]	Time 0.500 (0.611)	Data 1.17e-04 (1.41e-03)	Tok/s 10594 (23089)	Loss/tok 2.7751 (3.3864)	LR 1.563e-05
0: TRAIN [3][440/5173]	Time 0.696 (0.611)	Data 1.24e-04 (1.38e-03)	Tok/s 33434 (23145)	Loss/tok 3.5152 (3.3872)	LR 1.563e-05
0: TRAIN [3][450/5173]	Time 0.505 (0.612)	Data 1.17e-04 (1.35e-03)	Tok/s 10327 (23204)	Loss/tok 2.6553 (3.3894)	LR 1.563e-05
0: TRAIN [3][460/5173]	Time 0.635 (0.612)	Data 1.24e-04 (1.32e-03)	Tok/s 26707 (23199)	Loss/tok 3.4862 (3.3877)	LR 1.563e-05
0: TRAIN [3][470/5173]	Time 0.569 (0.612)	Data 1.17e-04 (1.30e-03)	Tok/s 18607 (23169)	Loss/tok 3.1643 (3.3882)	LR 1.563e-05
0: TRAIN [3][480/5173]	Time 0.561 (0.612)	Data 1.33e-04 (1.27e-03)	Tok/s 18821 (23160)	Loss/tok 3.2648 (3.3887)	LR 1.563e-05
0: TRAIN [3][490/5173]	Time 0.565 (0.611)	Data 1.26e-04 (1.25e-03)	Tok/s 18261 (23156)	Loss/tok 3.1468 (3.3891)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][500/5173]	Time 0.564 (0.611)	Data 1.21e-04 (1.23e-03)	Tok/s 18298 (23123)	Loss/tok 3.0580 (3.3889)	LR 1.563e-05
0: TRAIN [3][510/5173]	Time 0.567 (0.611)	Data 1.19e-04 (1.21e-03)	Tok/s 17814 (23079)	Loss/tok 3.2311 (3.3876)	LR 1.563e-05
0: TRAIN [3][520/5173]	Time 0.568 (0.610)	Data 1.24e-04 (1.19e-03)	Tok/s 18052 (23049)	Loss/tok 3.1069 (3.3868)	LR 1.563e-05
0: TRAIN [3][530/5173]	Time 0.703 (0.611)	Data 1.30e-04 (1.17e-03)	Tok/s 32857 (23120)	Loss/tok 3.6505 (3.3884)	LR 1.563e-05
0: TRAIN [3][540/5173]	Time 0.702 (0.612)	Data 1.28e-04 (1.15e-03)	Tok/s 33110 (23213)	Loss/tok 3.5162 (3.3926)	LR 1.563e-05
0: TRAIN [3][550/5173]	Time 0.564 (0.612)	Data 1.26e-04 (1.13e-03)	Tok/s 18146 (23194)	Loss/tok 3.2473 (3.3917)	LR 1.563e-05
0: TRAIN [3][560/5173]	Time 0.566 (0.611)	Data 1.19e-04 (1.11e-03)	Tok/s 18108 (23134)	Loss/tok 3.1476 (3.3902)	LR 1.563e-05
0: TRAIN [3][570/5173]	Time 0.560 (0.611)	Data 1.24e-04 (1.09e-03)	Tok/s 18450 (23113)	Loss/tok 3.2611 (3.3897)	LR 1.563e-05
0: TRAIN [3][580/5173]	Time 0.623 (0.611)	Data 1.28e-04 (1.08e-03)	Tok/s 26625 (23104)	Loss/tok 3.3383 (3.3896)	LR 1.563e-05
0: TRAIN [3][590/5173]	Time 0.561 (0.611)	Data 1.27e-04 (1.06e-03)	Tok/s 18197 (23104)	Loss/tok 3.1168 (3.3889)	LR 1.563e-05
0: TRAIN [3][600/5173]	Time 0.500 (0.610)	Data 1.29e-04 (1.05e-03)	Tok/s 10454 (23088)	Loss/tok 2.7321 (3.3890)	LR 1.563e-05
0: TRAIN [3][610/5173]	Time 0.703 (0.610)	Data 1.29e-04 (1.03e-03)	Tok/s 33128 (23097)	Loss/tok 3.5008 (3.3889)	LR 1.563e-05
0: TRAIN [3][620/5173]	Time 0.635 (0.610)	Data 1.22e-04 (1.02e-03)	Tok/s 26211 (23092)	Loss/tok 3.3665 (3.3882)	LR 1.563e-05
0: TRAIN [3][630/5173]	Time 0.568 (0.610)	Data 1.27e-04 (1.00e-03)	Tok/s 18246 (23069)	Loss/tok 3.1190 (3.3873)	LR 1.563e-05
0: TRAIN [3][640/5173]	Time 0.700 (0.610)	Data 1.19e-04 (9.87e-04)	Tok/s 33528 (23092)	Loss/tok 3.5641 (3.3878)	LR 1.563e-05
0: TRAIN [3][650/5173]	Time 0.641 (0.610)	Data 1.29e-04 (9.74e-04)	Tok/s 26307 (23080)	Loss/tok 3.3509 (3.3872)	LR 1.563e-05
0: TRAIN [3][660/5173]	Time 0.565 (0.610)	Data 1.23e-04 (9.62e-04)	Tok/s 18122 (23048)	Loss/tok 3.1527 (3.3859)	LR 1.563e-05
0: TRAIN [3][670/5173]	Time 0.566 (0.610)	Data 1.17e-04 (9.49e-04)	Tok/s 18448 (23066)	Loss/tok 3.1739 (3.3864)	LR 1.563e-05
0: TRAIN [3][680/5173]	Time 0.623 (0.610)	Data 1.33e-04 (9.37e-04)	Tok/s 26868 (23042)	Loss/tok 3.3921 (3.3857)	LR 1.563e-05
0: TRAIN [3][690/5173]	Time 0.509 (0.609)	Data 1.21e-04 (9.26e-04)	Tok/s 10493 (22952)	Loss/tok 2.7610 (3.3833)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][700/5173]	Time 0.563 (0.610)	Data 1.40e-04 (9.14e-04)	Tok/s 18590 (23046)	Loss/tok 3.1986 (3.3859)	LR 1.563e-05
0: TRAIN [3][710/5173]	Time 0.573 (0.610)	Data 1.24e-04 (9.03e-04)	Tok/s 18166 (23053)	Loss/tok 3.3127 (3.3862)	LR 1.563e-05
0: TRAIN [3][720/5173]	Time 0.501 (0.609)	Data 1.23e-04 (8.92e-04)	Tok/s 10401 (23045)	Loss/tok 2.7021 (3.3857)	LR 1.563e-05
0: TRAIN [3][730/5173]	Time 0.564 (0.610)	Data 1.17e-04 (8.82e-04)	Tok/s 18070 (23060)	Loss/tok 3.1423 (3.3864)	LR 1.563e-05
0: TRAIN [3][740/5173]	Time 0.574 (0.610)	Data 1.22e-04 (8.72e-04)	Tok/s 18253 (23071)	Loss/tok 3.1002 (3.3860)	LR 1.563e-05
0: TRAIN [3][750/5173]	Time 0.507 (0.610)	Data 1.21e-04 (8.62e-04)	Tok/s 10401 (23050)	Loss/tok 2.7019 (3.3853)	LR 1.563e-05
0: TRAIN [3][760/5173]	Time 0.632 (0.610)	Data 1.25e-04 (8.52e-04)	Tok/s 26475 (23135)	Loss/tok 3.4823 (3.3885)	LR 1.563e-05
0: TRAIN [3][770/5173]	Time 0.509 (0.610)	Data 1.24e-04 (8.43e-04)	Tok/s 10202 (23122)	Loss/tok 2.6895 (3.3877)	LR 1.563e-05
0: TRAIN [3][780/5173]	Time 0.562 (0.610)	Data 1.25e-04 (8.34e-04)	Tok/s 18738 (23132)	Loss/tok 3.2115 (3.3872)	LR 1.563e-05
0: TRAIN [3][790/5173]	Time 0.628 (0.610)	Data 1.25e-04 (8.25e-04)	Tok/s 26817 (23083)	Loss/tok 3.3238 (3.3856)	LR 1.563e-05
0: TRAIN [3][800/5173]	Time 0.570 (0.610)	Data 1.22e-04 (8.16e-04)	Tok/s 18507 (23112)	Loss/tok 3.1824 (3.3864)	LR 1.563e-05
0: TRAIN [3][810/5173]	Time 0.568 (0.610)	Data 1.27e-04 (8.08e-04)	Tok/s 18028 (23088)	Loss/tok 3.1295 (3.3858)	LR 1.563e-05
0: TRAIN [3][820/5173]	Time 0.633 (0.610)	Data 1.29e-04 (8.00e-04)	Tok/s 26805 (23105)	Loss/tok 3.3714 (3.3859)	LR 1.563e-05
0: TRAIN [3][830/5173]	Time 0.566 (0.610)	Data 1.29e-04 (7.91e-04)	Tok/s 18419 (23083)	Loss/tok 3.2265 (3.3855)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][840/5173]	Time 0.561 (0.610)	Data 1.24e-04 (7.83e-04)	Tok/s 18457 (23115)	Loss/tok 3.1837 (3.3863)	LR 1.563e-05
0: TRAIN [3][850/5173]	Time 0.695 (0.610)	Data 1.22e-04 (7.76e-04)	Tok/s 33860 (23139)	Loss/tok 3.6286 (3.3870)	LR 1.563e-05
0: TRAIN [3][860/5173]	Time 0.616 (0.610)	Data 1.23e-04 (7.68e-04)	Tok/s 27539 (23141)	Loss/tok 3.3116 (3.3864)	LR 1.563e-05
0: TRAIN [3][870/5173]	Time 0.574 (0.610)	Data 1.21e-04 (7.61e-04)	Tok/s 17857 (23155)	Loss/tok 3.1939 (3.3873)	LR 1.563e-05
0: TRAIN [3][880/5173]	Time 0.575 (0.610)	Data 1.22e-04 (7.54e-04)	Tok/s 17995 (23148)	Loss/tok 3.1885 (3.3872)	LR 1.563e-05
0: TRAIN [3][890/5173]	Time 0.564 (0.610)	Data 1.18e-04 (7.47e-04)	Tok/s 18718 (23153)	Loss/tok 3.1113 (3.3867)	LR 1.563e-05
0: TRAIN [3][900/5173]	Time 0.613 (0.610)	Data 1.22e-04 (7.40e-04)	Tok/s 27330 (23171)	Loss/tok 3.4899 (3.3869)	LR 1.563e-05
0: TRAIN [3][910/5173]	Time 0.632 (0.610)	Data 1.22e-04 (7.33e-04)	Tok/s 27009 (23163)	Loss/tok 3.3744 (3.3860)	LR 1.563e-05
0: TRAIN [3][920/5173]	Time 0.564 (0.610)	Data 1.13e-04 (7.26e-04)	Tok/s 18702 (23136)	Loss/tok 3.1993 (3.3851)	LR 1.563e-05
0: TRAIN [3][930/5173]	Time 0.565 (0.610)	Data 1.22e-04 (7.20e-04)	Tok/s 18495 (23152)	Loss/tok 3.2634 (3.3853)	LR 1.563e-05
0: TRAIN [3][940/5173]	Time 0.703 (0.610)	Data 1.35e-04 (7.14e-04)	Tok/s 33437 (23184)	Loss/tok 3.5953 (3.3861)	LR 1.563e-05
0: TRAIN [3][950/5173]	Time 0.565 (0.610)	Data 1.38e-04 (7.07e-04)	Tok/s 18150 (23203)	Loss/tok 3.1498 (3.3867)	LR 1.563e-05
0: TRAIN [3][960/5173]	Time 0.642 (0.611)	Data 1.31e-04 (7.01e-04)	Tok/s 25923 (23204)	Loss/tok 3.3127 (3.3867)	LR 1.563e-05
0: TRAIN [3][970/5173]	Time 0.627 (0.610)	Data 1.28e-04 (6.96e-04)	Tok/s 26682 (23186)	Loss/tok 3.4324 (3.3862)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][980/5173]	Time 0.641 (0.610)	Data 1.26e-04 (6.90e-04)	Tok/s 26108 (23199)	Loss/tok 3.4082 (3.3864)	LR 1.563e-05
0: TRAIN [3][990/5173]	Time 0.565 (0.611)	Data 1.06e-04 (6.84e-04)	Tok/s 18430 (23230)	Loss/tok 3.3180 (3.3876)	LR 1.563e-05
0: TRAIN [3][1000/5173]	Time 0.563 (0.611)	Data 1.26e-04 (6.79e-04)	Tok/s 18922 (23221)	Loss/tok 3.1249 (3.3871)	LR 1.563e-05
0: TRAIN [3][1010/5173]	Time 0.640 (0.610)	Data 1.11e-04 (6.73e-04)	Tok/s 26046 (23182)	Loss/tok 3.3780 (3.3860)	LR 1.563e-05
0: TRAIN [3][1020/5173]	Time 0.563 (0.610)	Data 1.22e-04 (6.68e-04)	Tok/s 18623 (23180)	Loss/tok 3.1739 (3.3856)	LR 1.563e-05
0: TRAIN [3][1030/5173]	Time 0.637 (0.610)	Data 1.18e-04 (6.62e-04)	Tok/s 25893 (23180)	Loss/tok 3.2602 (3.3848)	LR 1.563e-05
0: TRAIN [3][1040/5173]	Time 0.568 (0.610)	Data 1.19e-04 (6.57e-04)	Tok/s 18641 (23169)	Loss/tok 3.2799 (3.3848)	LR 1.563e-05
0: TRAIN [3][1050/5173]	Time 0.621 (0.610)	Data 1.20e-04 (6.52e-04)	Tok/s 27183 (23189)	Loss/tok 3.4274 (3.3856)	LR 1.563e-05
0: TRAIN [3][1060/5173]	Time 0.640 (0.610)	Data 1.24e-04 (6.47e-04)	Tok/s 25744 (23179)	Loss/tok 3.3511 (3.3850)	LR 1.563e-05
0: TRAIN [3][1070/5173]	Time 0.569 (0.610)	Data 1.19e-04 (6.42e-04)	Tok/s 18035 (23174)	Loss/tok 3.2197 (3.3849)	LR 1.563e-05
0: TRAIN [3][1080/5173]	Time 0.564 (0.610)	Data 1.19e-04 (6.38e-04)	Tok/s 17969 (23194)	Loss/tok 3.1575 (3.3853)	LR 1.563e-05
0: TRAIN [3][1090/5173]	Time 0.572 (0.610)	Data 1.38e-04 (6.33e-04)	Tok/s 18176 (23210)	Loss/tok 3.1323 (3.3862)	LR 1.563e-05
0: TRAIN [3][1100/5173]	Time 0.632 (0.611)	Data 1.26e-04 (6.28e-04)	Tok/s 26609 (23266)	Loss/tok 3.3668 (3.3881)	LR 1.563e-05
0: TRAIN [3][1110/5173]	Time 0.622 (0.611)	Data 1.17e-04 (6.24e-04)	Tok/s 27234 (23271)	Loss/tok 3.2732 (3.3877)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1120/5173]	Time 0.505 (0.611)	Data 1.31e-04 (6.20e-04)	Tok/s 10515 (23265)	Loss/tok 2.7112 (3.3878)	LR 1.563e-05
0: TRAIN [3][1130/5173]	Time 0.699 (0.611)	Data 1.35e-04 (6.15e-04)	Tok/s 33552 (23309)	Loss/tok 3.6428 (3.3893)	LR 1.563e-05
0: TRAIN [3][1140/5173]	Time 0.637 (0.612)	Data 1.25e-04 (6.11e-04)	Tok/s 26012 (23339)	Loss/tok 3.3851 (3.3905)	LR 1.563e-05
0: TRAIN [3][1150/5173]	Time 0.703 (0.611)	Data 1.31e-04 (6.07e-04)	Tok/s 33390 (23329)	Loss/tok 3.5237 (3.3902)	LR 1.563e-05
0: TRAIN [3][1160/5173]	Time 0.558 (0.611)	Data 1.24e-04 (6.03e-04)	Tok/s 18341 (23326)	Loss/tok 3.1783 (3.3897)	LR 1.563e-05
0: TRAIN [3][1170/5173]	Time 0.621 (0.612)	Data 1.28e-04 (5.99e-04)	Tok/s 27167 (23367)	Loss/tok 3.3434 (3.3910)	LR 1.563e-05
0: TRAIN [3][1180/5173]	Time 0.564 (0.612)	Data 1.21e-04 (5.95e-04)	Tok/s 18150 (23378)	Loss/tok 3.1510 (3.3918)	LR 1.563e-05
0: TRAIN [3][1190/5173]	Time 0.707 (0.612)	Data 1.22e-04 (5.91e-04)	Tok/s 32909 (23376)	Loss/tok 3.5894 (3.3917)	LR 1.563e-05
0: TRAIN [3][1200/5173]	Time 0.563 (0.612)	Data 1.19e-04 (5.87e-04)	Tok/s 18519 (23373)	Loss/tok 3.0908 (3.3915)	LR 1.563e-05
0: TRAIN [3][1210/5173]	Time 0.630 (0.612)	Data 1.20e-04 (5.84e-04)	Tok/s 26904 (23375)	Loss/tok 3.3700 (3.3918)	LR 1.563e-05
0: TRAIN [3][1220/5173]	Time 0.633 (0.612)	Data 1.39e-04 (5.80e-04)	Tok/s 26579 (23414)	Loss/tok 3.3761 (3.3933)	LR 1.563e-05
0: TRAIN [3][1230/5173]	Time 0.624 (0.612)	Data 1.19e-04 (5.76e-04)	Tok/s 26638 (23423)	Loss/tok 3.4404 (3.3934)	LR 1.563e-05
0: TRAIN [3][1240/5173]	Time 0.692 (0.612)	Data 1.24e-04 (5.73e-04)	Tok/s 33509 (23427)	Loss/tok 3.7631 (3.3934)	LR 1.563e-05
0: TRAIN [3][1250/5173]	Time 0.624 (0.612)	Data 1.23e-04 (5.69e-04)	Tok/s 27293 (23424)	Loss/tok 3.4061 (3.3927)	LR 1.563e-05
0: TRAIN [3][1260/5173]	Time 0.560 (0.612)	Data 1.27e-04 (5.66e-04)	Tok/s 18407 (23413)	Loss/tok 3.2830 (3.3926)	LR 1.563e-05
0: TRAIN [3][1270/5173]	Time 0.561 (0.612)	Data 1.16e-04 (5.62e-04)	Tok/s 18150 (23367)	Loss/tok 3.2254 (3.3917)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1280/5173]	Time 0.632 (0.612)	Data 1.21e-04 (5.59e-04)	Tok/s 26550 (23356)	Loss/tok 3.3501 (3.3916)	LR 1.563e-05
0: TRAIN [3][1290/5173]	Time 0.777 (0.612)	Data 1.24e-04 (5.56e-04)	Tok/s 38273 (23365)	Loss/tok 3.8189 (3.3919)	LR 1.563e-05
0: TRAIN [3][1300/5173]	Time 0.572 (0.611)	Data 1.23e-04 (5.52e-04)	Tok/s 18300 (23321)	Loss/tok 3.1865 (3.3910)	LR 1.563e-05
0: TRAIN [3][1310/5173]	Time 0.641 (0.611)	Data 1.28e-04 (5.49e-04)	Tok/s 26084 (23333)	Loss/tok 3.3194 (3.3916)	LR 1.563e-05
0: TRAIN [3][1320/5173]	Time 0.644 (0.611)	Data 1.23e-04 (5.46e-04)	Tok/s 26425 (23340)	Loss/tok 3.4373 (3.3920)	LR 1.563e-05
0: TRAIN [3][1330/5173]	Time 0.568 (0.611)	Data 1.22e-04 (5.43e-04)	Tok/s 18104 (23340)	Loss/tok 3.1784 (3.3921)	LR 1.563e-05
0: TRAIN [3][1340/5173]	Time 0.705 (0.612)	Data 1.21e-04 (5.40e-04)	Tok/s 33316 (23365)	Loss/tok 3.6631 (3.3927)	LR 1.563e-05
0: TRAIN [3][1350/5173]	Time 0.642 (0.612)	Data 1.21e-04 (5.37e-04)	Tok/s 26466 (23363)	Loss/tok 3.3758 (3.3924)	LR 1.563e-05
0: TRAIN [3][1360/5173]	Time 0.644 (0.612)	Data 1.18e-04 (5.34e-04)	Tok/s 25776 (23360)	Loss/tok 3.4278 (3.3920)	LR 1.563e-05
0: TRAIN [3][1370/5173]	Time 0.574 (0.612)	Data 1.23e-04 (5.31e-04)	Tok/s 17659 (23352)	Loss/tok 3.2402 (3.3917)	LR 1.563e-05
0: TRAIN [3][1380/5173]	Time 0.700 (0.611)	Data 1.24e-04 (5.28e-04)	Tok/s 33322 (23339)	Loss/tok 3.6265 (3.3909)	LR 1.563e-05
0: TRAIN [3][1390/5173]	Time 0.640 (0.611)	Data 1.23e-04 (5.25e-04)	Tok/s 26410 (23334)	Loss/tok 3.3376 (3.3909)	LR 1.563e-05
0: TRAIN [3][1400/5173]	Time 0.570 (0.612)	Data 1.34e-04 (5.22e-04)	Tok/s 18127 (23353)	Loss/tok 3.1734 (3.3911)	LR 1.563e-05
0: TRAIN [3][1410/5173]	Time 0.625 (0.612)	Data 1.24e-04 (5.20e-04)	Tok/s 26971 (23351)	Loss/tok 3.4212 (3.3910)	LR 1.563e-05
0: TRAIN [3][1420/5173]	Time 0.564 (0.611)	Data 1.16e-04 (5.17e-04)	Tok/s 18143 (23350)	Loss/tok 3.1697 (3.3906)	LR 1.563e-05
0: TRAIN [3][1430/5173]	Time 0.769 (0.612)	Data 1.23e-04 (5.14e-04)	Tok/s 39143 (23372)	Loss/tok 3.7799 (3.3915)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1440/5173]	Time 0.639 (0.612)	Data 1.44e-04 (5.11e-04)	Tok/s 26414 (23388)	Loss/tok 3.3976 (3.3922)	LR 1.563e-05
0: TRAIN [3][1450/5173]	Time 0.565 (0.612)	Data 1.23e-04 (5.09e-04)	Tok/s 18428 (23364)	Loss/tok 3.1795 (3.3917)	LR 1.563e-05
0: TRAIN [3][1460/5173]	Time 0.564 (0.611)	Data 1.14e-04 (5.06e-04)	Tok/s 18385 (23325)	Loss/tok 3.2002 (3.3906)	LR 1.563e-05
0: TRAIN [3][1470/5173]	Time 0.703 (0.611)	Data 1.23e-04 (5.04e-04)	Tok/s 33494 (23337)	Loss/tok 3.5244 (3.3910)	LR 1.563e-05
0: TRAIN [3][1480/5173]	Time 0.627 (0.611)	Data 1.22e-04 (5.01e-04)	Tok/s 26554 (23330)	Loss/tok 3.3940 (3.3908)	LR 1.563e-05
0: TRAIN [3][1490/5173]	Time 0.565 (0.611)	Data 1.20e-04 (4.99e-04)	Tok/s 18475 (23333)	Loss/tok 3.0965 (3.3905)	LR 1.563e-05
0: TRAIN [3][1500/5173]	Time 0.565 (0.611)	Data 1.24e-04 (4.96e-04)	Tok/s 18337 (23310)	Loss/tok 3.1289 (3.3898)	LR 1.563e-05
0: TRAIN [3][1510/5173]	Time 0.502 (0.611)	Data 1.32e-04 (4.94e-04)	Tok/s 10830 (23301)	Loss/tok 2.7406 (3.3898)	LR 1.563e-05
0: TRAIN [3][1520/5173]	Time 0.563 (0.611)	Data 1.12e-04 (4.91e-04)	Tok/s 18166 (23278)	Loss/tok 3.2068 (3.3890)	LR 1.563e-05
0: TRAIN [3][1530/5173]	Time 0.566 (0.611)	Data 1.33e-04 (4.89e-04)	Tok/s 18567 (23284)	Loss/tok 3.0990 (3.3892)	LR 1.563e-05
0: TRAIN [3][1540/5173]	Time 0.638 (0.611)	Data 1.32e-04 (4.87e-04)	Tok/s 26083 (23293)	Loss/tok 3.3326 (3.3891)	LR 1.563e-05
0: TRAIN [3][1550/5173]	Time 0.629 (0.611)	Data 1.23e-04 (4.84e-04)	Tok/s 26674 (23281)	Loss/tok 3.3758 (3.3883)	LR 1.563e-05
0: TRAIN [3][1560/5173]	Time 0.646 (0.611)	Data 1.21e-04 (4.82e-04)	Tok/s 25630 (23264)	Loss/tok 3.4735 (3.3877)	LR 1.563e-05
0: TRAIN [3][1570/5173]	Time 0.632 (0.611)	Data 1.24e-04 (4.80e-04)	Tok/s 26231 (23267)	Loss/tok 3.3519 (3.3876)	LR 1.563e-05
0: TRAIN [3][1580/5173]	Time 0.635 (0.611)	Data 1.20e-04 (4.78e-04)	Tok/s 26321 (23264)	Loss/tok 3.3995 (3.3873)	LR 1.563e-05
0: TRAIN [3][1590/5173]	Time 0.561 (0.611)	Data 1.26e-04 (4.75e-04)	Tok/s 18395 (23264)	Loss/tok 3.0956 (3.3875)	LR 1.563e-05
0: TRAIN [3][1600/5173]	Time 0.638 (0.611)	Data 1.29e-04 (4.73e-04)	Tok/s 25917 (23274)	Loss/tok 3.4497 (3.3878)	LR 1.563e-05
0: TRAIN [3][1610/5173]	Time 0.500 (0.611)	Data 1.18e-04 (4.71e-04)	Tok/s 10662 (23273)	Loss/tok 2.9008 (3.3877)	LR 1.563e-05
0: TRAIN [3][1620/5173]	Time 0.635 (0.611)	Data 1.22e-04 (4.69e-04)	Tok/s 26205 (23253)	Loss/tok 3.2717 (3.3869)	LR 1.563e-05
0: TRAIN [3][1630/5173]	Time 0.565 (0.610)	Data 1.24e-04 (4.67e-04)	Tok/s 18619 (23242)	Loss/tok 2.9821 (3.3864)	LR 1.563e-05
0: TRAIN [3][1640/5173]	Time 0.559 (0.610)	Data 1.22e-04 (4.65e-04)	Tok/s 18517 (23231)	Loss/tok 3.1685 (3.3861)	LR 1.563e-05
0: TRAIN [3][1650/5173]	Time 0.563 (0.610)	Data 1.15e-04 (4.63e-04)	Tok/s 18674 (23212)	Loss/tok 3.1195 (3.3864)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1660/5173]	Time 0.568 (0.610)	Data 1.24e-04 (4.61e-04)	Tok/s 18294 (23214)	Loss/tok 3.0177 (3.3863)	LR 1.563e-05
0: TRAIN [3][1670/5173]	Time 0.640 (0.610)	Data 1.34e-04 (4.59e-04)	Tok/s 26328 (23215)	Loss/tok 3.4224 (3.3866)	LR 1.563e-05
0: TRAIN [3][1680/5173]	Time 0.632 (0.610)	Data 1.21e-04 (4.57e-04)	Tok/s 27076 (23204)	Loss/tok 3.4649 (3.3861)	LR 1.563e-05
0: TRAIN [3][1690/5173]	Time 0.628 (0.610)	Data 1.19e-04 (4.55e-04)	Tok/s 26680 (23203)	Loss/tok 3.3470 (3.3861)	LR 1.563e-05
0: TRAIN [3][1700/5173]	Time 0.566 (0.610)	Data 1.16e-04 (4.53e-04)	Tok/s 18104 (23205)	Loss/tok 3.2639 (3.3861)	LR 1.563e-05
0: TRAIN [3][1710/5173]	Time 0.636 (0.610)	Data 1.18e-04 (4.51e-04)	Tok/s 25938 (23203)	Loss/tok 3.4351 (3.3861)	LR 1.563e-05
0: TRAIN [3][1720/5173]	Time 0.565 (0.610)	Data 1.24e-04 (4.49e-04)	Tok/s 17990 (23217)	Loss/tok 3.0071 (3.3861)	LR 1.563e-05
0: TRAIN [3][1730/5173]	Time 0.644 (0.610)	Data 1.27e-04 (4.47e-04)	Tok/s 26484 (23208)	Loss/tok 3.3005 (3.3856)	LR 1.563e-05
0: TRAIN [3][1740/5173]	Time 0.565 (0.610)	Data 3.05e-04 (4.45e-04)	Tok/s 18367 (23204)	Loss/tok 3.2111 (3.3854)	LR 1.563e-05
0: TRAIN [3][1750/5173]	Time 0.774 (0.610)	Data 1.23e-04 (4.43e-04)	Tok/s 38072 (23196)	Loss/tok 3.9349 (3.3856)	LR 1.563e-05
0: TRAIN [3][1760/5173]	Time 0.638 (0.610)	Data 1.20e-04 (4.42e-04)	Tok/s 26296 (23196)	Loss/tok 3.4303 (3.3852)	LR 1.563e-05
0: TRAIN [3][1770/5173]	Time 0.630 (0.610)	Data 1.18e-04 (4.40e-04)	Tok/s 26895 (23210)	Loss/tok 3.4201 (3.3858)	LR 1.563e-05
0: TRAIN [3][1780/5173]	Time 0.568 (0.610)	Data 1.18e-04 (4.38e-04)	Tok/s 18506 (23237)	Loss/tok 3.1140 (3.3867)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1790/5173]	Time 0.567 (0.610)	Data 1.31e-04 (4.37e-04)	Tok/s 18079 (23233)	Loss/tok 3.1521 (3.3868)	LR 1.563e-05
0: TRAIN [3][1800/5173]	Time 0.705 (0.610)	Data 1.22e-04 (4.35e-04)	Tok/s 33044 (23237)	Loss/tok 3.5658 (3.3868)	LR 1.563e-05
0: TRAIN [3][1810/5173]	Time 0.561 (0.610)	Data 1.23e-04 (4.33e-04)	Tok/s 18645 (23219)	Loss/tok 3.1336 (3.3861)	LR 1.563e-05
0: TRAIN [3][1820/5173]	Time 0.621 (0.610)	Data 1.31e-04 (4.32e-04)	Tok/s 27057 (23221)	Loss/tok 3.4714 (3.3863)	LR 1.563e-05
0: TRAIN [3][1830/5173]	Time 0.500 (0.610)	Data 1.31e-04 (4.30e-04)	Tok/s 10389 (23218)	Loss/tok 2.7007 (3.3863)	LR 1.563e-05
0: TRAIN [3][1840/5173]	Time 0.503 (0.610)	Data 1.19e-04 (4.28e-04)	Tok/s 10417 (23212)	Loss/tok 2.7332 (3.3863)	LR 1.563e-05
0: TRAIN [3][1850/5173]	Time 0.563 (0.610)	Data 1.22e-04 (4.27e-04)	Tok/s 18475 (23198)	Loss/tok 3.2369 (3.3857)	LR 1.563e-05
0: TRAIN [3][1860/5173]	Time 0.569 (0.610)	Data 1.23e-04 (4.25e-04)	Tok/s 17666 (23177)	Loss/tok 3.0982 (3.3851)	LR 1.563e-05
0: TRAIN [3][1870/5173]	Time 0.625 (0.610)	Data 1.25e-04 (4.23e-04)	Tok/s 26729 (23184)	Loss/tok 3.4701 (3.3851)	LR 1.563e-05
0: TRAIN [3][1880/5173]	Time 0.630 (0.610)	Data 1.28e-04 (4.22e-04)	Tok/s 26669 (23199)	Loss/tok 3.4513 (3.3855)	LR 1.563e-05
0: TRAIN [3][1890/5173]	Time 0.566 (0.610)	Data 1.55e-04 (4.20e-04)	Tok/s 18092 (23216)	Loss/tok 3.2288 (3.3863)	LR 1.563e-05
0: TRAIN [3][1900/5173]	Time 0.569 (0.610)	Data 1.18e-04 (4.19e-04)	Tok/s 18469 (23206)	Loss/tok 3.1420 (3.3860)	LR 1.563e-05
0: TRAIN [3][1910/5173]	Time 0.628 (0.610)	Data 1.22e-04 (4.17e-04)	Tok/s 26815 (23210)	Loss/tok 3.3648 (3.3858)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1920/5173]	Time 0.564 (0.610)	Data 1.22e-04 (4.16e-04)	Tok/s 18270 (23219)	Loss/tok 3.2768 (3.3862)	LR 1.563e-05
0: TRAIN [3][1930/5173]	Time 0.570 (0.610)	Data 1.19e-04 (4.14e-04)	Tok/s 18382 (23248)	Loss/tok 3.1296 (3.3870)	LR 1.563e-05
0: TRAIN [3][1940/5173]	Time 0.567 (0.610)	Data 1.18e-04 (4.13e-04)	Tok/s 18416 (23254)	Loss/tok 3.1544 (3.3871)	LR 1.563e-05
0: TRAIN [3][1950/5173]	Time 0.624 (0.610)	Data 1.23e-04 (4.11e-04)	Tok/s 26622 (23249)	Loss/tok 3.4547 (3.3867)	LR 1.563e-05
0: TRAIN [3][1960/5173]	Time 0.639 (0.610)	Data 1.16e-04 (4.10e-04)	Tok/s 26453 (23240)	Loss/tok 3.3281 (3.3864)	LR 1.563e-05
0: TRAIN [3][1970/5173]	Time 0.566 (0.610)	Data 1.30e-04 (4.08e-04)	Tok/s 17593 (23230)	Loss/tok 3.1770 (3.3859)	LR 1.563e-05
0: TRAIN [3][1980/5173]	Time 0.562 (0.610)	Data 1.34e-04 (4.07e-04)	Tok/s 18862 (23254)	Loss/tok 3.0838 (3.3869)	LR 1.563e-05
0: TRAIN [3][1990/5173]	Time 0.573 (0.611)	Data 1.32e-04 (4.06e-04)	Tok/s 17653 (23265)	Loss/tok 3.3080 (3.3872)	LR 1.563e-05
0: TRAIN [3][2000/5173]	Time 0.561 (0.611)	Data 1.23e-04 (4.04e-04)	Tok/s 18332 (23264)	Loss/tok 3.0253 (3.3868)	LR 1.563e-05
0: TRAIN [3][2010/5173]	Time 0.558 (0.611)	Data 1.26e-04 (4.03e-04)	Tok/s 18650 (23269)	Loss/tok 3.2324 (3.3871)	LR 1.563e-05
0: TRAIN [3][2020/5173]	Time 0.645 (0.611)	Data 1.18e-04 (4.01e-04)	Tok/s 26177 (23282)	Loss/tok 3.4102 (3.3872)	LR 1.563e-05
0: TRAIN [3][2030/5173]	Time 0.559 (0.611)	Data 1.17e-04 (4.00e-04)	Tok/s 19136 (23276)	Loss/tok 3.1572 (3.3874)	LR 1.563e-05
0: TRAIN [3][2040/5173]	Time 0.704 (0.611)	Data 1.17e-04 (3.99e-04)	Tok/s 33484 (23279)	Loss/tok 3.4569 (3.3872)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][2050/5173]	Time 0.565 (0.611)	Data 1.36e-04 (3.97e-04)	Tok/s 17973 (23276)	Loss/tok 3.3292 (3.3873)	LR 1.563e-05
0: TRAIN [3][2060/5173]	Time 0.704 (0.611)	Data 1.15e-04 (3.96e-04)	Tok/s 33148 (23286)	Loss/tok 3.5966 (3.3874)	LR 1.563e-05
0: TRAIN [3][2070/5173]	Time 0.496 (0.611)	Data 1.43e-04 (3.95e-04)	Tok/s 10442 (23296)	Loss/tok 2.7495 (3.3881)	LR 1.563e-05
0: TRAIN [3][2080/5173]	Time 0.630 (0.611)	Data 1.21e-04 (3.94e-04)	Tok/s 26665 (23289)	Loss/tok 3.3885 (3.3882)	LR 1.563e-05
0: TRAIN [3][2090/5173]	Time 0.629 (0.611)	Data 1.19e-04 (3.92e-04)	Tok/s 26854 (23288)	Loss/tok 3.2515 (3.3880)	LR 1.563e-05
0: TRAIN [3][2100/5173]	Time 0.637 (0.611)	Data 1.23e-04 (3.91e-04)	Tok/s 26521 (23284)	Loss/tok 3.3042 (3.3877)	LR 1.563e-05
0: TRAIN [3][2110/5173]	Time 0.637 (0.611)	Data 1.28e-04 (3.90e-04)	Tok/s 26808 (23279)	Loss/tok 3.3557 (3.3872)	LR 1.563e-05
0: TRAIN [3][2120/5173]	Time 0.645 (0.611)	Data 1.43e-04 (3.89e-04)	Tok/s 26461 (23314)	Loss/tok 3.3673 (3.3886)	LR 1.563e-05
0: TRAIN [3][2130/5173]	Time 0.646 (0.611)	Data 1.31e-04 (3.88e-04)	Tok/s 26043 (23316)	Loss/tok 3.4816 (3.3886)	LR 1.563e-05
0: TRAIN [3][2140/5173]	Time 0.570 (0.611)	Data 3.07e-04 (3.86e-04)	Tok/s 18271 (23300)	Loss/tok 3.1525 (3.3882)	LR 1.563e-05
0: TRAIN [3][2150/5173]	Time 0.696 (0.611)	Data 1.29e-04 (3.85e-04)	Tok/s 33322 (23297)	Loss/tok 3.6241 (3.3881)	LR 1.563e-05
0: TRAIN [3][2160/5173]	Time 0.616 (0.611)	Data 3.19e-04 (3.84e-04)	Tok/s 27767 (23296)	Loss/tok 3.4128 (3.3883)	LR 1.563e-05
0: TRAIN [3][2170/5173]	Time 0.694 (0.611)	Data 1.45e-04 (3.83e-04)	Tok/s 33468 (23297)	Loss/tok 3.5218 (3.3886)	LR 1.563e-05
0: TRAIN [3][2180/5173]	Time 0.768 (0.611)	Data 1.47e-04 (3.82e-04)	Tok/s 37877 (23306)	Loss/tok 3.8492 (3.3894)	LR 1.563e-05
0: TRAIN [3][2190/5173]	Time 0.705 (0.611)	Data 1.25e-04 (3.81e-04)	Tok/s 32942 (23308)	Loss/tok 3.5990 (3.3895)	LR 1.563e-05
0: TRAIN [3][2200/5173]	Time 0.639 (0.611)	Data 1.22e-04 (3.80e-04)	Tok/s 26209 (23299)	Loss/tok 3.4043 (3.3893)	LR 1.563e-05
0: TRAIN [3][2210/5173]	Time 0.701 (0.611)	Data 1.16e-04 (3.79e-04)	Tok/s 33576 (23292)	Loss/tok 3.5376 (3.3895)	LR 1.563e-05
0: TRAIN [3][2220/5173]	Time 0.561 (0.611)	Data 1.24e-04 (3.78e-04)	Tok/s 18658 (23280)	Loss/tok 3.1452 (3.3891)	LR 1.563e-05
0: TRAIN [3][2230/5173]	Time 0.693 (0.611)	Data 2.88e-04 (3.77e-04)	Tok/s 33423 (23275)	Loss/tok 3.5453 (3.3890)	LR 1.563e-05
0: TRAIN [3][2240/5173]	Time 0.499 (0.611)	Data 1.28e-04 (3.76e-04)	Tok/s 10232 (23266)	Loss/tok 2.6256 (3.3887)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][2250/5173]	Time 0.770 (0.611)	Data 1.29e-04 (3.75e-04)	Tok/s 38331 (23281)	Loss/tok 3.7066 (3.3891)	LR 1.563e-05
0: TRAIN [3][2260/5173]	Time 0.706 (0.611)	Data 1.34e-04 (3.74e-04)	Tok/s 33388 (23298)	Loss/tok 3.6396 (3.3897)	LR 1.563e-05
0: TRAIN [3][2270/5173]	Time 0.628 (0.611)	Data 1.24e-04 (3.73e-04)	Tok/s 26542 (23312)	Loss/tok 3.2472 (3.3902)	LR 1.563e-05
0: TRAIN [3][2280/5173]	Time 0.565 (0.611)	Data 1.28e-04 (3.72e-04)	Tok/s 18463 (23311)	Loss/tok 3.2409 (3.3901)	LR 1.563e-05
0: TRAIN [3][2290/5173]	Time 0.701 (0.611)	Data 1.28e-04 (3.70e-04)	Tok/s 33068 (23311)	Loss/tok 3.6817 (3.3901)	LR 1.563e-05
0: TRAIN [3][2300/5173]	Time 0.563 (0.611)	Data 1.38e-04 (3.69e-04)	Tok/s 18524 (23325)	Loss/tok 3.2867 (3.3906)	LR 1.563e-05
0: TRAIN [3][2310/5173]	Time 0.573 (0.611)	Data 1.28e-04 (3.68e-04)	Tok/s 17821 (23328)	Loss/tok 3.1372 (3.3908)	LR 1.563e-05
0: TRAIN [3][2320/5173]	Time 0.704 (0.611)	Data 1.23e-04 (3.68e-04)	Tok/s 33158 (23332)	Loss/tok 3.5185 (3.3910)	LR 1.563e-05
0: TRAIN [3][2330/5173]	Time 0.638 (0.611)	Data 1.39e-04 (3.67e-04)	Tok/s 26398 (23340)	Loss/tok 3.3850 (3.3911)	LR 1.563e-05
0: TRAIN [3][2340/5173]	Time 0.571 (0.611)	Data 1.29e-04 (3.66e-04)	Tok/s 17642 (23328)	Loss/tok 3.2521 (3.3906)	LR 1.563e-05
0: TRAIN [3][2350/5173]	Time 0.503 (0.611)	Data 1.25e-04 (3.65e-04)	Tok/s 10539 (23333)	Loss/tok 2.6770 (3.3908)	LR 1.563e-05
0: TRAIN [3][2360/5173]	Time 0.564 (0.611)	Data 1.38e-04 (3.64e-04)	Tok/s 18112 (23340)	Loss/tok 3.2194 (3.3912)	LR 1.563e-05
0: TRAIN [3][2370/5173]	Time 0.569 (0.611)	Data 1.28e-04 (3.63e-04)	Tok/s 18114 (23332)	Loss/tok 3.0722 (3.3908)	LR 1.563e-05
0: TRAIN [3][2380/5173]	Time 0.643 (0.611)	Data 1.25e-04 (3.62e-04)	Tok/s 26464 (23330)	Loss/tok 3.3538 (3.3908)	LR 1.563e-05
0: TRAIN [3][2390/5173]	Time 0.634 (0.611)	Data 1.81e-04 (3.61e-04)	Tok/s 26389 (23328)	Loss/tok 3.4822 (3.3907)	LR 1.563e-05
0: TRAIN [3][2400/5173]	Time 0.566 (0.611)	Data 1.35e-04 (3.61e-04)	Tok/s 18144 (23332)	Loss/tok 3.1128 (3.3908)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][2410/5173]	Time 0.564 (0.611)	Data 1.29e-04 (3.60e-04)	Tok/s 18371 (23343)	Loss/tok 3.1220 (3.3910)	LR 1.563e-05
0: TRAIN [3][2420/5173]	Time 0.506 (0.611)	Data 1.40e-04 (3.59e-04)	Tok/s 10449 (23325)	Loss/tok 2.7470 (3.3905)	LR 1.563e-05
0: TRAIN [3][2430/5173]	Time 0.625 (0.611)	Data 1.26e-04 (3.58e-04)	Tok/s 26638 (23329)	Loss/tok 3.3707 (3.3908)	LR 1.563e-05
0: TRAIN [3][2440/5173]	Time 0.502 (0.611)	Data 1.32e-04 (3.57e-04)	Tok/s 10532 (23328)	Loss/tok 2.6020 (3.3909)	LR 1.563e-05
0: TRAIN [3][2450/5173]	Time 0.563 (0.611)	Data 1.22e-04 (3.56e-04)	Tok/s 18277 (23310)	Loss/tok 3.0871 (3.3904)	LR 1.563e-05
0: TRAIN [3][2460/5173]	Time 0.569 (0.611)	Data 1.32e-04 (3.55e-04)	Tok/s 18519 (23313)	Loss/tok 3.1654 (3.3907)	LR 1.563e-05
0: TRAIN [3][2470/5173]	Time 0.637 (0.611)	Data 1.29e-04 (3.55e-04)	Tok/s 26126 (23328)	Loss/tok 3.3950 (3.3912)	LR 1.563e-05
0: TRAIN [3][2480/5173]	Time 0.703 (0.611)	Data 1.28e-04 (3.54e-04)	Tok/s 32936 (23344)	Loss/tok 3.5896 (3.3918)	LR 1.563e-05
0: TRAIN [3][2490/5173]	Time 0.632 (0.611)	Data 1.35e-04 (3.53e-04)	Tok/s 26519 (23354)	Loss/tok 3.3094 (3.3920)	LR 1.563e-05
0: TRAIN [3][2500/5173]	Time 0.565 (0.611)	Data 1.31e-04 (3.52e-04)	Tok/s 18331 (23356)	Loss/tok 3.0837 (3.3920)	LR 1.563e-05
0: TRAIN [3][2510/5173]	Time 0.567 (0.611)	Data 1.26e-04 (3.52e-04)	Tok/s 18276 (23358)	Loss/tok 3.0518 (3.3921)	LR 1.563e-05
0: TRAIN [3][2520/5173]	Time 0.501 (0.611)	Data 1.31e-04 (3.51e-04)	Tok/s 10679 (23354)	Loss/tok 2.6977 (3.3918)	LR 1.563e-05
0: TRAIN [3][2530/5173]	Time 0.689 (0.611)	Data 1.28e-04 (3.50e-04)	Tok/s 33746 (23363)	Loss/tok 3.4478 (3.3919)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][2540/5173]	Time 0.568 (0.611)	Data 1.30e-04 (3.49e-04)	Tok/s 18299 (23354)	Loss/tok 3.1113 (3.3916)	LR 1.563e-05
0: TRAIN [3][2550/5173]	Time 0.641 (0.611)	Data 1.32e-04 (3.48e-04)	Tok/s 26130 (23344)	Loss/tok 3.4820 (3.3913)	LR 1.563e-05
0: TRAIN [3][2560/5173]	Time 0.570 (0.611)	Data 1.28e-04 (3.47e-04)	Tok/s 18147 (23342)	Loss/tok 3.1046 (3.3912)	LR 1.563e-05
0: TRAIN [3][2570/5173]	Time 0.643 (0.611)	Data 1.26e-04 (3.47e-04)	Tok/s 26284 (23356)	Loss/tok 3.3007 (3.3916)	LR 1.563e-05
0: TRAIN [3][2580/5173]	Time 0.629 (0.612)	Data 1.35e-04 (3.46e-04)	Tok/s 26947 (23372)	Loss/tok 3.4451 (3.3921)	LR 1.563e-05
0: TRAIN [3][2590/5173]	Time 0.562 (0.611)	Data 1.25e-04 (3.45e-04)	Tok/s 18517 (23367)	Loss/tok 3.1539 (3.3919)	LR 1.563e-05
0: TRAIN [3][2600/5173]	Time 0.702 (0.611)	Data 1.29e-04 (3.44e-04)	Tok/s 33196 (23362)	Loss/tok 3.6093 (3.3917)	LR 1.563e-05
0: TRAIN [3][2610/5173]	Time 0.639 (0.611)	Data 1.40e-04 (3.44e-04)	Tok/s 26370 (23352)	Loss/tok 3.3363 (3.3912)	LR 1.563e-05
0: TRAIN [3][2620/5173]	Time 0.620 (0.611)	Data 1.47e-04 (3.43e-04)	Tok/s 27385 (23365)	Loss/tok 3.3332 (3.3915)	LR 1.563e-05
0: TRAIN [3][2630/5173]	Time 0.568 (0.611)	Data 1.23e-04 (3.42e-04)	Tok/s 18291 (23358)	Loss/tok 3.2471 (3.3913)	LR 1.563e-05
0: TRAIN [3][2640/5173]	Time 0.566 (0.611)	Data 1.21e-04 (3.42e-04)	Tok/s 18079 (23356)	Loss/tok 3.1631 (3.3911)	LR 1.563e-05
0: TRAIN [3][2650/5173]	Time 0.627 (0.611)	Data 1.29e-04 (3.41e-04)	Tok/s 26698 (23359)	Loss/tok 3.5119 (3.3911)	LR 1.563e-05
0: TRAIN [3][2660/5173]	Time 0.561 (0.611)	Data 3.12e-04 (3.40e-04)	Tok/s 18178 (23349)	Loss/tok 3.1623 (3.3908)	LR 1.563e-05
0: TRAIN [3][2670/5173]	Time 0.640 (0.611)	Data 1.27e-04 (3.39e-04)	Tok/s 25876 (23355)	Loss/tok 3.4361 (3.3910)	LR 1.563e-05
0: TRAIN [3][2680/5173]	Time 0.565 (0.611)	Data 2.99e-04 (3.39e-04)	Tok/s 18181 (23359)	Loss/tok 3.1309 (3.3911)	LR 1.563e-05
0: TRAIN [3][2690/5173]	Time 0.564 (0.611)	Data 1.25e-04 (3.38e-04)	Tok/s 17921 (23359)	Loss/tok 3.1739 (3.3911)	LR 1.563e-05
0: TRAIN [3][2700/5173]	Time 0.567 (0.611)	Data 1.25e-04 (3.37e-04)	Tok/s 18115 (23357)	Loss/tok 3.1652 (3.3911)	LR 1.563e-05
0: TRAIN [3][2710/5173]	Time 0.571 (0.611)	Data 1.31e-04 (3.36e-04)	Tok/s 18002 (23349)	Loss/tok 3.1945 (3.3910)	LR 1.563e-05
0: TRAIN [3][2720/5173]	Time 0.565 (0.611)	Data 1.35e-04 (3.36e-04)	Tok/s 18207 (23348)	Loss/tok 3.1250 (3.3907)	LR 1.563e-05
0: TRAIN [3][2730/5173]	Time 0.638 (0.611)	Data 1.39e-04 (3.35e-04)	Tok/s 26098 (23348)	Loss/tok 3.4636 (3.3908)	LR 1.563e-05
0: TRAIN [3][2740/5173]	Time 0.561 (0.611)	Data 1.30e-04 (3.34e-04)	Tok/s 18893 (23339)	Loss/tok 3.1644 (3.3904)	LR 1.563e-05
0: TRAIN [3][2750/5173]	Time 0.632 (0.611)	Data 1.34e-04 (3.34e-04)	Tok/s 26250 (23343)	Loss/tok 3.3077 (3.3904)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][2760/5173]	Time 0.558 (0.611)	Data 1.30e-04 (3.33e-04)	Tok/s 18451 (23339)	Loss/tok 3.1348 (3.3903)	LR 1.563e-05
0: TRAIN [3][2770/5173]	Time 0.566 (0.611)	Data 1.33e-04 (3.32e-04)	Tok/s 18107 (23346)	Loss/tok 3.1943 (3.3904)	LR 1.563e-05
0: TRAIN [3][2780/5173]	Time 0.696 (0.611)	Data 1.33e-04 (3.32e-04)	Tok/s 33515 (23348)	Loss/tok 3.5965 (3.3903)	LR 1.563e-05
0: TRAIN [3][2790/5173]	Time 0.643 (0.611)	Data 1.30e-04 (3.31e-04)	Tok/s 26284 (23334)	Loss/tok 3.3595 (3.3898)	LR 1.563e-05
0: TRAIN [3][2800/5173]	Time 0.638 (0.611)	Data 1.33e-04 (3.30e-04)	Tok/s 26024 (23349)	Loss/tok 3.5063 (3.3907)	LR 1.563e-05
0: TRAIN [3][2810/5173]	Time 0.706 (0.611)	Data 1.49e-04 (3.30e-04)	Tok/s 32951 (23365)	Loss/tok 3.5418 (3.3913)	LR 1.563e-05
0: TRAIN [3][2820/5173]	Time 0.706 (0.612)	Data 1.24e-04 (3.29e-04)	Tok/s 33327 (23369)	Loss/tok 3.4357 (3.3913)	LR 1.563e-05
0: TRAIN [3][2830/5173]	Time 0.569 (0.611)	Data 1.29e-04 (3.28e-04)	Tok/s 18095 (23364)	Loss/tok 3.1585 (3.3911)	LR 1.563e-05
0: TRAIN [3][2840/5173]	Time 0.567 (0.612)	Data 1.33e-04 (3.28e-04)	Tok/s 18062 (23380)	Loss/tok 3.1942 (3.3916)	LR 1.563e-05
0: TRAIN [3][2850/5173]	Time 0.506 (0.612)	Data 1.26e-04 (3.27e-04)	Tok/s 10534 (23373)	Loss/tok 2.7771 (3.3913)	LR 1.563e-05
0: TRAIN [3][2860/5173]	Time 0.504 (0.612)	Data 1.43e-04 (3.26e-04)	Tok/s 10725 (23381)	Loss/tok 2.8137 (3.3918)	LR 1.563e-05
0: TRAIN [3][2870/5173]	Time 0.568 (0.612)	Data 1.31e-04 (3.26e-04)	Tok/s 18195 (23380)	Loss/tok 3.2265 (3.3918)	LR 1.563e-05
0: TRAIN [3][2880/5173]	Time 0.639 (0.612)	Data 2.94e-04 (3.25e-04)	Tok/s 26022 (23386)	Loss/tok 3.3480 (3.3921)	LR 1.563e-05
0: TRAIN [3][2890/5173]	Time 0.645 (0.612)	Data 1.24e-04 (3.24e-04)	Tok/s 25973 (23386)	Loss/tok 3.4157 (3.3920)	LR 1.563e-05
0: TRAIN [3][2900/5173]	Time 0.629 (0.612)	Data 1.28e-04 (3.24e-04)	Tok/s 26572 (23393)	Loss/tok 3.4441 (3.3921)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][2910/5173]	Time 0.637 (0.612)	Data 1.35e-04 (3.23e-04)	Tok/s 26370 (23393)	Loss/tok 3.3179 (3.3924)	LR 1.563e-05
0: TRAIN [3][2920/5173]	Time 0.629 (0.612)	Data 1.38e-04 (3.23e-04)	Tok/s 26606 (23404)	Loss/tok 3.4613 (3.3927)	LR 1.563e-05
0: TRAIN [3][2930/5173]	Time 0.564 (0.612)	Data 1.31e-04 (3.22e-04)	Tok/s 18333 (23393)	Loss/tok 3.1283 (3.3922)	LR 1.563e-05
0: TRAIN [3][2940/5173]	Time 0.704 (0.612)	Data 1.34e-04 (3.21e-04)	Tok/s 33315 (23383)	Loss/tok 3.6319 (3.3921)	LR 1.563e-05
0: TRAIN [3][2950/5173]	Time 0.568 (0.612)	Data 1.33e-04 (3.21e-04)	Tok/s 18275 (23371)	Loss/tok 3.1582 (3.3918)	LR 1.563e-05
0: TRAIN [3][2960/5173]	Time 0.700 (0.612)	Data 1.31e-04 (3.20e-04)	Tok/s 32882 (23372)	Loss/tok 3.6600 (3.3918)	LR 1.563e-05
0: TRAIN [3][2970/5173]	Time 0.569 (0.612)	Data 1.27e-04 (3.20e-04)	Tok/s 18356 (23373)	Loss/tok 3.1477 (3.3917)	LR 1.563e-05
0: TRAIN [3][2980/5173]	Time 0.566 (0.612)	Data 1.34e-04 (3.19e-04)	Tok/s 18109 (23372)	Loss/tok 3.1685 (3.3916)	LR 1.563e-05
0: TRAIN [3][2990/5173]	Time 0.566 (0.612)	Data 1.34e-04 (3.18e-04)	Tok/s 18686 (23379)	Loss/tok 3.1728 (3.3919)	LR 1.563e-05
0: TRAIN [3][3000/5173]	Time 0.773 (0.612)	Data 1.30e-04 (3.18e-04)	Tok/s 38294 (23382)	Loss/tok 3.8304 (3.3922)	LR 1.563e-05
0: TRAIN [3][3010/5173]	Time 0.644 (0.612)	Data 3.23e-04 (3.17e-04)	Tok/s 25855 (23379)	Loss/tok 3.4276 (3.3922)	LR 1.563e-05
0: TRAIN [3][3020/5173]	Time 0.552 (0.612)	Data 1.55e-04 (3.17e-04)	Tok/s 18590 (23389)	Loss/tok 3.2314 (3.3928)	LR 1.563e-05
0: TRAIN [3][3030/5173]	Time 0.570 (0.612)	Data 1.63e-04 (3.16e-04)	Tok/s 18213 (23403)	Loss/tok 3.2039 (3.3934)	LR 1.563e-05
0: TRAIN [3][3040/5173]	Time 0.575 (0.612)	Data 1.21e-04 (3.15e-04)	Tok/s 18042 (23405)	Loss/tok 3.1735 (3.3933)	LR 1.563e-05
0: TRAIN [3][3050/5173]	Time 0.627 (0.612)	Data 1.33e-04 (3.15e-04)	Tok/s 26937 (23402)	Loss/tok 3.3700 (3.3930)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][3060/5173]	Time 0.562 (0.612)	Data 1.15e-04 (3.14e-04)	Tok/s 18718 (23399)	Loss/tok 3.1009 (3.3931)	LR 1.563e-05
0: TRAIN [3][3070/5173]	Time 0.567 (0.612)	Data 1.23e-04 (3.14e-04)	Tok/s 17912 (23396)	Loss/tok 3.2585 (3.3928)	LR 1.563e-05
0: TRAIN [3][3080/5173]	Time 0.761 (0.612)	Data 1.32e-04 (3.13e-04)	Tok/s 38641 (23401)	Loss/tok 3.8468 (3.3933)	LR 1.563e-05
0: TRAIN [3][3090/5173]	Time 0.561 (0.612)	Data 1.38e-04 (3.12e-04)	Tok/s 18678 (23398)	Loss/tok 3.2650 (3.3933)	LR 1.563e-05
0: TRAIN [3][3100/5173]	Time 0.630 (0.612)	Data 1.24e-04 (3.12e-04)	Tok/s 26927 (23406)	Loss/tok 3.5243 (3.3933)	LR 1.563e-05
0: TRAIN [3][3110/5173]	Time 0.571 (0.612)	Data 1.28e-04 (3.11e-04)	Tok/s 18168 (23405)	Loss/tok 3.2371 (3.3932)	LR 1.563e-05
0: TRAIN [3][3120/5173]	Time 0.568 (0.612)	Data 1.36e-04 (3.11e-04)	Tok/s 18366 (23393)	Loss/tok 3.1595 (3.3930)	LR 1.563e-05
0: TRAIN [3][3130/5173]	Time 0.567 (0.612)	Data 1.26e-04 (3.10e-04)	Tok/s 18382 (23379)	Loss/tok 2.9559 (3.3925)	LR 1.563e-05
0: TRAIN [3][3140/5173]	Time 0.642 (0.612)	Data 1.28e-04 (3.10e-04)	Tok/s 26480 (23385)	Loss/tok 3.3427 (3.3927)	LR 1.563e-05
0: TRAIN [3][3150/5173]	Time 0.573 (0.612)	Data 1.25e-04 (3.09e-04)	Tok/s 18092 (23386)	Loss/tok 3.1852 (3.3927)	LR 1.563e-05
0: TRAIN [3][3160/5173]	Time 0.561 (0.612)	Data 1.37e-04 (3.09e-04)	Tok/s 18517 (23383)	Loss/tok 3.1670 (3.3924)	LR 1.563e-05
0: TRAIN [3][3170/5173]	Time 0.565 (0.612)	Data 1.24e-04 (3.08e-04)	Tok/s 18193 (23378)	Loss/tok 3.1796 (3.3925)	LR 1.563e-05
0: TRAIN [3][3180/5173]	Time 0.704 (0.612)	Data 1.32e-04 (3.08e-04)	Tok/s 32805 (23377)	Loss/tok 3.5333 (3.3923)	LR 1.563e-05
0: TRAIN [3][3190/5173]	Time 0.627 (0.612)	Data 1.32e-04 (3.07e-04)	Tok/s 26748 (23377)	Loss/tok 3.4262 (3.3923)	LR 1.563e-05
0: TRAIN [3][3200/5173]	Time 0.629 (0.612)	Data 1.31e-04 (3.07e-04)	Tok/s 27231 (23378)	Loss/tok 3.3038 (3.3923)	LR 1.563e-05
0: TRAIN [3][3210/5173]	Time 0.561 (0.612)	Data 1.31e-04 (3.06e-04)	Tok/s 17887 (23381)	Loss/tok 3.2746 (3.3925)	LR 1.563e-05
0: TRAIN [3][3220/5173]	Time 0.568 (0.612)	Data 1.40e-04 (3.06e-04)	Tok/s 18006 (23384)	Loss/tok 3.1298 (3.3927)	LR 1.563e-05
0: TRAIN [3][3230/5173]	Time 0.634 (0.612)	Data 1.29e-04 (3.05e-04)	Tok/s 26546 (23374)	Loss/tok 3.2999 (3.3923)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][3240/5173]	Time 0.565 (0.612)	Data 1.16e-04 (3.04e-04)	Tok/s 18380 (23372)	Loss/tok 3.1877 (3.3923)	LR 1.563e-05
0: TRAIN [3][3250/5173]	Time 0.699 (0.612)	Data 1.26e-04 (3.04e-04)	Tok/s 33372 (23379)	Loss/tok 3.4559 (3.3926)	LR 1.563e-05
0: TRAIN [3][3260/5173]	Time 0.699 (0.612)	Data 3.07e-04 (3.03e-04)	Tok/s 33147 (23376)	Loss/tok 3.6106 (3.3926)	LR 1.563e-05
0: TRAIN [3][3270/5173]	Time 0.625 (0.612)	Data 1.49e-04 (3.03e-04)	Tok/s 26987 (23390)	Loss/tok 3.3783 (3.3932)	LR 1.563e-05
0: TRAIN [3][3280/5173]	Time 0.771 (0.612)	Data 3.02e-04 (3.03e-04)	Tok/s 38738 (23381)	Loss/tok 3.7364 (3.3931)	LR 1.563e-05
0: TRAIN [3][3290/5173]	Time 0.703 (0.612)	Data 1.17e-04 (3.02e-04)	Tok/s 33059 (23386)	Loss/tok 3.6130 (3.3932)	LR 1.563e-05
0: TRAIN [3][3300/5173]	Time 0.696 (0.612)	Data 1.46e-04 (3.02e-04)	Tok/s 33585 (23384)	Loss/tok 3.5191 (3.3932)	LR 1.563e-05
0: TRAIN [3][3310/5173]	Time 0.623 (0.612)	Data 1.29e-04 (3.01e-04)	Tok/s 26939 (23378)	Loss/tok 3.3778 (3.3930)	LR 1.563e-05
0: TRAIN [3][3320/5173]	Time 0.505 (0.612)	Data 1.26e-04 (3.01e-04)	Tok/s 10312 (23376)	Loss/tok 2.7440 (3.3931)	LR 1.563e-05
0: TRAIN [3][3330/5173]	Time 0.566 (0.611)	Data 1.18e-04 (3.00e-04)	Tok/s 18180 (23370)	Loss/tok 3.1144 (3.3928)	LR 1.563e-05
0: TRAIN [3][3340/5173]	Time 0.689 (0.611)	Data 3.03e-04 (3.00e-04)	Tok/s 33834 (23361)	Loss/tok 3.5461 (3.3927)	LR 1.563e-05
0: TRAIN [3][3350/5173]	Time 0.554 (0.611)	Data 1.19e-04 (2.99e-04)	Tok/s 19028 (23355)	Loss/tok 3.1390 (3.3925)	LR 1.563e-05
0: TRAIN [3][3360/5173]	Time 0.640 (0.611)	Data 1.33e-04 (2.99e-04)	Tok/s 26591 (23360)	Loss/tok 3.4343 (3.3927)	LR 1.563e-05
0: TRAIN [3][3370/5173]	Time 0.775 (0.611)	Data 1.31e-04 (2.98e-04)	Tok/s 39075 (23360)	Loss/tok 3.5967 (3.3926)	LR 1.563e-05
0: TRAIN [3][3380/5173]	Time 0.569 (0.611)	Data 1.24e-04 (2.98e-04)	Tok/s 18148 (23352)	Loss/tok 3.1773 (3.3923)	LR 1.563e-05
0: TRAIN [3][3390/5173]	Time 0.574 (0.611)	Data 1.18e-04 (2.97e-04)	Tok/s 18305 (23350)	Loss/tok 3.2088 (3.3924)	LR 1.563e-05
0: TRAIN [3][3400/5173]	Time 0.631 (0.611)	Data 1.22e-04 (2.97e-04)	Tok/s 26077 (23344)	Loss/tok 3.4006 (3.3922)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][3410/5173]	Time 0.569 (0.611)	Data 1.23e-04 (2.96e-04)	Tok/s 17835 (23347)	Loss/tok 3.2582 (3.3923)	LR 1.563e-05
0: TRAIN [3][3420/5173]	Time 0.641 (0.611)	Data 1.27e-04 (2.96e-04)	Tok/s 26206 (23346)	Loss/tok 3.4436 (3.3922)	LR 1.563e-05
0: TRAIN [3][3430/5173]	Time 0.570 (0.611)	Data 1.33e-04 (2.95e-04)	Tok/s 18185 (23334)	Loss/tok 3.1084 (3.3918)	LR 1.563e-05
0: TRAIN [3][3440/5173]	Time 0.632 (0.611)	Data 1.18e-04 (2.95e-04)	Tok/s 26409 (23328)	Loss/tok 3.2935 (3.3916)	LR 1.563e-05
0: TRAIN [3][3450/5173]	Time 0.567 (0.611)	Data 1.19e-04 (2.94e-04)	Tok/s 18050 (23333)	Loss/tok 3.0471 (3.3916)	LR 1.563e-05
0: TRAIN [3][3460/5173]	Time 0.562 (0.611)	Data 1.39e-04 (2.94e-04)	Tok/s 17999 (23341)	Loss/tok 3.1914 (3.3919)	LR 1.563e-05
0: TRAIN [3][3470/5173]	Time 0.568 (0.611)	Data 1.26e-04 (2.93e-04)	Tok/s 18552 (23331)	Loss/tok 3.1616 (3.3916)	LR 1.563e-05
0: TRAIN [3][3480/5173]	Time 0.573 (0.611)	Data 1.23e-04 (2.93e-04)	Tok/s 18239 (23333)	Loss/tok 3.1264 (3.3917)	LR 1.563e-05
0: TRAIN [3][3490/5173]	Time 0.633 (0.611)	Data 1.26e-04 (2.93e-04)	Tok/s 26454 (23314)	Loss/tok 3.3957 (3.3912)	LR 1.563e-05
0: TRAIN [3][3500/5173]	Time 0.566 (0.611)	Data 1.23e-04 (2.92e-04)	Tok/s 18252 (23305)	Loss/tok 3.2677 (3.3910)	LR 1.563e-05
0: TRAIN [3][3510/5173]	Time 0.647 (0.611)	Data 1.24e-04 (2.92e-04)	Tok/s 25595 (23297)	Loss/tok 3.3882 (3.3907)	LR 1.563e-05
0: TRAIN [3][3520/5173]	Time 0.563 (0.611)	Data 1.36e-04 (2.91e-04)	Tok/s 18446 (23315)	Loss/tok 3.1835 (3.3911)	LR 1.563e-05
0: TRAIN [3][3530/5173]	Time 0.564 (0.611)	Data 3.22e-04 (2.91e-04)	Tok/s 18392 (23321)	Loss/tok 2.9938 (3.3912)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][3540/5173]	Time 0.766 (0.611)	Data 1.28e-04 (2.90e-04)	Tok/s 39338 (23326)	Loss/tok 3.6690 (3.3911)	LR 1.563e-05
0: TRAIN [3][3550/5173]	Time 0.629 (0.611)	Data 1.19e-04 (2.90e-04)	Tok/s 26924 (23324)	Loss/tok 3.3470 (3.3909)	LR 1.563e-05
0: TRAIN [3][3560/5173]	Time 0.572 (0.611)	Data 1.33e-04 (2.90e-04)	Tok/s 18282 (23326)	Loss/tok 3.2356 (3.3911)	LR 1.563e-05
0: TRAIN [3][3570/5173]	Time 0.624 (0.611)	Data 1.35e-04 (2.89e-04)	Tok/s 26945 (23322)	Loss/tok 3.4011 (3.3910)	LR 1.563e-05
0: TRAIN [3][3580/5173]	Time 0.697 (0.611)	Data 1.28e-04 (2.89e-04)	Tok/s 33177 (23327)	Loss/tok 3.6026 (3.3912)	LR 1.563e-05
0: TRAIN [3][3590/5173]	Time 0.566 (0.611)	Data 1.24e-04 (2.88e-04)	Tok/s 18431 (23322)	Loss/tok 3.1966 (3.3910)	LR 1.563e-05
0: TRAIN [3][3600/5173]	Time 0.559 (0.611)	Data 1.30e-04 (2.88e-04)	Tok/s 18545 (23315)	Loss/tok 3.1982 (3.3911)	LR 1.563e-05
0: TRAIN [3][3610/5173]	Time 0.570 (0.611)	Data 1.18e-04 (2.87e-04)	Tok/s 18096 (23318)	Loss/tok 3.3445 (3.3912)	LR 1.563e-05
0: TRAIN [3][3620/5173]	Time 0.566 (0.611)	Data 1.35e-04 (2.87e-04)	Tok/s 18584 (23318)	Loss/tok 3.1679 (3.3911)	LR 1.563e-05
0: TRAIN [3][3630/5173]	Time 0.504 (0.611)	Data 1.26e-04 (2.87e-04)	Tok/s 10355 (23322)	Loss/tok 2.7532 (3.3914)	LR 1.563e-05
0: TRAIN [3][3640/5173]	Time 0.692 (0.611)	Data 1.21e-04 (2.86e-04)	Tok/s 33503 (23327)	Loss/tok 3.5258 (3.3915)	LR 1.563e-05
0: TRAIN [3][3650/5173]	Time 0.630 (0.611)	Data 1.27e-04 (2.86e-04)	Tok/s 26458 (23324)	Loss/tok 3.2625 (3.3913)	LR 1.563e-05
0: TRAIN [3][3660/5173]	Time 0.567 (0.611)	Data 1.25e-04 (2.85e-04)	Tok/s 18541 (23306)	Loss/tok 3.1736 (3.3908)	LR 1.563e-05
0: TRAIN [3][3670/5173]	Time 0.646 (0.611)	Data 1.26e-04 (2.85e-04)	Tok/s 26138 (23311)	Loss/tok 3.4241 (3.3908)	LR 1.563e-05
0: TRAIN [3][3680/5173]	Time 0.569 (0.611)	Data 1.33e-04 (2.84e-04)	Tok/s 18419 (23304)	Loss/tok 3.1454 (3.3904)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][3690/5173]	Time 0.559 (0.611)	Data 1.33e-04 (2.84e-04)	Tok/s 18494 (23305)	Loss/tok 3.1275 (3.3905)	LR 1.563e-05
0: TRAIN [3][3700/5173]	Time 0.773 (0.611)	Data 1.41e-04 (2.84e-04)	Tok/s 38827 (23305)	Loss/tok 3.7309 (3.3906)	LR 1.563e-05
0: TRAIN [3][3710/5173]	Time 0.635 (0.611)	Data 2.96e-04 (2.83e-04)	Tok/s 26356 (23294)	Loss/tok 3.4616 (3.3903)	LR 1.563e-05
0: TRAIN [3][3720/5173]	Time 0.622 (0.611)	Data 1.26e-04 (2.83e-04)	Tok/s 26880 (23287)	Loss/tok 3.3721 (3.3901)	LR 1.563e-05
0: TRAIN [3][3730/5173]	Time 0.704 (0.611)	Data 1.22e-04 (2.83e-04)	Tok/s 33314 (23302)	Loss/tok 3.5449 (3.3904)	LR 1.563e-05
0: TRAIN [3][3740/5173]	Time 0.567 (0.611)	Data 1.29e-04 (2.82e-04)	Tok/s 18304 (23293)	Loss/tok 3.2550 (3.3901)	LR 1.563e-05
0: TRAIN [3][3750/5173]	Time 0.564 (0.611)	Data 1.27e-04 (2.82e-04)	Tok/s 18108 (23298)	Loss/tok 3.1355 (3.3902)	LR 1.563e-05
0: TRAIN [3][3760/5173]	Time 0.630 (0.611)	Data 1.36e-04 (2.81e-04)	Tok/s 26750 (23317)	Loss/tok 3.3401 (3.3908)	LR 1.563e-05
0: TRAIN [3][3770/5173]	Time 0.701 (0.611)	Data 1.22e-04 (2.81e-04)	Tok/s 33131 (23325)	Loss/tok 3.5666 (3.3911)	LR 1.563e-05
0: TRAIN [3][3780/5173]	Time 0.637 (0.611)	Data 3.05e-04 (2.81e-04)	Tok/s 26435 (23331)	Loss/tok 3.3435 (3.3911)	LR 1.563e-05
0: TRAIN [3][3790/5173]	Time 0.631 (0.611)	Data 1.29e-04 (2.80e-04)	Tok/s 26768 (23331)	Loss/tok 3.3758 (3.3909)	LR 1.563e-05
0: TRAIN [3][3800/5173]	Time 0.693 (0.611)	Data 1.38e-04 (2.80e-04)	Tok/s 33520 (23332)	Loss/tok 3.5671 (3.3909)	LR 1.563e-05
0: TRAIN [3][3810/5173]	Time 0.627 (0.611)	Data 1.75e-04 (2.80e-04)	Tok/s 26710 (23329)	Loss/tok 3.2379 (3.3907)	LR 1.563e-05
0: TRAIN [3][3820/5173]	Time 0.568 (0.611)	Data 1.69e-04 (2.80e-04)	Tok/s 18333 (23332)	Loss/tok 3.2839 (3.3908)	LR 1.563e-05
0: TRAIN [3][3830/5173]	Time 0.643 (0.611)	Data 1.22e-04 (2.79e-04)	Tok/s 26407 (23329)	Loss/tok 3.2379 (3.3906)	LR 1.563e-05
0: TRAIN [3][3840/5173]	Time 0.634 (0.611)	Data 3.12e-04 (2.79e-04)	Tok/s 26719 (23327)	Loss/tok 3.4135 (3.3905)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][3850/5173]	Time 0.773 (0.611)	Data 1.38e-04 (2.78e-04)	Tok/s 38644 (23337)	Loss/tok 3.7681 (3.3910)	LR 1.563e-05
0: TRAIN [3][3860/5173]	Time 0.704 (0.611)	Data 1.22e-04 (2.78e-04)	Tok/s 33197 (23331)	Loss/tok 3.4546 (3.3908)	LR 1.563e-05
0: TRAIN [3][3870/5173]	Time 0.648 (0.611)	Data 1.33e-04 (2.78e-04)	Tok/s 25816 (23334)	Loss/tok 3.4597 (3.3909)	LR 1.563e-05
0: TRAIN [3][3880/5173]	Time 0.567 (0.611)	Data 1.23e-04 (2.77e-04)	Tok/s 18556 (23331)	Loss/tok 3.1670 (3.3907)	LR 1.563e-05
0: TRAIN [3][3890/5173]	Time 0.638 (0.611)	Data 1.23e-04 (2.77e-04)	Tok/s 25939 (23328)	Loss/tok 3.3541 (3.3905)	LR 1.563e-05
0: TRAIN [3][3900/5173]	Time 0.642 (0.611)	Data 1.26e-04 (2.76e-04)	Tok/s 26134 (23323)	Loss/tok 3.3030 (3.3903)	LR 1.563e-05
0: TRAIN [3][3910/5173]	Time 0.636 (0.611)	Data 1.30e-04 (2.76e-04)	Tok/s 26624 (23319)	Loss/tok 3.4088 (3.3902)	LR 1.563e-05
0: TRAIN [3][3920/5173]	Time 0.636 (0.611)	Data 1.33e-04 (2.76e-04)	Tok/s 26778 (23322)	Loss/tok 3.3938 (3.3901)	LR 1.563e-05
0: TRAIN [3][3930/5173]	Time 0.638 (0.611)	Data 1.36e-04 (2.75e-04)	Tok/s 26552 (23322)	Loss/tok 3.5141 (3.3899)	LR 1.563e-05
0: TRAIN [3][3940/5173]	Time 0.498 (0.611)	Data 1.38e-04 (2.75e-04)	Tok/s 10801 (23322)	Loss/tok 2.7494 (3.3900)	LR 1.563e-05
0: TRAIN [3][3950/5173]	Time 0.636 (0.611)	Data 1.31e-04 (2.75e-04)	Tok/s 26516 (23327)	Loss/tok 3.3121 (3.3900)	LR 1.563e-05
0: TRAIN [3][3960/5173]	Time 0.696 (0.611)	Data 1.34e-04 (2.75e-04)	Tok/s 33761 (23318)	Loss/tok 3.6046 (3.3898)	LR 1.563e-05
0: TRAIN [3][3970/5173]	Time 0.562 (0.611)	Data 1.31e-04 (2.74e-04)	Tok/s 18559 (23320)	Loss/tok 3.0334 (3.3899)	LR 1.563e-05
0: TRAIN [3][3980/5173]	Time 0.643 (0.611)	Data 1.24e-04 (2.74e-04)	Tok/s 25698 (23321)	Loss/tok 3.5212 (3.3899)	LR 1.563e-05
0: TRAIN [3][3990/5173]	Time 0.568 (0.611)	Data 1.20e-04 (2.73e-04)	Tok/s 18464 (23319)	Loss/tok 3.2455 (3.3900)	LR 1.563e-05
0: TRAIN [3][4000/5173]	Time 0.694 (0.611)	Data 1.29e-04 (2.73e-04)	Tok/s 33094 (23312)	Loss/tok 3.7481 (3.3900)	LR 1.563e-05
0: TRAIN [3][4010/5173]	Time 0.702 (0.611)	Data 1.25e-04 (2.73e-04)	Tok/s 33048 (23316)	Loss/tok 3.6036 (3.3900)	LR 1.563e-05
0: TRAIN [3][4020/5173]	Time 0.570 (0.611)	Data 1.25e-04 (2.72e-04)	Tok/s 17907 (23316)	Loss/tok 3.1114 (3.3900)	LR 1.563e-05
0: TRAIN [3][4030/5173]	Time 0.507 (0.611)	Data 1.28e-04 (2.72e-04)	Tok/s 10322 (23314)	Loss/tok 2.8257 (3.3902)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][4040/5173]	Time 0.506 (0.611)	Data 1.19e-04 (2.72e-04)	Tok/s 10604 (23315)	Loss/tok 2.7057 (3.3901)	LR 1.563e-05
0: TRAIN [3][4050/5173]	Time 0.647 (0.611)	Data 1.21e-04 (2.71e-04)	Tok/s 25731 (23309)	Loss/tok 3.2828 (3.3898)	LR 1.563e-05
0: TRAIN [3][4060/5173]	Time 0.703 (0.611)	Data 1.31e-04 (2.71e-04)	Tok/s 33279 (23308)	Loss/tok 3.4975 (3.3897)	LR 1.563e-05
0: TRAIN [3][4070/5173]	Time 0.501 (0.611)	Data 1.36e-04 (2.71e-04)	Tok/s 10647 (23305)	Loss/tok 2.7365 (3.3896)	LR 1.563e-05
0: TRAIN [3][4080/5173]	Time 0.564 (0.611)	Data 1.25e-04 (2.70e-04)	Tok/s 18445 (23306)	Loss/tok 3.2350 (3.3896)	LR 1.563e-05
0: TRAIN [3][4090/5173]	Time 0.563 (0.611)	Data 1.29e-04 (2.70e-04)	Tok/s 18792 (23306)	Loss/tok 3.1086 (3.3897)	LR 1.563e-05
0: TRAIN [3][4100/5173]	Time 0.574 (0.611)	Data 1.34e-04 (2.70e-04)	Tok/s 18079 (23299)	Loss/tok 3.1817 (3.3896)	LR 1.563e-05
0: TRAIN [3][4110/5173]	Time 0.625 (0.611)	Data 1.28e-04 (2.69e-04)	Tok/s 26957 (23293)	Loss/tok 3.3275 (3.3895)	LR 1.563e-05
0: TRAIN [3][4120/5173]	Time 0.622 (0.611)	Data 1.33e-04 (2.69e-04)	Tok/s 27209 (23296)	Loss/tok 3.3497 (3.3894)	LR 1.563e-05
0: TRAIN [3][4130/5173]	Time 0.694 (0.611)	Data 1.31e-04 (2.69e-04)	Tok/s 33644 (23303)	Loss/tok 3.6000 (3.3896)	LR 1.563e-05
0: TRAIN [3][4140/5173]	Time 0.565 (0.611)	Data 1.27e-04 (2.69e-04)	Tok/s 18691 (23301)	Loss/tok 3.1427 (3.3896)	LR 1.563e-05
0: TRAIN [3][4150/5173]	Time 0.701 (0.611)	Data 1.36e-04 (2.68e-04)	Tok/s 33439 (23300)	Loss/tok 3.6618 (3.3896)	LR 1.563e-05
0: TRAIN [3][4160/5173]	Time 0.565 (0.611)	Data 1.31e-04 (2.68e-04)	Tok/s 18398 (23286)	Loss/tok 3.1807 (3.3892)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][4170/5173]	Time 0.554 (0.611)	Data 1.42e-04 (2.68e-04)	Tok/s 18694 (23297)	Loss/tok 3.1038 (3.3896)	LR 1.563e-05
0: TRAIN [3][4180/5173]	Time 0.571 (0.611)	Data 1.27e-04 (2.67e-04)	Tok/s 18057 (23294)	Loss/tok 3.1729 (3.3895)	LR 1.563e-05
0: TRAIN [3][4190/5173]	Time 0.630 (0.611)	Data 1.38e-04 (2.67e-04)	Tok/s 26583 (23296)	Loss/tok 3.4739 (3.3894)	LR 1.563e-05
0: TRAIN [3][4200/5173]	Time 0.570 (0.611)	Data 1.32e-04 (2.67e-04)	Tok/s 18203 (23291)	Loss/tok 3.2352 (3.3893)	LR 1.563e-05
0: TRAIN [3][4210/5173]	Time 0.768 (0.611)	Data 1.27e-04 (2.67e-04)	Tok/s 38571 (23294)	Loss/tok 3.6885 (3.3892)	LR 1.563e-05
0: TRAIN [3][4220/5173]	Time 0.566 (0.611)	Data 1.34e-04 (2.66e-04)	Tok/s 18025 (23295)	Loss/tok 3.2138 (3.3892)	LR 1.563e-05
0: TRAIN [3][4230/5173]	Time 0.567 (0.611)	Data 1.23e-04 (2.66e-04)	Tok/s 18203 (23291)	Loss/tok 3.1001 (3.3891)	LR 1.563e-05
0: TRAIN [3][4240/5173]	Time 0.568 (0.611)	Data 1.30e-04 (2.66e-04)	Tok/s 18219 (23289)	Loss/tok 3.1187 (3.3890)	LR 1.563e-05
0: TRAIN [3][4250/5173]	Time 0.502 (0.611)	Data 1.30e-04 (2.65e-04)	Tok/s 10536 (23292)	Loss/tok 2.8446 (3.3891)	LR 1.563e-05
0: TRAIN [3][4260/5173]	Time 0.571 (0.611)	Data 1.27e-04 (2.65e-04)	Tok/s 18167 (23287)	Loss/tok 3.0702 (3.3890)	LR 1.563e-05
0: TRAIN [3][4270/5173]	Time 0.571 (0.611)	Data 2.83e-04 (2.65e-04)	Tok/s 18146 (23288)	Loss/tok 3.1415 (3.3889)	LR 1.563e-05
0: TRAIN [3][4280/5173]	Time 0.620 (0.611)	Data 1.15e-04 (2.65e-04)	Tok/s 27304 (23279)	Loss/tok 3.4099 (3.3888)	LR 1.563e-05
0: TRAIN [3][4290/5173]	Time 0.505 (0.611)	Data 1.15e-04 (2.64e-04)	Tok/s 10705 (23275)	Loss/tok 2.7139 (3.3887)	LR 1.563e-05
0: TRAIN [3][4300/5173]	Time 0.628 (0.611)	Data 1.19e-04 (2.64e-04)	Tok/s 26605 (23274)	Loss/tok 3.3536 (3.3888)	LR 1.563e-05
0: TRAIN [3][4310/5173]	Time 0.705 (0.611)	Data 1.20e-04 (2.64e-04)	Tok/s 33005 (23264)	Loss/tok 3.5164 (3.3885)	LR 1.563e-05
0: TRAIN [3][4320/5173]	Time 0.632 (0.610)	Data 1.21e-04 (2.63e-04)	Tok/s 26563 (23253)	Loss/tok 3.5572 (3.3883)	LR 1.563e-05
0: TRAIN [3][4330/5173]	Time 0.694 (0.610)	Data 1.24e-04 (2.63e-04)	Tok/s 33656 (23252)	Loss/tok 3.5518 (3.3882)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][4340/5173]	Time 0.627 (0.610)	Data 1.20e-04 (2.63e-04)	Tok/s 26850 (23256)	Loss/tok 3.4929 (3.3884)	LR 1.563e-05
0: TRAIN [3][4350/5173]	Time 0.566 (0.610)	Data 1.22e-04 (2.62e-04)	Tok/s 18183 (23251)	Loss/tok 3.1388 (3.3882)	LR 1.563e-05
0: TRAIN [3][4360/5173]	Time 0.497 (0.610)	Data 2.93e-04 (2.62e-04)	Tok/s 10433 (23249)	Loss/tok 2.7450 (3.3882)	LR 1.563e-05
0: TRAIN [3][4370/5173]	Time 0.769 (0.610)	Data 1.30e-04 (2.62e-04)	Tok/s 38332 (23253)	Loss/tok 3.8548 (3.3884)	LR 1.563e-05
0: TRAIN [3][4380/5173]	Time 0.772 (0.610)	Data 3.02e-04 (2.61e-04)	Tok/s 38680 (23260)	Loss/tok 3.6983 (3.3886)	LR 1.563e-05
0: TRAIN [3][4390/5173]	Time 0.569 (0.610)	Data 1.22e-04 (2.61e-04)	Tok/s 18160 (23258)	Loss/tok 3.1653 (3.3885)	LR 1.563e-05
0: TRAIN [3][4400/5173]	Time 0.704 (0.611)	Data 1.26e-04 (2.61e-04)	Tok/s 33647 (23263)	Loss/tok 3.5795 (3.3888)	LR 1.563e-05
0: TRAIN [3][4410/5173]	Time 0.507 (0.610)	Data 1.23e-04 (2.61e-04)	Tok/s 10218 (23253)	Loss/tok 2.7279 (3.3886)	LR 1.563e-05
0: TRAIN [3][4420/5173]	Time 0.501 (0.610)	Data 2.88e-04 (2.60e-04)	Tok/s 10527 (23245)	Loss/tok 2.7571 (3.3885)	LR 1.563e-05
0: TRAIN [3][4430/5173]	Time 0.564 (0.610)	Data 1.22e-04 (2.60e-04)	Tok/s 17667 (23243)	Loss/tok 3.2801 (3.3884)	LR 1.563e-05
0: TRAIN [3][4440/5173]	Time 0.568 (0.610)	Data 1.18e-04 (2.60e-04)	Tok/s 18056 (23247)	Loss/tok 3.1819 (3.3887)	LR 1.563e-05
0: TRAIN [3][4450/5173]	Time 0.569 (0.610)	Data 1.24e-04 (2.60e-04)	Tok/s 18103 (23236)	Loss/tok 3.0864 (3.3884)	LR 1.563e-05
0: TRAIN [3][4460/5173]	Time 0.704 (0.610)	Data 1.31e-04 (2.59e-04)	Tok/s 32639 (23243)	Loss/tok 3.6912 (3.3888)	LR 1.563e-05
0: TRAIN [3][4470/5173]	Time 0.570 (0.610)	Data 1.23e-04 (2.59e-04)	Tok/s 18426 (23246)	Loss/tok 3.1707 (3.3888)	LR 1.563e-05
0: TRAIN [3][4480/5173]	Time 0.565 (0.610)	Data 1.20e-04 (2.59e-04)	Tok/s 18239 (23238)	Loss/tok 3.1976 (3.3886)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][4490/5173]	Time 0.566 (0.610)	Data 1.25e-04 (2.59e-04)	Tok/s 18170 (23237)	Loss/tok 3.2573 (3.3885)	LR 1.563e-05
0: TRAIN [3][4500/5173]	Time 0.562 (0.610)	Data 1.18e-04 (2.58e-04)	Tok/s 18194 (23242)	Loss/tok 3.1792 (3.3885)	LR 1.563e-05
0: TRAIN [3][4510/5173]	Time 0.500 (0.610)	Data 1.16e-04 (2.58e-04)	Tok/s 10392 (23229)	Loss/tok 2.7328 (3.3882)	LR 1.563e-05
0: TRAIN [3][4520/5173]	Time 0.571 (0.610)	Data 1.23e-04 (2.58e-04)	Tok/s 18367 (23236)	Loss/tok 3.1417 (3.3886)	LR 1.563e-05
0: TRAIN [3][4530/5173]	Time 0.636 (0.610)	Data 1.16e-04 (2.57e-04)	Tok/s 26198 (23236)	Loss/tok 3.3476 (3.3887)	LR 1.563e-05
0: TRAIN [3][4540/5173]	Time 0.705 (0.610)	Data 1.22e-04 (2.57e-04)	Tok/s 32694 (23228)	Loss/tok 3.6507 (3.3886)	LR 1.563e-05
0: TRAIN [3][4550/5173]	Time 0.566 (0.610)	Data 1.15e-04 (2.57e-04)	Tok/s 17844 (23229)	Loss/tok 3.2051 (3.3885)	LR 1.563e-05
0: TRAIN [3][4560/5173]	Time 0.639 (0.610)	Data 2.77e-04 (2.57e-04)	Tok/s 25860 (23225)	Loss/tok 3.3312 (3.3883)	LR 1.563e-05
0: TRAIN [3][4570/5173]	Time 0.636 (0.610)	Data 1.18e-04 (2.56e-04)	Tok/s 26483 (23224)	Loss/tok 3.5010 (3.3883)	LR 1.563e-05
0: TRAIN [3][4580/5173]	Time 0.564 (0.610)	Data 1.15e-04 (2.56e-04)	Tok/s 18549 (23228)	Loss/tok 3.2247 (3.3885)	LR 1.563e-05
0: TRAIN [3][4590/5173]	Time 0.560 (0.610)	Data 1.16e-04 (2.56e-04)	Tok/s 18077 (23226)	Loss/tok 3.1839 (3.3884)	LR 1.563e-05
0: TRAIN [3][4600/5173]	Time 0.632 (0.610)	Data 1.16e-04 (2.55e-04)	Tok/s 26717 (23225)	Loss/tok 3.4211 (3.3884)	LR 1.563e-05
0: TRAIN [3][4610/5173]	Time 0.560 (0.610)	Data 1.14e-04 (2.55e-04)	Tok/s 18348 (23222)	Loss/tok 3.2434 (3.3885)	LR 1.563e-05
0: TRAIN [3][4620/5173]	Time 0.503 (0.610)	Data 1.16e-04 (2.55e-04)	Tok/s 10562 (23216)	Loss/tok 2.6768 (3.3883)	LR 1.563e-05
0: TRAIN [3][4630/5173]	Time 0.643 (0.610)	Data 1.20e-04 (2.55e-04)	Tok/s 25998 (23212)	Loss/tok 3.4358 (3.3881)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][4640/5173]	Time 0.573 (0.610)	Data 1.16e-04 (2.54e-04)	Tok/s 18289 (23205)	Loss/tok 3.2032 (3.3880)	LR 1.563e-05
0: TRAIN [3][4650/5173]	Time 0.762 (0.610)	Data 1.16e-04 (2.54e-04)	Tok/s 39058 (23212)	Loss/tok 3.8918 (3.3882)	LR 1.563e-05
0: TRAIN [3][4660/5173]	Time 0.563 (0.610)	Data 1.38e-04 (2.54e-04)	Tok/s 18543 (23216)	Loss/tok 3.1152 (3.3884)	LR 1.563e-05
0: TRAIN [3][4670/5173]	Time 0.565 (0.610)	Data 1.18e-04 (2.53e-04)	Tok/s 18059 (23219)	Loss/tok 3.2630 (3.3885)	LR 1.563e-05
0: TRAIN [3][4680/5173]	Time 0.564 (0.610)	Data 1.28e-04 (2.53e-04)	Tok/s 18686 (23215)	Loss/tok 3.2324 (3.3883)	LR 1.563e-05
0: TRAIN [3][4690/5173]	Time 0.633 (0.610)	Data 1.14e-04 (2.53e-04)	Tok/s 26445 (23214)	Loss/tok 3.3219 (3.3882)	LR 1.563e-05
0: TRAIN [3][4700/5173]	Time 0.690 (0.610)	Data 3.07e-04 (2.53e-04)	Tok/s 33836 (23210)	Loss/tok 3.5358 (3.3881)	LR 1.563e-05
0: TRAIN [3][4710/5173]	Time 0.638 (0.610)	Data 1.17e-04 (2.52e-04)	Tok/s 26371 (23204)	Loss/tok 3.3478 (3.3879)	LR 1.563e-05
0: TRAIN [3][4720/5173]	Time 0.636 (0.610)	Data 1.19e-04 (2.52e-04)	Tok/s 26172 (23202)	Loss/tok 3.4079 (3.3877)	LR 1.563e-05
0: TRAIN [3][4730/5173]	Time 0.693 (0.610)	Data 1.36e-04 (2.52e-04)	Tok/s 33428 (23198)	Loss/tok 3.6076 (3.3877)	LR 1.563e-05
0: TRAIN [3][4740/5173]	Time 0.633 (0.610)	Data 3.08e-04 (2.52e-04)	Tok/s 26613 (23194)	Loss/tok 3.3283 (3.3876)	LR 1.563e-05
0: TRAIN [3][4750/5173]	Time 0.570 (0.610)	Data 1.45e-04 (2.51e-04)	Tok/s 17993 (23195)	Loss/tok 3.1276 (3.3875)	LR 1.563e-05
0: TRAIN [3][4760/5173]	Time 0.765 (0.610)	Data 1.26e-04 (2.51e-04)	Tok/s 38944 (23191)	Loss/tok 3.6719 (3.3875)	LR 1.563e-05
0: TRAIN [3][4770/5173]	Time 0.640 (0.610)	Data 1.27e-04 (2.51e-04)	Tok/s 26292 (23193)	Loss/tok 3.3389 (3.3876)	LR 1.563e-05
0: TRAIN [3][4780/5173]	Time 0.565 (0.610)	Data 1.19e-04 (2.51e-04)	Tok/s 17961 (23197)	Loss/tok 3.1928 (3.3877)	LR 1.563e-05
0: TRAIN [3][4790/5173]	Time 0.641 (0.610)	Data 1.26e-04 (2.50e-04)	Tok/s 26164 (23197)	Loss/tok 3.4048 (3.3878)	LR 1.563e-05
0: TRAIN [3][4800/5173]	Time 0.704 (0.610)	Data 1.23e-04 (2.50e-04)	Tok/s 32646 (23201)	Loss/tok 3.7447 (3.3879)	LR 1.563e-05
0: TRAIN [3][4810/5173]	Time 0.769 (0.610)	Data 1.23e-04 (2.50e-04)	Tok/s 38667 (23206)	Loss/tok 3.6622 (3.3881)	LR 1.563e-05
0: TRAIN [3][4820/5173]	Time 0.570 (0.610)	Data 1.22e-04 (2.50e-04)	Tok/s 18388 (23206)	Loss/tok 3.2106 (3.3882)	LR 1.563e-05
0: TRAIN [3][4830/5173]	Time 0.563 (0.610)	Data 1.25e-04 (2.49e-04)	Tok/s 18113 (23196)	Loss/tok 3.1012 (3.3879)	LR 1.563e-05
0: TRAIN [3][4840/5173]	Time 0.563 (0.610)	Data 1.30e-04 (2.49e-04)	Tok/s 18860 (23195)	Loss/tok 3.0325 (3.3878)	LR 1.563e-05
0: TRAIN [3][4850/5173]	Time 0.568 (0.610)	Data 1.23e-04 (2.49e-04)	Tok/s 18232 (23194)	Loss/tok 3.2043 (3.3878)	LR 1.563e-05
0: TRAIN [3][4860/5173]	Time 0.566 (0.610)	Data 1.18e-04 (2.49e-04)	Tok/s 18600 (23189)	Loss/tok 3.0469 (3.3876)	LR 1.563e-05
0: TRAIN [3][4870/5173]	Time 0.567 (0.610)	Data 1.24e-04 (2.48e-04)	Tok/s 18264 (23190)	Loss/tok 3.1231 (3.3876)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][4880/5173]	Time 0.562 (0.610)	Data 1.25e-04 (2.48e-04)	Tok/s 18078 (23189)	Loss/tok 3.0195 (3.3876)	LR 1.563e-05
0: TRAIN [3][4890/5173]	Time 0.564 (0.610)	Data 1.29e-04 (2.48e-04)	Tok/s 18228 (23189)	Loss/tok 3.1150 (3.3877)	LR 1.563e-05
0: TRAIN [3][4900/5173]	Time 0.497 (0.610)	Data 1.26e-04 (2.48e-04)	Tok/s 10398 (23182)	Loss/tok 2.7568 (3.3875)	LR 1.563e-05
0: TRAIN [3][4910/5173]	Time 0.620 (0.610)	Data 1.31e-04 (2.47e-04)	Tok/s 27587 (23186)	Loss/tok 3.4964 (3.3878)	LR 1.563e-05
0: TRAIN [3][4920/5173]	Time 0.568 (0.610)	Data 3.05e-04 (2.47e-04)	Tok/s 18129 (23184)	Loss/tok 3.2685 (3.3877)	LR 1.563e-05
0: TRAIN [3][4930/5173]	Time 0.685 (0.610)	Data 1.30e-04 (2.47e-04)	Tok/s 33852 (23190)	Loss/tok 3.6347 (3.3880)	LR 1.563e-05
0: TRAIN [3][4940/5173]	Time 0.619 (0.610)	Data 1.24e-04 (2.47e-04)	Tok/s 27107 (23183)	Loss/tok 3.3743 (3.3877)	LR 1.563e-05
0: TRAIN [3][4950/5173]	Time 0.503 (0.610)	Data 1.19e-04 (2.47e-04)	Tok/s 10471 (23177)	Loss/tok 2.6850 (3.3875)	LR 1.563e-05
0: TRAIN [3][4960/5173]	Time 0.628 (0.610)	Data 1.26e-04 (2.46e-04)	Tok/s 26610 (23182)	Loss/tok 3.5538 (3.3877)	LR 1.563e-05
0: TRAIN [3][4970/5173]	Time 0.565 (0.610)	Data 1.22e-04 (2.46e-04)	Tok/s 18266 (23178)	Loss/tok 3.1720 (3.3876)	LR 1.563e-05
0: TRAIN [3][4980/5173]	Time 0.630 (0.610)	Data 2.78e-04 (2.46e-04)	Tok/s 26625 (23178)	Loss/tok 3.3555 (3.3876)	LR 1.563e-05
0: TRAIN [3][4990/5173]	Time 0.627 (0.610)	Data 1.33e-04 (2.46e-04)	Tok/s 27017 (23176)	Loss/tok 3.3866 (3.3875)	LR 1.563e-05
0: TRAIN [3][5000/5173]	Time 0.636 (0.610)	Data 1.18e-04 (2.45e-04)	Tok/s 26365 (23176)	Loss/tok 3.2571 (3.3877)	LR 1.563e-05
0: TRAIN [3][5010/5173]	Time 0.768 (0.610)	Data 1.24e-04 (2.45e-04)	Tok/s 38529 (23176)	Loss/tok 3.6276 (3.3878)	LR 1.563e-05
0: TRAIN [3][5020/5173]	Time 0.567 (0.610)	Data 1.31e-04 (2.45e-04)	Tok/s 17804 (23172)	Loss/tok 3.1148 (3.3877)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][5030/5173]	Time 0.560 (0.610)	Data 1.18e-04 (2.45e-04)	Tok/s 18189 (23176)	Loss/tok 3.1727 (3.3877)	LR 1.563e-05
0: TRAIN [3][5040/5173]	Time 0.567 (0.610)	Data 1.20e-04 (2.45e-04)	Tok/s 18267 (23171)	Loss/tok 3.1645 (3.3875)	LR 1.563e-05
0: TRAIN [3][5050/5173]	Time 0.644 (0.610)	Data 1.24e-04 (2.44e-04)	Tok/s 26330 (23166)	Loss/tok 3.3597 (3.3874)	LR 1.563e-05
0: TRAIN [3][5060/5173]	Time 0.573 (0.610)	Data 1.20e-04 (2.44e-04)	Tok/s 18220 (23169)	Loss/tok 3.1626 (3.3874)	LR 1.563e-05
0: TRAIN [3][5070/5173]	Time 0.639 (0.610)	Data 1.19e-04 (2.44e-04)	Tok/s 26851 (23164)	Loss/tok 3.3588 (3.3872)	LR 1.563e-05
0: TRAIN [3][5080/5173]	Time 0.770 (0.610)	Data 1.29e-04 (2.44e-04)	Tok/s 38835 (23159)	Loss/tok 3.8066 (3.3872)	LR 1.563e-05
0: TRAIN [3][5090/5173]	Time 0.567 (0.610)	Data 1.30e-04 (2.43e-04)	Tok/s 18081 (23151)	Loss/tok 3.1789 (3.3870)	LR 1.563e-05
0: TRAIN [3][5100/5173]	Time 0.501 (0.610)	Data 1.20e-04 (2.43e-04)	Tok/s 10529 (23155)	Loss/tok 2.7365 (3.3871)	LR 1.563e-05
0: TRAIN [3][5110/5173]	Time 0.632 (0.609)	Data 1.18e-04 (2.43e-04)	Tok/s 26425 (23152)	Loss/tok 3.2876 (3.3870)	LR 1.563e-05
0: TRAIN [3][5120/5173]	Time 0.691 (0.610)	Data 1.19e-04 (2.43e-04)	Tok/s 33826 (23154)	Loss/tok 3.4824 (3.3869)	LR 1.563e-05
0: TRAIN [3][5130/5173]	Time 0.702 (0.610)	Data 1.22e-04 (2.43e-04)	Tok/s 33536 (23157)	Loss/tok 3.4806 (3.3869)	LR 1.563e-05
0: TRAIN [3][5140/5173]	Time 0.629 (0.610)	Data 1.21e-04 (2.42e-04)	Tok/s 26620 (23156)	Loss/tok 3.4133 (3.3869)	LR 1.563e-05
0: TRAIN [3][5150/5173]	Time 0.558 (0.609)	Data 1.22e-04 (2.42e-04)	Tok/s 18348 (23154)	Loss/tok 3.1622 (3.3868)	LR 1.563e-05
0: TRAIN [3][5160/5173]	Time 0.641 (0.609)	Data 1.18e-04 (2.42e-04)	Tok/s 26392 (23155)	Loss/tok 3.3958 (3.3868)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][5170/5173]	Time 0.691 (0.610)	Data 1.26e-04 (2.42e-04)	Tok/s 33793 (23164)	Loss/tok 3.5698 (3.3870)	LR 1.563e-05
:::MLL 1585777991.072 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 525}}
:::MLL 1585777991.073 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [3][0/8]	Time 0.703 (0.703)	Decoder iters 131.0 (131.0)	Tok/s 23334 (23334)
0: Running moses detokenizer
0: BLEU(score=20.620891930024094, counts=[34778, 16182, 8683, 4850], totals=[64766, 61763, 58761, 55764], precisions=[53.69792792514591, 26.200152194679664, 14.776807746634672, 8.69736747722545], bp=1.0, sys_len=64766, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1585777997.271 eval_accuracy: {"value": 20.62, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 536}}
:::MLL 1585777997.272 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 3	Training Loss: 3.3868	Test BLEU: 20.62
0: Performance: Epoch: 3	Training: 69481 Tok/s
0: Finished epoch 3
:::MLL 1585777997.272 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 558}}
:::MLL 1585777997.272 block_start: {"value": null, "metadata": {"first_epoch_num": 5, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1585777997.273 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 515}}
0: Starting epoch 4
0: Executing preallocation
0: Sampler for epoch 4 uses seed 1928976903
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [4][0/5173]	Time 1.198 (1.198)	Data 5.35e-01 (5.35e-01)	Tok/s 14084 (14084)	Loss/tok 3.3813 (3.3813)	LR 1.563e-05
0: TRAIN [4][10/5173]	Time 0.629 (0.675)	Data 1.19e-04 (4.88e-02)	Tok/s 27093 (23626)	Loss/tok 3.4756 (3.4158)	LR 1.563e-05
0: TRAIN [4][20/5173]	Time 0.567 (0.629)	Data 1.23e-04 (2.56e-02)	Tok/s 18259 (21783)	Loss/tok 3.0808 (3.3619)	LR 1.563e-05
0: TRAIN [4][30/5173]	Time 0.569 (0.620)	Data 2.93e-04 (1.74e-02)	Tok/s 17880 (21908)	Loss/tok 3.1569 (3.3560)	LR 1.563e-05
0: TRAIN [4][40/5173]	Time 0.637 (0.625)	Data 1.21e-04 (1.32e-02)	Tok/s 26591 (23111)	Loss/tok 3.3664 (3.3832)	LR 1.563e-05
0: TRAIN [4][50/5173]	Time 0.640 (0.627)	Data 1.25e-04 (1.06e-02)	Tok/s 26359 (23609)	Loss/tok 3.4035 (3.3967)	LR 1.563e-05
0: TRAIN [4][60/5173]	Time 0.569 (0.630)	Data 1.20e-04 (8.90e-03)	Tok/s 18530 (24076)	Loss/tok 3.2353 (3.4114)	LR 1.563e-05
0: TRAIN [4][70/5173]	Time 0.642 (0.629)	Data 1.18e-04 (7.66e-03)	Tok/s 26119 (24102)	Loss/tok 3.4490 (3.4114)	LR 1.563e-05
0: TRAIN [4][80/5173]	Time 0.566 (0.629)	Data 1.34e-04 (6.73e-03)	Tok/s 18513 (24214)	Loss/tok 3.1231 (3.4103)	LR 1.563e-05
0: TRAIN [4][90/5173]	Time 0.571 (0.626)	Data 1.20e-04 (6.01e-03)	Tok/s 18113 (24063)	Loss/tok 3.3131 (3.4088)	LR 1.563e-05
0: TRAIN [4][100/5173]	Time 0.638 (0.626)	Data 1.29e-04 (5.43e-03)	Tok/s 26100 (24105)	Loss/tok 3.4690 (3.4106)	LR 1.563e-05
0: TRAIN [4][110/5173]	Time 0.565 (0.623)	Data 1.22e-04 (4.95e-03)	Tok/s 18415 (23833)	Loss/tok 3.1162 (3.4034)	LR 1.563e-05
0: TRAIN [4][120/5173]	Time 0.768 (0.623)	Data 1.24e-04 (4.55e-03)	Tok/s 38449 (23936)	Loss/tok 3.8630 (3.4068)	LR 1.563e-05
0: TRAIN [4][130/5173]	Time 0.564 (0.623)	Data 1.17e-04 (4.21e-03)	Tok/s 18533 (23990)	Loss/tok 3.2901 (3.4057)	LR 1.563e-05
0: TRAIN [4][140/5173]	Time 0.571 (0.621)	Data 1.18e-04 (3.92e-03)	Tok/s 18531 (23818)	Loss/tok 3.1934 (3.3987)	LR 1.563e-05
0: TRAIN [4][150/5173]	Time 0.694 (0.621)	Data 1.20e-04 (3.67e-03)	Tok/s 33317 (23804)	Loss/tok 3.6362 (3.3988)	LR 1.563e-05
0: TRAIN [4][160/5173]	Time 0.566 (0.619)	Data 1.17e-04 (3.45e-03)	Tok/s 18201 (23684)	Loss/tok 3.2294 (3.3959)	LR 1.563e-05
0: TRAIN [4][170/5173]	Time 0.560 (0.618)	Data 1.18e-04 (3.25e-03)	Tok/s 18250 (23607)	Loss/tok 3.1962 (3.3908)	LR 1.563e-05
0: TRAIN [4][180/5173]	Time 0.759 (0.618)	Data 1.19e-04 (3.08e-03)	Tok/s 38873 (23635)	Loss/tok 3.7808 (3.3919)	LR 1.563e-05
0: TRAIN [4][190/5173]	Time 0.629 (0.617)	Data 1.18e-04 (2.93e-03)	Tok/s 26754 (23609)	Loss/tok 3.3564 (3.3905)	LR 1.563e-05
0: TRAIN [4][200/5173]	Time 0.643 (0.616)	Data 1.17e-04 (2.79e-03)	Tok/s 26186 (23452)	Loss/tok 3.3238 (3.3870)	LR 1.563e-05
0: TRAIN [4][210/5173]	Time 0.627 (0.617)	Data 1.32e-04 (2.66e-03)	Tok/s 26921 (23578)	Loss/tok 3.3670 (3.3888)	LR 1.563e-05
0: TRAIN [4][220/5173]	Time 0.701 (0.617)	Data 1.21e-04 (2.55e-03)	Tok/s 33255 (23583)	Loss/tok 3.6265 (3.3883)	LR 1.563e-05
0: TRAIN [4][230/5173]	Time 0.640 (0.616)	Data 1.14e-04 (2.44e-03)	Tok/s 26206 (23470)	Loss/tok 3.3344 (3.3861)	LR 1.563e-05
0: TRAIN [4][240/5173]	Time 0.702 (0.617)	Data 1.27e-04 (2.34e-03)	Tok/s 33209 (23591)	Loss/tok 3.6863 (3.3917)	LR 1.563e-05
0: TRAIN [4][250/5173]	Time 0.569 (0.617)	Data 1.34e-04 (2.26e-03)	Tok/s 18276 (23629)	Loss/tok 3.2223 (3.3928)	LR 1.563e-05
0: TRAIN [4][260/5173]	Time 0.635 (0.617)	Data 3.08e-04 (2.18e-03)	Tok/s 26257 (23627)	Loss/tok 3.4125 (3.3923)	LR 1.563e-05
0: TRAIN [4][270/5173]	Time 0.633 (0.616)	Data 1.25e-04 (2.10e-03)	Tok/s 26231 (23571)	Loss/tok 3.4268 (3.3916)	LR 1.563e-05
0: TRAIN [4][280/5173]	Time 0.564 (0.616)	Data 1.29e-04 (2.03e-03)	Tok/s 18401 (23549)	Loss/tok 3.3024 (3.3913)	LR 1.563e-05
0: TRAIN [4][290/5173]	Time 0.494 (0.616)	Data 1.24e-04 (1.97e-03)	Tok/s 10621 (23621)	Loss/tok 2.7869 (3.3916)	LR 1.563e-05
0: TRAIN [4][300/5173]	Time 0.567 (0.616)	Data 1.24e-04 (1.91e-03)	Tok/s 18093 (23634)	Loss/tok 3.0904 (3.3936)	LR 1.563e-05
0: TRAIN [4][310/5173]	Time 0.693 (0.616)	Data 1.26e-04 (1.85e-03)	Tok/s 33473 (23669)	Loss/tok 3.6644 (3.3940)	LR 1.563e-05
0: TRAIN [4][320/5173]	Time 0.703 (0.616)	Data 1.22e-04 (1.80e-03)	Tok/s 33236 (23628)	Loss/tok 3.5845 (3.3923)	LR 1.563e-05
0: TRAIN [4][330/5173]	Time 0.573 (0.615)	Data 1.29e-04 (1.74e-03)	Tok/s 18026 (23582)	Loss/tok 3.0136 (3.3907)	LR 1.563e-05
0: TRAIN [4][340/5173]	Time 0.764 (0.616)	Data 1.32e-04 (1.70e-03)	Tok/s 39079 (23646)	Loss/tok 3.6996 (3.3933)	LR 1.563e-05
0: TRAIN [4][350/5173]	Time 0.632 (0.616)	Data 1.26e-04 (1.65e-03)	Tok/s 26442 (23663)	Loss/tok 3.2297 (3.3938)	LR 1.563e-05
0: TRAIN [4][360/5173]	Time 0.567 (0.615)	Data 1.21e-04 (1.61e-03)	Tok/s 18098 (23557)	Loss/tok 3.1014 (3.3913)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][370/5173]	Time 0.573 (0.615)	Data 1.28e-04 (1.57e-03)	Tok/s 17860 (23579)	Loss/tok 3.0803 (3.3919)	LR 1.563e-05
0: TRAIN [4][380/5173]	Time 0.643 (0.616)	Data 1.24e-04 (1.53e-03)	Tok/s 26242 (23625)	Loss/tok 3.2860 (3.3921)	LR 1.563e-05
0: TRAIN [4][390/5173]	Time 0.498 (0.615)	Data 1.24e-04 (1.50e-03)	Tok/s 10494 (23510)	Loss/tok 2.9123 (3.3904)	LR 1.563e-05
0: TRAIN [4][400/5173]	Time 0.564 (0.614)	Data 1.36e-04 (1.46e-03)	Tok/s 18510 (23439)	Loss/tok 3.2000 (3.3888)	LR 1.563e-05
0: TRAIN [4][410/5173]	Time 0.628 (0.614)	Data 1.25e-04 (1.43e-03)	Tok/s 27040 (23444)	Loss/tok 3.2447 (3.3893)	LR 1.563e-05
0: TRAIN [4][420/5173]	Time 0.559 (0.613)	Data 1.30e-04 (1.40e-03)	Tok/s 18234 (23342)	Loss/tok 3.1603 (3.3862)	LR 1.563e-05
0: TRAIN [4][430/5173]	Time 0.567 (0.613)	Data 1.36e-04 (1.37e-03)	Tok/s 18299 (23367)	Loss/tok 3.0853 (3.3858)	LR 1.563e-05
0: TRAIN [4][440/5173]	Time 0.635 (0.613)	Data 1.34e-04 (1.34e-03)	Tok/s 26579 (23390)	Loss/tok 3.3148 (3.3856)	LR 1.563e-05
0: TRAIN [4][450/5173]	Time 0.498 (0.613)	Data 1.34e-04 (1.32e-03)	Tok/s 10720 (23378)	Loss/tok 2.7914 (3.3860)	LR 1.563e-05
0: TRAIN [4][460/5173]	Time 0.702 (0.613)	Data 1.32e-04 (1.29e-03)	Tok/s 32743 (23386)	Loss/tok 3.7556 (3.3859)	LR 1.563e-05
0: TRAIN [4][470/5173]	Time 0.565 (0.614)	Data 1.22e-04 (1.27e-03)	Tok/s 17985 (23433)	Loss/tok 3.1433 (3.3882)	LR 1.563e-05
0: TRAIN [4][480/5173]	Time 0.566 (0.613)	Data 1.24e-04 (1.24e-03)	Tok/s 18306 (23337)	Loss/tok 3.1917 (3.3865)	LR 1.563e-05
0: TRAIN [4][490/5173]	Time 0.705 (0.613)	Data 1.25e-04 (1.22e-03)	Tok/s 33232 (23368)	Loss/tok 3.6139 (3.3882)	LR 1.563e-05
0: TRAIN [4][500/5173]	Time 0.706 (0.613)	Data 1.34e-04 (1.20e-03)	Tok/s 33225 (23361)	Loss/tok 3.5224 (3.3880)	LR 1.563e-05
0: TRAIN [4][510/5173]	Time 0.628 (0.613)	Data 1.21e-04 (1.18e-03)	Tok/s 26726 (23352)	Loss/tok 3.4160 (3.3870)	LR 1.563e-05
0: TRAIN [4][520/5173]	Time 0.636 (0.613)	Data 1.35e-04 (1.16e-03)	Tok/s 27004 (23362)	Loss/tok 3.4074 (3.3866)	LR 1.563e-05
0: TRAIN [4][530/5173]	Time 0.566 (0.612)	Data 1.76e-04 (1.14e-03)	Tok/s 18193 (23310)	Loss/tok 3.1938 (3.3852)	LR 1.563e-05
0: TRAIN [4][540/5173]	Time 0.555 (0.612)	Data 1.20e-04 (1.12e-03)	Tok/s 19004 (23315)	Loss/tok 3.2687 (3.3859)	LR 1.563e-05
0: TRAIN [4][550/5173]	Time 0.566 (0.612)	Data 1.19e-04 (1.10e-03)	Tok/s 18026 (23272)	Loss/tok 3.0920 (3.3837)	LR 1.563e-05
0: TRAIN [4][560/5173]	Time 0.642 (0.612)	Data 1.25e-04 (1.09e-03)	Tok/s 26138 (23339)	Loss/tok 3.4646 (3.3844)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][570/5173]	Time 0.636 (0.612)	Data 1.25e-04 (1.07e-03)	Tok/s 25919 (23353)	Loss/tok 3.4290 (3.3850)	LR 1.563e-05
0: TRAIN [4][580/5173]	Time 0.635 (0.612)	Data 1.18e-04 (1.05e-03)	Tok/s 26383 (23319)	Loss/tok 3.4161 (3.3841)	LR 1.563e-05
0: TRAIN [4][590/5173]	Time 0.634 (0.612)	Data 1.19e-04 (1.04e-03)	Tok/s 26554 (23313)	Loss/tok 3.3966 (3.3827)	LR 1.563e-05
0: TRAIN [4][600/5173]	Time 0.568 (0.612)	Data 1.21e-04 (1.02e-03)	Tok/s 18063 (23305)	Loss/tok 3.1724 (3.3825)	LR 1.563e-05
0: TRAIN [4][610/5173]	Time 0.694 (0.613)	Data 1.19e-04 (1.01e-03)	Tok/s 33771 (23377)	Loss/tok 3.5322 (3.3849)	LR 1.563e-05
0: TRAIN [4][620/5173]	Time 0.630 (0.613)	Data 1.24e-04 (9.94e-04)	Tok/s 26250 (23400)	Loss/tok 3.4607 (3.3850)	LR 1.563e-05
0: TRAIN [4][630/5173]	Time 0.638 (0.613)	Data 1.28e-04 (9.80e-04)	Tok/s 26587 (23389)	Loss/tok 3.2617 (3.3843)	LR 1.563e-05
0: TRAIN [4][640/5173]	Time 0.689 (0.613)	Data 1.20e-04 (9.66e-04)	Tok/s 34389 (23439)	Loss/tok 3.5775 (3.3860)	LR 1.563e-05
0: TRAIN [4][650/5173]	Time 0.570 (0.613)	Data 1.21e-04 (9.53e-04)	Tok/s 18007 (23456)	Loss/tok 3.0548 (3.3863)	LR 1.563e-05
0: TRAIN [4][660/5173]	Time 0.561 (0.613)	Data 1.22e-04 (9.41e-04)	Tok/s 18287 (23497)	Loss/tok 3.2729 (3.3867)	LR 1.563e-05
0: TRAIN [4][670/5173]	Time 0.696 (0.613)	Data 1.21e-04 (9.29e-04)	Tok/s 33897 (23500)	Loss/tok 3.4674 (3.3861)	LR 1.563e-05
0: TRAIN [4][680/5173]	Time 0.700 (0.614)	Data 1.20e-04 (9.17e-04)	Tok/s 33010 (23572)	Loss/tok 3.6322 (3.3882)	LR 1.563e-05
0: TRAIN [4][690/5173]	Time 0.565 (0.614)	Data 1.31e-04 (9.05e-04)	Tok/s 17972 (23542)	Loss/tok 3.1639 (3.3872)	LR 1.563e-05
0: TRAIN [4][700/5173]	Time 0.572 (0.614)	Data 1.25e-04 (8.94e-04)	Tok/s 17988 (23566)	Loss/tok 3.1727 (3.3880)	LR 1.563e-05
0: TRAIN [4][710/5173]	Time 0.566 (0.613)	Data 1.22e-04 (8.83e-04)	Tok/s 18518 (23517)	Loss/tok 3.0397 (3.3866)	LR 1.563e-05
0: TRAIN [4][720/5173]	Time 0.565 (0.613)	Data 2.88e-04 (8.73e-04)	Tok/s 18175 (23488)	Loss/tok 3.2526 (3.3861)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][730/5173]	Time 0.636 (0.613)	Data 1.34e-04 (8.63e-04)	Tok/s 26277 (23521)	Loss/tok 3.3960 (3.3868)	LR 1.563e-05
0: TRAIN [4][740/5173]	Time 0.568 (0.613)	Data 1.27e-04 (8.53e-04)	Tok/s 18368 (23463)	Loss/tok 3.1600 (3.3848)	LR 1.563e-05
0: TRAIN [4][750/5173]	Time 0.767 (0.613)	Data 1.29e-04 (8.43e-04)	Tok/s 38994 (23490)	Loss/tok 3.7089 (3.3865)	LR 1.563e-05
0: TRAIN [4][760/5173]	Time 0.556 (0.613)	Data 1.23e-04 (8.34e-04)	Tok/s 18728 (23498)	Loss/tok 3.1721 (3.3872)	LR 1.563e-05
0: TRAIN [4][770/5173]	Time 0.688 (0.613)	Data 3.15e-04 (8.25e-04)	Tok/s 34192 (23499)	Loss/tok 3.4971 (3.3873)	LR 1.563e-05
0: TRAIN [4][780/5173]	Time 0.564 (0.613)	Data 1.27e-04 (8.16e-04)	Tok/s 18006 (23534)	Loss/tok 3.2084 (3.3885)	LR 1.563e-05
0: TRAIN [4][790/5173]	Time 0.495 (0.613)	Data 1.23e-04 (8.08e-04)	Tok/s 10584 (23508)	Loss/tok 2.7739 (3.3880)	LR 1.563e-05
0: TRAIN [4][800/5173]	Time 0.562 (0.613)	Data 1.22e-04 (8.00e-04)	Tok/s 18735 (23490)	Loss/tok 3.2181 (3.3877)	LR 1.563e-05
0: TRAIN [4][810/5173]	Time 0.502 (0.613)	Data 1.24e-04 (7.91e-04)	Tok/s 10483 (23474)	Loss/tok 2.6971 (3.3873)	LR 1.563e-05
0: TRAIN [4][820/5173]	Time 0.630 (0.613)	Data 1.26e-04 (7.83e-04)	Tok/s 26595 (23459)	Loss/tok 3.4348 (3.3864)	LR 1.563e-05
0: TRAIN [4][830/5173]	Time 0.629 (0.613)	Data 1.28e-04 (7.76e-04)	Tok/s 26742 (23486)	Loss/tok 3.3631 (3.3877)	LR 1.563e-05
0: TRAIN [4][840/5173]	Time 0.625 (0.613)	Data 1.25e-04 (7.69e-04)	Tok/s 26958 (23525)	Loss/tok 3.3677 (3.3879)	LR 1.563e-05
0: TRAIN [4][850/5173]	Time 0.703 (0.613)	Data 1.20e-04 (7.61e-04)	Tok/s 32803 (23534)	Loss/tok 3.6548 (3.3886)	LR 1.563e-05
0: TRAIN [4][860/5173]	Time 0.567 (0.613)	Data 3.01e-04 (7.54e-04)	Tok/s 18101 (23532)	Loss/tok 3.1109 (3.3891)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][870/5173]	Time 0.576 (0.613)	Data 3.03e-04 (7.47e-04)	Tok/s 18264 (23514)	Loss/tok 3.0810 (3.3886)	LR 1.563e-05
0: TRAIN [4][880/5173]	Time 0.502 (0.613)	Data 1.25e-04 (7.40e-04)	Tok/s 10430 (23525)	Loss/tok 2.7549 (3.3891)	LR 1.563e-05
0: TRAIN [4][890/5173]	Time 0.563 (0.613)	Data 1.25e-04 (7.33e-04)	Tok/s 18260 (23483)	Loss/tok 3.2264 (3.3884)	LR 1.563e-05
0: TRAIN [4][900/5173]	Time 0.568 (0.613)	Data 1.26e-04 (7.27e-04)	Tok/s 18632 (23462)	Loss/tok 3.1133 (3.3876)	LR 1.563e-05
0: TRAIN [4][910/5173]	Time 0.569 (0.613)	Data 1.24e-04 (7.20e-04)	Tok/s 18159 (23472)	Loss/tok 3.1625 (3.3875)	LR 1.563e-05
0: TRAIN [4][920/5173]	Time 0.502 (0.613)	Data 1.24e-04 (7.14e-04)	Tok/s 10479 (23480)	Loss/tok 2.7407 (3.3879)	LR 1.563e-05
0: TRAIN [4][930/5173]	Time 0.503 (0.613)	Data 3.28e-04 (7.08e-04)	Tok/s 10589 (23482)	Loss/tok 2.6909 (3.3882)	LR 1.563e-05
0: TRAIN [4][940/5173]	Time 0.640 (0.613)	Data 1.39e-04 (7.02e-04)	Tok/s 26229 (23469)	Loss/tok 3.4217 (3.3879)	LR 1.563e-05
0: TRAIN [4][950/5173]	Time 0.503 (0.613)	Data 1.37e-04 (6.96e-04)	Tok/s 10674 (23447)	Loss/tok 2.7277 (3.3873)	LR 1.563e-05
0: TRAIN [4][960/5173]	Time 0.567 (0.613)	Data 1.31e-04 (6.90e-04)	Tok/s 17644 (23437)	Loss/tok 2.9960 (3.3872)	LR 1.563e-05
0: TRAIN [4][970/5173]	Time 0.566 (0.612)	Data 1.25e-04 (6.84e-04)	Tok/s 18663 (23419)	Loss/tok 3.1213 (3.3862)	LR 1.563e-05
0: TRAIN [4][980/5173]	Time 0.639 (0.613)	Data 1.20e-04 (6.79e-04)	Tok/s 26391 (23467)	Loss/tok 3.2095 (3.3870)	LR 1.563e-05
0: TRAIN [4][990/5173]	Time 0.626 (0.613)	Data 1.18e-04 (6.74e-04)	Tok/s 26754 (23455)	Loss/tok 3.2624 (3.3864)	LR 1.563e-05
0: TRAIN [4][1000/5173]	Time 0.564 (0.613)	Data 1.32e-04 (6.68e-04)	Tok/s 18443 (23452)	Loss/tok 3.2836 (3.3861)	LR 1.563e-05
0: TRAIN [4][1010/5173]	Time 0.572 (0.612)	Data 3.14e-04 (6.63e-04)	Tok/s 18261 (23430)	Loss/tok 3.1059 (3.3857)	LR 1.563e-05
0: TRAIN [4][1020/5173]	Time 0.774 (0.613)	Data 1.25e-04 (6.58e-04)	Tok/s 37987 (23451)	Loss/tok 3.8115 (3.3866)	LR 1.563e-05
0: TRAIN [4][1030/5173]	Time 0.645 (0.613)	Data 1.24e-04 (6.53e-04)	Tok/s 26071 (23436)	Loss/tok 3.3189 (3.3858)	LR 1.563e-05
0: TRAIN [4][1040/5173]	Time 0.564 (0.613)	Data 1.27e-04 (6.48e-04)	Tok/s 18215 (23468)	Loss/tok 3.1866 (3.3869)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][1050/5173]	Time 0.503 (0.613)	Data 1.31e-04 (6.43e-04)	Tok/s 10681 (23487)	Loss/tok 2.7012 (3.3884)	LR 1.563e-05
0: TRAIN [4][1060/5173]	Time 0.565 (0.613)	Data 1.26e-04 (6.38e-04)	Tok/s 18247 (23454)	Loss/tok 3.2095 (3.3875)	LR 1.563e-05
0: TRAIN [4][1070/5173]	Time 0.702 (0.613)	Data 1.30e-04 (6.34e-04)	Tok/s 32983 (23490)	Loss/tok 3.6059 (3.3887)	LR 1.563e-05
0: TRAIN [4][1080/5173]	Time 0.566 (0.613)	Data 1.22e-04 (6.29e-04)	Tok/s 18371 (23485)	Loss/tok 3.2665 (3.3883)	LR 1.563e-05
0: TRAIN [4][1090/5173]	Time 0.578 (0.613)	Data 1.24e-04 (6.25e-04)	Tok/s 18236 (23473)	Loss/tok 3.1283 (3.3878)	LR 1.563e-05
0: TRAIN [4][1100/5173]	Time 0.569 (0.613)	Data 1.23e-04 (6.20e-04)	Tok/s 17943 (23475)	Loss/tok 3.0880 (3.3876)	LR 1.563e-05
0: TRAIN [4][1110/5173]	Time 0.567 (0.613)	Data 1.22e-04 (6.16e-04)	Tok/s 18326 (23474)	Loss/tok 3.2196 (3.3878)	LR 1.563e-05
0: TRAIN [4][1120/5173]	Time 0.639 (0.613)	Data 1.28e-04 (6.11e-04)	Tok/s 26649 (23471)	Loss/tok 3.4787 (3.3877)	LR 1.563e-05
0: TRAIN [4][1130/5173]	Time 0.638 (0.613)	Data 1.23e-04 (6.07e-04)	Tok/s 26457 (23460)	Loss/tok 3.3967 (3.3872)	LR 1.563e-05
0: TRAIN [4][1140/5173]	Time 0.639 (0.613)	Data 1.26e-04 (6.03e-04)	Tok/s 26023 (23478)	Loss/tok 3.3347 (3.3873)	LR 1.563e-05
0: TRAIN [4][1150/5173]	Time 0.706 (0.613)	Data 1.25e-04 (5.99e-04)	Tok/s 32853 (23484)	Loss/tok 3.6244 (3.3879)	LR 1.563e-05
0: TRAIN [4][1160/5173]	Time 0.642 (0.613)	Data 1.29e-04 (5.95e-04)	Tok/s 26007 (23461)	Loss/tok 3.3972 (3.3870)	LR 1.563e-05
0: TRAIN [4][1170/5173]	Time 0.570 (0.612)	Data 1.22e-04 (5.91e-04)	Tok/s 17899 (23423)	Loss/tok 3.0927 (3.3861)	LR 1.563e-05
0: TRAIN [4][1180/5173]	Time 0.641 (0.612)	Data 1.28e-04 (5.87e-04)	Tok/s 26036 (23400)	Loss/tok 3.4804 (3.3850)	LR 1.563e-05
0: TRAIN [4][1190/5173]	Time 0.564 (0.612)	Data 1.29e-04 (5.83e-04)	Tok/s 18363 (23384)	Loss/tok 3.1859 (3.3849)	LR 1.563e-05
0: TRAIN [4][1200/5173]	Time 0.567 (0.612)	Data 1.34e-04 (5.79e-04)	Tok/s 17856 (23387)	Loss/tok 3.2554 (3.3850)	LR 1.563e-05
0: TRAIN [4][1210/5173]	Time 0.562 (0.612)	Data 1.31e-04 (5.75e-04)	Tok/s 18018 (23389)	Loss/tok 3.2174 (3.3848)	LR 1.563e-05
0: TRAIN [4][1220/5173]	Time 0.569 (0.612)	Data 1.25e-04 (5.72e-04)	Tok/s 18399 (23401)	Loss/tok 3.2075 (3.3845)	LR 1.563e-05
0: TRAIN [4][1230/5173]	Time 0.628 (0.612)	Data 1.24e-04 (5.68e-04)	Tok/s 26818 (23398)	Loss/tok 3.4218 (3.3843)	LR 1.563e-05
0: TRAIN [4][1240/5173]	Time 0.565 (0.612)	Data 1.24e-04 (5.65e-04)	Tok/s 18508 (23389)	Loss/tok 3.1823 (3.3841)	LR 1.563e-05
0: TRAIN [4][1250/5173]	Time 0.635 (0.612)	Data 1.29e-04 (5.61e-04)	Tok/s 26438 (23390)	Loss/tok 3.4872 (3.3842)	LR 1.563e-05
0: TRAIN [4][1260/5173]	Time 0.562 (0.612)	Data 1.24e-04 (5.58e-04)	Tok/s 18298 (23385)	Loss/tok 3.1522 (3.3841)	LR 1.563e-05
0: TRAIN [4][1270/5173]	Time 0.568 (0.612)	Data 1.24e-04 (5.55e-04)	Tok/s 18160 (23364)	Loss/tok 3.2377 (3.3835)	LR 1.563e-05
0: TRAIN [4][1280/5173]	Time 0.566 (0.612)	Data 1.22e-04 (5.51e-04)	Tok/s 18228 (23354)	Loss/tok 3.1662 (3.3828)	LR 1.563e-05
0: TRAIN [4][1290/5173]	Time 0.642 (0.612)	Data 1.26e-04 (5.48e-04)	Tok/s 26008 (23380)	Loss/tok 3.3958 (3.3835)	LR 1.563e-05
0: TRAIN [4][1300/5173]	Time 0.628 (0.612)	Data 1.20e-04 (5.45e-04)	Tok/s 26774 (23388)	Loss/tok 3.4009 (3.3835)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1310/5173]	Time 0.637 (0.612)	Data 1.21e-04 (5.42e-04)	Tok/s 26500 (23414)	Loss/tok 3.3789 (3.3842)	LR 1.563e-05
0: TRAIN [4][1320/5173]	Time 0.501 (0.612)	Data 1.25e-04 (5.39e-04)	Tok/s 10540 (23423)	Loss/tok 2.6776 (3.3846)	LR 1.563e-05
0: TRAIN [4][1330/5173]	Time 0.627 (0.612)	Data 1.23e-04 (5.36e-04)	Tok/s 27513 (23420)	Loss/tok 3.3602 (3.3846)	LR 1.563e-05
0: TRAIN [4][1340/5173]	Time 0.559 (0.612)	Data 1.25e-04 (5.33e-04)	Tok/s 18224 (23417)	Loss/tok 3.2272 (3.3843)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][1350/5173]	Time 0.499 (0.612)	Data 1.25e-04 (5.30e-04)	Tok/s 10649 (23407)	Loss/tok 2.7145 (3.3845)	LR 1.563e-05
0: TRAIN [4][1360/5173]	Time 0.568 (0.612)	Data 3.16e-04 (5.27e-04)	Tok/s 17851 (23397)	Loss/tok 3.1695 (3.3842)	LR 1.563e-05
0: TRAIN [4][1370/5173]	Time 0.643 (0.612)	Data 1.22e-04 (5.24e-04)	Tok/s 26280 (23371)	Loss/tok 3.3636 (3.3835)	LR 1.563e-05
0: TRAIN [4][1380/5173]	Time 0.509 (0.612)	Data 1.24e-04 (5.22e-04)	Tok/s 10536 (23349)	Loss/tok 2.7197 (3.3828)	LR 1.563e-05
0: TRAIN [4][1390/5173]	Time 0.640 (0.612)	Data 1.30e-04 (5.19e-04)	Tok/s 25798 (23344)	Loss/tok 3.4170 (3.3828)	LR 1.563e-05
0: TRAIN [4][1400/5173]	Time 0.701 (0.611)	Data 1.26e-04 (5.16e-04)	Tok/s 33285 (23330)	Loss/tok 3.5321 (3.3822)	LR 1.563e-05
0: TRAIN [4][1410/5173]	Time 0.505 (0.612)	Data 1.35e-04 (5.13e-04)	Tok/s 10303 (23342)	Loss/tok 2.7336 (3.3828)	LR 1.563e-05
0: TRAIN [4][1420/5173]	Time 0.567 (0.611)	Data 1.26e-04 (5.10e-04)	Tok/s 18140 (23324)	Loss/tok 3.1712 (3.3820)	LR 1.563e-05
0: TRAIN [4][1430/5173]	Time 0.641 (0.611)	Data 1.22e-04 (5.08e-04)	Tok/s 26554 (23327)	Loss/tok 3.4625 (3.3817)	LR 1.563e-05
0: TRAIN [4][1440/5173]	Time 0.565 (0.611)	Data 1.27e-04 (5.05e-04)	Tok/s 18222 (23316)	Loss/tok 3.2002 (3.3819)	LR 1.563e-05
0: TRAIN [4][1450/5173]	Time 0.629 (0.611)	Data 1.37e-04 (5.03e-04)	Tok/s 26793 (23310)	Loss/tok 3.3223 (3.3823)	LR 1.563e-05
0: TRAIN [4][1460/5173]	Time 0.504 (0.611)	Data 1.19e-04 (5.00e-04)	Tok/s 10517 (23287)	Loss/tok 2.8251 (3.3814)	LR 1.563e-05
0: TRAIN [4][1470/5173]	Time 0.639 (0.611)	Data 1.34e-04 (4.98e-04)	Tok/s 26417 (23301)	Loss/tok 3.3866 (3.3817)	LR 1.563e-05
0: TRAIN [4][1480/5173]	Time 0.568 (0.611)	Data 1.29e-04 (4.95e-04)	Tok/s 18088 (23302)	Loss/tok 3.2098 (3.3817)	LR 1.563e-05
0: TRAIN [4][1490/5173]	Time 0.497 (0.611)	Data 1.37e-04 (4.93e-04)	Tok/s 10648 (23294)	Loss/tok 2.7325 (3.3815)	LR 1.563e-05
0: TRAIN [4][1500/5173]	Time 0.505 (0.611)	Data 3.07e-04 (4.90e-04)	Tok/s 10215 (23265)	Loss/tok 2.6446 (3.3809)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][1510/5173]	Time 0.563 (0.611)	Data 1.46e-04 (4.88e-04)	Tok/s 18212 (23265)	Loss/tok 3.2139 (3.3815)	LR 1.563e-05
0: TRAIN [4][1520/5173]	Time 0.499 (0.611)	Data 1.27e-04 (4.86e-04)	Tok/s 10621 (23276)	Loss/tok 2.7170 (3.3814)	LR 1.563e-05
0: TRAIN [4][1530/5173]	Time 0.695 (0.611)	Data 1.30e-04 (4.83e-04)	Tok/s 34280 (23302)	Loss/tok 3.4788 (3.3822)	LR 1.563e-05
0: TRAIN [4][1540/5173]	Time 0.566 (0.611)	Data 1.24e-04 (4.81e-04)	Tok/s 17869 (23294)	Loss/tok 3.1398 (3.3820)	LR 1.563e-05
0: TRAIN [4][1550/5173]	Time 0.644 (0.611)	Data 1.24e-04 (4.79e-04)	Tok/s 26325 (23293)	Loss/tok 3.3288 (3.3815)	LR 1.563e-05
0: TRAIN [4][1560/5173]	Time 0.703 (0.611)	Data 1.28e-04 (4.76e-04)	Tok/s 33140 (23321)	Loss/tok 3.5752 (3.3818)	LR 1.563e-05
0: TRAIN [4][1570/5173]	Time 0.628 (0.611)	Data 1.23e-04 (4.74e-04)	Tok/s 26527 (23310)	Loss/tok 3.3818 (3.3812)	LR 1.563e-05
0: TRAIN [4][1580/5173]	Time 0.568 (0.611)	Data 1.25e-04 (4.72e-04)	Tok/s 17936 (23287)	Loss/tok 3.2151 (3.3806)	LR 1.563e-05
0: TRAIN [4][1590/5173]	Time 0.627 (0.611)	Data 1.20e-04 (4.70e-04)	Tok/s 26801 (23274)	Loss/tok 3.4201 (3.3805)	LR 1.563e-05
0: TRAIN [4][1600/5173]	Time 0.645 (0.611)	Data 1.25e-04 (4.68e-04)	Tok/s 26195 (23290)	Loss/tok 3.4552 (3.3810)	LR 1.563e-05
0: TRAIN [4][1610/5173]	Time 0.502 (0.611)	Data 3.23e-04 (4.66e-04)	Tok/s 10306 (23296)	Loss/tok 2.7500 (3.3812)	LR 1.563e-05
0: TRAIN [4][1620/5173]	Time 0.569 (0.611)	Data 1.24e-04 (4.64e-04)	Tok/s 18302 (23284)	Loss/tok 3.2960 (3.3809)	LR 1.563e-05
0: TRAIN [4][1630/5173]	Time 0.642 (0.611)	Data 1.29e-04 (4.62e-04)	Tok/s 25991 (23270)	Loss/tok 3.4429 (3.3809)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][1640/5173]	Time 0.562 (0.611)	Data 1.14e-04 (4.60e-04)	Tok/s 18566 (23280)	Loss/tok 3.1553 (3.3814)	LR 1.563e-05
0: TRAIN [4][1650/5173]	Time 0.638 (0.611)	Data 1.26e-04 (4.58e-04)	Tok/s 26088 (23289)	Loss/tok 3.3625 (3.3815)	LR 1.563e-05
0: TRAIN [4][1660/5173]	Time 0.641 (0.611)	Data 1.29e-04 (4.56e-04)	Tok/s 26158 (23312)	Loss/tok 3.3198 (3.3822)	LR 1.563e-05
0: TRAIN [4][1670/5173]	Time 0.500 (0.611)	Data 1.24e-04 (4.54e-04)	Tok/s 10794 (23293)	Loss/tok 2.6891 (3.3816)	LR 1.563e-05
0: TRAIN [4][1680/5173]	Time 0.567 (0.611)	Data 1.25e-04 (4.52e-04)	Tok/s 18379 (23300)	Loss/tok 3.1352 (3.3817)	LR 1.563e-05
0: TRAIN [4][1690/5173]	Time 0.644 (0.611)	Data 1.19e-04 (4.51e-04)	Tok/s 26009 (23290)	Loss/tok 3.4001 (3.3813)	LR 1.563e-05
0: TRAIN [4][1700/5173]	Time 0.640 (0.611)	Data 1.24e-04 (4.49e-04)	Tok/s 26185 (23308)	Loss/tok 3.3744 (3.3820)	LR 1.563e-05
0: TRAIN [4][1710/5173]	Time 0.639 (0.612)	Data 1.32e-04 (4.47e-04)	Tok/s 26439 (23324)	Loss/tok 3.4867 (3.3823)	LR 1.563e-05
0: TRAIN [4][1720/5173]	Time 0.704 (0.611)	Data 1.28e-04 (4.45e-04)	Tok/s 33021 (23321)	Loss/tok 3.5899 (3.3820)	LR 1.563e-05
0: TRAIN [4][1730/5173]	Time 0.637 (0.612)	Data 1.31e-04 (4.43e-04)	Tok/s 26396 (23328)	Loss/tok 3.3909 (3.3819)	LR 1.563e-05
0: TRAIN [4][1740/5173]	Time 0.627 (0.612)	Data 1.27e-04 (4.42e-04)	Tok/s 26552 (23337)	Loss/tok 3.4208 (3.3824)	LR 1.563e-05
0: TRAIN [4][1750/5173]	Time 0.562 (0.612)	Data 1.26e-04 (4.40e-04)	Tok/s 18383 (23340)	Loss/tok 3.1216 (3.3823)	LR 1.563e-05
0: TRAIN [4][1760/5173]	Time 0.565 (0.611)	Data 1.27e-04 (4.38e-04)	Tok/s 18539 (23312)	Loss/tok 3.2497 (3.3814)	LR 1.563e-05
0: TRAIN [4][1770/5173]	Time 0.563 (0.612)	Data 1.27e-04 (4.36e-04)	Tok/s 18460 (23327)	Loss/tok 3.1682 (3.3817)	LR 1.563e-05
0: TRAIN [4][1780/5173]	Time 0.567 (0.611)	Data 1.31e-04 (4.34e-04)	Tok/s 18049 (23308)	Loss/tok 3.1250 (3.3811)	LR 1.563e-05
0: TRAIN [4][1790/5173]	Time 0.571 (0.611)	Data 1.25e-04 (4.33e-04)	Tok/s 17964 (23280)	Loss/tok 3.1598 (3.3803)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][1800/5173]	Time 0.575 (0.611)	Data 1.25e-04 (4.31e-04)	Tok/s 18076 (23277)	Loss/tok 3.1866 (3.3805)	LR 1.563e-05
0: TRAIN [4][1810/5173]	Time 0.566 (0.611)	Data 1.20e-04 (4.30e-04)	Tok/s 18067 (23254)	Loss/tok 3.1625 (3.3801)	LR 1.563e-05
0: TRAIN [4][1820/5173]	Time 0.767 (0.611)	Data 1.27e-04 (4.28e-04)	Tok/s 39060 (23251)	Loss/tok 3.6861 (3.3801)	LR 1.563e-05
0: TRAIN [4][1830/5173]	Time 0.499 (0.611)	Data 1.20e-04 (4.26e-04)	Tok/s 10629 (23257)	Loss/tok 2.7100 (3.3803)	LR 1.563e-05
0: TRAIN [4][1840/5173]	Time 0.696 (0.611)	Data 1.26e-04 (4.25e-04)	Tok/s 33502 (23264)	Loss/tok 3.5983 (3.3805)	LR 1.563e-05
0: TRAIN [4][1850/5173]	Time 0.566 (0.611)	Data 1.24e-04 (4.23e-04)	Tok/s 18147 (23255)	Loss/tok 3.2053 (3.3803)	LR 1.563e-05
0: TRAIN [4][1860/5173]	Time 0.564 (0.611)	Data 1.29e-04 (4.22e-04)	Tok/s 18287 (23254)	Loss/tok 3.1205 (3.3800)	LR 1.563e-05
0: TRAIN [4][1870/5173]	Time 0.710 (0.611)	Data 1.20e-04 (4.20e-04)	Tok/s 32936 (23274)	Loss/tok 3.5738 (3.3809)	LR 1.563e-05
0: TRAIN [4][1880/5173]	Time 0.569 (0.611)	Data 1.31e-04 (4.19e-04)	Tok/s 18100 (23275)	Loss/tok 3.2999 (3.3815)	LR 1.563e-05
0: TRAIN [4][1890/5173]	Time 0.640 (0.611)	Data 1.22e-04 (4.17e-04)	Tok/s 26406 (23289)	Loss/tok 3.4139 (3.3821)	LR 1.563e-05
0: TRAIN [4][1900/5173]	Time 0.507 (0.611)	Data 1.27e-04 (4.16e-04)	Tok/s 10403 (23283)	Loss/tok 2.6342 (3.3820)	LR 1.563e-05
0: TRAIN [4][1910/5173]	Time 0.634 (0.611)	Data 1.27e-04 (4.14e-04)	Tok/s 26353 (23297)	Loss/tok 3.2481 (3.3820)	LR 1.563e-05
0: TRAIN [4][1920/5173]	Time 0.562 (0.611)	Data 1.25e-04 (4.13e-04)	Tok/s 18754 (23295)	Loss/tok 3.1533 (3.3819)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][1930/5173]	Time 0.641 (0.611)	Data 1.26e-04 (4.11e-04)	Tok/s 26185 (23300)	Loss/tok 3.3953 (3.3822)	LR 1.563e-05
0: TRAIN [4][1940/5173]	Time 0.499 (0.611)	Data 1.25e-04 (4.10e-04)	Tok/s 10429 (23286)	Loss/tok 2.6874 (3.3817)	LR 1.563e-05
0: TRAIN [4][1950/5173]	Time 0.626 (0.611)	Data 1.24e-04 (4.08e-04)	Tok/s 26766 (23297)	Loss/tok 3.3320 (3.3817)	LR 1.563e-05
0: TRAIN [4][1960/5173]	Time 0.501 (0.611)	Data 1.31e-04 (4.07e-04)	Tok/s 10723 (23286)	Loss/tok 2.6815 (3.3815)	LR 1.563e-05
0: TRAIN [4][1970/5173]	Time 0.624 (0.611)	Data 1.29e-04 (4.06e-04)	Tok/s 27015 (23292)	Loss/tok 3.4336 (3.3817)	LR 1.563e-05
0: TRAIN [4][1980/5173]	Time 0.573 (0.611)	Data 1.27e-04 (4.04e-04)	Tok/s 18138 (23262)	Loss/tok 3.0782 (3.3810)	LR 1.563e-05
0: TRAIN [4][1990/5173]	Time 0.500 (0.611)	Data 1.21e-04 (4.03e-04)	Tok/s 10688 (23239)	Loss/tok 2.6769 (3.3807)	LR 1.563e-05
0: TRAIN [4][2000/5173]	Time 0.567 (0.611)	Data 1.23e-04 (4.02e-04)	Tok/s 18029 (23236)	Loss/tok 3.1373 (3.3807)	LR 1.563e-05
0: TRAIN [4][2010/5173]	Time 0.705 (0.611)	Data 1.24e-04 (4.00e-04)	Tok/s 32916 (23240)	Loss/tok 3.5834 (3.3809)	LR 1.563e-05
0: TRAIN [4][2020/5173]	Time 0.570 (0.611)	Data 1.25e-04 (3.99e-04)	Tok/s 17975 (23239)	Loss/tok 3.2008 (3.3808)	LR 1.563e-05
0: TRAIN [4][2030/5173]	Time 0.701 (0.611)	Data 1.87e-04 (3.98e-04)	Tok/s 33219 (23222)	Loss/tok 3.6416 (3.3806)	LR 1.563e-05
0: TRAIN [4][2040/5173]	Time 0.695 (0.611)	Data 1.26e-04 (3.97e-04)	Tok/s 33300 (23216)	Loss/tok 3.5440 (3.3803)	LR 1.563e-05
0: TRAIN [4][2050/5173]	Time 0.570 (0.611)	Data 1.65e-04 (3.95e-04)	Tok/s 18485 (23238)	Loss/tok 3.1203 (3.3811)	LR 1.563e-05
0: TRAIN [4][2060/5173]	Time 0.498 (0.611)	Data 1.33e-04 (3.94e-04)	Tok/s 10426 (23233)	Loss/tok 2.6929 (3.3810)	LR 1.563e-05
0: TRAIN [4][2070/5173]	Time 0.703 (0.611)	Data 1.25e-04 (3.93e-04)	Tok/s 32880 (23248)	Loss/tok 3.7116 (3.3812)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][2080/5173]	Time 0.563 (0.611)	Data 1.29e-04 (3.92e-04)	Tok/s 18320 (23249)	Loss/tok 3.0992 (3.3815)	LR 1.563e-05
0: TRAIN [4][2090/5173]	Time 0.576 (0.611)	Data 1.26e-04 (3.91e-04)	Tok/s 18178 (23251)	Loss/tok 3.1453 (3.3813)	LR 1.563e-05
0: TRAIN [4][2100/5173]	Time 0.570 (0.611)	Data 3.06e-04 (3.90e-04)	Tok/s 18378 (23236)	Loss/tok 3.1212 (3.3808)	LR 1.563e-05
0: TRAIN [4][2110/5173]	Time 0.699 (0.611)	Data 1.23e-04 (3.88e-04)	Tok/s 33435 (23228)	Loss/tok 3.6018 (3.3805)	LR 1.563e-05
0: TRAIN [4][2120/5173]	Time 0.767 (0.611)	Data 1.29e-04 (3.87e-04)	Tok/s 38531 (23231)	Loss/tok 3.7339 (3.3808)	LR 1.563e-05
0: TRAIN [4][2130/5173]	Time 0.762 (0.611)	Data 1.27e-04 (3.86e-04)	Tok/s 38692 (23232)	Loss/tok 3.6912 (3.3809)	LR 1.563e-05
0: TRAIN [4][2140/5173]	Time 0.704 (0.611)	Data 3.23e-04 (3.85e-04)	Tok/s 33270 (23228)	Loss/tok 3.5071 (3.3807)	LR 1.563e-05
0: TRAIN [4][2150/5173]	Time 0.574 (0.611)	Data 1.45e-04 (3.84e-04)	Tok/s 18234 (23240)	Loss/tok 3.0755 (3.3810)	LR 1.563e-05
0: TRAIN [4][2160/5173]	Time 0.573 (0.611)	Data 1.13e-04 (3.83e-04)	Tok/s 18115 (23229)	Loss/tok 3.1634 (3.3806)	LR 1.563e-05
0: TRAIN [4][2170/5173]	Time 0.643 (0.611)	Data 1.34e-04 (3.81e-04)	Tok/s 26269 (23244)	Loss/tok 3.4103 (3.3811)	LR 1.563e-05
0: TRAIN [4][2180/5173]	Time 0.559 (0.611)	Data 1.33e-04 (3.80e-04)	Tok/s 18600 (23243)	Loss/tok 3.1670 (3.3811)	LR 1.563e-05
0: TRAIN [4][2190/5173]	Time 0.641 (0.611)	Data 1.23e-04 (3.79e-04)	Tok/s 26257 (23256)	Loss/tok 3.4666 (3.3817)	LR 1.563e-05
0: TRAIN [4][2200/5173]	Time 0.498 (0.611)	Data 1.22e-04 (3.78e-04)	Tok/s 10770 (23241)	Loss/tok 2.7841 (3.3814)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][2210/5173]	Time 0.568 (0.611)	Data 1.24e-04 (3.77e-04)	Tok/s 18220 (23253)	Loss/tok 3.1992 (3.3817)	LR 1.563e-05
0: TRAIN [4][2220/5173]	Time 0.636 (0.611)	Data 1.22e-04 (3.76e-04)	Tok/s 26598 (23249)	Loss/tok 3.3205 (3.3813)	LR 1.563e-05
0: TRAIN [4][2230/5173]	Time 0.647 (0.611)	Data 1.47e-04 (3.75e-04)	Tok/s 26047 (23251)	Loss/tok 3.3785 (3.3813)	LR 1.563e-05
0: TRAIN [4][2240/5173]	Time 0.634 (0.611)	Data 1.24e-04 (3.74e-04)	Tok/s 27183 (23256)	Loss/tok 3.3205 (3.3814)	LR 1.563e-05
0: TRAIN [4][2250/5173]	Time 0.564 (0.611)	Data 1.27e-04 (3.73e-04)	Tok/s 18188 (23262)	Loss/tok 3.2528 (3.3818)	LR 1.563e-05
0: TRAIN [4][2260/5173]	Time 0.624 (0.611)	Data 1.20e-04 (3.72e-04)	Tok/s 27069 (23261)	Loss/tok 3.4004 (3.3819)	LR 1.563e-05
0: TRAIN [4][2270/5173]	Time 0.565 (0.611)	Data 1.25e-04 (3.71e-04)	Tok/s 18155 (23267)	Loss/tok 3.1431 (3.3819)	LR 1.563e-05
0: TRAIN [4][2280/5173]	Time 0.573 (0.611)	Data 1.23e-04 (3.70e-04)	Tok/s 17704 (23271)	Loss/tok 2.9722 (3.3820)	LR 1.563e-05
0: TRAIN [4][2290/5173]	Time 0.570 (0.611)	Data 1.40e-04 (3.69e-04)	Tok/s 18078 (23268)	Loss/tok 3.1979 (3.3821)	LR 1.563e-05
0: TRAIN [4][2300/5173]	Time 0.561 (0.611)	Data 2.91e-04 (3.68e-04)	Tok/s 18334 (23262)	Loss/tok 3.2301 (3.3819)	LR 1.563e-05
0: TRAIN [4][2310/5173]	Time 0.568 (0.611)	Data 1.22e-04 (3.67e-04)	Tok/s 18332 (23256)	Loss/tok 3.1579 (3.3819)	LR 1.563e-05
0: TRAIN [4][2320/5173]	Time 0.627 (0.611)	Data 1.27e-04 (3.66e-04)	Tok/s 27213 (23268)	Loss/tok 3.4016 (3.3821)	LR 1.563e-05
0: TRAIN [4][2330/5173]	Time 0.569 (0.611)	Data 1.24e-04 (3.65e-04)	Tok/s 18131 (23261)	Loss/tok 3.2286 (3.3821)	LR 1.563e-05
0: TRAIN [4][2340/5173]	Time 0.642 (0.611)	Data 1.22e-04 (3.64e-04)	Tok/s 26148 (23264)	Loss/tok 3.4418 (3.3821)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][2350/5173]	Time 0.695 (0.611)	Data 1.29e-04 (3.63e-04)	Tok/s 33703 (23264)	Loss/tok 3.4887 (3.3822)	LR 1.563e-05
0: TRAIN [4][2360/5173]	Time 0.699 (0.611)	Data 2.93e-04 (3.62e-04)	Tok/s 33493 (23282)	Loss/tok 3.4676 (3.3830)	LR 1.563e-05
0: TRAIN [4][2370/5173]	Time 0.632 (0.611)	Data 1.25e-04 (3.61e-04)	Tok/s 26682 (23282)	Loss/tok 3.4360 (3.3829)	LR 1.563e-05
0: TRAIN [4][2380/5173]	Time 0.569 (0.611)	Data 3.09e-04 (3.60e-04)	Tok/s 18268 (23271)	Loss/tok 3.1352 (3.3826)	LR 1.563e-05
0: TRAIN [4][2390/5173]	Time 0.635 (0.611)	Data 1.23e-04 (3.59e-04)	Tok/s 25951 (23270)	Loss/tok 3.5265 (3.3827)	LR 1.563e-05
0: TRAIN [4][2400/5173]	Time 0.759 (0.611)	Data 1.21e-04 (3.58e-04)	Tok/s 39714 (23265)	Loss/tok 3.8139 (3.3828)	LR 1.563e-05
0: TRAIN [4][2410/5173]	Time 0.639 (0.611)	Data 1.21e-04 (3.57e-04)	Tok/s 25830 (23265)	Loss/tok 3.3459 (3.3829)	LR 1.563e-05
0: TRAIN [4][2420/5173]	Time 0.568 (0.611)	Data 1.34e-04 (3.56e-04)	Tok/s 18250 (23267)	Loss/tok 3.1053 (3.3828)	LR 1.563e-05
0: TRAIN [4][2430/5173]	Time 0.639 (0.611)	Data 1.32e-04 (3.55e-04)	Tok/s 25959 (23266)	Loss/tok 3.3898 (3.3826)	LR 1.563e-05
0: TRAIN [4][2440/5173]	Time 0.503 (0.611)	Data 1.25e-04 (3.55e-04)	Tok/s 10434 (23254)	Loss/tok 2.6001 (3.3826)	LR 1.563e-05
0: TRAIN [4][2450/5173]	Time 0.640 (0.611)	Data 1.28e-04 (3.54e-04)	Tok/s 25986 (23255)	Loss/tok 3.4153 (3.3826)	LR 1.563e-05
0: TRAIN [4][2460/5173]	Time 0.640 (0.611)	Data 1.23e-04 (3.53e-04)	Tok/s 26489 (23259)	Loss/tok 3.3287 (3.3825)	LR 1.563e-05
0: TRAIN [4][2470/5173]	Time 0.695 (0.611)	Data 1.28e-04 (3.52e-04)	Tok/s 33372 (23242)	Loss/tok 3.6580 (3.3822)	LR 1.563e-05
0: TRAIN [4][2480/5173]	Time 0.707 (0.611)	Data 1.23e-04 (3.51e-04)	Tok/s 33294 (23237)	Loss/tok 3.6255 (3.3821)	LR 1.563e-05
0: TRAIN [4][2490/5173]	Time 0.568 (0.611)	Data 1.27e-04 (3.50e-04)	Tok/s 18184 (23239)	Loss/tok 3.0995 (3.3821)	LR 1.563e-05
0: TRAIN [4][2500/5173]	Time 0.568 (0.611)	Data 1.36e-04 (3.49e-04)	Tok/s 18047 (23239)	Loss/tok 3.2269 (3.3822)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][2510/5173]	Time 0.565 (0.611)	Data 1.24e-04 (3.48e-04)	Tok/s 18026 (23241)	Loss/tok 3.2621 (3.3824)	LR 1.563e-05
0: TRAIN [4][2520/5173]	Time 0.626 (0.611)	Data 1.26e-04 (3.48e-04)	Tok/s 26571 (23254)	Loss/tok 3.4676 (3.3827)	LR 1.563e-05
0: TRAIN [4][2530/5173]	Time 0.553 (0.611)	Data 1.23e-04 (3.47e-04)	Tok/s 18671 (23255)	Loss/tok 3.1450 (3.3830)	LR 1.563e-05
0: TRAIN [4][2540/5173]	Time 0.703 (0.611)	Data 1.23e-04 (3.46e-04)	Tok/s 32937 (23247)	Loss/tok 3.5718 (3.3827)	LR 1.563e-05
0: TRAIN [4][2550/5173]	Time 0.703 (0.611)	Data 1.21e-04 (3.45e-04)	Tok/s 33136 (23252)	Loss/tok 3.5876 (3.3826)	LR 1.563e-05
0: TRAIN [4][2560/5173]	Time 0.561 (0.611)	Data 1.18e-04 (3.44e-04)	Tok/s 18205 (23253)	Loss/tok 3.1228 (3.3826)	LR 1.563e-05
0: TRAIN [4][2570/5173]	Time 0.569 (0.611)	Data 1.38e-04 (3.43e-04)	Tok/s 18116 (23269)	Loss/tok 3.3321 (3.3833)	LR 1.563e-05
0: TRAIN [4][2580/5173]	Time 0.703 (0.611)	Data 1.24e-04 (3.43e-04)	Tok/s 33267 (23261)	Loss/tok 3.5815 (3.3831)	LR 1.563e-05
0: TRAIN [4][2590/5173]	Time 0.636 (0.611)	Data 1.22e-04 (3.42e-04)	Tok/s 25986 (23250)	Loss/tok 3.3891 (3.3827)	LR 1.563e-05
0: TRAIN [4][2600/5173]	Time 0.502 (0.611)	Data 2.81e-04 (3.41e-04)	Tok/s 10394 (23241)	Loss/tok 2.7898 (3.3823)	LR 1.563e-05
0: TRAIN [4][2610/5173]	Time 0.646 (0.611)	Data 1.28e-04 (3.40e-04)	Tok/s 26084 (23244)	Loss/tok 3.3106 (3.3823)	LR 1.563e-05
0: TRAIN [4][2620/5173]	Time 0.568 (0.611)	Data 1.45e-04 (3.40e-04)	Tok/s 18099 (23246)	Loss/tok 3.3297 (3.3823)	LR 1.563e-05
0: TRAIN [4][2630/5173]	Time 0.501 (0.611)	Data 1.41e-04 (3.39e-04)	Tok/s 10333 (23238)	Loss/tok 2.6804 (3.3822)	LR 1.563e-05
0: TRAIN [4][2640/5173]	Time 0.570 (0.611)	Data 1.29e-04 (3.38e-04)	Tok/s 18147 (23223)	Loss/tok 3.0320 (3.3817)	LR 1.563e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][2650/5173]	Time 0.561 (0.611)	Data 1.21e-04 (3.37e-04)	Tok/s 18210 (23230)	Loss/tok 3.2469 (3.3818)	LR 1.563e-05
0: TRAIN [4][2660/5173]	Time 0.704 (0.611)	Data 1.27e-04 (3.37e-04)	Tok/s 33431 (23237)	Loss/tok 3.4651 (3.3819)	LR 1.563e-05
0: TRAIN [4][2670/5173]	Time 0.574 (0.611)	Data 1.28e-04 (3.36e-04)	Tok/s 18107 (23229)	Loss/tok 3.2564 (3.3818)	LR 1.563e-05
0: TRAIN [4][2680/5173]	Time 0.704 (0.611)	Data 1.28e-04 (3.35e-04)	Tok/s 32663 (23227)	Loss/tok 3.5889 (3.3818)	LR 1.563e-05
0: TRAIN [4][2690/5173]	Time 0.503 (0.611)	Data 1.34e-04 (3.34e-04)	Tok/s 10645 (23223)	Loss/tok 2.6993 (3.3819)	LR 1.563e-05
0: TRAIN [4][2700/5173]	Time 0.570 (0.611)	Data 1.23e-04 (3.34e-04)	Tok/s 18183 (23218)	Loss/tok 3.1480 (3.3819)	LR 1.563e-05
0: TRAIN [4][2710/5173]	Time 0.702 (0.611)	Data 1.26e-04 (3.33e-04)	Tok/s 33207 (23225)	Loss/tok 3.5485 (3.3821)	LR 1.563e-05
0: TRAIN [4][2720/5173]	Time 0.572 (0.611)	Data 1.23e-04 (3.32e-04)	Tok/s 18005 (23218)	Loss/tok 3.1351 (3.3818)	LR 1.563e-05
