Beginning trial 1 of 1
Gathering sys log on dss01
:::MLL 1571245635.276 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1571245635.277 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1571245635.277 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1571245635.277 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1571245635.278 submission_platform: {"value": "1xDSS8440", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1571245635.278 submission_entry: {"value": "{'hardware': 'DSS8440', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '5.0-0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz', 'num_cores': '40', 'num_vcpus': '40', 'accelerator': 'Tesla V100-PCIE-32GB', 'num_accelerators': '8', 'sys_mem_size': '754 GB', 'sys_storage_type': 'SATA SSD', 'sys_storage_size': '1x 447.1G + 1x 931.5G', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27700 Family [ConnectX-4]', 'num_network_cards': '1', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1571245635.279 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1571245635.279 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1571245640.725 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node dss01
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=5073' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=512 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=100 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=191016120535297615525 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191016120535297615525 ./run_and_time.sh
Run vars: id 191016120535297615525 gpus 8 mparams  --master_port=5073
NCCL_SOCKET_NTHREADS=2
NCCL_NSOCKS_PERTHREAD=8
STARTING TIMING RUN AT 2019-10-16 05:07:21 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=512
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=100
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=0
+ MATH=amp_fp16
+ [[ 0 -eq 1 ]]
+ LAUNCH_OPT='torch.distributed.launch --nproc_per_node 8  --master_port=5073'
running benchmark
+ echo 'running benchmark'
+ python -m torch.distributed.launch --nproc_per_node 8 --master_port=5073 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 512 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 100 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1571245643.455 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571245643.455 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571245643.455 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571245643.459 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571245643.462 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571245643.464 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571245643.470 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571245643.471 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=512, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=100)
0: L2 promotion: 128B
0: Using random master seed: 1871002761
dss01:464:464 [0] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:464:464 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:464:464 [0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:464:464 [0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:464:464 [0] NCCL INFO NET/IB : No device found.
dss01:464:464 [0] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
NCCL version 2.4.8+cuda10.1
dss01:465:465 [1] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:465:465 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:468:468 [4] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:468:468 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:466:466 [2] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:466:466 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:469:469 [5] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:469:469 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:471:471 [7] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:471:471 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:467:467 [3] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:467:467 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:470:470 [6] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:470:470 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:465:465 [1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:465:465 [1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:465:465 [1] NCCL INFO NET/IB : No device found.

dss01:468:468 [4] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:468:468 [4] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:468:468 [4] NCCL INFO NET/IB : No device found.

dss01:469:469 [5] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:469:469 [5] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:469:469 [5] NCCL INFO NET/IB : No device found.

dss01:466:466 [2] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:466:466 [2] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:466:466 [2] NCCL INFO NET/IB : No device found.

dss01:467:467 [3] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:467:467 [3] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:467:467 [3] NCCL INFO NET/IB : No device found.

dss01:470:470 [6] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:470:470 [6] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:470:470 [6] NCCL INFO NET/IB : No device found.

dss01:471:471 [7] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:471:471 [7] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:471:471 [7] NCCL INFO NET/IB : No device found.
dss01:465:465 [1] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:468:468 [4] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:469:469 [5] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:466:466 [2] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:470:470 [6] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:467:467 [3] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:471:471 [7] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:464:826 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555
dss01:465:827 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555
dss01:468:828 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa
dss01:469:829 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa
dss01:467:830 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555
dss01:466:831 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555
dss01:471:832 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa
dss01:470:833 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa
dss01:471:832 [7] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:464:826 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:465:827 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:466:831 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:467:830 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:468:828 [4] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:470:833 [6] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:469:829 [5] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:464:826 [0] NCCL INFO Channel 00 :    0   1   2   3   4   5   6   7
dss01:466:831 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/IPC
dss01:468:828 [4] NCCL INFO Ring 00 : 4[4] -> 5[5] via P2P/IPC
dss01:470:833 [6] NCCL INFO Ring 00 : 6[6] -> 7[7] via P2P/IPC
dss01:464:826 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/IPC
dss01:465:827 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via direct shared memory
dss01:467:830 [3] NCCL INFO Ring 00 : 3[3] -> 4[4] via direct shared memory
dss01:471:832 [7] NCCL INFO Ring 00 : 7[7] -> 0[0] via direct shared memory
dss01:469:829 [5] NCCL INFO Ring 00 : 5[5] -> 6[6] via direct shared memory
dss01:464:826 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
dss01:465:827 [1] NCCL INFO comm 0x7fff60007590 rank 1 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE
dss01:467:830 [3] NCCL INFO comm 0x7ffe94007590 rank 3 nranks 8 cudaDev 3 nvmlDev 3 - Init COMPLETE
dss01:471:832 [7] NCCL INFO comm 0x7ffe94007590 rank 7 nranks 8 cudaDev 7 nvmlDev 7 - Init COMPLETE
dss01:469:829 [5] NCCL INFO comm 0x7fff58007590 rank 5 nranks 8 cudaDev 5 nvmlDev 5 - Init COMPLETE
dss01:466:831 [2] NCCL INFO comm 0x7fff34007590 rank 2 nranks 8 cudaDev 2 nvmlDev 2 - Init COMPLETE
dss01:464:826 [0] NCCL INFO comm 0x7ffe30007590 rank 0 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE
dss01:464:464 [0] NCCL INFO Launch mode Parallel
0: Worker 0 is using worker seed: 1661561433
dss01:468:828 [4] NCCL INFO comm 0x7fff58007590 rank 4 nranks 8 cudaDev 4 nvmlDev 4 - Init COMPLETE
0: Building vocabulary from /data/vocab.bpe.32000
dss01:470:833 [6] NCCL INFO comm 0x7ffe94007590 rank 6 nranks 8 cudaDev 6 nvmlDev 6 - Init COMPLETE
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1571245666.664 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1571245670.473 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1571245670.474 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1571245670.475 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1571245671.369 global_batch_size: {"value": 4096, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 100, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 100
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1571245671.371 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1571245671.371 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1571245671.371 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1571245671.371 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1571245671.372 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1571245671.372 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1571245671.372 opt_learning_rate_warmup_steps: {"value": 100, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1571245671.547 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571245671.548 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 2799836643
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/968]	Time 1.310 (1.310)	Data 8.83e-01 (8.83e-01)	Tok/s 25709 (25709)	Loss/tok 10.6616 (10.6616)	LR 2.000e-05
0: TRAIN [0][10/968]	Time 0.471 (0.497)	Data 2.17e-04 (8.04e-02)	Tok/s 70749 (69395)	Loss/tok 9.6601 (10.1343)	LR 3.170e-05
0: TRAIN [0][20/968]	Time 0.310 (0.485)	Data 1.67e-04 (4.22e-02)	Tok/s 66248 (74656)	Loss/tok 9.0952 (9.7496)	LR 5.024e-05
0: TRAIN [0][30/968]	Time 0.540 (0.450)	Data 1.67e-04 (2.86e-02)	Tok/s 86648 (74456)	Loss/tok 8.9422 (9.5297)	LR 7.962e-05
0: TRAIN [0][40/968]	Time 0.311 (0.442)	Data 1.68e-04 (2.17e-02)	Tok/s 66113 (74508)	Loss/tok 8.3698 (9.3185)	LR 1.262e-04
0: TRAIN [0][50/968]	Time 0.541 (0.431)	Data 1.63e-04 (1.75e-02)	Tok/s 86134 (74479)	Loss/tok 8.2368 (9.1258)	LR 2.000e-04
0: TRAIN [0][60/968]	Time 0.420 (0.419)	Data 1.66e-04 (1.46e-02)	Tok/s 80665 (73949)	Loss/tok 8.0817 (8.9798)	LR 3.170e-04
0: TRAIN [0][70/968]	Time 0.311 (0.418)	Data 1.62e-04 (1.26e-02)	Tok/s 66542 (74132)	Loss/tok 7.7738 (8.8392)	LR 5.024e-04
0: TRAIN [0][80/968]	Time 0.421 (0.413)	Data 1.61e-04 (1.11e-02)	Tok/s 78962 (74042)	Loss/tok 7.9603 (8.7357)	LR 7.962e-04
0: TRAIN [0][90/968]	Time 0.312 (0.407)	Data 1.58e-04 (9.87e-03)	Tok/s 65723 (73646)	Loss/tok 7.6852 (8.6492)	LR 1.262e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][100/968]	Time 0.420 (0.403)	Data 1.66e-04 (8.91e-03)	Tok/s 80207 (73527)	Loss/tok 7.8543 (8.5734)	LR 1.910e-03
0: TRAIN [0][110/968]	Time 0.539 (0.402)	Data 1.50e-04 (8.12e-03)	Tok/s 86619 (73494)	Loss/tok 7.6549 (8.4884)	LR 2.000e-03
0: TRAIN [0][120/968]	Time 0.422 (0.402)	Data 1.55e-04 (7.46e-03)	Tok/s 79619 (73734)	Loss/tok 7.1846 (8.3911)	LR 2.000e-03
0: TRAIN [0][130/968]	Time 0.310 (0.396)	Data 1.58e-04 (6.91e-03)	Tok/s 66408 (73252)	Loss/tok 6.8411 (8.3103)	LR 2.000e-03
0: TRAIN [0][140/968]	Time 0.310 (0.390)	Data 1.58e-04 (6.43e-03)	Tok/s 66500 (72747)	Loss/tok 6.8473 (8.2325)	LR 2.000e-03
0: TRAIN [0][150/968]	Time 0.204 (0.389)	Data 1.57e-04 (6.02e-03)	Tok/s 51434 (72616)	Loss/tok 5.8305 (8.1441)	LR 2.000e-03
0: TRAIN [0][160/968]	Time 0.309 (0.387)	Data 1.64e-04 (5.65e-03)	Tok/s 68098 (72564)	Loss/tok 6.4363 (8.0579)	LR 2.000e-03
0: TRAIN [0][170/968]	Time 0.309 (0.385)	Data 1.47e-04 (5.33e-03)	Tok/s 66979 (72482)	Loss/tok 6.2187 (7.9734)	LR 2.000e-03
0: TRAIN [0][180/968]	Time 0.420 (0.386)	Data 1.56e-04 (5.05e-03)	Tok/s 80508 (72551)	Loss/tok 6.3123 (7.8804)	LR 2.000e-03
0: TRAIN [0][190/968]	Time 0.540 (0.388)	Data 1.55e-04 (4.79e-03)	Tok/s 86928 (72849)	Loss/tok 6.3221 (7.7805)	LR 2.000e-03
0: TRAIN [0][200/968]	Time 0.310 (0.389)	Data 1.69e-04 (4.56e-03)	Tok/s 66440 (72901)	Loss/tok 5.8247 (7.6961)	LR 2.000e-03
0: TRAIN [0][210/968]	Time 0.311 (0.389)	Data 1.61e-04 (4.35e-03)	Tok/s 67909 (72986)	Loss/tok 5.6329 (7.6117)	LR 2.000e-03
0: TRAIN [0][220/968]	Time 0.542 (0.389)	Data 1.59e-04 (4.16e-03)	Tok/s 86590 (72978)	Loss/tok 6.0899 (7.5339)	LR 2.000e-03
0: TRAIN [0][230/968]	Time 0.421 (0.389)	Data 1.70e-04 (3.99e-03)	Tok/s 79774 (72985)	Loss/tok 5.8535 (7.4574)	LR 2.000e-03
0: TRAIN [0][240/968]	Time 0.309 (0.387)	Data 1.67e-04 (3.83e-03)	Tok/s 66838 (72844)	Loss/tok 5.3693 (7.3921)	LR 2.000e-03
0: TRAIN [0][250/968]	Time 0.311 (0.385)	Data 1.54e-04 (3.69e-03)	Tok/s 66297 (72632)	Loss/tok 5.3417 (7.3318)	LR 2.000e-03
0: TRAIN [0][260/968]	Time 0.539 (0.383)	Data 1.57e-04 (3.55e-03)	Tok/s 86588 (72508)	Loss/tok 5.8014 (7.2681)	LR 2.000e-03
0: TRAIN [0][270/968]	Time 0.542 (0.383)	Data 1.68e-04 (3.43e-03)	Tok/s 85855 (72473)	Loss/tok 5.7108 (7.2014)	LR 2.000e-03
0: TRAIN [0][280/968]	Time 0.310 (0.384)	Data 1.72e-04 (3.31e-03)	Tok/s 65766 (72558)	Loss/tok 5.0586 (7.1310)	LR 2.000e-03
0: TRAIN [0][290/968]	Time 0.422 (0.384)	Data 1.78e-04 (3.21e-03)	Tok/s 79345 (72647)	Loss/tok 5.2284 (7.0636)	LR 2.000e-03
0: TRAIN [0][300/968]	Time 0.204 (0.382)	Data 1.62e-04 (3.11e-03)	Tok/s 51578 (72487)	Loss/tok 3.9465 (7.0080)	LR 2.000e-03
0: TRAIN [0][310/968]	Time 0.542 (0.381)	Data 1.88e-04 (3.01e-03)	Tok/s 86385 (72436)	Loss/tok 5.3414 (6.9482)	LR 2.000e-03
0: TRAIN [0][320/968]	Time 0.423 (0.382)	Data 1.80e-04 (2.93e-03)	Tok/s 80467 (72514)	Loss/tok 5.0237 (6.8860)	LR 2.000e-03
0: TRAIN [0][330/968]	Time 0.310 (0.382)	Data 1.57e-04 (2.84e-03)	Tok/s 66168 (72548)	Loss/tok 4.5919 (6.8269)	LR 2.000e-03
0: TRAIN [0][340/968]	Time 0.204 (0.381)	Data 1.99e-04 (2.76e-03)	Tok/s 51187 (72391)	Loss/tok 3.6970 (6.7753)	LR 2.000e-03
0: TRAIN [0][350/968]	Time 0.205 (0.381)	Data 1.69e-04 (2.69e-03)	Tok/s 51778 (72266)	Loss/tok 3.7170 (6.7227)	LR 2.000e-03
0: TRAIN [0][360/968]	Time 0.425 (0.381)	Data 1.57e-04 (2.62e-03)	Tok/s 78296 (72323)	Loss/tok 4.7657 (6.6652)	LR 2.000e-03
0: TRAIN [0][370/968]	Time 0.423 (0.382)	Data 1.54e-04 (2.55e-03)	Tok/s 79946 (72380)	Loss/tok 4.6403 (6.6077)	LR 2.000e-03
0: TRAIN [0][380/968]	Time 0.201 (0.381)	Data 1.50e-04 (2.49e-03)	Tok/s 51964 (72316)	Loss/tok 3.4539 (6.5579)	LR 2.000e-03
0: TRAIN [0][390/968]	Time 0.422 (0.380)	Data 1.47e-04 (2.43e-03)	Tok/s 80066 (72265)	Loss/tok 4.6729 (6.5097)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][400/968]	Time 0.311 (0.382)	Data 1.80e-04 (2.37e-03)	Tok/s 66614 (72339)	Loss/tok 4.1821 (6.4557)	LR 2.000e-03
0: TRAIN [0][410/968]	Time 0.422 (0.381)	Data 1.49e-04 (2.32e-03)	Tok/s 80135 (72341)	Loss/tok 4.5467 (6.4079)	LR 2.000e-03
0: TRAIN [0][420/968]	Time 0.423 (0.381)	Data 1.53e-04 (2.27e-03)	Tok/s 79444 (72371)	Loss/tok 4.4402 (6.3601)	LR 2.000e-03
0: TRAIN [0][430/968]	Time 0.310 (0.381)	Data 1.49e-04 (2.22e-03)	Tok/s 66711 (72417)	Loss/tok 4.0269 (6.3130)	LR 2.000e-03
0: TRAIN [0][440/968]	Time 0.682 (0.382)	Data 1.57e-04 (2.17e-03)	Tok/s 87303 (72493)	Loss/tok 4.8525 (6.2648)	LR 2.000e-03
0: TRAIN [0][450/968]	Time 0.422 (0.383)	Data 1.50e-04 (2.13e-03)	Tok/s 79778 (72560)	Loss/tok 4.3786 (6.2190)	LR 2.000e-03
0: TRAIN [0][460/968]	Time 0.309 (0.381)	Data 1.68e-04 (2.09e-03)	Tok/s 66935 (72481)	Loss/tok 4.0340 (6.1814)	LR 2.000e-03
0: TRAIN [0][470/968]	Time 0.310 (0.380)	Data 1.58e-04 (2.04e-03)	Tok/s 66499 (72288)	Loss/tok 3.9247 (6.1497)	LR 2.000e-03
0: TRAIN [0][480/968]	Time 0.423 (0.379)	Data 1.44e-04 (2.01e-03)	Tok/s 79676 (72260)	Loss/tok 4.2484 (6.1105)	LR 2.000e-03
0: TRAIN [0][490/968]	Time 0.422 (0.378)	Data 1.48e-04 (1.97e-03)	Tok/s 78829 (72107)	Loss/tok 4.2702 (6.0792)	LR 2.000e-03
0: TRAIN [0][500/968]	Time 0.424 (0.379)	Data 1.62e-04 (1.93e-03)	Tok/s 79509 (72175)	Loss/tok 4.1432 (6.0372)	LR 2.000e-03
0: TRAIN [0][510/968]	Time 0.313 (0.379)	Data 1.51e-04 (1.90e-03)	Tok/s 65461 (72248)	Loss/tok 3.8309 (5.9962)	LR 2.000e-03
0: TRAIN [0][520/968]	Time 0.422 (0.382)	Data 1.58e-04 (1.86e-03)	Tok/s 79656 (72385)	Loss/tok 4.2019 (5.9511)	LR 2.000e-03
0: TRAIN [0][530/968]	Time 0.545 (0.382)	Data 1.42e-04 (1.83e-03)	Tok/s 85312 (72444)	Loss/tok 4.3364 (5.9133)	LR 2.000e-03
0: TRAIN [0][540/968]	Time 0.422 (0.383)	Data 1.65e-04 (1.80e-03)	Tok/s 79452 (72511)	Loss/tok 4.1049 (5.8743)	LR 2.000e-03
0: TRAIN [0][550/968]	Time 0.311 (0.383)	Data 1.61e-04 (1.77e-03)	Tok/s 65534 (72498)	Loss/tok 3.7665 (5.8417)	LR 2.000e-03
0: TRAIN [0][560/968]	Time 0.423 (0.383)	Data 1.43e-04 (1.74e-03)	Tok/s 79582 (72568)	Loss/tok 3.9408 (5.8066)	LR 2.000e-03
0: TRAIN [0][570/968]	Time 0.424 (0.384)	Data 1.69e-04 (1.71e-03)	Tok/s 79188 (72575)	Loss/tok 4.0819 (5.7745)	LR 2.000e-03
0: TRAIN [0][580/968]	Time 0.310 (0.385)	Data 1.50e-04 (1.69e-03)	Tok/s 66268 (72599)	Loss/tok 3.7050 (5.7416)	LR 2.000e-03
0: TRAIN [0][590/968]	Time 0.312 (0.385)	Data 1.42e-04 (1.66e-03)	Tok/s 66498 (72597)	Loss/tok 3.7220 (5.7117)	LR 2.000e-03
0: TRAIN [0][600/968]	Time 0.313 (0.383)	Data 1.46e-04 (1.64e-03)	Tok/s 65726 (72464)	Loss/tok 3.7088 (5.6889)	LR 2.000e-03
0: TRAIN [0][610/968]	Time 0.311 (0.383)	Data 1.42e-04 (1.61e-03)	Tok/s 67175 (72450)	Loss/tok 3.6800 (5.6615)	LR 2.000e-03
0: TRAIN [0][620/968]	Time 0.543 (0.383)	Data 1.74e-04 (1.59e-03)	Tok/s 86543 (72499)	Loss/tok 4.1936 (5.6315)	LR 2.000e-03
0: TRAIN [0][630/968]	Time 0.423 (0.384)	Data 1.86e-04 (1.57e-03)	Tok/s 79311 (72550)	Loss/tok 3.8858 (5.6013)	LR 2.000e-03
0: TRAIN [0][640/968]	Time 0.312 (0.384)	Data 1.70e-04 (1.55e-03)	Tok/s 66291 (72510)	Loss/tok 3.7197 (5.5767)	LR 2.000e-03
0: TRAIN [0][650/968]	Time 0.423 (0.384)	Data 1.51e-04 (1.53e-03)	Tok/s 80037 (72530)	Loss/tok 3.8543 (5.5508)	LR 2.000e-03
0: TRAIN [0][660/968]	Time 0.311 (0.384)	Data 2.02e-04 (1.51e-03)	Tok/s 65752 (72532)	Loss/tok 3.6462 (5.5254)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][670/968]	Time 0.313 (0.385)	Data 1.87e-04 (1.49e-03)	Tok/s 66333 (72619)	Loss/tok 3.6208 (5.4979)	LR 2.000e-03
0: TRAIN [0][680/968]	Time 0.312 (0.385)	Data 1.50e-04 (1.47e-03)	Tok/s 65880 (72602)	Loss/tok 3.6768 (5.4755)	LR 2.000e-03
0: TRAIN [0][690/968]	Time 0.313 (0.384)	Data 1.45e-04 (1.45e-03)	Tok/s 66433 (72567)	Loss/tok 3.6473 (5.4537)	LR 2.000e-03
0: TRAIN [0][700/968]	Time 0.424 (0.384)	Data 1.66e-04 (1.43e-03)	Tok/s 79126 (72529)	Loss/tok 3.8781 (5.4328)	LR 2.000e-03
0: TRAIN [0][710/968]	Time 0.681 (0.385)	Data 1.52e-04 (1.41e-03)	Tok/s 87921 (72593)	Loss/tok 4.3189 (5.4081)	LR 2.000e-03
0: TRAIN [0][720/968]	Time 0.540 (0.385)	Data 2.43e-04 (1.40e-03)	Tok/s 86805 (72642)	Loss/tok 4.0742 (5.3839)	LR 2.000e-03
0: TRAIN [0][730/968]	Time 0.424 (0.386)	Data 1.81e-04 (1.38e-03)	Tok/s 79375 (72684)	Loss/tok 3.8480 (5.3607)	LR 2.000e-03
0: TRAIN [0][740/968]	Time 0.423 (0.385)	Data 1.44e-04 (1.36e-03)	Tok/s 79374 (72616)	Loss/tok 3.8353 (5.3427)	LR 2.000e-03
0: TRAIN [0][750/968]	Time 0.312 (0.385)	Data 1.75e-04 (1.35e-03)	Tok/s 67205 (72587)	Loss/tok 3.6462 (5.3232)	LR 2.000e-03
0: TRAIN [0][760/968]	Time 0.423 (0.385)	Data 1.54e-04 (1.33e-03)	Tok/s 79774 (72598)	Loss/tok 3.8348 (5.3034)	LR 2.000e-03
0: TRAIN [0][770/968]	Time 0.310 (0.385)	Data 1.51e-04 (1.32e-03)	Tok/s 66718 (72603)	Loss/tok 3.5729 (5.2839)	LR 2.000e-03
0: TRAIN [0][780/968]	Time 0.314 (0.384)	Data 1.77e-04 (1.30e-03)	Tok/s 64696 (72531)	Loss/tok 3.5536 (5.2672)	LR 2.000e-03
0: TRAIN [0][790/968]	Time 0.312 (0.384)	Data 1.58e-04 (1.29e-03)	Tok/s 66305 (72564)	Loss/tok 3.5327 (5.2481)	LR 2.000e-03
0: TRAIN [0][800/968]	Time 0.311 (0.385)	Data 1.50e-04 (1.27e-03)	Tok/s 65540 (72594)	Loss/tok 3.4708 (5.2293)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][810/968]	Time 0.543 (0.386)	Data 2.52e-04 (1.26e-03)	Tok/s 85661 (72672)	Loss/tok 3.9327 (5.2079)	LR 2.000e-03
0: TRAIN [0][820/968]	Time 0.204 (0.386)	Data 1.55e-04 (1.25e-03)	Tok/s 51658 (72625)	Loss/tok 3.0100 (5.1918)	LR 2.000e-03
0: TRAIN [0][830/968]	Time 0.311 (0.386)	Data 1.57e-04 (1.23e-03)	Tok/s 66012 (72665)	Loss/tok 3.4886 (5.1729)	LR 2.000e-03
0: TRAIN [0][840/968]	Time 0.422 (0.385)	Data 3.86e-04 (1.22e-03)	Tok/s 80558 (72615)	Loss/tok 3.8562 (5.1578)	LR 2.000e-03
0: TRAIN [0][850/968]	Time 0.313 (0.386)	Data 1.51e-04 (1.21e-03)	Tok/s 65689 (72667)	Loss/tok 3.5159 (5.1393)	LR 2.000e-03
0: TRAIN [0][860/968]	Time 0.311 (0.386)	Data 1.61e-04 (1.20e-03)	Tok/s 66342 (72655)	Loss/tok 3.4611 (5.1235)	LR 2.000e-03
0: TRAIN [0][870/968]	Time 0.422 (0.386)	Data 1.47e-04 (1.18e-03)	Tok/s 79584 (72618)	Loss/tok 3.6846 (5.1085)	LR 2.000e-03
0: TRAIN [0][880/968]	Time 0.310 (0.386)	Data 1.54e-04 (1.17e-03)	Tok/s 65706 (72623)	Loss/tok 3.4116 (5.0923)	LR 2.000e-03
0: TRAIN [0][890/968]	Time 0.421 (0.386)	Data 1.51e-04 (1.16e-03)	Tok/s 79988 (72667)	Loss/tok 3.7741 (5.0754)	LR 2.000e-03
0: TRAIN [0][900/968]	Time 0.312 (0.386)	Data 1.70e-04 (1.15e-03)	Tok/s 66340 (72662)	Loss/tok 3.4024 (5.0604)	LR 2.000e-03
0: TRAIN [0][910/968]	Time 0.314 (0.386)	Data 1.84e-04 (1.14e-03)	Tok/s 64951 (72636)	Loss/tok 3.5019 (5.0459)	LR 2.000e-03
0: TRAIN [0][920/968]	Time 0.204 (0.386)	Data 1.73e-04 (1.13e-03)	Tok/s 51062 (72595)	Loss/tok 2.8949 (5.0321)	LR 2.000e-03
0: TRAIN [0][930/968]	Time 0.312 (0.386)	Data 1.83e-04 (1.12e-03)	Tok/s 65704 (72618)	Loss/tok 3.5577 (5.0168)	LR 2.000e-03
0: TRAIN [0][940/968]	Time 0.313 (0.386)	Data 1.65e-04 (1.11e-03)	Tok/s 66065 (72615)	Loss/tok 3.4676 (5.0027)	LR 2.000e-03
0: TRAIN [0][950/968]	Time 0.311 (0.386)	Data 2.10e-04 (1.10e-03)	Tok/s 67069 (72636)	Loss/tok 3.4939 (4.9883)	LR 2.000e-03
0: TRAIN [0][960/968]	Time 0.204 (0.387)	Data 1.64e-04 (1.09e-03)	Tok/s 52299 (72666)	Loss/tok 2.9657 (4.9730)	LR 2.000e-03
:::MLL 1571246048.278 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1571246048.279 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.736 (0.736)	Decoder iters 149.0 (149.0)	Tok/s 21405 (21405)
0: Running moses detokenizer
0: BLEU(score=18.375715638206582, counts=[33188, 14629, 7552, 4029], totals=[63386, 60383, 57380, 54382], precisions=[52.35856498280377, 24.227017538048788, 13.161380271871732, 7.408701408554301], bp=0.9798541967859459, sys_len=63386, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571246050.370 eval_accuracy: {"value": 18.38, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1571246050.370 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 4.9638	Test BLEU: 18.38
0: Performance: Epoch: 0	Training: 581667 Tok/s
0: Finished epoch 0
:::MLL 1571246050.371 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1571246050.371 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571246050.372 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 4123154789
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [1][0/968]	Time 1.144 (1.144)	Data 7.18e-01 (7.18e-01)	Tok/s 29467 (29467)	Loss/tok 3.6331 (3.6331)	LR 2.000e-03
0: TRAIN [1][10/968]	Time 0.313 (0.450)	Data 1.52e-04 (6.55e-02)	Tok/s 66711 (69641)	Loss/tok 3.3749 (3.5941)	LR 2.000e-03
0: TRAIN [1][20/968]	Time 0.311 (0.435)	Data 1.73e-04 (3.44e-02)	Tok/s 66813 (71898)	Loss/tok 3.3640 (3.6356)	LR 2.000e-03
0: TRAIN [1][30/968]	Time 0.205 (0.417)	Data 1.50e-04 (2.33e-02)	Tok/s 51934 (72105)	Loss/tok 2.7881 (3.6176)	LR 2.000e-03
0: TRAIN [1][40/968]	Time 0.423 (0.415)	Data 1.75e-04 (1.77e-02)	Tok/s 79704 (72376)	Loss/tok 3.6693 (3.6371)	LR 2.000e-03
0: TRAIN [1][50/968]	Time 0.423 (0.415)	Data 1.60e-04 (1.43e-02)	Tok/s 79971 (72963)	Loss/tok 3.5805 (3.6358)	LR 2.000e-03
0: TRAIN [1][60/968]	Time 0.542 (0.417)	Data 1.47e-04 (1.19e-02)	Tok/s 85613 (73637)	Loss/tok 3.8400 (3.6413)	LR 2.000e-03
0: TRAIN [1][70/968]	Time 0.423 (0.414)	Data 1.43e-04 (1.03e-02)	Tok/s 78615 (73576)	Loss/tok 3.6520 (3.6398)	LR 2.000e-03
0: TRAIN [1][80/968]	Time 0.543 (0.408)	Data 1.60e-04 (9.03e-03)	Tok/s 86112 (73222)	Loss/tok 3.7335 (3.6294)	LR 2.000e-03
0: TRAIN [1][90/968]	Time 0.311 (0.402)	Data 1.49e-04 (8.06e-03)	Tok/s 66993 (72786)	Loss/tok 3.3826 (3.6169)	LR 2.000e-03
0: TRAIN [1][100/968]	Time 0.789 (0.403)	Data 1.44e-04 (7.27e-03)	Tok/s 75025 (72716)	Loss/tok 4.0035 (3.6221)	LR 2.000e-03
0: TRAIN [1][110/968]	Time 0.311 (0.403)	Data 1.45e-04 (6.64e-03)	Tok/s 65900 (72872)	Loss/tok 3.2439 (3.6207)	LR 2.000e-03
0: TRAIN [1][120/968]	Time 0.422 (0.403)	Data 1.47e-04 (6.10e-03)	Tok/s 79472 (72925)	Loss/tok 3.6106 (3.6188)	LR 2.000e-03
0: TRAIN [1][130/968]	Time 0.314 (0.403)	Data 1.51e-04 (5.65e-03)	Tok/s 64923 (73121)	Loss/tok 3.3089 (3.6141)	LR 2.000e-03
0: TRAIN [1][140/968]	Time 0.423 (0.399)	Data 1.54e-04 (5.26e-03)	Tok/s 80224 (72959)	Loss/tok 3.5590 (3.6061)	LR 2.000e-03
0: TRAIN [1][150/968]	Time 0.310 (0.398)	Data 1.64e-04 (4.92e-03)	Tok/s 65668 (72820)	Loss/tok 3.3450 (3.6012)	LR 2.000e-03
0: TRAIN [1][160/968]	Time 0.542 (0.395)	Data 1.49e-04 (4.63e-03)	Tok/s 85549 (72621)	Loss/tok 3.8434 (3.5988)	LR 2.000e-03
0: TRAIN [1][170/968]	Time 0.309 (0.396)	Data 1.49e-04 (4.37e-03)	Tok/s 65844 (72817)	Loss/tok 3.2837 (3.5992)	LR 2.000e-03
0: TRAIN [1][180/968]	Time 0.312 (0.395)	Data 1.56e-04 (4.13e-03)	Tok/s 66739 (72736)	Loss/tok 3.3604 (3.5946)	LR 2.000e-03
0: TRAIN [1][190/968]	Time 0.423 (0.395)	Data 1.59e-04 (3.93e-03)	Tok/s 78788 (72844)	Loss/tok 3.5429 (3.5933)	LR 2.000e-03
0: TRAIN [1][200/968]	Time 0.312 (0.393)	Data 1.56e-04 (3.74e-03)	Tok/s 65923 (72691)	Loss/tok 3.2787 (3.5861)	LR 2.000e-03
0: TRAIN [1][210/968]	Time 0.543 (0.390)	Data 1.82e-04 (3.57e-03)	Tok/s 86224 (72460)	Loss/tok 3.7771 (3.5806)	LR 2.000e-03
0: TRAIN [1][220/968]	Time 0.311 (0.390)	Data 1.80e-04 (3.42e-03)	Tok/s 66592 (72481)	Loss/tok 3.3275 (3.5799)	LR 2.000e-03
0: TRAIN [1][230/968]	Time 0.310 (0.391)	Data 1.61e-04 (3.28e-03)	Tok/s 67200 (72647)	Loss/tok 3.3157 (3.5794)	LR 2.000e-03
0: TRAIN [1][240/968]	Time 0.311 (0.389)	Data 1.59e-04 (3.15e-03)	Tok/s 66822 (72391)	Loss/tok 3.2661 (3.5767)	LR 2.000e-03
0: TRAIN [1][250/968]	Time 0.311 (0.388)	Data 1.62e-04 (3.03e-03)	Tok/s 66040 (72295)	Loss/tok 3.3473 (3.5732)	LR 2.000e-03
0: TRAIN [1][260/968]	Time 0.313 (0.388)	Data 2.88e-04 (2.92e-03)	Tok/s 67075 (72384)	Loss/tok 3.3235 (3.5734)	LR 2.000e-03
0: TRAIN [1][270/968]	Time 0.423 (0.389)	Data 1.59e-04 (2.82e-03)	Tok/s 79251 (72378)	Loss/tok 3.5669 (3.5738)	LR 2.000e-03
0: TRAIN [1][280/968]	Time 0.422 (0.388)	Data 1.80e-04 (2.73e-03)	Tok/s 80495 (72322)	Loss/tok 3.5867 (3.5696)	LR 2.000e-03
0: TRAIN [1][290/968]	Time 0.422 (0.386)	Data 1.59e-04 (2.64e-03)	Tok/s 79109 (72194)	Loss/tok 3.5889 (3.5665)	LR 2.000e-03
0: TRAIN [1][300/968]	Time 0.311 (0.387)	Data 1.50e-04 (2.56e-03)	Tok/s 66107 (72161)	Loss/tok 3.2819 (3.5694)	LR 2.000e-03
0: TRAIN [1][310/968]	Time 0.424 (0.387)	Data 1.74e-04 (2.48e-03)	Tok/s 78569 (72200)	Loss/tok 3.5276 (3.5679)	LR 2.000e-03
0: TRAIN [1][320/968]	Time 0.423 (0.387)	Data 1.46e-04 (2.41e-03)	Tok/s 79338 (72211)	Loss/tok 3.5390 (3.5646)	LR 2.000e-03
0: TRAIN [1][330/968]	Time 0.310 (0.387)	Data 1.81e-04 (2.34e-03)	Tok/s 67109 (72207)	Loss/tok 3.2731 (3.5630)	LR 2.000e-03
0: TRAIN [1][340/968]	Time 0.424 (0.386)	Data 1.71e-04 (2.28e-03)	Tok/s 79445 (72210)	Loss/tok 3.5041 (3.5621)	LR 2.000e-03
0: TRAIN [1][350/968]	Time 0.312 (0.387)	Data 1.57e-04 (2.22e-03)	Tok/s 66844 (72226)	Loss/tok 3.2296 (3.5619)	LR 2.000e-03
0: TRAIN [1][360/968]	Time 0.545 (0.387)	Data 1.87e-04 (2.16e-03)	Tok/s 85324 (72329)	Loss/tok 3.7018 (3.5611)	LR 2.000e-03
0: TRAIN [1][370/968]	Time 0.544 (0.388)	Data 1.71e-04 (2.11e-03)	Tok/s 86373 (72387)	Loss/tok 3.7300 (3.5588)	LR 2.000e-03
0: TRAIN [1][380/968]	Time 0.205 (0.388)	Data 1.91e-04 (2.06e-03)	Tok/s 51006 (72443)	Loss/tok 2.8633 (3.5592)	LR 2.000e-03
0: TRAIN [1][390/968]	Time 0.425 (0.389)	Data 1.82e-04 (2.01e-03)	Tok/s 79098 (72523)	Loss/tok 3.5016 (3.5592)	LR 2.000e-03
0: TRAIN [1][400/968]	Time 0.423 (0.389)	Data 1.66e-04 (1.96e-03)	Tok/s 78933 (72608)	Loss/tok 3.5517 (3.5574)	LR 2.000e-03
0: TRAIN [1][410/968]	Time 0.311 (0.387)	Data 1.95e-04 (1.92e-03)	Tok/s 66557 (72414)	Loss/tok 3.2411 (3.5527)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][420/968]	Time 0.425 (0.389)	Data 1.60e-04 (1.88e-03)	Tok/s 79512 (72561)	Loss/tok 3.4924 (3.5533)	LR 2.000e-03
0: TRAIN [1][430/968]	Time 0.312 (0.388)	Data 1.58e-04 (1.84e-03)	Tok/s 65607 (72529)	Loss/tok 3.3825 (3.5528)	LR 2.000e-03
0: TRAIN [1][440/968]	Time 0.543 (0.387)	Data 1.69e-04 (1.80e-03)	Tok/s 86185 (72455)	Loss/tok 3.7007 (3.5503)	LR 2.000e-03
0: TRAIN [1][450/968]	Time 0.312 (0.387)	Data 1.69e-04 (1.77e-03)	Tok/s 66293 (72464)	Loss/tok 3.2477 (3.5480)	LR 2.000e-03
0: TRAIN [1][460/968]	Time 0.312 (0.388)	Data 1.76e-04 (1.73e-03)	Tok/s 66974 (72454)	Loss/tok 3.3007 (3.5478)	LR 2.000e-03
0: TRAIN [1][470/968]	Time 0.547 (0.388)	Data 5.53e-04 (1.70e-03)	Tok/s 85590 (72493)	Loss/tok 3.7103 (3.5473)	LR 2.000e-03
0: TRAIN [1][480/968]	Time 0.207 (0.387)	Data 1.76e-04 (1.67e-03)	Tok/s 49827 (72437)	Loss/tok 2.8241 (3.5443)	LR 2.000e-03
0: TRAIN [1][490/968]	Time 0.424 (0.388)	Data 1.48e-04 (1.64e-03)	Tok/s 79235 (72565)	Loss/tok 3.5813 (3.5460)	LR 2.000e-03
0: TRAIN [1][500/968]	Time 0.312 (0.387)	Data 1.52e-04 (1.61e-03)	Tok/s 66382 (72424)	Loss/tok 3.1499 (3.5421)	LR 2.000e-03
0: TRAIN [1][510/968]	Time 0.424 (0.387)	Data 1.54e-04 (1.58e-03)	Tok/s 78676 (72478)	Loss/tok 3.4525 (3.5415)	LR 2.000e-03
0: TRAIN [1][520/968]	Time 0.312 (0.387)	Data 1.59e-04 (1.55e-03)	Tok/s 66997 (72411)	Loss/tok 3.2734 (3.5392)	LR 2.000e-03
0: TRAIN [1][530/968]	Time 0.207 (0.386)	Data 1.58e-04 (1.53e-03)	Tok/s 51901 (72335)	Loss/tok 2.8040 (3.5369)	LR 2.000e-03
0: TRAIN [1][540/968]	Time 0.311 (0.385)	Data 1.50e-04 (1.50e-03)	Tok/s 66632 (72290)	Loss/tok 3.2058 (3.5349)	LR 2.000e-03
0: TRAIN [1][550/968]	Time 0.543 (0.385)	Data 1.52e-04 (1.48e-03)	Tok/s 86368 (72320)	Loss/tok 3.6616 (3.5331)	LR 2.000e-03
0: TRAIN [1][560/968]	Time 0.312 (0.385)	Data 1.51e-04 (1.45e-03)	Tok/s 65309 (72262)	Loss/tok 3.2404 (3.5313)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][570/968]	Time 0.543 (0.386)	Data 1.54e-04 (1.43e-03)	Tok/s 85563 (72307)	Loss/tok 3.5793 (3.5303)	LR 2.000e-03
0: TRAIN [1][580/968]	Time 0.423 (0.385)	Data 1.61e-04 (1.41e-03)	Tok/s 79433 (72330)	Loss/tok 3.4493 (3.5283)	LR 2.000e-03
0: TRAIN [1][590/968]	Time 0.423 (0.386)	Data 1.64e-04 (1.39e-03)	Tok/s 79561 (72409)	Loss/tok 3.4095 (3.5290)	LR 2.000e-03
0: TRAIN [1][600/968]	Time 0.312 (0.386)	Data 1.58e-04 (1.37e-03)	Tok/s 66031 (72332)	Loss/tok 3.2557 (3.5264)	LR 2.000e-03
0: TRAIN [1][610/968]	Time 0.204 (0.385)	Data 1.68e-04 (1.35e-03)	Tok/s 51953 (72275)	Loss/tok 2.7264 (3.5235)	LR 2.000e-03
0: TRAIN [1][620/968]	Time 0.311 (0.384)	Data 1.61e-04 (1.33e-03)	Tok/s 66155 (72270)	Loss/tok 3.2297 (3.5218)	LR 2.000e-03
0: TRAIN [1][630/968]	Time 0.310 (0.385)	Data 1.48e-04 (1.31e-03)	Tok/s 66876 (72264)	Loss/tok 3.2363 (3.5213)	LR 2.000e-03
0: TRAIN [1][640/968]	Time 0.311 (0.385)	Data 1.70e-04 (1.29e-03)	Tok/s 66339 (72293)	Loss/tok 3.2566 (3.5213)	LR 2.000e-03
0: TRAIN [1][650/968]	Time 0.422 (0.385)	Data 1.51e-04 (1.28e-03)	Tok/s 79978 (72316)	Loss/tok 3.4191 (3.5193)	LR 2.000e-03
0: TRAIN [1][660/968]	Time 0.684 (0.386)	Data 6.40e-04 (1.26e-03)	Tok/s 85970 (72364)	Loss/tok 3.8760 (3.5185)	LR 2.000e-03
0: TRAIN [1][670/968]	Time 0.312 (0.385)	Data 1.64e-04 (1.24e-03)	Tok/s 66252 (72277)	Loss/tok 3.1698 (3.5155)	LR 2.000e-03
0: TRAIN [1][680/968]	Time 0.425 (0.386)	Data 1.52e-04 (1.23e-03)	Tok/s 79272 (72381)	Loss/tok 3.4517 (3.5168)	LR 2.000e-03
0: TRAIN [1][690/968]	Time 0.423 (0.387)	Data 1.57e-04 (1.22e-03)	Tok/s 78564 (72465)	Loss/tok 3.5458 (3.5168)	LR 2.000e-03
0: TRAIN [1][700/968]	Time 0.423 (0.387)	Data 1.51e-04 (1.20e-03)	Tok/s 80771 (72467)	Loss/tok 3.4017 (3.5155)	LR 2.000e-03
0: TRAIN [1][710/968]	Time 0.312 (0.387)	Data 1.55e-04 (1.19e-03)	Tok/s 65498 (72486)	Loss/tok 3.1771 (3.5156)	LR 2.000e-03
0: TRAIN [1][720/968]	Time 0.424 (0.387)	Data 1.82e-04 (1.17e-03)	Tok/s 79329 (72470)	Loss/tok 3.4382 (3.5136)	LR 2.000e-03
0: TRAIN [1][730/968]	Time 0.427 (0.387)	Data 1.54e-04 (1.16e-03)	Tok/s 78219 (72505)	Loss/tok 3.4838 (3.5133)	LR 2.000e-03
0: TRAIN [1][740/968]	Time 0.313 (0.387)	Data 1.54e-04 (1.14e-03)	Tok/s 66093 (72489)	Loss/tok 3.2582 (3.5119)	LR 2.000e-03
0: TRAIN [1][750/968]	Time 0.426 (0.387)	Data 1.52e-04 (1.13e-03)	Tok/s 78850 (72433)	Loss/tok 3.4665 (3.5101)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][760/968]	Time 0.311 (0.387)	Data 1.66e-04 (1.12e-03)	Tok/s 67119 (72457)	Loss/tok 3.2439 (3.5101)	LR 2.000e-03
0: TRAIN [1][770/968]	Time 0.312 (0.387)	Data 1.54e-04 (1.11e-03)	Tok/s 65577 (72472)	Loss/tok 3.1656 (3.5088)	LR 2.000e-03
0: TRAIN [1][780/968]	Time 0.312 (0.387)	Data 4.96e-04 (1.10e-03)	Tok/s 65731 (72451)	Loss/tok 3.1892 (3.5076)	LR 2.000e-03
0: TRAIN [1][790/968]	Time 0.543 (0.387)	Data 2.18e-04 (1.08e-03)	Tok/s 86274 (72502)	Loss/tok 3.5868 (3.5075)	LR 2.000e-03
0: TRAIN [1][800/968]	Time 0.312 (0.388)	Data 2.42e-04 (1.07e-03)	Tok/s 65849 (72520)	Loss/tok 3.2406 (3.5076)	LR 2.000e-03
0: TRAIN [1][810/968]	Time 0.311 (0.387)	Data 1.64e-04 (1.06e-03)	Tok/s 65947 (72466)	Loss/tok 3.1199 (3.5056)	LR 2.000e-03
0: TRAIN [1][820/968]	Time 0.311 (0.388)	Data 1.50e-04 (1.05e-03)	Tok/s 66847 (72494)	Loss/tok 3.1621 (3.5058)	LR 2.000e-03
0: TRAIN [1][830/968]	Time 0.311 (0.388)	Data 1.49e-04 (1.04e-03)	Tok/s 65774 (72503)	Loss/tok 3.1853 (3.5040)	LR 2.000e-03
0: TRAIN [1][840/968]	Time 0.684 (0.389)	Data 1.50e-04 (1.03e-03)	Tok/s 86749 (72551)	Loss/tok 3.8225 (3.5048)	LR 2.000e-03
0: TRAIN [1][850/968]	Time 0.423 (0.389)	Data 1.51e-04 (1.02e-03)	Tok/s 78800 (72545)	Loss/tok 3.4192 (3.5030)	LR 2.000e-03
0: TRAIN [1][860/968]	Time 0.545 (0.389)	Data 1.53e-04 (1.01e-03)	Tok/s 85707 (72579)	Loss/tok 3.5437 (3.5034)	LR 2.000e-03
0: TRAIN [1][870/968]	Time 0.312 (0.389)	Data 1.74e-04 (1.00e-03)	Tok/s 66626 (72576)	Loss/tok 3.2108 (3.5019)	LR 2.000e-03
0: TRAIN [1][880/968]	Time 0.206 (0.389)	Data 1.75e-04 (9.91e-04)	Tok/s 51111 (72582)	Loss/tok 2.7609 (3.5004)	LR 2.000e-03
0: TRAIN [1][890/968]	Time 0.314 (0.389)	Data 1.51e-04 (9.82e-04)	Tok/s 66143 (72560)	Loss/tok 3.2260 (3.4991)	LR 2.000e-03
0: TRAIN [1][900/968]	Time 0.312 (0.389)	Data 1.50e-04 (9.73e-04)	Tok/s 66196 (72575)	Loss/tok 3.1495 (3.4984)	LR 2.000e-03
0: TRAIN [1][910/968]	Time 0.544 (0.389)	Data 1.56e-04 (9.64e-04)	Tok/s 85863 (72530)	Loss/tok 3.5365 (3.4968)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][920/968]	Time 0.424 (0.389)	Data 1.44e-04 (9.56e-04)	Tok/s 79232 (72566)	Loss/tok 3.3883 (3.4955)	LR 2.000e-03
0: TRAIN [1][930/968]	Time 0.423 (0.389)	Data 1.69e-04 (9.47e-04)	Tok/s 79247 (72557)	Loss/tok 3.5163 (3.4945)	LR 2.000e-03
0: TRAIN [1][940/968]	Time 0.312 (0.389)	Data 1.51e-04 (9.39e-04)	Tok/s 66427 (72547)	Loss/tok 3.1121 (3.4935)	LR 2.000e-03
0: TRAIN [1][950/968]	Time 0.311 (0.388)	Data 1.50e-04 (9.31e-04)	Tok/s 66798 (72521)	Loss/tok 3.1407 (3.4916)	LR 2.000e-03
0: TRAIN [1][960/968]	Time 0.311 (0.388)	Data 1.61e-04 (9.23e-04)	Tok/s 67030 (72502)	Loss/tok 3.1877 (3.4899)	LR 2.000e-03
:::MLL 1571246427.961 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1571246427.962 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.729 (0.729)	Decoder iters 149.0 (149.0)	Tok/s 22660 (22660)
0: Running moses detokenizer
0: BLEU(score=21.232633806486994, counts=[35523, 16729, 9070, 5166], totals=[65435, 62432, 59429, 56430], precisions=[54.28746083900054, 26.795553562275757, 15.261909168924262, 9.15470494417863], bp=1.0, sys_len=65435, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571246429.912 eval_accuracy: {"value": 21.23, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1571246429.912 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.4900	Test BLEU: 21.23
0: Performance: Epoch: 1	Training: 580107 Tok/s
0: Finished epoch 1
:::MLL 1571246429.913 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1571246429.913 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571246429.913 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 1024534867
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [2][0/968]	Time 1.167 (1.167)	Data 7.18e-01 (7.18e-01)	Tok/s 28781 (28781)	Loss/tok 3.2947 (3.2947)	LR 2.000e-03
0: TRAIN [2][10/968]	Time 0.422 (0.432)	Data 1.84e-04 (6.55e-02)	Tok/s 79085 (66728)	Loss/tok 3.3222 (3.2649)	LR 2.000e-03
0: TRAIN [2][20/968]	Time 0.543 (0.412)	Data 1.85e-04 (3.44e-02)	Tok/s 86343 (70449)	Loss/tok 3.4370 (3.2804)	LR 2.000e-03
0: TRAIN [2][30/968]	Time 0.312 (0.398)	Data 2.20e-04 (2.34e-02)	Tok/s 65949 (70713)	Loss/tok 3.0930 (3.2721)	LR 2.000e-03
0: TRAIN [2][40/968]	Time 0.422 (0.394)	Data 1.88e-04 (1.77e-02)	Tok/s 79295 (71389)	Loss/tok 3.3246 (3.2644)	LR 2.000e-03
0: TRAIN [2][50/968]	Time 0.203 (0.378)	Data 1.96e-04 (1.43e-02)	Tok/s 52123 (70353)	Loss/tok 2.6605 (3.2444)	LR 2.000e-03
0: TRAIN [2][60/968]	Time 0.310 (0.374)	Data 1.86e-04 (1.20e-02)	Tok/s 66817 (70461)	Loss/tok 3.0583 (3.2430)	LR 2.000e-03
0: TRAIN [2][70/968]	Time 0.311 (0.374)	Data 1.83e-04 (1.03e-02)	Tok/s 66139 (70585)	Loss/tok 3.0608 (3.2495)	LR 2.000e-03
0: TRAIN [2][80/968]	Time 0.311 (0.374)	Data 1.85e-04 (9.07e-03)	Tok/s 66602 (70958)	Loss/tok 3.0924 (3.2540)	LR 2.000e-03
0: TRAIN [2][90/968]	Time 0.541 (0.376)	Data 2.16e-04 (8.09e-03)	Tok/s 85425 (70865)	Loss/tok 3.5277 (3.2662)	LR 2.000e-03
0: TRAIN [2][100/968]	Time 0.309 (0.367)	Data 1.98e-04 (7.31e-03)	Tok/s 66358 (69994)	Loss/tok 3.0761 (3.2540)	LR 2.000e-03
0: TRAIN [2][110/968]	Time 0.309 (0.366)	Data 1.91e-04 (6.67e-03)	Tok/s 66985 (69973)	Loss/tok 3.1091 (3.2596)	LR 2.000e-03
0: TRAIN [2][120/968]	Time 0.423 (0.367)	Data 1.83e-04 (6.14e-03)	Tok/s 79399 (70222)	Loss/tok 3.3304 (3.2635)	LR 2.000e-03
0: TRAIN [2][130/968]	Time 0.543 (0.372)	Data 2.05e-04 (5.68e-03)	Tok/s 86273 (70704)	Loss/tok 3.5713 (3.2741)	LR 2.000e-03
0: TRAIN [2][140/968]	Time 0.311 (0.375)	Data 2.14e-04 (5.29e-03)	Tok/s 66158 (71157)	Loss/tok 3.2086 (3.2820)	LR 2.000e-03
0: TRAIN [2][150/968]	Time 0.424 (0.377)	Data 2.14e-04 (4.96e-03)	Tok/s 79489 (71280)	Loss/tok 3.2705 (3.2851)	LR 2.000e-03
0: TRAIN [2][160/968]	Time 0.424 (0.380)	Data 1.99e-04 (4.66e-03)	Tok/s 79668 (71637)	Loss/tok 3.3536 (3.2932)	LR 2.000e-03
0: TRAIN [2][170/968]	Time 0.313 (0.382)	Data 2.05e-04 (4.40e-03)	Tok/s 66470 (71865)	Loss/tok 3.0419 (3.2983)	LR 2.000e-03
0: TRAIN [2][180/968]	Time 0.683 (0.381)	Data 1.98e-04 (4.17e-03)	Tok/s 87369 (71809)	Loss/tok 3.6640 (3.2977)	LR 2.000e-03
0: TRAIN [2][190/968]	Time 0.545 (0.381)	Data 2.06e-04 (3.96e-03)	Tok/s 86454 (71874)	Loss/tok 3.4880 (3.2987)	LR 2.000e-03
0: TRAIN [2][200/968]	Time 0.313 (0.381)	Data 1.75e-04 (3.78e-03)	Tok/s 66020 (71916)	Loss/tok 3.1147 (3.2962)	LR 2.000e-03
0: TRAIN [2][210/968]	Time 0.310 (0.382)	Data 1.87e-04 (3.61e-03)	Tok/s 65918 (72052)	Loss/tok 3.0238 (3.3005)	LR 2.000e-03
0: TRAIN [2][220/968]	Time 0.544 (0.386)	Data 1.73e-04 (3.45e-03)	Tok/s 85744 (72335)	Loss/tok 3.5038 (3.3076)	LR 2.000e-03
0: TRAIN [2][230/968]	Time 0.424 (0.384)	Data 1.90e-04 (3.31e-03)	Tok/s 79215 (72264)	Loss/tok 3.3891 (3.3062)	LR 2.000e-03
0: TRAIN [2][240/968]	Time 0.422 (0.386)	Data 1.79e-04 (3.18e-03)	Tok/s 78513 (72404)	Loss/tok 3.4366 (3.3101)	LR 2.000e-03
0: TRAIN [2][250/968]	Time 0.423 (0.384)	Data 1.91e-04 (3.06e-03)	Tok/s 79860 (72248)	Loss/tok 3.4063 (3.3061)	LR 2.000e-03
0: TRAIN [2][260/968]	Time 0.426 (0.384)	Data 5.27e-04 (2.96e-03)	Tok/s 79467 (72250)	Loss/tok 3.3301 (3.3063)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][270/968]	Time 0.313 (0.383)	Data 2.26e-04 (2.86e-03)	Tok/s 65469 (72136)	Loss/tok 3.1559 (3.3045)	LR 2.000e-03
0: TRAIN [2][280/968]	Time 0.311 (0.381)	Data 1.87e-04 (2.76e-03)	Tok/s 66817 (71991)	Loss/tok 3.0430 (3.3014)	LR 2.000e-03
0: TRAIN [2][290/968]	Time 0.313 (0.382)	Data 1.82e-04 (2.68e-03)	Tok/s 66845 (72054)	Loss/tok 3.0862 (3.3060)	LR 2.000e-03
0: TRAIN [2][300/968]	Time 0.425 (0.383)	Data 2.11e-04 (2.59e-03)	Tok/s 78960 (72120)	Loss/tok 3.3293 (3.3078)	LR 2.000e-03
0: TRAIN [2][310/968]	Time 0.682 (0.386)	Data 2.03e-04 (2.52e-03)	Tok/s 86864 (72311)	Loss/tok 3.7458 (3.3148)	LR 2.000e-03
0: TRAIN [2][320/968]	Time 0.424 (0.385)	Data 2.26e-04 (2.44e-03)	Tok/s 79109 (72211)	Loss/tok 3.3295 (3.3125)	LR 2.000e-03
0: TRAIN [2][330/968]	Time 0.424 (0.383)	Data 1.90e-04 (2.38e-03)	Tok/s 78751 (72011)	Loss/tok 3.2811 (3.3089)	LR 2.000e-03
0: TRAIN [2][340/968]	Time 0.313 (0.384)	Data 2.09e-04 (2.32e-03)	Tok/s 65810 (72080)	Loss/tok 3.1074 (3.3134)	LR 2.000e-03
0: TRAIN [2][350/968]	Time 0.423 (0.384)	Data 1.75e-04 (2.26e-03)	Tok/s 79441 (72077)	Loss/tok 3.2519 (3.3115)	LR 2.000e-03
0: TRAIN [2][360/968]	Time 0.545 (0.384)	Data 1.86e-04 (2.20e-03)	Tok/s 85565 (72031)	Loss/tok 3.5098 (3.3122)	LR 2.000e-03
0: TRAIN [2][370/968]	Time 0.312 (0.384)	Data 1.94e-04 (2.15e-03)	Tok/s 65711 (72085)	Loss/tok 3.1405 (3.3129)	LR 2.000e-03
0: TRAIN [2][380/968]	Time 0.205 (0.384)	Data 1.77e-04 (2.09e-03)	Tok/s 50748 (72073)	Loss/tok 2.6522 (3.3117)	LR 2.000e-03
0: TRAIN [2][390/968]	Time 0.424 (0.384)	Data 1.81e-04 (2.05e-03)	Tok/s 78801 (72083)	Loss/tok 3.4039 (3.3122)	LR 2.000e-03
0: TRAIN [2][400/968]	Time 0.312 (0.384)	Data 1.80e-04 (2.00e-03)	Tok/s 67412 (72092)	Loss/tok 3.1947 (3.3124)	LR 2.000e-03
0: TRAIN [2][410/968]	Time 0.313 (0.386)	Data 1.74e-04 (1.96e-03)	Tok/s 66589 (72230)	Loss/tok 3.0931 (3.3165)	LR 2.000e-03
0: TRAIN [2][420/968]	Time 0.312 (0.385)	Data 1.72e-04 (1.91e-03)	Tok/s 66750 (72157)	Loss/tok 3.0810 (3.3151)	LR 2.000e-03
0: TRAIN [2][430/968]	Time 0.313 (0.385)	Data 1.87e-04 (1.87e-03)	Tok/s 66375 (72198)	Loss/tok 3.0926 (3.3151)	LR 2.000e-03
0: TRAIN [2][440/968]	Time 0.315 (0.384)	Data 1.98e-04 (1.84e-03)	Tok/s 66029 (72132)	Loss/tok 3.1586 (3.3140)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][450/968]	Time 0.684 (0.386)	Data 1.91e-04 (1.80e-03)	Tok/s 86459 (72201)	Loss/tok 3.6839 (3.3190)	LR 2.000e-03
0: TRAIN [2][460/968]	Time 0.424 (0.387)	Data 1.77e-04 (1.77e-03)	Tok/s 79681 (72361)	Loss/tok 3.2695 (3.3215)	LR 2.000e-03
0: TRAIN [2][470/968]	Time 0.422 (0.388)	Data 1.75e-04 (1.73e-03)	Tok/s 79560 (72467)	Loss/tok 3.3219 (3.3212)	LR 2.000e-03
0: TRAIN [2][480/968]	Time 0.423 (0.387)	Data 1.75e-04 (1.70e-03)	Tok/s 79893 (72406)	Loss/tok 3.3298 (3.3205)	LR 2.000e-03
0: TRAIN [2][490/968]	Time 0.424 (0.387)	Data 1.74e-04 (1.67e-03)	Tok/s 79411 (72376)	Loss/tok 3.3196 (3.3196)	LR 2.000e-03
0: TRAIN [2][500/968]	Time 0.313 (0.389)	Data 1.75e-04 (1.64e-03)	Tok/s 65074 (72487)	Loss/tok 3.1627 (3.3232)	LR 2.000e-03
0: TRAIN [2][510/968]	Time 0.311 (0.388)	Data 2.00e-04 (1.61e-03)	Tok/s 66791 (72389)	Loss/tok 3.0565 (3.3212)	LR 2.000e-03
0: TRAIN [2][520/968]	Time 0.204 (0.389)	Data 2.03e-04 (1.59e-03)	Tok/s 52052 (72485)	Loss/tok 2.6879 (3.3230)	LR 2.000e-03
0: TRAIN [2][530/968]	Time 0.313 (0.388)	Data 1.88e-04 (1.56e-03)	Tok/s 66331 (72397)	Loss/tok 3.0670 (3.3207)	LR 2.000e-03
0: TRAIN [2][540/968]	Time 0.683 (0.388)	Data 1.79e-04 (1.54e-03)	Tok/s 87303 (72395)	Loss/tok 3.7144 (3.3202)	LR 2.000e-03
0: TRAIN [2][550/968]	Time 0.426 (0.389)	Data 1.90e-04 (1.51e-03)	Tok/s 78802 (72536)	Loss/tok 3.3210 (3.3232)	LR 2.000e-03
0: TRAIN [2][560/968]	Time 0.312 (0.388)	Data 1.87e-04 (1.49e-03)	Tok/s 65704 (72453)	Loss/tok 3.1809 (3.3214)	LR 2.000e-03
0: TRAIN [2][570/968]	Time 0.312 (0.388)	Data 2.38e-04 (1.47e-03)	Tok/s 66391 (72444)	Loss/tok 3.1842 (3.3225)	LR 2.000e-03
0: TRAIN [2][580/968]	Time 0.424 (0.388)	Data 1.72e-04 (1.44e-03)	Tok/s 78863 (72408)	Loss/tok 3.2421 (3.3210)	LR 2.000e-03
0: TRAIN [2][590/968]	Time 0.311 (0.386)	Data 2.13e-04 (1.42e-03)	Tok/s 67041 (72275)	Loss/tok 3.0895 (3.3184)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][600/968]	Time 0.312 (0.386)	Data 2.02e-04 (1.40e-03)	Tok/s 67241 (72238)	Loss/tok 3.1303 (3.3185)	LR 2.000e-03
0: TRAIN [2][610/968]	Time 0.205 (0.386)	Data 2.11e-04 (1.38e-03)	Tok/s 51774 (72229)	Loss/tok 2.6033 (3.3175)	LR 2.000e-03
0: TRAIN [2][620/968]	Time 0.314 (0.386)	Data 1.82e-04 (1.36e-03)	Tok/s 66480 (72257)	Loss/tok 3.1162 (3.3179)	LR 2.000e-03
0: TRAIN [2][630/968]	Time 0.544 (0.386)	Data 2.15e-04 (1.35e-03)	Tok/s 85860 (72259)	Loss/tok 3.5545 (3.3183)	LR 2.000e-03
0: TRAIN [2][640/968]	Time 0.311 (0.387)	Data 1.74e-04 (1.33e-03)	Tok/s 66592 (72283)	Loss/tok 3.0718 (3.3186)	LR 2.000e-03
0: TRAIN [2][650/968]	Time 0.423 (0.387)	Data 1.95e-04 (1.31e-03)	Tok/s 79454 (72346)	Loss/tok 3.2582 (3.3189)	LR 2.000e-03
0: TRAIN [2][660/968]	Time 0.682 (0.388)	Data 1.99e-04 (1.29e-03)	Tok/s 87770 (72365)	Loss/tok 3.6502 (3.3195)	LR 2.000e-03
0: TRAIN [2][670/968]	Time 0.424 (0.389)	Data 1.75e-04 (1.28e-03)	Tok/s 79563 (72452)	Loss/tok 3.2788 (3.3214)	LR 2.000e-03
0: TRAIN [2][680/968]	Time 0.543 (0.389)	Data 1.93e-04 (1.26e-03)	Tok/s 84728 (72495)	Loss/tok 3.4899 (3.3218)	LR 2.000e-03
0: TRAIN [2][690/968]	Time 0.424 (0.390)	Data 2.04e-04 (1.25e-03)	Tok/s 79727 (72595)	Loss/tok 3.3453 (3.3235)	LR 2.000e-03
0: TRAIN [2][700/968]	Time 0.311 (0.391)	Data 1.79e-04 (1.23e-03)	Tok/s 65996 (72669)	Loss/tok 3.0798 (3.3247)	LR 2.000e-03
0: TRAIN [2][710/968]	Time 0.203 (0.390)	Data 2.08e-04 (1.22e-03)	Tok/s 52106 (72584)	Loss/tok 2.7156 (3.3231)	LR 2.000e-03
0: TRAIN [2][720/968]	Time 0.422 (0.390)	Data 1.87e-04 (1.20e-03)	Tok/s 79040 (72614)	Loss/tok 3.2883 (3.3223)	LR 2.000e-03
0: TRAIN [2][730/968]	Time 0.423 (0.390)	Data 1.95e-04 (1.19e-03)	Tok/s 79496 (72594)	Loss/tok 3.3700 (3.3212)	LR 2.000e-03
0: TRAIN [2][740/968]	Time 0.424 (0.390)	Data 1.91e-04 (1.18e-03)	Tok/s 77907 (72619)	Loss/tok 3.3500 (3.3210)	LR 2.000e-03
0: TRAIN [2][750/968]	Time 0.311 (0.389)	Data 1.77e-04 (1.16e-03)	Tok/s 65224 (72605)	Loss/tok 3.0731 (3.3192)	LR 2.000e-03
0: TRAIN [2][760/968]	Time 0.424 (0.389)	Data 1.81e-04 (1.15e-03)	Tok/s 79770 (72621)	Loss/tok 3.2941 (3.3186)	LR 2.000e-03
0: TRAIN [2][770/968]	Time 0.312 (0.389)	Data 1.94e-04 (1.14e-03)	Tok/s 65924 (72630)	Loss/tok 3.0792 (3.3176)	LR 2.000e-03
0: TRAIN [2][780/968]	Time 0.423 (0.389)	Data 4.41e-04 (1.13e-03)	Tok/s 79613 (72616)	Loss/tok 3.3154 (3.3165)	LR 2.000e-03
0: TRAIN [2][790/968]	Time 0.312 (0.388)	Data 1.85e-04 (1.12e-03)	Tok/s 65721 (72571)	Loss/tok 3.0671 (3.3146)	LR 2.000e-03
0: TRAIN [2][800/968]	Time 0.312 (0.387)	Data 1.80e-04 (1.10e-03)	Tok/s 67467 (72520)	Loss/tok 3.0419 (3.3132)	LR 2.000e-03
0: TRAIN [2][810/968]	Time 0.312 (0.388)	Data 2.01e-04 (1.09e-03)	Tok/s 66233 (72571)	Loss/tok 3.0541 (3.3142)	LR 2.000e-03
0: TRAIN [2][820/968]	Time 0.204 (0.388)	Data 1.77e-04 (1.08e-03)	Tok/s 51013 (72554)	Loss/tok 2.7334 (3.3140)	LR 2.000e-03
0: TRAIN [2][830/968]	Time 0.423 (0.388)	Data 1.96e-04 (1.07e-03)	Tok/s 79225 (72584)	Loss/tok 3.3404 (3.3135)	LR 2.000e-03
0: TRAIN [2][840/968]	Time 0.423 (0.387)	Data 1.88e-04 (1.06e-03)	Tok/s 79044 (72534)	Loss/tok 3.2771 (3.3122)	LR 2.000e-03
0: TRAIN [2][850/968]	Time 0.423 (0.388)	Data 1.76e-04 (1.05e-03)	Tok/s 79483 (72540)	Loss/tok 3.3362 (3.3123)	LR 2.000e-03
0: TRAIN [2][860/968]	Time 0.312 (0.387)	Data 5.03e-04 (1.04e-03)	Tok/s 66222 (72506)	Loss/tok 3.0677 (3.3112)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][870/968]	Time 0.310 (0.387)	Data 2.09e-04 (1.03e-03)	Tok/s 66422 (72499)	Loss/tok 3.0510 (3.3115)	LR 2.000e-03
0: TRAIN [2][880/968]	Time 0.544 (0.388)	Data 2.05e-04 (1.02e-03)	Tok/s 85839 (72562)	Loss/tok 3.4445 (3.3135)	LR 2.000e-03
0: TRAIN [2][890/968]	Time 0.312 (0.388)	Data 1.90e-04 (1.01e-03)	Tok/s 66115 (72544)	Loss/tok 3.1030 (3.3128)	LR 2.000e-03
0: TRAIN [2][900/968]	Time 0.544 (0.388)	Data 1.71e-04 (1.00e-03)	Tok/s 86337 (72576)	Loss/tok 3.4263 (3.3125)	LR 2.000e-03
0: TRAIN [2][910/968]	Time 0.312 (0.388)	Data 2.05e-04 (9.96e-04)	Tok/s 66322 (72564)	Loss/tok 3.1685 (3.3125)	LR 2.000e-03
0: TRAIN [2][920/968]	Time 0.310 (0.387)	Data 1.78e-04 (9.87e-04)	Tok/s 67183 (72550)	Loss/tok 3.0441 (3.3112)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][930/968]	Time 0.311 (0.387)	Data 1.86e-04 (9.79e-04)	Tok/s 66518 (72503)	Loss/tok 3.0512 (3.3105)	LR 2.000e-03
0: TRAIN [2][940/968]	Time 0.311 (0.387)	Data 1.71e-04 (9.71e-04)	Tok/s 66897 (72542)	Loss/tok 3.0363 (3.3102)	LR 2.000e-03
0: TRAIN [2][950/968]	Time 0.205 (0.388)	Data 1.98e-04 (9.63e-04)	Tok/s 51037 (72600)	Loss/tok 2.6266 (3.3108)	LR 2.000e-03
0: TRAIN [2][960/968]	Time 0.311 (0.388)	Data 1.86e-04 (9.54e-04)	Tok/s 66005 (72583)	Loss/tok 3.1437 (3.3100)	LR 2.000e-03
:::MLL 1571246807.553 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1571246807.554 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.656 (0.656)	Decoder iters 111.0 (111.0)	Tok/s 24994 (24994)
0: Running moses detokenizer
0: BLEU(score=22.48182343883682, counts=[36262, 17610, 9812, 5685], totals=[65703, 62700, 59697, 56698], precisions=[55.19078276486614, 28.086124401913874, 16.436336834346786, 10.026808705774455], bp=1.0, sys_len=65703, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571246809.420 eval_accuracy: {"value": 22.48, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1571246809.420 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.3058	Test BLEU: 22.48
0: Performance: Epoch: 2	Training: 580529 Tok/s
0: Finished epoch 2
:::MLL 1571246809.420 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1571246809.421 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571246809.421 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 787281009
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [3][0/968]	Time 1.460 (1.460)	Data 7.08e-01 (7.08e-01)	Tok/s 41312 (41312)	Loss/tok 3.4650 (3.4650)	LR 2.000e-03
0: TRAIN [3][10/968]	Time 0.312 (0.491)	Data 1.76e-04 (6.45e-02)	Tok/s 65557 (70007)	Loss/tok 3.0341 (3.2811)	LR 2.000e-03
0: TRAIN [3][20/968]	Time 0.681 (0.450)	Data 1.47e-04 (3.39e-02)	Tok/s 87389 (71770)	Loss/tok 3.6678 (3.2720)	LR 2.000e-03
0: TRAIN [3][30/968]	Time 0.311 (0.420)	Data 1.57e-04 (2.30e-02)	Tok/s 65458 (71601)	Loss/tok 2.9760 (3.2287)	LR 2.000e-03
0: TRAIN [3][40/968]	Time 0.311 (0.413)	Data 1.99e-04 (1.74e-02)	Tok/s 65757 (71929)	Loss/tok 2.9573 (3.2224)	LR 2.000e-03
0: TRAIN [3][50/968]	Time 0.311 (0.409)	Data 1.75e-04 (1.41e-02)	Tok/s 66350 (72172)	Loss/tok 3.0632 (3.2225)	LR 2.000e-03
0: TRAIN [3][60/968]	Time 0.312 (0.405)	Data 2.33e-04 (1.18e-02)	Tok/s 66446 (72368)	Loss/tok 3.0197 (3.2143)	LR 2.000e-03
0: TRAIN [3][70/968]	Time 0.311 (0.398)	Data 1.62e-04 (1.02e-02)	Tok/s 65877 (72109)	Loss/tok 2.9837 (3.2020)	LR 2.000e-03
0: TRAIN [3][80/968]	Time 0.313 (0.395)	Data 1.69e-04 (8.94e-03)	Tok/s 65743 (71870)	Loss/tok 2.9861 (3.2017)	LR 2.000e-03
0: TRAIN [3][90/968]	Time 0.312 (0.396)	Data 1.55e-04 (7.97e-03)	Tok/s 65756 (72180)	Loss/tok 3.0045 (3.2050)	LR 2.000e-03
0: TRAIN [3][100/968]	Time 0.423 (0.404)	Data 2.32e-04 (7.20e-03)	Tok/s 79957 (73011)	Loss/tok 3.1906 (3.2178)	LR 2.000e-03
0: TRAIN [3][110/968]	Time 0.313 (0.397)	Data 1.52e-04 (6.57e-03)	Tok/s 66127 (72435)	Loss/tok 2.9620 (3.2085)	LR 2.000e-03
0: TRAIN [3][120/968]	Time 0.313 (0.396)	Data 1.50e-04 (6.04e-03)	Tok/s 65622 (72448)	Loss/tok 2.9506 (3.2111)	LR 2.000e-03
0: TRAIN [3][130/968]	Time 0.311 (0.397)	Data 1.66e-04 (5.59e-03)	Tok/s 66928 (72534)	Loss/tok 3.0515 (3.2164)	LR 2.000e-03
0: TRAIN [3][140/968]	Time 0.311 (0.394)	Data 1.48e-04 (5.21e-03)	Tok/s 65080 (72200)	Loss/tok 3.0384 (3.2147)	LR 2.000e-03
0: TRAIN [3][150/968]	Time 0.311 (0.392)	Data 1.46e-04 (4.87e-03)	Tok/s 67017 (72187)	Loss/tok 3.1102 (3.2131)	LR 2.000e-03
0: TRAIN [3][160/968]	Time 0.311 (0.390)	Data 1.70e-04 (4.58e-03)	Tok/s 67085 (72103)	Loss/tok 2.9281 (3.2076)	LR 2.000e-03
0: TRAIN [3][170/968]	Time 0.424 (0.391)	Data 1.59e-04 (4.32e-03)	Tok/s 79401 (72291)	Loss/tok 3.1927 (3.2084)	LR 2.000e-03
0: TRAIN [3][180/968]	Time 0.426 (0.388)	Data 1.86e-04 (4.09e-03)	Tok/s 78705 (72114)	Loss/tok 3.2226 (3.2050)	LR 2.000e-03
0: TRAIN [3][190/968]	Time 0.204 (0.389)	Data 1.52e-04 (3.89e-03)	Tok/s 52335 (72216)	Loss/tok 2.6217 (3.2077)	LR 2.000e-03
0: TRAIN [3][200/968]	Time 0.312 (0.387)	Data 1.55e-04 (3.70e-03)	Tok/s 66568 (72067)	Loss/tok 2.9413 (3.2037)	LR 2.000e-03
0: TRAIN [3][210/968]	Time 0.426 (0.391)	Data 1.56e-04 (3.54e-03)	Tok/s 78415 (72472)	Loss/tok 3.2608 (3.2124)	LR 2.000e-03
0: TRAIN [3][220/968]	Time 0.543 (0.390)	Data 1.63e-04 (3.39e-03)	Tok/s 86309 (72412)	Loss/tok 3.3858 (3.2112)	LR 2.000e-03
0: TRAIN [3][230/968]	Time 0.423 (0.389)	Data 1.53e-04 (3.25e-03)	Tok/s 79900 (72272)	Loss/tok 3.1609 (3.2084)	LR 2.000e-03
0: TRAIN [3][240/968]	Time 0.545 (0.388)	Data 1.50e-04 (3.12e-03)	Tok/s 85240 (72144)	Loss/tok 3.4671 (3.2099)	LR 2.000e-03
0: TRAIN [3][250/968]	Time 0.313 (0.390)	Data 2.04e-04 (3.00e-03)	Tok/s 65485 (72343)	Loss/tok 3.0581 (3.2123)	LR 2.000e-03
0: TRAIN [3][260/968]	Time 0.311 (0.389)	Data 1.55e-04 (2.89e-03)	Tok/s 67014 (72184)	Loss/tok 3.0066 (3.2158)	LR 2.000e-03
0: TRAIN [3][270/968]	Time 0.205 (0.388)	Data 1.54e-04 (2.79e-03)	Tok/s 51464 (72046)	Loss/tok 2.5853 (3.2146)	LR 2.000e-03
0: TRAIN [3][280/968]	Time 0.685 (0.389)	Data 1.62e-04 (2.70e-03)	Tok/s 86552 (72175)	Loss/tok 3.6301 (3.2186)	LR 2.000e-03
0: TRAIN [3][290/968]	Time 0.204 (0.389)	Data 1.75e-04 (2.61e-03)	Tok/s 50755 (72165)	Loss/tok 2.6339 (3.2191)	LR 2.000e-03
0: TRAIN [3][300/968]	Time 0.204 (0.390)	Data 1.52e-04 (2.53e-03)	Tok/s 51659 (72196)	Loss/tok 2.6127 (3.2219)	LR 2.000e-03
0: TRAIN [3][310/968]	Time 0.203 (0.390)	Data 1.65e-04 (2.45e-03)	Tok/s 52967 (72263)	Loss/tok 2.6058 (3.2224)	LR 2.000e-03
0: TRAIN [3][320/968]	Time 0.310 (0.389)	Data 1.49e-04 (2.38e-03)	Tok/s 67247 (72229)	Loss/tok 3.0222 (3.2199)	LR 2.000e-03
0: TRAIN [3][330/968]	Time 0.423 (0.390)	Data 1.51e-04 (2.32e-03)	Tok/s 79186 (72326)	Loss/tok 3.2080 (3.2213)	LR 2.000e-03
0: TRAIN [3][340/968]	Time 0.425 (0.390)	Data 1.52e-04 (2.25e-03)	Tok/s 79272 (72397)	Loss/tok 3.2254 (3.2201)	LR 2.000e-03
0: TRAIN [3][350/968]	Time 0.543 (0.390)	Data 1.54e-04 (2.19e-03)	Tok/s 85992 (72435)	Loss/tok 3.3888 (3.2183)	LR 2.000e-03
0: TRAIN [3][360/968]	Time 0.313 (0.389)	Data 1.66e-04 (2.14e-03)	Tok/s 65571 (72436)	Loss/tok 2.9966 (3.2177)	LR 2.000e-03
0: TRAIN [3][370/968]	Time 0.423 (0.387)	Data 1.57e-04 (2.08e-03)	Tok/s 79649 (72261)	Loss/tok 3.1368 (3.2143)	LR 2.000e-03
0: TRAIN [3][380/968]	Time 0.313 (0.388)	Data 1.56e-04 (2.04e-03)	Tok/s 64789 (72332)	Loss/tok 2.9437 (3.2144)	LR 2.000e-03
0: TRAIN [3][390/968]	Time 0.205 (0.388)	Data 8.52e-04 (1.99e-03)	Tok/s 51706 (72439)	Loss/tok 2.6292 (3.2136)	LR 2.000e-03
0: TRAIN [3][400/968]	Time 0.205 (0.388)	Data 1.49e-04 (1.94e-03)	Tok/s 51937 (72411)	Loss/tok 2.5545 (3.2129)	LR 2.000e-03
0: TRAIN [3][410/968]	Time 0.424 (0.388)	Data 1.49e-04 (1.90e-03)	Tok/s 79313 (72514)	Loss/tok 3.2284 (3.2129)	LR 2.000e-03
0: TRAIN [3][420/968]	Time 0.311 (0.389)	Data 1.52e-04 (1.86e-03)	Tok/s 67661 (72556)	Loss/tok 3.0037 (3.2145)	LR 2.000e-03
0: TRAIN [3][430/968]	Time 0.310 (0.390)	Data 1.46e-04 (1.82e-03)	Tok/s 66490 (72631)	Loss/tok 2.9686 (3.2155)	LR 2.000e-03
0: TRAIN [3][440/968]	Time 0.310 (0.389)	Data 1.44e-04 (1.78e-03)	Tok/s 67990 (72571)	Loss/tok 2.9820 (3.2157)	LR 2.000e-03
0: TRAIN [3][450/968]	Time 0.425 (0.389)	Data 1.46e-04 (1.75e-03)	Tok/s 79764 (72551)	Loss/tok 3.1154 (3.2135)	LR 2.000e-03
0: TRAIN [3][460/968]	Time 0.313 (0.388)	Data 1.52e-04 (1.71e-03)	Tok/s 67465 (72478)	Loss/tok 2.9316 (3.2118)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][470/968]	Time 0.311 (0.388)	Data 1.51e-04 (1.68e-03)	Tok/s 66898 (72488)	Loss/tok 3.0563 (3.2125)	LR 2.000e-03
0: TRAIN [3][480/968]	Time 0.423 (0.388)	Data 1.49e-04 (1.65e-03)	Tok/s 79131 (72487)	Loss/tok 3.1995 (3.2129)	LR 2.000e-03
0: TRAIN [3][490/968]	Time 0.312 (0.387)	Data 1.48e-04 (1.62e-03)	Tok/s 65879 (72476)	Loss/tok 3.0282 (3.2121)	LR 2.000e-03
0: TRAIN [3][500/968]	Time 0.312 (0.388)	Data 1.51e-04 (1.59e-03)	Tok/s 65913 (72502)	Loss/tok 3.0559 (3.2141)	LR 2.000e-03
0: TRAIN [3][510/968]	Time 0.205 (0.387)	Data 1.51e-04 (1.56e-03)	Tok/s 50499 (72447)	Loss/tok 2.6231 (3.2134)	LR 2.000e-03
0: TRAIN [3][520/968]	Time 0.311 (0.387)	Data 2.08e-04 (1.54e-03)	Tok/s 66906 (72457)	Loss/tok 3.0913 (3.2135)	LR 2.000e-03
0: TRAIN [3][530/968]	Time 0.204 (0.388)	Data 2.11e-04 (1.51e-03)	Tok/s 52837 (72512)	Loss/tok 2.6191 (3.2134)	LR 2.000e-03
0: TRAIN [3][540/968]	Time 0.309 (0.387)	Data 1.48e-04 (1.49e-03)	Tok/s 67233 (72444)	Loss/tok 2.9641 (3.2113)	LR 2.000e-03
0: TRAIN [3][550/968]	Time 0.684 (0.388)	Data 1.60e-04 (1.46e-03)	Tok/s 86719 (72555)	Loss/tok 3.5633 (3.2151)	LR 2.000e-03
0: TRAIN [3][560/968]	Time 0.424 (0.387)	Data 1.51e-04 (1.44e-03)	Tok/s 79142 (72507)	Loss/tok 3.1954 (3.2133)	LR 2.000e-03
0: TRAIN [3][570/968]	Time 0.312 (0.389)	Data 1.62e-04 (1.42e-03)	Tok/s 65710 (72616)	Loss/tok 2.9531 (3.2157)	LR 2.000e-03
0: TRAIN [3][580/968]	Time 0.312 (0.388)	Data 1.46e-04 (1.40e-03)	Tok/s 65841 (72560)	Loss/tok 2.9878 (3.2151)	LR 2.000e-03
0: TRAIN [3][590/968]	Time 0.206 (0.388)	Data 1.47e-04 (1.37e-03)	Tok/s 51300 (72582)	Loss/tok 2.6530 (3.2150)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][600/968]	Time 0.543 (0.389)	Data 1.46e-04 (1.35e-03)	Tok/s 86499 (72675)	Loss/tok 3.3695 (3.2166)	LR 2.000e-03
0: TRAIN [3][610/968]	Time 0.545 (0.390)	Data 1.48e-04 (1.33e-03)	Tok/s 85474 (72738)	Loss/tok 3.4935 (3.2178)	LR 2.000e-03
0: TRAIN [3][620/968]	Time 0.205 (0.389)	Data 1.62e-04 (1.32e-03)	Tok/s 51520 (72733)	Loss/tok 2.5592 (3.2177)	LR 2.000e-03
0: TRAIN [3][630/968]	Time 0.310 (0.389)	Data 1.68e-04 (1.30e-03)	Tok/s 67035 (72746)	Loss/tok 2.9763 (3.2167)	LR 2.000e-03
0: TRAIN [3][640/968]	Time 0.311 (0.389)	Data 1.46e-04 (1.28e-03)	Tok/s 66927 (72711)	Loss/tok 2.9367 (3.2147)	LR 2.000e-03
0: TRAIN [3][650/968]	Time 0.423 (0.388)	Data 1.52e-04 (1.26e-03)	Tok/s 78929 (72693)	Loss/tok 3.2438 (3.2141)	LR 2.000e-03
0: TRAIN [3][660/968]	Time 0.204 (0.388)	Data 1.49e-04 (1.25e-03)	Tok/s 51733 (72630)	Loss/tok 2.5893 (3.2140)	LR 2.000e-03
0: TRAIN [3][670/968]	Time 0.423 (0.387)	Data 1.61e-04 (1.23e-03)	Tok/s 79049 (72589)	Loss/tok 3.2700 (3.2130)	LR 2.000e-03
0: TRAIN [3][680/968]	Time 0.424 (0.387)	Data 1.46e-04 (1.22e-03)	Tok/s 79440 (72591)	Loss/tok 3.1756 (3.2127)	LR 2.000e-03
0: TRAIN [3][690/968]	Time 0.311 (0.386)	Data 1.42e-04 (1.20e-03)	Tok/s 66173 (72538)	Loss/tok 3.1111 (3.2113)	LR 2.000e-03
0: TRAIN [3][700/968]	Time 0.423 (0.387)	Data 1.44e-04 (1.18e-03)	Tok/s 79141 (72618)	Loss/tok 3.2293 (3.2128)	LR 2.000e-03
0: TRAIN [3][710/968]	Time 0.424 (0.387)	Data 1.46e-04 (1.17e-03)	Tok/s 79780 (72578)	Loss/tok 3.2192 (3.2131)	LR 2.000e-03
0: TRAIN [3][720/968]	Time 0.544 (0.387)	Data 1.49e-04 (1.16e-03)	Tok/s 85698 (72608)	Loss/tok 3.3806 (3.2126)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][730/968]	Time 0.684 (0.387)	Data 1.53e-04 (1.14e-03)	Tok/s 87202 (72608)	Loss/tok 3.6092 (3.2133)	LR 2.000e-03
0: TRAIN [3][740/968]	Time 0.311 (0.387)	Data 1.86e-04 (1.13e-03)	Tok/s 67307 (72575)	Loss/tok 2.9609 (3.2116)	LR 2.000e-03
0: TRAIN [3][750/968]	Time 0.683 (0.387)	Data 1.55e-04 (1.12e-03)	Tok/s 87439 (72584)	Loss/tok 3.5320 (3.2124)	LR 2.000e-03
0: TRAIN [3][760/968]	Time 0.311 (0.386)	Data 1.92e-04 (1.10e-03)	Tok/s 65877 (72566)	Loss/tok 2.9381 (3.2115)	LR 2.000e-03
0: TRAIN [3][770/968]	Time 0.206 (0.386)	Data 1.55e-04 (1.09e-03)	Tok/s 51007 (72549)	Loss/tok 2.6081 (3.2111)	LR 2.000e-03
0: TRAIN [3][780/968]	Time 0.311 (0.386)	Data 1.69e-04 (1.08e-03)	Tok/s 66302 (72511)	Loss/tok 2.9688 (3.2113)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][790/968]	Time 0.543 (0.386)	Data 1.45e-04 (1.07e-03)	Tok/s 86004 (72511)	Loss/tok 3.3565 (3.2114)	LR 2.000e-03
0: TRAIN [3][800/968]	Time 0.544 (0.387)	Data 1.76e-04 (1.06e-03)	Tok/s 85484 (72517)	Loss/tok 3.4753 (3.2124)	LR 2.000e-03
0: TRAIN [3][810/968]	Time 0.311 (0.386)	Data 1.54e-04 (1.05e-03)	Tok/s 66677 (72439)	Loss/tok 2.9516 (3.2111)	LR 2.000e-03
0: TRAIN [3][820/968]	Time 0.312 (0.386)	Data 1.83e-04 (1.04e-03)	Tok/s 66831 (72426)	Loss/tok 2.9984 (3.2110)	LR 2.000e-03
0: TRAIN [3][830/968]	Time 0.543 (0.386)	Data 1.59e-04 (1.03e-03)	Tok/s 85878 (72425)	Loss/tok 3.3804 (3.2114)	LR 2.000e-03
0: TRAIN [3][840/968]	Time 0.544 (0.386)	Data 1.64e-04 (1.02e-03)	Tok/s 85864 (72425)	Loss/tok 3.3945 (3.2114)	LR 2.000e-03
0: TRAIN [3][850/968]	Time 0.311 (0.386)	Data 1.49e-04 (1.01e-03)	Tok/s 66393 (72457)	Loss/tok 2.9967 (3.2113)	LR 2.000e-03
0: TRAIN [3][860/968]	Time 0.545 (0.386)	Data 1.59e-04 (9.96e-04)	Tok/s 85874 (72429)	Loss/tok 3.4073 (3.2111)	LR 2.000e-03
0: TRAIN [3][870/968]	Time 0.684 (0.386)	Data 1.50e-04 (9.86e-04)	Tok/s 87258 (72426)	Loss/tok 3.6595 (3.2120)	LR 2.000e-03
0: TRAIN [3][880/968]	Time 0.685 (0.386)	Data 1.47e-04 (9.77e-04)	Tok/s 87152 (72432)	Loss/tok 3.6125 (3.2123)	LR 2.000e-03
0: TRAIN [3][890/968]	Time 0.546 (0.386)	Data 1.62e-04 (9.68e-04)	Tok/s 85454 (72474)	Loss/tok 3.3907 (3.2135)	LR 2.000e-03
0: TRAIN [3][900/968]	Time 0.544 (0.387)	Data 1.49e-04 (9.59e-04)	Tok/s 85512 (72512)	Loss/tok 3.4129 (3.2142)	LR 2.000e-03
0: TRAIN [3][910/968]	Time 0.204 (0.386)	Data 1.50e-04 (9.51e-04)	Tok/s 52118 (72434)	Loss/tok 2.5976 (3.2134)	LR 2.000e-03
0: TRAIN [3][920/968]	Time 0.422 (0.387)	Data 1.58e-04 (9.42e-04)	Tok/s 79922 (72467)	Loss/tok 3.1384 (3.2139)	LR 2.000e-03
0: TRAIN [3][930/968]	Time 0.543 (0.387)	Data 1.62e-04 (9.34e-04)	Tok/s 85496 (72502)	Loss/tok 3.4024 (3.2142)	LR 2.000e-03
0: TRAIN [3][940/968]	Time 0.314 (0.387)	Data 1.49e-04 (9.26e-04)	Tok/s 65803 (72520)	Loss/tok 3.0340 (3.2144)	LR 2.000e-03
0: TRAIN [3][950/968]	Time 0.309 (0.387)	Data 1.54e-04 (9.18e-04)	Tok/s 68311 (72544)	Loss/tok 2.9999 (3.2153)	LR 2.000e-03
0: TRAIN [3][960/968]	Time 0.545 (0.388)	Data 1.51e-04 (9.10e-04)	Tok/s 86211 (72571)	Loss/tok 3.3435 (3.2157)	LR 2.000e-03
:::MLL 1571247187.030 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 524}}
:::MLL 1571247187.031 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.736 (0.736)	Decoder iters 149.0 (149.0)	Tok/s 22442 (22442)
0: Running moses detokenizer
0: BLEU(score=23.101115156449268, counts=[36664, 18046, 10149, 5931], totals=[65747, 62744, 59742, 56743], precisions=[55.765282066101875, 28.7613158230269, 16.98804860901878, 10.452390603246215], bp=1.0, sys_len=65747, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571247188.967 eval_accuracy: {"value": 23.1, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 535}}
:::MLL 1571247188.968 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 3	Training Loss: 3.2158	Test BLEU: 23.10
0: Performance: Epoch: 3	Training: 580590 Tok/s
0: Finished epoch 3
:::MLL 1571247188.968 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 557}}
:::MLL 1571247188.968 block_start: {"value": null, "metadata": {"first_epoch_num": 5, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571247188.969 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 514}}
0: Starting epoch 4
0: Executing preallocation
0: Sampler for epoch 4 uses seed 2332065770
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [4][0/968]	Time 0.944 (0.944)	Data 7.24e-01 (7.24e-01)	Tok/s 11024 (11024)	Loss/tok 2.5802 (2.5802)	LR 2.000e-03
0: TRAIN [4][10/968]	Time 0.311 (0.424)	Data 1.74e-04 (6.60e-02)	Tok/s 66271 (61623)	Loss/tok 3.0198 (3.1552)	LR 2.000e-03
0: TRAIN [4][20/968]	Time 0.311 (0.387)	Data 2.02e-04 (3.46e-02)	Tok/s 66372 (65398)	Loss/tok 2.9732 (3.1027)	LR 2.000e-03
0: TRAIN [4][30/968]	Time 0.317 (0.394)	Data 1.76e-04 (2.35e-02)	Tok/s 64614 (67953)	Loss/tok 2.9762 (3.1199)	LR 2.000e-03
0: TRAIN [4][40/968]	Time 0.425 (0.393)	Data 2.02e-04 (1.79e-02)	Tok/s 79156 (69542)	Loss/tok 3.2005 (3.1238)	LR 2.000e-03
0: TRAIN [4][50/968]	Time 0.207 (0.380)	Data 4.00e-04 (1.44e-02)	Tok/s 52035 (68942)	Loss/tok 2.5437 (3.1050)	LR 2.000e-03
0: TRAIN [4][60/968]	Time 0.423 (0.377)	Data 1.78e-04 (1.21e-02)	Tok/s 79805 (69204)	Loss/tok 3.2068 (3.1008)	LR 2.000e-03
0: TRAIN [4][70/968]	Time 0.543 (0.377)	Data 1.73e-04 (1.04e-02)	Tok/s 85327 (69807)	Loss/tok 3.2573 (3.0964)	LR 2.000e-03
0: TRAIN [4][80/968]	Time 0.544 (0.382)	Data 1.75e-04 (9.14e-03)	Tok/s 85392 (70646)	Loss/tok 3.3147 (3.1046)	LR 2.000e-03
0: TRAIN [4][90/968]	Time 0.311 (0.384)	Data 1.91e-04 (8.16e-03)	Tok/s 66896 (71020)	Loss/tok 2.8332 (3.1117)	LR 2.000e-03
0: TRAIN [4][100/968]	Time 0.311 (0.386)	Data 1.72e-04 (7.37e-03)	Tok/s 66345 (71037)	Loss/tok 2.9359 (3.1239)	LR 2.000e-03
0: TRAIN [4][110/968]	Time 0.422 (0.380)	Data 1.73e-04 (6.72e-03)	Tok/s 79750 (70716)	Loss/tok 3.0532 (3.1144)	LR 2.000e-03
0: TRAIN [4][120/968]	Time 0.311 (0.377)	Data 1.77e-04 (6.18e-03)	Tok/s 66212 (70598)	Loss/tok 2.9407 (3.1106)	LR 2.000e-03
0: TRAIN [4][130/968]	Time 0.425 (0.378)	Data 1.90e-04 (5.73e-03)	Tok/s 78677 (70845)	Loss/tok 3.1894 (3.1128)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [4][140/968]	Time 0.203 (0.381)	Data 2.23e-04 (5.34e-03)	Tok/s 51271 (71146)	Loss/tok 2.5849 (3.1209)	LR 2.000e-03
0: TRAIN [4][150/968]	Time 0.684 (0.383)	Data 1.77e-04 (5.00e-03)	Tok/s 87874 (71296)	Loss/tok 3.5539 (3.1303)	LR 2.000e-03
0: TRAIN [4][160/968]	Time 0.544 (0.381)	Data 1.96e-04 (4.70e-03)	Tok/s 85857 (71153)	Loss/tok 3.3010 (3.1286)	LR 2.000e-03
0: TRAIN [4][170/968]	Time 0.311 (0.380)	Data 1.89e-04 (4.43e-03)	Tok/s 65509 (71198)	Loss/tok 2.9092 (3.1273)	LR 2.000e-03
0: TRAIN [4][180/968]	Time 0.203 (0.377)	Data 1.72e-04 (4.20e-03)	Tok/s 50775 (71050)	Loss/tok 2.5331 (3.1231)	LR 2.000e-03
0: TRAIN [4][190/968]	Time 0.423 (0.380)	Data 2.07e-04 (3.99e-03)	Tok/s 79380 (71389)	Loss/tok 3.1318 (3.1272)	LR 2.000e-03
0: TRAIN [4][200/968]	Time 0.541 (0.383)	Data 1.79e-04 (3.80e-03)	Tok/s 85696 (71652)	Loss/tok 3.3675 (3.1349)	LR 2.000e-03
0: TRAIN [4][210/968]	Time 0.544 (0.385)	Data 1.82e-04 (3.63e-03)	Tok/s 85720 (71881)	Loss/tok 3.3329 (3.1386)	LR 2.000e-03
0: TRAIN [4][220/968]	Time 0.311 (0.384)	Data 1.70e-04 (3.48e-03)	Tok/s 66701 (71855)	Loss/tok 2.9291 (3.1376)	LR 2.000e-03
0: TRAIN [4][230/968]	Time 0.422 (0.384)	Data 1.83e-04 (3.34e-03)	Tok/s 78991 (71885)	Loss/tok 3.1338 (3.1380)	LR 2.000e-03
0: TRAIN [4][240/968]	Time 0.311 (0.385)	Data 1.94e-04 (3.20e-03)	Tok/s 66583 (72035)	Loss/tok 2.8814 (3.1383)	LR 2.000e-03
0: TRAIN [4][250/968]	Time 0.423 (0.385)	Data 1.89e-04 (3.08e-03)	Tok/s 79391 (72064)	Loss/tok 3.1361 (3.1384)	LR 2.000e-03
0: TRAIN [4][260/968]	Time 0.544 (0.385)	Data 1.74e-04 (2.97e-03)	Tok/s 86467 (72027)	Loss/tok 3.3155 (3.1405)	LR 2.000e-03
0: TRAIN [4][270/968]	Time 0.311 (0.384)	Data 1.99e-04 (2.87e-03)	Tok/s 66127 (72044)	Loss/tok 2.9081 (3.1395)	LR 2.000e-03
0: TRAIN [4][280/968]	Time 0.310 (0.384)	Data 1.65e-04 (2.78e-03)	Tok/s 66618 (72030)	Loss/tok 2.8799 (3.1399)	LR 2.000e-03
0: TRAIN [4][290/968]	Time 0.310 (0.380)	Data 1.85e-04 (2.69e-03)	Tok/s 67484 (71629)	Loss/tok 2.9977 (3.1347)	LR 2.000e-03
0: TRAIN [4][300/968]	Time 0.312 (0.379)	Data 1.99e-04 (2.61e-03)	Tok/s 65520 (71573)	Loss/tok 2.9323 (3.1322)	LR 2.000e-03
0: TRAIN [4][310/968]	Time 0.423 (0.380)	Data 1.79e-04 (2.53e-03)	Tok/s 79208 (71671)	Loss/tok 3.1509 (3.1335)	LR 2.000e-03
0: TRAIN [4][320/968]	Time 0.311 (0.379)	Data 2.09e-04 (2.46e-03)	Tok/s 66746 (71675)	Loss/tok 3.0008 (3.1324)	LR 2.000e-03
0: TRAIN [4][330/968]	Time 0.312 (0.381)	Data 1.95e-04 (2.39e-03)	Tok/s 66025 (71735)	Loss/tok 2.9430 (3.1359)	LR 2.000e-03
0: TRAIN [4][340/968]	Time 0.310 (0.381)	Data 1.85e-04 (2.32e-03)	Tok/s 66569 (71770)	Loss/tok 2.9288 (3.1371)	LR 2.000e-03
0: TRAIN [4][350/968]	Time 0.312 (0.381)	Data 1.64e-04 (2.26e-03)	Tok/s 65843 (71782)	Loss/tok 2.9607 (3.1363)	LR 2.000e-03
0: TRAIN [4][360/968]	Time 0.545 (0.382)	Data 2.04e-04 (2.21e-03)	Tok/s 85553 (71896)	Loss/tok 3.3315 (3.1386)	LR 2.000e-03
0: TRAIN [4][370/968]	Time 0.312 (0.382)	Data 1.87e-04 (2.15e-03)	Tok/s 66818 (71926)	Loss/tok 2.9749 (3.1380)	LR 2.000e-03
0: TRAIN [4][380/968]	Time 0.204 (0.382)	Data 1.73e-04 (2.10e-03)	Tok/s 51815 (71956)	Loss/tok 2.6272 (3.1386)	LR 2.000e-03
0: TRAIN [4][390/968]	Time 0.424 (0.383)	Data 1.85e-04 (2.05e-03)	Tok/s 79697 (72075)	Loss/tok 3.1238 (3.1394)	LR 2.000e-03
0: TRAIN [4][400/968]	Time 0.311 (0.382)	Data 1.88e-04 (2.01e-03)	Tok/s 67447 (72006)	Loss/tok 2.9026 (3.1383)	LR 2.000e-03
0: TRAIN [4][410/968]	Time 0.311 (0.381)	Data 1.87e-04 (1.96e-03)	Tok/s 66576 (71910)	Loss/tok 2.8776 (3.1360)	LR 2.000e-03
0: TRAIN [4][420/968]	Time 0.684 (0.381)	Data 1.84e-04 (1.92e-03)	Tok/s 87120 (71875)	Loss/tok 3.4714 (3.1354)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [4][430/968]	Time 0.315 (0.381)	Data 1.90e-04 (1.88e-03)	Tok/s 65501 (71899)	Loss/tok 2.9794 (3.1372)	LR 2.000e-03
0: TRAIN [4][440/968]	Time 0.312 (0.382)	Data 2.41e-04 (1.84e-03)	Tok/s 65538 (72007)	Loss/tok 2.9072 (3.1377)	LR 2.000e-03
0: TRAIN [4][450/968]	Time 0.423 (0.382)	Data 1.75e-04 (1.80e-03)	Tok/s 79697 (71957)	Loss/tok 3.2136 (3.1372)	LR 2.000e-03
0: TRAIN [4][460/968]	Time 0.209 (0.381)	Data 2.07e-04 (1.77e-03)	Tok/s 50457 (71868)	Loss/tok 2.6032 (3.1378)	LR 2.000e-03
0: TRAIN [4][470/968]	Time 0.423 (0.382)	Data 1.94e-04 (1.74e-03)	Tok/s 79086 (71989)	Loss/tok 3.1984 (3.1388)	LR 2.000e-03
0: TRAIN [4][480/968]	Time 0.316 (0.382)	Data 2.06e-04 (1.70e-03)	Tok/s 65420 (71982)	Loss/tok 3.0193 (3.1391)	LR 2.000e-03
0: TRAIN [4][490/968]	Time 0.313 (0.382)	Data 1.82e-04 (1.67e-03)	Tok/s 65794 (71947)	Loss/tok 3.0129 (3.1390)	LR 2.000e-03
0: TRAIN [4][500/968]	Time 0.424 (0.383)	Data 1.77e-04 (1.64e-03)	Tok/s 79551 (72018)	Loss/tok 3.1715 (3.1411)	LR 2.000e-03
0: TRAIN [4][510/968]	Time 0.423 (0.384)	Data 1.92e-04 (1.62e-03)	Tok/s 78841 (72078)	Loss/tok 3.1813 (3.1438)	LR 2.000e-03
0: TRAIN [4][520/968]	Time 0.316 (0.384)	Data 1.74e-04 (1.59e-03)	Tok/s 64851 (72053)	Loss/tok 2.9986 (3.1437)	LR 2.000e-03
0: TRAIN [4][530/968]	Time 0.422 (0.384)	Data 1.75e-04 (1.56e-03)	Tok/s 80000 (72102)	Loss/tok 3.2076 (3.1461)	LR 2.000e-03
0: TRAIN [4][540/968]	Time 0.424 (0.384)	Data 1.84e-04 (1.54e-03)	Tok/s 78978 (72098)	Loss/tok 3.2249 (3.1452)	LR 2.000e-03
0: TRAIN [4][550/968]	Time 0.683 (0.385)	Data 1.95e-04 (1.51e-03)	Tok/s 87746 (72158)	Loss/tok 3.4963 (3.1468)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [4][560/968]	Time 0.682 (0.385)	Data 1.72e-04 (1.49e-03)	Tok/s 87708 (72142)	Loss/tok 3.5225 (3.1474)	LR 2.000e-03
0: TRAIN [4][570/968]	Time 0.542 (0.385)	Data 1.79e-04 (1.47e-03)	Tok/s 85308 (72149)	Loss/tok 3.3322 (3.1476)	LR 2.000e-03
0: TRAIN [4][580/968]	Time 0.315 (0.385)	Data 1.72e-04 (1.44e-03)	Tok/s 65167 (72172)	Loss/tok 2.9882 (3.1484)	LR 2.000e-03
0: TRAIN [4][590/968]	Time 0.311 (0.385)	Data 2.00e-04 (1.42e-03)	Tok/s 66881 (72193)	Loss/tok 2.9080 (3.1495)	LR 2.000e-03
0: TRAIN [4][600/968]	Time 0.313 (0.387)	Data 1.75e-04 (1.40e-03)	Tok/s 65470 (72298)	Loss/tok 2.9410 (3.1521)	LR 2.000e-03
0: TRAIN [4][610/968]	Time 0.317 (0.387)	Data 1.96e-04 (1.38e-03)	Tok/s 65362 (72361)	Loss/tok 2.9473 (3.1547)	LR 2.000e-03
0: TRAIN [4][620/968]	Time 0.423 (0.388)	Data 1.73e-04 (1.36e-03)	Tok/s 79598 (72425)	Loss/tok 3.1877 (3.1551)	LR 2.000e-03
0: TRAIN [4][630/968]	Time 0.313 (0.388)	Data 1.83e-04 (1.34e-03)	Tok/s 66471 (72450)	Loss/tok 2.9569 (3.1550)	LR 2.000e-03
0: TRAIN [4][640/968]	Time 0.544 (0.388)	Data 2.15e-04 (1.33e-03)	Tok/s 85346 (72532)	Loss/tok 3.3483 (3.1555)	LR 2.000e-03
0: TRAIN [4][650/968]	Time 0.423 (0.389)	Data 1.72e-04 (1.31e-03)	Tok/s 79605 (72584)	Loss/tok 3.1185 (3.1558)	LR 2.000e-03
0: TRAIN [4][660/968]	Time 0.312 (0.389)	Data 1.72e-04 (1.29e-03)	Tok/s 66516 (72565)	Loss/tok 3.0173 (3.1561)	LR 2.000e-03
0: TRAIN [4][670/968]	Time 0.207 (0.388)	Data 1.81e-04 (1.28e-03)	Tok/s 50893 (72491)	Loss/tok 2.5179 (3.1548)	LR 2.000e-03
0: TRAIN [4][680/968]	Time 0.311 (0.388)	Data 1.80e-04 (1.26e-03)	Tok/s 66852 (72486)	Loss/tok 2.8897 (3.1541)	LR 2.000e-03
0: TRAIN [4][690/968]	Time 0.316 (0.388)	Data 4.16e-04 (1.25e-03)	Tok/s 65566 (72514)	Loss/tok 3.0035 (3.1555)	LR 2.000e-03
0: TRAIN [4][700/968]	Time 0.315 (0.388)	Data 2.06e-04 (1.23e-03)	Tok/s 65230 (72511)	Loss/tok 2.9717 (3.1543)	LR 2.000e-03
0: TRAIN [4][710/968]	Time 0.313 (0.387)	Data 1.93e-04 (1.22e-03)	Tok/s 64516 (72479)	Loss/tok 2.9509 (3.1536)	LR 2.000e-03
0: TRAIN [4][720/968]	Time 0.312 (0.387)	Data 1.73e-04 (1.20e-03)	Tok/s 65960 (72463)	Loss/tok 2.9868 (3.1538)	LR 2.000e-03
0: TRAIN [4][730/968]	Time 0.312 (0.388)	Data 1.85e-04 (1.19e-03)	Tok/s 67060 (72514)	Loss/tok 2.9606 (3.1548)	LR 2.000e-03
0: TRAIN [4][740/968]	Time 0.422 (0.388)	Data 1.78e-04 (1.17e-03)	Tok/s 79714 (72572)	Loss/tok 3.1465 (3.1553)	LR 2.000e-03
0: TRAIN [4][750/968]	Time 0.545 (0.388)	Data 1.96e-04 (1.16e-03)	Tok/s 85767 (72593)	Loss/tok 3.3589 (3.1551)	LR 2.000e-03
0: TRAIN [4][760/968]	Time 0.311 (0.389)	Data 1.81e-04 (1.15e-03)	Tok/s 65401 (72603)	Loss/tok 2.9547 (3.1561)	LR 2.000e-03
0: TRAIN [4][770/968]	Time 0.544 (0.389)	Data 1.72e-04 (1.14e-03)	Tok/s 85710 (72640)	Loss/tok 3.3152 (3.1559)	LR 2.000e-03
0: TRAIN [4][780/968]	Time 0.311 (0.389)	Data 1.70e-04 (1.12e-03)	Tok/s 66248 (72630)	Loss/tok 2.8900 (3.1560)	LR 2.000e-03
0: TRAIN [4][790/968]	Time 0.311 (0.388)	Data 1.71e-04 (1.11e-03)	Tok/s 65730 (72583)	Loss/tok 2.9416 (3.1553)	LR 2.000e-03
0: TRAIN [4][800/968]	Time 0.312 (0.388)	Data 1.78e-04 (1.10e-03)	Tok/s 66038 (72548)	Loss/tok 3.0654 (3.1553)	LR 2.000e-03
0: TRAIN [4][810/968]	Time 0.203 (0.388)	Data 2.03e-04 (1.09e-03)	Tok/s 51508 (72615)	Loss/tok 2.5907 (3.1562)	LR 2.000e-03
0: TRAIN [4][820/968]	Time 0.425 (0.388)	Data 2.05e-04 (1.08e-03)	Tok/s 79031 (72575)	Loss/tok 3.1442 (3.1555)	LR 2.000e-03
0: TRAIN [4][830/968]	Time 0.311 (0.387)	Data 1.89e-04 (1.07e-03)	Tok/s 66763 (72512)	Loss/tok 2.9929 (3.1543)	LR 2.000e-03
0: TRAIN [4][840/968]	Time 0.313 (0.387)	Data 1.88e-04 (1.06e-03)	Tok/s 66390 (72480)	Loss/tok 2.8706 (3.1536)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [4][850/968]	Time 0.310 (0.387)	Data 1.85e-04 (1.05e-03)	Tok/s 65819 (72454)	Loss/tok 2.9956 (3.1545)	LR 2.000e-03
0: TRAIN [4][860/968]	Time 0.310 (0.386)	Data 1.86e-04 (1.04e-03)	Tok/s 66396 (72441)	Loss/tok 2.9862 (3.1538)	LR 2.000e-03
0: TRAIN [4][870/968]	Time 0.684 (0.387)	Data 1.83e-04 (1.03e-03)	Tok/s 86965 (72486)	Loss/tok 3.4989 (3.1551)	LR 2.000e-03
0: TRAIN [4][880/968]	Time 0.545 (0.387)	Data 3.20e-04 (1.02e-03)	Tok/s 85448 (72525)	Loss/tok 3.3608 (3.1552)	LR 2.000e-03
0: TRAIN [4][890/968]	Time 0.685 (0.387)	Data 1.92e-04 (1.01e-03)	Tok/s 87346 (72463)	Loss/tok 3.5472 (3.1551)	LR 2.000e-03
0: TRAIN [4][900/968]	Time 0.314 (0.387)	Data 1.82e-04 (1.00e-03)	Tok/s 66146 (72469)	Loss/tok 2.9556 (3.1548)	LR 2.000e-03
0: TRAIN [4][910/968]	Time 0.686 (0.387)	Data 1.74e-04 (9.91e-04)	Tok/s 86320 (72512)	Loss/tok 3.5448 (3.1563)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [4][920/968]	Time 0.312 (0.388)	Data 1.80e-04 (9.82e-04)	Tok/s 65885 (72507)	Loss/tok 3.0091 (3.1570)	LR 2.000e-03
0: TRAIN [4][930/968]	Time 0.424 (0.388)	Data 1.78e-04 (9.74e-04)	Tok/s 79139 (72511)	Loss/tok 3.2229 (3.1564)	LR 2.000e-03
0: TRAIN [4][940/968]	Time 0.321 (0.387)	Data 1.81e-04 (9.66e-04)	Tok/s 64823 (72500)	Loss/tok 2.8683 (3.1562)	LR 2.000e-03
0: TRAIN [4][950/968]	Time 0.422 (0.388)	Data 1.78e-04 (9.58e-04)	Tok/s 78803 (72529)	Loss/tok 3.2560 (3.1579)	LR 2.000e-03
0: TRAIN [4][960/968]	Time 0.424 (0.388)	Data 1.73e-04 (9.50e-04)	Tok/s 78796 (72558)	Loss/tok 3.1698 (3.1588)	LR 2.000e-03
:::MLL 1571247566.897 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 524}}
:::MLL 1571247566.898 eval_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [4][0/3]	Time 0.742 (0.742)	Decoder iters 144.0 (144.0)	Tok/s 22553 (22553)
0: Running moses detokenizer
0: BLEU(score=23.448873652047556, counts=[36745, 18188, 10278, 6065], totals=[65522, 62519, 59517, 56519], precisions=[56.08040047617594, 29.09195604536221, 17.269015575381825, 10.730904651533113], bp=1.0, sys_len=65522, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571247568.840 eval_accuracy: {"value": 23.45, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 535}}
:::MLL 1571247568.841 eval_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 4	Training Loss: 3.1571	Test BLEU: 23.45
0: Performance: Epoch: 4	Training: 579948 Tok/s
0: Finished epoch 4
:::MLL 1571247568.841 block_stop: {"value": null, "metadata": {"first_epoch_num": 5, "file": "train.py", "lineno": 557}}
:::MLL 1571247568.841 block_start: {"value": null, "metadata": {"first_epoch_num": 6, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571247568.842 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 514}}
0: Starting epoch 5
0: Executing preallocation
0: Sampler for epoch 5 uses seed 3129350112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [5][0/968]	Time 0.933 (0.933)	Data 7.10e-01 (7.10e-01)	Tok/s 11340 (11340)	Loss/tok 2.6167 (2.6167)	LR 2.000e-03
0: TRAIN [5][10/968]	Time 0.309 (0.449)	Data 1.50e-04 (6.47e-02)	Tok/s 66496 (70296)	Loss/tok 2.9417 (3.0386)	LR 2.000e-03
0: TRAIN [5][20/968]	Time 0.541 (0.422)	Data 1.74e-04 (3.40e-02)	Tok/s 86819 (71979)	Loss/tok 3.1741 (3.0426)	LR 2.000e-03
0: TRAIN [5][30/968]	Time 0.309 (0.419)	Data 1.75e-04 (2.31e-02)	Tok/s 66248 (73303)	Loss/tok 2.9437 (3.0648)	LR 2.000e-03
0: TRAIN [5][40/968]	Time 0.423 (0.415)	Data 1.76e-04 (1.75e-02)	Tok/s 78858 (74036)	Loss/tok 3.0928 (3.0630)	LR 2.000e-03
0: TRAIN [5][50/968]	Time 0.204 (0.406)	Data 1.84e-04 (1.41e-02)	Tok/s 51318 (73429)	Loss/tok 2.5603 (3.0695)	LR 2.000e-03
0: TRAIN [5][60/968]	Time 0.423 (0.397)	Data 1.83e-04 (1.18e-02)	Tok/s 79690 (72631)	Loss/tok 3.0780 (3.0672)	LR 2.000e-03
0: TRAIN [5][70/968]	Time 0.422 (0.396)	Data 1.99e-04 (1.02e-02)	Tok/s 78906 (72941)	Loss/tok 3.1032 (3.0676)	LR 2.000e-03
0: TRAIN [5][80/968]	Time 0.311 (0.390)	Data 1.84e-04 (8.96e-03)	Tok/s 65742 (72533)	Loss/tok 2.8081 (3.0605)	LR 2.000e-03
0: TRAIN [5][90/968]	Time 0.312 (0.390)	Data 1.90e-04 (8.00e-03)	Tok/s 67128 (72806)	Loss/tok 2.9042 (3.0604)	LR 2.000e-03
0: TRAIN [5][100/968]	Time 0.312 (0.386)	Data 1.82e-04 (7.23e-03)	Tok/s 66964 (72648)	Loss/tok 2.8862 (3.0569)	LR 2.000e-03
0: TRAIN [5][110/968]	Time 0.204 (0.383)	Data 1.85e-04 (6.60e-03)	Tok/s 51846 (72332)	Loss/tok 2.5002 (3.0538)	LR 2.000e-03
0: TRAIN [5][120/968]	Time 0.312 (0.388)	Data 1.84e-04 (6.07e-03)	Tok/s 65869 (72574)	Loss/tok 2.8955 (3.0730)	LR 2.000e-03
0: TRAIN [5][130/968]	Time 0.544 (0.387)	Data 2.06e-04 (5.62e-03)	Tok/s 86675 (72530)	Loss/tok 3.2267 (3.0736)	LR 2.000e-03
0: TRAIN [5][140/968]	Time 0.544 (0.388)	Data 1.95e-04 (5.24e-03)	Tok/s 86088 (72618)	Loss/tok 3.2758 (3.0745)	LR 2.000e-03
0: TRAIN [5][150/968]	Time 0.544 (0.388)	Data 1.76e-04 (4.91e-03)	Tok/s 85920 (72566)	Loss/tok 3.2751 (3.0788)	LR 2.000e-03
0: TRAIN [5][160/968]	Time 0.310 (0.391)	Data 1.80e-04 (4.62e-03)	Tok/s 65012 (72759)	Loss/tok 2.8918 (3.0853)	LR 2.000e-03
0: TRAIN [5][170/968]	Time 0.204 (0.388)	Data 1.85e-04 (4.36e-03)	Tok/s 51282 (72556)	Loss/tok 2.6091 (3.0816)	LR 2.000e-03
0: TRAIN [5][180/968]	Time 0.543 (0.390)	Data 1.86e-04 (4.13e-03)	Tok/s 86188 (72574)	Loss/tok 3.2656 (3.0855)	LR 2.000e-03
0: TRAIN [5][190/968]	Time 0.310 (0.387)	Data 1.78e-04 (3.92e-03)	Tok/s 66301 (72395)	Loss/tok 2.9211 (3.0792)	LR 2.000e-03
0: TRAIN [5][200/968]	Time 0.312 (0.387)	Data 2.03e-04 (3.74e-03)	Tok/s 66181 (72485)	Loss/tok 2.9271 (3.0825)	LR 2.000e-03
0: TRAIN [5][210/968]	Time 0.683 (0.390)	Data 1.80e-04 (3.57e-03)	Tok/s 87435 (72563)	Loss/tok 3.4661 (3.0895)	LR 2.000e-03
0: TRAIN [5][220/968]	Time 0.313 (0.391)	Data 1.87e-04 (3.42e-03)	Tok/s 65367 (72544)	Loss/tok 2.9087 (3.0946)	LR 2.000e-03
0: TRAIN [5][230/968]	Time 0.313 (0.388)	Data 1.85e-04 (3.28e-03)	Tok/s 65987 (72336)	Loss/tok 2.9665 (3.0923)	LR 2.000e-03
0: TRAIN [5][240/968]	Time 0.541 (0.389)	Data 1.81e-04 (3.15e-03)	Tok/s 85525 (72460)	Loss/tok 3.3142 (3.0937)	LR 2.000e-03
0: TRAIN [5][250/968]	Time 0.545 (0.388)	Data 1.84e-04 (3.04e-03)	Tok/s 85330 (72362)	Loss/tok 3.3295 (3.0923)	LR 2.000e-03
0: TRAIN [5][260/968]	Time 0.423 (0.388)	Data 1.96e-04 (2.93e-03)	Tok/s 79891 (72422)	Loss/tok 3.0718 (3.0923)	LR 2.000e-03
0: TRAIN [5][270/968]	Time 0.544 (0.387)	Data 1.99e-04 (2.83e-03)	Tok/s 85583 (72429)	Loss/tok 3.2208 (3.0903)	LR 2.000e-03
0: TRAIN [5][280/968]	Time 0.311 (0.387)	Data 1.85e-04 (2.73e-03)	Tok/s 66474 (72458)	Loss/tok 2.9328 (3.0905)	LR 2.000e-03
0: TRAIN [5][290/968]	Time 0.313 (0.388)	Data 1.79e-04 (2.65e-03)	Tok/s 65707 (72553)	Loss/tok 2.8006 (3.0925)	LR 2.000e-03
0: TRAIN [5][300/968]	Time 0.423 (0.390)	Data 1.80e-04 (2.56e-03)	Tok/s 79568 (72742)	Loss/tok 3.1187 (3.0975)	LR 2.000e-03
0: TRAIN [5][310/968]	Time 0.311 (0.390)	Data 1.98e-04 (2.49e-03)	Tok/s 65567 (72730)	Loss/tok 2.8650 (3.0979)	LR 2.000e-03
0: TRAIN [5][320/968]	Time 0.543 (0.392)	Data 1.77e-04 (2.42e-03)	Tok/s 86065 (72841)	Loss/tok 3.2707 (3.1023)	LR 2.000e-03
0: TRAIN [5][330/968]	Time 0.204 (0.391)	Data 1.83e-04 (2.35e-03)	Tok/s 51724 (72798)	Loss/tok 2.6045 (3.1005)	LR 2.000e-03
0: TRAIN [5][340/968]	Time 0.311 (0.390)	Data 2.03e-04 (2.29e-03)	Tok/s 66334 (72731)	Loss/tok 2.9283 (3.0991)	LR 2.000e-03
0: TRAIN [5][350/968]	Time 0.543 (0.391)	Data 1.80e-04 (2.23e-03)	Tok/s 85364 (72867)	Loss/tok 3.3297 (3.1041)	LR 2.000e-03
0: TRAIN [5][360/968]	Time 0.311 (0.390)	Data 2.06e-04 (2.17e-03)	Tok/s 65675 (72827)	Loss/tok 2.9399 (3.1025)	LR 2.000e-03
0: TRAIN [5][370/968]	Time 0.425 (0.391)	Data 1.74e-04 (2.12e-03)	Tok/s 79007 (72926)	Loss/tok 3.1145 (3.1027)	LR 2.000e-03
0: TRAIN [5][380/968]	Time 0.311 (0.392)	Data 1.81e-04 (2.07e-03)	Tok/s 66655 (72980)	Loss/tok 2.8514 (3.1050)	LR 2.000e-03
0: TRAIN [5][390/968]	Time 0.425 (0.392)	Data 1.74e-04 (2.02e-03)	Tok/s 78586 (73028)	Loss/tok 3.1816 (3.1047)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][400/968]	Time 0.311 (0.390)	Data 1.81e-04 (1.98e-03)	Tok/s 67016 (72869)	Loss/tok 2.9518 (3.1036)	LR 2.000e-03
0: TRAIN [5][410/968]	Time 0.205 (0.391)	Data 1.91e-04 (1.93e-03)	Tok/s 52300 (72904)	Loss/tok 2.4984 (3.1048)	LR 2.000e-03
0: TRAIN [5][420/968]	Time 0.682 (0.392)	Data 1.87e-04 (1.89e-03)	Tok/s 87528 (73018)	Loss/tok 3.4161 (3.1075)	LR 2.000e-03
0: TRAIN [5][430/968]	Time 0.312 (0.392)	Data 1.63e-04 (1.85e-03)	Tok/s 66698 (72927)	Loss/tok 2.9230 (3.1076)	LR 2.000e-03
0: TRAIN [5][440/968]	Time 0.205 (0.391)	Data 1.79e-04 (1.81e-03)	Tok/s 50994 (72851)	Loss/tok 2.5225 (3.1062)	LR 2.000e-03
0: TRAIN [5][450/968]	Time 0.426 (0.390)	Data 1.87e-04 (1.78e-03)	Tok/s 77701 (72764)	Loss/tok 3.1298 (3.1063)	LR 2.000e-03
0: TRAIN [5][460/968]	Time 0.312 (0.390)	Data 2.32e-04 (1.74e-03)	Tok/s 66299 (72741)	Loss/tok 2.8960 (3.1059)	LR 2.000e-03
0: TRAIN [5][470/968]	Time 0.543 (0.391)	Data 1.88e-04 (1.71e-03)	Tok/s 86083 (72829)	Loss/tok 3.2849 (3.1081)	LR 2.000e-03
0: TRAIN [5][480/968]	Time 0.544 (0.390)	Data 1.78e-04 (1.68e-03)	Tok/s 85298 (72742)	Loss/tok 3.3124 (3.1072)	LR 2.000e-03
0: TRAIN [5][490/968]	Time 0.312 (0.391)	Data 1.75e-04 (1.65e-03)	Tok/s 66211 (72813)	Loss/tok 2.9232 (3.1127)	LR 2.000e-03
0: TRAIN [5][500/968]	Time 0.423 (0.392)	Data 1.90e-04 (1.62e-03)	Tok/s 79108 (72818)	Loss/tok 3.0905 (3.1139)	LR 2.000e-03
0: TRAIN [5][510/968]	Time 0.542 (0.392)	Data 1.94e-04 (1.59e-03)	Tok/s 85719 (72908)	Loss/tok 3.3132 (3.1152)	LR 2.000e-03
0: TRAIN [5][520/968]	Time 0.544 (0.392)	Data 1.67e-04 (1.57e-03)	Tok/s 84898 (72899)	Loss/tok 3.3455 (3.1151)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][530/968]	Time 0.312 (0.392)	Data 2.05e-04 (1.54e-03)	Tok/s 65672 (72912)	Loss/tok 3.0347 (3.1155)	LR 2.000e-03
0: TRAIN [5][540/968]	Time 0.314 (0.392)	Data 1.81e-04 (1.52e-03)	Tok/s 66262 (72855)	Loss/tok 2.9334 (3.1159)	LR 2.000e-03
0: TRAIN [5][550/968]	Time 0.312 (0.392)	Data 1.81e-04 (1.49e-03)	Tok/s 65431 (72907)	Loss/tok 2.9508 (3.1177)	LR 2.000e-03
0: TRAIN [5][560/968]	Time 0.205 (0.392)	Data 1.78e-04 (1.47e-03)	Tok/s 51336 (72920)	Loss/tok 2.5473 (3.1183)	LR 2.000e-03
0: TRAIN [5][570/968]	Time 0.311 (0.393)	Data 1.74e-04 (1.45e-03)	Tok/s 66913 (72931)	Loss/tok 2.9237 (3.1199)	LR 2.000e-03
0: TRAIN [5][580/968]	Time 0.312 (0.392)	Data 1.90e-04 (1.43e-03)	Tok/s 66224 (72866)	Loss/tok 2.8570 (3.1191)	LR 2.000e-03
0: TRAIN [5][590/968]	Time 0.207 (0.392)	Data 1.81e-04 (1.40e-03)	Tok/s 51550 (72847)	Loss/tok 2.4646 (3.1184)	LR 2.000e-03
0: TRAIN [5][600/968]	Time 0.543 (0.392)	Data 2.06e-04 (1.39e-03)	Tok/s 85191 (72807)	Loss/tok 3.2938 (3.1175)	LR 2.000e-03
0: TRAIN [5][610/968]	Time 0.425 (0.392)	Data 1.96e-04 (1.37e-03)	Tok/s 79657 (72857)	Loss/tok 3.0965 (3.1184)	LR 2.000e-03
0: TRAIN [5][620/968]	Time 0.315 (0.392)	Data 1.80e-04 (1.35e-03)	Tok/s 65988 (72801)	Loss/tok 2.9089 (3.1184)	LR 2.000e-03
0: TRAIN [5][630/968]	Time 0.312 (0.392)	Data 1.73e-04 (1.33e-03)	Tok/s 66597 (72814)	Loss/tok 2.9421 (3.1185)	LR 2.000e-03
0: TRAIN [5][640/968]	Time 0.429 (0.392)	Data 1.99e-04 (1.31e-03)	Tok/s 78654 (72844)	Loss/tok 3.1574 (3.1190)	LR 2.000e-03
0: TRAIN [5][650/968]	Time 0.427 (0.392)	Data 1.75e-04 (1.29e-03)	Tok/s 79241 (72856)	Loss/tok 3.1539 (3.1192)	LR 2.000e-03
0: TRAIN [5][660/968]	Time 0.313 (0.391)	Data 1.70e-04 (1.28e-03)	Tok/s 66576 (72725)	Loss/tok 2.8857 (3.1174)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][670/968]	Time 0.311 (0.390)	Data 1.74e-04 (1.26e-03)	Tok/s 66698 (72666)	Loss/tok 2.9919 (3.1173)	LR 2.000e-03
0: TRAIN [5][680/968]	Time 0.311 (0.390)	Data 1.90e-04 (1.25e-03)	Tok/s 66646 (72691)	Loss/tok 2.9148 (3.1183)	LR 2.000e-03
0: TRAIN [5][690/968]	Time 0.310 (0.390)	Data 1.80e-04 (1.23e-03)	Tok/s 66139 (72694)	Loss/tok 2.9565 (3.1171)	LR 2.000e-03
0: TRAIN [5][700/968]	Time 0.206 (0.390)	Data 1.86e-04 (1.22e-03)	Tok/s 51757 (72666)	Loss/tok 2.5026 (3.1178)	LR 2.000e-03
0: TRAIN [5][710/968]	Time 0.423 (0.390)	Data 1.69e-04 (1.20e-03)	Tok/s 79529 (72666)	Loss/tok 3.1178 (3.1172)	LR 2.000e-03
0: TRAIN [5][720/968]	Time 0.312 (0.389)	Data 1.65e-04 (1.19e-03)	Tok/s 66352 (72646)	Loss/tok 2.9397 (3.1160)	LR 2.000e-03
0: TRAIN [5][730/968]	Time 0.311 (0.389)	Data 1.67e-04 (1.17e-03)	Tok/s 65290 (72633)	Loss/tok 2.9096 (3.1161)	LR 2.000e-03
0: TRAIN [5][740/968]	Time 0.311 (0.389)	Data 1.76e-04 (1.16e-03)	Tok/s 66405 (72595)	Loss/tok 2.8736 (3.1167)	LR 2.000e-03
0: TRAIN [5][750/968]	Time 0.204 (0.388)	Data 1.70e-04 (1.15e-03)	Tok/s 52306 (72549)	Loss/tok 2.5634 (3.1158)	LR 2.000e-03
0: TRAIN [5][760/968]	Time 0.683 (0.389)	Data 1.83e-04 (1.13e-03)	Tok/s 87497 (72581)	Loss/tok 3.4267 (3.1176)	LR 2.000e-03
0: TRAIN [5][770/968]	Time 0.424 (0.389)	Data 1.95e-04 (1.12e-03)	Tok/s 79618 (72625)	Loss/tok 3.0952 (3.1186)	LR 2.000e-03
0: TRAIN [5][780/968]	Time 0.684 (0.390)	Data 2.22e-04 (1.11e-03)	Tok/s 86724 (72619)	Loss/tok 3.5476 (3.1195)	LR 2.000e-03
0: TRAIN [5][790/968]	Time 0.423 (0.390)	Data 1.74e-04 (1.10e-03)	Tok/s 79116 (72629)	Loss/tok 3.1608 (3.1204)	LR 2.000e-03
0: TRAIN [5][800/968]	Time 0.423 (0.390)	Data 1.88e-04 (1.09e-03)	Tok/s 79453 (72642)	Loss/tok 3.1123 (3.1199)	LR 2.000e-03
0: TRAIN [5][810/968]	Time 0.312 (0.389)	Data 1.72e-04 (1.08e-03)	Tok/s 65679 (72611)	Loss/tok 2.9788 (3.1187)	LR 2.000e-03
0: TRAIN [5][820/968]	Time 0.312 (0.389)	Data 1.78e-04 (1.06e-03)	Tok/s 65661 (72626)	Loss/tok 2.9845 (3.1194)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][830/968]	Time 0.312 (0.390)	Data 1.98e-04 (1.05e-03)	Tok/s 66027 (72661)	Loss/tok 2.9472 (3.1210)	LR 2.000e-03
0: TRAIN [5][840/968]	Time 0.544 (0.389)	Data 1.95e-04 (1.04e-03)	Tok/s 86009 (72607)	Loss/tok 3.2734 (3.1208)	LR 2.000e-03
0: TRAIN [5][850/968]	Time 0.313 (0.389)	Data 1.82e-04 (1.03e-03)	Tok/s 65140 (72544)	Loss/tok 2.9737 (3.1197)	LR 2.000e-03
0: TRAIN [5][860/968]	Time 0.313 (0.388)	Data 1.94e-04 (1.02e-03)	Tok/s 66304 (72496)	Loss/tok 2.9062 (3.1188)	LR 2.000e-03
0: TRAIN [5][870/968]	Time 0.313 (0.388)	Data 1.88e-04 (1.02e-03)	Tok/s 65610 (72460)	Loss/tok 3.0163 (3.1184)	LR 2.000e-03
0: TRAIN [5][880/968]	Time 0.310 (0.388)	Data 1.98e-04 (1.01e-03)	Tok/s 66901 (72524)	Loss/tok 2.9613 (3.1188)	LR 2.000e-03
0: TRAIN [5][890/968]	Time 0.204 (0.388)	Data 2.04e-04 (9.97e-04)	Tok/s 50993 (72541)	Loss/tok 2.5425 (3.1187)	LR 2.000e-03
0: TRAIN [5][900/968]	Time 0.312 (0.389)	Data 1.71e-04 (9.88e-04)	Tok/s 66073 (72566)	Loss/tok 2.9308 (3.1201)	LR 2.000e-03
0: TRAIN [5][910/968]	Time 0.311 (0.388)	Data 1.65e-04 (9.80e-04)	Tok/s 65662 (72542)	Loss/tok 2.8656 (3.1192)	LR 2.000e-03
0: TRAIN [5][920/968]	Time 0.312 (0.388)	Data 2.01e-04 (9.72e-04)	Tok/s 66092 (72559)	Loss/tok 2.8828 (3.1189)	LR 2.000e-03
0: TRAIN [5][930/968]	Time 0.423 (0.388)	Data 2.61e-04 (9.64e-04)	Tok/s 79968 (72570)	Loss/tok 3.1737 (3.1184)	LR 2.000e-03
0: TRAIN [5][940/968]	Time 0.425 (0.388)	Data 2.49e-04 (9.56e-04)	Tok/s 78074 (72565)	Loss/tok 3.0902 (3.1182)	LR 2.000e-03
0: TRAIN [5][950/968]	Time 0.313 (0.388)	Data 1.86e-04 (9.48e-04)	Tok/s 65447 (72541)	Loss/tok 2.9032 (3.1185)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][960/968]	Time 0.313 (0.388)	Data 2.11e-04 (9.40e-04)	Tok/s 66521 (72566)	Loss/tok 2.9913 (3.1192)	LR 2.000e-03
:::MLL 1571247946.588 epoch_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 524}}
:::MLL 1571247946.588 eval_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [5][0/3]	Time 0.645 (0.645)	Decoder iters 112.0 (112.0)	Tok/s 24869 (24869)
0: Running moses detokenizer
0: BLEU(score=23.490762515340588, counts=[36094, 17949, 10154, 5995], totals=[63284, 60281, 57278, 54280], precisions=[57.034953542759624, 29.775551168693287, 17.72757428681169, 11.044583640383198], bp=0.9782440686276851, sys_len=63284, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571247948.423 eval_accuracy: {"value": 23.49, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 535}}
:::MLL 1571247948.424 eval_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 5	Training Loss: 3.1183	Test BLEU: 23.49
0: Performance: Epoch: 5	Training: 580340 Tok/s
0: Finished epoch 5
:::MLL 1571247948.424 block_stop: {"value": null, "metadata": {"first_epoch_num": 6, "file": "train.py", "lineno": 557}}
:::MLL 1571247948.424 block_start: {"value": null, "metadata": {"first_epoch_num": 7, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571247948.425 epoch_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 514}}
0: Starting epoch 6
0: Executing preallocation
0: Sampler for epoch 6 uses seed 2304260212
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [6][0/968]	Time 1.031 (1.031)	Data 7.19e-01 (7.19e-01)	Tok/s 20135 (20135)	Loss/tok 2.8834 (2.8834)	LR 2.000e-03
0: TRAIN [6][10/968]	Time 0.421 (0.447)	Data 1.52e-04 (6.55e-02)	Tok/s 79699 (66174)	Loss/tok 3.0648 (3.0798)	LR 2.000e-03
0: TRAIN [6][20/968]	Time 0.312 (0.422)	Data 2.05e-04 (3.44e-02)	Tok/s 66490 (69412)	Loss/tok 2.8969 (3.0673)	LR 2.000e-03
0: TRAIN [6][30/968]	Time 0.421 (0.427)	Data 1.57e-04 (2.34e-02)	Tok/s 81043 (72233)	Loss/tok 3.0371 (3.0839)	LR 2.000e-03
0: TRAIN [6][40/968]	Time 0.422 (0.412)	Data 2.33e-04 (1.77e-02)	Tok/s 79015 (72393)	Loss/tok 3.0980 (3.0630)	LR 2.000e-03
0: TRAIN [6][50/968]	Time 0.424 (0.397)	Data 1.76e-04 (1.43e-02)	Tok/s 79134 (71733)	Loss/tok 2.9874 (3.0408)	LR 2.000e-03
0: TRAIN [6][60/968]	Time 0.311 (0.393)	Data 1.76e-04 (1.20e-02)	Tok/s 66327 (71635)	Loss/tok 2.7981 (3.0420)	LR 2.000e-03
0: TRAIN [6][70/968]	Time 0.543 (0.397)	Data 1.55e-04 (1.03e-02)	Tok/s 85995 (72431)	Loss/tok 3.1388 (3.0475)	LR 2.000e-03
0: TRAIN [6][80/968]	Time 0.310 (0.406)	Data 2.23e-04 (9.06e-03)	Tok/s 67423 (73104)	Loss/tok 2.7972 (3.0694)	LR 2.000e-03
0: TRAIN [6][90/968]	Time 0.684 (0.405)	Data 1.53e-04 (8.09e-03)	Tok/s 87093 (73076)	Loss/tok 3.4615 (3.0708)	LR 2.000e-03
0: TRAIN [6][100/968]	Time 0.425 (0.402)	Data 1.68e-04 (7.30e-03)	Tok/s 78718 (72986)	Loss/tok 3.0979 (3.0699)	LR 2.000e-03
0: TRAIN [6][110/968]	Time 0.311 (0.400)	Data 1.82e-04 (6.66e-03)	Tok/s 66200 (73025)	Loss/tok 2.8491 (3.0642)	LR 2.000e-03
0: TRAIN [6][120/968]	Time 0.311 (0.394)	Data 1.76e-04 (6.12e-03)	Tok/s 66336 (72491)	Loss/tok 3.0129 (3.0575)	LR 2.000e-03
0: TRAIN [6][130/968]	Time 0.311 (0.392)	Data 1.66e-04 (5.67e-03)	Tok/s 65869 (72439)	Loss/tok 2.8432 (3.0560)	LR 2.000e-03
0: TRAIN [6][140/968]	Time 0.311 (0.402)	Data 3.35e-04 (5.28e-03)	Tok/s 65905 (73090)	Loss/tok 2.9500 (3.0778)	LR 2.000e-03
0: TRAIN [6][150/968]	Time 0.310 (0.397)	Data 1.51e-04 (4.94e-03)	Tok/s 67829 (72789)	Loss/tok 2.9047 (3.0716)	LR 2.000e-03
0: TRAIN [6][160/968]	Time 0.312 (0.395)	Data 1.62e-04 (4.65e-03)	Tok/s 65439 (72535)	Loss/tok 2.8812 (3.0715)	LR 2.000e-03
0: TRAIN [6][170/968]	Time 0.205 (0.392)	Data 1.50e-04 (4.38e-03)	Tok/s 51554 (72333)	Loss/tok 2.5005 (3.0675)	LR 2.000e-03
0: TRAIN [6][180/968]	Time 0.682 (0.393)	Data 1.50e-04 (4.15e-03)	Tok/s 87325 (72390)	Loss/tok 3.3941 (3.0699)	LR 2.000e-03
0: TRAIN [6][190/968]	Time 0.423 (0.393)	Data 1.86e-04 (3.94e-03)	Tok/s 79449 (72587)	Loss/tok 3.0810 (3.0701)	LR 2.000e-03
0: TRAIN [6][200/968]	Time 0.545 (0.394)	Data 1.59e-04 (3.76e-03)	Tok/s 86605 (72677)	Loss/tok 3.2111 (3.0713)	LR 2.000e-03
0: TRAIN [6][210/968]	Time 0.311 (0.392)	Data 1.52e-04 (3.59e-03)	Tok/s 65669 (72400)	Loss/tok 2.8752 (3.0709)	LR 2.000e-03
0: TRAIN [6][220/968]	Time 0.424 (0.390)	Data 1.77e-04 (3.43e-03)	Tok/s 79391 (72312)	Loss/tok 3.0909 (3.0687)	LR 2.000e-03
0: TRAIN [6][230/968]	Time 0.424 (0.390)	Data 1.69e-04 (3.29e-03)	Tok/s 78628 (72255)	Loss/tok 3.0230 (3.0675)	LR 2.000e-03
0: TRAIN [6][240/968]	Time 0.423 (0.390)	Data 1.72e-04 (3.16e-03)	Tok/s 79873 (72354)	Loss/tok 3.0994 (3.0695)	LR 2.000e-03
0: TRAIN [6][250/968]	Time 0.311 (0.390)	Data 1.55e-04 (3.04e-03)	Tok/s 66309 (72430)	Loss/tok 2.7812 (3.0694)	LR 2.000e-03
0: TRAIN [6][260/968]	Time 0.423 (0.389)	Data 2.16e-04 (2.94e-03)	Tok/s 79450 (72388)	Loss/tok 3.0575 (3.0669)	LR 2.000e-03
0: TRAIN [6][270/968]	Time 0.204 (0.389)	Data 1.67e-04 (2.83e-03)	Tok/s 51141 (72375)	Loss/tok 2.5151 (3.0678)	LR 2.000e-03
0: TRAIN [6][280/968]	Time 0.426 (0.388)	Data 2.20e-04 (2.74e-03)	Tok/s 78846 (72347)	Loss/tok 3.0647 (3.0656)	LR 2.000e-03
0: TRAIN [6][290/968]	Time 0.425 (0.389)	Data 1.73e-04 (2.65e-03)	Tok/s 79177 (72456)	Loss/tok 3.0175 (3.0671)	LR 2.000e-03
0: TRAIN [6][300/968]	Time 0.203 (0.388)	Data 1.63e-04 (2.57e-03)	Tok/s 52406 (72419)	Loss/tok 2.5156 (3.0657)	LR 2.000e-03
0: TRAIN [6][310/968]	Time 0.423 (0.388)	Data 1.80e-04 (2.49e-03)	Tok/s 79742 (72468)	Loss/tok 3.0154 (3.0678)	LR 2.000e-03
0: TRAIN [6][320/968]	Time 0.206 (0.389)	Data 1.70e-04 (2.42e-03)	Tok/s 51578 (72557)	Loss/tok 2.4336 (3.0695)	LR 2.000e-03
0: TRAIN [6][330/968]	Time 0.310 (0.387)	Data 1.49e-04 (2.35e-03)	Tok/s 67252 (72445)	Loss/tok 2.8908 (3.0670)	LR 2.000e-03
0: TRAIN [6][340/968]	Time 0.311 (0.385)	Data 1.48e-04 (2.29e-03)	Tok/s 66159 (72260)	Loss/tok 2.8609 (3.0633)	LR 2.000e-03
0: TRAIN [6][350/968]	Time 0.312 (0.384)	Data 1.66e-04 (2.23e-03)	Tok/s 65892 (72217)	Loss/tok 2.8823 (3.0623)	LR 2.000e-03
0: TRAIN [6][360/968]	Time 0.684 (0.385)	Data 1.67e-04 (2.17e-03)	Tok/s 86943 (72327)	Loss/tok 3.4027 (3.0631)	LR 2.000e-03
0: TRAIN [6][370/968]	Time 0.310 (0.384)	Data 1.50e-04 (2.12e-03)	Tok/s 67183 (72207)	Loss/tok 2.8687 (3.0614)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [6][380/968]	Time 0.543 (0.384)	Data 1.56e-04 (2.07e-03)	Tok/s 86314 (72236)	Loss/tok 3.2678 (3.0629)	LR 2.000e-03
0: TRAIN [6][390/968]	Time 0.310 (0.384)	Data 1.57e-04 (2.02e-03)	Tok/s 67278 (72183)	Loss/tok 2.8494 (3.0635)	LR 2.000e-03
0: TRAIN [6][400/968]	Time 0.312 (0.384)	Data 1.77e-04 (1.97e-03)	Tok/s 66030 (72234)	Loss/tok 2.8500 (3.0646)	LR 2.000e-03
0: TRAIN [6][410/968]	Time 0.205 (0.385)	Data 1.53e-04 (1.93e-03)	Tok/s 51865 (72273)	Loss/tok 2.4560 (3.0649)	LR 2.000e-03
0: TRAIN [6][420/968]	Time 0.311 (0.385)	Data 1.49e-04 (1.89e-03)	Tok/s 65583 (72317)	Loss/tok 2.8578 (3.0652)	LR 2.000e-03
0: TRAIN [6][430/968]	Time 0.543 (0.384)	Data 1.61e-04 (1.85e-03)	Tok/s 85795 (72266)	Loss/tok 3.2991 (3.0663)	LR 2.000e-03
0: TRAIN [6][440/968]	Time 0.206 (0.386)	Data 1.45e-04 (1.81e-03)	Tok/s 50465 (72292)	Loss/tok 2.5243 (3.0703)	LR 2.000e-03
0: TRAIN [6][450/968]	Time 0.424 (0.386)	Data 1.48e-04 (1.77e-03)	Tok/s 78742 (72315)	Loss/tok 3.1569 (3.0716)	LR 2.000e-03
0: TRAIN [6][460/968]	Time 0.424 (0.386)	Data 1.48e-04 (1.74e-03)	Tok/s 79466 (72335)	Loss/tok 3.0750 (3.0723)	LR 2.000e-03
0: TRAIN [6][470/968]	Time 0.311 (0.387)	Data 1.63e-04 (1.70e-03)	Tok/s 66578 (72418)	Loss/tok 2.8831 (3.0742)	LR 2.000e-03
0: TRAIN [6][480/968]	Time 0.206 (0.386)	Data 1.52e-04 (1.67e-03)	Tok/s 52022 (72364)	Loss/tok 2.5867 (3.0725)	LR 2.000e-03
0: TRAIN [6][490/968]	Time 0.310 (0.385)	Data 1.49e-04 (1.64e-03)	Tok/s 65827 (72330)	Loss/tok 2.9263 (3.0717)	LR 2.000e-03
0: TRAIN [6][500/968]	Time 0.311 (0.385)	Data 1.47e-04 (1.61e-03)	Tok/s 66549 (72326)	Loss/tok 2.9736 (3.0709)	LR 2.000e-03
0: TRAIN [6][510/968]	Time 0.685 (0.385)	Data 1.48e-04 (1.58e-03)	Tok/s 85972 (72363)	Loss/tok 3.4861 (3.0718)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [6][520/968]	Time 0.541 (0.385)	Data 1.61e-04 (1.56e-03)	Tok/s 86526 (72334)	Loss/tok 3.2233 (3.0729)	LR 2.000e-03
0: TRAIN [6][530/968]	Time 0.543 (0.385)	Data 1.47e-04 (1.53e-03)	Tok/s 85688 (72351)	Loss/tok 3.2998 (3.0728)	LR 2.000e-03
0: TRAIN [6][540/968]	Time 0.542 (0.386)	Data 1.80e-04 (1.50e-03)	Tok/s 85811 (72473)	Loss/tok 3.2724 (3.0739)	LR 2.000e-03
0: TRAIN [6][550/968]	Time 0.423 (0.386)	Data 1.57e-04 (1.48e-03)	Tok/s 79766 (72427)	Loss/tok 3.0471 (3.0733)	LR 2.000e-03
0: TRAIN [6][560/968]	Time 0.312 (0.386)	Data 1.56e-04 (1.46e-03)	Tok/s 64525 (72515)	Loss/tok 2.9639 (3.0747)	LR 2.000e-03
0: TRAIN [6][570/968]	Time 0.544 (0.388)	Data 1.50e-04 (1.43e-03)	Tok/s 86107 (72601)	Loss/tok 3.3186 (3.0783)	LR 2.000e-03
0: TRAIN [6][580/968]	Time 0.313 (0.387)	Data 1.75e-04 (1.41e-03)	Tok/s 66050 (72519)	Loss/tok 2.8670 (3.0778)	LR 2.000e-03
0: TRAIN [6][590/968]	Time 0.423 (0.387)	Data 1.60e-04 (1.39e-03)	Tok/s 80197 (72525)	Loss/tok 3.0577 (3.0776)	LR 2.000e-03
0: TRAIN [6][600/968]	Time 0.425 (0.387)	Data 1.55e-04 (1.37e-03)	Tok/s 79472 (72494)	Loss/tok 3.0817 (3.0778)	LR 2.000e-03
0: TRAIN [6][610/968]	Time 0.545 (0.387)	Data 1.65e-04 (1.35e-03)	Tok/s 85822 (72531)	Loss/tok 3.2603 (3.0784)	LR 2.000e-03
0: TRAIN [6][620/968]	Time 0.683 (0.387)	Data 1.66e-04 (1.33e-03)	Tok/s 87228 (72547)	Loss/tok 3.3759 (3.0802)	LR 2.000e-03
0: TRAIN [6][630/968]	Time 0.543 (0.387)	Data 1.48e-04 (1.32e-03)	Tok/s 85780 (72539)	Loss/tok 3.2600 (3.0805)	LR 2.000e-03
0: TRAIN [6][640/968]	Time 0.209 (0.388)	Data 1.47e-04 (1.30e-03)	Tok/s 51213 (72525)	Loss/tok 2.5670 (3.0814)	LR 2.000e-03
0: TRAIN [6][650/968]	Time 0.313 (0.387)	Data 1.53e-04 (1.28e-03)	Tok/s 66934 (72515)	Loss/tok 2.9146 (3.0817)	LR 2.000e-03
0: TRAIN [6][660/968]	Time 0.423 (0.388)	Data 2.29e-04 (1.26e-03)	Tok/s 79613 (72533)	Loss/tok 3.1108 (3.0833)	LR 2.000e-03
0: TRAIN [6][670/968]	Time 0.311 (0.387)	Data 1.49e-04 (1.25e-03)	Tok/s 67329 (72491)	Loss/tok 2.9295 (3.0823)	LR 2.000e-03
0: TRAIN [6][680/968]	Time 0.683 (0.387)	Data 1.96e-04 (1.23e-03)	Tok/s 87834 (72498)	Loss/tok 3.3754 (3.0824)	LR 1.000e-03
0: TRAIN [6][690/968]	Time 0.310 (0.388)	Data 1.60e-04 (1.22e-03)	Tok/s 66047 (72557)	Loss/tok 2.7999 (3.0832)	LR 1.000e-03
0: TRAIN [6][700/968]	Time 0.422 (0.387)	Data 1.49e-04 (1.20e-03)	Tok/s 79484 (72521)	Loss/tok 3.0899 (3.0817)	LR 1.000e-03
0: TRAIN [6][710/968]	Time 0.542 (0.388)	Data 1.63e-04 (1.19e-03)	Tok/s 85434 (72560)	Loss/tok 3.2356 (3.0819)	LR 1.000e-03
0: TRAIN [6][720/968]	Time 0.423 (0.387)	Data 1.48e-04 (1.17e-03)	Tok/s 79727 (72543)	Loss/tok 3.0124 (3.0810)	LR 1.000e-03
0: TRAIN [6][730/968]	Time 0.312 (0.387)	Data 1.59e-04 (1.16e-03)	Tok/s 66607 (72542)	Loss/tok 2.9408 (3.0802)	LR 1.000e-03
0: TRAIN [6][740/968]	Time 0.205 (0.387)	Data 1.73e-04 (1.15e-03)	Tok/s 52068 (72499)	Loss/tok 2.5090 (3.0792)	LR 1.000e-03
0: TRAIN [6][750/968]	Time 0.313 (0.386)	Data 1.48e-04 (1.13e-03)	Tok/s 65884 (72477)	Loss/tok 2.9186 (3.0790)	LR 1.000e-03
0: TRAIN [6][760/968]	Time 0.683 (0.387)	Data 1.60e-04 (1.12e-03)	Tok/s 86960 (72513)	Loss/tok 3.3657 (3.0798)	LR 1.000e-03
0: TRAIN [6][770/968]	Time 0.206 (0.387)	Data 1.56e-04 (1.11e-03)	Tok/s 50240 (72527)	Loss/tok 2.5091 (3.0809)	LR 1.000e-03
0: TRAIN [6][780/968]	Time 0.312 (0.387)	Data 1.52e-04 (1.10e-03)	Tok/s 67645 (72495)	Loss/tok 2.8965 (3.0795)	LR 1.000e-03
0: TRAIN [6][790/968]	Time 0.425 (0.386)	Data 1.47e-04 (1.08e-03)	Tok/s 78892 (72454)	Loss/tok 3.0981 (3.0784)	LR 1.000e-03
0: TRAIN [6][800/968]	Time 0.686 (0.387)	Data 1.75e-04 (1.07e-03)	Tok/s 86387 (72453)	Loss/tok 3.4730 (3.0788)	LR 1.000e-03
0: TRAIN [6][810/968]	Time 0.205 (0.386)	Data 1.48e-04 (1.06e-03)	Tok/s 52282 (72455)	Loss/tok 2.4965 (3.0781)	LR 1.000e-03
0: TRAIN [6][820/968]	Time 0.312 (0.387)	Data 1.74e-04 (1.05e-03)	Tok/s 66344 (72460)	Loss/tok 2.8349 (3.0787)	LR 1.000e-03
0: TRAIN [6][830/968]	Time 0.545 (0.386)	Data 1.50e-04 (1.04e-03)	Tok/s 86139 (72476)	Loss/tok 3.2501 (3.0780)	LR 1.000e-03
0: TRAIN [6][840/968]	Time 0.545 (0.387)	Data 1.51e-04 (1.03e-03)	Tok/s 85183 (72533)	Loss/tok 3.2567 (3.0784)	LR 1.000e-03
0: TRAIN [6][850/968]	Time 0.311 (0.387)	Data 1.68e-04 (1.02e-03)	Tok/s 65742 (72504)	Loss/tok 2.8818 (3.0782)	LR 1.000e-03
0: TRAIN [6][860/968]	Time 0.425 (0.387)	Data 2.09e-04 (1.01e-03)	Tok/s 78941 (72512)	Loss/tok 3.0248 (3.0775)	LR 1.000e-03
0: TRAIN [6][870/968]	Time 0.311 (0.387)	Data 1.55e-04 (1.00e-03)	Tok/s 67121 (72509)	Loss/tok 2.8933 (3.0787)	LR 1.000e-03
0: TRAIN [6][880/968]	Time 0.205 (0.386)	Data 1.82e-04 (9.91e-04)	Tok/s 51527 (72440)	Loss/tok 2.5301 (3.0776)	LR 1.000e-03
0: TRAIN [6][890/968]	Time 0.544 (0.386)	Data 1.49e-04 (9.81e-04)	Tok/s 86155 (72428)	Loss/tok 3.2452 (3.0773)	LR 1.000e-03
0: TRAIN [6][900/968]	Time 0.312 (0.386)	Data 1.71e-04 (9.73e-04)	Tok/s 66826 (72456)	Loss/tok 2.8822 (3.0768)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [6][910/968]	Time 0.543 (0.387)	Data 1.77e-04 (9.64e-04)	Tok/s 86654 (72486)	Loss/tok 3.2395 (3.0777)	LR 1.000e-03
0: TRAIN [6][920/968]	Time 0.424 (0.386)	Data 1.51e-04 (9.55e-04)	Tok/s 79535 (72466)	Loss/tok 3.0769 (3.0767)	LR 1.000e-03
0: TRAIN [6][930/968]	Time 0.312 (0.387)	Data 1.51e-04 (9.47e-04)	Tok/s 65559 (72519)	Loss/tok 2.8898 (3.0770)	LR 1.000e-03
0: TRAIN [6][940/968]	Time 0.311 (0.387)	Data 1.48e-04 (9.38e-04)	Tok/s 66015 (72496)	Loss/tok 2.9006 (3.0767)	LR 1.000e-03
0: TRAIN [6][950/968]	Time 0.423 (0.387)	Data 1.55e-04 (9.30e-04)	Tok/s 79445 (72517)	Loss/tok 3.0514 (3.0762)	LR 1.000e-03
0: TRAIN [6][960/968]	Time 0.544 (0.387)	Data 1.65e-04 (9.22e-04)	Tok/s 85576 (72542)	Loss/tok 3.2475 (3.0764)	LR 1.000e-03
:::MLL 1571248326.016 epoch_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 524}}
:::MLL 1571248326.016 eval_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [6][0/3]	Time 0.620 (0.620)	Decoder iters 102.0 (102.0)	Tok/s 26000 (26000)
0: Running moses detokenizer
0: BLEU(score=24.51460880373646, counts=[36896, 18678, 10699, 6377], totals=[64517, 61514, 58511, 55514], precisions=[57.18802796162252, 30.363819618298272, 18.28545059903266, 11.487192419930107], bp=0.997538567575272, sys_len=64517, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1571248327.836 eval_accuracy: {"value": 24.51, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 535}}
:::MLL 1571248327.836 eval_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 6	Training Loss: 3.0754	Test BLEU: 24.51
0: Performance: Epoch: 6	Training: 580594 Tok/s
0: Finished epoch 6
:::MLL 1571248327.836 block_stop: {"value": null, "metadata": {"first_epoch_num": 7, "file": "train.py", "lineno": 557}}
0: Closing preprocessed data file
:::MLL 1571248327.837 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 568}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-10-16 05:52:20 PM
RESULT,RNN_TRANSLATOR,,2699,nvidia,2019-10-16 05:07:21 PM
