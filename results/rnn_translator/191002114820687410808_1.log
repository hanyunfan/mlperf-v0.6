Beginning trial 1 of 2
Gathering sys log on dss01
:::MLL 1570034994.864 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1570034994.864 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1570034994.865 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1570034994.865 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1570034994.865 submission_platform: {"value": "1xDSS8440", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1570034994.866 submission_entry: {"value": "{'hardware': 'DSS8440', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '5.0-0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz', 'num_cores': '40', 'num_vcpus': '40', 'accelerator': 'Tesla V100-PCIE-32GB', 'num_accelerators': '8', 'sys_mem_size': '754 GB', 'sys_storage_type': 'SATA SSD', 'sys_storage_size': '1x 447.1G + 1x 931.5G', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27700 Family [ConnectX-4]', 'num_network_cards': '1', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1570034994.866 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1570034994.867 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1570034999.927 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node dss01
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=4346' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=191002114820687410808 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191002114820687410808 ./run_and_time.sh
Run vars: id 191002114820687410808 gpus 8 mparams  --master_port=4346
NCCL_SOCKET_NTHREADS=2
STARTING TIMING RUN AT 2019-10-02 04:50:00 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=0
+ MATH=amp_fp16
+ [[ 0 -eq 1 ]]
+ LAUNCH_OPT='torch.distributed.launch --nproc_per_node 8  --master_port=4346'
+ echo 'running benchmark'
running benchmark
+ python -m torch.distributed.launch --nproc_per_node 8 --master_port=4346 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1570035002.763 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570035002.763 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570035002.763 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570035002.763 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570035002.764 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570035002.764 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570035002.764 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570035002.765 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 4053202042
dss01:464:464 [0] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:464:464 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:464:464 [0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:464:464 [0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:464:464 [0] NCCL INFO NET/IB : No device found.
NCCL version 2.4.6+cuda10.1
dss01:467:467 [3] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:467:467 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:468:468 [4] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:468:468 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:466:466 [2] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:466:466 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:469:469 [5] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:469:469 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:468:468 [4] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:468:468 [4] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:468:468 [4] NCCL INFO NET/IB : No device found.

dss01:469:469 [5] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:466:466 [2] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:467:467 [3] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
dss01:471:471 [7] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>

dss01:469:469 [5] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0

dss01:466:466 [2] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0

dss01:467:467 [3] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:469:469 [5] NCCL INFO NET/IB : No device found.
dss01:466:466 [2] NCCL INFO NET/IB : No device found.
dss01:467:467 [3] NCCL INFO NET/IB : No device found.
dss01:471:471 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:470:470 [6] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:470:470 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:471:471 [7] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:471:471 [7] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:471:471 [7] NCCL INFO NET/IB : No device found.

dss01:470:470 [6] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:470:470 [6] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:470:470 [6] NCCL INFO NET/IB : No device found.
dss01:465:465 [1] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:465:465 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:465:465 [1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:465:465 [1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:465:465 [1] NCCL INFO NET/IB : No device found.
dss01:464:826 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555
dss01:467:827 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555
dss01:469:828 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa
dss01:466:829 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555
dss01:468:830 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa
dss01:471:831 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa
dss01:470:832 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa
dss01:465:833 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555
dss01:470:832 [6] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:468:830 [4] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:469:828 [5] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:467:827 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:466:829 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:471:831 [7] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:464:826 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:465:833 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:464:826 [0] NCCL INFO Channel 00 :    0   1   2   3   4   5   6   7
dss01:466:829 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/IPC
dss01:470:832 [6] NCCL INFO Ring 00 : 6[6] -> 7[7] via P2P/IPC
dss01:464:826 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/IPC
dss01:468:830 [4] NCCL INFO Ring 00 : 4[4] -> 5[5] via P2P/IPC
dss01:467:827 [3] NCCL INFO Ring 00 : 3[3] -> 4[4] via direct shared memory
dss01:465:833 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via direct shared memory
dss01:469:828 [5] NCCL INFO Ring 00 : 5[5] -> 6[6] via direct shared memory
dss01:471:831 [7] NCCL INFO Ring 00 : 7[7] -> 0[0] via direct shared memory
dss01:464:826 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
dss01:467:827 [3] NCCL INFO comm 0x7fff48007590 rank 3 nranks 8 cudaDev 3 nvmlDev 3 - Init COMPLETE
dss01:465:833 [1] NCCL INFO comm 0x7fff6c007590 rank 1 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE
dss01:471:831 [7] NCCL INFO comm 0x7ffe98007590 rank 7 nranks 8 cudaDev 7 nvmlDev 7 - Init COMPLETE
dss01:469:828 [5] NCCL INFO comm 0x7fff78007590 rank 5 nranks 8 cudaDev 5 nvmlDev 5 - Init COMPLETE
dss01:466:829 [2] NCCL INFO comm 0x7fff70007590 rank 2 nranks 8 cudaDev 2 nvmlDev 2 - Init COMPLETE
dss01:470:832 [6] NCCL INFO comm 0x7ffe98007590 rank 6 nranks 8 cudaDev 6 nvmlDev 6 - Init COMPLETE
dss01:464:826 [0] NCCL INFO comm 0x7fff3c007590 rank 0 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE
dss01:464:464 [0] NCCL INFO Launch mode Parallel
0: Worker 0 is using worker seed: 358057808
dss01:468:830 [4] NCCL INFO comm 0x7ffe84007590 rank 4 nranks 8 cudaDev 4 nvmlDev 4 - Init COMPLETE
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1570035027.137 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1570035029.758 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1570035029.759 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1570035029.759 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1570035030.780 global_batch_size: {"value": 2048, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1570035030.782 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1570035030.782 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1570035030.783 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1570035030.783 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1570035030.783 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1570035030.783 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1570035030.784 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1570035030.806 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1570035030.808 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 4091240392
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/1938]	Time 1.000 (1.000)	Data 7.82e-01 (7.82e-01)	Tok/s 10228 (10228)	Loss/tok 10.6356 (10.6356)	LR 2.000e-05
0: TRAIN [0][10/1938]	Time 0.212 (0.316)	Data 1.21e-04 (7.12e-02)	Tok/s 48926 (48828)	Loss/tok 9.7738 (10.2089)	LR 2.518e-05
0: TRAIN [0][20/1938]	Time 0.213 (0.290)	Data 1.39e-04 (3.74e-02)	Tok/s 48525 (51628)	Loss/tok 9.2570 (9.8722)	LR 3.170e-05
0: TRAIN [0][30/1938]	Time 0.156 (0.281)	Data 1.49e-04 (2.54e-02)	Tok/s 35323 (53428)	Loss/tok 8.7253 (9.6377)	LR 3.991e-05
0: TRAIN [0][40/1938]	Time 0.331 (0.270)	Data 1.31e-04 (1.92e-02)	Tok/s 70739 (53276)	Loss/tok 8.9755 (9.4714)	LR 5.024e-05
0: TRAIN [0][50/1938]	Time 0.158 (0.265)	Data 1.60e-04 (1.55e-02)	Tok/s 32709 (53198)	Loss/tok 8.1644 (9.3200)	LR 6.325e-05
0: TRAIN [0][60/1938]	Time 0.212 (0.270)	Data 1.87e-04 (1.30e-02)	Tok/s 48598 (54670)	Loss/tok 8.2240 (9.1577)	LR 7.962e-05
0: TRAIN [0][70/1938]	Time 0.213 (0.265)	Data 1.29e-04 (1.11e-02)	Tok/s 47447 (54425)	Loss/tok 8.0808 (9.0437)	LR 1.002e-04
0: TRAIN [0][80/1938]	Time 0.273 (0.266)	Data 1.30e-04 (9.79e-03)	Tok/s 61322 (54927)	Loss/tok 8.0355 (8.9197)	LR 1.262e-04
0: TRAIN [0][90/1938]	Time 0.211 (0.262)	Data 1.31e-04 (8.72e-03)	Tok/s 48751 (54661)	Loss/tok 7.8993 (8.8314)	LR 1.589e-04
0: TRAIN [0][100/1938]	Time 0.333 (0.262)	Data 1.18e-04 (7.88e-03)	Tok/s 69525 (54946)	Loss/tok 8.0908 (8.7445)	LR 2.000e-04
0: TRAIN [0][110/1938]	Time 0.158 (0.262)	Data 1.15e-04 (7.18e-03)	Tok/s 33379 (55084)	Loss/tok 7.3085 (8.6694)	LR 2.518e-04
0: TRAIN [0][120/1938]	Time 0.212 (0.261)	Data 1.14e-04 (6.60e-03)	Tok/s 49032 (54964)	Loss/tok 7.7698 (8.6092)	LR 3.170e-04
0: TRAIN [0][130/1938]	Time 0.212 (0.260)	Data 1.27e-04 (6.10e-03)	Tok/s 48524 (54973)	Loss/tok 7.7100 (8.5508)	LR 3.991e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][140/1938]	Time 0.212 (0.259)	Data 1.61e-04 (5.68e-03)	Tok/s 48963 (54989)	Loss/tok 7.5869 (8.5032)	LR 4.798e-04
0: TRAIN [0][150/1938]	Time 0.212 (0.257)	Data 1.09e-04 (5.31e-03)	Tok/s 50862 (54721)	Loss/tok 7.5642 (8.4611)	LR 6.040e-04
0: TRAIN [0][160/1938]	Time 0.212 (0.256)	Data 1.27e-04 (4.99e-03)	Tok/s 49436 (54718)	Loss/tok 7.4758 (8.4148)	LR 7.604e-04
0: TRAIN [0][170/1938]	Time 0.211 (0.255)	Data 1.16e-04 (4.71e-03)	Tok/s 48578 (54685)	Loss/tok 7.4739 (8.3657)	LR 9.573e-04
0: TRAIN [0][180/1938]	Time 0.212 (0.254)	Data 1.13e-04 (4.45e-03)	Tok/s 48711 (54538)	Loss/tok 7.2821 (8.3179)	LR 1.205e-03
0: TRAIN [0][190/1938]	Time 0.274 (0.255)	Data 1.16e-04 (4.23e-03)	Tok/s 61726 (54793)	Loss/tok 7.2657 (8.2593)	LR 1.517e-03
0: TRAIN [0][200/1938]	Time 0.274 (0.255)	Data 1.21e-04 (4.02e-03)	Tok/s 62466 (54785)	Loss/tok 7.0883 (8.2037)	LR 1.910e-03
0: TRAIN [0][210/1938]	Time 0.333 (0.255)	Data 1.27e-04 (3.84e-03)	Tok/s 69708 (54844)	Loss/tok 7.1072 (8.1432)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.212 (0.254)	Data 1.40e-04 (3.68e-03)	Tok/s 48979 (54735)	Loss/tok 6.5809 (8.0849)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.156 (0.254)	Data 1.27e-04 (3.52e-03)	Tok/s 34371 (54793)	Loss/tok 5.7131 (8.0233)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.212 (0.253)	Data 1.40e-04 (3.38e-03)	Tok/s 48329 (54623)	Loss/tok 6.2921 (7.9697)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.334 (0.253)	Data 1.41e-04 (3.26e-03)	Tok/s 69469 (54556)	Loss/tok 6.6349 (7.9093)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.212 (0.251)	Data 1.63e-04 (3.14e-03)	Tok/s 48585 (54313)	Loss/tok 5.9657 (7.8580)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.211 (0.251)	Data 1.15e-04 (3.02e-03)	Tok/s 47917 (54460)	Loss/tok 6.0028 (7.7928)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.275 (0.251)	Data 1.24e-04 (2.92e-03)	Tok/s 61795 (54328)	Loss/tok 6.1857 (7.7388)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.213 (0.250)	Data 1.21e-04 (2.83e-03)	Tok/s 47695 (54293)	Loss/tok 5.5746 (7.6826)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.336 (0.250)	Data 1.29e-04 (2.74e-03)	Tok/s 69852 (54361)	Loss/tok 6.0536 (7.6204)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.332 (0.251)	Data 1.22e-04 (2.65e-03)	Tok/s 71156 (54423)	Loss/tok 6.0395 (7.5585)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.275 (0.250)	Data 1.22e-04 (2.57e-03)	Tok/s 61987 (54318)	Loss/tok 5.8179 (7.5036)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.335 (0.250)	Data 1.50e-04 (2.50e-03)	Tok/s 70588 (54347)	Loss/tok 5.8312 (7.4467)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.272 (0.251)	Data 1.23e-04 (2.43e-03)	Tok/s 61567 (54469)	Loss/tok 5.6751 (7.3853)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.275 (0.251)	Data 1.52e-04 (2.37e-03)	Tok/s 61636 (54447)	Loss/tok 5.5890 (7.3327)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.336 (0.251)	Data 1.18e-04 (2.31e-03)	Tok/s 68847 (54478)	Loss/tok 5.7461 (7.2774)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.275 (0.251)	Data 1.28e-04 (2.25e-03)	Tok/s 61068 (54577)	Loss/tok 5.2740 (7.2216)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.272 (0.250)	Data 1.23e-04 (2.19e-03)	Tok/s 61405 (54475)	Loss/tok 5.1361 (7.1742)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.158 (0.250)	Data 1.27e-04 (2.14e-03)	Tok/s 33232 (54386)	Loss/tok 4.1035 (7.1267)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][400/1938]	Time 0.213 (0.250)	Data 1.22e-04 (2.09e-03)	Tok/s 48476 (54342)	Loss/tok 4.7394 (7.0797)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.157 (0.250)	Data 1.17e-04 (2.04e-03)	Tok/s 33707 (54267)	Loss/tok 3.8661 (7.0341)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.336 (0.249)	Data 1.33e-04 (2.00e-03)	Tok/s 70443 (54241)	Loss/tok 5.1954 (6.9852)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.157 (0.250)	Data 1.15e-04 (1.95e-03)	Tok/s 33297 (54311)	Loss/tok 3.7507 (6.9324)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.275 (0.250)	Data 1.11e-04 (1.91e-03)	Tok/s 61323 (54314)	Loss/tok 4.8337 (6.8838)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.213 (0.250)	Data 1.44e-04 (1.87e-03)	Tok/s 49397 (54336)	Loss/tok 4.4431 (6.8358)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.411 (0.250)	Data 2.75e-04 (1.84e-03)	Tok/s 72468 (54389)	Loss/tok 5.1872 (6.7868)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.212 (0.250)	Data 1.44e-04 (1.80e-03)	Tok/s 49172 (54339)	Loss/tok 4.4714 (6.7448)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.212 (0.250)	Data 1.17e-04 (1.77e-03)	Tok/s 48263 (54379)	Loss/tok 4.2304 (6.6968)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.271 (0.250)	Data 1.01e-04 (1.73e-03)	Tok/s 61536 (54398)	Loss/tok 4.7273 (6.6527)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.274 (0.250)	Data 1.34e-04 (1.70e-03)	Tok/s 61336 (54397)	Loss/tok 4.4833 (6.6100)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.213 (0.250)	Data 1.15e-04 (1.67e-03)	Tok/s 47939 (54367)	Loss/tok 4.2904 (6.5698)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.276 (0.250)	Data 1.30e-04 (1.64e-03)	Tok/s 61640 (54421)	Loss/tok 4.4608 (6.5261)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.213 (0.250)	Data 1.08e-04 (1.61e-03)	Tok/s 49123 (54444)	Loss/tok 3.9625 (6.4847)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.275 (0.251)	Data 1.17e-04 (1.59e-03)	Tok/s 60921 (54560)	Loss/tok 4.4956 (6.4376)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.213 (0.251)	Data 1.29e-04 (1.56e-03)	Tok/s 48041 (54505)	Loss/tok 3.9640 (6.4025)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.213 (0.250)	Data 1.50e-04 (1.53e-03)	Tok/s 49487 (54387)	Loss/tok 3.9701 (6.3722)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.213 (0.251)	Data 1.20e-04 (1.51e-03)	Tok/s 48307 (54524)	Loss/tok 4.1610 (6.3276)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.213 (0.250)	Data 1.51e-04 (1.49e-03)	Tok/s 48734 (54387)	Loss/tok 3.9543 (6.2991)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.276 (0.250)	Data 1.32e-04 (1.46e-03)	Tok/s 60949 (54417)	Loss/tok 4.3074 (6.2643)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.213 (0.250)	Data 1.36e-04 (1.44e-03)	Tok/s 48224 (54396)	Loss/tok 4.0014 (6.2320)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.213 (0.250)	Data 1.26e-04 (1.42e-03)	Tok/s 49997 (54309)	Loss/tok 3.8534 (6.2037)	LR 2.000e-03
0: TRAIN [0][620/1938]	Time 0.275 (0.250)	Data 1.35e-04 (1.40e-03)	Tok/s 61067 (54377)	Loss/tok 4.2194 (6.1684)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.337 (0.251)	Data 1.51e-04 (1.38e-03)	Tok/s 69996 (54453)	Loss/tok 4.4504 (6.1321)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.337 (0.251)	Data 1.67e-04 (1.36e-03)	Tok/s 70035 (54407)	Loss/tok 4.4201 (6.1035)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][650/1938]	Time 0.210 (0.251)	Data 1.53e-04 (1.34e-03)	Tok/s 49964 (54427)	Loss/tok 3.8820 (6.0723)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.337 (0.251)	Data 1.37e-04 (1.32e-03)	Tok/s 69224 (54478)	Loss/tok 4.5070 (6.0408)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.213 (0.251)	Data 1.12e-04 (1.31e-03)	Tok/s 48697 (54451)	Loss/tok 3.8382 (6.0134)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.275 (0.251)	Data 1.41e-04 (1.29e-03)	Tok/s 60664 (54443)	Loss/tok 4.1646 (5.9861)	LR 2.000e-03
0: TRAIN [0][690/1938]	Time 0.214 (0.250)	Data 1.31e-04 (1.27e-03)	Tok/s 48690 (54341)	Loss/tok 3.8887 (5.9633)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.335 (0.250)	Data 1.34e-04 (1.25e-03)	Tok/s 70173 (54373)	Loss/tok 4.2968 (5.9353)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.335 (0.250)	Data 1.30e-04 (1.24e-03)	Tok/s 70229 (54239)	Loss/tok 4.4251 (5.9155)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.337 (0.250)	Data 1.31e-04 (1.22e-03)	Tok/s 68699 (54300)	Loss/tok 4.3128 (5.8865)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.213 (0.250)	Data 1.13e-04 (1.21e-03)	Tok/s 49034 (54305)	Loss/tok 3.7809 (5.8615)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.275 (0.250)	Data 1.34e-04 (1.20e-03)	Tok/s 60709 (54286)	Loss/tok 4.1510 (5.8381)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.156 (0.250)	Data 1.36e-04 (1.18e-03)	Tok/s 33268 (54284)	Loss/tok 3.1145 (5.8142)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.275 (0.250)	Data 1.08e-04 (1.17e-03)	Tok/s 60679 (54302)	Loss/tok 3.9813 (5.7894)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.274 (0.250)	Data 1.46e-04 (1.15e-03)	Tok/s 61684 (54400)	Loss/tok 3.9181 (5.7615)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.212 (0.250)	Data 1.21e-04 (1.14e-03)	Tok/s 49211 (54395)	Loss/tok 3.7163 (5.7391)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.275 (0.250)	Data 1.25e-04 (1.13e-03)	Tok/s 60983 (54375)	Loss/tok 4.1056 (5.7182)	LR 2.000e-03
0: TRAIN [0][800/1938]	Time 0.274 (0.250)	Data 1.14e-04 (1.12e-03)	Tok/s 60842 (54404)	Loss/tok 3.9529 (5.6946)	LR 2.000e-03
0: TRAIN [0][810/1938]	Time 0.276 (0.251)	Data 1.46e-04 (1.11e-03)	Tok/s 60300 (54444)	Loss/tok 4.0097 (5.6716)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.157 (0.251)	Data 1.42e-04 (1.09e-03)	Tok/s 32772 (54451)	Loss/tok 3.0521 (5.6498)	LR 2.000e-03
0: TRAIN [0][830/1938]	Time 0.337 (0.251)	Data 1.34e-04 (1.08e-03)	Tok/s 69288 (54485)	Loss/tok 4.1677 (5.6280)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][840/1938]	Time 0.158 (0.251)	Data 1.25e-04 (1.07e-03)	Tok/s 33288 (54484)	Loss/tok 3.0724 (5.6079)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.275 (0.251)	Data 1.23e-04 (1.06e-03)	Tok/s 60906 (54493)	Loss/tok 3.9332 (5.5881)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.212 (0.251)	Data 1.17e-04 (1.05e-03)	Tok/s 48323 (54471)	Loss/tok 3.7543 (5.5702)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.213 (0.251)	Data 1.18e-04 (1.04e-03)	Tok/s 48474 (54530)	Loss/tok 3.7520 (5.5486)	LR 2.000e-03
0: TRAIN [0][880/1938]	Time 0.213 (0.251)	Data 1.51e-04 (1.03e-03)	Tok/s 47993 (54508)	Loss/tok 3.7027 (5.5303)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.337 (0.252)	Data 1.82e-04 (1.02e-03)	Tok/s 69188 (54561)	Loss/tok 4.1931 (5.5094)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.212 (0.251)	Data 1.28e-04 (1.01e-03)	Tok/s 48469 (54531)	Loss/tok 3.6504 (5.4922)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.213 (0.252)	Data 1.15e-04 (9.99e-04)	Tok/s 48621 (54536)	Loss/tok 3.6171 (5.4743)	LR 2.000e-03
0: TRAIN [0][920/1938]	Time 0.274 (0.251)	Data 1.32e-04 (9.90e-04)	Tok/s 61265 (54515)	Loss/tok 3.7523 (5.4580)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.274 (0.252)	Data 1.11e-04 (9.82e-04)	Tok/s 61834 (54591)	Loss/tok 3.8605 (5.4380)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.275 (0.252)	Data 1.25e-04 (9.73e-04)	Tok/s 62011 (54612)	Loss/tok 3.9106 (5.4203)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.336 (0.252)	Data 2.15e-04 (9.64e-04)	Tok/s 68911 (54643)	Loss/tok 4.0395 (5.4024)	LR 2.000e-03
0: TRAIN [0][960/1938]	Time 0.214 (0.252)	Data 1.22e-04 (9.56e-04)	Tok/s 48261 (54643)	Loss/tok 3.3821 (5.3861)	LR 2.000e-03
0: TRAIN [0][970/1938]	Time 0.213 (0.252)	Data 1.36e-04 (9.48e-04)	Tok/s 48101 (54682)	Loss/tok 3.4643 (5.3679)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.276 (0.252)	Data 1.29e-04 (9.40e-04)	Tok/s 61017 (54679)	Loss/tok 3.9005 (5.3527)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.212 (0.252)	Data 1.36e-04 (9.32e-04)	Tok/s 48366 (54668)	Loss/tok 3.5233 (5.3378)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.337 (0.253)	Data 1.50e-04 (9.24e-04)	Tok/s 69028 (54774)	Loss/tok 4.0648 (5.3184)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.277 (0.252)	Data 2.96e-04 (9.16e-04)	Tok/s 60555 (54760)	Loss/tok 3.8245 (5.3040)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.213 (0.253)	Data 1.15e-04 (9.08e-04)	Tok/s 47863 (54783)	Loss/tok 3.5091 (5.2879)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.212 (0.253)	Data 1.09e-04 (9.00e-04)	Tok/s 48767 (54797)	Loss/tok 3.4867 (5.2726)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.212 (0.253)	Data 1.16e-04 (8.93e-04)	Tok/s 49432 (54822)	Loss/tok 3.6370 (5.2578)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.275 (0.253)	Data 1.18e-04 (8.86e-04)	Tok/s 61205 (54850)	Loss/tok 3.8024 (5.2427)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.214 (0.253)	Data 1.18e-04 (8.79e-04)	Tok/s 48324 (54873)	Loss/tok 3.5243 (5.2283)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.158 (0.253)	Data 1.32e-04 (8.72e-04)	Tok/s 33602 (54853)	Loss/tok 3.0311 (5.2158)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.213 (0.253)	Data 1.40e-04 (8.65e-04)	Tok/s 48896 (54854)	Loss/tok 3.5527 (5.2022)	LR 2.000e-03
0: TRAIN [0][1090/1938]	Time 0.212 (0.253)	Data 1.47e-04 (8.59e-04)	Tok/s 48413 (54868)	Loss/tok 3.4098 (5.1883)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.213 (0.253)	Data 1.46e-04 (8.53e-04)	Tok/s 48190 (54859)	Loss/tok 3.4381 (5.1755)	LR 2.000e-03
0: TRAIN [0][1110/1938]	Time 0.213 (0.253)	Data 1.30e-04 (8.46e-04)	Tok/s 48748 (54886)	Loss/tok 3.5795 (5.1621)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.213 (0.253)	Data 1.19e-04 (8.40e-04)	Tok/s 49093 (54862)	Loss/tok 3.5242 (5.1503)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.213 (0.253)	Data 1.38e-04 (8.34e-04)	Tok/s 48080 (54852)	Loss/tok 3.4920 (5.1382)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.214 (0.253)	Data 1.15e-04 (8.27e-04)	Tok/s 47321 (54866)	Loss/tok 3.4569 (5.1256)	LR 2.000e-03
0: TRAIN [0][1150/1938]	Time 0.277 (0.253)	Data 1.64e-04 (8.21e-04)	Tok/s 60034 (54863)	Loss/tok 3.6848 (5.1134)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.337 (0.253)	Data 1.07e-04 (8.15e-04)	Tok/s 68268 (54871)	Loss/tok 3.9121 (5.1008)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1170/1938]	Time 0.210 (0.253)	Data 1.39e-04 (8.09e-04)	Tok/s 49639 (54878)	Loss/tok 3.4207 (5.0883)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.156 (0.253)	Data 1.16e-04 (8.04e-04)	Tok/s 33139 (54861)	Loss/tok 2.8963 (5.0774)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.275 (0.253)	Data 1.19e-04 (7.98e-04)	Tok/s 60949 (54855)	Loss/tok 3.6399 (5.0662)	LR 2.000e-03
0: TRAIN [0][1200/1938]	Time 0.213 (0.253)	Data 1.17e-04 (7.93e-04)	Tok/s 50448 (54833)	Loss/tok 3.4042 (5.0558)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.212 (0.253)	Data 1.22e-04 (7.87e-04)	Tok/s 48545 (54846)	Loss/tok 3.5152 (5.0446)	LR 2.000e-03
0: TRAIN [0][1220/1938]	Time 0.213 (0.253)	Data 1.32e-04 (7.82e-04)	Tok/s 48605 (54886)	Loss/tok 3.4849 (5.0328)	LR 2.000e-03
0: TRAIN [0][1230/1938]	Time 0.213 (0.253)	Data 1.16e-04 (7.76e-04)	Tok/s 47952 (54890)	Loss/tok 3.4631 (5.0217)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.411 (0.253)	Data 1.18e-04 (7.71e-04)	Tok/s 72465 (54892)	Loss/tok 4.1363 (5.0106)	LR 2.000e-03
0: TRAIN [0][1250/1938]	Time 0.337 (0.253)	Data 1.30e-04 (7.66e-04)	Tok/s 68530 (54885)	Loss/tok 3.8959 (5.0001)	LR 2.000e-03
0: TRAIN [0][1260/1938]	Time 0.275 (0.253)	Data 1.16e-04 (7.61e-04)	Tok/s 61390 (54889)	Loss/tok 3.7002 (4.9895)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.213 (0.253)	Data 1.23e-04 (7.56e-04)	Tok/s 47150 (54897)	Loss/tok 3.5159 (4.9796)	LR 2.000e-03
0: TRAIN [0][1280/1938]	Time 0.158 (0.253)	Data 1.19e-04 (7.51e-04)	Tok/s 33806 (54863)	Loss/tok 2.9643 (4.9705)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.275 (0.253)	Data 1.31e-04 (7.46e-04)	Tok/s 60875 (54899)	Loss/tok 3.7681 (4.9601)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.214 (0.253)	Data 1.32e-04 (7.42e-04)	Tok/s 48100 (54872)	Loss/tok 3.3613 (4.9510)	LR 2.000e-03
0: TRAIN [0][1310/1938]	Time 0.213 (0.253)	Data 1.16e-04 (7.37e-04)	Tok/s 48943 (54872)	Loss/tok 3.4407 (4.9414)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.213 (0.253)	Data 1.15e-04 (7.32e-04)	Tok/s 49239 (54868)	Loss/tok 3.4127 (4.9320)	LR 2.000e-03
0: TRAIN [0][1330/1938]	Time 0.276 (0.253)	Data 1.23e-04 (7.28e-04)	Tok/s 60915 (54862)	Loss/tok 3.6958 (4.9226)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.213 (0.253)	Data 1.37e-04 (7.23e-04)	Tok/s 48204 (54862)	Loss/tok 3.3437 (4.9130)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.277 (0.253)	Data 1.31e-04 (7.19e-04)	Tok/s 60999 (54858)	Loss/tok 3.7404 (4.9038)	LR 2.000e-03
0: TRAIN [0][1360/1938]	Time 0.338 (0.253)	Data 1.35e-04 (7.15e-04)	Tok/s 69990 (54889)	Loss/tok 3.7980 (4.8936)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.275 (0.253)	Data 1.30e-04 (7.11e-04)	Tok/s 59996 (54912)	Loss/tok 3.7881 (4.8839)	LR 2.000e-03
0: TRAIN [0][1380/1938]	Time 0.157 (0.253)	Data 1.32e-04 (7.07e-04)	Tok/s 33831 (54911)	Loss/tok 2.8322 (4.8746)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.213 (0.253)	Data 1.38e-04 (7.03e-04)	Tok/s 48371 (54833)	Loss/tok 3.4614 (4.8680)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.213 (0.253)	Data 1.26e-04 (6.99e-04)	Tok/s 48359 (54802)	Loss/tok 3.2244 (4.8600)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.213 (0.252)	Data 1.11e-04 (6.95e-04)	Tok/s 48953 (54769)	Loss/tok 3.2997 (4.8520)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.157 (0.252)	Data 1.28e-04 (6.91e-04)	Tok/s 34529 (54736)	Loss/tok 2.9341 (4.8441)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1430/1938]	Time 0.275 (0.253)	Data 1.13e-04 (6.87e-04)	Tok/s 61019 (54765)	Loss/tok 3.6889 (4.8353)	LR 2.000e-03
0: TRAIN [0][1440/1938]	Time 0.338 (0.252)	Data 1.28e-04 (6.83e-04)	Tok/s 68490 (54755)	Loss/tok 3.7791 (4.8271)	LR 2.000e-03
0: TRAIN [0][1450/1938]	Time 0.213 (0.253)	Data 1.09e-04 (6.79e-04)	Tok/s 49162 (54764)	Loss/tok 3.4191 (4.8187)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.213 (0.253)	Data 1.14e-04 (6.75e-04)	Tok/s 47903 (54794)	Loss/tok 3.3885 (4.8096)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.213 (0.253)	Data 1.10e-04 (6.71e-04)	Tok/s 48781 (54801)	Loss/tok 3.4990 (4.8016)	LR 2.000e-03
0: TRAIN [0][1480/1938]	Time 0.277 (0.253)	Data 1.22e-04 (6.68e-04)	Tok/s 61031 (54810)	Loss/tok 3.5353 (4.7932)	LR 2.000e-03
0: TRAIN [0][1490/1938]	Time 0.212 (0.253)	Data 1.41e-04 (6.64e-04)	Tok/s 48022 (54780)	Loss/tok 3.2675 (4.7861)	LR 2.000e-03
0: TRAIN [0][1500/1938]	Time 0.337 (0.253)	Data 1.25e-04 (6.61e-04)	Tok/s 69589 (54806)	Loss/tok 3.8278 (4.7777)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.214 (0.253)	Data 1.12e-04 (6.57e-04)	Tok/s 49144 (54793)	Loss/tok 3.2723 (4.7704)	LR 2.000e-03
0: TRAIN [0][1520/1938]	Time 0.273 (0.253)	Data 1.14e-04 (6.53e-04)	Tok/s 61488 (54816)	Loss/tok 3.5054 (4.7622)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.212 (0.253)	Data 1.16e-04 (6.50e-04)	Tok/s 48649 (54807)	Loss/tok 3.4153 (4.7549)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.213 (0.253)	Data 1.27e-04 (6.47e-04)	Tok/s 48775 (54803)	Loss/tok 3.1391 (4.7473)	LR 2.000e-03
0: TRAIN [0][1550/1938]	Time 0.214 (0.253)	Data 1.28e-04 (6.43e-04)	Tok/s 47899 (54825)	Loss/tok 3.2874 (4.7391)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.213 (0.253)	Data 1.17e-04 (6.40e-04)	Tok/s 47561 (54813)	Loss/tok 3.2169 (4.7321)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.274 (0.253)	Data 1.15e-04 (6.37e-04)	Tok/s 60987 (54816)	Loss/tok 3.5404 (4.7249)	LR 2.000e-03
0: TRAIN [0][1580/1938]	Time 0.212 (0.253)	Data 1.10e-04 (6.33e-04)	Tok/s 48677 (54863)	Loss/tok 3.3163 (4.7164)	LR 2.000e-03
0: TRAIN [0][1590/1938]	Time 0.214 (0.253)	Data 1.09e-04 (6.30e-04)	Tok/s 48083 (54867)	Loss/tok 3.4890 (4.7095)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.275 (0.253)	Data 1.12e-04 (6.27e-04)	Tok/s 61017 (54863)	Loss/tok 3.6077 (4.7027)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.275 (0.253)	Data 1.31e-04 (6.24e-04)	Tok/s 61559 (54883)	Loss/tok 3.6789 (4.6953)	LR 2.000e-03
0: TRAIN [0][1620/1938]	Time 0.213 (0.253)	Data 1.13e-04 (6.21e-04)	Tok/s 49589 (54859)	Loss/tok 3.3608 (4.6891)	LR 2.000e-03
0: TRAIN [0][1630/1938]	Time 0.214 (0.253)	Data 1.13e-04 (6.18e-04)	Tok/s 48428 (54860)	Loss/tok 3.3748 (4.6824)	LR 2.000e-03
0: TRAIN [0][1640/1938]	Time 0.156 (0.253)	Data 1.25e-04 (6.15e-04)	Tok/s 34441 (54808)	Loss/tok 2.9207 (4.6768)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.213 (0.253)	Data 1.18e-04 (6.12e-04)	Tok/s 48012 (54815)	Loss/tok 3.4032 (4.6703)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.337 (0.253)	Data 1.26e-04 (6.09e-04)	Tok/s 69403 (54845)	Loss/tok 3.8297 (4.6632)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.213 (0.253)	Data 1.16e-04 (6.06e-04)	Tok/s 48290 (54845)	Loss/tok 3.3163 (4.6566)	LR 2.000e-03
0: TRAIN [0][1680/1938]	Time 0.213 (0.253)	Data 1.23e-04 (6.03e-04)	Tok/s 47876 (54826)	Loss/tok 3.4323 (4.6507)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.277 (0.253)	Data 1.23e-04 (6.00e-04)	Tok/s 60508 (54824)	Loss/tok 3.6056 (4.6442)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.214 (0.253)	Data 1.25e-04 (5.98e-04)	Tok/s 49654 (54850)	Loss/tok 3.3089 (4.6374)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1710/1938]	Time 0.410 (0.253)	Data 1.25e-04 (5.95e-04)	Tok/s 71533 (54901)	Loss/tok 3.9156 (4.6303)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.213 (0.253)	Data 1.30e-04 (5.92e-04)	Tok/s 49407 (54900)	Loss/tok 3.3283 (4.6244)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.213 (0.253)	Data 1.14e-04 (5.90e-04)	Tok/s 47997 (54908)	Loss/tok 3.2912 (4.6182)	LR 2.000e-03
0: TRAIN [0][1740/1938]	Time 0.214 (0.253)	Data 1.32e-04 (5.87e-04)	Tok/s 48593 (54884)	Loss/tok 3.3589 (4.6126)	LR 2.000e-03
0: TRAIN [0][1750/1938]	Time 0.213 (0.253)	Data 1.29e-04 (5.84e-04)	Tok/s 48635 (54887)	Loss/tok 3.3837 (4.6066)	LR 2.000e-03
0: TRAIN [0][1760/1938]	Time 0.276 (0.253)	Data 1.13e-04 (5.82e-04)	Tok/s 61335 (54892)	Loss/tok 3.6361 (4.6006)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.274 (0.253)	Data 1.07e-04 (5.79e-04)	Tok/s 61466 (54903)	Loss/tok 3.7259 (4.5944)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.276 (0.253)	Data 1.10e-04 (5.77e-04)	Tok/s 61422 (54858)	Loss/tok 3.4659 (4.5895)	LR 2.000e-03
0: TRAIN [0][1790/1938]	Time 0.213 (0.253)	Data 1.15e-04 (5.74e-04)	Tok/s 48839 (54857)	Loss/tok 3.2315 (4.5837)	LR 2.000e-03
0: TRAIN [0][1800/1938]	Time 0.213 (0.253)	Data 1.19e-04 (5.72e-04)	Tok/s 49270 (54860)	Loss/tok 3.2459 (4.5777)	LR 2.000e-03
0: TRAIN [0][1810/1938]	Time 0.214 (0.253)	Data 1.29e-04 (5.69e-04)	Tok/s 47916 (54889)	Loss/tok 3.3970 (4.5715)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.276 (0.253)	Data 5.10e-04 (5.67e-04)	Tok/s 60456 (54910)	Loss/tok 3.4927 (4.5651)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.338 (0.254)	Data 1.20e-04 (5.65e-04)	Tok/s 69112 (54936)	Loss/tok 3.7517 (4.5591)	LR 2.000e-03
0: TRAIN [0][1840/1938]	Time 0.213 (0.254)	Data 1.35e-04 (5.62e-04)	Tok/s 48858 (54930)	Loss/tok 3.5741 (4.5537)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.412 (0.254)	Data 1.15e-04 (5.60e-04)	Tok/s 71589 (54939)	Loss/tok 3.9075 (4.5483)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.275 (0.254)	Data 1.27e-04 (5.58e-04)	Tok/s 61253 (54953)	Loss/tok 3.5798 (4.5424)	LR 2.000e-03
0: TRAIN [0][1870/1938]	Time 0.277 (0.254)	Data 1.57e-04 (5.56e-04)	Tok/s 60414 (54949)	Loss/tok 3.5386 (4.5370)	LR 2.000e-03
0: TRAIN [0][1880/1938]	Time 0.278 (0.254)	Data 1.17e-04 (5.53e-04)	Tok/s 60688 (54956)	Loss/tok 3.6238 (4.5315)	LR 2.000e-03
0: TRAIN [0][1890/1938]	Time 0.214 (0.254)	Data 1.90e-04 (5.51e-04)	Tok/s 47544 (54973)	Loss/tok 3.3437 (4.5259)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1900/1938]	Time 0.278 (0.254)	Data 1.30e-04 (5.49e-04)	Tok/s 60752 (54981)	Loss/tok 3.5287 (4.5205)	LR 2.000e-03
0: TRAIN [0][1910/1938]	Time 0.337 (0.254)	Data 1.19e-04 (5.47e-04)	Tok/s 68917 (54981)	Loss/tok 3.6567 (4.5154)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.412 (0.254)	Data 1.19e-04 (5.45e-04)	Tok/s 72425 (55004)	Loss/tok 3.7928 (4.5096)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.274 (0.254)	Data 1.25e-04 (5.43e-04)	Tok/s 60862 (55014)	Loss/tok 3.5080 (4.5043)	LR 2.000e-03
:::MLL 1570035524.032 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1570035524.033 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.598 (0.598)	Decoder iters 93.0 (93.0)	Tok/s 26806 (26806)
0: Running moses detokenizer
0: BLEU(score=20.44331520161273, counts=[34662, 16013, 8570, 4783], totals=[64641, 61638, 58635, 55635], precisions=[53.62231401123126, 25.97910379960414, 14.615843779312698, 8.597106138222342], bp=0.999458694539076, sys_len=64641, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1570035525.845 eval_accuracy: {"value": 20.44, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1570035525.846 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 4.4988	Test BLEU: 20.44
0: Performance: Epoch: 0	Training: 440210 Tok/s
0: Finished epoch 0
:::MLL 1570035525.846 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1570035525.846 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1570035525.847 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 3572986038
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][0/1938]	Time 0.974 (0.974)	Data 6.85e-01 (6.85e-01)	Tok/s 17008 (17008)	Loss/tok 3.4294 (3.4294)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.213 (0.289)	Data 1.11e-04 (6.24e-02)	Tok/s 48095 (45998)	Loss/tok 3.3443 (3.3324)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.407 (0.275)	Data 1.04e-04 (3.28e-02)	Tok/s 72835 (49885)	Loss/tok 3.8923 (3.4465)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.271 (0.276)	Data 1.87e-04 (2.22e-02)	Tok/s 61094 (52651)	Loss/tok 3.4833 (3.4760)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.213 (0.262)	Data 1.04e-04 (1.68e-02)	Tok/s 48905 (51857)	Loss/tok 3.2859 (3.4358)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.213 (0.261)	Data 1.02e-04 (1.36e-02)	Tok/s 48217 (52792)	Loss/tok 3.1116 (3.4332)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.213 (0.261)	Data 9.73e-05 (1.14e-02)	Tok/s 48493 (53308)	Loss/tok 3.2195 (3.4341)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.213 (0.255)	Data 1.02e-04 (9.78e-03)	Tok/s 47984 (52760)	Loss/tok 3.2280 (3.4157)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.277 (0.255)	Data 9.68e-05 (8.58e-03)	Tok/s 61158 (53127)	Loss/tok 3.3952 (3.4230)	LR 2.000e-03
0: TRAIN [1][90/1938]	Time 0.276 (0.255)	Data 1.00e-04 (7.65e-03)	Tok/s 61491 (53247)	Loss/tok 3.4482 (3.4264)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.276 (0.253)	Data 1.02e-04 (6.91e-03)	Tok/s 61001 (53053)	Loss/tok 3.4006 (3.4217)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.157 (0.249)	Data 1.03e-04 (6.29e-03)	Tok/s 33426 (52455)	Loss/tok 2.6975 (3.4123)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.213 (0.247)	Data 1.03e-04 (5.78e-03)	Tok/s 49325 (52288)	Loss/tok 3.2201 (3.4057)	LR 2.000e-03
0: TRAIN [1][130/1938]	Time 0.274 (0.249)	Data 9.87e-05 (5.35e-03)	Tok/s 61535 (52804)	Loss/tok 3.4669 (3.4164)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.274 (0.252)	Data 1.02e-04 (4.98e-03)	Tok/s 62280 (53467)	Loss/tok 3.4836 (3.4290)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.276 (0.251)	Data 1.03e-04 (4.66e-03)	Tok/s 61134 (53391)	Loss/tok 3.3673 (3.4223)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.338 (0.252)	Data 9.68e-05 (4.37e-03)	Tok/s 68780 (53609)	Loss/tok 3.6493 (3.4258)	LR 2.000e-03
0: TRAIN [1][170/1938]	Time 0.338 (0.253)	Data 9.89e-05 (4.13e-03)	Tok/s 67703 (54019)	Loss/tok 3.7555 (3.4333)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.338 (0.254)	Data 1.05e-04 (3.91e-03)	Tok/s 69020 (54246)	Loss/tok 3.6458 (3.4390)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.337 (0.253)	Data 1.02e-04 (3.71e-03)	Tok/s 68953 (53997)	Loss/tok 3.7195 (3.4352)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.275 (0.254)	Data 9.54e-05 (3.53e-03)	Tok/s 61044 (54274)	Loss/tok 3.5503 (3.4376)	LR 2.000e-03
0: TRAIN [1][210/1938]	Time 0.213 (0.254)	Data 1.11e-04 (3.37e-03)	Tok/s 48068 (54314)	Loss/tok 3.1995 (3.4376)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.214 (0.255)	Data 9.47e-05 (3.22e-03)	Tok/s 48325 (54395)	Loss/tok 3.2068 (3.4403)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.277 (0.253)	Data 9.68e-05 (3.08e-03)	Tok/s 61519 (54233)	Loss/tok 3.3898 (3.4355)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.214 (0.253)	Data 1.02e-04 (2.96e-03)	Tok/s 48207 (54131)	Loss/tok 3.2189 (3.4331)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.213 (0.254)	Data 9.66e-05 (2.85e-03)	Tok/s 49371 (54411)	Loss/tok 3.2309 (3.4421)	LR 2.000e-03
0: TRAIN [1][260/1938]	Time 0.213 (0.254)	Data 9.54e-05 (2.74e-03)	Tok/s 50079 (54374)	Loss/tok 3.2208 (3.4403)	LR 2.000e-03
0: TRAIN [1][270/1938]	Time 0.276 (0.255)	Data 1.16e-04 (2.65e-03)	Tok/s 60711 (54513)	Loss/tok 3.3571 (3.4439)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.213 (0.255)	Data 1.08e-04 (2.56e-03)	Tok/s 47840 (54458)	Loss/tok 3.2581 (3.4425)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.276 (0.255)	Data 1.09e-04 (2.47e-03)	Tok/s 61163 (54576)	Loss/tok 3.3563 (3.4423)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.213 (0.254)	Data 9.94e-05 (2.39e-03)	Tok/s 48817 (54392)	Loss/tok 3.1775 (3.4373)	LR 2.000e-03
0: TRAIN [1][310/1938]	Time 0.275 (0.255)	Data 9.70e-05 (2.32e-03)	Tok/s 61315 (54552)	Loss/tok 3.4887 (3.4407)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.336 (0.254)	Data 1.23e-04 (2.25e-03)	Tok/s 69007 (54450)	Loss/tok 3.6867 (3.4380)	LR 2.000e-03
0: TRAIN [1][330/1938]	Time 0.276 (0.255)	Data 9.78e-05 (2.19e-03)	Tok/s 60566 (54670)	Loss/tok 3.4981 (3.4427)	LR 2.000e-03
0: TRAIN [1][340/1938]	Time 0.214 (0.256)	Data 1.26e-04 (2.13e-03)	Tok/s 49056 (54697)	Loss/tok 3.1926 (3.4438)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.213 (0.256)	Data 1.00e-04 (2.07e-03)	Tok/s 48581 (54747)	Loss/tok 3.1768 (3.4436)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.157 (0.255)	Data 1.03e-04 (2.01e-03)	Tok/s 34104 (54627)	Loss/tok 2.7266 (3.4418)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.275 (0.255)	Data 1.05e-04 (1.96e-03)	Tok/s 61460 (54618)	Loss/tok 3.5249 (3.4405)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][380/1938]	Time 0.277 (0.256)	Data 1.04e-04 (1.91e-03)	Tok/s 61687 (54753)	Loss/tok 3.3782 (3.4478)	LR 2.000e-03
0: TRAIN [1][390/1938]	Time 0.337 (0.257)	Data 1.11e-04 (1.87e-03)	Tok/s 69086 (54834)	Loss/tok 3.6037 (3.4507)	LR 2.000e-03
0: TRAIN [1][400/1938]	Time 0.410 (0.257)	Data 2.13e-04 (1.82e-03)	Tok/s 72544 (54899)	Loss/tok 3.8371 (3.4530)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.213 (0.257)	Data 1.04e-04 (1.78e-03)	Tok/s 47918 (54880)	Loss/tok 3.3339 (3.4506)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.274 (0.256)	Data 1.04e-04 (1.74e-03)	Tok/s 62204 (54807)	Loss/tok 3.4659 (3.4486)	LR 2.000e-03
0: TRAIN [1][430/1938]	Time 0.213 (0.256)	Data 1.00e-04 (1.71e-03)	Tok/s 48342 (54767)	Loss/tok 3.2731 (3.4471)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.213 (0.255)	Data 1.03e-04 (1.67e-03)	Tok/s 48079 (54642)	Loss/tok 3.1627 (3.4439)	LR 2.000e-03
0: TRAIN [1][450/1938]	Time 0.213 (0.255)	Data 9.75e-05 (1.64e-03)	Tok/s 47171 (54627)	Loss/tok 3.3089 (3.4416)	LR 2.000e-03
0: TRAIN [1][460/1938]	Time 0.214 (0.255)	Data 1.08e-04 (1.60e-03)	Tok/s 47909 (54656)	Loss/tok 3.2722 (3.4425)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.277 (0.256)	Data 1.03e-04 (1.57e-03)	Tok/s 59960 (54722)	Loss/tok 3.4362 (3.4453)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.337 (0.255)	Data 3.01e-04 (1.54e-03)	Tok/s 69080 (54679)	Loss/tok 3.6367 (3.4434)	LR 2.000e-03
0: TRAIN [1][490/1938]	Time 0.214 (0.256)	Data 1.02e-04 (1.51e-03)	Tok/s 48250 (54721)	Loss/tok 3.2232 (3.4455)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.214 (0.255)	Data 1.03e-04 (1.48e-03)	Tok/s 48013 (54626)	Loss/tok 3.2634 (3.4440)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.213 (0.255)	Data 9.99e-05 (1.46e-03)	Tok/s 49147 (54606)	Loss/tok 3.2287 (3.4427)	LR 2.000e-03
0: TRAIN [1][520/1938]	Time 0.214 (0.255)	Data 1.01e-04 (1.43e-03)	Tok/s 48889 (54584)	Loss/tok 3.2564 (3.4421)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.273 (0.255)	Data 9.73e-05 (1.41e-03)	Tok/s 61014 (54581)	Loss/tok 3.4973 (3.4415)	LR 2.000e-03
0: TRAIN [1][540/1938]	Time 0.275 (0.255)	Data 1.01e-04 (1.38e-03)	Tok/s 60990 (54655)	Loss/tok 3.4423 (3.4419)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.276 (0.255)	Data 1.02e-04 (1.36e-03)	Tok/s 60595 (54676)	Loss/tok 3.4298 (3.4405)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.337 (0.256)	Data 1.06e-04 (1.34e-03)	Tok/s 68837 (54783)	Loss/tok 3.6020 (3.4418)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.338 (0.255)	Data 1.01e-04 (1.31e-03)	Tok/s 68683 (54753)	Loss/tok 3.6563 (3.4416)	LR 2.000e-03
0: TRAIN [1][580/1938]	Time 0.412 (0.255)	Data 1.00e-04 (1.29e-03)	Tok/s 72256 (54729)	Loss/tok 3.7153 (3.4427)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.213 (0.255)	Data 9.47e-05 (1.27e-03)	Tok/s 48159 (54661)	Loss/tok 3.1223 (3.4400)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.276 (0.255)	Data 9.92e-05 (1.25e-03)	Tok/s 60504 (54672)	Loss/tok 3.5567 (3.4399)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.213 (0.255)	Data 1.04e-04 (1.24e-03)	Tok/s 48819 (54616)	Loss/tok 3.2895 (3.4383)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][620/1938]	Time 0.276 (0.255)	Data 1.03e-04 (1.22e-03)	Tok/s 60433 (54657)	Loss/tok 3.4808 (3.4390)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.214 (0.255)	Data 1.23e-04 (1.20e-03)	Tok/s 48931 (54685)	Loss/tok 3.2282 (3.4389)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.213 (0.254)	Data 1.35e-04 (1.18e-03)	Tok/s 48859 (54637)	Loss/tok 3.1418 (3.4377)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.339 (0.255)	Data 1.00e-04 (1.17e-03)	Tok/s 68625 (54684)	Loss/tok 3.5266 (3.4372)	LR 2.000e-03
0: TRAIN [1][660/1938]	Time 0.413 (0.255)	Data 1.04e-04 (1.15e-03)	Tok/s 72218 (54679)	Loss/tok 3.7567 (3.4367)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.214 (0.254)	Data 1.05e-04 (1.14e-03)	Tok/s 47909 (54651)	Loss/tok 3.0571 (3.4359)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.213 (0.254)	Data 1.00e-04 (1.12e-03)	Tok/s 48461 (54641)	Loss/tok 3.2209 (3.4351)	LR 2.000e-03
0: TRAIN [1][690/1938]	Time 0.276 (0.254)	Data 1.08e-04 (1.11e-03)	Tok/s 61241 (54626)	Loss/tok 3.3853 (3.4344)	LR 2.000e-03
0: TRAIN [1][700/1938]	Time 0.276 (0.255)	Data 1.06e-04 (1.09e-03)	Tok/s 60374 (54692)	Loss/tok 3.4324 (3.4351)	LR 2.000e-03
0: TRAIN [1][710/1938]	Time 0.274 (0.255)	Data 1.07e-04 (1.08e-03)	Tok/s 61616 (54725)	Loss/tok 3.2694 (3.4359)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.336 (0.255)	Data 1.16e-04 (1.06e-03)	Tok/s 70001 (54820)	Loss/tok 3.5123 (3.4381)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.213 (0.255)	Data 1.03e-04 (1.05e-03)	Tok/s 48789 (54762)	Loss/tok 3.1584 (3.4362)	LR 2.000e-03
0: TRAIN [1][740/1938]	Time 0.213 (0.256)	Data 1.05e-04 (1.04e-03)	Tok/s 47940 (54874)	Loss/tok 3.2177 (3.4376)	LR 2.000e-03
0: TRAIN [1][750/1938]	Time 0.274 (0.256)	Data 1.00e-04 (1.03e-03)	Tok/s 61550 (54925)	Loss/tok 3.4208 (3.4384)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.213 (0.256)	Data 1.02e-04 (1.01e-03)	Tok/s 49023 (54852)	Loss/tok 3.1911 (3.4376)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.213 (0.255)	Data 1.01e-04 (1.00e-03)	Tok/s 48033 (54788)	Loss/tok 3.1185 (3.4358)	LR 2.000e-03
0: TRAIN [1][780/1938]	Time 0.339 (0.256)	Data 1.28e-04 (9.90e-04)	Tok/s 69002 (54903)	Loss/tok 3.4726 (3.4375)	LR 2.000e-03
0: TRAIN [1][790/1938]	Time 0.278 (0.256)	Data 1.06e-04 (9.79e-04)	Tok/s 60893 (54967)	Loss/tok 3.3428 (3.4387)	LR 2.000e-03
0: TRAIN [1][800/1938]	Time 0.213 (0.256)	Data 1.00e-04 (9.68e-04)	Tok/s 47983 (54877)	Loss/tok 3.1953 (3.4374)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.157 (0.256)	Data 1.02e-04 (9.58e-04)	Tok/s 33288 (54838)	Loss/tok 2.6149 (3.4367)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.214 (0.256)	Data 1.11e-04 (9.47e-04)	Tok/s 49082 (54821)	Loss/tok 3.1680 (3.4358)	LR 2.000e-03
0: TRAIN [1][830/1938]	Time 0.214 (0.255)	Data 9.42e-05 (9.37e-04)	Tok/s 49190 (54786)	Loss/tok 3.1619 (3.4342)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.214 (0.255)	Data 9.94e-05 (9.27e-04)	Tok/s 47724 (54704)	Loss/tok 3.0893 (3.4320)	LR 2.000e-03
0: TRAIN [1][850/1938]	Time 0.278 (0.255)	Data 1.00e-04 (9.17e-04)	Tok/s 60028 (54728)	Loss/tok 3.4101 (3.4319)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.157 (0.255)	Data 1.17e-04 (9.08e-04)	Tok/s 33701 (54701)	Loss/tok 2.6968 (3.4307)	LR 2.000e-03
0: TRAIN [1][870/1938]	Time 0.275 (0.255)	Data 1.03e-04 (8.99e-04)	Tok/s 61259 (54707)	Loss/tok 3.3780 (3.4301)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.158 (0.255)	Data 1.03e-04 (8.90e-04)	Tok/s 32515 (54713)	Loss/tok 2.7159 (3.4298)	LR 2.000e-03
0: TRAIN [1][890/1938]	Time 0.338 (0.255)	Data 1.07e-04 (8.81e-04)	Tok/s 68105 (54697)	Loss/tok 3.6326 (3.4288)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][900/1938]	Time 0.275 (0.255)	Data 1.07e-04 (8.73e-04)	Tok/s 60841 (54705)	Loss/tok 3.4252 (3.4295)	LR 2.000e-03
0: TRAIN [1][910/1938]	Time 0.157 (0.255)	Data 9.92e-05 (8.64e-04)	Tok/s 33608 (54662)	Loss/tok 2.6803 (3.4280)	LR 2.000e-03
0: TRAIN [1][920/1938]	Time 0.212 (0.254)	Data 9.92e-05 (8.56e-04)	Tok/s 49072 (54644)	Loss/tok 3.2204 (3.4270)	LR 2.000e-03
0: TRAIN [1][930/1938]	Time 0.338 (0.254)	Data 1.05e-04 (8.49e-04)	Tok/s 68788 (54650)	Loss/tok 3.6369 (3.4265)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.337 (0.255)	Data 1.04e-04 (8.41e-04)	Tok/s 69203 (54664)	Loss/tok 3.6263 (3.4269)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.275 (0.254)	Data 1.04e-04 (8.33e-04)	Tok/s 61053 (54673)	Loss/tok 3.3721 (3.4261)	LR 2.000e-03
0: TRAIN [1][960/1938]	Time 0.275 (0.254)	Data 9.97e-05 (8.26e-04)	Tok/s 60347 (54630)	Loss/tok 3.4230 (3.4246)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.213 (0.254)	Data 1.07e-04 (8.18e-04)	Tok/s 48154 (54617)	Loss/tok 3.2348 (3.4235)	LR 2.000e-03
0: TRAIN [1][980/1938]	Time 0.338 (0.254)	Data 1.07e-04 (8.11e-04)	Tok/s 69976 (54611)	Loss/tok 3.5358 (3.4225)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.214 (0.254)	Data 1.01e-04 (8.05e-04)	Tok/s 47577 (54629)	Loss/tok 3.0373 (3.4216)	LR 2.000e-03
0: TRAIN [1][1000/1938]	Time 0.215 (0.254)	Data 1.03e-04 (7.98e-04)	Tok/s 48287 (54571)	Loss/tok 3.2485 (3.4204)	LR 2.000e-03
0: TRAIN [1][1010/1938]	Time 0.156 (0.253)	Data 1.32e-04 (7.91e-04)	Tok/s 32931 (54529)	Loss/tok 2.6632 (3.4194)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1020/1938]	Time 0.277 (0.253)	Data 1.11e-04 (7.85e-04)	Tok/s 60256 (54527)	Loss/tok 3.3976 (3.4191)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.337 (0.254)	Data 1.06e-04 (7.78e-04)	Tok/s 68994 (54563)	Loss/tok 3.6206 (3.4192)	LR 2.000e-03
0: TRAIN [1][1040/1938]	Time 0.275 (0.254)	Data 2.48e-04 (7.72e-04)	Tok/s 60490 (54601)	Loss/tok 3.4507 (3.4195)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.214 (0.254)	Data 1.43e-04 (7.66e-04)	Tok/s 48672 (54652)	Loss/tok 3.1618 (3.4197)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.273 (0.254)	Data 1.02e-04 (7.60e-04)	Tok/s 61741 (54690)	Loss/tok 3.4482 (3.4199)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.213 (0.254)	Data 9.80e-05 (7.53e-04)	Tok/s 48484 (54707)	Loss/tok 3.1501 (3.4195)	LR 2.000e-03
0: TRAIN [1][1080/1938]	Time 0.213 (0.254)	Data 1.03e-04 (7.48e-04)	Tok/s 49113 (54714)	Loss/tok 3.1763 (3.4185)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.213 (0.254)	Data 1.03e-04 (7.42e-04)	Tok/s 48009 (54698)	Loss/tok 3.0274 (3.4174)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.212 (0.254)	Data 1.04e-04 (7.36e-04)	Tok/s 49752 (54726)	Loss/tok 3.1487 (3.4173)	LR 2.000e-03
0: TRAIN [1][1110/1938]	Time 0.338 (0.254)	Data 1.76e-04 (7.30e-04)	Tok/s 69098 (54782)	Loss/tok 3.6037 (3.4177)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.337 (0.254)	Data 9.99e-05 (7.25e-04)	Tok/s 70164 (54776)	Loss/tok 3.5960 (3.4175)	LR 2.000e-03
0: TRAIN [1][1130/1938]	Time 0.213 (0.254)	Data 1.02e-04 (7.20e-04)	Tok/s 47522 (54728)	Loss/tok 3.1230 (3.4162)	LR 2.000e-03
0: TRAIN [1][1140/1938]	Time 0.277 (0.254)	Data 1.08e-04 (7.14e-04)	Tok/s 60789 (54727)	Loss/tok 3.3920 (3.4164)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.213 (0.254)	Data 1.04e-04 (7.09e-04)	Tok/s 48747 (54694)	Loss/tok 3.1198 (3.4151)	LR 2.000e-03
0: TRAIN [1][1160/1938]	Time 0.213 (0.254)	Data 1.08e-04 (7.04e-04)	Tok/s 48958 (54728)	Loss/tok 3.1791 (3.4149)	LR 2.000e-03
0: TRAIN [1][1170/1938]	Time 0.337 (0.254)	Data 1.03e-04 (6.99e-04)	Tok/s 69133 (54707)	Loss/tok 3.5352 (3.4143)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.275 (0.254)	Data 1.05e-04 (6.94e-04)	Tok/s 61581 (54744)	Loss/tok 3.3231 (3.4145)	LR 2.000e-03
0: TRAIN [1][1190/1938]	Time 0.276 (0.254)	Data 1.06e-04 (6.89e-04)	Tok/s 60484 (54758)	Loss/tok 3.3338 (3.4143)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.157 (0.254)	Data 9.97e-05 (6.85e-04)	Tok/s 33821 (54722)	Loss/tok 2.6491 (3.4138)	LR 2.000e-03
0: TRAIN [1][1210/1938]	Time 0.413 (0.254)	Data 9.99e-05 (6.80e-04)	Tok/s 72503 (54730)	Loss/tok 3.7786 (3.4141)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.215 (0.254)	Data 1.04e-04 (6.76e-04)	Tok/s 48313 (54774)	Loss/tok 3.2155 (3.4149)	LR 2.000e-03
0: TRAIN [1][1230/1938]	Time 0.274 (0.254)	Data 9.87e-05 (6.71e-04)	Tok/s 61707 (54761)	Loss/tok 3.3480 (3.4140)	LR 2.000e-03
0: TRAIN [1][1240/1938]	Time 0.213 (0.254)	Data 1.05e-04 (6.66e-04)	Tok/s 48911 (54761)	Loss/tok 3.2695 (3.4136)	LR 2.000e-03
0: TRAIN [1][1250/1938]	Time 0.213 (0.254)	Data 9.94e-05 (6.62e-04)	Tok/s 48176 (54800)	Loss/tok 3.1738 (3.4140)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.214 (0.254)	Data 1.07e-04 (6.57e-04)	Tok/s 48460 (54785)	Loss/tok 3.1652 (3.4134)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.214 (0.254)	Data 9.89e-05 (6.53e-04)	Tok/s 47622 (54774)	Loss/tok 3.1383 (3.4137)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.214 (0.254)	Data 1.31e-04 (6.49e-04)	Tok/s 47983 (54751)	Loss/tok 3.2315 (3.4128)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.158 (0.254)	Data 9.63e-05 (6.45e-04)	Tok/s 33358 (54752)	Loss/tok 2.6785 (3.4129)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1300/1938]	Time 0.337 (0.254)	Data 9.75e-05 (6.41e-04)	Tok/s 69859 (54782)	Loss/tok 3.5198 (3.4128)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.214 (0.254)	Data 1.27e-04 (6.37e-04)	Tok/s 48800 (54766)	Loss/tok 3.2547 (3.4124)	LR 2.000e-03
0: TRAIN [1][1320/1938]	Time 0.337 (0.254)	Data 1.02e-04 (6.33e-04)	Tok/s 68742 (54766)	Loss/tok 3.5195 (3.4119)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.413 (0.255)	Data 1.43e-04 (6.29e-04)	Tok/s 72615 (54804)	Loss/tok 3.6685 (3.4129)	LR 2.000e-03
0: TRAIN [1][1340/1938]	Time 0.275 (0.255)	Data 9.89e-05 (6.25e-04)	Tok/s 60489 (54837)	Loss/tok 3.3866 (3.4129)	LR 2.000e-03
0: TRAIN [1][1350/1938]	Time 0.338 (0.255)	Data 1.00e-04 (6.21e-04)	Tok/s 70224 (54851)	Loss/tok 3.5628 (3.4126)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.277 (0.255)	Data 1.03e-04 (6.17e-04)	Tok/s 61256 (54845)	Loss/tok 3.3529 (3.4125)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.158 (0.255)	Data 1.02e-04 (6.13e-04)	Tok/s 34082 (54811)	Loss/tok 2.7876 (3.4114)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.213 (0.254)	Data 9.87e-05 (6.10e-04)	Tok/s 48472 (54775)	Loss/tok 3.1149 (3.4110)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.213 (0.254)	Data 1.06e-04 (6.06e-04)	Tok/s 49355 (54743)	Loss/tok 3.0985 (3.4101)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.338 (0.254)	Data 9.89e-05 (6.03e-04)	Tok/s 69289 (54781)	Loss/tok 3.5441 (3.4102)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.274 (0.255)	Data 9.66e-05 (5.99e-04)	Tok/s 61352 (54788)	Loss/tok 3.3152 (3.4099)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.213 (0.254)	Data 1.28e-04 (5.96e-04)	Tok/s 48426 (54788)	Loss/tok 3.1836 (3.4094)	LR 2.000e-03
0: TRAIN [1][1430/1938]	Time 0.275 (0.254)	Data 1.01e-04 (5.92e-04)	Tok/s 62085 (54786)	Loss/tok 3.2957 (3.4093)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.338 (0.255)	Data 1.03e-04 (5.89e-04)	Tok/s 68738 (54797)	Loss/tok 3.4721 (3.4089)	LR 2.000e-03
0: TRAIN [1][1450/1938]	Time 0.277 (0.254)	Data 9.99e-05 (5.86e-04)	Tok/s 61270 (54750)	Loss/tok 3.3566 (3.4075)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.214 (0.255)	Data 1.03e-04 (5.83e-04)	Tok/s 47804 (54800)	Loss/tok 3.0915 (3.4079)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.214 (0.254)	Data 9.85e-05 (5.80e-04)	Tok/s 48506 (54779)	Loss/tok 3.2066 (3.4067)	LR 2.000e-03
0: TRAIN [1][1480/1938]	Time 0.276 (0.255)	Data 1.01e-04 (5.76e-04)	Tok/s 62047 (54810)	Loss/tok 3.2817 (3.4067)	LR 2.000e-03
0: TRAIN [1][1490/1938]	Time 0.277 (0.255)	Data 9.61e-05 (5.73e-04)	Tok/s 60354 (54823)	Loss/tok 3.3722 (3.4059)	LR 2.000e-03
0: TRAIN [1][1500/1938]	Time 0.276 (0.255)	Data 9.89e-05 (5.70e-04)	Tok/s 61195 (54832)	Loss/tok 3.4430 (3.4058)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.338 (0.255)	Data 9.99e-05 (5.67e-04)	Tok/s 68783 (54839)	Loss/tok 3.4972 (3.4058)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.213 (0.254)	Data 9.97e-05 (5.64e-04)	Tok/s 48760 (54792)	Loss/tok 3.0816 (3.4047)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.276 (0.254)	Data 1.03e-04 (5.61e-04)	Tok/s 59891 (54780)	Loss/tok 3.3206 (3.4043)	LR 2.000e-03
0: TRAIN [1][1540/1938]	Time 0.213 (0.254)	Data 1.04e-04 (5.58e-04)	Tok/s 48863 (54802)	Loss/tok 3.1759 (3.4040)	LR 2.000e-03
0: TRAIN [1][1550/1938]	Time 0.338 (0.254)	Data 9.51e-05 (5.55e-04)	Tok/s 68986 (54811)	Loss/tok 3.5249 (3.4038)	LR 2.000e-03
0: TRAIN [1][1560/1938]	Time 0.277 (0.254)	Data 1.04e-04 (5.53e-04)	Tok/s 62097 (54826)	Loss/tok 3.3072 (3.4031)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.277 (0.254)	Data 9.89e-05 (5.50e-04)	Tok/s 61255 (54837)	Loss/tok 3.2769 (3.4026)	LR 2.000e-03
0: TRAIN [1][1580/1938]	Time 0.214 (0.255)	Data 1.02e-04 (5.47e-04)	Tok/s 47625 (54856)	Loss/tok 3.0882 (3.4025)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1590/1938]	Time 0.278 (0.255)	Data 9.80e-05 (5.45e-04)	Tok/s 61034 (54899)	Loss/tok 3.4519 (3.4026)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.339 (0.255)	Data 1.02e-04 (5.42e-04)	Tok/s 69034 (54878)	Loss/tok 3.4890 (3.4021)	LR 2.000e-03
0: TRAIN [1][1610/1938]	Time 0.215 (0.255)	Data 1.04e-04 (5.39e-04)	Tok/s 47829 (54908)	Loss/tok 3.0616 (3.4024)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.214 (0.255)	Data 1.07e-04 (5.37e-04)	Tok/s 48454 (54917)	Loss/tok 3.2085 (3.4025)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.157 (0.255)	Data 1.03e-04 (5.34e-04)	Tok/s 33118 (54898)	Loss/tok 2.6922 (3.4020)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.213 (0.255)	Data 1.55e-04 (5.31e-04)	Tok/s 48363 (54893)	Loss/tok 3.1507 (3.4014)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.213 (0.255)	Data 1.07e-04 (5.29e-04)	Tok/s 48733 (54863)	Loss/tok 3.0922 (3.4006)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.276 (0.254)	Data 1.23e-04 (5.26e-04)	Tok/s 60395 (54846)	Loss/tok 3.4729 (3.3999)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.276 (0.255)	Data 1.18e-04 (5.24e-04)	Tok/s 60346 (54882)	Loss/tok 3.3033 (3.4007)	LR 2.000e-03
0: TRAIN [1][1680/1938]	Time 0.213 (0.255)	Data 1.40e-04 (5.22e-04)	Tok/s 48005 (54870)	Loss/tok 3.1923 (3.4001)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1690/1938]	Time 0.338 (0.255)	Data 1.05e-04 (5.19e-04)	Tok/s 68301 (54917)	Loss/tok 3.5929 (3.4008)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.338 (0.255)	Data 1.32e-04 (5.17e-04)	Tok/s 68267 (54913)	Loss/tok 3.6740 (3.4006)	LR 2.000e-03
0: TRAIN [1][1710/1938]	Time 0.156 (0.255)	Data 1.12e-04 (5.15e-04)	Tok/s 34035 (54911)	Loss/tok 2.8180 (3.4008)	LR 2.000e-03
0: TRAIN [1][1720/1938]	Time 0.276 (0.255)	Data 9.87e-05 (5.12e-04)	Tok/s 61515 (54902)	Loss/tok 3.2080 (3.4002)	LR 2.000e-03
0: TRAIN [1][1730/1938]	Time 0.212 (0.255)	Data 1.11e-04 (5.10e-04)	Tok/s 47901 (54892)	Loss/tok 3.1606 (3.3999)	LR 2.000e-03
0: TRAIN [1][1740/1938]	Time 0.213 (0.255)	Data 1.03e-04 (5.08e-04)	Tok/s 48727 (54871)	Loss/tok 3.2308 (3.3996)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.156 (0.255)	Data 9.30e-05 (5.05e-04)	Tok/s 33611 (54849)	Loss/tok 2.5953 (3.3991)	LR 2.000e-03
0: TRAIN [1][1760/1938]	Time 0.276 (0.254)	Data 9.58e-05 (5.03e-04)	Tok/s 60189 (54811)	Loss/tok 3.4156 (3.3981)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.275 (0.254)	Data 1.15e-04 (5.01e-04)	Tok/s 62045 (54813)	Loss/tok 3.3994 (3.3981)	LR 2.000e-03
0: TRAIN [1][1780/1938]	Time 0.274 (0.254)	Data 9.94e-05 (4.99e-04)	Tok/s 60886 (54821)	Loss/tok 3.3650 (3.3977)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.276 (0.255)	Data 9.63e-05 (4.96e-04)	Tok/s 60466 (54831)	Loss/tok 3.3058 (3.3976)	LR 2.000e-03
0: TRAIN [1][1800/1938]	Time 0.214 (0.255)	Data 1.08e-04 (4.94e-04)	Tok/s 48725 (54848)	Loss/tok 3.1316 (3.3971)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.276 (0.255)	Data 1.03e-04 (4.92e-04)	Tok/s 61880 (54865)	Loss/tok 3.3307 (3.3969)	LR 2.000e-03
0: TRAIN [1][1820/1938]	Time 0.337 (0.255)	Data 9.99e-05 (4.90e-04)	Tok/s 68594 (54892)	Loss/tok 3.5330 (3.3967)	LR 2.000e-03
0: TRAIN [1][1830/1938]	Time 0.213 (0.255)	Data 1.00e-04 (4.88e-04)	Tok/s 48349 (54914)	Loss/tok 3.1478 (3.3965)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.213 (0.255)	Data 1.03e-04 (4.86e-04)	Tok/s 47554 (54935)	Loss/tok 3.0956 (3.3966)	LR 2.000e-03
0: TRAIN [1][1850/1938]	Time 0.213 (0.255)	Data 1.09e-04 (4.84e-04)	Tok/s 48969 (54919)	Loss/tok 3.2164 (3.3965)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.213 (0.255)	Data 1.12e-04 (4.82e-04)	Tok/s 47654 (54912)	Loss/tok 3.0767 (3.3962)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.213 (0.255)	Data 1.04e-04 (4.80e-04)	Tok/s 47902 (54900)	Loss/tok 3.2169 (3.3956)	LR 2.000e-03
0: TRAIN [1][1880/1938]	Time 0.277 (0.255)	Data 1.27e-04 (4.78e-04)	Tok/s 60447 (54921)	Loss/tok 3.2676 (3.3957)	LR 2.000e-03
0: TRAIN [1][1890/1938]	Time 0.276 (0.255)	Data 1.08e-04 (4.76e-04)	Tok/s 61217 (54922)	Loss/tok 3.2473 (3.3953)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.276 (0.255)	Data 1.02e-04 (4.74e-04)	Tok/s 60348 (54908)	Loss/tok 3.3898 (3.3947)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.276 (0.255)	Data 1.06e-04 (4.72e-04)	Tok/s 60587 (54918)	Loss/tok 3.2784 (3.3941)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.213 (0.255)	Data 1.05e-04 (4.70e-04)	Tok/s 48962 (54906)	Loss/tok 3.1613 (3.3938)	LR 2.000e-03
0: TRAIN [1][1930/1938]	Time 0.338 (0.255)	Data 1.01e-04 (4.69e-04)	Tok/s 68178 (54858)	Loss/tok 3.5319 (3.3930)	LR 2.000e-03
:::MLL 1570036020.283 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1570036020.284 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.742 (0.742)	Decoder iters 149.0 (149.0)	Tok/s 21970 (21970)
0: Running moses detokenizer
0: BLEU(score=21.70913711936639, counts=[35538, 16940, 9277, 5312], totals=[65051, 62048, 59045, 56045], precisions=[54.63098184501391, 27.3014440433213, 15.711745279024473, 9.478097956998841], bp=1.0, sys_len=65051, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1570036022.279 eval_accuracy: {"value": 21.71, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1570036022.279 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.3942	Test BLEU: 21.71
0: Performance: Epoch: 1	Training: 439101 Tok/s
0: Finished epoch 1
:::MLL 1570036022.280 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1570036022.280 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1570036022.280 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 2638095867
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][0/1938]	Time 1.059 (1.059)	Data 6.99e-01 (6.99e-01)	Tok/s 21889 (21889)	Loss/tok 3.3756 (3.3756)	LR 2.000e-03
0: TRAIN [2][10/1938]	Time 0.214 (0.320)	Data 1.42e-04 (6.37e-02)	Tok/s 47686 (50544)	Loss/tok 3.0475 (3.2694)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.157 (0.290)	Data 2.28e-04 (3.34e-02)	Tok/s 33876 (52819)	Loss/tok 2.5164 (3.2731)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.214 (0.266)	Data 1.12e-04 (2.27e-02)	Tok/s 49833 (51274)	Loss/tok 3.0272 (3.2213)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.214 (0.260)	Data 9.73e-05 (1.72e-02)	Tok/s 48657 (52106)	Loss/tok 3.1022 (3.2086)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.275 (0.265)	Data 1.08e-04 (1.38e-02)	Tok/s 61506 (53708)	Loss/tok 3.2672 (3.2337)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.413 (0.267)	Data 1.13e-04 (1.16e-02)	Tok/s 72098 (54403)	Loss/tok 3.6854 (3.2539)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.277 (0.267)	Data 1.16e-04 (9.97e-03)	Tok/s 60194 (54519)	Loss/tok 3.2140 (3.2660)	LR 2.000e-03
0: TRAIN [2][80/1938]	Time 0.410 (0.265)	Data 1.01e-04 (8.75e-03)	Tok/s 72434 (54397)	Loss/tok 3.5824 (3.2683)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.213 (0.264)	Data 9.54e-05 (7.81e-03)	Tok/s 48575 (54377)	Loss/tok 3.0352 (3.2691)	LR 2.000e-03
0: TRAIN [2][100/1938]	Time 0.273 (0.261)	Data 1.69e-04 (7.05e-03)	Tok/s 61424 (54194)	Loss/tok 3.1510 (3.2595)	LR 2.000e-03
0: TRAIN [2][110/1938]	Time 0.156 (0.259)	Data 1.05e-04 (6.42e-03)	Tok/s 34240 (53937)	Loss/tok 2.6338 (3.2519)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.274 (0.255)	Data 1.26e-04 (5.90e-03)	Tok/s 61053 (53593)	Loss/tok 3.2735 (3.2398)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.277 (0.253)	Data 9.73e-05 (5.46e-03)	Tok/s 60418 (53322)	Loss/tok 3.2787 (3.2310)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.273 (0.253)	Data 1.17e-04 (5.08e-03)	Tok/s 62016 (53355)	Loss/tok 3.1888 (3.2308)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.214 (0.252)	Data 1.05e-04 (4.75e-03)	Tok/s 49210 (53254)	Loss/tok 2.9710 (3.2238)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.213 (0.252)	Data 9.87e-05 (4.47e-03)	Tok/s 48355 (53270)	Loss/tok 3.0747 (3.2272)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.213 (0.252)	Data 1.00e-04 (4.21e-03)	Tok/s 48605 (53465)	Loss/tok 3.0324 (3.2288)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.338 (0.252)	Data 1.07e-04 (3.98e-03)	Tok/s 68836 (53323)	Loss/tok 3.3984 (3.2287)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.213 (0.252)	Data 9.80e-05 (3.78e-03)	Tok/s 48339 (53425)	Loss/tok 2.9560 (3.2308)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.214 (0.252)	Data 1.01e-04 (3.60e-03)	Tok/s 47700 (53426)	Loss/tok 3.1505 (3.2345)	LR 2.000e-03
0: TRAIN [2][210/1938]	Time 0.213 (0.252)	Data 1.06e-04 (3.44e-03)	Tok/s 48265 (53521)	Loss/tok 3.0015 (3.2369)	LR 2.000e-03
0: TRAIN [2][220/1938]	Time 0.156 (0.252)	Data 1.03e-04 (3.28e-03)	Tok/s 33178 (53527)	Loss/tok 2.5893 (3.2386)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.157 (0.251)	Data 9.87e-05 (3.15e-03)	Tok/s 33652 (53354)	Loss/tok 2.6474 (3.2349)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][240/1938]	Time 0.156 (0.251)	Data 9.27e-05 (3.02e-03)	Tok/s 34158 (53389)	Loss/tok 2.6641 (3.2333)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.213 (0.252)	Data 1.22e-04 (2.91e-03)	Tok/s 48910 (53527)	Loss/tok 3.0973 (3.2354)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.214 (0.253)	Data 1.18e-04 (2.80e-03)	Tok/s 47492 (53747)	Loss/tok 3.0099 (3.2424)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.213 (0.253)	Data 9.61e-05 (2.70e-03)	Tok/s 48275 (53783)	Loss/tok 3.0222 (3.2437)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.277 (0.252)	Data 9.61e-05 (2.61e-03)	Tok/s 60061 (53692)	Loss/tok 3.2368 (3.2424)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.214 (0.252)	Data 1.03e-04 (2.52e-03)	Tok/s 49104 (53794)	Loss/tok 3.0960 (3.2430)	LR 2.000e-03
0: TRAIN [2][300/1938]	Time 0.214 (0.252)	Data 9.63e-05 (2.44e-03)	Tok/s 47309 (53730)	Loss/tok 3.0230 (3.2403)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.214 (0.251)	Data 9.99e-05 (2.37e-03)	Tok/s 48249 (53709)	Loss/tok 3.0281 (3.2389)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.213 (0.253)	Data 1.04e-04 (2.29e-03)	Tok/s 48362 (53932)	Loss/tok 3.0445 (3.2465)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.213 (0.254)	Data 1.00e-04 (2.23e-03)	Tok/s 48816 (54027)	Loss/tok 3.0447 (3.2496)	LR 2.000e-03
0: TRAIN [2][340/1938]	Time 0.214 (0.253)	Data 1.35e-04 (2.17e-03)	Tok/s 47681 (54066)	Loss/tok 3.0558 (3.2501)	LR 2.000e-03
0: TRAIN [2][350/1938]	Time 0.159 (0.254)	Data 1.11e-04 (2.11e-03)	Tok/s 33057 (54127)	Loss/tok 2.6668 (3.2503)	LR 2.000e-03
0: TRAIN [2][360/1938]	Time 0.214 (0.253)	Data 9.87e-05 (2.05e-03)	Tok/s 47985 (53948)	Loss/tok 2.9972 (3.2488)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.214 (0.253)	Data 1.15e-04 (2.00e-03)	Tok/s 48357 (53995)	Loss/tok 2.9649 (3.2491)	LR 2.000e-03
0: TRAIN [2][380/1938]	Time 0.275 (0.252)	Data 9.87e-05 (1.95e-03)	Tok/s 61732 (53895)	Loss/tok 3.3110 (3.2476)	LR 2.000e-03
0: TRAIN [2][390/1938]	Time 0.277 (0.253)	Data 9.73e-05 (1.90e-03)	Tok/s 60372 (54015)	Loss/tok 3.1753 (3.2508)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.213 (0.253)	Data 9.78e-05 (1.86e-03)	Tok/s 47983 (53973)	Loss/tok 3.0295 (3.2497)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.274 (0.253)	Data 9.51e-05 (1.82e-03)	Tok/s 61632 (53960)	Loss/tok 3.3514 (3.2522)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.276 (0.253)	Data 1.39e-04 (1.78e-03)	Tok/s 60790 (53982)	Loss/tok 3.3288 (3.2503)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.277 (0.253)	Data 9.39e-05 (1.74e-03)	Tok/s 60292 (53991)	Loss/tok 3.3910 (3.2514)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.276 (0.253)	Data 9.39e-05 (1.70e-03)	Tok/s 61254 (54079)	Loss/tok 3.1691 (3.2523)	LR 2.000e-03
0: TRAIN [2][450/1938]	Time 0.337 (0.253)	Data 1.04e-04 (1.67e-03)	Tok/s 68971 (54153)	Loss/tok 3.5177 (3.2544)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][460/1938]	Time 0.274 (0.254)	Data 1.07e-04 (1.63e-03)	Tok/s 60971 (54233)	Loss/tok 3.2381 (3.2550)	LR 2.000e-03
0: TRAIN [2][470/1938]	Time 0.276 (0.254)	Data 9.80e-05 (1.60e-03)	Tok/s 60653 (54259)	Loss/tok 3.1699 (3.2540)	LR 2.000e-03
0: TRAIN [2][480/1938]	Time 0.214 (0.253)	Data 1.25e-04 (1.57e-03)	Tok/s 47985 (54231)	Loss/tok 3.0143 (3.2520)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.214 (0.253)	Data 1.18e-04 (1.54e-03)	Tok/s 47934 (54174)	Loss/tok 3.1213 (3.2503)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.277 (0.253)	Data 9.49e-05 (1.51e-03)	Tok/s 60355 (54235)	Loss/tok 3.2367 (3.2527)	LR 2.000e-03
0: TRAIN [2][510/1938]	Time 0.213 (0.253)	Data 1.03e-04 (1.48e-03)	Tok/s 48198 (54259)	Loss/tok 3.1094 (3.2519)	LR 2.000e-03
0: TRAIN [2][520/1938]	Time 0.277 (0.254)	Data 9.92e-05 (1.46e-03)	Tok/s 60302 (54372)	Loss/tok 3.1829 (3.2537)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.214 (0.254)	Data 1.03e-04 (1.43e-03)	Tok/s 48805 (54418)	Loss/tok 3.1206 (3.2542)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.275 (0.254)	Data 9.63e-05 (1.41e-03)	Tok/s 60816 (54464)	Loss/tok 3.2679 (3.2547)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.336 (0.254)	Data 9.66e-05 (1.39e-03)	Tok/s 69353 (54454)	Loss/tok 3.4456 (3.2547)	LR 2.000e-03
0: TRAIN [2][560/1938]	Time 0.337 (0.254)	Data 9.44e-05 (1.36e-03)	Tok/s 68447 (54470)	Loss/tok 3.4536 (3.2544)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.277 (0.254)	Data 1.01e-04 (1.34e-03)	Tok/s 61683 (54444)	Loss/tok 3.2572 (3.2535)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.213 (0.253)	Data 9.58e-05 (1.32e-03)	Tok/s 49014 (54402)	Loss/tok 3.1224 (3.2520)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.337 (0.253)	Data 1.06e-04 (1.30e-03)	Tok/s 69243 (54341)	Loss/tok 3.3912 (3.2516)	LR 2.000e-03
0: TRAIN [2][600/1938]	Time 0.213 (0.253)	Data 1.11e-04 (1.28e-03)	Tok/s 48408 (54345)	Loss/tok 3.0381 (3.2517)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][610/1938]	Time 0.277 (0.253)	Data 1.15e-04 (1.26e-03)	Tok/s 60567 (54294)	Loss/tok 3.2475 (3.2519)	LR 2.000e-03
0: TRAIN [2][620/1938]	Time 0.276 (0.253)	Data 1.01e-04 (1.24e-03)	Tok/s 61097 (54314)	Loss/tok 3.3079 (3.2509)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.213 (0.253)	Data 1.16e-04 (1.22e-03)	Tok/s 47038 (54280)	Loss/tok 3.0378 (3.2502)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.156 (0.253)	Data 1.22e-04 (1.21e-03)	Tok/s 33908 (54317)	Loss/tok 2.7553 (3.2513)	LR 2.000e-03
0: TRAIN [2][650/1938]	Time 0.338 (0.253)	Data 1.01e-04 (1.19e-03)	Tok/s 68547 (54271)	Loss/tok 3.3789 (3.2498)	LR 2.000e-03
0: TRAIN [2][660/1938]	Time 0.213 (0.253)	Data 9.87e-05 (1.17e-03)	Tok/s 48683 (54315)	Loss/tok 3.0431 (3.2523)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.276 (0.253)	Data 1.02e-04 (1.16e-03)	Tok/s 61531 (54382)	Loss/tok 3.2615 (3.2542)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.277 (0.253)	Data 1.22e-04 (1.14e-03)	Tok/s 60436 (54382)	Loss/tok 3.2981 (3.2544)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.213 (0.254)	Data 9.89e-05 (1.13e-03)	Tok/s 47563 (54414)	Loss/tok 3.0505 (3.2546)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.338 (0.254)	Data 1.03e-04 (1.11e-03)	Tok/s 68956 (54580)	Loss/tok 3.3585 (3.2573)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.278 (0.255)	Data 1.01e-04 (1.10e-03)	Tok/s 60781 (54613)	Loss/tok 3.2431 (3.2566)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.213 (0.254)	Data 1.06e-04 (1.09e-03)	Tok/s 48945 (54578)	Loss/tok 3.1228 (3.2561)	LR 2.000e-03
0: TRAIN [2][730/1938]	Time 0.157 (0.255)	Data 1.74e-04 (1.07e-03)	Tok/s 33851 (54624)	Loss/tok 2.5252 (3.2571)	LR 2.000e-03
0: TRAIN [2][740/1938]	Time 0.214 (0.255)	Data 1.04e-04 (1.06e-03)	Tok/s 48665 (54676)	Loss/tok 3.0526 (3.2582)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][750/1938]	Time 0.276 (0.255)	Data 1.90e-04 (1.05e-03)	Tok/s 61086 (54658)	Loss/tok 3.2522 (3.2577)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.213 (0.255)	Data 1.02e-04 (1.04e-03)	Tok/s 48653 (54654)	Loss/tok 3.0183 (3.2589)	LR 2.000e-03
0: TRAIN [2][770/1938]	Time 0.276 (0.255)	Data 9.73e-05 (1.02e-03)	Tok/s 61366 (54698)	Loss/tok 3.2616 (3.2588)	LR 2.000e-03
0: TRAIN [2][780/1938]	Time 0.273 (0.255)	Data 1.02e-04 (1.01e-03)	Tok/s 60956 (54653)	Loss/tok 3.3757 (3.2576)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.213 (0.254)	Data 9.37e-05 (1.00e-03)	Tok/s 47582 (54590)	Loss/tok 2.9590 (3.2572)	LR 2.000e-03
0: TRAIN [2][800/1938]	Time 0.158 (0.254)	Data 1.01e-04 (9.89e-04)	Tok/s 33286 (54499)	Loss/tok 2.6652 (3.2568)	LR 2.000e-03
0: TRAIN [2][810/1938]	Time 0.336 (0.254)	Data 9.89e-05 (9.79e-04)	Tok/s 69806 (54487)	Loss/tok 3.4803 (3.2565)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.213 (0.254)	Data 3.36e-04 (9.69e-04)	Tok/s 48257 (54491)	Loss/tok 3.0644 (3.2562)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.213 (0.254)	Data 1.26e-04 (9.59e-04)	Tok/s 47955 (54499)	Loss/tok 3.1013 (3.2557)	LR 2.000e-03
0: TRAIN [2][840/1938]	Time 0.274 (0.254)	Data 9.99e-05 (9.49e-04)	Tok/s 61389 (54461)	Loss/tok 3.1737 (3.2547)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.276 (0.254)	Data 1.10e-04 (9.39e-04)	Tok/s 60337 (54496)	Loss/tok 3.2372 (3.2546)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.213 (0.254)	Data 9.47e-05 (9.30e-04)	Tok/s 48476 (54522)	Loss/tok 3.0453 (3.2547)	LR 2.000e-03
0: TRAIN [2][870/1938]	Time 0.212 (0.254)	Data 1.02e-04 (9.20e-04)	Tok/s 48023 (54548)	Loss/tok 3.0748 (3.2557)	LR 2.000e-03
0: TRAIN [2][880/1938]	Time 0.157 (0.254)	Data 1.25e-04 (9.11e-04)	Tok/s 33047 (54621)	Loss/tok 2.6805 (3.2573)	LR 2.000e-03
0: TRAIN [2][890/1938]	Time 0.213 (0.255)	Data 1.01e-04 (9.02e-04)	Tok/s 48387 (54645)	Loss/tok 3.1444 (3.2577)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][900/1938]	Time 0.276 (0.255)	Data 1.03e-04 (8.93e-04)	Tok/s 59684 (54641)	Loss/tok 3.3382 (3.2587)	LR 2.000e-03
0: TRAIN [2][910/1938]	Time 0.157 (0.254)	Data 9.92e-05 (8.84e-04)	Tok/s 34282 (54606)	Loss/tok 2.5079 (3.2579)	LR 2.000e-03
0: TRAIN [2][920/1938]	Time 0.338 (0.254)	Data 1.01e-04 (8.76e-04)	Tok/s 69574 (54629)	Loss/tok 3.4435 (3.2584)	LR 2.000e-03
0: TRAIN [2][930/1938]	Time 0.213 (0.254)	Data 1.17e-04 (8.68e-04)	Tok/s 48765 (54640)	Loss/tok 3.0958 (3.2579)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.276 (0.255)	Data 1.18e-04 (8.60e-04)	Tok/s 60477 (54672)	Loss/tok 3.1576 (3.2591)	LR 2.000e-03
0: TRAIN [2][950/1938]	Time 0.337 (0.255)	Data 1.40e-04 (8.53e-04)	Tok/s 69207 (54662)	Loss/tok 3.4262 (3.2586)	LR 2.000e-03
0: TRAIN [2][960/1938]	Time 0.276 (0.255)	Data 1.11e-04 (8.45e-04)	Tok/s 61117 (54666)	Loss/tok 3.1665 (3.2583)	LR 2.000e-03
0: TRAIN [2][970/1938]	Time 0.213 (0.255)	Data 9.75e-05 (8.38e-04)	Tok/s 48296 (54674)	Loss/tok 2.9801 (3.2578)	LR 2.000e-03
0: TRAIN [2][980/1938]	Time 0.338 (0.255)	Data 9.80e-05 (8.30e-04)	Tok/s 68806 (54722)	Loss/tok 3.3864 (3.2594)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.276 (0.255)	Data 1.51e-04 (8.23e-04)	Tok/s 61473 (54731)	Loss/tok 3.2096 (3.2596)	LR 2.000e-03
0: TRAIN [2][1000/1938]	Time 0.214 (0.255)	Data 1.31e-04 (8.16e-04)	Tok/s 48790 (54763)	Loss/tok 3.0650 (3.2614)	LR 2.000e-03
0: TRAIN [2][1010/1938]	Time 0.336 (0.255)	Data 1.23e-04 (8.09e-04)	Tok/s 69739 (54785)	Loss/tok 3.4109 (3.2616)	LR 2.000e-03
0: TRAIN [2][1020/1938]	Time 0.277 (0.255)	Data 1.18e-04 (8.02e-04)	Tok/s 59949 (54750)	Loss/tok 3.2290 (3.2611)	LR 2.000e-03
0: TRAIN [2][1030/1938]	Time 0.213 (0.255)	Data 1.64e-04 (7.96e-04)	Tok/s 48177 (54760)	Loss/tok 3.1393 (3.2618)	LR 2.000e-03
0: TRAIN [2][1040/1938]	Time 0.214 (0.255)	Data 1.02e-04 (7.89e-04)	Tok/s 48428 (54785)	Loss/tok 3.0467 (3.2610)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1050/1938]	Time 0.158 (0.255)	Data 1.04e-04 (7.82e-04)	Tok/s 33240 (54773)	Loss/tok 2.5331 (3.2618)	LR 2.000e-03
0: TRAIN [2][1060/1938]	Time 0.214 (0.255)	Data 9.99e-05 (7.76e-04)	Tok/s 49491 (54756)	Loss/tok 3.1227 (3.2614)	LR 2.000e-03
0: TRAIN [2][1070/1938]	Time 0.276 (0.255)	Data 1.15e-04 (7.71e-04)	Tok/s 60738 (54814)	Loss/tok 3.4001 (3.2627)	LR 2.000e-03
0: TRAIN [2][1080/1938]	Time 0.337 (0.255)	Data 1.17e-04 (7.65e-04)	Tok/s 68951 (54784)	Loss/tok 3.5120 (3.2620)	LR 2.000e-03
0: TRAIN [2][1090/1938]	Time 0.275 (0.255)	Data 7.92e-04 (7.59e-04)	Tok/s 61635 (54813)	Loss/tok 3.2017 (3.2618)	LR 2.000e-03
0: TRAIN [2][1100/1938]	Time 0.213 (0.255)	Data 1.20e-04 (7.53e-04)	Tok/s 48351 (54758)	Loss/tok 3.1836 (3.2611)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.274 (0.255)	Data 1.07e-04 (7.48e-04)	Tok/s 60638 (54811)	Loss/tok 3.2512 (3.2623)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.277 (0.255)	Data 1.01e-04 (7.42e-04)	Tok/s 60655 (54806)	Loss/tok 3.2405 (3.2617)	LR 2.000e-03
0: TRAIN [2][1130/1938]	Time 0.276 (0.255)	Data 1.59e-04 (7.36e-04)	Tok/s 61193 (54775)	Loss/tok 3.3183 (3.2610)	LR 2.000e-03
0: TRAIN [2][1140/1938]	Time 0.277 (0.255)	Data 1.21e-04 (7.31e-04)	Tok/s 61178 (54774)	Loss/tok 3.3126 (3.2603)	LR 2.000e-03
0: TRAIN [2][1150/1938]	Time 0.278 (0.255)	Data 1.06e-04 (7.26e-04)	Tok/s 61591 (54761)	Loss/tok 3.2266 (3.2604)	LR 2.000e-03
0: TRAIN [2][1160/1938]	Time 0.414 (0.255)	Data 1.10e-04 (7.20e-04)	Tok/s 72914 (54797)	Loss/tok 3.6466 (3.2609)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.338 (0.255)	Data 1.33e-04 (7.15e-04)	Tok/s 69679 (54816)	Loss/tok 3.3771 (3.2611)	LR 2.000e-03
0: TRAIN [2][1180/1938]	Time 0.157 (0.255)	Data 1.27e-04 (7.10e-04)	Tok/s 34232 (54801)	Loss/tok 2.7202 (3.2608)	LR 2.000e-03
0: TRAIN [2][1190/1938]	Time 0.215 (0.255)	Data 1.02e-04 (7.05e-04)	Tok/s 48618 (54785)	Loss/tok 3.0852 (3.2600)	LR 2.000e-03
0: TRAIN [2][1200/1938]	Time 0.213 (0.255)	Data 1.19e-04 (7.01e-04)	Tok/s 48965 (54771)	Loss/tok 3.0154 (3.2595)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.277 (0.255)	Data 1.02e-04 (6.96e-04)	Tok/s 60278 (54800)	Loss/tok 3.2497 (3.2601)	LR 2.000e-03
0: TRAIN [2][1220/1938]	Time 0.214 (0.255)	Data 1.09e-04 (6.91e-04)	Tok/s 48747 (54795)	Loss/tok 2.9488 (3.2601)	LR 2.000e-03
0: TRAIN [2][1230/1938]	Time 0.213 (0.255)	Data 1.03e-04 (6.87e-04)	Tok/s 48442 (54801)	Loss/tok 3.0194 (3.2595)	LR 2.000e-03
0: TRAIN [2][1240/1938]	Time 0.213 (0.255)	Data 1.05e-04 (6.82e-04)	Tok/s 48618 (54783)	Loss/tok 3.0782 (3.2594)	LR 2.000e-03
0: TRAIN [2][1250/1938]	Time 0.214 (0.255)	Data 1.14e-04 (6.77e-04)	Tok/s 48494 (54791)	Loss/tok 3.1115 (3.2595)	LR 2.000e-03
0: TRAIN [2][1260/1938]	Time 0.276 (0.255)	Data 1.07e-04 (6.73e-04)	Tok/s 60940 (54844)	Loss/tok 3.1593 (3.2596)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1270/1938]	Time 0.214 (0.255)	Data 1.08e-04 (6.69e-04)	Tok/s 48965 (54807)	Loss/tok 3.0717 (3.2588)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.214 (0.255)	Data 1.29e-04 (6.64e-04)	Tok/s 47774 (54810)	Loss/tok 3.0835 (3.2587)	LR 2.000e-03
0: TRAIN [2][1290/1938]	Time 0.213 (0.255)	Data 1.03e-04 (6.60e-04)	Tok/s 48174 (54822)	Loss/tok 2.9266 (3.2588)	LR 2.000e-03
0: TRAIN [2][1300/1938]	Time 0.214 (0.255)	Data 1.07e-04 (6.56e-04)	Tok/s 46961 (54794)	Loss/tok 3.1106 (3.2585)	LR 2.000e-03
0: TRAIN [2][1310/1938]	Time 0.214 (0.255)	Data 1.08e-04 (6.52e-04)	Tok/s 49351 (54804)	Loss/tok 2.9964 (3.2584)	LR 2.000e-03
0: TRAIN [2][1320/1938]	Time 0.339 (0.255)	Data 1.33e-04 (6.48e-04)	Tok/s 68656 (54821)	Loss/tok 3.4509 (3.2586)	LR 2.000e-03
0: TRAIN [2][1330/1938]	Time 0.158 (0.255)	Data 1.16e-04 (6.44e-04)	Tok/s 33565 (54822)	Loss/tok 2.5490 (3.2581)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.339 (0.255)	Data 1.06e-04 (6.40e-04)	Tok/s 69057 (54859)	Loss/tok 3.4077 (3.2593)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.214 (0.255)	Data 1.02e-04 (6.36e-04)	Tok/s 48388 (54886)	Loss/tok 3.1887 (3.2597)	LR 2.000e-03
0: TRAIN [2][1360/1938]	Time 0.276 (0.255)	Data 9.99e-05 (6.32e-04)	Tok/s 60792 (54877)	Loss/tok 3.2071 (3.2593)	LR 2.000e-03
0: TRAIN [2][1370/1938]	Time 0.213 (0.255)	Data 1.07e-04 (6.29e-04)	Tok/s 48246 (54898)	Loss/tok 3.0615 (3.2599)	LR 2.000e-03
0: TRAIN [2][1380/1938]	Time 0.213 (0.255)	Data 1.15e-04 (6.25e-04)	Tok/s 49087 (54885)	Loss/tok 3.0765 (3.2593)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.213 (0.255)	Data 1.21e-04 (6.21e-04)	Tok/s 48937 (54875)	Loss/tok 2.9386 (3.2594)	LR 2.000e-03
0: TRAIN [2][1400/1938]	Time 0.214 (0.255)	Data 1.20e-04 (6.18e-04)	Tok/s 48430 (54900)	Loss/tok 3.0411 (3.2594)	LR 2.000e-03
0: TRAIN [2][1410/1938]	Time 0.214 (0.255)	Data 1.18e-04 (6.14e-04)	Tok/s 48391 (54868)	Loss/tok 3.1022 (3.2585)	LR 2.000e-03
0: TRAIN [2][1420/1938]	Time 0.337 (0.255)	Data 1.26e-04 (6.11e-04)	Tok/s 69263 (54873)	Loss/tok 3.4399 (3.2590)	LR 2.000e-03
0: TRAIN [2][1430/1938]	Time 0.276 (0.255)	Data 9.99e-05 (6.07e-04)	Tok/s 60484 (54875)	Loss/tok 3.2520 (3.2587)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.157 (0.255)	Data 5.27e-04 (6.04e-04)	Tok/s 33619 (54821)	Loss/tok 2.6777 (3.2581)	LR 2.000e-03
0: TRAIN [2][1450/1938]	Time 0.158 (0.255)	Data 1.16e-04 (6.01e-04)	Tok/s 33333 (54789)	Loss/tok 2.5885 (3.2571)	LR 2.000e-03
0: TRAIN [2][1460/1938]	Time 0.276 (0.255)	Data 1.07e-04 (5.97e-04)	Tok/s 61554 (54779)	Loss/tok 3.1952 (3.2568)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.276 (0.255)	Data 1.06e-04 (5.94e-04)	Tok/s 61033 (54820)	Loss/tok 3.4521 (3.2577)	LR 2.000e-03
0: TRAIN [2][1480/1938]	Time 0.410 (0.255)	Data 1.76e-04 (5.91e-04)	Tok/s 72257 (54820)	Loss/tok 3.6114 (3.2580)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.213 (0.255)	Data 1.08e-04 (5.88e-04)	Tok/s 47948 (54815)	Loss/tok 3.0288 (3.2579)	LR 2.000e-03
0: TRAIN [2][1500/1938]	Time 0.275 (0.255)	Data 1.12e-04 (5.85e-04)	Tok/s 61661 (54822)	Loss/tok 3.3104 (3.2576)	LR 2.000e-03
0: TRAIN [2][1510/1938]	Time 0.214 (0.255)	Data 1.33e-04 (5.82e-04)	Tok/s 49054 (54834)	Loss/tok 3.1142 (3.2573)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.277 (0.255)	Data 1.13e-04 (5.79e-04)	Tok/s 60480 (54852)	Loss/tok 3.3061 (3.2576)	LR 2.000e-03
0: TRAIN [2][1530/1938]	Time 0.213 (0.255)	Data 1.07e-04 (5.76e-04)	Tok/s 48237 (54844)	Loss/tok 2.8996 (3.2573)	LR 2.000e-03
0: TRAIN [2][1540/1938]	Time 0.213 (0.255)	Data 1.04e-04 (5.74e-04)	Tok/s 48724 (54831)	Loss/tok 3.0605 (3.2570)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.214 (0.255)	Data 1.02e-04 (5.71e-04)	Tok/s 49022 (54812)	Loss/tok 3.0817 (3.2563)	LR 2.000e-03
0: TRAIN [2][1560/1938]	Time 0.214 (0.255)	Data 1.45e-04 (5.68e-04)	Tok/s 48643 (54819)	Loss/tok 3.0831 (3.2562)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1570/1938]	Time 0.338 (0.255)	Data 1.04e-04 (5.65e-04)	Tok/s 68907 (54839)	Loss/tok 3.4307 (3.2564)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.213 (0.255)	Data 1.68e-04 (5.62e-04)	Tok/s 49310 (54814)	Loss/tok 3.0346 (3.2561)	LR 2.000e-03
0: TRAIN [2][1590/1938]	Time 0.214 (0.255)	Data 1.13e-04 (5.59e-04)	Tok/s 47238 (54850)	Loss/tok 3.0010 (3.2566)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.275 (0.255)	Data 1.10e-04 (5.57e-04)	Tok/s 61959 (54857)	Loss/tok 3.1992 (3.2572)	LR 2.000e-03
0: TRAIN [2][1610/1938]	Time 0.213 (0.255)	Data 1.18e-04 (5.54e-04)	Tok/s 48363 (54841)	Loss/tok 3.0571 (3.2569)	LR 2.000e-03
0: TRAIN [2][1620/1938]	Time 0.339 (0.255)	Data 1.05e-04 (5.51e-04)	Tok/s 68336 (54840)	Loss/tok 3.5497 (3.2568)	LR 2.000e-03
0: TRAIN [2][1630/1938]	Time 0.337 (0.255)	Data 1.08e-04 (5.48e-04)	Tok/s 69351 (54852)	Loss/tok 3.3249 (3.2564)	LR 2.000e-03
0: TRAIN [2][1640/1938]	Time 0.214 (0.255)	Data 1.05e-04 (5.46e-04)	Tok/s 49059 (54862)	Loss/tok 3.0629 (3.2562)	LR 2.000e-03
0: TRAIN [2][1650/1938]	Time 0.216 (0.255)	Data 1.16e-04 (5.43e-04)	Tok/s 47513 (54873)	Loss/tok 2.9786 (3.2563)	LR 2.000e-03
0: TRAIN [2][1660/1938]	Time 0.277 (0.255)	Data 1.44e-04 (5.41e-04)	Tok/s 60055 (54862)	Loss/tok 3.3939 (3.2559)	LR 2.000e-03
0: TRAIN [2][1670/1938]	Time 0.214 (0.255)	Data 1.15e-04 (5.38e-04)	Tok/s 48511 (54837)	Loss/tok 3.0293 (3.2554)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.276 (0.255)	Data 1.23e-04 (5.35e-04)	Tok/s 60334 (54817)	Loss/tok 3.2372 (3.2550)	LR 2.000e-03
0: TRAIN [2][1690/1938]	Time 0.214 (0.254)	Data 1.06e-04 (5.33e-04)	Tok/s 49239 (54771)	Loss/tok 2.9376 (3.2539)	LR 2.000e-03
0: TRAIN [2][1700/1938]	Time 0.277 (0.254)	Data 1.02e-04 (5.30e-04)	Tok/s 60336 (54782)	Loss/tok 3.2566 (3.2539)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1710/1938]	Time 0.337 (0.254)	Data 9.75e-05 (5.28e-04)	Tok/s 69538 (54782)	Loss/tok 3.4102 (3.2540)	LR 2.000e-03
0: TRAIN [2][1720/1938]	Time 0.214 (0.254)	Data 9.54e-05 (5.26e-04)	Tok/s 48651 (54796)	Loss/tok 3.0374 (3.2540)	LR 2.000e-03
0: TRAIN [2][1730/1938]	Time 0.337 (0.255)	Data 1.07e-04 (5.23e-04)	Tok/s 69824 (54810)	Loss/tok 3.3771 (3.2543)	LR 2.000e-03
0: TRAIN [2][1740/1938]	Time 0.214 (0.254)	Data 9.30e-05 (5.21e-04)	Tok/s 48380 (54777)	Loss/tok 3.0831 (3.2538)	LR 2.000e-03
0: TRAIN [2][1750/1938]	Time 0.214 (0.254)	Data 9.85e-05 (5.19e-04)	Tok/s 48097 (54746)	Loss/tok 3.1514 (3.2532)	LR 2.000e-03
0: TRAIN [2][1760/1938]	Time 0.213 (0.254)	Data 9.94e-05 (5.16e-04)	Tok/s 48537 (54747)	Loss/tok 3.0693 (3.2531)	LR 2.000e-03
0: TRAIN [2][1770/1938]	Time 0.274 (0.254)	Data 9.78e-05 (5.14e-04)	Tok/s 60732 (54727)	Loss/tok 3.2959 (3.2529)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.214 (0.254)	Data 1.07e-04 (5.12e-04)	Tok/s 48304 (54752)	Loss/tok 2.9950 (3.2542)	LR 2.000e-03
0: TRAIN [2][1790/1938]	Time 0.277 (0.254)	Data 1.09e-04 (5.09e-04)	Tok/s 61281 (54733)	Loss/tok 3.2501 (3.2538)	LR 2.000e-03
0: TRAIN [2][1800/1938]	Time 0.214 (0.254)	Data 9.44e-05 (5.07e-04)	Tok/s 48726 (54737)	Loss/tok 3.0478 (3.2535)	LR 2.000e-03
0: TRAIN [2][1810/1938]	Time 0.340 (0.254)	Data 9.75e-05 (5.05e-04)	Tok/s 68826 (54709)	Loss/tok 3.3817 (3.2530)	LR 2.000e-03
0: TRAIN [2][1820/1938]	Time 0.275 (0.254)	Data 1.17e-04 (5.03e-04)	Tok/s 61249 (54743)	Loss/tok 3.2802 (3.2537)	LR 2.000e-03
0: TRAIN [2][1830/1938]	Time 0.158 (0.254)	Data 1.03e-04 (5.00e-04)	Tok/s 32801 (54743)	Loss/tok 2.6886 (3.2535)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1840/1938]	Time 0.211 (0.254)	Data 1.28e-04 (4.98e-04)	Tok/s 48881 (54747)	Loss/tok 3.0192 (3.2539)	LR 2.000e-03
0: TRAIN [2][1850/1938]	Time 0.274 (0.254)	Data 1.25e-04 (4.96e-04)	Tok/s 61424 (54762)	Loss/tok 3.1199 (3.2537)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.275 (0.254)	Data 1.37e-04 (4.94e-04)	Tok/s 61203 (54766)	Loss/tok 3.2141 (3.2533)	LR 2.000e-03
0: TRAIN [2][1870/1938]	Time 0.214 (0.254)	Data 1.04e-04 (4.92e-04)	Tok/s 48656 (54769)	Loss/tok 2.8839 (3.2539)	LR 2.000e-03
0: TRAIN [2][1880/1938]	Time 0.337 (0.254)	Data 1.11e-04 (4.90e-04)	Tok/s 69900 (54779)	Loss/tok 3.3462 (3.2535)	LR 2.000e-03
0: TRAIN [2][1890/1938]	Time 0.277 (0.254)	Data 1.06e-04 (4.88e-04)	Tok/s 59838 (54783)	Loss/tok 3.2864 (3.2533)	LR 2.000e-03
0: TRAIN [2][1900/1938]	Time 0.276 (0.255)	Data 1.10e-04 (4.86e-04)	Tok/s 61655 (54816)	Loss/tok 3.2938 (3.2540)	LR 2.000e-03
0: TRAIN [2][1910/1938]	Time 0.273 (0.255)	Data 1.05e-04 (4.84e-04)	Tok/s 61977 (54837)	Loss/tok 3.2270 (3.2542)	LR 2.000e-03
0: TRAIN [2][1920/1938]	Time 0.214 (0.255)	Data 5.30e-04 (4.83e-04)	Tok/s 48151 (54847)	Loss/tok 3.1656 (3.2545)	LR 2.000e-03
0: TRAIN [2][1930/1938]	Time 0.214 (0.255)	Data 1.01e-04 (4.81e-04)	Tok/s 48861 (54850)	Loss/tok 3.0684 (3.2545)	LR 2.000e-03
:::MLL 1570036517.128 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1570036517.129 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.620 (0.620)	Decoder iters 97.0 (97.0)	Tok/s 26074 (26074)
0: Running moses detokenizer
0: BLEU(score=22.7832362067271, counts=[35906, 17497, 9757, 5708], totals=[63947, 60944, 57941, 54945], precisions=[56.149623907298235, 28.70996324494618, 16.839543673737076, 10.388570388570388], bp=0.9886646688432368, sys_len=63947, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1570036518.924 eval_accuracy: {"value": 22.78, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1570036518.925 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.2553	Test BLEU: 22.78
0: Performance: Epoch: 2	Training: 438791 Tok/s
0: Finished epoch 2
:::MLL 1570036518.925 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1570036518.925 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1570036518.926 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 2181962343
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][0/1938]	Time 1.056 (1.056)	Data 6.97e-01 (6.97e-01)	Tok/s 22169 (22169)	Loss/tok 3.3356 (3.3356)	LR 2.000e-03
0: TRAIN [3][10/1938]	Time 0.213 (0.303)	Data 9.94e-05 (6.34e-02)	Tok/s 48125 (47381)	Loss/tok 2.9540 (3.0882)	LR 2.000e-03
0: TRAIN [3][20/1938]	Time 0.213 (0.272)	Data 1.15e-04 (3.33e-02)	Tok/s 48903 (49980)	Loss/tok 2.8454 (3.0808)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.273 (0.269)	Data 9.89e-05 (2.26e-02)	Tok/s 61397 (52619)	Loss/tok 3.1507 (3.1063)	LR 2.000e-03
0: TRAIN [3][40/1938]	Time 0.275 (0.263)	Data 1.05e-04 (1.71e-02)	Tok/s 61825 (52662)	Loss/tok 3.1954 (3.1188)	LR 2.000e-03
0: TRAIN [3][50/1938]	Time 0.337 (0.262)	Data 9.27e-05 (1.38e-02)	Tok/s 69067 (53371)	Loss/tok 3.4218 (3.1249)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.274 (0.261)	Data 9.80e-05 (1.15e-02)	Tok/s 61637 (53432)	Loss/tok 3.0641 (3.1297)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.411 (0.266)	Data 1.01e-04 (9.92e-03)	Tok/s 71547 (54465)	Loss/tok 3.6290 (3.1605)	LR 2.000e-03
0: TRAIN [3][80/1938]	Time 0.339 (0.264)	Data 4.47e-04 (8.71e-03)	Tok/s 68457 (54433)	Loss/tok 3.3193 (3.1588)	LR 2.000e-03
0: TRAIN [3][90/1938]	Time 0.409 (0.266)	Data 9.70e-05 (7.77e-03)	Tok/s 72612 (54907)	Loss/tok 3.4343 (3.1642)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.276 (0.266)	Data 9.92e-05 (7.01e-03)	Tok/s 60880 (55269)	Loss/tok 3.1854 (3.1629)	LR 2.000e-03
0: TRAIN [3][110/1938]	Time 0.337 (0.269)	Data 1.29e-04 (6.39e-03)	Tok/s 69892 (55777)	Loss/tok 3.4378 (3.1793)	LR 2.000e-03
0: TRAIN [3][120/1938]	Time 0.339 (0.269)	Data 1.09e-04 (5.87e-03)	Tok/s 68824 (55925)	Loss/tok 3.3961 (3.1846)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.214 (0.268)	Data 1.17e-04 (5.43e-03)	Tok/s 47065 (55853)	Loss/tok 2.9293 (3.1812)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.278 (0.267)	Data 1.16e-04 (5.05e-03)	Tok/s 60932 (55762)	Loss/tok 3.1976 (3.1782)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.276 (0.266)	Data 1.27e-04 (4.73e-03)	Tok/s 60387 (55507)	Loss/tok 3.1660 (3.1788)	LR 2.000e-03
0: TRAIN [3][160/1938]	Time 0.213 (0.266)	Data 1.14e-04 (4.44e-03)	Tok/s 47739 (55582)	Loss/tok 2.9651 (3.1827)	LR 2.000e-03
0: TRAIN [3][170/1938]	Time 0.213 (0.267)	Data 1.23e-04 (4.19e-03)	Tok/s 47537 (55917)	Loss/tok 2.9764 (3.1863)	LR 2.000e-03
0: TRAIN [3][180/1938]	Time 0.276 (0.267)	Data 1.16e-04 (3.96e-03)	Tok/s 60993 (55994)	Loss/tok 3.0687 (3.1878)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.339 (0.267)	Data 1.24e-04 (3.76e-03)	Tok/s 69623 (56022)	Loss/tok 3.3177 (3.1881)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.275 (0.268)	Data 1.17e-04 (3.58e-03)	Tok/s 61042 (56050)	Loss/tok 3.2674 (3.1936)	LR 2.000e-03
0: TRAIN [3][210/1938]	Time 0.275 (0.268)	Data 1.18e-04 (3.42e-03)	Tok/s 61349 (56172)	Loss/tok 3.2073 (3.1937)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.213 (0.265)	Data 1.34e-04 (3.27e-03)	Tok/s 47457 (55809)	Loss/tok 3.0818 (3.1881)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.213 (0.264)	Data 1.14e-04 (3.13e-03)	Tok/s 49355 (55616)	Loss/tok 2.9127 (3.1862)	LR 2.000e-03
0: TRAIN [3][240/1938]	Time 0.273 (0.264)	Data 1.14e-04 (3.01e-03)	Tok/s 59920 (55684)	Loss/tok 3.1728 (3.1885)	LR 2.000e-03
0: TRAIN [3][250/1938]	Time 0.214 (0.263)	Data 1.39e-04 (2.90e-03)	Tok/s 47531 (55555)	Loss/tok 2.9981 (3.1864)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.274 (0.262)	Data 1.24e-04 (2.79e-03)	Tok/s 60504 (55456)	Loss/tok 3.2010 (3.1835)	LR 2.000e-03
0: TRAIN [3][270/1938]	Time 0.276 (0.262)	Data 5.54e-04 (2.69e-03)	Tok/s 60772 (55448)	Loss/tok 3.1847 (3.1824)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.214 (0.261)	Data 1.36e-04 (2.60e-03)	Tok/s 49208 (55289)	Loss/tok 2.9794 (3.1776)	LR 2.000e-03
0: TRAIN [3][290/1938]	Time 0.275 (0.260)	Data 1.37e-04 (2.52e-03)	Tok/s 60926 (55197)	Loss/tok 3.2021 (3.1756)	LR 2.000e-03
0: TRAIN [3][300/1938]	Time 0.214 (0.259)	Data 1.14e-04 (2.44e-03)	Tok/s 48191 (55036)	Loss/tok 2.9304 (3.1721)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][310/1938]	Time 0.156 (0.258)	Data 1.14e-04 (2.37e-03)	Tok/s 33881 (54820)	Loss/tok 2.5488 (3.1716)	LR 2.000e-03
0: TRAIN [3][320/1938]	Time 0.276 (0.258)	Data 1.22e-04 (2.30e-03)	Tok/s 60799 (54876)	Loss/tok 3.1274 (3.1714)	LR 2.000e-03
0: TRAIN [3][330/1938]	Time 0.214 (0.258)	Data 1.25e-04 (2.23e-03)	Tok/s 48505 (54910)	Loss/tok 3.0485 (3.1695)	LR 2.000e-03
0: TRAIN [3][340/1938]	Time 0.277 (0.258)	Data 1.24e-04 (2.17e-03)	Tok/s 60355 (54961)	Loss/tok 3.1429 (3.1678)	LR 2.000e-03
0: TRAIN [3][350/1938]	Time 0.156 (0.257)	Data 1.31e-04 (2.11e-03)	Tok/s 34160 (54801)	Loss/tok 2.5836 (3.1659)	LR 2.000e-03
0: TRAIN [3][360/1938]	Time 0.213 (0.257)	Data 1.33e-04 (2.06e-03)	Tok/s 48421 (54772)	Loss/tok 2.9531 (3.1670)	LR 2.000e-03
0: TRAIN [3][370/1938]	Time 0.213 (0.257)	Data 1.31e-04 (2.01e-03)	Tok/s 48045 (54746)	Loss/tok 3.1145 (3.1703)	LR 2.000e-03
0: TRAIN [3][380/1938]	Time 0.276 (0.257)	Data 1.47e-04 (1.96e-03)	Tok/s 60089 (54815)	Loss/tok 3.2875 (3.1724)	LR 2.000e-03
0: TRAIN [3][390/1938]	Time 0.275 (0.257)	Data 1.39e-04 (1.91e-03)	Tok/s 62156 (54794)	Loss/tok 3.0416 (3.1721)	LR 2.000e-03
0: TRAIN [3][400/1938]	Time 0.338 (0.257)	Data 1.54e-04 (1.87e-03)	Tok/s 69458 (54852)	Loss/tok 3.3936 (3.1734)	LR 2.000e-03
0: TRAIN [3][410/1938]	Time 0.275 (0.257)	Data 1.31e-04 (1.82e-03)	Tok/s 61822 (54806)	Loss/tok 3.0114 (3.1727)	LR 2.000e-03
0: TRAIN [3][420/1938]	Time 0.276 (0.256)	Data 1.17e-04 (1.78e-03)	Tok/s 59245 (54691)	Loss/tok 3.1840 (3.1707)	LR 2.000e-03
0: TRAIN [3][430/1938]	Time 0.411 (0.257)	Data 1.17e-04 (1.75e-03)	Tok/s 71868 (54750)	Loss/tok 3.5782 (3.1721)	LR 2.000e-03
0: TRAIN [3][440/1938]	Time 0.214 (0.256)	Data 1.31e-04 (1.71e-03)	Tok/s 47775 (54649)	Loss/tok 2.9619 (3.1704)	LR 2.000e-03
0: TRAIN [3][450/1938]	Time 0.411 (0.256)	Data 1.29e-04 (1.67e-03)	Tok/s 72474 (54668)	Loss/tok 3.5048 (3.1708)	LR 2.000e-03
0: TRAIN [3][460/1938]	Time 0.276 (0.256)	Data 1.32e-04 (1.64e-03)	Tok/s 61474 (54685)	Loss/tok 3.1437 (3.1706)	LR 2.000e-03
0: TRAIN [3][470/1938]	Time 0.214 (0.256)	Data 1.46e-04 (1.61e-03)	Tok/s 48241 (54664)	Loss/tok 3.1013 (3.1736)	LR 2.000e-03
0: TRAIN [3][480/1938]	Time 0.158 (0.256)	Data 1.28e-04 (1.58e-03)	Tok/s 32811 (54673)	Loss/tok 2.5810 (3.1734)	LR 2.000e-03
0: TRAIN [3][490/1938]	Time 0.213 (0.256)	Data 3.29e-04 (1.55e-03)	Tok/s 48304 (54683)	Loss/tok 2.9616 (3.1739)	LR 2.000e-03
0: TRAIN [3][500/1938]	Time 0.278 (0.255)	Data 2.01e-04 (1.52e-03)	Tok/s 60136 (54604)	Loss/tok 3.1869 (3.1713)	LR 2.000e-03
0: TRAIN [3][510/1938]	Time 0.276 (0.256)	Data 1.19e-04 (1.49e-03)	Tok/s 60755 (54743)	Loss/tok 3.1638 (3.1754)	LR 2.000e-03
0: TRAIN [3][520/1938]	Time 0.214 (0.256)	Data 1.10e-04 (1.47e-03)	Tok/s 48332 (54762)	Loss/tok 3.0972 (3.1748)	LR 2.000e-03
0: TRAIN [3][530/1938]	Time 0.213 (0.256)	Data 1.15e-04 (1.44e-03)	Tok/s 48831 (54726)	Loss/tok 3.0557 (3.1735)	LR 2.000e-03
0: TRAIN [3][540/1938]	Time 0.214 (0.256)	Data 1.13e-04 (1.42e-03)	Tok/s 48739 (54761)	Loss/tok 3.0361 (3.1738)	LR 2.000e-03
0: TRAIN [3][550/1938]	Time 0.413 (0.256)	Data 1.12e-04 (1.40e-03)	Tok/s 72270 (54793)	Loss/tok 3.4817 (3.1751)	LR 2.000e-03
0: TRAIN [3][560/1938]	Time 0.412 (0.257)	Data 1.27e-04 (1.37e-03)	Tok/s 72291 (54911)	Loss/tok 3.5404 (3.1785)	LR 2.000e-03
0: TRAIN [3][570/1938]	Time 0.214 (0.257)	Data 1.16e-04 (1.35e-03)	Tok/s 48126 (54927)	Loss/tok 2.9260 (3.1785)	LR 2.000e-03
0: TRAIN [3][580/1938]	Time 0.214 (0.256)	Data 1.23e-04 (1.33e-03)	Tok/s 47752 (54805)	Loss/tok 2.9233 (3.1765)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][590/1938]	Time 0.213 (0.256)	Data 1.15e-04 (1.31e-03)	Tok/s 49053 (54803)	Loss/tok 3.0201 (3.1764)	LR 2.000e-03
0: TRAIN [3][600/1938]	Time 0.278 (0.256)	Data 1.46e-04 (1.29e-03)	Tok/s 60692 (54847)	Loss/tok 3.1904 (3.1758)	LR 2.000e-03
0: TRAIN [3][610/1938]	Time 0.276 (0.256)	Data 1.48e-04 (1.27e-03)	Tok/s 60959 (54777)	Loss/tok 3.1757 (3.1743)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][620/1938]	Time 0.333 (0.256)	Data 1.37e-04 (1.26e-03)	Tok/s 70098 (54758)	Loss/tok 3.3019 (3.1750)	LR 2.000e-03
0: TRAIN [3][630/1938]	Time 0.213 (0.256)	Data 1.73e-04 (1.24e-03)	Tok/s 47521 (54764)	Loss/tok 3.0908 (3.1776)	LR 2.000e-03
0: TRAIN [3][640/1938]	Time 0.336 (0.256)	Data 1.89e-04 (1.22e-03)	Tok/s 68885 (54849)	Loss/tok 3.4187 (3.1794)	LR 2.000e-03
0: TRAIN [3][650/1938]	Time 0.156 (0.256)	Data 1.62e-04 (1.21e-03)	Tok/s 34367 (54849)	Loss/tok 2.5164 (3.1804)	LR 2.000e-03
0: TRAIN [3][660/1938]	Time 0.276 (0.256)	Data 4.09e-04 (1.19e-03)	Tok/s 59788 (54892)	Loss/tok 3.2606 (3.1815)	LR 2.000e-03
0: TRAIN [3][670/1938]	Time 0.214 (0.256)	Data 1.31e-04 (1.18e-03)	Tok/s 49329 (54891)	Loss/tok 2.9930 (3.1813)	LR 1.000e-03
0: TRAIN [3][680/1938]	Time 0.213 (0.256)	Data 1.08e-04 (1.16e-03)	Tok/s 48696 (54812)	Loss/tok 3.0638 (3.1797)	LR 1.000e-03
0: TRAIN [3][690/1938]	Time 0.277 (0.256)	Data 1.45e-04 (1.15e-03)	Tok/s 61505 (54852)	Loss/tok 3.1244 (3.1793)	LR 1.000e-03
0: TRAIN [3][700/1938]	Time 0.214 (0.256)	Data 1.80e-04 (1.13e-03)	Tok/s 49759 (54779)	Loss/tok 2.9643 (3.1790)	LR 1.000e-03
0: TRAIN [3][710/1938]	Time 0.338 (0.255)	Data 1.21e-04 (1.12e-03)	Tok/s 69005 (54742)	Loss/tok 3.2798 (3.1784)	LR 1.000e-03
0: TRAIN [3][720/1938]	Time 0.276 (0.255)	Data 1.45e-04 (1.10e-03)	Tok/s 60548 (54773)	Loss/tok 3.2429 (3.1780)	LR 1.000e-03
0: TRAIN [3][730/1938]	Time 0.213 (0.255)	Data 1.13e-04 (1.09e-03)	Tok/s 48925 (54709)	Loss/tok 3.0550 (3.1766)	LR 1.000e-03
0: TRAIN [3][740/1938]	Time 0.337 (0.255)	Data 1.13e-04 (1.08e-03)	Tok/s 69204 (54715)	Loss/tok 3.2871 (3.1759)	LR 1.000e-03
0: TRAIN [3][750/1938]	Time 0.156 (0.255)	Data 1.39e-04 (1.07e-03)	Tok/s 33929 (54729)	Loss/tok 2.5841 (3.1755)	LR 1.000e-03
0: TRAIN [3][760/1938]	Time 0.213 (0.255)	Data 1.39e-04 (1.05e-03)	Tok/s 47961 (54705)	Loss/tok 2.9599 (3.1746)	LR 1.000e-03
0: TRAIN [3][770/1938]	Time 0.157 (0.255)	Data 1.33e-04 (1.04e-03)	Tok/s 33409 (54685)	Loss/tok 2.4839 (3.1740)	LR 1.000e-03
0: TRAIN [3][780/1938]	Time 0.277 (0.255)	Data 1.15e-04 (1.03e-03)	Tok/s 60061 (54788)	Loss/tok 3.2186 (3.1748)	LR 1.000e-03
0: TRAIN [3][790/1938]	Time 0.273 (0.255)	Data 1.13e-04 (1.02e-03)	Tok/s 61816 (54791)	Loss/tok 3.2490 (3.1742)	LR 1.000e-03
0: TRAIN [3][800/1938]	Time 0.277 (0.255)	Data 1.34e-04 (1.01e-03)	Tok/s 60554 (54798)	Loss/tok 3.0595 (3.1739)	LR 1.000e-03
0: TRAIN [3][810/1938]	Time 0.276 (0.255)	Data 1.17e-04 (9.96e-04)	Tok/s 61217 (54822)	Loss/tok 3.1834 (3.1731)	LR 1.000e-03
0: TRAIN [3][820/1938]	Time 0.278 (0.255)	Data 1.35e-04 (9.85e-04)	Tok/s 61159 (54769)	Loss/tok 3.1013 (3.1716)	LR 1.000e-03
0: TRAIN [3][830/1938]	Time 0.277 (0.255)	Data 1.12e-04 (9.75e-04)	Tok/s 60156 (54802)	Loss/tok 3.1484 (3.1718)	LR 1.000e-03
0: TRAIN [3][840/1938]	Time 0.339 (0.255)	Data 1.18e-04 (9.65e-04)	Tok/s 69319 (54819)	Loss/tok 3.2572 (3.1711)	LR 1.000e-03
0: TRAIN [3][850/1938]	Time 0.213 (0.255)	Data 2.74e-04 (9.55e-04)	Tok/s 48492 (54853)	Loss/tok 2.9387 (3.1714)	LR 1.000e-03
0: TRAIN [3][860/1938]	Time 0.277 (0.255)	Data 1.22e-04 (9.46e-04)	Tok/s 60325 (54836)	Loss/tok 3.1369 (3.1708)	LR 1.000e-03
0: TRAIN [3][870/1938]	Time 0.157 (0.255)	Data 1.15e-04 (9.37e-04)	Tok/s 33708 (54787)	Loss/tok 2.5876 (3.1707)	LR 1.000e-03
0: TRAIN [3][880/1938]	Time 0.275 (0.255)	Data 1.08e-04 (9.27e-04)	Tok/s 61244 (54807)	Loss/tok 3.2951 (3.1710)	LR 1.000e-03
0: TRAIN [3][890/1938]	Time 0.213 (0.255)	Data 1.48e-04 (9.18e-04)	Tok/s 48603 (54825)	Loss/tok 2.9713 (3.1714)	LR 1.000e-03
0: TRAIN [3][900/1938]	Time 0.214 (0.255)	Data 1.33e-04 (9.10e-04)	Tok/s 48168 (54819)	Loss/tok 2.9218 (3.1711)	LR 1.000e-03
0: TRAIN [3][910/1938]	Time 0.215 (0.255)	Data 1.46e-04 (9.01e-04)	Tok/s 47891 (54834)	Loss/tok 2.9368 (3.1707)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][920/1938]	Time 0.275 (0.255)	Data 1.15e-04 (8.93e-04)	Tok/s 60802 (54826)	Loss/tok 3.1016 (3.1699)	LR 1.000e-03
0: TRAIN [3][930/1938]	Time 0.277 (0.255)	Data 1.18e-04 (8.85e-04)	Tok/s 60407 (54748)	Loss/tok 3.0586 (3.1685)	LR 1.000e-03
0: TRAIN [3][940/1938]	Time 0.338 (0.255)	Data 1.13e-04 (8.77e-04)	Tok/s 67775 (54763)	Loss/tok 3.3175 (3.1685)	LR 1.000e-03
0: TRAIN [3][950/1938]	Time 0.213 (0.255)	Data 1.37e-04 (8.69e-04)	Tok/s 48358 (54815)	Loss/tok 2.9680 (3.1690)	LR 1.000e-03
0: TRAIN [3][960/1938]	Time 0.157 (0.255)	Data 1.24e-04 (8.61e-04)	Tok/s 34056 (54765)	Loss/tok 2.5520 (3.1680)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][970/1938]	Time 0.336 (0.255)	Data 1.19e-04 (8.54e-04)	Tok/s 69348 (54796)	Loss/tok 3.3408 (3.1683)	LR 1.000e-03
0: TRAIN [3][980/1938]	Time 0.276 (0.255)	Data 1.19e-04 (8.47e-04)	Tok/s 60046 (54751)	Loss/tok 3.2506 (3.1674)	LR 1.000e-03
0: TRAIN [3][990/1938]	Time 0.213 (0.254)	Data 1.34e-04 (8.40e-04)	Tok/s 47550 (54697)	Loss/tok 2.9135 (3.1660)	LR 1.000e-03
0: TRAIN [3][1000/1938]	Time 0.213 (0.255)	Data 1.51e-04 (8.33e-04)	Tok/s 47049 (54748)	Loss/tok 3.0297 (3.1663)	LR 1.000e-03
0: TRAIN [3][1010/1938]	Time 0.213 (0.255)	Data 1.36e-04 (8.26e-04)	Tok/s 49642 (54757)	Loss/tok 2.9187 (3.1663)	LR 1.000e-03
0: TRAIN [3][1020/1938]	Time 0.213 (0.254)	Data 1.33e-04 (8.19e-04)	Tok/s 48528 (54690)	Loss/tok 3.0636 (3.1653)	LR 1.000e-03
0: TRAIN [3][1030/1938]	Time 0.214 (0.254)	Data 1.43e-04 (8.12e-04)	Tok/s 48381 (54695)	Loss/tok 2.9465 (3.1647)	LR 1.000e-03
0: TRAIN [3][1040/1938]	Time 0.213 (0.254)	Data 1.45e-04 (8.06e-04)	Tok/s 48546 (54659)	Loss/tok 2.9976 (3.1635)	LR 1.000e-03
0: TRAIN [3][1050/1938]	Time 0.215 (0.254)	Data 1.18e-04 (7.99e-04)	Tok/s 47706 (54644)	Loss/tok 2.9374 (3.1628)	LR 1.000e-03
0: TRAIN [3][1060/1938]	Time 0.338 (0.254)	Data 1.24e-04 (7.93e-04)	Tok/s 69338 (54666)	Loss/tok 3.2616 (3.1628)	LR 1.000e-03
0: TRAIN [3][1070/1938]	Time 0.277 (0.254)	Data 1.27e-04 (7.87e-04)	Tok/s 60913 (54734)	Loss/tok 3.1286 (3.1635)	LR 1.000e-03
0: TRAIN [3][1080/1938]	Time 0.214 (0.254)	Data 1.31e-04 (7.81e-04)	Tok/s 48453 (54698)	Loss/tok 2.9601 (3.1626)	LR 1.000e-03
0: TRAIN [3][1090/1938]	Time 0.214 (0.254)	Data 1.25e-04 (7.75e-04)	Tok/s 48148 (54701)	Loss/tok 2.9590 (3.1636)	LR 1.000e-03
0: TRAIN [3][1100/1938]	Time 0.213 (0.254)	Data 1.12e-04 (7.69e-04)	Tok/s 48463 (54684)	Loss/tok 2.9457 (3.1627)	LR 1.000e-03
0: TRAIN [3][1110/1938]	Time 0.213 (0.254)	Data 1.36e-04 (7.64e-04)	Tok/s 48282 (54700)	Loss/tok 2.9044 (3.1628)	LR 1.000e-03
0: TRAIN [3][1120/1938]	Time 0.213 (0.254)	Data 1.21e-04 (7.58e-04)	Tok/s 48774 (54724)	Loss/tok 2.9259 (3.1636)	LR 1.000e-03
0: TRAIN [3][1130/1938]	Time 0.412 (0.254)	Data 1.13e-04 (7.53e-04)	Tok/s 72516 (54742)	Loss/tok 3.5098 (3.1640)	LR 1.000e-03
0: TRAIN [3][1140/1938]	Time 0.214 (0.254)	Data 1.17e-04 (7.47e-04)	Tok/s 48231 (54733)	Loss/tok 3.0487 (3.1637)	LR 1.000e-03
0: TRAIN [3][1150/1938]	Time 0.214 (0.254)	Data 1.12e-04 (7.42e-04)	Tok/s 48006 (54692)	Loss/tok 3.0064 (3.1631)	LR 1.000e-03
0: TRAIN [3][1160/1938]	Time 0.214 (0.254)	Data 1.07e-04 (7.36e-04)	Tok/s 48729 (54670)	Loss/tok 2.9177 (3.1628)	LR 1.000e-03
0: TRAIN [3][1170/1938]	Time 0.157 (0.254)	Data 1.09e-04 (7.31e-04)	Tok/s 34029 (54692)	Loss/tok 2.5513 (3.1625)	LR 1.000e-03
0: TRAIN [3][1180/1938]	Time 0.276 (0.255)	Data 1.62e-04 (7.26e-04)	Tok/s 60808 (54746)	Loss/tok 3.1272 (3.1637)	LR 1.000e-03
0: TRAIN [3][1190/1938]	Time 0.213 (0.255)	Data 1.33e-04 (7.21e-04)	Tok/s 48746 (54749)	Loss/tok 2.8988 (3.1636)	LR 1.000e-03
0: TRAIN [3][1200/1938]	Time 0.414 (0.255)	Data 1.48e-04 (7.16e-04)	Tok/s 71967 (54774)	Loss/tok 3.3910 (3.1639)	LR 1.000e-03
0: TRAIN [3][1210/1938]	Time 0.338 (0.255)	Data 1.17e-04 (7.11e-04)	Tok/s 69203 (54779)	Loss/tok 3.1963 (3.1635)	LR 1.000e-03
0: TRAIN [3][1220/1938]	Time 0.275 (0.255)	Data 1.10e-04 (7.07e-04)	Tok/s 60149 (54805)	Loss/tok 3.2294 (3.1636)	LR 1.000e-03
0: TRAIN [3][1230/1938]	Time 0.278 (0.255)	Data 1.18e-04 (7.02e-04)	Tok/s 60121 (54796)	Loss/tok 3.0289 (3.1632)	LR 1.000e-03
0: TRAIN [3][1240/1938]	Time 0.213 (0.255)	Data 1.27e-04 (6.97e-04)	Tok/s 48709 (54805)	Loss/tok 2.9380 (3.1630)	LR 1.000e-03
0: TRAIN [3][1250/1938]	Time 0.277 (0.255)	Data 1.20e-04 (6.93e-04)	Tok/s 60284 (54834)	Loss/tok 3.1228 (3.1630)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1260/1938]	Time 0.339 (0.255)	Data 1.16e-04 (6.88e-04)	Tok/s 68184 (54881)	Loss/tok 3.3547 (3.1644)	LR 1.000e-03
0: TRAIN [3][1270/1938]	Time 0.213 (0.255)	Data 1.11e-04 (6.84e-04)	Tok/s 48267 (54848)	Loss/tok 2.9479 (3.1638)	LR 1.000e-03
0: TRAIN [3][1280/1938]	Time 0.277 (0.255)	Data 1.18e-04 (6.80e-04)	Tok/s 60863 (54855)	Loss/tok 3.1261 (3.1641)	LR 1.000e-03
0: TRAIN [3][1290/1938]	Time 0.214 (0.256)	Data 1.28e-04 (6.75e-04)	Tok/s 48158 (54869)	Loss/tok 2.9102 (3.1644)	LR 1.000e-03
0: TRAIN [3][1300/1938]	Time 0.336 (0.256)	Data 1.20e-04 (6.72e-04)	Tok/s 69636 (54902)	Loss/tok 3.3154 (3.1655)	LR 1.000e-03
0: TRAIN [3][1310/1938]	Time 0.337 (0.256)	Data 3.48e-04 (6.68e-04)	Tok/s 68995 (54913)	Loss/tok 3.2280 (3.1651)	LR 1.000e-03
0: TRAIN [3][1320/1938]	Time 0.280 (0.256)	Data 1.26e-04 (6.64e-04)	Tok/s 59500 (54947)	Loss/tok 3.1214 (3.1652)	LR 1.000e-03
0: TRAIN [3][1330/1938]	Time 0.213 (0.256)	Data 1.16e-04 (6.60e-04)	Tok/s 48874 (54968)	Loss/tok 2.9972 (3.1652)	LR 1.000e-03
0: TRAIN [3][1340/1938]	Time 0.213 (0.256)	Data 1.32e-04 (6.56e-04)	Tok/s 48478 (54919)	Loss/tok 2.9235 (3.1645)	LR 1.000e-03
0: TRAIN [3][1350/1938]	Time 0.213 (0.256)	Data 1.14e-04 (6.52e-04)	Tok/s 48157 (54934)	Loss/tok 2.9817 (3.1646)	LR 1.000e-03
0: TRAIN [3][1360/1938]	Time 0.214 (0.256)	Data 1.60e-04 (6.49e-04)	Tok/s 48611 (54927)	Loss/tok 2.9698 (3.1641)	LR 1.000e-03
0: TRAIN [3][1370/1938]	Time 0.157 (0.256)	Data 1.44e-04 (6.45e-04)	Tok/s 33645 (54917)	Loss/tok 2.6147 (3.1641)	LR 1.000e-03
0: TRAIN [3][1380/1938]	Time 0.277 (0.256)	Data 1.15e-04 (6.41e-04)	Tok/s 60832 (54912)	Loss/tok 3.0489 (3.1636)	LR 1.000e-03
0: TRAIN [3][1390/1938]	Time 0.338 (0.256)	Data 1.43e-04 (6.38e-04)	Tok/s 69848 (54919)	Loss/tok 3.2701 (3.1635)	LR 1.000e-03
0: TRAIN [3][1400/1938]	Time 0.213 (0.256)	Data 1.19e-04 (6.34e-04)	Tok/s 48451 (54941)	Loss/tok 3.0350 (3.1637)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1410/1938]	Time 0.409 (0.256)	Data 1.16e-04 (6.30e-04)	Tok/s 73157 (54995)	Loss/tok 3.4426 (3.1648)	LR 1.000e-03
0: TRAIN [3][1420/1938]	Time 0.411 (0.256)	Data 1.28e-04 (6.27e-04)	Tok/s 73060 (54964)	Loss/tok 3.4624 (3.1642)	LR 1.000e-03
0: TRAIN [3][1430/1938]	Time 0.338 (0.256)	Data 1.17e-04 (6.24e-04)	Tok/s 68742 (54985)	Loss/tok 3.2732 (3.1642)	LR 1.000e-03
0: TRAIN [3][1440/1938]	Time 0.213 (0.256)	Data 1.13e-04 (6.20e-04)	Tok/s 47904 (54958)	Loss/tok 2.9362 (3.1635)	LR 1.000e-03
0: TRAIN [3][1450/1938]	Time 0.275 (0.256)	Data 1.22e-04 (6.17e-04)	Tok/s 60926 (54972)	Loss/tok 3.0420 (3.1636)	LR 1.000e-03
0: TRAIN [3][1460/1938]	Time 0.278 (0.256)	Data 1.21e-04 (6.14e-04)	Tok/s 59676 (54969)	Loss/tok 3.1344 (3.1638)	LR 1.000e-03
0: TRAIN [3][1470/1938]	Time 0.213 (0.256)	Data 1.12e-04 (6.11e-04)	Tok/s 48728 (54940)	Loss/tok 2.9107 (3.1630)	LR 1.000e-03
0: TRAIN [3][1480/1938]	Time 0.277 (0.256)	Data 1.34e-04 (6.07e-04)	Tok/s 60578 (54943)	Loss/tok 3.0902 (3.1630)	LR 1.000e-03
0: TRAIN [3][1490/1938]	Time 0.214 (0.256)	Data 1.23e-04 (6.04e-04)	Tok/s 48264 (54934)	Loss/tok 2.9468 (3.1627)	LR 5.000e-04
0: TRAIN [3][1500/1938]	Time 0.214 (0.256)	Data 1.06e-04 (6.01e-04)	Tok/s 49305 (54926)	Loss/tok 2.8432 (3.1626)	LR 5.000e-04
0: TRAIN [3][1510/1938]	Time 0.214 (0.256)	Data 1.03e-04 (5.98e-04)	Tok/s 48466 (54926)	Loss/tok 2.9244 (3.1622)	LR 5.000e-04
0: TRAIN [3][1520/1938]	Time 0.410 (0.256)	Data 1.04e-04 (5.95e-04)	Tok/s 71868 (54929)	Loss/tok 3.6388 (3.1625)	LR 5.000e-04
0: TRAIN [3][1530/1938]	Time 0.213 (0.256)	Data 1.05e-04 (5.92e-04)	Tok/s 49221 (54937)	Loss/tok 2.9722 (3.1627)	LR 5.000e-04
0: TRAIN [3][1540/1938]	Time 0.414 (0.256)	Data 1.14e-04 (5.89e-04)	Tok/s 70902 (54915)	Loss/tok 3.5898 (3.1626)	LR 5.000e-04
0: TRAIN [3][1550/1938]	Time 0.214 (0.256)	Data 1.20e-04 (5.86e-04)	Tok/s 48074 (54924)	Loss/tok 2.8041 (3.1624)	LR 5.000e-04
0: TRAIN [3][1560/1938]	Time 0.340 (0.256)	Data 1.28e-04 (5.83e-04)	Tok/s 67761 (54925)	Loss/tok 3.2228 (3.1619)	LR 5.000e-04
0: TRAIN [3][1570/1938]	Time 0.212 (0.256)	Data 1.44e-04 (5.80e-04)	Tok/s 48033 (54919)	Loss/tok 2.9542 (3.1617)	LR 5.000e-04
0: TRAIN [3][1580/1938]	Time 0.337 (0.256)	Data 1.17e-04 (5.78e-04)	Tok/s 68932 (54888)	Loss/tok 3.4166 (3.1611)	LR 5.000e-04
0: TRAIN [3][1590/1938]	Time 0.213 (0.255)	Data 1.31e-04 (5.75e-04)	Tok/s 48649 (54842)	Loss/tok 2.9236 (3.1602)	LR 5.000e-04
0: TRAIN [3][1600/1938]	Time 0.275 (0.256)	Data 1.33e-04 (5.72e-04)	Tok/s 61047 (54858)	Loss/tok 3.0572 (3.1602)	LR 5.000e-04
0: TRAIN [3][1610/1938]	Time 0.213 (0.256)	Data 1.24e-04 (5.69e-04)	Tok/s 48072 (54864)	Loss/tok 2.9550 (3.1600)	LR 5.000e-04
0: TRAIN [3][1620/1938]	Time 0.215 (0.255)	Data 1.21e-04 (5.67e-04)	Tok/s 48785 (54822)	Loss/tok 2.9050 (3.1591)	LR 5.000e-04
0: TRAIN [3][1630/1938]	Time 0.213 (0.255)	Data 1.33e-04 (5.64e-04)	Tok/s 48382 (54817)	Loss/tok 2.8248 (3.1584)	LR 5.000e-04
0: TRAIN [3][1640/1938]	Time 0.158 (0.255)	Data 1.48e-04 (5.61e-04)	Tok/s 32849 (54823)	Loss/tok 2.5420 (3.1582)	LR 5.000e-04
0: TRAIN [3][1650/1938]	Time 0.278 (0.255)	Data 1.15e-04 (5.59e-04)	Tok/s 60726 (54839)	Loss/tok 3.1788 (3.1587)	LR 5.000e-04
0: TRAIN [3][1660/1938]	Time 0.157 (0.255)	Data 1.49e-04 (5.56e-04)	Tok/s 33572 (54836)	Loss/tok 2.4387 (3.1587)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1670/1938]	Time 0.213 (0.255)	Data 1.19e-04 (5.54e-04)	Tok/s 48665 (54826)	Loss/tok 2.8836 (3.1580)	LR 5.000e-04
0: TRAIN [3][1680/1938]	Time 0.214 (0.255)	Data 1.25e-04 (5.51e-04)	Tok/s 47395 (54815)	Loss/tok 2.8883 (3.1573)	LR 5.000e-04
0: TRAIN [3][1690/1938]	Time 0.278 (0.255)	Data 1.14e-04 (5.49e-04)	Tok/s 60187 (54803)	Loss/tok 3.0754 (3.1567)	LR 5.000e-04
0: TRAIN [3][1700/1938]	Time 0.213 (0.255)	Data 1.33e-04 (5.47e-04)	Tok/s 48648 (54818)	Loss/tok 2.8370 (3.1565)	LR 5.000e-04
0: TRAIN [3][1710/1938]	Time 0.277 (0.255)	Data 1.14e-04 (5.44e-04)	Tok/s 60255 (54836)	Loss/tok 3.1476 (3.1561)	LR 5.000e-04
0: TRAIN [3][1720/1938]	Time 0.338 (0.255)	Data 1.32e-04 (5.42e-04)	Tok/s 69156 (54837)	Loss/tok 3.2305 (3.1560)	LR 5.000e-04
0: TRAIN [3][1730/1938]	Time 0.277 (0.255)	Data 1.20e-04 (5.40e-04)	Tok/s 60125 (54806)	Loss/tok 3.0643 (3.1554)	LR 5.000e-04
0: TRAIN [3][1740/1938]	Time 0.214 (0.255)	Data 1.17e-04 (5.37e-04)	Tok/s 48581 (54828)	Loss/tok 2.8579 (3.1556)	LR 5.000e-04
0: TRAIN [3][1750/1938]	Time 0.214 (0.255)	Data 1.32e-04 (5.35e-04)	Tok/s 49267 (54834)	Loss/tok 2.9477 (3.1552)	LR 5.000e-04
0: TRAIN [3][1760/1938]	Time 0.411 (0.255)	Data 1.22e-04 (5.33e-04)	Tok/s 72794 (54874)	Loss/tok 3.5140 (3.1563)	LR 5.000e-04
0: TRAIN [3][1770/1938]	Time 0.214 (0.255)	Data 1.25e-04 (5.30e-04)	Tok/s 48153 (54862)	Loss/tok 3.0200 (3.1557)	LR 5.000e-04
0: TRAIN [3][1780/1938]	Time 0.276 (0.255)	Data 1.26e-04 (5.28e-04)	Tok/s 60531 (54879)	Loss/tok 3.1162 (3.1557)	LR 5.000e-04
0: TRAIN [3][1790/1938]	Time 0.277 (0.255)	Data 1.10e-04 (5.26e-04)	Tok/s 60626 (54884)	Loss/tok 3.0414 (3.1550)	LR 5.000e-04
0: TRAIN [3][1800/1938]	Time 0.214 (0.255)	Data 1.18e-04 (5.24e-04)	Tok/s 48917 (54852)	Loss/tok 2.9264 (3.1542)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1810/1938]	Time 0.213 (0.255)	Data 1.15e-04 (5.21e-04)	Tok/s 48486 (54834)	Loss/tok 2.8551 (3.1536)	LR 5.000e-04
0: TRAIN [3][1820/1938]	Time 0.214 (0.255)	Data 1.09e-04 (5.19e-04)	Tok/s 47602 (54853)	Loss/tok 2.9653 (3.1538)	LR 5.000e-04
0: TRAIN [3][1830/1938]	Time 0.214 (0.255)	Data 1.20e-04 (5.17e-04)	Tok/s 48060 (54849)	Loss/tok 3.0113 (3.1533)	LR 5.000e-04
0: TRAIN [3][1840/1938]	Time 0.213 (0.255)	Data 1.19e-04 (5.15e-04)	Tok/s 47492 (54839)	Loss/tok 2.9090 (3.1525)	LR 5.000e-04
0: TRAIN [3][1850/1938]	Time 0.276 (0.255)	Data 1.42e-04 (5.13e-04)	Tok/s 61131 (54845)	Loss/tok 3.0729 (3.1520)	LR 5.000e-04
0: TRAIN [3][1860/1938]	Time 0.214 (0.255)	Data 1.30e-04 (5.11e-04)	Tok/s 48798 (54852)	Loss/tok 2.9447 (3.1518)	LR 5.000e-04
0: TRAIN [3][1870/1938]	Time 0.214 (0.255)	Data 1.35e-04 (5.09e-04)	Tok/s 47668 (54845)	Loss/tok 2.8867 (3.1513)	LR 5.000e-04
0: TRAIN [3][1880/1938]	Time 0.274 (0.255)	Data 1.11e-04 (5.07e-04)	Tok/s 59827 (54859)	Loss/tok 3.3230 (3.1512)	LR 5.000e-04
0: TRAIN [3][1890/1938]	Time 0.156 (0.255)	Data 1.35e-04 (5.05e-04)	Tok/s 33550 (54835)	Loss/tok 2.4715 (3.1504)	LR 5.000e-04
0: TRAIN [3][1900/1938]	Time 0.277 (0.255)	Data 1.38e-04 (5.03e-04)	Tok/s 61131 (54836)	Loss/tok 3.0889 (3.1503)	LR 5.000e-04
0: TRAIN [3][1910/1938]	Time 0.276 (0.255)	Data 1.09e-04 (5.01e-04)	Tok/s 61217 (54837)	Loss/tok 3.0310 (3.1501)	LR 5.000e-04
0: TRAIN [3][1920/1938]	Time 0.276 (0.255)	Data 1.28e-04 (4.99e-04)	Tok/s 61048 (54857)	Loss/tok 3.0640 (3.1499)	LR 5.000e-04
0: TRAIN [3][1930/1938]	Time 0.213 (0.255)	Data 1.11e-04 (4.97e-04)	Tok/s 48529 (54849)	Loss/tok 2.9267 (3.1491)	LR 5.000e-04
:::MLL 1570037013.882 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 524}}
:::MLL 1570037013.882 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.641 (0.641)	Decoder iters 102.0 (102.0)	Tok/s 25532 (25532)
0: Running moses detokenizer
0: BLEU(score=24.372417946636304, counts=[37253, 18760, 10783, 6457], totals=[65534, 62531, 59528, 56531], precisions=[56.84530167546617, 30.0011194447554, 18.114164762800698, 11.422051617696486], bp=1.0, sys_len=65534, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1570037015.742 eval_accuracy: {"value": 24.37, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 535}}
:::MLL 1570037015.743 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 3	Training Loss: 3.1468	Test BLEU: 24.37
0: Performance: Epoch: 3	Training: 438708 Tok/s
0: Finished epoch 3
:::MLL 1570037015.743 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 557}}
0: Closing preprocessed data file
:::MLL 1570037015.744 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 568}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-10-02 05:23:47 PM
RESULT,RNN_TRANSLATOR,,2027,nvidia,2019-10-02 04:50:00 PM
