Beginning trial 1 of 2
Gathering sys log on dss01
:::MLL 1571243877.419 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1571243877.420 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1571243877.420 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1571243877.421 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1571243877.421 submission_platform: {"value": "1xDSS8440", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1571243877.422 submission_entry: {"value": "{'hardware': 'DSS8440', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '5.0-0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz', 'num_cores': '40', 'num_vcpus': '40', 'accelerator': 'Tesla V100-PCIE-32GB', 'num_accelerators': '8', 'sys_mem_size': '754 GB', 'sys_storage_type': 'SATA SSD', 'sys_storage_size': '1x 447.1G + 1x 931.5G', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27700 Family [ConnectX-4]', 'num_network_cards': '1', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1571243877.422 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1571243877.423 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1571243882.782 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node dss01
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=4928' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=256 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=191016113618654615341 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191016113618654615341 ./run_and_time.sh
Run vars: id 191016113618654615341 gpus 8 mparams  --master_port=4928
NCCL_SOCKET_NTHREADS=2
NCCL_NSOCKS_PERTHREAD=8
STARTING TIMING RUN AT 2019-10-16 04:38:03 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=256
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=0
+ MATH=amp_fp16
+ [[ 0 -eq 1 ]]
+ LAUNCH_OPT='torch.distributed.launch --nproc_per_node 8  --master_port=4928'
+ echo 'running benchmark'
running benchmark
+ python -m torch.distributed.launch --nproc_per_node 8 --master_port=4928 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 256 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1571243885.503 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571243885.504 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571243885.505 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571243885.505 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571243885.506 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571243885.506 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571243885.508 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571243885.508 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=256, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 660714895
dss01:465:465 [0] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:465:465 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:465:465 [0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:465:465 [0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:465:465 [0] NCCL INFO NET/IB : No device found.
dss01:465:465 [0] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
NCCL version 2.4.8+cuda10.1
dss01:472:472 [7] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:472:472 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:466:466 [1] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:466:466 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:468:468 [3] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:468:468 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:471:471 [6] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:471:471 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:469:469 [4] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:469:469 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:467:467 [2] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:467:467 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:472:472 [7] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:472:472 [7] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:472:472 [7] NCCL INFO NET/IB : No device found.

dss01:468:468 [3] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:468:468 [3] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:468:468 [3] NCCL INFO NET/IB : No device found.

dss01:466:466 [1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:466:466 [1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:466:466 [1] NCCL INFO NET/IB : No device found.

dss01:471:471 [6] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:471:471 [6] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:471:471 [6] NCCL INFO NET/IB : No device found.

dss01:469:469 [4] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:469:469 [4] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:469:469 [4] NCCL INFO NET/IB : No device found.

dss01:467:467 [2] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:467:467 [2] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:467:467 [2] NCCL INFO NET/IB : No device found.
dss01:468:468 [3] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:466:466 [1] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:472:472 [7] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:471:471 [6] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:469:469 [4] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:467:467 [2] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:470:470 [5] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:470:470 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:470:470 [5] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:470:470 [5] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:470:470 [5] NCCL INFO NET/IB : No device found.
dss01:470:470 [5] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:465:827 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555
dss01:466:828 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555
dss01:468:830 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555
dss01:472:829 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa
dss01:467:833 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555
dss01:471:831 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa
dss01:469:832 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa
dss01:470:834 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa
dss01:470:834 [5] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:471:831 [6] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:472:829 [7] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:465:827 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:466:828 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:467:833 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:468:830 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:469:832 [4] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:465:827 [0] NCCL INFO Channel 00 :    0   1   2   3   4   5   6   7
dss01:469:832 [4] NCCL INFO Ring 00 : 4[4] -> 5[5] via P2P/IPC
dss01:471:831 [6] NCCL INFO Ring 00 : 6[6] -> 7[7] via P2P/IPC
dss01:465:827 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/IPC
dss01:467:833 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/IPC
dss01:468:830 [3] NCCL INFO Ring 00 : 3[3] -> 4[4] via direct shared memory
dss01:470:834 [5] NCCL INFO Ring 00 : 5[5] -> 6[6] via direct shared memory
dss01:472:829 [7] NCCL INFO Ring 00 : 7[7] -> 0[0] via direct shared memory
dss01:466:828 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via direct shared memory
dss01:465:827 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
dss01:466:828 [1] NCCL INFO comm 0x7fff34007590 rank 1 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE
dss01:470:834 [5] NCCL INFO comm 0x7ffe94007590 rank 5 nranks 8 cudaDev 5 nvmlDev 5 - Init COMPLETE
dss01:472:829 [7] NCCL INFO comm 0x7ffe94007590 rank 7 nranks 8 cudaDev 7 nvmlDev 7 - Init COMPLETE
dss01:468:830 [3] NCCL INFO comm 0x7fff50007590 rank 3 nranks 8 cudaDev 3 nvmlDev 3 - Init COMPLETE
dss01:469:832 [4] NCCL INFO comm 0x7ffe94007590 rank 4 nranks 8 cudaDev 4 nvmlDev 4 - Init COMPLETE
dss01:471:831 [6] NCCL INFO comm 0x7ffe94007590 rank 6 nranks 8 cudaDev 6 nvmlDev 6 - Init COMPLETE
dss01:465:827 [0] NCCL INFO comm 0x7ffe38007590 rank 0 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE
dss01:465:465 [0] NCCL INFO Launch mode Parallel
0: Worker 0 is using worker seed: 286377162
0: Building vocabulary from /data/vocab.bpe.32000
dss01:467:833 [2] NCCL INFO comm 0x7ffe60007590 rank 2 nranks 8 cudaDev 2 nvmlDev 2 - Init COMPLETE
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1571243909.054 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1571243911.912 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1571243911.912 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1571243911.913 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1571243912.938 global_batch_size: {"value": 2048, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1571243912.940 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1571243912.940 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1571243912.940 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1571243912.941 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1571243912.941 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1571243912.941 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1571243912.942 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1571243912.963 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571243912.963 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 2511810718
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/1938]	Time 1.016 (1.016)	Data 7.56e-01 (7.56e-01)	Tok/s 10146 (10146)	Loss/tok 10.5510 (10.5510)	LR 2.000e-05
0: TRAIN [0][10/1938]	Time 0.215 (0.278)	Data 1.02e-04 (6.88e-02)	Tok/s 48590 (41914)	Loss/tok 9.6691 (10.0651)	LR 2.518e-05
0: TRAIN [0][20/1938]	Time 0.215 (0.263)	Data 1.10e-04 (3.61e-02)	Tok/s 48781 (46449)	Loss/tok 9.3094 (9.7947)	LR 3.170e-05
0: TRAIN [0][30/1938]	Time 0.272 (0.257)	Data 2.64e-04 (2.45e-02)	Tok/s 62170 (48839)	Loss/tok 9.0985 (9.5832)	LR 3.991e-05
0: TRAIN [0][40/1938]	Time 0.274 (0.260)	Data 2.08e-04 (1.86e-02)	Tok/s 61531 (50401)	Loss/tok 8.9512 (9.3973)	LR 5.024e-05
0: TRAIN [0][50/1938]	Time 0.215 (0.262)	Data 1.05e-04 (1.50e-02)	Tok/s 47170 (51983)	Loss/tok 8.5092 (9.2478)	LR 6.325e-05
0: TRAIN [0][60/1938]	Time 0.215 (0.257)	Data 1.11e-04 (1.25e-02)	Tok/s 48610 (51943)	Loss/tok 8.1989 (9.1228)	LR 7.962e-05
0: TRAIN [0][70/1938]	Time 0.331 (0.258)	Data 1.02e-04 (1.08e-02)	Tok/s 70236 (52607)	Loss/tok 8.3189 (8.9927)	LR 1.002e-04
0: TRAIN [0][80/1938]	Time 0.215 (0.256)	Data 1.07e-04 (9.46e-03)	Tok/s 47763 (52643)	Loss/tok 7.8965 (8.8866)	LR 1.262e-04
0: TRAIN [0][90/1938]	Time 0.214 (0.257)	Data 1.08e-04 (8.43e-03)	Tok/s 48596 (53316)	Loss/tok 7.7497 (8.7795)	LR 1.589e-04
0: TRAIN [0][100/1938]	Time 0.404 (0.258)	Data 1.09e-04 (7.61e-03)	Tok/s 72990 (53659)	Loss/tok 8.1172 (8.6902)	LR 2.000e-04
0: TRAIN [0][110/1938]	Time 0.273 (0.259)	Data 1.13e-04 (6.93e-03)	Tok/s 61387 (54264)	Loss/tok 7.9259 (8.6085)	LR 2.518e-04
0: TRAIN [0][120/1938]	Time 0.274 (0.259)	Data 1.08e-04 (6.37e-03)	Tok/s 61201 (54602)	Loss/tok 7.9792 (8.5437)	LR 3.170e-04
0: TRAIN [0][130/1938]	Time 0.161 (0.259)	Data 1.47e-04 (5.89e-03)	Tok/s 32798 (54693)	Loss/tok 7.1700 (8.4898)	LR 3.991e-04
0: TRAIN [0][140/1938]	Time 0.272 (0.258)	Data 1.35e-04 (5.48e-03)	Tok/s 61132 (54570)	Loss/tok 7.8547 (8.4409)	LR 5.024e-04
0: TRAIN [0][150/1938]	Time 0.332 (0.260)	Data 1.63e-04 (5.13e-03)	Tok/s 69966 (55053)	Loss/tok 7.8608 (8.3898)	LR 6.325e-04
0: TRAIN [0][160/1938]	Time 0.404 (0.263)	Data 1.24e-04 (4.82e-03)	Tok/s 74329 (55780)	Loss/tok 7.8953 (8.3351)	LR 7.962e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][170/1938]	Time 0.214 (0.262)	Data 1.90e-04 (4.54e-03)	Tok/s 48043 (55707)	Loss/tok 7.4403 (8.2940)	LR 9.573e-04
0: TRAIN [0][180/1938]	Time 0.215 (0.262)	Data 1.27e-04 (4.30e-03)	Tok/s 49225 (55761)	Loss/tok 7.2078 (8.2469)	LR 1.205e-03
0: TRAIN [0][190/1938]	Time 0.332 (0.262)	Data 1.89e-04 (4.08e-03)	Tok/s 70743 (55785)	Loss/tok 7.2768 (8.1963)	LR 1.517e-03
0: TRAIN [0][200/1938]	Time 0.214 (0.262)	Data 1.19e-04 (3.88e-03)	Tok/s 48138 (55863)	Loss/tok 7.1478 (8.1426)	LR 1.910e-03
0: TRAIN [0][210/1938]	Time 0.273 (0.262)	Data 3.18e-04 (3.70e-03)	Tok/s 60972 (55950)	Loss/tok 7.0278 (8.0952)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.214 (0.262)	Data 1.18e-04 (3.54e-03)	Tok/s 48834 (55989)	Loss/tok 6.5792 (8.0409)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.274 (0.262)	Data 1.27e-04 (3.39e-03)	Tok/s 61293 (56101)	Loss/tok 6.6495 (7.9835)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.215 (0.262)	Data 1.11e-04 (3.26e-03)	Tok/s 47565 (56013)	Loss/tok 6.3518 (7.9296)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.273 (0.264)	Data 1.18e-04 (3.13e-03)	Tok/s 60585 (56256)	Loss/tok 6.4636 (7.8609)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.274 (0.263)	Data 1.32e-04 (3.02e-03)	Tok/s 61648 (56219)	Loss/tok 6.3442 (7.8017)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.215 (0.262)	Data 1.19e-04 (2.91e-03)	Tok/s 47413 (56061)	Loss/tok 5.9376 (7.7507)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.215 (0.262)	Data 9.54e-05 (2.81e-03)	Tok/s 48273 (56054)	Loss/tok 5.8467 (7.6956)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.215 (0.261)	Data 1.02e-04 (2.72e-03)	Tok/s 47776 (55845)	Loss/tok 5.7687 (7.6472)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.273 (0.261)	Data 1.10e-04 (2.63e-03)	Tok/s 61807 (55912)	Loss/tok 6.0577 (7.5883)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.160 (0.261)	Data 1.11e-04 (2.55e-03)	Tok/s 33311 (55738)	Loss/tok 4.5687 (7.5369)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.215 (0.261)	Data 1.27e-04 (2.47e-03)	Tok/s 47525 (55723)	Loss/tok 5.5211 (7.4805)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.406 (0.261)	Data 1.06e-04 (2.40e-03)	Tok/s 74016 (55762)	Loss/tok 5.8970 (7.4237)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.215 (0.260)	Data 1.11e-04 (2.34e-03)	Tok/s 48446 (55561)	Loss/tok 5.2321 (7.3787)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.160 (0.258)	Data 1.00e-04 (2.27e-03)	Tok/s 32860 (55281)	Loss/tok 4.0950 (7.3378)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.333 (0.259)	Data 1.04e-04 (2.21e-03)	Tok/s 70130 (55370)	Loss/tok 5.5931 (7.2799)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.215 (0.258)	Data 1.02e-04 (2.16e-03)	Tok/s 48739 (55345)	Loss/tok 4.9851 (7.2298)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.215 (0.258)	Data 1.01e-04 (2.10e-03)	Tok/s 46251 (55290)	Loss/tok 4.8699 (7.1818)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.214 (0.258)	Data 1.03e-04 (2.05e-03)	Tok/s 48473 (55322)	Loss/tok 4.7573 (7.1294)	LR 2.000e-03
0: TRAIN [0][400/1938]	Time 0.161 (0.258)	Data 1.03e-04 (2.00e-03)	Tok/s 33239 (55324)	Loss/tok 4.0679 (7.0794)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.405 (0.258)	Data 1.20e-04 (1.96e-03)	Tok/s 74047 (55414)	Loss/tok 5.3407 (7.0242)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.274 (0.259)	Data 9.80e-05 (1.91e-03)	Tok/s 60689 (55544)	Loss/tok 5.0361 (6.9665)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.405 (0.260)	Data 1.10e-04 (1.87e-03)	Tok/s 72657 (55708)	Loss/tok 5.2305 (6.9065)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.215 (0.260)	Data 1.01e-04 (1.83e-03)	Tok/s 48037 (55721)	Loss/tok 4.4220 (6.8580)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.215 (0.260)	Data 1.22e-04 (1.79e-03)	Tok/s 48112 (55758)	Loss/tok 4.4535 (6.8094)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.215 (0.260)	Data 1.06e-04 (1.76e-03)	Tok/s 47800 (55697)	Loss/tok 4.4142 (6.7679)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.274 (0.259)	Data 1.08e-04 (1.72e-03)	Tok/s 61382 (55691)	Loss/tok 4.5693 (6.7235)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.161 (0.259)	Data 1.01e-04 (1.69e-03)	Tok/s 32930 (55673)	Loss/tok 3.6884 (6.6797)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.216 (0.259)	Data 1.16e-04 (1.66e-03)	Tok/s 47325 (55699)	Loss/tok 4.1189 (6.6348)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.274 (0.260)	Data 1.07e-04 (1.63e-03)	Tok/s 61602 (55780)	Loss/tok 4.5224 (6.5879)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.274 (0.259)	Data 1.09e-04 (1.60e-03)	Tok/s 61112 (55766)	Loss/tok 4.4714 (6.5486)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.215 (0.259)	Data 1.03e-04 (1.57e-03)	Tok/s 48638 (55758)	Loss/tok 4.0328 (6.5090)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.215 (0.259)	Data 1.12e-04 (1.54e-03)	Tok/s 48093 (55740)	Loss/tok 4.0999 (6.4701)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.216 (0.259)	Data 2.47e-04 (1.52e-03)	Tok/s 47882 (55681)	Loss/tok 4.1282 (6.4350)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.274 (0.259)	Data 1.07e-04 (1.49e-03)	Tok/s 60856 (55644)	Loss/tok 4.5543 (6.3988)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.215 (0.259)	Data 1.07e-04 (1.47e-03)	Tok/s 48263 (55697)	Loss/tok 3.9191 (6.3588)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.406 (0.259)	Data 1.27e-04 (1.44e-03)	Tok/s 74220 (55724)	Loss/tok 4.6737 (6.3202)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.215 (0.259)	Data 1.04e-04 (1.42e-03)	Tok/s 48269 (55623)	Loss/tok 3.9405 (6.2893)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.216 (0.259)	Data 1.22e-04 (1.40e-03)	Tok/s 46475 (55626)	Loss/tok 3.8880 (6.2550)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.334 (0.259)	Data 1.18e-04 (1.38e-03)	Tok/s 70815 (55709)	Loss/tok 4.4516 (6.2165)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.215 (0.259)	Data 1.15e-04 (1.36e-03)	Tok/s 47644 (55654)	Loss/tok 3.8951 (6.1857)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][620/1938]	Time 0.274 (0.259)	Data 1.32e-04 (1.34e-03)	Tok/s 61891 (55653)	Loss/tok 4.1441 (6.1530)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.216 (0.259)	Data 1.03e-04 (1.32e-03)	Tok/s 48203 (55716)	Loss/tok 3.8641 (6.1173)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.161 (0.259)	Data 1.04e-04 (1.30e-03)	Tok/s 32802 (55676)	Loss/tok 3.2385 (6.0884)	LR 2.000e-03
0: TRAIN [0][650/1938]	Time 0.334 (0.259)	Data 1.25e-04 (1.28e-03)	Tok/s 69324 (55703)	Loss/tok 4.3033 (6.0571)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.161 (0.259)	Data 1.07e-04 (1.26e-03)	Tok/s 31711 (55713)	Loss/tok 3.1666 (6.0262)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.335 (0.259)	Data 1.13e-04 (1.25e-03)	Tok/s 69217 (55727)	Loss/tok 4.4592 (5.9970)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.215 (0.259)	Data 1.02e-04 (1.23e-03)	Tok/s 47936 (55702)	Loss/tok 3.9010 (5.9709)	LR 2.000e-03
0: TRAIN [0][690/1938]	Time 0.216 (0.259)	Data 1.06e-04 (1.22e-03)	Tok/s 47589 (55694)	Loss/tok 3.8481 (5.9443)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.407 (0.259)	Data 2.21e-04 (1.20e-03)	Tok/s 74570 (55707)	Loss/tok 4.4541 (5.9169)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.334 (0.259)	Data 1.20e-04 (1.19e-03)	Tok/s 69205 (55714)	Loss/tok 4.2727 (5.8905)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.216 (0.259)	Data 1.17e-04 (1.17e-03)	Tok/s 47008 (55724)	Loss/tok 3.6087 (5.8642)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.215 (0.259)	Data 1.45e-04 (1.16e-03)	Tok/s 48125 (55612)	Loss/tok 3.7526 (5.8440)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.216 (0.258)	Data 1.16e-04 (1.14e-03)	Tok/s 47824 (55559)	Loss/tok 3.6786 (5.8212)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.215 (0.258)	Data 9.89e-05 (1.13e-03)	Tok/s 47810 (55558)	Loss/tok 3.6578 (5.7970)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.333 (0.258)	Data 1.50e-04 (1.12e-03)	Tok/s 69528 (55557)	Loss/tok 4.1442 (5.7735)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.274 (0.258)	Data 1.14e-04 (1.10e-03)	Tok/s 61202 (55536)	Loss/tok 4.0611 (5.7517)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.274 (0.258)	Data 9.89e-05 (1.09e-03)	Tok/s 60745 (55522)	Loss/tok 3.9387 (5.7297)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.335 (0.258)	Data 1.25e-04 (1.08e-03)	Tok/s 69223 (55569)	Loss/tok 4.0774 (5.7043)	LR 2.000e-03
0: TRAIN [0][800/1938]	Time 0.275 (0.259)	Data 1.22e-04 (1.07e-03)	Tok/s 60803 (55610)	Loss/tok 3.8621 (5.6805)	LR 2.000e-03
0: TRAIN [0][810/1938]	Time 0.215 (0.259)	Data 1.31e-04 (1.06e-03)	Tok/s 48902 (55640)	Loss/tok 3.5320 (5.6579)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.216 (0.258)	Data 1.00e-04 (1.04e-03)	Tok/s 47921 (55518)	Loss/tok 3.6483 (5.6421)	LR 2.000e-03
0: TRAIN [0][830/1938]	Time 0.215 (0.258)	Data 1.16e-04 (1.03e-03)	Tok/s 48307 (55484)	Loss/tok 3.6600 (5.6230)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.215 (0.258)	Data 1.07e-04 (1.02e-03)	Tok/s 46639 (55495)	Loss/tok 3.6731 (5.6020)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.215 (0.258)	Data 1.34e-04 (1.01e-03)	Tok/s 48423 (55411)	Loss/tok 3.6316 (5.5851)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.215 (0.257)	Data 1.08e-04 (1.00e-03)	Tok/s 48007 (55376)	Loss/tok 3.5756 (5.5673)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.215 (0.257)	Data 1.04e-04 (9.92e-04)	Tok/s 48704 (55345)	Loss/tok 3.7227 (5.5503)	LR 2.000e-03
0: TRAIN [0][880/1938]	Time 0.216 (0.257)	Data 1.17e-04 (9.82e-04)	Tok/s 48457 (55332)	Loss/tok 3.5431 (5.5323)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.215 (0.257)	Data 1.04e-04 (9.73e-04)	Tok/s 47541 (55270)	Loss/tok 3.6049 (5.5161)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.216 (0.256)	Data 1.07e-04 (9.63e-04)	Tok/s 48375 (55210)	Loss/tok 3.6602 (5.5007)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.275 (0.256)	Data 1.20e-04 (9.54e-04)	Tok/s 60946 (55179)	Loss/tok 3.7398 (5.4840)	LR 2.000e-03
0: TRAIN [0][920/1938]	Time 0.274 (0.257)	Data 1.18e-04 (9.45e-04)	Tok/s 61402 (55248)	Loss/tok 3.7787 (5.4633)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.215 (0.256)	Data 1.06e-04 (9.37e-04)	Tok/s 48644 (55219)	Loss/tok 3.5938 (5.4478)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.215 (0.256)	Data 1.05e-04 (9.28e-04)	Tok/s 48257 (55215)	Loss/tok 3.5526 (5.4312)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.160 (0.256)	Data 1.42e-04 (9.20e-04)	Tok/s 33220 (55190)	Loss/tok 3.0169 (5.4157)	LR 2.000e-03
0: TRAIN [0][960/1938]	Time 0.335 (0.256)	Data 1.11e-04 (9.11e-04)	Tok/s 69910 (55207)	Loss/tok 4.0369 (5.3986)	LR 2.000e-03
0: TRAIN [0][970/1938]	Time 0.217 (0.257)	Data 1.12e-04 (9.03e-04)	Tok/s 47707 (55229)	Loss/tok 3.5740 (5.3823)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.216 (0.257)	Data 1.10e-04 (8.95e-04)	Tok/s 48119 (55237)	Loss/tok 3.6393 (5.3660)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.216 (0.257)	Data 1.27e-04 (8.88e-04)	Tok/s 47716 (55275)	Loss/tok 3.4346 (5.3489)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.216 (0.257)	Data 1.37e-04 (8.80e-04)	Tok/s 48284 (55237)	Loss/tok 3.5949 (5.3350)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.409 (0.257)	Data 1.05e-04 (8.73e-04)	Tok/s 72207 (55263)	Loss/tok 4.1721 (5.3192)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.335 (0.257)	Data 1.03e-04 (8.65e-04)	Tok/s 68999 (55207)	Loss/tok 4.0425 (5.3062)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.216 (0.257)	Data 1.07e-04 (8.58e-04)	Tok/s 48646 (55207)	Loss/tok 3.5370 (5.2915)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.160 (0.257)	Data 1.15e-04 (8.51e-04)	Tok/s 33045 (55184)	Loss/tok 2.9857 (5.2784)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.335 (0.256)	Data 1.11e-04 (8.44e-04)	Tok/s 69323 (55158)	Loss/tok 3.9628 (5.2651)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.275 (0.256)	Data 1.09e-04 (8.38e-04)	Tok/s 60290 (55129)	Loss/tok 3.9401 (5.2525)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.217 (0.256)	Data 9.08e-05 (8.31e-04)	Tok/s 48247 (55045)	Loss/tok 3.4531 (5.2416)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.275 (0.256)	Data 9.80e-05 (8.24e-04)	Tok/s 61903 (55001)	Loss/tok 3.7315 (5.2298)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1090/1938]	Time 0.216 (0.255)	Data 1.25e-04 (8.18e-04)	Tok/s 48002 (54981)	Loss/tok 3.5507 (5.2174)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.275 (0.256)	Data 1.13e-04 (8.11e-04)	Tok/s 60721 (55014)	Loss/tok 3.7992 (5.2030)	LR 2.000e-03
0: TRAIN [0][1110/1938]	Time 0.216 (0.255)	Data 1.01e-04 (8.05e-04)	Tok/s 47803 (54971)	Loss/tok 3.5821 (5.1914)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.275 (0.255)	Data 9.89e-05 (7.99e-04)	Tok/s 61026 (54978)	Loss/tok 3.6758 (5.1787)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.215 (0.255)	Data 1.12e-04 (7.93e-04)	Tok/s 47642 (54967)	Loss/tok 3.4746 (5.1661)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.274 (0.255)	Data 1.09e-04 (7.87e-04)	Tok/s 61370 (54935)	Loss/tok 3.8700 (5.1550)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1150/1938]	Time 0.272 (0.255)	Data 1.17e-04 (7.81e-04)	Tok/s 61642 (54922)	Loss/tok 3.8112 (5.1430)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.406 (0.255)	Data 1.14e-04 (7.76e-04)	Tok/s 72731 (54911)	Loss/tok 4.1396 (5.1316)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.275 (0.255)	Data 1.12e-04 (7.70e-04)	Tok/s 61309 (54908)	Loss/tok 3.6973 (5.1201)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.407 (0.255)	Data 1.25e-04 (7.65e-04)	Tok/s 73727 (54918)	Loss/tok 4.1997 (5.1080)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.274 (0.255)	Data 1.30e-04 (7.59e-04)	Tok/s 60894 (54919)	Loss/tok 3.8266 (5.0963)	LR 2.000e-03
0: TRAIN [0][1200/1938]	Time 0.275 (0.255)	Data 1.05e-04 (7.54e-04)	Tok/s 61002 (54890)	Loss/tok 3.6621 (5.0857)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.216 (0.255)	Data 1.02e-04 (7.48e-04)	Tok/s 48688 (54914)	Loss/tok 3.5028 (5.0732)	LR 2.000e-03
0: TRAIN [0][1220/1938]	Time 0.275 (0.255)	Data 1.70e-04 (7.43e-04)	Tok/s 61128 (54874)	Loss/tok 3.7627 (5.0634)	LR 2.000e-03
0: TRAIN [0][1230/1938]	Time 0.275 (0.255)	Data 1.15e-04 (7.39e-04)	Tok/s 60898 (54869)	Loss/tok 3.8498 (5.0529)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.216 (0.255)	Data 1.15e-04 (7.34e-04)	Tok/s 47699 (54834)	Loss/tok 3.4048 (5.0432)	LR 2.000e-03
0: TRAIN [0][1250/1938]	Time 0.216 (0.255)	Data 9.99e-05 (7.29e-04)	Tok/s 48479 (54809)	Loss/tok 3.4885 (5.0334)	LR 2.000e-03
0: TRAIN [0][1260/1938]	Time 0.216 (0.255)	Data 9.80e-05 (7.24e-04)	Tok/s 47751 (54798)	Loss/tok 3.5030 (5.0228)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.275 (0.255)	Data 1.02e-04 (7.20e-04)	Tok/s 60700 (54765)	Loss/tok 3.6354 (5.0134)	LR 2.000e-03
0: TRAIN [0][1280/1938]	Time 0.216 (0.254)	Data 3.52e-04 (7.15e-04)	Tok/s 47928 (54746)	Loss/tok 3.4804 (5.0039)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.216 (0.254)	Data 1.11e-04 (7.11e-04)	Tok/s 47214 (54755)	Loss/tok 3.4224 (4.9932)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.274 (0.255)	Data 1.11e-04 (7.07e-04)	Tok/s 61084 (54780)	Loss/tok 3.5924 (4.9825)	LR 2.000e-03
0: TRAIN [0][1310/1938]	Time 0.216 (0.255)	Data 1.31e-04 (7.02e-04)	Tok/s 48254 (54778)	Loss/tok 3.4457 (4.9725)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.215 (0.254)	Data 1.29e-04 (6.98e-04)	Tok/s 48033 (54750)	Loss/tok 3.2945 (4.9633)	LR 2.000e-03
0: TRAIN [0][1330/1938]	Time 0.216 (0.254)	Data 9.92e-05 (6.93e-04)	Tok/s 48487 (54734)	Loss/tok 3.5466 (4.9544)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.335 (0.254)	Data 9.51e-05 (6.89e-04)	Tok/s 69825 (54714)	Loss/tok 3.8370 (4.9452)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.216 (0.254)	Data 1.01e-04 (6.85e-04)	Tok/s 47204 (54688)	Loss/tok 3.2877 (4.9368)	LR 2.000e-03
0: TRAIN [0][1360/1938]	Time 0.216 (0.254)	Data 1.02e-04 (6.80e-04)	Tok/s 47613 (54667)	Loss/tok 3.4280 (4.9281)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.275 (0.254)	Data 1.03e-04 (6.76e-04)	Tok/s 60946 (54695)	Loss/tok 3.6676 (4.9181)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1380/1938]	Time 0.274 (0.254)	Data 9.80e-05 (6.72e-04)	Tok/s 61910 (54709)	Loss/tok 3.7103 (4.9087)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.275 (0.254)	Data 1.01e-04 (6.68e-04)	Tok/s 60798 (54689)	Loss/tok 3.7457 (4.9002)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.334 (0.254)	Data 1.15e-04 (6.64e-04)	Tok/s 70155 (54643)	Loss/tok 3.8181 (4.8925)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.275 (0.254)	Data 9.99e-05 (6.60e-04)	Tok/s 61030 (54689)	Loss/tok 3.7115 (4.8823)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.215 (0.254)	Data 1.21e-04 (6.56e-04)	Tok/s 47432 (54721)	Loss/tok 3.3636 (4.8729)	LR 2.000e-03
0: TRAIN [0][1430/1938]	Time 0.216 (0.254)	Data 1.25e-04 (6.52e-04)	Tok/s 47656 (54715)	Loss/tok 3.4754 (4.8645)	LR 2.000e-03
0: TRAIN [0][1440/1938]	Time 0.216 (0.254)	Data 1.12e-04 (6.49e-04)	Tok/s 46448 (54730)	Loss/tok 3.3694 (4.8556)	LR 2.000e-03
0: TRAIN [0][1450/1938]	Time 0.216 (0.254)	Data 9.63e-05 (6.45e-04)	Tok/s 47610 (54734)	Loss/tok 3.5512 (4.8474)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.274 (0.254)	Data 9.70e-05 (6.42e-04)	Tok/s 60359 (54757)	Loss/tok 3.7246 (4.8386)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.274 (0.254)	Data 1.15e-04 (6.38e-04)	Tok/s 61137 (54760)	Loss/tok 3.7209 (4.8302)	LR 2.000e-03
0: TRAIN [0][1480/1938]	Time 0.334 (0.254)	Data 1.03e-04 (6.34e-04)	Tok/s 70752 (54740)	Loss/tok 3.8616 (4.8227)	LR 2.000e-03
0: TRAIN [0][1490/1938]	Time 0.276 (0.255)	Data 1.03e-04 (6.31e-04)	Tok/s 60974 (54783)	Loss/tok 3.6486 (4.8132)	LR 2.000e-03
0: TRAIN [0][1500/1938]	Time 0.274 (0.254)	Data 1.20e-04 (6.28e-04)	Tok/s 60913 (54786)	Loss/tok 3.6710 (4.8051)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.406 (0.255)	Data 1.03e-04 (6.24e-04)	Tok/s 74249 (54816)	Loss/tok 3.9235 (4.7967)	LR 2.000e-03
0: TRAIN [0][1520/1938]	Time 0.216 (0.255)	Data 1.29e-04 (6.21e-04)	Tok/s 48027 (54801)	Loss/tok 3.3622 (4.7893)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.216 (0.254)	Data 1.13e-04 (6.17e-04)	Tok/s 48286 (54767)	Loss/tok 3.4720 (4.7826)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.406 (0.254)	Data 1.03e-04 (6.14e-04)	Tok/s 71693 (54777)	Loss/tok 4.1706 (4.7749)	LR 2.000e-03
0: TRAIN [0][1550/1938]	Time 0.277 (0.255)	Data 1.56e-04 (6.11e-04)	Tok/s 60669 (54807)	Loss/tok 3.4945 (4.7669)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.159 (0.255)	Data 1.10e-04 (6.08e-04)	Tok/s 33655 (54805)	Loss/tok 2.8628 (4.7598)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.216 (0.255)	Data 1.19e-04 (6.05e-04)	Tok/s 47693 (54798)	Loss/tok 3.2927 (4.7524)	LR 2.000e-03
0: TRAIN [0][1580/1938]	Time 0.216 (0.255)	Data 1.15e-04 (6.02e-04)	Tok/s 48101 (54778)	Loss/tok 3.3800 (4.7456)	LR 2.000e-03
0: TRAIN [0][1590/1938]	Time 0.276 (0.255)	Data 1.14e-04 (5.99e-04)	Tok/s 60426 (54815)	Loss/tok 3.6851 (4.7374)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.275 (0.255)	Data 1.34e-04 (5.96e-04)	Tok/s 61544 (54802)	Loss/tok 3.5245 (4.7306)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.215 (0.255)	Data 1.19e-04 (5.93e-04)	Tok/s 47011 (54801)	Loss/tok 3.4235 (4.7236)	LR 2.000e-03
0: TRAIN [0][1620/1938]	Time 0.407 (0.255)	Data 1.03e-04 (5.90e-04)	Tok/s 73828 (54789)	Loss/tok 4.0267 (4.7169)	LR 2.000e-03
0: TRAIN [0][1630/1938]	Time 0.215 (0.255)	Data 9.80e-05 (5.87e-04)	Tok/s 48358 (54814)	Loss/tok 3.4853 (4.7093)	LR 2.000e-03
0: TRAIN [0][1640/1938]	Time 0.215 (0.255)	Data 1.18e-04 (5.84e-04)	Tok/s 47975 (54801)	Loss/tok 3.3491 (4.7028)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.275 (0.255)	Data 9.82e-05 (5.81e-04)	Tok/s 61044 (54842)	Loss/tok 3.6848 (4.6948)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.216 (0.255)	Data 1.17e-04 (5.78e-04)	Tok/s 47218 (54819)	Loss/tok 3.3300 (4.6887)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.216 (0.255)	Data 1.03e-04 (5.76e-04)	Tok/s 47274 (54857)	Loss/tok 3.3433 (4.6811)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1680/1938]	Time 0.334 (0.255)	Data 1.01e-04 (5.73e-04)	Tok/s 69849 (54851)	Loss/tok 3.7361 (4.6746)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.274 (0.255)	Data 1.09e-04 (5.70e-04)	Tok/s 61326 (54885)	Loss/tok 3.6746 (4.6675)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.275 (0.255)	Data 1.10e-04 (5.68e-04)	Tok/s 60952 (54907)	Loss/tok 3.6054 (4.6603)	LR 2.000e-03
0: TRAIN [0][1710/1938]	Time 0.274 (0.255)	Data 1.15e-04 (5.65e-04)	Tok/s 60909 (54910)	Loss/tok 3.6846 (4.6540)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.216 (0.255)	Data 1.09e-04 (5.62e-04)	Tok/s 48387 (54873)	Loss/tok 3.4183 (4.6486)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.276 (0.255)	Data 1.18e-04 (5.60e-04)	Tok/s 60940 (54852)	Loss/tok 3.5653 (4.6429)	LR 2.000e-03
0: TRAIN [0][1740/1938]	Time 0.275 (0.255)	Data 1.18e-04 (5.57e-04)	Tok/s 60373 (54856)	Loss/tok 3.5607 (4.6365)	LR 2.000e-03
0: TRAIN [0][1750/1938]	Time 0.409 (0.255)	Data 9.87e-05 (5.55e-04)	Tok/s 72565 (54862)	Loss/tok 4.0667 (4.6305)	LR 2.000e-03
0: TRAIN [0][1760/1938]	Time 0.334 (0.255)	Data 1.04e-04 (5.52e-04)	Tok/s 68946 (54882)	Loss/tok 3.8576 (4.6243)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.408 (0.255)	Data 1.23e-04 (5.50e-04)	Tok/s 72044 (54910)	Loss/tok 3.9345 (4.6176)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.215 (0.255)	Data 1.01e-04 (5.48e-04)	Tok/s 48499 (54925)	Loss/tok 3.3796 (4.6115)	LR 2.000e-03
0: TRAIN [0][1790/1938]	Time 0.275 (0.255)	Data 1.24e-04 (5.45e-04)	Tok/s 60882 (54917)	Loss/tok 3.6174 (4.6058)	LR 2.000e-03
0: TRAIN [0][1800/1938]	Time 0.335 (0.255)	Data 1.05e-04 (5.43e-04)	Tok/s 69107 (54925)	Loss/tok 3.7488 (4.5997)	LR 2.000e-03
0: TRAIN [0][1810/1938]	Time 0.275 (0.255)	Data 1.04e-04 (5.40e-04)	Tok/s 60899 (54888)	Loss/tok 3.4669 (4.5947)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1820/1938]	Time 0.274 (0.255)	Data 1.03e-04 (5.38e-04)	Tok/s 61152 (54924)	Loss/tok 3.6176 (4.5885)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.216 (0.255)	Data 1.13e-04 (5.36e-04)	Tok/s 48367 (54932)	Loss/tok 3.3012 (4.5825)	LR 2.000e-03
0: TRAIN [0][1840/1938]	Time 0.275 (0.255)	Data 1.04e-04 (5.34e-04)	Tok/s 61022 (54944)	Loss/tok 3.6482 (4.5768)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.216 (0.255)	Data 1.39e-04 (5.31e-04)	Tok/s 48527 (54933)	Loss/tok 3.2779 (4.5715)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.215 (0.255)	Data 9.89e-05 (5.29e-04)	Tok/s 47675 (54912)	Loss/tok 3.3219 (4.5665)	LR 2.000e-03
0: TRAIN [0][1870/1938]	Time 0.215 (0.255)	Data 1.14e-04 (5.27e-04)	Tok/s 47815 (54876)	Loss/tok 3.4074 (4.5617)	LR 2.000e-03
0: TRAIN [0][1880/1938]	Time 0.215 (0.255)	Data 1.03e-04 (5.25e-04)	Tok/s 48658 (54855)	Loss/tok 3.2695 (4.5569)	LR 2.000e-03
0: TRAIN [0][1890/1938]	Time 0.275 (0.255)	Data 1.03e-04 (5.23e-04)	Tok/s 61348 (54848)	Loss/tok 3.4936 (4.5516)	LR 2.000e-03
0: TRAIN [0][1900/1938]	Time 0.217 (0.255)	Data 1.25e-04 (5.20e-04)	Tok/s 47734 (54860)	Loss/tok 3.2437 (4.5460)	LR 2.000e-03
0: TRAIN [0][1910/1938]	Time 0.275 (0.255)	Data 1.13e-04 (5.19e-04)	Tok/s 60934 (54841)	Loss/tok 3.5942 (4.5413)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.334 (0.255)	Data 1.04e-04 (5.16e-04)	Tok/s 70324 (54860)	Loss/tok 3.7732 (4.5358)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.275 (0.255)	Data 9.92e-05 (5.14e-04)	Tok/s 60848 (54850)	Loss/tok 3.6243 (4.5308)	LR 2.000e-03
:::MLL 1571244407.732 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1571244407.733 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/2]	Time 0.991 (0.991)	Decoder iters 94.0 (94.0)	Tok/s 28732 (28732)
0: Running moses detokenizer
0: BLEU(score=19.9876120871794, counts=[33960, 15680, 8361, 4633], totals=[62647, 59644, 56641, 53645], precisions=[54.20850160422686, 26.289316611897256, 14.761391924577604, 8.636406002423339], bp=0.9681310449649152, sys_len=62647, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571244410.017 eval_accuracy: {"value": 19.99, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1571244410.018 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 4.5286	Test BLEU: 19.99
0: Performance: Epoch: 0	Training: 438738 Tok/s
0: Finished epoch 0
:::MLL 1571244410.018 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1571244410.019 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571244410.019 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 2458134906
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][0/1938]	Time 0.925 (0.925)	Data 7.11e-01 (7.11e-01)	Tok/s 11191 (11191)	Loss/tok 3.1860 (3.1860)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.274 (0.309)	Data 1.31e-04 (6.48e-02)	Tok/s 61214 (48982)	Loss/tok 3.4841 (3.4012)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.215 (0.279)	Data 9.78e-05 (3.40e-02)	Tok/s 47296 (51093)	Loss/tok 3.2251 (3.4292)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.275 (0.266)	Data 1.23e-04 (2.31e-02)	Tok/s 60605 (51490)	Loss/tok 3.5102 (3.4075)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.335 (0.259)	Data 1.02e-04 (1.75e-02)	Tok/s 69520 (51087)	Loss/tok 3.7409 (3.4170)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.216 (0.254)	Data 9.99e-05 (1.41e-02)	Tok/s 48103 (50929)	Loss/tok 3.1505 (3.4098)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.215 (0.254)	Data 7.49e-04 (1.18e-02)	Tok/s 47160 (51641)	Loss/tok 3.1918 (3.4138)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.161 (0.253)	Data 1.02e-04 (1.02e-02)	Tok/s 33011 (51793)	Loss/tok 2.7240 (3.4190)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.334 (0.249)	Data 1.08e-04 (8.91e-03)	Tok/s 70270 (51381)	Loss/tok 3.7140 (3.4123)	LR 2.000e-03
0: TRAIN [1][90/1938]	Time 0.161 (0.247)	Data 1.22e-04 (7.95e-03)	Tok/s 32351 (51333)	Loss/tok 2.8872 (3.4094)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.215 (0.250)	Data 1.15e-04 (7.17e-03)	Tok/s 48375 (52092)	Loss/tok 3.2864 (3.4146)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.216 (0.249)	Data 9.80e-05 (6.54e-03)	Tok/s 48977 (52260)	Loss/tok 3.2907 (3.4129)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.215 (0.249)	Data 9.82e-05 (6.01e-03)	Tok/s 47479 (52507)	Loss/tok 3.1647 (3.4131)	LR 2.000e-03
0: TRAIN [1][130/1938]	Time 0.276 (0.250)	Data 9.82e-05 (5.56e-03)	Tok/s 60823 (52699)	Loss/tok 3.4106 (3.4113)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.276 (0.249)	Data 1.18e-04 (5.17e-03)	Tok/s 59824 (52775)	Loss/tok 3.4332 (3.4116)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.216 (0.250)	Data 1.22e-04 (4.83e-03)	Tok/s 48046 (52900)	Loss/tok 3.1988 (3.4145)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.215 (0.249)	Data 1.34e-04 (4.54e-03)	Tok/s 48554 (52850)	Loss/tok 3.1651 (3.4103)	LR 2.000e-03
0: TRAIN [1][170/1938]	Time 0.215 (0.249)	Data 1.28e-04 (4.28e-03)	Tok/s 48593 (52999)	Loss/tok 3.1321 (3.4098)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.275 (0.249)	Data 1.38e-04 (4.05e-03)	Tok/s 60787 (53212)	Loss/tok 3.4532 (3.4089)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.274 (0.249)	Data 1.18e-04 (3.85e-03)	Tok/s 61218 (53116)	Loss/tok 3.5040 (3.4057)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.275 (0.249)	Data 1.05e-04 (3.66e-03)	Tok/s 61727 (53211)	Loss/tok 3.4109 (3.4065)	LR 2.000e-03
0: TRAIN [1][210/1938]	Time 0.275 (0.249)	Data 1.24e-04 (3.50e-03)	Tok/s 61221 (53371)	Loss/tok 3.5360 (3.4068)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.335 (0.249)	Data 1.07e-04 (3.34e-03)	Tok/s 70439 (53323)	Loss/tok 3.6446 (3.4048)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.275 (0.249)	Data 1.14e-04 (3.20e-03)	Tok/s 60775 (53413)	Loss/tok 3.4753 (3.4066)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.275 (0.250)	Data 1.34e-04 (3.07e-03)	Tok/s 61570 (53545)	Loss/tok 3.4716 (3.4084)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.215 (0.250)	Data 1.08e-04 (2.96e-03)	Tok/s 47756 (53520)	Loss/tok 3.2826 (3.4106)	LR 2.000e-03
0: TRAIN [1][260/1938]	Time 0.161 (0.250)	Data 5.01e-04 (2.85e-03)	Tok/s 32624 (53567)	Loss/tok 2.8429 (3.4113)	LR 2.000e-03
0: TRAIN [1][270/1938]	Time 0.215 (0.250)	Data 1.08e-04 (2.75e-03)	Tok/s 47171 (53628)	Loss/tok 3.2188 (3.4155)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.277 (0.251)	Data 1.01e-04 (2.66e-03)	Tok/s 60925 (53669)	Loss/tok 3.4122 (3.4180)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.274 (0.251)	Data 1.48e-04 (2.57e-03)	Tok/s 61280 (53713)	Loss/tok 3.4523 (3.4179)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.215 (0.250)	Data 1.29e-04 (2.49e-03)	Tok/s 48172 (53603)	Loss/tok 3.2027 (3.4181)	LR 2.000e-03
0: TRAIN [1][310/1938]	Time 0.216 (0.250)	Data 1.25e-04 (2.41e-03)	Tok/s 46555 (53694)	Loss/tok 3.3215 (3.4223)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.275 (0.251)	Data 1.26e-04 (2.34e-03)	Tok/s 60263 (53830)	Loss/tok 3.5419 (3.4255)	LR 2.000e-03
0: TRAIN [1][330/1938]	Time 0.334 (0.251)	Data 1.32e-04 (2.28e-03)	Tok/s 70030 (53928)	Loss/tok 3.6915 (3.4277)	LR 2.000e-03
0: TRAIN [1][340/1938]	Time 0.161 (0.251)	Data 1.19e-04 (2.21e-03)	Tok/s 32038 (53866)	Loss/tok 2.6030 (3.4268)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.409 (0.251)	Data 1.34e-04 (2.15e-03)	Tok/s 73008 (53851)	Loss/tok 3.8370 (3.4275)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.275 (0.251)	Data 1.06e-04 (2.10e-03)	Tok/s 61364 (53791)	Loss/tok 3.4245 (3.4250)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.216 (0.250)	Data 1.08e-04 (2.04e-03)	Tok/s 48250 (53733)	Loss/tok 3.3193 (3.4217)	LR 2.000e-03
0: TRAIN [1][380/1938]	Time 0.216 (0.250)	Data 1.21e-04 (1.99e-03)	Tok/s 48143 (53787)	Loss/tok 3.1358 (3.4230)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][390/1938]	Time 0.274 (0.251)	Data 1.47e-04 (1.95e-03)	Tok/s 61307 (53888)	Loss/tok 3.4724 (3.4269)	LR 2.000e-03
0: TRAIN [1][400/1938]	Time 0.335 (0.251)	Data 1.50e-04 (1.90e-03)	Tok/s 70327 (53784)	Loss/tok 3.6342 (3.4242)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.409 (0.251)	Data 1.48e-04 (1.86e-03)	Tok/s 71777 (53858)	Loss/tok 3.9253 (3.4284)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.216 (0.251)	Data 1.41e-04 (1.82e-03)	Tok/s 47950 (53802)	Loss/tok 3.2578 (3.4258)	LR 2.000e-03
0: TRAIN [1][430/1938]	Time 0.216 (0.252)	Data 1.42e-04 (1.78e-03)	Tok/s 48240 (53902)	Loss/tok 3.3025 (3.4295)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.159 (0.251)	Data 1.51e-04 (1.74e-03)	Tok/s 32659 (53838)	Loss/tok 2.6485 (3.4272)	LR 2.000e-03
0: TRAIN [1][450/1938]	Time 0.160 (0.251)	Data 1.65e-04 (1.71e-03)	Tok/s 32785 (53744)	Loss/tok 2.7080 (3.4253)	LR 2.000e-03
0: TRAIN [1][460/1938]	Time 0.216 (0.250)	Data 1.49e-04 (1.67e-03)	Tok/s 49362 (53606)	Loss/tok 3.2710 (3.4241)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.334 (0.250)	Data 1.30e-04 (1.64e-03)	Tok/s 69353 (53663)	Loss/tok 3.6239 (3.4240)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.274 (0.250)	Data 1.15e-04 (1.61e-03)	Tok/s 61027 (53613)	Loss/tok 3.4434 (3.4219)	LR 2.000e-03
0: TRAIN [1][490/1938]	Time 0.216 (0.250)	Data 1.35e-04 (1.58e-03)	Tok/s 47685 (53563)	Loss/tok 3.2534 (3.4204)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.335 (0.250)	Data 1.15e-04 (1.55e-03)	Tok/s 69803 (53659)	Loss/tok 3.5877 (3.4217)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.274 (0.250)	Data 1.28e-04 (1.52e-03)	Tok/s 61114 (53636)	Loss/tok 3.5190 (3.4202)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][520/1938]	Time 0.214 (0.250)	Data 1.35e-04 (1.50e-03)	Tok/s 48444 (53669)	Loss/tok 3.2001 (3.4204)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.275 (0.250)	Data 1.20e-04 (1.47e-03)	Tok/s 61699 (53694)	Loss/tok 3.4344 (3.4203)	LR 2.000e-03
0: TRAIN [1][540/1938]	Time 0.407 (0.250)	Data 1.37e-04 (1.45e-03)	Tok/s 72031 (53759)	Loss/tok 3.8725 (3.4224)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.215 (0.250)	Data 3.24e-04 (1.42e-03)	Tok/s 48251 (53778)	Loss/tok 3.2711 (3.4220)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.275 (0.251)	Data 1.20e-04 (1.40e-03)	Tok/s 61014 (53822)	Loss/tok 3.4980 (3.4233)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.215 (0.251)	Data 1.17e-04 (1.38e-03)	Tok/s 47878 (53800)	Loss/tok 3.1649 (3.4231)	LR 2.000e-03
0: TRAIN [1][580/1938]	Time 0.216 (0.251)	Data 1.24e-04 (1.36e-03)	Tok/s 48618 (53818)	Loss/tok 3.1522 (3.4229)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.275 (0.251)	Data 1.38e-04 (1.34e-03)	Tok/s 60037 (53874)	Loss/tok 3.5364 (3.4229)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.216 (0.251)	Data 1.39e-04 (1.32e-03)	Tok/s 49019 (54003)	Loss/tok 3.2573 (3.4247)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.336 (0.252)	Data 1.25e-04 (1.30e-03)	Tok/s 68855 (54117)	Loss/tok 3.5570 (3.4256)	LR 2.000e-03
0: TRAIN [1][620/1938]	Time 0.335 (0.252)	Data 1.37e-04 (1.28e-03)	Tok/s 69194 (54065)	Loss/tok 3.5697 (3.4242)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.275 (0.252)	Data 1.18e-04 (1.26e-03)	Tok/s 60774 (54114)	Loss/tok 3.5241 (3.4253)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.217 (0.252)	Data 1.54e-04 (1.24e-03)	Tok/s 47770 (54135)	Loss/tok 3.1383 (3.4248)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.216 (0.252)	Data 1.16e-04 (1.23e-03)	Tok/s 47910 (54195)	Loss/tok 3.2032 (3.4255)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][660/1938]	Time 0.335 (0.253)	Data 1.22e-04 (1.21e-03)	Tok/s 70573 (54259)	Loss/tok 3.5386 (3.4278)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.275 (0.252)	Data 1.32e-04 (1.19e-03)	Tok/s 61064 (54189)	Loss/tok 3.4661 (3.4264)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.216 (0.253)	Data 1.44e-04 (1.18e-03)	Tok/s 47498 (54265)	Loss/tok 3.1598 (3.4285)	LR 2.000e-03
0: TRAIN [1][690/1938]	Time 0.274 (0.253)	Data 1.15e-04 (1.16e-03)	Tok/s 61985 (54295)	Loss/tok 3.4357 (3.4284)	LR 2.000e-03
0: TRAIN [1][700/1938]	Time 0.335 (0.253)	Data 1.16e-04 (1.15e-03)	Tok/s 68357 (54378)	Loss/tok 3.6666 (3.4293)	LR 2.000e-03
0: TRAIN [1][710/1938]	Time 0.216 (0.253)	Data 1.27e-04 (1.14e-03)	Tok/s 48217 (54399)	Loss/tok 3.1987 (3.4290)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.160 (0.253)	Data 1.35e-04 (1.12e-03)	Tok/s 33324 (54393)	Loss/tok 2.7828 (3.4280)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.275 (0.253)	Data 1.15e-04 (1.11e-03)	Tok/s 61824 (54406)	Loss/tok 3.5098 (3.4279)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][740/1938]	Time 0.216 (0.253)	Data 1.28e-04 (1.10e-03)	Tok/s 47335 (54417)	Loss/tok 3.2474 (3.4272)	LR 2.000e-03
0: TRAIN [1][750/1938]	Time 0.160 (0.253)	Data 1.29e-04 (1.08e-03)	Tok/s 32967 (54405)	Loss/tok 2.6561 (3.4279)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.335 (0.254)	Data 1.22e-04 (1.07e-03)	Tok/s 69350 (54490)	Loss/tok 3.6050 (3.4302)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.275 (0.254)	Data 1.14e-04 (1.06e-03)	Tok/s 60995 (54576)	Loss/tok 3.3993 (3.4307)	LR 2.000e-03
0: TRAIN [1][780/1938]	Time 0.274 (0.254)	Data 1.28e-04 (1.05e-03)	Tok/s 61192 (54541)	Loss/tok 3.4268 (3.4290)	LR 2.000e-03
0: TRAIN [1][790/1938]	Time 0.161 (0.254)	Data 1.22e-04 (1.03e-03)	Tok/s 32917 (54512)	Loss/tok 2.7264 (3.4278)	LR 2.000e-03
0: TRAIN [1][800/1938]	Time 0.408 (0.254)	Data 1.42e-04 (1.02e-03)	Tok/s 72754 (54515)	Loss/tok 3.7996 (3.4280)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.215 (0.254)	Data 1.63e-04 (1.01e-03)	Tok/s 46767 (54566)	Loss/tok 3.2198 (3.4286)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.335 (0.255)	Data 1.54e-04 (1.00e-03)	Tok/s 69893 (54645)	Loss/tok 3.7097 (3.4293)	LR 2.000e-03
0: TRAIN [1][830/1938]	Time 0.215 (0.255)	Data 1.97e-04 (9.92e-04)	Tok/s 47194 (54641)	Loss/tok 3.1244 (3.4289)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.275 (0.255)	Data 6.52e-04 (9.83e-04)	Tok/s 60426 (54657)	Loss/tok 3.4537 (3.4288)	LR 2.000e-03
0: TRAIN [1][850/1938]	Time 0.216 (0.255)	Data 1.19e-04 (9.73e-04)	Tok/s 48266 (54703)	Loss/tok 3.2124 (3.4287)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.161 (0.255)	Data 1.31e-04 (9.63e-04)	Tok/s 32556 (54649)	Loss/tok 2.8472 (3.4277)	LR 2.000e-03
0: TRAIN [1][870/1938]	Time 0.336 (0.255)	Data 1.29e-04 (9.54e-04)	Tok/s 69481 (54646)	Loss/tok 3.5962 (3.4278)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.216 (0.255)	Data 1.47e-04 (9.44e-04)	Tok/s 47826 (54620)	Loss/tok 3.1733 (3.4277)	LR 2.000e-03
0: TRAIN [1][890/1938]	Time 0.335 (0.255)	Data 1.29e-04 (9.35e-04)	Tok/s 68539 (54671)	Loss/tok 3.6655 (3.4290)	LR 2.000e-03
0: TRAIN [1][900/1938]	Time 0.335 (0.255)	Data 1.41e-04 (9.27e-04)	Tok/s 69569 (54672)	Loss/tok 3.5550 (3.4287)	LR 2.000e-03
0: TRAIN [1][910/1938]	Time 0.216 (0.255)	Data 1.26e-04 (9.18e-04)	Tok/s 47991 (54716)	Loss/tok 3.1927 (3.4294)	LR 2.000e-03
0: TRAIN [1][920/1938]	Time 0.216 (0.255)	Data 1.23e-04 (9.09e-04)	Tok/s 48444 (54694)	Loss/tok 3.2208 (3.4290)	LR 2.000e-03
0: TRAIN [1][930/1938]	Time 0.216 (0.256)	Data 1.33e-04 (9.01e-04)	Tok/s 46902 (54774)	Loss/tok 3.2435 (3.4309)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.275 (0.256)	Data 1.46e-04 (8.93e-04)	Tok/s 61837 (54805)	Loss/tok 3.3737 (3.4303)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.215 (0.256)	Data 1.38e-04 (8.85e-04)	Tok/s 48173 (54856)	Loss/tok 3.0542 (3.4301)	LR 2.000e-03
0: TRAIN [1][960/1938]	Time 0.160 (0.256)	Data 1.13e-04 (8.78e-04)	Tok/s 32693 (54871)	Loss/tok 2.7119 (3.4304)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.216 (0.256)	Data 1.42e-04 (8.70e-04)	Tok/s 47587 (54896)	Loss/tok 3.0473 (3.4297)	LR 2.000e-03
0: TRAIN [1][980/1938]	Time 0.215 (0.256)	Data 1.28e-04 (8.63e-04)	Tok/s 48169 (54868)	Loss/tok 3.3218 (3.4286)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.216 (0.256)	Data 1.28e-04 (8.56e-04)	Tok/s 47686 (54859)	Loss/tok 3.2201 (3.4275)	LR 2.000e-03
0: TRAIN [1][1000/1938]	Time 0.217 (0.256)	Data 2.21e-04 (8.49e-04)	Tok/s 48102 (54874)	Loss/tok 3.2235 (3.4279)	LR 2.000e-03
0: TRAIN [1][1010/1938]	Time 0.274 (0.256)	Data 1.22e-04 (8.42e-04)	Tok/s 61237 (54915)	Loss/tok 3.4641 (3.4284)	LR 2.000e-03
0: TRAIN [1][1020/1938]	Time 0.160 (0.256)	Data 1.39e-04 (8.35e-04)	Tok/s 32099 (54896)	Loss/tok 2.8117 (3.4276)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.216 (0.256)	Data 1.14e-04 (8.29e-04)	Tok/s 47627 (54836)	Loss/tok 3.1573 (3.4262)	LR 2.000e-03
0: TRAIN [1][1040/1938]	Time 0.216 (0.255)	Data 1.35e-04 (8.22e-04)	Tok/s 47825 (54793)	Loss/tok 3.1620 (3.4249)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.216 (0.255)	Data 1.39e-04 (8.16e-04)	Tok/s 47506 (54744)	Loss/tok 3.0814 (3.4238)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.276 (0.255)	Data 1.15e-04 (8.10e-04)	Tok/s 60710 (54725)	Loss/tok 3.3085 (3.4233)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.216 (0.255)	Data 1.37e-04 (8.03e-04)	Tok/s 47734 (54745)	Loss/tok 3.1943 (3.4223)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1080/1938]	Time 0.275 (0.255)	Data 1.14e-04 (7.98e-04)	Tok/s 61024 (54786)	Loss/tok 3.4868 (3.4227)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.275 (0.255)	Data 1.34e-04 (7.91e-04)	Tok/s 61031 (54787)	Loss/tok 3.3358 (3.4221)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.216 (0.255)	Data 1.68e-04 (7.86e-04)	Tok/s 47879 (54771)	Loss/tok 3.2224 (3.4216)	LR 2.000e-03
0: TRAIN [1][1110/1938]	Time 0.161 (0.255)	Data 1.53e-04 (7.80e-04)	Tok/s 32548 (54764)	Loss/tok 2.5809 (3.4209)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.216 (0.255)	Data 1.17e-04 (7.74e-04)	Tok/s 47968 (54776)	Loss/tok 3.0454 (3.4202)	LR 2.000e-03
0: TRAIN [1][1130/1938]	Time 0.216 (0.255)	Data 1.33e-04 (7.68e-04)	Tok/s 48103 (54822)	Loss/tok 3.0774 (3.4208)	LR 2.000e-03
0: TRAIN [1][1140/1938]	Time 0.275 (0.255)	Data 1.31e-04 (7.63e-04)	Tok/s 61801 (54838)	Loss/tok 3.3661 (3.4211)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.336 (0.255)	Data 1.54e-04 (7.58e-04)	Tok/s 69437 (54840)	Loss/tok 3.4639 (3.4201)	LR 2.000e-03
0: TRAIN [1][1160/1938]	Time 0.215 (0.255)	Data 1.43e-04 (7.53e-04)	Tok/s 47052 (54822)	Loss/tok 3.1400 (3.4192)	LR 2.000e-03
0: TRAIN [1][1170/1938]	Time 0.337 (0.255)	Data 1.41e-04 (7.48e-04)	Tok/s 69955 (54829)	Loss/tok 3.4501 (3.4181)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.274 (0.255)	Data 8.33e-04 (7.43e-04)	Tok/s 62084 (54821)	Loss/tok 3.3628 (3.4173)	LR 2.000e-03
0: TRAIN [1][1190/1938]	Time 0.216 (0.255)	Data 1.15e-04 (7.38e-04)	Tok/s 47708 (54790)	Loss/tok 3.0953 (3.4165)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.215 (0.255)	Data 1.15e-04 (7.33e-04)	Tok/s 48050 (54807)	Loss/tok 3.2097 (3.4166)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1210/1938]	Time 0.215 (0.255)	Data 1.26e-04 (7.28e-04)	Tok/s 47477 (54818)	Loss/tok 3.2247 (3.4167)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1220/1938]	Time 0.216 (0.255)	Data 1.13e-04 (7.23e-04)	Tok/s 48666 (54790)	Loss/tok 3.1773 (3.4158)	LR 2.000e-03
0: TRAIN [1][1230/1938]	Time 0.215 (0.255)	Data 1.25e-04 (7.19e-04)	Tok/s 47147 (54815)	Loss/tok 3.1635 (3.4157)	LR 2.000e-03
0: TRAIN [1][1240/1938]	Time 0.216 (0.255)	Data 1.20e-04 (7.14e-04)	Tok/s 47526 (54812)	Loss/tok 3.1202 (3.4146)	LR 2.000e-03
0: TRAIN [1][1250/1938]	Time 0.334 (0.255)	Data 1.13e-04 (7.09e-04)	Tok/s 70229 (54835)	Loss/tok 3.5622 (3.4146)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.216 (0.255)	Data 1.41e-04 (7.05e-04)	Tok/s 47005 (54816)	Loss/tok 3.1815 (3.4140)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.275 (0.255)	Data 1.34e-04 (7.00e-04)	Tok/s 60242 (54801)	Loss/tok 3.3348 (3.4129)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.215 (0.255)	Data 1.14e-04 (6.96e-04)	Tok/s 48960 (54773)	Loss/tok 3.1142 (3.4120)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.275 (0.255)	Data 1.37e-04 (6.92e-04)	Tok/s 60912 (54770)	Loss/tok 3.4306 (3.4112)	LR 2.000e-03
0: TRAIN [1][1300/1938]	Time 0.275 (0.255)	Data 1.39e-04 (6.88e-04)	Tok/s 60926 (54804)	Loss/tok 3.3786 (3.4119)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.275 (0.255)	Data 1.15e-04 (6.84e-04)	Tok/s 61008 (54786)	Loss/tok 3.3110 (3.4116)	LR 2.000e-03
0: TRAIN [1][1320/1938]	Time 0.335 (0.255)	Data 1.50e-04 (6.79e-04)	Tok/s 68445 (54768)	Loss/tok 3.6028 (3.4112)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.335 (0.255)	Data 1.30e-04 (6.75e-04)	Tok/s 69222 (54765)	Loss/tok 3.5443 (3.4113)	LR 2.000e-03
0: TRAIN [1][1340/1938]	Time 0.275 (0.255)	Data 1.32e-04 (6.71e-04)	Tok/s 61444 (54760)	Loss/tok 3.4122 (3.4118)	LR 2.000e-03
0: TRAIN [1][1350/1938]	Time 0.407 (0.255)	Data 1.27e-04 (6.67e-04)	Tok/s 73473 (54697)	Loss/tok 3.7583 (3.4109)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.160 (0.254)	Data 1.13e-04 (6.64e-04)	Tok/s 32414 (54661)	Loss/tok 2.6850 (3.4102)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.216 (0.254)	Data 1.45e-04 (6.60e-04)	Tok/s 48472 (54641)	Loss/tok 3.1974 (3.4092)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.215 (0.254)	Data 1.33e-04 (6.56e-04)	Tok/s 47108 (54657)	Loss/tok 3.2035 (3.4088)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.275 (0.254)	Data 1.15e-04 (6.53e-04)	Tok/s 60923 (54651)	Loss/tok 3.3197 (3.4083)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1400/1938]	Time 0.161 (0.254)	Data 1.29e-04 (6.49e-04)	Tok/s 32880 (54648)	Loss/tok 2.8665 (3.4081)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.275 (0.254)	Data 1.23e-04 (6.45e-04)	Tok/s 60417 (54642)	Loss/tok 3.3972 (3.4080)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.216 (0.254)	Data 1.38e-04 (6.42e-04)	Tok/s 47827 (54623)	Loss/tok 3.1418 (3.4071)	LR 2.000e-03
0: TRAIN [1][1430/1938]	Time 0.216 (0.254)	Data 1.07e-04 (6.38e-04)	Tok/s 47495 (54631)	Loss/tok 3.2934 (3.4072)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.334 (0.255)	Data 1.28e-04 (6.35e-04)	Tok/s 69708 (54686)	Loss/tok 3.5240 (3.4085)	LR 2.000e-03
0: TRAIN [1][1450/1938]	Time 0.159 (0.254)	Data 1.19e-04 (6.31e-04)	Tok/s 33506 (54670)	Loss/tok 2.7424 (3.4075)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.216 (0.254)	Data 1.30e-04 (6.28e-04)	Tok/s 48249 (54689)	Loss/tok 3.0417 (3.4076)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.275 (0.254)	Data 1.30e-04 (6.24e-04)	Tok/s 60778 (54686)	Loss/tok 3.3581 (3.4068)	LR 2.000e-03
0: TRAIN [1][1480/1938]	Time 0.160 (0.254)	Data 1.30e-04 (6.21e-04)	Tok/s 32290 (54676)	Loss/tok 2.7246 (3.4061)	LR 2.000e-03
0: TRAIN [1][1490/1938]	Time 0.335 (0.255)	Data 1.23e-04 (6.18e-04)	Tok/s 69647 (54707)	Loss/tok 3.5892 (3.4065)	LR 2.000e-03
0: TRAIN [1][1500/1938]	Time 0.216 (0.254)	Data 1.15e-04 (6.15e-04)	Tok/s 48550 (54699)	Loss/tok 3.1221 (3.4060)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.216 (0.254)	Data 1.47e-04 (6.12e-04)	Tok/s 48330 (54673)	Loss/tok 3.1300 (3.4051)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.336 (0.255)	Data 1.18e-04 (6.09e-04)	Tok/s 68894 (54714)	Loss/tok 3.5375 (3.4053)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.336 (0.255)	Data 1.19e-04 (6.06e-04)	Tok/s 69538 (54728)	Loss/tok 3.4773 (3.4055)	LR 2.000e-03
0: TRAIN [1][1540/1938]	Time 0.216 (0.255)	Data 1.44e-04 (6.03e-04)	Tok/s 47887 (54738)	Loss/tok 3.2368 (3.4060)	LR 2.000e-03
0: TRAIN [1][1550/1938]	Time 0.405 (0.255)	Data 1.23e-04 (6.00e-04)	Tok/s 72708 (54731)	Loss/tok 3.7628 (3.4057)	LR 2.000e-03
0: TRAIN [1][1560/1938]	Time 0.335 (0.255)	Data 1.42e-04 (5.97e-04)	Tok/s 68648 (54749)	Loss/tok 3.6569 (3.4060)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.334 (0.255)	Data 1.23e-04 (5.95e-04)	Tok/s 70124 (54782)	Loss/tok 3.5130 (3.4065)	LR 2.000e-03
0: TRAIN [1][1580/1938]	Time 0.275 (0.255)	Data 1.42e-04 (5.92e-04)	Tok/s 60888 (54756)	Loss/tok 3.3805 (3.4054)	LR 2.000e-03
0: TRAIN [1][1590/1938]	Time 0.407 (0.255)	Data 1.17e-04 (5.89e-04)	Tok/s 73915 (54752)	Loss/tok 3.6499 (3.4051)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.215 (0.255)	Data 2.76e-04 (5.87e-04)	Tok/s 47485 (54696)	Loss/tok 3.1502 (3.4044)	LR 2.000e-03
0: TRAIN [1][1610/1938]	Time 0.275 (0.255)	Data 1.32e-04 (5.84e-04)	Tok/s 61025 (54697)	Loss/tok 3.2136 (3.4039)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.276 (0.255)	Data 7.35e-04 (5.82e-04)	Tok/s 60841 (54673)	Loss/tok 3.3212 (3.4032)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.276 (0.255)	Data 1.28e-04 (5.79e-04)	Tok/s 61609 (54674)	Loss/tok 3.2622 (3.4027)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.215 (0.254)	Data 4.20e-04 (5.77e-04)	Tok/s 48122 (54671)	Loss/tok 3.0724 (3.4020)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.334 (0.255)	Data 1.34e-04 (5.74e-04)	Tok/s 69262 (54672)	Loss/tok 3.5407 (3.4019)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.217 (0.254)	Data 3.00e-04 (5.72e-04)	Tok/s 48172 (54660)	Loss/tok 3.1155 (3.4016)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1670/1938]	Time 0.216 (0.255)	Data 1.39e-04 (5.70e-04)	Tok/s 48652 (54659)	Loss/tok 3.1692 (3.4016)	LR 2.000e-03
0: TRAIN [1][1680/1938]	Time 0.335 (0.255)	Data 1.99e-04 (5.67e-04)	Tok/s 70719 (54669)	Loss/tok 3.4338 (3.4015)	LR 2.000e-03
0: TRAIN [1][1690/1938]	Time 0.216 (0.254)	Data 1.15e-04 (5.64e-04)	Tok/s 48228 (54625)	Loss/tok 3.2398 (3.4007)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.162 (0.254)	Data 1.21e-04 (5.62e-04)	Tok/s 32603 (54600)	Loss/tok 2.6633 (3.4002)	LR 2.000e-03
0: TRAIN [1][1710/1938]	Time 0.215 (0.254)	Data 1.39e-04 (5.59e-04)	Tok/s 47430 (54589)	Loss/tok 2.9809 (3.3995)	LR 2.000e-03
0: TRAIN [1][1720/1938]	Time 0.216 (0.254)	Data 1.14e-04 (5.57e-04)	Tok/s 48396 (54577)	Loss/tok 3.1486 (3.3990)	LR 2.000e-03
0: TRAIN [1][1730/1938]	Time 0.275 (0.254)	Data 1.51e-04 (5.55e-04)	Tok/s 61385 (54584)	Loss/tok 3.4033 (3.3985)	LR 2.000e-03
0: TRAIN [1][1740/1938]	Time 0.275 (0.254)	Data 1.18e-04 (5.52e-04)	Tok/s 60397 (54605)	Loss/tok 3.2931 (3.3990)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.215 (0.254)	Data 1.18e-04 (5.50e-04)	Tok/s 48000 (54605)	Loss/tok 3.2280 (3.3986)	LR 2.000e-03
0: TRAIN [1][1760/1938]	Time 0.275 (0.254)	Data 1.17e-04 (5.47e-04)	Tok/s 60389 (54607)	Loss/tok 3.5026 (3.3982)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.335 (0.254)	Data 1.25e-04 (5.45e-04)	Tok/s 69145 (54625)	Loss/tok 3.6148 (3.3981)	LR 2.000e-03
0: TRAIN [1][1780/1938]	Time 0.274 (0.254)	Data 1.30e-04 (5.43e-04)	Tok/s 60628 (54641)	Loss/tok 3.3388 (3.3985)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.217 (0.254)	Data 1.10e-04 (5.40e-04)	Tok/s 48562 (54633)	Loss/tok 3.0898 (3.3978)	LR 2.000e-03
0: TRAIN [1][1800/1938]	Time 0.216 (0.254)	Data 1.75e-04 (5.38e-04)	Tok/s 48610 (54632)	Loss/tok 3.1783 (3.3972)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.216 (0.254)	Data 1.38e-04 (5.36e-04)	Tok/s 48799 (54626)	Loss/tok 3.2220 (3.3968)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1820/1938]	Time 0.215 (0.254)	Data 1.29e-04 (5.33e-04)	Tok/s 47341 (54640)	Loss/tok 3.0729 (3.3966)	LR 2.000e-03
0: TRAIN [1][1830/1938]	Time 0.217 (0.255)	Data 1.22e-04 (5.31e-04)	Tok/s 46961 (54668)	Loss/tok 3.0420 (3.3968)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.215 (0.255)	Data 1.22e-04 (5.29e-04)	Tok/s 48000 (54684)	Loss/tok 3.1794 (3.3973)	LR 2.000e-03
0: TRAIN [1][1850/1938]	Time 0.216 (0.255)	Data 1.36e-04 (5.27e-04)	Tok/s 49252 (54711)	Loss/tok 3.2140 (3.3977)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.216 (0.255)	Data 1.15e-04 (5.25e-04)	Tok/s 47836 (54723)	Loss/tok 3.0667 (3.3975)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.335 (0.255)	Data 1.33e-04 (5.23e-04)	Tok/s 69626 (54733)	Loss/tok 3.6141 (3.3977)	LR 2.000e-03
0: TRAIN [1][1880/1938]	Time 0.407 (0.255)	Data 1.31e-04 (5.21e-04)	Tok/s 73967 (54751)	Loss/tok 3.6111 (3.3976)	LR 2.000e-03
0: TRAIN [1][1890/1938]	Time 0.275 (0.255)	Data 1.14e-04 (5.19e-04)	Tok/s 60970 (54739)	Loss/tok 3.3964 (3.3972)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.215 (0.255)	Data 1.26e-04 (5.17e-04)	Tok/s 47900 (54747)	Loss/tok 3.0933 (3.3972)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.275 (0.255)	Data 1.25e-04 (5.15e-04)	Tok/s 60460 (54735)	Loss/tok 3.3779 (3.3966)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.275 (0.255)	Data 1.26e-04 (5.13e-04)	Tok/s 60468 (54748)	Loss/tok 3.3035 (3.3963)	LR 2.000e-03
0: TRAIN [1][1930/1938]	Time 0.334 (0.255)	Data 1.21e-04 (5.11e-04)	Tok/s 69334 (54772)	Loss/tok 3.5374 (3.3963)	LR 2.000e-03
:::MLL 1571244905.531 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1571244905.531 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/2]	Time 1.002 (1.002)	Decoder iters 96.0 (96.0)	Tok/s 29029 (29029)
0: Running moses detokenizer
0: BLEU(score=21.94117255555461, counts=[35078, 16969, 9340, 5383], totals=[62363, 59360, 56358, 53360], precisions=[56.2480958260507, 28.586590296495956, 16.572625004435928, 10.08808095952024], bp=0.9635900831581411, sys_len=62363, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571244907.845 eval_accuracy: {"value": 21.94, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1571244907.846 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.3973	Test BLEU: 21.94
0: Performance: Epoch: 1	Training: 438127 Tok/s
0: Finished epoch 1
:::MLL 1571244907.846 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1571244907.847 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571244907.847 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 4097996993
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][0/1938]	Time 0.883 (0.883)	Data 7.20e-01 (7.20e-01)	Tok/s 5971 (5971)	Loss/tok 2.6743 (2.6743)	LR 2.000e-03
0: TRAIN [2][10/1938]	Time 0.215 (0.295)	Data 1.03e-04 (6.56e-02)	Tok/s 47489 (46493)	Loss/tok 2.9679 (3.1297)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.274 (0.275)	Data 9.54e-05 (3.44e-02)	Tok/s 61008 (50434)	Loss/tok 3.1597 (3.1627)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.160 (0.254)	Data 1.37e-04 (2.33e-02)	Tok/s 33526 (49000)	Loss/tok 2.6535 (3.1300)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.216 (0.247)	Data 1.04e-04 (1.77e-02)	Tok/s 47185 (49347)	Loss/tok 2.9693 (3.1221)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.160 (0.261)	Data 1.06e-04 (1.42e-02)	Tok/s 33173 (51721)	Loss/tok 2.6601 (3.2360)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.276 (0.266)	Data 1.15e-04 (1.19e-02)	Tok/s 60345 (53147)	Loss/tok 3.2786 (3.2617)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.216 (0.265)	Data 1.12e-04 (1.03e-02)	Tok/s 47919 (53681)	Loss/tok 3.0429 (3.2605)	LR 2.000e-03
0: TRAIN [2][80/1938]	Time 0.216 (0.267)	Data 9.44e-05 (9.00e-03)	Tok/s 47860 (54429)	Loss/tok 3.0449 (3.2620)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.217 (0.266)	Data 9.42e-05 (8.02e-03)	Tok/s 46486 (54403)	Loss/tok 2.9973 (3.2650)	LR 2.000e-03
0: TRAIN [2][100/1938]	Time 0.216 (0.265)	Data 1.41e-04 (7.24e-03)	Tok/s 49517 (54588)	Loss/tok 2.9911 (3.2624)	LR 2.000e-03
0: TRAIN [2][110/1938]	Time 0.161 (0.261)	Data 4.45e-04 (6.60e-03)	Tok/s 32935 (53943)	Loss/tok 2.7839 (3.2521)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.274 (0.262)	Data 1.27e-04 (6.06e-03)	Tok/s 60936 (54284)	Loss/tok 3.2555 (3.2623)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.216 (0.264)	Data 9.87e-05 (5.61e-03)	Tok/s 47130 (54628)	Loss/tok 3.0751 (3.2715)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.216 (0.263)	Data 9.56e-05 (5.22e-03)	Tok/s 48390 (54758)	Loss/tok 3.0228 (3.2696)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.335 (0.265)	Data 9.99e-05 (4.88e-03)	Tok/s 70137 (55101)	Loss/tok 3.3330 (3.2808)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.216 (0.264)	Data 9.82e-05 (4.59e-03)	Tok/s 47830 (55092)	Loss/tok 3.1002 (3.2760)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.216 (0.264)	Data 1.12e-04 (4.32e-03)	Tok/s 47009 (55109)	Loss/tok 3.0646 (3.2757)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.216 (0.263)	Data 1.07e-04 (4.09e-03)	Tok/s 48158 (55074)	Loss/tok 3.0565 (3.2750)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.275 (0.262)	Data 9.99e-05 (3.88e-03)	Tok/s 61544 (54953)	Loss/tok 3.1467 (3.2706)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.274 (0.261)	Data 9.49e-05 (3.70e-03)	Tok/s 61059 (54850)	Loss/tok 3.3081 (3.2673)	LR 2.000e-03
0: TRAIN [2][210/1938]	Time 0.215 (0.261)	Data 1.06e-04 (3.53e-03)	Tok/s 47990 (54986)	Loss/tok 2.9436 (3.2664)	LR 2.000e-03
0: TRAIN [2][220/1938]	Time 0.334 (0.259)	Data 9.35e-05 (3.37e-03)	Tok/s 70077 (54744)	Loss/tok 3.4774 (3.2624)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.216 (0.260)	Data 9.51e-05 (3.23e-03)	Tok/s 48776 (54909)	Loss/tok 3.1662 (3.2637)	LR 2.000e-03
0: TRAIN [2][240/1938]	Time 0.275 (0.260)	Data 1.16e-04 (3.10e-03)	Tok/s 60732 (54918)	Loss/tok 3.2345 (3.2600)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.216 (0.259)	Data 1.15e-04 (2.98e-03)	Tok/s 48628 (54786)	Loss/tok 3.0236 (3.2572)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.216 (0.259)	Data 6.31e-04 (2.88e-03)	Tok/s 47562 (54967)	Loss/tok 3.0042 (3.2608)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.276 (0.259)	Data 1.20e-04 (2.78e-03)	Tok/s 60858 (54897)	Loss/tok 3.3950 (3.2600)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.276 (0.261)	Data 1.00e-04 (2.68e-03)	Tok/s 60532 (55209)	Loss/tok 3.2519 (3.2682)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.216 (0.260)	Data 9.87e-05 (2.59e-03)	Tok/s 47961 (55127)	Loss/tok 3.1432 (3.2667)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][300/1938]	Time 0.335 (0.260)	Data 1.29e-04 (2.51e-03)	Tok/s 69675 (55146)	Loss/tok 3.5102 (3.2687)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.215 (0.261)	Data 1.28e-04 (2.43e-03)	Tok/s 47086 (55273)	Loss/tok 3.0464 (3.2737)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.275 (0.260)	Data 1.01e-04 (2.36e-03)	Tok/s 62426 (55138)	Loss/tok 3.1769 (3.2722)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.275 (0.260)	Data 1.96e-04 (2.29e-03)	Tok/s 60652 (55203)	Loss/tok 3.3428 (3.2705)	LR 2.000e-03
0: TRAIN [2][340/1938]	Time 0.161 (0.259)	Data 1.09e-04 (2.23e-03)	Tok/s 32815 (55104)	Loss/tok 2.6228 (3.2667)	LR 2.000e-03
0: TRAIN [2][350/1938]	Time 0.159 (0.259)	Data 1.28e-04 (2.17e-03)	Tok/s 32620 (55006)	Loss/tok 2.5775 (3.2658)	LR 2.000e-03
0: TRAIN [2][360/1938]	Time 0.217 (0.259)	Data 1.48e-04 (2.11e-03)	Tok/s 47495 (55095)	Loss/tok 2.9593 (3.2691)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.216 (0.259)	Data 1.35e-04 (2.06e-03)	Tok/s 47215 (55042)	Loss/tok 3.0406 (3.2684)	LR 2.000e-03
0: TRAIN [2][380/1938]	Time 0.273 (0.258)	Data 1.03e-04 (2.01e-03)	Tok/s 61910 (54872)	Loss/tok 3.2309 (3.2676)	LR 2.000e-03
0: TRAIN [2][390/1938]	Time 0.216 (0.259)	Data 1.20e-04 (1.96e-03)	Tok/s 49098 (55005)	Loss/tok 3.0868 (3.2707)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.336 (0.259)	Data 1.10e-04 (1.92e-03)	Tok/s 69588 (55046)	Loss/tok 3.5163 (3.2716)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.216 (0.259)	Data 1.83e-04 (1.87e-03)	Tok/s 48831 (55045)	Loss/tok 3.0462 (3.2699)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.215 (0.258)	Data 1.02e-04 (1.83e-03)	Tok/s 48226 (55018)	Loss/tok 3.0788 (3.2687)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.275 (0.259)	Data 1.02e-04 (1.79e-03)	Tok/s 61505 (55197)	Loss/tok 3.2616 (3.2710)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][440/1938]	Time 0.215 (0.259)	Data 1.15e-04 (1.75e-03)	Tok/s 48069 (55099)	Loss/tok 2.9766 (3.2704)	LR 2.000e-03
0: TRAIN [2][450/1938]	Time 0.275 (0.259)	Data 1.05e-04 (1.72e-03)	Tok/s 60633 (55104)	Loss/tok 3.2179 (3.2721)	LR 2.000e-03
0: TRAIN [2][460/1938]	Time 0.161 (0.260)	Data 1.41e-04 (1.68e-03)	Tok/s 32198 (55140)	Loss/tok 2.7632 (3.2753)	LR 2.000e-03
0: TRAIN [2][470/1938]	Time 0.276 (0.259)	Data 1.10e-04 (1.65e-03)	Tok/s 61310 (55091)	Loss/tok 3.2195 (3.2736)	LR 2.000e-03
0: TRAIN [2][480/1938]	Time 0.276 (0.259)	Data 1.01e-04 (1.62e-03)	Tok/s 61522 (55088)	Loss/tok 3.1233 (3.2731)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.216 (0.259)	Data 1.70e-04 (1.59e-03)	Tok/s 48615 (55017)	Loss/tok 3.0836 (3.2715)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.217 (0.258)	Data 1.21e-04 (1.56e-03)	Tok/s 48228 (54937)	Loss/tok 3.0845 (3.2710)	LR 2.000e-03
0: TRAIN [2][510/1938]	Time 0.215 (0.258)	Data 1.10e-04 (1.53e-03)	Tok/s 48241 (54991)	Loss/tok 2.9974 (3.2705)	LR 2.000e-03
0: TRAIN [2][520/1938]	Time 0.407 (0.259)	Data 1.53e-04 (1.50e-03)	Tok/s 72964 (55036)	Loss/tok 3.6849 (3.2720)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.161 (0.258)	Data 1.05e-04 (1.48e-03)	Tok/s 32372 (54932)	Loss/tok 2.5510 (3.2716)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.216 (0.258)	Data 9.75e-05 (1.45e-03)	Tok/s 48073 (54815)	Loss/tok 3.0889 (3.2705)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.275 (0.258)	Data 1.18e-04 (1.43e-03)	Tok/s 61601 (54909)	Loss/tok 3.3278 (3.2729)	LR 2.000e-03
0: TRAIN [2][560/1938]	Time 0.275 (0.258)	Data 1.15e-04 (1.40e-03)	Tok/s 60803 (54983)	Loss/tok 3.3292 (3.2728)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.336 (0.258)	Data 1.21e-04 (1.38e-03)	Tok/s 69982 (54929)	Loss/tok 3.3820 (3.2720)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.215 (0.258)	Data 1.18e-04 (1.36e-03)	Tok/s 48295 (54934)	Loss/tok 3.0821 (3.2719)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.276 (0.258)	Data 1.24e-04 (1.34e-03)	Tok/s 61002 (54984)	Loss/tok 3.1546 (3.2732)	LR 2.000e-03
0: TRAIN [2][600/1938]	Time 0.215 (0.258)	Data 1.07e-04 (1.32e-03)	Tok/s 47175 (54960)	Loss/tok 3.1456 (3.2723)	LR 2.000e-03
0: TRAIN [2][610/1938]	Time 0.335 (0.258)	Data 1.19e-04 (1.30e-03)	Tok/s 69800 (54952)	Loss/tok 3.3268 (3.2715)	LR 2.000e-03
0: TRAIN [2][620/1938]	Time 0.275 (0.257)	Data 4.68e-04 (1.28e-03)	Tok/s 62000 (54855)	Loss/tok 3.2702 (3.2697)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.215 (0.258)	Data 1.22e-04 (1.26e-03)	Tok/s 46956 (54927)	Loss/tok 3.0478 (3.2714)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][640/1938]	Time 0.408 (0.258)	Data 1.16e-04 (1.24e-03)	Tok/s 73711 (54919)	Loss/tok 3.5344 (3.2719)	LR 2.000e-03
0: TRAIN [2][650/1938]	Time 0.216 (0.257)	Data 1.07e-04 (1.23e-03)	Tok/s 47744 (54890)	Loss/tok 3.0474 (3.2705)	LR 2.000e-03
0: TRAIN [2][660/1938]	Time 0.216 (0.257)	Data 4.25e-04 (1.21e-03)	Tok/s 48563 (54826)	Loss/tok 3.0561 (3.2686)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.216 (0.257)	Data 1.19e-04 (1.19e-03)	Tok/s 48419 (54801)	Loss/tok 2.9891 (3.2671)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.335 (0.257)	Data 1.24e-04 (1.18e-03)	Tok/s 70002 (54827)	Loss/tok 3.3611 (3.2665)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.216 (0.257)	Data 1.22e-04 (1.16e-03)	Tok/s 47671 (54843)	Loss/tok 3.1036 (3.2659)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.334 (0.256)	Data 1.31e-04 (1.15e-03)	Tok/s 70088 (54792)	Loss/tok 3.5409 (3.2650)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.275 (0.256)	Data 9.92e-05 (1.13e-03)	Tok/s 62270 (54772)	Loss/tok 3.2811 (3.2648)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.216 (0.256)	Data 9.99e-05 (1.12e-03)	Tok/s 47413 (54738)	Loss/tok 3.0095 (3.2646)	LR 2.000e-03
0: TRAIN [2][730/1938]	Time 0.335 (0.256)	Data 1.19e-04 (1.11e-03)	Tok/s 68522 (54750)	Loss/tok 3.4357 (3.2648)	LR 2.000e-03
0: TRAIN [2][740/1938]	Time 0.335 (0.256)	Data 9.97e-05 (1.09e-03)	Tok/s 69181 (54718)	Loss/tok 3.3811 (3.2641)	LR 2.000e-03
0: TRAIN [2][750/1938]	Time 0.216 (0.256)	Data 1.01e-04 (1.08e-03)	Tok/s 47101 (54737)	Loss/tok 3.1004 (3.2645)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.215 (0.256)	Data 1.44e-04 (1.07e-03)	Tok/s 47342 (54727)	Loss/tok 2.9968 (3.2638)	LR 2.000e-03
0: TRAIN [2][770/1938]	Time 0.335 (0.256)	Data 1.03e-04 (1.06e-03)	Tok/s 68816 (54717)	Loss/tok 3.3461 (3.2629)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][780/1938]	Time 0.215 (0.256)	Data 1.03e-04 (1.04e-03)	Tok/s 47691 (54730)	Loss/tok 3.0550 (3.2638)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.274 (0.256)	Data 1.06e-04 (1.03e-03)	Tok/s 60977 (54779)	Loss/tok 3.2656 (3.2638)	LR 2.000e-03
0: TRAIN [2][800/1938]	Time 0.334 (0.256)	Data 1.20e-04 (1.02e-03)	Tok/s 69644 (54767)	Loss/tok 3.4993 (3.2633)	LR 2.000e-03
0: TRAIN [2][810/1938]	Time 0.275 (0.256)	Data 1.06e-04 (1.01e-03)	Tok/s 60898 (54773)	Loss/tok 3.1838 (3.2637)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.217 (0.256)	Data 1.38e-04 (9.99e-04)	Tok/s 47592 (54801)	Loss/tok 3.0521 (3.2647)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.275 (0.256)	Data 1.77e-04 (9.88e-04)	Tok/s 60442 (54805)	Loss/tok 3.2725 (3.2650)	LR 2.000e-03
0: TRAIN [2][840/1938]	Time 0.275 (0.256)	Data 1.29e-04 (9.78e-04)	Tok/s 61009 (54782)	Loss/tok 3.3070 (3.2644)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.216 (0.256)	Data 1.09e-04 (9.68e-04)	Tok/s 47666 (54807)	Loss/tok 3.1090 (3.2660)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.276 (0.256)	Data 2.16e-04 (9.59e-04)	Tok/s 61048 (54813)	Loss/tok 3.1616 (3.2662)	LR 2.000e-03
0: TRAIN [2][870/1938]	Time 0.162 (0.256)	Data 1.03e-04 (9.49e-04)	Tok/s 32916 (54800)	Loss/tok 2.6995 (3.2655)	LR 2.000e-03
0: TRAIN [2][880/1938]	Time 0.274 (0.256)	Data 3.19e-04 (9.40e-04)	Tok/s 60337 (54836)	Loss/tok 3.2399 (3.2650)	LR 2.000e-03
0: TRAIN [2][890/1938]	Time 0.275 (0.256)	Data 1.06e-04 (9.30e-04)	Tok/s 60817 (54880)	Loss/tok 3.2077 (3.2654)	LR 2.000e-03
0: TRAIN [2][900/1938]	Time 0.215 (0.257)	Data 1.03e-04 (9.21e-04)	Tok/s 47831 (54948)	Loss/tok 3.0272 (3.2675)	LR 2.000e-03
0: TRAIN [2][910/1938]	Time 0.275 (0.257)	Data 1.03e-04 (9.12e-04)	Tok/s 61377 (54882)	Loss/tok 3.1886 (3.2662)	LR 2.000e-03
0: TRAIN [2][920/1938]	Time 0.275 (0.256)	Data 1.05e-04 (9.04e-04)	Tok/s 60837 (54861)	Loss/tok 3.3059 (3.2654)	LR 2.000e-03
0: TRAIN [2][930/1938]	Time 0.216 (0.256)	Data 1.02e-04 (8.96e-04)	Tok/s 48631 (54780)	Loss/tok 2.9732 (3.2647)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.216 (0.256)	Data 1.01e-04 (8.87e-04)	Tok/s 47034 (54804)	Loss/tok 3.0629 (3.2650)	LR 2.000e-03
0: TRAIN [2][950/1938]	Time 0.275 (0.256)	Data 1.06e-04 (8.79e-04)	Tok/s 60638 (54808)	Loss/tok 3.2960 (3.2641)	LR 2.000e-03
0: TRAIN [2][960/1938]	Time 0.216 (0.256)	Data 1.04e-04 (8.71e-04)	Tok/s 47436 (54743)	Loss/tok 2.9884 (3.2630)	LR 2.000e-03
0: TRAIN [2][970/1938]	Time 0.276 (0.256)	Data 1.08e-04 (8.63e-04)	Tok/s 61003 (54816)	Loss/tok 3.3194 (3.2646)	LR 2.000e-03
0: TRAIN [2][980/1938]	Time 0.217 (0.256)	Data 1.04e-04 (8.56e-04)	Tok/s 47754 (54825)	Loss/tok 3.0264 (3.2652)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.216 (0.256)	Data 1.22e-04 (8.49e-04)	Tok/s 47796 (54803)	Loss/tok 3.0377 (3.2645)	LR 2.000e-03
0: TRAIN [2][1000/1938]	Time 0.276 (0.256)	Data 1.44e-04 (8.42e-04)	Tok/s 61237 (54820)	Loss/tok 3.1568 (3.2650)	LR 2.000e-03
0: TRAIN [2][1010/1938]	Time 0.409 (0.256)	Data 1.00e-04 (8.35e-04)	Tok/s 73207 (54855)	Loss/tok 3.6101 (3.2662)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1020/1938]	Time 0.275 (0.256)	Data 1.07e-04 (8.28e-04)	Tok/s 60348 (54819)	Loss/tok 3.3325 (3.2654)	LR 2.000e-03
0: TRAIN [2][1030/1938]	Time 0.406 (0.256)	Data 1.79e-04 (8.21e-04)	Tok/s 73678 (54841)	Loss/tok 3.4932 (3.2657)	LR 2.000e-03
0: TRAIN [2][1040/1938]	Time 0.275 (0.256)	Data 1.16e-04 (8.16e-04)	Tok/s 59839 (54852)	Loss/tok 3.4209 (3.2656)	LR 2.000e-03
0: TRAIN [2][1050/1938]	Time 0.337 (0.256)	Data 1.01e-04 (8.09e-04)	Tok/s 69390 (54837)	Loss/tok 3.4324 (3.2656)	LR 2.000e-03
0: TRAIN [2][1060/1938]	Time 0.216 (0.256)	Data 1.08e-04 (8.03e-04)	Tok/s 47171 (54827)	Loss/tok 3.1331 (3.2649)	LR 2.000e-03
0: TRAIN [2][1070/1938]	Time 0.216 (0.256)	Data 1.13e-04 (7.97e-04)	Tok/s 46582 (54850)	Loss/tok 3.0900 (3.2649)	LR 2.000e-03
0: TRAIN [2][1080/1938]	Time 0.275 (0.256)	Data 1.14e-04 (7.90e-04)	Tok/s 61592 (54864)	Loss/tok 3.1546 (3.2650)	LR 2.000e-03
0: TRAIN [2][1090/1938]	Time 0.160 (0.257)	Data 9.89e-05 (7.84e-04)	Tok/s 32959 (54872)	Loss/tok 2.6905 (3.2652)	LR 2.000e-03
0: TRAIN [2][1100/1938]	Time 0.161 (0.256)	Data 1.20e-04 (7.78e-04)	Tok/s 32693 (54841)	Loss/tok 2.5517 (3.2644)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.216 (0.256)	Data 9.92e-05 (7.72e-04)	Tok/s 46971 (54772)	Loss/tok 3.0886 (3.2633)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.216 (0.256)	Data 1.02e-04 (7.67e-04)	Tok/s 47281 (54805)	Loss/tok 3.0987 (3.2644)	LR 2.000e-03
0: TRAIN [2][1130/1938]	Time 0.216 (0.256)	Data 1.25e-04 (7.61e-04)	Tok/s 47061 (54824)	Loss/tok 3.0673 (3.2647)	LR 2.000e-03
0: TRAIN [2][1140/1938]	Time 0.161 (0.256)	Data 1.34e-04 (7.56e-04)	Tok/s 33295 (54842)	Loss/tok 2.6983 (3.2652)	LR 2.000e-03
0: TRAIN [2][1150/1938]	Time 0.216 (0.256)	Data 1.01e-04 (7.50e-04)	Tok/s 48159 (54796)	Loss/tok 2.9652 (3.2646)	LR 2.000e-03
0: TRAIN [2][1160/1938]	Time 0.159 (0.256)	Data 1.00e-04 (7.45e-04)	Tok/s 33626 (54788)	Loss/tok 2.6093 (3.2646)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.215 (0.256)	Data 3.66e-04 (7.40e-04)	Tok/s 48369 (54806)	Loss/tok 3.2303 (3.2651)	LR 2.000e-03
0: TRAIN [2][1180/1938]	Time 0.407 (0.256)	Data 1.47e-04 (7.35e-04)	Tok/s 74338 (54795)	Loss/tok 3.4954 (3.2648)	LR 2.000e-03
0: TRAIN [2][1190/1938]	Time 0.216 (0.256)	Data 1.04e-04 (7.30e-04)	Tok/s 48175 (54776)	Loss/tok 3.1174 (3.2643)	LR 2.000e-03
0: TRAIN [2][1200/1938]	Time 0.215 (0.256)	Data 1.04e-04 (7.24e-04)	Tok/s 48420 (54797)	Loss/tok 2.9640 (3.2648)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.216 (0.256)	Data 1.23e-04 (7.19e-04)	Tok/s 46983 (54753)	Loss/tok 3.0878 (3.2641)	LR 2.000e-03
0: TRAIN [2][1220/1938]	Time 0.407 (0.256)	Data 1.03e-04 (7.14e-04)	Tok/s 72992 (54724)	Loss/tok 3.5489 (3.2639)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1230/1938]	Time 0.275 (0.256)	Data 1.05e-04 (7.09e-04)	Tok/s 60859 (54742)	Loss/tok 3.3403 (3.2647)	LR 2.000e-03
0: TRAIN [2][1240/1938]	Time 0.334 (0.256)	Data 1.06e-04 (7.05e-04)	Tok/s 69808 (54737)	Loss/tok 3.5023 (3.2642)	LR 2.000e-03
0: TRAIN [2][1250/1938]	Time 0.216 (0.256)	Data 1.06e-04 (7.00e-04)	Tok/s 47961 (54761)	Loss/tok 2.9896 (3.2646)	LR 2.000e-03
0: TRAIN [2][1260/1938]	Time 0.216 (0.256)	Data 1.05e-04 (6.96e-04)	Tok/s 47432 (54751)	Loss/tok 2.9982 (3.2645)	LR 2.000e-03
0: TRAIN [2][1270/1938]	Time 0.276 (0.256)	Data 1.08e-04 (6.92e-04)	Tok/s 61395 (54769)	Loss/tok 3.3870 (3.2645)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.275 (0.256)	Data 1.04e-04 (6.87e-04)	Tok/s 61087 (54757)	Loss/tok 3.2946 (3.2643)	LR 2.000e-03
0: TRAIN [2][1290/1938]	Time 0.275 (0.256)	Data 1.08e-04 (6.83e-04)	Tok/s 60602 (54782)	Loss/tok 3.3424 (3.2646)	LR 2.000e-03
0: TRAIN [2][1300/1938]	Time 0.275 (0.256)	Data 1.26e-04 (6.78e-04)	Tok/s 61592 (54794)	Loss/tok 3.2419 (3.2646)	LR 2.000e-03
0: TRAIN [2][1310/1938]	Time 0.274 (0.256)	Data 1.04e-04 (6.74e-04)	Tok/s 62145 (54844)	Loss/tok 3.2473 (3.2649)	LR 2.000e-03
0: TRAIN [2][1320/1938]	Time 0.407 (0.256)	Data 1.16e-04 (6.70e-04)	Tok/s 74477 (54859)	Loss/tok 3.5689 (3.2648)	LR 2.000e-03
0: TRAIN [2][1330/1938]	Time 0.407 (0.257)	Data 9.87e-05 (6.66e-04)	Tok/s 73233 (54897)	Loss/tok 3.5550 (3.2651)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.216 (0.257)	Data 3.18e-04 (6.62e-04)	Tok/s 47680 (54915)	Loss/tok 3.0226 (3.2649)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.159 (0.257)	Data 9.63e-05 (6.58e-04)	Tok/s 33511 (54896)	Loss/tok 2.6598 (3.2646)	LR 2.000e-03
0: TRAIN [2][1360/1938]	Time 0.335 (0.257)	Data 1.13e-04 (6.54e-04)	Tok/s 69159 (54955)	Loss/tok 3.4440 (3.2657)	LR 2.000e-03
0: TRAIN [2][1370/1938]	Time 0.216 (0.257)	Data 1.01e-04 (6.50e-04)	Tok/s 47335 (54949)	Loss/tok 2.9642 (3.2655)	LR 2.000e-03
0: TRAIN [2][1380/1938]	Time 0.216 (0.257)	Data 1.02e-04 (6.47e-04)	Tok/s 49581 (54921)	Loss/tok 3.0006 (3.2646)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.405 (0.257)	Data 1.07e-04 (6.43e-04)	Tok/s 73309 (54920)	Loss/tok 3.4989 (3.2644)	LR 2.000e-03
0: TRAIN [2][1400/1938]	Time 0.215 (0.257)	Data 1.09e-04 (6.39e-04)	Tok/s 47925 (54902)	Loss/tok 3.0322 (3.2638)	LR 2.000e-03
0: TRAIN [2][1410/1938]	Time 0.335 (0.256)	Data 1.19e-04 (6.36e-04)	Tok/s 69920 (54881)	Loss/tok 3.4633 (3.2640)	LR 2.000e-03
0: TRAIN [2][1420/1938]	Time 0.217 (0.257)	Data 3.27e-04 (6.33e-04)	Tok/s 48155 (54897)	Loss/tok 3.0135 (3.2639)	LR 2.000e-03
0: TRAIN [2][1430/1938]	Time 0.160 (0.256)	Data 1.25e-04 (6.29e-04)	Tok/s 33727 (54845)	Loss/tok 2.6705 (3.2631)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.275 (0.256)	Data 1.23e-04 (6.26e-04)	Tok/s 61448 (54854)	Loss/tok 3.2173 (3.2628)	LR 2.000e-03
0: TRAIN [2][1450/1938]	Time 0.276 (0.256)	Data 1.05e-04 (6.22e-04)	Tok/s 60850 (54859)	Loss/tok 3.1323 (3.2628)	LR 2.000e-03
0: TRAIN [2][1460/1938]	Time 0.275 (0.256)	Data 1.03e-04 (6.19e-04)	Tok/s 61151 (54847)	Loss/tok 3.4057 (3.2621)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.335 (0.256)	Data 1.12e-04 (6.16e-04)	Tok/s 69680 (54885)	Loss/tok 3.4438 (3.2628)	LR 2.000e-03
0: TRAIN [2][1480/1938]	Time 0.216 (0.256)	Data 1.06e-04 (6.13e-04)	Tok/s 48280 (54853)	Loss/tok 2.9476 (3.2623)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.336 (0.256)	Data 1.07e-04 (6.09e-04)	Tok/s 69526 (54872)	Loss/tok 3.3324 (3.2625)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1500/1938]	Time 0.215 (0.256)	Data 1.04e-04 (6.06e-04)	Tok/s 48404 (54899)	Loss/tok 3.1319 (3.2623)	LR 2.000e-03
0: TRAIN [2][1510/1938]	Time 0.275 (0.256)	Data 1.19e-04 (6.03e-04)	Tok/s 62147 (54901)	Loss/tok 3.2310 (3.2619)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.215 (0.256)	Data 1.01e-04 (6.00e-04)	Tok/s 48539 (54880)	Loss/tok 2.9747 (3.2614)	LR 2.000e-03
0: TRAIN [2][1530/1938]	Time 0.161 (0.256)	Data 1.18e-04 (5.97e-04)	Tok/s 32672 (54840)	Loss/tok 2.7161 (3.2608)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1540/1938]	Time 0.215 (0.256)	Data 1.10e-04 (5.93e-04)	Tok/s 47771 (54807)	Loss/tok 2.9532 (3.2600)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.216 (0.256)	Data 1.08e-04 (5.90e-04)	Tok/s 48488 (54819)	Loss/tok 3.0320 (3.2609)	LR 2.000e-03
0: TRAIN [2][1560/1938]	Time 0.215 (0.256)	Data 1.19e-04 (5.87e-04)	Tok/s 48162 (54810)	Loss/tok 3.0253 (3.2603)	LR 2.000e-03
0: TRAIN [2][1570/1938]	Time 0.160 (0.256)	Data 1.17e-04 (5.84e-04)	Tok/s 32885 (54804)	Loss/tok 2.6533 (3.2610)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.334 (0.256)	Data 9.99e-05 (5.81e-04)	Tok/s 69426 (54828)	Loss/tok 3.4545 (3.2615)	LR 2.000e-03
0: TRAIN [2][1590/1938]	Time 0.218 (0.256)	Data 1.26e-04 (5.78e-04)	Tok/s 46761 (54762)	Loss/tok 3.0708 (3.2606)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.216 (0.256)	Data 1.05e-04 (5.76e-04)	Tok/s 47820 (54784)	Loss/tok 3.0894 (3.2608)	LR 2.000e-03
0: TRAIN [2][1610/1938]	Time 0.275 (0.256)	Data 1.08e-04 (5.73e-04)	Tok/s 60284 (54809)	Loss/tok 3.2384 (3.2610)	LR 2.000e-03
0: TRAIN [2][1620/1938]	Time 0.216 (0.256)	Data 1.08e-04 (5.70e-04)	Tok/s 47961 (54781)	Loss/tok 2.9836 (3.2601)	LR 2.000e-03
0: TRAIN [2][1630/1938]	Time 0.216 (0.256)	Data 1.18e-04 (5.68e-04)	Tok/s 46777 (54775)	Loss/tok 3.0205 (3.2597)	LR 2.000e-03
0: TRAIN [2][1640/1938]	Time 0.215 (0.256)	Data 1.39e-04 (5.65e-04)	Tok/s 47571 (54786)	Loss/tok 3.0447 (3.2596)	LR 2.000e-03
0: TRAIN [2][1650/1938]	Time 0.215 (0.256)	Data 1.18e-04 (5.62e-04)	Tok/s 47589 (54771)	Loss/tok 3.0127 (3.2592)	LR 2.000e-03
0: TRAIN [2][1660/1938]	Time 0.275 (0.256)	Data 1.33e-04 (5.60e-04)	Tok/s 60637 (54775)	Loss/tok 3.3268 (3.2590)	LR 2.000e-03
0: TRAIN [2][1670/1938]	Time 0.216 (0.255)	Data 1.46e-04 (5.57e-04)	Tok/s 47957 (54764)	Loss/tok 3.0023 (3.2587)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.160 (0.255)	Data 9.97e-05 (5.55e-04)	Tok/s 33021 (54706)	Loss/tok 2.5661 (3.2582)	LR 2.000e-03
0: TRAIN [2][1690/1938]	Time 0.275 (0.255)	Data 1.03e-04 (5.52e-04)	Tok/s 61260 (54694)	Loss/tok 3.3016 (3.2578)	LR 2.000e-03
0: TRAIN [2][1700/1938]	Time 0.275 (0.255)	Data 1.15e-04 (5.50e-04)	Tok/s 61074 (54697)	Loss/tok 3.3541 (3.2579)	LR 2.000e-03
0: TRAIN [2][1710/1938]	Time 0.335 (0.255)	Data 1.13e-04 (5.47e-04)	Tok/s 70314 (54681)	Loss/tok 3.3243 (3.2575)	LR 2.000e-03
0: TRAIN [2][1720/1938]	Time 0.161 (0.255)	Data 1.57e-04 (5.45e-04)	Tok/s 32530 (54681)	Loss/tok 2.6108 (3.2574)	LR 2.000e-03
0: TRAIN [2][1730/1938]	Time 0.275 (0.255)	Data 1.01e-04 (5.42e-04)	Tok/s 61493 (54673)	Loss/tok 3.2808 (3.2571)	LR 2.000e-03
0: TRAIN [2][1740/1938]	Time 0.161 (0.255)	Data 1.01e-04 (5.40e-04)	Tok/s 32511 (54642)	Loss/tok 2.7179 (3.2565)	LR 2.000e-03
0: TRAIN [2][1750/1938]	Time 0.275 (0.255)	Data 1.05e-04 (5.38e-04)	Tok/s 61298 (54657)	Loss/tok 3.2211 (3.2567)	LR 2.000e-03
0: TRAIN [2][1760/1938]	Time 0.274 (0.255)	Data 1.06e-04 (5.35e-04)	Tok/s 62034 (54659)	Loss/tok 3.2884 (3.2573)	LR 2.000e-03
0: TRAIN [2][1770/1938]	Time 0.216 (0.255)	Data 1.14e-04 (5.33e-04)	Tok/s 47518 (54643)	Loss/tok 2.9744 (3.2569)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.275 (0.255)	Data 1.37e-04 (5.30e-04)	Tok/s 61152 (54674)	Loss/tok 3.2430 (3.2571)	LR 2.000e-03
0: TRAIN [2][1790/1938]	Time 0.274 (0.255)	Data 1.07e-04 (5.28e-04)	Tok/s 62593 (54666)	Loss/tok 3.1905 (3.2567)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1800/1938]	Time 0.215 (0.255)	Data 1.05e-04 (5.26e-04)	Tok/s 48462 (54641)	Loss/tok 3.0193 (3.2562)	LR 2.000e-03
0: TRAIN [2][1810/1938]	Time 0.216 (0.255)	Data 1.24e-04 (5.24e-04)	Tok/s 47893 (54643)	Loss/tok 2.9972 (3.2560)	LR 2.000e-03
0: TRAIN [2][1820/1938]	Time 0.215 (0.255)	Data 1.20e-04 (5.21e-04)	Tok/s 48344 (54636)	Loss/tok 3.1663 (3.2559)	LR 2.000e-03
0: TRAIN [2][1830/1938]	Time 0.216 (0.255)	Data 1.03e-04 (5.19e-04)	Tok/s 48111 (54631)	Loss/tok 3.1632 (3.2561)	LR 2.000e-03
0: TRAIN [2][1840/1938]	Time 0.215 (0.255)	Data 1.19e-04 (5.17e-04)	Tok/s 47482 (54652)	Loss/tok 3.0554 (3.2565)	LR 2.000e-03
0: TRAIN [2][1850/1938]	Time 0.274 (0.255)	Data 1.05e-04 (5.15e-04)	Tok/s 61336 (54652)	Loss/tok 3.1654 (3.2565)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.216 (0.255)	Data 1.03e-04 (5.13e-04)	Tok/s 48858 (54636)	Loss/tok 3.0849 (3.2559)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1870/1938]	Time 0.215 (0.255)	Data 1.05e-04 (5.11e-04)	Tok/s 47521 (54650)	Loss/tok 2.9828 (3.2563)	LR 2.000e-03
0: TRAIN [2][1880/1938]	Time 0.216 (0.255)	Data 1.04e-04 (5.08e-04)	Tok/s 47543 (54653)	Loss/tok 3.0783 (3.2563)	LR 2.000e-03
0: TRAIN [2][1890/1938]	Time 0.216 (0.255)	Data 1.95e-04 (5.06e-04)	Tok/s 47420 (54664)	Loss/tok 3.0519 (3.2564)	LR 2.000e-03
0: TRAIN [2][1900/1938]	Time 0.217 (0.255)	Data 1.08e-04 (5.04e-04)	Tok/s 47975 (54666)	Loss/tok 3.1323 (3.2563)	LR 2.000e-03
0: TRAIN [2][1910/1938]	Time 0.336 (0.255)	Data 1.08e-04 (5.02e-04)	Tok/s 69308 (54693)	Loss/tok 3.4280 (3.2567)	LR 2.000e-03
0: TRAIN [2][1920/1938]	Time 0.216 (0.255)	Data 5.27e-04 (5.00e-04)	Tok/s 47371 (54712)	Loss/tok 3.0459 (3.2572)	LR 2.000e-03
0: TRAIN [2][1930/1938]	Time 0.335 (0.255)	Data 1.02e-04 (4.98e-04)	Tok/s 69408 (54725)	Loss/tok 3.3597 (3.2572)	LR 2.000e-03
:::MLL 1571245403.501 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1571245403.501 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/2]	Time 1.023 (1.023)	Decoder iters 96.0 (96.0)	Tok/s 28955 (28955)
0: Running moses detokenizer
0: BLEU(score=23.10170668448498, counts=[36077, 17761, 9958, 5802], totals=[64157, 61154, 58152, 55155], precisions=[56.232367473541466, 29.043071589757005, 17.124088595405144, 10.519445199891216], bp=0.991943101827661, sys_len=64157, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571245405.835 eval_accuracy: {"value": 23.1, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1571245405.835 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.2565	Test BLEU: 23.10
0: Performance: Epoch: 2	Training: 438025 Tok/s
0: Finished epoch 2
:::MLL 1571245405.835 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1571245405.836 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571245405.836 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 965141686
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][0/1938]	Time 0.929 (0.929)	Data 6.97e-01 (6.97e-01)	Tok/s 11148 (11148)	Loss/tok 2.9089 (2.9089)	LR 2.000e-03
0: TRAIN [3][10/1938]	Time 0.335 (0.331)	Data 1.07e-04 (6.35e-02)	Tok/s 68862 (52551)	Loss/tok 3.2793 (3.2052)	LR 2.000e-03
0: TRAIN [3][20/1938]	Time 0.275 (0.292)	Data 1.21e-04 (3.33e-02)	Tok/s 60567 (51747)	Loss/tok 3.2481 (3.2150)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.216 (0.285)	Data 1.09e-04 (2.26e-02)	Tok/s 48321 (53843)	Loss/tok 3.0392 (3.2013)	LR 2.000e-03
0: TRAIN [3][40/1938]	Time 0.160 (0.282)	Data 1.26e-04 (1.71e-02)	Tok/s 34002 (54805)	Loss/tok 2.6231 (3.1964)	LR 2.000e-03
0: TRAIN [3][50/1938]	Time 0.335 (0.279)	Data 1.28e-04 (1.38e-02)	Tok/s 70036 (55512)	Loss/tok 3.3898 (3.1984)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.215 (0.276)	Data 1.13e-04 (1.15e-02)	Tok/s 48356 (55470)	Loss/tok 3.0300 (3.1943)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.276 (0.277)	Data 1.34e-04 (9.94e-03)	Tok/s 61821 (55825)	Loss/tok 3.1956 (3.2104)	LR 2.000e-03
0: TRAIN [3][80/1938]	Time 0.335 (0.275)	Data 1.14e-04 (8.73e-03)	Tok/s 69429 (55863)	Loss/tok 3.2885 (3.2044)	LR 2.000e-03
0: TRAIN [3][90/1938]	Time 0.216 (0.274)	Data 1.13e-04 (7.78e-03)	Tok/s 47263 (55978)	Loss/tok 2.9508 (3.2109)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.275 (0.274)	Data 1.14e-04 (7.02e-03)	Tok/s 61471 (56260)	Loss/tok 3.1605 (3.2078)	LR 2.000e-03
0: TRAIN [3][110/1938]	Time 0.216 (0.271)	Data 1.13e-04 (6.40e-03)	Tok/s 48083 (55978)	Loss/tok 3.1172 (3.2087)	LR 2.000e-03
0: TRAIN [3][120/1938]	Time 0.215 (0.271)	Data 1.13e-04 (5.88e-03)	Tok/s 49037 (56031)	Loss/tok 3.0250 (3.2101)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.276 (0.268)	Data 1.24e-04 (5.44e-03)	Tok/s 60982 (55797)	Loss/tok 3.1351 (3.2012)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.407 (0.266)	Data 1.26e-04 (5.07e-03)	Tok/s 73370 (55467)	Loss/tok 3.5047 (3.1967)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.275 (0.266)	Data 1.02e-04 (4.74e-03)	Tok/s 59593 (55395)	Loss/tok 3.1814 (3.1982)	LR 2.000e-03
0: TRAIN [3][160/1938]	Time 0.216 (0.265)	Data 9.87e-05 (4.45e-03)	Tok/s 47898 (55261)	Loss/tok 3.0180 (3.1935)	LR 2.000e-03
0: TRAIN [3][170/1938]	Time 0.335 (0.264)	Data 9.68e-05 (4.20e-03)	Tok/s 68696 (55258)	Loss/tok 3.3508 (3.1897)	LR 2.000e-03
0: TRAIN [3][180/1938]	Time 0.216 (0.264)	Data 9.70e-05 (3.97e-03)	Tok/s 47460 (55337)	Loss/tok 2.9990 (3.1884)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.274 (0.265)	Data 1.33e-04 (3.77e-03)	Tok/s 61667 (55575)	Loss/tok 3.1913 (3.1913)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.216 (0.265)	Data 1.11e-04 (3.59e-03)	Tok/s 47500 (55750)	Loss/tok 2.9501 (3.1904)	LR 2.000e-03
0: TRAIN [3][210/1938]	Time 0.335 (0.265)	Data 9.92e-05 (3.43e-03)	Tok/s 70303 (55821)	Loss/tok 3.3702 (3.1917)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.273 (0.264)	Data 1.96e-04 (3.28e-03)	Tok/s 61750 (55662)	Loss/tok 3.1086 (3.1883)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.406 (0.264)	Data 1.33e-04 (3.14e-03)	Tok/s 73471 (55729)	Loss/tok 3.4709 (3.1888)	LR 2.000e-03
0: TRAIN [3][240/1938]	Time 0.161 (0.263)	Data 1.25e-04 (3.02e-03)	Tok/s 32626 (55509)	Loss/tok 2.5450 (3.1844)	LR 2.000e-03
0: TRAIN [3][250/1938]	Time 0.335 (0.263)	Data 9.82e-05 (2.90e-03)	Tok/s 68871 (55568)	Loss/tok 3.3463 (3.1866)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.216 (0.262)	Data 1.00e-04 (2.80e-03)	Tok/s 47792 (55398)	Loss/tok 3.0368 (3.1837)	LR 2.000e-03
0: TRAIN [3][270/1938]	Time 0.215 (0.261)	Data 1.14e-04 (2.70e-03)	Tok/s 47558 (55305)	Loss/tok 2.9464 (3.1827)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.216 (0.261)	Data 1.15e-04 (2.61e-03)	Tok/s 48033 (55292)	Loss/tok 2.8233 (3.1800)	LR 2.000e-03
