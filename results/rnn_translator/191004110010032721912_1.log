Beginning trial 1 of 2
Gathering sys log on dss01
:::MLL 1570204908.615 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1570204908.616 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1570204908.616 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1570204908.617 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1570204908.617 submission_platform: {"value": "1xDSS8440", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1570204908.618 submission_entry: {"value": "{'hardware': 'DSS8440', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '5.0-0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz', 'num_cores': '40', 'num_vcpus': '40', 'accelerator': 'Tesla V100-PCIE-32GB', 'num_accelerators': '8', 'sys_mem_size': '754 GB', 'sys_storage_type': 'SATA SSD', 'sys_storage_size': '1x 447.1G + 1x 931.5G', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27700 Family [ConnectX-4]', 'num_network_cards': '1', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1570204908.618 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1570204908.618 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1570204914.168 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node dss01
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=4318' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=191004110010032721912 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191004110010032721912 ./run_and_time.sh
Run vars: id 191004110010032721912 gpus 8 mparams  --master_port=4318
NCCL_SOCKET_NTHREADS=2
NCCL_NSOCKS_PERTHREAD=8
STARTING TIMING RUN AT 2019-10-04 04:01:54 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=0
+ MATH=amp_fp16
+ [[ 0 -eq 1 ]]
+ LAUNCH_OPT='torch.distributed.launch --nproc_per_node 8  --master_port=4318'
running benchmark
+ echo 'running benchmark'
+ python -m torch.distributed.launch --nproc_per_node 8 --master_port=4318 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1570204916.901 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570204916.901 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570204916.901 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570204916.901 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570204916.901 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570204916.901 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570204916.901 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570204916.902 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 734378054
dss01:464:464 [0] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:464:464 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:464:464 [0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:464:464 [0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:464:464 [0] NCCL INFO NET/IB : No device found.
dss01:464:464 [0] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
NCCL version 2.4.8+cuda10.1
dss01:465:465 [1] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:465:465 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:467:467 [3] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:467:467 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:471:471 [7] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:471:471 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:465:465 [1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:465:465 [1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:465:465 [1] NCCL INFO NET/IB : No device found.

dss01:467:467 [3] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:467:467 [3] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:467:467 [3] NCCL INFO NET/IB : No device found.

dss01:471:471 [7] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:471:471 [7] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:471:471 [7] NCCL INFO NET/IB : No device found.
dss01:465:465 [1] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:467:467 [3] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:471:471 [7] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:470:470 [6] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:470:470 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:470:470 [6] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:470:470 [6] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:470:470 [6] NCCL INFO NET/IB : No device found.
dss01:470:470 [6] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:468:468 [4] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:468:468 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:468:468 [4] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:468:468 [4] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:468:468 [4] NCCL INFO NET/IB : No device found.
dss01:468:468 [4] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:466:466 [2] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:466:466 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:466:466 [2] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:466:466 [2] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:466:466 [2] NCCL INFO NET/IB : No device found.
dss01:466:466 [2] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:469:469 [5] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:469:469 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:469:469 [5] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:469:469 [5] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:469:469 [5] NCCL INFO NET/IB : No device found.
dss01:469:469 [5] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:464:826 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555
dss01:471:828 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa
dss01:465:827 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555
dss01:467:829 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555
dss01:468:831 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa
dss01:470:830 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa
dss01:466:832 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555
dss01:469:833 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa
dss01:471:828 [7] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:464:826 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:465:827 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:466:832 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:467:829 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:468:831 [4] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:470:830 [6] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:469:833 [5] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:464:826 [0] NCCL INFO Channel 00 :    0   1   2   3   4   5   6   7
dss01:470:830 [6] NCCL INFO Ring 00 : 6[6] -> 7[7] via P2P/IPC
dss01:464:826 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/IPC
dss01:466:832 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/IPC
dss01:468:831 [4] NCCL INFO Ring 00 : 4[4] -> 5[5] via P2P/IPC
dss01:469:833 [5] NCCL INFO Ring 00 : 5[5] -> 6[6] via direct shared memory
dss01:471:828 [7] NCCL INFO Ring 00 : 7[7] -> 0[0] via direct shared memory
dss01:465:827 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via direct shared memory
dss01:467:829 [3] NCCL INFO Ring 00 : 3[3] -> 4[4] via direct shared memory
dss01:464:826 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
dss01:471:828 [7] NCCL INFO comm 0x7ffe98007590 rank 7 nranks 8 cudaDev 7 nvmlDev 7 - Init COMPLETE
dss01:465:827 [1] NCCL INFO comm 0x7ffe70007590 rank 1 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE
dss01:469:833 [5] NCCL INFO comm 0x7ffe98007590 rank 5 nranks 8 cudaDev 5 nvmlDev 5 - Init COMPLETE
dss01:467:829 [3] NCCL INFO comm 0x7fff38007590 rank 3 nranks 8 cudaDev 3 nvmlDev 3 - Init COMPLETE
dss01:470:830 [6] NCCL INFO comm 0x7ffe98007590 rank 6 nranks 8 cudaDev 6 nvmlDev 6 - Init COMPLETE
dss01:468:831 [4] NCCL INFO comm 0x7fff64007590 rank 4 nranks 8 cudaDev 4 nvmlDev 4 - Init COMPLETE
dss01:464:826 [0] NCCL INFO comm 0x7fff38007590 rank 0 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE
dss01:464:464 [0] NCCL INFO Launch mode Parallel
0: Worker 0 is using worker seed: 1831769491
0: Building vocabulary from /data/vocab.bpe.32000
dss01:466:832 [2] NCCL INFO comm 0x7ffe84007590 rank 2 nranks 8 cudaDev 2 nvmlDev 2 - Init COMPLETE
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1570204940.204 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1570204942.952 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1570204942.952 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1570204942.953 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1570204943.992 global_batch_size: {"value": 2048, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1570204943.993 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1570204943.994 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1570204943.994 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1570204943.994 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1570204943.995 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1570204943.995 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1570204943.995 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1570204944.020 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1570204944.021 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 2214730401
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/1938]	Time 1.012 (1.012)	Data 7.86e-01 (7.86e-01)	Tok/s 10221 (10221)	Loss/tok 10.6274 (10.6274)	LR 2.000e-05
0: TRAIN [0][10/1938]	Time 0.215 (0.330)	Data 1.10e-04 (7.16e-02)	Tok/s 47349 (50723)	Loss/tok 9.6592 (10.1576)	LR 2.518e-05
0: TRAIN [0][20/1938]	Time 0.215 (0.303)	Data 1.34e-04 (3.76e-02)	Tok/s 48349 (53710)	Loss/tok 9.1992 (9.8393)	LR 3.170e-05
0: TRAIN [0][30/1938]	Time 0.336 (0.300)	Data 1.25e-04 (2.55e-02)	Tok/s 68652 (56719)	Loss/tok 9.1392 (9.6104)	LR 3.991e-05
0: TRAIN [0][40/1938]	Time 0.274 (0.291)	Data 1.11e-04 (1.93e-02)	Tok/s 61204 (56698)	Loss/tok 8.8675 (9.4556)	LR 5.024e-05
0: TRAIN [0][50/1938]	Time 0.277 (0.280)	Data 1.24e-04 (1.55e-02)	Tok/s 60462 (55677)	Loss/tok 8.6592 (9.3359)	LR 6.325e-05
0: TRAIN [0][60/1938]	Time 0.276 (0.277)	Data 1.04e-04 (1.30e-02)	Tok/s 60433 (55643)	Loss/tok 8.5096 (9.2115)	LR 7.962e-05
0: TRAIN [0][70/1938]	Time 0.215 (0.275)	Data 1.02e-04 (1.12e-02)	Tok/s 47735 (55715)	Loss/tok 8.1374 (9.0936)	LR 1.002e-04
0: TRAIN [0][80/1938]	Time 0.216 (0.275)	Data 1.17e-04 (9.82e-03)	Tok/s 47717 (55827)	Loss/tok 8.0433 (8.9813)	LR 1.262e-04
0: TRAIN [0][90/1938]	Time 0.160 (0.272)	Data 1.05e-04 (8.76e-03)	Tok/s 32959 (55689)	Loss/tok 7.5599 (8.8907)	LR 1.589e-04
0: TRAIN [0][100/1938]	Time 0.215 (0.270)	Data 1.24e-04 (7.90e-03)	Tok/s 48931 (55628)	Loss/tok 7.8580 (8.8079)	LR 2.000e-04
0: TRAIN [0][110/1938]	Time 0.335 (0.273)	Data 1.09e-04 (7.20e-03)	Tok/s 70009 (56214)	Loss/tok 8.0348 (8.7220)	LR 2.518e-04
0: TRAIN [0][120/1938]	Time 0.216 (0.277)	Data 1.42e-04 (6.61e-03)	Tok/s 48875 (56765)	Loss/tok 7.7504 (8.6456)	LR 3.170e-04
0: TRAIN [0][130/1938]	Time 0.215 (0.276)	Data 1.05e-04 (6.12e-03)	Tok/s 47542 (56839)	Loss/tok 7.7353 (8.5895)	LR 3.991e-04
0: TRAIN [0][140/1938]	Time 0.216 (0.275)	Data 1.03e-04 (5.69e-03)	Tok/s 47344 (56783)	Loss/tok 7.6569 (8.5402)	LR 5.024e-04
0: TRAIN [0][150/1938]	Time 0.160 (0.272)	Data 1.01e-04 (5.32e-03)	Tok/s 32888 (56449)	Loss/tok 6.9389 (8.4963)	LR 6.325e-04
0: TRAIN [0][160/1938]	Time 0.275 (0.274)	Data 1.04e-04 (5.00e-03)	Tok/s 60845 (56828)	Loss/tok 7.6462 (8.4416)	LR 7.962e-04
0: TRAIN [0][170/1938]	Time 0.215 (0.272)	Data 1.20e-04 (4.71e-03)	Tok/s 49087 (56529)	Loss/tok 7.3018 (8.3967)	LR 1.002e-03
0: TRAIN [0][180/1938]	Time 0.159 (0.270)	Data 1.05e-04 (4.46e-03)	Tok/s 33480 (56346)	Loss/tok 6.4997 (8.3456)	LR 1.262e-03
0: TRAIN [0][190/1938]	Time 0.216 (0.269)	Data 1.18e-04 (4.23e-03)	Tok/s 48757 (56240)	Loss/tok 6.9858 (8.2912)	LR 1.589e-03
0: TRAIN [0][200/1938]	Time 0.215 (0.267)	Data 1.11e-04 (4.03e-03)	Tok/s 48316 (56044)	Loss/tok 6.7127 (8.2351)	LR 2.000e-03
0: TRAIN [0][210/1938]	Time 0.215 (0.266)	Data 1.15e-04 (3.84e-03)	Tok/s 48448 (55855)	Loss/tok 6.5420 (8.1782)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.215 (0.267)	Data 1.14e-04 (3.67e-03)	Tok/s 47901 (55972)	Loss/tok 6.4452 (8.1104)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.278 (0.268)	Data 1.07e-04 (3.52e-03)	Tok/s 59316 (56062)	Loss/tok 6.5503 (8.0428)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.277 (0.267)	Data 1.16e-04 (3.38e-03)	Tok/s 60456 (55959)	Loss/tok 6.4982 (7.9823)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.277 (0.267)	Data 1.08e-04 (3.25e-03)	Tok/s 61162 (56009)	Loss/tok 6.2853 (7.9183)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.217 (0.267)	Data 1.05e-04 (3.13e-03)	Tok/s 47498 (55961)	Loss/tok 5.9387 (7.8573)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.216 (0.266)	Data 1.04e-04 (3.02e-03)	Tok/s 47978 (55863)	Loss/tok 5.8554 (7.8013)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.161 (0.265)	Data 1.55e-04 (2.91e-03)	Tok/s 32335 (55729)	Loss/tok 5.0288 (7.7453)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.279 (0.265)	Data 1.05e-04 (2.82e-03)	Tok/s 59583 (55709)	Loss/tok 5.9848 (7.6864)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.216 (0.265)	Data 1.04e-04 (2.73e-03)	Tok/s 47883 (55758)	Loss/tok 5.5213 (7.6228)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.414 (0.265)	Data 1.40e-04 (2.64e-03)	Tok/s 71917 (55746)	Loss/tok 6.0723 (7.5626)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.216 (0.265)	Data 1.42e-04 (2.57e-03)	Tok/s 47042 (55695)	Loss/tok 5.3843 (7.5064)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][330/1938]	Time 0.160 (0.265)	Data 9.89e-05 (2.49e-03)	Tok/s 32888 (55633)	Loss/tok 4.3908 (7.4510)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.338 (0.264)	Data 1.02e-04 (2.42e-03)	Tok/s 68547 (55541)	Loss/tok 5.7600 (7.3987)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.277 (0.264)	Data 1.01e-04 (2.36e-03)	Tok/s 60715 (55486)	Loss/tok 5.4599 (7.3456)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.339 (0.263)	Data 1.00e-04 (2.29e-03)	Tok/s 68862 (55437)	Loss/tok 5.5629 (7.2945)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.216 (0.263)	Data 9.66e-05 (2.23e-03)	Tok/s 47546 (55436)	Loss/tok 4.9303 (7.2414)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.279 (0.263)	Data 1.57e-04 (2.18e-03)	Tok/s 61224 (55437)	Loss/tok 5.2168 (7.1882)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.216 (0.262)	Data 1.55e-04 (2.13e-03)	Tok/s 49268 (55377)	Loss/tok 4.6146 (7.1392)	LR 2.000e-03
0: TRAIN [0][400/1938]	Time 0.278 (0.262)	Data 1.64e-04 (2.08e-03)	Tok/s 60406 (55294)	Loss/tok 5.0158 (7.0910)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.275 (0.262)	Data 2.01e-04 (2.03e-03)	Tok/s 60842 (55340)	Loss/tok 4.8784 (7.0368)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.218 (0.262)	Data 9.99e-05 (1.99e-03)	Tok/s 47837 (55310)	Loss/tok 4.4769 (6.9881)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.216 (0.262)	Data 2.70e-04 (1.95e-03)	Tok/s 48749 (55380)	Loss/tok 4.2946 (6.9319)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.218 (0.262)	Data 1.63e-04 (1.91e-03)	Tok/s 47782 (55384)	Loss/tok 4.4047 (6.8821)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.160 (0.262)	Data 1.31e-04 (1.87e-03)	Tok/s 32377 (55264)	Loss/tok 3.7410 (6.8376)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][460/1938]	Time 0.413 (0.262)	Data 1.55e-04 (1.84e-03)	Tok/s 73030 (55255)	Loss/tok 4.8459 (6.7886)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.412 (0.262)	Data 1.65e-04 (1.80e-03)	Tok/s 71987 (55183)	Loss/tok 5.0687 (6.7456)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.278 (0.262)	Data 1.82e-04 (1.77e-03)	Tok/s 60334 (55154)	Loss/tok 4.5970 (6.7034)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.163 (0.262)	Data 1.40e-04 (1.73e-03)	Tok/s 31716 (55112)	Loss/tok 3.5097 (6.6609)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.216 (0.261)	Data 1.14e-04 (1.70e-03)	Tok/s 47151 (54922)	Loss/tok 4.0691 (6.6277)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.278 (0.261)	Data 1.64e-04 (1.67e-03)	Tok/s 60170 (54892)	Loss/tok 4.6114 (6.5876)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.278 (0.260)	Data 1.66e-04 (1.64e-03)	Tok/s 60364 (54797)	Loss/tok 4.4880 (6.5518)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.216 (0.259)	Data 1.43e-04 (1.62e-03)	Tok/s 48127 (54666)	Loss/tok 4.0700 (6.5195)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.341 (0.259)	Data 1.92e-04 (1.59e-03)	Tok/s 68142 (54682)	Loss/tok 4.7279 (6.4802)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.217 (0.259)	Data 1.35e-04 (1.56e-03)	Tok/s 48318 (54703)	Loss/tok 3.9366 (6.4403)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.217 (0.260)	Data 1.22e-04 (1.54e-03)	Tok/s 47638 (54862)	Loss/tok 3.8804 (6.3926)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.339 (0.261)	Data 1.51e-04 (1.51e-03)	Tok/s 68753 (54921)	Loss/tok 4.4246 (6.3527)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.340 (0.261)	Data 1.27e-04 (1.49e-03)	Tok/s 68205 (54935)	Loss/tok 4.5397 (6.3154)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.339 (0.261)	Data 1.02e-04 (1.47e-03)	Tok/s 69424 (54907)	Loss/tok 4.4567 (6.2813)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.280 (0.260)	Data 1.90e-04 (1.45e-03)	Tok/s 59171 (54876)	Loss/tok 4.2937 (6.2489)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.215 (0.260)	Data 1.30e-04 (1.42e-03)	Tok/s 48158 (54897)	Loss/tok 4.1413 (6.2145)	LR 2.000e-03
0: TRAIN [0][620/1938]	Time 0.217 (0.260)	Data 1.34e-04 (1.40e-03)	Tok/s 47638 (54806)	Loss/tok 4.0737 (6.1885)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][630/1938]	Time 0.281 (0.260)	Data 9.54e-05 (1.38e-03)	Tok/s 59572 (54756)	Loss/tok 4.2396 (6.1589)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.216 (0.259)	Data 1.13e-04 (1.36e-03)	Tok/s 47514 (54680)	Loss/tok 3.7998 (6.1319)	LR 2.000e-03
0: TRAIN [0][650/1938]	Time 0.340 (0.260)	Data 1.57e-04 (1.34e-03)	Tok/s 67815 (54713)	Loss/tok 4.4888 (6.1005)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.217 (0.259)	Data 1.86e-04 (1.33e-03)	Tok/s 47785 (54695)	Loss/tok 3.8572 (6.0723)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.276 (0.260)	Data 1.87e-04 (1.31e-03)	Tok/s 61297 (54780)	Loss/tok 4.1405 (6.0388)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.216 (0.260)	Data 1.26e-04 (1.29e-03)	Tok/s 48251 (54851)	Loss/tok 3.7455 (6.0064)	LR 2.000e-03
0: TRAIN [0][690/1938]	Time 0.278 (0.260)	Data 1.72e-04 (1.28e-03)	Tok/s 60123 (54894)	Loss/tok 3.9814 (5.9760)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.161 (0.260)	Data 1.73e-04 (1.26e-03)	Tok/s 32179 (54851)	Loss/tok 3.3286 (5.9511)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.278 (0.260)	Data 1.47e-04 (1.25e-03)	Tok/s 60071 (54841)	Loss/tok 4.1010 (5.9254)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.339 (0.260)	Data 1.44e-04 (1.23e-03)	Tok/s 68735 (54803)	Loss/tok 4.3043 (5.9017)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.159 (0.260)	Data 1.77e-04 (1.22e-03)	Tok/s 32877 (54794)	Loss/tok 3.1714 (5.8762)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.216 (0.260)	Data 1.52e-04 (1.21e-03)	Tok/s 47130 (54866)	Loss/tok 3.7853 (5.8479)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.216 (0.260)	Data 2.69e-04 (1.19e-03)	Tok/s 48251 (54882)	Loss/tok 3.8492 (5.8230)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.217 (0.260)	Data 1.61e-04 (1.18e-03)	Tok/s 47113 (54840)	Loss/tok 3.8362 (5.8013)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.279 (0.260)	Data 1.72e-04 (1.16e-03)	Tok/s 60357 (54868)	Loss/tok 3.8207 (5.7760)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.339 (0.260)	Data 2.75e-04 (1.15e-03)	Tok/s 68345 (54895)	Loss/tok 4.2490 (5.7517)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.216 (0.260)	Data 1.32e-04 (1.14e-03)	Tok/s 48057 (54830)	Loss/tok 3.6045 (5.7319)	LR 2.000e-03
0: TRAIN [0][800/1938]	Time 0.278 (0.260)	Data 1.74e-04 (1.13e-03)	Tok/s 61045 (54853)	Loss/tok 3.9258 (5.7088)	LR 2.000e-03
0: TRAIN [0][810/1938]	Time 0.338 (0.260)	Data 1.61e-04 (1.12e-03)	Tok/s 69978 (54868)	Loss/tok 4.1320 (5.6867)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.217 (0.259)	Data 1.26e-04 (1.10e-03)	Tok/s 48506 (54785)	Loss/tok 3.7998 (5.6692)	LR 2.000e-03
0: TRAIN [0][830/1938]	Time 0.277 (0.260)	Data 1.39e-04 (1.09e-03)	Tok/s 61166 (54865)	Loss/tok 4.1246 (5.6445)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.161 (0.259)	Data 4.01e-04 (1.08e-03)	Tok/s 32898 (54809)	Loss/tok 3.0481 (5.6261)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.160 (0.259)	Data 1.84e-04 (1.07e-03)	Tok/s 33165 (54775)	Loss/tok 3.0964 (5.6073)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.340 (0.260)	Data 1.77e-04 (1.06e-03)	Tok/s 69047 (54828)	Loss/tok 4.1603 (5.5852)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.216 (0.259)	Data 1.12e-04 (1.05e-03)	Tok/s 47422 (54773)	Loss/tok 3.6148 (5.5684)	LR 2.000e-03
0: TRAIN [0][880/1938]	Time 0.279 (0.259)	Data 1.99e-04 (1.04e-03)	Tok/s 59684 (54768)	Loss/tok 3.9194 (5.5493)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.215 (0.259)	Data 2.00e-04 (1.03e-03)	Tok/s 48104 (54766)	Loss/tok 3.5400 (5.5312)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.278 (0.259)	Data 1.91e-04 (1.02e-03)	Tok/s 59559 (54709)	Loss/tok 3.8657 (5.5151)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.217 (0.259)	Data 1.79e-04 (1.01e-03)	Tok/s 49104 (54739)	Loss/tok 3.6097 (5.4957)	LR 2.000e-03
0: TRAIN [0][920/1938]	Time 0.278 (0.259)	Data 4.11e-04 (1.00e-03)	Tok/s 61397 (54761)	Loss/tok 3.9167 (5.4764)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][930/1938]	Time 0.158 (0.259)	Data 1.80e-04 (9.96e-04)	Tok/s 34001 (54736)	Loss/tok 3.0721 (5.4598)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.279 (0.259)	Data 1.46e-04 (9.87e-04)	Tok/s 60055 (54742)	Loss/tok 3.8534 (5.4421)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.217 (0.259)	Data 1.75e-04 (9.79e-04)	Tok/s 48091 (54753)	Loss/tok 3.6744 (5.4251)	LR 2.000e-03
0: TRAIN [0][960/1938]	Time 0.217 (0.259)	Data 1.91e-04 (9.71e-04)	Tok/s 46789 (54702)	Loss/tok 3.7249 (5.4101)	LR 2.000e-03
0: TRAIN [0][970/1938]	Time 0.216 (0.259)	Data 1.77e-04 (9.63e-04)	Tok/s 47887 (54679)	Loss/tok 3.5458 (5.3953)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.216 (0.259)	Data 1.40e-04 (9.54e-04)	Tok/s 46940 (54688)	Loss/tok 3.6279 (5.3795)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.279 (0.259)	Data 1.73e-04 (9.46e-04)	Tok/s 61156 (54659)	Loss/tok 3.7571 (5.3646)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.219 (0.259)	Data 1.49e-04 (9.39e-04)	Tok/s 47710 (54620)	Loss/tok 3.5752 (5.3510)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.279 (0.258)	Data 1.75e-04 (9.31e-04)	Tok/s 60432 (54611)	Loss/tok 3.6719 (5.3362)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.216 (0.258)	Data 1.34e-04 (9.24e-04)	Tok/s 47782 (54536)	Loss/tok 3.5344 (5.3241)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.217 (0.258)	Data 1.44e-04 (9.16e-04)	Tok/s 47251 (54537)	Loss/tok 3.5502 (5.3097)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.341 (0.258)	Data 1.64e-04 (9.09e-04)	Tok/s 67748 (54525)	Loss/tok 4.0212 (5.2958)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.216 (0.258)	Data 1.70e-04 (9.02e-04)	Tok/s 46960 (54527)	Loss/tok 3.5319 (5.2815)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.217 (0.258)	Data 1.58e-04 (8.95e-04)	Tok/s 48641 (54504)	Loss/tok 3.3865 (5.2681)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.277 (0.258)	Data 1.77e-04 (8.88e-04)	Tok/s 59782 (54509)	Loss/tok 3.7904 (5.2537)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.280 (0.258)	Data 1.67e-04 (8.82e-04)	Tok/s 60267 (54512)	Loss/tok 3.8461 (5.2398)	LR 2.000e-03
0: TRAIN [0][1090/1938]	Time 0.161 (0.258)	Data 1.54e-04 (8.75e-04)	Tok/s 33871 (54485)	Loss/tok 2.9121 (5.2270)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.279 (0.258)	Data 1.39e-04 (8.69e-04)	Tok/s 60251 (54479)	Loss/tok 3.8442 (5.2137)	LR 2.000e-03
0: TRAIN [0][1110/1938]	Time 0.340 (0.258)	Data 1.64e-04 (8.62e-04)	Tok/s 68146 (54476)	Loss/tok 4.0412 (5.2007)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.340 (0.258)	Data 1.40e-04 (8.56e-04)	Tok/s 69334 (54468)	Loss/tok 3.9337 (5.1879)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.216 (0.258)	Data 1.46e-04 (8.50e-04)	Tok/s 47387 (54455)	Loss/tok 3.4700 (5.1757)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.280 (0.258)	Data 1.68e-04 (8.44e-04)	Tok/s 59779 (54492)	Loss/tok 3.8252 (5.1612)	LR 2.000e-03
0: TRAIN [0][1150/1938]	Time 0.216 (0.258)	Data 1.18e-04 (8.38e-04)	Tok/s 47629 (54469)	Loss/tok 3.3890 (5.1494)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.217 (0.258)	Data 1.59e-04 (8.32e-04)	Tok/s 48321 (54479)	Loss/tok 3.3893 (5.1369)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.413 (0.258)	Data 1.39e-04 (8.27e-04)	Tok/s 72019 (54483)	Loss/tok 4.1167 (5.1249)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.216 (0.258)	Data 1.59e-04 (8.21e-04)	Tok/s 48153 (54423)	Loss/tok 3.4581 (5.1149)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.341 (0.258)	Data 1.62e-04 (8.16e-04)	Tok/s 67300 (54456)	Loss/tok 4.0153 (5.1022)	LR 2.000e-03
0: TRAIN [0][1200/1938]	Time 0.217 (0.258)	Data 1.96e-04 (8.11e-04)	Tok/s 46941 (54439)	Loss/tok 3.5626 (5.0915)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.340 (0.258)	Data 1.14e-04 (8.05e-04)	Tok/s 67985 (54470)	Loss/tok 3.9827 (5.0794)	LR 2.000e-03
0: TRAIN [0][1220/1938]	Time 0.279 (0.258)	Data 1.85e-04 (8.00e-04)	Tok/s 59615 (54440)	Loss/tok 3.7241 (5.0692)	LR 2.000e-03
0: TRAIN [0][1230/1938]	Time 0.279 (0.258)	Data 1.49e-04 (7.95e-04)	Tok/s 59613 (54426)	Loss/tok 3.6198 (5.0585)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.278 (0.258)	Data 2.12e-04 (7.90e-04)	Tok/s 60267 (54432)	Loss/tok 3.7656 (5.0477)	LR 2.000e-03
0: TRAIN [0][1250/1938]	Time 0.217 (0.257)	Data 1.30e-04 (7.85e-04)	Tok/s 48057 (54395)	Loss/tok 3.4291 (5.0380)	LR 2.000e-03
0: TRAIN [0][1260/1938]	Time 0.164 (0.257)	Data 1.25e-04 (7.80e-04)	Tok/s 31942 (54395)	Loss/tok 3.0167 (5.0270)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1270/1938]	Time 0.160 (0.258)	Data 1.52e-04 (7.75e-04)	Tok/s 33160 (54408)	Loss/tok 2.8527 (5.0162)	LR 2.000e-03
0: TRAIN [0][1280/1938]	Time 0.216 (0.258)	Data 1.19e-04 (7.70e-04)	Tok/s 47533 (54394)	Loss/tok 3.4020 (5.0063)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.217 (0.258)	Data 1.11e-04 (7.65e-04)	Tok/s 48239 (54435)	Loss/tok 3.3474 (4.9947)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.279 (0.258)	Data 1.03e-04 (7.60e-04)	Tok/s 60524 (54398)	Loss/tok 3.6876 (4.9855)	LR 2.000e-03
0: TRAIN [0][1310/1938]	Time 0.279 (0.258)	Data 1.07e-04 (7.56e-04)	Tok/s 60048 (54410)	Loss/tok 3.6551 (4.9748)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.339 (0.258)	Data 1.20e-04 (7.51e-04)	Tok/s 68219 (54411)	Loss/tok 3.8225 (4.9652)	LR 2.000e-03
0: TRAIN [0][1330/1938]	Time 0.415 (0.258)	Data 1.55e-04 (7.46e-04)	Tok/s 72644 (54406)	Loss/tok 4.0946 (4.9554)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.161 (0.258)	Data 1.02e-04 (7.42e-04)	Tok/s 33043 (54388)	Loss/tok 2.9718 (4.9464)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.279 (0.257)	Data 1.02e-04 (7.37e-04)	Tok/s 60754 (54343)	Loss/tok 3.5381 (4.9379)	LR 2.000e-03
0: TRAIN [0][1360/1938]	Time 0.217 (0.258)	Data 1.40e-04 (7.32e-04)	Tok/s 47276 (54395)	Loss/tok 3.3102 (4.9274)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.217 (0.258)	Data 1.60e-04 (7.28e-04)	Tok/s 47294 (54414)	Loss/tok 3.4252 (4.9174)	LR 2.000e-03
0: TRAIN [0][1380/1938]	Time 0.341 (0.258)	Data 1.22e-04 (7.24e-04)	Tok/s 69715 (54397)	Loss/tok 3.8541 (4.9088)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.414 (0.258)	Data 1.07e-04 (7.20e-04)	Tok/s 72155 (54416)	Loss/tok 4.1716 (4.8995)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.282 (0.258)	Data 1.22e-04 (7.15e-04)	Tok/s 59496 (54398)	Loss/tok 3.5793 (4.8909)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.217 (0.258)	Data 9.99e-05 (7.11e-04)	Tok/s 46719 (54430)	Loss/tok 3.4309 (4.8811)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.216 (0.258)	Data 1.70e-04 (7.07e-04)	Tok/s 48542 (54436)	Loss/tok 3.3685 (4.8721)	LR 2.000e-03
0: TRAIN [0][1430/1938]	Time 0.217 (0.258)	Data 9.97e-05 (7.03e-04)	Tok/s 46626 (54433)	Loss/tok 3.3869 (4.8639)	LR 2.000e-03
0: TRAIN [0][1440/1938]	Time 0.160 (0.258)	Data 9.92e-05 (6.99e-04)	Tok/s 32886 (54451)	Loss/tok 2.9009 (4.8545)	LR 2.000e-03
0: TRAIN [0][1450/1938]	Time 0.217 (0.258)	Data 1.15e-04 (6.95e-04)	Tok/s 46448 (54447)	Loss/tok 3.4166 (4.8461)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.280 (0.258)	Data 9.82e-05 (6.91e-04)	Tok/s 60155 (54422)	Loss/tok 3.7040 (4.8382)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.277 (0.258)	Data 1.08e-04 (6.87e-04)	Tok/s 60067 (54430)	Loss/tok 3.7550 (4.8294)	LR 2.000e-03
0: TRAIN [0][1480/1938]	Time 0.279 (0.258)	Data 1.58e-04 (6.84e-04)	Tok/s 59910 (54425)	Loss/tok 3.6700 (4.8213)	LR 2.000e-03
0: TRAIN [0][1490/1938]	Time 0.217 (0.258)	Data 1.27e-04 (6.80e-04)	Tok/s 46859 (54403)	Loss/tok 3.3426 (4.8138)	LR 2.000e-03
0: TRAIN [0][1500/1938]	Time 0.217 (0.257)	Data 9.68e-05 (6.77e-04)	Tok/s 47410 (54369)	Loss/tok 3.3100 (4.8067)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.217 (0.258)	Data 1.05e-04 (6.73e-04)	Tok/s 47524 (54396)	Loss/tok 3.3726 (4.7981)	LR 2.000e-03
0: TRAIN [0][1520/1938]	Time 0.342 (0.258)	Data 1.04e-04 (6.70e-04)	Tok/s 68087 (54436)	Loss/tok 3.8374 (4.7890)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.278 (0.258)	Data 1.83e-04 (6.66e-04)	Tok/s 61003 (54433)	Loss/tok 3.5033 (4.7812)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1540/1938]	Time 0.340 (0.258)	Data 1.59e-04 (6.63e-04)	Tok/s 68840 (54459)	Loss/tok 3.9003 (4.7730)	LR 2.000e-03
0: TRAIN [0][1550/1938]	Time 0.279 (0.258)	Data 1.21e-04 (6.59e-04)	Tok/s 60649 (54446)	Loss/tok 3.5017 (4.7659)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.216 (0.258)	Data 9.85e-05 (6.56e-04)	Tok/s 47415 (54467)	Loss/tok 3.2698 (4.7583)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.341 (0.258)	Data 1.03e-04 (6.52e-04)	Tok/s 68706 (54455)	Loss/tok 3.7941 (4.7513)	LR 2.000e-03
0: TRAIN [0][1580/1938]	Time 0.277 (0.258)	Data 1.13e-04 (6.49e-04)	Tok/s 61440 (54474)	Loss/tok 3.6040 (4.7433)	LR 2.000e-03
0: TRAIN [0][1590/1938]	Time 0.415 (0.258)	Data 1.10e-04 (6.46e-04)	Tok/s 73020 (54490)	Loss/tok 3.8975 (4.7352)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.216 (0.258)	Data 1.00e-04 (6.43e-04)	Tok/s 46405 (54489)	Loss/tok 3.4562 (4.7281)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.278 (0.258)	Data 9.51e-05 (6.39e-04)	Tok/s 61879 (54468)	Loss/tok 3.5203 (4.7215)	LR 2.000e-03
0: TRAIN [0][1620/1938]	Time 0.217 (0.258)	Data 9.61e-05 (6.36e-04)	Tok/s 47659 (54445)	Loss/tok 3.2405 (4.7149)	LR 2.000e-03
0: TRAIN [0][1630/1938]	Time 0.414 (0.258)	Data 9.49e-05 (6.33e-04)	Tok/s 72839 (54433)	Loss/tok 3.9676 (4.7084)	LR 2.000e-03
0: TRAIN [0][1640/1938]	Time 0.216 (0.258)	Data 9.27e-05 (6.29e-04)	Tok/s 47640 (54427)	Loss/tok 3.1870 (4.7016)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.278 (0.258)	Data 1.44e-04 (6.26e-04)	Tok/s 59383 (54372)	Loss/tok 3.6428 (4.6962)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.279 (0.258)	Data 9.39e-05 (6.23e-04)	Tok/s 60413 (54383)	Loss/tok 3.5682 (4.6893)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.217 (0.258)	Data 9.66e-05 (6.20e-04)	Tok/s 47598 (54372)	Loss/tok 3.5422 (4.6829)	LR 2.000e-03
0: TRAIN [0][1680/1938]	Time 0.217 (0.258)	Data 9.70e-05 (6.17e-04)	Tok/s 47390 (54392)	Loss/tok 3.4187 (4.6758)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.416 (0.258)	Data 9.87e-05 (6.14e-04)	Tok/s 71874 (54405)	Loss/tok 3.9890 (4.6692)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.216 (0.258)	Data 9.75e-05 (6.11e-04)	Tok/s 47872 (54410)	Loss/tok 3.3336 (4.6625)	LR 2.000e-03
0: TRAIN [0][1710/1938]	Time 0.216 (0.258)	Data 9.63e-05 (6.08e-04)	Tok/s 47566 (54402)	Loss/tok 3.2539 (4.6563)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.217 (0.258)	Data 1.09e-04 (6.05e-04)	Tok/s 47956 (54394)	Loss/tok 3.3144 (4.6502)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.416 (0.258)	Data 1.02e-04 (6.03e-04)	Tok/s 71690 (54397)	Loss/tok 3.8919 (4.6435)	LR 2.000e-03
0: TRAIN [0][1740/1938]	Time 0.217 (0.258)	Data 1.34e-04 (6.00e-04)	Tok/s 47581 (54394)	Loss/tok 3.2600 (4.6372)	LR 2.000e-03
0: TRAIN [0][1750/1938]	Time 0.217 (0.258)	Data 1.63e-04 (5.97e-04)	Tok/s 47740 (54374)	Loss/tok 3.2589 (4.6318)	LR 2.000e-03
0: TRAIN [0][1760/1938]	Time 0.281 (0.258)	Data 9.54e-05 (5.95e-04)	Tok/s 59723 (54371)	Loss/tok 3.4798 (4.6255)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.217 (0.258)	Data 1.64e-04 (5.92e-04)	Tok/s 47350 (54352)	Loss/tok 3.3231 (4.6200)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.218 (0.258)	Data 1.54e-04 (5.90e-04)	Tok/s 48389 (54332)	Loss/tok 3.3349 (4.6147)	LR 2.000e-03
0: TRAIN [0][1790/1938]	Time 0.217 (0.257)	Data 1.05e-04 (5.87e-04)	Tok/s 47976 (54315)	Loss/tok 3.3139 (4.6091)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1800/1938]	Time 0.159 (0.257)	Data 1.66e-04 (5.85e-04)	Tok/s 32647 (54294)	Loss/tok 2.8022 (4.6034)	LR 2.000e-03
0: TRAIN [0][1810/1938]	Time 0.280 (0.257)	Data 1.00e-04 (5.82e-04)	Tok/s 59052 (54288)	Loss/tok 3.5730 (4.5978)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.160 (0.257)	Data 1.02e-04 (5.80e-04)	Tok/s 33080 (54250)	Loss/tok 2.7950 (4.5929)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.340 (0.257)	Data 2.76e-04 (5.77e-04)	Tok/s 69066 (54239)	Loss/tok 3.7192 (4.5876)	LR 2.000e-03
0: TRAIN [0][1840/1938]	Time 0.415 (0.257)	Data 9.99e-05 (5.75e-04)	Tok/s 72104 (54248)	Loss/tok 3.9377 (4.5817)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.216 (0.257)	Data 1.03e-04 (5.72e-04)	Tok/s 47782 (54270)	Loss/tok 3.5401 (4.5758)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.217 (0.257)	Data 1.06e-04 (5.70e-04)	Tok/s 47384 (54248)	Loss/tok 3.4100 (4.5709)	LR 2.000e-03
0: TRAIN [0][1870/1938]	Time 0.160 (0.257)	Data 9.97e-05 (5.67e-04)	Tok/s 32529 (54247)	Loss/tok 2.8490 (4.5657)	LR 2.000e-03
0: TRAIN [0][1880/1938]	Time 0.280 (0.257)	Data 1.07e-04 (5.65e-04)	Tok/s 59874 (54243)	Loss/tok 3.5526 (4.5604)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1890/1938]	Time 0.413 (0.257)	Data 1.03e-04 (5.63e-04)	Tok/s 70908 (54279)	Loss/tok 3.9719 (4.5545)	LR 2.000e-03
0: TRAIN [0][1900/1938]	Time 0.217 (0.258)	Data 1.08e-04 (5.60e-04)	Tok/s 46748 (54270)	Loss/tok 3.2252 (4.5494)	LR 2.000e-03
0: TRAIN [0][1910/1938]	Time 0.278 (0.258)	Data 1.28e-04 (5.58e-04)	Tok/s 60085 (54271)	Loss/tok 3.5088 (4.5442)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.216 (0.258)	Data 1.11e-04 (5.56e-04)	Tok/s 47624 (54280)	Loss/tok 3.1916 (4.5389)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.279 (0.258)	Data 9.58e-05 (5.53e-04)	Tok/s 61574 (54295)	Loss/tok 3.4804 (4.5333)	LR 2.000e-03
:::MLL 1570205443.963 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1570205443.963 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.749 (0.749)	Decoder iters 149.0 (149.0)	Tok/s 21599 (21599)
0: Running moses detokenizer
0: BLEU(score=20.0852653709637, counts=[34652, 15871, 8475, 4682], totals=[65109, 62106, 59103, 56106], precisions=[53.221520834293266, 25.554696808681932, 14.339373635856047, 8.344918547035968], bp=1.0, sys_len=65109, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1570205446.005 eval_accuracy: {"value": 20.09, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1570205446.005 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 4.5298	Test BLEU: 20.09
0: Performance: Epoch: 0	Training: 434247 Tok/s
0: Finished epoch 0
:::MLL 1570205446.006 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1570205446.006 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1570205446.006 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 2826016739
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][0/1938]	Time 0.945 (0.945)	Data 6.92e-01 (6.92e-01)	Tok/s 10884 (10884)	Loss/tok 3.2748 (3.2748)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.216 (0.319)	Data 9.47e-05 (6.30e-02)	Tok/s 47767 (48888)	Loss/tok 3.1897 (3.4770)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.279 (0.289)	Data 1.08e-04 (3.30e-02)	Tok/s 60991 (51154)	Loss/tok 3.4581 (3.4672)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.279 (0.280)	Data 9.16e-05 (2.24e-02)	Tok/s 59615 (52687)	Loss/tok 3.3879 (3.4393)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.341 (0.278)	Data 9.35e-05 (1.70e-02)	Tok/s 68060 (53596)	Loss/tok 3.7268 (3.4664)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.161 (0.271)	Data 1.02e-04 (1.37e-02)	Tok/s 33121 (53305)	Loss/tok 2.7634 (3.4487)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.414 (0.270)	Data 1.02e-04 (1.14e-02)	Tok/s 71052 (53457)	Loss/tok 3.8790 (3.4600)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.277 (0.268)	Data 1.56e-04 (9.84e-03)	Tok/s 60918 (53674)	Loss/tok 3.4568 (3.4590)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.216 (0.262)	Data 7.68e-04 (8.65e-03)	Tok/s 48025 (52842)	Loss/tok 3.2931 (3.4451)	LR 2.000e-03
0: TRAIN [1][90/1938]	Time 0.279 (0.261)	Data 1.74e-04 (7.72e-03)	Tok/s 59152 (52992)	Loss/tok 3.4473 (3.4433)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.278 (0.260)	Data 1.58e-04 (6.98e-03)	Tok/s 61076 (53042)	Loss/tok 3.4530 (3.4382)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.280 (0.263)	Data 1.76e-04 (6.36e-03)	Tok/s 59608 (53772)	Loss/tok 3.4312 (3.4508)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.160 (0.262)	Data 1.31e-04 (5.85e-03)	Tok/s 33032 (53549)	Loss/tok 2.7581 (3.4503)	LR 2.000e-03
0: TRAIN [1][130/1938]	Time 0.218 (0.262)	Data 1.08e-04 (5.42e-03)	Tok/s 47371 (53632)	Loss/tok 3.2389 (3.4560)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.217 (0.261)	Data 1.22e-04 (5.04e-03)	Tok/s 47605 (53552)	Loss/tok 3.0977 (3.4522)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.217 (0.261)	Data 1.88e-04 (4.72e-03)	Tok/s 47577 (53442)	Loss/tok 3.2986 (3.4552)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.216 (0.260)	Data 3.30e-04 (4.44e-03)	Tok/s 48555 (53260)	Loss/tok 3.1924 (3.4512)	LR 2.000e-03
0: TRAIN [1][170/1938]	Time 0.216 (0.258)	Data 2.06e-04 (4.19e-03)	Tok/s 48035 (53126)	Loss/tok 3.1676 (3.4484)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.216 (0.259)	Data 1.62e-04 (3.97e-03)	Tok/s 47649 (53312)	Loss/tok 3.2653 (3.4494)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.215 (0.258)	Data 1.50e-04 (3.77e-03)	Tok/s 48125 (53241)	Loss/tok 3.2063 (3.4457)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.217 (0.258)	Data 1.58e-04 (3.59e-03)	Tok/s 47407 (53359)	Loss/tok 3.1554 (3.4461)	LR 2.000e-03
0: TRAIN [1][210/1938]	Time 0.277 (0.257)	Data 2.32e-04 (3.43e-03)	Tok/s 61219 (53374)	Loss/tok 3.3079 (3.4431)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.279 (0.257)	Data 1.84e-04 (3.28e-03)	Tok/s 58801 (53219)	Loss/tok 3.4779 (3.4431)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.216 (0.256)	Data 1.54e-04 (3.15e-03)	Tok/s 47423 (53254)	Loss/tok 3.2065 (3.4390)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.340 (0.257)	Data 1.67e-04 (3.03e-03)	Tok/s 68875 (53405)	Loss/tok 3.5520 (3.4414)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.216 (0.259)	Data 1.70e-04 (2.91e-03)	Tok/s 47517 (53638)	Loss/tok 3.2210 (3.4506)	LR 2.000e-03
0: TRAIN [1][260/1938]	Time 0.218 (0.258)	Data 6.65e-04 (2.81e-03)	Tok/s 47497 (53554)	Loss/tok 3.3083 (3.4459)	LR 2.000e-03
0: TRAIN [1][270/1938]	Time 0.216 (0.257)	Data 1.15e-04 (2.71e-03)	Tok/s 47347 (53494)	Loss/tok 3.0538 (3.4432)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.217 (0.258)	Data 1.98e-04 (2.62e-03)	Tok/s 48187 (53646)	Loss/tok 3.4082 (3.4469)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.217 (0.257)	Data 1.99e-04 (2.54e-03)	Tok/s 48580 (53522)	Loss/tok 3.2357 (3.4448)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.217 (0.257)	Data 1.29e-04 (2.46e-03)	Tok/s 47670 (53433)	Loss/tok 3.2566 (3.4424)	LR 2.000e-03
0: TRAIN [1][310/1938]	Time 0.279 (0.257)	Data 1.34e-04 (2.39e-03)	Tok/s 59589 (53391)	Loss/tok 3.4756 (3.4421)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.279 (0.257)	Data 1.61e-04 (2.32e-03)	Tok/s 59617 (53449)	Loss/tok 3.5204 (3.4418)	LR 2.000e-03
0: TRAIN [1][330/1938]	Time 0.278 (0.257)	Data 1.28e-04 (2.25e-03)	Tok/s 61688 (53596)	Loss/tok 3.3909 (3.4426)	LR 2.000e-03
0: TRAIN [1][340/1938]	Time 0.414 (0.257)	Data 1.40e-04 (2.19e-03)	Tok/s 72950 (53581)	Loss/tok 3.6131 (3.4425)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.281 (0.257)	Data 1.34e-04 (2.13e-03)	Tok/s 58381 (53542)	Loss/tok 3.5550 (3.4420)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.161 (0.257)	Data 1.61e-04 (2.08e-03)	Tok/s 33183 (53497)	Loss/tok 2.8716 (3.4414)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.217 (0.256)	Data 1.52e-04 (2.03e-03)	Tok/s 46904 (53431)	Loss/tok 3.1804 (3.4386)	LR 2.000e-03
0: TRAIN [1][380/1938]	Time 0.280 (0.256)	Data 1.31e-04 (1.98e-03)	Tok/s 59561 (53353)	Loss/tok 3.4812 (3.4384)	LR 2.000e-03
0: TRAIN [1][390/1938]	Time 0.414 (0.256)	Data 1.82e-04 (1.93e-03)	Tok/s 71856 (53425)	Loss/tok 3.7967 (3.4393)	LR 2.000e-03
0: TRAIN [1][400/1938]	Time 0.341 (0.256)	Data 1.54e-04 (1.89e-03)	Tok/s 68598 (53425)	Loss/tok 3.7006 (3.4394)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.217 (0.256)	Data 1.86e-04 (1.84e-03)	Tok/s 45888 (53485)	Loss/tok 3.2060 (3.4390)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][420/1938]	Time 0.415 (0.257)	Data 1.23e-04 (1.81e-03)	Tok/s 71623 (53553)	Loss/tok 3.7421 (3.4401)	LR 2.000e-03
0: TRAIN [1][430/1938]	Time 0.278 (0.257)	Data 1.14e-04 (1.77e-03)	Tok/s 60486 (53532)	Loss/tok 3.4251 (3.4389)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.280 (0.257)	Data 1.77e-04 (1.73e-03)	Tok/s 60529 (53580)	Loss/tok 3.3838 (3.4389)	LR 2.000e-03
0: TRAIN [1][450/1938]	Time 0.340 (0.257)	Data 1.19e-04 (1.69e-03)	Tok/s 68399 (53614)	Loss/tok 3.6034 (3.4411)	LR 2.000e-03
0: TRAIN [1][460/1938]	Time 0.217 (0.257)	Data 2.76e-04 (1.66e-03)	Tok/s 47432 (53587)	Loss/tok 3.2673 (3.4391)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.217 (0.256)	Data 1.52e-04 (1.63e-03)	Tok/s 48123 (53530)	Loss/tok 3.1339 (3.4374)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.277 (0.256)	Data 1.55e-04 (1.60e-03)	Tok/s 60663 (53570)	Loss/tok 3.5117 (3.4378)	LR 2.000e-03
0: TRAIN [1][490/1938]	Time 0.280 (0.256)	Data 1.07e-04 (1.57e-03)	Tok/s 59327 (53516)	Loss/tok 3.4516 (3.4359)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.279 (0.256)	Data 1.18e-04 (1.54e-03)	Tok/s 59442 (53525)	Loss/tok 3.6012 (3.4357)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.415 (0.256)	Data 1.06e-04 (1.51e-03)	Tok/s 72163 (53488)	Loss/tok 3.8160 (3.4353)	LR 2.000e-03
0: TRAIN [1][520/1938]	Time 0.280 (0.256)	Data 1.08e-04 (1.49e-03)	Tok/s 59114 (53456)	Loss/tok 3.5123 (3.4340)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.216 (0.256)	Data 1.46e-04 (1.46e-03)	Tok/s 47041 (53557)	Loss/tok 3.1878 (3.4351)	LR 2.000e-03
0: TRAIN [1][540/1938]	Time 0.280 (0.256)	Data 1.05e-04 (1.44e-03)	Tok/s 59729 (53563)	Loss/tok 3.3679 (3.4344)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.280 (0.256)	Data 1.36e-04 (1.41e-03)	Tok/s 60153 (53575)	Loss/tok 3.5021 (3.4341)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.217 (0.256)	Data 1.08e-04 (1.39e-03)	Tok/s 46060 (53551)	Loss/tok 3.0726 (3.4335)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.218 (0.256)	Data 1.50e-04 (1.37e-03)	Tok/s 46907 (53596)	Loss/tok 3.1072 (3.4338)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][580/1938]	Time 0.414 (0.257)	Data 1.07e-04 (1.35e-03)	Tok/s 70957 (53661)	Loss/tok 3.8961 (3.4358)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.278 (0.257)	Data 1.42e-04 (1.33e-03)	Tok/s 60145 (53778)	Loss/tok 3.4757 (3.4372)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.161 (0.257)	Data 1.01e-04 (1.31e-03)	Tok/s 32504 (53758)	Loss/tok 2.8418 (3.4365)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.278 (0.257)	Data 1.11e-04 (1.29e-03)	Tok/s 60215 (53723)	Loss/tok 3.5005 (3.4348)	LR 2.000e-03
0: TRAIN [1][620/1938]	Time 0.216 (0.257)	Data 1.32e-04 (1.27e-03)	Tok/s 47925 (53713)	Loss/tok 3.1693 (3.4335)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.217 (0.257)	Data 1.06e-04 (1.25e-03)	Tok/s 47135 (53722)	Loss/tok 3.2553 (3.4327)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.280 (0.257)	Data 1.16e-04 (1.23e-03)	Tok/s 59695 (53728)	Loss/tok 3.4069 (3.4332)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.280 (0.257)	Data 1.34e-04 (1.22e-03)	Tok/s 60293 (53781)	Loss/tok 3.3871 (3.4325)	LR 2.000e-03
0: TRAIN [1][660/1938]	Time 0.217 (0.257)	Data 1.00e-04 (1.20e-03)	Tok/s 48462 (53829)	Loss/tok 3.1166 (3.4328)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.217 (0.257)	Data 1.26e-04 (1.18e-03)	Tok/s 47899 (53783)	Loss/tok 3.3080 (3.4323)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.162 (0.257)	Data 1.03e-04 (1.17e-03)	Tok/s 32571 (53734)	Loss/tok 2.6446 (3.4317)	LR 2.000e-03
0: TRAIN [1][690/1938]	Time 0.217 (0.256)	Data 1.41e-04 (1.15e-03)	Tok/s 47618 (53654)	Loss/tok 3.2375 (3.4307)	LR 2.000e-03
0: TRAIN [1][700/1938]	Time 0.275 (0.256)	Data 1.01e-04 (1.14e-03)	Tok/s 61014 (53611)	Loss/tok 3.4698 (3.4291)	LR 2.000e-03
0: TRAIN [1][710/1938]	Time 0.217 (0.256)	Data 9.99e-05 (1.12e-03)	Tok/s 48374 (53610)	Loss/tok 3.1789 (3.4280)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.342 (0.256)	Data 1.03e-04 (1.11e-03)	Tok/s 69001 (53664)	Loss/tok 3.4952 (3.4284)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.217 (0.256)	Data 1.11e-04 (1.10e-03)	Tok/s 47934 (53641)	Loss/tok 3.2267 (3.4274)	LR 2.000e-03
0: TRAIN [1][740/1938]	Time 0.280 (0.256)	Data 1.17e-04 (1.08e-03)	Tok/s 59759 (53627)	Loss/tok 3.4766 (3.4263)	LR 2.000e-03
0: TRAIN [1][750/1938]	Time 0.217 (0.256)	Data 1.68e-04 (1.07e-03)	Tok/s 47161 (53624)	Loss/tok 3.1442 (3.4260)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.217 (0.256)	Data 1.29e-04 (1.06e-03)	Tok/s 47776 (53599)	Loss/tok 3.1686 (3.4269)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][770/1938]	Time 0.412 (0.256)	Data 1.94e-04 (1.05e-03)	Tok/s 71265 (53620)	Loss/tok 3.8058 (3.4277)	LR 2.000e-03
0: TRAIN [1][780/1938]	Time 0.161 (0.256)	Data 1.33e-04 (1.03e-03)	Tok/s 32333 (53583)	Loss/tok 2.6539 (3.4265)	LR 2.000e-03
0: TRAIN [1][790/1938]	Time 0.216 (0.256)	Data 1.05e-04 (1.02e-03)	Tok/s 47243 (53576)	Loss/tok 3.1775 (3.4257)	LR 2.000e-03
0: TRAIN [1][800/1938]	Time 0.278 (0.256)	Data 1.22e-04 (1.01e-03)	Tok/s 60057 (53612)	Loss/tok 3.3557 (3.4257)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.277 (0.256)	Data 1.33e-04 (1.00e-03)	Tok/s 61384 (53600)	Loss/tok 3.3406 (3.4241)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.280 (0.256)	Data 1.48e-04 (9.91e-04)	Tok/s 59891 (53671)	Loss/tok 3.3951 (3.4248)	LR 2.000e-03
0: TRAIN [1][830/1938]	Time 0.340 (0.256)	Data 1.45e-04 (9.81e-04)	Tok/s 68519 (53717)	Loss/tok 3.4867 (3.4244)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][840/1938]	Time 0.159 (0.256)	Data 2.05e-04 (9.72e-04)	Tok/s 33483 (53707)	Loss/tok 2.5509 (3.4250)	LR 2.000e-03
0: TRAIN [1][850/1938]	Time 0.343 (0.257)	Data 1.06e-04 (9.62e-04)	Tok/s 67398 (53778)	Loss/tok 3.6381 (3.4268)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.217 (0.257)	Data 9.87e-05 (9.53e-04)	Tok/s 47428 (53764)	Loss/tok 3.1642 (3.4261)	LR 2.000e-03
0: TRAIN [1][870/1938]	Time 0.161 (0.256)	Data 1.32e-04 (9.43e-04)	Tok/s 32319 (53724)	Loss/tok 2.6803 (3.4250)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.217 (0.256)	Data 1.50e-04 (9.35e-04)	Tok/s 47564 (53702)	Loss/tok 3.2091 (3.4239)	LR 2.000e-03
0: TRAIN [1][890/1938]	Time 0.279 (0.257)	Data 1.69e-04 (9.25e-04)	Tok/s 60103 (53768)	Loss/tok 3.2979 (3.4252)	LR 2.000e-03
0: TRAIN [1][900/1938]	Time 0.215 (0.257)	Data 2.01e-04 (9.17e-04)	Tok/s 47234 (53790)	Loss/tok 3.1316 (3.4257)	LR 2.000e-03
0: TRAIN [1][910/1938]	Time 0.216 (0.256)	Data 1.51e-04 (9.08e-04)	Tok/s 47765 (53775)	Loss/tok 3.2379 (3.4244)	LR 2.000e-03
0: TRAIN [1][920/1938]	Time 0.280 (0.256)	Data 1.13e-04 (9.00e-04)	Tok/s 59853 (53762)	Loss/tok 3.3518 (3.4230)	LR 2.000e-03
0: TRAIN [1][930/1938]	Time 0.217 (0.256)	Data 1.34e-04 (8.92e-04)	Tok/s 47382 (53728)	Loss/tok 3.1915 (3.4216)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.162 (0.256)	Data 1.18e-04 (8.84e-04)	Tok/s 32761 (53729)	Loss/tok 2.7675 (3.4210)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.161 (0.256)	Data 1.20e-04 (8.76e-04)	Tok/s 32557 (53678)	Loss/tok 2.6483 (3.4200)	LR 2.000e-03
0: TRAIN [1][960/1938]	Time 0.217 (0.256)	Data 1.12e-04 (8.68e-04)	Tok/s 48351 (53669)	Loss/tok 3.1125 (3.4197)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.217 (0.256)	Data 1.60e-04 (8.61e-04)	Tok/s 48025 (53667)	Loss/tok 3.2281 (3.4200)	LR 2.000e-03
0: TRAIN [1][980/1938]	Time 0.216 (0.256)	Data 1.20e-04 (8.54e-04)	Tok/s 48075 (53708)	Loss/tok 3.2684 (3.4202)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.280 (0.256)	Data 1.33e-04 (8.46e-04)	Tok/s 59581 (53723)	Loss/tok 3.3570 (3.4200)	LR 2.000e-03
0: TRAIN [1][1000/1938]	Time 0.218 (0.256)	Data 1.39e-04 (8.39e-04)	Tok/s 47373 (53721)	Loss/tok 3.0334 (3.4189)	LR 2.000e-03
0: TRAIN [1][1010/1938]	Time 0.340 (0.256)	Data 1.16e-04 (8.32e-04)	Tok/s 68205 (53713)	Loss/tok 3.5598 (3.4182)	LR 2.000e-03
0: TRAIN [1][1020/1938]	Time 0.161 (0.256)	Data 1.43e-04 (8.25e-04)	Tok/s 32008 (53718)	Loss/tok 2.7796 (3.4172)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.217 (0.256)	Data 1.03e-04 (8.18e-04)	Tok/s 47802 (53703)	Loss/tok 3.2056 (3.4163)	LR 2.000e-03
0: TRAIN [1][1040/1938]	Time 0.280 (0.256)	Data 1.09e-04 (8.12e-04)	Tok/s 60079 (53713)	Loss/tok 3.3552 (3.4158)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.218 (0.256)	Data 1.12e-04 (8.05e-04)	Tok/s 48376 (53678)	Loss/tok 3.1077 (3.4142)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.279 (0.256)	Data 1.05e-04 (7.99e-04)	Tok/s 60376 (53710)	Loss/tok 3.4384 (3.4147)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.341 (0.256)	Data 1.12e-04 (7.92e-04)	Tok/s 68670 (53704)	Loss/tok 3.5551 (3.4142)	LR 2.000e-03
0: TRAIN [1][1080/1938]	Time 0.280 (0.256)	Data 1.06e-04 (7.86e-04)	Tok/s 59473 (53735)	Loss/tok 3.4931 (3.4142)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.276 (0.256)	Data 2.64e-04 (7.81e-04)	Tok/s 61344 (53789)	Loss/tok 3.3686 (3.4146)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.217 (0.256)	Data 1.04e-04 (7.75e-04)	Tok/s 49441 (53792)	Loss/tok 3.1738 (3.4144)	LR 2.000e-03
0: TRAIN [1][1110/1938]	Time 0.280 (0.257)	Data 1.05e-04 (7.69e-04)	Tok/s 60404 (53861)	Loss/tok 3.3239 (3.4156)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.340 (0.256)	Data 1.18e-04 (7.64e-04)	Tok/s 68429 (53836)	Loss/tok 3.5478 (3.4147)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1130/1938]	Time 0.214 (0.256)	Data 1.38e-04 (7.58e-04)	Tok/s 48112 (53815)	Loss/tok 3.0917 (3.4139)	LR 2.000e-03
0: TRAIN [1][1140/1938]	Time 0.341 (0.257)	Data 1.50e-04 (7.53e-04)	Tok/s 67661 (53862)	Loss/tok 3.5603 (3.4149)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.341 (0.257)	Data 1.04e-04 (7.47e-04)	Tok/s 68255 (53930)	Loss/tok 3.5724 (3.4159)	LR 2.000e-03
0: TRAIN [1][1160/1938]	Time 0.279 (0.257)	Data 1.74e-04 (7.42e-04)	Tok/s 60974 (53929)	Loss/tok 3.3170 (3.4170)	LR 2.000e-03
0: TRAIN [1][1170/1938]	Time 0.276 (0.257)	Data 1.15e-04 (7.37e-04)	Tok/s 60709 (53947)	Loss/tok 3.3999 (3.4166)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.279 (0.257)	Data 1.08e-04 (7.31e-04)	Tok/s 59252 (53933)	Loss/tok 3.3511 (3.4157)	LR 2.000e-03
0: TRAIN [1][1190/1938]	Time 0.278 (0.257)	Data 1.07e-04 (7.26e-04)	Tok/s 61321 (53966)	Loss/tok 3.3516 (3.4147)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.281 (0.257)	Data 1.04e-04 (7.21e-04)	Tok/s 60015 (53991)	Loss/tok 3.4419 (3.4142)	LR 2.000e-03
0: TRAIN [1][1210/1938]	Time 0.278 (0.257)	Data 9.68e-05 (7.16e-04)	Tok/s 60732 (54003)	Loss/tok 3.3320 (3.4140)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.160 (0.257)	Data 1.04e-04 (7.11e-04)	Tok/s 33405 (53991)	Loss/tok 2.7664 (3.4139)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1230/1938]	Time 0.276 (0.257)	Data 1.05e-04 (7.07e-04)	Tok/s 60884 (53984)	Loss/tok 3.4166 (3.4133)	LR 2.000e-03
0: TRAIN [1][1240/1938]	Time 0.217 (0.257)	Data 1.09e-04 (7.02e-04)	Tok/s 48151 (53972)	Loss/tok 3.1985 (3.4135)	LR 2.000e-03
0: TRAIN [1][1250/1938]	Time 0.215 (0.257)	Data 1.06e-04 (6.98e-04)	Tok/s 47465 (54001)	Loss/tok 3.1338 (3.4142)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.216 (0.257)	Data 1.02e-04 (6.93e-04)	Tok/s 47535 (53963)	Loss/tok 3.1711 (3.4135)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.217 (0.257)	Data 9.85e-05 (6.88e-04)	Tok/s 47576 (53926)	Loss/tok 3.1110 (3.4126)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.216 (0.257)	Data 1.01e-04 (6.84e-04)	Tok/s 47681 (53922)	Loss/tok 3.1696 (3.4116)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.216 (0.257)	Data 1.14e-04 (6.80e-04)	Tok/s 47605 (53974)	Loss/tok 3.1152 (3.4126)	LR 2.000e-03
0: TRAIN [1][1300/1938]	Time 0.277 (0.257)	Data 1.10e-04 (6.75e-04)	Tok/s 61206 (53948)	Loss/tok 3.3914 (3.4119)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.217 (0.257)	Data 9.49e-05 (6.71e-04)	Tok/s 47303 (53916)	Loss/tok 3.1569 (3.4111)	LR 2.000e-03
0: TRAIN [1][1320/1938]	Time 0.217 (0.257)	Data 9.89e-05 (6.67e-04)	Tok/s 47673 (53911)	Loss/tok 3.1159 (3.4108)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.342 (0.257)	Data 1.00e-04 (6.63e-04)	Tok/s 68809 (53935)	Loss/tok 3.5532 (3.4108)	LR 2.000e-03
0: TRAIN [1][1340/1938]	Time 0.340 (0.258)	Data 9.23e-05 (6.59e-04)	Tok/s 67942 (53990)	Loss/tok 3.5786 (3.4122)	LR 2.000e-03
0: TRAIN [1][1350/1938]	Time 0.280 (0.258)	Data 9.56e-05 (6.54e-04)	Tok/s 59582 (54043)	Loss/tok 3.4800 (3.4126)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.277 (0.258)	Data 1.23e-04 (6.50e-04)	Tok/s 60719 (54030)	Loss/tok 3.4484 (3.4120)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.280 (0.258)	Data 9.85e-05 (6.47e-04)	Tok/s 60163 (54090)	Loss/tok 3.2976 (3.4120)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.216 (0.258)	Data 1.03e-04 (6.43e-04)	Tok/s 48940 (54118)	Loss/tok 3.0123 (3.4116)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.217 (0.258)	Data 1.03e-04 (6.39e-04)	Tok/s 47949 (54127)	Loss/tok 3.2450 (3.4111)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.217 (0.258)	Data 1.64e-04 (6.35e-04)	Tok/s 48557 (54137)	Loss/tok 3.2003 (3.4113)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.216 (0.258)	Data 1.45e-04 (6.32e-04)	Tok/s 48431 (54176)	Loss/tok 3.2233 (3.4122)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.412 (0.259)	Data 6.91e-04 (6.29e-04)	Tok/s 72783 (54216)	Loss/tok 3.6074 (3.4129)	LR 2.000e-03
0: TRAIN [1][1430/1938]	Time 0.216 (0.259)	Data 9.73e-05 (6.26e-04)	Tok/s 47314 (54203)	Loss/tok 3.2759 (3.4125)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.278 (0.259)	Data 1.29e-04 (6.22e-04)	Tok/s 59853 (54208)	Loss/tok 3.4070 (3.4120)	LR 2.000e-03
0: TRAIN [1][1450/1938]	Time 0.217 (0.259)	Data 1.18e-04 (6.19e-04)	Tok/s 48060 (54210)	Loss/tok 3.1382 (3.4120)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.216 (0.258)	Data 1.07e-04 (6.15e-04)	Tok/s 47819 (54180)	Loss/tok 3.1912 (3.4111)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.217 (0.258)	Data 1.30e-04 (6.12e-04)	Tok/s 46991 (54186)	Loss/tok 3.2022 (3.4110)	LR 2.000e-03
0: TRAIN [1][1480/1938]	Time 0.415 (0.258)	Data 1.03e-04 (6.09e-04)	Tok/s 70764 (54174)	Loss/tok 3.7460 (3.4104)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1490/1938]	Time 0.217 (0.258)	Data 1.81e-04 (6.05e-04)	Tok/s 47050 (54198)	Loss/tok 3.2177 (3.4099)	LR 2.000e-03
0: TRAIN [1][1500/1938]	Time 0.279 (0.259)	Data 9.82e-05 (6.02e-04)	Tok/s 58987 (54210)	Loss/tok 3.5045 (3.4098)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.217 (0.259)	Data 9.70e-05 (5.99e-04)	Tok/s 47881 (54210)	Loss/tok 3.1736 (3.4093)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.162 (0.258)	Data 1.07e-04 (5.96e-04)	Tok/s 32276 (54188)	Loss/tok 2.6905 (3.4083)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.340 (0.258)	Data 1.09e-04 (5.93e-04)	Tok/s 67960 (54204)	Loss/tok 3.6259 (3.4084)	LR 2.000e-03
0: TRAIN [1][1540/1938]	Time 0.216 (0.258)	Data 1.02e-04 (5.90e-04)	Tok/s 47867 (54189)	Loss/tok 3.1836 (3.4073)	LR 2.000e-03
0: TRAIN [1][1550/1938]	Time 0.216 (0.258)	Data 9.63e-05 (5.87e-04)	Tok/s 48093 (54171)	Loss/tok 3.0931 (3.4064)	LR 2.000e-03
0: TRAIN [1][1560/1938]	Time 0.280 (0.258)	Data 1.14e-04 (5.84e-04)	Tok/s 59936 (54160)	Loss/tok 3.4499 (3.4059)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.340 (0.258)	Data 1.02e-04 (5.81e-04)	Tok/s 69439 (54148)	Loss/tok 3.3915 (3.4049)	LR 2.000e-03
0: TRAIN [1][1580/1938]	Time 0.217 (0.258)	Data 1.03e-04 (5.78e-04)	Tok/s 48224 (54177)	Loss/tok 3.1738 (3.4045)	LR 2.000e-03
0: TRAIN [1][1590/1938]	Time 0.277 (0.258)	Data 1.04e-04 (5.75e-04)	Tok/s 60741 (54171)	Loss/tok 3.4439 (3.4044)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.278 (0.258)	Data 1.06e-04 (5.72e-04)	Tok/s 61195 (54183)	Loss/tok 3.3441 (3.4041)	LR 2.000e-03
0: TRAIN [1][1610/1938]	Time 0.280 (0.258)	Data 1.11e-04 (5.69e-04)	Tok/s 59325 (54214)	Loss/tok 3.3632 (3.4042)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.216 (0.259)	Data 1.01e-04 (5.67e-04)	Tok/s 47849 (54229)	Loss/tok 3.2195 (3.4048)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.217 (0.258)	Data 1.05e-04 (5.64e-04)	Tok/s 48578 (54215)	Loss/tok 3.1798 (3.4048)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.341 (0.259)	Data 1.08e-04 (5.61e-04)	Tok/s 67695 (54226)	Loss/tok 3.6031 (3.4050)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1650/1938]	Time 0.279 (0.259)	Data 1.04e-04 (5.59e-04)	Tok/s 60529 (54212)	Loss/tok 3.4011 (3.4048)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.218 (0.258)	Data 1.11e-04 (5.56e-04)	Tok/s 47334 (54200)	Loss/tok 3.1470 (3.4044)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.342 (0.259)	Data 1.26e-04 (5.54e-04)	Tok/s 67961 (54213)	Loss/tok 3.5348 (3.4049)	LR 2.000e-03
0: TRAIN [1][1680/1938]	Time 0.162 (0.259)	Data 1.12e-04 (5.51e-04)	Tok/s 33056 (54219)	Loss/tok 2.6335 (3.4049)	LR 2.000e-03
0: TRAIN [1][1690/1938]	Time 0.162 (0.259)	Data 1.24e-04 (5.48e-04)	Tok/s 32651 (54206)	Loss/tok 2.7868 (3.4044)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.341 (0.259)	Data 1.04e-04 (5.46e-04)	Tok/s 68017 (54199)	Loss/tok 3.5815 (3.4037)	LR 2.000e-03
0: TRAIN [1][1710/1938]	Time 0.417 (0.259)	Data 1.06e-04 (5.44e-04)	Tok/s 71549 (54208)	Loss/tok 3.6088 (3.4037)	LR 2.000e-03
0: TRAIN [1][1720/1938]	Time 0.279 (0.259)	Data 1.25e-04 (5.41e-04)	Tok/s 60294 (54190)	Loss/tok 3.3171 (3.4028)	LR 2.000e-03
0: TRAIN [1][1730/1938]	Time 0.340 (0.258)	Data 1.24e-04 (5.39e-04)	Tok/s 68901 (54186)	Loss/tok 3.4878 (3.4025)	LR 2.000e-03
0: TRAIN [1][1740/1938]	Time 0.216 (0.259)	Data 1.12e-04 (5.37e-04)	Tok/s 48104 (54201)	Loss/tok 3.1563 (3.4021)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.218 (0.259)	Data 1.15e-04 (5.34e-04)	Tok/s 48179 (54210)	Loss/tok 3.1151 (3.4019)	LR 2.000e-03
0: TRAIN [1][1760/1938]	Time 0.217 (0.259)	Data 2.22e-04 (5.32e-04)	Tok/s 48462 (54249)	Loss/tok 3.0027 (3.4021)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.216 (0.259)	Data 9.82e-05 (5.30e-04)	Tok/s 47962 (54242)	Loss/tok 3.1757 (3.4016)	LR 2.000e-03
0: TRAIN [1][1780/1938]	Time 0.278 (0.259)	Data 1.07e-04 (5.28e-04)	Tok/s 59947 (54236)	Loss/tok 3.4209 (3.4012)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.282 (0.259)	Data 1.08e-04 (5.25e-04)	Tok/s 59030 (54274)	Loss/tok 3.4763 (3.4013)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1800/1938]	Time 0.277 (0.259)	Data 1.03e-04 (5.23e-04)	Tok/s 60780 (54266)	Loss/tok 3.3940 (3.4006)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.217 (0.259)	Data 1.18e-04 (5.21e-04)	Tok/s 47719 (54251)	Loss/tok 3.2211 (3.4003)	LR 2.000e-03
0: TRAIN [1][1820/1938]	Time 0.217 (0.259)	Data 1.14e-04 (5.19e-04)	Tok/s 47745 (54211)	Loss/tok 3.1845 (3.3994)	LR 2.000e-03
0: TRAIN [1][1830/1938]	Time 0.279 (0.258)	Data 1.15e-04 (5.17e-04)	Tok/s 59650 (54178)	Loss/tok 3.4357 (3.3987)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.275 (0.258)	Data 1.08e-04 (5.14e-04)	Tok/s 60201 (54183)	Loss/tok 3.4933 (3.3983)	LR 2.000e-03
0: TRAIN [1][1850/1938]	Time 0.280 (0.258)	Data 1.37e-04 (5.12e-04)	Tok/s 58741 (54184)	Loss/tok 3.4400 (3.3979)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.278 (0.258)	Data 1.32e-04 (5.10e-04)	Tok/s 59741 (54153)	Loss/tok 3.4025 (3.3970)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.217 (0.258)	Data 1.95e-04 (5.08e-04)	Tok/s 48285 (54144)	Loss/tok 3.0123 (3.3966)	LR 2.000e-03
0: TRAIN [1][1880/1938]	Time 0.278 (0.258)	Data 1.29e-04 (5.06e-04)	Tok/s 60552 (54143)	Loss/tok 3.3864 (3.3964)	LR 2.000e-03
0: TRAIN [1][1890/1938]	Time 0.217 (0.258)	Data 1.04e-04 (5.04e-04)	Tok/s 47751 (54141)	Loss/tok 3.1082 (3.3960)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.278 (0.258)	Data 1.70e-04 (5.02e-04)	Tok/s 60483 (54124)	Loss/tok 3.1900 (3.3954)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.278 (0.258)	Data 1.35e-04 (5.00e-04)	Tok/s 60997 (54127)	Loss/tok 3.3151 (3.3948)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.340 (0.258)	Data 1.06e-04 (4.98e-04)	Tok/s 68656 (54143)	Loss/tok 3.4437 (3.3944)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1930/1938]	Time 0.217 (0.258)	Data 1.60e-04 (4.97e-04)	Tok/s 48024 (54138)	Loss/tok 3.0639 (3.3943)	LR 2.000e-03
:::MLL 1570205947.198 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1570205947.198 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.617 (0.617)	Decoder iters 100.0 (100.0)	Tok/s 25242 (25242)
0: Running moses detokenizer
0: BLEU(score=21.71425612780651, counts=[35019, 16834, 9250, 5269], totals=[62646, 59643, 56640, 53643], precisions=[55.89981802509338, 28.224603054842984, 16.331214689265536, 9.822344015062543], bp=0.9681150905788869, sys_len=62646, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1570205948.971 eval_accuracy: {"value": 21.71, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1570205948.971 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.3969	Test BLEU: 21.71
0: Performance: Epoch: 1	Training: 433238 Tok/s
0: Finished epoch 1
:::MLL 1570205948.972 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1570205948.972 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1570205948.972 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 4096709150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [2][0/1938]	Time 0.870 (0.870)	Data 6.67e-01 (6.67e-01)	Tok/s 6134 (6134)	Loss/tok 2.6832 (2.6832)	LR 2.000e-03
0: TRAIN [2][10/1938]	Time 0.216 (0.307)	Data 1.01e-04 (6.07e-02)	Tok/s 49013 (47380)	Loss/tok 2.9699 (3.2423)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.339 (0.270)	Data 1.19e-04 (3.19e-02)	Tok/s 68985 (48470)	Loss/tok 3.5433 (3.2144)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.280 (0.257)	Data 9.18e-05 (2.16e-02)	Tok/s 59684 (48955)	Loss/tok 3.2183 (3.1865)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.217 (0.256)	Data 2.59e-04 (1.64e-02)	Tok/s 47504 (50244)	Loss/tok 2.9849 (3.1984)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.161 (0.252)	Data 5.01e-04 (1.32e-02)	Tok/s 32775 (50130)	Loss/tok 2.6165 (3.1988)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.278 (0.256)	Data 1.82e-04 (1.11e-02)	Tok/s 60406 (51492)	Loss/tok 3.3176 (3.2036)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.216 (0.257)	Data 1.29e-04 (9.52e-03)	Tok/s 47881 (51999)	Loss/tok 3.1861 (3.2146)	LR 2.000e-03
0: TRAIN [2][80/1938]	Time 0.161 (0.260)	Data 1.10e-04 (8.36e-03)	Tok/s 32938 (52717)	Loss/tok 2.6940 (3.2304)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.278 (0.263)	Data 1.42e-04 (7.46e-03)	Tok/s 59988 (53317)	Loss/tok 3.3098 (3.2405)	LR 2.000e-03
0: TRAIN [2][100/1938]	Time 0.216 (0.263)	Data 1.02e-04 (6.73e-03)	Tok/s 47889 (53218)	Loss/tok 3.0042 (3.2490)	LR 2.000e-03
0: TRAIN [2][110/1938]	Time 0.162 (0.261)	Data 1.08e-04 (6.13e-03)	Tok/s 31963 (53154)	Loss/tok 2.5411 (3.2489)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.161 (0.261)	Data 1.01e-04 (5.64e-03)	Tok/s 32930 (53178)	Loss/tok 2.5759 (3.2561)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.217 (0.261)	Data 9.89e-05 (5.22e-03)	Tok/s 47910 (53304)	Loss/tok 3.0274 (3.2514)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.340 (0.259)	Data 1.61e-04 (4.85e-03)	Tok/s 68822 (52978)	Loss/tok 3.4362 (3.2517)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.278 (0.259)	Data 1.02e-04 (4.54e-03)	Tok/s 60414 (53016)	Loss/tok 3.2374 (3.2566)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.217 (0.258)	Data 9.58e-05 (4.26e-03)	Tok/s 46730 (52779)	Loss/tok 3.1167 (3.2521)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.279 (0.258)	Data 1.38e-04 (4.02e-03)	Tok/s 60354 (53041)	Loss/tok 3.2065 (3.2529)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.278 (0.258)	Data 9.73e-05 (3.80e-03)	Tok/s 59963 (53149)	Loss/tok 3.3514 (3.2543)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.280 (0.258)	Data 9.20e-05 (3.61e-03)	Tok/s 59790 (53140)	Loss/tok 3.2443 (3.2550)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.341 (0.259)	Data 9.92e-05 (3.44e-03)	Tok/s 68634 (53324)	Loss/tok 3.4177 (3.2588)	LR 2.000e-03
0: TRAIN [2][210/1938]	Time 0.280 (0.259)	Data 1.02e-04 (3.28e-03)	Tok/s 60108 (53373)	Loss/tok 3.1870 (3.2574)	LR 2.000e-03
0: TRAIN [2][220/1938]	Time 0.278 (0.257)	Data 1.30e-04 (3.13e-03)	Tok/s 60356 (53225)	Loss/tok 3.1951 (3.2519)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.276 (0.257)	Data 1.05e-04 (3.00e-03)	Tok/s 60882 (53170)	Loss/tok 3.2167 (3.2496)	LR 2.000e-03
0: TRAIN [2][240/1938]	Time 0.161 (0.257)	Data 9.70e-05 (2.88e-03)	Tok/s 32093 (53214)	Loss/tok 2.5483 (3.2510)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.280 (0.256)	Data 9.23e-05 (2.77e-03)	Tok/s 60344 (53007)	Loss/tok 3.3920 (3.2484)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.280 (0.255)	Data 9.23e-05 (2.67e-03)	Tok/s 59671 (52939)	Loss/tok 3.2497 (3.2445)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.281 (0.255)	Data 9.75e-05 (2.58e-03)	Tok/s 59185 (52987)	Loss/tok 3.2360 (3.2459)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.216 (0.255)	Data 9.66e-05 (2.49e-03)	Tok/s 47980 (52888)	Loss/tok 3.0544 (3.2471)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.280 (0.255)	Data 9.56e-05 (2.41e-03)	Tok/s 59810 (52874)	Loss/tok 3.2168 (3.2486)	LR 2.000e-03
0: TRAIN [2][300/1938]	Time 0.217 (0.254)	Data 1.00e-04 (2.33e-03)	Tok/s 48029 (52851)	Loss/tok 3.0501 (3.2470)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.341 (0.256)	Data 9.66e-05 (2.26e-03)	Tok/s 68234 (53140)	Loss/tok 3.4438 (3.2504)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.281 (0.255)	Data 9.58e-05 (2.19e-03)	Tok/s 59817 (53111)	Loss/tok 3.3151 (3.2485)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.216 (0.256)	Data 1.06e-04 (2.13e-03)	Tok/s 46414 (53188)	Loss/tok 3.0616 (3.2518)	LR 2.000e-03
0: TRAIN [2][340/1938]	Time 0.216 (0.255)	Data 9.78e-05 (2.07e-03)	Tok/s 48044 (53049)	Loss/tok 3.0044 (3.2480)	LR 2.000e-03
0: TRAIN [2][350/1938]	Time 0.340 (0.255)	Data 1.37e-04 (2.01e-03)	Tok/s 68675 (53177)	Loss/tok 3.4299 (3.2491)	LR 2.000e-03
0: TRAIN [2][360/1938]	Time 0.341 (0.256)	Data 1.00e-04 (1.96e-03)	Tok/s 68994 (53318)	Loss/tok 3.4555 (3.2507)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.217 (0.256)	Data 1.10e-04 (1.91e-03)	Tok/s 47438 (53268)	Loss/tok 3.0410 (3.2490)	LR 2.000e-03
0: TRAIN [2][380/1938]	Time 0.216 (0.256)	Data 9.63e-05 (1.86e-03)	Tok/s 47506 (53231)	Loss/tok 2.9571 (3.2493)	LR 2.000e-03
0: TRAIN [2][390/1938]	Time 0.216 (0.255)	Data 1.18e-04 (1.82e-03)	Tok/s 47951 (53226)	Loss/tok 3.0381 (3.2472)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.341 (0.255)	Data 9.56e-05 (1.78e-03)	Tok/s 68768 (53153)	Loss/tok 3.3992 (3.2457)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.217 (0.255)	Data 1.02e-04 (1.74e-03)	Tok/s 47242 (53164)	Loss/tok 3.1747 (3.2449)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.163 (0.254)	Data 9.49e-05 (1.70e-03)	Tok/s 32638 (53045)	Loss/tok 2.6592 (3.2423)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.278 (0.254)	Data 1.02e-04 (1.66e-03)	Tok/s 60604 (53045)	Loss/tok 3.3695 (3.2420)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.161 (0.254)	Data 9.89e-05 (1.63e-03)	Tok/s 32806 (53033)	Loss/tok 2.6065 (3.2414)	LR 2.000e-03
0: TRAIN [2][450/1938]	Time 0.281 (0.254)	Data 9.85e-05 (1.59e-03)	Tok/s 60579 (53073)	Loss/tok 3.2374 (3.2428)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][460/1938]	Time 0.217 (0.254)	Data 1.00e-04 (1.56e-03)	Tok/s 48256 (53152)	Loss/tok 3.0552 (3.2451)	LR 2.000e-03
0: TRAIN [2][470/1938]	Time 0.278 (0.255)	Data 1.05e-04 (1.53e-03)	Tok/s 61102 (53181)	Loss/tok 3.2968 (3.2459)	LR 2.000e-03
0: TRAIN [2][480/1938]	Time 0.279 (0.255)	Data 1.02e-04 (1.50e-03)	Tok/s 60473 (53199)	Loss/tok 3.2340 (3.2458)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.280 (0.255)	Data 9.44e-05 (1.47e-03)	Tok/s 59293 (53222)	Loss/tok 3.2831 (3.2468)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.279 (0.255)	Data 9.66e-05 (1.44e-03)	Tok/s 60429 (53258)	Loss/tok 3.3253 (3.2462)	LR 2.000e-03
0: TRAIN [2][510/1938]	Time 0.279 (0.255)	Data 1.02e-04 (1.42e-03)	Tok/s 59945 (53232)	Loss/tok 3.2571 (3.2452)	LR 2.000e-03
0: TRAIN [2][520/1938]	Time 0.217 (0.254)	Data 9.73e-05 (1.39e-03)	Tok/s 47989 (53152)	Loss/tok 3.0715 (3.2430)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.217 (0.254)	Data 9.92e-05 (1.37e-03)	Tok/s 46954 (53107)	Loss/tok 2.9955 (3.2424)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.216 (0.254)	Data 9.87e-05 (1.35e-03)	Tok/s 46375 (53132)	Loss/tok 3.0352 (3.2421)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.216 (0.254)	Data 1.03e-04 (1.32e-03)	Tok/s 47971 (53193)	Loss/tok 3.0852 (3.2437)	LR 2.000e-03
0: TRAIN [2][560/1938]	Time 0.161 (0.254)	Data 9.68e-05 (1.30e-03)	Tok/s 32949 (53216)	Loss/tok 2.6610 (3.2431)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.341 (0.254)	Data 9.54e-05 (1.28e-03)	Tok/s 68075 (53272)	Loss/tok 3.5612 (3.2439)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.217 (0.255)	Data 9.58e-05 (1.26e-03)	Tok/s 48235 (53353)	Loss/tok 3.0349 (3.2464)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.277 (0.254)	Data 9.66e-05 (1.24e-03)	Tok/s 60781 (53292)	Loss/tok 3.2936 (3.2452)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][600/1938]	Time 0.341 (0.255)	Data 1.03e-04 (1.22e-03)	Tok/s 68488 (53376)	Loss/tok 3.4664 (3.2460)	LR 2.000e-03
0: TRAIN [2][610/1938]	Time 0.280 (0.255)	Data 9.92e-05 (1.20e-03)	Tok/s 60044 (53357)	Loss/tok 3.2778 (3.2450)	LR 2.000e-03
0: TRAIN [2][620/1938]	Time 0.217 (0.254)	Data 9.47e-05 (1.19e-03)	Tok/s 49373 (53340)	Loss/tok 3.0983 (3.2439)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.415 (0.255)	Data 9.47e-05 (1.17e-03)	Tok/s 72408 (53352)	Loss/tok 3.6133 (3.2448)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.280 (0.255)	Data 1.03e-04 (1.15e-03)	Tok/s 60542 (53393)	Loss/tok 3.1657 (3.2451)	LR 2.000e-03
0: TRAIN [2][650/1938]	Time 0.341 (0.254)	Data 9.87e-05 (1.14e-03)	Tok/s 68542 (53356)	Loss/tok 3.5587 (3.2441)	LR 2.000e-03
0: TRAIN [2][660/1938]	Time 0.414 (0.255)	Data 9.47e-05 (1.12e-03)	Tok/s 72186 (53458)	Loss/tok 3.5080 (3.2462)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.278 (0.256)	Data 1.03e-04 (1.10e-03)	Tok/s 60379 (53563)	Loss/tok 3.2267 (3.2487)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.340 (0.256)	Data 1.01e-04 (1.09e-03)	Tok/s 67849 (53605)	Loss/tok 3.4371 (3.2493)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.416 (0.256)	Data 1.01e-04 (1.08e-03)	Tok/s 72402 (53623)	Loss/tok 3.4475 (3.2492)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.280 (0.256)	Data 9.68e-05 (1.06e-03)	Tok/s 60458 (53645)	Loss/tok 3.2435 (3.2489)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.217 (0.256)	Data 9.68e-05 (1.05e-03)	Tok/s 47297 (53656)	Loss/tok 3.1003 (3.2503)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.217 (0.257)	Data 9.78e-05 (1.04e-03)	Tok/s 47665 (53715)	Loss/tok 3.0018 (3.2525)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][730/1938]	Time 0.276 (0.257)	Data 1.21e-04 (1.02e-03)	Tok/s 60594 (53751)	Loss/tok 3.3466 (3.2538)	LR 2.000e-03
0: TRAIN [2][740/1938]	Time 0.217 (0.256)	Data 6.56e-04 (1.01e-03)	Tok/s 47663 (53687)	Loss/tok 3.0493 (3.2527)	LR 2.000e-03
0: TRAIN [2][750/1938]	Time 0.279 (0.256)	Data 1.08e-04 (1.00e-03)	Tok/s 60518 (53675)	Loss/tok 3.2051 (3.2516)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.280 (0.256)	Data 9.75e-05 (9.88e-04)	Tok/s 60276 (53686)	Loss/tok 3.2009 (3.2515)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][770/1938]	Time 0.341 (0.257)	Data 1.17e-04 (9.77e-04)	Tok/s 68234 (53776)	Loss/tok 3.4696 (3.2554)	LR 2.000e-03
0: TRAIN [2][780/1938]	Time 0.217 (0.257)	Data 1.08e-04 (9.66e-04)	Tok/s 47792 (53801)	Loss/tok 3.1270 (3.2550)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.216 (0.257)	Data 1.00e-04 (9.55e-04)	Tok/s 48742 (53776)	Loss/tok 3.0345 (3.2541)	LR 2.000e-03
0: TRAIN [2][800/1938]	Time 0.277 (0.257)	Data 1.04e-04 (9.45e-04)	Tok/s 59721 (53806)	Loss/tok 3.3122 (3.2550)	LR 2.000e-03
0: TRAIN [2][810/1938]	Time 0.217 (0.257)	Data 9.82e-05 (9.34e-04)	Tok/s 49088 (53777)	Loss/tok 3.0315 (3.2544)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.276 (0.257)	Data 1.19e-04 (9.25e-04)	Tok/s 60638 (53776)	Loss/tok 3.2857 (3.2541)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.160 (0.257)	Data 9.49e-05 (9.15e-04)	Tok/s 33412 (53783)	Loss/tok 2.5556 (3.2537)	LR 2.000e-03
0: TRAIN [2][840/1938]	Time 0.415 (0.257)	Data 1.12e-04 (9.05e-04)	Tok/s 72475 (53793)	Loss/tok 3.4851 (3.2539)	LR 2.000e-03
