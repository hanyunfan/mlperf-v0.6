Beginning trial 1 of 1
Gathering sys log on dss01
:::MLL 1571254851.719 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1571254851.720 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1571254851.720 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1571254851.721 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1571254851.721 submission_platform: {"value": "1xDSS8440", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1571254851.721 submission_entry: {"value": "{'hardware': 'DSS8440', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '5.0-0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz', 'num_cores': '40', 'num_vcpus': '40', 'accelerator': 'Tesla V100-PCIE-32GB', 'num_accelerators': '8', 'sys_mem_size': '754 GB', 'sys_storage_type': 'SATA SSD', 'sys_storage_size': '1x 447.1G + 1x 931.5G', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27700 Family [ConnectX-4]', 'num_network_cards': '1', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1571254851.722 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1571254851.722 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1571254857.223 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node dss01
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=4721' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=128 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=400 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=191016143912487525874 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191016143912487525874 ./run_and_time.sh
Run vars: id 191016143912487525874 gpus 8 mparams  --master_port=4721
NCCL_SOCKET_NTHREADS=2
NCCL_NSOCKS_PERTHREAD=8
STARTING TIMING RUN AT 2019-10-16 07:40:57 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=128
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=400
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=0
+ MATH=amp_fp16
+ [[ 0 -eq 1 ]]
+ LAUNCH_OPT='torch.distributed.launch --nproc_per_node 8  --master_port=4721'
+ echo 'running benchmark'
running benchmark
+ python -m torch.distributed.launch --nproc_per_node 8 --master_port=4721 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 128 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 400 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1571254860.019 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571254860.019 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571254860.019 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571254860.019 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571254860.019 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571254860.019 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571254860.019 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571254860.028 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=128, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=400)
0: L2 promotion: 128B
0: Using random master seed: 340765668
dss01:465:465 [0] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:465:465 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:465:465 [0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:465:465 [0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:465:465 [0] NCCL INFO NET/IB : No device found.
dss01:465:465 [0] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
NCCL version 2.4.8+cuda10.1
dss01:468:468 [3] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:468:468 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:470:470 [5] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:470:470 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:466:466 [1] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:466:466 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:467:467 [2] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:467:467 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:469:469 [4] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:469:469 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:471:471 [6] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:471:471 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:468:468 [3] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:468:468 [3] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:468:468 [3] NCCL INFO NET/IB : No device found.

dss01:470:470 [5] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:470:470 [5] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:470:470 [5] NCCL INFO NET/IB : No device found.

dss01:469:469 [4] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:469:469 [4] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:469:469 [4] NCCL INFO NET/IB : No device found.

dss01:466:466 [1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:467:467 [2] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:466:466 [1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0

dss01:467:467 [2] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:466:466 [1] NCCL INFO NET/IB : No device found.
dss01:467:467 [2] NCCL INFO NET/IB : No device found.
dss01:468:468 [3] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>

dss01:471:471 [6] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:471:471 [6] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:471:471 [6] NCCL INFO NET/IB : No device found.
dss01:470:470 [5] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:471:471 [6] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:466:466 [1] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:469:469 [4] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:467:467 [2] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:472:472 [7] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:472:472 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:472:472 [7] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:472:472 [7] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:472:472 [7] NCCL INFO NET/IB : No device found.
dss01:472:472 [7] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:465:827 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555
dss01:468:828 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555
dss01:466:829 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555
dss01:467:831 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555
dss01:470:832 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa
dss01:469:830 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa
dss01:471:833 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa
dss01:472:834 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa
dss01:465:827 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:466:829 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:467:831 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:468:828 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:469:830 [4] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:470:832 [5] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:471:833 [6] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:472:834 [7] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:465:827 [0] NCCL INFO Channel 00 :    0   1   2   3   4   5   6   7
dss01:471:833 [6] NCCL INFO Ring 00 : 6[6] -> 7[7] via P2P/IPC
dss01:465:827 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/IPC
dss01:467:831 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/IPC
dss01:469:830 [4] NCCL INFO Ring 00 : 4[4] -> 5[5] via P2P/IPC
dss01:472:834 [7] NCCL INFO Ring 00 : 7[7] -> 0[0] via direct shared memory
dss01:466:829 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via direct shared memory
dss01:470:832 [5] NCCL INFO Ring 00 : 5[5] -> 6[6] via direct shared memory
dss01:468:828 [3] NCCL INFO Ring 00 : 3[3] -> 4[4] via direct shared memory
dss01:465:827 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
dss01:472:834 [7] NCCL INFO comm 0x7fff68007590 rank 7 nranks 8 cudaDev 7 nvmlDev 7 - Init COMPLETE
dss01:470:832 [5] NCCL INFO comm 0x7ffe90007590 rank 5 nranks 8 cudaDev 5 nvmlDev 5 - Init COMPLETE
dss01:468:828 [3] NCCL INFO comm 0x7ffe98007590 rank 3 nranks 8 cudaDev 3 nvmlDev 3 - Init COMPLETE
dss01:466:829 [1] NCCL INFO comm 0x7fff48007590 rank 1 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE
dss01:471:833 [6] NCCL INFO comm 0x7ffe90007590 rank 6 nranks 8 cudaDev 6 nvmlDev 6 - Init COMPLETE
dss01:465:827 [0] NCCL INFO comm 0x7ffe58007590 rank 0 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE
dss01:465:465 [0] NCCL INFO Launch mode Parallel
dss01:467:831 [2] NCCL INFO comm 0x7fff5c007590 rank 2 nranks 8 cudaDev 2 nvmlDev 2 - Init COMPLETE
dss01:469:830 [4] NCCL INFO comm 0x7ffe98007590 rank 4 nranks 8 cudaDev 4 nvmlDev 4 - Init COMPLETE
0: Worker 0 is using worker seed: 2941129634
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1571254883.632 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1571254886.223 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1571254886.224 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1571254886.224 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1571254887.238 global_batch_size: {"value": 1024, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 400, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 400
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1571254887.240 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1571254887.240 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1571254887.240 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1571254887.241 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1571254887.241 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1571254887.241 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1571254887.242 opt_learning_rate_warmup_steps: {"value": 400, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1571254887.274 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571254887.274 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 4262684488
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/3880]	Time 0.913 (0.913)	Data 7.27e-01 (7.27e-01)	Tok/s 5557 (5557)	Loss/tok 10.5104 (10.5104)	LR 2.000e-05
0: TRAIN [0][10/3880]	Time 0.162 (0.256)	Data 9.16e-05 (6.62e-02)	Tok/s 31697 (35088)	Loss/tok 9.6125 (10.1100)	LR 2.244e-05
0: TRAIN [0][20/3880]	Time 0.162 (0.228)	Data 1.12e-04 (3.47e-02)	Tok/s 31202 (38595)	Loss/tok 9.2532 (9.8185)	LR 2.518e-05
0: TRAIN [0][30/3880]	Time 0.192 (0.209)	Data 1.03e-04 (2.36e-02)	Tok/s 43107 (37124)	Loss/tok 9.1569 (9.6511)	LR 2.825e-05
0: TRAIN [0][40/3880]	Time 0.163 (0.200)	Data 1.15e-04 (1.79e-02)	Tok/s 31277 (36864)	Loss/tok 8.8909 (9.5061)	LR 3.170e-05
0: TRAIN [0][50/3880]	Time 0.191 (0.195)	Data 9.44e-05 (1.44e-02)	Tok/s 44295 (36773)	Loss/tok 8.7505 (9.3816)	LR 3.557e-05
0: TRAIN [0][60/3880]	Time 0.220 (0.194)	Data 9.37e-05 (1.20e-02)	Tok/s 52400 (37635)	Loss/tok 8.7540 (9.2590)	LR 3.991e-05
0: TRAIN [0][70/3880]	Time 0.191 (0.193)	Data 8.18e-05 (1.04e-02)	Tok/s 44758 (38184)	Loss/tok 8.5688 (9.1551)	LR 4.477e-05
0: TRAIN [0][80/3880]	Time 0.257 (0.192)	Data 9.51e-05 (9.09e-03)	Tok/s 57521 (38165)	Loss/tok 8.4775 (9.0727)	LR 5.024e-05
0: TRAIN [0][90/3880]	Time 0.190 (0.191)	Data 7.72e-05 (8.10e-03)	Tok/s 43898 (38318)	Loss/tok 8.2743 (8.9879)	LR 5.637e-05
0: TRAIN [0][100/3880]	Time 0.256 (0.189)	Data 9.42e-05 (7.31e-03)	Tok/s 58146 (37816)	Loss/tok 8.4690 (8.9224)	LR 6.325e-05
0: TRAIN [0][110/3880]	Time 0.191 (0.189)	Data 9.51e-05 (6.66e-03)	Tok/s 43813 (38288)	Loss/tok 8.0242 (8.8425)	LR 7.096e-05
0: TRAIN [0][120/3880]	Time 0.162 (0.189)	Data 1.26e-04 (6.12e-03)	Tok/s 31735 (38452)	Loss/tok 7.7774 (8.7735)	LR 7.962e-05
0: TRAIN [0][130/3880]	Time 0.190 (0.188)	Data 1.18e-04 (5.66e-03)	Tok/s 43448 (38570)	Loss/tok 8.0350 (8.7131)	LR 8.934e-05
0: TRAIN [0][140/3880]	Time 0.163 (0.188)	Data 1.14e-04 (5.26e-03)	Tok/s 32269 (38634)	Loss/tok 7.7829 (8.6588)	LR 1.002e-04
0: TRAIN [0][150/3880]	Time 0.219 (0.188)	Data 8.11e-05 (4.92e-03)	Tok/s 53207 (38825)	Loss/tok 8.0182 (8.6080)	LR 1.125e-04
0: TRAIN [0][160/3880]	Time 0.162 (0.188)	Data 1.10e-04 (4.63e-03)	Tok/s 32677 (38896)	Loss/tok 7.7436 (8.5633)	LR 1.262e-04
0: TRAIN [0][170/3880]	Time 0.162 (0.188)	Data 8.20e-05 (4.36e-03)	Tok/s 31363 (39221)	Loss/tok 7.7389 (8.5207)	LR 1.416e-04
0: TRAIN [0][180/3880]	Time 0.162 (0.188)	Data 8.11e-05 (4.13e-03)	Tok/s 31126 (39251)	Loss/tok 7.5012 (8.4848)	LR 1.589e-04
0: TRAIN [0][190/3880]	Time 0.162 (0.188)	Data 7.68e-05 (3.92e-03)	Tok/s 31351 (39311)	Loss/tok 7.5749 (8.4519)	LR 1.783e-04
0: TRAIN [0][200/3880]	Time 0.220 (0.188)	Data 9.70e-05 (3.73e-03)	Tok/s 52844 (39278)	Loss/tok 7.9522 (8.4214)	LR 2.000e-04
0: TRAIN [0][210/3880]	Time 0.162 (0.186)	Data 8.11e-05 (3.55e-03)	Tok/s 31161 (38749)	Loss/tok 7.5314 (8.3953)	LR 2.244e-04
0: TRAIN [0][220/3880]	Time 0.161 (0.186)	Data 8.39e-05 (3.40e-03)	Tok/s 31420 (38838)	Loss/tok 7.5633 (8.3672)	LR 2.518e-04
0: TRAIN [0][230/3880]	Time 0.220 (0.185)	Data 8.13e-05 (3.26e-03)	Tok/s 53519 (38688)	Loss/tok 7.9126 (8.3400)	LR 2.825e-04
0: TRAIN [0][240/3880]	Time 0.161 (0.186)	Data 8.85e-05 (3.13e-03)	Tok/s 31895 (38781)	Loss/tok 7.4354 (8.3126)	LR 3.170e-04
0: TRAIN [0][250/3880]	Time 0.256 (0.186)	Data 8.18e-05 (3.00e-03)	Tok/s 58278 (38956)	Loss/tok 7.8514 (8.2837)	LR 3.557e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][260/3880]	Time 0.220 (0.186)	Data 8.27e-05 (2.89e-03)	Tok/s 52697 (39052)	Loss/tok 7.6617 (8.2549)	LR 3.945e-04
0: TRAIN [0][270/3880]	Time 0.162 (0.186)	Data 9.89e-05 (2.79e-03)	Tok/s 32088 (39171)	Loss/tok 7.2341 (8.2241)	LR 4.426e-04
0: TRAIN [0][280/3880]	Time 0.162 (0.186)	Data 8.18e-05 (2.69e-03)	Tok/s 31979 (39092)	Loss/tok 7.2248 (8.1952)	LR 4.966e-04
0: TRAIN [0][290/3880]	Time 0.219 (0.186)	Data 9.80e-05 (2.61e-03)	Tok/s 52461 (39023)	Loss/tok 7.4900 (8.1670)	LR 5.572e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][300/3880]	Time 0.221 (0.186)	Data 3.40e-04 (2.52e-03)	Tok/s 53242 (39068)	Loss/tok 7.4588 (8.1357)	LR 6.181e-04
0: TRAIN [0][310/3880]	Time 0.191 (0.186)	Data 9.58e-05 (2.44e-03)	Tok/s 43467 (39077)	Loss/tok 7.0229 (8.1052)	LR 6.935e-04
0: TRAIN [0][320/3880]	Time 0.191 (0.185)	Data 8.18e-05 (2.37e-03)	Tok/s 42726 (39042)	Loss/tok 7.1872 (8.0752)	LR 7.781e-04
0: TRAIN [0][330/3880]	Time 0.161 (0.186)	Data 8.32e-05 (2.30e-03)	Tok/s 31264 (39159)	Loss/tok 6.7829 (8.0403)	LR 8.730e-04
0: TRAIN [0][340/3880]	Time 0.162 (0.185)	Data 9.89e-05 (2.24e-03)	Tok/s 32669 (39172)	Loss/tok 6.7210 (8.0073)	LR 9.796e-04
0: TRAIN [0][350/3880]	Time 0.161 (0.185)	Data 1.03e-04 (2.18e-03)	Tok/s 31496 (39197)	Loss/tok 6.6854 (7.9747)	LR 1.099e-03
0: TRAIN [0][360/3880]	Time 0.190 (0.185)	Data 8.63e-05 (2.12e-03)	Tok/s 44375 (39202)	Loss/tok 6.9405 (7.9414)	LR 1.233e-03
0: TRAIN [0][370/3880]	Time 0.162 (0.185)	Data 8.73e-05 (2.07e-03)	Tok/s 31967 (39172)	Loss/tok 6.4915 (7.9115)	LR 1.384e-03
0: TRAIN [0][380/3880]	Time 0.219 (0.185)	Data 1.09e-04 (2.01e-03)	Tok/s 53045 (39200)	Loss/tok 6.8471 (7.8792)	LR 1.552e-03
0: TRAIN [0][390/3880]	Time 0.190 (0.185)	Data 7.80e-05 (1.97e-03)	Tok/s 44763 (39091)	Loss/tok 6.6032 (7.8515)	LR 1.742e-03
0: TRAIN [0][400/3880]	Time 0.256 (0.185)	Data 4.12e-04 (1.92e-03)	Tok/s 57685 (39191)	Loss/tok 6.9802 (7.8165)	LR 1.954e-03
0: TRAIN [0][410/3880]	Time 0.191 (0.185)	Data 9.99e-05 (1.88e-03)	Tok/s 43554 (39219)	Loss/tok 6.5344 (7.7832)	LR 2.000e-03
0: TRAIN [0][420/3880]	Time 0.162 (0.185)	Data 7.99e-05 (1.83e-03)	Tok/s 31500 (39233)	Loss/tok 6.2421 (7.7497)	LR 2.000e-03
0: TRAIN [0][430/3880]	Time 0.220 (0.185)	Data 7.89e-05 (1.79e-03)	Tok/s 53153 (39329)	Loss/tok 6.4397 (7.7124)	LR 2.000e-03
0: TRAIN [0][440/3880]	Time 0.162 (0.185)	Data 9.35e-05 (1.76e-03)	Tok/s 32200 (39297)	Loss/tok 5.8547 (7.6792)	LR 2.000e-03
0: TRAIN [0][450/3880]	Time 0.161 (0.185)	Data 9.42e-05 (1.72e-03)	Tok/s 32278 (39380)	Loss/tok 5.9618 (7.6420)	LR 2.000e-03
0: TRAIN [0][460/3880]	Time 0.162 (0.185)	Data 8.87e-05 (1.68e-03)	Tok/s 31622 (39294)	Loss/tok 5.6809 (7.6113)	LR 2.000e-03
0: TRAIN [0][470/3880]	Time 0.191 (0.185)	Data 1.39e-04 (1.65e-03)	Tok/s 44342 (39301)	Loss/tok 5.8932 (7.5757)	LR 2.000e-03
0: TRAIN [0][480/3880]	Time 0.162 (0.185)	Data 8.18e-05 (1.62e-03)	Tok/s 32558 (39215)	Loss/tok 5.6089 (7.5443)	LR 2.000e-03
0: TRAIN [0][490/3880]	Time 0.162 (0.184)	Data 9.54e-05 (1.59e-03)	Tok/s 31634 (39128)	Loss/tok 5.4277 (7.5136)	LR 2.000e-03
0: TRAIN [0][500/3880]	Time 0.191 (0.184)	Data 7.89e-05 (1.56e-03)	Tok/s 43766 (39082)	Loss/tok 5.7611 (7.4804)	LR 2.000e-03
0: TRAIN [0][510/3880]	Time 0.136 (0.184)	Data 8.01e-05 (1.53e-03)	Tok/s 19358 (39057)	Loss/tok 4.4756 (7.4472)	LR 2.000e-03
0: TRAIN [0][520/3880]	Time 0.221 (0.184)	Data 1.02e-04 (1.50e-03)	Tok/s 52921 (39093)	Loss/tok 5.8681 (7.4090)	LR 2.000e-03
0: TRAIN [0][530/3880]	Time 0.191 (0.184)	Data 8.13e-05 (1.47e-03)	Tok/s 44026 (39156)	Loss/tok 5.3271 (7.3695)	LR 2.000e-03
0: TRAIN [0][540/3880]	Time 0.190 (0.184)	Data 7.75e-05 (1.45e-03)	Tok/s 43729 (39150)	Loss/tok 5.3416 (7.3335)	LR 2.000e-03
0: TRAIN [0][550/3880]	Time 0.162 (0.184)	Data 7.61e-05 (1.42e-03)	Tok/s 31131 (39119)	Loss/tok 4.8185 (7.2989)	LR 2.000e-03
0: TRAIN [0][560/3880]	Time 0.162 (0.184)	Data 9.06e-05 (1.40e-03)	Tok/s 32209 (39092)	Loss/tok 4.8290 (7.2658)	LR 2.000e-03
0: TRAIN [0][570/3880]	Time 0.190 (0.184)	Data 1.02e-04 (1.38e-03)	Tok/s 43312 (39068)	Loss/tok 5.1446 (7.2316)	LR 2.000e-03
0: TRAIN [0][580/3880]	Time 0.137 (0.184)	Data 9.27e-05 (1.35e-03)	Tok/s 19460 (39037)	Loss/tok 3.8049 (7.1973)	LR 2.000e-03
0: TRAIN [0][590/3880]	Time 0.220 (0.184)	Data 1.01e-04 (1.33e-03)	Tok/s 52878 (38986)	Loss/tok 5.1652 (7.1634)	LR 2.000e-03
0: TRAIN [0][600/3880]	Time 0.191 (0.183)	Data 1.16e-04 (1.31e-03)	Tok/s 43513 (38926)	Loss/tok 4.9923 (7.1318)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][610/3880]	Time 0.162 (0.183)	Data 1.16e-04 (1.29e-03)	Tok/s 31471 (38977)	Loss/tok 4.5962 (7.0931)	LR 2.000e-03
0: TRAIN [0][620/3880]	Time 0.162 (0.184)	Data 1.10e-04 (1.28e-03)	Tok/s 31452 (39006)	Loss/tok 4.6804 (7.0569)	LR 2.000e-03
0: TRAIN [0][630/3880]	Time 0.162 (0.184)	Data 1.11e-04 (1.26e-03)	Tok/s 31935 (39051)	Loss/tok 4.6154 (7.0192)	LR 2.000e-03
0: TRAIN [0][640/3880]	Time 0.137 (0.184)	Data 1.08e-04 (1.24e-03)	Tok/s 20096 (39063)	Loss/tok 3.8909 (6.9846)	LR 2.000e-03
0: TRAIN [0][650/3880]	Time 0.191 (0.184)	Data 9.66e-05 (1.22e-03)	Tok/s 44200 (39067)	Loss/tok 4.8602 (6.9505)	LR 2.000e-03
0: TRAIN [0][660/3880]	Time 0.161 (0.183)	Data 8.18e-05 (1.20e-03)	Tok/s 31989 (38997)	Loss/tok 4.2852 (6.9216)	LR 2.000e-03
0: TRAIN [0][670/3880]	Time 0.162 (0.183)	Data 7.25e-05 (1.19e-03)	Tok/s 32404 (38936)	Loss/tok 4.4589 (6.8927)	LR 2.000e-03
0: TRAIN [0][680/3880]	Time 0.162 (0.183)	Data 9.51e-05 (1.17e-03)	Tok/s 31801 (39014)	Loss/tok 4.2933 (6.8558)	LR 2.000e-03
0: TRAIN [0][690/3880]	Time 0.257 (0.183)	Data 7.92e-05 (1.16e-03)	Tok/s 57823 (38950)	Loss/tok 5.1165 (6.8276)	LR 2.000e-03
0: TRAIN [0][700/3880]	Time 0.191 (0.183)	Data 1.06e-04 (1.14e-03)	Tok/s 43875 (38906)	Loss/tok 4.4173 (6.7977)	LR 2.000e-03
0: TRAIN [0][710/3880]	Time 0.163 (0.183)	Data 7.77e-05 (1.13e-03)	Tok/s 31788 (38870)	Loss/tok 4.2661 (6.7696)	LR 2.000e-03
0: TRAIN [0][720/3880]	Time 0.162 (0.183)	Data 1.31e-04 (1.11e-03)	Tok/s 32237 (38889)	Loss/tok 4.3025 (6.7371)	LR 2.000e-03
0: TRAIN [0][730/3880]	Time 0.137 (0.183)	Data 2.28e-04 (1.10e-03)	Tok/s 19184 (38841)	Loss/tok 3.5142 (6.7096)	LR 2.000e-03
0: TRAIN [0][740/3880]	Time 0.191 (0.183)	Data 7.51e-05 (1.09e-03)	Tok/s 43395 (38827)	Loss/tok 4.5537 (6.6811)	LR 2.000e-03
0: TRAIN [0][750/3880]	Time 0.162 (0.183)	Data 1.31e-04 (1.07e-03)	Tok/s 31830 (38732)	Loss/tok 4.1755 (6.6574)	LR 2.000e-03
0: TRAIN [0][760/3880]	Time 0.163 (0.182)	Data 8.99e-05 (1.06e-03)	Tok/s 31843 (38702)	Loss/tok 4.3602 (6.6311)	LR 2.000e-03
0: TRAIN [0][770/3880]	Time 0.257 (0.182)	Data 1.46e-04 (1.05e-03)	Tok/s 59171 (38704)	Loss/tok 4.8210 (6.6020)	LR 2.000e-03
0: TRAIN [0][780/3880]	Time 0.191 (0.182)	Data 9.08e-05 (1.04e-03)	Tok/s 43789 (38716)	Loss/tok 4.2829 (6.5727)	LR 2.000e-03
0: TRAIN [0][790/3880]	Time 0.162 (0.182)	Data 7.51e-05 (1.02e-03)	Tok/s 32167 (38685)	Loss/tok 4.1256 (6.5476)	LR 2.000e-03
0: TRAIN [0][800/3880]	Time 0.191 (0.182)	Data 1.08e-04 (1.01e-03)	Tok/s 43847 (38718)	Loss/tok 4.4255 (6.5185)	LR 2.000e-03
0: TRAIN [0][810/3880]	Time 0.221 (0.182)	Data 1.12e-04 (1.00e-03)	Tok/s 53235 (38717)	Loss/tok 4.5657 (6.4919)	LR 2.000e-03
0: TRAIN [0][820/3880]	Time 0.161 (0.182)	Data 1.16e-04 (9.90e-04)	Tok/s 32288 (38772)	Loss/tok 4.0180 (6.4620)	LR 2.000e-03
0: TRAIN [0][830/3880]	Time 0.191 (0.183)	Data 1.03e-04 (9.79e-04)	Tok/s 45047 (38778)	Loss/tok 4.2389 (6.4351)	LR 2.000e-03
0: TRAIN [0][840/3880]	Time 0.137 (0.183)	Data 1.03e-04 (9.69e-04)	Tok/s 19955 (38801)	Loss/tok 3.5653 (6.4082)	LR 2.000e-03
0: TRAIN [0][850/3880]	Time 0.221 (0.182)	Data 8.42e-05 (9.58e-04)	Tok/s 53223 (38748)	Loss/tok 4.6882 (6.3872)	LR 2.000e-03
0: TRAIN [0][860/3880]	Time 0.163 (0.183)	Data 7.37e-05 (9.49e-04)	Tok/s 32689 (38792)	Loss/tok 4.0535 (6.3600)	LR 2.000e-03
0: TRAIN [0][870/3880]	Time 0.257 (0.183)	Data 8.49e-05 (9.39e-04)	Tok/s 57513 (38829)	Loss/tok 4.7613 (6.3330)	LR 2.000e-03
0: TRAIN [0][880/3880]	Time 0.162 (0.183)	Data 1.30e-04 (9.30e-04)	Tok/s 31976 (38811)	Loss/tok 3.8672 (6.3108)	LR 2.000e-03
0: TRAIN [0][890/3880]	Time 0.221 (0.183)	Data 1.23e-04 (9.22e-04)	Tok/s 53590 (38799)	Loss/tok 4.4911 (6.2886)	LR 2.000e-03
0: TRAIN [0][900/3880]	Time 0.162 (0.183)	Data 1.03e-04 (9.12e-04)	Tok/s 31928 (38817)	Loss/tok 4.0582 (6.2640)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][910/3880]	Time 0.191 (0.182)	Data 7.56e-05 (9.04e-04)	Tok/s 44484 (38752)	Loss/tok 4.2669 (6.2456)	LR 2.000e-03
0: TRAIN [0][920/3880]	Time 0.221 (0.182)	Data 7.84e-05 (8.95e-04)	Tok/s 53144 (38790)	Loss/tok 4.4005 (6.2210)	LR 2.000e-03
0: TRAIN [0][930/3880]	Time 0.191 (0.183)	Data 8.23e-05 (8.86e-04)	Tok/s 43397 (38846)	Loss/tok 4.2954 (6.1962)	LR 2.000e-03
0: TRAIN [0][940/3880]	Time 0.137 (0.183)	Data 7.84e-05 (8.78e-04)	Tok/s 18844 (38795)	Loss/tok 3.0524 (6.1769)	LR 2.000e-03
0: TRAIN [0][950/3880]	Time 0.162 (0.182)	Data 7.56e-05 (8.69e-04)	Tok/s 31929 (38790)	Loss/tok 3.9418 (6.1569)	LR 2.000e-03
0: TRAIN [0][960/3880]	Time 0.191 (0.182)	Data 7.51e-05 (8.61e-04)	Tok/s 43616 (38770)	Loss/tok 4.1467 (6.1374)	LR 2.000e-03
0: TRAIN [0][970/3880]	Time 0.191 (0.182)	Data 8.85e-05 (8.53e-04)	Tok/s 44055 (38782)	Loss/tok 4.1462 (6.1165)	LR 2.000e-03
0: TRAIN [0][980/3880]	Time 0.191 (0.183)	Data 9.06e-05 (8.46e-04)	Tok/s 44245 (38837)	Loss/tok 4.2153 (6.0923)	LR 2.000e-03
0: TRAIN [0][990/3880]	Time 0.162 (0.183)	Data 8.49e-05 (8.38e-04)	Tok/s 31262 (38820)	Loss/tok 3.8134 (6.0736)	LR 2.000e-03
0: TRAIN [0][1000/3880]	Time 0.162 (0.182)	Data 7.32e-05 (8.30e-04)	Tok/s 31207 (38775)	Loss/tok 3.8843 (6.0574)	LR 2.000e-03
0: TRAIN [0][1010/3880]	Time 0.192 (0.182)	Data 7.61e-05 (8.23e-04)	Tok/s 43535 (38778)	Loss/tok 4.2291 (6.0382)	LR 2.000e-03
0: TRAIN [0][1020/3880]	Time 0.137 (0.182)	Data 7.68e-05 (8.16e-04)	Tok/s 17847 (38766)	Loss/tok 3.2055 (6.0198)	LR 2.000e-03
0: TRAIN [0][1030/3880]	Time 0.191 (0.182)	Data 8.87e-05 (8.09e-04)	Tok/s 45044 (38736)	Loss/tok 4.0227 (6.0029)	LR 2.000e-03
0: TRAIN [0][1040/3880]	Time 0.136 (0.182)	Data 1.08e-04 (8.03e-04)	Tok/s 19692 (38729)	Loss/tok 3.4552 (5.9849)	LR 2.000e-03
0: TRAIN [0][1050/3880]	Time 0.162 (0.182)	Data 7.75e-05 (7.96e-04)	Tok/s 32128 (38734)	Loss/tok 3.8530 (5.9667)	LR 2.000e-03
0: TRAIN [0][1060/3880]	Time 0.162 (0.182)	Data 8.96e-05 (7.90e-04)	Tok/s 32357 (38745)	Loss/tok 3.8649 (5.9483)	LR 2.000e-03
0: TRAIN [0][1070/3880]	Time 0.221 (0.182)	Data 1.04e-04 (7.83e-04)	Tok/s 52183 (38803)	Loss/tok 4.4990 (5.9273)	LR 2.000e-03
0: TRAIN [0][1080/3880]	Time 0.162 (0.182)	Data 7.70e-05 (7.77e-04)	Tok/s 32217 (38796)	Loss/tok 3.8844 (5.9105)	LR 2.000e-03
0: TRAIN [0][1090/3880]	Time 0.221 (0.182)	Data 9.44e-05 (7.71e-04)	Tok/s 53643 (38757)	Loss/tok 4.3602 (5.8957)	LR 2.000e-03
0: TRAIN [0][1100/3880]	Time 0.191 (0.182)	Data 3.37e-04 (7.65e-04)	Tok/s 44861 (38735)	Loss/tok 4.0737 (5.8798)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1110/3880]	Time 0.191 (0.182)	Data 8.94e-05 (7.59e-04)	Tok/s 43055 (38779)	Loss/tok 4.0272 (5.8613)	LR 2.000e-03
0: TRAIN [0][1120/3880]	Time 0.257 (0.182)	Data 7.92e-05 (7.53e-04)	Tok/s 57257 (38797)	Loss/tok 4.6842 (5.8442)	LR 2.000e-03
0: TRAIN [0][1130/3880]	Time 0.136 (0.182)	Data 7.68e-05 (7.47e-04)	Tok/s 19129 (38764)	Loss/tok 3.0408 (5.8293)	LR 2.000e-03
0: TRAIN [0][1140/3880]	Time 0.162 (0.182)	Data 1.08e-04 (7.41e-04)	Tok/s 31313 (38760)	Loss/tok 3.8208 (5.8138)	LR 2.000e-03
0: TRAIN [0][1150/3880]	Time 0.220 (0.182)	Data 1.22e-04 (7.36e-04)	Tok/s 53125 (38727)	Loss/tok 4.3254 (5.8000)	LR 2.000e-03
0: TRAIN [0][1160/3880]	Time 0.192 (0.182)	Data 8.77e-05 (7.30e-04)	Tok/s 43162 (38690)	Loss/tok 3.9603 (5.7863)	LR 2.000e-03
0: TRAIN [0][1170/3880]	Time 0.191 (0.182)	Data 1.11e-04 (7.25e-04)	Tok/s 42663 (38709)	Loss/tok 4.0773 (5.7703)	LR 2.000e-03
0: TRAIN [0][1180/3880]	Time 0.222 (0.182)	Data 1.13e-04 (7.20e-04)	Tok/s 51598 (38717)	Loss/tok 4.3429 (5.7545)	LR 2.000e-03
0: TRAIN [0][1190/3880]	Time 0.191 (0.182)	Data 9.30e-05 (7.15e-04)	Tok/s 44215 (38776)	Loss/tok 4.0022 (5.7365)	LR 2.000e-03
0: TRAIN [0][1200/3880]	Time 0.162 (0.182)	Data 1.03e-04 (7.10e-04)	Tok/s 32768 (38768)	Loss/tok 3.8455 (5.7222)	LR 2.000e-03
0: TRAIN [0][1210/3880]	Time 0.162 (0.182)	Data 8.85e-05 (7.05e-04)	Tok/s 32577 (38769)	Loss/tok 3.6504 (5.7073)	LR 2.000e-03
0: TRAIN [0][1220/3880]	Time 0.192 (0.182)	Data 1.05e-04 (7.00e-04)	Tok/s 43132 (38747)	Loss/tok 4.0183 (5.6943)	LR 2.000e-03
0: TRAIN [0][1230/3880]	Time 0.162 (0.182)	Data 1.11e-04 (6.95e-04)	Tok/s 32842 (38710)	Loss/tok 3.7944 (5.6821)	LR 2.000e-03
0: TRAIN [0][1240/3880]	Time 0.191 (0.182)	Data 1.23e-04 (6.90e-04)	Tok/s 43805 (38739)	Loss/tok 3.9141 (5.6670)	LR 2.000e-03
0: TRAIN [0][1250/3880]	Time 0.163 (0.182)	Data 1.14e-04 (6.86e-04)	Tok/s 30931 (38728)	Loss/tok 3.7624 (5.6541)	LR 2.000e-03
0: TRAIN [0][1260/3880]	Time 0.162 (0.182)	Data 1.42e-04 (6.82e-04)	Tok/s 32320 (38719)	Loss/tok 3.8206 (5.6407)	LR 2.000e-03
0: TRAIN [0][1270/3880]	Time 0.257 (0.182)	Data 8.06e-05 (6.78e-04)	Tok/s 58721 (38726)	Loss/tok 4.2835 (5.6267)	LR 2.000e-03
0: TRAIN [0][1280/3880]	Time 0.163 (0.182)	Data 1.00e-04 (6.73e-04)	Tok/s 32412 (38710)	Loss/tok 3.7775 (5.6145)	LR 2.000e-03
0: TRAIN [0][1290/3880]	Time 0.221 (0.182)	Data 1.07e-04 (6.69e-04)	Tok/s 53349 (38733)	Loss/tok 4.0449 (5.6004)	LR 2.000e-03
0: TRAIN [0][1300/3880]	Time 0.163 (0.182)	Data 1.14e-04 (6.65e-04)	Tok/s 32779 (38723)	Loss/tok 3.7208 (5.5880)	LR 2.000e-03
0: TRAIN [0][1310/3880]	Time 0.162 (0.182)	Data 1.17e-04 (6.60e-04)	Tok/s 32234 (38708)	Loss/tok 3.6470 (5.5760)	LR 2.000e-03
0: TRAIN [0][1320/3880]	Time 0.162 (0.182)	Data 1.18e-04 (6.57e-04)	Tok/s 31965 (38738)	Loss/tok 3.5136 (5.5622)	LR 2.000e-03
0: TRAIN [0][1330/3880]	Time 0.162 (0.182)	Data 1.13e-04 (6.52e-04)	Tok/s 31332 (38734)	Loss/tok 3.7065 (5.5501)	LR 2.000e-03
0: TRAIN [0][1340/3880]	Time 0.162 (0.182)	Data 1.05e-04 (6.48e-04)	Tok/s 32032 (38738)	Loss/tok 3.5810 (5.5378)	LR 2.000e-03
0: TRAIN [0][1350/3880]	Time 0.162 (0.182)	Data 9.51e-05 (6.44e-04)	Tok/s 31862 (38729)	Loss/tok 3.5508 (5.5260)	LR 2.000e-03
0: TRAIN [0][1360/3880]	Time 0.162 (0.182)	Data 1.25e-04 (6.40e-04)	Tok/s 31698 (38733)	Loss/tok 3.6540 (5.5139)	LR 2.000e-03
0: TRAIN [0][1370/3880]	Time 0.221 (0.182)	Data 9.80e-05 (6.37e-04)	Tok/s 53753 (38731)	Loss/tok 4.1313 (5.5024)	LR 2.000e-03
0: TRAIN [0][1380/3880]	Time 0.162 (0.182)	Data 1.13e-04 (6.33e-04)	Tok/s 32382 (38715)	Loss/tok 3.7162 (5.4912)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1390/3880]	Time 0.191 (0.182)	Data 1.34e-04 (6.30e-04)	Tok/s 43095 (38733)	Loss/tok 3.9766 (5.4788)	LR 2.000e-03
0: TRAIN [0][1400/3880]	Time 0.162 (0.182)	Data 1.14e-04 (6.26e-04)	Tok/s 32054 (38745)	Loss/tok 3.6285 (5.4667)	LR 2.000e-03
0: TRAIN [0][1410/3880]	Time 0.258 (0.182)	Data 4.12e-04 (6.23e-04)	Tok/s 58828 (38766)	Loss/tok 4.1767 (5.4547)	LR 2.000e-03
0: TRAIN [0][1420/3880]	Time 0.191 (0.182)	Data 9.92e-05 (6.19e-04)	Tok/s 44083 (38763)	Loss/tok 4.0426 (5.4437)	LR 2.000e-03
0: TRAIN [0][1430/3880]	Time 0.162 (0.182)	Data 1.13e-04 (6.15e-04)	Tok/s 31789 (38801)	Loss/tok 3.4907 (5.4308)	LR 2.000e-03
0: TRAIN [0][1440/3880]	Time 0.162 (0.182)	Data 1.02e-04 (6.12e-04)	Tok/s 30980 (38824)	Loss/tok 3.5053 (5.4184)	LR 2.000e-03
0: TRAIN [0][1450/3880]	Time 0.162 (0.182)	Data 7.72e-05 (6.08e-04)	Tok/s 31174 (38805)	Loss/tok 3.5034 (5.4082)	LR 2.000e-03
0: TRAIN [0][1460/3880]	Time 0.136 (0.182)	Data 1.12e-04 (6.05e-04)	Tok/s 19625 (38781)	Loss/tok 2.8852 (5.3982)	LR 2.000e-03
0: TRAIN [0][1470/3880]	Time 0.162 (0.182)	Data 8.27e-05 (6.01e-04)	Tok/s 30986 (38733)	Loss/tok 3.6421 (5.3897)	LR 2.000e-03
0: TRAIN [0][1480/3880]	Time 0.137 (0.182)	Data 1.36e-04 (5.98e-04)	Tok/s 19307 (38748)	Loss/tok 2.9234 (5.3785)	LR 2.000e-03
0: TRAIN [0][1490/3880]	Time 0.192 (0.182)	Data 8.49e-05 (5.95e-04)	Tok/s 44540 (38754)	Loss/tok 3.8939 (5.3678)	LR 2.000e-03
0: TRAIN [0][1500/3880]	Time 0.191 (0.182)	Data 9.16e-05 (5.92e-04)	Tok/s 43665 (38791)	Loss/tok 3.9188 (5.3561)	LR 2.000e-03
0: TRAIN [0][1510/3880]	Time 0.162 (0.182)	Data 1.17e-04 (5.89e-04)	Tok/s 30869 (38805)	Loss/tok 3.6807 (5.3453)	LR 2.000e-03
0: TRAIN [0][1520/3880]	Time 0.136 (0.182)	Data 8.75e-05 (5.86e-04)	Tok/s 19153 (38790)	Loss/tok 3.0885 (5.3358)	LR 2.000e-03
0: TRAIN [0][1530/3880]	Time 0.191 (0.182)	Data 1.21e-04 (5.83e-04)	Tok/s 43131 (38809)	Loss/tok 3.9056 (5.3252)	LR 2.000e-03
0: TRAIN [0][1540/3880]	Time 0.162 (0.182)	Data 1.11e-04 (5.80e-04)	Tok/s 33106 (38804)	Loss/tok 3.6272 (5.3155)	LR 2.000e-03
0: TRAIN [0][1550/3880]	Time 0.163 (0.182)	Data 2.24e-04 (5.77e-04)	Tok/s 31158 (38800)	Loss/tok 3.5281 (5.3060)	LR 2.000e-03
0: TRAIN [0][1560/3880]	Time 0.162 (0.182)	Data 1.10e-04 (5.74e-04)	Tok/s 32643 (38806)	Loss/tok 3.4894 (5.2964)	LR 2.000e-03
0: TRAIN [0][1570/3880]	Time 0.162 (0.182)	Data 7.89e-05 (5.71e-04)	Tok/s 31855 (38814)	Loss/tok 3.4601 (5.2871)	LR 2.000e-03
0: TRAIN [0][1580/3880]	Time 0.162 (0.182)	Data 7.80e-05 (5.67e-04)	Tok/s 30760 (38794)	Loss/tok 3.7797 (5.2782)	LR 2.000e-03
0: TRAIN [0][1590/3880]	Time 0.192 (0.182)	Data 1.07e-04 (5.65e-04)	Tok/s 44289 (38786)	Loss/tok 3.8412 (5.2693)	LR 2.000e-03
0: TRAIN [0][1600/3880]	Time 0.162 (0.182)	Data 1.19e-04 (5.62e-04)	Tok/s 32401 (38763)	Loss/tok 3.4356 (5.2612)	LR 2.000e-03
0: TRAIN [0][1610/3880]	Time 0.136 (0.182)	Data 7.92e-05 (5.59e-04)	Tok/s 19028 (38754)	Loss/tok 2.9502 (5.2525)	LR 2.000e-03
0: TRAIN [0][1620/3880]	Time 0.257 (0.182)	Data 1.17e-04 (5.56e-04)	Tok/s 58529 (38792)	Loss/tok 4.1417 (5.2424)	LR 2.000e-03
0: TRAIN [0][1630/3880]	Time 0.191 (0.182)	Data 1.06e-04 (5.53e-04)	Tok/s 43817 (38784)	Loss/tok 3.7672 (5.2339)	LR 2.000e-03
0: TRAIN [0][1640/3880]	Time 0.191 (0.182)	Data 9.27e-05 (5.50e-04)	Tok/s 44179 (38807)	Loss/tok 3.6336 (5.2241)	LR 2.000e-03
0: TRAIN [0][1650/3880]	Time 0.162 (0.182)	Data 8.39e-05 (5.48e-04)	Tok/s 32236 (38840)	Loss/tok 3.5529 (5.2141)	LR 2.000e-03
0: TRAIN [0][1660/3880]	Time 0.192 (0.182)	Data 9.49e-05 (5.45e-04)	Tok/s 42840 (38776)	Loss/tok 3.8275 (5.2078)	LR 2.000e-03
0: TRAIN [0][1670/3880]	Time 0.162 (0.182)	Data 1.08e-04 (5.42e-04)	Tok/s 32380 (38754)	Loss/tok 3.6658 (5.2003)	LR 2.000e-03
0: TRAIN [0][1680/3880]	Time 0.162 (0.182)	Data 1.25e-04 (5.40e-04)	Tok/s 32409 (38754)	Loss/tok 3.7322 (5.1917)	LR 2.000e-03
0: TRAIN [0][1690/3880]	Time 0.221 (0.182)	Data 8.58e-05 (5.37e-04)	Tok/s 52423 (38751)	Loss/tok 4.0467 (5.1837)	LR 2.000e-03
0: TRAIN [0][1700/3880]	Time 0.137 (0.182)	Data 8.13e-05 (5.35e-04)	Tok/s 19421 (38696)	Loss/tok 2.8862 (5.1774)	LR 2.000e-03
0: TRAIN [0][1710/3880]	Time 0.221 (0.182)	Data 1.26e-04 (5.32e-04)	Tok/s 53566 (38669)	Loss/tok 4.0495 (5.1703)	LR 2.000e-03
0: TRAIN [0][1720/3880]	Time 0.162 (0.182)	Data 1.19e-04 (5.30e-04)	Tok/s 32134 (38666)	Loss/tok 3.5486 (5.1621)	LR 2.000e-03
0: TRAIN [0][1730/3880]	Time 0.162 (0.182)	Data 1.00e-04 (5.27e-04)	Tok/s 31333 (38680)	Loss/tok 3.4585 (5.1536)	LR 2.000e-03
0: TRAIN [0][1740/3880]	Time 0.191 (0.182)	Data 1.19e-04 (5.25e-04)	Tok/s 43521 (38672)	Loss/tok 3.9184 (5.1460)	LR 2.000e-03
0: TRAIN [0][1750/3880]	Time 0.191 (0.182)	Data 2.88e-04 (5.22e-04)	Tok/s 43556 (38665)	Loss/tok 3.7438 (5.1382)	LR 2.000e-03
0: TRAIN [0][1760/3880]	Time 0.191 (0.182)	Data 3.92e-04 (5.20e-04)	Tok/s 43753 (38662)	Loss/tok 3.9528 (5.1305)	LR 2.000e-03
0: TRAIN [0][1770/3880]	Time 0.136 (0.182)	Data 8.42e-05 (5.17e-04)	Tok/s 19093 (38636)	Loss/tok 2.9919 (5.1234)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1780/3880]	Time 0.192 (0.182)	Data 7.87e-05 (5.15e-04)	Tok/s 43364 (38657)	Loss/tok 3.8448 (5.1149)	LR 2.000e-03
0: TRAIN [0][1790/3880]	Time 0.257 (0.182)	Data 4.22e-04 (5.13e-04)	Tok/s 58466 (38643)	Loss/tok 4.1616 (5.1079)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1800/3880]	Time 0.162 (0.182)	Data 1.10e-04 (5.11e-04)	Tok/s 32496 (38651)	Loss/tok 3.2600 (5.0999)	LR 2.000e-03
0: TRAIN [0][1810/3880]	Time 0.162 (0.182)	Data 7.68e-05 (5.09e-04)	Tok/s 31279 (38646)	Loss/tok 3.4239 (5.0929)	LR 2.000e-03
0: TRAIN [0][1820/3880]	Time 0.191 (0.182)	Data 1.43e-04 (5.06e-04)	Tok/s 44026 (38643)	Loss/tok 3.8336 (5.0859)	LR 2.000e-03
0: TRAIN [0][1830/3880]	Time 0.162 (0.182)	Data 4.01e-04 (5.05e-04)	Tok/s 32814 (38646)	Loss/tok 3.4312 (5.0780)	LR 2.000e-03
0: TRAIN [0][1840/3880]	Time 0.162 (0.182)	Data 7.82e-05 (5.02e-04)	Tok/s 31507 (38658)	Loss/tok 3.4988 (5.0702)	LR 2.000e-03
0: TRAIN [0][1850/3880]	Time 0.257 (0.182)	Data 7.96e-05 (5.00e-04)	Tok/s 58149 (38651)	Loss/tok 4.0836 (5.0632)	LR 2.000e-03
0: TRAIN [0][1860/3880]	Time 0.221 (0.182)	Data 7.70e-05 (4.98e-04)	Tok/s 52657 (38671)	Loss/tok 3.9046 (5.0552)	LR 2.000e-03
0: TRAIN [0][1870/3880]	Time 0.221 (0.182)	Data 1.05e-04 (4.96e-04)	Tok/s 52406 (38697)	Loss/tok 3.8050 (5.0471)	LR 2.000e-03
0: TRAIN [0][1880/3880]	Time 0.162 (0.182)	Data 1.07e-04 (4.94e-04)	Tok/s 31681 (38675)	Loss/tok 3.5868 (5.0410)	LR 2.000e-03
0: TRAIN [0][1890/3880]	Time 0.256 (0.182)	Data 8.01e-05 (4.92e-04)	Tok/s 57581 (38704)	Loss/tok 4.2445 (5.0328)	LR 2.000e-03
0: TRAIN [0][1900/3880]	Time 0.221 (0.182)	Data 1.43e-04 (4.90e-04)	Tok/s 53304 (38689)	Loss/tok 3.8556 (5.0266)	LR 2.000e-03
0: TRAIN [0][1910/3880]	Time 0.191 (0.182)	Data 9.63e-05 (4.88e-04)	Tok/s 43963 (38700)	Loss/tok 3.8630 (5.0195)	LR 2.000e-03
0: TRAIN [0][1920/3880]	Time 0.162 (0.182)	Data 9.80e-05 (4.86e-04)	Tok/s 32098 (38685)	Loss/tok 3.5300 (5.0133)	LR 2.000e-03
0: TRAIN [0][1930/3880]	Time 0.191 (0.182)	Data 7.72e-05 (4.84e-04)	Tok/s 44777 (38709)	Loss/tok 3.6978 (5.0056)	LR 2.000e-03
0: TRAIN [0][1940/3880]	Time 0.136 (0.182)	Data 1.22e-04 (4.82e-04)	Tok/s 19756 (38714)	Loss/tok 3.1202 (4.9986)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1950/3880]	Time 0.221 (0.182)	Data 9.37e-05 (4.80e-04)	Tok/s 53399 (38724)	Loss/tok 3.9836 (4.9919)	LR 2.000e-03
0: TRAIN [0][1960/3880]	Time 0.162 (0.182)	Data 9.27e-05 (4.78e-04)	Tok/s 31608 (38718)	Loss/tok 3.6785 (4.9856)	LR 2.000e-03
0: TRAIN [0][1970/3880]	Time 0.163 (0.182)	Data 9.47e-05 (4.76e-04)	Tok/s 32438 (38715)	Loss/tok 3.3370 (4.9792)	LR 2.000e-03
0: TRAIN [0][1980/3880]	Time 0.162 (0.182)	Data 9.13e-05 (4.75e-04)	Tok/s 32379 (38714)	Loss/tok 3.5475 (4.9730)	LR 2.000e-03
0: TRAIN [0][1990/3880]	Time 0.163 (0.182)	Data 1.09e-04 (4.73e-04)	Tok/s 31644 (38700)	Loss/tok 3.4450 (4.9671)	LR 2.000e-03
0: TRAIN [0][2000/3880]	Time 0.191 (0.182)	Data 1.34e-04 (4.71e-04)	Tok/s 43338 (38712)	Loss/tok 3.8048 (4.9603)	LR 2.000e-03
0: TRAIN [0][2010/3880]	Time 0.162 (0.182)	Data 1.04e-04 (4.69e-04)	Tok/s 31638 (38712)	Loss/tok 3.6152 (4.9542)	LR 2.000e-03
0: TRAIN [0][2020/3880]	Time 0.191 (0.182)	Data 8.11e-05 (4.67e-04)	Tok/s 43409 (38717)	Loss/tok 3.7564 (4.9479)	LR 2.000e-03
0: TRAIN [0][2030/3880]	Time 0.191 (0.182)	Data 8.61e-05 (4.65e-04)	Tok/s 45084 (38708)	Loss/tok 3.5746 (4.9420)	LR 2.000e-03
0: TRAIN [0][2040/3880]	Time 0.138 (0.182)	Data 1.26e-04 (4.64e-04)	Tok/s 19407 (38698)	Loss/tok 2.9573 (4.9359)	LR 2.000e-03
0: TRAIN [0][2050/3880]	Time 0.190 (0.182)	Data 1.50e-04 (4.62e-04)	Tok/s 42859 (38718)	Loss/tok 3.8901 (4.9289)	LR 2.000e-03
0: TRAIN [0][2060/3880]	Time 0.162 (0.182)	Data 8.89e-05 (4.60e-04)	Tok/s 31565 (38718)	Loss/tok 3.4787 (4.9226)	LR 2.000e-03
0: TRAIN [0][2070/3880]	Time 0.162 (0.182)	Data 1.01e-04 (4.58e-04)	Tok/s 32060 (38685)	Loss/tok 3.4014 (4.9177)	LR 2.000e-03
0: TRAIN [0][2080/3880]	Time 0.136 (0.182)	Data 1.08e-04 (4.57e-04)	Tok/s 19472 (38675)	Loss/tok 2.8442 (4.9122)	LR 2.000e-03
0: TRAIN [0][2090/3880]	Time 0.162 (0.182)	Data 1.17e-04 (4.55e-04)	Tok/s 31302 (38676)	Loss/tok 3.4220 (4.9060)	LR 2.000e-03
0: TRAIN [0][2100/3880]	Time 0.162 (0.182)	Data 9.08e-05 (4.53e-04)	Tok/s 31059 (38685)	Loss/tok 3.6024 (4.9000)	LR 2.000e-03
0: TRAIN [0][2110/3880]	Time 0.162 (0.182)	Data 1.17e-04 (4.52e-04)	Tok/s 31455 (38677)	Loss/tok 3.4640 (4.8945)	LR 2.000e-03
0: TRAIN [0][2120/3880]	Time 0.162 (0.182)	Data 1.03e-04 (4.50e-04)	Tok/s 30915 (38659)	Loss/tok 3.4376 (4.8893)	LR 2.000e-03
0: TRAIN [0][2130/3880]	Time 0.162 (0.182)	Data 1.42e-04 (4.48e-04)	Tok/s 30593 (38633)	Loss/tok 3.3517 (4.8842)	LR 2.000e-03
0: TRAIN [0][2140/3880]	Time 0.162 (0.182)	Data 8.08e-05 (4.47e-04)	Tok/s 31707 (38628)	Loss/tok 3.4413 (4.8788)	LR 2.000e-03
0: TRAIN [0][2150/3880]	Time 0.221 (0.182)	Data 8.23e-05 (4.45e-04)	Tok/s 52620 (38630)	Loss/tok 3.9720 (4.8733)	LR 2.000e-03
0: TRAIN [0][2160/3880]	Time 0.162 (0.182)	Data 1.44e-04 (4.44e-04)	Tok/s 31275 (38656)	Loss/tok 3.4875 (4.8670)	LR 2.000e-03
0: TRAIN [0][2170/3880]	Time 0.138 (0.182)	Data 1.31e-04 (4.43e-04)	Tok/s 18655 (38639)	Loss/tok 2.7484 (4.8620)	LR 2.000e-03
0: TRAIN [0][2180/3880]	Time 0.191 (0.182)	Data 9.78e-05 (4.41e-04)	Tok/s 44718 (38640)	Loss/tok 3.7690 (4.8563)	LR 2.000e-03
0: TRAIN [0][2190/3880]	Time 0.191 (0.182)	Data 8.25e-05 (4.39e-04)	Tok/s 44024 (38641)	Loss/tok 3.5850 (4.8507)	LR 2.000e-03
0: TRAIN [0][2200/3880]	Time 0.162 (0.182)	Data 1.07e-04 (4.38e-04)	Tok/s 33511 (38615)	Loss/tok 3.3716 (4.8458)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2210/3880]	Time 0.191 (0.182)	Data 7.37e-05 (4.36e-04)	Tok/s 42507 (38644)	Loss/tok 3.7181 (4.8398)	LR 2.000e-03
0: TRAIN [0][2220/3880]	Time 0.191 (0.182)	Data 7.46e-05 (4.35e-04)	Tok/s 44673 (38644)	Loss/tok 3.6324 (4.8347)	LR 2.000e-03
0: TRAIN [0][2230/3880]	Time 0.136 (0.182)	Data 9.11e-05 (4.33e-04)	Tok/s 19521 (38627)	Loss/tok 2.8610 (4.8299)	LR 2.000e-03
0: TRAIN [0][2240/3880]	Time 0.162 (0.182)	Data 7.08e-05 (4.32e-04)	Tok/s 32521 (38636)	Loss/tok 3.4261 (4.8244)	LR 2.000e-03
0: TRAIN [0][2250/3880]	Time 0.220 (0.182)	Data 7.32e-05 (4.30e-04)	Tok/s 51670 (38660)	Loss/tok 3.9710 (4.8185)	LR 2.000e-03
0: TRAIN [0][2260/3880]	Time 0.162 (0.182)	Data 7.51e-05 (4.29e-04)	Tok/s 31308 (38655)	Loss/tok 3.5818 (4.8135)	LR 2.000e-03
0: TRAIN [0][2270/3880]	Time 0.221 (0.182)	Data 7.63e-05 (4.27e-04)	Tok/s 52563 (38661)	Loss/tok 3.8988 (4.8079)	LR 2.000e-03
0: TRAIN [0][2280/3880]	Time 0.161 (0.182)	Data 1.10e-04 (4.26e-04)	Tok/s 32844 (38645)	Loss/tok 3.3416 (4.8034)	LR 2.000e-03
0: TRAIN [0][2290/3880]	Time 0.162 (0.182)	Data 8.99e-05 (4.25e-04)	Tok/s 32952 (38645)	Loss/tok 3.5968 (4.7984)	LR 2.000e-03
0: TRAIN [0][2300/3880]	Time 0.136 (0.182)	Data 8.06e-05 (4.23e-04)	Tok/s 19535 (38646)	Loss/tok 2.9630 (4.7933)	LR 2.000e-03
0: TRAIN [0][2310/3880]	Time 0.191 (0.182)	Data 1.04e-04 (4.22e-04)	Tok/s 43996 (38644)	Loss/tok 3.5626 (4.7882)	LR 2.000e-03
0: TRAIN [0][2320/3880]	Time 0.162 (0.182)	Data 3.14e-04 (4.21e-04)	Tok/s 31525 (38635)	Loss/tok 3.5463 (4.7836)	LR 2.000e-03
0: TRAIN [0][2330/3880]	Time 0.162 (0.182)	Data 1.09e-04 (4.20e-04)	Tok/s 32624 (38626)	Loss/tok 3.3041 (4.7785)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2340/3880]	Time 0.191 (0.182)	Data 8.25e-05 (4.18e-04)	Tok/s 44017 (38635)	Loss/tok 3.6080 (4.7732)	LR 2.000e-03
0: TRAIN [0][2350/3880]	Time 0.162 (0.182)	Data 7.70e-05 (4.17e-04)	Tok/s 31233 (38642)	Loss/tok 3.3598 (4.7683)	LR 2.000e-03
0: TRAIN [0][2360/3880]	Time 0.162 (0.182)	Data 8.01e-05 (4.16e-04)	Tok/s 32532 (38634)	Loss/tok 3.4016 (4.7636)	LR 2.000e-03
0: TRAIN [0][2370/3880]	Time 0.191 (0.182)	Data 8.94e-05 (4.14e-04)	Tok/s 43804 (38628)	Loss/tok 3.8113 (4.7590)	LR 2.000e-03
0: TRAIN [0][2380/3880]	Time 0.162 (0.182)	Data 7.72e-05 (4.13e-04)	Tok/s 31078 (38609)	Loss/tok 3.4003 (4.7548)	LR 2.000e-03
0: TRAIN [0][2390/3880]	Time 0.221 (0.182)	Data 9.08e-05 (4.12e-04)	Tok/s 52773 (38607)	Loss/tok 3.8031 (4.7500)	LR 2.000e-03
0: TRAIN [0][2400/3880]	Time 0.136 (0.182)	Data 7.63e-05 (4.11e-04)	Tok/s 19841 (38612)	Loss/tok 2.9722 (4.7455)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][2410/3880]	Time 0.162 (0.182)	Data 7.89e-05 (4.09e-04)	Tok/s 31540 (38603)	Loss/tok 3.2197 (4.7410)	LR 2.000e-03
0: TRAIN [0][2420/3880]	Time 0.162 (0.182)	Data 7.96e-05 (4.08e-04)	Tok/s 31269 (38616)	Loss/tok 3.4842 (4.7359)	LR 2.000e-03
0: TRAIN [0][2430/3880]	Time 0.191 (0.182)	Data 7.58e-05 (4.07e-04)	Tok/s 43610 (38615)	Loss/tok 3.7739 (4.7313)	LR 2.000e-03
0: TRAIN [0][2440/3880]	Time 0.191 (0.182)	Data 8.77e-05 (4.05e-04)	Tok/s 44208 (38614)	Loss/tok 3.5404 (4.7269)	LR 2.000e-03
0: TRAIN [0][2450/3880]	Time 0.162 (0.182)	Data 8.34e-05 (4.04e-04)	Tok/s 31838 (38635)	Loss/tok 3.2809 (4.7217)	LR 2.000e-03
0: TRAIN [0][2460/3880]	Time 0.220 (0.182)	Data 8.27e-05 (4.03e-04)	Tok/s 54511 (38665)	Loss/tok 3.6783 (4.7164)	LR 2.000e-03
0: TRAIN [0][2470/3880]	Time 0.162 (0.182)	Data 7.27e-05 (4.02e-04)	Tok/s 31830 (38665)	Loss/tok 3.4046 (4.7120)	LR 2.000e-03
0: TRAIN [0][2480/3880]	Time 0.137 (0.182)	Data 7.53e-05 (4.00e-04)	Tok/s 19585 (38687)	Loss/tok 2.8586 (4.7073)	LR 2.000e-03
0: TRAIN [0][2490/3880]	Time 0.221 (0.182)	Data 1.05e-04 (3.99e-04)	Tok/s 52465 (38695)	Loss/tok 3.7358 (4.7027)	LR 2.000e-03
0: TRAIN [0][2500/3880]	Time 0.191 (0.182)	Data 7.65e-05 (3.98e-04)	Tok/s 44058 (38689)	Loss/tok 3.4773 (4.6984)	LR 2.000e-03
0: TRAIN [0][2510/3880]	Time 0.162 (0.182)	Data 7.46e-05 (3.97e-04)	Tok/s 31939 (38705)	Loss/tok 3.4853 (4.6937)	LR 2.000e-03
0: TRAIN [0][2520/3880]	Time 0.191 (0.182)	Data 9.01e-05 (3.95e-04)	Tok/s 44294 (38697)	Loss/tok 3.6540 (4.6894)	LR 2.000e-03
0: TRAIN [0][2530/3880]	Time 0.257 (0.182)	Data 7.34e-05 (3.94e-04)	Tok/s 57916 (38713)	Loss/tok 4.0707 (4.6849)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][2540/3880]	Time 0.162 (0.182)	Data 7.63e-05 (3.93e-04)	Tok/s 31921 (38692)	Loss/tok 3.4182 (4.6813)	LR 2.000e-03
0: TRAIN [0][2550/3880]	Time 0.191 (0.182)	Data 7.70e-05 (3.92e-04)	Tok/s 43094 (38711)	Loss/tok 3.6222 (4.6769)	LR 2.000e-03
0: TRAIN [0][2560/3880]	Time 0.191 (0.182)	Data 9.42e-05 (3.91e-04)	Tok/s 44144 (38720)	Loss/tok 3.5369 (4.6726)	LR 2.000e-03
0: TRAIN [0][2570/3880]	Time 0.191 (0.182)	Data 7.37e-05 (3.90e-04)	Tok/s 43545 (38715)	Loss/tok 3.5720 (4.6684)	LR 2.000e-03
0: TRAIN [0][2580/3880]	Time 0.162 (0.182)	Data 9.01e-05 (3.88e-04)	Tok/s 31746 (38727)	Loss/tok 3.4148 (4.6639)	LR 2.000e-03
0: TRAIN [0][2590/3880]	Time 0.221 (0.182)	Data 1.16e-04 (3.87e-04)	Tok/s 52217 (38722)	Loss/tok 3.7582 (4.6600)	LR 2.000e-03
0: TRAIN [0][2600/3880]	Time 0.191 (0.182)	Data 5.49e-04 (3.86e-04)	Tok/s 44311 (38696)	Loss/tok 3.7074 (4.6565)	LR 2.000e-03
0: TRAIN [0][2610/3880]	Time 0.162 (0.182)	Data 4.73e-04 (3.86e-04)	Tok/s 31979 (38676)	Loss/tok 3.1381 (4.6529)	LR 2.000e-03
0: TRAIN [0][2620/3880]	Time 0.162 (0.182)	Data 1.13e-04 (3.84e-04)	Tok/s 31996 (38679)	Loss/tok 3.4769 (4.6489)	LR 2.000e-03
0: TRAIN [0][2630/3880]	Time 0.162 (0.182)	Data 9.82e-05 (3.83e-04)	Tok/s 32213 (38675)	Loss/tok 3.3523 (4.6450)	LR 2.000e-03
0: TRAIN [0][2640/3880]	Time 0.192 (0.182)	Data 2.38e-04 (3.82e-04)	Tok/s 43272 (38671)	Loss/tok 3.6635 (4.6410)	LR 2.000e-03
0: TRAIN [0][2650/3880]	Time 0.191 (0.182)	Data 8.34e-05 (3.81e-04)	Tok/s 44491 (38692)	Loss/tok 3.7212 (4.6368)	LR 2.000e-03
0: TRAIN [0][2660/3880]	Time 0.162 (0.182)	Data 8.20e-05 (3.80e-04)	Tok/s 31263 (38673)	Loss/tok 3.4402 (4.6333)	LR 2.000e-03
0: TRAIN [0][2670/3880]	Time 0.162 (0.182)	Data 8.73e-05 (3.79e-04)	Tok/s 32131 (38657)	Loss/tok 3.4692 (4.6298)	LR 2.000e-03
0: TRAIN [0][2680/3880]	Time 0.162 (0.182)	Data 9.25e-05 (3.78e-04)	Tok/s 31628 (38621)	Loss/tok 3.4908 (4.6267)	LR 2.000e-03
0: TRAIN [0][2690/3880]	Time 0.162 (0.182)	Data 1.34e-04 (3.77e-04)	Tok/s 33066 (38612)	Loss/tok 3.2962 (4.6229)	LR 2.000e-03
0: TRAIN [0][2700/3880]	Time 0.191 (0.182)	Data 8.80e-05 (3.76e-04)	Tok/s 43702 (38601)	Loss/tok 3.6235 (4.6193)	LR 2.000e-03
0: TRAIN [0][2710/3880]	Time 0.137 (0.182)	Data 3.33e-04 (3.75e-04)	Tok/s 19486 (38579)	Loss/tok 2.7531 (4.6160)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][2720/3880]	Time 0.162 (0.182)	Data 9.87e-05 (3.74e-04)	Tok/s 31988 (38588)	Loss/tok 3.3965 (4.6124)	LR 2.000e-03
0: TRAIN [0][2730/3880]	Time 0.162 (0.182)	Data 1.01e-04 (3.73e-04)	Tok/s 31464 (38586)	Loss/tok 3.4200 (4.6086)	LR 2.000e-03
0: TRAIN [0][2740/3880]	Time 0.162 (0.182)	Data 1.14e-04 (3.72e-04)	Tok/s 31920 (38581)	Loss/tok 3.4490 (4.6049)	LR 2.000e-03
0: TRAIN [0][2750/3880]	Time 0.191 (0.182)	Data 9.70e-05 (3.71e-04)	Tok/s 44265 (38583)	Loss/tok 3.6969 (4.6012)	LR 2.000e-03
0: TRAIN [0][2760/3880]	Time 0.221 (0.182)	Data 1.06e-04 (3.70e-04)	Tok/s 52453 (38591)	Loss/tok 3.7959 (4.5972)	LR 2.000e-03
0: TRAIN [0][2770/3880]	Time 0.191 (0.182)	Data 8.92e-05 (3.69e-04)	Tok/s 42634 (38569)	Loss/tok 3.6888 (4.5941)	LR 2.000e-03
0: TRAIN [0][2780/3880]	Time 0.162 (0.182)	Data 1.03e-04 (3.68e-04)	Tok/s 32131 (38578)	Loss/tok 3.3237 (4.5904)	LR 2.000e-03
0: TRAIN [0][2790/3880]	Time 0.191 (0.182)	Data 1.09e-04 (3.68e-04)	Tok/s 43697 (38587)	Loss/tok 3.5327 (4.5869)	LR 2.000e-03
0: TRAIN [0][2800/3880]	Time 0.162 (0.182)	Data 1.05e-04 (3.67e-04)	Tok/s 32398 (38603)	Loss/tok 3.2907 (4.5829)	LR 2.000e-03
0: TRAIN [0][2810/3880]	Time 0.162 (0.182)	Data 1.36e-04 (3.66e-04)	Tok/s 32661 (38594)	Loss/tok 3.4366 (4.5795)	LR 2.000e-03
0: TRAIN [0][2820/3880]	Time 0.221 (0.182)	Data 8.51e-05 (3.65e-04)	Tok/s 52778 (38608)	Loss/tok 3.7640 (4.5757)	LR 2.000e-03
0: TRAIN [0][2830/3880]	Time 0.221 (0.182)	Data 1.39e-04 (3.64e-04)	Tok/s 53460 (38600)	Loss/tok 3.7398 (4.5721)	LR 2.000e-03
0: TRAIN [0][2840/3880]	Time 0.221 (0.182)	Data 1.90e-04 (3.64e-04)	Tok/s 52049 (38599)	Loss/tok 3.8108 (4.5686)	LR 2.000e-03
0: TRAIN [0][2850/3880]	Time 0.221 (0.182)	Data 1.44e-04 (3.63e-04)	Tok/s 53302 (38613)	Loss/tok 3.8694 (4.5646)	LR 2.000e-03
0: TRAIN [0][2860/3880]	Time 0.162 (0.182)	Data 9.35e-05 (3.62e-04)	Tok/s 32335 (38601)	Loss/tok 3.1473 (4.5613)	LR 2.000e-03
0: TRAIN [0][2870/3880]	Time 0.162 (0.182)	Data 1.60e-04 (3.61e-04)	Tok/s 30575 (38600)	Loss/tok 3.2322 (4.5579)	LR 2.000e-03
0: TRAIN [0][2880/3880]	Time 0.256 (0.182)	Data 1.21e-04 (3.61e-04)	Tok/s 58696 (38600)	Loss/tok 3.8403 (4.5544)	LR 2.000e-03
0: TRAIN [0][2890/3880]	Time 0.162 (0.182)	Data 2.97e-04 (3.60e-04)	Tok/s 31978 (38599)	Loss/tok 3.3034 (4.5510)	LR 2.000e-03
0: TRAIN [0][2900/3880]	Time 0.191 (0.182)	Data 8.34e-05 (3.59e-04)	Tok/s 43237 (38607)	Loss/tok 3.5214 (4.5476)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][2910/3880]	Time 0.191 (0.182)	Data 8.94e-05 (3.58e-04)	Tok/s 43966 (38605)	Loss/tok 3.6109 (4.5444)	LR 2.000e-03
0: TRAIN [0][2920/3880]	Time 0.162 (0.182)	Data 9.37e-05 (3.57e-04)	Tok/s 32023 (38595)	Loss/tok 3.3117 (4.5413)	LR 2.000e-03
0: TRAIN [0][2930/3880]	Time 0.191 (0.182)	Data 8.92e-05 (3.57e-04)	Tok/s 44164 (38598)	Loss/tok 3.4398 (4.5378)	LR 2.000e-03
0: TRAIN [0][2940/3880]	Time 0.191 (0.182)	Data 7.87e-05 (3.56e-04)	Tok/s 43672 (38578)	Loss/tok 3.6271 (4.5347)	LR 2.000e-03
0: TRAIN [0][2950/3880]	Time 0.162 (0.182)	Data 4.70e-04 (3.55e-04)	Tok/s 32665 (38576)	Loss/tok 3.1993 (4.5315)	LR 2.000e-03
0: TRAIN [0][2960/3880]	Time 0.191 (0.182)	Data 7.80e-05 (3.54e-04)	Tok/s 43659 (38579)	Loss/tok 3.5942 (4.5281)	LR 2.000e-03
0: TRAIN [0][2970/3880]	Time 0.162 (0.182)	Data 9.68e-05 (3.53e-04)	Tok/s 31600 (38574)	Loss/tok 3.2602 (4.5249)	LR 2.000e-03
0: TRAIN [0][2980/3880]	Time 0.257 (0.182)	Data 9.70e-05 (3.53e-04)	Tok/s 59132 (38585)	Loss/tok 3.7970 (4.5212)	LR 2.000e-03
0: TRAIN [0][2990/3880]	Time 0.136 (0.182)	Data 8.32e-05 (3.52e-04)	Tok/s 19576 (38583)	Loss/tok 2.8552 (4.5181)	LR 2.000e-03
0: TRAIN [0][3000/3880]	Time 0.191 (0.182)	Data 9.78e-05 (3.51e-04)	Tok/s 44077 (38598)	Loss/tok 3.5432 (4.5147)	LR 2.000e-03
0: TRAIN [0][3010/3880]	Time 0.136 (0.182)	Data 1.15e-04 (3.50e-04)	Tok/s 19227 (38584)	Loss/tok 3.0405 (4.5117)	LR 2.000e-03
0: TRAIN [0][3020/3880]	Time 0.190 (0.182)	Data 7.56e-05 (3.49e-04)	Tok/s 44492 (38580)	Loss/tok 3.5354 (4.5087)	LR 2.000e-03
0: TRAIN [0][3030/3880]	Time 0.162 (0.182)	Data 1.22e-04 (3.49e-04)	Tok/s 32072 (38564)	Loss/tok 3.2846 (4.5056)	LR 2.000e-03
0: TRAIN [0][3040/3880]	Time 0.162 (0.181)	Data 8.65e-05 (3.48e-04)	Tok/s 32299 (38541)	Loss/tok 3.4703 (4.5030)	LR 2.000e-03
0: TRAIN [0][3050/3880]	Time 0.162 (0.182)	Data 1.42e-04 (3.47e-04)	Tok/s 31784 (38543)	Loss/tok 3.5212 (4.4998)	LR 2.000e-03
0: TRAIN [0][3060/3880]	Time 0.162 (0.182)	Data 7.25e-05 (3.46e-04)	Tok/s 32340 (38564)	Loss/tok 3.2299 (4.4961)	LR 2.000e-03
0: TRAIN [0][3070/3880]	Time 0.221 (0.182)	Data 7.32e-05 (3.45e-04)	Tok/s 52999 (38570)	Loss/tok 3.6782 (4.4929)	LR 2.000e-03
0: TRAIN [0][3080/3880]	Time 0.162 (0.182)	Data 1.33e-04 (3.45e-04)	Tok/s 31104 (38559)	Loss/tok 3.3382 (4.4901)	LR 2.000e-03
0: TRAIN [0][3090/3880]	Time 0.161 (0.182)	Data 8.85e-05 (3.44e-04)	Tok/s 31766 (38556)	Loss/tok 3.5355 (4.4870)	LR 2.000e-03
0: TRAIN [0][3100/3880]	Time 0.162 (0.182)	Data 7.39e-05 (3.43e-04)	Tok/s 32553 (38563)	Loss/tok 3.2994 (4.4836)	LR 2.000e-03
0: TRAIN [0][3110/3880]	Time 0.191 (0.182)	Data 7.13e-05 (3.42e-04)	Tok/s 44903 (38571)	Loss/tok 3.5266 (4.4805)	LR 2.000e-03
0: TRAIN [0][3120/3880]	Time 0.221 (0.182)	Data 7.68e-05 (3.41e-04)	Tok/s 52017 (38587)	Loss/tok 3.8118 (4.4774)	LR 2.000e-03
0: TRAIN [0][3130/3880]	Time 0.162 (0.182)	Data 7.34e-05 (3.41e-04)	Tok/s 31991 (38599)	Loss/tok 3.2642 (4.4741)	LR 2.000e-03
0: TRAIN [0][3140/3880]	Time 0.191 (0.182)	Data 8.82e-05 (3.40e-04)	Tok/s 43892 (38605)	Loss/tok 3.6721 (4.4710)	LR 2.000e-03
0: TRAIN [0][3150/3880]	Time 0.191 (0.182)	Data 1.67e-04 (3.39e-04)	Tok/s 44195 (38596)	Loss/tok 3.4657 (4.4681)	LR 2.000e-03
0: TRAIN [0][3160/3880]	Time 0.162 (0.182)	Data 8.89e-05 (3.38e-04)	Tok/s 31363 (38585)	Loss/tok 3.4893 (4.4653)	LR 2.000e-03
0: TRAIN [0][3170/3880]	Time 0.191 (0.182)	Data 8.61e-05 (3.38e-04)	Tok/s 44239 (38571)	Loss/tok 3.6025 (4.4627)	LR 2.000e-03
0: TRAIN [0][3180/3880]	Time 0.191 (0.182)	Data 9.94e-05 (3.37e-04)	Tok/s 44359 (38560)	Loss/tok 3.6177 (4.4599)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][3190/3880]	Time 0.191 (0.182)	Data 7.51e-05 (3.36e-04)	Tok/s 43482 (38562)	Loss/tok 3.4171 (4.4571)	LR 2.000e-03
0: TRAIN [0][3200/3880]	Time 0.162 (0.182)	Data 7.51e-05 (3.35e-04)	Tok/s 30781 (38556)	Loss/tok 3.4196 (4.4543)	LR 2.000e-03
0: TRAIN [0][3210/3880]	Time 0.162 (0.182)	Data 7.58e-05 (3.35e-04)	Tok/s 31824 (38559)	Loss/tok 3.2846 (4.4515)	LR 2.000e-03
0: TRAIN [0][3220/3880]	Time 0.191 (0.182)	Data 7.56e-05 (3.34e-04)	Tok/s 45008 (38564)	Loss/tok 3.4724 (4.4487)	LR 2.000e-03
0: TRAIN [0][3230/3880]	Time 0.162 (0.181)	Data 7.22e-05 (3.33e-04)	Tok/s 31844 (38543)	Loss/tok 3.2844 (4.4462)	LR 2.000e-03
0: TRAIN [0][3240/3880]	Time 0.136 (0.181)	Data 7.20e-05 (3.32e-04)	Tok/s 19016 (38543)	Loss/tok 2.6658 (4.4433)	LR 2.000e-03
0: TRAIN [0][3250/3880]	Time 0.220 (0.182)	Data 7.44e-05 (3.32e-04)	Tok/s 53382 (38559)	Loss/tok 3.7153 (4.4402)	LR 2.000e-03
0: TRAIN [0][3260/3880]	Time 0.257 (0.181)	Data 8.03e-05 (3.31e-04)	Tok/s 57179 (38548)	Loss/tok 4.0335 (4.4376)	LR 2.000e-03
0: TRAIN [0][3270/3880]	Time 0.162 (0.181)	Data 5.37e-04 (3.30e-04)	Tok/s 32003 (38535)	Loss/tok 3.2199 (4.4350)	LR 2.000e-03
0: TRAIN [0][3280/3880]	Time 0.162 (0.181)	Data 1.08e-04 (3.30e-04)	Tok/s 31650 (38522)	Loss/tok 3.4284 (4.4326)	LR 2.000e-03
0: TRAIN [0][3290/3880]	Time 0.220 (0.181)	Data 1.14e-04 (3.29e-04)	Tok/s 53372 (38520)	Loss/tok 3.5431 (4.4299)	LR 2.000e-03
0: TRAIN [0][3300/3880]	Time 0.162 (0.181)	Data 1.25e-04 (3.28e-04)	Tok/s 32302 (38540)	Loss/tok 3.1853 (4.4270)	LR 2.000e-03
0: TRAIN [0][3310/3880]	Time 0.191 (0.181)	Data 9.37e-05 (3.28e-04)	Tok/s 44024 (38540)	Loss/tok 3.6035 (4.4242)	LR 2.000e-03
0: TRAIN [0][3320/3880]	Time 0.162 (0.181)	Data 7.92e-05 (3.27e-04)	Tok/s 31238 (38523)	Loss/tok 3.3836 (4.4217)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][3330/3880]	Time 0.162 (0.181)	Data 1.26e-04 (3.26e-04)	Tok/s 31541 (38514)	Loss/tok 3.3729 (4.4191)	LR 2.000e-03
0: TRAIN [0][3340/3880]	Time 0.162 (0.181)	Data 7.46e-05 (3.26e-04)	Tok/s 31531 (38511)	Loss/tok 3.3840 (4.4163)	LR 2.000e-03
0: TRAIN [0][3350/3880]	Time 0.257 (0.181)	Data 9.61e-05 (3.25e-04)	Tok/s 59703 (38513)	Loss/tok 3.7964 (4.4137)	LR 2.000e-03
0: TRAIN [0][3360/3880]	Time 0.162 (0.181)	Data 7.44e-05 (3.24e-04)	Tok/s 32409 (38519)	Loss/tok 3.3960 (4.4109)	LR 2.000e-03
0: TRAIN [0][3370/3880]	Time 0.162 (0.181)	Data 7.25e-05 (3.24e-04)	Tok/s 31699 (38518)	Loss/tok 3.2631 (4.4081)	LR 2.000e-03
0: TRAIN [0][3380/3880]	Time 0.258 (0.181)	Data 7.44e-05 (3.23e-04)	Tok/s 57371 (38524)	Loss/tok 4.0967 (4.4054)	LR 2.000e-03
0: TRAIN [0][3390/3880]	Time 0.192 (0.181)	Data 7.61e-05 (3.22e-04)	Tok/s 43933 (38523)	Loss/tok 3.3907 (4.4027)	LR 2.000e-03
0: TRAIN [0][3400/3880]	Time 0.162 (0.181)	Data 9.08e-05 (3.22e-04)	Tok/s 31880 (38526)	Loss/tok 3.2810 (4.4000)	LR 2.000e-03
0: TRAIN [0][3410/3880]	Time 0.221 (0.181)	Data 8.30e-05 (3.21e-04)	Tok/s 53054 (38529)	Loss/tok 3.6980 (4.3975)	LR 2.000e-03
0: TRAIN [0][3420/3880]	Time 0.162 (0.181)	Data 7.80e-05 (3.20e-04)	Tok/s 31341 (38519)	Loss/tok 3.3023 (4.3950)	LR 2.000e-03
0: TRAIN [0][3430/3880]	Time 0.162 (0.181)	Data 8.30e-05 (3.20e-04)	Tok/s 31967 (38541)	Loss/tok 3.3921 (4.3922)	LR 2.000e-03
0: TRAIN [0][3440/3880]	Time 0.221 (0.181)	Data 9.06e-05 (3.19e-04)	Tok/s 51771 (38554)	Loss/tok 3.7688 (4.3893)	LR 2.000e-03
0: TRAIN [0][3450/3880]	Time 0.191 (0.181)	Data 7.99e-05 (3.18e-04)	Tok/s 43066 (38558)	Loss/tok 3.4926 (4.3865)	LR 2.000e-03
0: TRAIN [0][3460/3880]	Time 0.191 (0.182)	Data 8.61e-05 (3.18e-04)	Tok/s 43986 (38576)	Loss/tok 3.5125 (4.3836)	LR 2.000e-03
0: TRAIN [0][3470/3880]	Time 0.137 (0.182)	Data 1.21e-04 (3.17e-04)	Tok/s 19340 (38575)	Loss/tok 2.9686 (4.3810)	LR 2.000e-03
0: TRAIN [0][3480/3880]	Time 0.162 (0.181)	Data 8.96e-05 (3.16e-04)	Tok/s 31823 (38563)	Loss/tok 3.3718 (4.3786)	LR 2.000e-03
0: TRAIN [0][3490/3880]	Time 0.191 (0.181)	Data 1.11e-04 (3.16e-04)	Tok/s 43576 (38564)	Loss/tok 3.5398 (4.3762)	LR 2.000e-03
0: TRAIN [0][3500/3880]	Time 0.162 (0.181)	Data 9.18e-05 (3.15e-04)	Tok/s 32534 (38561)	Loss/tok 3.1704 (4.3738)	LR 2.000e-03
0: TRAIN [0][3510/3880]	Time 0.164 (0.181)	Data 6.96e-05 (3.15e-04)	Tok/s 31857 (38560)	Loss/tok 3.3243 (4.3712)	LR 2.000e-03
0: TRAIN [0][3520/3880]	Time 0.162 (0.181)	Data 9.06e-05 (3.14e-04)	Tok/s 32293 (38552)	Loss/tok 3.2081 (4.3688)	LR 2.000e-03
0: TRAIN [0][3530/3880]	Time 0.221 (0.181)	Data 9.32e-05 (3.13e-04)	Tok/s 52945 (38567)	Loss/tok 3.8655 (4.3662)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][3540/3880]	Time 0.257 (0.181)	Data 7.65e-05 (3.13e-04)	Tok/s 56763 (38558)	Loss/tok 3.9658 (4.3642)	LR 2.000e-03
0: TRAIN [0][3550/3880]	Time 0.191 (0.181)	Data 7.53e-05 (3.12e-04)	Tok/s 43347 (38567)	Loss/tok 3.4289 (4.3616)	LR 2.000e-03
0: TRAIN [0][3560/3880]	Time 0.191 (0.182)	Data 1.05e-04 (3.12e-04)	Tok/s 44118 (38569)	Loss/tok 3.4321 (4.3591)	LR 2.000e-03
0: TRAIN [0][3570/3880]	Time 0.162 (0.182)	Data 7.27e-05 (3.11e-04)	Tok/s 31972 (38573)	Loss/tok 3.2691 (4.3565)	LR 2.000e-03
0: TRAIN [0][3580/3880]	Time 0.162 (0.181)	Data 7.89e-05 (3.11e-04)	Tok/s 32099 (38566)	Loss/tok 3.3003 (4.3541)	LR 2.000e-03
0: TRAIN [0][3590/3880]	Time 0.191 (0.181)	Data 7.72e-05 (3.10e-04)	Tok/s 43647 (38573)	Loss/tok 3.6557 (4.3517)	LR 2.000e-03
0: TRAIN [0][3600/3880]	Time 0.162 (0.182)	Data 8.39e-05 (3.09e-04)	Tok/s 31009 (38576)	Loss/tok 3.1683 (4.3494)	LR 2.000e-03
0: TRAIN [0][3610/3880]	Time 0.162 (0.182)	Data 1.35e-04 (3.09e-04)	Tok/s 32741 (38579)	Loss/tok 3.0751 (4.3469)	LR 2.000e-03
0: TRAIN [0][3620/3880]	Time 0.191 (0.182)	Data 8.92e-05 (3.08e-04)	Tok/s 44938 (38583)	Loss/tok 3.3708 (4.3445)	LR 2.000e-03
0: TRAIN [0][3630/3880]	Time 0.191 (0.182)	Data 9.06e-05 (3.08e-04)	Tok/s 44211 (38591)	Loss/tok 3.5202 (4.3421)	LR 2.000e-03
0: TRAIN [0][3640/3880]	Time 0.221 (0.182)	Data 8.85e-05 (3.07e-04)	Tok/s 52671 (38584)	Loss/tok 3.7981 (4.3399)	LR 2.000e-03
0: TRAIN [0][3650/3880]	Time 0.191 (0.182)	Data 9.54e-05 (3.06e-04)	Tok/s 44564 (38591)	Loss/tok 3.5482 (4.3375)	LR 2.000e-03
0: TRAIN [0][3660/3880]	Time 0.191 (0.182)	Data 8.08e-05 (3.06e-04)	Tok/s 44283 (38593)	Loss/tok 3.5159 (4.3353)	LR 2.000e-03
0: TRAIN [0][3670/3880]	Time 0.162 (0.181)	Data 7.46e-05 (3.05e-04)	Tok/s 32691 (38586)	Loss/tok 3.2072 (4.3330)	LR 2.000e-03
0: TRAIN [0][3680/3880]	Time 0.221 (0.182)	Data 8.30e-05 (3.05e-04)	Tok/s 52603 (38595)	Loss/tok 3.6329 (4.3306)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][3690/3880]	Time 0.220 (0.182)	Data 7.39e-05 (3.04e-04)	Tok/s 52974 (38602)	Loss/tok 3.6899 (4.3282)	LR 2.000e-03
0: TRAIN [0][3700/3880]	Time 0.221 (0.182)	Data 7.34e-05 (3.03e-04)	Tok/s 52834 (38613)	Loss/tok 3.6103 (4.3259)	LR 2.000e-03
0: TRAIN [0][3710/3880]	Time 0.162 (0.182)	Data 7.92e-05 (3.03e-04)	Tok/s 32969 (38614)	Loss/tok 3.0466 (4.3236)	LR 2.000e-03
0: TRAIN [0][3720/3880]	Time 0.221 (0.182)	Data 7.20e-05 (3.02e-04)	Tok/s 53306 (38633)	Loss/tok 3.6089 (4.3212)	LR 2.000e-03
0: TRAIN [0][3730/3880]	Time 0.221 (0.182)	Data 1.42e-04 (3.02e-04)	Tok/s 51269 (38638)	Loss/tok 3.7921 (4.3189)	LR 2.000e-03
0: TRAIN [0][3740/3880]	Time 0.136 (0.182)	Data 7.96e-05 (3.01e-04)	Tok/s 19537 (38620)	Loss/tok 2.8187 (4.3170)	LR 2.000e-03
0: TRAIN [0][3750/3880]	Time 0.162 (0.182)	Data 9.04e-05 (3.01e-04)	Tok/s 32099 (38606)	Loss/tok 3.3005 (4.3149)	LR 2.000e-03
0: TRAIN [0][3760/3880]	Time 0.136 (0.182)	Data 1.01e-04 (3.00e-04)	Tok/s 19018 (38609)	Loss/tok 2.7979 (4.3127)	LR 2.000e-03
0: TRAIN [0][3770/3880]	Time 0.162 (0.182)	Data 8.77e-05 (3.00e-04)	Tok/s 31438 (38611)	Loss/tok 3.1896 (4.3106)	LR 2.000e-03
0: TRAIN [0][3780/3880]	Time 0.191 (0.182)	Data 7.87e-05 (2.99e-04)	Tok/s 44354 (38609)	Loss/tok 3.5842 (4.3085)	LR 2.000e-03
0: TRAIN [0][3790/3880]	Time 0.191 (0.182)	Data 9.61e-05 (2.99e-04)	Tok/s 45297 (38624)	Loss/tok 3.4575 (4.3061)	LR 2.000e-03
0: TRAIN [0][3800/3880]	Time 0.191 (0.182)	Data 7.75e-05 (2.98e-04)	Tok/s 44155 (38622)	Loss/tok 3.4535 (4.3039)	LR 2.000e-03
0: TRAIN [0][3810/3880]	Time 0.191 (0.182)	Data 7.46e-05 (2.97e-04)	Tok/s 44341 (38613)	Loss/tok 3.6212 (4.3019)	LR 2.000e-03
0: TRAIN [0][3820/3880]	Time 0.162 (0.182)	Data 8.11e-05 (2.97e-04)	Tok/s 31484 (38624)	Loss/tok 3.2783 (4.2996)	LR 2.000e-03
0: TRAIN [0][3830/3880]	Time 0.163 (0.182)	Data 7.77e-05 (2.96e-04)	Tok/s 31466 (38628)	Loss/tok 3.1937 (4.2976)	LR 2.000e-03
0: TRAIN [0][3840/3880]	Time 0.191 (0.182)	Data 7.58e-05 (2.96e-04)	Tok/s 43935 (38635)	Loss/tok 3.5063 (4.2954)	LR 2.000e-03
0: TRAIN [0][3850/3880]	Time 0.257 (0.182)	Data 7.46e-05 (2.95e-04)	Tok/s 58841 (38628)	Loss/tok 3.8604 (4.2936)	LR 2.000e-03
0: TRAIN [0][3860/3880]	Time 0.162 (0.182)	Data 7.63e-05 (2.95e-04)	Tok/s 31492 (38615)	Loss/tok 3.2187 (4.2917)	LR 2.000e-03
0: TRAIN [0][3870/3880]	Time 0.191 (0.182)	Data 7.41e-05 (2.94e-04)	Tok/s 43649 (38608)	Loss/tok 3.4163 (4.2896)	LR 2.000e-03
:::MLL 1571255592.287 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1571255592.288 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.756 (0.756)	Decoder iters 142.0 (142.0)	Tok/s 22325 (22325)
0: Running moses detokenizer
0: BLEU(score=20.31343240204641, counts=[35607, 16510, 8831, 4925], totals=[66844, 63841, 60838, 57840], precisions=[53.268804978756506, 25.861123729264893, 14.515598803379467, 8.514868603042878], bp=1.0, sys_len=66844, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571255594.354 eval_accuracy: {"value": 20.31, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1571255594.355 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 4.2871	Test BLEU: 20.31
0: Performance: Epoch: 0	Training: 308807 Tok/s
0: Finished epoch 0
:::MLL 1571255594.355 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1571255594.355 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571255594.356 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 45808789
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][0/3880]	Time 0.900 (0.900)	Data 6.61e-01 (6.61e-01)	Tok/s 9341 (9341)	Loss/tok 3.3960 (3.3960)	LR 2.000e-03
0: TRAIN [1][10/3880]	Time 0.136 (0.255)	Data 9.44e-05 (6.02e-02)	Tok/s 18983 (36729)	Loss/tok 2.8565 (3.4456)	LR 2.000e-03
0: TRAIN [1][20/3880]	Time 0.257 (0.219)	Data 7.46e-05 (3.16e-02)	Tok/s 56930 (37150)	Loss/tok 3.9344 (3.4500)	LR 2.000e-03
0: TRAIN [1][30/3880]	Time 0.192 (0.204)	Data 7.25e-05 (2.14e-02)	Tok/s 43352 (36638)	Loss/tok 3.4183 (3.4113)	LR 2.000e-03
0: TRAIN [1][40/3880]	Time 0.191 (0.200)	Data 7.70e-05 (1.62e-02)	Tok/s 43172 (37722)	Loss/tok 3.5265 (3.4068)	LR 2.000e-03
0: TRAIN [1][50/3880]	Time 0.191 (0.198)	Data 7.99e-05 (1.31e-02)	Tok/s 44411 (38424)	Loss/tok 3.4848 (3.4153)	LR 2.000e-03
0: TRAIN [1][60/3880]	Time 0.162 (0.194)	Data 9.99e-05 (1.09e-02)	Tok/s 32170 (38142)	Loss/tok 3.1779 (3.4017)	LR 2.000e-03
0: TRAIN [1][70/3880]	Time 0.162 (0.195)	Data 8.32e-05 (9.41e-03)	Tok/s 31198 (39311)	Loss/tok 3.2985 (3.4137)	LR 2.000e-03
0: TRAIN [1][80/3880]	Time 0.191 (0.193)	Data 8.56e-05 (8.26e-03)	Tok/s 43290 (39165)	Loss/tok 3.4075 (3.4116)	LR 2.000e-03
0: TRAIN [1][90/3880]	Time 0.191 (0.191)	Data 8.25e-05 (7.36e-03)	Tok/s 44202 (38983)	Loss/tok 3.3060 (3.4043)	LR 2.000e-03
0: TRAIN [1][100/3880]	Time 0.137 (0.190)	Data 9.66e-05 (6.64e-03)	Tok/s 19741 (38881)	Loss/tok 2.6854 (3.3988)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][110/3880]	Time 0.162 (0.190)	Data 8.89e-05 (6.05e-03)	Tok/s 32435 (38955)	Loss/tok 3.1489 (3.4054)	LR 2.000e-03
0: TRAIN [1][120/3880]	Time 0.162 (0.189)	Data 8.94e-05 (5.56e-03)	Tok/s 31794 (39191)	Loss/tok 3.0536 (3.4087)	LR 2.000e-03
0: TRAIN [1][130/3880]	Time 0.221 (0.189)	Data 1.06e-04 (5.14e-03)	Tok/s 53175 (39388)	Loss/tok 3.5835 (3.4118)	LR 2.000e-03
0: TRAIN [1][140/3880]	Time 0.191 (0.190)	Data 8.89e-05 (4.78e-03)	Tok/s 42892 (39775)	Loss/tok 3.4353 (3.4156)	LR 2.000e-03
0: TRAIN [1][150/3880]	Time 0.162 (0.188)	Data 8.77e-05 (4.47e-03)	Tok/s 33127 (39333)	Loss/tok 3.3194 (3.4104)	LR 2.000e-03
0: TRAIN [1][160/3880]	Time 0.162 (0.188)	Data 8.92e-05 (4.20e-03)	Tok/s 32136 (39227)	Loss/tok 3.1990 (3.4145)	LR 2.000e-03
0: TRAIN [1][170/3880]	Time 0.162 (0.187)	Data 8.75e-05 (3.96e-03)	Tok/s 32197 (39222)	Loss/tok 3.3046 (3.4141)	LR 2.000e-03
0: TRAIN [1][180/3880]	Time 0.136 (0.186)	Data 9.66e-05 (3.75e-03)	Tok/s 19844 (39009)	Loss/tok 2.8590 (3.4083)	LR 2.000e-03
0: TRAIN [1][190/3880]	Time 0.138 (0.187)	Data 7.70e-05 (3.55e-03)	Tok/s 19740 (39196)	Loss/tok 2.6862 (3.4146)	LR 2.000e-03
0: TRAIN [1][200/3880]	Time 0.162 (0.187)	Data 7.89e-05 (3.38e-03)	Tok/s 31297 (39261)	Loss/tok 3.1512 (3.4142)	LR 2.000e-03
0: TRAIN [1][210/3880]	Time 0.162 (0.186)	Data 7.44e-05 (3.23e-03)	Tok/s 31787 (39078)	Loss/tok 3.4302 (3.4100)	LR 2.000e-03
0: TRAIN [1][220/3880]	Time 0.162 (0.186)	Data 9.01e-05 (3.09e-03)	Tok/s 31832 (39140)	Loss/tok 3.2672 (3.4113)	LR 2.000e-03
0: TRAIN [1][230/3880]	Time 0.191 (0.186)	Data 7.75e-05 (2.96e-03)	Tok/s 43215 (39238)	Loss/tok 3.6346 (3.4138)	LR 2.000e-03
0: TRAIN [1][240/3880]	Time 0.162 (0.185)	Data 7.41e-05 (2.84e-03)	Tok/s 30814 (38979)	Loss/tok 3.2829 (3.4084)	LR 2.000e-03
0: TRAIN [1][250/3880]	Time 0.162 (0.185)	Data 1.23e-04 (2.73e-03)	Tok/s 30968 (38889)	Loss/tok 3.2184 (3.4079)	LR 2.000e-03
0: TRAIN [1][260/3880]	Time 0.162 (0.185)	Data 1.01e-04 (2.63e-03)	Tok/s 32452 (38972)	Loss/tok 3.2669 (3.4104)	LR 2.000e-03
0: TRAIN [1][270/3880]	Time 0.191 (0.185)	Data 8.63e-05 (2.53e-03)	Tok/s 43856 (38849)	Loss/tok 3.5841 (3.4097)	LR 2.000e-03
0: TRAIN [1][280/3880]	Time 0.191 (0.184)	Data 9.39e-05 (2.45e-03)	Tok/s 43249 (38735)	Loss/tok 3.3958 (3.4044)	LR 2.000e-03
0: TRAIN [1][290/3880]	Time 0.136 (0.184)	Data 8.75e-05 (2.37e-03)	Tok/s 19486 (38685)	Loss/tok 2.7321 (3.4055)	LR 2.000e-03
0: TRAIN [1][300/3880]	Time 0.162 (0.184)	Data 8.77e-05 (2.29e-03)	Tok/s 32221 (38634)	Loss/tok 3.3339 (3.4048)	LR 2.000e-03
0: TRAIN [1][310/3880]	Time 0.221 (0.184)	Data 2.75e-04 (2.22e-03)	Tok/s 52964 (38667)	Loss/tok 3.7339 (3.4042)	LR 2.000e-03
0: TRAIN [1][320/3880]	Time 0.162 (0.184)	Data 7.65e-05 (2.16e-03)	Tok/s 31738 (38639)	Loss/tok 3.2810 (3.4046)	LR 2.000e-03
0: TRAIN [1][330/3880]	Time 0.221 (0.184)	Data 9.37e-05 (2.09e-03)	Tok/s 51990 (38751)	Loss/tok 3.6856 (3.4062)	LR 2.000e-03
0: TRAIN [1][340/3880]	Time 0.162 (0.184)	Data 7.68e-05 (2.03e-03)	Tok/s 33018 (38570)	Loss/tok 3.3044 (3.4071)	LR 2.000e-03
0: TRAIN [1][350/3880]	Time 0.191 (0.183)	Data 1.04e-04 (1.98e-03)	Tok/s 44117 (38485)	Loss/tok 3.3998 (3.4033)	LR 2.000e-03
0: TRAIN [1][360/3880]	Time 0.191 (0.183)	Data 9.18e-05 (1.93e-03)	Tok/s 43587 (38501)	Loss/tok 3.5063 (3.4027)	LR 2.000e-03
0: TRAIN [1][370/3880]	Time 0.191 (0.183)	Data 8.51e-05 (1.88e-03)	Tok/s 44385 (38564)	Loss/tok 3.3457 (3.4050)	LR 2.000e-03
0: TRAIN [1][380/3880]	Time 0.191 (0.183)	Data 8.73e-05 (1.83e-03)	Tok/s 44493 (38663)	Loss/tok 3.3741 (3.4060)	LR 2.000e-03
0: TRAIN [1][390/3880]	Time 0.191 (0.183)	Data 9.54e-05 (1.79e-03)	Tok/s 44850 (38716)	Loss/tok 3.2964 (3.4064)	LR 2.000e-03
0: TRAIN [1][400/3880]	Time 0.136 (0.183)	Data 1.16e-04 (1.75e-03)	Tok/s 19677 (38674)	Loss/tok 2.8021 (3.4059)	LR 2.000e-03
0: TRAIN [1][410/3880]	Time 0.136 (0.183)	Data 1.00e-04 (1.71e-03)	Tok/s 19489 (38635)	Loss/tok 2.8056 (3.4030)	LR 2.000e-03
0: TRAIN [1][420/3880]	Time 0.161 (0.183)	Data 1.19e-04 (1.67e-03)	Tok/s 31605 (38601)	Loss/tok 3.0957 (3.4025)	LR 2.000e-03
0: TRAIN [1][430/3880]	Time 0.221 (0.183)	Data 1.21e-04 (1.63e-03)	Tok/s 52229 (38626)	Loss/tok 3.6406 (3.4025)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][440/3880]	Time 0.191 (0.183)	Data 7.99e-05 (1.60e-03)	Tok/s 44049 (38672)	Loss/tok 3.5496 (3.4026)	LR 2.000e-03
0: TRAIN [1][450/3880]	Time 0.191 (0.183)	Data 7.72e-05 (1.56e-03)	Tok/s 44498 (38738)	Loss/tok 3.4729 (3.4018)	LR 2.000e-03
0: TRAIN [1][460/3880]	Time 0.161 (0.183)	Data 8.03e-05 (1.53e-03)	Tok/s 32447 (38828)	Loss/tok 3.2966 (3.4027)	LR 2.000e-03
0: TRAIN [1][470/3880]	Time 0.162 (0.183)	Data 7.37e-05 (1.50e-03)	Tok/s 32263 (38911)	Loss/tok 3.3215 (3.4036)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][480/3880]	Time 0.162 (0.183)	Data 1.08e-04 (1.47e-03)	Tok/s 31451 (38842)	Loss/tok 3.1755 (3.4029)	LR 2.000e-03
0: TRAIN [1][490/3880]	Time 0.191 (0.183)	Data 7.65e-05 (1.44e-03)	Tok/s 43950 (38938)	Loss/tok 3.3511 (3.4055)	LR 2.000e-03
0: TRAIN [1][500/3880]	Time 0.162 (0.183)	Data 1.05e-04 (1.42e-03)	Tok/s 31215 (38905)	Loss/tok 3.1877 (3.4069)	LR 2.000e-03
0: TRAIN [1][510/3880]	Time 0.162 (0.183)	Data 1.27e-04 (1.39e-03)	Tok/s 32833 (38839)	Loss/tok 3.1767 (3.4059)	LR 2.000e-03
0: TRAIN [1][520/3880]	Time 0.221 (0.183)	Data 9.49e-05 (1.37e-03)	Tok/s 52116 (38834)	Loss/tok 3.7363 (3.4058)	LR 2.000e-03
0: TRAIN [1][530/3880]	Time 0.162 (0.183)	Data 9.39e-05 (1.34e-03)	Tok/s 31701 (38791)	Loss/tok 3.2247 (3.4048)	LR 2.000e-03
0: TRAIN [1][540/3880]	Time 0.162 (0.183)	Data 9.11e-05 (1.32e-03)	Tok/s 31617 (38763)	Loss/tok 3.1560 (3.4041)	LR 2.000e-03
0: TRAIN [1][550/3880]	Time 0.191 (0.183)	Data 1.27e-04 (1.30e-03)	Tok/s 43737 (38786)	Loss/tok 3.2591 (3.4026)	LR 2.000e-03
0: TRAIN [1][560/3880]	Time 0.221 (0.183)	Data 9.68e-05 (1.28e-03)	Tok/s 52161 (38854)	Loss/tok 3.6883 (3.4040)	LR 2.000e-03
0: TRAIN [1][570/3880]	Time 0.162 (0.183)	Data 1.06e-04 (1.26e-03)	Tok/s 32525 (38854)	Loss/tok 3.2264 (3.4047)	LR 2.000e-03
0: TRAIN [1][580/3880]	Time 0.162 (0.183)	Data 1.32e-04 (1.24e-03)	Tok/s 30487 (38858)	Loss/tok 3.3412 (3.4056)	LR 2.000e-03
0: TRAIN [1][590/3880]	Time 0.163 (0.183)	Data 1.03e-04 (1.22e-03)	Tok/s 31742 (38857)	Loss/tok 3.2759 (3.4052)	LR 2.000e-03
0: TRAIN [1][600/3880]	Time 0.221 (0.183)	Data 9.56e-05 (1.20e-03)	Tok/s 53434 (38816)	Loss/tok 3.4348 (3.4037)	LR 2.000e-03
0: TRAIN [1][610/3880]	Time 0.191 (0.183)	Data 8.51e-05 (1.18e-03)	Tok/s 43811 (38783)	Loss/tok 3.3580 (3.4016)	LR 2.000e-03
0: TRAIN [1][620/3880]	Time 0.162 (0.183)	Data 2.20e-04 (1.16e-03)	Tok/s 32346 (38850)	Loss/tok 3.3116 (3.4024)	LR 2.000e-03
0: TRAIN [1][630/3880]	Time 0.221 (0.183)	Data 8.73e-05 (1.15e-03)	Tok/s 53073 (38807)	Loss/tok 3.5038 (3.4019)	LR 2.000e-03
0: TRAIN [1][640/3880]	Time 0.162 (0.182)	Data 9.11e-05 (1.13e-03)	Tok/s 32552 (38733)	Loss/tok 3.2667 (3.4004)	LR 2.000e-03
0: TRAIN [1][650/3880]	Time 0.191 (0.183)	Data 8.08e-05 (1.11e-03)	Tok/s 43678 (38744)	Loss/tok 3.4699 (3.4011)	LR 2.000e-03
0: TRAIN [1][660/3880]	Time 0.136 (0.183)	Data 1.08e-04 (1.10e-03)	Tok/s 19545 (38765)	Loss/tok 2.7697 (3.4019)	LR 2.000e-03
0: TRAIN [1][670/3880]	Time 0.136 (0.182)	Data 9.16e-05 (1.08e-03)	Tok/s 18661 (38747)	Loss/tok 2.8034 (3.4008)	LR 2.000e-03
0: TRAIN [1][680/3880]	Time 0.163 (0.183)	Data 7.84e-05 (1.07e-03)	Tok/s 32208 (38769)	Loss/tok 3.2524 (3.4023)	LR 2.000e-03
0: TRAIN [1][690/3880]	Time 0.162 (0.183)	Data 9.27e-05 (1.06e-03)	Tok/s 31593 (38753)	Loss/tok 3.1598 (3.4020)	LR 2.000e-03
0: TRAIN [1][700/3880]	Time 0.162 (0.182)	Data 9.56e-05 (1.04e-03)	Tok/s 31940 (38709)	Loss/tok 3.2701 (3.4002)	LR 2.000e-03
0: TRAIN [1][710/3880]	Time 0.162 (0.182)	Data 8.77e-05 (1.03e-03)	Tok/s 30959 (38722)	Loss/tok 3.1079 (3.3998)	LR 2.000e-03
0: TRAIN [1][720/3880]	Time 0.191 (0.182)	Data 7.99e-05 (1.02e-03)	Tok/s 43823 (38765)	Loss/tok 3.4385 (3.3999)	LR 2.000e-03
0: TRAIN [1][730/3880]	Time 0.162 (0.182)	Data 7.99e-05 (1.00e-03)	Tok/s 32567 (38756)	Loss/tok 3.0837 (3.3994)	LR 2.000e-03
0: TRAIN [1][740/3880]	Time 0.162 (0.182)	Data 9.23e-05 (9.90e-04)	Tok/s 31723 (38782)	Loss/tok 3.1945 (3.3988)	LR 2.000e-03
0: TRAIN [1][750/3880]	Time 0.162 (0.182)	Data 7.49e-05 (9.78e-04)	Tok/s 32536 (38726)	Loss/tok 3.2806 (3.3976)	LR 2.000e-03
0: TRAIN [1][760/3880]	Time 0.256 (0.182)	Data 8.37e-05 (9.67e-04)	Tok/s 58374 (38767)	Loss/tok 3.5838 (3.3990)	LR 2.000e-03
0: TRAIN [1][770/3880]	Time 0.221 (0.182)	Data 8.46e-05 (9.55e-04)	Tok/s 52906 (38745)	Loss/tok 3.5222 (3.3981)	LR 2.000e-03
0: TRAIN [1][780/3880]	Time 0.137 (0.182)	Data 1.35e-04 (9.45e-04)	Tok/s 18821 (38677)	Loss/tok 2.7267 (3.3975)	LR 2.000e-03
0: TRAIN [1][790/3880]	Time 0.137 (0.182)	Data 9.54e-05 (9.35e-04)	Tok/s 19089 (38594)	Loss/tok 2.6471 (3.3968)	LR 2.000e-03
0: TRAIN [1][800/3880]	Time 0.162 (0.182)	Data 8.18e-05 (9.24e-04)	Tok/s 31240 (38572)	Loss/tok 3.1485 (3.3960)	LR 2.000e-03
0: TRAIN [1][810/3880]	Time 0.191 (0.182)	Data 8.70e-05 (9.14e-04)	Tok/s 43758 (38602)	Loss/tok 3.5645 (3.3965)	LR 2.000e-03
0: TRAIN [1][820/3880]	Time 0.191 (0.182)	Data 8.82e-05 (9.04e-04)	Tok/s 44734 (38618)	Loss/tok 3.3690 (3.3968)	LR 2.000e-03
0: TRAIN [1][830/3880]	Time 0.163 (0.182)	Data 9.32e-05 (8.95e-04)	Tok/s 32567 (38630)	Loss/tok 3.0624 (3.3968)	LR 2.000e-03
0: TRAIN [1][840/3880]	Time 0.221 (0.182)	Data 3.83e-04 (8.85e-04)	Tok/s 53625 (38638)	Loss/tok 3.4315 (3.3967)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][850/3880]	Time 0.187 (0.182)	Data 9.82e-05 (8.76e-04)	Tok/s 45973 (38653)	Loss/tok 3.2419 (3.3965)	LR 2.000e-03
0: TRAIN [1][860/3880]	Time 0.137 (0.182)	Data 1.28e-04 (8.67e-04)	Tok/s 19364 (38624)	Loss/tok 2.5911 (3.3969)	LR 2.000e-03
0: TRAIN [1][870/3880]	Time 0.162 (0.182)	Data 1.76e-04 (8.59e-04)	Tok/s 31772 (38666)	Loss/tok 3.0570 (3.3978)	LR 2.000e-03
0: TRAIN [1][880/3880]	Time 0.191 (0.182)	Data 1.17e-04 (8.51e-04)	Tok/s 43965 (38706)	Loss/tok 3.3706 (3.3985)	LR 2.000e-03
0: TRAIN [1][890/3880]	Time 0.162 (0.182)	Data 8.61e-05 (8.42e-04)	Tok/s 32318 (38737)	Loss/tok 3.2605 (3.3987)	LR 2.000e-03
0: TRAIN [1][900/3880]	Time 0.163 (0.182)	Data 1.24e-04 (8.34e-04)	Tok/s 31307 (38727)	Loss/tok 3.1908 (3.3978)	LR 2.000e-03
0: TRAIN [1][910/3880]	Time 0.162 (0.182)	Data 8.70e-05 (8.26e-04)	Tok/s 31637 (38725)	Loss/tok 3.0236 (3.3973)	LR 2.000e-03
0: TRAIN [1][920/3880]	Time 0.191 (0.182)	Data 1.01e-04 (8.19e-04)	Tok/s 42970 (38703)	Loss/tok 3.4786 (3.3973)	LR 2.000e-03
0: TRAIN [1][930/3880]	Time 0.162 (0.182)	Data 7.80e-05 (8.11e-04)	Tok/s 32507 (38711)	Loss/tok 3.2296 (3.3980)	LR 2.000e-03
0: TRAIN [1][940/3880]	Time 0.191 (0.182)	Data 7.92e-05 (8.03e-04)	Tok/s 44138 (38673)	Loss/tok 3.6109 (3.3976)	LR 2.000e-03
0: TRAIN [1][950/3880]	Time 0.162 (0.182)	Data 8.46e-05 (7.96e-04)	Tok/s 33009 (38589)	Loss/tok 2.8865 (3.3957)	LR 2.000e-03
0: TRAIN [1][960/3880]	Time 0.162 (0.182)	Data 1.12e-04 (7.89e-04)	Tok/s 32315 (38602)	Loss/tok 3.2300 (3.3957)	LR 2.000e-03
0: TRAIN [1][970/3880]	Time 0.191 (0.182)	Data 7.84e-05 (7.82e-04)	Tok/s 44147 (38632)	Loss/tok 3.3781 (3.3947)	LR 2.000e-03
0: TRAIN [1][980/3880]	Time 0.162 (0.182)	Data 9.51e-05 (7.74e-04)	Tok/s 31308 (38595)	Loss/tok 3.1741 (3.3940)	LR 2.000e-03
0: TRAIN [1][990/3880]	Time 0.191 (0.182)	Data 9.25e-05 (7.68e-04)	Tok/s 44359 (38589)	Loss/tok 3.2781 (3.3938)	LR 2.000e-03
0: TRAIN [1][1000/3880]	Time 0.191 (0.182)	Data 1.17e-04 (7.61e-04)	Tok/s 43804 (38584)	Loss/tok 3.4132 (3.3935)	LR 2.000e-03
0: TRAIN [1][1010/3880]	Time 0.162 (0.181)	Data 1.03e-04 (7.55e-04)	Tok/s 32027 (38515)	Loss/tok 3.2268 (3.3920)	LR 2.000e-03
0: TRAIN [1][1020/3880]	Time 0.191 (0.182)	Data 1.33e-04 (7.49e-04)	Tok/s 44144 (38560)	Loss/tok 3.4153 (3.3925)	LR 2.000e-03
0: TRAIN [1][1030/3880]	Time 0.162 (0.182)	Data 7.87e-05 (7.43e-04)	Tok/s 31923 (38579)	Loss/tok 3.1222 (3.3936)	LR 2.000e-03
0: TRAIN [1][1040/3880]	Time 0.137 (0.182)	Data 5.43e-04 (7.37e-04)	Tok/s 19283 (38615)	Loss/tok 2.8879 (3.3941)	LR 2.000e-03
0: TRAIN [1][1050/3880]	Time 0.162 (0.182)	Data 9.11e-05 (7.31e-04)	Tok/s 30815 (38628)	Loss/tok 3.2977 (3.3945)	LR 2.000e-03
0: TRAIN [1][1060/3880]	Time 0.162 (0.182)	Data 1.28e-04 (7.25e-04)	Tok/s 30833 (38643)	Loss/tok 3.1401 (3.3949)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1070/3880]	Time 0.159 (0.182)	Data 1.15e-04 (7.19e-04)	Tok/s 32089 (38652)	Loss/tok 3.2717 (3.3955)	LR 2.000e-03
0: TRAIN [1][1080/3880]	Time 0.137 (0.182)	Data 1.56e-04 (7.14e-04)	Tok/s 18607 (38652)	Loss/tok 2.5580 (3.3955)	LR 2.000e-03
0: TRAIN [1][1090/3880]	Time 0.162 (0.182)	Data 1.29e-04 (7.08e-04)	Tok/s 30996 (38645)	Loss/tok 3.1181 (3.3953)	LR 2.000e-03
0: TRAIN [1][1100/3880]	Time 0.256 (0.182)	Data 1.32e-04 (7.03e-04)	Tok/s 58434 (38616)	Loss/tok 3.6579 (3.3948)	LR 2.000e-03
0: TRAIN [1][1110/3880]	Time 0.190 (0.182)	Data 1.25e-04 (6.98e-04)	Tok/s 44999 (38606)	Loss/tok 3.4424 (3.3944)	LR 2.000e-03
0: TRAIN [1][1120/3880]	Time 0.162 (0.182)	Data 3.06e-04 (6.93e-04)	Tok/s 31685 (38622)	Loss/tok 3.0721 (3.3948)	LR 2.000e-03
0: TRAIN [1][1130/3880]	Time 0.192 (0.182)	Data 1.55e-04 (6.89e-04)	Tok/s 43494 (38619)	Loss/tok 3.3542 (3.3942)	LR 2.000e-03
0: TRAIN [1][1140/3880]	Time 0.162 (0.182)	Data 1.36e-04 (6.84e-04)	Tok/s 32200 (38614)	Loss/tok 3.1006 (3.3934)	LR 2.000e-03
0: TRAIN [1][1150/3880]	Time 0.191 (0.182)	Data 9.13e-05 (6.79e-04)	Tok/s 44361 (38607)	Loss/tok 3.3627 (3.3929)	LR 2.000e-03
0: TRAIN [1][1160/3880]	Time 0.191 (0.182)	Data 7.39e-05 (6.74e-04)	Tok/s 43536 (38665)	Loss/tok 3.4863 (3.3948)	LR 2.000e-03
0: TRAIN [1][1170/3880]	Time 0.191 (0.182)	Data 1.40e-04 (6.69e-04)	Tok/s 43467 (38627)	Loss/tok 3.3587 (3.3945)	LR 2.000e-03
0: TRAIN [1][1180/3880]	Time 0.137 (0.182)	Data 9.47e-05 (6.65e-04)	Tok/s 18727 (38620)	Loss/tok 2.6923 (3.3941)	LR 2.000e-03
0: TRAIN [1][1190/3880]	Time 0.162 (0.182)	Data 1.58e-04 (6.60e-04)	Tok/s 31890 (38678)	Loss/tok 3.1489 (3.3943)	LR 2.000e-03
0: TRAIN [1][1200/3880]	Time 0.191 (0.182)	Data 1.48e-04 (6.56e-04)	Tok/s 43362 (38658)	Loss/tok 3.5134 (3.3939)	LR 2.000e-03
0: TRAIN [1][1210/3880]	Time 0.256 (0.182)	Data 1.47e-04 (6.51e-04)	Tok/s 58918 (38661)	Loss/tok 3.6682 (3.3936)	LR 2.000e-03
0: TRAIN [1][1220/3880]	Time 0.138 (0.182)	Data 1.07e-04 (6.47e-04)	Tok/s 18930 (38643)	Loss/tok 2.6729 (3.3939)	LR 2.000e-03
0: TRAIN [1][1230/3880]	Time 0.162 (0.182)	Data 8.01e-05 (6.43e-04)	Tok/s 31734 (38683)	Loss/tok 3.2388 (3.3947)	LR 2.000e-03
0: TRAIN [1][1240/3880]	Time 0.136 (0.182)	Data 8.37e-05 (6.38e-04)	Tok/s 19226 (38671)	Loss/tok 2.7573 (3.3946)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1250/3880]	Time 0.164 (0.182)	Data 8.99e-05 (6.34e-04)	Tok/s 31498 (38651)	Loss/tok 2.9394 (3.3942)	LR 2.000e-03
0: TRAIN [1][1260/3880]	Time 0.162 (0.182)	Data 9.06e-05 (6.30e-04)	Tok/s 32090 (38619)	Loss/tok 3.1894 (3.3938)	LR 2.000e-03
0: TRAIN [1][1270/3880]	Time 0.221 (0.182)	Data 7.87e-05 (6.26e-04)	Tok/s 53131 (38618)	Loss/tok 3.6378 (3.3938)	LR 2.000e-03
0: TRAIN [1][1280/3880]	Time 0.163 (0.182)	Data 1.29e-04 (6.22e-04)	Tok/s 31373 (38603)	Loss/tok 3.2786 (3.3934)	LR 2.000e-03
0: TRAIN [1][1290/3880]	Time 0.162 (0.182)	Data 1.09e-04 (6.18e-04)	Tok/s 32518 (38562)	Loss/tok 3.2381 (3.3925)	LR 2.000e-03
0: TRAIN [1][1300/3880]	Time 0.191 (0.182)	Data 1.03e-04 (6.14e-04)	Tok/s 45099 (38554)	Loss/tok 3.3771 (3.3922)	LR 2.000e-03
0: TRAIN [1][1310/3880]	Time 0.221 (0.182)	Data 2.01e-04 (6.10e-04)	Tok/s 53437 (38556)	Loss/tok 3.3720 (3.3913)	LR 2.000e-03
0: TRAIN [1][1320/3880]	Time 0.221 (0.182)	Data 7.70e-05 (6.07e-04)	Tok/s 52186 (38568)	Loss/tok 3.7868 (3.3921)	LR 2.000e-03
0: TRAIN [1][1330/3880]	Time 0.162 (0.181)	Data 1.21e-04 (6.03e-04)	Tok/s 31386 (38544)	Loss/tok 3.1857 (3.3909)	LR 2.000e-03
0: TRAIN [1][1340/3880]	Time 0.162 (0.181)	Data 8.61e-05 (6.00e-04)	Tok/s 32506 (38514)	Loss/tok 3.2208 (3.3901)	LR 2.000e-03
0: TRAIN [1][1350/3880]	Time 0.256 (0.181)	Data 1.22e-04 (5.96e-04)	Tok/s 57759 (38509)	Loss/tok 3.8639 (3.3901)	LR 2.000e-03
0: TRAIN [1][1360/3880]	Time 0.220 (0.182)	Data 2.19e-04 (5.93e-04)	Tok/s 52799 (38560)	Loss/tok 3.6067 (3.3916)	LR 2.000e-03
0: TRAIN [1][1370/3880]	Time 0.221 (0.182)	Data 9.54e-05 (5.90e-04)	Tok/s 53065 (38573)	Loss/tok 3.5880 (3.3918)	LR 2.000e-03
0: TRAIN [1][1380/3880]	Time 0.191 (0.181)	Data 1.29e-04 (5.86e-04)	Tok/s 45465 (38525)	Loss/tok 3.4412 (3.3908)	LR 2.000e-03
0: TRAIN [1][1390/3880]	Time 0.192 (0.181)	Data 9.13e-05 (5.83e-04)	Tok/s 44676 (38538)	Loss/tok 3.2734 (3.3902)	LR 2.000e-03
0: TRAIN [1][1400/3880]	Time 0.162 (0.181)	Data 1.44e-04 (5.80e-04)	Tok/s 32052 (38571)	Loss/tok 3.2398 (3.3905)	LR 2.000e-03
0: TRAIN [1][1410/3880]	Time 0.221 (0.182)	Data 1.06e-04 (5.76e-04)	Tok/s 51958 (38604)	Loss/tok 3.7763 (3.3911)	LR 2.000e-03
0: TRAIN [1][1420/3880]	Time 0.256 (0.182)	Data 1.17e-04 (5.73e-04)	Tok/s 57808 (38609)	Loss/tok 3.6561 (3.3915)	LR 2.000e-03
0: TRAIN [1][1430/3880]	Time 0.162 (0.182)	Data 9.16e-05 (5.70e-04)	Tok/s 32240 (38588)	Loss/tok 3.0405 (3.3912)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1440/3880]	Time 0.162 (0.182)	Data 7.56e-05 (5.67e-04)	Tok/s 32519 (38593)	Loss/tok 3.1645 (3.3920)	LR 2.000e-03
0: TRAIN [1][1450/3880]	Time 0.162 (0.182)	Data 9.70e-05 (5.64e-04)	Tok/s 31502 (38596)	Loss/tok 3.0687 (3.3919)	LR 2.000e-03
0: TRAIN [1][1460/3880]	Time 0.221 (0.182)	Data 7.84e-05 (5.61e-04)	Tok/s 51468 (38576)	Loss/tok 3.5352 (3.3917)	LR 2.000e-03
0: TRAIN [1][1470/3880]	Time 0.162 (0.182)	Data 9.30e-05 (5.57e-04)	Tok/s 30922 (38603)	Loss/tok 3.1647 (3.3928)	LR 2.000e-03
0: TRAIN [1][1480/3880]	Time 0.191 (0.182)	Data 7.65e-05 (5.54e-04)	Tok/s 43963 (38616)	Loss/tok 3.3834 (3.3927)	LR 2.000e-03
0: TRAIN [1][1490/3880]	Time 0.221 (0.182)	Data 7.84e-05 (5.51e-04)	Tok/s 52711 (38635)	Loss/tok 3.7077 (3.3929)	LR 2.000e-03
0: TRAIN [1][1500/3880]	Time 0.221 (0.182)	Data 9.92e-05 (5.48e-04)	Tok/s 53410 (38640)	Loss/tok 3.5183 (3.3927)	LR 2.000e-03
0: TRAIN [1][1510/3880]	Time 0.162 (0.182)	Data 9.39e-05 (5.45e-04)	Tok/s 32172 (38655)	Loss/tok 3.0379 (3.3930)	LR 2.000e-03
0: TRAIN [1][1520/3880]	Time 0.162 (0.182)	Data 9.27e-05 (5.42e-04)	Tok/s 31576 (38649)	Loss/tok 3.4088 (3.3932)	LR 2.000e-03
0: TRAIN [1][1530/3880]	Time 0.162 (0.182)	Data 8.75e-05 (5.39e-04)	Tok/s 31539 (38616)	Loss/tok 2.9481 (3.3922)	LR 2.000e-03
0: TRAIN [1][1540/3880]	Time 0.221 (0.182)	Data 9.08e-05 (5.36e-04)	Tok/s 53572 (38636)	Loss/tok 3.6393 (3.3926)	LR 2.000e-03
0: TRAIN [1][1550/3880]	Time 0.162 (0.182)	Data 8.89e-05 (5.34e-04)	Tok/s 31411 (38627)	Loss/tok 3.0671 (3.3921)	LR 2.000e-03
0: TRAIN [1][1560/3880]	Time 0.138 (0.182)	Data 9.23e-05 (5.31e-04)	Tok/s 18525 (38616)	Loss/tok 2.6381 (3.3921)	LR 2.000e-03
0: TRAIN [1][1570/3880]	Time 0.162 (0.182)	Data 1.20e-04 (5.28e-04)	Tok/s 32516 (38624)	Loss/tok 3.3342 (3.3919)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1580/3880]	Time 0.256 (0.182)	Data 8.08e-05 (5.26e-04)	Tok/s 57284 (38611)	Loss/tok 3.8664 (3.3919)	LR 2.000e-03
0: TRAIN [1][1590/3880]	Time 0.220 (0.182)	Data 9.11e-05 (5.23e-04)	Tok/s 53419 (38620)	Loss/tok 3.4528 (3.3913)	LR 2.000e-03
0: TRAIN [1][1600/3880]	Time 0.162 (0.182)	Data 7.61e-05 (5.20e-04)	Tok/s 31515 (38629)	Loss/tok 3.1390 (3.3913)	LR 2.000e-03
0: TRAIN [1][1610/3880]	Time 0.191 (0.182)	Data 7.56e-05 (5.17e-04)	Tok/s 44085 (38627)	Loss/tok 3.3895 (3.3912)	LR 2.000e-03
0: TRAIN [1][1620/3880]	Time 0.192 (0.182)	Data 1.20e-04 (5.15e-04)	Tok/s 43284 (38650)	Loss/tok 3.4215 (3.3919)	LR 2.000e-03
0: TRAIN [1][1630/3880]	Time 0.221 (0.182)	Data 8.65e-05 (5.12e-04)	Tok/s 54005 (38695)	Loss/tok 3.5263 (3.3930)	LR 2.000e-03
0: TRAIN [1][1640/3880]	Time 0.162 (0.182)	Data 4.19e-04 (5.10e-04)	Tok/s 31068 (38730)	Loss/tok 3.1465 (3.3936)	LR 2.000e-03
0: TRAIN [1][1650/3880]	Time 0.221 (0.182)	Data 1.10e-04 (5.08e-04)	Tok/s 53198 (38735)	Loss/tok 3.3738 (3.3932)	LR 2.000e-03
0: TRAIN [1][1660/3880]	Time 0.162 (0.182)	Data 1.61e-04 (5.05e-04)	Tok/s 32327 (38729)	Loss/tok 3.1966 (3.3927)	LR 2.000e-03
0: TRAIN [1][1670/3880]	Time 0.162 (0.182)	Data 9.51e-05 (5.03e-04)	Tok/s 31551 (38722)	Loss/tok 3.0689 (3.3924)	LR 2.000e-03
0: TRAIN [1][1680/3880]	Time 0.221 (0.182)	Data 7.94e-05 (5.01e-04)	Tok/s 52016 (38721)	Loss/tok 3.5660 (3.3919)	LR 2.000e-03
0: TRAIN [1][1690/3880]	Time 0.221 (0.182)	Data 9.04e-05 (4.98e-04)	Tok/s 52709 (38709)	Loss/tok 3.4881 (3.3912)	LR 2.000e-03
0: TRAIN [1][1700/3880]	Time 0.221 (0.182)	Data 8.44e-05 (4.96e-04)	Tok/s 52753 (38717)	Loss/tok 3.6237 (3.3914)	LR 2.000e-03
0: TRAIN [1][1710/3880]	Time 0.162 (0.182)	Data 1.40e-04 (4.94e-04)	Tok/s 32788 (38698)	Loss/tok 3.1365 (3.3909)	LR 2.000e-03
0: TRAIN [1][1720/3880]	Time 0.136 (0.182)	Data 9.27e-05 (4.92e-04)	Tok/s 19492 (38683)	Loss/tok 2.6419 (3.3905)	LR 2.000e-03
0: TRAIN [1][1730/3880]	Time 0.191 (0.182)	Data 7.68e-05 (4.89e-04)	Tok/s 44816 (38693)	Loss/tok 3.3833 (3.3906)	LR 2.000e-03
0: TRAIN [1][1740/3880]	Time 0.191 (0.182)	Data 9.63e-05 (4.87e-04)	Tok/s 43806 (38706)	Loss/tok 3.5332 (3.3906)	LR 2.000e-03
0: TRAIN [1][1750/3880]	Time 0.162 (0.182)	Data 9.08e-05 (4.85e-04)	Tok/s 31979 (38680)	Loss/tok 3.1264 (3.3896)	LR 2.000e-03
0: TRAIN [1][1760/3880]	Time 0.221 (0.182)	Data 8.63e-05 (4.83e-04)	Tok/s 52128 (38687)	Loss/tok 3.5559 (3.3891)	LR 2.000e-03
0: TRAIN [1][1770/3880]	Time 0.162 (0.182)	Data 1.31e-04 (4.81e-04)	Tok/s 31547 (38706)	Loss/tok 3.0588 (3.3897)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1780/3880]	Time 0.137 (0.182)	Data 3.68e-04 (4.79e-04)	Tok/s 19608 (38704)	Loss/tok 2.7101 (3.3894)	LR 2.000e-03
0: TRAIN [1][1790/3880]	Time 0.162 (0.182)	Data 9.85e-05 (4.77e-04)	Tok/s 31309 (38705)	Loss/tok 3.0361 (3.3891)	LR 2.000e-03
0: TRAIN [1][1800/3880]	Time 0.162 (0.182)	Data 9.80e-05 (4.75e-04)	Tok/s 32466 (38719)	Loss/tok 3.1556 (3.3896)	LR 2.000e-03
0: TRAIN [1][1810/3880]	Time 0.162 (0.182)	Data 1.10e-04 (4.73e-04)	Tok/s 31738 (38711)	Loss/tok 3.1151 (3.3892)	LR 2.000e-03
0: TRAIN [1][1820/3880]	Time 0.162 (0.182)	Data 8.13e-05 (4.70e-04)	Tok/s 31906 (38715)	Loss/tok 3.0883 (3.3888)	LR 2.000e-03
0: TRAIN [1][1830/3880]	Time 0.256 (0.182)	Data 7.72e-05 (4.68e-04)	Tok/s 58169 (38741)	Loss/tok 3.6826 (3.3893)	LR 2.000e-03
0: TRAIN [1][1840/3880]	Time 0.191 (0.182)	Data 1.12e-04 (4.67e-04)	Tok/s 43321 (38754)	Loss/tok 3.5695 (3.3897)	LR 2.000e-03
0: TRAIN [1][1850/3880]	Time 0.162 (0.182)	Data 1.15e-04 (4.65e-04)	Tok/s 31116 (38709)	Loss/tok 3.0676 (3.3889)	LR 2.000e-03
0: TRAIN [1][1860/3880]	Time 0.191 (0.182)	Data 1.24e-04 (4.63e-04)	Tok/s 43301 (38732)	Loss/tok 3.6935 (3.3892)	LR 2.000e-03
0: TRAIN [1][1870/3880]	Time 0.162 (0.182)	Data 1.20e-04 (4.61e-04)	Tok/s 32496 (38726)	Loss/tok 3.1843 (3.3887)	LR 2.000e-03
0: TRAIN [1][1880/3880]	Time 0.137 (0.182)	Data 1.10e-04 (4.60e-04)	Tok/s 19851 (38734)	Loss/tok 2.6773 (3.3889)	LR 2.000e-03
0: TRAIN [1][1890/3880]	Time 0.136 (0.182)	Data 1.25e-04 (4.58e-04)	Tok/s 19218 (38734)	Loss/tok 2.6653 (3.3885)	LR 2.000e-03
0: TRAIN [1][1900/3880]	Time 0.256 (0.182)	Data 1.11e-04 (4.56e-04)	Tok/s 57704 (38749)	Loss/tok 3.6705 (3.3889)	LR 2.000e-03
0: TRAIN [1][1910/3880]	Time 0.162 (0.182)	Data 7.39e-05 (4.54e-04)	Tok/s 32471 (38720)	Loss/tok 3.0984 (3.3883)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1920/3880]	Time 0.191 (0.182)	Data 9.04e-05 (4.52e-04)	Tok/s 45241 (38725)	Loss/tok 3.4086 (3.3884)	LR 2.000e-03
0: TRAIN [1][1930/3880]	Time 0.162 (0.182)	Data 7.77e-05 (4.50e-04)	Tok/s 32263 (38706)	Loss/tok 3.0805 (3.3880)	LR 2.000e-03
0: TRAIN [1][1940/3880]	Time 0.191 (0.182)	Data 5.54e-04 (4.49e-04)	Tok/s 43777 (38717)	Loss/tok 3.4497 (3.3882)	LR 2.000e-03
0: TRAIN [1][1950/3880]	Time 0.136 (0.182)	Data 1.54e-04 (4.47e-04)	Tok/s 19092 (38692)	Loss/tok 2.4803 (3.3880)	LR 2.000e-03
0: TRAIN [1][1960/3880]	Time 0.162 (0.182)	Data 9.04e-05 (4.45e-04)	Tok/s 31400 (38666)	Loss/tok 3.2076 (3.3877)	LR 2.000e-03
0: TRAIN [1][1970/3880]	Time 0.162 (0.182)	Data 8.65e-05 (4.43e-04)	Tok/s 32667 (38655)	Loss/tok 3.1897 (3.3873)	LR 2.000e-03
0: TRAIN [1][1980/3880]	Time 0.191 (0.182)	Data 1.44e-04 (4.42e-04)	Tok/s 44284 (38626)	Loss/tok 3.4831 (3.3869)	LR 2.000e-03
0: TRAIN [1][1990/3880]	Time 0.191 (0.182)	Data 9.54e-05 (4.40e-04)	Tok/s 44107 (38653)	Loss/tok 3.3264 (3.3870)	LR 2.000e-03
0: TRAIN [1][2000/3880]	Time 0.192 (0.182)	Data 9.42e-05 (4.38e-04)	Tok/s 43478 (38642)	Loss/tok 3.3865 (3.3866)	LR 2.000e-03
0: TRAIN [1][2010/3880]	Time 0.191 (0.182)	Data 1.33e-04 (4.37e-04)	Tok/s 43765 (38645)	Loss/tok 3.3791 (3.3870)	LR 2.000e-03
0: TRAIN [1][2020/3880]	Time 0.191 (0.182)	Data 2.27e-04 (4.35e-04)	Tok/s 44788 (38643)	Loss/tok 3.3452 (3.3864)	LR 2.000e-03
0: TRAIN [1][2030/3880]	Time 0.162 (0.182)	Data 1.03e-04 (4.34e-04)	Tok/s 31763 (38673)	Loss/tok 3.2739 (3.3873)	LR 2.000e-03
0: TRAIN [1][2040/3880]	Time 0.162 (0.182)	Data 7.25e-05 (4.32e-04)	Tok/s 32383 (38644)	Loss/tok 3.1449 (3.3865)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][2050/3880]	Time 0.162 (0.182)	Data 9.35e-05 (4.30e-04)	Tok/s 31472 (38631)	Loss/tok 3.0827 (3.3862)	LR 2.000e-03
0: TRAIN [1][2060/3880]	Time 0.161 (0.182)	Data 1.30e-04 (4.29e-04)	Tok/s 30724 (38634)	Loss/tok 3.2468 (3.3867)	LR 2.000e-03
0: TRAIN [1][2070/3880]	Time 0.191 (0.182)	Data 1.34e-04 (4.27e-04)	Tok/s 43814 (38656)	Loss/tok 3.4377 (3.3871)	LR 2.000e-03
0: TRAIN [1][2080/3880]	Time 0.136 (0.182)	Data 9.25e-05 (4.26e-04)	Tok/s 19093 (38644)	Loss/tok 2.6864 (3.3865)	LR 2.000e-03
0: TRAIN [1][2090/3880]	Time 0.136 (0.182)	Data 9.23e-05 (4.25e-04)	Tok/s 19439 (38641)	Loss/tok 2.8149 (3.3863)	LR 2.000e-03
0: TRAIN [1][2100/3880]	Time 0.137 (0.182)	Data 1.95e-04 (4.23e-04)	Tok/s 18942 (38674)	Loss/tok 2.6117 (3.3874)	LR 2.000e-03
0: TRAIN [1][2110/3880]	Time 0.221 (0.182)	Data 1.42e-04 (4.22e-04)	Tok/s 53531 (38689)	Loss/tok 3.4125 (3.3876)	LR 2.000e-03
0: TRAIN [1][2120/3880]	Time 0.136 (0.182)	Data 1.15e-04 (4.20e-04)	Tok/s 19610 (38669)	Loss/tok 2.7885 (3.3871)	LR 2.000e-03
0: TRAIN [1][2130/3880]	Time 0.162 (0.182)	Data 9.42e-05 (4.19e-04)	Tok/s 31575 (38657)	Loss/tok 3.1389 (3.3865)	LR 2.000e-03
0: TRAIN [1][2140/3880]	Time 0.191 (0.182)	Data 8.87e-05 (4.18e-04)	Tok/s 43263 (38640)	Loss/tok 3.4639 (3.3859)	LR 2.000e-03
0: TRAIN [1][2150/3880]	Time 0.163 (0.182)	Data 1.55e-04 (4.16e-04)	Tok/s 32324 (38651)	Loss/tok 3.1427 (3.3859)	LR 2.000e-03
0: TRAIN [1][2160/3880]	Time 0.221 (0.182)	Data 9.23e-05 (4.15e-04)	Tok/s 52565 (38658)	Loss/tok 3.6578 (3.3857)	LR 2.000e-03
0: TRAIN [1][2170/3880]	Time 0.162 (0.182)	Data 8.23e-05 (4.13e-04)	Tok/s 31993 (38668)	Loss/tok 3.0436 (3.3860)	LR 2.000e-03
0: TRAIN [1][2180/3880]	Time 0.190 (0.182)	Data 9.97e-05 (4.12e-04)	Tok/s 43928 (38663)	Loss/tok 3.2400 (3.3858)	LR 2.000e-03
0: TRAIN [1][2190/3880]	Time 0.163 (0.182)	Data 1.23e-04 (4.11e-04)	Tok/s 31749 (38666)	Loss/tok 3.2010 (3.3859)	LR 2.000e-03
0: TRAIN [1][2200/3880]	Time 0.191 (0.182)	Data 8.06e-05 (4.09e-04)	Tok/s 45286 (38668)	Loss/tok 3.3315 (3.3857)	LR 2.000e-03
0: TRAIN [1][2210/3880]	Time 0.162 (0.182)	Data 1.26e-04 (4.08e-04)	Tok/s 32986 (38653)	Loss/tok 3.0487 (3.3852)	LR 2.000e-03
0: TRAIN [1][2220/3880]	Time 0.162 (0.182)	Data 1.29e-04 (4.07e-04)	Tok/s 31682 (38648)	Loss/tok 2.9206 (3.3846)	LR 2.000e-03
0: TRAIN [1][2230/3880]	Time 0.162 (0.182)	Data 2.91e-04 (4.06e-04)	Tok/s 31763 (38657)	Loss/tok 3.1267 (3.3851)	LR 2.000e-03
0: TRAIN [1][2240/3880]	Time 0.257 (0.182)	Data 9.51e-05 (4.05e-04)	Tok/s 58056 (38675)	Loss/tok 3.7748 (3.3857)	LR 2.000e-03
0: TRAIN [1][2250/3880]	Time 0.192 (0.182)	Data 9.51e-05 (4.03e-04)	Tok/s 43692 (38675)	Loss/tok 3.3887 (3.3855)	LR 2.000e-03
0: TRAIN [1][2260/3880]	Time 0.220 (0.182)	Data 1.07e-04 (4.02e-04)	Tok/s 52493 (38695)	Loss/tok 3.4507 (3.3855)	LR 2.000e-03
0: TRAIN [1][2270/3880]	Time 0.163 (0.182)	Data 1.47e-04 (4.01e-04)	Tok/s 31589 (38678)	Loss/tok 3.1359 (3.3850)	LR 2.000e-03
0: TRAIN [1][2280/3880]	Time 0.220 (0.182)	Data 1.05e-04 (4.00e-04)	Tok/s 52465 (38667)	Loss/tok 3.5330 (3.3846)	LR 2.000e-03
0: TRAIN [1][2290/3880]	Time 0.221 (0.182)	Data 8.89e-05 (3.98e-04)	Tok/s 52450 (38663)	Loss/tok 3.5403 (3.3846)	LR 2.000e-03
0: TRAIN [1][2300/3880]	Time 0.136 (0.182)	Data 1.24e-04 (3.97e-04)	Tok/s 19863 (38678)	Loss/tok 2.6370 (3.3850)	LR 2.000e-03
0: TRAIN [1][2310/3880]	Time 0.191 (0.182)	Data 8.44e-05 (3.96e-04)	Tok/s 43883 (38657)	Loss/tok 3.2563 (3.3844)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2320/3880]	Time 0.258 (0.182)	Data 1.11e-04 (3.95e-04)	Tok/s 57821 (38671)	Loss/tok 3.6816 (3.3846)	LR 2.000e-03
0: TRAIN [1][2330/3880]	Time 0.191 (0.182)	Data 1.01e-04 (3.93e-04)	Tok/s 44488 (38652)	Loss/tok 3.3788 (3.3842)	LR 2.000e-03
0: TRAIN [1][2340/3880]	Time 0.162 (0.182)	Data 8.96e-05 (3.92e-04)	Tok/s 32225 (38660)	Loss/tok 3.2586 (3.3842)	LR 2.000e-03
0: TRAIN [1][2350/3880]	Time 0.191 (0.182)	Data 9.23e-05 (3.91e-04)	Tok/s 43652 (38640)	Loss/tok 3.3453 (3.3837)	LR 2.000e-03
0: TRAIN [1][2360/3880]	Time 0.162 (0.182)	Data 1.31e-04 (3.90e-04)	Tok/s 31995 (38617)	Loss/tok 3.0501 (3.3833)	LR 2.000e-03
0: TRAIN [1][2370/3880]	Time 0.161 (0.182)	Data 1.10e-04 (3.88e-04)	Tok/s 31855 (38618)	Loss/tok 3.1332 (3.3831)	LR 2.000e-03
0: TRAIN [1][2380/3880]	Time 0.191 (0.182)	Data 9.68e-05 (3.87e-04)	Tok/s 44193 (38610)	Loss/tok 3.2784 (3.3825)	LR 2.000e-03
0: TRAIN [1][2390/3880]	Time 0.191 (0.182)	Data 7.39e-05 (3.86e-04)	Tok/s 43795 (38614)	Loss/tok 3.3835 (3.3823)	LR 2.000e-03
0: TRAIN [1][2400/3880]	Time 0.162 (0.182)	Data 8.70e-05 (3.85e-04)	Tok/s 31552 (38615)	Loss/tok 3.3024 (3.3822)	LR 2.000e-03
0: TRAIN [1][2410/3880]	Time 0.162 (0.182)	Data 7.87e-05 (3.83e-04)	Tok/s 31911 (38604)	Loss/tok 3.1886 (3.3816)	LR 2.000e-03
0: TRAIN [1][2420/3880]	Time 0.191 (0.182)	Data 9.23e-05 (3.82e-04)	Tok/s 44098 (38620)	Loss/tok 3.3235 (3.3815)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][2430/3880]	Time 0.256 (0.182)	Data 7.51e-05 (3.81e-04)	Tok/s 57680 (38642)	Loss/tok 3.8455 (3.3820)	LR 2.000e-03
0: TRAIN [1][2440/3880]	Time 0.191 (0.182)	Data 7.75e-05 (3.80e-04)	Tok/s 44136 (38645)	Loss/tok 3.2350 (3.3819)	LR 2.000e-03
0: TRAIN [1][2450/3880]	Time 0.162 (0.182)	Data 9.42e-05 (3.79e-04)	Tok/s 31511 (38638)	Loss/tok 3.1567 (3.3820)	LR 2.000e-03
0: TRAIN [1][2460/3880]	Time 0.162 (0.182)	Data 7.63e-05 (3.77e-04)	Tok/s 31934 (38632)	Loss/tok 3.1404 (3.3820)	LR 2.000e-03
0: TRAIN [1][2470/3880]	Time 0.162 (0.182)	Data 1.03e-04 (3.77e-04)	Tok/s 32161 (38639)	Loss/tok 3.2215 (3.3819)	LR 2.000e-03
0: TRAIN [1][2480/3880]	Time 0.162 (0.182)	Data 7.82e-05 (3.76e-04)	Tok/s 33245 (38623)	Loss/tok 3.2059 (3.3817)	LR 2.000e-03
0: TRAIN [1][2490/3880]	Time 0.221 (0.182)	Data 7.75e-05 (3.74e-04)	Tok/s 53516 (38637)	Loss/tok 3.3473 (3.3813)	LR 2.000e-03
0: TRAIN [1][2500/3880]	Time 0.221 (0.182)	Data 7.65e-05 (3.73e-04)	Tok/s 52701 (38628)	Loss/tok 3.4843 (3.3810)	LR 2.000e-03
0: TRAIN [1][2510/3880]	Time 0.221 (0.182)	Data 1.01e-04 (3.72e-04)	Tok/s 51927 (38613)	Loss/tok 3.6803 (3.3807)	LR 2.000e-03
0: TRAIN [1][2520/3880]	Time 0.221 (0.182)	Data 7.63e-05 (3.71e-04)	Tok/s 52932 (38598)	Loss/tok 3.4653 (3.3803)	LR 2.000e-03
0: TRAIN [1][2530/3880]	Time 0.162 (0.182)	Data 9.01e-05 (3.70e-04)	Tok/s 31829 (38603)	Loss/tok 3.0810 (3.3803)	LR 2.000e-03
0: TRAIN [1][2540/3880]	Time 0.162 (0.182)	Data 8.96e-05 (3.69e-04)	Tok/s 31688 (38604)	Loss/tok 3.0451 (3.3802)	LR 2.000e-03
0: TRAIN [1][2550/3880]	Time 0.136 (0.182)	Data 8.68e-05 (3.68e-04)	Tok/s 18736 (38599)	Loss/tok 2.8268 (3.3804)	LR 2.000e-03
0: TRAIN [1][2560/3880]	Time 0.221 (0.182)	Data 7.53e-05 (3.67e-04)	Tok/s 52377 (38602)	Loss/tok 3.5384 (3.3802)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][2570/3880]	Time 0.186 (0.182)	Data 8.77e-05 (3.66e-04)	Tok/s 46381 (38610)	Loss/tok 3.2564 (3.3801)	LR 2.000e-03
0: TRAIN [1][2580/3880]	Time 0.162 (0.182)	Data 5.02e-04 (3.65e-04)	Tok/s 31187 (38620)	Loss/tok 3.2935 (3.3803)	LR 2.000e-03
0: TRAIN [1][2590/3880]	Time 0.162 (0.182)	Data 1.37e-04 (3.65e-04)	Tok/s 31934 (38599)	Loss/tok 3.1854 (3.3798)	LR 2.000e-03
0: TRAIN [1][2600/3880]	Time 0.162 (0.182)	Data 1.26e-04 (3.64e-04)	Tok/s 32431 (38599)	Loss/tok 3.1738 (3.3796)	LR 2.000e-03
0: TRAIN [1][2610/3880]	Time 0.191 (0.182)	Data 1.30e-04 (3.63e-04)	Tok/s 43734 (38620)	Loss/tok 3.3762 (3.3797)	LR 1.000e-03
0: TRAIN [1][2620/3880]	Time 0.137 (0.182)	Data 9.35e-05 (3.62e-04)	Tok/s 19722 (38593)	Loss/tok 2.6787 (3.3793)	LR 1.000e-03
0: TRAIN [1][2630/3880]	Time 0.162 (0.182)	Data 8.37e-05 (3.61e-04)	Tok/s 31725 (38601)	Loss/tok 3.0526 (3.3791)	LR 1.000e-03
0: TRAIN [1][2640/3880]	Time 0.162 (0.182)	Data 1.13e-04 (3.60e-04)	Tok/s 31145 (38588)	Loss/tok 3.0366 (3.3787)	LR 1.000e-03
0: TRAIN [1][2650/3880]	Time 0.162 (0.182)	Data 8.68e-05 (3.59e-04)	Tok/s 31892 (38596)	Loss/tok 3.0933 (3.3786)	LR 1.000e-03
0: TRAIN [1][2660/3880]	Time 0.220 (0.182)	Data 9.56e-05 (3.58e-04)	Tok/s 53317 (38601)	Loss/tok 3.4094 (3.3785)	LR 1.000e-03
0: TRAIN [1][2670/3880]	Time 0.162 (0.182)	Data 1.52e-04 (3.58e-04)	Tok/s 31809 (38600)	Loss/tok 2.9992 (3.3782)	LR 1.000e-03
0: TRAIN [1][2680/3880]	Time 0.191 (0.182)	Data 8.65e-05 (3.57e-04)	Tok/s 43227 (38619)	Loss/tok 3.2653 (3.3776)	LR 1.000e-03
0: TRAIN [1][2690/3880]	Time 0.162 (0.182)	Data 9.94e-05 (3.56e-04)	Tok/s 31929 (38593)	Loss/tok 3.1918 (3.3770)	LR 1.000e-03
0: TRAIN [1][2700/3880]	Time 0.136 (0.182)	Data 7.75e-05 (3.55e-04)	Tok/s 19064 (38590)	Loss/tok 2.6353 (3.3768)	LR 1.000e-03
0: TRAIN [1][2710/3880]	Time 0.162 (0.182)	Data 7.89e-05 (3.54e-04)	Tok/s 32830 (38602)	Loss/tok 2.9813 (3.3768)	LR 1.000e-03
0: TRAIN [1][2720/3880]	Time 0.191 (0.182)	Data 9.37e-05 (3.53e-04)	Tok/s 44051 (38614)	Loss/tok 3.3614 (3.3770)	LR 1.000e-03
0: TRAIN [1][2730/3880]	Time 0.136 (0.182)	Data 8.37e-05 (3.52e-04)	Tok/s 18947 (38611)	Loss/tok 2.6295 (3.3767)	LR 1.000e-03
0: TRAIN [1][2740/3880]	Time 0.162 (0.182)	Data 8.11e-05 (3.51e-04)	Tok/s 31308 (38619)	Loss/tok 3.0581 (3.3764)	LR 1.000e-03
0: TRAIN [1][2750/3880]	Time 0.192 (0.182)	Data 7.99e-05 (3.50e-04)	Tok/s 43190 (38630)	Loss/tok 3.1912 (3.3761)	LR 1.000e-03
0: TRAIN [1][2760/3880]	Time 0.191 (0.182)	Data 7.70e-05 (3.49e-04)	Tok/s 43902 (38647)	Loss/tok 3.3156 (3.3761)	LR 1.000e-03
0: TRAIN [1][2770/3880]	Time 0.191 (0.182)	Data 7.72e-05 (3.49e-04)	Tok/s 44392 (38659)	Loss/tok 3.3707 (3.3760)	LR 1.000e-03
0: TRAIN [1][2780/3880]	Time 0.162 (0.182)	Data 7.49e-05 (3.48e-04)	Tok/s 33088 (38656)	Loss/tok 3.1680 (3.3755)	LR 1.000e-03
0: TRAIN [1][2790/3880]	Time 0.164 (0.182)	Data 8.85e-05 (3.47e-04)	Tok/s 31353 (38646)	Loss/tok 3.1362 (3.3753)	LR 1.000e-03
0: TRAIN [1][2800/3880]	Time 0.191 (0.182)	Data 1.18e-04 (3.46e-04)	Tok/s 43846 (38661)	Loss/tok 3.2558 (3.3754)	LR 1.000e-03
0: TRAIN [1][2810/3880]	Time 0.162 (0.182)	Data 8.44e-05 (3.45e-04)	Tok/s 32263 (38645)	Loss/tok 2.9817 (3.3748)	LR 1.000e-03
0: TRAIN [1][2820/3880]	Time 0.220 (0.182)	Data 7.89e-05 (3.44e-04)	Tok/s 52402 (38641)	Loss/tok 3.3774 (3.3744)	LR 1.000e-03
0: TRAIN [1][2830/3880]	Time 0.162 (0.182)	Data 7.65e-05 (3.43e-04)	Tok/s 32568 (38644)	Loss/tok 3.2137 (3.3743)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2840/3880]	Time 0.162 (0.182)	Data 9.06e-05 (3.42e-04)	Tok/s 31871 (38636)	Loss/tok 3.0161 (3.3741)	LR 1.000e-03
0: TRAIN [1][2850/3880]	Time 0.191 (0.182)	Data 1.41e-04 (3.42e-04)	Tok/s 44792 (38641)	Loss/tok 3.2109 (3.3738)	LR 1.000e-03
0: TRAIN [1][2860/3880]	Time 0.162 (0.182)	Data 1.21e-04 (3.41e-04)	Tok/s 32145 (38657)	Loss/tok 3.3606 (3.3740)	LR 1.000e-03
0: TRAIN [1][2870/3880]	Time 0.191 (0.182)	Data 8.89e-05 (3.40e-04)	Tok/s 43477 (38666)	Loss/tok 3.2651 (3.3738)	LR 1.000e-03
0: TRAIN [1][2880/3880]	Time 0.137 (0.182)	Data 9.35e-05 (3.39e-04)	Tok/s 19331 (38645)	Loss/tok 2.5972 (3.3731)	LR 1.000e-03
0: TRAIN [1][2890/3880]	Time 0.162 (0.182)	Data 1.26e-04 (3.38e-04)	Tok/s 31831 (38651)	Loss/tok 3.0161 (3.3734)	LR 1.000e-03
0: TRAIN [1][2900/3880]	Time 0.191 (0.182)	Data 9.68e-05 (3.38e-04)	Tok/s 43183 (38655)	Loss/tok 3.3110 (3.3732)	LR 1.000e-03
0: TRAIN [1][2910/3880]	Time 0.191 (0.182)	Data 9.06e-05 (3.37e-04)	Tok/s 43843 (38655)	Loss/tok 3.3241 (3.3728)	LR 1.000e-03
0: TRAIN [1][2920/3880]	Time 0.162 (0.182)	Data 1.48e-04 (3.36e-04)	Tok/s 32105 (38649)	Loss/tok 3.0300 (3.3721)	LR 1.000e-03
0: TRAIN [1][2930/3880]	Time 0.162 (0.182)	Data 1.28e-04 (3.35e-04)	Tok/s 33227 (38648)	Loss/tok 3.0639 (3.3718)	LR 1.000e-03
0: TRAIN [1][2940/3880]	Time 0.162 (0.182)	Data 1.22e-04 (3.35e-04)	Tok/s 32282 (38647)	Loss/tok 3.1320 (3.3716)	LR 1.000e-03
0: TRAIN [1][2950/3880]	Time 0.221 (0.182)	Data 1.39e-04 (3.34e-04)	Tok/s 53024 (38643)	Loss/tok 3.3374 (3.3710)	LR 1.000e-03
0: TRAIN [1][2960/3880]	Time 0.163 (0.182)	Data 1.16e-04 (3.33e-04)	Tok/s 31934 (38652)	Loss/tok 3.0371 (3.3710)	LR 1.000e-03
0: TRAIN [1][2970/3880]	Time 0.193 (0.182)	Data 8.23e-05 (3.33e-04)	Tok/s 43516 (38657)	Loss/tok 3.3110 (3.3706)	LR 1.000e-03
0: TRAIN [1][2980/3880]	Time 0.191 (0.182)	Data 8.11e-05 (3.32e-04)	Tok/s 43136 (38649)	Loss/tok 3.3773 (3.3704)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2990/3880]	Time 0.191 (0.182)	Data 7.68e-05 (3.31e-04)	Tok/s 44180 (38658)	Loss/tok 3.2183 (3.3701)	LR 1.000e-03
0: TRAIN [1][3000/3880]	Time 0.136 (0.182)	Data 7.70e-05 (3.30e-04)	Tok/s 19636 (38654)	Loss/tok 2.7249 (3.3698)	LR 1.000e-03
0: TRAIN [1][3010/3880]	Time 0.137 (0.182)	Data 7.30e-05 (3.29e-04)	Tok/s 19022 (38658)	Loss/tok 2.8554 (3.3691)	LR 1.000e-03
0: TRAIN [1][3020/3880]	Time 0.162 (0.182)	Data 7.46e-05 (3.29e-04)	Tok/s 32864 (38665)	Loss/tok 3.0459 (3.3688)	LR 1.000e-03
0: TRAIN [1][3030/3880]	Time 0.162 (0.182)	Data 9.01e-05 (3.28e-04)	Tok/s 32919 (38667)	Loss/tok 3.0744 (3.3684)	LR 1.000e-03
0: TRAIN [1][3040/3880]	Time 0.191 (0.182)	Data 9.30e-05 (3.27e-04)	Tok/s 43116 (38665)	Loss/tok 3.2075 (3.3680)	LR 1.000e-03
0: TRAIN [1][3050/3880]	Time 0.255 (0.182)	Data 7.80e-05 (3.26e-04)	Tok/s 58235 (38659)	Loss/tok 3.4775 (3.3675)	LR 1.000e-03
0: TRAIN [1][3060/3880]	Time 0.136 (0.182)	Data 9.11e-05 (3.26e-04)	Tok/s 18951 (38638)	Loss/tok 2.6096 (3.3670)	LR 1.000e-03
0: TRAIN [1][3070/3880]	Time 0.162 (0.182)	Data 9.13e-05 (3.25e-04)	Tok/s 31606 (38628)	Loss/tok 3.0297 (3.3665)	LR 1.000e-03
0: TRAIN [1][3080/3880]	Time 0.191 (0.182)	Data 8.75e-05 (3.24e-04)	Tok/s 42698 (38627)	Loss/tok 3.4133 (3.3659)	LR 1.000e-03
0: TRAIN [1][3090/3880]	Time 0.136 (0.182)	Data 9.35e-05 (3.24e-04)	Tok/s 19742 (38628)	Loss/tok 2.6144 (3.3659)	LR 1.000e-03
0: TRAIN [1][3100/3880]	Time 0.162 (0.182)	Data 8.13e-05 (3.23e-04)	Tok/s 32335 (38629)	Loss/tok 2.9512 (3.3657)	LR 1.000e-03
0: TRAIN [1][3110/3880]	Time 0.191 (0.182)	Data 7.89e-05 (3.22e-04)	Tok/s 45397 (38634)	Loss/tok 3.1936 (3.3656)	LR 1.000e-03
0: TRAIN [1][3120/3880]	Time 0.192 (0.182)	Data 7.37e-05 (3.21e-04)	Tok/s 43458 (38641)	Loss/tok 3.2408 (3.3655)	LR 1.000e-03
0: TRAIN [1][3130/3880]	Time 0.256 (0.182)	Data 1.02e-04 (3.21e-04)	Tok/s 58368 (38651)	Loss/tok 3.7524 (3.3655)	LR 1.000e-03
0: TRAIN [1][3140/3880]	Time 0.162 (0.182)	Data 7.58e-05 (3.20e-04)	Tok/s 31279 (38637)	Loss/tok 3.0802 (3.3649)	LR 1.000e-03
0: TRAIN [1][3150/3880]	Time 0.162 (0.182)	Data 1.03e-04 (3.20e-04)	Tok/s 32242 (38652)	Loss/tok 3.1671 (3.3648)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][3160/3880]	Time 0.221 (0.182)	Data 9.85e-05 (3.19e-04)	Tok/s 53267 (38657)	Loss/tok 3.4881 (3.3646)	LR 1.000e-03
0: TRAIN [1][3170/3880]	Time 0.162 (0.182)	Data 7.80e-05 (3.18e-04)	Tok/s 31929 (38652)	Loss/tok 3.1493 (3.3643)	LR 1.000e-03
0: TRAIN [1][3180/3880]	Time 0.191 (0.182)	Data 4.12e-04 (3.18e-04)	Tok/s 43996 (38666)	Loss/tok 3.2780 (3.3642)	LR 1.000e-03
0: TRAIN [1][3190/3880]	Time 0.191 (0.182)	Data 8.70e-05 (3.17e-04)	Tok/s 44196 (38658)	Loss/tok 3.2285 (3.3637)	LR 1.000e-03
0: TRAIN [1][3200/3880]	Time 0.162 (0.182)	Data 7.30e-05 (3.16e-04)	Tok/s 32060 (38652)	Loss/tok 3.1661 (3.3634)	LR 1.000e-03
0: TRAIN [1][3210/3880]	Time 0.191 (0.182)	Data 1.04e-04 (3.16e-04)	Tok/s 44027 (38638)	Loss/tok 3.2955 (3.3629)	LR 1.000e-03
0: TRAIN [1][3220/3880]	Time 0.221 (0.182)	Data 9.01e-05 (3.15e-04)	Tok/s 53994 (38653)	Loss/tok 3.5226 (3.3629)	LR 1.000e-03
0: TRAIN [1][3230/3880]	Time 0.191 (0.182)	Data 7.63e-05 (3.14e-04)	Tok/s 44425 (38664)	Loss/tok 3.1913 (3.3629)	LR 1.000e-03
0: TRAIN [1][3240/3880]	Time 0.221 (0.182)	Data 8.18e-05 (3.13e-04)	Tok/s 52783 (38671)	Loss/tok 3.4199 (3.3627)	LR 1.000e-03
0: TRAIN [1][3250/3880]	Time 0.191 (0.182)	Data 7.37e-05 (3.13e-04)	Tok/s 43577 (38675)	Loss/tok 3.1707 (3.3625)	LR 1.000e-03
0: TRAIN [1][3260/3880]	Time 0.257 (0.182)	Data 4.60e-04 (3.12e-04)	Tok/s 58416 (38692)	Loss/tok 3.7063 (3.3629)	LR 1.000e-03
0: TRAIN [1][3270/3880]	Time 0.136 (0.182)	Data 8.87e-05 (3.12e-04)	Tok/s 19068 (38684)	Loss/tok 2.6380 (3.3625)	LR 1.000e-03
0: TRAIN [1][3280/3880]	Time 0.191 (0.182)	Data 7.80e-05 (3.11e-04)	Tok/s 44123 (38685)	Loss/tok 3.4153 (3.3623)	LR 1.000e-03
0: TRAIN [1][3290/3880]	Time 0.191 (0.182)	Data 1.18e-04 (3.10e-04)	Tok/s 43237 (38691)	Loss/tok 3.2462 (3.3621)	LR 1.000e-03
0: TRAIN [1][3300/3880]	Time 0.222 (0.182)	Data 9.61e-05 (3.10e-04)	Tok/s 52575 (38691)	Loss/tok 3.4407 (3.3618)	LR 1.000e-03
0: TRAIN [1][3310/3880]	Time 0.138 (0.182)	Data 9.35e-05 (3.09e-04)	Tok/s 18547 (38673)	Loss/tok 2.7229 (3.3613)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][3320/3880]	Time 0.221 (0.182)	Data 5.38e-04 (3.09e-04)	Tok/s 53027 (38681)	Loss/tok 3.4888 (3.3612)	LR 1.000e-03
0: TRAIN [1][3330/3880]	Time 0.162 (0.182)	Data 7.87e-05 (3.08e-04)	Tok/s 32470 (38687)	Loss/tok 3.0816 (3.3609)	LR 1.000e-03
0: TRAIN [1][3340/3880]	Time 0.164 (0.182)	Data 7.92e-05 (3.07e-04)	Tok/s 32424 (38680)	Loss/tok 3.0846 (3.3606)	LR 1.000e-03
0: TRAIN [1][3350/3880]	Time 0.191 (0.182)	Data 7.51e-05 (3.07e-04)	Tok/s 45078 (38663)	Loss/tok 3.0623 (3.3600)	LR 1.000e-03
0: TRAIN [1][3360/3880]	Time 0.162 (0.182)	Data 1.21e-04 (3.06e-04)	Tok/s 32429 (38672)	Loss/tok 3.0206 (3.3600)	LR 1.000e-03
0: TRAIN [1][3370/3880]	Time 0.162 (0.182)	Data 9.80e-05 (3.05e-04)	Tok/s 31681 (38666)	Loss/tok 3.0740 (3.3597)	LR 1.000e-03
0: TRAIN [1][3380/3880]	Time 0.162 (0.182)	Data 8.75e-05 (3.05e-04)	Tok/s 31248 (38678)	Loss/tok 3.0587 (3.3596)	LR 1.000e-03
0: TRAIN [1][3390/3880]	Time 0.220 (0.182)	Data 1.20e-04 (3.04e-04)	Tok/s 52633 (38677)	Loss/tok 3.6218 (3.3593)	LR 1.000e-03
0: TRAIN [1][3400/3880]	Time 0.162 (0.182)	Data 1.12e-04 (3.04e-04)	Tok/s 32221 (38650)	Loss/tok 3.0352 (3.3588)	LR 1.000e-03
0: TRAIN [1][3410/3880]	Time 0.221 (0.182)	Data 7.72e-05 (3.03e-04)	Tok/s 53154 (38648)	Loss/tok 3.5542 (3.3586)	LR 1.000e-03
0: TRAIN [1][3420/3880]	Time 0.162 (0.182)	Data 9.39e-05 (3.03e-04)	Tok/s 30777 (38634)	Loss/tok 3.0458 (3.3581)	LR 5.000e-04
0: TRAIN [1][3430/3880]	Time 0.136 (0.182)	Data 8.56e-05 (3.02e-04)	Tok/s 19038 (38631)	Loss/tok 2.7155 (3.3580)	LR 5.000e-04
0: TRAIN [1][3440/3880]	Time 0.136 (0.182)	Data 9.04e-05 (3.01e-04)	Tok/s 19457 (38632)	Loss/tok 2.5428 (3.3580)	LR 5.000e-04
0: TRAIN [1][3450/3880]	Time 0.162 (0.182)	Data 7.87e-05 (3.01e-04)	Tok/s 30890 (38642)	Loss/tok 2.9023 (3.3577)	LR 5.000e-04
0: TRAIN [1][3460/3880]	Time 0.191 (0.182)	Data 7.84e-05 (3.00e-04)	Tok/s 44068 (38639)	Loss/tok 3.2696 (3.3572)	LR 5.000e-04
0: TRAIN [1][3470/3880]	Time 0.162 (0.182)	Data 7.77e-05 (3.00e-04)	Tok/s 31757 (38645)	Loss/tok 3.0246 (3.3569)	LR 5.000e-04
0: TRAIN [1][3480/3880]	Time 0.163 (0.182)	Data 3.70e-04 (2.99e-04)	Tok/s 32614 (38656)	Loss/tok 3.1791 (3.3570)	LR 5.000e-04
0: TRAIN [1][3490/3880]	Time 0.162 (0.182)	Data 7.58e-05 (2.98e-04)	Tok/s 30866 (38648)	Loss/tok 3.0000 (3.3566)	LR 5.000e-04
0: TRAIN [1][3500/3880]	Time 0.162 (0.182)	Data 7.61e-05 (2.98e-04)	Tok/s 32564 (38632)	Loss/tok 3.1124 (3.3560)	LR 5.000e-04
0: TRAIN [1][3510/3880]	Time 0.136 (0.182)	Data 7.84e-05 (2.97e-04)	Tok/s 18834 (38615)	Loss/tok 2.6745 (3.3556)	LR 5.000e-04
0: TRAIN [1][3520/3880]	Time 0.221 (0.182)	Data 1.12e-04 (2.97e-04)	Tok/s 52396 (38625)	Loss/tok 3.4274 (3.3557)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][3530/3880]	Time 0.191 (0.182)	Data 7.87e-05 (2.96e-04)	Tok/s 44507 (38637)	Loss/tok 3.1730 (3.3555)	LR 5.000e-04
0: TRAIN [1][3540/3880]	Time 0.162 (0.182)	Data 7.99e-05 (2.96e-04)	Tok/s 31597 (38643)	Loss/tok 3.1213 (3.3554)	LR 5.000e-04
0: TRAIN [1][3550/3880]	Time 0.191 (0.182)	Data 9.87e-05 (2.95e-04)	Tok/s 43865 (38643)	Loss/tok 3.2105 (3.3552)	LR 5.000e-04
0: TRAIN [1][3560/3880]	Time 0.191 (0.182)	Data 7.87e-05 (2.95e-04)	Tok/s 44349 (38640)	Loss/tok 3.2912 (3.3549)	LR 5.000e-04
0: TRAIN [1][3570/3880]	Time 0.191 (0.182)	Data 7.72e-05 (2.94e-04)	Tok/s 44997 (38629)	Loss/tok 3.2506 (3.3544)	LR 5.000e-04
0: TRAIN [1][3580/3880]	Time 0.162 (0.182)	Data 9.85e-05 (2.94e-04)	Tok/s 32509 (38617)	Loss/tok 3.1004 (3.3539)	LR 5.000e-04
0: TRAIN [1][3590/3880]	Time 0.136 (0.182)	Data 8.27e-05 (2.93e-04)	Tok/s 18979 (38604)	Loss/tok 2.6053 (3.3535)	LR 5.000e-04
0: TRAIN [1][3600/3880]	Time 0.191 (0.181)	Data 8.70e-05 (2.93e-04)	Tok/s 44026 (38592)	Loss/tok 3.2167 (3.3530)	LR 5.000e-04
0: TRAIN [1][3610/3880]	Time 0.162 (0.181)	Data 9.11e-05 (2.92e-04)	Tok/s 32161 (38572)	Loss/tok 3.0179 (3.3526)	LR 5.000e-04
0: TRAIN [1][3620/3880]	Time 0.162 (0.181)	Data 1.03e-04 (2.92e-04)	Tok/s 32309 (38566)	Loss/tok 2.9048 (3.3521)	LR 5.000e-04
0: TRAIN [1][3630/3880]	Time 0.259 (0.181)	Data 9.47e-05 (2.91e-04)	Tok/s 58591 (38565)	Loss/tok 3.4391 (3.3520)	LR 5.000e-04
0: TRAIN [1][3640/3880]	Time 0.191 (0.181)	Data 7.72e-05 (2.91e-04)	Tok/s 44786 (38573)	Loss/tok 3.2892 (3.3519)	LR 5.000e-04
0: TRAIN [1][3650/3880]	Time 0.162 (0.181)	Data 1.57e-04 (2.91e-04)	Tok/s 31504 (38561)	Loss/tok 3.1723 (3.3515)	LR 5.000e-04
0: TRAIN [1][3660/3880]	Time 0.162 (0.181)	Data 7.41e-05 (2.90e-04)	Tok/s 32595 (38552)	Loss/tok 3.1751 (3.3511)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][3670/3880]	Time 0.186 (0.181)	Data 8.30e-05 (2.89e-04)	Tok/s 45465 (38559)	Loss/tok 3.2735 (3.3509)	LR 5.000e-04
0: TRAIN [1][3680/3880]	Time 0.162 (0.181)	Data 1.25e-04 (2.89e-04)	Tok/s 32609 (38558)	Loss/tok 2.9523 (3.3506)	LR 5.000e-04
0: TRAIN [1][3690/3880]	Time 0.136 (0.181)	Data 8.85e-05 (2.88e-04)	Tok/s 20084 (38553)	Loss/tok 2.6463 (3.3504)	LR 5.000e-04
0: TRAIN [1][3700/3880]	Time 0.162 (0.181)	Data 1.13e-04 (2.88e-04)	Tok/s 32408 (38566)	Loss/tok 3.0517 (3.3503)	LR 5.000e-04
0: TRAIN [1][3710/3880]	Time 0.191 (0.181)	Data 7.70e-05 (2.87e-04)	Tok/s 44026 (38553)	Loss/tok 3.2040 (3.3498)	LR 5.000e-04
0: TRAIN [1][3720/3880]	Time 0.162 (0.181)	Data 8.27e-05 (2.87e-04)	Tok/s 31381 (38569)	Loss/tok 2.9983 (3.3497)	LR 5.000e-04
0: TRAIN [1][3730/3880]	Time 0.191 (0.181)	Data 7.65e-05 (2.86e-04)	Tok/s 44576 (38576)	Loss/tok 3.3884 (3.3497)	LR 5.000e-04
0: TRAIN [1][3740/3880]	Time 0.162 (0.181)	Data 7.41e-05 (2.86e-04)	Tok/s 32581 (38579)	Loss/tok 2.9740 (3.3494)	LR 5.000e-04
0: TRAIN [1][3750/3880]	Time 0.163 (0.181)	Data 7.80e-04 (2.85e-04)	Tok/s 31941 (38561)	Loss/tok 3.0209 (3.3489)	LR 5.000e-04
0: TRAIN [1][3760/3880]	Time 0.162 (0.181)	Data 7.80e-05 (2.85e-04)	Tok/s 32671 (38571)	Loss/tok 2.9906 (3.3490)	LR 5.000e-04
0: TRAIN [1][3770/3880]	Time 0.191 (0.181)	Data 8.20e-05 (2.85e-04)	Tok/s 43831 (38568)	Loss/tok 3.2584 (3.3485)	LR 5.000e-04
0: TRAIN [1][3780/3880]	Time 0.191 (0.182)	Data 8.75e-05 (2.84e-04)	Tok/s 44362 (38587)	Loss/tok 3.1837 (3.3486)	LR 5.000e-04
0: TRAIN [1][3790/3880]	Time 0.162 (0.182)	Data 1.04e-04 (2.84e-04)	Tok/s 31321 (38592)	Loss/tok 2.9890 (3.3485)	LR 5.000e-04
0: TRAIN [1][3800/3880]	Time 0.162 (0.182)	Data 7.82e-05 (2.83e-04)	Tok/s 31588 (38589)	Loss/tok 3.1557 (3.3480)	LR 5.000e-04
0: TRAIN [1][3810/3880]	Time 0.221 (0.182)	Data 8.89e-05 (2.83e-04)	Tok/s 51699 (38590)	Loss/tok 3.5977 (3.3479)	LR 5.000e-04
0: TRAIN [1][3820/3880]	Time 0.163 (0.182)	Data 8.85e-05 (2.82e-04)	Tok/s 31474 (38591)	Loss/tok 3.1677 (3.3475)	LR 5.000e-04
0: TRAIN [1][3830/3880]	Time 0.191 (0.182)	Data 9.01e-05 (2.82e-04)	Tok/s 44137 (38602)	Loss/tok 3.3228 (3.3475)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][3840/3880]	Time 0.220 (0.182)	Data 7.41e-05 (2.81e-04)	Tok/s 54261 (38605)	Loss/tok 3.2901 (3.3471)	LR 5.000e-04
0: TRAIN [1][3850/3880]	Time 0.162 (0.182)	Data 8.85e-05 (2.81e-04)	Tok/s 31688 (38602)	Loss/tok 2.9475 (3.3469)	LR 5.000e-04
0: TRAIN [1][3860/3880]	Time 0.221 (0.182)	Data 7.99e-05 (2.80e-04)	Tok/s 53783 (38604)	Loss/tok 3.4044 (3.3466)	LR 5.000e-04
0: TRAIN [1][3870/3880]	Time 0.192 (0.182)	Data 9.80e-05 (2.80e-04)	Tok/s 44095 (38615)	Loss/tok 3.1657 (3.3465)	LR 5.000e-04
:::MLL 1571256299.591 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1571256299.592 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.663 (0.663)	Decoder iters 115.0 (115.0)	Tok/s 24653 (24653)
0: Running moses detokenizer
0: BLEU(score=23.272866960902647, counts=[36527, 18005, 10073, 5908], totals=[65033, 62030, 59027, 56030], precisions=[56.16686912798118, 29.02627760760922, 17.06507191624172, 10.544351240406925], bp=1.0, sys_len=65033, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571256301.442 eval_accuracy: {"value": 23.27, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1571256301.443 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.3476	Test BLEU: 23.27
0: Performance: Epoch: 1	Training: 308726 Tok/s
0: Finished epoch 1
:::MLL 1571256301.443 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1571256301.443 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571256301.444 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 2127636045
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][0/3880]	Time 0.875 (0.875)	Data 6.93e-01 (6.93e-01)	Tok/s 5961 (5961)	Loss/tok 2.9702 (2.9702)	LR 5.000e-04
0: TRAIN [2][10/3880]	Time 0.162 (0.238)	Data 9.68e-05 (6.31e-02)	Tok/s 31262 (33350)	Loss/tok 3.0142 (3.0805)	LR 5.000e-04
0: TRAIN [2][20/3880]	Time 0.167 (0.210)	Data 8.99e-05 (3.31e-02)	Tok/s 31391 (35840)	Loss/tok 3.1181 (3.1275)	LR 5.000e-04
0: TRAIN [2][30/3880]	Time 0.220 (0.204)	Data 5.80e-04 (2.25e-02)	Tok/s 52398 (37985)	Loss/tok 3.3806 (3.1541)	LR 5.000e-04
0: TRAIN [2][40/3880]	Time 0.162 (0.194)	Data 9.63e-05 (1.70e-02)	Tok/s 31103 (36464)	Loss/tok 2.9013 (3.1294)	LR 5.000e-04
0: TRAIN [2][50/3880]	Time 0.191 (0.191)	Data 9.51e-05 (1.37e-02)	Tok/s 44220 (36881)	Loss/tok 3.2522 (3.1341)	LR 5.000e-04
0: TRAIN [2][60/3880]	Time 0.191 (0.189)	Data 9.16e-05 (1.15e-02)	Tok/s 43893 (37189)	Loss/tok 3.3227 (3.1308)	LR 5.000e-04
0: TRAIN [2][70/3880]	Time 0.136 (0.187)	Data 9.58e-05 (9.88e-03)	Tok/s 20014 (36903)	Loss/tok 2.5043 (3.1225)	LR 5.000e-04
0: TRAIN [2][80/3880]	Time 0.220 (0.188)	Data 1.17e-04 (8.67e-03)	Tok/s 52617 (37751)	Loss/tok 3.2658 (3.1411)	LR 5.000e-04
0: TRAIN [2][90/3880]	Time 0.191 (0.188)	Data 1.12e-04 (7.73e-03)	Tok/s 43152 (37989)	Loss/tok 3.2587 (3.1526)	LR 5.000e-04
0: TRAIN [2][100/3880]	Time 0.162 (0.185)	Data 1.45e-04 (6.98e-03)	Tok/s 30963 (37359)	Loss/tok 2.7770 (3.1411)	LR 5.000e-04
0: TRAIN [2][110/3880]	Time 0.138 (0.186)	Data 1.01e-04 (6.36e-03)	Tok/s 19206 (37717)	Loss/tok 2.6692 (3.1491)	LR 5.000e-04
0: TRAIN [2][120/3880]	Time 0.191 (0.186)	Data 9.61e-05 (5.84e-03)	Tok/s 43588 (37830)	Loss/tok 3.2342 (3.1550)	LR 5.000e-04
0: TRAIN [2][130/3880]	Time 0.256 (0.186)	Data 8.92e-05 (5.40e-03)	Tok/s 57685 (38057)	Loss/tok 3.5444 (3.1589)	LR 5.000e-04
0: TRAIN [2][140/3880]	Time 0.162 (0.185)	Data 9.61e-05 (5.03e-03)	Tok/s 32570 (38091)	Loss/tok 2.9728 (3.1557)	LR 5.000e-04
0: TRAIN [2][150/3880]	Time 0.162 (0.185)	Data 1.06e-04 (4.70e-03)	Tok/s 32207 (37850)	Loss/tok 2.9488 (3.1521)	LR 5.000e-04
0: TRAIN [2][160/3880]	Time 0.162 (0.184)	Data 1.08e-04 (4.42e-03)	Tok/s 31277 (37662)	Loss/tok 2.9743 (3.1501)	LR 5.000e-04
0: TRAIN [2][170/3880]	Time 0.137 (0.183)	Data 8.37e-05 (4.16e-03)	Tok/s 19313 (37464)	Loss/tok 2.6545 (3.1457)	LR 5.000e-04
0: TRAIN [2][180/3880]	Time 0.162 (0.183)	Data 1.02e-04 (3.94e-03)	Tok/s 32537 (37531)	Loss/tok 3.0718 (3.1449)	LR 5.000e-04
0: TRAIN [2][190/3880]	Time 0.191 (0.182)	Data 8.70e-05 (3.74e-03)	Tok/s 44289 (37406)	Loss/tok 2.9935 (3.1391)	LR 5.000e-04
0: TRAIN [2][200/3880]	Time 0.191 (0.182)	Data 1.13e-04 (3.56e-03)	Tok/s 43693 (37474)	Loss/tok 3.1020 (3.1392)	LR 5.000e-04
0: TRAIN [2][210/3880]	Time 0.162 (0.182)	Data 1.02e-04 (3.40e-03)	Tok/s 32372 (37402)	Loss/tok 2.8964 (3.1389)	LR 5.000e-04
0: TRAIN [2][220/3880]	Time 0.162 (0.181)	Data 9.47e-05 (3.25e-03)	Tok/s 32518 (37380)	Loss/tok 3.1010 (3.1373)	LR 5.000e-04
0: TRAIN [2][230/3880]	Time 0.222 (0.181)	Data 8.70e-05 (3.11e-03)	Tok/s 52602 (37472)	Loss/tok 3.2501 (3.1380)	LR 5.000e-04
0: TRAIN [2][240/3880]	Time 0.162 (0.181)	Data 9.49e-05 (2.99e-03)	Tok/s 32165 (37442)	Loss/tok 2.9749 (3.1356)	LR 5.000e-04
0: TRAIN [2][250/3880]	Time 0.162 (0.181)	Data 8.34e-05 (2.87e-03)	Tok/s 31372 (37259)	Loss/tok 2.9205 (3.1322)	LR 5.000e-04
0: TRAIN [2][260/3880]	Time 0.162 (0.181)	Data 1.03e-04 (2.77e-03)	Tok/s 32006 (37445)	Loss/tok 3.0706 (3.1377)	LR 5.000e-04
0: TRAIN [2][270/3880]	Time 0.191 (0.181)	Data 1.02e-04 (2.67e-03)	Tok/s 43203 (37676)	Loss/tok 3.0633 (3.1404)	LR 5.000e-04
0: TRAIN [2][280/3880]	Time 0.191 (0.181)	Data 9.94e-05 (2.58e-03)	Tok/s 44067 (37701)	Loss/tok 3.1543 (3.1405)	LR 5.000e-04
0: TRAIN [2][290/3880]	Time 0.191 (0.181)	Data 1.29e-04 (2.49e-03)	Tok/s 42994 (37764)	Loss/tok 3.1513 (3.1409)	LR 5.000e-04
0: TRAIN [2][300/3880]	Time 0.136 (0.181)	Data 1.41e-04 (2.42e-03)	Tok/s 18835 (37697)	Loss/tok 2.4091 (3.1397)	LR 5.000e-04
0: TRAIN [2][310/3880]	Time 0.192 (0.181)	Data 1.15e-04 (2.35e-03)	Tok/s 42907 (37748)	Loss/tok 3.2071 (3.1392)	LR 5.000e-04
0: TRAIN [2][320/3880]	Time 0.163 (0.181)	Data 1.02e-04 (2.28e-03)	Tok/s 31504 (37732)	Loss/tok 2.8981 (3.1383)	LR 5.000e-04
0: TRAIN [2][330/3880]	Time 0.163 (0.181)	Data 5.01e-04 (2.22e-03)	Tok/s 32545 (37791)	Loss/tok 3.0153 (3.1404)	LR 5.000e-04
0: TRAIN [2][340/3880]	Time 0.162 (0.181)	Data 1.23e-04 (2.16e-03)	Tok/s 31673 (37733)	Loss/tok 2.9225 (3.1395)	LR 5.000e-04
0: TRAIN [2][350/3880]	Time 0.138 (0.181)	Data 1.02e-04 (2.10e-03)	Tok/s 19013 (37830)	Loss/tok 2.4965 (3.1448)	LR 5.000e-04
0: TRAIN [2][360/3880]	Time 0.162 (0.181)	Data 1.33e-04 (2.05e-03)	Tok/s 31913 (37840)	Loss/tok 2.9998 (3.1465)	LR 2.500e-04
0: TRAIN [2][370/3880]	Time 0.136 (0.181)	Data 9.18e-05 (1.99e-03)	Tok/s 19510 (37826)	Loss/tok 2.4999 (3.1476)	LR 2.500e-04
0: TRAIN [2][380/3880]	Time 0.136 (0.182)	Data 1.12e-04 (1.94e-03)	Tok/s 19507 (37956)	Loss/tok 2.5522 (3.1493)	LR 2.500e-04
0: TRAIN [2][390/3880]	Time 0.192 (0.182)	Data 1.08e-04 (1.90e-03)	Tok/s 44007 (37914)	Loss/tok 3.1510 (3.1479)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][400/3880]	Time 0.221 (0.182)	Data 1.11e-04 (1.85e-03)	Tok/s 52846 (37921)	Loss/tok 3.3301 (3.1473)	LR 2.500e-04
0: TRAIN [2][410/3880]	Time 0.220 (0.182)	Data 1.04e-04 (1.81e-03)	Tok/s 53066 (38027)	Loss/tok 3.3507 (3.1483)	LR 2.500e-04
0: TRAIN [2][420/3880]	Time 0.191 (0.182)	Data 8.37e-05 (1.77e-03)	Tok/s 43333 (38026)	Loss/tok 3.2395 (3.1476)	LR 2.500e-04
0: TRAIN [2][430/3880]	Time 0.163 (0.182)	Data 8.51e-05 (1.73e-03)	Tok/s 31184 (38052)	Loss/tok 2.9372 (3.1472)	LR 2.500e-04
0: TRAIN [2][440/3880]	Time 0.162 (0.181)	Data 9.73e-05 (1.70e-03)	Tok/s 32284 (38017)	Loss/tok 2.9778 (3.1456)	LR 2.500e-04
0: TRAIN [2][450/3880]	Time 0.191 (0.181)	Data 1.25e-04 (1.66e-03)	Tok/s 43474 (38001)	Loss/tok 3.1321 (3.1441)	LR 2.500e-04
0: TRAIN [2][460/3880]	Time 0.192 (0.181)	Data 9.78e-05 (1.63e-03)	Tok/s 44614 (38023)	Loss/tok 3.1175 (3.1426)	LR 2.500e-04
0: TRAIN [2][470/3880]	Time 0.191 (0.181)	Data 9.68e-05 (1.59e-03)	Tok/s 43160 (37976)	Loss/tok 3.0532 (3.1413)	LR 2.500e-04
0: TRAIN [2][480/3880]	Time 0.162 (0.181)	Data 1.21e-04 (1.56e-03)	Tok/s 31044 (37926)	Loss/tok 2.9061 (3.1413)	LR 2.500e-04
0: TRAIN [2][490/3880]	Time 0.161 (0.181)	Data 1.02e-04 (1.54e-03)	Tok/s 32360 (37942)	Loss/tok 3.0576 (3.1424)	LR 2.500e-04
0: TRAIN [2][500/3880]	Time 0.162 (0.181)	Data 1.33e-04 (1.51e-03)	Tok/s 31953 (37889)	Loss/tok 3.0450 (3.1410)	LR 2.500e-04
0: TRAIN [2][510/3880]	Time 0.221 (0.181)	Data 8.34e-05 (1.48e-03)	Tok/s 52312 (37857)	Loss/tok 3.3729 (3.1415)	LR 2.500e-04
0: TRAIN [2][520/3880]	Time 0.192 (0.181)	Data 1.04e-04 (1.45e-03)	Tok/s 43244 (37868)	Loss/tok 3.2677 (3.1420)	LR 2.500e-04
0: TRAIN [2][530/3880]	Time 0.221 (0.181)	Data 1.09e-04 (1.43e-03)	Tok/s 52030 (37924)	Loss/tok 3.4916 (3.1441)	LR 2.500e-04
0: TRAIN [2][540/3880]	Time 0.162 (0.181)	Data 8.49e-05 (1.40e-03)	Tok/s 32771 (37897)	Loss/tok 3.0343 (3.1434)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][550/3880]	Time 0.220 (0.181)	Data 9.58e-05 (1.38e-03)	Tok/s 52451 (37932)	Loss/tok 3.3202 (3.1428)	LR 2.500e-04
0: TRAIN [2][560/3880]	Time 0.191 (0.181)	Data 8.18e-05 (1.36e-03)	Tok/s 44308 (37971)	Loss/tok 3.0844 (3.1418)	LR 2.500e-04
0: TRAIN [2][570/3880]	Time 0.137 (0.181)	Data 9.70e-05 (1.33e-03)	Tok/s 19436 (37875)	Loss/tok 2.5496 (3.1407)	LR 2.500e-04
0: TRAIN [2][580/3880]	Time 0.258 (0.181)	Data 8.61e-05 (1.31e-03)	Tok/s 57950 (37992)	Loss/tok 3.4490 (3.1422)	LR 2.500e-04
0: TRAIN [2][590/3880]	Time 0.221 (0.181)	Data 9.37e-05 (1.29e-03)	Tok/s 53162 (37961)	Loss/tok 3.2407 (3.1415)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][600/3880]	Time 0.191 (0.181)	Data 1.07e-04 (1.27e-03)	Tok/s 44596 (38001)	Loss/tok 2.9597 (3.1416)	LR 2.500e-04
0: TRAIN [2][610/3880]	Time 0.163 (0.181)	Data 9.39e-05 (1.25e-03)	Tok/s 32358 (37960)	Loss/tok 2.9828 (3.1411)	LR 2.500e-04
0: TRAIN [2][620/3880]	Time 0.163 (0.181)	Data 1.25e-04 (1.24e-03)	Tok/s 32876 (37994)	Loss/tok 3.0210 (3.1404)	LR 2.500e-04
0: TRAIN [2][630/3880]	Time 0.220 (0.181)	Data 1.08e-04 (1.22e-03)	Tok/s 53259 (38007)	Loss/tok 3.2528 (3.1402)	LR 2.500e-04
0: TRAIN [2][640/3880]	Time 0.162 (0.181)	Data 3.08e-04 (1.20e-03)	Tok/s 33384 (37981)	Loss/tok 3.0437 (3.1402)	LR 2.500e-04
0: TRAIN [2][650/3880]	Time 0.221 (0.181)	Data 9.56e-05 (1.18e-03)	Tok/s 52598 (38040)	Loss/tok 3.3054 (3.1408)	LR 2.500e-04
0: TRAIN [2][660/3880]	Time 0.162 (0.181)	Data 1.05e-04 (1.17e-03)	Tok/s 31439 (38082)	Loss/tok 2.9744 (3.1428)	LR 2.500e-04
0: TRAIN [2][670/3880]	Time 0.162 (0.181)	Data 1.13e-04 (1.15e-03)	Tok/s 30976 (38075)	Loss/tok 3.0080 (3.1424)	LR 2.500e-04
0: TRAIN [2][680/3880]	Time 0.221 (0.181)	Data 1.17e-04 (1.14e-03)	Tok/s 53377 (38077)	Loss/tok 3.2726 (3.1422)	LR 2.500e-04
0: TRAIN [2][690/3880]	Time 0.162 (0.181)	Data 9.92e-05 (1.12e-03)	Tok/s 30644 (38100)	Loss/tok 2.9645 (3.1422)	LR 2.500e-04
0: TRAIN [2][700/3880]	Time 0.162 (0.181)	Data 1.34e-04 (1.11e-03)	Tok/s 32064 (38089)	Loss/tok 3.0829 (3.1414)	LR 2.500e-04
0: TRAIN [2][710/3880]	Time 0.193 (0.181)	Data 9.35e-05 (1.10e-03)	Tok/s 43573 (38097)	Loss/tok 2.9900 (3.1405)	LR 2.500e-04
0: TRAIN [2][720/3880]	Time 0.162 (0.181)	Data 1.15e-04 (1.08e-03)	Tok/s 31806 (38095)	Loss/tok 3.0350 (3.1417)	LR 2.500e-04
0: TRAIN [2][730/3880]	Time 0.164 (0.181)	Data 1.12e-04 (1.07e-03)	Tok/s 32702 (38142)	Loss/tok 2.9608 (3.1415)	LR 2.500e-04
0: TRAIN [2][740/3880]	Time 0.224 (0.181)	Data 3.36e-04 (1.06e-03)	Tok/s 51899 (38166)	Loss/tok 3.1510 (3.1427)	LR 2.500e-04
0: TRAIN [2][750/3880]	Time 0.162 (0.181)	Data 1.44e-04 (1.05e-03)	Tok/s 31795 (38145)	Loss/tok 2.8909 (3.1413)	LR 2.500e-04
0: TRAIN [2][760/3880]	Time 0.162 (0.181)	Data 1.06e-04 (1.03e-03)	Tok/s 32471 (38114)	Loss/tok 3.1119 (3.1407)	LR 2.500e-04
0: TRAIN [2][770/3880]	Time 0.192 (0.181)	Data 1.19e-04 (1.02e-03)	Tok/s 43289 (38140)	Loss/tok 3.1980 (3.1406)	LR 2.500e-04
0: TRAIN [2][780/3880]	Time 0.223 (0.181)	Data 1.65e-04 (1.01e-03)	Tok/s 52166 (38157)	Loss/tok 3.3635 (3.1414)	LR 2.500e-04
0: TRAIN [2][790/3880]	Time 0.191 (0.181)	Data 8.80e-05 (9.99e-04)	Tok/s 43517 (38148)	Loss/tok 3.1342 (3.1405)	LR 2.500e-04
0: TRAIN [2][800/3880]	Time 0.137 (0.181)	Data 1.01e-04 (9.88e-04)	Tok/s 18402 (38133)	Loss/tok 2.4947 (3.1408)	LR 2.500e-04
0: TRAIN [2][810/3880]	Time 0.137 (0.181)	Data 8.68e-05 (9.78e-04)	Tok/s 19463 (38108)	Loss/tok 2.4862 (3.1395)	LR 2.500e-04
0: TRAIN [2][820/3880]	Time 0.162 (0.181)	Data 1.09e-04 (9.68e-04)	Tok/s 31039 (38102)	Loss/tok 3.1116 (3.1390)	LR 2.500e-04
0: TRAIN [2][830/3880]	Time 0.162 (0.181)	Data 8.58e-05 (9.58e-04)	Tok/s 30981 (38159)	Loss/tok 2.9369 (3.1392)	LR 2.500e-04
0: TRAIN [2][840/3880]	Time 0.191 (0.181)	Data 8.30e-05 (9.47e-04)	Tok/s 44114 (38102)	Loss/tok 3.3480 (3.1384)	LR 2.500e-04
0: TRAIN [2][850/3880]	Time 0.162 (0.181)	Data 8.49e-05 (9.38e-04)	Tok/s 30905 (38094)	Loss/tok 2.8504 (3.1383)	LR 2.500e-04
0: TRAIN [2][860/3880]	Time 0.162 (0.181)	Data 2.04e-04 (9.28e-04)	Tok/s 32482 (38034)	Loss/tok 3.0199 (3.1372)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][870/3880]	Time 0.191 (0.180)	Data 9.42e-05 (9.19e-04)	Tok/s 44723 (38035)	Loss/tok 3.2210 (3.1369)	LR 2.500e-04
0: TRAIN [2][880/3880]	Time 0.138 (0.180)	Data 1.07e-04 (9.10e-04)	Tok/s 19301 (38024)	Loss/tok 2.5394 (3.1368)	LR 2.500e-04
0: TRAIN [2][890/3880]	Time 0.138 (0.180)	Data 1.23e-04 (9.02e-04)	Tok/s 19699 (38007)	Loss/tok 2.7319 (3.1372)	LR 2.500e-04
0: TRAIN [2][900/3880]	Time 0.164 (0.180)	Data 1.05e-04 (8.93e-04)	Tok/s 31690 (38019)	Loss/tok 2.9279 (3.1372)	LR 2.500e-04
0: TRAIN [2][910/3880]	Time 0.256 (0.180)	Data 9.82e-05 (8.85e-04)	Tok/s 57332 (38037)	Loss/tok 3.6202 (3.1383)	LR 2.500e-04
0: TRAIN [2][920/3880]	Time 0.221 (0.181)	Data 1.05e-04 (8.76e-04)	Tok/s 53130 (38043)	Loss/tok 3.3364 (3.1386)	LR 2.500e-04
0: TRAIN [2][930/3880]	Time 0.257 (0.181)	Data 1.04e-04 (8.68e-04)	Tok/s 57904 (38174)	Loss/tok 3.4861 (3.1418)	LR 2.500e-04
0: TRAIN [2][940/3880]	Time 0.162 (0.181)	Data 1.12e-04 (8.60e-04)	Tok/s 31845 (38145)	Loss/tok 2.8450 (3.1407)	LR 2.500e-04
0: TRAIN [2][950/3880]	Time 0.256 (0.181)	Data 1.18e-04 (8.52e-04)	Tok/s 58104 (38106)	Loss/tok 3.3719 (3.1403)	LR 2.500e-04
0: TRAIN [2][960/3880]	Time 0.191 (0.181)	Data 1.13e-04 (8.45e-04)	Tok/s 44335 (38189)	Loss/tok 2.9765 (3.1424)	LR 2.500e-04
0: TRAIN [2][970/3880]	Time 0.221 (0.181)	Data 1.08e-04 (8.37e-04)	Tok/s 53125 (38192)	Loss/tok 3.3433 (3.1419)	LR 2.500e-04
0: TRAIN [2][980/3880]	Time 0.162 (0.181)	Data 1.04e-04 (8.30e-04)	Tok/s 32678 (38194)	Loss/tok 2.8777 (3.1416)	LR 2.500e-04
0: TRAIN [2][990/3880]	Time 0.162 (0.181)	Data 3.21e-04 (8.23e-04)	Tok/s 31993 (38198)	Loss/tok 2.9643 (3.1413)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1000/3880]	Time 0.162 (0.181)	Data 1.08e-04 (8.16e-04)	Tok/s 33425 (38205)	Loss/tok 2.9526 (3.1415)	LR 2.500e-04
0: TRAIN [2][1010/3880]	Time 0.191 (0.181)	Data 1.53e-04 (8.09e-04)	Tok/s 44357 (38227)	Loss/tok 3.4006 (3.1423)	LR 2.500e-04
0: TRAIN [2][1020/3880]	Time 0.162 (0.181)	Data 1.08e-04 (8.02e-04)	Tok/s 31602 (38176)	Loss/tok 2.9208 (3.1419)	LR 2.500e-04
0: TRAIN [2][1030/3880]	Time 0.162 (0.181)	Data 8.77e-05 (7.95e-04)	Tok/s 31730 (38163)	Loss/tok 2.8990 (3.1411)	LR 2.500e-04
0: TRAIN [2][1040/3880]	Time 0.191 (0.181)	Data 1.07e-04 (7.90e-04)	Tok/s 44533 (38150)	Loss/tok 3.1789 (3.1402)	LR 2.500e-04
0: TRAIN [2][1050/3880]	Time 0.191 (0.181)	Data 1.50e-04 (7.83e-04)	Tok/s 44028 (38148)	Loss/tok 3.1382 (3.1399)	LR 2.500e-04
0: TRAIN [2][1060/3880]	Time 0.221 (0.181)	Data 1.23e-04 (7.77e-04)	Tok/s 54388 (38195)	Loss/tok 3.2015 (3.1398)	LR 2.500e-04
0: TRAIN [2][1070/3880]	Time 0.191 (0.181)	Data 1.16e-04 (7.71e-04)	Tok/s 44504 (38205)	Loss/tok 3.0287 (3.1398)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1080/3880]	Time 0.162 (0.181)	Data 1.06e-04 (7.65e-04)	Tok/s 31557 (38177)	Loss/tok 2.9995 (3.1392)	LR 2.500e-04
0: TRAIN [2][1090/3880]	Time 0.191 (0.181)	Data 1.08e-04 (7.59e-04)	Tok/s 44995 (38225)	Loss/tok 3.0456 (3.1396)	LR 2.500e-04
0: TRAIN [2][1100/3880]	Time 0.192 (0.181)	Data 1.02e-04 (7.53e-04)	Tok/s 43555 (38200)	Loss/tok 3.0596 (3.1387)	LR 2.500e-04
0: TRAIN [2][1110/3880]	Time 0.258 (0.181)	Data 9.92e-05 (7.47e-04)	Tok/s 58547 (38238)	Loss/tok 3.4595 (3.1396)	LR 2.500e-04
0: TRAIN [2][1120/3880]	Time 0.162 (0.181)	Data 8.23e-05 (7.41e-04)	Tok/s 31647 (38170)	Loss/tok 2.8681 (3.1384)	LR 2.500e-04
0: TRAIN [2][1130/3880]	Time 0.191 (0.181)	Data 1.09e-04 (7.36e-04)	Tok/s 43598 (38182)	Loss/tok 3.2139 (3.1386)	LR 2.500e-04
0: TRAIN [2][1140/3880]	Time 0.191 (0.181)	Data 9.04e-05 (7.30e-04)	Tok/s 45245 (38170)	Loss/tok 3.0534 (3.1380)	LR 2.500e-04
0: TRAIN [2][1150/3880]	Time 0.163 (0.181)	Data 1.05e-04 (7.25e-04)	Tok/s 31823 (38158)	Loss/tok 2.9795 (3.1383)	LR 2.500e-04
0: TRAIN [2][1160/3880]	Time 0.191 (0.181)	Data 6.35e-04 (7.20e-04)	Tok/s 43491 (38116)	Loss/tok 3.2364 (3.1374)	LR 2.500e-04
0: TRAIN [2][1170/3880]	Time 0.138 (0.181)	Data 9.99e-05 (7.15e-04)	Tok/s 19426 (38115)	Loss/tok 2.4825 (3.1378)	LR 1.250e-04
0: TRAIN [2][1180/3880]	Time 0.162 (0.181)	Data 8.68e-05 (7.10e-04)	Tok/s 32497 (38140)	Loss/tok 2.9616 (3.1383)	LR 1.250e-04
0: TRAIN [2][1190/3880]	Time 0.191 (0.181)	Data 2.21e-04 (7.05e-04)	Tok/s 43310 (38146)	Loss/tok 3.2938 (3.1389)	LR 1.250e-04
0: TRAIN [2][1200/3880]	Time 0.191 (0.181)	Data 9.94e-05 (7.00e-04)	Tok/s 43616 (38131)	Loss/tok 3.1292 (3.1384)	LR 1.250e-04
0: TRAIN [2][1210/3880]	Time 0.191 (0.181)	Data 9.97e-05 (6.95e-04)	Tok/s 42389 (38118)	Loss/tok 3.1850 (3.1385)	LR 1.250e-04
0: TRAIN [2][1220/3880]	Time 0.192 (0.181)	Data 8.85e-05 (6.90e-04)	Tok/s 45506 (38179)	Loss/tok 3.0554 (3.1397)	LR 1.250e-04
0: TRAIN [2][1230/3880]	Time 0.137 (0.181)	Data 1.14e-04 (6.85e-04)	Tok/s 20287 (38159)	Loss/tok 2.4559 (3.1396)	LR 1.250e-04
0: TRAIN [2][1240/3880]	Time 0.257 (0.181)	Data 1.16e-04 (6.81e-04)	Tok/s 57647 (38241)	Loss/tok 3.4145 (3.1410)	LR 1.250e-04
0: TRAIN [2][1250/3880]	Time 0.162 (0.181)	Data 1.21e-04 (6.76e-04)	Tok/s 32381 (38200)	Loss/tok 2.9003 (3.1399)	LR 1.250e-04
0: TRAIN [2][1260/3880]	Time 0.163 (0.181)	Data 3.36e-04 (6.72e-04)	Tok/s 32166 (38215)	Loss/tok 2.9568 (3.1396)	LR 1.250e-04
0: TRAIN [2][1270/3880]	Time 0.256 (0.181)	Data 2.41e-04 (6.68e-04)	Tok/s 58639 (38241)	Loss/tok 3.5839 (3.1403)	LR 1.250e-04
0: TRAIN [2][1280/3880]	Time 0.221 (0.181)	Data 1.05e-04 (6.64e-04)	Tok/s 53760 (38290)	Loss/tok 3.2029 (3.1415)	LR 1.250e-04
0: TRAIN [2][1290/3880]	Time 0.191 (0.181)	Data 1.26e-04 (6.60e-04)	Tok/s 44671 (38270)	Loss/tok 3.1256 (3.1412)	LR 1.250e-04
0: TRAIN [2][1300/3880]	Time 0.257 (0.181)	Data 9.11e-05 (6.56e-04)	Tok/s 58201 (38291)	Loss/tok 3.4104 (3.1414)	LR 1.250e-04
0: TRAIN [2][1310/3880]	Time 0.162 (0.181)	Data 1.21e-04 (6.51e-04)	Tok/s 31103 (38315)	Loss/tok 2.9267 (3.1418)	LR 1.250e-04
0: TRAIN [2][1320/3880]	Time 0.191 (0.181)	Data 7.96e-05 (6.47e-04)	Tok/s 43430 (38327)	Loss/tok 3.3029 (3.1418)	LR 1.250e-04
0: TRAIN [2][1330/3880]	Time 0.191 (0.181)	Data 1.18e-04 (6.43e-04)	Tok/s 43806 (38362)	Loss/tok 3.2681 (3.1422)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1340/3880]	Time 0.162 (0.181)	Data 1.13e-04 (6.39e-04)	Tok/s 31518 (38373)	Loss/tok 2.9840 (3.1424)	LR 1.250e-04
0: TRAIN [2][1350/3880]	Time 0.191 (0.181)	Data 1.17e-04 (6.36e-04)	Tok/s 43113 (38396)	Loss/tok 3.1238 (3.1433)	LR 1.250e-04
0: TRAIN [2][1360/3880]	Time 0.162 (0.181)	Data 8.54e-05 (6.32e-04)	Tok/s 32665 (38385)	Loss/tok 3.0487 (3.1434)	LR 1.250e-04
0: TRAIN [2][1370/3880]	Time 0.161 (0.181)	Data 1.39e-04 (6.28e-04)	Tok/s 32020 (38369)	Loss/tok 2.9873 (3.1430)	LR 1.250e-04
0: TRAIN [2][1380/3880]	Time 0.191 (0.181)	Data 8.15e-05 (6.24e-04)	Tok/s 44061 (38341)	Loss/tok 3.1496 (3.1423)	LR 1.250e-04
0: TRAIN [2][1390/3880]	Time 0.162 (0.181)	Data 1.05e-04 (6.21e-04)	Tok/s 31763 (38332)	Loss/tok 3.0252 (3.1421)	LR 1.250e-04
0: TRAIN [2][1400/3880]	Time 0.256 (0.181)	Data 8.63e-05 (6.17e-04)	Tok/s 58364 (38345)	Loss/tok 3.3645 (3.1424)	LR 1.250e-04
0: TRAIN [2][1410/3880]	Time 0.257 (0.181)	Data 8.65e-05 (6.13e-04)	Tok/s 58008 (38333)	Loss/tok 3.4753 (3.1423)	LR 1.250e-04
0: TRAIN [2][1420/3880]	Time 0.162 (0.181)	Data 1.61e-04 (6.10e-04)	Tok/s 31616 (38297)	Loss/tok 2.9205 (3.1417)	LR 1.250e-04
0: TRAIN [2][1430/3880]	Time 0.164 (0.181)	Data 8.34e-05 (6.06e-04)	Tok/s 32015 (38324)	Loss/tok 2.8573 (3.1415)	LR 1.250e-04
0: TRAIN [2][1440/3880]	Time 0.162 (0.181)	Data 1.04e-04 (6.02e-04)	Tok/s 32294 (38341)	Loss/tok 2.9954 (3.1418)	LR 1.250e-04
0: TRAIN [2][1450/3880]	Time 0.221 (0.181)	Data 1.01e-04 (5.99e-04)	Tok/s 53289 (38354)	Loss/tok 3.3110 (3.1423)	LR 1.250e-04
0: TRAIN [2][1460/3880]	Time 0.136 (0.181)	Data 9.08e-05 (5.96e-04)	Tok/s 19853 (38317)	Loss/tok 2.4340 (3.1422)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1470/3880]	Time 0.257 (0.181)	Data 1.04e-04 (5.92e-04)	Tok/s 57035 (38370)	Loss/tok 3.4787 (3.1429)	LR 1.250e-04
0: TRAIN [2][1480/3880]	Time 0.191 (0.181)	Data 8.73e-05 (5.89e-04)	Tok/s 44710 (38393)	Loss/tok 3.0189 (3.1431)	LR 1.250e-04
0: TRAIN [2][1490/3880]	Time 0.162 (0.181)	Data 8.99e-05 (5.86e-04)	Tok/s 31377 (38422)	Loss/tok 3.0565 (3.1441)	LR 1.250e-04
0: TRAIN [2][1500/3880]	Time 0.162 (0.181)	Data 1.07e-04 (5.83e-04)	Tok/s 31912 (38405)	Loss/tok 2.9408 (3.1437)	LR 1.250e-04
0: TRAIN [2][1510/3880]	Time 0.191 (0.181)	Data 1.09e-04 (5.79e-04)	Tok/s 44635 (38370)	Loss/tok 3.1921 (3.1429)	LR 1.250e-04
0: TRAIN [2][1520/3880]	Time 0.221 (0.181)	Data 1.30e-04 (5.76e-04)	Tok/s 52269 (38371)	Loss/tok 3.4045 (3.1427)	LR 1.250e-04
0: TRAIN [2][1530/3880]	Time 0.137 (0.181)	Data 9.97e-05 (5.73e-04)	Tok/s 19471 (38385)	Loss/tok 2.4573 (3.1431)	LR 1.250e-04
0: TRAIN [2][1540/3880]	Time 0.163 (0.181)	Data 1.14e-04 (5.70e-04)	Tok/s 32068 (38369)	Loss/tok 2.8588 (3.1429)	LR 1.250e-04
0: TRAIN [2][1550/3880]	Time 0.162 (0.181)	Data 9.73e-05 (5.68e-04)	Tok/s 31188 (38351)	Loss/tok 2.9494 (3.1427)	LR 1.250e-04
0: TRAIN [2][1560/3880]	Time 0.137 (0.181)	Data 1.07e-04 (5.66e-04)	Tok/s 19337 (38345)	Loss/tok 2.6496 (3.1425)	LR 1.250e-04
0: TRAIN [2][1570/3880]	Time 0.162 (0.181)	Data 1.15e-04 (5.63e-04)	Tok/s 33400 (38335)	Loss/tok 2.8901 (3.1425)	LR 1.250e-04
0: TRAIN [2][1580/3880]	Time 0.191 (0.181)	Data 1.09e-04 (5.60e-04)	Tok/s 43902 (38308)	Loss/tok 3.2777 (3.1420)	LR 1.250e-04
0: TRAIN [2][1590/3880]	Time 0.162 (0.181)	Data 1.04e-04 (5.57e-04)	Tok/s 31993 (38314)	Loss/tok 3.0324 (3.1422)	LR 1.250e-04
0: TRAIN [2][1600/3880]	Time 0.220 (0.181)	Data 1.26e-04 (5.54e-04)	Tok/s 53745 (38307)	Loss/tok 3.2008 (3.1416)	LR 1.250e-04
0: TRAIN [2][1610/3880]	Time 0.163 (0.181)	Data 1.32e-04 (5.52e-04)	Tok/s 31955 (38281)	Loss/tok 3.0134 (3.1411)	LR 1.250e-04
0: TRAIN [2][1620/3880]	Time 0.191 (0.181)	Data 1.24e-04 (5.49e-04)	Tok/s 44429 (38281)	Loss/tok 3.0255 (3.1407)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1630/3880]	Time 0.258 (0.181)	Data 1.28e-04 (5.47e-04)	Tok/s 58572 (38324)	Loss/tok 3.3037 (3.1417)	LR 1.250e-04
0: TRAIN [2][1640/3880]	Time 0.162 (0.181)	Data 1.45e-04 (5.44e-04)	Tok/s 31962 (38302)	Loss/tok 2.8974 (3.1415)	LR 1.250e-04
0: TRAIN [2][1650/3880]	Time 0.162 (0.181)	Data 1.13e-04 (5.41e-04)	Tok/s 31439 (38270)	Loss/tok 3.0566 (3.1415)	LR 1.250e-04
0: TRAIN [2][1660/3880]	Time 0.191 (0.181)	Data 9.18e-05 (5.39e-04)	Tok/s 43783 (38274)	Loss/tok 3.1925 (3.1411)	LR 1.250e-04
0: TRAIN [2][1670/3880]	Time 0.163 (0.181)	Data 9.87e-05 (5.37e-04)	Tok/s 31361 (38266)	Loss/tok 2.8902 (3.1420)	LR 1.250e-04
0: TRAIN [2][1680/3880]	Time 0.162 (0.181)	Data 7.65e-05 (5.34e-04)	Tok/s 32310 (38243)	Loss/tok 2.9095 (3.1418)	LR 1.250e-04
0: TRAIN [2][1690/3880]	Time 0.191 (0.181)	Data 9.92e-05 (5.31e-04)	Tok/s 44418 (38266)	Loss/tok 3.0811 (3.1422)	LR 1.250e-04
0: TRAIN [2][1700/3880]	Time 0.191 (0.181)	Data 8.87e-05 (5.29e-04)	Tok/s 43941 (38254)	Loss/tok 3.1062 (3.1415)	LR 1.250e-04
0: TRAIN [2][1710/3880]	Time 0.162 (0.181)	Data 9.44e-05 (5.26e-04)	Tok/s 31462 (38264)	Loss/tok 2.9964 (3.1418)	LR 1.250e-04
0: TRAIN [2][1720/3880]	Time 0.162 (0.181)	Data 8.58e-05 (5.24e-04)	Tok/s 30948 (38250)	Loss/tok 3.0290 (3.1415)	LR 1.250e-04
0: TRAIN [2][1730/3880]	Time 0.162 (0.181)	Data 9.51e-05 (5.22e-04)	Tok/s 32189 (38246)	Loss/tok 2.9720 (3.1412)	LR 1.250e-04
0: TRAIN [2][1740/3880]	Time 0.162 (0.181)	Data 8.85e-05 (5.19e-04)	Tok/s 31991 (38255)	Loss/tok 2.8698 (3.1410)	LR 1.250e-04
0: TRAIN [2][1750/3880]	Time 0.162 (0.181)	Data 9.87e-05 (5.17e-04)	Tok/s 32267 (38246)	Loss/tok 2.9218 (3.1416)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1760/3880]	Time 0.221 (0.181)	Data 8.06e-05 (5.14e-04)	Tok/s 52956 (38261)	Loss/tok 3.3401 (3.1417)	LR 1.250e-04
0: TRAIN [2][1770/3880]	Time 0.162 (0.181)	Data 8.06e-05 (5.12e-04)	Tok/s 30841 (38264)	Loss/tok 3.0790 (3.1420)	LR 1.250e-04
0: TRAIN [2][1780/3880]	Time 0.162 (0.181)	Data 1.01e-04 (5.09e-04)	Tok/s 31418 (38229)	Loss/tok 3.0867 (3.1413)	LR 1.250e-04
0: TRAIN [2][1790/3880]	Time 0.161 (0.181)	Data 8.32e-05 (5.07e-04)	Tok/s 31258 (38237)	Loss/tok 3.3301 (3.1415)	LR 1.250e-04
0: TRAIN [2][1800/3880]	Time 0.191 (0.181)	Data 1.08e-04 (5.05e-04)	Tok/s 43997 (38258)	Loss/tok 3.1136 (3.1417)	LR 1.250e-04
0: TRAIN [2][1810/3880]	Time 0.162 (0.181)	Data 9.82e-05 (5.03e-04)	Tok/s 32999 (38275)	Loss/tok 2.8596 (3.1416)	LR 1.250e-04
0: TRAIN [2][1820/3880]	Time 0.191 (0.181)	Data 9.35e-05 (5.01e-04)	Tok/s 45085 (38280)	Loss/tok 2.9344 (3.1412)	LR 1.250e-04
0: TRAIN [2][1830/3880]	Time 0.162 (0.181)	Data 1.13e-04 (4.99e-04)	Tok/s 32298 (38314)	Loss/tok 2.9294 (3.1416)	LR 1.250e-04
0: TRAIN [2][1840/3880]	Time 0.162 (0.181)	Data 1.06e-04 (4.97e-04)	Tok/s 32161 (38305)	Loss/tok 3.0625 (3.1413)	LR 1.250e-04
0: TRAIN [2][1850/3880]	Time 0.162 (0.181)	Data 9.89e-05 (4.94e-04)	Tok/s 31713 (38283)	Loss/tok 2.8912 (3.1411)	LR 1.250e-04
0: TRAIN [2][1860/3880]	Time 0.162 (0.181)	Data 9.75e-05 (4.92e-04)	Tok/s 31230 (38315)	Loss/tok 2.9469 (3.1420)	LR 1.250e-04
0: TRAIN [2][1870/3880]	Time 0.162 (0.181)	Data 1.12e-04 (4.90e-04)	Tok/s 31805 (38310)	Loss/tok 2.9717 (3.1421)	LR 1.250e-04
0: TRAIN [2][1880/3880]	Time 0.191 (0.181)	Data 1.03e-04 (4.88e-04)	Tok/s 44438 (38317)	Loss/tok 3.1473 (3.1422)	LR 1.250e-04
0: TRAIN [2][1890/3880]	Time 0.162 (0.181)	Data 8.46e-05 (4.87e-04)	Tok/s 32684 (38305)	Loss/tok 2.8388 (3.1418)	LR 1.250e-04
0: TRAIN [2][1900/3880]	Time 0.136 (0.181)	Data 9.92e-05 (4.85e-04)	Tok/s 19058 (38269)	Loss/tok 2.5685 (3.1413)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1910/3880]	Time 0.191 (0.181)	Data 8.13e-05 (4.82e-04)	Tok/s 44202 (38261)	Loss/tok 3.2436 (3.1412)	LR 1.250e-04
0: TRAIN [2][1920/3880]	Time 0.191 (0.181)	Data 9.25e-05 (4.80e-04)	Tok/s 43967 (38261)	Loss/tok 3.1666 (3.1413)	LR 1.250e-04
0: TRAIN [2][1930/3880]	Time 0.162 (0.181)	Data 7.82e-05 (4.78e-04)	Tok/s 31698 (38253)	Loss/tok 2.9692 (3.1410)	LR 1.250e-04
0: TRAIN [2][1940/3880]	Time 0.221 (0.181)	Data 8.06e-05 (4.76e-04)	Tok/s 52917 (38290)	Loss/tok 3.2656 (3.1413)	LR 1.250e-04
0: TRAIN [2][1950/3880]	Time 0.190 (0.181)	Data 1.26e-04 (4.75e-04)	Tok/s 43540 (38312)	Loss/tok 3.0460 (3.1414)	LR 1.250e-04
0: TRAIN [2][1960/3880]	Time 0.191 (0.181)	Data 1.09e-04 (4.73e-04)	Tok/s 43623 (38320)	Loss/tok 3.1003 (3.1418)	LR 1.250e-04
0: TRAIN [2][1970/3880]	Time 0.162 (0.181)	Data 9.68e-05 (4.71e-04)	Tok/s 30900 (38295)	Loss/tok 3.0832 (3.1413)	LR 1.250e-04
0: TRAIN [2][1980/3880]	Time 0.191 (0.181)	Data 1.01e-04 (4.69e-04)	Tok/s 45417 (38291)	Loss/tok 3.1544 (3.1411)	LR 1.250e-04
0: TRAIN [2][1990/3880]	Time 0.162 (0.181)	Data 8.30e-05 (4.67e-04)	Tok/s 32258 (38298)	Loss/tok 3.0375 (3.1411)	LR 1.250e-04
0: TRAIN [2][2000/3880]	Time 0.163 (0.181)	Data 8.58e-05 (4.65e-04)	Tok/s 32458 (38324)	Loss/tok 3.0077 (3.1413)	LR 1.250e-04
0: TRAIN [2][2010/3880]	Time 0.162 (0.181)	Data 9.25e-05 (4.63e-04)	Tok/s 32404 (38321)	Loss/tok 3.1464 (3.1413)	LR 1.250e-04
0: TRAIN [2][2020/3880]	Time 0.191 (0.181)	Data 1.01e-04 (4.62e-04)	Tok/s 43611 (38324)	Loss/tok 3.1249 (3.1411)	LR 1.250e-04
0: TRAIN [2][2030/3880]	Time 0.191 (0.181)	Data 7.92e-05 (4.60e-04)	Tok/s 43725 (38332)	Loss/tok 3.1745 (3.1411)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2040/3880]	Time 0.162 (0.181)	Data 9.11e-05 (4.58e-04)	Tok/s 31227 (38346)	Loss/tok 2.8677 (3.1415)	LR 1.250e-04
0: TRAIN [2][2050/3880]	Time 0.221 (0.181)	Data 8.20e-05 (4.56e-04)	Tok/s 53619 (38343)	Loss/tok 3.3213 (3.1416)	LR 1.250e-04
0: TRAIN [2][2060/3880]	Time 0.190 (0.181)	Data 1.28e-04 (4.55e-04)	Tok/s 43530 (38358)	Loss/tok 3.2224 (3.1420)	LR 1.250e-04
0: TRAIN [2][2070/3880]	Time 0.191 (0.181)	Data 1.26e-04 (4.53e-04)	Tok/s 43037 (38369)	Loss/tok 3.0172 (3.1420)	LR 1.250e-04
0: TRAIN [2][2080/3880]	Time 0.191 (0.181)	Data 1.22e-04 (4.51e-04)	Tok/s 43900 (38388)	Loss/tok 3.0913 (3.1419)	LR 1.250e-04
0: TRAIN [2][2090/3880]	Time 0.162 (0.181)	Data 1.09e-04 (4.50e-04)	Tok/s 32514 (38386)	Loss/tok 3.0069 (3.1417)	LR 1.250e-04
0: TRAIN [2][2100/3880]	Time 0.191 (0.181)	Data 1.12e-04 (4.48e-04)	Tok/s 44522 (38382)	Loss/tok 3.2070 (3.1417)	LR 1.250e-04
0: TRAIN [2][2110/3880]	Time 0.162 (0.181)	Data 1.12e-04 (4.46e-04)	Tok/s 32586 (38370)	Loss/tok 3.1004 (3.1414)	LR 1.250e-04
0: TRAIN [2][2120/3880]	Time 0.221 (0.181)	Data 7.87e-05 (4.45e-04)	Tok/s 52571 (38388)	Loss/tok 3.2939 (3.1415)	LR 1.250e-04
0: TRAIN [2][2130/3880]	Time 0.191 (0.181)	Data 1.22e-04 (4.43e-04)	Tok/s 43532 (38402)	Loss/tok 3.0703 (3.1419)	LR 1.250e-04
0: TRAIN [2][2140/3880]	Time 0.163 (0.181)	Data 1.01e-04 (4.42e-04)	Tok/s 31808 (38410)	Loss/tok 3.0100 (3.1421)	LR 1.250e-04
0: TRAIN [2][2150/3880]	Time 0.162 (0.181)	Data 1.25e-04 (4.40e-04)	Tok/s 31905 (38405)	Loss/tok 2.8416 (3.1418)	LR 1.250e-04
0: TRAIN [2][2160/3880]	Time 0.162 (0.181)	Data 1.22e-04 (4.39e-04)	Tok/s 31474 (38420)	Loss/tok 3.0252 (3.1419)	LR 1.250e-04
0: TRAIN [2][2170/3880]	Time 0.191 (0.181)	Data 1.18e-04 (4.37e-04)	Tok/s 43995 (38412)	Loss/tok 3.0772 (3.1417)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2180/3880]	Time 0.191 (0.181)	Data 8.18e-05 (4.36e-04)	Tok/s 43904 (38423)	Loss/tok 3.0222 (3.1416)	LR 1.250e-04
0: TRAIN [2][2190/3880]	Time 0.191 (0.181)	Data 4.21e-04 (4.35e-04)	Tok/s 44574 (38447)	Loss/tok 3.1180 (3.1416)	LR 1.250e-04
0: TRAIN [2][2200/3880]	Time 0.162 (0.181)	Data 1.00e-04 (4.33e-04)	Tok/s 32106 (38466)	Loss/tok 2.8349 (3.1416)	LR 1.250e-04
0: TRAIN [2][2210/3880]	Time 0.162 (0.181)	Data 8.61e-05 (4.31e-04)	Tok/s 32254 (38461)	Loss/tok 2.9982 (3.1415)	LR 1.250e-04
0: TRAIN [2][2220/3880]	Time 0.137 (0.181)	Data 5.87e-04 (4.30e-04)	Tok/s 19375 (38437)	Loss/tok 2.6527 (3.1410)	LR 1.250e-04
0: TRAIN [2][2230/3880]	Time 0.162 (0.181)	Data 9.73e-05 (4.29e-04)	Tok/s 32118 (38434)	Loss/tok 2.9254 (3.1407)	LR 1.250e-04
0: TRAIN [2][2240/3880]	Time 0.256 (0.181)	Data 1.08e-04 (4.27e-04)	Tok/s 58225 (38431)	Loss/tok 3.3286 (3.1405)	LR 1.250e-04
0: TRAIN [2][2250/3880]	Time 0.162 (0.181)	Data 1.02e-04 (4.26e-04)	Tok/s 31441 (38448)	Loss/tok 2.9716 (3.1406)	LR 1.250e-04
0: TRAIN [2][2260/3880]	Time 0.221 (0.181)	Data 1.10e-04 (4.24e-04)	Tok/s 52396 (38454)	Loss/tok 3.4210 (3.1411)	LR 1.250e-04
0: TRAIN [2][2270/3880]	Time 0.220 (0.181)	Data 9.54e-05 (4.23e-04)	Tok/s 52838 (38461)	Loss/tok 3.4123 (3.1413)	LR 1.250e-04
0: TRAIN [2][2280/3880]	Time 0.137 (0.181)	Data 9.51e-05 (4.22e-04)	Tok/s 19849 (38444)	Loss/tok 2.5612 (3.1412)	LR 1.250e-04
0: TRAIN [2][2290/3880]	Time 0.192 (0.181)	Data 9.54e-05 (4.21e-04)	Tok/s 43785 (38452)	Loss/tok 3.2089 (3.1412)	LR 1.250e-04
0: TRAIN [2][2300/3880]	Time 0.191 (0.181)	Data 7.80e-05 (4.19e-04)	Tok/s 43905 (38418)	Loss/tok 3.0788 (3.1405)	LR 1.250e-04
0: TRAIN [2][2310/3880]	Time 0.162 (0.181)	Data 9.70e-05 (4.18e-04)	Tok/s 32019 (38400)	Loss/tok 2.9443 (3.1398)	LR 1.250e-04
0: TRAIN [2][2320/3880]	Time 0.162 (0.181)	Data 7.80e-05 (4.16e-04)	Tok/s 31349 (38390)	Loss/tok 3.0101 (3.1396)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2330/3880]	Time 0.258 (0.181)	Data 8.68e-05 (4.15e-04)	Tok/s 57091 (38400)	Loss/tok 3.5189 (3.1403)	LR 1.250e-04
0: TRAIN [2][2340/3880]	Time 0.257 (0.181)	Data 8.01e-05 (4.14e-04)	Tok/s 58715 (38405)	Loss/tok 3.2497 (3.1401)	LR 1.250e-04
0: TRAIN [2][2350/3880]	Time 0.191 (0.181)	Data 9.11e-05 (4.12e-04)	Tok/s 44256 (38409)	Loss/tok 3.0237 (3.1400)	LR 1.250e-04
0: TRAIN [2][2360/3880]	Time 0.257 (0.181)	Data 7.96e-05 (4.11e-04)	Tok/s 57898 (38417)	Loss/tok 3.3545 (3.1400)	LR 1.250e-04
0: TRAIN [2][2370/3880]	Time 0.162 (0.181)	Data 1.05e-04 (4.09e-04)	Tok/s 32374 (38422)	Loss/tok 2.9944 (3.1402)	LR 1.250e-04
0: TRAIN [2][2380/3880]	Time 0.162 (0.181)	Data 8.25e-05 (4.08e-04)	Tok/s 31852 (38418)	Loss/tok 2.9267 (3.1400)	LR 1.250e-04
0: TRAIN [2][2390/3880]	Time 0.257 (0.181)	Data 8.06e-05 (4.07e-04)	Tok/s 57660 (38438)	Loss/tok 3.5036 (3.1405)	LR 1.250e-04
0: TRAIN [2][2400/3880]	Time 0.191 (0.181)	Data 1.03e-04 (4.06e-04)	Tok/s 44555 (38450)	Loss/tok 2.9557 (3.1409)	LR 1.250e-04
0: TRAIN [2][2410/3880]	Time 0.162 (0.181)	Data 5.03e-04 (4.04e-04)	Tok/s 32210 (38440)	Loss/tok 2.9761 (3.1408)	LR 1.250e-04
0: TRAIN [2][2420/3880]	Time 0.191 (0.181)	Data 9.23e-05 (4.03e-04)	Tok/s 44582 (38459)	Loss/tok 3.1365 (3.1410)	LR 1.250e-04
0: TRAIN [2][2430/3880]	Time 0.162 (0.181)	Data 8.68e-05 (4.02e-04)	Tok/s 32781 (38466)	Loss/tok 2.9403 (3.1416)	LR 1.250e-04
0: TRAIN [2][2440/3880]	Time 0.191 (0.181)	Data 6.47e-04 (4.01e-04)	Tok/s 44186 (38432)	Loss/tok 2.9890 (3.1411)	LR 1.250e-04
0: TRAIN [2][2450/3880]	Time 0.162 (0.181)	Data 1.28e-04 (4.00e-04)	Tok/s 31942 (38417)	Loss/tok 3.1422 (3.1409)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2460/3880]	Time 0.221 (0.181)	Data 1.46e-04 (3.99e-04)	Tok/s 52722 (38398)	Loss/tok 3.3682 (3.1406)	LR 1.250e-04
0: TRAIN [2][2470/3880]	Time 0.257 (0.181)	Data 1.21e-04 (3.98e-04)	Tok/s 57703 (38410)	Loss/tok 3.4946 (3.1413)	LR 1.250e-04
0: TRAIN [2][2480/3880]	Time 0.191 (0.181)	Data 1.05e-04 (3.97e-04)	Tok/s 44124 (38404)	Loss/tok 3.1748 (3.1411)	LR 1.250e-04
0: TRAIN [2][2490/3880]	Time 0.191 (0.181)	Data 1.45e-04 (3.96e-04)	Tok/s 43951 (38433)	Loss/tok 3.1675 (3.1417)	LR 1.250e-04
0: TRAIN [2][2500/3880]	Time 0.191 (0.181)	Data 8.65e-05 (3.94e-04)	Tok/s 43821 (38437)	Loss/tok 3.1433 (3.1417)	LR 1.250e-04
0: TRAIN [2][2510/3880]	Time 0.191 (0.181)	Data 1.04e-04 (3.93e-04)	Tok/s 43148 (38421)	Loss/tok 3.0797 (3.1415)	LR 1.250e-04
0: TRAIN [2][2520/3880]	Time 0.191 (0.181)	Data 9.92e-05 (3.92e-04)	Tok/s 43743 (38419)	Loss/tok 3.2059 (3.1415)	LR 1.250e-04
0: TRAIN [2][2530/3880]	Time 0.162 (0.181)	Data 8.73e-05 (3.91e-04)	Tok/s 31809 (38416)	Loss/tok 2.9959 (3.1414)	LR 1.250e-04
0: TRAIN [2][2540/3880]	Time 0.162 (0.181)	Data 1.10e-04 (3.90e-04)	Tok/s 32911 (38414)	Loss/tok 2.9529 (3.1412)	LR 1.250e-04
0: TRAIN [2][2550/3880]	Time 0.191 (0.181)	Data 9.92e-05 (3.89e-04)	Tok/s 44435 (38416)	Loss/tok 3.0315 (3.1410)	LR 1.250e-04
0: TRAIN [2][2560/3880]	Time 0.162 (0.181)	Data 9.66e-05 (3.88e-04)	Tok/s 31024 (38400)	Loss/tok 2.8699 (3.1408)	LR 1.250e-04
0: TRAIN [2][2570/3880]	Time 0.257 (0.181)	Data 1.40e-04 (3.87e-04)	Tok/s 57655 (38406)	Loss/tok 3.3254 (3.1408)	LR 1.250e-04
0: TRAIN [2][2580/3880]	Time 0.162 (0.181)	Data 1.09e-04 (3.86e-04)	Tok/s 31387 (38404)	Loss/tok 2.9889 (3.1409)	LR 1.250e-04
0: TRAIN [2][2590/3880]	Time 0.191 (0.181)	Data 1.15e-04 (3.85e-04)	Tok/s 43023 (38401)	Loss/tok 3.1857 (3.1411)	LR 1.250e-04
0: TRAIN [2][2600/3880]	Time 0.163 (0.181)	Data 9.39e-05 (3.84e-04)	Tok/s 31747 (38407)	Loss/tok 2.9887 (3.1411)	LR 1.250e-04
0: TRAIN [2][2610/3880]	Time 0.191 (0.181)	Data 1.12e-04 (3.83e-04)	Tok/s 44161 (38432)	Loss/tok 3.0951 (3.1412)	LR 1.250e-04
0: TRAIN [2][2620/3880]	Time 0.136 (0.181)	Data 1.37e-04 (3.81e-04)	Tok/s 19318 (38427)	Loss/tok 2.5948 (3.1410)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2630/3880]	Time 0.160 (0.181)	Data 8.87e-05 (3.80e-04)	Tok/s 32022 (38431)	Loss/tok 3.0151 (3.1408)	LR 1.250e-04
0: TRAIN [2][2640/3880]	Time 0.162 (0.181)	Data 9.92e-05 (3.79e-04)	Tok/s 32894 (38423)	Loss/tok 3.0256 (3.1405)	LR 1.250e-04
0: TRAIN [2][2650/3880]	Time 0.161 (0.181)	Data 1.23e-04 (3.78e-04)	Tok/s 31787 (38438)	Loss/tok 2.8442 (3.1408)	LR 1.250e-04
0: TRAIN [2][2660/3880]	Time 0.162 (0.181)	Data 9.66e-05 (3.77e-04)	Tok/s 31914 (38435)	Loss/tok 2.8431 (3.1405)	LR 1.250e-04
0: TRAIN [2][2670/3880]	Time 0.221 (0.181)	Data 1.03e-04 (3.77e-04)	Tok/s 52739 (38441)	Loss/tok 3.2547 (3.1405)	LR 1.250e-04
0: TRAIN [2][2680/3880]	Time 0.162 (0.181)	Data 1.21e-04 (3.75e-04)	Tok/s 31454 (38422)	Loss/tok 2.9894 (3.1400)	LR 1.250e-04
0: TRAIN [2][2690/3880]	Time 0.163 (0.181)	Data 1.12e-04 (3.74e-04)	Tok/s 32118 (38431)	Loss/tok 2.9729 (3.1401)	LR 1.250e-04
0: TRAIN [2][2700/3880]	Time 0.191 (0.181)	Data 9.49e-05 (3.73e-04)	Tok/s 43642 (38436)	Loss/tok 3.1923 (3.1399)	LR 1.250e-04
0: TRAIN [2][2710/3880]	Time 0.191 (0.181)	Data 9.51e-05 (3.73e-04)	Tok/s 43831 (38448)	Loss/tok 3.0693 (3.1401)	LR 1.250e-04
0: TRAIN [2][2720/3880]	Time 0.162 (0.181)	Data 8.75e-05 (3.71e-04)	Tok/s 31220 (38444)	Loss/tok 2.8715 (3.1401)	LR 1.250e-04
0: TRAIN [2][2730/3880]	Time 0.191 (0.181)	Data 1.23e-04 (3.70e-04)	Tok/s 44486 (38420)	Loss/tok 3.0501 (3.1396)	LR 1.250e-04
0: TRAIN [2][2740/3880]	Time 0.137 (0.181)	Data 1.20e-04 (3.69e-04)	Tok/s 19626 (38408)	Loss/tok 2.5798 (3.1395)	LR 1.250e-04
0: TRAIN [2][2750/3880]	Time 0.162 (0.181)	Data 9.97e-05 (3.69e-04)	Tok/s 31648 (38404)	Loss/tok 3.1397 (3.1394)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2760/3880]	Time 0.222 (0.181)	Data 8.15e-05 (3.68e-04)	Tok/s 52028 (38412)	Loss/tok 3.3371 (3.1393)	LR 1.250e-04
0: TRAIN [2][2770/3880]	Time 0.221 (0.181)	Data 1.29e-04 (3.67e-04)	Tok/s 53347 (38416)	Loss/tok 3.1524 (3.1390)	LR 1.250e-04
0: TRAIN [2][2780/3880]	Time 0.162 (0.181)	Data 9.92e-05 (3.66e-04)	Tok/s 33064 (38418)	Loss/tok 3.0373 (3.1390)	LR 1.250e-04
0: TRAIN [2][2790/3880]	Time 0.191 (0.181)	Data 9.58e-05 (3.65e-04)	Tok/s 43175 (38398)	Loss/tok 3.0981 (3.1386)	LR 1.250e-04
0: TRAIN [2][2800/3880]	Time 0.162 (0.181)	Data 1.08e-04 (3.64e-04)	Tok/s 31717 (38396)	Loss/tok 3.0288 (3.1383)	LR 1.250e-04
0: TRAIN [2][2810/3880]	Time 0.162 (0.181)	Data 1.19e-04 (3.63e-04)	Tok/s 32291 (38384)	Loss/tok 3.1015 (3.1381)	LR 1.250e-04
0: TRAIN [2][2820/3880]	Time 0.163 (0.181)	Data 1.10e-04 (3.63e-04)	Tok/s 32109 (38373)	Loss/tok 2.8844 (3.1379)	LR 1.250e-04
0: TRAIN [2][2830/3880]	Time 0.162 (0.181)	Data 9.78e-05 (3.62e-04)	Tok/s 32740 (38380)	Loss/tok 2.9639 (3.1378)	LR 1.250e-04
0: TRAIN [2][2840/3880]	Time 0.163 (0.181)	Data 1.40e-04 (3.61e-04)	Tok/s 31239 (38373)	Loss/tok 2.8613 (3.1380)	LR 1.250e-04
0: TRAIN [2][2850/3880]	Time 0.162 (0.181)	Data 1.15e-04 (3.60e-04)	Tok/s 31030 (38353)	Loss/tok 2.9250 (3.1375)	LR 1.250e-04
0: TRAIN [2][2860/3880]	Time 0.191 (0.181)	Data 2.43e-04 (3.60e-04)	Tok/s 44534 (38355)	Loss/tok 3.0923 (3.1373)	LR 1.250e-04
0: TRAIN [2][2870/3880]	Time 0.191 (0.181)	Data 1.22e-04 (3.59e-04)	Tok/s 43732 (38344)	Loss/tok 3.1132 (3.1370)	LR 1.250e-04
0: TRAIN [2][2880/3880]	Time 0.191 (0.181)	Data 1.10e-04 (3.58e-04)	Tok/s 44196 (38354)	Loss/tok 3.2045 (3.1373)	LR 1.250e-04
0: TRAIN [2][2890/3880]	Time 0.162 (0.181)	Data 1.27e-04 (3.57e-04)	Tok/s 32304 (38352)	Loss/tok 2.8726 (3.1372)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2900/3880]	Time 0.221 (0.181)	Data 1.35e-04 (3.57e-04)	Tok/s 52376 (38356)	Loss/tok 3.3375 (3.1375)	LR 1.250e-04
0: TRAIN [2][2910/3880]	Time 0.137 (0.181)	Data 1.03e-04 (3.56e-04)	Tok/s 18906 (38340)	Loss/tok 2.6227 (3.1374)	LR 1.250e-04
0: TRAIN [2][2920/3880]	Time 0.221 (0.181)	Data 1.38e-04 (3.55e-04)	Tok/s 52809 (38348)	Loss/tok 3.4216 (3.1375)	LR 1.250e-04
0: TRAIN [2][2930/3880]	Time 0.191 (0.181)	Data 1.08e-04 (3.55e-04)	Tok/s 43831 (38343)	Loss/tok 3.1670 (3.1374)	LR 1.250e-04
0: TRAIN [2][2940/3880]	Time 0.191 (0.181)	Data 1.26e-04 (3.54e-04)	Tok/s 44063 (38360)	Loss/tok 3.1113 (3.1377)	LR 1.250e-04
0: TRAIN [2][2950/3880]	Time 0.256 (0.181)	Data 1.17e-04 (3.53e-04)	Tok/s 57984 (38362)	Loss/tok 3.5197 (3.1377)	LR 1.250e-04
0: TRAIN [2][2960/3880]	Time 0.137 (0.181)	Data 1.13e-04 (3.52e-04)	Tok/s 18972 (38364)	Loss/tok 2.5945 (3.1374)	LR 1.250e-04
0: TRAIN [2][2970/3880]	Time 0.191 (0.181)	Data 8.34e-05 (3.51e-04)	Tok/s 43733 (38374)	Loss/tok 2.9622 (3.1373)	LR 1.250e-04
0: TRAIN [2][2980/3880]	Time 0.162 (0.181)	Data 1.05e-04 (3.51e-04)	Tok/s 31121 (38384)	Loss/tok 2.8918 (3.1375)	LR 1.250e-04
0: TRAIN [2][2990/3880]	Time 0.191 (0.181)	Data 1.06e-04 (3.50e-04)	Tok/s 43829 (38384)	Loss/tok 3.1683 (3.1376)	LR 1.250e-04
0: TRAIN [2][3000/3880]	Time 0.191 (0.181)	Data 1.04e-04 (3.49e-04)	Tok/s 43150 (38390)	Loss/tok 3.2112 (3.1375)	LR 1.250e-04
0: TRAIN [2][3010/3880]	Time 0.162 (0.181)	Data 1.08e-04 (3.48e-04)	Tok/s 32248 (38379)	Loss/tok 2.9715 (3.1375)	LR 1.250e-04
0: TRAIN [2][3020/3880]	Time 0.136 (0.181)	Data 1.49e-04 (3.48e-04)	Tok/s 18716 (38381)	Loss/tok 2.4940 (3.1378)	LR 1.250e-04
0: TRAIN [2][3030/3880]	Time 0.190 (0.181)	Data 9.54e-05 (3.47e-04)	Tok/s 44220 (38366)	Loss/tok 3.1329 (3.1374)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3040/3880]	Time 0.162 (0.181)	Data 9.20e-05 (3.46e-04)	Tok/s 31365 (38382)	Loss/tok 3.1255 (3.1382)	LR 1.250e-04
0: TRAIN [2][3050/3880]	Time 0.162 (0.181)	Data 1.82e-04 (3.45e-04)	Tok/s 31076 (38383)	Loss/tok 2.8976 (3.1381)	LR 1.250e-04
0: TRAIN [2][3060/3880]	Time 0.162 (0.181)	Data 9.94e-05 (3.45e-04)	Tok/s 31445 (38387)	Loss/tok 2.9839 (3.1381)	LR 1.250e-04
0: TRAIN [2][3070/3880]	Time 0.137 (0.181)	Data 8.82e-05 (3.44e-04)	Tok/s 18820 (38394)	Loss/tok 2.5643 (3.1380)	LR 1.250e-04
0: TRAIN [2][3080/3880]	Time 0.221 (0.181)	Data 7.46e-05 (3.44e-04)	Tok/s 53164 (38403)	Loss/tok 3.3568 (3.1380)	LR 1.250e-04
0: TRAIN [2][3090/3880]	Time 0.221 (0.181)	Data 1.17e-04 (3.43e-04)	Tok/s 52487 (38419)	Loss/tok 3.3488 (3.1385)	LR 1.250e-04
0: TRAIN [2][3100/3880]	Time 0.162 (0.181)	Data 8.87e-05 (3.42e-04)	Tok/s 32530 (38416)	Loss/tok 2.8380 (3.1382)	LR 1.250e-04
0: TRAIN [2][3110/3880]	Time 0.162 (0.181)	Data 8.23e-05 (3.41e-04)	Tok/s 30672 (38413)	Loss/tok 2.8987 (3.1384)	LR 1.250e-04
0: TRAIN [2][3120/3880]	Time 0.162 (0.181)	Data 9.70e-05 (3.41e-04)	Tok/s 31375 (38410)	Loss/tok 3.0020 (3.1382)	LR 1.250e-04
0: TRAIN [2][3130/3880]	Time 0.221 (0.181)	Data 1.07e-04 (3.40e-04)	Tok/s 52163 (38421)	Loss/tok 3.2703 (3.1382)	LR 1.250e-04
0: TRAIN [2][3140/3880]	Time 0.191 (0.181)	Data 1.02e-04 (3.39e-04)	Tok/s 44990 (38436)	Loss/tok 3.0829 (3.1385)	LR 1.250e-04
0: TRAIN [2][3150/3880]	Time 0.256 (0.181)	Data 4.02e-04 (3.38e-04)	Tok/s 59333 (38454)	Loss/tok 3.3247 (3.1390)	LR 1.250e-04
0: TRAIN [2][3160/3880]	Time 0.256 (0.181)	Data 1.00e-04 (3.38e-04)	Tok/s 57221 (38461)	Loss/tok 3.5139 (3.1390)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3170/3880]	Time 0.191 (0.181)	Data 9.94e-05 (3.37e-04)	Tok/s 43877 (38451)	Loss/tok 3.1730 (3.1389)	LR 1.250e-04
0: TRAIN [2][3180/3880]	Time 0.192 (0.181)	Data 9.78e-05 (3.36e-04)	Tok/s 44641 (38451)	Loss/tok 3.0443 (3.1388)	LR 1.250e-04
0: TRAIN [2][3190/3880]	Time 0.136 (0.181)	Data 9.61e-05 (3.36e-04)	Tok/s 18917 (38460)	Loss/tok 2.4801 (3.1392)	LR 1.250e-04
0: TRAIN [2][3200/3880]	Time 0.162 (0.181)	Data 1.32e-04 (3.35e-04)	Tok/s 31632 (38448)	Loss/tok 2.8536 (3.1389)	LR 1.250e-04
0: TRAIN [2][3210/3880]	Time 0.220 (0.181)	Data 1.06e-04 (3.34e-04)	Tok/s 53331 (38471)	Loss/tok 3.2040 (3.1395)	LR 1.250e-04
0: TRAIN [2][3220/3880]	Time 0.221 (0.181)	Data 7.87e-05 (3.33e-04)	Tok/s 52719 (38492)	Loss/tok 3.2510 (3.1397)	LR 1.250e-04
0: TRAIN [2][3230/3880]	Time 0.220 (0.181)	Data 1.22e-04 (3.33e-04)	Tok/s 53694 (38504)	Loss/tok 3.1569 (3.1401)	LR 1.250e-04
0: TRAIN [2][3240/3880]	Time 0.191 (0.181)	Data 7.75e-05 (3.32e-04)	Tok/s 44587 (38512)	Loss/tok 3.1140 (3.1401)	LR 1.250e-04
0: TRAIN [2][3250/3880]	Time 0.162 (0.181)	Data 7.65e-05 (3.31e-04)	Tok/s 31254 (38498)	Loss/tok 2.8522 (3.1397)	LR 1.250e-04
0: TRAIN [2][3260/3880]	Time 0.191 (0.181)	Data 4.33e-04 (3.31e-04)	Tok/s 43580 (38508)	Loss/tok 3.2466 (3.1396)	LR 1.250e-04
0: TRAIN [2][3270/3880]	Time 0.191 (0.181)	Data 1.66e-04 (3.30e-04)	Tok/s 44471 (38513)	Loss/tok 3.0771 (3.1395)	LR 1.250e-04
0: TRAIN [2][3280/3880]	Time 0.191 (0.182)	Data 3.85e-04 (3.30e-04)	Tok/s 43752 (38539)	Loss/tok 3.0334 (3.1398)	LR 1.250e-04
0: TRAIN [2][3290/3880]	Time 0.162 (0.182)	Data 8.13e-05 (3.29e-04)	Tok/s 31189 (38546)	Loss/tok 2.8700 (3.1398)	LR 1.250e-04
0: TRAIN [2][3300/3880]	Time 0.163 (0.182)	Data 3.67e-04 (3.28e-04)	Tok/s 32364 (38540)	Loss/tok 2.8697 (3.1395)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3310/3880]	Time 0.162 (0.181)	Data 9.30e-05 (3.28e-04)	Tok/s 31637 (38532)	Loss/tok 3.0243 (3.1394)	LR 1.250e-04
0: TRAIN [2][3320/3880]	Time 0.137 (0.181)	Data 8.37e-05 (3.27e-04)	Tok/s 19217 (38534)	Loss/tok 2.4761 (3.1394)	LR 1.250e-04
0: TRAIN [2][3330/3880]	Time 0.191 (0.181)	Data 1.55e-04 (3.26e-04)	Tok/s 44512 (38530)	Loss/tok 3.0040 (3.1395)	LR 1.250e-04
0: TRAIN [2][3340/3880]	Time 0.162 (0.181)	Data 8.27e-05 (3.26e-04)	Tok/s 31443 (38530)	Loss/tok 3.0348 (3.1393)	LR 1.250e-04
0: TRAIN [2][3350/3880]	Time 0.191 (0.182)	Data 1.01e-04 (3.25e-04)	Tok/s 44292 (38541)	Loss/tok 3.3289 (3.1392)	LR 1.250e-04
0: TRAIN [2][3360/3880]	Time 0.162 (0.181)	Data 8.15e-05 (3.25e-04)	Tok/s 31448 (38532)	Loss/tok 2.9431 (3.1391)	LR 1.250e-04
0: TRAIN [2][3370/3880]	Time 0.163 (0.182)	Data 2.83e-04 (3.24e-04)	Tok/s 32345 (38548)	Loss/tok 2.8806 (3.1394)	LR 1.250e-04
0: TRAIN [2][3380/3880]	Time 0.220 (0.182)	Data 8.11e-05 (3.23e-04)	Tok/s 52863 (38546)	Loss/tok 3.2382 (3.1392)	LR 1.250e-04
0: TRAIN [2][3390/3880]	Time 0.162 (0.181)	Data 8.23e-05 (3.23e-04)	Tok/s 32876 (38522)	Loss/tok 3.0305 (3.1389)	LR 1.250e-04
0: TRAIN [2][3400/3880]	Time 0.162 (0.181)	Data 8.32e-05 (3.22e-04)	Tok/s 32092 (38520)	Loss/tok 3.0417 (3.1387)	LR 1.250e-04
0: TRAIN [2][3410/3880]	Time 0.162 (0.181)	Data 1.01e-04 (3.21e-04)	Tok/s 30535 (38518)	Loss/tok 3.0434 (3.1386)	LR 1.250e-04
0: TRAIN [2][3420/3880]	Time 0.190 (0.181)	Data 8.01e-05 (3.21e-04)	Tok/s 43965 (38514)	Loss/tok 3.2566 (3.1386)	LR 1.250e-04
0: TRAIN [2][3430/3880]	Time 0.162 (0.181)	Data 8.13e-05 (3.20e-04)	Tok/s 32164 (38503)	Loss/tok 2.9920 (3.1384)	LR 1.250e-04
0: TRAIN [2][3440/3880]	Time 0.191 (0.181)	Data 8.23e-05 (3.19e-04)	Tok/s 43828 (38512)	Loss/tok 3.3383 (3.1387)	LR 1.250e-04
0: TRAIN [2][3450/3880]	Time 0.191 (0.181)	Data 7.94e-05 (3.19e-04)	Tok/s 43990 (38513)	Loss/tok 3.1620 (3.1386)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3460/3880]	Time 0.191 (0.181)	Data 8.63e-05 (3.18e-04)	Tok/s 44214 (38521)	Loss/tok 3.0848 (3.1386)	LR 1.250e-04
0: TRAIN [2][3470/3880]	Time 0.161 (0.181)	Data 1.36e-04 (3.17e-04)	Tok/s 32369 (38520)	Loss/tok 2.9376 (3.1387)	LR 1.250e-04
0: TRAIN [2][3480/3880]	Time 0.191 (0.181)	Data 8.34e-05 (3.17e-04)	Tok/s 44073 (38528)	Loss/tok 3.0790 (3.1389)	LR 1.250e-04
0: TRAIN [2][3490/3880]	Time 0.221 (0.181)	Data 9.47e-05 (3.16e-04)	Tok/s 53429 (38520)	Loss/tok 3.3811 (3.1388)	LR 1.250e-04
0: TRAIN [2][3500/3880]	Time 0.162 (0.181)	Data 8.77e-05 (3.16e-04)	Tok/s 32949 (38511)	Loss/tok 2.9320 (3.1387)	LR 1.250e-04
0: TRAIN [2][3510/3880]	Time 0.192 (0.181)	Data 7.82e-05 (3.15e-04)	Tok/s 43829 (38520)	Loss/tok 3.2465 (3.1387)	LR 1.250e-04
0: TRAIN [2][3520/3880]	Time 0.162 (0.181)	Data 9.70e-05 (3.14e-04)	Tok/s 32642 (38521)	Loss/tok 3.0233 (3.1385)	LR 1.250e-04
0: TRAIN [2][3530/3880]	Time 0.162 (0.181)	Data 8.77e-05 (3.14e-04)	Tok/s 32295 (38520)	Loss/tok 2.8720 (3.1386)	LR 1.250e-04
0: TRAIN [2][3540/3880]	Time 0.221 (0.181)	Data 1.58e-04 (3.13e-04)	Tok/s 52755 (38528)	Loss/tok 3.4486 (3.1387)	LR 1.250e-04
0: TRAIN [2][3550/3880]	Time 0.191 (0.181)	Data 1.20e-04 (3.13e-04)	Tok/s 44909 (38520)	Loss/tok 3.1922 (3.1385)	LR 1.250e-04
0: TRAIN [2][3560/3880]	Time 0.256 (0.181)	Data 8.65e-05 (3.12e-04)	Tok/s 57732 (38528)	Loss/tok 3.6339 (3.1389)	LR 1.250e-04
0: TRAIN [2][3570/3880]	Time 0.191 (0.181)	Data 4.22e-04 (3.12e-04)	Tok/s 44643 (38524)	Loss/tok 3.1100 (3.1388)	LR 1.250e-04
0: TRAIN [2][3580/3880]	Time 0.190 (0.181)	Data 1.36e-04 (3.11e-04)	Tok/s 44758 (38513)	Loss/tok 3.1223 (3.1385)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3590/3880]	Time 0.162 (0.181)	Data 3.29e-04 (3.11e-04)	Tok/s 32778 (38511)	Loss/tok 2.9582 (3.1384)	LR 1.250e-04
0: TRAIN [2][3600/3880]	Time 0.162 (0.181)	Data 9.56e-05 (3.10e-04)	Tok/s 31863 (38509)	Loss/tok 3.0296 (3.1382)	LR 1.250e-04
0: TRAIN [2][3610/3880]	Time 0.163 (0.181)	Data 1.11e-04 (3.10e-04)	Tok/s 31532 (38506)	Loss/tok 3.0064 (3.1380)	LR 1.250e-04
0: TRAIN [2][3620/3880]	Time 0.162 (0.181)	Data 8.06e-05 (3.09e-04)	Tok/s 33222 (38511)	Loss/tok 2.8396 (3.1382)	LR 1.250e-04
0: TRAIN [2][3630/3880]	Time 0.162 (0.181)	Data 1.07e-04 (3.09e-04)	Tok/s 31948 (38522)	Loss/tok 2.9013 (3.1385)	LR 1.250e-04
0: TRAIN [2][3640/3880]	Time 0.257 (0.181)	Data 1.10e-04 (3.08e-04)	Tok/s 59062 (38535)	Loss/tok 3.5335 (3.1388)	LR 1.250e-04
0: TRAIN [2][3650/3880]	Time 0.191 (0.181)	Data 4.13e-04 (3.08e-04)	Tok/s 43780 (38540)	Loss/tok 3.2170 (3.1388)	LR 1.250e-04
0: TRAIN [2][3660/3880]	Time 0.162 (0.181)	Data 8.96e-05 (3.07e-04)	Tok/s 31729 (38539)	Loss/tok 2.9696 (3.1389)	LR 1.250e-04
0: TRAIN [2][3670/3880]	Time 0.191 (0.181)	Data 1.28e-04 (3.06e-04)	Tok/s 43859 (38534)	Loss/tok 3.1654 (3.1386)	LR 1.250e-04
0: TRAIN [2][3680/3880]	Time 0.191 (0.181)	Data 8.44e-05 (3.06e-04)	Tok/s 43128 (38543)	Loss/tok 3.1593 (3.1387)	LR 1.250e-04
0: TRAIN [2][3690/3880]	Time 0.162 (0.181)	Data 1.07e-04 (3.05e-04)	Tok/s 32289 (38540)	Loss/tok 2.8959 (3.1386)	LR 1.250e-04
0: TRAIN [2][3700/3880]	Time 0.221 (0.181)	Data 9.04e-05 (3.05e-04)	Tok/s 51560 (38543)	Loss/tok 3.4524 (3.1387)	LR 1.250e-04
0: TRAIN [2][3710/3880]	Time 0.221 (0.182)	Data 1.04e-04 (3.04e-04)	Tok/s 53470 (38545)	Loss/tok 3.2896 (3.1388)	LR 1.250e-04
0: TRAIN [2][3720/3880]	Time 0.221 (0.182)	Data 1.31e-04 (3.04e-04)	Tok/s 51559 (38550)	Loss/tok 3.3191 (3.1389)	LR 1.250e-04
0: TRAIN [2][3730/3880]	Time 0.190 (0.181)	Data 1.24e-04 (3.03e-04)	Tok/s 43397 (38542)	Loss/tok 3.0834 (3.1388)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3740/3880]	Time 0.162 (0.182)	Data 8.44e-05 (3.03e-04)	Tok/s 32436 (38557)	Loss/tok 2.8808 (3.1393)	LR 1.250e-04
0: TRAIN [2][3750/3880]	Time 0.162 (0.182)	Data 8.32e-05 (3.02e-04)	Tok/s 31828 (38575)	Loss/tok 3.0497 (3.1399)	LR 1.250e-04
0: TRAIN [2][3760/3880]	Time 0.136 (0.182)	Data 8.30e-05 (3.02e-04)	Tok/s 19192 (38568)	Loss/tok 2.4924 (3.1400)	LR 1.250e-04
0: TRAIN [2][3770/3880]	Time 0.162 (0.182)	Data 1.08e-04 (3.01e-04)	Tok/s 31396 (38561)	Loss/tok 2.8387 (3.1398)	LR 1.250e-04
0: TRAIN [2][3780/3880]	Time 0.221 (0.182)	Data 7.96e-05 (3.01e-04)	Tok/s 53337 (38579)	Loss/tok 3.1722 (3.1403)	LR 1.250e-04
0: TRAIN [2][3790/3880]	Time 0.221 (0.182)	Data 2.29e-04 (3.00e-04)	Tok/s 52456 (38591)	Loss/tok 3.2589 (3.1406)	LR 1.250e-04
0: TRAIN [2][3800/3880]	Time 0.191 (0.182)	Data 9.35e-05 (3.00e-04)	Tok/s 44907 (38591)	Loss/tok 3.0768 (3.1404)	LR 1.250e-04
0: TRAIN [2][3810/3880]	Time 0.162 (0.182)	Data 9.25e-05 (2.99e-04)	Tok/s 32365 (38572)	Loss/tok 3.0302 (3.1402)	LR 1.250e-04
0: TRAIN [2][3820/3880]	Time 0.162 (0.182)	Data 8.06e-05 (2.98e-04)	Tok/s 31625 (38559)	Loss/tok 2.9850 (3.1399)	LR 1.250e-04
0: TRAIN [2][3830/3880]	Time 0.162 (0.182)	Data 1.05e-04 (2.98e-04)	Tok/s 32964 (38551)	Loss/tok 2.9930 (3.1396)	LR 1.250e-04
0: TRAIN [2][3840/3880]	Time 0.221 (0.182)	Data 8.87e-05 (2.97e-04)	Tok/s 52995 (38554)	Loss/tok 3.2484 (3.1396)	LR 1.250e-04
0: TRAIN [2][3850/3880]	Time 0.137 (0.182)	Data 1.15e-04 (2.97e-04)	Tok/s 19573 (38562)	Loss/tok 2.5306 (3.1397)	LR 1.250e-04
0: TRAIN [2][3860/3880]	Time 0.162 (0.182)	Data 8.08e-05 (2.97e-04)	Tok/s 31917 (38556)	Loss/tok 2.9701 (3.1394)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3870/3880]	Time 0.221 (0.182)	Data 8.39e-05 (2.96e-04)	Tok/s 53615 (38574)	Loss/tok 3.1187 (3.1396)	LR 1.250e-04
:::MLL 1571257006.922 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1571257006.922 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.651 (0.651)	Decoder iters 104.0 (104.0)	Tok/s 25344 (25344)
0: Running moses detokenizer
0: BLEU(score=23.733732237459023, counts=[37098, 18512, 10468, 6185], totals=[65779, 62776, 59773, 56775], precisions=[56.39793855181745, 29.4889766789856, 17.51292389540428, 10.893879348304711], bp=1.0, sys_len=65779, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571257008.807 eval_accuracy: {"value": 23.73, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1571257008.807 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.1395	Test BLEU: 23.73
0: Performance: Epoch: 2	Training: 308645 Tok/s
0: Finished epoch 2
:::MLL 1571257008.808 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1571257008.808 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571257008.808 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 227491537
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][0/3880]	Time 0.830 (0.830)	Data 6.93e-01 (6.93e-01)	Tok/s 3131 (3131)	Loss/tok 2.5563 (2.5563)	LR 1.250e-04
0: TRAIN [3][10/3880]	Time 0.162 (0.236)	Data 1.03e-04 (6.31e-02)	Tok/s 33168 (31164)	Loss/tok 2.8175 (3.0036)	LR 1.250e-04
0: TRAIN [3][20/3880]	Time 0.221 (0.218)	Data 1.08e-04 (3.31e-02)	Tok/s 53866 (37519)	Loss/tok 3.0628 (3.0958)	LR 1.250e-04
0: TRAIN [3][30/3880]	Time 0.162 (0.205)	Data 9.61e-05 (2.25e-02)	Tok/s 31935 (37421)	Loss/tok 2.9434 (3.0882)	LR 1.250e-04
0: TRAIN [3][40/3880]	Time 0.221 (0.199)	Data 9.97e-05 (1.70e-02)	Tok/s 51215 (37809)	Loss/tok 3.3822 (3.1013)	LR 1.250e-04
0: TRAIN [3][50/3880]	Time 0.191 (0.197)	Data 1.01e-04 (1.37e-02)	Tok/s 43828 (38416)	Loss/tok 3.1261 (3.1114)	LR 1.250e-04
0: TRAIN [3][60/3880]	Time 0.192 (0.196)	Data 1.04e-04 (1.15e-02)	Tok/s 43829 (39008)	Loss/tok 3.1126 (3.1149)	LR 1.250e-04
0: TRAIN [3][70/3880]	Time 0.191 (0.194)	Data 1.02e-04 (9.88e-03)	Tok/s 44392 (38928)	Loss/tok 3.1437 (3.1193)	LR 1.250e-04
0: TRAIN [3][80/3880]	Time 0.191 (0.192)	Data 8.34e-05 (8.67e-03)	Tok/s 44962 (38749)	Loss/tok 3.1427 (3.1158)	LR 1.250e-04
0: TRAIN [3][90/3880]	Time 0.191 (0.190)	Data 7.84e-05 (7.73e-03)	Tok/s 44113 (38370)	Loss/tok 2.9874 (3.1030)	LR 1.250e-04
0: TRAIN [3][100/3880]	Time 0.162 (0.189)	Data 1.17e-04 (6.97e-03)	Tok/s 31809 (38180)	Loss/tok 2.7507 (3.1036)	LR 1.250e-04
0: TRAIN [3][110/3880]	Time 0.191 (0.187)	Data 1.04e-04 (6.36e-03)	Tok/s 44654 (38058)	Loss/tok 2.9846 (3.0973)	LR 1.250e-04
0: TRAIN [3][120/3880]	Time 0.136 (0.186)	Data 1.03e-04 (5.85e-03)	Tok/s 19376 (37830)	Loss/tok 2.5303 (3.0996)	LR 1.250e-04
0: TRAIN [3][130/3880]	Time 0.191 (0.187)	Data 8.44e-05 (5.41e-03)	Tok/s 43530 (38191)	Loss/tok 3.2363 (3.1064)	LR 1.250e-04
0: TRAIN [3][140/3880]	Time 0.192 (0.185)	Data 1.03e-04 (5.03e-03)	Tok/s 44931 (37755)	Loss/tok 2.9494 (3.0957)	LR 1.250e-04
0: TRAIN [3][150/3880]	Time 0.162 (0.185)	Data 8.01e-05 (4.71e-03)	Tok/s 32259 (37980)	Loss/tok 2.8919 (3.0929)	LR 1.250e-04
0: TRAIN [3][160/3880]	Time 0.191 (0.185)	Data 8.27e-05 (4.42e-03)	Tok/s 44460 (38077)	Loss/tok 3.1200 (3.0929)	LR 1.250e-04
0: TRAIN [3][170/3880]	Time 0.191 (0.185)	Data 8.63e-05 (4.17e-03)	Tok/s 43556 (38014)	Loss/tok 3.1908 (3.0907)	LR 1.250e-04
0: TRAIN [3][180/3880]	Time 0.162 (0.184)	Data 9.51e-05 (3.94e-03)	Tok/s 31747 (37800)	Loss/tok 2.6895 (3.0866)	LR 1.250e-04
0: TRAIN [3][190/3880]	Time 0.257 (0.185)	Data 9.13e-05 (3.74e-03)	Tok/s 56991 (38163)	Loss/tok 3.4951 (3.0966)	LR 1.250e-04
0: TRAIN [3][200/3880]	Time 0.222 (0.185)	Data 1.10e-04 (3.56e-03)	Tok/s 52001 (38338)	Loss/tok 3.4297 (3.1014)	LR 1.250e-04
0: TRAIN [3][210/3880]	Time 0.191 (0.184)	Data 1.09e-04 (3.40e-03)	Tok/s 45368 (38241)	Loss/tok 3.1751 (3.1008)	LR 1.250e-04
0: TRAIN [3][220/3880]	Time 0.162 (0.185)	Data 1.03e-04 (3.25e-03)	Tok/s 31162 (38642)	Loss/tok 2.8701 (3.1093)	LR 1.250e-04
0: TRAIN [3][230/3880]	Time 0.191 (0.186)	Data 1.05e-04 (3.12e-03)	Tok/s 43723 (38807)	Loss/tok 3.0789 (3.1132)	LR 1.250e-04
0: TRAIN [3][240/3880]	Time 0.162 (0.185)	Data 1.07e-04 (2.99e-03)	Tok/s 31564 (38744)	Loss/tok 3.0017 (3.1117)	LR 1.250e-04
0: TRAIN [3][250/3880]	Time 0.162 (0.185)	Data 8.25e-05 (2.87e-03)	Tok/s 32099 (38764)	Loss/tok 3.0116 (3.1120)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][260/3880]	Time 0.191 (0.185)	Data 1.07e-04 (2.77e-03)	Tok/s 44850 (38625)	Loss/tok 3.0997 (3.1091)	LR 1.250e-04
0: TRAIN [3][270/3880]	Time 0.162 (0.184)	Data 7.94e-05 (2.67e-03)	Tok/s 31273 (38545)	Loss/tok 2.9297 (3.1068)	LR 1.250e-04
0: TRAIN [3][280/3880]	Time 0.163 (0.184)	Data 8.25e-05 (2.58e-03)	Tok/s 30944 (38484)	Loss/tok 2.9609 (3.1075)	LR 1.250e-04
0: TRAIN [3][290/3880]	Time 0.221 (0.184)	Data 8.44e-05 (2.49e-03)	Tok/s 51902 (38571)	Loss/tok 3.4762 (3.1108)	LR 1.250e-04
0: TRAIN [3][300/3880]	Time 0.221 (0.184)	Data 1.33e-04 (2.41e-03)	Tok/s 53164 (38529)	Loss/tok 3.4790 (3.1122)	LR 1.250e-04
0: TRAIN [3][310/3880]	Time 0.191 (0.184)	Data 8.49e-05 (2.34e-03)	Tok/s 43639 (38571)	Loss/tok 3.1386 (3.1099)	LR 1.250e-04
0: TRAIN [3][320/3880]	Time 0.191 (0.185)	Data 7.99e-05 (2.27e-03)	Tok/s 43919 (38786)	Loss/tok 3.0683 (3.1102)	LR 1.250e-04
0: TRAIN [3][330/3880]	Time 0.191 (0.185)	Data 7.87e-05 (2.20e-03)	Tok/s 44179 (38976)	Loss/tok 3.2379 (3.1127)	LR 1.250e-04
0: TRAIN [3][340/3880]	Time 0.163 (0.185)	Data 1.06e-04 (2.14e-03)	Tok/s 32207 (38940)	Loss/tok 3.0346 (3.1124)	LR 1.250e-04
0: TRAIN [3][350/3880]	Time 0.136 (0.185)	Data 8.06e-05 (2.08e-03)	Tok/s 19272 (38950)	Loss/tok 2.4180 (3.1141)	LR 1.250e-04
0: TRAIN [3][360/3880]	Time 0.162 (0.185)	Data 1.19e-04 (2.03e-03)	Tok/s 31080 (38894)	Loss/tok 2.8164 (3.1133)	LR 1.250e-04
0: TRAIN [3][370/3880]	Time 0.191 (0.184)	Data 7.94e-05 (1.98e-03)	Tok/s 45832 (38795)	Loss/tok 3.1707 (3.1121)	LR 1.250e-04
0: TRAIN [3][380/3880]	Time 0.162 (0.184)	Data 9.25e-05 (1.93e-03)	Tok/s 31844 (38549)	Loss/tok 2.8698 (3.1086)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][390/3880]	Time 0.136 (0.183)	Data 8.11e-05 (1.88e-03)	Tok/s 19593 (38570)	Loss/tok 2.4815 (3.1084)	LR 1.250e-04
0: TRAIN [3][400/3880]	Time 0.162 (0.183)	Data 9.54e-05 (1.84e-03)	Tok/s 31825 (38486)	Loss/tok 2.8755 (3.1068)	LR 1.250e-04
0: TRAIN [3][410/3880]	Time 0.162 (0.183)	Data 7.70e-05 (1.79e-03)	Tok/s 32245 (38504)	Loss/tok 2.9260 (3.1072)	LR 1.250e-04
0: TRAIN [3][420/3880]	Time 0.190 (0.184)	Data 8.82e-05 (1.75e-03)	Tok/s 44374 (38665)	Loss/tok 3.1820 (3.1121)	LR 1.250e-04
0: TRAIN [3][430/3880]	Time 0.162 (0.183)	Data 7.39e-05 (1.71e-03)	Tok/s 32400 (38565)	Loss/tok 2.9345 (3.1094)	LR 1.250e-04
0: TRAIN [3][440/3880]	Time 0.162 (0.183)	Data 7.75e-05 (1.68e-03)	Tok/s 31474 (38594)	Loss/tok 2.8607 (3.1105)	LR 1.250e-04
0: TRAIN [3][450/3880]	Time 0.162 (0.183)	Data 7.89e-05 (1.64e-03)	Tok/s 32483 (38522)	Loss/tok 2.8685 (3.1087)	LR 1.250e-04
0: TRAIN [3][460/3880]	Time 0.136 (0.183)	Data 7.84e-05 (1.61e-03)	Tok/s 19262 (38391)	Loss/tok 2.6641 (3.1069)	LR 1.250e-04
0: TRAIN [3][470/3880]	Time 0.162 (0.183)	Data 9.23e-05 (1.58e-03)	Tok/s 31317 (38415)	Loss/tok 2.7503 (3.1067)	LR 1.250e-04
0: TRAIN [3][480/3880]	Time 0.191 (0.183)	Data 9.39e-05 (1.55e-03)	Tok/s 43734 (38491)	Loss/tok 3.1407 (3.1088)	LR 1.250e-04
0: TRAIN [3][490/3880]	Time 0.191 (0.183)	Data 7.75e-05 (1.52e-03)	Tok/s 43835 (38467)	Loss/tok 3.1382 (3.1080)	LR 1.250e-04
0: TRAIN [3][500/3880]	Time 0.162 (0.183)	Data 7.75e-05 (1.49e-03)	Tok/s 31170 (38479)	Loss/tok 2.9863 (3.1085)	LR 1.250e-04
0: TRAIN [3][510/3880]	Time 0.191 (0.182)	Data 7.41e-05 (1.46e-03)	Tok/s 43957 (38415)	Loss/tok 3.2274 (3.1072)	LR 1.250e-04
0: TRAIN [3][520/3880]	Time 0.191 (0.183)	Data 7.75e-05 (1.44e-03)	Tok/s 43876 (38426)	Loss/tok 3.2122 (3.1071)	LR 1.250e-04
0: TRAIN [3][530/3880]	Time 0.137 (0.182)	Data 1.20e-04 (1.41e-03)	Tok/s 18318 (38412)	Loss/tok 2.5072 (3.1049)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][540/3880]	Time 0.136 (0.182)	Data 8.92e-05 (1.39e-03)	Tok/s 19223 (38262)	Loss/tok 2.3663 (3.1032)	LR 1.250e-04
0: TRAIN [3][550/3880]	Time 0.136 (0.182)	Data 9.01e-05 (1.36e-03)	Tok/s 20018 (38149)	Loss/tok 2.6400 (3.1010)	LR 1.250e-04
0: TRAIN [3][560/3880]	Time 0.162 (0.182)	Data 8.56e-05 (1.34e-03)	Tok/s 32202 (38158)	Loss/tok 2.9259 (3.1010)	LR 1.250e-04
0: TRAIN [3][570/3880]	Time 0.162 (0.182)	Data 9.04e-05 (1.32e-03)	Tok/s 31634 (38125)	Loss/tok 2.9658 (3.1003)	LR 1.250e-04
0: TRAIN [3][580/3880]	Time 0.162 (0.181)	Data 9.42e-05 (1.30e-03)	Tok/s 31698 (38075)	Loss/tok 2.9264 (3.1019)	LR 1.250e-04
0: TRAIN [3][590/3880]	Time 0.137 (0.182)	Data 1.48e-04 (1.28e-03)	Tok/s 18768 (38124)	Loss/tok 2.5358 (3.1043)	LR 1.250e-04
0: TRAIN [3][600/3880]	Time 0.162 (0.182)	Data 8.30e-05 (1.26e-03)	Tok/s 32581 (38201)	Loss/tok 2.9436 (3.1048)	LR 1.250e-04
0: TRAIN [3][610/3880]	Time 0.191 (0.182)	Data 1.03e-04 (1.24e-03)	Tok/s 43307 (38202)	Loss/tok 3.2216 (3.1053)	LR 1.250e-04
0: TRAIN [3][620/3880]	Time 0.162 (0.182)	Data 1.19e-04 (1.22e-03)	Tok/s 32148 (38236)	Loss/tok 2.8868 (3.1051)	LR 1.250e-04
0: TRAIN [3][630/3880]	Time 0.136 (0.182)	Data 7.70e-05 (1.21e-03)	Tok/s 19839 (38259)	Loss/tok 2.4820 (3.1053)	LR 1.250e-04
0: TRAIN [3][640/3880]	Time 0.257 (0.182)	Data 7.87e-05 (1.19e-03)	Tok/s 57529 (38300)	Loss/tok 3.5485 (3.1071)	LR 1.250e-04
0: TRAIN [3][650/3880]	Time 0.162 (0.182)	Data 1.27e-04 (1.17e-03)	Tok/s 32691 (38298)	Loss/tok 3.0859 (3.1076)	LR 1.250e-04
0: TRAIN [3][660/3880]	Time 0.163 (0.182)	Data 1.04e-04 (1.16e-03)	Tok/s 31974 (38349)	Loss/tok 2.9222 (3.1086)	LR 1.250e-04
0: TRAIN [3][670/3880]	Time 0.162 (0.182)	Data 7.84e-05 (1.14e-03)	Tok/s 32347 (38295)	Loss/tok 2.8477 (3.1073)	LR 1.250e-04
0: TRAIN [3][680/3880]	Time 0.162 (0.182)	Data 7.63e-05 (1.12e-03)	Tok/s 31684 (38281)	Loss/tok 2.8621 (3.1061)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][690/3880]	Time 0.221 (0.182)	Data 8.23e-05 (1.11e-03)	Tok/s 52555 (38300)	Loss/tok 3.2406 (3.1077)	LR 1.250e-04
0: TRAIN [3][700/3880]	Time 0.191 (0.182)	Data 9.16e-05 (1.10e-03)	Tok/s 44078 (38347)	Loss/tok 3.0996 (3.1099)	LR 1.250e-04
0: TRAIN [3][710/3880]	Time 0.136 (0.182)	Data 1.02e-04 (1.08e-03)	Tok/s 19326 (38317)	Loss/tok 2.4939 (3.1088)	LR 1.250e-04
0: TRAIN [3][720/3880]	Time 0.162 (0.182)	Data 9.54e-05 (1.07e-03)	Tok/s 31371 (38272)	Loss/tok 2.8250 (3.1081)	LR 1.250e-04
0: TRAIN [3][730/3880]	Time 0.191 (0.182)	Data 9.39e-05 (1.05e-03)	Tok/s 43565 (38371)	Loss/tok 3.0606 (3.1096)	LR 1.250e-04
0: TRAIN [3][740/3880]	Time 0.221 (0.182)	Data 1.11e-04 (1.04e-03)	Tok/s 52021 (38402)	Loss/tok 3.3024 (3.1106)	LR 1.250e-04
0: TRAIN [3][750/3880]	Time 0.191 (0.182)	Data 8.42e-05 (1.03e-03)	Tok/s 43438 (38399)	Loss/tok 3.0428 (3.1108)	LR 1.250e-04
0: TRAIN [3][760/3880]	Time 0.162 (0.182)	Data 7.44e-05 (1.02e-03)	Tok/s 32458 (38356)	Loss/tok 2.8034 (3.1093)	LR 1.250e-04
0: TRAIN [3][770/3880]	Time 0.162 (0.182)	Data 7.39e-05 (1.00e-03)	Tok/s 31952 (38372)	Loss/tok 2.8741 (3.1094)	LR 1.250e-04
0: TRAIN [3][780/3880]	Time 0.191 (0.182)	Data 7.51e-05 (9.93e-04)	Tok/s 44702 (38399)	Loss/tok 3.0994 (3.1098)	LR 1.250e-04
0: TRAIN [3][790/3880]	Time 0.162 (0.182)	Data 7.27e-05 (9.82e-04)	Tok/s 30525 (38328)	Loss/tok 2.8504 (3.1081)	LR 1.250e-04
0: TRAIN [3][800/3880]	Time 0.220 (0.182)	Data 7.99e-05 (9.71e-04)	Tok/s 53715 (38305)	Loss/tok 3.2019 (3.1083)	LR 1.250e-04
0: TRAIN [3][810/3880]	Time 0.162 (0.182)	Data 8.20e-05 (9.59e-04)	Tok/s 32517 (38282)	Loss/tok 2.9054 (3.1080)	LR 1.250e-04
0: TRAIN [3][820/3880]	Time 0.162 (0.182)	Data 8.80e-05 (9.49e-04)	Tok/s 32776 (38291)	Loss/tok 2.9561 (3.1080)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][830/3880]	Time 0.162 (0.182)	Data 9.85e-05 (9.38e-04)	Tok/s 30986 (38266)	Loss/tok 2.8296 (3.1074)	LR 1.250e-04
0: TRAIN [3][840/3880]	Time 0.191 (0.182)	Data 8.03e-05 (9.28e-04)	Tok/s 44682 (38261)	Loss/tok 2.9848 (3.1066)	LR 1.250e-04
0: TRAIN [3][850/3880]	Time 0.162 (0.182)	Data 9.32e-05 (9.18e-04)	Tok/s 32116 (38304)	Loss/tok 3.0114 (3.1078)	LR 1.250e-04
0: TRAIN [3][860/3880]	Time 0.191 (0.182)	Data 7.82e-05 (9.09e-04)	Tok/s 43915 (38332)	Loss/tok 3.0327 (3.1079)	LR 1.250e-04
0: TRAIN [3][870/3880]	Time 0.191 (0.182)	Data 7.56e-05 (8.99e-04)	Tok/s 43716 (38282)	Loss/tok 3.1816 (3.1079)	LR 1.250e-04
0: TRAIN [3][880/3880]	Time 0.137 (0.182)	Data 1.10e-04 (8.90e-04)	Tok/s 19623 (38222)	Loss/tok 2.4587 (3.1067)	LR 1.250e-04
0: TRAIN [3][890/3880]	Time 0.191 (0.182)	Data 7.49e-05 (8.81e-04)	Tok/s 44012 (38242)	Loss/tok 3.2122 (3.1070)	LR 1.250e-04
0: TRAIN [3][900/3880]	Time 0.191 (0.182)	Data 7.77e-05 (8.72e-04)	Tok/s 42999 (38286)	Loss/tok 3.1347 (3.1071)	LR 1.250e-04
0: TRAIN [3][910/3880]	Time 0.163 (0.182)	Data 8.11e-05 (8.63e-04)	Tok/s 31075 (38321)	Loss/tok 2.8443 (3.1073)	LR 1.250e-04
0: TRAIN [3][920/3880]	Time 0.221 (0.182)	Data 8.06e-05 (8.55e-04)	Tok/s 52730 (38390)	Loss/tok 3.2025 (3.1074)	LR 1.250e-04
0: TRAIN [3][930/3880]	Time 0.191 (0.182)	Data 8.15e-05 (8.47e-04)	Tok/s 43258 (38426)	Loss/tok 3.0308 (3.1078)	LR 1.250e-04
0: TRAIN [3][940/3880]	Time 0.191 (0.182)	Data 8.06e-05 (8.39e-04)	Tok/s 44333 (38394)	Loss/tok 3.2117 (3.1071)	LR 1.250e-04
0: TRAIN [3][950/3880]	Time 0.192 (0.182)	Data 7.92e-05 (8.31e-04)	Tok/s 44028 (38385)	Loss/tok 3.0587 (3.1066)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][960/3880]	Time 0.222 (0.182)	Data 9.25e-05 (8.23e-04)	Tok/s 52528 (38439)	Loss/tok 3.2785 (3.1087)	LR 1.250e-04
0: TRAIN [3][970/3880]	Time 0.162 (0.182)	Data 7.77e-05 (8.16e-04)	Tok/s 31235 (38416)	Loss/tok 3.0855 (3.1098)	LR 1.250e-04
0: TRAIN [3][980/3880]	Time 0.191 (0.182)	Data 7.63e-05 (8.09e-04)	Tok/s 45065 (38410)	Loss/tok 3.0684 (3.1100)	LR 1.250e-04
0: TRAIN [3][990/3880]	Time 0.162 (0.182)	Data 8.11e-05 (8.02e-04)	Tok/s 31442 (38432)	Loss/tok 2.9262 (3.1101)	LR 1.250e-04
0: TRAIN [3][1000/3880]	Time 0.191 (0.182)	Data 1.02e-04 (7.95e-04)	Tok/s 43366 (38456)	Loss/tok 3.0220 (3.1101)	LR 1.250e-04
0: TRAIN [3][1010/3880]	Time 0.162 (0.182)	Data 7.61e-05 (7.88e-04)	Tok/s 31553 (38410)	Loss/tok 2.8861 (3.1092)	LR 1.250e-04
0: TRAIN [3][1020/3880]	Time 0.191 (0.182)	Data 9.01e-05 (7.81e-04)	Tok/s 44930 (38433)	Loss/tok 3.1812 (3.1092)	LR 1.250e-04
0: TRAIN [3][1030/3880]	Time 0.192 (0.182)	Data 8.23e-05 (7.74e-04)	Tok/s 44330 (38430)	Loss/tok 3.0391 (3.1086)	LR 1.250e-04
0: TRAIN [3][1040/3880]	Time 0.258 (0.182)	Data 1.18e-04 (7.68e-04)	Tok/s 56738 (38446)	Loss/tok 3.4516 (3.1090)	LR 1.250e-04
0: TRAIN [3][1050/3880]	Time 0.191 (0.182)	Data 1.12e-04 (7.62e-04)	Tok/s 44134 (38480)	Loss/tok 3.1430 (3.1100)	LR 1.250e-04
0: TRAIN [3][1060/3880]	Time 0.191 (0.182)	Data 9.08e-05 (7.55e-04)	Tok/s 44219 (38469)	Loss/tok 3.2989 (3.1096)	LR 1.250e-04
0: TRAIN [3][1070/3880]	Time 0.191 (0.182)	Data 7.68e-05 (7.49e-04)	Tok/s 43721 (38484)	Loss/tok 2.9838 (3.1096)	LR 1.250e-04
0: TRAIN [3][1080/3880]	Time 0.191 (0.182)	Data 7.65e-05 (7.43e-04)	Tok/s 43443 (38509)	Loss/tok 2.9390 (3.1096)	LR 1.250e-04
0: TRAIN [3][1090/3880]	Time 0.162 (0.182)	Data 1.37e-04 (7.37e-04)	Tok/s 31578 (38547)	Loss/tok 2.9052 (3.1106)	LR 1.250e-04
0: TRAIN [3][1100/3880]	Time 0.162 (0.182)	Data 9.75e-05 (7.33e-04)	Tok/s 31647 (38553)	Loss/tok 2.9692 (3.1103)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1110/3880]	Time 0.221 (0.182)	Data 7.65e-05 (7.27e-04)	Tok/s 53903 (38559)	Loss/tok 3.1987 (3.1107)	LR 1.250e-04
0: TRAIN [3][1120/3880]	Time 0.221 (0.182)	Data 7.92e-05 (7.22e-04)	Tok/s 53644 (38607)	Loss/tok 3.0537 (3.1113)	LR 1.250e-04
0: TRAIN [3][1130/3880]	Time 0.162 (0.182)	Data 9.32e-05 (7.16e-04)	Tok/s 32360 (38621)	Loss/tok 2.9117 (3.1113)	LR 1.250e-04
0: TRAIN [3][1140/3880]	Time 0.162 (0.182)	Data 7.75e-05 (7.11e-04)	Tok/s 31489 (38567)	Loss/tok 2.8933 (3.1103)	LR 1.250e-04
0: TRAIN [3][1150/3880]	Time 0.221 (0.182)	Data 1.06e-04 (7.05e-04)	Tok/s 53198 (38567)	Loss/tok 3.2108 (3.1102)	LR 1.250e-04
0: TRAIN [3][1160/3880]	Time 0.162 (0.182)	Data 7.68e-05 (7.00e-04)	Tok/s 32031 (38560)	Loss/tok 3.0617 (3.1100)	LR 1.250e-04
0: TRAIN [3][1170/3880]	Time 0.162 (0.182)	Data 8.23e-05 (6.95e-04)	Tok/s 31335 (38564)	Loss/tok 3.0025 (3.1100)	LR 1.250e-04
0: TRAIN [3][1180/3880]	Time 0.136 (0.182)	Data 9.08e-05 (6.90e-04)	Tok/s 19305 (38559)	Loss/tok 2.6224 (3.1109)	LR 1.250e-04
0: TRAIN [3][1190/3880]	Time 0.191 (0.182)	Data 7.80e-05 (6.86e-04)	Tok/s 43505 (38542)	Loss/tok 3.1485 (3.1109)	LR 1.250e-04
0: TRAIN [3][1200/3880]	Time 0.191 (0.182)	Data 7.92e-05 (6.81e-04)	Tok/s 44372 (38582)	Loss/tok 3.0405 (3.1113)	LR 1.250e-04
0: TRAIN [3][1210/3880]	Time 0.191 (0.182)	Data 7.58e-05 (6.77e-04)	Tok/s 43689 (38580)	Loss/tok 3.0934 (3.1112)	LR 1.250e-04
0: TRAIN [3][1220/3880]	Time 0.162 (0.182)	Data 1.49e-04 (6.72e-04)	Tok/s 33313 (38586)	Loss/tok 2.9456 (3.1112)	LR 1.250e-04
0: TRAIN [3][1230/3880]	Time 0.191 (0.182)	Data 1.57e-04 (6.68e-04)	Tok/s 43645 (38598)	Loss/tok 3.0647 (3.1110)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1240/3880]	Time 0.158 (0.182)	Data 1.08e-04 (6.64e-04)	Tok/s 32789 (38599)	Loss/tok 2.9135 (3.1110)	LR 1.250e-04
0: TRAIN [3][1250/3880]	Time 0.162 (0.182)	Data 1.66e-04 (6.59e-04)	Tok/s 32456 (38616)	Loss/tok 3.0090 (3.1107)	LR 1.250e-04
0: TRAIN [3][1260/3880]	Time 0.162 (0.182)	Data 1.41e-04 (6.55e-04)	Tok/s 31517 (38601)	Loss/tok 2.8931 (3.1108)	LR 1.250e-04
0: TRAIN [3][1270/3880]	Time 0.162 (0.182)	Data 7.84e-05 (6.51e-04)	Tok/s 32536 (38574)	Loss/tok 2.8420 (3.1101)	LR 1.250e-04
0: TRAIN [3][1280/3880]	Time 0.162 (0.182)	Data 9.25e-05 (6.46e-04)	Tok/s 32615 (38610)	Loss/tok 2.8052 (3.1105)	LR 1.250e-04
0: TRAIN [3][1290/3880]	Time 0.162 (0.182)	Data 8.75e-05 (6.42e-04)	Tok/s 31597 (38622)	Loss/tok 3.0342 (3.1102)	LR 1.250e-04
0: TRAIN [3][1300/3880]	Time 0.162 (0.182)	Data 8.94e-05 (6.38e-04)	Tok/s 31889 (38677)	Loss/tok 3.0017 (3.1109)	LR 1.250e-04
0: TRAIN [3][1310/3880]	Time 0.162 (0.182)	Data 8.37e-05 (6.34e-04)	Tok/s 32267 (38681)	Loss/tok 2.8601 (3.1117)	LR 1.250e-04
0: TRAIN [3][1320/3880]	Time 0.191 (0.182)	Data 8.08e-05 (6.30e-04)	Tok/s 43344 (38657)	Loss/tok 3.0648 (3.1115)	LR 1.250e-04
0: TRAIN [3][1330/3880]	Time 0.162 (0.182)	Data 7.53e-05 (6.26e-04)	Tok/s 31397 (38659)	Loss/tok 2.8636 (3.1111)	LR 1.250e-04
0: TRAIN [3][1340/3880]	Time 0.221 (0.182)	Data 7.41e-05 (6.22e-04)	Tok/s 52287 (38675)	Loss/tok 3.3727 (3.1116)	LR 1.250e-04
0: TRAIN [3][1350/3880]	Time 0.221 (0.182)	Data 8.30e-05 (6.18e-04)	Tok/s 52796 (38682)	Loss/tok 3.1858 (3.1115)	LR 1.250e-04
0: TRAIN [3][1360/3880]	Time 0.136 (0.182)	Data 7.65e-05 (6.14e-04)	Tok/s 19687 (38664)	Loss/tok 2.5427 (3.1110)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1370/3880]	Time 0.221 (0.182)	Data 9.37e-05 (6.10e-04)	Tok/s 52632 (38654)	Loss/tok 3.2745 (3.1108)	LR 1.250e-04
0: TRAIN [3][1380/3880]	Time 0.136 (0.182)	Data 8.70e-05 (6.07e-04)	Tok/s 19273 (38613)	Loss/tok 2.4046 (3.1100)	LR 1.250e-04
0: TRAIN [3][1390/3880]	Time 0.162 (0.182)	Data 8.37e-05 (6.03e-04)	Tok/s 31595 (38642)	Loss/tok 2.9750 (3.1110)	LR 1.250e-04
0: TRAIN [3][1400/3880]	Time 0.163 (0.182)	Data 8.08e-05 (5.99e-04)	Tok/s 31415 (38625)	Loss/tok 3.0505 (3.1104)	LR 1.250e-04
0: TRAIN [3][1410/3880]	Time 0.136 (0.182)	Data 7.80e-05 (5.96e-04)	Tok/s 19405 (38633)	Loss/tok 2.5101 (3.1107)	LR 1.250e-04
0: TRAIN [3][1420/3880]	Time 0.256 (0.182)	Data 1.50e-04 (5.92e-04)	Tok/s 59041 (38619)	Loss/tok 3.4152 (3.1105)	LR 1.250e-04
0: TRAIN [3][1430/3880]	Time 0.221 (0.182)	Data 7.46e-05 (5.89e-04)	Tok/s 52990 (38661)	Loss/tok 3.3736 (3.1116)	LR 1.250e-04
0: TRAIN [3][1440/3880]	Time 0.162 (0.182)	Data 7.75e-05 (5.85e-04)	Tok/s 32088 (38631)	Loss/tok 2.8802 (3.1109)	LR 1.250e-04
0: TRAIN [3][1450/3880]	Time 0.257 (0.182)	Data 1.05e-04 (5.82e-04)	Tok/s 55536 (38651)	Loss/tok 3.7142 (3.1110)	LR 1.250e-04
0: TRAIN [3][1460/3880]	Time 0.191 (0.182)	Data 7.46e-05 (5.79e-04)	Tok/s 43370 (38666)	Loss/tok 2.9747 (3.1113)	LR 1.250e-04
0: TRAIN [3][1470/3880]	Time 0.162 (0.182)	Data 8.56e-05 (5.76e-04)	Tok/s 31912 (38645)	Loss/tok 2.9024 (3.1111)	LR 1.250e-04
0: TRAIN [3][1480/3880]	Time 0.191 (0.182)	Data 7.87e-05 (5.73e-04)	Tok/s 44461 (38693)	Loss/tok 3.2092 (3.1121)	LR 1.250e-04
0: TRAIN [3][1490/3880]	Time 0.191 (0.182)	Data 8.25e-05 (5.70e-04)	Tok/s 43700 (38689)	Loss/tok 3.0617 (3.1119)	LR 1.250e-04
0: TRAIN [3][1500/3880]	Time 0.191 (0.182)	Data 8.89e-05 (5.66e-04)	Tok/s 43970 (38676)	Loss/tok 2.9656 (3.1110)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1510/3880]	Time 0.220 (0.182)	Data 8.03e-05 (5.63e-04)	Tok/s 52383 (38693)	Loss/tok 3.2680 (3.1109)	LR 1.250e-04
0: TRAIN [3][1520/3880]	Time 0.163 (0.182)	Data 5.71e-04 (5.61e-04)	Tok/s 32242 (38705)	Loss/tok 2.9240 (3.1110)	LR 1.250e-04
0: TRAIN [3][1530/3880]	Time 0.162 (0.182)	Data 9.35e-05 (5.58e-04)	Tok/s 32050 (38740)	Loss/tok 3.0335 (3.1116)	LR 1.250e-04
0: TRAIN [3][1540/3880]	Time 0.162 (0.183)	Data 1.06e-04 (5.55e-04)	Tok/s 31314 (38758)	Loss/tok 3.0431 (3.1121)	LR 1.250e-04
0: TRAIN [3][1550/3880]	Time 0.221 (0.183)	Data 9.42e-05 (5.52e-04)	Tok/s 52724 (38755)	Loss/tok 3.4477 (3.1126)	LR 1.250e-04
0: TRAIN [3][1560/3880]	Time 0.162 (0.183)	Data 8.27e-05 (5.49e-04)	Tok/s 32375 (38750)	Loss/tok 2.9335 (3.1127)	LR 1.250e-04
0: TRAIN [3][1570/3880]	Time 0.162 (0.183)	Data 3.06e-04 (5.46e-04)	Tok/s 32076 (38753)	Loss/tok 3.0024 (3.1125)	LR 1.250e-04
0: TRAIN [3][1580/3880]	Time 0.136 (0.182)	Data 7.77e-05 (5.43e-04)	Tok/s 18101 (38737)	Loss/tok 2.4892 (3.1121)	LR 1.250e-04
0: TRAIN [3][1590/3880]	Time 0.222 (0.182)	Data 1.21e-04 (5.41e-04)	Tok/s 52402 (38697)	Loss/tok 3.2707 (3.1116)	LR 1.250e-04
0: TRAIN [3][1600/3880]	Time 0.191 (0.182)	Data 8.63e-05 (5.38e-04)	Tok/s 44202 (38714)	Loss/tok 3.0913 (3.1119)	LR 1.250e-04
0: TRAIN [3][1610/3880]	Time 0.162 (0.182)	Data 8.11e-05 (5.36e-04)	Tok/s 31497 (38692)	Loss/tok 2.9322 (3.1114)	LR 1.250e-04
0: TRAIN [3][1620/3880]	Time 0.162 (0.182)	Data 7.37e-05 (5.33e-04)	Tok/s 31870 (38678)	Loss/tok 2.7841 (3.1105)	LR 1.250e-04
0: TRAIN [3][1630/3880]	Time 0.162 (0.182)	Data 1.13e-04 (5.30e-04)	Tok/s 32264 (38656)	Loss/tok 2.8317 (3.1098)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1640/3880]	Time 0.258 (0.182)	Data 1.02e-04 (5.28e-04)	Tok/s 57747 (38623)	Loss/tok 3.5398 (3.1096)	LR 1.250e-04
0: TRAIN [3][1650/3880]	Time 0.162 (0.182)	Data 3.65e-04 (5.25e-04)	Tok/s 31651 (38596)	Loss/tok 2.8649 (3.1088)	LR 1.250e-04
0: TRAIN [3][1660/3880]	Time 0.191 (0.182)	Data 1.19e-04 (5.23e-04)	Tok/s 44234 (38590)	Loss/tok 2.9787 (3.1083)	LR 1.250e-04
0: TRAIN [3][1670/3880]	Time 0.257 (0.182)	Data 1.27e-04 (5.20e-04)	Tok/s 58111 (38593)	Loss/tok 3.4185 (3.1084)	LR 1.250e-04
0: TRAIN [3][1680/3880]	Time 0.137 (0.182)	Data 1.01e-04 (5.18e-04)	Tok/s 19031 (38568)	Loss/tok 2.4532 (3.1088)	LR 1.250e-04
0: TRAIN [3][1690/3880]	Time 0.191 (0.182)	Data 8.23e-05 (5.15e-04)	Tok/s 44373 (38559)	Loss/tok 3.1720 (3.1084)	LR 1.250e-04
0: TRAIN [3][1700/3880]	Time 0.191 (0.182)	Data 1.04e-04 (5.13e-04)	Tok/s 43400 (38566)	Loss/tok 3.1798 (3.1086)	LR 1.250e-04
0: TRAIN [3][1710/3880]	Time 0.257 (0.182)	Data 8.27e-05 (5.11e-04)	Tok/s 58307 (38584)	Loss/tok 3.3796 (3.1089)	LR 1.250e-04
0: TRAIN [3][1720/3880]	Time 0.162 (0.182)	Data 1.38e-04 (5.08e-04)	Tok/s 30834 (38583)	Loss/tok 2.8777 (3.1085)	LR 1.250e-04
0: TRAIN [3][1730/3880]	Time 0.191 (0.182)	Data 1.04e-04 (5.06e-04)	Tok/s 44158 (38592)	Loss/tok 3.1303 (3.1086)	LR 1.250e-04
0: TRAIN [3][1740/3880]	Time 0.191 (0.182)	Data 8.20e-05 (5.04e-04)	Tok/s 43266 (38625)	Loss/tok 3.2518 (3.1101)	LR 1.250e-04
0: TRAIN [3][1750/3880]	Time 0.162 (0.182)	Data 8.30e-05 (5.01e-04)	Tok/s 31253 (38640)	Loss/tok 3.0629 (3.1107)	LR 1.250e-04
0: TRAIN [3][1760/3880]	Time 0.191 (0.182)	Data 1.29e-04 (4.99e-04)	Tok/s 43480 (38642)	Loss/tok 3.1588 (3.1108)	LR 1.250e-04
0: TRAIN [3][1770/3880]	Time 0.221 (0.182)	Data 8.01e-05 (4.97e-04)	Tok/s 51768 (38651)	Loss/tok 3.3750 (3.1109)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1780/3880]	Time 0.162 (0.182)	Data 7.65e-05 (4.94e-04)	Tok/s 31686 (38671)	Loss/tok 2.8070 (3.1114)	LR 1.250e-04
0: TRAIN [3][1790/3880]	Time 0.161 (0.182)	Data 1.22e-04 (4.92e-04)	Tok/s 32029 (38680)	Loss/tok 2.8238 (3.1117)	LR 1.250e-04
0: TRAIN [3][1800/3880]	Time 0.163 (0.182)	Data 5.80e-04 (4.90e-04)	Tok/s 31645 (38684)	Loss/tok 2.9180 (3.1123)	LR 1.250e-04
0: TRAIN [3][1810/3880]	Time 0.137 (0.182)	Data 7.61e-05 (4.88e-04)	Tok/s 19183 (38694)	Loss/tok 2.5625 (3.1127)	LR 1.250e-04
0: TRAIN [3][1820/3880]	Time 0.162 (0.182)	Data 1.03e-04 (4.86e-04)	Tok/s 31518 (38682)	Loss/tok 2.9159 (3.1120)	LR 1.250e-04
0: TRAIN [3][1830/3880]	Time 0.162 (0.182)	Data 8.27e-05 (4.84e-04)	Tok/s 32794 (38684)	Loss/tok 2.8489 (3.1123)	LR 1.250e-04
0: TRAIN [3][1840/3880]	Time 0.162 (0.182)	Data 1.03e-04 (4.82e-04)	Tok/s 32154 (38689)	Loss/tok 2.9902 (3.1124)	LR 1.250e-04
0: TRAIN [3][1850/3880]	Time 0.162 (0.182)	Data 8.96e-05 (4.80e-04)	Tok/s 31760 (38666)	Loss/tok 2.9956 (3.1120)	LR 1.250e-04
0: TRAIN [3][1860/3880]	Time 0.221 (0.182)	Data 1.21e-04 (4.78e-04)	Tok/s 53096 (38718)	Loss/tok 3.2348 (3.1134)	LR 1.250e-04
0: TRAIN [3][1870/3880]	Time 0.162 (0.182)	Data 7.87e-05 (4.76e-04)	Tok/s 32178 (38710)	Loss/tok 2.8941 (3.1135)	LR 1.250e-04
0: TRAIN [3][1880/3880]	Time 0.222 (0.182)	Data 6.60e-04 (4.74e-04)	Tok/s 52331 (38705)	Loss/tok 3.2088 (3.1131)	LR 1.250e-04
0: TRAIN [3][1890/3880]	Time 0.192 (0.182)	Data 7.96e-05 (4.72e-04)	Tok/s 43980 (38730)	Loss/tok 3.2325 (3.1139)	LR 1.250e-04
0: TRAIN [3][1900/3880]	Time 0.162 (0.183)	Data 8.49e-05 (4.70e-04)	Tok/s 31733 (38747)	Loss/tok 2.8850 (3.1141)	LR 1.250e-04
0: TRAIN [3][1910/3880]	Time 0.222 (0.183)	Data 7.94e-05 (4.68e-04)	Tok/s 53138 (38746)	Loss/tok 3.3316 (3.1140)	LR 1.250e-04
0: TRAIN [3][1920/3880]	Time 0.221 (0.183)	Data 9.87e-05 (4.66e-04)	Tok/s 52358 (38758)	Loss/tok 3.4317 (3.1142)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1930/3880]	Time 0.191 (0.183)	Data 9.42e-05 (4.65e-04)	Tok/s 44139 (38765)	Loss/tok 3.0570 (3.1142)	LR 1.250e-04
0: TRAIN [3][1940/3880]	Time 0.221 (0.183)	Data 8.77e-05 (4.63e-04)	Tok/s 53433 (38768)	Loss/tok 3.0915 (3.1139)	LR 1.250e-04
0: TRAIN [3][1950/3880]	Time 0.162 (0.182)	Data 7.80e-05 (4.61e-04)	Tok/s 31623 (38739)	Loss/tok 2.7854 (3.1131)	LR 1.250e-04
0: TRAIN [3][1960/3880]	Time 0.162 (0.182)	Data 1.27e-04 (4.59e-04)	Tok/s 32091 (38726)	Loss/tok 2.9938 (3.1127)	LR 1.250e-04
0: TRAIN [3][1970/3880]	Time 0.191 (0.182)	Data 1.02e-04 (4.58e-04)	Tok/s 43602 (38714)	Loss/tok 3.0316 (3.1125)	LR 1.250e-04
0: TRAIN [3][1980/3880]	Time 0.163 (0.182)	Data 7.68e-05 (4.56e-04)	Tok/s 31473 (38690)	Loss/tok 2.9392 (3.1120)	LR 1.250e-04
0: TRAIN [3][1990/3880]	Time 0.191 (0.182)	Data 9.51e-05 (4.54e-04)	Tok/s 44924 (38692)	Loss/tok 3.1141 (3.1117)	LR 1.250e-04
0: TRAIN [3][2000/3880]	Time 0.256 (0.182)	Data 8.13e-05 (4.52e-04)	Tok/s 58753 (38691)	Loss/tok 3.3796 (3.1115)	LR 1.250e-04
0: TRAIN [3][2010/3880]	Time 0.136 (0.182)	Data 8.03e-05 (4.50e-04)	Tok/s 19772 (38677)	Loss/tok 2.6469 (3.1112)	LR 1.250e-04
0: TRAIN [3][2020/3880]	Time 0.162 (0.182)	Data 7.77e-05 (4.49e-04)	Tok/s 31182 (38665)	Loss/tok 2.9439 (3.1110)	LR 1.250e-04
0: TRAIN [3][2030/3880]	Time 0.164 (0.182)	Data 1.10e-04 (4.47e-04)	Tok/s 31222 (38661)	Loss/tok 3.0202 (3.1112)	LR 1.250e-04
0: TRAIN [3][2040/3880]	Time 0.192 (0.182)	Data 8.82e-05 (4.45e-04)	Tok/s 43382 (38658)	Loss/tok 3.0530 (3.1110)	LR 1.250e-04
0: TRAIN [3][2050/3880]	Time 0.162 (0.182)	Data 1.07e-04 (4.44e-04)	Tok/s 33175 (38660)	Loss/tok 2.9389 (3.1111)	LR 1.250e-04
0: TRAIN [3][2060/3880]	Time 0.137 (0.182)	Data 7.61e-05 (4.42e-04)	Tok/s 19810 (38647)	Loss/tok 2.5269 (3.1107)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2070/3880]	Time 0.217 (0.182)	Data 8.01e-05 (4.40e-04)	Tok/s 53534 (38648)	Loss/tok 3.2679 (3.1109)	LR 1.250e-04
0: TRAIN [3][2080/3880]	Time 0.191 (0.182)	Data 1.17e-04 (4.39e-04)	Tok/s 44221 (38627)	Loss/tok 3.0636 (3.1104)	LR 1.250e-04
0: TRAIN [3][2090/3880]	Time 0.162 (0.182)	Data 1.24e-04 (4.37e-04)	Tok/s 32345 (38620)	Loss/tok 2.9214 (3.1099)	LR 1.250e-04
0: TRAIN [3][2100/3880]	Time 0.162 (0.182)	Data 1.40e-04 (4.36e-04)	Tok/s 31295 (38615)	Loss/tok 3.0357 (3.1101)	LR 1.250e-04
0: TRAIN [3][2110/3880]	Time 0.191 (0.182)	Data 1.21e-04 (4.34e-04)	Tok/s 44278 (38623)	Loss/tok 3.0423 (3.1097)	LR 1.250e-04
0: TRAIN [3][2120/3880]	Time 0.138 (0.182)	Data 9.32e-05 (4.32e-04)	Tok/s 18990 (38608)	Loss/tok 2.5563 (3.1094)	LR 1.250e-04
0: TRAIN [3][2130/3880]	Time 0.191 (0.182)	Data 1.01e-04 (4.31e-04)	Tok/s 43711 (38610)	Loss/tok 3.2349 (3.1096)	LR 1.250e-04
0: TRAIN [3][2140/3880]	Time 0.136 (0.182)	Data 9.51e-05 (4.29e-04)	Tok/s 18735 (38594)	Loss/tok 2.6000 (3.1093)	LR 1.250e-04
0: TRAIN [3][2150/3880]	Time 0.191 (0.182)	Data 8.82e-05 (4.28e-04)	Tok/s 44442 (38593)	Loss/tok 2.9590 (3.1091)	LR 1.250e-04
0: TRAIN [3][2160/3880]	Time 0.221 (0.182)	Data 7.65e-05 (4.26e-04)	Tok/s 51814 (38596)	Loss/tok 3.3801 (3.1091)	LR 1.250e-04
0: TRAIN [3][2170/3880]	Time 0.191 (0.182)	Data 8.94e-05 (4.25e-04)	Tok/s 42942 (38593)	Loss/tok 3.1503 (3.1089)	LR 1.250e-04
0: TRAIN [3][2180/3880]	Time 0.191 (0.182)	Data 7.39e-05 (4.23e-04)	Tok/s 44124 (38580)	Loss/tok 3.0707 (3.1087)	LR 1.250e-04
0: TRAIN [3][2190/3880]	Time 0.221 (0.182)	Data 7.99e-05 (4.22e-04)	Tok/s 51536 (38602)	Loss/tok 3.2904 (3.1091)	LR 1.250e-04
0: TRAIN [3][2200/3880]	Time 0.191 (0.182)	Data 7.72e-05 (4.20e-04)	Tok/s 43533 (38604)	Loss/tok 3.1458 (3.1092)	LR 1.250e-04
0: TRAIN [3][2210/3880]	Time 0.165 (0.182)	Data 8.77e-05 (4.19e-04)	Tok/s 30811 (38611)	Loss/tok 2.9735 (3.1094)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2220/3880]	Time 0.158 (0.182)	Data 7.72e-05 (4.17e-04)	Tok/s 33094 (38591)	Loss/tok 3.0154 (3.1089)	LR 1.250e-04
0: TRAIN [3][2230/3880]	Time 0.136 (0.182)	Data 7.68e-05 (4.16e-04)	Tok/s 19124 (38589)	Loss/tok 2.5293 (3.1089)	LR 1.250e-04
0: TRAIN [3][2240/3880]	Time 0.162 (0.182)	Data 9.66e-05 (4.14e-04)	Tok/s 31454 (38570)	Loss/tok 3.0091 (3.1084)	LR 1.250e-04
0: TRAIN [3][2250/3880]	Time 0.190 (0.182)	Data 1.35e-04 (4.13e-04)	Tok/s 43796 (38589)	Loss/tok 3.0692 (3.1085)	LR 1.250e-04
0: TRAIN [3][2260/3880]	Time 0.191 (0.182)	Data 1.05e-04 (4.12e-04)	Tok/s 42928 (38597)	Loss/tok 3.0305 (3.1086)	LR 1.250e-04
0: TRAIN [3][2270/3880]	Time 0.192 (0.182)	Data 1.03e-04 (4.10e-04)	Tok/s 43081 (38625)	Loss/tok 3.0720 (3.1088)	LR 1.250e-04
0: TRAIN [3][2280/3880]	Time 0.256 (0.182)	Data 1.06e-04 (4.09e-04)	Tok/s 58013 (38628)	Loss/tok 3.4111 (3.1088)	LR 1.250e-04
0: TRAIN [3][2290/3880]	Time 0.191 (0.182)	Data 9.27e-05 (4.08e-04)	Tok/s 43456 (38620)	Loss/tok 3.1480 (3.1083)	LR 1.250e-04
0: TRAIN [3][2300/3880]	Time 0.162 (0.182)	Data 9.04e-05 (4.06e-04)	Tok/s 32067 (38631)	Loss/tok 2.9534 (3.1083)	LR 1.250e-04
0: TRAIN [3][2310/3880]	Time 0.221 (0.182)	Data 1.13e-04 (4.05e-04)	Tok/s 53534 (38633)	Loss/tok 3.1651 (3.1081)	LR 1.250e-04
0: TRAIN [3][2320/3880]	Time 0.191 (0.182)	Data 8.37e-05 (4.04e-04)	Tok/s 43885 (38625)	Loss/tok 3.1298 (3.1081)	LR 1.250e-04
0: TRAIN [3][2330/3880]	Time 0.162 (0.182)	Data 1.07e-04 (4.02e-04)	Tok/s 32090 (38620)	Loss/tok 3.0222 (3.1080)	LR 1.250e-04
0: TRAIN [3][2340/3880]	Time 0.162 (0.182)	Data 7.80e-05 (4.01e-04)	Tok/s 31347 (38638)	Loss/tok 2.9502 (3.1083)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2350/3880]	Time 0.217 (0.182)	Data 7.61e-05 (4.00e-04)	Tok/s 53387 (38668)	Loss/tok 3.3134 (3.1088)	LR 1.250e-04
0: TRAIN [3][2360/3880]	Time 0.162 (0.182)	Data 3.99e-04 (3.99e-04)	Tok/s 31626 (38668)	Loss/tok 2.8314 (3.1088)	LR 1.250e-04
0: TRAIN [3][2370/3880]	Time 0.136 (0.182)	Data 8.96e-05 (3.97e-04)	Tok/s 19425 (38656)	Loss/tok 2.4661 (3.1086)	LR 1.250e-04
0: TRAIN [3][2380/3880]	Time 0.163 (0.182)	Data 7.58e-05 (3.96e-04)	Tok/s 31457 (38661)	Loss/tok 2.8205 (3.1087)	LR 1.250e-04
0: TRAIN [3][2390/3880]	Time 0.256 (0.182)	Data 7.44e-05 (3.95e-04)	Tok/s 57739 (38679)	Loss/tok 3.3962 (3.1090)	LR 1.250e-04
0: TRAIN [3][2400/3880]	Time 0.162 (0.182)	Data 8.73e-05 (3.93e-04)	Tok/s 31809 (38670)	Loss/tok 2.8660 (3.1088)	LR 1.250e-04
0: TRAIN [3][2410/3880]	Time 0.220 (0.182)	Data 8.70e-05 (3.92e-04)	Tok/s 52980 (38663)	Loss/tok 3.3097 (3.1085)	LR 1.250e-04
0: TRAIN [3][2420/3880]	Time 0.221 (0.182)	Data 7.75e-05 (3.91e-04)	Tok/s 52530 (38676)	Loss/tok 3.2454 (3.1084)	LR 1.250e-04
0: TRAIN [3][2430/3880]	Time 0.222 (0.182)	Data 8.89e-05 (3.90e-04)	Tok/s 52368 (38688)	Loss/tok 3.1855 (3.1084)	LR 1.250e-04
0: TRAIN [3][2440/3880]	Time 0.192 (0.182)	Data 9.80e-05 (3.89e-04)	Tok/s 44411 (38702)	Loss/tok 2.9897 (3.1089)	LR 1.250e-04
0: TRAIN [3][2450/3880]	Time 0.191 (0.182)	Data 5.92e-04 (3.88e-04)	Tok/s 42888 (38719)	Loss/tok 3.0744 (3.1092)	LR 1.250e-04
0: TRAIN [3][2460/3880]	Time 0.221 (0.182)	Data 1.39e-04 (3.86e-04)	Tok/s 53216 (38718)	Loss/tok 3.1002 (3.1090)	LR 1.250e-04
0: TRAIN [3][2470/3880]	Time 0.191 (0.182)	Data 1.04e-04 (3.86e-04)	Tok/s 43850 (38719)	Loss/tok 3.1072 (3.1093)	LR 1.250e-04
0: TRAIN [3][2480/3880]	Time 0.162 (0.182)	Data 8.63e-05 (3.84e-04)	Tok/s 31416 (38723)	Loss/tok 2.9200 (3.1091)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2490/3880]	Time 0.162 (0.182)	Data 1.13e-04 (3.83e-04)	Tok/s 31440 (38713)	Loss/tok 2.9339 (3.1089)	LR 1.250e-04
0: TRAIN [3][2500/3880]	Time 0.163 (0.182)	Data 1.22e-04 (3.82e-04)	Tok/s 30907 (38699)	Loss/tok 2.8882 (3.1085)	LR 1.250e-04
0: TRAIN [3][2510/3880]	Time 0.136 (0.182)	Data 1.22e-04 (3.81e-04)	Tok/s 19460 (38675)	Loss/tok 2.6057 (3.1082)	LR 1.250e-04
0: TRAIN [3][2520/3880]	Time 0.162 (0.182)	Data 7.77e-05 (3.80e-04)	Tok/s 31698 (38671)	Loss/tok 3.0483 (3.1081)	LR 1.250e-04
0: TRAIN [3][2530/3880]	Time 0.162 (0.182)	Data 2.25e-03 (3.80e-04)	Tok/s 32070 (38661)	Loss/tok 2.9367 (3.1078)	LR 1.250e-04
0: TRAIN [3][2540/3880]	Time 0.162 (0.182)	Data 9.85e-05 (3.79e-04)	Tok/s 32158 (38637)	Loss/tok 3.0824 (3.1075)	LR 1.250e-04
0: TRAIN [3][2550/3880]	Time 0.221 (0.182)	Data 1.15e-04 (3.78e-04)	Tok/s 52575 (38634)	Loss/tok 3.5349 (3.1078)	LR 1.250e-04
0: TRAIN [3][2560/3880]	Time 0.162 (0.182)	Data 7.82e-05 (3.77e-04)	Tok/s 31554 (38636)	Loss/tok 2.8088 (3.1078)	LR 1.250e-04
0: TRAIN [3][2570/3880]	Time 0.191 (0.182)	Data 9.66e-05 (3.75e-04)	Tok/s 43946 (38641)	Loss/tok 3.0292 (3.1076)	LR 1.250e-04
0: TRAIN [3][2580/3880]	Time 0.258 (0.182)	Data 9.94e-05 (3.74e-04)	Tok/s 57856 (38630)	Loss/tok 3.5167 (3.1077)	LR 1.250e-04
0: TRAIN [3][2590/3880]	Time 0.221 (0.182)	Data 7.99e-05 (3.73e-04)	Tok/s 52730 (38636)	Loss/tok 3.1785 (3.1076)	LR 1.250e-04
0: TRAIN [3][2600/3880]	Time 0.162 (0.182)	Data 7.96e-05 (3.72e-04)	Tok/s 31884 (38633)	Loss/tok 2.8757 (3.1075)	LR 1.250e-04
0: TRAIN [3][2610/3880]	Time 0.191 (0.182)	Data 9.35e-05 (3.71e-04)	Tok/s 43466 (38638)	Loss/tok 3.0902 (3.1071)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2620/3880]	Time 0.221 (0.182)	Data 7.44e-05 (3.70e-04)	Tok/s 51624 (38637)	Loss/tok 3.4041 (3.1072)	LR 1.250e-04
0: TRAIN [3][2630/3880]	Time 0.192 (0.182)	Data 8.65e-05 (3.69e-04)	Tok/s 43215 (38614)	Loss/tok 3.1253 (3.1068)	LR 1.250e-04
0: TRAIN [3][2640/3880]	Time 0.163 (0.182)	Data 1.03e-04 (3.68e-04)	Tok/s 30921 (38611)	Loss/tok 2.7951 (3.1068)	LR 1.250e-04
0: TRAIN [3][2650/3880]	Time 0.256 (0.182)	Data 1.24e-04 (3.67e-04)	Tok/s 56525 (38630)	Loss/tok 3.5763 (3.1075)	LR 1.250e-04
0: TRAIN [3][2660/3880]	Time 0.191 (0.182)	Data 7.49e-05 (3.66e-04)	Tok/s 43957 (38621)	Loss/tok 3.0743 (3.1075)	LR 1.250e-04
0: TRAIN [3][2670/3880]	Time 0.257 (0.182)	Data 1.13e-04 (3.65e-04)	Tok/s 56558 (38635)	Loss/tok 3.5438 (3.1081)	LR 1.250e-04
0: TRAIN [3][2680/3880]	Time 0.162 (0.182)	Data 9.06e-05 (3.64e-04)	Tok/s 33200 (38634)	Loss/tok 2.9161 (3.1079)	LR 1.250e-04
0: TRAIN [3][2690/3880]	Time 0.221 (0.182)	Data 1.43e-04 (3.63e-04)	Tok/s 53035 (38637)	Loss/tok 3.3465 (3.1079)	LR 1.250e-04
0: TRAIN [3][2700/3880]	Time 0.191 (0.182)	Data 1.15e-04 (3.62e-04)	Tok/s 43379 (38627)	Loss/tok 3.2113 (3.1079)	LR 1.250e-04
0: TRAIN [3][2710/3880]	Time 0.163 (0.182)	Data 7.84e-05 (3.61e-04)	Tok/s 32351 (38611)	Loss/tok 2.9004 (3.1077)	LR 1.250e-04
0: TRAIN [3][2720/3880]	Time 0.162 (0.182)	Data 1.05e-04 (3.60e-04)	Tok/s 33192 (38615)	Loss/tok 3.0333 (3.1076)	LR 1.250e-04
0: TRAIN [3][2730/3880]	Time 0.220 (0.182)	Data 9.58e-05 (3.59e-04)	Tok/s 53737 (38625)	Loss/tok 3.2229 (3.1077)	LR 1.250e-04
0: TRAIN [3][2740/3880]	Time 0.162 (0.182)	Data 9.94e-05 (3.58e-04)	Tok/s 31068 (38631)	Loss/tok 2.8726 (3.1080)	LR 1.250e-04
0: TRAIN [3][2750/3880]	Time 0.221 (0.182)	Data 1.23e-04 (3.57e-04)	Tok/s 52368 (38631)	Loss/tok 3.0749 (3.1077)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2760/3880]	Time 0.162 (0.182)	Data 1.29e-04 (3.57e-04)	Tok/s 31180 (38625)	Loss/tok 2.9115 (3.1075)	LR 1.250e-04
0: TRAIN [3][2770/3880]	Time 0.162 (0.182)	Data 1.26e-04 (3.56e-04)	Tok/s 31325 (38622)	Loss/tok 2.8696 (3.1074)	LR 1.250e-04
0: TRAIN [3][2780/3880]	Time 0.162 (0.182)	Data 8.25e-05 (3.55e-04)	Tok/s 31854 (38633)	Loss/tok 2.8951 (3.1073)	LR 1.250e-04
0: TRAIN [3][2790/3880]	Time 0.162 (0.182)	Data 8.13e-05 (3.54e-04)	Tok/s 32010 (38636)	Loss/tok 3.0574 (3.1075)	LR 1.250e-04
0: TRAIN [3][2800/3880]	Time 0.221 (0.182)	Data 1.34e-04 (3.53e-04)	Tok/s 53333 (38636)	Loss/tok 3.2460 (3.1075)	LR 1.250e-04
0: TRAIN [3][2810/3880]	Time 0.259 (0.182)	Data 1.24e-04 (3.53e-04)	Tok/s 57602 (38626)	Loss/tok 3.3928 (3.1076)	LR 1.250e-04
0: TRAIN [3][2820/3880]	Time 0.191 (0.182)	Data 1.68e-04 (3.52e-04)	Tok/s 43760 (38612)	Loss/tok 3.1064 (3.1073)	LR 1.250e-04
0: TRAIN [3][2830/3880]	Time 0.162 (0.182)	Data 9.32e-05 (3.51e-04)	Tok/s 31609 (38628)	Loss/tok 2.9342 (3.1074)	LR 1.250e-04
0: TRAIN [3][2840/3880]	Time 0.191 (0.182)	Data 7.63e-05 (3.50e-04)	Tok/s 43367 (38624)	Loss/tok 3.1373 (3.1071)	LR 1.250e-04
0: TRAIN [3][2850/3880]	Time 0.221 (0.182)	Data 8.92e-05 (3.49e-04)	Tok/s 53841 (38637)	Loss/tok 3.1735 (3.1070)	LR 1.250e-04
0: TRAIN [3][2860/3880]	Time 0.136 (0.182)	Data 9.06e-05 (3.48e-04)	Tok/s 18948 (38636)	Loss/tok 2.4639 (3.1069)	LR 1.250e-04
0: TRAIN [3][2870/3880]	Time 0.191 (0.182)	Data 7.77e-05 (3.47e-04)	Tok/s 44438 (38643)	Loss/tok 3.0641 (3.1070)	LR 1.250e-04
0: TRAIN [3][2880/3880]	Time 0.162 (0.182)	Data 7.34e-05 (3.46e-04)	Tok/s 32336 (38641)	Loss/tok 2.8942 (3.1071)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2890/3880]	Time 0.191 (0.182)	Data 7.34e-05 (3.46e-04)	Tok/s 44071 (38634)	Loss/tok 3.1751 (3.1068)	LR 1.250e-04
0: TRAIN [3][2900/3880]	Time 0.138 (0.182)	Data 7.41e-05 (3.45e-04)	Tok/s 18827 (38619)	Loss/tok 2.5713 (3.1067)	LR 1.250e-04
0: TRAIN [3][2910/3880]	Time 0.221 (0.182)	Data 7.56e-05 (3.44e-04)	Tok/s 52761 (38628)	Loss/tok 3.2544 (3.1069)	LR 1.250e-04
0: TRAIN [3][2920/3880]	Time 0.162 (0.182)	Data 1.49e-04 (3.43e-04)	Tok/s 31779 (38631)	Loss/tok 3.0727 (3.1070)	LR 1.250e-04
0: TRAIN [3][2930/3880]	Time 0.190 (0.182)	Data 1.19e-04 (3.42e-04)	Tok/s 44757 (38632)	Loss/tok 3.1416 (3.1068)	LR 1.250e-04
0: TRAIN [3][2940/3880]	Time 0.162 (0.182)	Data 7.56e-05 (3.42e-04)	Tok/s 32339 (38632)	Loss/tok 3.0399 (3.1068)	LR 1.250e-04
0: TRAIN [3][2950/3880]	Time 0.191 (0.182)	Data 7.41e-05 (3.41e-04)	Tok/s 44276 (38628)	Loss/tok 3.0031 (3.1067)	LR 1.250e-04
0: TRAIN [3][2960/3880]	Time 0.162 (0.182)	Data 7.82e-05 (3.40e-04)	Tok/s 32174 (38618)	Loss/tok 2.8364 (3.1064)	LR 1.250e-04
0: TRAIN [3][2970/3880]	Time 0.162 (0.182)	Data 1.01e-04 (3.39e-04)	Tok/s 31592 (38602)	Loss/tok 2.7746 (3.1060)	LR 1.250e-04
0: TRAIN [3][2980/3880]	Time 0.221 (0.182)	Data 7.53e-05 (3.38e-04)	Tok/s 53835 (38611)	Loss/tok 3.3287 (3.1064)	LR 1.250e-04
0: TRAIN [3][2990/3880]	Time 0.162 (0.182)	Data 7.39e-05 (3.37e-04)	Tok/s 31867 (38612)	Loss/tok 3.0849 (3.1065)	LR 1.250e-04
0: TRAIN [3][3000/3880]	Time 0.162 (0.182)	Data 7.49e-05 (3.36e-04)	Tok/s 32852 (38601)	Loss/tok 2.8392 (3.1061)	LR 1.250e-04
0: TRAIN [3][3010/3880]	Time 0.191 (0.182)	Data 7.41e-05 (3.36e-04)	Tok/s 42693 (38597)	Loss/tok 3.3197 (3.1060)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3020/3880]	Time 0.191 (0.182)	Data 7.08e-05 (3.35e-04)	Tok/s 44670 (38591)	Loss/tok 3.0022 (3.1062)	LR 1.250e-04
0: TRAIN [3][3030/3880]	Time 0.137 (0.182)	Data 1.52e-04 (3.34e-04)	Tok/s 19526 (38584)	Loss/tok 2.5312 (3.1064)	LR 1.250e-04
0: TRAIN [3][3040/3880]	Time 0.191 (0.182)	Data 1.28e-04 (3.34e-04)	Tok/s 44827 (38606)	Loss/tok 2.9531 (3.1069)	LR 1.250e-04
0: TRAIN [3][3050/3880]	Time 0.162 (0.182)	Data 1.25e-04 (3.33e-04)	Tok/s 33074 (38603)	Loss/tok 2.8173 (3.1068)	LR 1.250e-04
0: TRAIN [3][3060/3880]	Time 0.163 (0.182)	Data 1.49e-04 (3.33e-04)	Tok/s 32444 (38591)	Loss/tok 2.8357 (3.1065)	LR 1.250e-04
0: TRAIN [3][3070/3880]	Time 0.162 (0.182)	Data 1.24e-04 (3.32e-04)	Tok/s 32643 (38573)	Loss/tok 2.8710 (3.1061)	LR 1.250e-04
0: TRAIN [3][3080/3880]	Time 0.162 (0.182)	Data 1.10e-04 (3.31e-04)	Tok/s 31896 (38567)	Loss/tok 3.0495 (3.1061)	LR 1.250e-04
0: TRAIN [3][3090/3880]	Time 0.191 (0.182)	Data 1.06e-04 (3.31e-04)	Tok/s 44261 (38567)	Loss/tok 3.0845 (3.1061)	LR 1.250e-04
0: TRAIN [3][3100/3880]	Time 0.162 (0.182)	Data 1.13e-04 (3.30e-04)	Tok/s 31194 (38566)	Loss/tok 2.9759 (3.1060)	LR 1.250e-04
0: TRAIN [3][3110/3880]	Time 0.256 (0.182)	Data 1.06e-04 (3.30e-04)	Tok/s 57327 (38586)	Loss/tok 3.6015 (3.1062)	LR 1.250e-04
0: TRAIN [3][3120/3880]	Time 0.162 (0.182)	Data 1.54e-04 (3.29e-04)	Tok/s 31961 (38593)	Loss/tok 2.8976 (3.1062)	LR 1.250e-04
0: TRAIN [3][3130/3880]	Time 0.162 (0.182)	Data 9.49e-05 (3.29e-04)	Tok/s 32087 (38584)	Loss/tok 2.8851 (3.1060)	LR 1.250e-04
0: TRAIN [3][3140/3880]	Time 0.221 (0.182)	Data 1.01e-04 (3.28e-04)	Tok/s 53489 (38594)	Loss/tok 3.1013 (3.1061)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3150/3880]	Time 0.191 (0.182)	Data 1.09e-04 (3.27e-04)	Tok/s 44352 (38595)	Loss/tok 3.1796 (3.1061)	LR 1.250e-04
0: TRAIN [3][3160/3880]	Time 0.220 (0.182)	Data 1.48e-04 (3.27e-04)	Tok/s 51901 (38616)	Loss/tok 3.3604 (3.1066)	LR 1.250e-04
0: TRAIN [3][3170/3880]	Time 0.191 (0.182)	Data 1.55e-04 (3.26e-04)	Tok/s 44456 (38624)	Loss/tok 3.0140 (3.1069)	LR 1.250e-04
0: TRAIN [3][3180/3880]	Time 0.191 (0.182)	Data 1.03e-04 (3.25e-04)	Tok/s 43969 (38619)	Loss/tok 3.0374 (3.1067)	LR 1.250e-04
0: TRAIN [3][3190/3880]	Time 0.191 (0.182)	Data 9.08e-05 (3.25e-04)	Tok/s 44271 (38624)	Loss/tok 3.1163 (3.1068)	LR 1.250e-04
0: TRAIN [3][3200/3880]	Time 0.161 (0.182)	Data 1.39e-04 (3.24e-04)	Tok/s 32138 (38621)	Loss/tok 2.8513 (3.1069)	LR 1.250e-04
0: TRAIN [3][3210/3880]	Time 0.191 (0.182)	Data 1.02e-04 (3.23e-04)	Tok/s 43233 (38628)	Loss/tok 3.0440 (3.1069)	LR 1.250e-04
0: TRAIN [3][3220/3880]	Time 0.191 (0.182)	Data 1.34e-04 (3.23e-04)	Tok/s 43535 (38625)	Loss/tok 3.0969 (3.1071)	LR 1.250e-04
0: TRAIN [3][3230/3880]	Time 0.221 (0.182)	Data 9.82e-05 (3.22e-04)	Tok/s 53201 (38630)	Loss/tok 3.1271 (3.1070)	LR 1.250e-04
0: TRAIN [3][3240/3880]	Time 0.191 (0.182)	Data 1.11e-04 (3.22e-04)	Tok/s 44736 (38636)	Loss/tok 3.1330 (3.1068)	LR 1.250e-04
0: TRAIN [3][3250/3880]	Time 0.136 (0.182)	Data 8.89e-05 (3.21e-04)	Tok/s 19650 (38629)	Loss/tok 2.4891 (3.1067)	LR 1.250e-04
0: TRAIN [3][3260/3880]	Time 0.191 (0.182)	Data 1.01e-04 (3.21e-04)	Tok/s 43858 (38638)	Loss/tok 3.0029 (3.1070)	LR 1.250e-04
0: TRAIN [3][3270/3880]	Time 0.191 (0.182)	Data 9.56e-05 (3.20e-04)	Tok/s 44630 (38639)	Loss/tok 3.1592 (3.1070)	LR 1.250e-04
0: TRAIN [3][3280/3880]	Time 0.136 (0.182)	Data 1.16e-04 (3.19e-04)	Tok/s 18817 (38623)	Loss/tok 2.5761 (3.1070)	LR 1.250e-04
0: TRAIN [3][3290/3880]	Time 0.191 (0.182)	Data 1.08e-04 (3.19e-04)	Tok/s 42667 (38620)	Loss/tok 3.2106 (3.1069)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3300/3880]	Time 0.162 (0.182)	Data 8.85e-05 (3.18e-04)	Tok/s 31054 (38625)	Loss/tok 2.8093 (3.1069)	LR 1.250e-04
0: TRAIN [3][3310/3880]	Time 0.191 (0.182)	Data 9.11e-05 (3.17e-04)	Tok/s 43582 (38634)	Loss/tok 3.0678 (3.1067)	LR 1.250e-04
0: TRAIN [3][3320/3880]	Time 0.164 (0.182)	Data 8.54e-05 (3.17e-04)	Tok/s 32157 (38638)	Loss/tok 2.9361 (3.1065)	LR 1.250e-04
0: TRAIN [3][3330/3880]	Time 0.137 (0.182)	Data 1.11e-04 (3.16e-04)	Tok/s 19838 (38617)	Loss/tok 2.5031 (3.1060)	LR 1.250e-04
0: TRAIN [3][3340/3880]	Time 0.136 (0.182)	Data 8.75e-05 (3.16e-04)	Tok/s 19117 (38617)	Loss/tok 2.6109 (3.1061)	LR 1.250e-04
0: TRAIN [3][3350/3880]	Time 0.162 (0.182)	Data 1.13e-04 (3.15e-04)	Tok/s 32083 (38601)	Loss/tok 2.9666 (3.1057)	LR 1.250e-04
0: TRAIN [3][3360/3880]	Time 0.162 (0.182)	Data 1.02e-04 (3.14e-04)	Tok/s 31641 (38588)	Loss/tok 2.9101 (3.1055)	LR 1.250e-04
0: TRAIN [3][3370/3880]	Time 0.191 (0.182)	Data 1.15e-04 (3.14e-04)	Tok/s 43673 (38594)	Loss/tok 3.1032 (3.1057)	LR 1.250e-04
0: TRAIN [3][3380/3880]	Time 0.191 (0.182)	Data 1.03e-04 (3.13e-04)	Tok/s 44204 (38594)	Loss/tok 2.9576 (3.1056)	LR 1.250e-04
0: TRAIN [3][3390/3880]	Time 0.162 (0.182)	Data 8.03e-05 (3.13e-04)	Tok/s 32180 (38591)	Loss/tok 2.9304 (3.1055)	LR 1.250e-04
0: TRAIN [3][3400/3880]	Time 0.191 (0.182)	Data 1.08e-04 (3.12e-04)	Tok/s 43581 (38583)	Loss/tok 3.1437 (3.1052)	LR 1.250e-04
0: TRAIN [3][3410/3880]	Time 0.136 (0.182)	Data 9.37e-05 (3.11e-04)	Tok/s 19522 (38580)	Loss/tok 2.4868 (3.1051)	LR 1.250e-04
0: TRAIN [3][3420/3880]	Time 0.191 (0.182)	Data 1.01e-04 (3.11e-04)	Tok/s 43299 (38589)	Loss/tok 3.1248 (3.1050)	LR 1.250e-04
0: TRAIN [3][3430/3880]	Time 0.221 (0.182)	Data 9.85e-05 (3.10e-04)	Tok/s 52017 (38602)	Loss/tok 3.3196 (3.1055)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3440/3880]	Time 0.220 (0.182)	Data 1.22e-04 (3.10e-04)	Tok/s 53506 (38602)	Loss/tok 3.2123 (3.1054)	LR 1.250e-04
0: TRAIN [3][3450/3880]	Time 0.221 (0.182)	Data 1.19e-04 (3.09e-04)	Tok/s 53802 (38621)	Loss/tok 3.1392 (3.1056)	LR 1.250e-04
0: TRAIN [3][3460/3880]	Time 0.162 (0.182)	Data 9.35e-05 (3.09e-04)	Tok/s 31327 (38619)	Loss/tok 2.8952 (3.1055)	LR 1.250e-04
0: TRAIN [3][3470/3880]	Time 0.191 (0.182)	Data 1.01e-04 (3.08e-04)	Tok/s 43614 (38615)	Loss/tok 2.9554 (3.1053)	LR 1.250e-04
0: TRAIN [3][3480/3880]	Time 0.162 (0.182)	Data 1.09e-04 (3.08e-04)	Tok/s 32466 (38617)	Loss/tok 2.7116 (3.1050)	LR 1.250e-04
0: TRAIN [3][3490/3880]	Time 0.221 (0.182)	Data 1.13e-04 (3.07e-04)	Tok/s 52214 (38622)	Loss/tok 3.4192 (3.1051)	LR 1.250e-04
0: TRAIN [3][3500/3880]	Time 0.136 (0.182)	Data 1.12e-04 (3.06e-04)	Tok/s 19364 (38609)	Loss/tok 2.4347 (3.1047)	LR 1.250e-04
0: TRAIN [3][3510/3880]	Time 0.221 (0.182)	Data 1.09e-04 (3.06e-04)	Tok/s 52864 (38598)	Loss/tok 3.2394 (3.1046)	LR 1.250e-04
0: TRAIN [3][3520/3880]	Time 0.191 (0.182)	Data 5.13e-04 (3.05e-04)	Tok/s 44229 (38606)	Loss/tok 2.9268 (3.1047)	LR 1.250e-04
0: TRAIN [3][3530/3880]	Time 0.191 (0.182)	Data 7.44e-05 (3.05e-04)	Tok/s 44479 (38615)	Loss/tok 3.1161 (3.1049)	LR 1.250e-04
0: TRAIN [3][3540/3880]	Time 0.162 (0.182)	Data 9.80e-05 (3.04e-04)	Tok/s 31958 (38613)	Loss/tok 3.0780 (3.1048)	LR 1.250e-04
0: TRAIN [3][3550/3880]	Time 0.191 (0.182)	Data 9.25e-05 (3.04e-04)	Tok/s 44851 (38615)	Loss/tok 3.1367 (3.1047)	LR 1.250e-04
0: TRAIN [3][3560/3880]	Time 0.162 (0.182)	Data 1.24e-04 (3.03e-04)	Tok/s 31119 (38611)	Loss/tok 2.8410 (3.1048)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3570/3880]	Time 0.137 (0.182)	Data 9.63e-05 (3.03e-04)	Tok/s 19345 (38606)	Loss/tok 2.5530 (3.1050)	LR 1.250e-04
0: TRAIN [3][3580/3880]	Time 0.191 (0.182)	Data 8.70e-05 (3.02e-04)	Tok/s 44101 (38613)	Loss/tok 3.0169 (3.1049)	LR 1.250e-04
0: TRAIN [3][3590/3880]	Time 0.222 (0.182)	Data 9.54e-05 (3.02e-04)	Tok/s 53381 (38620)	Loss/tok 3.2094 (3.1050)	LR 1.250e-04
0: TRAIN [3][3600/3880]	Time 0.136 (0.182)	Data 9.78e-05 (3.01e-04)	Tok/s 20213 (38611)	Loss/tok 2.6543 (3.1047)	LR 1.250e-04
0: TRAIN [3][3610/3880]	Time 0.191 (0.182)	Data 7.80e-05 (3.01e-04)	Tok/s 43600 (38614)	Loss/tok 3.1645 (3.1048)	LR 1.250e-04
0: TRAIN [3][3620/3880]	Time 0.162 (0.182)	Data 7.37e-05 (3.00e-04)	Tok/s 30839 (38626)	Loss/tok 2.9196 (3.1049)	LR 1.250e-04
0: TRAIN [3][3630/3880]	Time 0.191 (0.182)	Data 1.15e-04 (3.00e-04)	Tok/s 44686 (38627)	Loss/tok 3.2818 (3.1049)	LR 1.250e-04
0: TRAIN [3][3640/3880]	Time 0.162 (0.182)	Data 7.30e-05 (2.99e-04)	Tok/s 31917 (38621)	Loss/tok 2.9366 (3.1047)	LR 1.250e-04
0: TRAIN [3][3650/3880]	Time 0.162 (0.182)	Data 7.39e-05 (2.98e-04)	Tok/s 30720 (38618)	Loss/tok 2.6650 (3.1048)	LR 1.250e-04
0: TRAIN [3][3660/3880]	Time 0.136 (0.182)	Data 7.53e-05 (2.98e-04)	Tok/s 19630 (38599)	Loss/tok 2.6217 (3.1046)	LR 1.250e-04
0: TRAIN [3][3670/3880]	Time 0.162 (0.182)	Data 7.61e-05 (2.97e-04)	Tok/s 31727 (38596)	Loss/tok 3.0419 (3.1044)	LR 1.250e-04
0: TRAIN [3][3680/3880]	Time 0.191 (0.182)	Data 7.70e-05 (2.97e-04)	Tok/s 43702 (38592)	Loss/tok 3.0130 (3.1043)	LR 1.250e-04
0: TRAIN [3][3690/3880]	Time 0.222 (0.182)	Data 7.53e-05 (2.96e-04)	Tok/s 52881 (38592)	Loss/tok 3.2788 (3.1041)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3700/3880]	Time 0.137 (0.182)	Data 8.11e-05 (2.96e-04)	Tok/s 19227 (38600)	Loss/tok 2.7822 (3.1043)	LR 1.250e-04
0: TRAIN [3][3710/3880]	Time 0.136 (0.182)	Data 7.37e-05 (2.95e-04)	Tok/s 19441 (38590)	Loss/tok 2.5245 (3.1041)	LR 1.250e-04
0: TRAIN [3][3720/3880]	Time 0.162 (0.182)	Data 5.32e-04 (2.95e-04)	Tok/s 31558 (38577)	Loss/tok 2.9859 (3.1038)	LR 1.250e-04
0: TRAIN [3][3730/3880]	Time 0.191 (0.182)	Data 1.11e-04 (2.94e-04)	Tok/s 44511 (38574)	Loss/tok 3.1256 (3.1038)	LR 1.250e-04
0: TRAIN [3][3740/3880]	Time 0.191 (0.182)	Data 7.82e-05 (2.94e-04)	Tok/s 43985 (38586)	Loss/tok 3.1126 (3.1041)	LR 1.250e-04
0: TRAIN [3][3750/3880]	Time 0.191 (0.182)	Data 9.25e-05 (2.93e-04)	Tok/s 44712 (38578)	Loss/tok 3.1915 (3.1040)	LR 1.250e-04
0: TRAIN [3][3760/3880]	Time 0.164 (0.182)	Data 1.09e-04 (2.93e-04)	Tok/s 32407 (38583)	Loss/tok 2.8944 (3.1041)	LR 1.250e-04
0: TRAIN [3][3770/3880]	Time 0.258 (0.182)	Data 9.44e-05 (2.92e-04)	Tok/s 57169 (38597)	Loss/tok 3.4243 (3.1045)	LR 1.250e-04
0: TRAIN [3][3780/3880]	Time 0.191 (0.182)	Data 7.44e-05 (2.92e-04)	Tok/s 44037 (38606)	Loss/tok 3.1244 (3.1045)	LR 1.250e-04
0: TRAIN [3][3790/3880]	Time 0.191 (0.182)	Data 9.85e-05 (2.91e-04)	Tok/s 43745 (38612)	Loss/tok 3.0783 (3.1045)	LR 1.250e-04
0: TRAIN [3][3800/3880]	Time 0.221 (0.182)	Data 7.68e-05 (2.91e-04)	Tok/s 53880 (38610)	Loss/tok 3.1533 (3.1044)	LR 1.250e-04
0: TRAIN [3][3810/3880]	Time 0.192 (0.182)	Data 7.53e-05 (2.90e-04)	Tok/s 44355 (38604)	Loss/tok 3.0844 (3.1043)	LR 1.250e-04
0: TRAIN [3][3820/3880]	Time 0.162 (0.182)	Data 7.94e-05 (2.90e-04)	Tok/s 32054 (38610)	Loss/tok 3.0194 (3.1046)	LR 1.250e-04
0: TRAIN [3][3830/3880]	Time 0.163 (0.182)	Data 8.44e-05 (2.89e-04)	Tok/s 30886 (38604)	Loss/tok 3.1001 (3.1046)	LR 1.250e-04
0: TRAIN [3][3840/3880]	Time 0.191 (0.182)	Data 8.06e-05 (2.89e-04)	Tok/s 45192 (38595)	Loss/tok 3.1915 (3.1044)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3850/3880]	Time 0.191 (0.182)	Data 8.92e-05 (2.88e-04)	Tok/s 45610 (38602)	Loss/tok 3.0593 (3.1044)	LR 1.250e-04
0: TRAIN [3][3860/3880]	Time 0.193 (0.182)	Data 7.51e-05 (2.88e-04)	Tok/s 43358 (38599)	Loss/tok 3.0214 (3.1042)	LR 1.250e-04
0: TRAIN [3][3870/3880]	Time 0.162 (0.182)	Data 8.63e-05 (2.87e-04)	Tok/s 32318 (38591)	Loss/tok 3.0107 (3.1041)	LR 1.250e-04
:::MLL 1571257714.296 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 524}}
:::MLL 1571257714.296 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.627 (0.627)	Decoder iters 97.0 (97.0)	Tok/s 26131 (26131)
0: Running moses detokenizer
0: BLEU(score=23.805742836108287, counts=[36925, 18424, 10417, 6149], totals=[65288, 62285, 59282, 56284], precisions=[56.55710084548462, 29.580155735730916, 17.571944266387774, 10.924952028995808], bp=1.0, sys_len=65288, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571257716.133 eval_accuracy: {"value": 23.81, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 535}}
:::MLL 1571257716.133 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 3	Training Loss: 3.1033	Test BLEU: 23.81
0: Performance: Epoch: 3	Training: 308642 Tok/s
0: Finished epoch 3
:::MLL 1571257716.133 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 557}}
:::MLL 1571257716.134 block_start: {"value": null, "metadata": {"first_epoch_num": 5, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571257716.134 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 514}}
0: Starting epoch 4
0: Executing preallocation
0: Sampler for epoch 4 uses seed 1183194530
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][0/3880]	Time 0.831 (0.831)	Data 6.87e-01 (6.87e-01)	Tok/s 3114 (3114)	Loss/tok 2.4851 (2.4851)	LR 1.250e-04
0: TRAIN [4][10/3880]	Time 0.191 (0.247)	Data 1.16e-04 (6.25e-02)	Tok/s 44734 (35819)	Loss/tok 3.2054 (3.0816)	LR 1.250e-04
0: TRAIN [4][20/3880]	Time 0.221 (0.222)	Data 1.52e-04 (3.28e-02)	Tok/s 52608 (39209)	Loss/tok 3.2757 (3.1017)	LR 1.250e-04
0: TRAIN [4][30/3880]	Time 0.136 (0.205)	Data 7.94e-05 (2.23e-02)	Tok/s 18738 (37186)	Loss/tok 2.6567 (3.0834)	LR 1.250e-04
0: TRAIN [4][40/3880]	Time 0.256 (0.200)	Data 1.45e-04 (1.69e-02)	Tok/s 59438 (37852)	Loss/tok 3.4098 (3.0919)	LR 1.250e-04
0: TRAIN [4][50/3880]	Time 0.221 (0.195)	Data 8.08e-05 (1.36e-02)	Tok/s 51726 (37203)	Loss/tok 3.2661 (3.0829)	LR 1.250e-04
0: TRAIN [4][60/3880]	Time 0.191 (0.192)	Data 9.82e-05 (1.14e-02)	Tok/s 43944 (37221)	Loss/tok 3.0246 (3.0780)	LR 1.250e-04
0: TRAIN [4][70/3880]	Time 0.162 (0.191)	Data 7.70e-05 (9.78e-03)	Tok/s 30766 (37507)	Loss/tok 2.9758 (3.0868)	LR 1.250e-04
0: TRAIN [4][80/3880]	Time 0.162 (0.188)	Data 5.52e-04 (8.59e-03)	Tok/s 32504 (37105)	Loss/tok 2.8580 (3.0766)	LR 1.250e-04
0: TRAIN [4][90/3880]	Time 0.162 (0.187)	Data 8.89e-05 (7.66e-03)	Tok/s 31619 (37284)	Loss/tok 2.7344 (3.0714)	LR 1.250e-04
0: TRAIN [4][100/3880]	Time 0.162 (0.185)	Data 7.56e-05 (6.91e-03)	Tok/s 32182 (36851)	Loss/tok 2.8941 (3.0604)	LR 1.250e-04
0: TRAIN [4][110/3880]	Time 0.162 (0.185)	Data 7.77e-05 (6.29e-03)	Tok/s 31434 (36827)	Loss/tok 3.0494 (3.0619)	LR 1.250e-04
0: TRAIN [4][120/3880]	Time 0.221 (0.185)	Data 7.58e-05 (5.78e-03)	Tok/s 52266 (37184)	Loss/tok 3.2863 (3.0681)	LR 1.250e-04
0: TRAIN [4][130/3880]	Time 0.162 (0.184)	Data 7.51e-05 (5.34e-03)	Tok/s 32360 (37044)	Loss/tok 2.9425 (3.0708)	LR 1.250e-04
0: TRAIN [4][140/3880]	Time 0.221 (0.185)	Data 6.18e-04 (4.97e-03)	Tok/s 52949 (37478)	Loss/tok 3.3371 (3.0856)	LR 1.250e-04
0: TRAIN [4][150/3880]	Time 0.191 (0.185)	Data 8.08e-05 (4.65e-03)	Tok/s 45254 (37501)	Loss/tok 3.0868 (3.0872)	LR 1.250e-04
0: TRAIN [4][160/3880]	Time 0.163 (0.184)	Data 8.18e-05 (4.37e-03)	Tok/s 31296 (37364)	Loss/tok 2.9279 (3.0821)	LR 1.250e-04
0: TRAIN [4][170/3880]	Time 0.162 (0.183)	Data 3.64e-04 (4.12e-03)	Tok/s 31546 (37165)	Loss/tok 2.9299 (3.0789)	LR 1.250e-04
0: TRAIN [4][180/3880]	Time 0.259 (0.184)	Data 7.46e-05 (3.90e-03)	Tok/s 57655 (37460)	Loss/tok 3.3793 (3.0845)	LR 1.250e-04
0: TRAIN [4][190/3880]	Time 0.162 (0.184)	Data 7.58e-05 (3.70e-03)	Tok/s 31207 (37663)	Loss/tok 3.0119 (3.0857)	LR 1.250e-04
0: TRAIN [4][200/3880]	Time 0.191 (0.184)	Data 8.49e-05 (3.52e-03)	Tok/s 42756 (37755)	Loss/tok 3.1900 (3.0954)	LR 1.250e-04
0: TRAIN [4][210/3880]	Time 0.191 (0.184)	Data 7.56e-05 (3.35e-03)	Tok/s 42916 (37746)	Loss/tok 3.1517 (3.0929)	LR 1.250e-04
0: TRAIN [4][220/3880]	Time 0.136 (0.183)	Data 9.61e-05 (3.21e-03)	Tok/s 18924 (37671)	Loss/tok 2.4765 (3.0903)	LR 1.250e-04
0: TRAIN [4][230/3880]	Time 0.191 (0.184)	Data 8.49e-05 (3.07e-03)	Tok/s 42936 (37887)	Loss/tok 3.0111 (3.0936)	LR 1.250e-04
0: TRAIN [4][240/3880]	Time 0.192 (0.183)	Data 9.70e-05 (2.95e-03)	Tok/s 43951 (37881)	Loss/tok 3.0084 (3.0894)	LR 1.250e-04
0: TRAIN [4][250/3880]	Time 0.192 (0.183)	Data 7.87e-05 (2.83e-03)	Tok/s 43391 (37878)	Loss/tok 3.0836 (3.0863)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][260/3880]	Time 0.191 (0.183)	Data 8.46e-05 (2.73e-03)	Tok/s 43821 (37858)	Loss/tok 3.2326 (3.0859)	LR 1.250e-04
0: TRAIN [4][270/3880]	Time 0.162 (0.183)	Data 7.84e-05 (2.63e-03)	Tok/s 32530 (37804)	Loss/tok 2.6867 (3.0810)	LR 1.250e-04
0: TRAIN [4][280/3880]	Time 0.191 (0.182)	Data 8.06e-05 (2.54e-03)	Tok/s 43707 (37754)	Loss/tok 2.9881 (3.0789)	LR 1.250e-04
0: TRAIN [4][290/3880]	Time 0.221 (0.182)	Data 7.87e-05 (2.46e-03)	Tok/s 52421 (37803)	Loss/tok 3.3610 (3.0801)	LR 1.250e-04
0: TRAIN [4][300/3880]	Time 0.137 (0.182)	Data 7.63e-05 (2.38e-03)	Tok/s 20238 (37829)	Loss/tok 2.4168 (3.0788)	LR 1.250e-04
0: TRAIN [4][310/3880]	Time 0.162 (0.183)	Data 7.89e-05 (2.30e-03)	Tok/s 33149 (38004)	Loss/tok 3.0548 (3.0802)	LR 1.250e-04
0: TRAIN [4][320/3880]	Time 0.191 (0.182)	Data 8.15e-05 (2.23e-03)	Tok/s 43350 (37883)	Loss/tok 3.2205 (3.0788)	LR 1.250e-04
0: TRAIN [4][330/3880]	Time 0.191 (0.182)	Data 7.56e-05 (2.17e-03)	Tok/s 43929 (37780)	Loss/tok 3.1452 (3.0785)	LR 1.250e-04
0: TRAIN [4][340/3880]	Time 0.162 (0.181)	Data 9.11e-05 (2.11e-03)	Tok/s 32344 (37627)	Loss/tok 2.8158 (3.0763)	LR 1.250e-04
0: TRAIN [4][350/3880]	Time 0.162 (0.182)	Data 8.96e-05 (2.05e-03)	Tok/s 32350 (37755)	Loss/tok 2.8705 (3.0783)	LR 1.250e-04
0: TRAIN [4][360/3880]	Time 0.162 (0.182)	Data 9.99e-05 (2.00e-03)	Tok/s 32937 (37855)	Loss/tok 2.9009 (3.0831)	LR 1.250e-04
0: TRAIN [4][370/3880]	Time 0.162 (0.182)	Data 1.01e-04 (1.94e-03)	Tok/s 31177 (37788)	Loss/tok 2.8025 (3.0811)	LR 1.250e-04
0: TRAIN [4][380/3880]	Time 0.137 (0.182)	Data 7.56e-05 (1.90e-03)	Tok/s 18967 (37727)	Loss/tok 2.5697 (3.0802)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][390/3880]	Time 0.220 (0.181)	Data 7.41e-05 (1.85e-03)	Tok/s 53990 (37662)	Loss/tok 3.3304 (3.0784)	LR 1.250e-04
0: TRAIN [4][400/3880]	Time 0.191 (0.181)	Data 2.44e-04 (1.81e-03)	Tok/s 43968 (37667)	Loss/tok 2.8845 (3.0770)	LR 1.250e-04
0: TRAIN [4][410/3880]	Time 0.162 (0.181)	Data 9.82e-05 (1.77e-03)	Tok/s 31918 (37637)	Loss/tok 2.9016 (3.0771)	LR 1.250e-04
0: TRAIN [4][420/3880]	Time 0.162 (0.181)	Data 9.32e-05 (1.73e-03)	Tok/s 31277 (37480)	Loss/tok 2.8349 (3.0742)	LR 1.250e-04
0: TRAIN [4][430/3880]	Time 0.221 (0.181)	Data 1.26e-04 (1.69e-03)	Tok/s 53024 (37590)	Loss/tok 3.2417 (3.0769)	LR 1.250e-04
0: TRAIN [4][440/3880]	Time 0.162 (0.181)	Data 7.92e-05 (1.65e-03)	Tok/s 31803 (37573)	Loss/tok 2.9930 (3.0745)	LR 1.250e-04
0: TRAIN [4][450/3880]	Time 0.257 (0.181)	Data 7.46e-05 (1.62e-03)	Tok/s 57589 (37472)	Loss/tok 3.6084 (3.0743)	LR 1.250e-04
0: TRAIN [4][460/3880]	Time 0.258 (0.181)	Data 1.07e-04 (1.59e-03)	Tok/s 58251 (37589)	Loss/tok 3.2833 (3.0756)	LR 1.250e-04
0: TRAIN [4][470/3880]	Time 0.191 (0.181)	Data 8.63e-05 (1.55e-03)	Tok/s 44645 (37715)	Loss/tok 3.0858 (3.0763)	LR 1.250e-04
0: TRAIN [4][480/3880]	Time 0.191 (0.181)	Data 4.68e-04 (1.53e-03)	Tok/s 43961 (37779)	Loss/tok 3.1533 (3.0762)	LR 1.250e-04
0: TRAIN [4][490/3880]	Time 0.162 (0.181)	Data 7.77e-05 (1.50e-03)	Tok/s 31832 (37861)	Loss/tok 2.8140 (3.0758)	LR 1.250e-04
0: TRAIN [4][500/3880]	Time 0.221 (0.181)	Data 1.55e-04 (1.47e-03)	Tok/s 51742 (37874)	Loss/tok 3.6052 (3.0759)	LR 1.250e-04
0: TRAIN [4][510/3880]	Time 0.162 (0.181)	Data 9.25e-05 (1.44e-03)	Tok/s 31304 (37803)	Loss/tok 2.9890 (3.0735)	LR 1.250e-04
0: TRAIN [4][520/3880]	Time 0.136 (0.181)	Data 1.03e-04 (1.42e-03)	Tok/s 19324 (37780)	Loss/tok 2.3258 (3.0731)	LR 1.250e-04
0: TRAIN [4][530/3880]	Time 0.162 (0.181)	Data 1.24e-04 (1.39e-03)	Tok/s 32396 (37705)	Loss/tok 3.0211 (3.0726)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][540/3880]	Time 0.220 (0.181)	Data 8.23e-05 (1.37e-03)	Tok/s 51624 (37745)	Loss/tok 3.2836 (3.0740)	LR 1.250e-04
0: TRAIN [4][550/3880]	Time 0.191 (0.181)	Data 1.03e-04 (1.35e-03)	Tok/s 43713 (37722)	Loss/tok 3.0528 (3.0725)	LR 1.250e-04
0: TRAIN [4][560/3880]	Time 0.162 (0.181)	Data 1.22e-04 (1.33e-03)	Tok/s 30601 (37770)	Loss/tok 2.8817 (3.0741)	LR 1.250e-04
0: TRAIN [4][570/3880]	Time 0.162 (0.181)	Data 1.09e-04 (1.30e-03)	Tok/s 31835 (37898)	Loss/tok 2.8224 (3.0755)	LR 1.250e-04
0: TRAIN [4][580/3880]	Time 0.258 (0.181)	Data 7.65e-05 (1.28e-03)	Tok/s 57516 (37874)	Loss/tok 3.3452 (3.0759)	LR 1.250e-04
0: TRAIN [4][590/3880]	Time 0.191 (0.181)	Data 9.68e-05 (1.26e-03)	Tok/s 43131 (37961)	Loss/tok 2.9882 (3.0763)	LR 1.250e-04
0: TRAIN [4][600/3880]	Time 0.162 (0.181)	Data 1.14e-04 (1.24e-03)	Tok/s 31588 (37929)	Loss/tok 2.8816 (3.0749)	LR 1.250e-04
0: TRAIN [4][610/3880]	Time 0.162 (0.181)	Data 9.08e-05 (1.23e-03)	Tok/s 31746 (37946)	Loss/tok 2.9880 (3.0757)	LR 1.250e-04
0: TRAIN [4][620/3880]	Time 0.221 (0.181)	Data 1.14e-04 (1.21e-03)	Tok/s 53287 (37973)	Loss/tok 3.0381 (3.0755)	LR 1.250e-04
0: TRAIN [4][630/3880]	Time 0.191 (0.182)	Data 8.06e-05 (1.19e-03)	Tok/s 44010 (38058)	Loss/tok 2.9709 (3.0767)	LR 1.250e-04
0: TRAIN [4][640/3880]	Time 0.221 (0.181)	Data 1.27e-04 (1.17e-03)	Tok/s 52970 (38048)	Loss/tok 3.1473 (3.0755)	LR 1.250e-04
0: TRAIN [4][650/3880]	Time 0.136 (0.181)	Data 1.01e-04 (1.16e-03)	Tok/s 19447 (37989)	Loss/tok 2.4659 (3.0738)	LR 1.250e-04
0: TRAIN [4][660/3880]	Time 0.191 (0.181)	Data 7.68e-05 (1.14e-03)	Tok/s 44247 (37983)	Loss/tok 2.8855 (3.0724)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][670/3880]	Time 0.221 (0.181)	Data 1.11e-04 (1.13e-03)	Tok/s 53133 (38021)	Loss/tok 3.2789 (3.0735)	LR 1.250e-04
0: TRAIN [4][680/3880]	Time 0.136 (0.181)	Data 1.02e-04 (1.11e-03)	Tok/s 19707 (38021)	Loss/tok 2.5325 (3.0741)	LR 1.250e-04
0: TRAIN [4][690/3880]	Time 0.162 (0.181)	Data 7.94e-05 (1.10e-03)	Tok/s 32688 (38060)	Loss/tok 2.8933 (3.0758)	LR 1.250e-04
0: TRAIN [4][700/3880]	Time 0.137 (0.181)	Data 9.25e-05 (1.08e-03)	Tok/s 19388 (38086)	Loss/tok 2.5100 (3.0767)	LR 1.250e-04
0: TRAIN [4][710/3880]	Time 0.163 (0.181)	Data 1.43e-04 (1.07e-03)	Tok/s 31834 (38059)	Loss/tok 2.9146 (3.0755)	LR 1.250e-04
0: TRAIN [4][720/3880]	Time 0.221 (0.181)	Data 7.82e-05 (1.05e-03)	Tok/s 52715 (38021)	Loss/tok 3.0765 (3.0749)	LR 1.250e-04
0: TRAIN [4][730/3880]	Time 0.162 (0.181)	Data 7.70e-05 (1.04e-03)	Tok/s 31948 (38033)	Loss/tok 2.8434 (3.0758)	LR 1.250e-04
0: TRAIN [4][740/3880]	Time 0.162 (0.181)	Data 7.96e-05 (1.03e-03)	Tok/s 32046 (38043)	Loss/tok 2.8674 (3.0761)	LR 1.250e-04
0: TRAIN [4][750/3880]	Time 0.162 (0.181)	Data 1.00e-04 (1.02e-03)	Tok/s 32019 (38020)	Loss/tok 2.9630 (3.0753)	LR 1.250e-04
0: TRAIN [4][760/3880]	Time 0.164 (0.181)	Data 7.58e-05 (1.00e-03)	Tok/s 31545 (37956)	Loss/tok 2.9564 (3.0747)	LR 1.250e-04
0: TRAIN [4][770/3880]	Time 0.163 (0.181)	Data 7.70e-05 (9.93e-04)	Tok/s 31862 (37972)	Loss/tok 2.8799 (3.0749)	LR 1.250e-04
0: TRAIN [4][780/3880]	Time 0.256 (0.181)	Data 8.82e-05 (9.81e-04)	Tok/s 59503 (38077)	Loss/tok 3.2751 (3.0789)	LR 1.250e-04
0: TRAIN [4][790/3880]	Time 0.191 (0.182)	Data 8.34e-05 (9.70e-04)	Tok/s 43549 (38111)	Loss/tok 3.0063 (3.0786)	LR 1.250e-04
0: TRAIN [4][800/3880]	Time 0.191 (0.182)	Data 8.03e-05 (9.60e-04)	Tok/s 45087 (38166)	Loss/tok 3.0826 (3.0796)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][810/3880]	Time 0.136 (0.182)	Data 8.58e-05 (9.49e-04)	Tok/s 19589 (38191)	Loss/tok 2.5814 (3.0802)	LR 1.250e-04
0: TRAIN [4][820/3880]	Time 0.162 (0.182)	Data 8.58e-05 (9.38e-04)	Tok/s 32391 (38191)	Loss/tok 2.9137 (3.0793)	LR 1.250e-04
0: TRAIN [4][830/3880]	Time 0.191 (0.182)	Data 8.85e-05 (9.28e-04)	Tok/s 43802 (38186)	Loss/tok 3.0329 (3.0792)	LR 1.250e-04
0: TRAIN [4][840/3880]	Time 0.162 (0.182)	Data 1.42e-04 (9.18e-04)	Tok/s 32469 (38169)	Loss/tok 2.9154 (3.0792)	LR 1.250e-04
0: TRAIN [4][850/3880]	Time 0.191 (0.182)	Data 7.94e-05 (9.09e-04)	Tok/s 43617 (38222)	Loss/tok 3.2183 (3.0801)	LR 1.250e-04
0: TRAIN [4][860/3880]	Time 0.257 (0.182)	Data 7.61e-05 (8.99e-04)	Tok/s 58269 (38244)	Loss/tok 3.3838 (3.0804)	LR 1.250e-04
0: TRAIN [4][870/3880]	Time 0.136 (0.182)	Data 7.70e-05 (8.90e-04)	Tok/s 19347 (38217)	Loss/tok 2.4632 (3.0796)	LR 1.250e-04
0: TRAIN [4][880/3880]	Time 0.162 (0.182)	Data 9.25e-05 (8.81e-04)	Tok/s 31747 (38241)	Loss/tok 2.8767 (3.0802)	LR 1.250e-04
0: TRAIN [4][890/3880]	Time 0.192 (0.182)	Data 7.49e-05 (8.72e-04)	Tok/s 43496 (38272)	Loss/tok 3.1362 (3.0803)	LR 1.250e-04
0: TRAIN [4][900/3880]	Time 0.162 (0.182)	Data 7.68e-05 (8.64e-04)	Tok/s 31812 (38243)	Loss/tok 2.8537 (3.0798)	LR 1.250e-04
0: TRAIN [4][910/3880]	Time 0.222 (0.182)	Data 9.44e-05 (8.56e-04)	Tok/s 52350 (38286)	Loss/tok 3.2791 (3.0797)	LR 1.250e-04
0: TRAIN [4][920/3880]	Time 0.220 (0.182)	Data 7.58e-05 (8.47e-04)	Tok/s 52832 (38297)	Loss/tok 3.1766 (3.0803)	LR 1.250e-04
0: TRAIN [4][930/3880]	Time 0.221 (0.182)	Data 8.54e-05 (8.40e-04)	Tok/s 53063 (38305)	Loss/tok 3.1517 (3.0806)	LR 1.250e-04
0: TRAIN [4][940/3880]	Time 0.162 (0.182)	Data 9.20e-05 (8.31e-04)	Tok/s 31631 (38280)	Loss/tok 2.8424 (3.0798)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][950/3880]	Time 0.163 (0.182)	Data 1.04e-04 (8.24e-04)	Tok/s 32381 (38251)	Loss/tok 2.9367 (3.0797)	LR 1.250e-04
0: TRAIN [4][960/3880]	Time 0.221 (0.182)	Data 7.87e-05 (8.16e-04)	Tok/s 53049 (38289)	Loss/tok 3.2824 (3.0811)	LR 1.250e-04
0: TRAIN [4][970/3880]	Time 0.163 (0.182)	Data 3.89e-04 (8.09e-04)	Tok/s 31873 (38293)	Loss/tok 2.8994 (3.0810)	LR 1.250e-04
0: TRAIN [4][980/3880]	Time 0.163 (0.182)	Data 9.75e-05 (8.01e-04)	Tok/s 31819 (38301)	Loss/tok 3.0035 (3.0814)	LR 1.250e-04
0: TRAIN [4][990/3880]	Time 0.191 (0.182)	Data 8.11e-05 (7.94e-04)	Tok/s 43952 (38333)	Loss/tok 3.0753 (3.0825)	LR 1.250e-04
0: TRAIN [4][1000/3880]	Time 0.221 (0.182)	Data 1.21e-04 (7.87e-04)	Tok/s 52954 (38317)	Loss/tok 3.2257 (3.0823)	LR 1.250e-04
0: TRAIN [4][1010/3880]	Time 0.162 (0.182)	Data 8.39e-05 (7.80e-04)	Tok/s 31725 (38336)	Loss/tok 2.8848 (3.0820)	LR 1.250e-04
0: TRAIN [4][1020/3880]	Time 0.256 (0.182)	Data 9.44e-05 (7.74e-04)	Tok/s 57537 (38384)	Loss/tok 3.5063 (3.0837)	LR 1.250e-04
0: TRAIN [4][1030/3880]	Time 0.191 (0.182)	Data 9.58e-05 (7.67e-04)	Tok/s 43464 (38380)	Loss/tok 3.0597 (3.0836)	LR 1.250e-04
0: TRAIN [4][1040/3880]	Time 0.137 (0.182)	Data 9.44e-05 (7.61e-04)	Tok/s 18218 (38354)	Loss/tok 2.5133 (3.0832)	LR 1.250e-04
0: TRAIN [4][1050/3880]	Time 0.191 (0.182)	Data 7.94e-05 (7.55e-04)	Tok/s 44524 (38381)	Loss/tok 3.1037 (3.0839)	LR 1.250e-04
0: TRAIN [4][1060/3880]	Time 0.257 (0.182)	Data 8.73e-05 (7.48e-04)	Tok/s 58167 (38373)	Loss/tok 3.3050 (3.0842)	LR 1.250e-04
0: TRAIN [4][1070/3880]	Time 0.257 (0.182)	Data 8.68e-05 (7.42e-04)	Tok/s 58997 (38344)	Loss/tok 3.3264 (3.0836)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1080/3880]	Time 0.191 (0.182)	Data 7.89e-05 (7.36e-04)	Tok/s 44095 (38333)	Loss/tok 3.1321 (3.0835)	LR 1.250e-04
0: TRAIN [4][1090/3880]	Time 0.257 (0.182)	Data 8.73e-05 (7.31e-04)	Tok/s 58343 (38354)	Loss/tok 3.2704 (3.0840)	LR 1.250e-04
0: TRAIN [4][1100/3880]	Time 0.221 (0.182)	Data 9.35e-05 (7.25e-04)	Tok/s 52439 (38354)	Loss/tok 3.2102 (3.0835)	LR 1.250e-04
0: TRAIN [4][1110/3880]	Time 0.220 (0.182)	Data 1.18e-04 (7.19e-04)	Tok/s 52625 (38381)	Loss/tok 3.1745 (3.0843)	LR 1.250e-04
0: TRAIN [4][1120/3880]	Time 0.191 (0.182)	Data 9.39e-05 (7.14e-04)	Tok/s 43578 (38386)	Loss/tok 3.0694 (3.0841)	LR 1.250e-04
0: TRAIN [4][1130/3880]	Time 0.162 (0.182)	Data 1.07e-04 (7.08e-04)	Tok/s 32029 (38339)	Loss/tok 2.8625 (3.0839)	LR 1.250e-04
0: TRAIN [4][1140/3880]	Time 0.190 (0.182)	Data 7.56e-05 (7.03e-04)	Tok/s 44699 (38358)	Loss/tok 3.0957 (3.0842)	LR 1.250e-04
0: TRAIN [4][1150/3880]	Time 0.162 (0.182)	Data 7.41e-05 (6.97e-04)	Tok/s 31407 (38341)	Loss/tok 2.6810 (3.0840)	LR 1.250e-04
0: TRAIN [4][1160/3880]	Time 0.136 (0.182)	Data 7.51e-05 (6.92e-04)	Tok/s 19615 (38324)	Loss/tok 2.3979 (3.0836)	LR 1.250e-04
0: TRAIN [4][1170/3880]	Time 0.256 (0.182)	Data 8.11e-05 (6.87e-04)	Tok/s 57772 (38300)	Loss/tok 3.5718 (3.0842)	LR 1.250e-04
0: TRAIN [4][1180/3880]	Time 0.162 (0.182)	Data 1.22e-04 (6.82e-04)	Tok/s 31630 (38298)	Loss/tok 2.8542 (3.0837)	LR 1.250e-04
0: TRAIN [4][1190/3880]	Time 0.136 (0.182)	Data 9.20e-05 (6.77e-04)	Tok/s 19621 (38279)	Loss/tok 2.5040 (3.0837)	LR 1.250e-04
0: TRAIN [4][1200/3880]	Time 0.136 (0.182)	Data 7.89e-05 (6.72e-04)	Tok/s 19201 (38294)	Loss/tok 2.6071 (3.0837)	LR 1.250e-04
0: TRAIN [4][1210/3880]	Time 0.136 (0.182)	Data 7.89e-05 (6.68e-04)	Tok/s 19294 (38241)	Loss/tok 2.4895 (3.0827)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1220/3880]	Time 0.191 (0.182)	Data 8.03e-05 (6.63e-04)	Tok/s 43995 (38245)	Loss/tok 3.0147 (3.0824)	LR 1.250e-04
0: TRAIN [4][1230/3880]	Time 0.191 (0.182)	Data 8.18e-05 (6.58e-04)	Tok/s 44716 (38259)	Loss/tok 3.0852 (3.0824)	LR 1.250e-04
0: TRAIN [4][1240/3880]	Time 0.136 (0.182)	Data 8.08e-05 (6.54e-04)	Tok/s 19124 (38238)	Loss/tok 2.5105 (3.0818)	LR 1.250e-04
0: TRAIN [4][1250/3880]	Time 0.162 (0.181)	Data 8.11e-05 (6.49e-04)	Tok/s 32815 (38213)	Loss/tok 2.9075 (3.0811)	LR 1.250e-04
0: TRAIN [4][1260/3880]	Time 0.191 (0.181)	Data 9.51e-05 (6.45e-04)	Tok/s 44147 (38222)	Loss/tok 3.0472 (3.0807)	LR 1.250e-04
0: TRAIN [4][1270/3880]	Time 0.191 (0.181)	Data 1.10e-04 (6.40e-04)	Tok/s 44854 (38210)	Loss/tok 3.1883 (3.0803)	LR 1.250e-04
0: TRAIN [4][1280/3880]	Time 0.191 (0.181)	Data 1.01e-04 (6.36e-04)	Tok/s 44745 (38227)	Loss/tok 3.0698 (3.0803)	LR 1.250e-04
0: TRAIN [4][1290/3880]	Time 0.162 (0.181)	Data 8.49e-05 (6.32e-04)	Tok/s 32416 (38194)	Loss/tok 2.9896 (3.0801)	LR 1.250e-04
0: TRAIN [4][1300/3880]	Time 0.162 (0.181)	Data 7.92e-05 (6.28e-04)	Tok/s 31601 (38193)	Loss/tok 2.9755 (3.0802)	LR 1.250e-04
0: TRAIN [4][1310/3880]	Time 0.221 (0.181)	Data 8.08e-05 (6.24e-04)	Tok/s 54213 (38255)	Loss/tok 3.0626 (3.0814)	LR 1.250e-04
0: TRAIN [4][1320/3880]	Time 0.191 (0.181)	Data 7.84e-05 (6.20e-04)	Tok/s 44679 (38214)	Loss/tok 3.0529 (3.0809)	LR 1.250e-04
0: TRAIN [4][1330/3880]	Time 0.162 (0.181)	Data 7.84e-05 (6.16e-04)	Tok/s 31686 (38220)	Loss/tok 2.9682 (3.0806)	LR 1.250e-04
0: TRAIN [4][1340/3880]	Time 0.136 (0.181)	Data 7.82e-05 (6.12e-04)	Tok/s 19322 (38227)	Loss/tok 2.5830 (3.0813)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1350/3880]	Time 0.164 (0.181)	Data 8.68e-05 (6.08e-04)	Tok/s 32277 (38246)	Loss/tok 2.9272 (3.0813)	LR 1.250e-04
0: TRAIN [4][1360/3880]	Time 0.162 (0.181)	Data 7.58e-05 (6.04e-04)	Tok/s 32273 (38218)	Loss/tok 2.9505 (3.0808)	LR 1.250e-04
0: TRAIN [4][1370/3880]	Time 0.221 (0.181)	Data 9.47e-05 (6.01e-04)	Tok/s 53045 (38226)	Loss/tok 3.1670 (3.0813)	LR 1.250e-04
0: TRAIN [4][1380/3880]	Time 0.162 (0.181)	Data 9.11e-05 (5.97e-04)	Tok/s 31871 (38233)	Loss/tok 2.7568 (3.0814)	LR 1.250e-04
0: TRAIN [4][1390/3880]	Time 0.191 (0.181)	Data 8.61e-05 (5.94e-04)	Tok/s 44388 (38230)	Loss/tok 2.9907 (3.0805)	LR 1.250e-04
0: TRAIN [4][1400/3880]	Time 0.162 (0.181)	Data 7.68e-05 (5.90e-04)	Tok/s 31514 (38258)	Loss/tok 2.7601 (3.0810)	LR 1.250e-04
0: TRAIN [4][1410/3880]	Time 0.191 (0.181)	Data 8.75e-05 (5.87e-04)	Tok/s 44009 (38271)	Loss/tok 3.0867 (3.0806)	LR 1.250e-04
0: TRAIN [4][1420/3880]	Time 0.222 (0.182)	Data 9.47e-05 (5.83e-04)	Tok/s 51801 (38299)	Loss/tok 3.1259 (3.0813)	LR 1.250e-04
0: TRAIN [4][1430/3880]	Time 0.162 (0.182)	Data 9.99e-05 (5.80e-04)	Tok/s 32013 (38301)	Loss/tok 2.9358 (3.0812)	LR 1.250e-04
0: TRAIN [4][1440/3880]	Time 0.162 (0.182)	Data 8.46e-05 (5.77e-04)	Tok/s 32299 (38296)	Loss/tok 2.8571 (3.0806)	LR 1.250e-04
0: TRAIN [4][1450/3880]	Time 0.162 (0.181)	Data 7.75e-05 (5.74e-04)	Tok/s 31813 (38290)	Loss/tok 2.9786 (3.0806)	LR 1.250e-04
0: TRAIN [4][1460/3880]	Time 0.192 (0.181)	Data 7.53e-05 (5.70e-04)	Tok/s 43112 (38290)	Loss/tok 3.2546 (3.0808)	LR 1.250e-04
0: TRAIN [4][1470/3880]	Time 0.162 (0.182)	Data 7.92e-05 (5.67e-04)	Tok/s 32793 (38317)	Loss/tok 2.7965 (3.0817)	LR 1.250e-04
0: TRAIN [4][1480/3880]	Time 0.162 (0.182)	Data 7.61e-05 (5.64e-04)	Tok/s 31633 (38320)	Loss/tok 2.8026 (3.0813)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1490/3880]	Time 0.257 (0.182)	Data 8.75e-05 (5.61e-04)	Tok/s 57508 (38341)	Loss/tok 3.4098 (3.0818)	LR 1.250e-04
0: TRAIN [4][1500/3880]	Time 0.191 (0.182)	Data 7.82e-05 (5.58e-04)	Tok/s 44237 (38341)	Loss/tok 3.0581 (3.0818)	LR 1.250e-04
0: TRAIN [4][1510/3880]	Time 0.162 (0.182)	Data 8.75e-05 (5.55e-04)	Tok/s 31323 (38337)	Loss/tok 2.9291 (3.0824)	LR 1.250e-04
0: TRAIN [4][1520/3880]	Time 0.191 (0.181)	Data 9.18e-05 (5.52e-04)	Tok/s 43174 (38302)	Loss/tok 3.1555 (3.0817)	LR 1.250e-04
0: TRAIN [4][1530/3880]	Time 0.222 (0.181)	Data 3.42e-04 (5.49e-04)	Tok/s 51891 (38308)	Loss/tok 3.3177 (3.0816)	LR 1.250e-04
0: TRAIN [4][1540/3880]	Time 0.163 (0.181)	Data 8.23e-05 (5.46e-04)	Tok/s 31896 (38294)	Loss/tok 3.1246 (3.0812)	LR 1.250e-04
0: TRAIN [4][1550/3880]	Time 0.221 (0.182)	Data 9.08e-05 (5.43e-04)	Tok/s 52718 (38321)	Loss/tok 3.3214 (3.0816)	LR 1.250e-04
0: TRAIN [4][1560/3880]	Time 0.162 (0.182)	Data 8.77e-05 (5.41e-04)	Tok/s 31531 (38334)	Loss/tok 2.8753 (3.0819)	LR 1.250e-04
0: TRAIN [4][1570/3880]	Time 0.162 (0.181)	Data 9.01e-05 (5.38e-04)	Tok/s 31530 (38317)	Loss/tok 2.8283 (3.0816)	LR 1.250e-04
0: TRAIN [4][1580/3880]	Time 0.191 (0.182)	Data 9.66e-05 (5.35e-04)	Tok/s 42778 (38350)	Loss/tok 3.0877 (3.0818)	LR 1.250e-04
0: TRAIN [4][1590/3880]	Time 0.221 (0.182)	Data 8.75e-05 (5.32e-04)	Tok/s 52188 (38375)	Loss/tok 3.2448 (3.0825)	LR 1.250e-04
0: TRAIN [4][1600/3880]	Time 0.136 (0.182)	Data 1.12e-04 (5.30e-04)	Tok/s 19051 (38380)	Loss/tok 2.4903 (3.0823)	LR 1.250e-04
0: TRAIN [4][1610/3880]	Time 0.136 (0.182)	Data 1.12e-04 (5.27e-04)	Tok/s 19532 (38399)	Loss/tok 2.5900 (3.0832)	LR 1.250e-04
0: TRAIN [4][1620/3880]	Time 0.162 (0.182)	Data 9.85e-05 (5.25e-04)	Tok/s 32329 (38402)	Loss/tok 2.9207 (3.0831)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1630/3880]	Time 0.221 (0.182)	Data 7.82e-05 (5.22e-04)	Tok/s 52883 (38452)	Loss/tok 3.1239 (3.0843)	LR 1.250e-04
0: TRAIN [4][1640/3880]	Time 0.162 (0.182)	Data 6.43e-04 (5.20e-04)	Tok/s 31696 (38447)	Loss/tok 2.8700 (3.0842)	LR 1.250e-04
0: TRAIN [4][1650/3880]	Time 0.191 (0.182)	Data 8.25e-05 (5.19e-04)	Tok/s 44398 (38399)	Loss/tok 3.1112 (3.0835)	LR 1.250e-04
0: TRAIN [4][1660/3880]	Time 0.162 (0.182)	Data 8.34e-05 (5.16e-04)	Tok/s 31420 (38379)	Loss/tok 2.9842 (3.0832)	LR 1.250e-04
0: TRAIN [4][1670/3880]	Time 0.162 (0.182)	Data 9.47e-05 (5.14e-04)	Tok/s 32503 (38371)	Loss/tok 3.0348 (3.0827)	LR 1.250e-04
0: TRAIN [4][1680/3880]	Time 0.191 (0.182)	Data 7.96e-05 (5.12e-04)	Tok/s 44657 (38360)	Loss/tok 3.0477 (3.0823)	LR 1.250e-04
0: TRAIN [4][1690/3880]	Time 0.162 (0.182)	Data 7.92e-05 (5.09e-04)	Tok/s 32557 (38356)	Loss/tok 2.7329 (3.0819)	LR 1.250e-04
0: TRAIN [4][1700/3880]	Time 0.162 (0.182)	Data 7.30e-05 (5.07e-04)	Tok/s 32197 (38353)	Loss/tok 2.8800 (3.0819)	LR 1.250e-04
0: TRAIN [4][1710/3880]	Time 0.221 (0.182)	Data 1.53e-04 (5.04e-04)	Tok/s 53162 (38385)	Loss/tok 3.2375 (3.0823)	LR 1.250e-04
0: TRAIN [4][1720/3880]	Time 0.220 (0.182)	Data 7.22e-05 (5.02e-04)	Tok/s 53653 (38396)	Loss/tok 3.1691 (3.0822)	LR 1.250e-04
0: TRAIN [4][1730/3880]	Time 0.162 (0.182)	Data 7.65e-05 (5.00e-04)	Tok/s 31902 (38375)	Loss/tok 2.9369 (3.0818)	LR 1.250e-04
0: TRAIN [4][1740/3880]	Time 0.221 (0.182)	Data 7.39e-05 (4.97e-04)	Tok/s 52596 (38364)	Loss/tok 3.2440 (3.0814)	LR 1.250e-04
0: TRAIN [4][1750/3880]	Time 0.191 (0.182)	Data 7.46e-05 (4.95e-04)	Tok/s 42994 (38386)	Loss/tok 3.2114 (3.0819)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1760/3880]	Time 0.191 (0.182)	Data 9.35e-05 (4.93e-04)	Tok/s 44458 (38416)	Loss/tok 3.0241 (3.0824)	LR 1.250e-04
0: TRAIN [4][1770/3880]	Time 0.162 (0.182)	Data 7.18e-05 (4.90e-04)	Tok/s 31653 (38394)	Loss/tok 2.8580 (3.0819)	LR 1.250e-04
0: TRAIN [4][1780/3880]	Time 0.257 (0.182)	Data 7.53e-05 (4.88e-04)	Tok/s 57159 (38390)	Loss/tok 3.6629 (3.0823)	LR 1.250e-04
0: TRAIN [4][1790/3880]	Time 0.221 (0.182)	Data 7.39e-05 (4.86e-04)	Tok/s 53093 (38362)	Loss/tok 3.2099 (3.0821)	LR 1.250e-04
0: TRAIN [4][1800/3880]	Time 0.162 (0.182)	Data 7.58e-05 (4.84e-04)	Tok/s 32094 (38365)	Loss/tok 2.9471 (3.0821)	LR 1.250e-04
0: TRAIN [4][1810/3880]	Time 0.191 (0.182)	Data 9.11e-05 (4.81e-04)	Tok/s 44380 (38386)	Loss/tok 3.0139 (3.0823)	LR 1.250e-04
0: TRAIN [4][1820/3880]	Time 0.162 (0.182)	Data 8.32e-05 (4.79e-04)	Tok/s 31163 (38392)	Loss/tok 2.8148 (3.0821)	LR 1.250e-04
0: TRAIN [4][1830/3880]	Time 0.191 (0.182)	Data 1.50e-04 (4.77e-04)	Tok/s 44282 (38402)	Loss/tok 3.0113 (3.0819)	LR 1.250e-04
0: TRAIN [4][1840/3880]	Time 0.191 (0.182)	Data 8.77e-05 (4.75e-04)	Tok/s 44721 (38398)	Loss/tok 2.9701 (3.0822)	LR 1.250e-04
0: TRAIN [4][1850/3880]	Time 0.221 (0.182)	Data 8.06e-05 (4.73e-04)	Tok/s 52478 (38392)	Loss/tok 3.4102 (3.0823)	LR 1.250e-04
0: TRAIN [4][1860/3880]	Time 0.162 (0.182)	Data 7.51e-05 (4.71e-04)	Tok/s 32805 (38422)	Loss/tok 2.9319 (3.0826)	LR 1.250e-04
0: TRAIN [4][1870/3880]	Time 0.162 (0.182)	Data 7.63e-05 (4.69e-04)	Tok/s 31153 (38439)	Loss/tok 2.9596 (3.0831)	LR 1.250e-04
0: TRAIN [4][1880/3880]	Time 0.191 (0.182)	Data 8.94e-05 (4.67e-04)	Tok/s 44313 (38448)	Loss/tok 3.0877 (3.0830)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1890/3880]	Time 0.187 (0.182)	Data 8.68e-05 (4.66e-04)	Tok/s 45565 (38453)	Loss/tok 3.2079 (3.0830)	LR 1.250e-04
0: TRAIN [4][1900/3880]	Time 0.162 (0.182)	Data 7.49e-05 (4.64e-04)	Tok/s 31348 (38421)	Loss/tok 2.8470 (3.0825)	LR 1.250e-04
0: TRAIN [4][1910/3880]	Time 0.191 (0.182)	Data 8.25e-05 (4.62e-04)	Tok/s 44135 (38410)	Loss/tok 2.9993 (3.0823)	LR 1.250e-04
0: TRAIN [4][1920/3880]	Time 0.137 (0.182)	Data 9.23e-05 (4.60e-04)	Tok/s 20038 (38433)	Loss/tok 2.5369 (3.0825)	LR 1.250e-04
0: TRAIN [4][1930/3880]	Time 0.162 (0.182)	Data 9.70e-05 (4.58e-04)	Tok/s 30635 (38451)	Loss/tok 2.9260 (3.0825)	LR 1.250e-04
0: TRAIN [4][1940/3880]	Time 0.162 (0.182)	Data 7.89e-05 (4.56e-04)	Tok/s 32551 (38446)	Loss/tok 2.8786 (3.0825)	LR 1.250e-04
0: TRAIN [4][1950/3880]	Time 0.192 (0.182)	Data 7.68e-05 (4.55e-04)	Tok/s 42494 (38443)	Loss/tok 3.2733 (3.0829)	LR 1.250e-04
0: TRAIN [4][1960/3880]	Time 0.191 (0.182)	Data 7.58e-05 (4.53e-04)	Tok/s 44769 (38433)	Loss/tok 3.1253 (3.0827)	LR 1.250e-04
0: TRAIN [4][1970/3880]	Time 0.191 (0.182)	Data 7.56e-05 (4.51e-04)	Tok/s 42541 (38421)	Loss/tok 3.1612 (3.0826)	LR 1.250e-04
0: TRAIN [4][1980/3880]	Time 0.192 (0.182)	Data 7.58e-05 (4.49e-04)	Tok/s 43804 (38431)	Loss/tok 3.0700 (3.0828)	LR 1.250e-04
0: TRAIN [4][1990/3880]	Time 0.136 (0.181)	Data 7.51e-05 (4.47e-04)	Tok/s 19516 (38397)	Loss/tok 2.4623 (3.0824)	LR 1.250e-04
0: TRAIN [4][2000/3880]	Time 0.220 (0.181)	Data 1.06e-04 (4.45e-04)	Tok/s 52685 (38404)	Loss/tok 3.3511 (3.0826)	LR 1.250e-04
0: TRAIN [4][2010/3880]	Time 0.191 (0.182)	Data 8.70e-05 (4.44e-04)	Tok/s 43989 (38419)	Loss/tok 3.1778 (3.0829)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2020/3880]	Time 0.220 (0.182)	Data 8.87e-05 (4.42e-04)	Tok/s 52757 (38427)	Loss/tok 3.2497 (3.0829)	LR 1.250e-04
0: TRAIN [4][2030/3880]	Time 0.192 (0.182)	Data 7.92e-05 (4.40e-04)	Tok/s 43116 (38450)	Loss/tok 3.0917 (3.0837)	LR 1.250e-04
0: TRAIN [4][2040/3880]	Time 0.191 (0.182)	Data 8.06e-05 (4.38e-04)	Tok/s 43963 (38452)	Loss/tok 3.0317 (3.0836)	LR 1.250e-04
0: TRAIN [4][2050/3880]	Time 0.162 (0.182)	Data 7.53e-05 (4.37e-04)	Tok/s 31840 (38469)	Loss/tok 2.9217 (3.0841)	LR 1.250e-04
0: TRAIN [4][2060/3880]	Time 0.221 (0.182)	Data 8.06e-05 (4.35e-04)	Tok/s 53602 (38474)	Loss/tok 3.3351 (3.0841)	LR 1.250e-04
0: TRAIN [4][2070/3880]	Time 0.162 (0.182)	Data 8.65e-05 (4.33e-04)	Tok/s 30497 (38465)	Loss/tok 2.9688 (3.0838)	LR 1.250e-04
0: TRAIN [4][2080/3880]	Time 0.191 (0.182)	Data 7.46e-05 (4.31e-04)	Tok/s 44485 (38470)	Loss/tok 3.0460 (3.0836)	LR 1.250e-04
0: TRAIN [4][2090/3880]	Time 0.191 (0.182)	Data 7.58e-05 (4.30e-04)	Tok/s 43323 (38473)	Loss/tok 3.1000 (3.0834)	LR 1.250e-04
0: TRAIN [4][2100/3880]	Time 0.136 (0.182)	Data 7.39e-05 (4.29e-04)	Tok/s 18792 (38474)	Loss/tok 2.5582 (3.0837)	LR 1.250e-04
0: TRAIN [4][2110/3880]	Time 0.162 (0.182)	Data 8.49e-05 (4.27e-04)	Tok/s 31877 (38501)	Loss/tok 2.9264 (3.0843)	LR 1.250e-04
0: TRAIN [4][2120/3880]	Time 0.193 (0.182)	Data 8.94e-05 (4.25e-04)	Tok/s 43103 (38529)	Loss/tok 3.0552 (3.0849)	LR 1.250e-04
0: TRAIN [4][2130/3880]	Time 0.162 (0.182)	Data 7.41e-05 (4.24e-04)	Tok/s 32075 (38514)	Loss/tok 2.7349 (3.0845)	LR 1.250e-04
0: TRAIN [4][2140/3880]	Time 0.191 (0.182)	Data 9.54e-05 (4.22e-04)	Tok/s 43496 (38494)	Loss/tok 3.1294 (3.0841)	LR 1.250e-04
0: TRAIN [4][2150/3880]	Time 0.162 (0.182)	Data 1.03e-04 (4.21e-04)	Tok/s 32145 (38497)	Loss/tok 2.9852 (3.0848)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2160/3880]	Time 0.191 (0.182)	Data 9.75e-05 (4.19e-04)	Tok/s 43666 (38532)	Loss/tok 3.0138 (3.0858)	LR 1.250e-04
0: TRAIN [4][2170/3880]	Time 0.191 (0.182)	Data 9.23e-05 (4.18e-04)	Tok/s 42975 (38553)	Loss/tok 2.9929 (3.0856)	LR 1.250e-04
0: TRAIN [4][2180/3880]	Time 0.191 (0.182)	Data 9.66e-05 (4.16e-04)	Tok/s 44356 (38522)	Loss/tok 2.9625 (3.0852)	LR 1.250e-04
0: TRAIN [4][2190/3880]	Time 0.162 (0.182)	Data 8.01e-05 (4.15e-04)	Tok/s 31301 (38533)	Loss/tok 2.8624 (3.0851)	LR 1.250e-04
0: TRAIN [4][2200/3880]	Time 0.162 (0.182)	Data 7.96e-05 (4.13e-04)	Tok/s 30899 (38518)	Loss/tok 2.8457 (3.0850)	LR 1.250e-04
0: TRAIN [4][2210/3880]	Time 0.191 (0.182)	Data 8.51e-05 (4.12e-04)	Tok/s 43550 (38515)	Loss/tok 3.0898 (3.0846)	LR 1.250e-04
0: TRAIN [4][2220/3880]	Time 0.191 (0.182)	Data 1.17e-04 (4.11e-04)	Tok/s 43716 (38485)	Loss/tok 3.1496 (3.0841)	LR 1.250e-04
0: TRAIN [4][2230/3880]	Time 0.162 (0.182)	Data 1.84e-04 (4.10e-04)	Tok/s 31212 (38480)	Loss/tok 2.8544 (3.0838)	LR 1.250e-04
0: TRAIN [4][2240/3880]	Time 0.162 (0.182)	Data 1.09e-04 (4.09e-04)	Tok/s 31980 (38486)	Loss/tok 2.8427 (3.0837)	LR 1.250e-04
0: TRAIN [4][2250/3880]	Time 0.136 (0.182)	Data 9.39e-05 (4.07e-04)	Tok/s 19717 (38497)	Loss/tok 2.4954 (3.0839)	LR 1.250e-04
0: TRAIN [4][2260/3880]	Time 0.194 (0.182)	Data 7.99e-05 (4.06e-04)	Tok/s 43686 (38514)	Loss/tok 3.1870 (3.0839)	LR 1.250e-04
0: TRAIN [4][2270/3880]	Time 0.162 (0.182)	Data 1.04e-04 (4.05e-04)	Tok/s 32278 (38532)	Loss/tok 2.9546 (3.0843)	LR 1.250e-04
0: TRAIN [4][2280/3880]	Time 0.163 (0.182)	Data 1.10e-04 (4.04e-04)	Tok/s 32562 (38536)	Loss/tok 3.0478 (3.0845)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2290/3880]	Time 0.162 (0.182)	Data 9.80e-05 (4.02e-04)	Tok/s 32455 (38539)	Loss/tok 2.8170 (3.0848)	LR 1.250e-04
0: TRAIN [4][2300/3880]	Time 0.191 (0.182)	Data 9.20e-05 (4.01e-04)	Tok/s 43220 (38544)	Loss/tok 3.2113 (3.0849)	LR 1.250e-04
0: TRAIN [4][2310/3880]	Time 0.257 (0.182)	Data 1.01e-04 (4.00e-04)	Tok/s 57868 (38526)	Loss/tok 3.4422 (3.0849)	LR 1.250e-04
0: TRAIN [4][2320/3880]	Time 0.221 (0.182)	Data 7.44e-05 (3.99e-04)	Tok/s 53260 (38532)	Loss/tok 3.3134 (3.0848)	LR 1.250e-04
0: TRAIN [4][2330/3880]	Time 0.163 (0.182)	Data 4.58e-04 (3.97e-04)	Tok/s 31283 (38524)	Loss/tok 2.9303 (3.0846)	LR 1.250e-04
0: TRAIN [4][2340/3880]	Time 0.191 (0.182)	Data 1.57e-04 (3.96e-04)	Tok/s 43637 (38532)	Loss/tok 3.0943 (3.0849)	LR 1.250e-04
0: TRAIN [4][2350/3880]	Time 0.163 (0.182)	Data 8.34e-05 (3.95e-04)	Tok/s 32291 (38535)	Loss/tok 2.9987 (3.0853)	LR 1.250e-04
0: TRAIN [4][2360/3880]	Time 0.191 (0.182)	Data 9.16e-05 (3.94e-04)	Tok/s 43835 (38518)	Loss/tok 3.2868 (3.0854)	LR 1.250e-04
0: TRAIN [4][2370/3880]	Time 0.221 (0.182)	Data 9.32e-05 (3.93e-04)	Tok/s 52477 (38538)	Loss/tok 3.2013 (3.0857)	LR 1.250e-04
0: TRAIN [4][2380/3880]	Time 0.164 (0.182)	Data 1.22e-04 (3.92e-04)	Tok/s 31387 (38514)	Loss/tok 2.9708 (3.0853)	LR 1.250e-04
0: TRAIN [4][2390/3880]	Time 0.191 (0.182)	Data 8.96e-05 (3.91e-04)	Tok/s 45020 (38544)	Loss/tok 3.0782 (3.0858)	LR 1.250e-04
0: TRAIN [4][2400/3880]	Time 0.162 (0.182)	Data 1.11e-04 (3.89e-04)	Tok/s 32990 (38546)	Loss/tok 3.0320 (3.0861)	LR 1.250e-04
0: TRAIN [4][2410/3880]	Time 0.162 (0.182)	Data 8.03e-05 (3.88e-04)	Tok/s 32002 (38554)	Loss/tok 2.9009 (3.0858)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2420/3880]	Time 0.191 (0.182)	Data 1.09e-04 (3.87e-04)	Tok/s 43981 (38567)	Loss/tok 3.1097 (3.0862)	LR 1.250e-04
0: TRAIN [4][2430/3880]	Time 0.256 (0.182)	Data 1.08e-04 (3.86e-04)	Tok/s 57746 (38560)	Loss/tok 3.5086 (3.0863)	LR 1.250e-04
0: TRAIN [4][2440/3880]	Time 0.191 (0.182)	Data 1.18e-04 (3.85e-04)	Tok/s 43349 (38549)	Loss/tok 3.1578 (3.0859)	LR 1.250e-04
0: TRAIN [4][2450/3880]	Time 0.137 (0.182)	Data 8.75e-05 (3.84e-04)	Tok/s 19730 (38532)	Loss/tok 2.5649 (3.0855)	LR 1.250e-04
0: TRAIN [4][2460/3880]	Time 0.163 (0.182)	Data 8.80e-05 (3.82e-04)	Tok/s 31980 (38548)	Loss/tok 2.8406 (3.0858)	LR 1.250e-04
0: TRAIN [4][2470/3880]	Time 0.191 (0.182)	Data 7.92e-05 (3.81e-04)	Tok/s 44830 (38540)	Loss/tok 3.1327 (3.0859)	LR 1.250e-04
0: TRAIN [4][2480/3880]	Time 0.191 (0.182)	Data 8.11e-05 (3.80e-04)	Tok/s 43777 (38527)	Loss/tok 2.9699 (3.0856)	LR 1.250e-04
0: TRAIN [4][2490/3880]	Time 0.162 (0.182)	Data 1.00e-04 (3.79e-04)	Tok/s 31919 (38525)	Loss/tok 2.8836 (3.0855)	LR 1.250e-04
0: TRAIN [4][2500/3880]	Time 0.162 (0.182)	Data 9.68e-05 (3.78e-04)	Tok/s 32462 (38545)	Loss/tok 2.7790 (3.0860)	LR 1.250e-04
0: TRAIN [4][2510/3880]	Time 0.165 (0.182)	Data 7.08e-04 (3.77e-04)	Tok/s 31580 (38553)	Loss/tok 3.0404 (3.0861)	LR 1.250e-04
0: TRAIN [4][2520/3880]	Time 0.191 (0.182)	Data 4.09e-04 (3.76e-04)	Tok/s 44181 (38542)	Loss/tok 3.1072 (3.0857)	LR 1.250e-04
0: TRAIN [4][2530/3880]	Time 0.258 (0.182)	Data 8.99e-05 (3.75e-04)	Tok/s 56831 (38560)	Loss/tok 3.5702 (3.0860)	LR 1.250e-04
0: TRAIN [4][2540/3880]	Time 0.162 (0.182)	Data 7.89e-05 (3.74e-04)	Tok/s 32454 (38538)	Loss/tok 3.0144 (3.0859)	LR 1.250e-04
0: TRAIN [4][2550/3880]	Time 0.161 (0.182)	Data 1.23e-04 (3.73e-04)	Tok/s 32307 (38512)	Loss/tok 2.6917 (3.0854)	LR 1.250e-04
0: TRAIN [4][2560/3880]	Time 0.191 (0.182)	Data 7.84e-05 (3.72e-04)	Tok/s 44132 (38496)	Loss/tok 3.0917 (3.0851)	LR 1.250e-04
0: TRAIN [4][2570/3880]	Time 0.191 (0.182)	Data 9.06e-05 (3.71e-04)	Tok/s 43638 (38488)	Loss/tok 3.1965 (3.0849)	LR 1.250e-04
0: TRAIN [4][2580/3880]	Time 0.221 (0.182)	Data 8.37e-05 (3.70e-04)	Tok/s 52872 (38485)	Loss/tok 3.3177 (3.0850)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2590/3880]	Time 0.162 (0.182)	Data 7.94e-05 (3.69e-04)	Tok/s 31911 (38489)	Loss/tok 3.1013 (3.0852)	LR 1.250e-04
0: TRAIN [4][2600/3880]	Time 0.191 (0.182)	Data 9.13e-05 (3.67e-04)	Tok/s 44740 (38498)	Loss/tok 3.0751 (3.0851)	LR 1.250e-04
0: TRAIN [4][2610/3880]	Time 0.162 (0.182)	Data 8.13e-05 (3.67e-04)	Tok/s 32054 (38510)	Loss/tok 2.8741 (3.0857)	LR 1.250e-04
0: TRAIN [4][2620/3880]	Time 0.256 (0.182)	Data 7.61e-05 (3.65e-04)	Tok/s 58480 (38516)	Loss/tok 3.2592 (3.0856)	LR 1.250e-04
0: TRAIN [4][2630/3880]	Time 0.137 (0.182)	Data 8.32e-05 (3.64e-04)	Tok/s 19189 (38495)	Loss/tok 2.6053 (3.0852)	LR 1.250e-04
0: TRAIN [4][2640/3880]	Time 0.162 (0.182)	Data 8.77e-05 (3.63e-04)	Tok/s 31474 (38479)	Loss/tok 2.9075 (3.0847)	LR 1.250e-04
0: TRAIN [4][2650/3880]	Time 0.162 (0.182)	Data 1.03e-04 (3.63e-04)	Tok/s 31224 (38486)	Loss/tok 3.0013 (3.0850)	LR 1.250e-04
0: TRAIN [4][2660/3880]	Time 0.191 (0.182)	Data 1.24e-04 (3.61e-04)	Tok/s 43883 (38482)	Loss/tok 3.0644 (3.0847)	LR 1.250e-04
0: TRAIN [4][2670/3880]	Time 0.137 (0.182)	Data 1.28e-04 (3.61e-04)	Tok/s 18737 (38486)	Loss/tok 2.4833 (3.0847)	LR 1.250e-04
0: TRAIN [4][2680/3880]	Time 0.164 (0.182)	Data 7.84e-05 (3.60e-04)	Tok/s 32175 (38474)	Loss/tok 2.8003 (3.0843)	LR 1.250e-04
0: TRAIN [4][2690/3880]	Time 0.191 (0.182)	Data 1.10e-04 (3.59e-04)	Tok/s 44318 (38474)	Loss/tok 3.0591 (3.0841)	LR 1.250e-04
0: TRAIN [4][2700/3880]	Time 0.136 (0.182)	Data 8.08e-05 (3.58e-04)	Tok/s 18894 (38479)	Loss/tok 2.4983 (3.0841)	LR 1.250e-04
0: TRAIN [4][2710/3880]	Time 0.257 (0.182)	Data 1.21e-04 (3.57e-04)	Tok/s 58548 (38501)	Loss/tok 3.3515 (3.0846)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2720/3880]	Time 0.256 (0.182)	Data 9.06e-05 (3.56e-04)	Tok/s 58208 (38510)	Loss/tok 3.5202 (3.0849)	LR 1.250e-04
0: TRAIN [4][2730/3880]	Time 0.162 (0.182)	Data 1.62e-04 (3.55e-04)	Tok/s 31577 (38489)	Loss/tok 3.2662 (3.0846)	LR 1.250e-04
0: TRAIN [4][2740/3880]	Time 0.257 (0.182)	Data 8.34e-05 (3.54e-04)	Tok/s 57674 (38474)	Loss/tok 3.4265 (3.0846)	LR 1.250e-04
0: TRAIN [4][2750/3880]	Time 0.191 (0.182)	Data 9.35e-05 (3.53e-04)	Tok/s 43904 (38495)	Loss/tok 3.0664 (3.0846)	LR 1.250e-04
0: TRAIN [4][2760/3880]	Time 0.136 (0.182)	Data 9.04e-05 (3.52e-04)	Tok/s 19729 (38519)	Loss/tok 2.4950 (3.0852)	LR 1.250e-04
0: TRAIN [4][2770/3880]	Time 0.163 (0.182)	Data 1.48e-04 (3.51e-04)	Tok/s 31776 (38507)	Loss/tok 3.1119 (3.0850)	LR 1.250e-04
0: TRAIN [4][2780/3880]	Time 0.162 (0.182)	Data 8.54e-05 (3.50e-04)	Tok/s 31779 (38501)	Loss/tok 2.9523 (3.0850)	LR 1.250e-04
0: TRAIN [4][2790/3880]	Time 0.162 (0.182)	Data 8.80e-05 (3.49e-04)	Tok/s 31509 (38509)	Loss/tok 2.9255 (3.0851)	LR 1.250e-04
0: TRAIN [4][2800/3880]	Time 0.191 (0.182)	Data 8.30e-05 (3.48e-04)	Tok/s 44538 (38494)	Loss/tok 3.1270 (3.0848)	LR 1.250e-04
0: TRAIN [4][2810/3880]	Time 0.137 (0.182)	Data 7.96e-05 (3.47e-04)	Tok/s 19130 (38512)	Loss/tok 2.4968 (3.0853)	LR 1.250e-04
0: TRAIN [4][2820/3880]	Time 0.162 (0.182)	Data 1.11e-04 (3.47e-04)	Tok/s 32010 (38512)	Loss/tok 2.8579 (3.0853)	LR 1.250e-04
0: TRAIN [4][2830/3880]	Time 0.162 (0.182)	Data 8.77e-05 (3.46e-04)	Tok/s 31681 (38514)	Loss/tok 2.9970 (3.0855)	LR 1.250e-04
0: TRAIN [4][2840/3880]	Time 0.137 (0.182)	Data 8.73e-05 (3.45e-04)	Tok/s 19521 (38503)	Loss/tok 2.6768 (3.0856)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2850/3880]	Time 0.191 (0.182)	Data 1.12e-04 (3.44e-04)	Tok/s 44981 (38495)	Loss/tok 3.0841 (3.0855)	LR 1.250e-04
0: TRAIN [4][2860/3880]	Time 0.191 (0.182)	Data 1.30e-04 (3.43e-04)	Tok/s 44175 (38502)	Loss/tok 3.0590 (3.0856)	LR 1.250e-04
0: TRAIN [4][2870/3880]	Time 0.191 (0.182)	Data 9.54e-05 (3.43e-04)	Tok/s 44400 (38512)	Loss/tok 2.9685 (3.0857)	LR 1.250e-04
0: TRAIN [4][2880/3880]	Time 0.191 (0.182)	Data 2.48e-04 (3.42e-04)	Tok/s 43485 (38528)	Loss/tok 3.0415 (3.0857)	LR 1.250e-04
0: TRAIN [4][2890/3880]	Time 0.192 (0.182)	Data 1.23e-04 (3.41e-04)	Tok/s 44017 (38517)	Loss/tok 3.0854 (3.0855)	LR 1.250e-04
0: TRAIN [4][2900/3880]	Time 0.221 (0.182)	Data 8.94e-05 (3.40e-04)	Tok/s 53909 (38525)	Loss/tok 3.1378 (3.0854)	LR 1.250e-04
0: TRAIN [4][2910/3880]	Time 0.221 (0.182)	Data 1.13e-04 (3.40e-04)	Tok/s 53765 (38530)	Loss/tok 3.1514 (3.0855)	LR 1.250e-04
0: TRAIN [4][2920/3880]	Time 0.162 (0.182)	Data 9.54e-05 (3.39e-04)	Tok/s 31071 (38522)	Loss/tok 2.8908 (3.0853)	LR 1.250e-04
0: TRAIN [4][2930/3880]	Time 0.162 (0.182)	Data 1.03e-04 (3.38e-04)	Tok/s 31156 (38516)	Loss/tok 2.9696 (3.0853)	LR 1.250e-04
0: TRAIN [4][2940/3880]	Time 0.256 (0.182)	Data 1.01e-04 (3.37e-04)	Tok/s 56383 (38526)	Loss/tok 3.4883 (3.0856)	LR 1.250e-04
0: TRAIN [4][2950/3880]	Time 0.162 (0.182)	Data 1.03e-04 (3.37e-04)	Tok/s 31118 (38529)	Loss/tok 2.8606 (3.0858)	LR 1.250e-04
0: TRAIN [4][2960/3880]	Time 0.162 (0.182)	Data 1.25e-04 (3.36e-04)	Tok/s 32803 (38512)	Loss/tok 2.8749 (3.0853)	LR 1.250e-04
0: TRAIN [4][2970/3880]	Time 0.191 (0.182)	Data 7.72e-05 (3.35e-04)	Tok/s 45232 (38540)	Loss/tok 2.9977 (3.0857)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2980/3880]	Time 0.162 (0.182)	Data 8.68e-05 (3.34e-04)	Tok/s 32411 (38545)	Loss/tok 2.7837 (3.0859)	LR 1.250e-04
0: TRAIN [4][2990/3880]	Time 0.221 (0.182)	Data 7.65e-05 (3.33e-04)	Tok/s 52329 (38558)	Loss/tok 3.3507 (3.0861)	LR 1.250e-04
0: TRAIN [4][3000/3880]	Time 0.221 (0.182)	Data 1.19e-04 (3.33e-04)	Tok/s 52949 (38590)	Loss/tok 3.3853 (3.0869)	LR 1.250e-04
0: TRAIN [4][3010/3880]	Time 0.191 (0.182)	Data 7.30e-05 (3.32e-04)	Tok/s 42860 (38588)	Loss/tok 3.2196 (3.0869)	LR 1.250e-04
0: TRAIN [4][3020/3880]	Time 0.191 (0.182)	Data 7.56e-05 (3.31e-04)	Tok/s 44208 (38597)	Loss/tok 3.0352 (3.0868)	LR 1.250e-04
0: TRAIN [4][3030/3880]	Time 0.190 (0.182)	Data 8.11e-05 (3.30e-04)	Tok/s 43642 (38586)	Loss/tok 3.1203 (3.0866)	LR 1.250e-04
0: TRAIN [4][3040/3880]	Time 0.162 (0.182)	Data 7.65e-05 (3.29e-04)	Tok/s 32528 (38596)	Loss/tok 2.8024 (3.0868)	LR 1.250e-04
0: TRAIN [4][3050/3880]	Time 0.191 (0.182)	Data 1.33e-04 (3.29e-04)	Tok/s 44706 (38594)	Loss/tok 3.0831 (3.0864)	LR 1.250e-04
0: TRAIN [4][3060/3880]	Time 0.191 (0.182)	Data 1.32e-04 (3.28e-04)	Tok/s 43577 (38602)	Loss/tok 2.9701 (3.0866)	LR 1.250e-04
0: TRAIN [4][3070/3880]	Time 0.191 (0.182)	Data 1.46e-04 (3.27e-04)	Tok/s 43317 (38583)	Loss/tok 3.0929 (3.0862)	LR 1.250e-04
0: TRAIN [4][3080/3880]	Time 0.191 (0.182)	Data 8.68e-05 (3.27e-04)	Tok/s 44238 (38568)	Loss/tok 2.8816 (3.0858)	LR 1.250e-04
0: TRAIN [4][3090/3880]	Time 0.221 (0.182)	Data 1.20e-04 (3.26e-04)	Tok/s 52499 (38578)	Loss/tok 3.3996 (3.0861)	LR 1.250e-04
0: TRAIN [4][3100/3880]	Time 0.137 (0.182)	Data 1.33e-04 (3.25e-04)	Tok/s 19148 (38562)	Loss/tok 2.5641 (3.0858)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3110/3880]	Time 0.162 (0.182)	Data 1.37e-04 (3.25e-04)	Tok/s 32268 (38570)	Loss/tok 2.9283 (3.0860)	LR 1.250e-04
0: TRAIN [4][3120/3880]	Time 0.162 (0.182)	Data 1.10e-04 (3.24e-04)	Tok/s 30736 (38552)	Loss/tok 2.8796 (3.0856)	LR 1.250e-04
0: TRAIN [4][3130/3880]	Time 0.191 (0.182)	Data 9.11e-05 (3.24e-04)	Tok/s 43665 (38543)	Loss/tok 3.1185 (3.0855)	LR 1.250e-04
0: TRAIN [4][3140/3880]	Time 0.191 (0.182)	Data 8.01e-05 (3.23e-04)	Tok/s 43964 (38540)	Loss/tok 3.1264 (3.0854)	LR 1.250e-04
0: TRAIN [4][3150/3880]	Time 0.136 (0.182)	Data 1.44e-04 (3.22e-04)	Tok/s 18886 (38532)	Loss/tok 2.4720 (3.0852)	LR 1.250e-04
0: TRAIN [4][3160/3880]	Time 0.221 (0.182)	Data 8.54e-05 (3.21e-04)	Tok/s 53168 (38543)	Loss/tok 3.0889 (3.0852)	LR 1.250e-04
0: TRAIN [4][3170/3880]	Time 0.190 (0.182)	Data 8.63e-05 (3.21e-04)	Tok/s 42762 (38543)	Loss/tok 3.1457 (3.0850)	LR 1.250e-04
0: TRAIN [4][3180/3880]	Time 0.191 (0.182)	Data 7.20e-05 (3.20e-04)	Tok/s 44981 (38540)	Loss/tok 3.1182 (3.0851)	LR 1.250e-04
0: TRAIN [4][3190/3880]	Time 0.191 (0.182)	Data 7.89e-05 (3.19e-04)	Tok/s 43803 (38529)	Loss/tok 2.9965 (3.0850)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][3200/3880]	Time 0.256 (0.182)	Data 7.51e-05 (3.18e-04)	Tok/s 58635 (38521)	Loss/tok 3.3297 (3.0848)	LR 1.250e-04
0: TRAIN [4][3210/3880]	Time 0.221 (0.182)	Data 1.17e-04 (3.18e-04)	Tok/s 52713 (38517)	Loss/tok 3.3934 (3.0847)	LR 1.250e-04
0: TRAIN [4][3220/3880]	Time 0.221 (0.182)	Data 7.80e-05 (3.17e-04)	Tok/s 53411 (38513)	Loss/tok 3.2140 (3.0846)	LR 1.250e-04
0: TRAIN [4][3230/3880]	Time 0.191 (0.182)	Data 7.61e-05 (3.16e-04)	Tok/s 44365 (38511)	Loss/tok 3.1811 (3.0845)	LR 1.250e-04
0: TRAIN [4][3240/3880]	Time 0.162 (0.182)	Data 9.04e-05 (3.16e-04)	Tok/s 31711 (38498)	Loss/tok 3.0482 (3.0842)	LR 1.250e-04
0: TRAIN [4][3250/3880]	Time 0.162 (0.182)	Data 8.49e-05 (3.15e-04)	Tok/s 32645 (38503)	Loss/tok 3.1300 (3.0842)	LR 1.250e-04
0: TRAIN [4][3260/3880]	Time 0.191 (0.182)	Data 8.01e-05 (3.14e-04)	Tok/s 44101 (38506)	Loss/tok 3.1604 (3.0843)	LR 1.250e-04
0: TRAIN [4][3270/3880]	Time 0.257 (0.182)	Data 2.28e-04 (3.14e-04)	Tok/s 58500 (38530)	Loss/tok 3.3335 (3.0848)	LR 1.250e-04
0: TRAIN [4][3280/3880]	Time 0.221 (0.182)	Data 9.06e-05 (3.13e-04)	Tok/s 52805 (38528)	Loss/tok 3.2255 (3.0848)	LR 1.250e-04
0: TRAIN [4][3290/3880]	Time 0.220 (0.182)	Data 7.49e-05 (3.12e-04)	Tok/s 52126 (38513)	Loss/tok 3.0721 (3.0845)	LR 1.250e-04
0: TRAIN [4][3300/3880]	Time 0.136 (0.182)	Data 7.53e-05 (3.11e-04)	Tok/s 19147 (38501)	Loss/tok 2.4102 (3.0842)	LR 1.250e-04
0: TRAIN [4][3310/3880]	Time 0.220 (0.182)	Data 7.75e-05 (3.11e-04)	Tok/s 53883 (38527)	Loss/tok 3.2292 (3.0851)	LR 1.250e-04
0: TRAIN [4][3320/3880]	Time 0.162 (0.182)	Data 7.27e-05 (3.10e-04)	Tok/s 32078 (38528)	Loss/tok 2.9952 (3.0853)	LR 1.250e-04
0: TRAIN [4][3330/3880]	Time 0.161 (0.182)	Data 1.07e-04 (3.09e-04)	Tok/s 32089 (38528)	Loss/tok 2.7902 (3.0852)	LR 1.250e-04
0: TRAIN [4][3340/3880]	Time 0.137 (0.182)	Data 1.34e-04 (3.09e-04)	Tok/s 19309 (38529)	Loss/tok 2.5369 (3.0854)	LR 1.250e-04
0: TRAIN [4][3350/3880]	Time 0.191 (0.182)	Data 8.87e-05 (3.08e-04)	Tok/s 44523 (38526)	Loss/tok 3.0644 (3.0852)	LR 1.250e-04
0: TRAIN [4][3360/3880]	Time 0.191 (0.182)	Data 8.70e-05 (3.08e-04)	Tok/s 43007 (38516)	Loss/tok 3.1142 (3.0851)	LR 1.250e-04
0: TRAIN [4][3370/3880]	Time 0.191 (0.182)	Data 9.92e-05 (3.07e-04)	Tok/s 44355 (38502)	Loss/tok 3.0303 (3.0848)	LR 1.250e-04
0: TRAIN [4][3380/3880]	Time 0.137 (0.181)	Data 9.80e-05 (3.06e-04)	Tok/s 19478 (38479)	Loss/tok 2.5145 (3.0845)	LR 1.250e-04
0: TRAIN [4][3390/3880]	Time 0.221 (0.181)	Data 1.59e-04 (3.06e-04)	Tok/s 52248 (38475)	Loss/tok 3.2383 (3.0844)	LR 1.250e-04
0: TRAIN [4][3400/3880]	Time 0.162 (0.182)	Data 7.99e-05 (3.05e-04)	Tok/s 30496 (38485)	Loss/tok 2.9196 (3.0846)	LR 1.250e-04
0: TRAIN [4][3410/3880]	Time 0.162 (0.182)	Data 7.65e-05 (3.05e-04)	Tok/s 31242 (38487)	Loss/tok 3.0300 (3.0847)	LR 1.250e-04
0: TRAIN [4][3420/3880]	Time 0.162 (0.182)	Data 1.02e-04 (3.04e-04)	Tok/s 31748 (38490)	Loss/tok 3.1727 (3.0845)	LR 1.250e-04
0: TRAIN [4][3430/3880]	Time 0.192 (0.181)	Data 7.53e-05 (3.04e-04)	Tok/s 43680 (38476)	Loss/tok 2.9995 (3.0842)	LR 1.250e-04
0: TRAIN [4][3440/3880]	Time 0.221 (0.181)	Data 9.06e-05 (3.03e-04)	Tok/s 51900 (38482)	Loss/tok 3.1966 (3.0841)	LR 1.250e-04
0: TRAIN [4][3450/3880]	Time 0.136 (0.181)	Data 1.09e-04 (3.02e-04)	Tok/s 19950 (38483)	Loss/tok 2.6059 (3.0841)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3460/3880]	Time 0.159 (0.181)	Data 5.06e-04 (3.02e-04)	Tok/s 33229 (38478)	Loss/tok 2.9047 (3.0840)	LR 1.250e-04
0: TRAIN [4][3470/3880]	Time 0.162 (0.181)	Data 1.01e-04 (3.02e-04)	Tok/s 32558 (38490)	Loss/tok 2.8532 (3.0843)	LR 1.250e-04
0: TRAIN [4][3480/3880]	Time 0.162 (0.182)	Data 8.46e-05 (3.01e-04)	Tok/s 31299 (38515)	Loss/tok 2.9490 (3.0848)	LR 1.250e-04
0: TRAIN [4][3490/3880]	Time 0.162 (0.182)	Data 1.08e-04 (3.00e-04)	Tok/s 32636 (38513)	Loss/tok 2.9525 (3.0846)	LR 1.250e-04
0: TRAIN [4][3500/3880]	Time 0.191 (0.182)	Data 1.04e-04 (3.00e-04)	Tok/s 44530 (38513)	Loss/tok 3.0718 (3.0847)	LR 1.250e-04
0: TRAIN [4][3510/3880]	Time 0.221 (0.182)	Data 8.44e-05 (2.99e-04)	Tok/s 53372 (38517)	Loss/tok 3.1287 (3.0848)	LR 1.250e-04
0: TRAIN [4][3520/3880]	Time 0.191 (0.182)	Data 9.73e-05 (2.99e-04)	Tok/s 44721 (38522)	Loss/tok 2.9457 (3.0848)	LR 1.250e-04
0: TRAIN [4][3530/3880]	Time 0.162 (0.182)	Data 1.21e-04 (2.98e-04)	Tok/s 32001 (38511)	Loss/tok 2.7917 (3.0846)	LR 1.250e-04
0: TRAIN [4][3540/3880]	Time 0.191 (0.182)	Data 1.13e-04 (2.98e-04)	Tok/s 43519 (38517)	Loss/tok 3.2343 (3.0847)	LR 1.250e-04
0: TRAIN [4][3550/3880]	Time 0.191 (0.182)	Data 1.42e-04 (2.97e-04)	Tok/s 44182 (38528)	Loss/tok 3.1164 (3.0851)	LR 1.250e-04
0: TRAIN [4][3560/3880]	Time 0.162 (0.182)	Data 8.89e-05 (2.97e-04)	Tok/s 33557 (38534)	Loss/tok 2.8883 (3.0852)	LR 1.250e-04
0: TRAIN [4][3570/3880]	Time 0.191 (0.182)	Data 9.32e-05 (2.96e-04)	Tok/s 43808 (38538)	Loss/tok 3.0979 (3.0853)	LR 1.250e-04
0: TRAIN [4][3580/3880]	Time 0.220 (0.182)	Data 8.87e-05 (2.96e-04)	Tok/s 53264 (38546)	Loss/tok 3.0521 (3.0858)	LR 1.250e-04
0: TRAIN [4][3590/3880]	Time 0.162 (0.182)	Data 1.47e-04 (2.95e-04)	Tok/s 32614 (38540)	Loss/tok 2.7394 (3.0856)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][3600/3880]	Time 0.191 (0.182)	Data 7.77e-05 (2.95e-04)	Tok/s 44083 (38539)	Loss/tok 3.0866 (3.0857)	LR 1.250e-04
0: TRAIN [4][3610/3880]	Time 0.191 (0.182)	Data 8.18e-05 (2.95e-04)	Tok/s 43886 (38539)	Loss/tok 3.0332 (3.0857)	LR 1.250e-04
0: TRAIN [4][3620/3880]	Time 0.191 (0.182)	Data 9.63e-05 (2.94e-04)	Tok/s 44460 (38552)	Loss/tok 3.2219 (3.0856)	LR 1.250e-04
0: TRAIN [4][3630/3880]	Time 0.138 (0.182)	Data 7.61e-05 (2.93e-04)	Tok/s 19031 (38551)	Loss/tok 2.6302 (3.0855)	LR 1.250e-04
0: TRAIN [4][3640/3880]	Time 0.191 (0.182)	Data 7.94e-05 (2.93e-04)	Tok/s 44599 (38546)	Loss/tok 3.0712 (3.0854)	LR 1.250e-04
0: TRAIN [4][3650/3880]	Time 0.162 (0.182)	Data 7.92e-05 (2.92e-04)	Tok/s 31424 (38541)	Loss/tok 2.8261 (3.0851)	LR 1.250e-04
0: TRAIN [4][3660/3880]	Time 0.190 (0.182)	Data 9.51e-05 (2.92e-04)	Tok/s 43896 (38560)	Loss/tok 3.0730 (3.0857)	LR 1.250e-04
0: TRAIN [4][3670/3880]	Time 0.162 (0.182)	Data 9.35e-05 (2.91e-04)	Tok/s 30807 (38558)	Loss/tok 2.8171 (3.0855)	LR 1.250e-04
0: TRAIN [4][3680/3880]	Time 0.136 (0.182)	Data 1.02e-04 (2.91e-04)	Tok/s 19543 (38545)	Loss/tok 2.3881 (3.0852)	LR 1.250e-04
0: TRAIN [4][3690/3880]	Time 0.191 (0.182)	Data 1.27e-04 (2.90e-04)	Tok/s 43117 (38559)	Loss/tok 3.1314 (3.0854)	LR 1.250e-04
0: TRAIN [4][3700/3880]	Time 0.162 (0.182)	Data 7.80e-05 (2.90e-04)	Tok/s 32779 (38561)	Loss/tok 3.0247 (3.0854)	LR 1.250e-04
0: TRAIN [4][3710/3880]	Time 0.191 (0.182)	Data 7.70e-05 (2.89e-04)	Tok/s 43503 (38550)	Loss/tok 3.0925 (3.0852)	LR 1.250e-04
0: TRAIN [4][3720/3880]	Time 0.162 (0.182)	Data 9.16e-05 (2.89e-04)	Tok/s 32890 (38564)	Loss/tok 3.0120 (3.0853)	LR 1.250e-04
0: TRAIN [4][3730/3880]	Time 0.163 (0.182)	Data 1.29e-04 (2.88e-04)	Tok/s 31085 (38560)	Loss/tok 2.8989 (3.0852)	LR 1.250e-04
0: TRAIN [4][3740/3880]	Time 0.136 (0.182)	Data 1.09e-04 (2.88e-04)	Tok/s 19650 (38546)	Loss/tok 2.4825 (3.0849)	LR 1.250e-04
0: TRAIN [4][3750/3880]	Time 0.221 (0.182)	Data 7.99e-05 (2.87e-04)	Tok/s 52649 (38549)	Loss/tok 3.3433 (3.0849)	LR 1.250e-04
0: TRAIN [4][3760/3880]	Time 0.221 (0.182)	Data 9.04e-05 (2.87e-04)	Tok/s 52909 (38546)	Loss/tok 3.1082 (3.0850)	LR 1.250e-04
0: TRAIN [4][3770/3880]	Time 0.162 (0.182)	Data 8.46e-05 (2.86e-04)	Tok/s 31766 (38549)	Loss/tok 3.0350 (3.0850)	LR 1.250e-04
0: TRAIN [4][3780/3880]	Time 0.220 (0.182)	Data 2.32e-04 (2.86e-04)	Tok/s 53378 (38551)	Loss/tok 3.2975 (3.0852)	LR 1.250e-04
0: TRAIN [4][3790/3880]	Time 0.191 (0.182)	Data 8.15e-05 (2.85e-04)	Tok/s 43388 (38562)	Loss/tok 3.0701 (3.0852)	LR 1.250e-04
0: TRAIN [4][3800/3880]	Time 0.191 (0.182)	Data 7.99e-05 (2.85e-04)	Tok/s 44076 (38570)	Loss/tok 3.0046 (3.0851)	LR 1.250e-04
0: TRAIN [4][3810/3880]	Time 0.162 (0.182)	Data 8.08e-05 (2.84e-04)	Tok/s 32045 (38570)	Loss/tok 3.0490 (3.0851)	LR 1.250e-04
0: TRAIN [4][3820/3880]	Time 0.257 (0.182)	Data 4.44e-04 (2.84e-04)	Tok/s 58095 (38574)	Loss/tok 3.3573 (3.0853)	LR 1.250e-04
0: TRAIN [4][3830/3880]	Time 0.162 (0.182)	Data 1.01e-04 (2.83e-04)	Tok/s 32217 (38563)	Loss/tok 2.8786 (3.0852)	LR 1.250e-04
0: TRAIN [4][3840/3880]	Time 0.162 (0.182)	Data 8.85e-05 (2.83e-04)	Tok/s 33055 (38565)	Loss/tok 2.9792 (3.0854)	LR 1.250e-04
0: TRAIN [4][3850/3880]	Time 0.191 (0.182)	Data 1.07e-04 (2.82e-04)	Tok/s 43535 (38575)	Loss/tok 3.0906 (3.0856)	LR 1.250e-04
0: TRAIN [4][3860/3880]	Time 0.162 (0.182)	Data 7.80e-05 (2.82e-04)	Tok/s 31785 (38573)	Loss/tok 3.0006 (3.0854)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3870/3880]	Time 0.136 (0.182)	Data 7.75e-05 (2.81e-04)	Tok/s 18543 (38567)	Loss/tok 2.5069 (3.0854)	LR 1.250e-04
:::MLL 1571258421.724 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 524}}
:::MLL 1571258421.725 eval_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [4][0/3]	Time 0.637 (0.637)	Decoder iters 99.0 (99.0)	Tok/s 25797 (25797)
0: Running moses detokenizer
0: BLEU(score=23.9198175894185, counts=[37060, 18520, 10503, 6211], totals=[65409, 62406, 59403, 56405], precisions=[56.658869574523386, 29.676633657020158, 17.68092520579769, 11.011435156457761], bp=1.0, sys_len=65409, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571258423.603 eval_accuracy: {"value": 23.92, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 535}}
:::MLL 1571258423.604 eval_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 4	Training Loss: 3.0851	Test BLEU: 23.92
0: Performance: Epoch: 4	Training: 308600 Tok/s
0: Finished epoch 4
:::MLL 1571258423.604 block_stop: {"value": null, "metadata": {"first_epoch_num": 5, "file": "train.py", "lineno": 557}}
:::MLL 1571258423.605 block_start: {"value": null, "metadata": {"first_epoch_num": 6, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571258423.605 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 514}}
0: Starting epoch 5
0: Executing preallocation
0: Sampler for epoch 5 uses seed 2616355186
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [5][0/3880]	Time 0.885 (0.885)	Data 6.97e-01 (6.97e-01)	Tok/s 5969 (5969)	Loss/tok 2.8515 (2.8515)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [5][10/3880]	Time 0.191 (0.245)	Data 8.89e-05 (6.34e-02)	Tok/s 43621 (35432)	Loss/tok 3.0111 (2.9955)	LR 1.250e-04
0: TRAIN [5][20/3880]	Time 0.162 (0.202)	Data 8.85e-05 (3.33e-02)	Tok/s 30903 (31677)	Loss/tok 2.7238 (2.9393)	LR 1.250e-04
0: TRAIN [5][30/3880]	Time 0.191 (0.198)	Data 1.06e-04 (2.26e-02)	Tok/s 44533 (35043)	Loss/tok 3.1521 (3.0031)	LR 1.250e-04
0: TRAIN [5][40/3880]	Time 0.162 (0.193)	Data 8.11e-05 (1.71e-02)	Tok/s 32084 (35960)	Loss/tok 2.9205 (3.0051)	LR 1.250e-04
0: TRAIN [5][50/3880]	Time 0.191 (0.192)	Data 9.01e-05 (1.38e-02)	Tok/s 44033 (36786)	Loss/tok 2.9811 (3.0164)	LR 1.250e-04
0: TRAIN [5][60/3880]	Time 0.220 (0.192)	Data 9.18e-05 (1.15e-02)	Tok/s 51783 (37759)	Loss/tok 3.1806 (3.0284)	LR 1.250e-04
0: TRAIN [5][70/3880]	Time 0.191 (0.190)	Data 1.06e-04 (9.92e-03)	Tok/s 43084 (37719)	Loss/tok 3.2299 (3.0403)	LR 1.250e-04
0: TRAIN [5][80/3880]	Time 0.220 (0.191)	Data 1.19e-04 (8.71e-03)	Tok/s 52673 (38581)	Loss/tok 3.5306 (3.0564)	LR 1.250e-04
0: TRAIN [5][90/3880]	Time 0.162 (0.190)	Data 8.94e-05 (7.76e-03)	Tok/s 31851 (38587)	Loss/tok 2.9067 (3.0673)	LR 1.250e-04
0: TRAIN [5][100/3880]	Time 0.221 (0.190)	Data 9.32e-05 (7.01e-03)	Tok/s 51878 (38980)	Loss/tok 3.1524 (3.0708)	LR 1.250e-04
0: TRAIN [5][110/3880]	Time 0.191 (0.189)	Data 8.70e-05 (6.38e-03)	Tok/s 44134 (38763)	Loss/tok 3.0315 (3.0661)	LR 1.250e-04
0: TRAIN [5][120/3880]	Time 0.162 (0.188)	Data 8.01e-05 (5.86e-03)	Tok/s 31365 (38755)	Loss/tok 2.7622 (3.0674)	LR 1.250e-04
0: TRAIN [5][130/3880]	Time 0.162 (0.187)	Data 1.05e-04 (5.42e-03)	Tok/s 32125 (38305)	Loss/tok 2.8970 (3.0611)	LR 1.250e-04
0: TRAIN [5][140/3880]	Time 0.193 (0.186)	Data 9.51e-05 (5.05e-03)	Tok/s 42537 (38159)	Loss/tok 3.0515 (3.0601)	LR 1.250e-04
0: TRAIN [5][150/3880]	Time 0.221 (0.186)	Data 9.18e-05 (4.72e-03)	Tok/s 52047 (38491)	Loss/tok 3.2727 (3.0697)	LR 1.250e-04
0: TRAIN [5][160/3880]	Time 0.191 (0.186)	Data 8.85e-05 (4.43e-03)	Tok/s 43680 (38689)	Loss/tok 3.1704 (3.0739)	LR 1.250e-04
