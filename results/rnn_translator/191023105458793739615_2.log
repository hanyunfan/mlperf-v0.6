Beginning trial 2 of 2
Gathering sys log on dss01
:::MLL 1571849092.219 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1571849092.220 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1571849092.220 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1571849092.220 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1571849092.221 submission_platform: {"value": "1xDSS8440", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1571849092.221 submission_entry: {"value": "{'hardware': 'DSS8440', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '5.0-0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz', 'num_cores': '40', 'num_vcpus': '40', 'accelerator': 'Tesla V100-PCIE-32GB', 'num_accelerators': '8', 'sys_mem_size': '754 GB', 'sys_storage_type': 'SATA SSD', 'sys_storage_size': '1x 447.1G + 1x 931.5G', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27700 Family [ConnectX-4]', 'num_network_cards': '1', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1571849092.222 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1571849092.222 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1571849096.976 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node dss01
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=4473' -e LR=8.0e-3 -e TRAIN_BATCH_SIZE=1024 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=191023105458793739615 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191023105458793739615 ./run_and_time.sh
Run vars: id 191023105458793739615 gpus 8 mparams  --master_port=4473
NCCL_SOCKET_NTHREADS=2
NCCL_NSOCKS_PERTHREAD=8
STARTING TIMING RUN AT 2019-10-23 04:44:57 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=8.0e-3
+ TRAIN_BATCH_SIZE=1024
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=0
+ MATH=amp_fp16
+ [[ 0 -eq 1 ]]
+ LAUNCH_OPT='torch.distributed.launch --nproc_per_node 8  --master_port=4473'
+ echo 'running benchmark'
running benchmark
+ python -m torch.distributed.launch --nproc_per_node 8 --master_port=4473 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 1024 --test-batch-size 128 --optimizer FusedAdam --lr 8.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1571849099.694 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571849099.694 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571849099.699 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571849099.700 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571849099.701 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571849099.702 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571849099.712 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571849099.714 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.008, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=1024, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 982269560
dss01:1841:1841 [0] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:1841:1841 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:1841:1841 [0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:1841:1841 [0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:1841:1841 [0] NCCL INFO NET/IB : No device found.
dss01:1841:1841 [0] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
NCCL version 2.4.8+cuda10.1
dss01:1843:1843 [2] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:1843:1843 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:1846:1846 [5] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:1846:1846 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:1847:1847 [6] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:1847:1847 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:1845:1845 [4] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:1845:1845 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:1844:1844 [3] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:1844:1844 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:1848:1848 [7] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:1848:1848 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:1842:1842 [1] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:1842:1842 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:1843:1843 [2] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:1843:1843 [2] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:1843:1843 [2] NCCL INFO NET/IB : No device found.

dss01:1846:1846 [5] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:1846:1846 [5] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:1846:1846 [5] NCCL INFO NET/IB : No device found.

dss01:1845:1845 [4] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:1847:1847 [6] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:1845:1845 [4] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0

dss01:1847:1847 [6] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:1845:1845 [4] NCCL INFO NET/IB : No device found.
dss01:1847:1847 [6] NCCL INFO NET/IB : No device found.

dss01:1848:1848 [7] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:1848:1848 [7] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:1848:1848 [7] NCCL INFO NET/IB : No device found.

dss01:1842:1842 [1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:1842:1842 [1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:1842:1842 [1] NCCL INFO NET/IB : No device found.

dss01:1844:1844 [3] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:1844:1844 [3] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:1844:1844 [3] NCCL INFO NET/IB : No device found.
dss01:1843:1843 [2] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:1848:1848 [7] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:1845:1845 [4] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:1847:1847 [6] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:1842:1842 [1] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:1844:1844 [3] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:1846:1846 [5] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:1841:2203 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555
dss01:1846:2204 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa
dss01:1847:2205 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa
dss01:1848:2206 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa
dss01:1844:2208 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555
dss01:1843:2209 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555
dss01:1845:2207 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa
dss01:1842:2210 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555
dss01:1847:2205 [6] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:1848:2206 [7] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:1842:2210 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:1841:2203 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:1843:2209 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:1844:2208 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:1845:2207 [4] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:1846:2204 [5] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:1841:2203 [0] NCCL INFO Channel 00 :    0   1   2   3   4   5   6   7
dss01:1845:2207 [4] NCCL INFO Ring 00 : 4[4] -> 5[5] via P2P/IPC
dss01:1847:2205 [6] NCCL INFO Ring 00 : 6[6] -> 7[7] via P2P/IPC
dss01:1841:2203 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/IPC
dss01:1843:2209 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/IPC
dss01:1848:2206 [7] NCCL INFO Ring 00 : 7[7] -> 0[0] via direct shared memory
dss01:1842:2210 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via direct shared memory
dss01:1844:2208 [3] NCCL INFO Ring 00 : 3[3] -> 4[4] via direct shared memory
dss01:1846:2204 [5] NCCL INFO Ring 00 : 5[5] -> 6[6] via direct shared memory
dss01:1841:2203 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
dss01:1842:2210 [1] NCCL INFO comm 0x7fff78007590 rank 1 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE
dss01:1848:2206 [7] NCCL INFO comm 0x7ffe98007590 rank 7 nranks 8 cudaDev 7 nvmlDev 7 - Init COMPLETE
dss01:1846:2204 [5] NCCL INFO comm 0x7ffe98007590 rank 5 nranks 8 cudaDev 5 nvmlDev 5 - Init COMPLETE
dss01:1843:2209 [2] NCCL INFO comm 0x7fff7c007590 rank 2 nranks 8 cudaDev 2 nvmlDev 2 - Init COMPLETE
dss01:1844:2208 [3] NCCL INFO comm 0x7fff54007590 rank 3 nranks 8 cudaDev 3 nvmlDev 3 - Init COMPLETE
dss01:1845:2207 [4] NCCL INFO comm 0x7fff54007590 rank 4 nranks 8 cudaDev 4 nvmlDev 4 - Init COMPLETE
dss01:1841:2203 [0] NCCL INFO comm 0x7ffe38007590 rank 0 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE
dss01:1841:1841 [0] NCCL INFO Launch mode Parallel
dss01:1847:2205 [6] NCCL INFO comm 0x7ffe98007590 rank 6 nranks 8 cudaDev 6 nvmlDev 6 - Init COMPLETE
0: Worker 0 is using worker seed: 2552316485
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.008}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.008
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1571849122.369 opt_base_learning_rate: {"value": 0.008, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:522: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
  self.num_layers, self.dropout, self.training, self.bidirectional)
/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:522: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
  self.num_layers, self.dropout, self.training, self.bidirectional)
/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:522: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
  self.num_layers, self.dropout, self.training, self.bidirectional)
/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:522: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
  self.num_layers, self.dropout, self.training, self.bidirectional)
/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:522: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
  self.num_layers, self.dropout, self.training, self.bidirectional)
/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:522: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
  self.num_layers, self.dropout, self.training, self.bidirectional)
/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:522: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
  self.num_layers, self.dropout, self.training, self.bidirectional)
/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:522: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
  self.num_layers, self.dropout, self.training, self.bidirectional)
:::MLL 1571849126.426 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1571849126.426 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1571849126.426 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1571849127.516 global_batch_size: {"value": 8192, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1571849127.522 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1571849127.523 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1571849127.523 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1571849127.523 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1571849127.524 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1571849127.524 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1571849127.524 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1571849127.527 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571849127.528 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 43316974
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/484]	Time 1.624 (1.624)	Data 1.05e+00 (1.05e+00)	Tok/s 25586 (25586)	Loss/tok 10.6522 (10.6522)	LR 8.000e-05
0: TRAIN [0][10/484]	Time 1.085 (0.903)	Data 2.48e-04 (9.56e-02)	Tok/s 85674 (75240)	Loss/tok 9.3692 (9.8405)	LR 1.007e-04
0: TRAIN [0][20/484]	Time 0.796 (0.807)	Data 2.55e-04 (5.02e-02)	Tok/s 84351 (77755)	Loss/tok 8.6919 (9.4240)	LR 1.268e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][30/484]	Time 0.797 (0.776)	Data 2.82e-04 (3.41e-02)	Tok/s 83821 (77942)	Loss/tok 8.4695 (9.1808)	LR 1.560e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
0: TRAIN [0][40/484]	Time 0.320 (0.721)	Data 2.44e-04 (2.58e-02)	Tok/s 65574 (77391)	Loss/tok 7.5350 (9.0274)	LR 1.919e-04
0: TRAIN [0][50/484]	Time 0.798 (0.721)	Data 2.59e-04 (2.08e-02)	Tok/s 84223 (78233)	Loss/tok 8.0623 (8.8266)	LR 2.416e-04
0: TRAIN [0][60/484]	Time 0.544 (0.732)	Data 2.56e-04 (1.75e-02)	Tok/s 76477 (78692)	Loss/tok 7.8290 (8.6742)	LR 3.042e-04
0: TRAIN [0][70/484]	Time 0.542 (0.735)	Data 2.49e-04 (1.50e-02)	Tok/s 76007 (79154)	Loss/tok 7.7478 (8.5607)	LR 3.829e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
0: TRAIN [0][80/484]	Time 0.549 (0.728)	Data 2.43e-04 (1.32e-02)	Tok/s 75437 (79140)	Loss/tok 7.7729 (8.4942)	LR 4.711e-04
0: TRAIN [0][90/484]	Time 0.543 (0.728)	Data 2.57e-04 (1.18e-02)	Tok/s 76594 (79265)	Loss/tok 7.5740 (8.4222)	LR 5.930e-04
0: TRAIN [0][100/484]	Time 0.800 (0.735)	Data 2.49e-04 (1.06e-02)	Tok/s 83669 (79365)	Loss/tok 8.4124 (8.3597)	LR 7.466e-04
0: TRAIN [0][110/484]	Time 0.798 (0.733)	Data 2.56e-04 (9.71e-03)	Tok/s 84207 (79292)	Loss/tok 7.6002 (8.3016)	LR 9.399e-04
0: TRAIN [0][120/484]	Time 0.546 (0.722)	Data 2.46e-04 (8.93e-03)	Tok/s 75292 (79129)	Loss/tok 7.1464 (8.2376)	LR 1.183e-03
0: TRAIN [0][130/484]	Time 0.546 (0.722)	Data 2.51e-04 (8.27e-03)	Tok/s 75806 (79013)	Loss/tok 6.8956 (8.1655)	LR 1.490e-03
0: TRAIN [0][140/484]	Time 0.318 (0.719)	Data 2.60e-04 (7.70e-03)	Tok/s 66353 (78983)	Loss/tok 6.0615 (8.0882)	LR 1.875e-03
0: TRAIN [0][150/484]	Time 1.085 (0.721)	Data 2.60e-04 (7.21e-03)	Tok/s 85326 (78927)	Loss/tok 7.1933 (8.0116)	LR 2.361e-03
0: TRAIN [0][160/484]	Time 0.799 (0.718)	Data 2.42e-04 (6.77e-03)	Tok/s 84253 (78886)	Loss/tok 6.7584 (7.9373)	LR 2.972e-03
0: TRAIN [0][170/484]	Time 0.320 (0.715)	Data 2.52e-04 (6.39e-03)	Tok/s 65883 (78804)	Loss/tok 5.5968 (7.8637)	LR 3.742e-03
0: TRAIN [0][180/484]	Time 0.316 (0.717)	Data 2.55e-04 (6.05e-03)	Tok/s 66786 (78822)	Loss/tok 5.4935 (7.7894)	LR 4.711e-03
0: TRAIN [0][190/484]	Time 0.547 (0.712)	Data 2.60e-04 (5.75e-03)	Tok/s 76103 (78729)	Loss/tok 6.1317 (7.7237)	LR 5.930e-03
0: TRAIN [0][200/484]	Time 0.797 (0.712)	Data 2.52e-04 (5.48e-03)	Tok/s 84542 (78800)	Loss/tok 6.3029 (7.6495)	LR 7.466e-03
0: TRAIN [0][210/484]	Time 0.802 (0.714)	Data 2.58e-04 (5.23e-03)	Tok/s 83425 (78857)	Loss/tok 6.2261 (7.5758)	LR 8.000e-03
0: TRAIN [0][220/484]	Time 0.798 (0.714)	Data 2.63e-04 (5.01e-03)	Tok/s 84608 (78846)	Loss/tok 5.9481 (7.5012)	LR 8.000e-03
0: TRAIN [0][230/484]	Time 0.803 (0.721)	Data 2.44e-04 (4.80e-03)	Tok/s 84032 (79001)	Loss/tok 5.6709 (7.4101)	LR 8.000e-03
0: TRAIN [0][240/484]	Time 1.084 (0.724)	Data 2.46e-04 (4.61e-03)	Tok/s 86587 (79037)	Loss/tok 5.7252 (7.3265)	LR 8.000e-03
0: TRAIN [0][250/484]	Time 0.545 (0.728)	Data 2.44e-04 (4.44e-03)	Tok/s 76036 (79105)	Loss/tok 5.0346 (7.2390)	LR 8.000e-03
0: TRAIN [0][260/484]	Time 0.798 (0.727)	Data 2.54e-04 (4.28e-03)	Tok/s 84394 (79139)	Loss/tok 5.1183 (7.1604)	LR 8.000e-03
0: TRAIN [0][270/484]	Time 1.089 (0.731)	Data 4.41e-04 (4.13e-03)	Tok/s 85920 (79094)	Loss/tok 5.2607 (7.0764)	LR 8.000e-03
0: TRAIN [0][280/484]	Time 0.800 (0.730)	Data 2.52e-04 (3.99e-03)	Tok/s 83748 (79092)	Loss/tok 4.8657 (7.0006)	LR 8.000e-03
0: TRAIN [0][290/484]	Time 0.543 (0.726)	Data 2.50e-04 (3.87e-03)	Tok/s 76414 (79031)	Loss/tok 4.3875 (6.9338)	LR 8.000e-03
0: TRAIN [0][300/484]	Time 0.546 (0.722)	Data 2.39e-04 (3.75e-03)	Tok/s 75883 (78951)	Loss/tok 4.2539 (6.8682)	LR 8.000e-03
0: TRAIN [0][310/484]	Time 1.427 (0.725)	Data 2.35e-04 (3.63e-03)	Tok/s 83166 (79025)	Loss/tok 4.9191 (6.7841)	LR 8.000e-03
0: TRAIN [0][320/484]	Time 0.546 (0.725)	Data 2.54e-04 (3.53e-03)	Tok/s 75483 (79001)	Loss/tok 4.0596 (6.7134)	LR 8.000e-03
0: TRAIN [0][330/484]	Time 1.087 (0.726)	Data 2.66e-04 (3.43e-03)	Tok/s 85824 (79060)	Loss/tok 4.5302 (6.6375)	LR 8.000e-03
0: TRAIN [0][340/484]	Time 0.800 (0.726)	Data 2.39e-04 (3.34e-03)	Tok/s 83094 (79119)	Loss/tok 4.2519 (6.5637)	LR 8.000e-03
0: TRAIN [0][350/484]	Time 0.546 (0.726)	Data 7.68e-04 (3.25e-03)	Tok/s 76130 (79117)	Loss/tok 3.8832 (6.4982)	LR 8.000e-03
0: TRAIN [0][360/484]	Time 0.546 (0.726)	Data 2.41e-04 (3.17e-03)	Tok/s 74549 (79111)	Loss/tok 3.9118 (6.4352)	LR 8.000e-03
0: TRAIN [0][370/484]	Time 0.547 (0.728)	Data 4.32e-04 (3.09e-03)	Tok/s 76149 (79126)	Loss/tok 3.8813 (6.3693)	LR 8.000e-03
0: TRAIN [0][380/484]	Time 0.798 (0.724)	Data 2.36e-04 (3.02e-03)	Tok/s 84867 (79066)	Loss/tok 4.1059 (6.3186)	LR 8.000e-03
0: TRAIN [0][390/484]	Time 0.546 (0.720)	Data 2.41e-04 (2.95e-03)	Tok/s 75498 (78950)	Loss/tok 3.7533 (6.2730)	LR 8.000e-03
0: TRAIN [0][400/484]	Time 1.090 (0.723)	Data 4.99e-04 (2.88e-03)	Tok/s 85669 (79000)	Loss/tok 4.2235 (6.2109)	LR 8.000e-03
0: TRAIN [0][410/484]	Time 0.324 (0.723)	Data 2.52e-04 (2.82e-03)	Tok/s 65235 (78981)	Loss/tok 3.2058 (6.1568)	LR 8.000e-03
0: TRAIN [0][420/484]	Time 1.088 (0.722)	Data 2.39e-04 (2.75e-03)	Tok/s 85912 (78968)	Loss/tok 4.2547 (6.1081)	LR 8.000e-03
0: TRAIN [0][430/484]	Time 0.317 (0.719)	Data 2.56e-04 (2.70e-03)	Tok/s 65746 (78892)	Loss/tok 3.0746 (6.0668)	LR 8.000e-03
0: TRAIN [0][440/484]	Time 0.800 (0.719)	Data 2.56e-04 (2.64e-03)	Tok/s 84328 (78872)	Loss/tok 3.9710 (6.0192)	LR 8.000e-03
0: TRAIN [0][450/484]	Time 1.091 (0.718)	Data 2.53e-04 (2.59e-03)	Tok/s 84690 (78845)	Loss/tok 4.1667 (5.9753)	LR 8.000e-03
0: TRAIN [0][460/484]	Time 0.546 (0.720)	Data 2.36e-04 (2.54e-03)	Tok/s 75726 (78930)	Loss/tok 3.6179 (5.9227)	LR 8.000e-03
0: TRAIN [0][470/484]	Time 0.544 (0.720)	Data 2.88e-04 (2.49e-03)	Tok/s 76013 (78949)	Loss/tok 3.6303 (5.8794)	LR 8.000e-03
0: TRAIN [0][480/484]	Time 0.542 (0.724)	Data 2.52e-04 (2.45e-03)	Tok/s 76171 (78987)	Loss/tok 3.5544 (5.8295)	LR 8.000e-03
:::MLL 1571849480.456 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1571849480.456 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.984 (0.984)	Decoder iters 149.0 (149.0)	Tok/s 18768 (18768)
0: Running moses detokenizer
0: BLEU(score=13.588440302706665, counts=[32044, 13381, 6660, 3424], totals=[77760, 74757, 71754, 68756], precisions=[41.208847736625515, 17.899327153310058, 9.281712517769044, 4.979929024376054], bp=1.0, sys_len=77760, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571849483.297 eval_accuracy: {"value": 13.59, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1571849483.298 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 5.8155	Test BLEU: 13.59
0: Performance: Epoch: 0	Training: 631981 Tok/s
0: Finished epoch 0
:::MLL 1571849483.298 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1571849483.298 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571849483.299 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 858620946
0: TRAIN [1][0/484]	Time 1.605 (1.605)	Data 8.09e-01 (8.09e-01)	Tok/s 41718 (41718)	Loss/tok 3.8489 (3.8489)	LR 8.000e-03
0: TRAIN [1][10/484]	Time 0.544 (0.824)	Data 2.44e-04 (7.38e-02)	Tok/s 75841 (75741)	Loss/tok 3.5828 (3.8102)	LR 8.000e-03
0: TRAIN [1][20/484]	Time 0.547 (0.759)	Data 6.49e-04 (3.88e-02)	Tok/s 75695 (76907)	Loss/tok 3.5178 (3.7967)	LR 8.000e-03
0: TRAIN [1][30/484]	Time 0.801 (0.726)	Data 3.09e-04 (2.64e-02)	Tok/s 83665 (77062)	Loss/tok 3.9063 (3.7934)	LR 8.000e-03
0: TRAIN [1][40/484]	Time 1.433 (0.723)	Data 2.64e-04 (2.00e-02)	Tok/s 82748 (76561)	Loss/tok 4.2212 (3.8258)	LR 8.000e-03
0: TRAIN [1][50/484]	Time 0.799 (0.724)	Data 2.75e-04 (1.61e-02)	Tok/s 84163 (76645)	Loss/tok 3.9069 (3.8359)	LR 8.000e-03
0: TRAIN [1][60/484]	Time 1.084 (0.730)	Data 2.47e-04 (1.35e-02)	Tok/s 85669 (77229)	Loss/tok 3.9761 (3.8351)	LR 8.000e-03
0: TRAIN [1][70/484]	Time 0.549 (0.734)	Data 2.53e-04 (1.17e-02)	Tok/s 75331 (77710)	Loss/tok 3.6056 (3.8301)	LR 8.000e-03
0: TRAIN [1][80/484]	Time 0.805 (0.734)	Data 2.59e-04 (1.03e-02)	Tok/s 83449 (77949)	Loss/tok 3.7799 (3.8240)	LR 8.000e-03
0: TRAIN [1][90/484]	Time 0.549 (0.741)	Data 2.63e-04 (9.17e-03)	Tok/s 75301 (78164)	Loss/tok 3.4870 (3.8207)	LR 8.000e-03
0: TRAIN [1][100/484]	Time 1.086 (0.745)	Data 2.58e-04 (8.29e-03)	Tok/s 85589 (78167)	Loss/tok 3.9626 (3.8214)	LR 8.000e-03
0: TRAIN [1][110/484]	Time 0.544 (0.765)	Data 2.68e-04 (7.57e-03)	Tok/s 75726 (78476)	Loss/tok 3.4945 (3.8322)	LR 8.000e-03
0: TRAIN [1][120/484]	Time 0.805 (0.756)	Data 2.48e-04 (6.97e-03)	Tok/s 83347 (78495)	Loss/tok 3.6911 (3.8171)	LR 8.000e-03
0: TRAIN [1][130/484]	Time 0.544 (0.760)	Data 2.61e-04 (6.45e-03)	Tok/s 76652 (78721)	Loss/tok 3.3825 (3.8143)	LR 8.000e-03
0: TRAIN [1][140/484]	Time 0.546 (0.754)	Data 3.36e-04 (6.02e-03)	Tok/s 76092 (78754)	Loss/tok 3.5249 (3.8059)	LR 8.000e-03
0: TRAIN [1][150/484]	Time 0.802 (0.747)	Data 2.74e-04 (5.64e-03)	Tok/s 83642 (78771)	Loss/tok 3.7371 (3.7945)	LR 8.000e-03
0: TRAIN [1][160/484]	Time 0.543 (0.742)	Data 3.02e-04 (5.30e-03)	Tok/s 76375 (78684)	Loss/tok 3.4686 (3.7865)	LR 8.000e-03
0: TRAIN [1][170/484]	Time 0.547 (0.740)	Data 2.43e-04 (5.01e-03)	Tok/s 76089 (78708)	Loss/tok 3.4488 (3.7802)	LR 8.000e-03
0: TRAIN [1][180/484]	Time 0.543 (0.735)	Data 2.57e-04 (4.75e-03)	Tok/s 76358 (78683)	Loss/tok 3.4178 (3.7712)	LR 8.000e-03
0: TRAIN [1][190/484]	Time 0.545 (0.735)	Data 2.58e-04 (4.51e-03)	Tok/s 76521 (78807)	Loss/tok 3.4071 (3.7641)	LR 8.000e-03
0: TRAIN [1][200/484]	Time 0.543 (0.732)	Data 2.63e-04 (4.30e-03)	Tok/s 75709 (78817)	Loss/tok 3.3983 (3.7566)	LR 8.000e-03
0: TRAIN [1][210/484]	Time 0.800 (0.732)	Data 2.47e-04 (4.11e-03)	Tok/s 84180 (78834)	Loss/tok 3.6806 (3.7537)	LR 8.000e-03
0: TRAIN [1][220/484]	Time 0.546 (0.734)	Data 2.67e-04 (3.94e-03)	Tok/s 76788 (78881)	Loss/tok 3.4270 (3.7509)	LR 8.000e-03
0: TRAIN [1][230/484]	Time 0.545 (0.737)	Data 3.44e-04 (3.78e-03)	Tok/s 75611 (78940)	Loss/tok 3.4524 (3.7503)	LR 8.000e-03
0: TRAIN [1][240/484]	Time 0.543 (0.734)	Data 2.58e-04 (3.63e-03)	Tok/s 75854 (78902)	Loss/tok 3.4397 (3.7461)	LR 8.000e-03
0: TRAIN [1][250/484]	Time 0.800 (0.733)	Data 2.83e-04 (3.50e-03)	Tok/s 83748 (78907)	Loss/tok 3.6855 (3.7438)	LR 8.000e-03
0: TRAIN [1][260/484]	Time 0.550 (0.736)	Data 2.87e-04 (3.38e-03)	Tok/s 74253 (78951)	Loss/tok 3.4036 (3.7435)	LR 8.000e-03
0: TRAIN [1][270/484]	Time 0.800 (0.737)	Data 2.69e-04 (3.26e-03)	Tok/s 84497 (79066)	Loss/tok 3.6474 (3.7399)	LR 8.000e-03
0: TRAIN [1][280/484]	Time 1.089 (0.741)	Data 2.60e-04 (3.16e-03)	Tok/s 86348 (79142)	Loss/tok 3.7875 (3.7403)	LR 8.000e-03
0: TRAIN [1][290/484]	Time 1.427 (0.738)	Data 2.53e-04 (3.06e-03)	Tok/s 83100 (79021)	Loss/tok 4.0177 (3.7362)	LR 8.000e-03
0: TRAIN [1][300/484]	Time 0.802 (0.738)	Data 2.44e-04 (2.97e-03)	Tok/s 84078 (79072)	Loss/tok 3.6493 (3.7331)	LR 8.000e-03
0: TRAIN [1][310/484]	Time 0.804 (0.734)	Data 2.84e-04 (2.88e-03)	Tok/s 83107 (79014)	Loss/tok 3.6503 (3.7273)	LR 8.000e-03
0: TRAIN [1][320/484]	Time 0.543 (0.734)	Data 2.51e-04 (2.80e-03)	Tok/s 75836 (78956)	Loss/tok 3.4661 (3.7260)	LR 8.000e-03
0: TRAIN [1][330/484]	Time 0.552 (0.736)	Data 2.66e-04 (2.72e-03)	Tok/s 74277 (78977)	Loss/tok 3.4207 (3.7246)	LR 8.000e-03
0: TRAIN [1][340/484]	Time 0.324 (0.736)	Data 2.38e-04 (2.65e-03)	Tok/s 65715 (78987)	Loss/tok 2.9432 (3.7212)	LR 8.000e-03
0: TRAIN [1][350/484]	Time 0.547 (0.734)	Data 2.39e-04 (2.58e-03)	Tok/s 75711 (78966)	Loss/tok 3.4114 (3.7167)	LR 8.000e-03
0: TRAIN [1][360/484]	Time 0.548 (0.734)	Data 2.41e-04 (2.52e-03)	Tok/s 75315 (78970)	Loss/tok 3.4028 (3.7137)	LR 8.000e-03
0: TRAIN [1][370/484]	Time 0.801 (0.731)	Data 2.55e-04 (2.46e-03)	Tok/s 84072 (78942)	Loss/tok 3.5895 (3.7085)	LR 8.000e-03
0: TRAIN [1][380/484]	Time 0.801 (0.731)	Data 2.47e-04 (2.40e-03)	Tok/s 84602 (79001)	Loss/tok 3.5342 (3.7036)	LR 8.000e-03
0: TRAIN [1][390/484]	Time 0.551 (0.728)	Data 7.58e-04 (2.35e-03)	Tok/s 75245 (78945)	Loss/tok 3.3078 (3.6990)	LR 8.000e-03
0: TRAIN [1][400/484]	Time 0.549 (0.726)	Data 2.40e-04 (2.29e-03)	Tok/s 75127 (78933)	Loss/tok 3.3515 (3.6941)	LR 8.000e-03
0: TRAIN [1][410/484]	Time 0.549 (0.724)	Data 2.68e-04 (2.25e-03)	Tok/s 75064 (78887)	Loss/tok 3.2844 (3.6897)	LR 8.000e-03
0: TRAIN [1][420/484]	Time 0.547 (0.722)	Data 2.45e-04 (2.20e-03)	Tok/s 76087 (78833)	Loss/tok 3.3065 (3.6852)	LR 8.000e-03
0: TRAIN [1][430/484]	Time 0.545 (0.722)	Data 2.65e-04 (2.16e-03)	Tok/s 76909 (78847)	Loss/tok 3.3276 (3.6827)	LR 8.000e-03
0: TRAIN [1][440/484]	Time 0.544 (0.725)	Data 2.63e-04 (2.11e-03)	Tok/s 75910 (78878)	Loss/tok 3.3477 (3.6830)	LR 8.000e-03
0: TRAIN [1][450/484]	Time 0.546 (0.727)	Data 2.70e-04 (2.07e-03)	Tok/s 75987 (78944)	Loss/tok 3.3783 (3.6825)	LR 8.000e-03
0: TRAIN [1][460/484]	Time 0.545 (0.726)	Data 2.59e-04 (2.03e-03)	Tok/s 75914 (78906)	Loss/tok 3.3709 (3.6799)	LR 8.000e-03
0: TRAIN [1][470/484]	Time 0.545 (0.724)	Data 2.71e-04 (1.99e-03)	Tok/s 76818 (78861)	Loss/tok 3.3482 (3.6765)	LR 8.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][480/484]	Time 1.089 (0.724)	Data 2.84e-04 (1.96e-03)	Tok/s 85505 (78849)	Loss/tok 3.8292 (3.6755)	LR 8.000e-03
:::MLL 1571849836.811 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1571849836.812 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.816 (0.816)	Decoder iters 149.0 (149.0)	Tok/s 20147 (20147)
0: Running moses detokenizer
0: BLEU(score=19.087517034086858, counts=[33441, 15054, 7913, 4334], totals=[64315, 61312, 58309, 55312], precisions=[51.99564642773848, 24.55310542797495, 13.570803821022484, 7.835551055828753], bp=0.9944027249210027, sys_len=64315, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571849838.919 eval_accuracy: {"value": 19.09, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1571849838.919 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.6722	Test BLEU: 19.09
0: Performance: Epoch: 1	Training: 631067 Tok/s
0: Finished epoch 1
:::MLL 1571849838.920 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1571849838.920 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571849838.920 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 3155309793
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [2][0/484]	Time 1.338 (1.338)	Data 7.78e-01 (7.78e-01)	Tok/s 30735 (30735)	Loss/tok 3.3531 (3.3531)	LR 8.000e-03
0: TRAIN [2][10/484]	Time 0.320 (0.576)	Data 2.54e-04 (7.09e-02)	Tok/s 66277 (69846)	Loss/tok 2.7634 (3.2351)	LR 8.000e-03
0: TRAIN [2][20/484]	Time 0.320 (0.617)	Data 3.10e-04 (3.73e-02)	Tok/s 65425 (73433)	Loss/tok 2.8091 (3.3735)	LR 8.000e-03
0: TRAIN [2][30/484]	Time 0.802 (0.660)	Data 3.05e-04 (2.54e-02)	Tok/s 83393 (75065)	Loss/tok 3.5035 (3.4507)	LR 8.000e-03
0: TRAIN [2][40/484]	Time 0.320 (0.667)	Data 2.97e-04 (1.93e-02)	Tok/s 66682 (75685)	Loss/tok 2.8274 (3.4694)	LR 8.000e-03
0: TRAIN [2][50/484]	Time 0.799 (0.668)	Data 2.86e-04 (1.55e-02)	Tok/s 84210 (76398)	Loss/tok 3.5225 (3.4613)	LR 8.000e-03
0: TRAIN [2][60/484]	Time 0.805 (0.679)	Data 3.04e-04 (1.31e-02)	Tok/s 84254 (77085)	Loss/tok 3.5155 (3.4657)	LR 8.000e-03
0: TRAIN [2][70/484]	Time 1.091 (0.690)	Data 3.73e-04 (1.13e-02)	Tok/s 85795 (77602)	Loss/tok 3.6916 (3.4708)	LR 8.000e-03
0: TRAIN [2][80/484]	Time 0.799 (0.688)	Data 3.01e-04 (9.91e-03)	Tok/s 83750 (77861)	Loss/tok 3.5227 (3.4654)	LR 8.000e-03
0: TRAIN [2][90/484]	Time 0.546 (0.687)	Data 3.04e-04 (8.85e-03)	Tok/s 76279 (77998)	Loss/tok 3.2916 (3.4600)	LR 8.000e-03
0: TRAIN [2][100/484]	Time 0.808 (0.687)	Data 2.98e-04 (8.01e-03)	Tok/s 82906 (78013)	Loss/tok 3.5299 (3.4604)	LR 8.000e-03
0: TRAIN [2][110/484]	Time 0.802 (0.707)	Data 4.68e-04 (7.32e-03)	Tok/s 83306 (78355)	Loss/tok 3.4499 (3.4772)	LR 8.000e-03
0: TRAIN [2][120/484]	Time 0.547 (0.702)	Data 3.15e-04 (6.74e-03)	Tok/s 75251 (78397)	Loss/tok 3.2037 (3.4702)	LR 8.000e-03
0: TRAIN [2][130/484]	Time 0.547 (0.698)	Data 3.06e-04 (6.25e-03)	Tok/s 74567 (78424)	Loss/tok 3.3637 (3.4647)	LR 8.000e-03
0: TRAIN [2][140/484]	Time 1.431 (0.705)	Data 6.36e-04 (5.83e-03)	Tok/s 82955 (78579)	Loss/tok 3.8348 (3.4708)	LR 8.000e-03
0: TRAIN [2][150/484]	Time 1.428 (0.718)	Data 2.97e-04 (5.46e-03)	Tok/s 83723 (78828)	Loss/tok 3.8766 (3.4827)	LR 8.000e-03
0: TRAIN [2][160/484]	Time 0.807 (0.722)	Data 2.97e-04 (5.14e-03)	Tok/s 83106 (78811)	Loss/tok 3.5384 (3.4883)	LR 8.000e-03
0: TRAIN [2][170/484]	Time 0.326 (0.718)	Data 2.94e-04 (4.86e-03)	Tok/s 64637 (78703)	Loss/tok 2.8359 (3.4871)	LR 8.000e-03
0: TRAIN [2][180/484]	Time 0.544 (0.720)	Data 2.86e-04 (4.61e-03)	Tok/s 77127 (78795)	Loss/tok 3.2901 (3.4881)	LR 8.000e-03
0: TRAIN [2][190/484]	Time 0.547 (0.719)	Data 2.95e-04 (4.38e-03)	Tok/s 76156 (78729)	Loss/tok 3.3099 (3.4889)	LR 8.000e-03
0: TRAIN [2][200/484]	Time 0.546 (0.720)	Data 2.98e-04 (4.18e-03)	Tok/s 75692 (78743)	Loss/tok 3.2837 (3.4913)	LR 8.000e-03
0: TRAIN [2][210/484]	Time 0.322 (0.714)	Data 3.13e-04 (4.00e-03)	Tok/s 66183 (78541)	Loss/tok 2.7991 (3.4880)	LR 8.000e-03
0: TRAIN [2][220/484]	Time 1.431 (0.717)	Data 2.83e-04 (3.83e-03)	Tok/s 83317 (78527)	Loss/tok 4.0177 (3.4948)	LR 8.000e-03
0: TRAIN [2][230/484]	Time 0.545 (0.713)	Data 4.44e-04 (3.68e-03)	Tok/s 75695 (78441)	Loss/tok 3.2878 (3.4939)	LR 8.000e-03
0: TRAIN [2][240/484]	Time 0.800 (0.714)	Data 2.89e-04 (3.54e-03)	Tok/s 84134 (78532)	Loss/tok 3.4925 (3.4932)	LR 8.000e-03
0: TRAIN [2][250/484]	Time 0.549 (0.716)	Data 3.44e-04 (3.42e-03)	Tok/s 75420 (78551)	Loss/tok 3.2529 (3.4953)	LR 8.000e-03
0: TRAIN [2][260/484]	Time 1.091 (0.720)	Data 3.54e-04 (3.30e-03)	Tok/s 85338 (78665)	Loss/tok 3.6632 (3.4969)	LR 8.000e-03
0: TRAIN [2][270/484]	Time 0.804 (0.727)	Data 3.35e-04 (3.19e-03)	Tok/s 83824 (78809)	Loss/tok 3.4290 (3.5017)	LR 8.000e-03
0: TRAIN [2][280/484]	Time 0.548 (0.725)	Data 3.12e-04 (3.08e-03)	Tok/s 75453 (78790)	Loss/tok 3.3288 (3.5002)	LR 8.000e-03
0: TRAIN [2][290/484]	Time 0.542 (0.720)	Data 2.87e-04 (2.99e-03)	Tok/s 75696 (78699)	Loss/tok 3.2420 (3.4954)	LR 8.000e-03
0: TRAIN [2][300/484]	Time 0.550 (0.720)	Data 3.36e-04 (2.90e-03)	Tok/s 74237 (78669)	Loss/tok 3.2850 (3.4961)	LR 8.000e-03
0: TRAIN [2][310/484]	Time 1.089 (0.722)	Data 3.08e-04 (2.82e-03)	Tok/s 85674 (78694)	Loss/tok 3.6595 (3.4967)	LR 8.000e-03
0: TRAIN [2][320/484]	Time 0.802 (0.722)	Data 2.97e-04 (2.74e-03)	Tok/s 84456 (78755)	Loss/tok 3.4571 (3.4947)	LR 8.000e-03
0: TRAIN [2][330/484]	Time 0.549 (0.722)	Data 3.03e-04 (2.67e-03)	Tok/s 74826 (78747)	Loss/tok 3.2409 (3.4953)	LR 8.000e-03
0: TRAIN [2][340/484]	Time 0.545 (0.724)	Data 3.13e-04 (2.60e-03)	Tok/s 75802 (78779)	Loss/tok 3.3037 (3.4957)	LR 8.000e-03
0: TRAIN [2][350/484]	Time 0.321 (0.722)	Data 2.46e-04 (2.53e-03)	Tok/s 66132 (78756)	Loss/tok 2.7796 (3.4931)	LR 8.000e-03
0: TRAIN [2][360/484]	Time 0.542 (0.721)	Data 2.36e-04 (2.47e-03)	Tok/s 75988 (78727)	Loss/tok 3.2300 (3.4920)	LR 8.000e-03
0: TRAIN [2][370/484]	Time 0.549 (0.723)	Data 9.26e-04 (2.41e-03)	Tok/s 74728 (78770)	Loss/tok 3.2243 (3.4934)	LR 8.000e-03
0: TRAIN [2][380/484]	Time 1.426 (0.722)	Data 2.39e-04 (2.36e-03)	Tok/s 83022 (78766)	Loss/tok 3.8136 (3.4923)	LR 8.000e-03
0: TRAIN [2][390/484]	Time 0.803 (0.720)	Data 2.46e-04 (2.30e-03)	Tok/s 83362 (78723)	Loss/tok 3.5382 (3.4901)	LR 8.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][400/484]	Time 0.546 (0.719)	Data 2.44e-04 (2.25e-03)	Tok/s 75314 (78704)	Loss/tok 3.2909 (3.4892)	LR 8.000e-03
0: TRAIN [2][410/484]	Time 1.090 (0.721)	Data 5.28e-04 (2.20e-03)	Tok/s 85489 (78715)	Loss/tok 3.7032 (3.4908)	LR 8.000e-03
0: TRAIN [2][420/484]	Time 1.086 (0.721)	Data 3.99e-04 (2.16e-03)	Tok/s 85486 (78743)	Loss/tok 3.6627 (3.4904)	LR 8.000e-03
0: TRAIN [2][430/484]	Time 0.542 (0.719)	Data 2.55e-04 (2.12e-03)	Tok/s 76108 (78710)	Loss/tok 3.2090 (3.4880)	LR 8.000e-03
0: TRAIN [2][440/484]	Time 1.090 (0.721)	Data 2.48e-04 (2.08e-03)	Tok/s 85714 (78782)	Loss/tok 3.6439 (3.4881)	LR 8.000e-03
0: TRAIN [2][450/484]	Time 0.551 (0.721)	Data 2.57e-04 (2.04e-03)	Tok/s 73838 (78778)	Loss/tok 3.2184 (3.4874)	LR 8.000e-03
0: TRAIN [2][460/484]	Time 1.088 (0.722)	Data 2.53e-04 (2.00e-03)	Tok/s 85789 (78747)	Loss/tok 3.6597 (3.4890)	LR 8.000e-03
0: TRAIN [2][470/484]	Time 0.800 (0.726)	Data 2.63e-04 (1.96e-03)	Tok/s 83192 (78816)	Loss/tok 3.5185 (3.4919)	LR 8.000e-03
0: TRAIN [2][480/484]	Time 0.544 (0.724)	Data 2.74e-04 (1.93e-03)	Tok/s 76227 (78799)	Loss/tok 3.1847 (3.4889)	LR 8.000e-03
:::MLL 1571850192.775 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1571850192.775 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.813 (0.813)	Decoder iters 149.0 (149.0)	Tok/s 20835 (20835)
0: Running moses detokenizer
0: BLEU(score=19.10492537334797, counts=[34650, 15962, 8503, 4737], totals=[68539, 65536, 62533, 59533], precisions=[50.55515837698245, 24.3560791015625, 13.59762045639902, 7.956931449784153], bp=1.0, sys_len=68539, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571850195.150 eval_accuracy: {"value": 19.1, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1571850195.150 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.4891	Test BLEU: 19.10
0: Performance: Epoch: 2	Training: 630806 Tok/s
0: Finished epoch 2
:::MLL 1571850195.151 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1571850195.151 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571850195.151 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 2351548555
0: TRAIN [3][0/484]	Time 1.322 (1.322)	Data 7.53e-01 (7.53e-01)	Tok/s 31134 (31134)	Loss/tok 3.2030 (3.2030)	LR 8.000e-03
0: TRAIN [3][10/484]	Time 0.800 (0.800)	Data 3.00e-04 (6.87e-02)	Tok/s 83978 (70848)	Loss/tok 3.4683 (3.4740)	LR 8.000e-03
0: TRAIN [3][20/484]	Time 0.801 (0.741)	Data 2.58e-04 (3.61e-02)	Tok/s 83711 (74900)	Loss/tok 3.4442 (3.4103)	LR 8.000e-03
0: TRAIN [3][30/484]	Time 0.548 (0.722)	Data 2.59e-04 (2.45e-02)	Tok/s 75627 (76253)	Loss/tok 3.2106 (3.3873)	LR 8.000e-03
0: TRAIN [3][40/484]	Time 1.084 (0.728)	Data 2.82e-04 (1.86e-02)	Tok/s 85909 (76661)	Loss/tok 3.6020 (3.4046)	LR 8.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][50/484]	Time 0.802 (0.685)	Data 2.45e-04 (1.50e-02)	Tok/s 83367 (75921)	Loss/tok 3.4619 (3.3748)	LR 8.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [3][60/484]	Time 0.800 (0.702)	Data 3.24e-04 (1.26e-02)	Tok/s 83829 (76717)	Loss/tok 3.4266 (3.3878)	LR 8.000e-03
0: TRAIN [3][70/484]	Time 1.090 (0.703)	Data 2.44e-04 (1.09e-02)	Tok/s 85632 (77122)	Loss/tok 3.6165 (3.3842)	LR 8.000e-03
0: TRAIN [3][80/484]	Time 0.801 (0.698)	Data 2.56e-04 (9.56e-03)	Tok/s 84503 (77133)	Loss/tok 3.3969 (3.3779)	LR 8.000e-03
0: TRAIN [3][90/484]	Time 0.324 (0.702)	Data 2.47e-04 (8.54e-03)	Tok/s 65455 (77338)	Loss/tok 2.7417 (3.3820)	LR 8.000e-03
0: TRAIN [3][100/484]	Time 0.319 (0.712)	Data 2.59e-04 (7.72e-03)	Tok/s 66008 (77515)	Loss/tok 2.7300 (3.3977)	LR 8.000e-03
0: TRAIN [3][110/484]	Time 0.546 (0.714)	Data 2.54e-04 (7.05e-03)	Tok/s 76207 (77769)	Loss/tok 3.1673 (3.3961)	LR 8.000e-03
0: TRAIN [3][120/484]	Time 0.544 (0.719)	Data 2.50e-04 (6.49e-03)	Tok/s 75240 (77782)	Loss/tok 3.2797 (3.4067)	LR 8.000e-03
0: TRAIN [3][130/484]	Time 0.551 (0.714)	Data 2.51e-04 (6.01e-03)	Tok/s 75262 (77801)	Loss/tok 3.1981 (3.4014)	LR 8.000e-03
0: TRAIN [3][140/484]	Time 0.547 (0.715)	Data 2.59e-04 (5.60e-03)	Tok/s 75544 (78010)	Loss/tok 3.1955 (3.3990)	LR 8.000e-03
0: TRAIN [3][150/484]	Time 0.544 (0.715)	Data 2.52e-04 (5.25e-03)	Tok/s 75950 (78032)	Loss/tok 3.1452 (3.4004)	LR 8.000e-03
0: TRAIN [3][160/484]	Time 0.316 (0.710)	Data 2.63e-04 (4.94e-03)	Tok/s 66666 (77993)	Loss/tok 2.7057 (3.3956)	LR 8.000e-03
0: TRAIN [3][170/484]	Time 0.548 (0.713)	Data 2.58e-04 (4.67e-03)	Tok/s 75532 (78162)	Loss/tok 3.2189 (3.3983)	LR 8.000e-03
0: TRAIN [3][180/484]	Time 1.086 (0.720)	Data 2.52e-04 (4.42e-03)	Tok/s 86586 (78380)	Loss/tok 3.5496 (3.4037)	LR 8.000e-03
0: TRAIN [3][190/484]	Time 0.800 (0.723)	Data 3.09e-04 (4.21e-03)	Tok/s 84502 (78498)	Loss/tok 3.4054 (3.4077)	LR 8.000e-03
0: TRAIN [3][200/484]	Time 0.544 (0.718)	Data 2.45e-04 (4.01e-03)	Tok/s 75951 (78474)	Loss/tok 3.1547 (3.4027)	LR 8.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [3][210/484]	Time 0.320 (0.724)	Data 2.49e-04 (3.83e-03)	Tok/s 66564 (78534)	Loss/tok 2.7596 (3.4090)	LR 8.000e-03
0: TRAIN [3][220/484]	Time 0.798 (0.723)	Data 2.61e-04 (3.67e-03)	Tok/s 84118 (78534)	Loss/tok 3.4540 (3.4104)	LR 8.000e-03
0: TRAIN [3][230/484]	Time 0.548 (0.724)	Data 2.55e-04 (3.52e-03)	Tok/s 75870 (78560)	Loss/tok 3.2525 (3.4123)	LR 8.000e-03
0: TRAIN [3][240/484]	Time 0.800 (0.726)	Data 2.46e-04 (3.39e-03)	Tok/s 84064 (78700)	Loss/tok 3.4154 (3.4143)	LR 8.000e-03
0: TRAIN [3][250/484]	Time 0.801 (0.721)	Data 2.49e-04 (3.26e-03)	Tok/s 84309 (78631)	Loss/tok 3.4243 (3.4105)	LR 8.000e-03
0: TRAIN [3][260/484]	Time 0.546 (0.720)	Data 2.43e-04 (3.15e-03)	Tok/s 75858 (78644)	Loss/tok 3.1236 (3.4087)	LR 8.000e-03
0: TRAIN [3][270/484]	Time 1.088 (0.716)	Data 3.05e-04 (3.04e-03)	Tok/s 86116 (78567)	Loss/tok 3.6094 (3.4061)	LR 8.000e-03
0: TRAIN [3][280/484]	Time 0.803 (0.715)	Data 2.42e-04 (2.94e-03)	Tok/s 83609 (78530)	Loss/tok 3.4350 (3.4057)	LR 8.000e-03
0: TRAIN [3][290/484]	Time 0.545 (0.719)	Data 2.49e-04 (2.85e-03)	Tok/s 75511 (78622)	Loss/tok 3.1839 (3.4094)	LR 8.000e-03
0: TRAIN [3][300/484]	Time 0.545 (0.717)	Data 2.76e-04 (2.77e-03)	Tok/s 75626 (78577)	Loss/tok 3.2513 (3.4102)	LR 8.000e-03
0: TRAIN [3][310/484]	Time 1.089 (0.721)	Data 2.54e-04 (2.69e-03)	Tok/s 85196 (78651)	Loss/tok 3.6338 (3.4144)	LR 8.000e-03
0: TRAIN [3][320/484]	Time 0.799 (0.722)	Data 2.60e-04 (2.61e-03)	Tok/s 84358 (78641)	Loss/tok 3.4567 (3.4166)	LR 8.000e-03
0: TRAIN [3][330/484]	Time 1.089 (0.729)	Data 3.41e-04 (2.54e-03)	Tok/s 85382 (78744)	Loss/tok 3.5973 (3.4245)	LR 8.000e-03
0: TRAIN [3][340/484]	Time 0.550 (0.730)	Data 2.94e-04 (2.48e-03)	Tok/s 75131 (78748)	Loss/tok 3.2050 (3.4256)	LR 8.000e-03
0: TRAIN [3][350/484]	Time 0.803 (0.727)	Data 2.43e-04 (2.41e-03)	Tok/s 83757 (78733)	Loss/tok 3.4633 (3.4230)	LR 8.000e-03
0: TRAIN [3][360/484]	Time 0.798 (0.725)	Data 2.70e-04 (2.35e-03)	Tok/s 83621 (78725)	Loss/tok 3.4449 (3.4210)	LR 8.000e-03
0: TRAIN [3][370/484]	Time 0.547 (0.722)	Data 2.60e-04 (2.30e-03)	Tok/s 75649 (78703)	Loss/tok 3.2295 (3.4182)	LR 8.000e-03
0: TRAIN [3][380/484]	Time 0.543 (0.722)	Data 2.59e-04 (2.24e-03)	Tok/s 75320 (78723)	Loss/tok 3.2282 (3.4176)	LR 8.000e-03
0: TRAIN [3][390/484]	Time 0.801 (0.721)	Data 2.44e-04 (2.19e-03)	Tok/s 83297 (78725)	Loss/tok 3.4542 (3.4164)	LR 8.000e-03
0: TRAIN [3][400/484]	Time 0.548 (0.722)	Data 2.42e-04 (2.15e-03)	Tok/s 75603 (78742)	Loss/tok 3.1366 (3.4171)	LR 8.000e-03
0: TRAIN [3][410/484]	Time 0.810 (0.722)	Data 2.74e-04 (2.10e-03)	Tok/s 83382 (78766)	Loss/tok 3.4211 (3.4157)	LR 8.000e-03
0: TRAIN [3][420/484]	Time 0.541 (0.723)	Data 2.66e-04 (2.06e-03)	Tok/s 76725 (78800)	Loss/tok 3.2037 (3.4158)	LR 8.000e-03
0: TRAIN [3][430/484]	Time 0.802 (0.726)	Data 2.48e-04 (2.02e-03)	Tok/s 83526 (78887)	Loss/tok 3.4181 (3.4183)	LR 8.000e-03
0: TRAIN [3][440/484]	Time 0.802 (0.725)	Data 4.67e-04 (1.98e-03)	Tok/s 84480 (78869)	Loss/tok 3.3833 (3.4167)	LR 8.000e-03
0: TRAIN [3][450/484]	Time 0.545 (0.726)	Data 2.39e-04 (1.94e-03)	Tok/s 75204 (78912)	Loss/tok 3.2295 (3.4175)	LR 8.000e-03
0: TRAIN [3][460/484]	Time 0.546 (0.725)	Data 2.60e-04 (1.90e-03)	Tok/s 76666 (78882)	Loss/tok 3.2216 (3.4168)	LR 8.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][470/484]	Time 0.547 (0.725)	Data 2.47e-04 (1.87e-03)	Tok/s 75304 (78893)	Loss/tok 3.1755 (3.4178)	LR 8.000e-03
0: TRAIN [3][480/484]	Time 0.546 (0.726)	Data 2.43e-04 (1.83e-03)	Tok/s 74979 (78891)	Loss/tok 3.1919 (3.4181)	LR 8.000e-03
:::MLL 1571850548.698 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 524}}
:::MLL 1571850548.698 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.660 (0.660)	Decoder iters 122.0 (122.0)	Tok/s 23008 (23008)
0: Running moses detokenizer
0: BLEU(score=19.822165430911667, counts=[33065, 15408, 8324, 4682], totals=[59558, 56555, 53554, 50558], precisions=[55.517310856643945, 27.244275484042085, 15.543190051163311, 9.260651133351795], bp=0.9176556753303006, sys_len=59558, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571850550.515 eval_accuracy: {"value": 19.82, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 535}}
:::MLL 1571850550.516 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 3	Training Loss: 3.4156	Test BLEU: 19.82
0: Performance: Epoch: 3	Training: 630961 Tok/s
0: Finished epoch 3
:::MLL 1571850550.516 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 557}}
:::MLL 1571850550.517 block_start: {"value": null, "metadata": {"first_epoch_num": 5, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571850550.517 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 514}}
0: Starting epoch 4
0: Executing preallocation
0: Sampler for epoch 4 uses seed 3103697802
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [4][0/484]	Time 1.861 (1.861)	Data 7.57e-01 (7.57e-01)	Tok/s 50428 (50428)	Loss/tok 3.5214 (3.5214)	LR 8.000e-03
0: TRAIN [4][10/484]	Time 1.514 (0.785)	Data 2.44e-04 (6.91e-02)	Tok/s 77997 (73706)	Loss/tok 3.8217 (3.3868)	LR 8.000e-03
0: TRAIN [4][20/484]	Time 0.547 (0.741)	Data 2.61e-04 (3.63e-02)	Tok/s 75455 (75426)	Loss/tok 3.2449 (3.3862)	LR 8.000e-03
0: TRAIN [4][30/484]	Time 0.546 (0.713)	Data 2.49e-04 (2.47e-02)	Tok/s 75539 (76388)	Loss/tok 3.1635 (3.3596)	LR 8.000e-03
0: TRAIN [4][40/484]	Time 0.318 (0.713)	Data 2.57e-04 (1.87e-02)	Tok/s 66512 (77028)	Loss/tok 2.6797 (3.3590)	LR 8.000e-03
0: TRAIN [4][50/484]	Time 0.320 (0.727)	Data 2.41e-04 (1.51e-02)	Tok/s 66282 (77811)	Loss/tok 2.7501 (3.3701)	LR 8.000e-03
0: TRAIN [4][60/484]	Time 0.802 (0.711)	Data 2.51e-04 (1.27e-02)	Tok/s 83385 (77852)	Loss/tok 3.3863 (3.3520)	LR 8.000e-03
0: TRAIN [4][70/484]	Time 0.319 (0.716)	Data 2.30e-04 (1.09e-02)	Tok/s 66597 (77929)	Loss/tok 2.7171 (3.3597)	LR 8.000e-03
0: TRAIN [4][80/484]	Time 1.422 (0.719)	Data 2.42e-04 (9.61e-03)	Tok/s 83639 (78054)	Loss/tok 3.7358 (3.3636)	LR 8.000e-03
0: TRAIN [4][90/484]	Time 0.545 (0.711)	Data 2.82e-04 (8.59e-03)	Tok/s 75669 (78112)	Loss/tok 3.1426 (3.3550)	LR 8.000e-03
0: TRAIN [4][100/484]	Time 0.808 (0.706)	Data 2.66e-04 (7.77e-03)	Tok/s 83178 (78162)	Loss/tok 3.3745 (3.3482)	LR 8.000e-03
0: TRAIN [4][110/484]	Time 0.803 (0.716)	Data 2.91e-04 (7.09e-03)	Tok/s 84523 (78417)	Loss/tok 3.3675 (3.3560)	LR 8.000e-03
0: TRAIN [4][120/484]	Time 0.544 (0.711)	Data 2.97e-04 (6.53e-03)	Tok/s 75738 (78388)	Loss/tok 3.1014 (3.3502)	LR 8.000e-03
0: TRAIN [4][130/484]	Time 0.804 (0.715)	Data 2.41e-04 (6.05e-03)	Tok/s 83100 (78494)	Loss/tok 3.3597 (3.3553)	LR 8.000e-03
0: TRAIN [4][140/484]	Time 0.550 (0.715)	Data 2.45e-04 (5.64e-03)	Tok/s 74896 (78449)	Loss/tok 3.1368 (3.3574)	LR 8.000e-03
0: TRAIN [4][150/484]	Time 0.547 (0.715)	Data 2.48e-04 (5.28e-03)	Tok/s 75277 (78446)	Loss/tok 3.1137 (3.3584)	LR 8.000e-03
0: TRAIN [4][160/484]	Time 0.548 (0.713)	Data 2.57e-04 (4.97e-03)	Tok/s 75088 (78465)	Loss/tok 3.1331 (3.3580)	LR 8.000e-03
0: TRAIN [4][170/484]	Time 0.542 (0.721)	Data 2.67e-04 (4.69e-03)	Tok/s 76108 (78630)	Loss/tok 3.2557 (3.3642)	LR 8.000e-03
0: TRAIN [4][180/484]	Time 0.318 (0.721)	Data 2.58e-04 (4.45e-03)	Tok/s 66861 (78554)	Loss/tok 2.7445 (3.3663)	LR 8.000e-03
0: TRAIN [4][190/484]	Time 0.544 (0.727)	Data 2.41e-04 (4.23e-03)	Tok/s 75310 (78587)	Loss/tok 3.1638 (3.3738)	LR 8.000e-03
0: TRAIN [4][200/484]	Time 0.543 (0.728)	Data 2.55e-04 (4.03e-03)	Tok/s 76181 (78717)	Loss/tok 3.2018 (3.3738)	LR 8.000e-03
0: TRAIN [4][210/484]	Time 0.319 (0.722)	Data 2.58e-04 (3.86e-03)	Tok/s 65842 (78564)	Loss/tok 2.7035 (3.3692)	LR 8.000e-03
0: TRAIN [4][220/484]	Time 1.085 (0.719)	Data 2.62e-04 (3.69e-03)	Tok/s 85534 (78511)	Loss/tok 3.6098 (3.3673)	LR 8.000e-03
0: TRAIN [4][230/484]	Time 0.548 (0.725)	Data 2.68e-04 (3.55e-03)	Tok/s 76130 (78687)	Loss/tok 3.2085 (3.3719)	LR 8.000e-03
0: TRAIN [4][240/484]	Time 0.547 (0.726)	Data 2.50e-04 (3.41e-03)	Tok/s 76272 (78811)	Loss/tok 3.1817 (3.3724)	LR 8.000e-03
0: TRAIN [4][250/484]	Time 0.547 (0.731)	Data 2.47e-04 (3.29e-03)	Tok/s 75931 (78930)	Loss/tok 3.1713 (3.3764)	LR 8.000e-03
0: TRAIN [4][260/484]	Time 0.545 (0.726)	Data 2.44e-04 (3.17e-03)	Tok/s 75218 (78826)	Loss/tok 3.1666 (3.3730)	LR 8.000e-03
0: TRAIN [4][270/484]	Time 1.091 (0.726)	Data 2.63e-04 (3.06e-03)	Tok/s 84970 (78842)	Loss/tok 3.5884 (3.3724)	LR 8.000e-03
0: TRAIN [4][280/484]	Time 0.546 (0.725)	Data 2.57e-04 (2.96e-03)	Tok/s 75379 (78796)	Loss/tok 3.1721 (3.3741)	LR 8.000e-03
0: TRAIN [4][290/484]	Time 0.806 (0.725)	Data 2.61e-04 (2.87e-03)	Tok/s 82729 (78827)	Loss/tok 3.4452 (3.3735)	LR 8.000e-03
0: TRAIN [4][300/484]	Time 0.804 (0.725)	Data 2.69e-04 (2.78e-03)	Tok/s 84055 (78804)	Loss/tok 3.3450 (3.3741)	LR 8.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [4][310/484]	Time 1.423 (0.728)	Data 2.45e-04 (2.70e-03)	Tok/s 83962 (78870)	Loss/tok 3.7309 (3.3773)	LR 8.000e-03
0: TRAIN [4][320/484]	Time 1.088 (0.723)	Data 2.59e-04 (2.63e-03)	Tok/s 85934 (78765)	Loss/tok 3.5334 (3.3740)	LR 8.000e-03
0: TRAIN [4][330/484]	Time 0.546 (0.721)	Data 6.35e-04 (2.56e-03)	Tok/s 75533 (78688)	Loss/tok 3.1808 (3.3736)	LR 8.000e-03
0: TRAIN [4][340/484]	Time 0.546 (0.718)	Data 2.50e-04 (2.49e-03)	Tok/s 75152 (78645)	Loss/tok 3.1950 (3.3707)	LR 8.000e-03
0: TRAIN [4][350/484]	Time 0.548 (0.716)	Data 2.54e-04 (2.43e-03)	Tok/s 74789 (78638)	Loss/tok 3.1963 (3.3686)	LR 8.000e-03
0: TRAIN [4][360/484]	Time 0.547 (0.714)	Data 2.57e-04 (2.37e-03)	Tok/s 75638 (78628)	Loss/tok 3.1832 (3.3669)	LR 8.000e-03
0: TRAIN [4][370/484]	Time 0.546 (0.717)	Data 2.61e-04 (2.31e-03)	Tok/s 75335 (78688)	Loss/tok 3.1151 (3.3696)	LR 8.000e-03
0: TRAIN [4][380/484]	Time 0.808 (0.719)	Data 2.66e-04 (2.25e-03)	Tok/s 82964 (78806)	Loss/tok 3.3858 (3.3701)	LR 8.000e-03
0: TRAIN [4][390/484]	Time 0.544 (0.721)	Data 2.62e-04 (2.21e-03)	Tok/s 76205 (78805)	Loss/tok 3.1272 (3.3714)	LR 8.000e-03
0: TRAIN [4][400/484]	Time 0.801 (0.722)	Data 2.43e-04 (2.16e-03)	Tok/s 84319 (78832)	Loss/tok 3.3870 (3.3725)	LR 8.000e-03
0: TRAIN [4][410/484]	Time 0.802 (0.721)	Data 3.11e-04 (2.11e-03)	Tok/s 83697 (78790)	Loss/tok 3.4387 (3.3726)	LR 8.000e-03
0: TRAIN [4][420/484]	Time 1.426 (0.722)	Data 2.65e-04 (2.07e-03)	Tok/s 83167 (78818)	Loss/tok 3.7542 (3.3748)	LR 8.000e-03
0: TRAIN [4][430/484]	Time 0.549 (0.721)	Data 2.49e-04 (2.03e-03)	Tok/s 74452 (78820)	Loss/tok 3.1486 (3.3739)	LR 8.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [4][440/484]	Time 1.424 (0.724)	Data 2.33e-04 (1.99e-03)	Tok/s 83097 (78852)	Loss/tok 3.7096 (3.3762)	LR 8.000e-03
0: TRAIN [4][450/484]	Time 1.084 (0.724)	Data 2.42e-04 (1.95e-03)	Tok/s 85828 (78857)	Loss/tok 3.5945 (3.3771)	LR 8.000e-03
0: TRAIN [4][460/484]	Time 0.800 (0.724)	Data 2.48e-04 (1.91e-03)	Tok/s 83164 (78886)	Loss/tok 3.4003 (3.3764)	LR 8.000e-03
0: TRAIN [4][470/484]	Time 0.801 (0.724)	Data 2.50e-04 (1.88e-03)	Tok/s 84163 (78932)	Loss/tok 3.3970 (3.3761)	LR 8.000e-03
0: TRAIN [4][480/484]	Time 0.545 (0.723)	Data 2.54e-04 (1.84e-03)	Tok/s 76423 (78940)	Loss/tok 3.2008 (3.3754)	LR 8.000e-03
:::MLL 1571850903.981 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 524}}
:::MLL 1571850903.982 eval_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [4][0/3]	Time 0.771 (0.771)	Decoder iters 149.0 (149.0)	Tok/s 21300 (21300)
0: Running moses detokenizer
0: BLEU(score=20.832328723859884, counts=[35080, 16495, 8973, 5067], totals=[65730, 62727, 59724, 56726], precisions=[53.36984634109235, 26.296491144164396, 15.02411091018686, 8.932411945139794], bp=1.0, sys_len=65730, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571850906.000 eval_accuracy: {"value": 20.83, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 535}}
:::MLL 1571850906.000 eval_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 4	Training Loss: 3.3777	Test BLEU: 20.83
0: Performance: Epoch: 4	Training: 631438 Tok/s
0: Finished epoch 4
:::MLL 1571850906.001 block_stop: {"value": null, "metadata": {"first_epoch_num": 5, "file": "train.py", "lineno": 557}}
:::MLL 1571850906.001 block_start: {"value": null, "metadata": {"first_epoch_num": 6, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571850906.001 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 514}}
0: Starting epoch 5
0: Executing preallocation
0: Sampler for epoch 5 uses seed 2668966872
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [5][0/484]	Time 1.337 (1.337)	Data 7.69e-01 (7.69e-01)	Tok/s 30846 (30846)	Loss/tok 3.1003 (3.1003)	LR 8.000e-03
0: TRAIN [5][10/484]	Time 0.547 (0.767)	Data 2.40e-04 (7.01e-02)	Tok/s 76991 (74821)	Loss/tok 3.0909 (3.3035)	LR 8.000e-03
0: TRAIN [5][20/484]	Time 1.083 (0.713)	Data 2.36e-04 (3.69e-02)	Tok/s 86422 (76480)	Loss/tok 3.4462 (3.2837)	LR 8.000e-03
0: TRAIN [5][30/484]	Time 0.798 (0.728)	Data 3.00e-04 (2.51e-02)	Tok/s 83577 (77709)	Loss/tok 3.3535 (3.3007)	LR 8.000e-03
0: TRAIN [5][40/484]	Time 0.543 (0.715)	Data 2.53e-04 (1.90e-02)	Tok/s 76180 (77308)	Loss/tok 3.1691 (3.3000)	LR 8.000e-03
0: TRAIN [5][50/484]	Time 0.547 (0.738)	Data 2.58e-04 (1.53e-02)	Tok/s 74723 (77754)	Loss/tok 3.1487 (3.3317)	LR 8.000e-03
0: TRAIN [5][60/484]	Time 0.546 (0.731)	Data 2.34e-04 (1.29e-02)	Tok/s 75167 (78217)	Loss/tok 3.1433 (3.3217)	LR 8.000e-03
0: TRAIN [5][70/484]	Time 0.544 (0.719)	Data 2.65e-04 (1.11e-02)	Tok/s 75901 (77902)	Loss/tok 3.1252 (3.3218)	LR 8.000e-03
0: TRAIN [5][80/484]	Time 0.799 (0.741)	Data 2.71e-04 (9.75e-03)	Tok/s 83722 (78533)	Loss/tok 3.3941 (3.3391)	LR 8.000e-03
0: TRAIN [5][90/484]	Time 0.542 (0.746)	Data 2.66e-04 (8.70e-03)	Tok/s 75244 (78770)	Loss/tok 3.0862 (3.3432)	LR 8.000e-03
0: TRAIN [5][100/484]	Time 0.799 (0.732)	Data 2.61e-04 (7.87e-03)	Tok/s 84358 (78611)	Loss/tok 3.3332 (3.3334)	LR 8.000e-03
0: TRAIN [5][110/484]	Time 0.546 (0.731)	Data 2.54e-04 (7.19e-03)	Tok/s 76167 (78832)	Loss/tok 3.1097 (3.3309)	LR 8.000e-03
0: TRAIN [5][120/484]	Time 0.799 (0.731)	Data 2.59e-04 (6.61e-03)	Tok/s 84139 (78940)	Loss/tok 3.2772 (3.3301)	LR 8.000e-03
0: TRAIN [5][130/484]	Time 0.545 (0.727)	Data 2.57e-04 (6.13e-03)	Tok/s 76225 (78931)	Loss/tok 3.1433 (3.3274)	LR 8.000e-03
0: TRAIN [5][140/484]	Time 0.801 (0.724)	Data 2.57e-04 (5.72e-03)	Tok/s 84647 (78945)	Loss/tok 3.3284 (3.3236)	LR 8.000e-03
0: TRAIN [5][150/484]	Time 0.544 (0.728)	Data 2.53e-04 (5.36e-03)	Tok/s 76127 (79078)	Loss/tok 3.1138 (3.3267)	LR 8.000e-03
0: TRAIN [5][160/484]	Time 0.547 (0.735)	Data 2.49e-04 (5.04e-03)	Tok/s 75005 (79101)	Loss/tok 3.1207 (3.3360)	LR 8.000e-03
0: TRAIN [5][170/484]	Time 1.087 (0.742)	Data 2.46e-04 (4.76e-03)	Tok/s 85529 (79341)	Loss/tok 3.5024 (3.3406)	LR 8.000e-03
0: TRAIN [5][180/484]	Time 0.545 (0.738)	Data 2.58e-04 (4.51e-03)	Tok/s 76432 (79319)	Loss/tok 3.1271 (3.3372)	LR 8.000e-03
0: TRAIN [5][190/484]	Time 0.800 (0.740)	Data 2.70e-04 (4.29e-03)	Tok/s 84486 (79349)	Loss/tok 3.3329 (3.3396)	LR 8.000e-03
0: TRAIN [5][200/484]	Time 0.799 (0.737)	Data 2.59e-04 (4.09e-03)	Tok/s 83618 (79303)	Loss/tok 3.3438 (3.3377)	LR 8.000e-03
0: TRAIN [5][210/484]	Time 0.549 (0.740)	Data 2.47e-04 (3.91e-03)	Tok/s 74352 (79435)	Loss/tok 3.1393 (3.3397)	LR 8.000e-03
0: TRAIN [5][220/484]	Time 0.544 (0.733)	Data 2.48e-04 (3.75e-03)	Tok/s 76077 (79242)	Loss/tok 3.1437 (3.3363)	LR 8.000e-03
0: TRAIN [5][230/484]	Time 0.543 (0.726)	Data 2.80e-04 (3.60e-03)	Tok/s 75214 (79114)	Loss/tok 3.1510 (3.3316)	LR 8.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [5][240/484]	Time 0.546 (0.728)	Data 2.50e-04 (3.46e-03)	Tok/s 75492 (79152)	Loss/tok 3.1486 (3.3334)	LR 8.000e-03
0: TRAIN [5][250/484]	Time 0.544 (0.727)	Data 2.40e-04 (3.33e-03)	Tok/s 76248 (79190)	Loss/tok 3.1463 (3.3322)	LR 8.000e-03
0: TRAIN [5][260/484]	Time 0.543 (0.725)	Data 2.44e-04 (3.21e-03)	Tok/s 75607 (79160)	Loss/tok 3.1710 (3.3323)	LR 8.000e-03
0: TRAIN [5][270/484]	Time 1.091 (0.731)	Data 2.63e-04 (3.10e-03)	Tok/s 85823 (79246)	Loss/tok 3.5540 (3.3389)	LR 8.000e-03
0: TRAIN [5][280/484]	Time 0.548 (0.726)	Data 2.50e-04 (3.00e-03)	Tok/s 75253 (79134)	Loss/tok 3.1186 (3.3365)	LR 8.000e-03
0: TRAIN [5][290/484]	Time 0.548 (0.724)	Data 2.66e-04 (2.91e-03)	Tok/s 75580 (79098)	Loss/tok 3.1607 (3.3350)	LR 8.000e-03
0: TRAIN [5][300/484]	Time 0.542 (0.724)	Data 2.49e-04 (2.82e-03)	Tok/s 75632 (79155)	Loss/tok 3.2143 (3.3347)	LR 8.000e-03
0: TRAIN [5][310/484]	Time 0.545 (0.727)	Data 2.63e-04 (2.74e-03)	Tok/s 75313 (79248)	Loss/tok 3.1402 (3.3369)	LR 8.000e-03
0: TRAIN [5][320/484]	Time 0.800 (0.724)	Data 2.70e-04 (2.66e-03)	Tok/s 83720 (79212)	Loss/tok 3.4078 (3.3345)	LR 8.000e-03
0: TRAIN [5][330/484]	Time 1.080 (0.722)	Data 3.16e-04 (2.59e-03)	Tok/s 86324 (79187)	Loss/tok 3.5025 (3.3337)	LR 8.000e-03
0: TRAIN [5][340/484]	Time 0.320 (0.724)	Data 2.62e-04 (2.52e-03)	Tok/s 65452 (79119)	Loss/tok 2.7211 (3.3375)	LR 8.000e-03
0: TRAIN [5][350/484]	Time 0.546 (0.719)	Data 2.70e-04 (2.46e-03)	Tok/s 74905 (79017)	Loss/tok 3.1905 (3.3342)	LR 8.000e-03
0: TRAIN [5][360/484]	Time 0.804 (0.721)	Data 2.47e-04 (2.40e-03)	Tok/s 83098 (79094)	Loss/tok 3.3655 (3.3357)	LR 8.000e-03
0: TRAIN [5][370/484]	Time 0.544 (0.721)	Data 3.21e-04 (2.34e-03)	Tok/s 75884 (79013)	Loss/tok 3.1367 (3.3380)	LR 8.000e-03
0: TRAIN [5][380/484]	Time 0.320 (0.719)	Data 2.73e-04 (2.29e-03)	Tok/s 66025 (78945)	Loss/tok 2.7173 (3.3396)	LR 8.000e-03
0: TRAIN [5][390/484]	Time 1.091 (0.720)	Data 2.57e-04 (2.24e-03)	Tok/s 85459 (79001)	Loss/tok 3.5276 (3.3414)	LR 8.000e-03
0: TRAIN [5][400/484]	Time 0.318 (0.717)	Data 2.44e-04 (2.19e-03)	Tok/s 66498 (78924)	Loss/tok 2.7104 (3.3392)	LR 8.000e-03
0: TRAIN [5][410/484]	Time 0.319 (0.717)	Data 2.43e-04 (2.14e-03)	Tok/s 66514 (78929)	Loss/tok 2.7212 (3.3394)	LR 8.000e-03
0: TRAIN [5][420/484]	Time 1.425 (0.717)	Data 2.44e-04 (2.10e-03)	Tok/s 83396 (78892)	Loss/tok 3.6831 (3.3416)	LR 8.000e-03
0: TRAIN [5][430/484]	Time 1.087 (0.718)	Data 2.55e-04 (2.06e-03)	Tok/s 86238 (78922)	Loss/tok 3.5377 (3.3421)	LR 8.000e-03
0: TRAIN [5][440/484]	Time 0.800 (0.719)	Data 2.77e-04 (2.01e-03)	Tok/s 83440 (78947)	Loss/tok 3.3465 (3.3436)	LR 8.000e-03
0: TRAIN [5][450/484]	Time 0.546 (0.719)	Data 2.52e-04 (1.98e-03)	Tok/s 76106 (78941)	Loss/tok 3.1745 (3.3437)	LR 8.000e-03
0: TRAIN [5][460/484]	Time 1.088 (0.721)	Data 2.53e-04 (1.94e-03)	Tok/s 85534 (78989)	Loss/tok 3.5924 (3.3461)	LR 8.000e-03
0: TRAIN [5][470/484]	Time 0.799 (0.722)	Data 2.47e-04 (1.90e-03)	Tok/s 83807 (78991)	Loss/tok 3.3784 (3.3468)	LR 8.000e-03
0: TRAIN [5][480/484]	Time 1.085 (0.724)	Data 2.54e-04 (1.87e-03)	Tok/s 85695 (79006)	Loss/tok 3.5518 (3.3500)	LR 8.000e-03
:::MLL 1571851258.956 epoch_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 524}}
:::MLL 1571851258.956 eval_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [5][0/3]	Time 0.682 (0.682)	Decoder iters 122.0 (122.0)	Tok/s 23575 (23575)
0: Running moses detokenizer
0: BLEU(score=20.880497488239115, counts=[34618, 16307, 8822, 4967], totals=[64413, 61410, 58408, 55411], precisions=[53.743809479452906, 26.55430711610487, 15.104095329406931, 8.96392413058779], bp=0.9959252975021898, sys_len=64413, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571851260.851 eval_accuracy: {"value": 20.88, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 535}}
:::MLL 1571851260.852 eval_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 5	Training Loss: 3.3504	Test BLEU: 20.88
0: Performance: Epoch: 5	Training: 632103 Tok/s
0: Finished epoch 5
:::MLL 1571851260.852 block_stop: {"value": null, "metadata": {"first_epoch_num": 6, "file": "train.py", "lineno": 557}}
:::MLL 1571851260.853 block_start: {"value": null, "metadata": {"first_epoch_num": 7, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571851260.853 epoch_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 514}}
0: Starting epoch 6
0: Executing preallocation
0: Sampler for epoch 6 uses seed 3962703464
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [6][0/484]	Time 1.328 (1.328)	Data 7.65e-01 (7.65e-01)	Tok/s 30978 (30978)	Loss/tok 3.0892 (3.0892)	LR 8.000e-03
0: TRAIN [6][10/484]	Time 1.090 (0.784)	Data 2.53e-04 (6.98e-02)	Tok/s 85751 (75688)	Loss/tok 3.4867 (3.2874)	LR 8.000e-03
0: TRAIN [6][20/484]	Time 0.322 (0.760)	Data 2.58e-04 (3.67e-02)	Tok/s 65610 (77838)	Loss/tok 2.6690 (3.2880)	LR 8.000e-03
0: TRAIN [6][30/484]	Time 0.543 (0.725)	Data 2.74e-04 (2.49e-02)	Tok/s 75686 (78173)	Loss/tok 3.1404 (3.2702)	LR 8.000e-03
0: TRAIN [6][40/484]	Time 0.321 (0.726)	Data 3.04e-04 (1.89e-02)	Tok/s 65543 (78779)	Loss/tok 2.7078 (3.2702)	LR 8.000e-03
0: TRAIN [6][50/484]	Time 1.426 (0.743)	Data 2.94e-04 (1.53e-02)	Tok/s 83146 (78762)	Loss/tok 3.7130 (3.3008)	LR 8.000e-03
0: TRAIN [6][60/484]	Time 0.321 (0.730)	Data 2.95e-04 (1.28e-02)	Tok/s 66195 (78569)	Loss/tok 2.6695 (3.3001)	LR 8.000e-03
0: TRAIN [6][70/484]	Time 0.547 (0.726)	Data 2.72e-04 (1.11e-02)	Tok/s 74733 (78729)	Loss/tok 3.1121 (3.2993)	LR 8.000e-03
0: TRAIN [6][80/484]	Time 0.799 (0.710)	Data 2.58e-04 (9.73e-03)	Tok/s 83688 (78516)	Loss/tok 3.3205 (3.2862)	LR 8.000e-03
0: TRAIN [6][90/484]	Time 0.545 (0.711)	Data 7.16e-04 (8.69e-03)	Tok/s 75236 (78470)	Loss/tok 3.1682 (3.2899)	LR 8.000e-03
0: TRAIN [6][100/484]	Time 0.802 (0.712)	Data 2.37e-04 (7.86e-03)	Tok/s 83567 (78460)	Loss/tok 3.2759 (3.2938)	LR 8.000e-03
0: TRAIN [6][110/484]	Time 0.320 (0.712)	Data 2.52e-04 (7.18e-03)	Tok/s 67219 (78433)	Loss/tok 2.7282 (3.2973)	LR 8.000e-03
0: TRAIN [6][120/484]	Time 0.801 (0.711)	Data 2.45e-04 (6.60e-03)	Tok/s 83840 (78621)	Loss/tok 3.3986 (3.2954)	LR 8.000e-03
0: TRAIN [6][130/484]	Time 0.802 (0.717)	Data 2.50e-04 (6.12e-03)	Tok/s 83490 (78790)	Loss/tok 3.3047 (3.3017)	LR 8.000e-03
0: TRAIN [6][140/484]	Time 0.545 (0.715)	Data 2.52e-04 (5.70e-03)	Tok/s 76187 (78738)	Loss/tok 3.0856 (3.3021)	LR 8.000e-03
0: TRAIN [6][150/484]	Time 1.086 (0.716)	Data 2.63e-04 (5.35e-03)	Tok/s 85698 (78790)	Loss/tok 3.5224 (3.3042)	LR 8.000e-03
0: TRAIN [6][160/484]	Time 0.547 (0.709)	Data 2.73e-04 (5.03e-03)	Tok/s 75548 (78652)	Loss/tok 3.0750 (3.2996)	LR 8.000e-03
0: TRAIN [6][170/484]	Time 1.085 (0.705)	Data 2.53e-04 (4.75e-03)	Tok/s 85353 (78524)	Loss/tok 3.5337 (3.2972)	LR 8.000e-03
0: TRAIN [6][180/484]	Time 0.799 (0.701)	Data 2.41e-04 (4.50e-03)	Tok/s 83918 (78478)	Loss/tok 3.2938 (3.2940)	LR 8.000e-03
0: TRAIN [6][190/484]	Time 1.085 (0.704)	Data 2.55e-04 (4.28e-03)	Tok/s 85278 (78574)	Loss/tok 3.4892 (3.2966)	LR 8.000e-03
0: TRAIN [6][200/484]	Time 0.800 (0.709)	Data 2.77e-04 (4.08e-03)	Tok/s 83638 (78713)	Loss/tok 3.4242 (3.3022)	LR 8.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [6][210/484]	Time 0.546 (0.713)	Data 2.56e-04 (3.90e-03)	Tok/s 75883 (78812)	Loss/tok 3.1198 (3.3068)	LR 8.000e-03
0: TRAIN [6][220/484]	Time 0.545 (0.713)	Data 2.44e-04 (3.74e-03)	Tok/s 75882 (78838)	Loss/tok 3.1371 (3.3074)	LR 8.000e-03
0: TRAIN [6][230/484]	Time 0.541 (0.716)	Data 2.58e-04 (3.58e-03)	Tok/s 75957 (78900)	Loss/tok 3.1498 (3.3143)	LR 8.000e-03
0: TRAIN [6][240/484]	Time 0.544 (0.717)	Data 2.62e-04 (3.45e-03)	Tok/s 76108 (78905)	Loss/tok 3.1452 (3.3188)	LR 8.000e-03
0: TRAIN [6][250/484]	Time 0.543 (0.715)	Data 2.51e-04 (3.32e-03)	Tok/s 75775 (78892)	Loss/tok 3.1133 (3.3185)	LR 8.000e-03
0: TRAIN [6][260/484]	Time 1.083 (0.714)	Data 2.79e-04 (3.20e-03)	Tok/s 86481 (78837)	Loss/tok 3.5833 (3.3204)	LR 8.000e-03
0: TRAIN [6][270/484]	Time 0.320 (0.717)	Data 2.57e-04 (3.09e-03)	Tok/s 65708 (78947)	Loss/tok 2.6819 (3.3241)	LR 8.000e-03
0: TRAIN [6][280/484]	Time 0.544 (0.714)	Data 2.44e-04 (2.99e-03)	Tok/s 75257 (78893)	Loss/tok 3.1205 (3.3216)	LR 8.000e-03
0: TRAIN [6][290/484]	Time 0.547 (0.712)	Data 2.68e-04 (2.90e-03)	Tok/s 75691 (78843)	Loss/tok 3.1923 (3.3208)	LR 8.000e-03
0: TRAIN [6][300/484]	Time 0.802 (0.714)	Data 2.54e-04 (2.81e-03)	Tok/s 83886 (78920)	Loss/tok 3.2805 (3.3210)	LR 8.000e-03
0: TRAIN [6][310/484]	Time 0.543 (0.716)	Data 2.61e-04 (2.73e-03)	Tok/s 76356 (78920)	Loss/tok 3.1447 (3.3235)	LR 8.000e-03
0: TRAIN [6][320/484]	Time 0.546 (0.712)	Data 2.74e-04 (2.65e-03)	Tok/s 75383 (78814)	Loss/tok 3.1194 (3.3221)	LR 8.000e-03
0: TRAIN [6][330/484]	Time 0.546 (0.711)	Data 2.64e-04 (2.58e-03)	Tok/s 75071 (78755)	Loss/tok 3.1517 (3.3219)	LR 8.000e-03
0: TRAIN [6][340/484]	Time 0.549 (0.711)	Data 2.77e-04 (2.51e-03)	Tok/s 75710 (78749)	Loss/tok 3.2206 (3.3228)	LR 8.000e-03
0: TRAIN [6][350/484]	Time 1.085 (0.715)	Data 2.47e-04 (2.45e-03)	Tok/s 86372 (78820)	Loss/tok 3.5061 (3.3260)	LR 8.000e-03
0: TRAIN [6][360/484]	Time 0.320 (0.715)	Data 2.56e-04 (2.39e-03)	Tok/s 66374 (78835)	Loss/tok 2.6988 (3.3255)	LR 8.000e-03
0: TRAIN [6][370/484]	Time 0.542 (0.719)	Data 2.45e-04 (2.33e-03)	Tok/s 76566 (78870)	Loss/tok 3.1241 (3.3291)	LR 8.000e-03
0: TRAIN [6][380/484]	Time 0.545 (0.720)	Data 2.71e-04 (2.28e-03)	Tok/s 76225 (78923)	Loss/tok 3.1349 (3.3310)	LR 8.000e-03
0: TRAIN [6][390/484]	Time 0.319 (0.719)	Data 2.94e-04 (2.23e-03)	Tok/s 65554 (78942)	Loss/tok 2.7131 (3.3301)	LR 8.000e-03
0: TRAIN [6][400/484]	Time 0.546 (0.718)	Data 2.86e-04 (2.18e-03)	Tok/s 75516 (78925)	Loss/tok 3.2212 (3.3293)	LR 8.000e-03
0: TRAIN [6][410/484]	Time 0.545 (0.717)	Data 3.04e-04 (2.13e-03)	Tok/s 76200 (78935)	Loss/tok 3.1411 (3.3294)	LR 8.000e-03
0: TRAIN [6][420/484]	Time 0.801 (0.720)	Data 2.96e-04 (2.09e-03)	Tok/s 84087 (78951)	Loss/tok 3.3039 (3.3324)	LR 8.000e-03
0: TRAIN [6][430/484]	Time 0.320 (0.722)	Data 3.14e-04 (2.05e-03)	Tok/s 65746 (78978)	Loss/tok 2.7347 (3.3345)	LR 8.000e-03
0: TRAIN [6][440/484]	Time 1.084 (0.726)	Data 3.68e-04 (2.01e-03)	Tok/s 85878 (79011)	Loss/tok 3.5040 (3.3387)	LR 8.000e-03
0: TRAIN [6][450/484]	Time 0.545 (0.724)	Data 3.13e-04 (1.97e-03)	Tok/s 75544 (79009)	Loss/tok 3.1226 (3.3368)	LR 8.000e-03
0: TRAIN [6][460/484]	Time 0.546 (0.722)	Data 3.40e-04 (1.94e-03)	Tok/s 75390 (78973)	Loss/tok 3.1100 (3.3358)	LR 8.000e-03
0: TRAIN [6][470/484]	Time 1.089 (0.723)	Data 2.84e-04 (1.90e-03)	Tok/s 85649 (78994)	Loss/tok 3.4920 (3.3357)	LR 8.000e-03
0: TRAIN [6][480/484]	Time 0.801 (0.723)	Data 3.03e-04 (1.87e-03)	Tok/s 83854 (79019)	Loss/tok 3.3298 (3.3355)	LR 8.000e-03
:::MLL 1571851613.648 epoch_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 524}}
:::MLL 1571851613.648 eval_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [6][0/3]	Time 0.697 (0.697)	Decoder iters 118.0 (118.0)	Tok/s 23173 (23173)
0: Running moses detokenizer
0: BLEU(score=21.204747108139962, counts=[35000, 16540, 8995, 5074], totals=[64722, 61719, 58716, 55718], precisions=[54.07743889249405, 26.79887878935174, 15.319504053409633, 9.106572382353997], bp=1.0, sys_len=64722, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571851615.598 eval_accuracy: {"value": 21.2, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 535}}
:::MLL 1571851615.598 eval_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 6	Training Loss: 3.3374	Test BLEU: 21.20
0: Performance: Epoch: 6	Training: 632210 Tok/s
0: Finished epoch 6
:::MLL 1571851615.599 block_stop: {"value": null, "metadata": {"first_epoch_num": 7, "file": "train.py", "lineno": 557}}
:::MLL 1571851615.599 block_start: {"value": null, "metadata": {"first_epoch_num": 8, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571851615.599 epoch_start: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 514}}
0: Starting epoch 7
0: Executing preallocation
0: Sampler for epoch 7 uses seed 3351538135
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [7][0/484]	Time 1.127 (1.127)	Data 7.88e-01 (7.88e-01)	Tok/s 18774 (18774)	Loss/tok 2.6240 (2.6240)	LR 8.000e-03
0: TRAIN [7][10/484]	Time 1.083 (0.796)	Data 2.50e-04 (7.19e-02)	Tok/s 86256 (71738)	Loss/tok 3.4540 (3.3281)	LR 8.000e-03
0: TRAIN [7][20/484]	Time 0.802 (0.784)	Data 3.13e-04 (3.78e-02)	Tok/s 84167 (75361)	Loss/tok 3.2936 (3.3302)	LR 8.000e-03
0: TRAIN [7][30/484]	Time 0.319 (0.735)	Data 5.92e-04 (2.57e-02)	Tok/s 66495 (75995)	Loss/tok 2.6971 (3.2947)	LR 8.000e-03
0: TRAIN [7][40/484]	Time 0.544 (0.742)	Data 3.35e-04 (1.95e-02)	Tok/s 75491 (76872)	Loss/tok 3.1221 (3.3101)	LR 8.000e-03
0: TRAIN [7][50/484]	Time 0.546 (0.754)	Data 3.13e-04 (1.58e-02)	Tok/s 75959 (77176)	Loss/tok 3.0977 (3.3261)	LR 8.000e-03
0: TRAIN [7][60/484]	Time 0.799 (0.748)	Data 3.01e-04 (1.32e-02)	Tok/s 83947 (77362)	Loss/tok 3.3032 (3.3303)	LR 8.000e-03
0: TRAIN [7][70/484]	Time 0.550 (0.738)	Data 3.14e-04 (1.14e-02)	Tok/s 74206 (77584)	Loss/tok 3.0809 (3.3217)	LR 8.000e-03
0: TRAIN [7][80/484]	Time 0.544 (0.734)	Data 3.04e-04 (1.00e-02)	Tok/s 75289 (77942)	Loss/tok 3.0942 (3.3141)	LR 8.000e-03
0: TRAIN [7][90/484]	Time 1.086 (0.740)	Data 3.29e-04 (8.98e-03)	Tok/s 85542 (78245)	Loss/tok 3.4858 (3.3179)	LR 8.000e-03
0: TRAIN [7][100/484]	Time 1.085 (0.756)	Data 3.12e-04 (8.12e-03)	Tok/s 86185 (78598)	Loss/tok 3.4403 (3.3301)	LR 8.000e-03
0: TRAIN [7][110/484]	Time 1.086 (0.752)	Data 3.32e-04 (7.42e-03)	Tok/s 85296 (78652)	Loss/tok 3.5264 (3.3265)	LR 8.000e-03
0: TRAIN [7][120/484]	Time 0.546 (0.755)	Data 3.02e-04 (6.83e-03)	Tok/s 75686 (78759)	Loss/tok 3.1112 (3.3317)	LR 8.000e-03
0: TRAIN [7][130/484]	Time 1.087 (0.754)	Data 3.64e-04 (6.34e-03)	Tok/s 86456 (78761)	Loss/tok 3.4270 (3.3324)	LR 8.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [7][140/484]	Time 1.424 (0.757)	Data 3.01e-04 (5.91e-03)	Tok/s 83882 (78901)	Loss/tok 3.6484 (3.3335)	LR 8.000e-03
0: TRAIN [7][150/484]	Time 1.088 (0.755)	Data 3.08e-04 (5.54e-03)	Tok/s 85793 (78999)	Loss/tok 3.4837 (3.3316)	LR 8.000e-03
0: TRAIN [7][160/484]	Time 0.801 (0.759)	Data 3.26e-04 (5.21e-03)	Tok/s 84424 (79124)	Loss/tok 3.3043 (3.3344)	LR 8.000e-03
0: TRAIN [7][170/484]	Time 0.547 (0.752)	Data 3.21e-04 (4.93e-03)	Tok/s 76159 (79110)	Loss/tok 3.0856 (3.3289)	LR 8.000e-03
0: TRAIN [7][180/484]	Time 0.548 (0.752)	Data 3.20e-04 (4.67e-03)	Tok/s 75797 (79102)	Loss/tok 3.1294 (3.3296)	LR 8.000e-03
0: TRAIN [7][190/484]	Time 0.802 (0.747)	Data 3.19e-04 (4.44e-03)	Tok/s 83637 (79035)	Loss/tok 3.2889 (3.3266)	LR 8.000e-03
0: TRAIN [7][200/484]	Time 0.544 (0.743)	Data 3.19e-04 (4.24e-03)	Tok/s 76383 (79090)	Loss/tok 3.1232 (3.3225)	LR 8.000e-03
0: TRAIN [7][210/484]	Time 0.320 (0.739)	Data 3.09e-04 (4.05e-03)	Tok/s 65842 (79063)	Loss/tok 2.6602 (3.3191)	LR 8.000e-03
0: TRAIN [7][220/484]	Time 1.086 (0.734)	Data 3.14e-04 (3.88e-03)	Tok/s 85909 (78946)	Loss/tok 3.5087 (3.3167)	LR 8.000e-03
0: TRAIN [7][230/484]	Time 0.798 (0.733)	Data 3.10e-04 (3.73e-03)	Tok/s 84114 (79033)	Loss/tok 3.3117 (3.3140)	LR 8.000e-03
0: TRAIN [7][240/484]	Time 0.546 (0.734)	Data 3.22e-04 (3.59e-03)	Tok/s 74522 (79067)	Loss/tok 3.1510 (3.3150)	LR 8.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [7][250/484]	Time 1.425 (0.732)	Data 3.16e-04 (3.46e-03)	Tok/s 83773 (78999)	Loss/tok 3.6865 (3.3156)	LR 8.000e-03
0: TRAIN [7][260/484]	Time 1.092 (0.731)	Data 3.29e-04 (3.34e-03)	Tok/s 85423 (78895)	Loss/tok 3.5343 (3.3168)	LR 8.000e-03
0: TRAIN [7][270/484]	Time 0.542 (0.734)	Data 3.20e-04 (3.23e-03)	Tok/s 77369 (78973)	Loss/tok 3.1114 (3.3194)	LR 8.000e-03
0: TRAIN [7][280/484]	Time 0.545 (0.731)	Data 2.96e-04 (3.13e-03)	Tok/s 76113 (78971)	Loss/tok 3.1232 (3.3165)	LR 8.000e-03
0: TRAIN [7][290/484]	Time 0.808 (0.734)	Data 3.02e-04 (3.03e-03)	Tok/s 83341 (79046)	Loss/tok 3.3181 (3.3187)	LR 8.000e-03
0: TRAIN [7][300/484]	Time 0.548 (0.730)	Data 3.55e-04 (2.94e-03)	Tok/s 76321 (78908)	Loss/tok 3.0935 (3.3168)	LR 8.000e-03
0: TRAIN [7][310/484]	Time 0.546 (0.727)	Data 3.03e-04 (2.86e-03)	Tok/s 74802 (78864)	Loss/tok 3.1342 (3.3146)	LR 8.000e-03
0: TRAIN [7][320/484]	Time 0.798 (0.731)	Data 3.09e-04 (2.78e-03)	Tok/s 84581 (78942)	Loss/tok 3.3358 (3.3183)	LR 8.000e-03
0: TRAIN [7][330/484]	Time 1.088 (0.727)	Data 2.98e-04 (2.71e-03)	Tok/s 85745 (78879)	Loss/tok 3.5203 (3.3156)	LR 8.000e-03
0: TRAIN [7][340/484]	Time 0.542 (0.723)	Data 2.89e-04 (2.64e-03)	Tok/s 77365 (78821)	Loss/tok 3.0943 (3.3124)	LR 8.000e-03
0: TRAIN [7][350/484]	Time 0.801 (0.723)	Data 3.05e-04 (2.57e-03)	Tok/s 83809 (78829)	Loss/tok 3.2996 (3.3123)	LR 8.000e-03
0: TRAIN [7][360/484]	Time 0.543 (0.720)	Data 3.10e-04 (2.51e-03)	Tok/s 75984 (78787)	Loss/tok 3.0933 (3.3105)	LR 8.000e-03
0: TRAIN [7][370/484]	Time 0.801 (0.718)	Data 3.00e-04 (2.45e-03)	Tok/s 83984 (78764)	Loss/tok 3.2781 (3.3088)	LR 8.000e-03
0: TRAIN [7][380/484]	Time 0.542 (0.719)	Data 3.17e-04 (2.39e-03)	Tok/s 76573 (78832)	Loss/tok 3.1050 (3.3090)	LR 8.000e-03
0: TRAIN [7][390/484]	Time 0.546 (0.719)	Data 3.10e-04 (2.34e-03)	Tok/s 75679 (78792)	Loss/tok 3.0815 (3.3088)	LR 8.000e-03
0: TRAIN [7][400/484]	Time 0.802 (0.719)	Data 3.07e-04 (2.29e-03)	Tok/s 83611 (78802)	Loss/tok 3.3648 (3.3093)	LR 8.000e-03
0: TRAIN [7][410/484]	Time 1.424 (0.720)	Data 3.04e-04 (2.24e-03)	Tok/s 83982 (78808)	Loss/tok 3.6506 (3.3109)	LR 8.000e-03
0: TRAIN [7][420/484]	Time 1.088 (0.724)	Data 3.93e-04 (2.20e-03)	Tok/s 85947 (78900)	Loss/tok 3.4829 (3.3134)	LR 8.000e-03
0: TRAIN [7][430/484]	Time 0.802 (0.723)	Data 3.19e-04 (2.15e-03)	Tok/s 83349 (78890)	Loss/tok 3.3238 (3.3129)	LR 8.000e-03
0: TRAIN [7][440/484]	Time 1.428 (0.724)	Data 3.12e-04 (2.11e-03)	Tok/s 83402 (78914)	Loss/tok 3.7145 (3.3142)	LR 8.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [7][450/484]	Time 1.082 (0.725)	Data 3.18e-04 (2.07e-03)	Tok/s 85904 (78919)	Loss/tok 3.5825 (3.3168)	LR 8.000e-03
0: TRAIN [7][460/484]	Time 0.550 (0.726)	Data 3.02e-04 (2.03e-03)	Tok/s 75617 (78931)	Loss/tok 3.1073 (3.3178)	LR 8.000e-03
0: TRAIN [7][470/484]	Time 0.799 (0.725)	Data 3.14e-04 (2.00e-03)	Tok/s 84342 (78958)	Loss/tok 3.3063 (3.3176)	LR 8.000e-03
0: TRAIN [7][480/484]	Time 0.546 (0.724)	Data 2.99e-04 (1.96e-03)	Tok/s 76112 (78955)	Loss/tok 3.1076 (3.3160)	LR 8.000e-03
:::MLL 1571851968.633 epoch_stop: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 524}}
:::MLL 1571851968.633 eval_start: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [7][0/3]	Time 0.755 (0.755)	Decoder iters 149.0 (149.0)	Tok/s 21539 (21539)
0: Running moses detokenizer
0: BLEU(score=21.280219389162006, counts=[34927, 16552, 9010, 5126], totals=[64541, 61538, 58535, 55537], precisions=[54.115988286515545, 26.897201729012966, 15.39250021354745, 9.229882780848804], bp=0.9979104924146104, sys_len=64541, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571851970.639 eval_accuracy: {"value": 21.28, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 535}}
:::MLL 1571851970.639 eval_stop: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 7	Training Loss: 3.3160	Test BLEU: 21.28
0: Performance: Epoch: 7	Training: 631849 Tok/s
0: Finished epoch 7
:::MLL 1571851970.640 block_stop: {"value": null, "metadata": {"first_epoch_num": 8, "file": "train.py", "lineno": 557}}
0: Closing preprocessed data file
:::MLL 1571851970.640 run_stop: {"value": null, "metadata": {"status": "aborted", "file": "train.py", "lineno": 568}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-10-23 05:33:02 PM
RESULT,RNN_TRANSLATOR,,2885,nvidia,2019-10-23 04:44:57 PM
