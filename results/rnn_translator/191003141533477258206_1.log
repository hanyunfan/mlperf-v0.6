Beginning trial 1 of 2
Gathering sys log on dss01
:::MLL 1570130568.039 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1570130568.040 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1570130568.040 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1570130568.041 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1570130568.041 submission_platform: {"value": "1xDSS8440", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1570130568.041 submission_entry: {"value": "{'hardware': 'DSS8440', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '5.0-0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz', 'num_cores': '40', 'num_vcpus': '40', 'accelerator': 'Tesla V100-PCIE-32GB', 'num_accelerators': '8', 'sys_mem_size': '754 GB', 'sys_storage_type': 'SATA SSD', 'sys_storage_size': '1x 447.1G + 1x 931.5G', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27700 Family [ConnectX-4]', 'num_network_cards': '1', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1570130568.042 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1570130568.042 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1570130573.837 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node dss01
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=5020' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=191003141533477258206 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191003141533477258206 ./run_and_time.sh
Run vars: id 191003141533477258206 gpus 8 mparams  --master_port=5020
NCCL_SOCKET_NTHREADS=4
NCCL_NSOCKS_PERTHREAD=8
STARTING TIMING RUN AT 2019-10-03 07:22:54 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=0
+ MATH=amp_fp16
+ [[ 0 -eq 1 ]]
+ LAUNCH_OPT='torch.distributed.launch --nproc_per_node 8  --master_port=5020'
running benchmark
+ echo 'running benchmark'
+ python -m torch.distributed.launch --nproc_per_node 8 --master_port=5020 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1570130576.540 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570130576.543 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570130576.544 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570130576.544 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570130576.545 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570130576.545 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570130576.556 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570130576.557 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 1270624178
dss01:464:464 [0] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:464:464 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:464:464 [0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:464:464 [0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:464:464 [0] NCCL INFO NET/IB : No device found.
dss01:464:464 [0] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
NCCL version 2.4.8+cuda10.1
dss01:466:466 [2] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:466:466 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:470:470 [6] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:470:470 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:467:467 [3] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:467:467 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:465:465 [1] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:465:465 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:468:468 [4] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:468:468 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:471:471 [7] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:471:471 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:469:469 [5] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:469:469 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:470:470 [6] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:470:470 [6] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:470:470 [6] NCCL INFO NET/IB : No device found.

dss01:466:466 [2] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:466:466 [2] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:466:466 [2] NCCL INFO NET/IB : No device found.

dss01:465:465 [1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:468:468 [4] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:465:465 [1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0

dss01:468:468 [4] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:465:465 [1] NCCL INFO NET/IB : No device found.
dss01:468:468 [4] NCCL INFO NET/IB : No device found.

dss01:469:469 [5] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:469:469 [5] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:469:469 [5] NCCL INFO NET/IB : No device found.

dss01:467:467 [3] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:467:467 [3] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:467:467 [3] NCCL INFO NET/IB : No device found.

dss01:471:471 [7] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:471:471 [7] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:471:471 [7] NCCL INFO NET/IB : No device found.
dss01:470:470 [6] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:466:466 [2] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:468:468 [4] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:469:469 [5] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:465:465 [1] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:471:471 [7] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:467:467 [3] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:464:826 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555
dss01:466:827 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555
dss01:468:829 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa
dss01:470:828 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa
dss01:471:830 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa
dss01:467:831 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555
dss01:469:832 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa
dss01:465:833 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555
dss01:469:832 [5] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:470:828 [6] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:471:830 [7] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:464:826 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:465:833 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:466:827 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:467:831 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:468:829 [4] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:464:826 [0] NCCL INFO Channel 00 :    0   1   2   3   4   5   6   7
dss01:468:829 [4] NCCL INFO Ring 00 : 4[4] -> 5[5] via P2P/IPC
dss01:470:828 [6] NCCL INFO Ring 00 : 6[6] -> 7[7] via P2P/IPC
dss01:464:826 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/IPC
dss01:466:827 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/IPC
dss01:467:831 [3] NCCL INFO Ring 00 : 3[3] -> 4[4] via direct shared memory
dss01:469:832 [5] NCCL INFO Ring 00 : 5[5] -> 6[6] via direct shared memory
dss01:471:830 [7] NCCL INFO Ring 00 : 7[7] -> 0[0] via direct shared memory
dss01:465:833 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via direct shared memory
dss01:464:826 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
dss01:467:831 [3] NCCL INFO comm 0x7fff50007590 rank 3 nranks 8 cudaDev 3 nvmlDev 3 - Init COMPLETE
dss01:471:830 [7] NCCL INFO comm 0x7ffe98007590 rank 7 nranks 8 cudaDev 7 nvmlDev 7 - Init COMPLETE
dss01:465:833 [1] NCCL INFO comm 0x7fff2c007590 rank 1 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE
dss01:469:832 [5] NCCL INFO comm 0x7fff70007590 rank 5 nranks 8 cudaDev 5 nvmlDev 5 - Init COMPLETE
dss01:468:829 [4] NCCL INFO comm 0x7fff50007590 rank 4 nranks 8 cudaDev 4 nvmlDev 4 - Init COMPLETE
dss01:470:828 [6] NCCL INFO comm 0x7ffe98007590 rank 6 nranks 8 cudaDev 6 nvmlDev 6 - Init COMPLETE
dss01:464:826 [0] NCCL INFO comm 0x7ffe98007590 rank 0 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE
dss01:464:464 [0] NCCL INFO Launch mode Parallel
0: Worker 0 is using worker seed: 3274953353
0: Building vocabulary from /data/vocab.bpe.32000
dss01:466:827 [2] NCCL INFO comm 0x7fff28007590 rank 2 nranks 8 cudaDev 2 nvmlDev 2 - Init COMPLETE
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1570130599.046 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1570130601.961 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1570130601.961 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1570130601.962 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1570130602.999 global_batch_size: {"value": 2048, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1570130603.001 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1570130603.001 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1570130603.001 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1570130603.002 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1570130603.002 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1570130603.002 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1570130603.002 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1570130603.023 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1570130603.023 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 3050633345
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/1938]	Time 1.061 (1.061)	Data 7.72e-01 (7.72e-01)	Tok/s 15799 (15799)	Loss/tok 10.7807 (10.7807)	LR 2.000e-05
0: TRAIN [0][10/1938]	Time 0.214 (0.322)	Data 1.13e-04 (7.03e-02)	Tok/s 47504 (49431)	Loss/tok 9.7501 (10.3423)	LR 2.518e-05
0: TRAIN [0][20/1938]	Time 0.216 (0.292)	Data 1.22e-04 (3.69e-02)	Tok/s 47226 (52438)	Loss/tok 9.3776 (9.9611)	LR 3.170e-05
0: TRAIN [0][30/1938]	Time 0.277 (0.286)	Data 1.54e-04 (2.50e-02)	Tok/s 60757 (53825)	Loss/tok 9.1748 (9.7192)	LR 3.991e-05
0: TRAIN [0][40/1938]	Time 0.215 (0.282)	Data 1.40e-04 (1.90e-02)	Tok/s 46829 (54787)	Loss/tok 8.8343 (9.5392)	LR 5.024e-05
0: TRAIN [0][50/1938]	Time 0.278 (0.275)	Data 1.09e-04 (1.53e-02)	Tok/s 59806 (54643)	Loss/tok 8.6149 (9.3925)	LR 6.325e-05
0: TRAIN [0][60/1938]	Time 0.336 (0.270)	Data 1.28e-04 (1.28e-02)	Tok/s 69458 (54253)	Loss/tok 8.4732 (9.2887)	LR 7.962e-05
0: TRAIN [0][70/1938]	Time 0.338 (0.267)	Data 1.73e-04 (1.10e-02)	Tok/s 69431 (54286)	Loss/tok 8.4208 (9.1644)	LR 1.002e-04
0: TRAIN [0][80/1938]	Time 0.411 (0.266)	Data 1.42e-04 (9.67e-03)	Tok/s 72860 (54352)	Loss/tok 8.2434 (9.0373)	LR 1.262e-04
0: TRAIN [0][90/1938]	Time 0.409 (0.266)	Data 1.40e-04 (8.62e-03)	Tok/s 72624 (54589)	Loss/tok 8.2041 (8.9233)	LR 1.589e-04
0: TRAIN [0][100/1938]	Time 0.276 (0.266)	Data 1.02e-04 (7.78e-03)	Tok/s 60439 (54770)	Loss/tok 7.9977 (8.8286)	LR 2.000e-04
0: TRAIN [0][110/1938]	Time 0.277 (0.265)	Data 1.07e-04 (7.09e-03)	Tok/s 60250 (54855)	Loss/tok 7.9238 (8.7487)	LR 2.518e-04
0: TRAIN [0][120/1938]	Time 0.161 (0.263)	Data 1.38e-04 (6.51e-03)	Tok/s 32089 (54625)	Loss/tok 7.0385 (8.6828)	LR 3.170e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][130/1938]	Time 0.215 (0.261)	Data 1.14e-04 (6.03e-03)	Tok/s 48437 (54337)	Loss/tok 7.6961 (8.6244)	LR 3.900e-04
0: TRAIN [0][140/1938]	Time 0.411 (0.262)	Data 1.36e-04 (5.61e-03)	Tok/s 72699 (54472)	Loss/tok 7.9911 (8.5667)	LR 4.909e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][150/1938]	Time 0.278 (0.259)	Data 1.13e-04 (5.24e-03)	Tok/s 61088 (54127)	Loss/tok 7.8259 (8.5204)	LR 6.040e-04
0: TRAIN [0][160/1938]	Time 0.215 (0.259)	Data 3.73e-04 (4.93e-03)	Tok/s 48967 (54045)	Loss/tok 7.4558 (8.4714)	LR 7.604e-04
0: TRAIN [0][170/1938]	Time 0.215 (0.260)	Data 1.18e-04 (4.65e-03)	Tok/s 47367 (54351)	Loss/tok 7.3044 (8.4159)	LR 9.573e-04
0: TRAIN [0][180/1938]	Time 0.276 (0.261)	Data 1.21e-04 (4.40e-03)	Tok/s 61492 (54647)	Loss/tok 7.5166 (8.3613)	LR 1.205e-03
0: TRAIN [0][190/1938]	Time 0.215 (0.261)	Data 1.47e-04 (4.17e-03)	Tok/s 47840 (54715)	Loss/tok 7.0016 (8.3032)	LR 1.517e-03
0: TRAIN [0][200/1938]	Time 0.216 (0.261)	Data 1.44e-04 (3.97e-03)	Tok/s 48414 (54697)	Loss/tok 6.9163 (8.2489)	LR 1.910e-03
0: TRAIN [0][210/1938]	Time 0.278 (0.262)	Data 1.18e-04 (3.79e-03)	Tok/s 60075 (54937)	Loss/tok 6.9203 (8.1819)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.414 (0.263)	Data 1.08e-04 (3.62e-03)	Tok/s 72167 (55028)	Loss/tok 7.0740 (8.1189)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.338 (0.262)	Data 1.07e-04 (3.47e-03)	Tok/s 69200 (54995)	Loss/tok 6.9014 (8.0608)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.278 (0.262)	Data 1.31e-04 (3.33e-03)	Tok/s 60097 (55012)	Loss/tok 6.5768 (8.0000)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.276 (0.262)	Data 1.15e-04 (3.21e-03)	Tok/s 60811 (55053)	Loss/tok 6.3754 (7.9368)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.276 (0.262)	Data 1.46e-04 (3.09e-03)	Tok/s 61260 (55108)	Loss/tok 6.2237 (7.8728)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.160 (0.261)	Data 1.12e-04 (2.98e-03)	Tok/s 32537 (54973)	Loss/tok 5.0214 (7.8177)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.277 (0.261)	Data 1.12e-04 (2.88e-03)	Tok/s 60734 (55059)	Loss/tok 6.0760 (7.7558)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.161 (0.260)	Data 1.49e-04 (2.78e-03)	Tok/s 32972 (54854)	Loss/tok 5.0338 (7.7043)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.216 (0.259)	Data 1.44e-04 (2.69e-03)	Tok/s 49424 (54750)	Loss/tok 5.6429 (7.6517)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.278 (0.259)	Data 1.75e-04 (2.61e-03)	Tok/s 60073 (54678)	Loss/tok 5.7703 (7.5962)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.338 (0.259)	Data 1.26e-04 (2.54e-03)	Tok/s 68573 (54730)	Loss/tok 5.8961 (7.5339)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.216 (0.258)	Data 1.40e-04 (2.46e-03)	Tok/s 47371 (54660)	Loss/tok 5.2962 (7.4799)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.217 (0.258)	Data 1.68e-04 (2.40e-03)	Tok/s 49111 (54680)	Loss/tok 5.2353 (7.4221)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.216 (0.257)	Data 1.70e-04 (2.33e-03)	Tok/s 47165 (54513)	Loss/tok 5.1315 (7.3740)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.410 (0.258)	Data 1.22e-04 (2.27e-03)	Tok/s 72879 (54634)	Loss/tok 5.9842 (7.3128)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.278 (0.257)	Data 1.40e-04 (2.21e-03)	Tok/s 60341 (54561)	Loss/tok 5.1711 (7.2626)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.279 (0.258)	Data 1.15e-04 (2.16e-03)	Tok/s 60520 (54719)	Loss/tok 5.0475 (7.1975)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.161 (0.257)	Data 1.25e-04 (2.11e-03)	Tok/s 32283 (54560)	Loss/tok 3.9588 (7.1510)	LR 2.000e-03
0: TRAIN [0][400/1938]	Time 0.215 (0.258)	Data 1.24e-04 (2.06e-03)	Tok/s 48573 (54629)	Loss/tok 4.7585 (7.0924)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.217 (0.257)	Data 1.30e-04 (2.01e-03)	Tok/s 46728 (54512)	Loss/tok 4.5414 (7.0471)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.160 (0.257)	Data 1.30e-04 (1.97e-03)	Tok/s 32813 (54429)	Loss/tok 3.8097 (6.9997)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.217 (0.257)	Data 1.44e-04 (1.93e-03)	Tok/s 47357 (54439)	Loss/tok 4.4867 (6.9484)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.216 (0.257)	Data 1.15e-04 (1.89e-03)	Tok/s 46078 (54438)	Loss/tok 4.4819 (6.8997)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.278 (0.256)	Data 1.26e-04 (1.85e-03)	Tok/s 61008 (54360)	Loss/tok 4.7585 (6.8548)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.340 (0.257)	Data 1.81e-04 (1.81e-03)	Tok/s 68737 (54370)	Loss/tok 4.9094 (6.8050)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.280 (0.257)	Data 1.33e-04 (1.78e-03)	Tok/s 60450 (54393)	Loss/tok 4.8927 (6.7574)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.341 (0.257)	Data 1.23e-04 (1.74e-03)	Tok/s 68789 (54394)	Loss/tok 4.9637 (6.7140)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.338 (0.257)	Data 1.19e-04 (1.71e-03)	Tok/s 68689 (54362)	Loss/tok 4.8264 (6.6723)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.217 (0.256)	Data 1.27e-04 (1.68e-03)	Tok/s 48656 (54344)	Loss/tok 4.1872 (6.6299)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.278 (0.257)	Data 1.36e-04 (1.65e-03)	Tok/s 60288 (54401)	Loss/tok 4.4587 (6.5844)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.216 (0.256)	Data 1.03e-04 (1.62e-03)	Tok/s 48010 (54282)	Loss/tok 4.1071 (6.5495)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.159 (0.256)	Data 1.39e-04 (1.59e-03)	Tok/s 33502 (54306)	Loss/tok 3.5389 (6.5063)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.280 (0.257)	Data 1.25e-04 (1.56e-03)	Tok/s 60252 (54365)	Loss/tok 4.4739 (6.4647)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][550/1938]	Time 0.217 (0.257)	Data 1.41e-04 (1.54e-03)	Tok/s 46429 (54349)	Loss/tok 4.1664 (6.4275)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.277 (0.257)	Data 1.11e-04 (1.51e-03)	Tok/s 60044 (54305)	Loss/tok 4.3315 (6.3917)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.216 (0.256)	Data 1.05e-04 (1.49e-03)	Tok/s 47746 (54251)	Loss/tok 3.8770 (6.3575)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.160 (0.256)	Data 1.22e-04 (1.46e-03)	Tok/s 32564 (54185)	Loss/tok 3.3259 (6.3249)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.217 (0.257)	Data 1.45e-04 (1.44e-03)	Tok/s 46676 (54251)	Loss/tok 3.9236 (6.2860)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.216 (0.256)	Data 1.47e-04 (1.42e-03)	Tok/s 47499 (54207)	Loss/tok 4.0920 (6.2550)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.277 (0.257)	Data 1.39e-04 (1.40e-03)	Tok/s 60436 (54269)	Loss/tok 4.2617 (6.2196)	LR 2.000e-03
0: TRAIN [0][620/1938]	Time 0.216 (0.257)	Data 1.32e-04 (1.38e-03)	Tok/s 48142 (54272)	Loss/tok 3.9462 (6.1872)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.215 (0.256)	Data 1.25e-04 (1.36e-03)	Tok/s 49241 (54163)	Loss/tok 3.9578 (6.1613)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.217 (0.256)	Data 1.03e-04 (1.34e-03)	Tok/s 46471 (54098)	Loss/tok 4.0236 (6.1337)	LR 2.000e-03
0: TRAIN [0][650/1938]	Time 0.217 (0.256)	Data 1.26e-04 (1.32e-03)	Tok/s 47310 (54121)	Loss/tok 3.8812 (6.1015)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.278 (0.256)	Data 1.63e-04 (1.30e-03)	Tok/s 60913 (54153)	Loss/tok 4.0966 (6.0702)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.279 (0.256)	Data 1.16e-04 (1.29e-03)	Tok/s 61787 (54124)	Loss/tok 4.0863 (6.0426)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.279 (0.256)	Data 1.06e-04 (1.27e-03)	Tok/s 60678 (54156)	Loss/tok 4.1789 (6.0126)	LR 2.000e-03
0: TRAIN [0][690/1938]	Time 0.280 (0.256)	Data 1.35e-04 (1.25e-03)	Tok/s 59670 (54125)	Loss/tok 4.1336 (5.9859)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.161 (0.256)	Data 1.08e-04 (1.24e-03)	Tok/s 33381 (54091)	Loss/tok 3.1693 (5.9600)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.279 (0.256)	Data 1.19e-04 (1.22e-03)	Tok/s 60399 (54134)	Loss/tok 4.0306 (5.9312)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.217 (0.256)	Data 1.14e-04 (1.21e-03)	Tok/s 47628 (54150)	Loss/tok 3.8327 (5.9041)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.218 (0.256)	Data 1.03e-04 (1.19e-03)	Tok/s 48126 (54142)	Loss/tok 3.6356 (5.8794)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.340 (0.257)	Data 1.48e-04 (1.18e-03)	Tok/s 68558 (54238)	Loss/tok 4.1455 (5.8490)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.216 (0.256)	Data 1.29e-04 (1.16e-03)	Tok/s 48491 (54203)	Loss/tok 3.6777 (5.8261)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.216 (0.256)	Data 1.41e-04 (1.15e-03)	Tok/s 47275 (54113)	Loss/tok 3.7552 (5.8064)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.413 (0.256)	Data 1.26e-04 (1.14e-03)	Tok/s 72216 (54121)	Loss/tok 4.4166 (5.7818)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.217 (0.256)	Data 1.03e-04 (1.12e-03)	Tok/s 48071 (54092)	Loss/tok 3.8096 (5.7603)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.278 (0.256)	Data 1.51e-04 (1.11e-03)	Tok/s 60898 (54084)	Loss/tok 4.0744 (5.7383)	LR 2.000e-03
0: TRAIN [0][800/1938]	Time 0.217 (0.256)	Data 1.41e-04 (1.10e-03)	Tok/s 46956 (54078)	Loss/tok 3.7095 (5.7162)	LR 2.000e-03
0: TRAIN [0][810/1938]	Time 0.279 (0.256)	Data 1.08e-04 (1.09e-03)	Tok/s 60830 (54058)	Loss/tok 3.8943 (5.6951)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.279 (0.255)	Data 1.02e-04 (1.08e-03)	Tok/s 60507 (53989)	Loss/tok 3.9366 (5.6766)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][830/1938]	Time 0.216 (0.256)	Data 1.05e-04 (1.06e-03)	Tok/s 47844 (54065)	Loss/tok 3.6210 (5.6521)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.216 (0.255)	Data 9.78e-05 (1.05e-03)	Tok/s 47955 (54037)	Loss/tok 3.6052 (5.6330)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.217 (0.255)	Data 1.20e-04 (1.04e-03)	Tok/s 47500 (54047)	Loss/tok 3.5519 (5.6124)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.278 (0.256)	Data 1.19e-04 (1.03e-03)	Tok/s 60977 (54053)	Loss/tok 3.9587 (5.5930)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.279 (0.255)	Data 1.38e-04 (1.02e-03)	Tok/s 59504 (54045)	Loss/tok 4.0055 (5.5741)	LR 2.000e-03
0: TRAIN [0][880/1938]	Time 0.216 (0.256)	Data 1.08e-04 (1.01e-03)	Tok/s 47741 (54058)	Loss/tok 3.6228 (5.5546)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.217 (0.255)	Data 1.06e-04 (1.00e-03)	Tok/s 47977 (54020)	Loss/tok 3.6876 (5.5375)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.341 (0.256)	Data 1.05e-04 (9.92e-04)	Tok/s 67361 (54084)	Loss/tok 4.3078 (5.5163)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.341 (0.256)	Data 1.10e-04 (9.83e-04)	Tok/s 68857 (54115)	Loss/tok 4.0997 (5.4971)	LR 2.000e-03
0: TRAIN [0][920/1938]	Time 0.216 (0.256)	Data 1.75e-04 (9.74e-04)	Tok/s 47648 (54096)	Loss/tok 3.5442 (5.4799)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.282 (0.256)	Data 1.24e-04 (9.65e-04)	Tok/s 58992 (54071)	Loss/tok 3.9503 (5.4636)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.217 (0.256)	Data 1.27e-04 (9.56e-04)	Tok/s 47416 (54038)	Loss/tok 3.6265 (5.4479)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.279 (0.256)	Data 1.32e-04 (9.48e-04)	Tok/s 60577 (54077)	Loss/tok 3.7436 (5.4293)	LR 2.000e-03
0: TRAIN [0][960/1938]	Time 0.217 (0.256)	Data 1.35e-04 (9.40e-04)	Tok/s 47610 (54033)	Loss/tok 3.5556 (5.4148)	LR 2.000e-03
0: TRAIN [0][970/1938]	Time 0.217 (0.256)	Data 1.39e-04 (9.31e-04)	Tok/s 47316 (54039)	Loss/tok 3.4901 (5.3983)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.216 (0.255)	Data 1.25e-04 (9.23e-04)	Tok/s 47873 (53984)	Loss/tok 3.5729 (5.3838)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.217 (0.256)	Data 1.18e-04 (9.15e-04)	Tok/s 48684 (54001)	Loss/tok 3.6504 (5.3672)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.277 (0.255)	Data 9.32e-05 (9.08e-04)	Tok/s 60869 (53915)	Loss/tok 3.9614 (5.3549)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.415 (0.255)	Data 1.31e-04 (9.00e-04)	Tok/s 72769 (53910)	Loss/tok 4.1992 (5.3399)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.340 (0.255)	Data 1.08e-04 (8.93e-04)	Tok/s 68738 (53964)	Loss/tok 3.9632 (5.3220)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.216 (0.256)	Data 1.26e-04 (8.85e-04)	Tok/s 48766 (53987)	Loss/tok 3.4596 (5.3060)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.342 (0.255)	Data 4.71e-04 (8.78e-04)	Tok/s 67989 (53967)	Loss/tok 4.0667 (5.2924)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.278 (0.256)	Data 1.40e-04 (8.71e-04)	Tok/s 60626 (54041)	Loss/tok 3.8424 (5.2761)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.217 (0.256)	Data 1.11e-04 (8.64e-04)	Tok/s 47615 (54070)	Loss/tok 3.4417 (5.2615)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.161 (0.256)	Data 1.25e-04 (8.57e-04)	Tok/s 32552 (54037)	Loss/tok 3.0860 (5.2490)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.217 (0.256)	Data 1.10e-04 (8.51e-04)	Tok/s 48519 (53998)	Loss/tok 3.6426 (5.2372)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1090/1938]	Time 0.160 (0.255)	Data 1.26e-04 (8.44e-04)	Tok/s 33883 (53908)	Loss/tok 3.1544 (5.2269)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.341 (0.256)	Data 1.47e-04 (8.38e-04)	Tok/s 68977 (53954)	Loss/tok 3.9629 (5.2118)	LR 2.000e-03
0: TRAIN [0][1110/1938]	Time 0.279 (0.256)	Data 1.30e-04 (8.32e-04)	Tok/s 60283 (53941)	Loss/tok 3.7498 (5.1994)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.217 (0.256)	Data 1.11e-04 (8.26e-04)	Tok/s 48742 (53975)	Loss/tok 3.4780 (5.1851)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.217 (0.256)	Data 4.17e-04 (8.20e-04)	Tok/s 47779 (53981)	Loss/tok 3.4866 (5.1722)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.218 (0.255)	Data 1.06e-04 (8.14e-04)	Tok/s 47124 (53909)	Loss/tok 3.5050 (5.1620)	LR 2.000e-03
0: TRAIN [0][1150/1938]	Time 0.216 (0.255)	Data 1.58e-04 (8.08e-04)	Tok/s 47960 (53912)	Loss/tok 3.4584 (5.1496)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.279 (0.255)	Data 1.20e-04 (8.02e-04)	Tok/s 60224 (53903)	Loss/tok 3.6927 (5.1377)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.281 (0.255)	Data 1.23e-04 (7.97e-04)	Tok/s 59616 (53924)	Loss/tok 3.7977 (5.1249)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.279 (0.256)	Data 1.25e-04 (7.91e-04)	Tok/s 59829 (53957)	Loss/tok 3.8524 (5.1118)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.220 (0.256)	Data 1.25e-04 (7.86e-04)	Tok/s 46653 (53998)	Loss/tok 3.4282 (5.0984)	LR 2.000e-03
0: TRAIN [0][1200/1938]	Time 0.218 (0.256)	Data 1.21e-04 (7.81e-04)	Tok/s 47547 (54042)	Loss/tok 3.5289 (5.0855)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.219 (0.256)	Data 1.12e-04 (7.76e-04)	Tok/s 47456 (54071)	Loss/tok 3.4294 (5.0733)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1220/1938]	Time 0.277 (0.257)	Data 1.75e-04 (7.71e-04)	Tok/s 60483 (54076)	Loss/tok 3.6529 (5.0621)	LR 2.000e-03
0: TRAIN [0][1230/1938]	Time 0.217 (0.256)	Data 1.03e-04 (7.66e-04)	Tok/s 47473 (54000)	Loss/tok 3.4887 (5.0535)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.279 (0.256)	Data 1.06e-04 (7.61e-04)	Tok/s 60415 (53988)	Loss/tok 3.6876 (5.0433)	LR 2.000e-03
0: TRAIN [0][1250/1938]	Time 0.217 (0.256)	Data 1.85e-04 (7.56e-04)	Tok/s 46470 (54033)	Loss/tok 3.5346 (5.0309)	LR 2.000e-03
0: TRAIN [0][1260/1938]	Time 0.281 (0.256)	Data 1.30e-04 (7.51e-04)	Tok/s 60391 (54047)	Loss/tok 3.7755 (5.0200)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.415 (0.257)	Data 1.65e-04 (7.46e-04)	Tok/s 71444 (54070)	Loss/tok 4.0271 (5.0084)	LR 2.000e-03
0: TRAIN [0][1280/1938]	Time 0.218 (0.257)	Data 1.09e-04 (7.41e-04)	Tok/s 47248 (54058)	Loss/tok 3.5201 (4.9986)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.218 (0.257)	Data 1.01e-04 (7.36e-04)	Tok/s 46818 (54045)	Loss/tok 3.3926 (4.9888)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.163 (0.257)	Data 1.38e-04 (7.32e-04)	Tok/s 32111 (54057)	Loss/tok 2.9122 (4.9784)	LR 2.000e-03
0: TRAIN [0][1310/1938]	Time 0.279 (0.257)	Data 1.10e-04 (7.27e-04)	Tok/s 59206 (54057)	Loss/tok 3.7242 (4.9684)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.340 (0.257)	Data 2.53e-04 (7.22e-04)	Tok/s 69038 (54046)	Loss/tok 3.8669 (4.9587)	LR 2.000e-03
0: TRAIN [0][1330/1938]	Time 0.415 (0.256)	Data 1.23e-04 (7.18e-04)	Tok/s 71878 (54019)	Loss/tok 4.2031 (4.9500)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.218 (0.256)	Data 1.69e-04 (7.13e-04)	Tok/s 46744 (54000)	Loss/tok 3.4528 (4.9412)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.339 (0.256)	Data 1.51e-04 (7.09e-04)	Tok/s 68274 (54000)	Loss/tok 3.8285 (4.9316)	LR 2.000e-03
0: TRAIN [0][1360/1938]	Time 0.217 (0.256)	Data 1.24e-04 (7.05e-04)	Tok/s 46955 (54007)	Loss/tok 3.3251 (4.9220)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.217 (0.256)	Data 1.07e-04 (7.01e-04)	Tok/s 47208 (54012)	Loss/tok 3.5138 (4.9127)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1380/1938]	Time 0.217 (0.256)	Data 1.74e-04 (6.97e-04)	Tok/s 46876 (53983)	Loss/tok 3.3748 (4.9044)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.281 (0.256)	Data 1.44e-04 (6.93e-04)	Tok/s 59397 (53987)	Loss/tok 3.6137 (4.8951)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.278 (0.257)	Data 1.08e-04 (6.89e-04)	Tok/s 59938 (54047)	Loss/tok 3.6952 (4.8848)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.278 (0.257)	Data 1.24e-04 (6.85e-04)	Tok/s 60160 (54057)	Loss/tok 3.5979 (4.8754)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.279 (0.256)	Data 9.66e-05 (6.81e-04)	Tok/s 59818 (54034)	Loss/tok 3.6584 (4.8673)	LR 2.000e-03
0: TRAIN [0][1430/1938]	Time 0.159 (0.256)	Data 9.54e-05 (6.77e-04)	Tok/s 32372 (54026)	Loss/tok 2.7916 (4.8589)	LR 2.000e-03
0: TRAIN [0][1440/1938]	Time 0.277 (0.257)	Data 1.50e-04 (6.73e-04)	Tok/s 59560 (54051)	Loss/tok 3.7221 (4.8499)	LR 2.000e-03
0: TRAIN [0][1450/1938]	Time 0.217 (0.257)	Data 1.04e-04 (6.69e-04)	Tok/s 48013 (54051)	Loss/tok 3.3301 (4.8414)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.277 (0.257)	Data 8.87e-05 (6.65e-04)	Tok/s 60627 (54059)	Loss/tok 3.4905 (4.8327)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.217 (0.257)	Data 9.35e-05 (6.62e-04)	Tok/s 46897 (54079)	Loss/tok 3.3829 (4.8242)	LR 2.000e-03
0: TRAIN [0][1480/1938]	Time 0.160 (0.257)	Data 9.39e-05 (6.58e-04)	Tok/s 33612 (54081)	Loss/tok 2.9383 (4.8161)	LR 2.000e-03
0: TRAIN [0][1490/1938]	Time 0.340 (0.257)	Data 9.30e-05 (6.54e-04)	Tok/s 67768 (54077)	Loss/tok 3.9697 (4.8086)	LR 2.000e-03
0: TRAIN [0][1500/1938]	Time 0.278 (0.257)	Data 1.59e-04 (6.51e-04)	Tok/s 60653 (54083)	Loss/tok 3.5727 (4.8003)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.279 (0.257)	Data 6.57e-04 (6.47e-04)	Tok/s 59870 (54065)	Loss/tok 3.6716 (4.7933)	LR 2.000e-03
0: TRAIN [0][1520/1938]	Time 0.277 (0.257)	Data 1.07e-04 (6.44e-04)	Tok/s 60935 (54065)	Loss/tok 3.7218 (4.7853)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.341 (0.257)	Data 9.75e-05 (6.40e-04)	Tok/s 69144 (54071)	Loss/tok 3.8182 (4.7775)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.161 (0.257)	Data 1.29e-04 (6.37e-04)	Tok/s 32256 (54055)	Loss/tok 2.9293 (4.7702)	LR 2.000e-03
0: TRAIN [0][1550/1938]	Time 0.217 (0.256)	Data 1.44e-04 (6.34e-04)	Tok/s 47347 (54037)	Loss/tok 3.3057 (4.7632)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.279 (0.257)	Data 1.08e-04 (6.31e-04)	Tok/s 60273 (54072)	Loss/tok 3.6349 (4.7547)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.280 (0.257)	Data 1.22e-04 (6.28e-04)	Tok/s 60249 (54083)	Loss/tok 3.5860 (4.7470)	LR 2.000e-03
0: TRAIN [0][1580/1938]	Time 0.216 (0.256)	Data 9.44e-05 (6.24e-04)	Tok/s 47372 (54066)	Loss/tok 3.2602 (4.7401)	LR 2.000e-03
0: TRAIN [0][1590/1938]	Time 0.280 (0.257)	Data 9.20e-05 (6.21e-04)	Tok/s 60581 (54090)	Loss/tok 3.6182 (4.7323)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.413 (0.257)	Data 1.11e-04 (6.18e-04)	Tok/s 71774 (54125)	Loss/tok 3.9327 (4.7240)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.340 (0.257)	Data 1.02e-04 (6.15e-04)	Tok/s 69054 (54137)	Loss/tok 3.7080 (4.7168)	LR 2.000e-03
0: TRAIN [0][1620/1938]	Time 0.161 (0.257)	Data 1.71e-04 (6.12e-04)	Tok/s 33780 (54145)	Loss/tok 2.8164 (4.7096)	LR 2.000e-03
0: TRAIN [0][1630/1938]	Time 0.217 (0.257)	Data 1.50e-04 (6.09e-04)	Tok/s 47462 (54131)	Loss/tok 3.3675 (4.7031)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1640/1938]	Time 0.415 (0.257)	Data 1.24e-04 (6.06e-04)	Tok/s 72895 (54164)	Loss/tok 3.9776 (4.6956)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.279 (0.257)	Data 9.49e-05 (6.03e-04)	Tok/s 59698 (54143)	Loss/tok 3.5872 (4.6896)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.413 (0.257)	Data 1.13e-04 (6.00e-04)	Tok/s 72047 (54145)	Loss/tok 4.0443 (4.6832)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.217 (0.257)	Data 1.22e-04 (5.98e-04)	Tok/s 47811 (54168)	Loss/tok 3.2268 (4.6758)	LR 2.000e-03
0: TRAIN [0][1680/1938]	Time 0.277 (0.257)	Data 9.61e-05 (5.95e-04)	Tok/s 58971 (54199)	Loss/tok 3.6683 (4.6686)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.342 (0.257)	Data 1.05e-04 (5.92e-04)	Tok/s 68415 (54228)	Loss/tok 3.7944 (4.6614)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.217 (0.257)	Data 1.09e-04 (5.89e-04)	Tok/s 48291 (54195)	Loss/tok 3.2587 (4.6556)	LR 2.000e-03
0: TRAIN [0][1710/1938]	Time 0.341 (0.257)	Data 1.07e-04 (5.86e-04)	Tok/s 68520 (54177)	Loss/tok 3.7687 (4.6497)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.342 (0.257)	Data 2.36e-04 (5.84e-04)	Tok/s 68129 (54192)	Loss/tok 3.7995 (4.6430)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.415 (0.257)	Data 1.12e-04 (5.81e-04)	Tok/s 72003 (54178)	Loss/tok 3.9344 (4.6371)	LR 2.000e-03
0: TRAIN [0][1740/1938]	Time 0.217 (0.257)	Data 7.05e-04 (5.79e-04)	Tok/s 48325 (54180)	Loss/tok 3.2720 (4.6308)	LR 2.000e-03
0: TRAIN [0][1750/1938]	Time 0.416 (0.257)	Data 1.11e-04 (5.77e-04)	Tok/s 72297 (54194)	Loss/tok 3.8879 (4.6242)	LR 2.000e-03
0: TRAIN [0][1760/1938]	Time 0.216 (0.257)	Data 1.47e-04 (5.74e-04)	Tok/s 47634 (54173)	Loss/tok 3.4480 (4.6188)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.216 (0.257)	Data 9.51e-05 (5.71e-04)	Tok/s 47486 (54189)	Loss/tok 3.4199 (4.6129)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.217 (0.257)	Data 3.30e-04 (5.69e-04)	Tok/s 47399 (54186)	Loss/tok 3.3267 (4.6070)	LR 2.000e-03
0: TRAIN [0][1790/1938]	Time 0.278 (0.257)	Data 1.34e-04 (5.66e-04)	Tok/s 60685 (54203)	Loss/tok 3.4944 (4.6007)	LR 2.000e-03
0: TRAIN [0][1800/1938]	Time 0.216 (0.257)	Data 9.66e-05 (5.64e-04)	Tok/s 47867 (54178)	Loss/tok 3.3441 (4.5955)	LR 2.000e-03
0: TRAIN [0][1810/1938]	Time 0.217 (0.257)	Data 1.20e-04 (5.61e-04)	Tok/s 48278 (54176)	Loss/tok 3.3005 (4.5898)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.217 (0.257)	Data 1.09e-04 (5.59e-04)	Tok/s 47313 (54164)	Loss/tok 3.3778 (4.5843)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.280 (0.257)	Data 1.38e-04 (5.56e-04)	Tok/s 59358 (54187)	Loss/tok 3.5132 (4.5780)	LR 2.000e-03
0: TRAIN [0][1840/1938]	Time 0.216 (0.257)	Data 1.06e-04 (5.54e-04)	Tok/s 48367 (54176)	Loss/tok 3.2271 (4.5725)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.279 (0.258)	Data 9.54e-05 (5.52e-04)	Tok/s 60492 (54202)	Loss/tok 3.5151 (4.5663)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.219 (0.257)	Data 9.54e-05 (5.49e-04)	Tok/s 47760 (54193)	Loss/tok 3.2221 (4.5608)	LR 2.000e-03
0: TRAIN [0][1870/1938]	Time 0.341 (0.258)	Data 9.54e-05 (5.47e-04)	Tok/s 68499 (54192)	Loss/tok 3.8702 (4.5554)	LR 2.000e-03
0: TRAIN [0][1880/1938]	Time 0.161 (0.258)	Data 9.70e-05 (5.45e-04)	Tok/s 32298 (54213)	Loss/tok 2.9209 (4.5496)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1890/1938]	Time 0.217 (0.258)	Data 1.17e-04 (5.42e-04)	Tok/s 48141 (54228)	Loss/tok 3.3051 (4.5441)	LR 2.000e-03
0: TRAIN [0][1900/1938]	Time 0.217 (0.258)	Data 1.10e-04 (5.41e-04)	Tok/s 47880 (54225)	Loss/tok 3.2509 (4.5388)	LR 2.000e-03
0: TRAIN [0][1910/1938]	Time 0.217 (0.258)	Data 9.68e-05 (5.38e-04)	Tok/s 47403 (54232)	Loss/tok 3.3583 (4.5331)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.216 (0.258)	Data 1.07e-04 (5.36e-04)	Tok/s 47928 (54217)	Loss/tok 3.3748 (4.5282)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.217 (0.258)	Data 1.10e-04 (5.34e-04)	Tok/s 49318 (54251)	Loss/tok 3.2155 (4.5223)	LR 2.000e-03
:::MLL 1570131103.371 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1570131103.371 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.761 (0.761)	Decoder iters 149.0 (149.0)	Tok/s 21473 (21473)
0: Running moses detokenizer
0: BLEU(score=20.089691429279373, counts=[34128, 15722, 8394, 4699], totals=[64071, 61068, 58066, 55068], precisions=[53.26590813316477, 25.745071068317287, 14.455963903144697, 8.533086365947556], bp=0.9906017922618643, sys_len=64071, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1570131105.368 eval_accuracy: {"value": 20.09, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1570131105.368 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 4.5193	Test BLEU: 20.09
0: Performance: Epoch: 0	Training: 433832 Tok/s
0: Finished epoch 0
:::MLL 1570131105.368 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1570131105.369 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1570131105.369 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 2778486657
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][0/1938]	Time 0.927 (0.927)	Data 6.96e-01 (6.96e-01)	Tok/s 11013 (11013)	Loss/tok 3.3616 (3.3616)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.280 (0.310)	Data 1.08e-04 (6.34e-02)	Tok/s 59762 (49709)	Loss/tok 3.4284 (3.4087)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.279 (0.280)	Data 1.19e-04 (3.33e-02)	Tok/s 60720 (51707)	Loss/tok 3.4789 (3.4013)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.216 (0.274)	Data 1.22e-04 (2.26e-02)	Tok/s 47972 (52900)	Loss/tok 3.1783 (3.4300)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.279 (0.268)	Data 1.57e-04 (1.71e-02)	Tok/s 60040 (53150)	Loss/tok 3.3798 (3.4199)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.217 (0.266)	Data 1.58e-04 (1.38e-02)	Tok/s 47482 (53440)	Loss/tok 3.3268 (3.4261)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.278 (0.264)	Data 1.24e-04 (1.15e-02)	Tok/s 60331 (53645)	Loss/tok 3.4927 (3.4318)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.278 (0.262)	Data 1.21e-04 (9.92e-03)	Tok/s 60074 (53606)	Loss/tok 3.5061 (3.4322)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.217 (0.262)	Data 1.29e-04 (8.71e-03)	Tok/s 47654 (53831)	Loss/tok 3.3014 (3.4349)	LR 2.000e-03
0: TRAIN [1][90/1938]	Time 0.217 (0.260)	Data 1.11e-04 (7.76e-03)	Tok/s 47302 (53803)	Loss/tok 3.1763 (3.4302)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.216 (0.261)	Data 1.00e-04 (7.01e-03)	Tok/s 47509 (53801)	Loss/tok 3.2206 (3.4342)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.414 (0.260)	Data 1.06e-04 (6.39e-03)	Tok/s 70433 (53753)	Loss/tok 3.9353 (3.4387)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.217 (0.260)	Data 9.92e-05 (5.87e-03)	Tok/s 48322 (53796)	Loss/tok 3.2051 (3.4390)	LR 2.000e-03
0: TRAIN [1][130/1938]	Time 0.217 (0.257)	Data 1.22e-04 (5.43e-03)	Tok/s 47584 (53345)	Loss/tok 3.2164 (3.4288)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.217 (0.257)	Data 1.04e-04 (5.05e-03)	Tok/s 48091 (53271)	Loss/tok 3.1593 (3.4336)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.216 (0.261)	Data 1.79e-04 (4.73e-03)	Tok/s 46938 (53902)	Loss/tok 3.2313 (3.4466)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.217 (0.260)	Data 9.94e-05 (4.44e-03)	Tok/s 48167 (53871)	Loss/tok 3.3099 (3.4486)	LR 2.000e-03
0: TRAIN [1][170/1938]	Time 0.342 (0.260)	Data 1.02e-04 (4.19e-03)	Tok/s 68767 (53917)	Loss/tok 3.6087 (3.4472)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.278 (0.261)	Data 1.03e-04 (3.96e-03)	Tok/s 60927 (54057)	Loss/tok 3.3342 (3.4504)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.162 (0.262)	Data 4.10e-04 (3.77e-03)	Tok/s 32762 (54213)	Loss/tok 2.7516 (3.4541)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.217 (0.261)	Data 1.10e-04 (3.59e-03)	Tok/s 46647 (54181)	Loss/tok 3.2178 (3.4547)	LR 2.000e-03
0: TRAIN [1][210/1938]	Time 0.218 (0.260)	Data 1.12e-04 (3.42e-03)	Tok/s 47550 (54002)	Loss/tok 3.2745 (3.4489)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.341 (0.260)	Data 1.02e-04 (3.27e-03)	Tok/s 68834 (53978)	Loss/tok 3.6112 (3.4491)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.281 (0.259)	Data 9.58e-05 (3.14e-03)	Tok/s 60203 (53981)	Loss/tok 3.4519 (3.4506)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.342 (0.261)	Data 1.18e-04 (3.01e-03)	Tok/s 68376 (54187)	Loss/tok 3.7301 (3.4603)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.217 (0.260)	Data 1.15e-04 (2.89e-03)	Tok/s 48110 (54007)	Loss/tok 3.3154 (3.4558)	LR 2.000e-03
0: TRAIN [1][260/1938]	Time 0.218 (0.259)	Data 9.89e-05 (2.79e-03)	Tok/s 47638 (53927)	Loss/tok 3.2755 (3.4557)	LR 2.000e-03
0: TRAIN [1][270/1938]	Time 0.217 (0.259)	Data 1.12e-04 (2.69e-03)	Tok/s 48036 (53943)	Loss/tok 3.1459 (3.4540)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.279 (0.259)	Data 1.01e-04 (2.60e-03)	Tok/s 60495 (54000)	Loss/tok 3.4530 (3.4535)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.216 (0.260)	Data 9.73e-05 (2.51e-03)	Tok/s 47801 (54107)	Loss/tok 3.1705 (3.4537)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.340 (0.259)	Data 9.94e-05 (2.43e-03)	Tok/s 68789 (53981)	Loss/tok 3.6729 (3.4515)	LR 2.000e-03
0: TRAIN [1][310/1938]	Time 0.216 (0.258)	Data 9.56e-05 (2.36e-03)	Tok/s 48325 (53936)	Loss/tok 3.2156 (3.4491)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.161 (0.258)	Data 9.37e-05 (2.29e-03)	Tok/s 33477 (53940)	Loss/tok 2.7611 (3.4487)	LR 2.000e-03
0: TRAIN [1][330/1938]	Time 0.342 (0.259)	Data 9.35e-05 (2.22e-03)	Tok/s 68335 (54072)	Loss/tok 3.6601 (3.4531)	LR 2.000e-03
0: TRAIN [1][340/1938]	Time 0.217 (0.259)	Data 9.18e-05 (2.16e-03)	Tok/s 48583 (54099)	Loss/tok 3.1985 (3.4519)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.341 (0.259)	Data 9.54e-05 (2.10e-03)	Tok/s 67334 (54170)	Loss/tok 3.7753 (3.4526)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.217 (0.260)	Data 9.32e-05 (2.05e-03)	Tok/s 47105 (54300)	Loss/tok 3.1927 (3.4570)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][370/1938]	Time 0.216 (0.261)	Data 9.49e-05 (1.99e-03)	Tok/s 48106 (54433)	Loss/tok 3.2425 (3.4620)	LR 2.000e-03
0: TRAIN [1][380/1938]	Time 0.218 (0.261)	Data 1.00e-04 (1.94e-03)	Tok/s 47196 (54439)	Loss/tok 3.1814 (3.4611)	LR 2.000e-03
0: TRAIN [1][390/1938]	Time 0.278 (0.261)	Data 9.94e-05 (1.90e-03)	Tok/s 59919 (54471)	Loss/tok 3.4225 (3.4590)	LR 2.000e-03
0: TRAIN [1][400/1938]	Time 0.217 (0.261)	Data 9.25e-05 (1.85e-03)	Tok/s 46420 (54367)	Loss/tok 3.3335 (3.4575)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.341 (0.260)	Data 9.32e-05 (1.81e-03)	Tok/s 68351 (54334)	Loss/tok 3.6836 (3.4561)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.279 (0.260)	Data 9.54e-05 (1.77e-03)	Tok/s 60082 (54251)	Loss/tok 3.4210 (3.4527)	LR 2.000e-03
0: TRAIN [1][430/1938]	Time 0.161 (0.259)	Data 1.02e-04 (1.73e-03)	Tok/s 33148 (54069)	Loss/tok 2.8171 (3.4493)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.216 (0.258)	Data 9.39e-05 (1.69e-03)	Tok/s 48264 (54003)	Loss/tok 3.1884 (3.4479)	LR 2.000e-03
0: TRAIN [1][450/1938]	Time 0.216 (0.258)	Data 9.56e-05 (1.66e-03)	Tok/s 48026 (53929)	Loss/tok 3.1734 (3.4461)	LR 2.000e-03
0: TRAIN [1][460/1938]	Time 0.341 (0.258)	Data 1.04e-04 (1.62e-03)	Tok/s 68406 (53969)	Loss/tok 3.5846 (3.4468)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.342 (0.258)	Data 9.89e-05 (1.59e-03)	Tok/s 68097 (54054)	Loss/tok 3.5710 (3.4468)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.342 (0.259)	Data 1.00e-04 (1.56e-03)	Tok/s 67807 (54119)	Loss/tok 3.7041 (3.4478)	LR 2.000e-03
0: TRAIN [1][490/1938]	Time 0.217 (0.258)	Data 9.87e-05 (1.53e-03)	Tok/s 48330 (54027)	Loss/tok 3.1488 (3.4468)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.160 (0.258)	Data 1.01e-04 (1.50e-03)	Tok/s 32197 (54019)	Loss/tok 2.8319 (3.4471)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.160 (0.258)	Data 1.12e-04 (1.47e-03)	Tok/s 33651 (53977)	Loss/tok 2.8342 (3.4454)	LR 2.000e-03
0: TRAIN [1][520/1938]	Time 0.217 (0.258)	Data 1.26e-04 (1.45e-03)	Tok/s 48108 (53990)	Loss/tok 3.0394 (3.4444)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.279 (0.258)	Data 1.02e-04 (1.42e-03)	Tok/s 61518 (54101)	Loss/tok 3.4918 (3.4461)	LR 2.000e-03
0: TRAIN [1][540/1938]	Time 0.217 (0.259)	Data 9.70e-05 (1.40e-03)	Tok/s 48636 (54134)	Loss/tok 3.2844 (3.4473)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.417 (0.259)	Data 9.89e-05 (1.38e-03)	Tok/s 71238 (54196)	Loss/tok 3.7245 (3.4485)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.278 (0.259)	Data 1.17e-04 (1.35e-03)	Tok/s 60856 (54234)	Loss/tok 3.3590 (3.4490)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.279 (0.259)	Data 1.28e-04 (1.33e-03)	Tok/s 60488 (54216)	Loss/tok 3.5041 (3.4479)	LR 2.000e-03
0: TRAIN [1][580/1938]	Time 0.217 (0.259)	Data 9.32e-05 (1.31e-03)	Tok/s 46239 (54291)	Loss/tok 3.2040 (3.4484)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.161 (0.259)	Data 1.02e-04 (1.29e-03)	Tok/s 32720 (54265)	Loss/tok 2.8125 (3.4474)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][600/1938]	Time 0.217 (0.260)	Data 1.01e-04 (1.27e-03)	Tok/s 47388 (54293)	Loss/tok 3.2025 (3.4481)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.217 (0.259)	Data 1.09e-04 (1.25e-03)	Tok/s 47225 (54300)	Loss/tok 3.1867 (3.4469)	LR 2.000e-03
0: TRAIN [1][620/1938]	Time 0.217 (0.259)	Data 8.65e-05 (1.23e-03)	Tok/s 48226 (54293)	Loss/tok 3.2438 (3.4463)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.280 (0.259)	Data 1.09e-04 (1.22e-03)	Tok/s 60107 (54304)	Loss/tok 3.3697 (3.4454)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.343 (0.260)	Data 9.68e-05 (1.20e-03)	Tok/s 67806 (54377)	Loss/tok 3.6729 (3.4477)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.216 (0.260)	Data 9.16e-05 (1.18e-03)	Tok/s 48499 (54423)	Loss/tok 3.1661 (3.4483)	LR 2.000e-03
0: TRAIN [1][660/1938]	Time 0.217 (0.260)	Data 9.42e-05 (1.16e-03)	Tok/s 47531 (54336)	Loss/tok 3.1683 (3.4461)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.217 (0.260)	Data 1.06e-04 (1.15e-03)	Tok/s 48457 (54428)	Loss/tok 3.1311 (3.4472)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.279 (0.260)	Data 1.42e-04 (1.13e-03)	Tok/s 60387 (54416)	Loss/tok 3.4658 (3.4471)	LR 2.000e-03
0: TRAIN [1][690/1938]	Time 0.217 (0.260)	Data 1.30e-04 (1.12e-03)	Tok/s 47548 (54403)	Loss/tok 3.2044 (3.4456)	LR 2.000e-03
0: TRAIN [1][700/1938]	Time 0.280 (0.260)	Data 9.63e-05 (1.10e-03)	Tok/s 60277 (54444)	Loss/tok 3.4685 (3.4454)	LR 2.000e-03
0: TRAIN [1][710/1938]	Time 0.217 (0.260)	Data 9.63e-05 (1.09e-03)	Tok/s 47000 (54427)	Loss/tok 3.2530 (3.4441)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.160 (0.260)	Data 9.73e-05 (1.08e-03)	Tok/s 33049 (54417)	Loss/tok 2.7205 (3.4441)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.217 (0.260)	Data 1.19e-04 (1.06e-03)	Tok/s 47830 (54425)	Loss/tok 3.1923 (3.4436)	LR 2.000e-03
0: TRAIN [1][740/1938]	Time 0.161 (0.260)	Data 9.01e-05 (1.05e-03)	Tok/s 33126 (54351)	Loss/tok 2.6791 (3.4422)	LR 2.000e-03
0: TRAIN [1][750/1938]	Time 0.281 (0.260)	Data 1.15e-04 (1.04e-03)	Tok/s 59558 (54348)	Loss/tok 3.4866 (3.4419)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.218 (0.259)	Data 9.82e-05 (1.03e-03)	Tok/s 47747 (54257)	Loss/tok 3.3251 (3.4399)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.217 (0.259)	Data 1.13e-04 (1.02e-03)	Tok/s 46945 (54242)	Loss/tok 3.1371 (3.4391)	LR 2.000e-03
0: TRAIN [1][780/1938]	Time 0.217 (0.259)	Data 1.01e-04 (1.00e-03)	Tok/s 48305 (54226)	Loss/tok 3.2712 (3.4385)	LR 2.000e-03
0: TRAIN [1][790/1938]	Time 0.278 (0.259)	Data 1.06e-04 (9.93e-04)	Tok/s 60898 (54227)	Loss/tok 3.3103 (3.4382)	LR 2.000e-03
0: TRAIN [1][800/1938]	Time 0.278 (0.259)	Data 1.06e-04 (9.82e-04)	Tok/s 59842 (54203)	Loss/tok 3.4167 (3.4376)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.281 (0.259)	Data 1.12e-04 (9.71e-04)	Tok/s 60367 (54206)	Loss/tok 3.4892 (3.4377)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.342 (0.259)	Data 1.08e-04 (9.61e-04)	Tok/s 68016 (54169)	Loss/tok 3.5311 (3.4368)	LR 2.000e-03
0: TRAIN [1][830/1938]	Time 0.281 (0.259)	Data 9.68e-05 (9.51e-04)	Tok/s 59085 (54184)	Loss/tok 3.3812 (3.4358)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.217 (0.259)	Data 9.63e-05 (9.42e-04)	Tok/s 47683 (54200)	Loss/tok 3.1830 (3.4348)	LR 2.000e-03
0: TRAIN [1][850/1938]	Time 0.280 (0.259)	Data 9.27e-05 (9.32e-04)	Tok/s 59275 (54229)	Loss/tok 3.3651 (3.4352)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.217 (0.259)	Data 9.63e-05 (9.22e-04)	Tok/s 47487 (54195)	Loss/tok 3.1817 (3.4338)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][870/1938]	Time 0.216 (0.259)	Data 9.37e-05 (9.13e-04)	Tok/s 47732 (54201)	Loss/tok 3.1582 (3.4329)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.217 (0.258)	Data 1.03e-04 (9.04e-04)	Tok/s 46815 (54185)	Loss/tok 3.1239 (3.4317)	LR 2.000e-03
0: TRAIN [1][890/1938]	Time 0.217 (0.258)	Data 1.16e-04 (8.95e-04)	Tok/s 48374 (54193)	Loss/tok 3.1745 (3.4305)	LR 2.000e-03
0: TRAIN [1][900/1938]	Time 0.161 (0.258)	Data 1.00e-04 (8.87e-04)	Tok/s 32498 (54131)	Loss/tok 2.7807 (3.4287)	LR 2.000e-03
0: TRAIN [1][910/1938]	Time 0.161 (0.258)	Data 9.66e-05 (8.78e-04)	Tok/s 32227 (54103)	Loss/tok 2.7273 (3.4280)	LR 2.000e-03
0: TRAIN [1][920/1938]	Time 0.216 (0.258)	Data 1.01e-04 (8.70e-04)	Tok/s 49130 (54088)	Loss/tok 3.1855 (3.4272)	LR 2.000e-03
0: TRAIN [1][930/1938]	Time 0.279 (0.258)	Data 8.89e-05 (8.62e-04)	Tok/s 59688 (54094)	Loss/tok 3.5603 (3.4267)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.343 (0.258)	Data 1.09e-04 (8.54e-04)	Tok/s 66485 (54117)	Loss/tok 3.6879 (3.4266)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.217 (0.258)	Data 1.09e-04 (8.46e-04)	Tok/s 49190 (54131)	Loss/tok 3.1173 (3.4262)	LR 2.000e-03
0: TRAIN [1][960/1938]	Time 0.218 (0.258)	Data 9.99e-05 (8.39e-04)	Tok/s 47412 (54107)	Loss/tok 3.1466 (3.4254)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.217 (0.257)	Data 1.15e-04 (8.32e-04)	Tok/s 47605 (54091)	Loss/tok 3.1848 (3.4249)	LR 2.000e-03
0: TRAIN [1][980/1938]	Time 0.281 (0.258)	Data 1.38e-04 (8.25e-04)	Tok/s 59450 (54119)	Loss/tok 3.4619 (3.4253)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.415 (0.258)	Data 1.36e-04 (8.17e-04)	Tok/s 72529 (54107)	Loss/tok 3.8104 (3.4257)	LR 2.000e-03
0: TRAIN [1][1000/1938]	Time 0.217 (0.258)	Data 1.19e-04 (8.10e-04)	Tok/s 47602 (54143)	Loss/tok 3.2413 (3.4261)	LR 2.000e-03
0: TRAIN [1][1010/1938]	Time 0.279 (0.257)	Data 1.17e-04 (8.04e-04)	Tok/s 60651 (54041)	Loss/tok 3.4066 (3.4245)	LR 2.000e-03
0: TRAIN [1][1020/1938]	Time 0.415 (0.258)	Data 9.94e-05 (7.97e-04)	Tok/s 71643 (54077)	Loss/tok 3.7562 (3.4254)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.216 (0.257)	Data 1.02e-04 (7.90e-04)	Tok/s 46992 (54014)	Loss/tok 3.2888 (3.4239)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1040/1938]	Time 0.342 (0.258)	Data 1.51e-04 (7.84e-04)	Tok/s 68075 (54044)	Loss/tok 3.5481 (3.4251)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.217 (0.258)	Data 1.35e-04 (7.78e-04)	Tok/s 47925 (54087)	Loss/tok 3.1253 (3.4259)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.216 (0.258)	Data 1.38e-04 (7.72e-04)	Tok/s 47402 (54042)	Loss/tok 3.1739 (3.4248)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.216 (0.257)	Data 1.35e-04 (7.66e-04)	Tok/s 48094 (54007)	Loss/tok 3.2608 (3.4235)	LR 2.000e-03
0: TRAIN [1][1080/1938]	Time 0.161 (0.257)	Data 1.13e-04 (7.60e-04)	Tok/s 32253 (53965)	Loss/tok 2.8017 (3.4223)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.218 (0.257)	Data 9.97e-05 (7.55e-04)	Tok/s 46268 (53970)	Loss/tok 3.3581 (3.4224)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.217 (0.257)	Data 1.27e-04 (7.49e-04)	Tok/s 47438 (53990)	Loss/tok 3.1395 (3.4217)	LR 2.000e-03
0: TRAIN [1][1110/1938]	Time 0.216 (0.257)	Data 1.32e-04 (7.44e-04)	Tok/s 48251 (53961)	Loss/tok 3.1190 (3.4215)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.217 (0.257)	Data 1.23e-04 (7.39e-04)	Tok/s 47875 (53948)	Loss/tok 3.1395 (3.4206)	LR 2.000e-03
0: TRAIN [1][1130/1938]	Time 0.161 (0.257)	Data 1.02e-04 (7.34e-04)	Tok/s 32359 (53936)	Loss/tok 2.7126 (3.4206)	LR 2.000e-03
0: TRAIN [1][1140/1938]	Time 0.217 (0.257)	Data 1.13e-04 (7.28e-04)	Tok/s 48061 (53942)	Loss/tok 3.1583 (3.4204)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.280 (0.257)	Data 1.06e-04 (7.23e-04)	Tok/s 60105 (53928)	Loss/tok 3.3800 (3.4190)	LR 2.000e-03
0: TRAIN [1][1160/1938]	Time 0.160 (0.257)	Data 1.00e-04 (7.17e-04)	Tok/s 33446 (53926)	Loss/tok 2.6968 (3.4188)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1170/1938]	Time 0.280 (0.257)	Data 1.63e-04 (7.12e-04)	Tok/s 60284 (53916)	Loss/tok 3.2492 (3.4187)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.217 (0.257)	Data 1.26e-04 (7.08e-04)	Tok/s 48408 (53881)	Loss/tok 3.1353 (3.4178)	LR 2.000e-03
0: TRAIN [1][1190/1938]	Time 0.216 (0.257)	Data 1.07e-04 (7.03e-04)	Tok/s 48392 (53865)	Loss/tok 3.0793 (3.4172)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.277 (0.257)	Data 1.06e-04 (6.98e-04)	Tok/s 60610 (53850)	Loss/tok 3.3474 (3.4163)	LR 2.000e-03
0: TRAIN [1][1210/1938]	Time 0.217 (0.257)	Data 1.08e-04 (6.94e-04)	Tok/s 47897 (53870)	Loss/tok 3.1577 (3.4163)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.277 (0.257)	Data 1.14e-04 (6.89e-04)	Tok/s 60642 (53852)	Loss/tok 3.3446 (3.4154)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1230/1938]	Time 0.277 (0.257)	Data 9.27e-05 (6.85e-04)	Tok/s 60487 (53861)	Loss/tok 3.3493 (3.4156)	LR 2.000e-03
0: TRAIN [1][1240/1938]	Time 0.161 (0.256)	Data 9.23e-05 (6.80e-04)	Tok/s 32758 (53796)	Loss/tok 2.6711 (3.4141)	LR 2.000e-03
0: TRAIN [1][1250/1938]	Time 0.217 (0.256)	Data 1.06e-04 (6.76e-04)	Tok/s 47042 (53800)	Loss/tok 3.1609 (3.4139)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.415 (0.257)	Data 1.04e-04 (6.72e-04)	Tok/s 71091 (53825)	Loss/tok 3.7789 (3.4141)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.217 (0.257)	Data 9.99e-05 (6.67e-04)	Tok/s 48795 (53857)	Loss/tok 3.1587 (3.4145)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.341 (0.257)	Data 1.24e-04 (6.63e-04)	Tok/s 67991 (53911)	Loss/tok 3.5209 (3.4153)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.216 (0.257)	Data 1.27e-04 (6.59e-04)	Tok/s 46871 (53891)	Loss/tok 3.0875 (3.4145)	LR 2.000e-03
0: TRAIN [1][1300/1938]	Time 0.280 (0.257)	Data 9.66e-05 (6.55e-04)	Tok/s 59900 (53909)	Loss/tok 3.4023 (3.4147)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.217 (0.257)	Data 1.44e-04 (6.51e-04)	Tok/s 47462 (53912)	Loss/tok 3.1943 (3.4141)	LR 2.000e-03
0: TRAIN [1][1320/1938]	Time 0.161 (0.257)	Data 1.23e-04 (6.47e-04)	Tok/s 32624 (53918)	Loss/tok 2.6219 (3.4142)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.217 (0.257)	Data 1.42e-04 (6.43e-04)	Tok/s 47037 (53927)	Loss/tok 3.2144 (3.4142)	LR 2.000e-03
0: TRAIN [1][1340/1938]	Time 0.217 (0.257)	Data 1.22e-04 (6.39e-04)	Tok/s 47369 (53923)	Loss/tok 3.1197 (3.4140)	LR 2.000e-03
0: TRAIN [1][1350/1938]	Time 0.341 (0.257)	Data 1.42e-04 (6.35e-04)	Tok/s 68564 (53935)	Loss/tok 3.4308 (3.4137)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.217 (0.257)	Data 1.23e-04 (6.31e-04)	Tok/s 48808 (53893)	Loss/tok 3.0670 (3.4126)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.218 (0.257)	Data 1.24e-04 (6.28e-04)	Tok/s 45589 (53861)	Loss/tok 3.0770 (3.4120)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.217 (0.257)	Data 9.73e-05 (6.24e-04)	Tok/s 46830 (53895)	Loss/tok 2.9557 (3.4120)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.281 (0.257)	Data 1.24e-04 (6.20e-04)	Tok/s 58707 (53870)	Loss/tok 3.4650 (3.4113)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.277 (0.257)	Data 9.85e-05 (6.17e-04)	Tok/s 60273 (53892)	Loss/tok 3.4066 (3.4109)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.277 (0.257)	Data 1.08e-04 (6.13e-04)	Tok/s 60624 (53906)	Loss/tok 3.4736 (3.4105)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.280 (0.257)	Data 1.53e-04 (6.10e-04)	Tok/s 60638 (53909)	Loss/tok 3.2488 (3.4103)	LR 2.000e-03
0: TRAIN [1][1430/1938]	Time 0.416 (0.257)	Data 1.14e-04 (6.07e-04)	Tok/s 71723 (53933)	Loss/tok 3.7235 (3.4101)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.216 (0.257)	Data 1.19e-04 (6.03e-04)	Tok/s 48071 (53912)	Loss/tok 3.2284 (3.4095)	LR 2.000e-03
0: TRAIN [1][1450/1938]	Time 0.217 (0.257)	Data 1.20e-04 (6.00e-04)	Tok/s 47975 (53878)	Loss/tok 3.1944 (3.4085)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.217 (0.257)	Data 1.40e-04 (5.97e-04)	Tok/s 47448 (53864)	Loss/tok 3.1961 (3.4077)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.217 (0.257)	Data 1.28e-04 (5.93e-04)	Tok/s 47609 (53845)	Loss/tok 3.2176 (3.4069)	LR 2.000e-03
0: TRAIN [1][1480/1938]	Time 0.342 (0.257)	Data 1.46e-04 (5.90e-04)	Tok/s 69845 (53842)	Loss/tok 3.5514 (3.4067)	LR 2.000e-03
0: TRAIN [1][1490/1938]	Time 0.218 (0.257)	Data 1.47e-04 (5.88e-04)	Tok/s 47410 (53860)	Loss/tok 3.0941 (3.4064)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1500/1938]	Time 0.280 (0.257)	Data 1.57e-04 (5.85e-04)	Tok/s 61205 (53865)	Loss/tok 3.4543 (3.4061)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.279 (0.257)	Data 9.35e-05 (5.82e-04)	Tok/s 60489 (53867)	Loss/tok 3.2848 (3.4055)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.342 (0.257)	Data 1.17e-04 (5.79e-04)	Tok/s 67384 (53879)	Loss/tok 3.6103 (3.4050)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.342 (0.257)	Data 1.30e-04 (5.76e-04)	Tok/s 68440 (53907)	Loss/tok 3.5337 (3.4048)	LR 2.000e-03
0: TRAIN [1][1540/1938]	Time 0.339 (0.257)	Data 1.18e-04 (5.73e-04)	Tok/s 69647 (53936)	Loss/tok 3.5161 (3.4048)	LR 2.000e-03
0: TRAIN [1][1550/1938]	Time 0.216 (0.257)	Data 1.20e-04 (5.70e-04)	Tok/s 46988 (53945)	Loss/tok 3.2483 (3.4041)	LR 2.000e-03
0: TRAIN [1][1560/1938]	Time 0.279 (0.257)	Data 1.05e-04 (5.67e-04)	Tok/s 59855 (53969)	Loss/tok 3.4406 (3.4044)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.217 (0.257)	Data 1.29e-04 (5.64e-04)	Tok/s 47157 (53971)	Loss/tok 3.1490 (3.4043)	LR 2.000e-03
0: TRAIN [1][1580/1938]	Time 0.278 (0.257)	Data 1.09e-04 (5.62e-04)	Tok/s 58906 (53992)	Loss/tok 3.4500 (3.4052)	LR 2.000e-03
0: TRAIN [1][1590/1938]	Time 0.412 (0.257)	Data 1.16e-04 (5.59e-04)	Tok/s 72961 (53986)	Loss/tok 3.7647 (3.4049)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.341 (0.257)	Data 1.52e-04 (5.56e-04)	Tok/s 68527 (54009)	Loss/tok 3.6099 (3.4048)	LR 2.000e-03
0: TRAIN [1][1610/1938]	Time 0.281 (0.258)	Data 1.31e-04 (5.53e-04)	Tok/s 59968 (54026)	Loss/tok 3.4080 (3.4051)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.280 (0.258)	Data 1.47e-04 (5.51e-04)	Tok/s 59805 (54036)	Loss/tok 3.3030 (3.4050)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.217 (0.258)	Data 9.85e-05 (5.48e-04)	Tok/s 47518 (54064)	Loss/tok 3.0653 (3.4052)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1640/1938]	Time 0.217 (0.258)	Data 1.49e-04 (5.45e-04)	Tok/s 47290 (54059)	Loss/tok 3.1640 (3.4050)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.162 (0.258)	Data 1.16e-04 (5.43e-04)	Tok/s 32393 (54031)	Loss/tok 2.6052 (3.4042)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.161 (0.258)	Data 1.02e-04 (5.40e-04)	Tok/s 32997 (54044)	Loss/tok 2.6516 (3.4042)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.216 (0.258)	Data 9.37e-05 (5.37e-04)	Tok/s 47586 (54047)	Loss/tok 3.1269 (3.4036)	LR 2.000e-03
0: TRAIN [1][1680/1938]	Time 0.279 (0.258)	Data 1.49e-04 (5.35e-04)	Tok/s 59626 (54010)	Loss/tok 3.4235 (3.4030)	LR 2.000e-03
0: TRAIN [1][1690/1938]	Time 0.416 (0.258)	Data 9.75e-05 (5.33e-04)	Tok/s 72195 (54042)	Loss/tok 3.7203 (3.4033)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.277 (0.258)	Data 1.20e-04 (5.31e-04)	Tok/s 60279 (54029)	Loss/tok 3.4307 (3.4026)	LR 2.000e-03
0: TRAIN [1][1710/1938]	Time 0.276 (0.258)	Data 1.27e-04 (5.28e-04)	Tok/s 60105 (54033)	Loss/tok 3.3760 (3.4028)	LR 2.000e-03
0: TRAIN [1][1720/1938]	Time 0.217 (0.258)	Data 9.66e-05 (5.26e-04)	Tok/s 48402 (54001)	Loss/tok 3.1843 (3.4017)	LR 2.000e-03
0: TRAIN [1][1730/1938]	Time 0.281 (0.258)	Data 1.31e-04 (5.24e-04)	Tok/s 59669 (53996)	Loss/tok 3.2113 (3.4015)	LR 2.000e-03
0: TRAIN [1][1740/1938]	Time 0.218 (0.258)	Data 9.68e-05 (5.22e-04)	Tok/s 47710 (54022)	Loss/tok 3.0552 (3.4017)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.280 (0.258)	Data 1.48e-04 (5.19e-04)	Tok/s 59960 (54047)	Loss/tok 3.3039 (3.4017)	LR 2.000e-03
0: TRAIN [1][1760/1938]	Time 0.218 (0.258)	Data 1.18e-04 (5.17e-04)	Tok/s 48327 (54052)	Loss/tok 3.1647 (3.4017)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.160 (0.258)	Data 1.41e-04 (5.15e-04)	Tok/s 32353 (54054)	Loss/tok 2.7968 (3.4021)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1780/1938]	Time 0.218 (0.258)	Data 1.03e-04 (5.13e-04)	Tok/s 46693 (54066)	Loss/tok 3.1076 (3.4020)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.217 (0.258)	Data 1.04e-04 (5.11e-04)	Tok/s 47075 (54073)	Loss/tok 3.1603 (3.4016)	LR 2.000e-03
0: TRAIN [1][1800/1938]	Time 0.217 (0.258)	Data 1.24e-04 (5.09e-04)	Tok/s 47833 (54107)	Loss/tok 3.1574 (3.4023)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.341 (0.258)	Data 9.63e-05 (5.06e-04)	Tok/s 68134 (54111)	Loss/tok 3.6481 (3.4022)	LR 2.000e-03
0: TRAIN [1][1820/1938]	Time 0.341 (0.258)	Data 9.63e-05 (5.04e-04)	Tok/s 68077 (54104)	Loss/tok 3.4710 (3.4019)	LR 2.000e-03
0: TRAIN [1][1830/1938]	Time 0.217 (0.258)	Data 9.25e-05 (5.02e-04)	Tok/s 46840 (54066)	Loss/tok 3.1486 (3.4010)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.217 (0.258)	Data 1.01e-04 (5.00e-04)	Tok/s 48306 (54075)	Loss/tok 3.1060 (3.4007)	LR 2.000e-03
0: TRAIN [1][1850/1938]	Time 0.340 (0.258)	Data 1.98e-04 (4.98e-04)	Tok/s 68659 (54069)	Loss/tok 3.5556 (3.4004)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.341 (0.258)	Data 1.31e-04 (4.96e-04)	Tok/s 68637 (54086)	Loss/tok 3.5061 (3.4003)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.216 (0.258)	Data 1.12e-04 (4.94e-04)	Tok/s 47767 (54093)	Loss/tok 3.0795 (3.3996)	LR 2.000e-03
0: TRAIN [1][1880/1938]	Time 0.217 (0.258)	Data 3.43e-04 (4.92e-04)	Tok/s 47252 (54078)	Loss/tok 3.0891 (3.3992)	LR 2.000e-03
0: TRAIN [1][1890/1938]	Time 0.415 (0.258)	Data 9.73e-05 (4.90e-04)	Tok/s 71393 (54087)	Loss/tok 3.8054 (3.3995)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.342 (0.258)	Data 1.38e-04 (4.88e-04)	Tok/s 68090 (54105)	Loss/tok 3.5750 (3.3991)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.282 (0.258)	Data 1.16e-04 (4.86e-04)	Tok/s 59789 (54106)	Loss/tok 3.2603 (3.3986)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.217 (0.258)	Data 1.26e-04 (4.85e-04)	Tok/s 46960 (54091)	Loss/tok 3.0387 (3.3980)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1930/1938]	Time 0.282 (0.258)	Data 1.29e-04 (4.83e-04)	Tok/s 59338 (54121)	Loss/tok 3.2939 (3.3982)	LR 2.000e-03
:::MLL 1570131606.791 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1570131606.792 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.749 (0.749)	Decoder iters 149.0 (149.0)	Tok/s 22252 (22252)
0: Running moses detokenizer
0: BLEU(score=21.89676776173702, counts=[36069, 17331, 9561, 5491], totals=[66063, 63060, 60057, 57058], precisions=[54.59788383815449, 27.483349191246432, 15.919876117688196, 9.62354095832311], bp=1.0, sys_len=66063, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1570131609.015 eval_accuracy: {"value": 21.9, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1570131609.016 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.3980	Test BLEU: 21.90
0: Performance: Epoch: 1	Training: 432911 Tok/s
0: Finished epoch 1
:::MLL 1570131609.016 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1570131609.016 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1570131609.017 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 3886653272
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][0/1938]	Time 0.935 (0.935)	Data 7.02e-01 (7.02e-01)	Tok/s 11100 (11100)	Loss/tok 3.0235 (3.0235)	LR 2.000e-03
0: TRAIN [2][10/1938]	Time 0.340 (0.322)	Data 1.02e-04 (6.40e-02)	Tok/s 68150 (50888)	Loss/tok 3.5052 (3.2525)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.216 (0.282)	Data 8.87e-05 (3.36e-02)	Tok/s 48326 (51001)	Loss/tok 3.1450 (3.2180)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.414 (0.288)	Data 1.60e-04 (2.28e-02)	Tok/s 72102 (53960)	Loss/tok 3.5252 (3.2668)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.162 (0.278)	Data 1.02e-04 (1.73e-02)	Tok/s 31876 (53390)	Loss/tok 2.5737 (3.2658)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.279 (0.268)	Data 1.30e-04 (1.39e-02)	Tok/s 60310 (52273)	Loss/tok 3.1642 (3.2407)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.217 (0.269)	Data 1.06e-04 (1.16e-02)	Tok/s 47841 (52942)	Loss/tok 3.0746 (3.2539)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.281 (0.267)	Data 9.25e-05 (1.00e-02)	Tok/s 60352 (53111)	Loss/tok 3.1667 (3.2468)	LR 2.000e-03
0: TRAIN [2][80/1938]	Time 0.217 (0.272)	Data 9.87e-05 (8.79e-03)	Tok/s 46114 (54115)	Loss/tok 3.0565 (3.2682)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.217 (0.268)	Data 1.03e-04 (7.84e-03)	Tok/s 48404 (53743)	Loss/tok 3.0000 (3.2593)	LR 2.000e-03
0: TRAIN [2][100/1938]	Time 0.217 (0.263)	Data 9.73e-05 (7.07e-03)	Tok/s 47549 (53169)	Loss/tok 3.1485 (3.2506)	LR 2.000e-03
0: TRAIN [2][110/1938]	Time 0.278 (0.263)	Data 9.54e-05 (6.45e-03)	Tok/s 60298 (53315)	Loss/tok 3.1483 (3.2551)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.161 (0.261)	Data 9.16e-05 (5.92e-03)	Tok/s 33069 (52918)	Loss/tok 2.6716 (3.2526)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.217 (0.261)	Data 1.06e-04 (5.48e-03)	Tok/s 46352 (53080)	Loss/tok 3.1375 (3.2587)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.217 (0.263)	Data 1.12e-04 (5.10e-03)	Tok/s 47813 (53456)	Loss/tok 3.0732 (3.2654)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.217 (0.262)	Data 9.85e-05 (4.77e-03)	Tok/s 47640 (53562)	Loss/tok 3.0324 (3.2633)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][160/1938]	Time 0.216 (0.263)	Data 1.14e-04 (4.48e-03)	Tok/s 48593 (53705)	Loss/tok 3.2132 (3.2716)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.278 (0.264)	Data 9.87e-05 (4.23e-03)	Tok/s 60020 (53919)	Loss/tok 3.2367 (3.2725)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.277 (0.264)	Data 1.09e-04 (4.00e-03)	Tok/s 61111 (54139)	Loss/tok 3.2448 (3.2749)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.280 (0.265)	Data 1.04e-04 (3.80e-03)	Tok/s 58151 (54217)	Loss/tok 3.2672 (3.2753)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.280 (0.263)	Data 1.24e-04 (3.62e-03)	Tok/s 59939 (53968)	Loss/tok 3.1392 (3.2697)	LR 2.000e-03
0: TRAIN [2][210/1938]	Time 0.343 (0.263)	Data 1.42e-04 (3.45e-03)	Tok/s 68427 (53930)	Loss/tok 3.5003 (3.2709)	LR 2.000e-03
0: TRAIN [2][220/1938]	Time 0.342 (0.264)	Data 1.46e-04 (3.30e-03)	Tok/s 69286 (54267)	Loss/tok 3.3176 (3.2717)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.342 (0.264)	Data 9.58e-05 (3.16e-03)	Tok/s 67682 (54422)	Loss/tok 3.4699 (3.2723)	LR 2.000e-03
0: TRAIN [2][240/1938]	Time 0.343 (0.264)	Data 9.78e-05 (3.03e-03)	Tok/s 68107 (54472)	Loss/tok 3.3312 (3.2701)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.216 (0.264)	Data 1.55e-04 (2.92e-03)	Tok/s 48120 (54400)	Loss/tok 3.0212 (3.2677)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.280 (0.264)	Data 1.16e-04 (2.81e-03)	Tok/s 61467 (54643)	Loss/tok 3.3362 (3.2698)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.279 (0.265)	Data 9.37e-05 (2.71e-03)	Tok/s 59378 (54767)	Loss/tok 3.3467 (3.2721)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.218 (0.265)	Data 1.96e-04 (2.62e-03)	Tok/s 47273 (54684)	Loss/tok 3.0531 (3.2727)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.279 (0.265)	Data 9.63e-05 (2.53e-03)	Tok/s 60555 (54656)	Loss/tok 3.1978 (3.2734)	LR 2.000e-03
0: TRAIN [2][300/1938]	Time 0.279 (0.264)	Data 1.01e-04 (2.45e-03)	Tok/s 59832 (54595)	Loss/tok 3.1836 (3.2717)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.216 (0.264)	Data 1.25e-04 (2.38e-03)	Tok/s 49117 (54651)	Loss/tok 2.9839 (3.2742)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.217 (0.264)	Data 9.63e-05 (2.31e-03)	Tok/s 48150 (54584)	Loss/tok 3.1136 (3.2759)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.218 (0.264)	Data 1.19e-04 (2.24e-03)	Tok/s 48096 (54603)	Loss/tok 3.0913 (3.2746)	LR 2.000e-03
0: TRAIN [2][340/1938]	Time 0.281 (0.263)	Data 9.61e-05 (2.18e-03)	Tok/s 60158 (54548)	Loss/tok 3.1823 (3.2724)	LR 2.000e-03
0: TRAIN [2][350/1938]	Time 0.341 (0.264)	Data 3.54e-04 (2.12e-03)	Tok/s 66854 (54597)	Loss/tok 3.5812 (3.2730)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][360/1938]	Time 0.281 (0.265)	Data 1.04e-04 (2.07e-03)	Tok/s 59030 (54721)	Loss/tok 3.2847 (3.2779)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.217 (0.263)	Data 1.06e-04 (2.01e-03)	Tok/s 48239 (54543)	Loss/tok 3.0357 (3.2746)	LR 2.000e-03
0: TRAIN [2][380/1938]	Time 0.218 (0.263)	Data 1.17e-04 (1.96e-03)	Tok/s 47807 (54496)	Loss/tok 2.9879 (3.2745)	LR 2.000e-03
0: TRAIN [2][390/1938]	Time 0.217 (0.262)	Data 1.09e-04 (1.92e-03)	Tok/s 47246 (54374)	Loss/tok 3.0874 (3.2722)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.341 (0.262)	Data 9.13e-05 (1.87e-03)	Tok/s 68687 (54288)	Loss/tok 3.3682 (3.2692)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.217 (0.262)	Data 1.01e-04 (1.83e-03)	Tok/s 47934 (54373)	Loss/tok 3.0940 (3.2706)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.341 (0.262)	Data 1.50e-04 (1.79e-03)	Tok/s 66853 (54369)	Loss/tok 3.4141 (3.2707)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.217 (0.262)	Data 1.13e-04 (1.75e-03)	Tok/s 47214 (54258)	Loss/tok 3.0997 (3.2688)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.278 (0.261)	Data 1.10e-04 (1.71e-03)	Tok/s 60899 (54247)	Loss/tok 3.2709 (3.2676)	LR 2.000e-03
0: TRAIN [2][450/1938]	Time 0.340 (0.261)	Data 1.11e-04 (1.68e-03)	Tok/s 68588 (54308)	Loss/tok 3.4883 (3.2697)	LR 2.000e-03
0: TRAIN [2][460/1938]	Time 0.217 (0.262)	Data 2.47e-04 (1.64e-03)	Tok/s 47350 (54347)	Loss/tok 3.0918 (3.2721)	LR 2.000e-03
0: TRAIN [2][470/1938]	Time 0.279 (0.262)	Data 1.05e-04 (1.61e-03)	Tok/s 61174 (54306)	Loss/tok 3.2142 (3.2711)	LR 2.000e-03
0: TRAIN [2][480/1938]	Time 0.341 (0.261)	Data 1.42e-04 (1.58e-03)	Tok/s 68708 (54224)	Loss/tok 3.5086 (3.2699)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.279 (0.261)	Data 1.52e-04 (1.55e-03)	Tok/s 60333 (54291)	Loss/tok 3.2688 (3.2714)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.216 (0.261)	Data 9.92e-05 (1.52e-03)	Tok/s 49408 (54249)	Loss/tok 3.0797 (3.2696)	LR 2.000e-03
0: TRAIN [2][510/1938]	Time 0.342 (0.261)	Data 1.75e-04 (1.50e-03)	Tok/s 68241 (54227)	Loss/tok 3.5416 (3.2704)	LR 2.000e-03
0: TRAIN [2][520/1938]	Time 0.218 (0.261)	Data 9.68e-05 (1.47e-03)	Tok/s 48061 (54172)	Loss/tok 3.0164 (3.2690)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.217 (0.260)	Data 1.30e-04 (1.45e-03)	Tok/s 47567 (54150)	Loss/tok 3.1345 (3.2681)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.161 (0.260)	Data 1.15e-04 (1.42e-03)	Tok/s 32669 (54094)	Loss/tok 2.5015 (3.2665)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.278 (0.260)	Data 1.83e-04 (1.40e-03)	Tok/s 59808 (54050)	Loss/tok 3.3416 (3.2655)	LR 2.000e-03
0: TRAIN [2][560/1938]	Time 0.278 (0.259)	Data 1.29e-04 (1.38e-03)	Tok/s 60459 (54011)	Loss/tok 3.1970 (3.2651)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.278 (0.259)	Data 1.40e-04 (1.35e-03)	Tok/s 60581 (53999)	Loss/tok 3.1812 (3.2637)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.278 (0.259)	Data 1.92e-04 (1.33e-03)	Tok/s 60313 (54028)	Loss/tok 3.3108 (3.2643)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.217 (0.260)	Data 1.17e-04 (1.31e-03)	Tok/s 46289 (54082)	Loss/tok 3.0219 (3.2644)	LR 2.000e-03
0: TRAIN [2][600/1938]	Time 0.217 (0.259)	Data 1.59e-04 (1.29e-03)	Tok/s 46690 (54080)	Loss/tok 2.9576 (3.2628)	LR 2.000e-03
0: TRAIN [2][610/1938]	Time 0.217 (0.259)	Data 1.23e-04 (1.27e-03)	Tok/s 48620 (54105)	Loss/tok 2.9806 (3.2623)	LR 2.000e-03
0: TRAIN [2][620/1938]	Time 0.161 (0.259)	Data 1.06e-04 (1.26e-03)	Tok/s 32572 (54074)	Loss/tok 2.7615 (3.2606)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][630/1938]	Time 0.342 (0.259)	Data 1.66e-04 (1.24e-03)	Tok/s 69108 (54052)	Loss/tok 3.4367 (3.2611)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.217 (0.259)	Data 1.73e-04 (1.22e-03)	Tok/s 48549 (54005)	Loss/tok 3.0391 (3.2608)	LR 2.000e-03
0: TRAIN [2][650/1938]	Time 0.342 (0.258)	Data 1.26e-04 (1.21e-03)	Tok/s 68711 (53922)	Loss/tok 3.4338 (3.2600)	LR 2.000e-03
0: TRAIN [2][660/1938]	Time 0.217 (0.259)	Data 2.84e-04 (1.19e-03)	Tok/s 48272 (53962)	Loss/tok 3.0858 (3.2611)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.216 (0.259)	Data 1.30e-04 (1.17e-03)	Tok/s 47218 (53989)	Loss/tok 3.1499 (3.2612)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.217 (0.259)	Data 1.45e-04 (1.16e-03)	Tok/s 47136 (53987)	Loss/tok 3.0650 (3.2618)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.161 (0.259)	Data 1.65e-04 (1.15e-03)	Tok/s 32594 (53996)	Loss/tok 2.7554 (3.2612)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.161 (0.258)	Data 1.75e-04 (1.13e-03)	Tok/s 32417 (53946)	Loss/tok 2.7479 (3.2598)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.217 (0.258)	Data 1.26e-04 (1.12e-03)	Tok/s 48230 (53940)	Loss/tok 3.1595 (3.2590)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.161 (0.258)	Data 1.20e-04 (1.11e-03)	Tok/s 32264 (53907)	Loss/tok 2.5850 (3.2590)	LR 2.000e-03
0: TRAIN [2][730/1938]	Time 0.342 (0.259)	Data 1.13e-04 (1.09e-03)	Tok/s 67565 (54010)	Loss/tok 3.4849 (3.2616)	LR 2.000e-03
0: TRAIN [2][740/1938]	Time 0.341 (0.259)	Data 1.23e-04 (1.08e-03)	Tok/s 68793 (54044)	Loss/tok 3.4378 (3.2615)	LR 2.000e-03
0: TRAIN [2][750/1938]	Time 0.218 (0.259)	Data 9.66e-05 (1.07e-03)	Tok/s 47399 (54048)	Loss/tok 3.1389 (3.2618)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.280 (0.259)	Data 1.55e-04 (1.06e-03)	Tok/s 60695 (54013)	Loss/tok 3.1540 (3.2605)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][770/1938]	Time 0.217 (0.259)	Data 1.39e-04 (1.04e-03)	Tok/s 47505 (54030)	Loss/tok 3.0215 (3.2600)	LR 2.000e-03
0: TRAIN [2][780/1938]	Time 0.216 (0.259)	Data 1.43e-04 (1.03e-03)	Tok/s 49073 (54033)	Loss/tok 3.0574 (3.2600)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.340 (0.259)	Data 1.70e-04 (1.02e-03)	Tok/s 69079 (54029)	Loss/tok 3.4713 (3.2603)	LR 2.000e-03
0: TRAIN [2][800/1938]	Time 0.161 (0.259)	Data 1.27e-04 (1.01e-03)	Tok/s 33553 (54043)	Loss/tok 2.7434 (3.2595)	LR 2.000e-03
0: TRAIN [2][810/1938]	Time 0.160 (0.258)	Data 9.20e-05 (9.99e-04)	Tok/s 32904 (53964)	Loss/tok 2.5897 (3.2579)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.343 (0.258)	Data 1.18e-04 (9.88e-04)	Tok/s 68696 (53971)	Loss/tok 3.4136 (3.2572)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.216 (0.258)	Data 1.16e-04 (9.78e-04)	Tok/s 46473 (53990)	Loss/tok 3.1464 (3.2581)	LR 2.000e-03
0: TRAIN [2][840/1938]	Time 0.278 (0.258)	Data 9.75e-05 (9.68e-04)	Tok/s 60695 (53992)	Loss/tok 3.2506 (3.2588)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.218 (0.259)	Data 1.52e-04 (9.58e-04)	Tok/s 46210 (54018)	Loss/tok 3.0967 (3.2600)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.217 (0.259)	Data 1.17e-04 (9.48e-04)	Tok/s 47139 (54005)	Loss/tok 3.0620 (3.2600)	LR 2.000e-03
0: TRAIN [2][870/1938]	Time 0.277 (0.259)	Data 1.02e-04 (9.39e-04)	Tok/s 60589 (54046)	Loss/tok 3.3724 (3.2612)	LR 2.000e-03
0: TRAIN [2][880/1938]	Time 0.279 (0.259)	Data 1.25e-04 (9.30e-04)	Tok/s 61092 (54012)	Loss/tok 3.2163 (3.2602)	LR 2.000e-03
0: TRAIN [2][890/1938]	Time 0.218 (0.259)	Data 1.37e-04 (9.21e-04)	Tok/s 47156 (54041)	Loss/tok 3.0016 (3.2605)	LR 2.000e-03
0: TRAIN [2][900/1938]	Time 0.342 (0.259)	Data 9.70e-05 (9.12e-04)	Tok/s 67440 (54077)	Loss/tok 3.3857 (3.2611)	LR 2.000e-03
0: TRAIN [2][910/1938]	Time 0.218 (0.259)	Data 1.61e-04 (9.03e-04)	Tok/s 48217 (54065)	Loss/tok 3.1726 (3.2610)	LR 2.000e-03
0: TRAIN [2][920/1938]	Time 0.277 (0.259)	Data 1.18e-04 (8.95e-04)	Tok/s 60566 (54093)	Loss/tok 3.3108 (3.2617)	LR 2.000e-03
0: TRAIN [2][930/1938]	Time 0.161 (0.259)	Data 1.20e-04 (8.87e-04)	Tok/s 33750 (54055)	Loss/tok 2.6496 (3.2611)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.216 (0.258)	Data 9.42e-05 (8.78e-04)	Tok/s 47529 (53982)	Loss/tok 3.0008 (3.2597)	LR 2.000e-03
0: TRAIN [2][950/1938]	Time 0.218 (0.258)	Data 8.94e-05 (8.70e-04)	Tok/s 47202 (54001)	Loss/tok 3.0118 (3.2598)	LR 2.000e-03
0: TRAIN [2][960/1938]	Time 0.217 (0.258)	Data 9.32e-05 (8.62e-04)	Tok/s 47547 (53968)	Loss/tok 2.9801 (3.2589)	LR 2.000e-03
0: TRAIN [2][970/1938]	Time 0.282 (0.258)	Data 9.39e-05 (8.55e-04)	Tok/s 59128 (53979)	Loss/tok 3.3143 (3.2583)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][980/1938]	Time 0.217 (0.258)	Data 1.60e-04 (8.48e-04)	Tok/s 47699 (53996)	Loss/tok 3.0124 (3.2584)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.280 (0.258)	Data 1.07e-04 (8.41e-04)	Tok/s 59338 (54015)	Loss/tok 3.3475 (3.2589)	LR 2.000e-03
0: TRAIN [2][1000/1938]	Time 0.414 (0.259)	Data 1.84e-04 (8.33e-04)	Tok/s 72157 (54075)	Loss/tok 3.6728 (3.2599)	LR 2.000e-03
0: TRAIN [2][1010/1938]	Time 0.217 (0.259)	Data 1.81e-04 (8.27e-04)	Tok/s 48291 (54072)	Loss/tok 3.0059 (3.2595)	LR 2.000e-03
0: TRAIN [2][1020/1938]	Time 0.216 (0.258)	Data 1.70e-04 (8.20e-04)	Tok/s 48531 (54032)	Loss/tok 3.0102 (3.2585)	LR 2.000e-03
0: TRAIN [2][1030/1938]	Time 0.217 (0.258)	Data 1.16e-04 (8.14e-04)	Tok/s 47580 (54022)	Loss/tok 3.0616 (3.2586)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][1040/1938]	Time 0.280 (0.258)	Data 1.60e-04 (8.08e-04)	Tok/s 59755 (54029)	Loss/tok 3.1768 (3.2588)	LR 2.000e-03
0: TRAIN [2][1050/1938]	Time 0.217 (0.258)	Data 1.39e-04 (8.01e-04)	Tok/s 47803 (54017)	Loss/tok 2.9755 (3.2585)	LR 2.000e-03
0: TRAIN [2][1060/1938]	Time 0.219 (0.258)	Data 1.09e-04 (7.95e-04)	Tok/s 48243 (53942)	Loss/tok 2.9963 (3.2571)	LR 2.000e-03
0: TRAIN [2][1070/1938]	Time 0.340 (0.258)	Data 1.81e-04 (7.89e-04)	Tok/s 68994 (53933)	Loss/tok 3.3789 (3.2565)	LR 2.000e-03
0: TRAIN [2][1080/1938]	Time 0.279 (0.258)	Data 1.87e-04 (7.83e-04)	Tok/s 59894 (53930)	Loss/tok 3.2690 (3.2563)	LR 2.000e-03
0: TRAIN [2][1090/1938]	Time 0.217 (0.258)	Data 1.09e-04 (7.77e-04)	Tok/s 48602 (53960)	Loss/tok 3.0233 (3.2572)	LR 2.000e-03
0: TRAIN [2][1100/1938]	Time 0.342 (0.258)	Data 9.70e-05 (7.70e-04)	Tok/s 67102 (54017)	Loss/tok 3.4235 (3.2593)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.279 (0.258)	Data 1.08e-04 (7.65e-04)	Tok/s 60428 (53993)	Loss/tok 3.2124 (3.2590)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.161 (0.258)	Data 9.44e-05 (7.59e-04)	Tok/s 31849 (54022)	Loss/tok 2.5732 (3.2589)	LR 2.000e-03
0: TRAIN [2][1130/1938]	Time 0.279 (0.259)	Data 1.28e-04 (7.54e-04)	Tok/s 60309 (54067)	Loss/tok 3.2415 (3.2597)	LR 2.000e-03
0: TRAIN [2][1140/1938]	Time 0.216 (0.259)	Data 1.78e-04 (7.49e-04)	Tok/s 46993 (54076)	Loss/tok 3.0947 (3.2599)	LR 2.000e-03
0: TRAIN [2][1150/1938]	Time 0.217 (0.259)	Data 1.27e-04 (7.44e-04)	Tok/s 47420 (54058)	Loss/tok 2.9609 (3.2597)	LR 2.000e-03
0: TRAIN [2][1160/1938]	Time 0.279 (0.258)	Data 1.17e-04 (7.39e-04)	Tok/s 60124 (54008)	Loss/tok 3.2336 (3.2589)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.218 (0.258)	Data 1.62e-04 (7.33e-04)	Tok/s 47135 (54007)	Loss/tok 3.1286 (3.2588)	LR 2.000e-03
0: TRAIN [2][1180/1938]	Time 0.344 (0.258)	Data 2.35e-04 (7.29e-04)	Tok/s 67788 (54029)	Loss/tok 3.4984 (3.2592)	LR 2.000e-03
0: TRAIN [2][1190/1938]	Time 0.280 (0.258)	Data 1.92e-04 (7.25e-04)	Tok/s 60205 (54031)	Loss/tok 3.2159 (3.2593)	LR 2.000e-03
0: TRAIN [2][1200/1938]	Time 0.217 (0.259)	Data 1.72e-04 (7.20e-04)	Tok/s 48293 (54051)	Loss/tok 2.9554 (3.2596)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.341 (0.259)	Data 1.23e-04 (7.16e-04)	Tok/s 68693 (54090)	Loss/tok 3.4344 (3.2603)	LR 2.000e-03
0: TRAIN [2][1220/1938]	Time 0.219 (0.259)	Data 2.04e-04 (7.11e-04)	Tok/s 48047 (54073)	Loss/tok 2.9884 (3.2600)	LR 2.000e-03
0: TRAIN [2][1230/1938]	Time 0.216 (0.258)	Data 1.19e-04 (7.07e-04)	Tok/s 48104 (54040)	Loss/tok 3.1349 (3.2593)	LR 2.000e-03
0: TRAIN [2][1240/1938]	Time 0.279 (0.258)	Data 1.85e-04 (7.03e-04)	Tok/s 60579 (54022)	Loss/tok 3.1628 (3.2592)	LR 2.000e-03
0: TRAIN [2][1250/1938]	Time 0.217 (0.259)	Data 1.29e-04 (6.98e-04)	Tok/s 47387 (54039)	Loss/tok 3.0667 (3.2596)	LR 2.000e-03
0: TRAIN [2][1260/1938]	Time 0.217 (0.258)	Data 1.38e-04 (6.94e-04)	Tok/s 47238 (54020)	Loss/tok 3.1038 (3.2592)	LR 2.000e-03
0: TRAIN [2][1270/1938]	Time 0.341 (0.258)	Data 1.48e-04 (6.90e-04)	Tok/s 68294 (54016)	Loss/tok 3.4602 (3.2594)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.277 (0.258)	Data 1.11e-04 (6.85e-04)	Tok/s 60785 (54022)	Loss/tok 3.1967 (3.2593)	LR 2.000e-03
0: TRAIN [2][1290/1938]	Time 0.280 (0.259)	Data 1.42e-04 (6.81e-04)	Tok/s 59348 (54045)	Loss/tok 3.2115 (3.2597)	LR 2.000e-03
0: TRAIN [2][1300/1938]	Time 0.218 (0.259)	Data 4.47e-04 (6.77e-04)	Tok/s 47557 (54067)	Loss/tok 3.1782 (3.2603)	LR 2.000e-03
0: TRAIN [2][1310/1938]	Time 0.279 (0.258)	Data 1.76e-04 (6.73e-04)	Tok/s 60050 (54018)	Loss/tok 3.2945 (3.2595)	LR 2.000e-03
0: TRAIN [2][1320/1938]	Time 0.415 (0.258)	Data 1.08e-04 (6.69e-04)	Tok/s 72474 (53972)	Loss/tok 3.5861 (3.2591)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1330/1938]	Time 0.162 (0.258)	Data 1.05e-04 (6.65e-04)	Tok/s 33081 (53956)	Loss/tok 2.5845 (3.2592)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.341 (0.258)	Data 1.19e-04 (6.62e-04)	Tok/s 68045 (53935)	Loss/tok 3.6155 (3.2588)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.280 (0.258)	Data 1.41e-04 (6.58e-04)	Tok/s 59546 (53903)	Loss/tok 3.2882 (3.2579)	LR 2.000e-03
0: TRAIN [2][1360/1938]	Time 0.217 (0.258)	Data 1.43e-04 (6.54e-04)	Tok/s 48201 (53885)	Loss/tok 2.9663 (3.2572)	LR 2.000e-03
0: TRAIN [2][1370/1938]	Time 0.161 (0.258)	Data 1.48e-04 (6.50e-04)	Tok/s 32801 (53869)	Loss/tok 2.5510 (3.2568)	LR 2.000e-03
0: TRAIN [2][1380/1938]	Time 0.219 (0.258)	Data 1.82e-04 (6.47e-04)	Tok/s 46256 (53883)	Loss/tok 3.0775 (3.2568)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.161 (0.257)	Data 1.12e-04 (6.43e-04)	Tok/s 32742 (53824)	Loss/tok 2.6681 (3.2558)	LR 2.000e-03
0: TRAIN [2][1400/1938]	Time 0.216 (0.257)	Data 1.19e-04 (6.40e-04)	Tok/s 47439 (53806)	Loss/tok 3.0885 (3.2558)	LR 2.000e-03
0: TRAIN [2][1410/1938]	Time 0.278 (0.258)	Data 1.35e-04 (6.36e-04)	Tok/s 59691 (53835)	Loss/tok 3.2763 (3.2568)	LR 2.000e-03
0: TRAIN [2][1420/1938]	Time 0.280 (0.258)	Data 1.85e-04 (6.32e-04)	Tok/s 59635 (53890)	Loss/tok 3.2605 (3.2578)	LR 2.000e-03
0: TRAIN [2][1430/1938]	Time 0.280 (0.258)	Data 1.85e-04 (6.29e-04)	Tok/s 59891 (53913)	Loss/tok 3.2178 (3.2583)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.278 (0.258)	Data 1.38e-04 (6.26e-04)	Tok/s 59649 (53932)	Loss/tok 3.2928 (3.2583)	LR 2.000e-03
0: TRAIN [2][1450/1938]	Time 0.218 (0.258)	Data 1.59e-04 (6.22e-04)	Tok/s 46721 (53907)	Loss/tok 3.1401 (3.2580)	LR 2.000e-03
0: TRAIN [2][1460/1938]	Time 0.279 (0.258)	Data 2.16e-04 (6.19e-04)	Tok/s 59935 (53893)	Loss/tok 3.3274 (3.2581)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.217 (0.258)	Data 1.14e-04 (6.16e-04)	Tok/s 47116 (53902)	Loss/tok 3.1516 (3.2585)	LR 2.000e-03
0: TRAIN [2][1480/1938]	Time 0.216 (0.258)	Data 1.13e-04 (6.12e-04)	Tok/s 47383 (53916)	Loss/tok 2.9911 (3.2580)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.216 (0.258)	Data 1.17e-04 (6.09e-04)	Tok/s 47670 (53944)	Loss/tok 3.1011 (3.2581)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1500/1938]	Time 0.161 (0.258)	Data 1.24e-04 (6.06e-04)	Tok/s 32581 (53915)	Loss/tok 2.6047 (3.2584)	LR 2.000e-03
0: TRAIN [2][1510/1938]	Time 0.280 (0.258)	Data 1.10e-04 (6.03e-04)	Tok/s 59808 (53940)	Loss/tok 3.2151 (3.2587)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.281 (0.258)	Data 1.30e-04 (5.99e-04)	Tok/s 60472 (53961)	Loss/tok 3.1890 (3.2597)	LR 2.000e-03
0: TRAIN [2][1530/1938]	Time 0.281 (0.258)	Data 1.37e-04 (5.97e-04)	Tok/s 60145 (53987)	Loss/tok 3.2415 (3.2601)	LR 2.000e-03
0: TRAIN [2][1540/1938]	Time 0.217 (0.259)	Data 2.03e-04 (5.94e-04)	Tok/s 47646 (54000)	Loss/tok 2.8856 (3.2606)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.339 (0.258)	Data 1.37e-04 (5.91e-04)	Tok/s 69735 (53989)	Loss/tok 3.3816 (3.2602)	LR 2.000e-03
0: TRAIN [2][1560/1938]	Time 0.341 (0.259)	Data 1.53e-04 (5.88e-04)	Tok/s 68920 (54015)	Loss/tok 3.4092 (3.2604)	LR 2.000e-03
0: TRAIN [2][1570/1938]	Time 0.218 (0.258)	Data 1.66e-04 (5.85e-04)	Tok/s 48992 (53970)	Loss/tok 3.0416 (3.2596)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.343 (0.258)	Data 1.15e-04 (5.82e-04)	Tok/s 68031 (53949)	Loss/tok 3.4390 (3.2590)	LR 2.000e-03
0: TRAIN [2][1590/1938]	Time 0.281 (0.258)	Data 1.44e-04 (5.80e-04)	Tok/s 60164 (53935)	Loss/tok 3.2076 (3.2581)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.416 (0.258)	Data 1.02e-04 (5.77e-04)	Tok/s 71680 (53929)	Loss/tok 3.6092 (3.2583)	LR 2.000e-03
0: TRAIN [2][1610/1938]	Time 0.217 (0.258)	Data 1.22e-04 (5.74e-04)	Tok/s 47312 (53922)	Loss/tok 3.1097 (3.2580)	LR 2.000e-03
0: TRAIN [2][1620/1938]	Time 0.412 (0.258)	Data 9.49e-05 (5.72e-04)	Tok/s 72941 (53929)	Loss/tok 3.5387 (3.2578)	LR 2.000e-03
0: TRAIN [2][1630/1938]	Time 0.217 (0.258)	Data 1.06e-04 (5.69e-04)	Tok/s 47471 (53909)	Loss/tok 3.0525 (3.2576)	LR 2.000e-03
0: TRAIN [2][1640/1938]	Time 0.278 (0.258)	Data 1.99e-04 (5.67e-04)	Tok/s 60737 (53910)	Loss/tok 3.2797 (3.2576)	LR 2.000e-03
0: TRAIN [2][1650/1938]	Time 0.277 (0.258)	Data 2.07e-04 (5.64e-04)	Tok/s 61156 (53915)	Loss/tok 3.2117 (3.2571)	LR 2.000e-03
0: TRAIN [2][1660/1938]	Time 0.281 (0.258)	Data 1.62e-04 (5.62e-04)	Tok/s 60831 (53913)	Loss/tok 3.2567 (3.2569)	LR 2.000e-03
0: TRAIN [2][1670/1938]	Time 0.341 (0.258)	Data 1.25e-04 (5.60e-04)	Tok/s 68430 (53926)	Loss/tok 3.5006 (3.2572)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.216 (0.258)	Data 1.37e-04 (5.58e-04)	Tok/s 47822 (53933)	Loss/tok 3.0213 (3.2570)	LR 2.000e-03
0: TRAIN [2][1690/1938]	Time 0.340 (0.258)	Data 1.75e-04 (5.56e-04)	Tok/s 68584 (53937)	Loss/tok 3.4611 (3.2567)	LR 2.000e-03
0: TRAIN [2][1700/1938]	Time 0.278 (0.258)	Data 3.06e-04 (5.53e-04)	Tok/s 61684 (53980)	Loss/tok 3.3143 (3.2577)	LR 2.000e-03
0: TRAIN [2][1710/1938]	Time 0.278 (0.258)	Data 1.33e-04 (5.51e-04)	Tok/s 59238 (53998)	Loss/tok 3.2559 (3.2575)	LR 2.000e-03
0: TRAIN [2][1720/1938]	Time 0.343 (0.258)	Data 1.86e-04 (5.49e-04)	Tok/s 68103 (54017)	Loss/tok 3.3704 (3.2573)	LR 2.000e-03
0: TRAIN [2][1730/1938]	Time 0.279 (0.258)	Data 1.61e-04 (5.47e-04)	Tok/s 60360 (54040)	Loss/tok 3.2217 (3.2573)	LR 2.000e-03
0: TRAIN [2][1740/1938]	Time 0.280 (0.258)	Data 1.57e-04 (5.45e-04)	Tok/s 60505 (54058)	Loss/tok 3.3016 (3.2576)	LR 2.000e-03
0: TRAIN [2][1750/1938]	Time 0.280 (0.259)	Data 1.06e-04 (5.43e-04)	Tok/s 59889 (54080)	Loss/tok 3.2005 (3.2577)	LR 2.000e-03
0: TRAIN [2][1760/1938]	Time 0.217 (0.258)	Data 1.46e-04 (5.41e-04)	Tok/s 47380 (54072)	Loss/tok 3.1051 (3.2573)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1770/1938]	Time 0.217 (0.258)	Data 1.46e-04 (5.38e-04)	Tok/s 46978 (54085)	Loss/tok 3.0724 (3.2571)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.218 (0.258)	Data 9.99e-05 (5.36e-04)	Tok/s 47147 (54070)	Loss/tok 3.0878 (3.2568)	LR 2.000e-03
0: TRAIN [2][1790/1938]	Time 0.342 (0.258)	Data 1.21e-04 (5.34e-04)	Tok/s 69055 (54065)	Loss/tok 3.3666 (3.2564)	LR 2.000e-03
0: TRAIN [2][1800/1938]	Time 0.217 (0.258)	Data 1.26e-04 (5.31e-04)	Tok/s 48354 (54051)	Loss/tok 3.0810 (3.2558)	LR 2.000e-03
0: TRAIN [2][1810/1938]	Time 0.341 (0.258)	Data 1.62e-04 (5.29e-04)	Tok/s 68946 (54055)	Loss/tok 3.3921 (3.2560)	LR 2.000e-03
0: TRAIN [2][1820/1938]	Time 0.216 (0.258)	Data 1.71e-04 (5.27e-04)	Tok/s 48107 (54038)	Loss/tok 3.1576 (3.2555)	LR 2.000e-03
0: TRAIN [2][1830/1938]	Time 0.217 (0.258)	Data 2.10e-04 (5.25e-04)	Tok/s 47113 (54038)	Loss/tok 3.0753 (3.2553)	LR 2.000e-03
0: TRAIN [2][1840/1938]	Time 0.280 (0.258)	Data 1.34e-04 (5.23e-04)	Tok/s 59787 (54033)	Loss/tok 3.3133 (3.2550)	LR 2.000e-03
0: TRAIN [2][1850/1938]	Time 0.216 (0.258)	Data 1.34e-04 (5.21e-04)	Tok/s 48415 (54030)	Loss/tok 3.0806 (3.2545)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.340 (0.258)	Data 9.58e-05 (5.19e-04)	Tok/s 68015 (54047)	Loss/tok 3.4366 (3.2548)	LR 2.000e-03
0: TRAIN [2][1870/1938]	Time 0.417 (0.258)	Data 9.32e-05 (5.17e-04)	Tok/s 72764 (54049)	Loss/tok 3.4792 (3.2547)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1880/1938]	Time 0.280 (0.258)	Data 1.12e-04 (5.15e-04)	Tok/s 60295 (54052)	Loss/tok 3.2120 (3.2545)	LR 2.000e-03
0: TRAIN [2][1890/1938]	Time 0.217 (0.258)	Data 1.01e-04 (5.13e-04)	Tok/s 48586 (54049)	Loss/tok 3.0201 (3.2544)	LR 2.000e-03
0: TRAIN [2][1900/1938]	Time 0.217 (0.258)	Data 1.00e-04 (5.11e-04)	Tok/s 47921 (54071)	Loss/tok 2.9394 (3.2550)	LR 2.000e-03
0: TRAIN [2][1910/1938]	Time 0.217 (0.258)	Data 3.53e-04 (5.09e-04)	Tok/s 47316 (54083)	Loss/tok 3.1026 (3.2554)	LR 2.000e-03
0: TRAIN [2][1920/1938]	Time 0.216 (0.258)	Data 1.80e-04 (5.07e-04)	Tok/s 47920 (54085)	Loss/tok 3.1712 (3.2555)	LR 2.000e-03
0: TRAIN [2][1930/1938]	Time 0.217 (0.258)	Data 9.68e-05 (5.05e-04)	Tok/s 47181 (54088)	Loss/tok 3.0360 (3.2552)	LR 2.000e-03
:::MLL 1570132110.738 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1570132110.739 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.700 (0.700)	Decoder iters 127.0 (127.0)	Tok/s 23587 (23587)
0: Running moses detokenizer
0: BLEU(score=22.73095422254263, counts=[36671, 17897, 10047, 5869], totals=[66297, 63294, 60292, 57295], precisions=[55.31321175920479, 28.27598192561696, 16.663902341935913, 10.243476743171307], bp=1.0, sys_len=66297, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1570132112.662 eval_accuracy: {"value": 22.73, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1570132112.663 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.2577	Test BLEU: 22.73
0: Performance: Epoch: 2	Training: 432711 Tok/s
0: Finished epoch 2
:::MLL 1570132112.663 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1570132112.663 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1570132112.664 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 1772055535
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][0/1938]	Time 1.005 (1.005)	Data 6.82e-01 (6.82e-01)	Tok/s 16790 (16790)	Loss/tok 2.9981 (2.9981)	LR 2.000e-03
0: TRAIN [3][10/1938]	Time 0.341 (0.322)	Data 1.39e-04 (6.22e-02)	Tok/s 69098 (51580)	Loss/tok 3.3234 (3.1145)	LR 2.000e-03
0: TRAIN [3][20/1938]	Time 0.162 (0.288)	Data 1.21e-04 (3.27e-02)	Tok/s 31728 (51495)	Loss/tok 2.5862 (3.1286)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.161 (0.292)	Data 1.46e-04 (2.22e-02)	Tok/s 31833 (53850)	Loss/tok 2.7060 (3.1999)	LR 2.000e-03
0: TRAIN [3][40/1938]	Time 0.412 (0.287)	Data 1.44e-04 (1.68e-02)	Tok/s 72918 (54141)	Loss/tok 3.4415 (3.2040)	LR 2.000e-03
0: TRAIN [3][50/1938]	Time 0.278 (0.276)	Data 1.19e-04 (1.35e-02)	Tok/s 60591 (52978)	Loss/tok 3.1247 (3.1890)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.279 (0.274)	Data 1.47e-04 (1.13e-02)	Tok/s 60001 (53576)	Loss/tok 3.2514 (3.1874)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.280 (0.277)	Data 2.02e-04 (9.77e-03)	Tok/s 60588 (54370)	Loss/tok 3.0839 (3.1987)	LR 2.000e-03
0: TRAIN [3][80/1938]	Time 0.217 (0.273)	Data 1.24e-04 (8.58e-03)	Tok/s 47958 (54013)	Loss/tok 2.9845 (3.1902)	LR 2.000e-03
0: TRAIN [3][90/1938]	Time 0.217 (0.268)	Data 1.67e-04 (7.65e-03)	Tok/s 47536 (53521)	Loss/tok 2.9823 (3.1764)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.415 (0.272)	Data 1.33e-04 (6.91e-03)	Tok/s 71754 (54171)	Loss/tok 3.5357 (3.1913)	LR 2.000e-03
0: TRAIN [3][110/1938]	Time 0.280 (0.271)	Data 1.80e-04 (6.30e-03)	Tok/s 60429 (54346)	Loss/tok 3.1129 (3.1907)	LR 2.000e-03
0: TRAIN [3][120/1938]	Time 0.217 (0.269)	Data 1.78e-04 (5.79e-03)	Tok/s 46536 (54183)	Loss/tok 3.0848 (3.1876)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.218 (0.269)	Data 1.36e-04 (5.36e-03)	Tok/s 46625 (54254)	Loss/tok 3.0669 (3.1907)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.278 (0.270)	Data 1.28e-04 (4.99e-03)	Tok/s 61518 (54454)	Loss/tok 3.1366 (3.1958)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.160 (0.268)	Data 1.30e-04 (4.68e-03)	Tok/s 32740 (54106)	Loss/tok 2.5261 (3.1897)	LR 2.000e-03
0: TRAIN [3][160/1938]	Time 0.279 (0.267)	Data 1.41e-04 (4.39e-03)	Tok/s 59288 (54143)	Loss/tok 3.2409 (3.1898)	LR 2.000e-03
0: TRAIN [3][170/1938]	Time 0.216 (0.267)	Data 1.05e-04 (4.14e-03)	Tok/s 48053 (54135)	Loss/tok 2.9852 (3.1896)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][180/1938]	Time 0.212 (0.266)	Data 1.41e-04 (3.92e-03)	Tok/s 48733 (54193)	Loss/tok 3.0312 (3.1895)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.281 (0.265)	Data 1.31e-04 (3.72e-03)	Tok/s 59861 (54085)	Loss/tok 3.2633 (3.1870)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.279 (0.264)	Data 1.33e-04 (3.55e-03)	Tok/s 60663 (54016)	Loss/tok 3.1288 (3.1835)	LR 2.000e-03
0: TRAIN [3][210/1938]	Time 0.160 (0.263)	Data 1.22e-04 (3.38e-03)	Tok/s 33674 (53859)	Loss/tok 2.6257 (3.1799)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.217 (0.262)	Data 1.26e-04 (3.24e-03)	Tok/s 47147 (53843)	Loss/tok 2.9554 (3.1814)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.277 (0.262)	Data 1.50e-04 (3.10e-03)	Tok/s 60340 (53945)	Loss/tok 3.1239 (3.1827)	LR 2.000e-03
0: TRAIN [3][240/1938]	Time 0.279 (0.262)	Data 1.53e-04 (2.98e-03)	Tok/s 60230 (53974)	Loss/tok 3.2092 (3.1837)	LR 2.000e-03
0: TRAIN [3][250/1938]	Time 0.278 (0.263)	Data 4.91e-04 (2.87e-03)	Tok/s 59946 (54068)	Loss/tok 3.1758 (3.1854)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.413 (0.263)	Data 1.41e-04 (2.76e-03)	Tok/s 72839 (54165)	Loss/tok 3.4976 (3.1876)	LR 2.000e-03
0: TRAIN [3][270/1938]	Time 0.278 (0.263)	Data 1.27e-04 (2.67e-03)	Tok/s 59959 (54106)	Loss/tok 3.1385 (3.1839)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.281 (0.263)	Data 1.86e-04 (2.58e-03)	Tok/s 59896 (54133)	Loss/tok 3.2099 (3.1851)	LR 2.000e-03
0: TRAIN [3][290/1938]	Time 0.216 (0.262)	Data 1.52e-04 (2.49e-03)	Tok/s 48184 (54145)	Loss/tok 3.0485 (3.1852)	LR 2.000e-03
0: TRAIN [3][300/1938]	Time 0.161 (0.261)	Data 1.32e-04 (2.41e-03)	Tok/s 33067 (53863)	Loss/tok 2.5875 (3.1811)	LR 2.000e-03
0: TRAIN [3][310/1938]	Time 0.218 (0.261)	Data 1.10e-04 (2.34e-03)	Tok/s 47338 (53926)	Loss/tok 2.9327 (3.1810)	LR 2.000e-03
0: TRAIN [3][320/1938]	Time 0.162 (0.260)	Data 1.45e-04 (2.27e-03)	Tok/s 32731 (53784)	Loss/tok 2.4935 (3.1779)	LR 2.000e-03
0: TRAIN [3][330/1938]	Time 0.278 (0.261)	Data 1.08e-04 (2.21e-03)	Tok/s 61055 (53890)	Loss/tok 3.1834 (3.1795)	LR 2.000e-03
0: TRAIN [3][340/1938]	Time 0.217 (0.260)	Data 1.81e-04 (2.15e-03)	Tok/s 47597 (53773)	Loss/tok 3.0046 (3.1764)	LR 2.000e-03
0: TRAIN [3][350/1938]	Time 0.340 (0.260)	Data 1.93e-04 (2.09e-03)	Tok/s 68668 (53956)	Loss/tok 3.3262 (3.1785)	LR 2.000e-03
0: TRAIN [3][360/1938]	Time 0.341 (0.261)	Data 1.52e-04 (2.04e-03)	Tok/s 68713 (54092)	Loss/tok 3.3306 (3.1804)	LR 2.000e-03
0: TRAIN [3][370/1938]	Time 0.280 (0.262)	Data 1.41e-04 (1.99e-03)	Tok/s 59438 (54267)	Loss/tok 3.1379 (3.1802)	LR 2.000e-03
0: TRAIN [3][380/1938]	Time 0.281 (0.261)	Data 1.49e-04 (1.94e-03)	Tok/s 59692 (54180)	Loss/tok 3.1560 (3.1780)	LR 2.000e-03
0: TRAIN [3][390/1938]	Time 0.281 (0.262)	Data 1.06e-04 (1.89e-03)	Tok/s 59645 (54334)	Loss/tok 3.1585 (3.1804)	LR 2.000e-03
0: TRAIN [3][400/1938]	Time 0.217 (0.261)	Data 1.39e-04 (1.85e-03)	Tok/s 47005 (54238)	Loss/tok 3.0340 (3.1788)	LR 2.000e-03
0: TRAIN [3][410/1938]	Time 0.342 (0.261)	Data 1.14e-04 (1.81e-03)	Tok/s 68249 (54261)	Loss/tok 3.4013 (3.1796)	LR 2.000e-03
0: TRAIN [3][420/1938]	Time 0.341 (0.261)	Data 9.51e-05 (1.77e-03)	Tok/s 68982 (54110)	Loss/tok 3.3335 (3.1785)	LR 2.000e-03
0: TRAIN [3][430/1938]	Time 0.278 (0.261)	Data 1.18e-04 (1.73e-03)	Tok/s 60047 (54121)	Loss/tok 3.2879 (3.1768)	LR 2.000e-03
0: TRAIN [3][440/1938]	Time 0.160 (0.261)	Data 1.18e-04 (1.69e-03)	Tok/s 32399 (54068)	Loss/tok 2.4680 (3.1788)	LR 2.000e-03
0: TRAIN [3][450/1938]	Time 0.217 (0.261)	Data 1.12e-04 (1.66e-03)	Tok/s 47450 (54143)	Loss/tok 2.9524 (3.1789)	LR 2.000e-03
0: TRAIN [3][460/1938]	Time 0.415 (0.261)	Data 1.08e-04 (1.63e-03)	Tok/s 71471 (54192)	Loss/tok 3.4811 (3.1808)	LR 2.000e-03
0: TRAIN [3][470/1938]	Time 0.414 (0.261)	Data 1.76e-04 (1.60e-03)	Tok/s 70814 (54249)	Loss/tok 3.6165 (3.1823)	LR 2.000e-03
0: TRAIN [3][480/1938]	Time 0.218 (0.261)	Data 4.74e-04 (1.57e-03)	Tok/s 46457 (54190)	Loss/tok 3.0465 (3.1801)	LR 2.000e-03
0: TRAIN [3][490/1938]	Time 0.282 (0.261)	Data 1.52e-04 (1.54e-03)	Tok/s 60039 (54249)	Loss/tok 3.1511 (3.1802)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][500/1938]	Time 0.158 (0.262)	Data 1.34e-04 (1.51e-03)	Tok/s 32816 (54286)	Loss/tok 2.4894 (3.1838)	LR 2.000e-03
0: TRAIN [3][510/1938]	Time 0.216 (0.261)	Data 1.43e-04 (1.48e-03)	Tok/s 48188 (54293)	Loss/tok 2.9929 (3.1832)	LR 2.000e-03
0: TRAIN [3][520/1938]	Time 0.216 (0.262)	Data 1.49e-04 (1.46e-03)	Tok/s 47203 (54362)	Loss/tok 2.9437 (3.1828)	LR 2.000e-03
0: TRAIN [3][530/1938]	Time 0.218 (0.262)	Data 1.04e-04 (1.43e-03)	Tok/s 47261 (54359)	Loss/tok 2.9889 (3.1820)	LR 2.000e-03
0: TRAIN [3][540/1938]	Time 0.280 (0.262)	Data 1.32e-04 (1.41e-03)	Tok/s 60355 (54373)	Loss/tok 3.2475 (3.1824)	LR 2.000e-03
0: TRAIN [3][550/1938]	Time 0.217 (0.261)	Data 1.18e-04 (1.39e-03)	Tok/s 48098 (54386)	Loss/tok 3.0096 (3.1821)	LR 2.000e-03
0: TRAIN [3][560/1938]	Time 0.217 (0.261)	Data 1.53e-04 (1.36e-03)	Tok/s 46865 (54414)	Loss/tok 2.9613 (3.1818)	LR 2.000e-03
0: TRAIN [3][570/1938]	Time 0.217 (0.261)	Data 1.38e-04 (1.34e-03)	Tok/s 47700 (54390)	Loss/tok 3.0773 (3.1815)	LR 2.000e-03
0: TRAIN [3][580/1938]	Time 0.217 (0.261)	Data 1.45e-04 (1.32e-03)	Tok/s 47785 (54437)	Loss/tok 3.0489 (3.1828)	LR 2.000e-03
0: TRAIN [3][590/1938]	Time 0.340 (0.261)	Data 1.10e-04 (1.30e-03)	Tok/s 68116 (54409)	Loss/tok 3.3698 (3.1849)	LR 2.000e-03
0: TRAIN [3][600/1938]	Time 0.343 (0.261)	Data 1.17e-04 (1.28e-03)	Tok/s 68013 (54427)	Loss/tok 3.4878 (3.1852)	LR 2.000e-03
0: TRAIN [3][610/1938]	Time 0.280 (0.261)	Data 1.12e-04 (1.27e-03)	Tok/s 59404 (54400)	Loss/tok 3.1769 (3.1844)	LR 2.000e-03
0: TRAIN [3][620/1938]	Time 0.279 (0.261)	Data 1.68e-04 (1.25e-03)	Tok/s 60032 (54385)	Loss/tok 3.1775 (3.1844)	LR 2.000e-03
0: TRAIN [3][630/1938]	Time 0.161 (0.261)	Data 1.03e-04 (1.23e-03)	Tok/s 34004 (54375)	Loss/tok 2.5947 (3.1841)	LR 2.000e-03
0: TRAIN [3][640/1938]	Time 0.216 (0.261)	Data 1.04e-04 (1.21e-03)	Tok/s 48212 (54351)	Loss/tok 2.9741 (3.1832)	LR 2.000e-03
0: TRAIN [3][650/1938]	Time 0.217 (0.261)	Data 1.86e-04 (1.20e-03)	Tok/s 48320 (54344)	Loss/tok 3.0862 (3.1837)	LR 2.000e-03
0: TRAIN [3][660/1938]	Time 0.280 (0.261)	Data 1.42e-04 (1.18e-03)	Tok/s 58899 (54383)	Loss/tok 3.1016 (3.1839)	LR 2.000e-03
0: TRAIN [3][670/1938]	Time 0.278 (0.261)	Data 1.01e-04 (1.17e-03)	Tok/s 60904 (54337)	Loss/tok 3.1699 (3.1835)	LR 2.000e-03
0: TRAIN [3][680/1938]	Time 0.415 (0.261)	Data 5.39e-04 (1.15e-03)	Tok/s 72471 (54318)	Loss/tok 3.3312 (3.1840)	LR 1.000e-03
0: TRAIN [3][690/1938]	Time 0.161 (0.260)	Data 1.60e-04 (1.14e-03)	Tok/s 33697 (54219)	Loss/tok 2.5960 (3.1824)	LR 1.000e-03
0: TRAIN [3][700/1938]	Time 0.280 (0.260)	Data 1.01e-04 (1.12e-03)	Tok/s 60248 (54218)	Loss/tok 3.1533 (3.1819)	LR 1.000e-03
0: TRAIN [3][710/1938]	Time 0.280 (0.260)	Data 1.31e-04 (1.11e-03)	Tok/s 59980 (54209)	Loss/tok 3.1411 (3.1816)	LR 1.000e-03
0: TRAIN [3][720/1938]	Time 0.217 (0.260)	Data 1.72e-04 (1.09e-03)	Tok/s 47357 (54168)	Loss/tok 2.9227 (3.1806)	LR 1.000e-03
0: TRAIN [3][730/1938]	Time 0.217 (0.260)	Data 1.37e-04 (1.08e-03)	Tok/s 48209 (54195)	Loss/tok 2.9877 (3.1809)	LR 1.000e-03
0: TRAIN [3][740/1938]	Time 0.281 (0.260)	Data 1.15e-04 (1.07e-03)	Tok/s 59190 (54230)	Loss/tok 3.1160 (3.1820)	LR 1.000e-03
0: TRAIN [3][750/1938]	Time 0.279 (0.260)	Data 1.57e-04 (1.06e-03)	Tok/s 60935 (54231)	Loss/tok 3.0922 (3.1815)	LR 1.000e-03
0: TRAIN [3][760/1938]	Time 0.341 (0.260)	Data 9.97e-05 (1.04e-03)	Tok/s 67745 (54266)	Loss/tok 3.3148 (3.1818)	LR 1.000e-03
0: TRAIN [3][770/1938]	Time 0.216 (0.261)	Data 1.41e-04 (1.03e-03)	Tok/s 48671 (54305)	Loss/tok 3.1498 (3.1819)	LR 1.000e-03
0: TRAIN [3][780/1938]	Time 0.217 (0.260)	Data 1.19e-04 (1.02e-03)	Tok/s 48328 (54231)	Loss/tok 2.9675 (3.1798)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][790/1938]	Time 0.341 (0.260)	Data 1.22e-04 (1.01e-03)	Tok/s 68121 (54234)	Loss/tok 3.4515 (3.1805)	LR 1.000e-03
0: TRAIN [3][800/1938]	Time 0.217 (0.260)	Data 1.17e-04 (9.98e-04)	Tok/s 47346 (54259)	Loss/tok 2.8752 (3.1802)	LR 1.000e-03
0: TRAIN [3][810/1938]	Time 0.341 (0.260)	Data 1.13e-04 (9.88e-04)	Tok/s 68091 (54255)	Loss/tok 3.2319 (3.1797)	LR 1.000e-03
0: TRAIN [3][820/1938]	Time 0.280 (0.260)	Data 1.16e-04 (9.77e-04)	Tok/s 59372 (54242)	Loss/tok 3.1745 (3.1788)	LR 1.000e-03
0: TRAIN [3][830/1938]	Time 0.280 (0.260)	Data 1.08e-04 (9.67e-04)	Tok/s 60703 (54272)	Loss/tok 3.0888 (3.1788)	LR 1.000e-03
0: TRAIN [3][840/1938]	Time 0.340 (0.260)	Data 1.18e-04 (9.57e-04)	Tok/s 68510 (54288)	Loss/tok 3.2830 (3.1783)	LR 1.000e-03
0: TRAIN [3][850/1938]	Time 0.218 (0.260)	Data 1.46e-04 (9.47e-04)	Tok/s 47437 (54287)	Loss/tok 2.8602 (3.1777)	LR 1.000e-03
0: TRAIN [3][860/1938]	Time 0.162 (0.260)	Data 1.26e-04 (9.38e-04)	Tok/s 32800 (54275)	Loss/tok 2.6324 (3.1774)	LR 1.000e-03
0: TRAIN [3][870/1938]	Time 0.217 (0.260)	Data 1.14e-04 (9.28e-04)	Tok/s 46470 (54265)	Loss/tok 3.0513 (3.1772)	LR 1.000e-03
0: TRAIN [3][880/1938]	Time 0.161 (0.260)	Data 1.12e-04 (9.19e-04)	Tok/s 32205 (54265)	Loss/tok 2.5336 (3.1773)	LR 1.000e-03
0: TRAIN [3][890/1938]	Time 0.218 (0.260)	Data 1.12e-04 (9.11e-04)	Tok/s 47343 (54299)	Loss/tok 2.8760 (3.1774)	LR 1.000e-03
0: TRAIN [3][900/1938]	Time 0.279 (0.260)	Data 1.64e-04 (9.02e-04)	Tok/s 60239 (54300)	Loss/tok 3.1513 (3.1765)	LR 1.000e-03
0: TRAIN [3][910/1938]	Time 0.281 (0.260)	Data 1.30e-04 (8.94e-04)	Tok/s 60900 (54314)	Loss/tok 3.1462 (3.1757)	LR 1.000e-03
0: TRAIN [3][920/1938]	Time 0.217 (0.260)	Data 1.18e-04 (8.86e-04)	Tok/s 46496 (54294)	Loss/tok 2.9702 (3.1745)	LR 1.000e-03
0: TRAIN [3][930/1938]	Time 0.279 (0.260)	Data 1.50e-04 (8.78e-04)	Tok/s 59478 (54298)	Loss/tok 3.1166 (3.1746)	LR 1.000e-03
0: TRAIN [3][940/1938]	Time 0.217 (0.260)	Data 1.26e-04 (8.70e-04)	Tok/s 48090 (54302)	Loss/tok 2.9091 (3.1738)	LR 1.000e-03
0: TRAIN [3][950/1938]	Time 0.341 (0.260)	Data 1.24e-04 (8.62e-04)	Tok/s 68895 (54300)	Loss/tok 3.2950 (3.1733)	LR 1.000e-03
0: TRAIN [3][960/1938]	Time 0.217 (0.260)	Data 1.07e-04 (8.54e-04)	Tok/s 46726 (54290)	Loss/tok 2.9716 (3.1729)	LR 1.000e-03
0: TRAIN [3][970/1938]	Time 0.218 (0.260)	Data 1.51e-04 (8.47e-04)	Tok/s 47100 (54292)	Loss/tok 2.9770 (3.1728)	LR 1.000e-03
0: TRAIN [3][980/1938]	Time 0.217 (0.260)	Data 1.80e-04 (8.40e-04)	Tok/s 47893 (54269)	Loss/tok 2.8140 (3.1718)	LR 1.000e-03
0: TRAIN [3][990/1938]	Time 0.218 (0.260)	Data 1.15e-04 (8.32e-04)	Tok/s 47826 (54313)	Loss/tok 2.8744 (3.1724)	LR 1.000e-03
0: TRAIN [3][1000/1938]	Time 0.216 (0.260)	Data 1.27e-04 (8.25e-04)	Tok/s 47751 (54316)	Loss/tok 2.9186 (3.1719)	LR 1.000e-03
0: TRAIN [3][1010/1938]	Time 0.216 (0.260)	Data 1.22e-04 (8.19e-04)	Tok/s 48352 (54249)	Loss/tok 3.0608 (3.1706)	LR 1.000e-03
0: TRAIN [3][1020/1938]	Time 0.217 (0.260)	Data 1.23e-04 (8.12e-04)	Tok/s 46805 (54260)	Loss/tok 2.9129 (3.1700)	LR 1.000e-03
0: TRAIN [3][1030/1938]	Time 0.280 (0.260)	Data 3.09e-04 (8.06e-04)	Tok/s 60226 (54270)	Loss/tok 3.0371 (3.1694)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1040/1938]	Time 0.342 (0.260)	Data 1.15e-04 (8.00e-04)	Tok/s 67636 (54287)	Loss/tok 3.3380 (3.1693)	LR 1.000e-03
0: TRAIN [3][1050/1938]	Time 0.218 (0.259)	Data 1.18e-04 (7.94e-04)	Tok/s 47410 (54237)	Loss/tok 2.9373 (3.1684)	LR 1.000e-03
0: TRAIN [3][1060/1938]	Time 0.218 (0.259)	Data 1.60e-04 (7.88e-04)	Tok/s 47520 (54221)	Loss/tok 2.9004 (3.1679)	LR 1.000e-03
0: TRAIN [3][1070/1938]	Time 0.217 (0.259)	Data 1.43e-04 (7.82e-04)	Tok/s 47108 (54184)	Loss/tok 2.8959 (3.1678)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1080/1938]	Time 0.282 (0.259)	Data 1.12e-04 (7.76e-04)	Tok/s 58707 (54214)	Loss/tok 3.1548 (3.1682)	LR 1.000e-03
0: TRAIN [3][1090/1938]	Time 0.280 (0.259)	Data 1.57e-04 (7.70e-04)	Tok/s 59954 (54213)	Loss/tok 3.0809 (3.1676)	LR 1.000e-03
0: TRAIN [3][1100/1938]	Time 0.341 (0.260)	Data 1.03e-04 (7.64e-04)	Tok/s 68503 (54261)	Loss/tok 3.2879 (3.1683)	LR 1.000e-03
0: TRAIN [3][1110/1938]	Time 0.342 (0.260)	Data 1.18e-04 (7.58e-04)	Tok/s 67800 (54289)	Loss/tok 3.4015 (3.1691)	LR 1.000e-03
0: TRAIN [3][1120/1938]	Time 0.342 (0.260)	Data 1.27e-04 (7.53e-04)	Tok/s 68767 (54269)	Loss/tok 3.3189 (3.1683)	LR 1.000e-03
0: TRAIN [3][1130/1938]	Time 0.217 (0.260)	Data 1.50e-04 (7.48e-04)	Tok/s 47808 (54280)	Loss/tok 2.9996 (3.1685)	LR 1.000e-03
0: TRAIN [3][1140/1938]	Time 0.280 (0.260)	Data 1.13e-04 (7.42e-04)	Tok/s 59530 (54265)	Loss/tok 3.1034 (3.1681)	LR 1.000e-03
0: TRAIN [3][1150/1938]	Time 0.343 (0.260)	Data 1.12e-04 (7.37e-04)	Tok/s 68052 (54263)	Loss/tok 3.3659 (3.1680)	LR 1.000e-03
0: TRAIN [3][1160/1938]	Time 0.217 (0.259)	Data 1.26e-04 (7.31e-04)	Tok/s 47696 (54198)	Loss/tok 2.9811 (3.1667)	LR 1.000e-03
0: TRAIN [3][1170/1938]	Time 0.280 (0.259)	Data 1.06e-04 (7.26e-04)	Tok/s 59763 (54203)	Loss/tok 3.0680 (3.1659)	LR 1.000e-03
0: TRAIN [3][1180/1938]	Time 0.341 (0.260)	Data 1.14e-04 (7.21e-04)	Tok/s 67899 (54240)	Loss/tok 3.3303 (3.1665)	LR 1.000e-03
0: TRAIN [3][1190/1938]	Time 0.217 (0.259)	Data 1.33e-04 (7.16e-04)	Tok/s 47591 (54192)	Loss/tok 3.0245 (3.1655)	LR 1.000e-03
0: TRAIN [3][1200/1938]	Time 0.218 (0.259)	Data 1.12e-04 (7.11e-04)	Tok/s 46890 (54188)	Loss/tok 2.9972 (3.1657)	LR 1.000e-03
0: TRAIN [3][1210/1938]	Time 0.280 (0.259)	Data 1.18e-04 (7.06e-04)	Tok/s 59703 (54187)	Loss/tok 3.1277 (3.1654)	LR 1.000e-03
0: TRAIN [3][1220/1938]	Time 0.280 (0.259)	Data 1.09e-04 (7.02e-04)	Tok/s 59878 (54229)	Loss/tok 3.2495 (3.1660)	LR 1.000e-03
0: TRAIN [3][1230/1938]	Time 0.217 (0.260)	Data 1.12e-04 (6.97e-04)	Tok/s 47577 (54251)	Loss/tok 3.0479 (3.1664)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1240/1938]	Time 0.217 (0.260)	Data 1.22e-04 (6.92e-04)	Tok/s 48249 (54283)	Loss/tok 3.0020 (3.1669)	LR 1.000e-03
0: TRAIN [3][1250/1938]	Time 0.280 (0.260)	Data 1.06e-04 (6.88e-04)	Tok/s 59560 (54285)	Loss/tok 3.1482 (3.1662)	LR 1.000e-03
0: TRAIN [3][1260/1938]	Time 0.279 (0.260)	Data 1.07e-04 (6.84e-04)	Tok/s 60937 (54277)	Loss/tok 3.0962 (3.1667)	LR 1.000e-03
0: TRAIN [3][1270/1938]	Time 0.162 (0.260)	Data 1.55e-04 (6.79e-04)	Tok/s 33589 (54277)	Loss/tok 2.5028 (3.1670)	LR 1.000e-03
0: TRAIN [3][1280/1938]	Time 0.414 (0.260)	Data 1.30e-04 (6.75e-04)	Tok/s 69765 (54266)	Loss/tok 3.6624 (3.1676)	LR 1.000e-03
0: TRAIN [3][1290/1938]	Time 0.217 (0.260)	Data 1.10e-04 (6.71e-04)	Tok/s 47552 (54274)	Loss/tok 3.0809 (3.1682)	LR 1.000e-03
0: TRAIN [3][1300/1938]	Time 0.340 (0.260)	Data 1.21e-04 (6.67e-04)	Tok/s 69465 (54313)	Loss/tok 3.2506 (3.1692)	LR 1.000e-03
0: TRAIN [3][1310/1938]	Time 0.280 (0.260)	Data 1.37e-04 (6.63e-04)	Tok/s 60158 (54321)	Loss/tok 3.2233 (3.1692)	LR 1.000e-03
0: TRAIN [3][1320/1938]	Time 0.414 (0.260)	Data 1.36e-04 (6.60e-04)	Tok/s 71176 (54333)	Loss/tok 3.4709 (3.1694)	LR 1.000e-03
0: TRAIN [3][1330/1938]	Time 0.341 (0.260)	Data 1.11e-04 (6.56e-04)	Tok/s 67899 (54340)	Loss/tok 3.3610 (3.1695)	LR 1.000e-03
0: TRAIN [3][1340/1938]	Time 0.217 (0.260)	Data 1.22e-04 (6.52e-04)	Tok/s 47501 (54340)	Loss/tok 2.9290 (3.1696)	LR 1.000e-03
0: TRAIN [3][1350/1938]	Time 0.280 (0.260)	Data 1.11e-04 (6.48e-04)	Tok/s 61440 (54342)	Loss/tok 3.1490 (3.1691)	LR 1.000e-03
0: TRAIN [3][1360/1938]	Time 0.279 (0.260)	Data 1.07e-04 (6.44e-04)	Tok/s 60284 (54362)	Loss/tok 3.2590 (3.1691)	LR 1.000e-03
0: TRAIN [3][1370/1938]	Time 0.160 (0.260)	Data 1.03e-04 (6.40e-04)	Tok/s 33561 (54353)	Loss/tok 2.6371 (3.1690)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1380/1938]	Time 0.279 (0.260)	Data 1.07e-04 (6.36e-04)	Tok/s 60242 (54321)	Loss/tok 3.3184 (3.1689)	LR 1.000e-03
0: TRAIN [3][1390/1938]	Time 0.218 (0.260)	Data 1.58e-04 (6.33e-04)	Tok/s 47324 (54318)	Loss/tok 2.9403 (3.1687)	LR 1.000e-03
0: TRAIN [3][1400/1938]	Time 0.217 (0.260)	Data 1.09e-04 (6.29e-04)	Tok/s 47374 (54298)	Loss/tok 2.9302 (3.1681)	LR 1.000e-03
0: TRAIN [3][1410/1938]	Time 0.341 (0.260)	Data 1.33e-04 (6.26e-04)	Tok/s 68633 (54297)	Loss/tok 3.2641 (3.1679)	LR 1.000e-03
0: TRAIN [3][1420/1938]	Time 0.217 (0.260)	Data 1.26e-04 (6.22e-04)	Tok/s 47859 (54285)	Loss/tok 2.9212 (3.1676)	LR 1.000e-03
0: TRAIN [3][1430/1938]	Time 0.162 (0.260)	Data 6.12e-04 (6.20e-04)	Tok/s 32545 (54288)	Loss/tok 2.5510 (3.1677)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][1440/1938]	Time 0.160 (0.260)	Data 1.15e-04 (6.17e-04)	Tok/s 33739 (54284)	Loss/tok 2.4967 (3.1679)	LR 1.000e-03
0: TRAIN [3][1450/1938]	Time 0.217 (0.260)	Data 1.19e-04 (6.13e-04)	Tok/s 46903 (54228)	Loss/tok 2.9712 (3.1668)	LR 1.000e-03
0: TRAIN [3][1460/1938]	Time 0.280 (0.260)	Data 1.38e-04 (6.10e-04)	Tok/s 58334 (54199)	Loss/tok 3.1665 (3.1661)	LR 1.000e-03
0: TRAIN [3][1470/1938]	Time 0.281 (0.259)	Data 1.26e-04 (6.07e-04)	Tok/s 59737 (54191)	Loss/tok 3.1822 (3.1657)	LR 1.000e-03
0: TRAIN [3][1480/1938]	Time 0.279 (0.260)	Data 1.42e-04 (6.03e-04)	Tok/s 59758 (54213)	Loss/tok 3.1447 (3.1661)	LR 1.000e-03
0: TRAIN [3][1490/1938]	Time 0.280 (0.260)	Data 1.13e-04 (6.00e-04)	Tok/s 58965 (54211)	Loss/tok 3.1661 (3.1659)	LR 5.000e-04
0: TRAIN [3][1500/1938]	Time 0.161 (0.259)	Data 1.50e-04 (5.97e-04)	Tok/s 32149 (54209)	Loss/tok 2.5850 (3.1656)	LR 5.000e-04
0: TRAIN [3][1510/1938]	Time 0.278 (0.259)	Data 1.15e-04 (5.94e-04)	Tok/s 59961 (54205)	Loss/tok 3.1856 (3.1651)	LR 5.000e-04
0: TRAIN [3][1520/1938]	Time 0.159 (0.259)	Data 1.04e-04 (5.91e-04)	Tok/s 33011 (54191)	Loss/tok 2.5744 (3.1647)	LR 5.000e-04
0: TRAIN [3][1530/1938]	Time 0.217 (0.259)	Data 2.25e-04 (5.88e-04)	Tok/s 48248 (54157)	Loss/tok 3.0527 (3.1640)	LR 5.000e-04
0: TRAIN [3][1540/1938]	Time 0.279 (0.259)	Data 1.51e-04 (5.85e-04)	Tok/s 59891 (54168)	Loss/tok 3.0845 (3.1636)	LR 5.000e-04
0: TRAIN [3][1550/1938]	Time 0.416 (0.259)	Data 1.27e-04 (5.82e-04)	Tok/s 70496 (54165)	Loss/tok 3.5112 (3.1641)	LR 5.000e-04
0: TRAIN [3][1560/1938]	Time 0.217 (0.259)	Data 1.58e-04 (5.79e-04)	Tok/s 46754 (54151)	Loss/tok 2.9101 (3.1637)	LR 5.000e-04
0: TRAIN [3][1570/1938]	Time 0.218 (0.259)	Data 1.20e-04 (5.76e-04)	Tok/s 47928 (54151)	Loss/tok 2.8329 (3.1634)	LR 5.000e-04
0: TRAIN [3][1580/1938]	Time 0.278 (0.259)	Data 1.36e-04 (5.74e-04)	Tok/s 60717 (54159)	Loss/tok 3.1385 (3.1629)	LR 5.000e-04
0: TRAIN [3][1590/1938]	Time 0.278 (0.259)	Data 1.52e-04 (5.71e-04)	Tok/s 60063 (54163)	Loss/tok 3.2276 (3.1624)	LR 5.000e-04
0: TRAIN [3][1600/1938]	Time 0.217 (0.259)	Data 2.28e-04 (5.68e-04)	Tok/s 47571 (54188)	Loss/tok 2.9583 (3.1627)	LR 5.000e-04
0: TRAIN [3][1610/1938]	Time 0.340 (0.259)	Data 1.43e-04 (5.66e-04)	Tok/s 67777 (54196)	Loss/tok 3.2165 (3.1626)	LR 5.000e-04
0: TRAIN [3][1620/1938]	Time 0.218 (0.259)	Data 1.34e-04 (5.63e-04)	Tok/s 47055 (54218)	Loss/tok 2.9607 (3.1624)	LR 5.000e-04
0: TRAIN [3][1630/1938]	Time 0.217 (0.259)	Data 1.57e-04 (5.61e-04)	Tok/s 47395 (54192)	Loss/tok 2.8812 (3.1618)	LR 5.000e-04
0: TRAIN [3][1640/1938]	Time 0.217 (0.259)	Data 1.24e-04 (5.58e-04)	Tok/s 47477 (54180)	Loss/tok 2.9349 (3.1613)	LR 5.000e-04
0: TRAIN [3][1650/1938]	Time 0.279 (0.259)	Data 1.96e-04 (5.55e-04)	Tok/s 60088 (54170)	Loss/tok 3.1521 (3.1607)	LR 5.000e-04
0: TRAIN [3][1660/1938]	Time 0.218 (0.259)	Data 1.67e-04 (5.53e-04)	Tok/s 47751 (54189)	Loss/tok 2.8900 (3.1607)	LR 5.000e-04
0: TRAIN [3][1670/1938]	Time 0.217 (0.259)	Data 1.57e-04 (5.51e-04)	Tok/s 47239 (54189)	Loss/tok 2.8082 (3.1601)	LR 5.000e-04
0: TRAIN [3][1680/1938]	Time 0.281 (0.259)	Data 1.08e-04 (5.48e-04)	Tok/s 60042 (54185)	Loss/tok 2.9536 (3.1594)	LR 5.000e-04
0: TRAIN [3][1690/1938]	Time 0.279 (0.259)	Data 1.02e-04 (5.46e-04)	Tok/s 60934 (54188)	Loss/tok 3.1575 (3.1593)	LR 5.000e-04
0: TRAIN [3][1700/1938]	Time 0.217 (0.259)	Data 1.51e-04 (5.44e-04)	Tok/s 47596 (54210)	Loss/tok 2.9308 (3.1593)	LR 5.000e-04
0: TRAIN [3][1710/1938]	Time 0.279 (0.259)	Data 1.41e-04 (5.42e-04)	Tok/s 59952 (54206)	Loss/tok 3.2432 (3.1589)	LR 5.000e-04
0: TRAIN [3][1720/1938]	Time 0.280 (0.259)	Data 1.11e-04 (5.39e-04)	Tok/s 59596 (54219)	Loss/tok 3.1858 (3.1584)	LR 5.000e-04
0: TRAIN [3][1730/1938]	Time 0.281 (0.259)	Data 1.36e-04 (5.37e-04)	Tok/s 60721 (54211)	Loss/tok 3.0187 (3.1578)	LR 5.000e-04
0: TRAIN [3][1740/1938]	Time 0.279 (0.259)	Data 2.41e-04 (5.35e-04)	Tok/s 59082 (54202)	Loss/tok 3.1852 (3.1574)	LR 5.000e-04
0: TRAIN [3][1750/1938]	Time 0.280 (0.259)	Data 1.17e-04 (5.33e-04)	Tok/s 60262 (54190)	Loss/tok 3.1591 (3.1568)	LR 5.000e-04
0: TRAIN [3][1760/1938]	Time 0.217 (0.259)	Data 1.04e-04 (5.30e-04)	Tok/s 47513 (54214)	Loss/tok 3.0347 (3.1571)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1770/1938]	Time 0.415 (0.259)	Data 1.17e-04 (5.28e-04)	Tok/s 71505 (54202)	Loss/tok 3.4013 (3.1569)	LR 5.000e-04
0: TRAIN [3][1780/1938]	Time 0.217 (0.259)	Data 1.70e-04 (5.26e-04)	Tok/s 48118 (54184)	Loss/tok 2.9158 (3.1565)	LR 5.000e-04
0: TRAIN [3][1790/1938]	Time 0.216 (0.259)	Data 1.53e-04 (5.24e-04)	Tok/s 48409 (54168)	Loss/tok 2.8422 (3.1561)	LR 5.000e-04
0: TRAIN [3][1800/1938]	Time 0.280 (0.259)	Data 1.45e-04 (5.22e-04)	Tok/s 60498 (54179)	Loss/tok 3.0666 (3.1560)	LR 5.000e-04
0: TRAIN [3][1810/1938]	Time 0.279 (0.259)	Data 1.55e-04 (5.20e-04)	Tok/s 59784 (54160)	Loss/tok 3.1650 (3.1554)	LR 5.000e-04
0: TRAIN [3][1820/1938]	Time 0.217 (0.259)	Data 1.60e-04 (5.18e-04)	Tok/s 47800 (54140)	Loss/tok 2.9709 (3.1550)	LR 5.000e-04
0: TRAIN [3][1830/1938]	Time 0.217 (0.259)	Data 1.76e-04 (5.16e-04)	Tok/s 47891 (54154)	Loss/tok 2.8046 (3.1548)	LR 5.000e-04
0: TRAIN [3][1840/1938]	Time 0.217 (0.259)	Data 1.79e-04 (5.14e-04)	Tok/s 47539 (54146)	Loss/tok 3.0575 (3.1542)	LR 5.000e-04
0: TRAIN [3][1850/1938]	Time 0.218 (0.259)	Data 1.16e-04 (5.12e-04)	Tok/s 47574 (54128)	Loss/tok 2.9452 (3.1536)	LR 5.000e-04
0: TRAIN [3][1860/1938]	Time 0.276 (0.259)	Data 1.02e-04 (5.10e-04)	Tok/s 60436 (54146)	Loss/tok 3.1475 (3.1538)	LR 5.000e-04
0: TRAIN [3][1870/1938]	Time 0.161 (0.259)	Data 1.05e-04 (5.08e-04)	Tok/s 33313 (54152)	Loss/tok 2.4994 (3.1537)	LR 5.000e-04
0: TRAIN [3][1880/1938]	Time 0.280 (0.259)	Data 1.61e-04 (5.06e-04)	Tok/s 60883 (54114)	Loss/tok 3.1010 (3.1530)	LR 5.000e-04
0: TRAIN [3][1890/1938]	Time 0.280 (0.259)	Data 1.11e-04 (5.04e-04)	Tok/s 60090 (54109)	Loss/tok 2.9882 (3.1530)	LR 5.000e-04
0: TRAIN [3][1900/1938]	Time 0.163 (0.259)	Data 1.66e-04 (5.02e-04)	Tok/s 31857 (54092)	Loss/tok 2.4712 (3.1528)	LR 5.000e-04
0: TRAIN [3][1910/1938]	Time 0.342 (0.259)	Data 9.75e-05 (5.00e-04)	Tok/s 67777 (54084)	Loss/tok 3.2384 (3.1528)	LR 5.000e-04
0: TRAIN [3][1920/1938]	Time 0.341 (0.259)	Data 1.41e-04 (4.98e-04)	Tok/s 67733 (54069)	Loss/tok 3.1791 (3.1521)	LR 5.000e-04
0: TRAIN [3][1930/1938]	Time 0.279 (0.259)	Data 9.97e-05 (4.96e-04)	Tok/s 59372 (54075)	Loss/tok 3.1135 (3.1518)	LR 5.000e-04
:::MLL 1570132614.413 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 524}}
:::MLL 1570132614.413 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.620 (0.620)	Decoder iters 93.0 (93.0)	Tok/s 26483 (26483)
0: Running moses detokenizer
0: BLEU(score=24.404121413052124, counts=[37069, 18691, 10747, 6427], totals=[65203, 62200, 59197, 56199], precisions=[56.85167860374523, 30.04983922829582, 18.15463621467304, 11.436146550650367], bp=1.0, sys_len=65203, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1570132616.241 eval_accuracy: {"value": 24.4, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 535}}
:::MLL 1570132616.242 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 3	Training Loss: 3.1496	Test BLEU: 24.40
0: Performance: Epoch: 3	Training: 432660 Tok/s
0: Finished epoch 3
:::MLL 1570132616.242 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 557}}
0: Closing preprocessed data file
:::MLL 1570132616.242 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 568}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-10-03 07:57:07 PM
RESULT,RNN_TRANSLATOR,,2053,nvidia,2019-10-03 07:22:54 PM
