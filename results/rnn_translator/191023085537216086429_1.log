Beginning trial 1 of 2
Gathering sys log on dss01
:::MLL 1571839041.666 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1571839041.667 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1571839041.667 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1571839041.668 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1571839041.668 submission_platform: {"value": "1xDSS8440", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1571839041.668 submission_entry: {"value": "{'hardware': 'DSS8440', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '5.0-0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz', 'num_cores': '40', 'num_vcpus': '40', 'accelerator': 'Tesla V100-PCIE-32GB', 'num_accelerators': '8', 'sys_mem_size': '754 GB', 'sys_storage_type': 'SATA SSD', 'sys_storage_size': '1x 447.1G + 1x 931.5G', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27700 Family [ConnectX-4]', 'num_network_cards': '1', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1571839041.669 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1571839041.669 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1571839047.368 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node dss01
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=4946' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=512 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=191023085537216086429 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191023085537216086429 ./run_and_time.sh
Run vars: id 191023085537216086429 gpus 8 mparams  --master_port=4946
NCCL_SOCKET_NTHREADS=2
NCCL_NSOCKS_PERTHREAD=8
STARTING TIMING RUN AT 2019-10-23 01:57:28 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=512
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=0
+ MATH=amp_fp16
+ [[ 0 -eq 1 ]]
+ LAUNCH_OPT='torch.distributed.launch --nproc_per_node 8  --master_port=4946'
+ echo 'running benchmark'
running benchmark
+ python -m torch.distributed.launch --nproc_per_node 8 --master_port=4946 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 512 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1571839050.260 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571839050.261 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571839050.263 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571839050.263 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571839050.263 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571839050.263 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571839050.264 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571839050.276 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=512, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 1394823936
dss01:465:465 [0] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:465:465 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:465:465 [0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:465:465 [0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:465:465 [0] NCCL INFO NET/IB : No device found.
dss01:465:465 [0] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
NCCL version 2.4.8+cuda10.1
dss01:468:468 [3] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:468:468 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:466:466 [1] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:466:466 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:468:468 [3] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:468:468 [3] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:468:468 [3] NCCL INFO NET/IB : No device found.
dss01:467:467 [2] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:467:467 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:469:469 [4] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:469:469 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:470:470 [5] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:470:470 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:471:471 [6] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:471:471 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:472:472 [7] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:472:472 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:466:466 [1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:466:466 [1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:466:466 [1] NCCL INFO NET/IB : No device found.
dss01:468:468 [3] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>

dss01:467:467 [2] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:467:467 [2] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:467:467 [2] NCCL INFO NET/IB : No device found.

dss01:472:472 [7] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:472:472 [7] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:472:472 [7] NCCL INFO NET/IB : No device found.

dss01:470:470 [5] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:470:470 [5] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:470:470 [5] NCCL INFO NET/IB : No device found.

dss01:469:469 [4] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:469:469 [4] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:469:469 [4] NCCL INFO NET/IB : No device found.

dss01:471:471 [6] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:471:471 [6] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:471:471 [6] NCCL INFO NET/IB : No device found.
dss01:466:466 [1] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:470:470 [5] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:469:469 [4] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:472:472 [7] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:471:471 [6] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:467:467 [2] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:465:827 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555
dss01:468:828 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555
dss01:470:830 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa
dss01:466:829 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555
dss01:469:831 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa
dss01:467:832 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555
dss01:471:834 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa
dss01:472:833 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa
dss01:470:830 [5] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:471:834 [6] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:472:833 [7] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:465:827 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:466:829 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:467:832 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:468:828 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:469:831 [4] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:465:827 [0] NCCL INFO Channel 00 :    0   1   2   3   4   5   6   7
dss01:469:831 [4] NCCL INFO Ring 00 : 4[4] -> 5[5] via P2P/IPC
dss01:471:834 [6] NCCL INFO Ring 00 : 6[6] -> 7[7] via P2P/IPC
dss01:467:832 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/IPC
dss01:465:827 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/IPC
dss01:468:828 [3] NCCL INFO Ring 00 : 3[3] -> 4[4] via direct shared memory
dss01:466:829 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via direct shared memory
dss01:472:833 [7] NCCL INFO Ring 00 : 7[7] -> 0[0] via direct shared memory
dss01:470:830 [5] NCCL INFO Ring 00 : 5[5] -> 6[6] via direct shared memory
dss01:465:827 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
dss01:468:828 [3] NCCL INFO comm 0x7fff50007590 rank 3 nranks 8 cudaDev 3 nvmlDev 3 - Init COMPLETE
dss01:472:833 [7] NCCL INFO comm 0x7fff58007590 rank 7 nranks 8 cudaDev 7 nvmlDev 7 - Init COMPLETE
dss01:470:830 [5] NCCL INFO comm 0x7fff68007590 rank 5 nranks 8 cudaDev 5 nvmlDev 5 - Init COMPLETE
dss01:469:831 [4] NCCL INFO comm 0x7fff7c007590 rank 4 nranks 8 cudaDev 4 nvmlDev 4 - Init COMPLETE
dss01:471:834 [6] NCCL INFO comm 0x7ffe98007590 rank 6 nranks 8 cudaDev 6 nvmlDev 6 - Init COMPLETE
dss01:466:829 [1] NCCL INFO comm 0x7fff30007590 rank 1 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE
dss01:467:832 [2] NCCL INFO comm 0x7fff54007590 rank 2 nranks 8 cudaDev 2 nvmlDev 2 - Init COMPLETE
dss01:465:827 [0] NCCL INFO comm 0x7ffe58007590 rank 0 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE
dss01:465:465 [0] NCCL INFO Launch mode Parallel
0: Worker 0 is using worker seed: 3638766362
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1571839076.681 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1571839080.619 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1571839080.619 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1571839080.620 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1571839081.588 global_batch_size: {"value": 4096, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1571839081.590 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1571839081.590 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1571839081.590 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1571839081.591 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1571839081.591 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1571839081.591 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1571839081.591 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1571839081.708 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571839081.709 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 3228352339
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/968]	Time 1.341 (1.341)	Data 9.10e-01 (9.10e-01)	Tok/s 25113 (25113)	Loss/tok 10.6923 (10.6923)	LR 2.000e-05
0: TRAIN [0][10/968]	Time 0.422 (0.534)	Data 6.81e-04 (8.30e-02)	Tok/s 79913 (71738)	Loss/tok 9.7909 (10.2369)	LR 2.518e-05
0: TRAIN [0][20/968]	Time 0.539 (0.461)	Data 1.87e-04 (4.35e-02)	Tok/s 86198 (72191)	Loss/tok 9.3959 (9.9190)	LR 3.170e-05
0: TRAIN [0][30/968]	Time 0.421 (0.449)	Data 1.93e-04 (2.96e-02)	Tok/s 79626 (74038)	Loss/tok 9.0283 (9.6730)	LR 3.991e-05
0: TRAIN [0][40/968]	Time 0.311 (0.443)	Data 1.71e-04 (2.24e-02)	Tok/s 66669 (73906)	Loss/tok 8.7069 (9.5016)	LR 5.024e-05
0: TRAIN [0][50/968]	Time 0.311 (0.426)	Data 1.78e-04 (1.80e-02)	Tok/s 66956 (73296)	Loss/tok 8.4772 (9.3672)	LR 6.325e-05
0: TRAIN [0][60/968]	Time 0.421 (0.416)	Data 1.73e-04 (1.51e-02)	Tok/s 79860 (73055)	Loss/tok 8.4120 (9.2325)	LR 7.962e-05
0: TRAIN [0][70/968]	Time 0.538 (0.414)	Data 1.73e-04 (1.30e-02)	Tok/s 86352 (73348)	Loss/tok 8.2253 (9.0937)	LR 1.002e-04
0: TRAIN [0][80/968]	Time 0.310 (0.410)	Data 1.70e-04 (1.14e-02)	Tok/s 66786 (73136)	Loss/tok 7.9712 (8.9816)	LR 1.262e-04
0: TRAIN [0][90/968]	Time 0.310 (0.405)	Data 1.68e-04 (1.02e-02)	Tok/s 66509 (72670)	Loss/tok 7.8669 (8.8897)	LR 1.589e-04
0: TRAIN [0][100/968]	Time 0.422 (0.405)	Data 2.33e-04 (9.21e-03)	Tok/s 79932 (72755)	Loss/tok 8.0133 (8.7962)	LR 2.000e-04
0: TRAIN [0][110/968]	Time 0.313 (0.403)	Data 1.76e-04 (8.39e-03)	Tok/s 65966 (72951)	Loss/tok 7.7336 (8.7194)	LR 2.518e-04
0: TRAIN [0][120/968]	Time 0.684 (0.402)	Data 1.85e-04 (7.72e-03)	Tok/s 87388 (72831)	Loss/tok 8.0623 (8.6552)	LR 3.170e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][130/968]	Time 0.422 (0.401)	Data 1.72e-04 (7.14e-03)	Tok/s 79973 (72921)	Loss/tok 7.8559 (8.6009)	LR 3.900e-04
0: TRAIN [0][140/968]	Time 0.310 (0.403)	Data 1.63e-04 (6.65e-03)	Tok/s 66350 (73206)	Loss/tok 7.5818 (8.5415)	LR 4.909e-04
0: TRAIN [0][150/968]	Time 0.309 (0.400)	Data 1.58e-04 (6.22e-03)	Tok/s 66699 (73154)	Loss/tok 7.5111 (8.4922)	LR 6.181e-04
0: TRAIN [0][160/968]	Time 0.309 (0.399)	Data 1.76e-04 (5.85e-03)	Tok/s 67289 (73185)	Loss/tok 7.4036 (8.4404)	LR 7.781e-04
0: TRAIN [0][170/968]	Time 0.422 (0.399)	Data 1.56e-04 (5.52e-03)	Tok/s 79595 (73255)	Loss/tok 7.4938 (8.3867)	LR 9.796e-04
0: TRAIN [0][180/968]	Time 0.681 (0.402)	Data 1.54e-04 (5.22e-03)	Tok/s 87381 (73473)	Loss/tok 7.4912 (8.3236)	LR 1.233e-03
0: TRAIN [0][190/968]	Time 0.309 (0.398)	Data 1.70e-04 (4.96e-03)	Tok/s 66537 (73143)	Loss/tok 7.1309 (8.2781)	LR 1.552e-03
0: TRAIN [0][200/968]	Time 0.203 (0.395)	Data 1.72e-04 (4.72e-03)	Tok/s 51798 (72887)	Loss/tok 6.0872 (8.2270)	LR 1.954e-03
0: TRAIN [0][210/968]	Time 0.542 (0.395)	Data 4.00e-04 (4.51e-03)	Tok/s 86398 (73015)	Loss/tok 6.9896 (8.1624)	LR 2.000e-03
0: TRAIN [0][220/968]	Time 0.423 (0.396)	Data 1.63e-04 (4.31e-03)	Tok/s 80474 (73197)	Loss/tok 6.5642 (8.0913)	LR 2.000e-03
0: TRAIN [0][230/968]	Time 0.421 (0.396)	Data 1.79e-04 (4.13e-03)	Tok/s 80150 (73221)	Loss/tok 6.5507 (8.0241)	LR 2.000e-03
0: TRAIN [0][240/968]	Time 0.422 (0.395)	Data 1.67e-04 (3.97e-03)	Tok/s 79736 (73273)	Loss/tok 6.3137 (7.9567)	LR 2.000e-03
0: TRAIN [0][250/968]	Time 0.425 (0.395)	Data 1.98e-04 (3.82e-03)	Tok/s 78778 (73314)	Loss/tok 6.2290 (7.8879)	LR 2.000e-03
0: TRAIN [0][260/968]	Time 0.425 (0.394)	Data 2.69e-04 (3.68e-03)	Tok/s 79409 (73282)	Loss/tok 6.0897 (7.8231)	LR 2.000e-03
0: TRAIN [0][270/968]	Time 0.312 (0.392)	Data 1.66e-04 (3.55e-03)	Tok/s 66129 (73091)	Loss/tok 5.6275 (7.7658)	LR 2.000e-03
0: TRAIN [0][280/968]	Time 0.421 (0.393)	Data 5.10e-04 (3.43e-03)	Tok/s 80213 (73143)	Loss/tok 5.9712 (7.6978)	LR 2.000e-03
0: TRAIN [0][290/968]	Time 0.681 (0.393)	Data 1.61e-04 (3.32e-03)	Tok/s 88033 (73186)	Loss/tok 6.1365 (7.6299)	LR 2.000e-03
0: TRAIN [0][300/968]	Time 0.312 (0.395)	Data 1.85e-04 (3.22e-03)	Tok/s 66830 (73321)	Loss/tok 5.4308 (7.5556)	LR 2.000e-03
0: TRAIN [0][310/968]	Time 0.311 (0.396)	Data 2.00e-04 (3.12e-03)	Tok/s 66857 (73360)	Loss/tok 5.2488 (7.4899)	LR 2.000e-03
0: TRAIN [0][320/968]	Time 0.310 (0.395)	Data 1.77e-04 (3.03e-03)	Tok/s 66502 (73280)	Loss/tok 5.2016 (7.4321)	LR 2.000e-03
0: TRAIN [0][330/968]	Time 0.203 (0.392)	Data 1.76e-04 (2.94e-03)	Tok/s 51819 (73023)	Loss/tok 4.2281 (7.3852)	LR 2.000e-03
0: TRAIN [0][340/968]	Time 0.311 (0.392)	Data 1.84e-04 (2.86e-03)	Tok/s 65687 (73029)	Loss/tok 4.9420 (7.3260)	LR 2.000e-03
0: TRAIN [0][350/968]	Time 0.310 (0.391)	Data 1.82e-04 (2.78e-03)	Tok/s 66663 (73051)	Loss/tok 4.8947 (7.2667)	LR 2.000e-03
0: TRAIN [0][360/968]	Time 0.423 (0.391)	Data 1.77e-04 (2.71e-03)	Tok/s 78894 (73040)	Loss/tok 5.1080 (7.2079)	LR 2.000e-03
0: TRAIN [0][370/968]	Time 0.309 (0.392)	Data 2.20e-04 (2.64e-03)	Tok/s 66335 (73090)	Loss/tok 4.6581 (7.1445)	LR 2.000e-03
0: TRAIN [0][380/968]	Time 0.681 (0.392)	Data 1.67e-04 (2.58e-03)	Tok/s 87822 (73085)	Loss/tok 5.4531 (7.0863)	LR 2.000e-03
0: TRAIN [0][390/968]	Time 0.424 (0.392)	Data 1.54e-04 (2.52e-03)	Tok/s 78754 (73147)	Loss/tok 4.8316 (7.0249)	LR 2.000e-03
0: TRAIN [0][400/968]	Time 0.423 (0.392)	Data 1.92e-04 (2.46e-03)	Tok/s 79672 (73088)	Loss/tok 4.6851 (6.9706)	LR 2.000e-03
0: TRAIN [0][410/968]	Time 0.544 (0.391)	Data 1.88e-04 (2.40e-03)	Tok/s 86367 (73013)	Loss/tok 4.9838 (6.9193)	LR 2.000e-03
0: TRAIN [0][420/968]	Time 0.541 (0.391)	Data 1.75e-04 (2.35e-03)	Tok/s 86439 (73046)	Loss/tok 4.7906 (6.8627)	LR 2.000e-03
0: TRAIN [0][430/968]	Time 0.682 (0.392)	Data 1.82e-04 (2.30e-03)	Tok/s 86765 (73080)	Loss/tok 5.0917 (6.8052)	LR 2.000e-03
0: TRAIN [0][440/968]	Time 0.312 (0.392)	Data 1.53e-04 (2.25e-03)	Tok/s 66201 (73042)	Loss/tok 4.1592 (6.7580)	LR 2.000e-03
0: TRAIN [0][450/968]	Time 0.685 (0.392)	Data 2.08e-04 (2.21e-03)	Tok/s 86211 (73002)	Loss/tok 4.9901 (6.7100)	LR 2.000e-03
0: TRAIN [0][460/968]	Time 0.424 (0.391)	Data 1.58e-04 (2.16e-03)	Tok/s 78163 (72958)	Loss/tok 4.5087 (6.6648)	LR 2.000e-03
0: TRAIN [0][470/968]	Time 0.423 (0.390)	Data 1.64e-04 (2.12e-03)	Tok/s 79011 (72882)	Loss/tok 4.3770 (6.6221)	LR 2.000e-03
0: TRAIN [0][480/968]	Time 0.685 (0.391)	Data 1.53e-04 (2.08e-03)	Tok/s 87518 (72981)	Loss/tok 4.7218 (6.5680)	LR 2.000e-03
0: TRAIN [0][490/968]	Time 0.545 (0.392)	Data 1.84e-04 (2.04e-03)	Tok/s 85873 (73022)	Loss/tok 4.5093 (6.5196)	LR 2.000e-03
0: TRAIN [0][500/968]	Time 0.545 (0.391)	Data 1.61e-04 (2.01e-03)	Tok/s 85918 (72967)	Loss/tok 4.5627 (6.4782)	LR 2.000e-03
0: TRAIN [0][510/968]	Time 0.310 (0.391)	Data 1.74e-04 (1.97e-03)	Tok/s 66578 (72955)	Loss/tok 3.9538 (6.4361)	LR 2.000e-03
0: TRAIN [0][520/968]	Time 0.205 (0.390)	Data 1.72e-04 (1.94e-03)	Tok/s 51786 (72808)	Loss/tok 3.2569 (6.4024)	LR 2.000e-03
0: TRAIN [0][530/968]	Time 0.543 (0.390)	Data 4.71e-04 (1.90e-03)	Tok/s 86434 (72853)	Loss/tok 4.4586 (6.3582)	LR 2.000e-03
0: TRAIN [0][540/968]	Time 0.423 (0.391)	Data 2.09e-04 (1.87e-03)	Tok/s 80289 (72899)	Loss/tok 4.1489 (6.3156)	LR 2.000e-03
0: TRAIN [0][550/968]	Time 0.423 (0.390)	Data 2.05e-04 (1.84e-03)	Tok/s 79588 (72891)	Loss/tok 4.1684 (6.2770)	LR 2.000e-03
0: TRAIN [0][560/968]	Time 0.311 (0.390)	Data 1.87e-04 (1.81e-03)	Tok/s 65787 (72854)	Loss/tok 3.8943 (6.2416)	LR 2.000e-03
0: TRAIN [0][570/968]	Time 0.205 (0.389)	Data 1.67e-04 (1.78e-03)	Tok/s 51226 (72736)	Loss/tok 3.1808 (6.2102)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][580/968]	Time 0.423 (0.389)	Data 1.72e-04 (1.76e-03)	Tok/s 79029 (72723)	Loss/tok 4.1364 (6.1753)	LR 2.000e-03
0: TRAIN [0][590/968]	Time 0.421 (0.389)	Data 1.63e-04 (1.73e-03)	Tok/s 78924 (72740)	Loss/tok 4.0589 (6.1397)	LR 2.000e-03
0: TRAIN [0][600/968]	Time 0.424 (0.390)	Data 1.92e-04 (1.70e-03)	Tok/s 78934 (72801)	Loss/tok 4.1187 (6.1005)	LR 2.000e-03
0: TRAIN [0][610/968]	Time 0.422 (0.389)	Data 1.89e-04 (1.68e-03)	Tok/s 79210 (72798)	Loss/tok 4.0036 (6.0678)	LR 2.000e-03
0: TRAIN [0][620/968]	Time 0.426 (0.389)	Data 1.68e-04 (1.65e-03)	Tok/s 79158 (72822)	Loss/tok 4.0766 (6.0338)	LR 2.000e-03
0: TRAIN [0][630/968]	Time 0.206 (0.389)	Data 1.69e-04 (1.63e-03)	Tok/s 51459 (72784)	Loss/tok 3.0863 (6.0039)	LR 2.000e-03
0: TRAIN [0][640/968]	Time 0.312 (0.389)	Data 1.75e-04 (1.61e-03)	Tok/s 65682 (72817)	Loss/tok 3.6757 (5.9704)	LR 2.000e-03
0: TRAIN [0][650/968]	Time 0.423 (0.389)	Data 1.78e-04 (1.59e-03)	Tok/s 79000 (72832)	Loss/tok 3.9570 (5.9383)	LR 2.000e-03
0: TRAIN [0][660/968]	Time 0.422 (0.389)	Data 1.62e-04 (1.57e-03)	Tok/s 80453 (72772)	Loss/tok 3.8898 (5.9114)	LR 2.000e-03
0: TRAIN [0][670/968]	Time 0.424 (0.389)	Data 1.82e-04 (1.55e-03)	Tok/s 78965 (72783)	Loss/tok 3.9173 (5.8819)	LR 2.000e-03
0: TRAIN [0][680/968]	Time 0.312 (0.388)	Data 1.72e-04 (1.53e-03)	Tok/s 66813 (72732)	Loss/tok 3.6518 (5.8560)	LR 2.000e-03
0: TRAIN [0][690/968]	Time 0.312 (0.389)	Data 2.16e-04 (1.51e-03)	Tok/s 66728 (72762)	Loss/tok 3.6461 (5.8271)	LR 2.000e-03
0: TRAIN [0][700/968]	Time 0.542 (0.389)	Data 1.83e-04 (1.49e-03)	Tok/s 86263 (72765)	Loss/tok 4.1000 (5.7994)	LR 2.000e-03
0: TRAIN [0][710/968]	Time 0.544 (0.388)	Data 2.11e-04 (1.47e-03)	Tok/s 85431 (72753)	Loss/tok 4.0519 (5.7730)	LR 2.000e-03
0: TRAIN [0][720/968]	Time 0.310 (0.388)	Data 1.83e-04 (1.45e-03)	Tok/s 66315 (72703)	Loss/tok 3.7731 (5.7489)	LR 2.000e-03
0: TRAIN [0][730/968]	Time 0.310 (0.388)	Data 1.83e-04 (1.43e-03)	Tok/s 65654 (72722)	Loss/tok 3.6943 (5.7240)	LR 2.000e-03
0: TRAIN [0][740/968]	Time 0.424 (0.387)	Data 1.88e-04 (1.42e-03)	Tok/s 79370 (72619)	Loss/tok 3.8683 (5.7038)	LR 2.000e-03
0: TRAIN [0][750/968]	Time 0.311 (0.386)	Data 1.57e-04 (1.40e-03)	Tok/s 65834 (72550)	Loss/tok 3.4953 (5.6829)	LR 2.000e-03
0: TRAIN [0][760/968]	Time 0.311 (0.386)	Data 1.54e-04 (1.38e-03)	Tok/s 66554 (72541)	Loss/tok 3.6537 (5.6596)	LR 2.000e-03
0: TRAIN [0][770/968]	Time 0.311 (0.385)	Data 1.60e-04 (1.37e-03)	Tok/s 66210 (72528)	Loss/tok 3.6752 (5.6372)	LR 2.000e-03
0: TRAIN [0][780/968]	Time 0.312 (0.386)	Data 1.78e-04 (1.35e-03)	Tok/s 66705 (72543)	Loss/tok 3.5943 (5.6129)	LR 2.000e-03
0: TRAIN [0][790/968]	Time 0.426 (0.386)	Data 1.74e-04 (1.34e-03)	Tok/s 78314 (72566)	Loss/tok 3.8562 (5.5891)	LR 2.000e-03
0: TRAIN [0][800/968]	Time 0.422 (0.386)	Data 1.57e-04 (1.32e-03)	Tok/s 80529 (72538)	Loss/tok 3.8188 (5.5675)	LR 2.000e-03
0: TRAIN [0][810/968]	Time 0.423 (0.386)	Data 2.33e-04 (1.31e-03)	Tok/s 79758 (72535)	Loss/tok 3.6732 (5.5449)	LR 2.000e-03
0: TRAIN [0][820/968]	Time 0.204 (0.386)	Data 1.96e-04 (1.30e-03)	Tok/s 52349 (72552)	Loss/tok 2.9540 (5.5229)	LR 2.000e-03
0: TRAIN [0][830/968]	Time 0.684 (0.387)	Data 1.83e-04 (1.28e-03)	Tok/s 87823 (72611)	Loss/tok 4.2166 (5.4990)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][840/968]	Time 0.421 (0.387)	Data 1.67e-04 (1.27e-03)	Tok/s 81149 (72623)	Loss/tok 3.7614 (5.4782)	LR 2.000e-03
0: TRAIN [0][850/968]	Time 0.424 (0.387)	Data 1.65e-04 (1.26e-03)	Tok/s 79144 (72637)	Loss/tok 3.7835 (5.4571)	LR 2.000e-03
0: TRAIN [0][860/968]	Time 0.312 (0.387)	Data 1.70e-04 (1.25e-03)	Tok/s 65511 (72675)	Loss/tok 3.4790 (5.4355)	LR 2.000e-03
0: TRAIN [0][870/968]	Time 0.425 (0.387)	Data 1.57e-04 (1.23e-03)	Tok/s 78965 (72653)	Loss/tok 3.7389 (5.4178)	LR 2.000e-03
0: TRAIN [0][880/968]	Time 0.684 (0.387)	Data 1.88e-04 (1.22e-03)	Tok/s 86374 (72670)	Loss/tok 4.2289 (5.3982)	LR 2.000e-03
0: TRAIN [0][890/968]	Time 0.204 (0.387)	Data 1.63e-04 (1.21e-03)	Tok/s 51422 (72610)	Loss/tok 2.8762 (5.3816)	LR 2.000e-03
0: TRAIN [0][900/968]	Time 0.424 (0.387)	Data 1.75e-04 (1.20e-03)	Tok/s 78405 (72608)	Loss/tok 3.6820 (5.3631)	LR 2.000e-03
0: TRAIN [0][910/968]	Time 0.312 (0.387)	Data 1.67e-04 (1.19e-03)	Tok/s 65760 (72661)	Loss/tok 3.4638 (5.3427)	LR 2.000e-03
0: TRAIN [0][920/968]	Time 0.545 (0.388)	Data 1.84e-04 (1.18e-03)	Tok/s 84524 (72699)	Loss/tok 3.9501 (5.3232)	LR 2.000e-03
0: TRAIN [0][930/968]	Time 0.423 (0.388)	Data 4.83e-04 (1.17e-03)	Tok/s 80134 (72653)	Loss/tok 3.7270 (5.3082)	LR 2.000e-03
0: TRAIN [0][940/968]	Time 0.310 (0.388)	Data 1.58e-04 (1.16e-03)	Tok/s 66294 (72661)	Loss/tok 3.4597 (5.2916)	LR 2.000e-03
0: TRAIN [0][950/968]	Time 0.312 (0.388)	Data 1.89e-04 (1.15e-03)	Tok/s 66874 (72693)	Loss/tok 3.4719 (5.2732)	LR 2.000e-03
0: TRAIN [0][960/968]	Time 0.421 (0.388)	Data 1.55e-04 (1.14e-03)	Tok/s 80235 (72712)	Loss/tok 3.6686 (5.2559)	LR 2.000e-03
:::MLL 1571839458.698 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1571839458.698 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.830 (0.830)	Decoder iters 149.0 (149.0)	Tok/s 20474 (20474)
0: Running moses detokenizer
0: BLEU(score=18.407430850891217, counts=[33379, 14789, 7679, 4148], totals=[65430, 62427, 59424, 56425], precisions=[51.014825003820874, 23.69007000176206, 12.922388260635433, 7.351351351351352], bp=1.0, sys_len=65430, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571839461.010 eval_accuracy: {"value": 18.41, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1571839461.011 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 5.2488	Test BLEU: 18.41
0: Performance: Epoch: 0	Training: 581164 Tok/s
0: Finished epoch 0
:::MLL 1571839461.011 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1571839461.011 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571839461.012 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 926582472
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][0/968]	Time 1.162 (1.162)	Data 7.29e-01 (7.29e-01)	Tok/s 28643 (28643)	Loss/tok 3.7678 (3.7678)	LR 2.000e-03
0: TRAIN [1][10/968]	Time 0.782 (0.494)	Data 1.79e-04 (6.65e-02)	Tok/s 77081 (69744)	Loss/tok 3.8808 (3.6938)	LR 2.000e-03
0: TRAIN [1][20/968]	Time 0.543 (0.462)	Data 1.40e-04 (3.49e-02)	Tok/s 85917 (72967)	Loss/tok 3.8375 (3.6924)	LR 2.000e-03
0: TRAIN [1][30/968]	Time 0.313 (0.436)	Data 1.37e-04 (2.37e-02)	Tok/s 66423 (72790)	Loss/tok 3.3290 (3.6659)	LR 2.000e-03
0: TRAIN [1][40/968]	Time 0.205 (0.426)	Data 1.35e-04 (1.79e-02)	Tok/s 51831 (72815)	Loss/tok 2.9150 (3.6681)	LR 2.000e-03
0: TRAIN [1][50/968]	Time 0.683 (0.422)	Data 1.38e-04 (1.45e-02)	Tok/s 87653 (72965)	Loss/tok 3.8953 (3.6621)	LR 2.000e-03
0: TRAIN [1][60/968]	Time 0.313 (0.419)	Data 1.39e-04 (1.21e-02)	Tok/s 65815 (73285)	Loss/tok 3.3781 (3.6547)	LR 2.000e-03
0: TRAIN [1][70/968]	Time 0.204 (0.414)	Data 1.60e-04 (1.04e-02)	Tok/s 52065 (72997)	Loss/tok 2.7978 (3.6505)	LR 2.000e-03
0: TRAIN [1][80/968]	Time 0.205 (0.410)	Data 1.38e-04 (9.16e-03)	Tok/s 51888 (72469)	Loss/tok 2.8912 (3.6540)	LR 2.000e-03
0: TRAIN [1][90/968]	Time 0.312 (0.401)	Data 1.31e-04 (8.17e-03)	Tok/s 66699 (72060)	Loss/tok 3.2947 (3.6386)	LR 2.000e-03
0: TRAIN [1][100/968]	Time 0.311 (0.402)	Data 1.53e-04 (7.37e-03)	Tok/s 66566 (72159)	Loss/tok 3.3716 (3.6413)	LR 2.000e-03
0: TRAIN [1][110/968]	Time 0.543 (0.406)	Data 1.42e-04 (6.72e-03)	Tok/s 85521 (72747)	Loss/tok 3.8334 (3.6497)	LR 2.000e-03
0: TRAIN [1][120/968]	Time 0.542 (0.402)	Data 1.50e-04 (6.18e-03)	Tok/s 85518 (72496)	Loss/tok 3.7801 (3.6435)	LR 2.000e-03
0: TRAIN [1][130/968]	Time 0.313 (0.400)	Data 1.37e-04 (5.72e-03)	Tok/s 65074 (72374)	Loss/tok 3.3821 (3.6385)	LR 2.000e-03
0: TRAIN [1][140/968]	Time 0.544 (0.397)	Data 1.80e-04 (5.33e-03)	Tok/s 85492 (72267)	Loss/tok 3.8523 (3.6311)	LR 2.000e-03
0: TRAIN [1][150/968]	Time 0.311 (0.393)	Data 1.39e-04 (4.99e-03)	Tok/s 66740 (71988)	Loss/tok 3.3697 (3.6235)	LR 2.000e-03
0: TRAIN [1][160/968]	Time 0.424 (0.391)	Data 1.39e-04 (4.68e-03)	Tok/s 78919 (71941)	Loss/tok 3.5559 (3.6157)	LR 2.000e-03
0: TRAIN [1][170/968]	Time 0.423 (0.388)	Data 1.39e-04 (4.42e-03)	Tok/s 78778 (71840)	Loss/tok 3.6221 (3.6086)	LR 2.000e-03
0: TRAIN [1][180/968]	Time 0.314 (0.390)	Data 1.36e-04 (4.18e-03)	Tok/s 65114 (72066)	Loss/tok 3.3659 (3.6078)	LR 2.000e-03
0: TRAIN [1][190/968]	Time 0.206 (0.388)	Data 1.34e-04 (3.97e-03)	Tok/s 52020 (72023)	Loss/tok 2.7948 (3.6020)	LR 2.000e-03
0: TRAIN [1][200/968]	Time 0.314 (0.387)	Data 1.60e-04 (3.78e-03)	Tok/s 66316 (71840)	Loss/tok 3.2351 (3.5997)	LR 2.000e-03
0: TRAIN [1][210/968]	Time 0.313 (0.384)	Data 1.35e-04 (3.61e-03)	Tok/s 66027 (71634)	Loss/tok 3.3177 (3.5920)	LR 2.000e-03
0: TRAIN [1][220/968]	Time 0.314 (0.383)	Data 1.38e-04 (3.45e-03)	Tok/s 65397 (71579)	Loss/tok 3.3688 (3.5874)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][230/968]	Time 0.423 (0.385)	Data 1.39e-04 (3.31e-03)	Tok/s 79477 (71742)	Loss/tok 3.6182 (3.5915)	LR 2.000e-03
0: TRAIN [1][240/968]	Time 0.423 (0.382)	Data 1.35e-04 (3.18e-03)	Tok/s 79512 (71546)	Loss/tok 3.5408 (3.5848)	LR 2.000e-03
0: TRAIN [1][250/968]	Time 0.206 (0.380)	Data 1.43e-04 (3.06e-03)	Tok/s 50261 (71404)	Loss/tok 2.8229 (3.5802)	LR 2.000e-03
0: TRAIN [1][260/968]	Time 0.312 (0.382)	Data 1.66e-04 (2.95e-03)	Tok/s 66964 (71553)	Loss/tok 3.2995 (3.5835)	LR 2.000e-03
0: TRAIN [1][270/968]	Time 0.427 (0.384)	Data 1.38e-04 (2.84e-03)	Tok/s 77948 (71731)	Loss/tok 3.5416 (3.5855)	LR 2.000e-03
0: TRAIN [1][280/968]	Time 0.423 (0.386)	Data 1.38e-04 (2.75e-03)	Tok/s 79316 (71956)	Loss/tok 3.5506 (3.5894)	LR 2.000e-03
0: TRAIN [1][290/968]	Time 0.684 (0.387)	Data 1.40e-04 (2.66e-03)	Tok/s 86820 (72068)	Loss/tok 4.0696 (3.5910)	LR 2.000e-03
0: TRAIN [1][300/968]	Time 0.311 (0.388)	Data 1.43e-04 (2.58e-03)	Tok/s 66582 (72163)	Loss/tok 3.3282 (3.5896)	LR 2.000e-03
0: TRAIN [1][310/968]	Time 0.422 (0.389)	Data 1.42e-04 (2.50e-03)	Tok/s 80048 (72267)	Loss/tok 3.5044 (3.5890)	LR 2.000e-03
0: TRAIN [1][320/968]	Time 0.312 (0.389)	Data 1.46e-04 (2.43e-03)	Tok/s 66132 (72387)	Loss/tok 3.2587 (3.5862)	LR 2.000e-03
0: TRAIN [1][330/968]	Time 0.311 (0.390)	Data 1.44e-04 (2.36e-03)	Tok/s 65462 (72450)	Loss/tok 3.3386 (3.5855)	LR 2.000e-03
0: TRAIN [1][340/968]	Time 0.312 (0.390)	Data 1.41e-04 (2.29e-03)	Tok/s 66192 (72420)	Loss/tok 3.3167 (3.5836)	LR 2.000e-03
0: TRAIN [1][350/968]	Time 0.426 (0.390)	Data 1.29e-04 (2.23e-03)	Tok/s 78573 (72450)	Loss/tok 3.6032 (3.5824)	LR 2.000e-03
0: TRAIN [1][360/968]	Time 0.314 (0.389)	Data 1.38e-04 (2.18e-03)	Tok/s 65433 (72360)	Loss/tok 3.3320 (3.5792)	LR 2.000e-03
0: TRAIN [1][370/968]	Time 0.205 (0.389)	Data 1.30e-04 (2.12e-03)	Tok/s 51550 (72309)	Loss/tok 2.8497 (3.5774)	LR 2.000e-03
0: TRAIN [1][380/968]	Time 0.312 (0.388)	Data 1.39e-04 (2.07e-03)	Tok/s 66633 (72331)	Loss/tok 3.3128 (3.5759)	LR 2.000e-03
0: TRAIN [1][390/968]	Time 0.424 (0.389)	Data 1.55e-04 (2.02e-03)	Tok/s 79907 (72463)	Loss/tok 3.4710 (3.5754)	LR 2.000e-03
0: TRAIN [1][400/968]	Time 0.314 (0.388)	Data 1.48e-04 (1.97e-03)	Tok/s 66101 (72344)	Loss/tok 3.2377 (3.5715)	LR 2.000e-03
0: TRAIN [1][410/968]	Time 0.309 (0.387)	Data 1.46e-04 (1.93e-03)	Tok/s 66718 (72216)	Loss/tok 3.2868 (3.5674)	LR 2.000e-03
0: TRAIN [1][420/968]	Time 0.311 (0.387)	Data 1.40e-04 (1.89e-03)	Tok/s 66493 (72249)	Loss/tok 3.3330 (3.5669)	LR 2.000e-03
0: TRAIN [1][430/968]	Time 0.312 (0.387)	Data 2.15e-04 (1.84e-03)	Tok/s 66501 (72242)	Loss/tok 3.2823 (3.5663)	LR 2.000e-03
0: TRAIN [1][440/968]	Time 0.544 (0.386)	Data 1.34e-04 (1.81e-03)	Tok/s 85233 (72228)	Loss/tok 3.8856 (3.5657)	LR 2.000e-03
0: TRAIN [1][450/968]	Time 0.424 (0.387)	Data 1.44e-04 (1.77e-03)	Tok/s 79497 (72257)	Loss/tok 3.4203 (3.5637)	LR 2.000e-03
0: TRAIN [1][460/968]	Time 0.543 (0.388)	Data 1.31e-04 (1.73e-03)	Tok/s 86068 (72369)	Loss/tok 3.7712 (3.5634)	LR 2.000e-03
0: TRAIN [1][470/968]	Time 0.424 (0.389)	Data 1.39e-04 (1.70e-03)	Tok/s 79986 (72492)	Loss/tok 3.4375 (3.5656)	LR 2.000e-03
0: TRAIN [1][480/968]	Time 0.543 (0.390)	Data 1.31e-04 (1.67e-03)	Tok/s 86335 (72578)	Loss/tok 3.7353 (3.5658)	LR 2.000e-03
0: TRAIN [1][490/968]	Time 0.313 (0.390)	Data 1.38e-04 (1.64e-03)	Tok/s 66912 (72598)	Loss/tok 3.3102 (3.5639)	LR 2.000e-03
0: TRAIN [1][500/968]	Time 0.425 (0.389)	Data 1.36e-04 (1.61e-03)	Tok/s 79316 (72450)	Loss/tok 3.4849 (3.5605)	LR 2.000e-03
0: TRAIN [1][510/968]	Time 0.314 (0.388)	Data 1.29e-04 (1.58e-03)	Tok/s 65869 (72449)	Loss/tok 3.3402 (3.5597)	LR 2.000e-03
0: TRAIN [1][520/968]	Time 0.423 (0.389)	Data 1.35e-04 (1.55e-03)	Tok/s 79197 (72529)	Loss/tok 3.4872 (3.5611)	LR 2.000e-03
0: TRAIN [1][530/968]	Time 0.424 (0.389)	Data 1.44e-04 (1.53e-03)	Tok/s 78952 (72500)	Loss/tok 3.5884 (3.5583)	LR 2.000e-03
0: TRAIN [1][540/968]	Time 0.311 (0.388)	Data 1.34e-04 (1.50e-03)	Tok/s 65322 (72449)	Loss/tok 3.2895 (3.5562)	LR 2.000e-03
0: TRAIN [1][550/968]	Time 0.313 (0.389)	Data 1.34e-04 (1.48e-03)	Tok/s 66226 (72529)	Loss/tok 3.2546 (3.5560)	LR 2.000e-03
0: TRAIN [1][560/968]	Time 0.424 (0.389)	Data 1.54e-04 (1.45e-03)	Tok/s 79049 (72503)	Loss/tok 3.4363 (3.5543)	LR 2.000e-03
0: TRAIN [1][570/968]	Time 0.313 (0.389)	Data 1.40e-04 (1.43e-03)	Tok/s 66513 (72545)	Loss/tok 3.3157 (3.5540)	LR 2.000e-03
0: TRAIN [1][580/968]	Time 0.310 (0.389)	Data 1.68e-04 (1.41e-03)	Tok/s 66631 (72506)	Loss/tok 3.2451 (3.5523)	LR 2.000e-03
0: TRAIN [1][590/968]	Time 0.312 (0.388)	Data 1.48e-04 (1.39e-03)	Tok/s 65946 (72429)	Loss/tok 3.2687 (3.5494)	LR 2.000e-03
0: TRAIN [1][600/968]	Time 0.314 (0.388)	Data 1.30e-04 (1.37e-03)	Tok/s 65052 (72394)	Loss/tok 3.2830 (3.5470)	LR 2.000e-03
0: TRAIN [1][610/968]	Time 0.424 (0.387)	Data 1.46e-04 (1.35e-03)	Tok/s 79342 (72376)	Loss/tok 3.5222 (3.5444)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][620/968]	Time 0.545 (0.388)	Data 1.43e-04 (1.33e-03)	Tok/s 85945 (72437)	Loss/tok 3.6827 (3.5441)	LR 2.000e-03
0: TRAIN [1][630/968]	Time 0.313 (0.388)	Data 1.41e-04 (1.31e-03)	Tok/s 66652 (72443)	Loss/tok 3.2418 (3.5431)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][640/968]	Time 0.310 (0.388)	Data 1.48e-04 (1.29e-03)	Tok/s 66552 (72470)	Loss/tok 3.2100 (3.5428)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][650/968]	Time 0.313 (0.389)	Data 1.62e-04 (1.27e-03)	Tok/s 64719 (72499)	Loss/tok 3.2059 (3.5427)	LR 2.000e-03
0: TRAIN [1][660/968]	Time 0.426 (0.388)	Data 1.46e-04 (1.26e-03)	Tok/s 79060 (72459)	Loss/tok 3.4696 (3.5404)	LR 2.000e-03
0: TRAIN [1][670/968]	Time 0.423 (0.388)	Data 1.29e-04 (1.24e-03)	Tok/s 79395 (72477)	Loss/tok 3.4698 (3.5393)	LR 2.000e-03
0: TRAIN [1][680/968]	Time 0.311 (0.388)	Data 1.57e-04 (1.22e-03)	Tok/s 66641 (72463)	Loss/tok 3.1843 (3.5373)	LR 2.000e-03
0: TRAIN [1][690/968]	Time 0.423 (0.388)	Data 1.45e-04 (1.21e-03)	Tok/s 80119 (72446)	Loss/tok 3.4607 (3.5348)	LR 2.000e-03
0: TRAIN [1][700/968]	Time 0.423 (0.388)	Data 1.47e-04 (1.19e-03)	Tok/s 79516 (72495)	Loss/tok 3.4460 (3.5331)	LR 2.000e-03
0: TRAIN [1][710/968]	Time 0.543 (0.388)	Data 1.54e-04 (1.18e-03)	Tok/s 85927 (72559)	Loss/tok 3.6060 (3.5323)	LR 2.000e-03
0: TRAIN [1][720/968]	Time 0.422 (0.389)	Data 1.31e-04 (1.17e-03)	Tok/s 79209 (72559)	Loss/tok 3.5893 (3.5327)	LR 2.000e-03
0: TRAIN [1][730/968]	Time 0.424 (0.387)	Data 1.38e-04 (1.15e-03)	Tok/s 79629 (72468)	Loss/tok 3.4638 (3.5300)	LR 2.000e-03
0: TRAIN [1][740/968]	Time 0.424 (0.388)	Data 1.56e-04 (1.14e-03)	Tok/s 79277 (72548)	Loss/tok 3.4873 (3.5304)	LR 2.000e-03
0: TRAIN [1][750/968]	Time 0.204 (0.387)	Data 1.44e-04 (1.13e-03)	Tok/s 51952 (72458)	Loss/tok 2.7426 (3.5277)	LR 2.000e-03
0: TRAIN [1][760/968]	Time 0.423 (0.387)	Data 1.38e-04 (1.11e-03)	Tok/s 79629 (72483)	Loss/tok 3.3912 (3.5260)	LR 2.000e-03
0: TRAIN [1][770/968]	Time 0.684 (0.387)	Data 1.34e-04 (1.10e-03)	Tok/s 85494 (72463)	Loss/tok 3.8901 (3.5247)	LR 2.000e-03
0: TRAIN [1][780/968]	Time 0.311 (0.387)	Data 1.37e-04 (1.09e-03)	Tok/s 67140 (72438)	Loss/tok 3.2488 (3.5229)	LR 2.000e-03
0: TRAIN [1][790/968]	Time 0.422 (0.385)	Data 1.45e-04 (1.08e-03)	Tok/s 80156 (72268)	Loss/tok 3.4415 (3.5200)	LR 2.000e-03
0: TRAIN [1][800/968]	Time 0.422 (0.385)	Data 1.42e-04 (1.06e-03)	Tok/s 79288 (72300)	Loss/tok 3.5019 (3.5191)	LR 2.000e-03
0: TRAIN [1][810/968]	Time 0.310 (0.386)	Data 1.42e-04 (1.05e-03)	Tok/s 66665 (72331)	Loss/tok 3.2261 (3.5192)	LR 2.000e-03
0: TRAIN [1][820/968]	Time 0.314 (0.386)	Data 1.35e-04 (1.04e-03)	Tok/s 66109 (72367)	Loss/tok 3.2553 (3.5188)	LR 2.000e-03
0: TRAIN [1][830/968]	Time 0.426 (0.386)	Data 1.30e-04 (1.03e-03)	Tok/s 79188 (72365)	Loss/tok 3.4463 (3.5175)	LR 2.000e-03
0: TRAIN [1][840/968]	Time 0.312 (0.386)	Data 1.60e-04 (1.02e-03)	Tok/s 66333 (72377)	Loss/tok 3.2198 (3.5161)	LR 2.000e-03
0: TRAIN [1][850/968]	Time 0.206 (0.386)	Data 1.47e-04 (1.01e-03)	Tok/s 50837 (72391)	Loss/tok 2.8046 (3.5149)	LR 2.000e-03
0: TRAIN [1][860/968]	Time 0.542 (0.386)	Data 1.74e-04 (1.00e-03)	Tok/s 86253 (72411)	Loss/tok 3.6152 (3.5143)	LR 2.000e-03
0: TRAIN [1][870/968]	Time 0.424 (0.387)	Data 1.45e-04 (9.92e-04)	Tok/s 79566 (72452)	Loss/tok 3.3558 (3.5132)	LR 2.000e-03
0: TRAIN [1][880/968]	Time 0.205 (0.386)	Data 1.51e-04 (9.82e-04)	Tok/s 50947 (72407)	Loss/tok 2.7443 (3.5124)	LR 2.000e-03
0: TRAIN [1][890/968]	Time 0.542 (0.386)	Data 1.39e-04 (9.73e-04)	Tok/s 85820 (72371)	Loss/tok 3.6136 (3.5105)	LR 2.000e-03
0: TRAIN [1][900/968]	Time 0.314 (0.387)	Data 1.37e-04 (9.64e-04)	Tok/s 65880 (72447)	Loss/tok 3.1963 (3.5126)	LR 2.000e-03
0: TRAIN [1][910/968]	Time 0.311 (0.388)	Data 1.33e-04 (9.55e-04)	Tok/s 65843 (72498)	Loss/tok 3.2648 (3.5122)	LR 2.000e-03
0: TRAIN [1][920/968]	Time 0.311 (0.387)	Data 1.37e-04 (9.46e-04)	Tok/s 66935 (72450)	Loss/tok 3.1782 (3.5103)	LR 2.000e-03
0: TRAIN [1][930/968]	Time 0.424 (0.388)	Data 1.40e-04 (9.38e-04)	Tok/s 80234 (72506)	Loss/tok 3.4173 (3.5098)	LR 2.000e-03
0: TRAIN [1][940/968]	Time 0.311 (0.387)	Data 1.35e-04 (9.29e-04)	Tok/s 65163 (72447)	Loss/tok 3.1692 (3.5081)	LR 2.000e-03
0: TRAIN [1][950/968]	Time 0.425 (0.387)	Data 1.45e-04 (9.22e-04)	Tok/s 78972 (72455)	Loss/tok 3.4405 (3.5065)	LR 2.000e-03
0: TRAIN [1][960/968]	Time 0.313 (0.388)	Data 1.32e-04 (9.13e-04)	Tok/s 66479 (72499)	Loss/tok 3.1751 (3.5061)	LR 2.000e-03
:::MLL 1571839838.463 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1571839838.463 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.730 (0.730)	Decoder iters 147.0 (147.0)	Tok/s 22211 (22211)
0: Running moses detokenizer
0: BLEU(score=21.438924382445816, counts=[35123, 16698, 9110, 5148], totals=[64525, 61522, 58520, 55523], precisions=[54.43316543975203, 27.1415103540197, 15.567327409432673, 9.271833294310467], bp=0.9976625578568467, sys_len=64525, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571839840.449 eval_accuracy: {"value": 21.44, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1571839840.449 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.5026	Test BLEU: 21.44
0: Performance: Epoch: 1	Training: 580048 Tok/s
0: Finished epoch 1
:::MLL 1571839840.450 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1571839840.450 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571839840.450 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 1609775663
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][0/968]	Time 1.042 (1.042)	Data 6.97e-01 (6.97e-01)	Tok/s 20280 (20280)	Loss/tok 3.1593 (3.1593)	LR 2.000e-03
0: TRAIN [2][10/968]	Time 0.793 (0.464)	Data 1.45e-04 (6.35e-02)	Tok/s 74754 (66806)	Loss/tok 3.7360 (3.3469)	LR 2.000e-03
0: TRAIN [2][20/968]	Time 0.310 (0.430)	Data 1.56e-04 (3.34e-02)	Tok/s 66920 (69758)	Loss/tok 3.1319 (3.3597)	LR 2.000e-03
0: TRAIN [2][30/968]	Time 0.313 (0.422)	Data 1.44e-04 (2.27e-02)	Tok/s 66153 (71268)	Loss/tok 3.1408 (3.3673)	LR 2.000e-03
0: TRAIN [2][40/968]	Time 0.684 (0.429)	Data 2.06e-04 (1.72e-02)	Tok/s 86762 (73031)	Loss/tok 3.6656 (3.3774)	LR 2.000e-03
0: TRAIN [2][50/968]	Time 0.423 (0.415)	Data 1.72e-04 (1.38e-02)	Tok/s 79772 (72544)	Loss/tok 3.3262 (3.3575)	LR 2.000e-03
0: TRAIN [2][60/968]	Time 0.544 (0.410)	Data 1.96e-04 (1.16e-02)	Tok/s 85887 (72516)	Loss/tok 3.5397 (3.3516)	LR 2.000e-03
0: TRAIN [2][70/968]	Time 0.312 (0.405)	Data 1.61e-04 (9.99e-03)	Tok/s 66194 (72030)	Loss/tok 3.1791 (3.3491)	LR 2.000e-03
0: TRAIN [2][80/968]	Time 0.312 (0.410)	Data 1.94e-04 (8.79e-03)	Tok/s 66011 (72475)	Loss/tok 3.2090 (3.3659)	LR 2.000e-03
0: TRAIN [2][90/968]	Time 0.425 (0.410)	Data 1.65e-04 (7.84e-03)	Tok/s 77918 (72841)	Loss/tok 3.3506 (3.3661)	LR 2.000e-03
0: TRAIN [2][100/968]	Time 0.683 (0.409)	Data 1.74e-04 (7.08e-03)	Tok/s 87064 (72700)	Loss/tok 3.6382 (3.3675)	LR 2.000e-03
0: TRAIN [2][110/968]	Time 0.313 (0.410)	Data 1.56e-04 (6.46e-03)	Tok/s 66399 (72810)	Loss/tok 3.0909 (3.3712)	LR 2.000e-03
0: TRAIN [2][120/968]	Time 0.424 (0.406)	Data 1.74e-04 (5.94e-03)	Tok/s 79398 (72611)	Loss/tok 3.2706 (3.3628)	LR 2.000e-03
0: TRAIN [2][130/968]	Time 0.312 (0.403)	Data 2.31e-04 (5.50e-03)	Tok/s 66806 (72462)	Loss/tok 3.1506 (3.3598)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][140/968]	Time 0.424 (0.402)	Data 1.64e-04 (5.12e-03)	Tok/s 79884 (72406)	Loss/tok 3.3400 (3.3592)	LR 2.000e-03
0: TRAIN [2][150/968]	Time 0.542 (0.397)	Data 1.61e-04 (4.79e-03)	Tok/s 84952 (72107)	Loss/tok 3.5700 (3.3528)	LR 2.000e-03
0: TRAIN [2][160/968]	Time 0.425 (0.398)	Data 1.76e-04 (4.51e-03)	Tok/s 79759 (72289)	Loss/tok 3.2408 (3.3536)	LR 2.000e-03
0: TRAIN [2][170/968]	Time 0.544 (0.397)	Data 1.74e-04 (4.26e-03)	Tok/s 84388 (72262)	Loss/tok 3.5850 (3.3490)	LR 2.000e-03
0: TRAIN [2][180/968]	Time 0.423 (0.395)	Data 1.89e-04 (4.03e-03)	Tok/s 78375 (72164)	Loss/tok 3.4218 (3.3489)	LR 2.000e-03
0: TRAIN [2][190/968]	Time 0.313 (0.395)	Data 1.67e-04 (3.83e-03)	Tok/s 66198 (72263)	Loss/tok 3.1427 (3.3472)	LR 2.000e-03
0: TRAIN [2][200/968]	Time 0.311 (0.394)	Data 1.68e-04 (3.65e-03)	Tok/s 65832 (72261)	Loss/tok 3.0493 (3.3458)	LR 2.000e-03
0: TRAIN [2][210/968]	Time 0.424 (0.397)	Data 1.68e-04 (3.48e-03)	Tok/s 79152 (72548)	Loss/tok 3.3695 (3.3479)	LR 2.000e-03
0: TRAIN [2][220/968]	Time 0.313 (0.399)	Data 1.50e-04 (3.34e-03)	Tok/s 66564 (72781)	Loss/tok 3.1184 (3.3521)	LR 2.000e-03
0: TRAIN [2][230/968]	Time 0.423 (0.398)	Data 1.70e-04 (3.20e-03)	Tok/s 78558 (72649)	Loss/tok 3.2635 (3.3503)	LR 2.000e-03
0: TRAIN [2][240/968]	Time 0.312 (0.398)	Data 1.56e-04 (3.07e-03)	Tok/s 66640 (72738)	Loss/tok 3.1726 (3.3477)	LR 2.000e-03
0: TRAIN [2][250/968]	Time 0.424 (0.397)	Data 1.77e-04 (2.96e-03)	Tok/s 80009 (72688)	Loss/tok 3.2692 (3.3466)	LR 2.000e-03
0: TRAIN [2][260/968]	Time 0.312 (0.397)	Data 1.56e-04 (2.85e-03)	Tok/s 66071 (72637)	Loss/tok 3.0998 (3.3458)	LR 2.000e-03
0: TRAIN [2][270/968]	Time 0.546 (0.398)	Data 1.63e-04 (2.75e-03)	Tok/s 84868 (72775)	Loss/tok 3.5194 (3.3471)	LR 2.000e-03
0: TRAIN [2][280/968]	Time 0.205 (0.397)	Data 1.71e-04 (2.66e-03)	Tok/s 51889 (72661)	Loss/tok 2.7058 (3.3441)	LR 2.000e-03
0: TRAIN [2][290/968]	Time 0.684 (0.397)	Data 1.85e-04 (2.58e-03)	Tok/s 86973 (72599)	Loss/tok 3.7427 (3.3455)	LR 2.000e-03
0: TRAIN [2][300/968]	Time 0.311 (0.396)	Data 1.60e-04 (2.50e-03)	Tok/s 66956 (72645)	Loss/tok 3.0945 (3.3431)	LR 2.000e-03
0: TRAIN [2][310/968]	Time 0.425 (0.396)	Data 1.90e-04 (2.42e-03)	Tok/s 78725 (72632)	Loss/tok 3.2284 (3.3434)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][320/968]	Time 0.423 (0.398)	Data 1.68e-04 (2.35e-03)	Tok/s 79528 (72670)	Loss/tok 3.2949 (3.3470)	LR 2.000e-03
0: TRAIN [2][330/968]	Time 0.544 (0.399)	Data 1.57e-04 (2.28e-03)	Tok/s 85715 (72867)	Loss/tok 3.4825 (3.3487)	LR 2.000e-03
0: TRAIN [2][340/968]	Time 0.312 (0.399)	Data 1.60e-04 (2.22e-03)	Tok/s 66383 (72882)	Loss/tok 3.1219 (3.3485)	LR 2.000e-03
0: TRAIN [2][350/968]	Time 0.313 (0.397)	Data 1.80e-04 (2.16e-03)	Tok/s 65221 (72715)	Loss/tok 3.0745 (3.3464)	LR 2.000e-03
0: TRAIN [2][360/968]	Time 0.421 (0.397)	Data 1.65e-04 (2.11e-03)	Tok/s 79976 (72754)	Loss/tok 3.3219 (3.3451)	LR 2.000e-03
0: TRAIN [2][370/968]	Time 0.426 (0.395)	Data 1.79e-04 (2.06e-03)	Tok/s 78832 (72642)	Loss/tok 3.3212 (3.3414)	LR 2.000e-03
0: TRAIN [2][380/968]	Time 0.423 (0.395)	Data 1.73e-04 (2.01e-03)	Tok/s 79524 (72667)	Loss/tok 3.3780 (3.3398)	LR 2.000e-03
0: TRAIN [2][390/968]	Time 0.423 (0.394)	Data 1.79e-04 (1.96e-03)	Tok/s 79612 (72611)	Loss/tok 3.3498 (3.3379)	LR 2.000e-03
0: TRAIN [2][400/968]	Time 0.545 (0.394)	Data 1.59e-04 (1.92e-03)	Tok/s 84793 (72650)	Loss/tok 3.5462 (3.3367)	LR 2.000e-03
0: TRAIN [2][410/968]	Time 0.205 (0.392)	Data 1.54e-04 (1.87e-03)	Tok/s 51000 (72463)	Loss/tok 2.7179 (3.3338)	LR 2.000e-03
0: TRAIN [2][420/968]	Time 0.687 (0.394)	Data 5.11e-04 (1.83e-03)	Tok/s 86255 (72630)	Loss/tok 3.7516 (3.3369)	LR 2.000e-03
0: TRAIN [2][430/968]	Time 0.424 (0.393)	Data 1.59e-04 (1.80e-03)	Tok/s 79702 (72554)	Loss/tok 3.2807 (3.3345)	LR 2.000e-03
0: TRAIN [2][440/968]	Time 0.425 (0.392)	Data 1.66e-04 (1.76e-03)	Tok/s 79330 (72462)	Loss/tok 3.2952 (3.3323)	LR 2.000e-03
0: TRAIN [2][450/968]	Time 0.312 (0.390)	Data 1.82e-04 (1.72e-03)	Tok/s 65848 (72292)	Loss/tok 3.0781 (3.3295)	LR 2.000e-03
0: TRAIN [2][460/968]	Time 0.312 (0.390)	Data 1.64e-04 (1.69e-03)	Tok/s 66043 (72307)	Loss/tok 3.1026 (3.3282)	LR 2.000e-03
0: TRAIN [2][470/968]	Time 0.423 (0.389)	Data 1.80e-04 (1.66e-03)	Tok/s 79973 (72284)	Loss/tok 3.2844 (3.3259)	LR 2.000e-03
0: TRAIN [2][480/968]	Time 0.313 (0.391)	Data 1.89e-04 (1.63e-03)	Tok/s 66072 (72434)	Loss/tok 3.1117 (3.3289)	LR 2.000e-03
0: TRAIN [2][490/968]	Time 0.424 (0.391)	Data 1.72e-04 (1.60e-03)	Tok/s 79573 (72483)	Loss/tok 3.2748 (3.3290)	LR 2.000e-03
0: TRAIN [2][500/968]	Time 0.426 (0.392)	Data 1.76e-04 (1.57e-03)	Tok/s 78383 (72534)	Loss/tok 3.3797 (3.3315)	LR 2.000e-03
0: TRAIN [2][510/968]	Time 0.310 (0.391)	Data 1.71e-04 (1.54e-03)	Tok/s 66271 (72465)	Loss/tok 3.0703 (3.3298)	LR 2.000e-03
0: TRAIN [2][520/968]	Time 0.205 (0.391)	Data 1.53e-04 (1.52e-03)	Tok/s 51475 (72445)	Loss/tok 2.6978 (3.3297)	LR 2.000e-03
0: TRAIN [2][530/968]	Time 0.546 (0.392)	Data 1.70e-04 (1.49e-03)	Tok/s 84628 (72551)	Loss/tok 3.4765 (3.3308)	LR 2.000e-03
0: TRAIN [2][540/968]	Time 0.424 (0.392)	Data 1.75e-04 (1.47e-03)	Tok/s 79109 (72538)	Loss/tok 3.3526 (3.3304)	LR 2.000e-03
0: TRAIN [2][550/968]	Time 0.423 (0.392)	Data 1.79e-04 (1.44e-03)	Tok/s 79501 (72531)	Loss/tok 3.2751 (3.3289)	LR 2.000e-03
0: TRAIN [2][560/968]	Time 0.311 (0.392)	Data 1.85e-04 (1.42e-03)	Tok/s 66464 (72542)	Loss/tok 3.0789 (3.3276)	LR 2.000e-03
0: TRAIN [2][570/968]	Time 0.425 (0.393)	Data 1.60e-04 (1.40e-03)	Tok/s 79191 (72624)	Loss/tok 3.4181 (3.3297)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][580/968]	Time 0.313 (0.392)	Data 1.54e-04 (1.38e-03)	Tok/s 66056 (72518)	Loss/tok 3.0188 (3.3283)	LR 2.000e-03
0: TRAIN [2][590/968]	Time 0.543 (0.391)	Data 1.58e-04 (1.36e-03)	Tok/s 85821 (72537)	Loss/tok 3.5283 (3.3271)	LR 2.000e-03
0: TRAIN [2][600/968]	Time 0.313 (0.391)	Data 1.71e-04 (1.34e-03)	Tok/s 66782 (72517)	Loss/tok 3.1558 (3.3252)	LR 2.000e-03
0: TRAIN [2][610/968]	Time 0.204 (0.391)	Data 1.68e-04 (1.32e-03)	Tok/s 51631 (72572)	Loss/tok 2.6229 (3.3258)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][620/968]	Time 0.312 (0.391)	Data 1.60e-04 (1.30e-03)	Tok/s 65704 (72547)	Loss/tok 3.1720 (3.3252)	LR 2.000e-03
0: TRAIN [2][630/968]	Time 0.312 (0.391)	Data 1.54e-04 (1.28e-03)	Tok/s 66198 (72611)	Loss/tok 3.0699 (3.3252)	LR 2.000e-03
0: TRAIN [2][640/968]	Time 0.423 (0.393)	Data 1.58e-04 (1.27e-03)	Tok/s 79776 (72742)	Loss/tok 3.3767 (3.3268)	LR 2.000e-03
0: TRAIN [2][650/968]	Time 0.205 (0.392)	Data 1.78e-04 (1.25e-03)	Tok/s 51470 (72677)	Loss/tok 2.6713 (3.3251)	LR 2.000e-03
0: TRAIN [2][660/968]	Time 0.424 (0.393)	Data 1.78e-04 (1.23e-03)	Tok/s 78870 (72747)	Loss/tok 3.3928 (3.3271)	LR 2.000e-03
0: TRAIN [2][670/968]	Time 0.428 (0.391)	Data 1.76e-04 (1.22e-03)	Tok/s 78472 (72619)	Loss/tok 3.2651 (3.3248)	LR 2.000e-03
0: TRAIN [2][680/968]	Time 0.312 (0.391)	Data 1.81e-04 (1.20e-03)	Tok/s 66285 (72571)	Loss/tok 3.0242 (3.3239)	LR 2.000e-03
0: TRAIN [2][690/968]	Time 0.422 (0.392)	Data 1.61e-04 (1.19e-03)	Tok/s 79670 (72624)	Loss/tok 3.3106 (3.3247)	LR 2.000e-03
0: TRAIN [2][700/968]	Time 0.311 (0.391)	Data 1.61e-04 (1.17e-03)	Tok/s 66530 (72638)	Loss/tok 3.0516 (3.3240)	LR 2.000e-03
0: TRAIN [2][710/968]	Time 0.312 (0.391)	Data 1.82e-04 (1.16e-03)	Tok/s 65456 (72615)	Loss/tok 3.0307 (3.3223)	LR 2.000e-03
0: TRAIN [2][720/968]	Time 0.311 (0.390)	Data 1.59e-04 (1.15e-03)	Tok/s 66571 (72590)	Loss/tok 3.1060 (3.3211)	LR 2.000e-03
0: TRAIN [2][730/968]	Time 0.310 (0.390)	Data 1.67e-04 (1.13e-03)	Tok/s 66693 (72588)	Loss/tok 3.1055 (3.3205)	LR 2.000e-03
0: TRAIN [2][740/968]	Time 0.310 (0.391)	Data 1.77e-04 (1.12e-03)	Tok/s 66633 (72620)	Loss/tok 3.0561 (3.3208)	LR 2.000e-03
0: TRAIN [2][750/968]	Time 0.311 (0.390)	Data 5.27e-04 (1.11e-03)	Tok/s 66707 (72588)	Loss/tok 3.1260 (3.3202)	LR 2.000e-03
0: TRAIN [2][760/968]	Time 0.421 (0.391)	Data 1.62e-04 (1.09e-03)	Tok/s 79532 (72627)	Loss/tok 3.3351 (3.3208)	LR 2.000e-03
0: TRAIN [2][770/968]	Time 0.424 (0.390)	Data 1.51e-04 (1.08e-03)	Tok/s 79247 (72565)	Loss/tok 3.3236 (3.3194)	LR 2.000e-03
0: TRAIN [2][780/968]	Time 0.426 (0.390)	Data 1.61e-04 (1.07e-03)	Tok/s 78815 (72582)	Loss/tok 3.2018 (3.3190)	LR 2.000e-03
0: TRAIN [2][790/968]	Time 0.312 (0.389)	Data 1.75e-04 (1.06e-03)	Tok/s 65825 (72544)	Loss/tok 3.1209 (3.3175)	LR 2.000e-03
0: TRAIN [2][800/968]	Time 0.313 (0.390)	Data 1.57e-04 (1.05e-03)	Tok/s 65645 (72573)	Loss/tok 3.1099 (3.3179)	LR 2.000e-03
0: TRAIN [2][810/968]	Time 0.545 (0.389)	Data 1.80e-04 (1.04e-03)	Tok/s 86323 (72565)	Loss/tok 3.4194 (3.3168)	LR 2.000e-03
0: TRAIN [2][820/968]	Time 0.424 (0.389)	Data 1.73e-04 (1.03e-03)	Tok/s 78891 (72546)	Loss/tok 3.3127 (3.3166)	LR 2.000e-03
0: TRAIN [2][830/968]	Time 0.314 (0.389)	Data 1.84e-04 (1.02e-03)	Tok/s 66132 (72490)	Loss/tok 3.1457 (3.3168)	LR 2.000e-03
0: TRAIN [2][840/968]	Time 0.313 (0.389)	Data 1.64e-04 (1.01e-03)	Tok/s 66608 (72523)	Loss/tok 2.9951 (3.3166)	LR 2.000e-03
0: TRAIN [2][850/968]	Time 0.312 (0.389)	Data 1.59e-04 (9.98e-04)	Tok/s 65935 (72479)	Loss/tok 3.0643 (3.3154)	LR 2.000e-03
0: TRAIN [2][860/968]	Time 0.543 (0.388)	Data 1.53e-04 (9.89e-04)	Tok/s 85447 (72475)	Loss/tok 3.4912 (3.3147)	LR 2.000e-03
0: TRAIN [2][870/968]	Time 0.424 (0.388)	Data 4.53e-04 (9.80e-04)	Tok/s 79439 (72452)	Loss/tok 3.3159 (3.3133)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][880/968]	Time 0.203 (0.387)	Data 1.77e-04 (9.70e-04)	Tok/s 51852 (72397)	Loss/tok 2.6999 (3.3121)	LR 2.000e-03
0: TRAIN [2][890/968]	Time 0.424 (0.388)	Data 1.90e-04 (9.62e-04)	Tok/s 79635 (72426)	Loss/tok 3.2657 (3.3121)	LR 2.000e-03
0: TRAIN [2][900/968]	Time 0.205 (0.388)	Data 1.89e-04 (9.53e-04)	Tok/s 51324 (72434)	Loss/tok 2.5720 (3.3115)	LR 2.000e-03
0: TRAIN [2][910/968]	Time 0.311 (0.387)	Data 1.75e-04 (9.44e-04)	Tok/s 67366 (72416)	Loss/tok 3.0422 (3.3102)	LR 2.000e-03
0: TRAIN [2][920/968]	Time 0.424 (0.388)	Data 1.51e-04 (9.36e-04)	Tok/s 80451 (72450)	Loss/tok 3.2948 (3.3108)	LR 2.000e-03
0: TRAIN [2][930/968]	Time 0.310 (0.388)	Data 1.81e-04 (9.28e-04)	Tok/s 65765 (72498)	Loss/tok 3.0338 (3.3111)	LR 2.000e-03
0: TRAIN [2][940/968]	Time 0.422 (0.388)	Data 1.66e-04 (9.20e-04)	Tok/s 79847 (72512)	Loss/tok 3.2164 (3.3108)	LR 2.000e-03
0: TRAIN [2][950/968]	Time 0.314 (0.388)	Data 1.69e-04 (9.12e-04)	Tok/s 66030 (72515)	Loss/tok 3.0997 (3.3110)	LR 2.000e-03
0: TRAIN [2][960/968]	Time 0.311 (0.388)	Data 1.66e-04 (9.04e-04)	Tok/s 66575 (72484)	Loss/tok 3.0574 (3.3106)	LR 2.000e-03
:::MLL 1571840218.269 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1571840218.269 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.587 (0.587)	Decoder iters 88.0 (88.0)	Tok/s 27143 (27143)
0: Running moses detokenizer
0: BLEU(score=22.367004466005227, counts=[35816, 17290, 9555, 5492], totals=[63919, 60916, 57913, 54915], precisions=[56.03341729376242, 28.383347560575217, 16.49888626042512, 10.00091049804243], bp=0.9882267396993468, sys_len=63919, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571840220.057 eval_accuracy: {"value": 22.37, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1571840220.058 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.3108	Test BLEU: 22.37
0: Performance: Epoch: 2	Training: 580059 Tok/s
0: Finished epoch 2
:::MLL 1571840220.058 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1571840220.058 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571840220.059 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 892875209
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][0/968]	Time 1.279 (1.279)	Data 7.20e-01 (7.20e-01)	Tok/s 36398 (36398)	Loss/tok 3.3459 (3.3459)	LR 2.000e-03
0: TRAIN [3][10/968]	Time 0.310 (0.470)	Data 1.52e-04 (6.56e-02)	Tok/s 64990 (71283)	Loss/tok 2.9651 (3.2081)	LR 2.000e-03
0: TRAIN [3][20/968]	Time 0.312 (0.463)	Data 1.97e-04 (3.45e-02)	Tok/s 66901 (73136)	Loss/tok 3.0598 (3.2545)	LR 2.000e-03
0: TRAIN [3][30/968]	Time 0.311 (0.448)	Data 1.96e-04 (2.34e-02)	Tok/s 66863 (73903)	Loss/tok 3.0505 (3.2437)	LR 2.000e-03
0: TRAIN [3][40/968]	Time 0.207 (0.441)	Data 1.67e-04 (1.78e-02)	Tok/s 50742 (73884)	Loss/tok 2.6143 (3.2458)	LR 2.000e-03
0: TRAIN [3][50/968]	Time 0.543 (0.448)	Data 1.97e-04 (1.43e-02)	Tok/s 86488 (74829)	Loss/tok 3.4566 (3.2796)	LR 2.000e-03
0: TRAIN [3][60/968]	Time 0.424 (0.443)	Data 1.65e-04 (1.20e-02)	Tok/s 79477 (74921)	Loss/tok 3.3016 (3.2807)	LR 2.000e-03
0: TRAIN [3][70/968]	Time 0.311 (0.431)	Data 1.81e-04 (1.03e-02)	Tok/s 66630 (74302)	Loss/tok 2.9687 (3.2656)	LR 2.000e-03
0: TRAIN [3][80/968]	Time 0.423 (0.421)	Data 1.75e-04 (9.09e-03)	Tok/s 79211 (73716)	Loss/tok 3.2425 (3.2533)	LR 2.000e-03
0: TRAIN [3][90/968]	Time 0.542 (0.419)	Data 1.61e-04 (8.11e-03)	Tok/s 85839 (73791)	Loss/tok 3.3771 (3.2510)	LR 2.000e-03
0: TRAIN [3][100/968]	Time 0.543 (0.424)	Data 1.85e-04 (7.33e-03)	Tok/s 85554 (74062)	Loss/tok 3.3624 (3.2645)	LR 2.000e-03
0: TRAIN [3][110/968]	Time 0.543 (0.419)	Data 1.93e-04 (6.68e-03)	Tok/s 85975 (73723)	Loss/tok 3.3902 (3.2599)	LR 2.000e-03
0: TRAIN [3][120/968]	Time 0.423 (0.422)	Data 1.79e-04 (6.15e-03)	Tok/s 79777 (73970)	Loss/tok 3.2379 (3.2677)	LR 2.000e-03
0: TRAIN [3][130/968]	Time 0.424 (0.425)	Data 1.91e-04 (5.69e-03)	Tok/s 79568 (74097)	Loss/tok 3.2389 (3.2770)	LR 2.000e-03
0: TRAIN [3][140/968]	Time 0.426 (0.424)	Data 1.70e-04 (5.30e-03)	Tok/s 79140 (74133)	Loss/tok 3.1999 (3.2772)	LR 2.000e-03
0: TRAIN [3][150/968]	Time 0.310 (0.420)	Data 1.65e-04 (4.96e-03)	Tok/s 65436 (73916)	Loss/tok 3.0180 (3.2700)	LR 2.000e-03
0: TRAIN [3][160/968]	Time 0.423 (0.419)	Data 1.58e-04 (4.67e-03)	Tok/s 79366 (73915)	Loss/tok 3.1626 (3.2694)	LR 2.000e-03
0: TRAIN [3][170/968]	Time 0.542 (0.418)	Data 1.66e-04 (4.40e-03)	Tok/s 85473 (73891)	Loss/tok 3.4147 (3.2671)	LR 2.000e-03
0: TRAIN [3][180/968]	Time 0.425 (0.420)	Data 1.93e-04 (4.17e-03)	Tok/s 78803 (74117)	Loss/tok 3.1925 (3.2714)	LR 2.000e-03
0: TRAIN [3][190/968]	Time 0.425 (0.418)	Data 2.07e-04 (3.96e-03)	Tok/s 79519 (74062)	Loss/tok 3.2952 (3.2685)	LR 2.000e-03
0: TRAIN [3][200/968]	Time 0.312 (0.417)	Data 1.77e-04 (3.78e-03)	Tok/s 65564 (74105)	Loss/tok 3.0329 (3.2663)	LR 2.000e-03
0: TRAIN [3][210/968]	Time 0.423 (0.416)	Data 1.88e-04 (3.61e-03)	Tok/s 78934 (74091)	Loss/tok 3.1663 (3.2636)	LR 2.000e-03
0: TRAIN [3][220/968]	Time 0.425 (0.416)	Data 1.88e-04 (3.45e-03)	Tok/s 78862 (74190)	Loss/tok 3.1959 (3.2624)	LR 2.000e-03
0: TRAIN [3][230/968]	Time 0.425 (0.414)	Data 1.75e-04 (3.31e-03)	Tok/s 79452 (74068)	Loss/tok 3.1920 (3.2576)	LR 2.000e-03
0: TRAIN [3][240/968]	Time 0.425 (0.411)	Data 2.11e-04 (3.18e-03)	Tok/s 78742 (73911)	Loss/tok 3.1994 (3.2544)	LR 2.000e-03
0: TRAIN [3][250/968]	Time 0.424 (0.411)	Data 1.87e-04 (3.06e-03)	Tok/s 79252 (73862)	Loss/tok 3.1616 (3.2534)	LR 2.000e-03
0: TRAIN [3][260/968]	Time 0.542 (0.409)	Data 1.60e-04 (2.95e-03)	Tok/s 85232 (73778)	Loss/tok 3.4010 (3.2510)	LR 2.000e-03
0: TRAIN [3][270/968]	Time 0.424 (0.408)	Data 2.31e-04 (2.85e-03)	Tok/s 79251 (73825)	Loss/tok 3.2912 (3.2482)	LR 2.000e-03
0: TRAIN [3][280/968]	Time 0.204 (0.407)	Data 1.74e-04 (2.75e-03)	Tok/s 50805 (73772)	Loss/tok 2.5195 (3.2452)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][290/968]	Time 0.311 (0.407)	Data 1.81e-04 (2.67e-03)	Tok/s 65824 (73665)	Loss/tok 3.0497 (3.2478)	LR 2.000e-03
0: TRAIN [3][300/968]	Time 0.424 (0.404)	Data 1.61e-04 (2.58e-03)	Tok/s 79598 (73458)	Loss/tok 3.2471 (3.2441)	LR 2.000e-03
0: TRAIN [3][310/968]	Time 0.312 (0.403)	Data 1.70e-04 (2.51e-03)	Tok/s 66244 (73388)	Loss/tok 2.9579 (3.2430)	LR 2.000e-03
0: TRAIN [3][320/968]	Time 0.205 (0.403)	Data 1.97e-04 (2.43e-03)	Tok/s 51803 (73388)	Loss/tok 2.5884 (3.2436)	LR 2.000e-03
0: TRAIN [3][330/968]	Time 0.313 (0.401)	Data 1.75e-04 (2.37e-03)	Tok/s 65861 (73226)	Loss/tok 3.0177 (3.2403)	LR 2.000e-03
0: TRAIN [3][340/968]	Time 0.205 (0.401)	Data 1.67e-04 (2.30e-03)	Tok/s 51795 (73239)	Loss/tok 2.6410 (3.2418)	LR 2.000e-03
0: TRAIN [3][350/968]	Time 0.424 (0.399)	Data 4.55e-04 (2.24e-03)	Tok/s 79928 (73085)	Loss/tok 3.2705 (3.2398)	LR 2.000e-03
0: TRAIN [3][360/968]	Time 0.423 (0.399)	Data 1.59e-04 (2.18e-03)	Tok/s 79330 (73087)	Loss/tok 3.1431 (3.2383)	LR 2.000e-03
0: TRAIN [3][370/968]	Time 0.424 (0.397)	Data 1.72e-04 (2.13e-03)	Tok/s 78456 (72970)	Loss/tok 3.1902 (3.2355)	LR 2.000e-03
0: TRAIN [3][380/968]	Time 0.423 (0.397)	Data 1.87e-04 (2.08e-03)	Tok/s 78830 (72927)	Loss/tok 3.2573 (3.2352)	LR 2.000e-03
0: TRAIN [3][390/968]	Time 0.422 (0.398)	Data 3.49e-04 (2.03e-03)	Tok/s 79857 (73017)	Loss/tok 3.1655 (3.2362)	LR 2.000e-03
0: TRAIN [3][400/968]	Time 0.312 (0.396)	Data 1.71e-04 (1.98e-03)	Tok/s 65922 (72925)	Loss/tok 3.0504 (3.2338)	LR 2.000e-03
0: TRAIN [3][410/968]	Time 0.207 (0.396)	Data 2.02e-04 (1.94e-03)	Tok/s 50626 (72861)	Loss/tok 2.6934 (3.2345)	LR 2.000e-03
0: TRAIN [3][420/968]	Time 0.310 (0.396)	Data 1.64e-04 (1.90e-03)	Tok/s 66440 (72890)	Loss/tok 2.9388 (3.2335)	LR 2.000e-03
0: TRAIN [3][430/968]	Time 0.311 (0.397)	Data 1.80e-04 (1.86e-03)	Tok/s 66957 (72944)	Loss/tok 3.0607 (3.2353)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][440/968]	Time 0.543 (0.396)	Data 1.74e-04 (1.82e-03)	Tok/s 85428 (72915)	Loss/tok 3.3599 (3.2337)	LR 2.000e-03
0: TRAIN [3][450/968]	Time 0.311 (0.396)	Data 1.99e-04 (1.79e-03)	Tok/s 66918 (72927)	Loss/tok 3.1203 (3.2326)	LR 2.000e-03
0: TRAIN [3][460/968]	Time 0.424 (0.395)	Data 1.73e-04 (1.75e-03)	Tok/s 80091 (72953)	Loss/tok 3.1852 (3.2323)	LR 2.000e-03
0: TRAIN [3][470/968]	Time 0.684 (0.396)	Data 1.65e-04 (1.72e-03)	Tok/s 86999 (73042)	Loss/tok 3.6117 (3.2332)	LR 2.000e-03
0: TRAIN [3][480/968]	Time 0.205 (0.395)	Data 1.83e-04 (1.68e-03)	Tok/s 51433 (72954)	Loss/tok 2.5721 (3.2329)	LR 2.000e-03
0: TRAIN [3][490/968]	Time 0.204 (0.394)	Data 1.62e-04 (1.65e-03)	Tok/s 51830 (72867)	Loss/tok 2.5941 (3.2313)	LR 2.000e-03
0: TRAIN [3][500/968]	Time 0.427 (0.395)	Data 1.65e-04 (1.63e-03)	Tok/s 79145 (72887)	Loss/tok 3.2307 (3.2328)	LR 2.000e-03
0: TRAIN [3][510/968]	Time 0.423 (0.395)	Data 1.65e-04 (1.60e-03)	Tok/s 79285 (72930)	Loss/tok 3.1814 (3.2324)	LR 2.000e-03
0: TRAIN [3][520/968]	Time 0.205 (0.394)	Data 1.62e-04 (1.57e-03)	Tok/s 51675 (72886)	Loss/tok 2.6476 (3.2313)	LR 2.000e-03
0: TRAIN [3][530/968]	Time 0.311 (0.394)	Data 1.61e-04 (1.54e-03)	Tok/s 67636 (72886)	Loss/tok 3.0413 (3.2303)	LR 2.000e-03
0: TRAIN [3][540/968]	Time 0.542 (0.394)	Data 1.73e-04 (1.52e-03)	Tok/s 87039 (72866)	Loss/tok 3.4319 (3.2302)	LR 2.000e-03
0: TRAIN [3][550/968]	Time 0.311 (0.393)	Data 1.74e-04 (1.50e-03)	Tok/s 67087 (72787)	Loss/tok 2.9623 (3.2283)	LR 2.000e-03
0: TRAIN [3][560/968]	Time 0.312 (0.391)	Data 1.60e-04 (1.47e-03)	Tok/s 67251 (72644)	Loss/tok 3.0924 (3.2258)	LR 2.000e-03
0: TRAIN [3][570/968]	Time 0.312 (0.390)	Data 2.12e-04 (1.45e-03)	Tok/s 65807 (72608)	Loss/tok 2.9607 (3.2244)	LR 2.000e-03
0: TRAIN [3][580/968]	Time 0.312 (0.390)	Data 1.74e-04 (1.43e-03)	Tok/s 65211 (72612)	Loss/tok 2.9748 (3.2238)	LR 2.000e-03
0: TRAIN [3][590/968]	Time 0.204 (0.390)	Data 1.65e-04 (1.41e-03)	Tok/s 51918 (72582)	Loss/tok 2.5709 (3.2237)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][600/968]	Time 0.543 (0.390)	Data 1.62e-04 (1.39e-03)	Tok/s 86237 (72618)	Loss/tok 3.4368 (3.2238)	LR 2.000e-03
0: TRAIN [3][610/968]	Time 0.311 (0.390)	Data 1.72e-04 (1.37e-03)	Tok/s 65688 (72619)	Loss/tok 3.0529 (3.2239)	LR 2.000e-03
0: TRAIN [3][620/968]	Time 0.311 (0.389)	Data 1.90e-04 (1.35e-03)	Tok/s 66480 (72548)	Loss/tok 3.0009 (3.2227)	LR 2.000e-03
0: TRAIN [3][630/968]	Time 0.424 (0.390)	Data 1.92e-04 (1.33e-03)	Tok/s 79219 (72641)	Loss/tok 3.2242 (3.2231)	LR 2.000e-03
0: TRAIN [3][640/968]	Time 0.313 (0.390)	Data 1.95e-04 (1.31e-03)	Tok/s 64946 (72688)	Loss/tok 3.0173 (3.2233)	LR 2.000e-03
0: TRAIN [3][650/968]	Time 0.422 (0.390)	Data 1.53e-04 (1.30e-03)	Tok/s 79976 (72718)	Loss/tok 3.1872 (3.2232)	LR 2.000e-03
0: TRAIN [3][660/968]	Time 0.311 (0.389)	Data 1.56e-04 (1.28e-03)	Tok/s 66027 (72676)	Loss/tok 3.0381 (3.2217)	LR 2.000e-03
0: TRAIN [3][670/968]	Time 0.546 (0.389)	Data 1.70e-04 (1.26e-03)	Tok/s 85653 (72659)	Loss/tok 3.4304 (3.2208)	LR 2.000e-03
0: TRAIN [3][680/968]	Time 0.204 (0.389)	Data 1.95e-04 (1.25e-03)	Tok/s 52143 (72609)	Loss/tok 2.5758 (3.2216)	LR 2.000e-03
0: TRAIN [3][690/968]	Time 0.425 (0.390)	Data 1.72e-04 (1.23e-03)	Tok/s 79515 (72651)	Loss/tok 3.2406 (3.2219)	LR 2.000e-03
0: TRAIN [3][700/968]	Time 0.546 (0.391)	Data 1.90e-04 (1.22e-03)	Tok/s 85675 (72722)	Loss/tok 3.3559 (3.2243)	LR 2.000e-03
0: TRAIN [3][710/968]	Time 0.685 (0.392)	Data 1.90e-04 (1.20e-03)	Tok/s 86486 (72811)	Loss/tok 3.5994 (3.2273)	LR 2.000e-03
0: TRAIN [3][720/968]	Time 0.423 (0.392)	Data 2.04e-04 (1.19e-03)	Tok/s 79548 (72877)	Loss/tok 3.1413 (3.2274)	LR 2.000e-03
0: TRAIN [3][730/968]	Time 0.684 (0.392)	Data 1.64e-04 (1.17e-03)	Tok/s 86404 (72841)	Loss/tok 3.6629 (3.2274)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][740/968]	Time 0.313 (0.392)	Data 1.77e-04 (1.16e-03)	Tok/s 66677 (72859)	Loss/tok 3.0917 (3.2279)	LR 2.000e-03
0: TRAIN [3][750/968]	Time 0.205 (0.392)	Data 1.79e-04 (1.15e-03)	Tok/s 51502 (72847)	Loss/tok 2.6135 (3.2277)	LR 2.000e-03
0: TRAIN [3][760/968]	Time 0.312 (0.392)	Data 1.95e-04 (1.14e-03)	Tok/s 64970 (72804)	Loss/tok 3.0355 (3.2274)	LR 2.000e-03
0: TRAIN [3][770/968]	Time 0.542 (0.392)	Data 1.61e-04 (1.12e-03)	Tok/s 86112 (72837)	Loss/tok 3.3345 (3.2272)	LR 2.000e-03
0: TRAIN [3][780/968]	Time 0.310 (0.392)	Data 1.82e-04 (1.11e-03)	Tok/s 66736 (72838)	Loss/tok 3.0413 (3.2261)	LR 2.000e-03
0: TRAIN [3][790/968]	Time 0.314 (0.391)	Data 1.67e-04 (1.10e-03)	Tok/s 66255 (72792)	Loss/tok 3.0347 (3.2251)	LR 2.000e-03
0: TRAIN [3][800/968]	Time 0.686 (0.391)	Data 1.63e-04 (1.09e-03)	Tok/s 86443 (72819)	Loss/tok 3.6284 (3.2258)	LR 2.000e-03
0: TRAIN [3][810/968]	Time 0.312 (0.391)	Data 1.93e-04 (1.08e-03)	Tok/s 64918 (72772)	Loss/tok 3.0565 (3.2246)	LR 2.000e-03
0: TRAIN [3][820/968]	Time 0.204 (0.391)	Data 1.53e-04 (1.07e-03)	Tok/s 50398 (72739)	Loss/tok 2.6040 (3.2252)	LR 2.000e-03
0: TRAIN [3][830/968]	Time 0.312 (0.390)	Data 1.76e-04 (1.06e-03)	Tok/s 65004 (72725)	Loss/tok 3.0658 (3.2246)	LR 2.000e-03
0: TRAIN [3][840/968]	Time 0.687 (0.390)	Data 1.85e-04 (1.05e-03)	Tok/s 86756 (72707)	Loss/tok 3.5699 (3.2245)	LR 2.000e-03
0: TRAIN [3][850/968]	Time 0.426 (0.390)	Data 1.54e-04 (1.04e-03)	Tok/s 78466 (72705)	Loss/tok 3.1768 (3.2237)	LR 2.000e-03
0: TRAIN [3][860/968]	Time 0.310 (0.390)	Data 1.76e-04 (1.03e-03)	Tok/s 65902 (72667)	Loss/tok 2.9822 (3.2226)	LR 2.000e-03
0: TRAIN [3][870/968]	Time 0.543 (0.390)	Data 1.88e-04 (1.02e-03)	Tok/s 85353 (72662)	Loss/tok 3.4085 (3.2229)	LR 2.000e-03
0: TRAIN [3][880/968]	Time 0.423 (0.390)	Data 1.89e-04 (1.01e-03)	Tok/s 79887 (72698)	Loss/tok 3.1995 (3.2230)	LR 2.000e-03
0: TRAIN [3][890/968]	Time 0.312 (0.390)	Data 1.87e-04 (9.98e-04)	Tok/s 65604 (72725)	Loss/tok 3.0866 (3.2231)	LR 2.000e-03
0: TRAIN [3][900/968]	Time 0.310 (0.390)	Data 2.07e-04 (9.89e-04)	Tok/s 67422 (72688)	Loss/tok 3.0921 (3.2225)	LR 2.000e-03
0: TRAIN [3][910/968]	Time 0.543 (0.390)	Data 1.76e-04 (9.80e-04)	Tok/s 85570 (72664)	Loss/tok 3.4115 (3.2227)	LR 2.000e-03
0: TRAIN [3][920/968]	Time 0.312 (0.389)	Data 1.76e-04 (9.71e-04)	Tok/s 66045 (72647)	Loss/tok 2.9637 (3.2224)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][930/968]	Time 0.423 (0.389)	Data 1.70e-04 (9.63e-04)	Tok/s 79467 (72581)	Loss/tok 3.2333 (3.2220)	LR 2.000e-03
0: TRAIN [3][940/968]	Time 0.311 (0.388)	Data 1.96e-04 (9.54e-04)	Tok/s 67719 (72535)	Loss/tok 2.9971 (3.2210)	LR 2.000e-03
0: TRAIN [3][950/968]	Time 0.544 (0.388)	Data 1.58e-04 (9.46e-04)	Tok/s 85965 (72506)	Loss/tok 3.3553 (3.2203)	LR 2.000e-03
0: TRAIN [3][960/968]	Time 0.545 (0.388)	Data 1.77e-04 (9.39e-04)	Tok/s 85682 (72487)	Loss/tok 3.4577 (3.2202)	LR 2.000e-03
:::MLL 1571840597.773 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 524}}
:::MLL 1571840597.773 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.628 (0.628)	Decoder iters 97.0 (97.0)	Tok/s 25763 (25763)
0: Running moses detokenizer
0: BLEU(score=22.99746349619975, counts=[36367, 17740, 9920, 5813], totals=[64986, 61983, 58980, 55983], precisions=[55.96128396885483, 28.6207508510398, 16.819260766361477, 10.383509279602736], bp=1.0, sys_len=64986, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571840599.622 eval_accuracy: {"value": 23.0, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 535}}
:::MLL 1571840599.622 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 3	Training Loss: 3.2195	Test BLEU: 23.00
0: Performance: Epoch: 3	Training: 580122 Tok/s
0: Finished epoch 3
:::MLL 1571840599.622 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 557}}
:::MLL 1571840599.623 block_start: {"value": null, "metadata": {"first_epoch_num": 5, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571840599.623 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 514}}
0: Starting epoch 4
0: Executing preallocation
0: Sampler for epoch 4 uses seed 3217212273
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [4][0/968]	Time 1.168 (1.168)	Data 7.18e-01 (7.18e-01)	Tok/s 28531 (28531)	Loss/tok 3.1598 (3.1598)	LR 2.000e-03
0: TRAIN [4][10/968]	Time 0.425 (0.505)	Data 1.91e-04 (6.54e-02)	Tok/s 78985 (70718)	Loss/tok 3.1851 (3.1876)	LR 2.000e-03
0: TRAIN [4][20/968]	Time 0.424 (0.424)	Data 1.88e-04 (3.44e-02)	Tok/s 79853 (69701)	Loss/tok 3.0684 (3.1172)	LR 2.000e-03
0: TRAIN [4][30/968]	Time 0.312 (0.407)	Data 1.85e-04 (2.34e-02)	Tok/s 66069 (70036)	Loss/tok 2.9317 (3.1155)	LR 2.000e-03
0: TRAIN [4][40/968]	Time 0.311 (0.413)	Data 1.52e-04 (1.77e-02)	Tok/s 66791 (71442)	Loss/tok 2.9836 (3.1421)	LR 2.000e-03
0: TRAIN [4][50/968]	Time 0.683 (0.431)	Data 1.65e-04 (1.43e-02)	Tok/s 86667 (73185)	Loss/tok 3.4964 (3.1823)	LR 2.000e-03
0: TRAIN [4][60/968]	Time 0.310 (0.418)	Data 1.65e-04 (1.20e-02)	Tok/s 65662 (72903)	Loss/tok 2.9039 (3.1637)	LR 2.000e-03
0: TRAIN [4][70/968]	Time 0.423 (0.412)	Data 2.02e-04 (1.03e-02)	Tok/s 79985 (72621)	Loss/tok 3.0166 (3.1584)	LR 2.000e-03
0: TRAIN [4][80/968]	Time 0.422 (0.406)	Data 1.70e-04 (9.05e-03)	Tok/s 80206 (72540)	Loss/tok 3.1371 (3.1486)	LR 2.000e-03
0: TRAIN [4][90/968]	Time 0.312 (0.413)	Data 1.94e-04 (8.08e-03)	Tok/s 66693 (73209)	Loss/tok 2.9405 (3.1646)	LR 2.000e-03
0: TRAIN [4][100/968]	Time 0.311 (0.411)	Data 1.60e-04 (7.29e-03)	Tok/s 65099 (73225)	Loss/tok 2.9686 (3.1633)	LR 2.000e-03
0: TRAIN [4][110/968]	Time 0.206 (0.409)	Data 1.84e-04 (6.66e-03)	Tok/s 50703 (73173)	Loss/tok 2.5423 (3.1632)	LR 2.000e-03
0: TRAIN [4][120/968]	Time 0.312 (0.408)	Data 1.64e-04 (6.13e-03)	Tok/s 66852 (73147)	Loss/tok 2.9589 (3.1638)	LR 2.000e-03
0: TRAIN [4][130/968]	Time 0.311 (0.404)	Data 1.65e-04 (5.67e-03)	Tok/s 67340 (73015)	Loss/tok 2.9541 (3.1598)	LR 2.000e-03
0: TRAIN [4][140/968]	Time 0.313 (0.400)	Data 1.78e-04 (5.28e-03)	Tok/s 67076 (72837)	Loss/tok 2.9320 (3.1535)	LR 2.000e-03
0: TRAIN [4][150/968]	Time 0.545 (0.399)	Data 1.78e-04 (4.95e-03)	Tok/s 85710 (72927)	Loss/tok 3.3453 (3.1529)	LR 2.000e-03
0: TRAIN [4][160/968]	Time 0.311 (0.398)	Data 1.91e-04 (4.65e-03)	Tok/s 67795 (72847)	Loss/tok 2.9172 (3.1508)	LR 2.000e-03
0: TRAIN [4][170/968]	Time 0.311 (0.398)	Data 1.60e-04 (4.39e-03)	Tok/s 66431 (72884)	Loss/tok 2.9319 (3.1544)	LR 2.000e-03
0: TRAIN [4][180/968]	Time 0.310 (0.395)	Data 2.05e-04 (4.16e-03)	Tok/s 66043 (72683)	Loss/tok 3.0355 (3.1504)	LR 2.000e-03
0: TRAIN [4][190/968]	Time 0.311 (0.393)	Data 1.76e-04 (3.95e-03)	Tok/s 67885 (72518)	Loss/tok 2.9864 (3.1464)	LR 2.000e-03
0: TRAIN [4][200/968]	Time 0.423 (0.393)	Data 1.91e-04 (3.76e-03)	Tok/s 79316 (72685)	Loss/tok 3.1825 (3.1477)	LR 2.000e-03
0: TRAIN [4][210/968]	Time 0.542 (0.391)	Data 1.67e-04 (3.59e-03)	Tok/s 86932 (72479)	Loss/tok 3.2422 (3.1441)	LR 2.000e-03
0: TRAIN [4][220/968]	Time 0.424 (0.392)	Data 1.83e-04 (3.44e-03)	Tok/s 78641 (72652)	Loss/tok 3.1263 (3.1451)	LR 2.000e-03
0: TRAIN [4][230/968]	Time 0.424 (0.393)	Data 1.72e-04 (3.30e-03)	Tok/s 79813 (72716)	Loss/tok 3.0930 (3.1475)	LR 2.000e-03
0: TRAIN [4][240/968]	Time 0.313 (0.393)	Data 1.68e-04 (3.17e-03)	Tok/s 65972 (72788)	Loss/tok 2.9544 (3.1481)	LR 2.000e-03
0: TRAIN [4][250/968]	Time 0.426 (0.393)	Data 1.54e-04 (3.05e-03)	Tok/s 78758 (72813)	Loss/tok 3.1063 (3.1474)	LR 2.000e-03
0: TRAIN [4][260/968]	Time 0.545 (0.394)	Data 1.73e-04 (2.94e-03)	Tok/s 86039 (72951)	Loss/tok 3.2756 (3.1496)	LR 2.000e-03
0: TRAIN [4][270/968]	Time 0.544 (0.395)	Data 1.66e-04 (2.84e-03)	Tok/s 85838 (72964)	Loss/tok 3.2827 (3.1521)	LR 2.000e-03
0: TRAIN [4][280/968]	Time 0.424 (0.395)	Data 1.66e-04 (2.75e-03)	Tok/s 78736 (72945)	Loss/tok 3.1909 (3.1548)	LR 2.000e-03
0: TRAIN [4][290/968]	Time 0.313 (0.393)	Data 1.64e-04 (2.66e-03)	Tok/s 66535 (72768)	Loss/tok 3.0015 (3.1522)	LR 2.000e-03
0: TRAIN [4][300/968]	Time 0.423 (0.392)	Data 1.90e-04 (2.58e-03)	Tok/s 79527 (72761)	Loss/tok 3.1389 (3.1495)	LR 2.000e-03
0: TRAIN [4][310/968]	Time 0.426 (0.392)	Data 1.58e-04 (2.50e-03)	Tok/s 79142 (72820)	Loss/tok 3.1595 (3.1487)	LR 2.000e-03
0: TRAIN [4][320/968]	Time 0.424 (0.392)	Data 1.63e-04 (2.43e-03)	Tok/s 79158 (72781)	Loss/tok 3.2317 (3.1480)	LR 2.000e-03
0: TRAIN [4][330/968]	Time 0.423 (0.392)	Data 1.81e-04 (2.36e-03)	Tok/s 79691 (72871)	Loss/tok 3.1203 (3.1495)	LR 2.000e-03
0: TRAIN [4][340/968]	Time 0.313 (0.392)	Data 1.81e-04 (2.30e-03)	Tok/s 65566 (72837)	Loss/tok 2.9805 (3.1515)	LR 2.000e-03
0: TRAIN [4][350/968]	Time 0.310 (0.393)	Data 1.59e-04 (2.24e-03)	Tok/s 67858 (72922)	Loss/tok 3.0124 (3.1529)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [4][360/968]	Time 0.683 (0.395)	Data 1.56e-04 (2.18e-03)	Tok/s 87277 (73008)	Loss/tok 3.5887 (3.1566)	LR 2.000e-03
0: TRAIN [4][370/968]	Time 0.425 (0.394)	Data 1.83e-04 (2.12e-03)	Tok/s 79396 (72895)	Loss/tok 3.1572 (3.1563)	LR 2.000e-03
0: TRAIN [4][380/968]	Time 0.545 (0.392)	Data 1.58e-04 (2.07e-03)	Tok/s 85578 (72796)	Loss/tok 3.3837 (3.1552)	LR 2.000e-03
0: TRAIN [4][390/968]	Time 0.313 (0.393)	Data 1.60e-04 (2.03e-03)	Tok/s 66204 (72831)	Loss/tok 2.9864 (3.1574)	LR 2.000e-03
0: TRAIN [4][400/968]	Time 0.543 (0.393)	Data 1.73e-04 (1.98e-03)	Tok/s 86208 (72875)	Loss/tok 3.3811 (3.1588)	LR 2.000e-03
0: TRAIN [4][410/968]	Time 0.544 (0.393)	Data 2.15e-04 (1.94e-03)	Tok/s 85894 (72882)	Loss/tok 3.2633 (3.1585)	LR 2.000e-03
0: TRAIN [4][420/968]	Time 0.312 (0.393)	Data 1.56e-04 (1.89e-03)	Tok/s 66947 (72885)	Loss/tok 2.9772 (3.1588)	LR 2.000e-03
0: TRAIN [4][430/968]	Time 0.545 (0.393)	Data 1.65e-04 (1.85e-03)	Tok/s 85952 (72855)	Loss/tok 3.3850 (3.1599)	LR 2.000e-03
0: TRAIN [4][440/968]	Time 0.422 (0.392)	Data 2.04e-04 (1.82e-03)	Tok/s 79919 (72773)	Loss/tok 3.1483 (3.1591)	LR 2.000e-03
0: TRAIN [4][450/968]	Time 0.312 (0.393)	Data 1.77e-04 (1.78e-03)	Tok/s 65690 (72826)	Loss/tok 2.9853 (3.1618)	LR 2.000e-03
0: TRAIN [4][460/968]	Time 0.426 (0.392)	Data 1.92e-04 (1.75e-03)	Tok/s 78435 (72769)	Loss/tok 3.1788 (3.1611)	LR 2.000e-03
0: TRAIN [4][470/968]	Time 0.686 (0.393)	Data 1.92e-04 (1.71e-03)	Tok/s 86585 (72755)	Loss/tok 3.5475 (3.1643)	LR 2.000e-03
0: TRAIN [4][480/968]	Time 0.313 (0.393)	Data 1.60e-04 (1.68e-03)	Tok/s 66008 (72695)	Loss/tok 2.9322 (3.1642)	LR 2.000e-03
0: TRAIN [4][490/968]	Time 0.313 (0.392)	Data 1.53e-04 (1.65e-03)	Tok/s 65926 (72694)	Loss/tok 2.9590 (3.1640)	LR 2.000e-03
0: TRAIN [4][500/968]	Time 0.313 (0.392)	Data 1.56e-04 (1.62e-03)	Tok/s 66175 (72670)	Loss/tok 2.8846 (3.1644)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [4][510/968]	Time 0.310 (0.392)	Data 1.84e-04 (1.59e-03)	Tok/s 66344 (72700)	Loss/tok 2.8711 (3.1656)	LR 2.000e-03
0: TRAIN [4][520/968]	Time 0.205 (0.392)	Data 1.82e-04 (1.57e-03)	Tok/s 52099 (72647)	Loss/tok 2.5996 (3.1649)	LR 2.000e-03
0: TRAIN [4][530/968]	Time 0.423 (0.390)	Data 1.89e-04 (1.54e-03)	Tok/s 79093 (72496)	Loss/tok 3.2276 (3.1626)	LR 2.000e-03
0: TRAIN [4][540/968]	Time 0.685 (0.390)	Data 1.76e-04 (1.52e-03)	Tok/s 86423 (72460)	Loss/tok 3.5488 (3.1626)	LR 2.000e-03
0: TRAIN [4][550/968]	Time 0.425 (0.388)	Data 1.99e-04 (1.49e-03)	Tok/s 79306 (72321)	Loss/tok 3.1646 (3.1611)	LR 2.000e-03
0: TRAIN [4][560/968]	Time 0.423 (0.389)	Data 1.84e-04 (1.47e-03)	Tok/s 78825 (72386)	Loss/tok 3.2049 (3.1617)	LR 2.000e-03
0: TRAIN [4][570/968]	Time 0.313 (0.388)	Data 1.98e-04 (1.45e-03)	Tok/s 66199 (72363)	Loss/tok 2.9119 (3.1613)	LR 2.000e-03
0: TRAIN [4][580/968]	Time 0.310 (0.389)	Data 1.95e-04 (1.42e-03)	Tok/s 65620 (72398)	Loss/tok 2.8839 (3.1614)	LR 2.000e-03
0: TRAIN [4][590/968]	Time 0.424 (0.388)	Data 1.78e-04 (1.40e-03)	Tok/s 78657 (72317)	Loss/tok 3.1792 (3.1595)	LR 2.000e-03
0: TRAIN [4][600/968]	Time 0.311 (0.387)	Data 1.76e-04 (1.38e-03)	Tok/s 67035 (72300)	Loss/tok 3.0469 (3.1593)	LR 2.000e-03
0: TRAIN [4][610/968]	Time 0.546 (0.389)	Data 1.85e-04 (1.36e-03)	Tok/s 85428 (72455)	Loss/tok 3.2167 (3.1624)	LR 2.000e-03
0: TRAIN [4][620/968]	Time 0.685 (0.389)	Data 1.57e-04 (1.34e-03)	Tok/s 87119 (72452)	Loss/tok 3.5516 (3.1640)	LR 2.000e-03
0: TRAIN [4][630/968]	Time 0.313 (0.389)	Data 1.52e-04 (1.33e-03)	Tok/s 66273 (72438)	Loss/tok 3.0136 (3.1632)	LR 2.000e-03
0: TRAIN [4][640/968]	Time 0.426 (0.389)	Data 3.19e-04 (1.31e-03)	Tok/s 78162 (72444)	Loss/tok 3.1810 (3.1637)	LR 2.000e-03
0: TRAIN [4][650/968]	Time 0.313 (0.388)	Data 1.84e-04 (1.29e-03)	Tok/s 66855 (72369)	Loss/tok 3.0116 (3.1619)	LR 2.000e-03
0: TRAIN [4][660/968]	Time 0.427 (0.388)	Data 1.62e-04 (1.27e-03)	Tok/s 78255 (72358)	Loss/tok 3.2742 (3.1615)	LR 2.000e-03
0: TRAIN [4][670/968]	Time 0.314 (0.387)	Data 1.69e-04 (1.26e-03)	Tok/s 65408 (72254)	Loss/tok 2.9123 (3.1598)	LR 2.000e-03
0: TRAIN [4][680/968]	Time 0.310 (0.387)	Data 1.73e-04 (1.24e-03)	Tok/s 65808 (72286)	Loss/tok 2.9225 (3.1604)	LR 2.000e-03
0: TRAIN [4][690/968]	Time 0.547 (0.388)	Data 1.77e-04 (1.23e-03)	Tok/s 85526 (72360)	Loss/tok 3.2757 (3.1626)	LR 2.000e-03
0: TRAIN [4][700/968]	Time 0.312 (0.388)	Data 1.54e-04 (1.21e-03)	Tok/s 65381 (72344)	Loss/tok 2.9473 (3.1626)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [4][710/968]	Time 0.425 (0.388)	Data 1.58e-04 (1.20e-03)	Tok/s 78490 (72395)	Loss/tok 3.1685 (3.1625)	LR 2.000e-03
0: TRAIN [4][720/968]	Time 0.683 (0.387)	Data 1.61e-04 (1.18e-03)	Tok/s 88155 (72323)	Loss/tok 3.4468 (3.1623)	LR 2.000e-03
0: TRAIN [4][730/968]	Time 0.311 (0.387)	Data 1.72e-04 (1.17e-03)	Tok/s 65909 (72297)	Loss/tok 2.9397 (3.1616)	LR 2.000e-03
0: TRAIN [4][740/968]	Time 0.424 (0.387)	Data 1.56e-04 (1.16e-03)	Tok/s 79405 (72340)	Loss/tok 3.1441 (3.1620)	LR 2.000e-03
0: TRAIN [4][750/968]	Time 0.542 (0.387)	Data 1.71e-04 (1.14e-03)	Tok/s 86328 (72391)	Loss/tok 3.3061 (3.1618)	LR 2.000e-03
0: TRAIN [4][760/968]	Time 0.314 (0.388)	Data 1.59e-04 (1.13e-03)	Tok/s 65660 (72424)	Loss/tok 2.9408 (3.1629)	LR 2.000e-03
0: TRAIN [4][770/968]	Time 0.424 (0.387)	Data 1.91e-04 (1.12e-03)	Tok/s 79777 (72405)	Loss/tok 3.0876 (3.1621)	LR 2.000e-03
0: TRAIN [4][780/968]	Time 0.424 (0.388)	Data 1.69e-04 (1.11e-03)	Tok/s 79567 (72462)	Loss/tok 3.0895 (3.1619)	LR 2.000e-03
0: TRAIN [4][790/968]	Time 0.425 (0.387)	Data 2.27e-04 (1.09e-03)	Tok/s 78583 (72431)	Loss/tok 3.1391 (3.1604)	LR 2.000e-03
0: TRAIN [4][800/968]	Time 0.312 (0.387)	Data 1.68e-04 (1.08e-03)	Tok/s 66380 (72443)	Loss/tok 2.9957 (3.1595)	LR 2.000e-03
0: TRAIN [4][810/968]	Time 0.311 (0.387)	Data 1.68e-04 (1.07e-03)	Tok/s 66058 (72407)	Loss/tok 2.9814 (3.1590)	LR 2.000e-03
0: TRAIN [4][820/968]	Time 0.203 (0.387)	Data 1.59e-04 (1.06e-03)	Tok/s 51529 (72442)	Loss/tok 2.5745 (3.1593)	LR 2.000e-03
0: TRAIN [4][830/968]	Time 0.544 (0.387)	Data 1.61e-04 (1.05e-03)	Tok/s 85808 (72445)	Loss/tok 3.2932 (3.1590)	LR 2.000e-03
0: TRAIN [4][840/968]	Time 0.423 (0.387)	Data 1.53e-04 (1.04e-03)	Tok/s 79323 (72425)	Loss/tok 3.1340 (3.1582)	LR 2.000e-03
0: TRAIN [4][850/968]	Time 0.683 (0.387)	Data 1.71e-04 (1.03e-03)	Tok/s 87627 (72411)	Loss/tok 3.4808 (3.1584)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [4][860/968]	Time 0.424 (0.388)	Data 1.55e-04 (1.02e-03)	Tok/s 79772 (72490)	Loss/tok 3.1904 (3.1595)	LR 2.000e-03
0: TRAIN [4][870/968]	Time 0.205 (0.388)	Data 1.58e-04 (1.01e-03)	Tok/s 51505 (72506)	Loss/tok 2.5398 (3.1603)	LR 2.000e-03
0: TRAIN [4][880/968]	Time 0.312 (0.387)	Data 1.61e-04 (1.00e-03)	Tok/s 66916 (72458)	Loss/tok 3.0212 (3.1598)	LR 2.000e-03
0: TRAIN [4][890/968]	Time 0.205 (0.387)	Data 1.57e-04 (9.91e-04)	Tok/s 51104 (72455)	Loss/tok 2.5686 (3.1599)	LR 2.000e-03
0: TRAIN [4][900/968]	Time 0.686 (0.387)	Data 1.59e-04 (9.82e-04)	Tok/s 87428 (72437)	Loss/tok 3.5622 (3.1597)	LR 2.000e-03
0: TRAIN [4][910/968]	Time 0.543 (0.388)	Data 1.67e-04 (9.73e-04)	Tok/s 85909 (72458)	Loss/tok 3.3691 (3.1615)	LR 2.000e-03
0: TRAIN [4][920/968]	Time 0.313 (0.387)	Data 1.75e-04 (9.65e-04)	Tok/s 65193 (72394)	Loss/tok 3.0124 (3.1607)	LR 2.000e-03
0: TRAIN [4][930/968]	Time 0.313 (0.388)	Data 1.93e-04 (9.56e-04)	Tok/s 67051 (72452)	Loss/tok 2.9353 (3.1624)	LR 2.000e-03
0: TRAIN [4][940/968]	Time 0.682 (0.388)	Data 1.69e-04 (9.48e-04)	Tok/s 86935 (72494)	Loss/tok 3.4376 (3.1638)	LR 2.000e-03
0: TRAIN [4][950/968]	Time 0.312 (0.388)	Data 1.61e-04 (9.40e-04)	Tok/s 66463 (72478)	Loss/tok 3.0133 (3.1631)	LR 2.000e-03
0: TRAIN [4][960/968]	Time 0.423 (0.388)	Data 2.73e-04 (9.32e-04)	Tok/s 80075 (72480)	Loss/tok 3.2138 (3.1625)	LR 2.000e-03
:::MLL 1571840977.450 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 524}}
:::MLL 1571840977.450 eval_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [4][0/3]	Time 0.613 (0.613)	Decoder iters 95.0 (95.0)	Tok/s 26334 (26334)
0: Running moses detokenizer
0: BLEU(score=23.653667822408263, counts=[36582, 18093, 10254, 6024], totals=[64713, 61710, 58707, 55709], precisions=[56.52959992582634, 29.319397180359747, 17.46640094026266, 10.81333357267228], bp=1.0, sys_len=64713, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571840979.252 eval_accuracy: {"value": 23.65, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 535}}
:::MLL 1571840979.252 eval_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 4	Training Loss: 3.1602	Test BLEU: 23.65
0: Performance: Epoch: 4	Training: 579974 Tok/s
0: Finished epoch 4
:::MLL 1571840979.253 block_stop: {"value": null, "metadata": {"first_epoch_num": 5, "file": "train.py", "lineno": 557}}
:::MLL 1571840979.253 block_start: {"value": null, "metadata": {"first_epoch_num": 6, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571840979.253 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 514}}
0: Starting epoch 5
0: Executing preallocation
0: Sampler for epoch 5 uses seed 1897686827
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [5][0/968]	Time 1.068 (1.068)	Data 7.54e-01 (7.54e-01)	Tok/s 19190 (19190)	Loss/tok 2.8776 (2.8776)	LR 2.000e-03
0: TRAIN [5][10/968]	Time 0.423 (0.442)	Data 1.38e-04 (6.88e-02)	Tok/s 80032 (68964)	Loss/tok 3.0084 (3.0061)	LR 2.000e-03
0: TRAIN [5][20/968]	Time 0.424 (0.428)	Data 1.62e-04 (3.61e-02)	Tok/s 78886 (73081)	Loss/tok 3.0978 (3.0387)	LR 2.000e-03
0: TRAIN [5][30/968]	Time 0.311 (0.413)	Data 1.55e-04 (2.45e-02)	Tok/s 66792 (72052)	Loss/tok 2.9582 (3.0559)	LR 2.000e-03
0: TRAIN [5][40/968]	Time 0.542 (0.409)	Data 1.44e-04 (1.86e-02)	Tok/s 86592 (72224)	Loss/tok 3.1274 (3.0699)	LR 2.000e-03
0: TRAIN [5][50/968]	Time 0.311 (0.403)	Data 1.56e-04 (1.50e-02)	Tok/s 66390 (72474)	Loss/tok 2.8570 (3.0634)	LR 2.000e-03
0: TRAIN [5][60/968]	Time 0.421 (0.394)	Data 2.33e-04 (1.25e-02)	Tok/s 80395 (72083)	Loss/tok 3.1092 (3.0534)	LR 2.000e-03
0: TRAIN [5][70/968]	Time 0.543 (0.390)	Data 1.47e-04 (1.08e-02)	Tok/s 85974 (72118)	Loss/tok 3.2585 (3.0499)	LR 2.000e-03
0: TRAIN [5][80/968]	Time 0.312 (0.390)	Data 2.70e-04 (9.49e-03)	Tok/s 66245 (72012)	Loss/tok 2.8707 (3.0587)	LR 2.000e-03
0: TRAIN [5][90/968]	Time 0.425 (0.391)	Data 5.86e-04 (8.48e-03)	Tok/s 79444 (72440)	Loss/tok 3.0729 (3.0601)	LR 2.000e-03
0: TRAIN [5][100/968]	Time 0.311 (0.391)	Data 1.32e-04 (7.66e-03)	Tok/s 66664 (72190)	Loss/tok 2.9343 (3.0701)	LR 2.000e-03
0: TRAIN [5][110/968]	Time 0.312 (0.393)	Data 2.12e-04 (6.98e-03)	Tok/s 66897 (72422)	Loss/tok 2.8889 (3.0816)	LR 2.000e-03
0: TRAIN [5][120/968]	Time 0.542 (0.392)	Data 1.41e-04 (6.42e-03)	Tok/s 85550 (72285)	Loss/tok 3.2710 (3.0855)	LR 2.000e-03
0: TRAIN [5][130/968]	Time 0.425 (0.392)	Data 2.11e-04 (5.94e-03)	Tok/s 78921 (72287)	Loss/tok 3.1269 (3.0855)	LR 2.000e-03
0: TRAIN [5][140/968]	Time 0.310 (0.393)	Data 1.48e-04 (5.53e-03)	Tok/s 66496 (72276)	Loss/tok 2.9545 (3.0926)	LR 2.000e-03
0: TRAIN [5][150/968]	Time 0.313 (0.389)	Data 1.36e-04 (5.17e-03)	Tok/s 66273 (72013)	Loss/tok 2.9532 (3.0872)	LR 2.000e-03
0: TRAIN [5][160/968]	Time 0.313 (0.388)	Data 1.35e-04 (4.86e-03)	Tok/s 65555 (72084)	Loss/tok 2.9364 (3.0857)	LR 2.000e-03
0: TRAIN [5][170/968]	Time 0.542 (0.390)	Data 1.37e-04 (4.59e-03)	Tok/s 85968 (72398)	Loss/tok 3.1554 (3.0864)	LR 2.000e-03
0: TRAIN [5][180/968]	Time 0.311 (0.388)	Data 1.42e-04 (4.34e-03)	Tok/s 66058 (72240)	Loss/tok 2.9148 (3.0850)	LR 2.000e-03
0: TRAIN [5][190/968]	Time 0.424 (0.385)	Data 1.46e-04 (4.12e-03)	Tok/s 79883 (71982)	Loss/tok 3.0785 (3.0808)	LR 2.000e-03
0: TRAIN [5][200/968]	Time 0.425 (0.383)	Data 1.37e-04 (3.92e-03)	Tok/s 79475 (71830)	Loss/tok 3.1790 (3.0807)	LR 2.000e-03
0: TRAIN [5][210/968]	Time 0.314 (0.383)	Data 1.91e-04 (3.74e-03)	Tok/s 65693 (71718)	Loss/tok 2.9459 (3.0823)	LR 2.000e-03
0: TRAIN [5][220/968]	Time 0.320 (0.383)	Data 1.50e-04 (3.58e-03)	Tok/s 64206 (71769)	Loss/tok 2.9301 (3.0831)	LR 2.000e-03
0: TRAIN [5][230/968]	Time 0.684 (0.385)	Data 1.38e-04 (3.43e-03)	Tok/s 86742 (71882)	Loss/tok 3.4696 (3.0872)	LR 2.000e-03
0: TRAIN [5][240/968]	Time 0.314 (0.386)	Data 1.53e-04 (3.30e-03)	Tok/s 65433 (72035)	Loss/tok 2.9136 (3.0902)	LR 2.000e-03
0: TRAIN [5][250/968]	Time 0.313 (0.387)	Data 1.47e-04 (3.17e-03)	Tok/s 66691 (72134)	Loss/tok 2.8750 (3.0898)	LR 2.000e-03
0: TRAIN [5][260/968]	Time 0.424 (0.388)	Data 1.63e-04 (3.06e-03)	Tok/s 79762 (72255)	Loss/tok 3.1933 (3.0932)	LR 2.000e-03
0: TRAIN [5][270/968]	Time 0.315 (0.387)	Data 1.40e-04 (2.95e-03)	Tok/s 65981 (72310)	Loss/tok 2.8647 (3.0922)	LR 2.000e-03
0: TRAIN [5][280/968]	Time 0.315 (0.388)	Data 1.84e-04 (2.85e-03)	Tok/s 66028 (72356)	Loss/tok 2.7903 (3.0930)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][290/968]	Time 0.424 (0.389)	Data 1.51e-04 (2.76e-03)	Tok/s 79414 (72478)	Loss/tok 3.1138 (3.0956)	LR 2.000e-03
0: TRAIN [5][300/968]	Time 0.543 (0.388)	Data 1.77e-04 (2.68e-03)	Tok/s 86070 (72388)	Loss/tok 3.3382 (3.0950)	LR 2.000e-03
0: TRAIN [5][310/968]	Time 0.686 (0.389)	Data 5.41e-04 (2.60e-03)	Tok/s 87481 (72430)	Loss/tok 3.3309 (3.0959)	LR 2.000e-03
0: TRAIN [5][320/968]	Time 0.310 (0.388)	Data 3.90e-04 (2.52e-03)	Tok/s 67165 (72373)	Loss/tok 2.8776 (3.0967)	LR 2.000e-03
0: TRAIN [5][330/968]	Time 0.427 (0.387)	Data 1.47e-04 (2.45e-03)	Tok/s 78244 (72288)	Loss/tok 3.0456 (3.0942)	LR 2.000e-03
0: TRAIN [5][340/968]	Time 0.206 (0.385)	Data 1.62e-04 (2.38e-03)	Tok/s 51124 (72106)	Loss/tok 2.5596 (3.0917)	LR 2.000e-03
0: TRAIN [5][350/968]	Time 0.424 (0.385)	Data 1.49e-04 (2.32e-03)	Tok/s 79138 (72173)	Loss/tok 3.1442 (3.0918)	LR 2.000e-03
0: TRAIN [5][360/968]	Time 0.427 (0.387)	Data 1.55e-04 (2.26e-03)	Tok/s 78895 (72328)	Loss/tok 3.1507 (3.0957)	LR 2.000e-03
0: TRAIN [5][370/968]	Time 0.312 (0.388)	Data 2.66e-04 (2.21e-03)	Tok/s 66812 (72332)	Loss/tok 2.9237 (3.0981)	LR 2.000e-03
0: TRAIN [5][380/968]	Time 0.312 (0.387)	Data 1.77e-04 (2.15e-03)	Tok/s 66486 (72284)	Loss/tok 2.9559 (3.0968)	LR 2.000e-03
0: TRAIN [5][390/968]	Time 0.427 (0.385)	Data 1.63e-04 (2.10e-03)	Tok/s 78589 (72144)	Loss/tok 3.1017 (3.0942)	LR 2.000e-03
0: TRAIN [5][400/968]	Time 0.313 (0.386)	Data 1.54e-04 (2.05e-03)	Tok/s 64335 (72246)	Loss/tok 2.8784 (3.0952)	LR 2.000e-03
0: TRAIN [5][410/968]	Time 0.683 (0.387)	Data 1.49e-04 (2.01e-03)	Tok/s 87682 (72239)	Loss/tok 3.3617 (3.0972)	LR 2.000e-03
0: TRAIN [5][420/968]	Time 0.544 (0.387)	Data 1.80e-04 (1.96e-03)	Tok/s 85242 (72205)	Loss/tok 3.3033 (3.0982)	LR 2.000e-03
0: TRAIN [5][430/968]	Time 0.426 (0.388)	Data 1.58e-04 (1.92e-03)	Tok/s 78483 (72298)	Loss/tok 3.2006 (3.1006)	LR 2.000e-03
0: TRAIN [5][440/968]	Time 0.311 (0.388)	Data 1.53e-04 (1.88e-03)	Tok/s 66557 (72242)	Loss/tok 2.9666 (3.1009)	LR 2.000e-03
0: TRAIN [5][450/968]	Time 0.425 (0.388)	Data 1.39e-04 (1.84e-03)	Tok/s 79717 (72273)	Loss/tok 3.1698 (3.1012)	LR 2.000e-03
0: TRAIN [5][460/968]	Time 0.424 (0.387)	Data 1.57e-04 (1.81e-03)	Tok/s 79355 (72274)	Loss/tok 3.1954 (3.1001)	LR 2.000e-03
0: TRAIN [5][470/968]	Time 0.204 (0.387)	Data 1.43e-04 (1.77e-03)	Tok/s 51802 (72271)	Loss/tok 2.5437 (3.0994)	LR 2.000e-03
0: TRAIN [5][480/968]	Time 0.205 (0.388)	Data 1.45e-04 (1.74e-03)	Tok/s 51519 (72298)	Loss/tok 2.5809 (3.1022)	LR 2.000e-03
0: TRAIN [5][490/968]	Time 0.311 (0.388)	Data 1.80e-04 (1.71e-03)	Tok/s 66942 (72311)	Loss/tok 2.8978 (3.1020)	LR 2.000e-03
0: TRAIN [5][500/968]	Time 0.313 (0.387)	Data 1.45e-04 (1.68e-03)	Tok/s 66562 (72274)	Loss/tok 2.8906 (3.1014)	LR 2.000e-03
0: TRAIN [5][510/968]	Time 0.424 (0.386)	Data 1.54e-04 (1.65e-03)	Tok/s 78998 (72252)	Loss/tok 3.1110 (3.0999)	LR 2.000e-03
0: TRAIN [5][520/968]	Time 0.312 (0.387)	Data 1.43e-04 (1.62e-03)	Tok/s 65666 (72341)	Loss/tok 2.9534 (3.1026)	LR 2.000e-03
0: TRAIN [5][530/968]	Time 0.204 (0.386)	Data 3.43e-04 (1.59e-03)	Tok/s 51143 (72228)	Loss/tok 2.5690 (3.1009)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][540/968]	Time 0.545 (0.387)	Data 1.63e-04 (1.56e-03)	Tok/s 85426 (72303)	Loss/tok 3.3206 (3.1022)	LR 2.000e-03
0: TRAIN [5][550/968]	Time 0.204 (0.387)	Data 1.47e-04 (1.54e-03)	Tok/s 52310 (72334)	Loss/tok 2.5396 (3.1047)	LR 2.000e-03
0: TRAIN [5][560/968]	Time 0.424 (0.388)	Data 1.73e-04 (1.51e-03)	Tok/s 78516 (72424)	Loss/tok 3.1513 (3.1060)	LR 2.000e-03
0: TRAIN [5][570/968]	Time 0.544 (0.388)	Data 1.41e-04 (1.49e-03)	Tok/s 85764 (72467)	Loss/tok 3.2404 (3.1066)	LR 2.000e-03
0: TRAIN [5][580/968]	Time 0.544 (0.388)	Data 1.51e-04 (1.47e-03)	Tok/s 85951 (72423)	Loss/tok 3.3262 (3.1068)	LR 2.000e-03
0: TRAIN [5][590/968]	Time 0.313 (0.388)	Data 1.60e-04 (1.45e-03)	Tok/s 65176 (72412)	Loss/tok 2.9861 (3.1078)	LR 2.000e-03
0: TRAIN [5][600/968]	Time 0.426 (0.388)	Data 1.39e-04 (1.42e-03)	Tok/s 78522 (72404)	Loss/tok 3.1874 (3.1077)	LR 2.000e-03
0: TRAIN [5][610/968]	Time 0.684 (0.389)	Data 1.40e-04 (1.40e-03)	Tok/s 87577 (72440)	Loss/tok 3.3940 (3.1097)	LR 2.000e-03
0: TRAIN [5][620/968]	Time 0.312 (0.388)	Data 1.40e-04 (1.38e-03)	Tok/s 65504 (72442)	Loss/tok 2.9244 (3.1097)	LR 2.000e-03
0: TRAIN [5][630/968]	Time 0.311 (0.389)	Data 1.46e-04 (1.36e-03)	Tok/s 65829 (72477)	Loss/tok 2.9486 (3.1097)	LR 2.000e-03
0: TRAIN [5][640/968]	Time 0.425 (0.390)	Data 1.71e-04 (1.35e-03)	Tok/s 79223 (72561)	Loss/tok 3.1387 (3.1132)	LR 2.000e-03
0: TRAIN [5][650/968]	Time 0.310 (0.389)	Data 1.50e-04 (1.33e-03)	Tok/s 66416 (72543)	Loss/tok 2.8871 (3.1122)	LR 2.000e-03
0: TRAIN [5][660/968]	Time 0.316 (0.389)	Data 1.40e-04 (1.31e-03)	Tok/s 65556 (72517)	Loss/tok 2.9238 (3.1116)	LR 2.000e-03
0: TRAIN [5][670/968]	Time 0.423 (0.390)	Data 1.51e-04 (1.29e-03)	Tok/s 79939 (72582)	Loss/tok 3.1199 (3.1124)	LR 2.000e-03
0: TRAIN [5][680/968]	Time 0.313 (0.389)	Data 1.40e-04 (1.28e-03)	Tok/s 65987 (72564)	Loss/tok 2.9942 (3.1128)	LR 2.000e-03
0: TRAIN [5][690/968]	Time 0.317 (0.389)	Data 1.46e-04 (1.26e-03)	Tok/s 64734 (72505)	Loss/tok 2.9132 (3.1121)	LR 2.000e-03
0: TRAIN [5][700/968]	Time 0.543 (0.390)	Data 1.42e-04 (1.24e-03)	Tok/s 85128 (72542)	Loss/tok 3.3706 (3.1136)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][710/968]	Time 0.426 (0.391)	Data 1.41e-04 (1.23e-03)	Tok/s 77896 (72611)	Loss/tok 3.1832 (3.1166)	LR 2.000e-03
0: TRAIN [5][720/968]	Time 0.313 (0.390)	Data 1.39e-04 (1.22e-03)	Tok/s 66183 (72539)	Loss/tok 2.9387 (3.1164)	LR 2.000e-03
0: TRAIN [5][730/968]	Time 0.313 (0.390)	Data 1.35e-04 (1.20e-03)	Tok/s 65697 (72502)	Loss/tok 2.9684 (3.1171)	LR 2.000e-03
0: TRAIN [5][740/968]	Time 0.310 (0.390)	Data 1.41e-04 (1.19e-03)	Tok/s 66487 (72490)	Loss/tok 2.9513 (3.1167)	LR 2.000e-03
0: TRAIN [5][750/968]	Time 0.316 (0.389)	Data 1.37e-04 (1.17e-03)	Tok/s 65830 (72388)	Loss/tok 2.8916 (3.1160)	LR 2.000e-03
0: TRAIN [5][760/968]	Time 0.425 (0.388)	Data 1.41e-04 (1.16e-03)	Tok/s 79137 (72344)	Loss/tok 3.0938 (3.1153)	LR 2.000e-03
0: TRAIN [5][770/968]	Time 0.314 (0.388)	Data 1.41e-04 (1.15e-03)	Tok/s 65132 (72292)	Loss/tok 2.9535 (3.1140)	LR 2.000e-03
0: TRAIN [5][780/968]	Time 0.542 (0.387)	Data 1.37e-04 (1.13e-03)	Tok/s 87143 (72223)	Loss/tok 3.2017 (3.1127)	LR 2.000e-03
0: TRAIN [5][790/968]	Time 0.205 (0.386)	Data 1.36e-04 (1.12e-03)	Tok/s 51073 (72205)	Loss/tok 2.6091 (3.1119)	LR 2.000e-03
0: TRAIN [5][800/968]	Time 0.423 (0.387)	Data 1.76e-04 (1.11e-03)	Tok/s 79698 (72212)	Loss/tok 3.1574 (3.1122)	LR 2.000e-03
0: TRAIN [5][810/968]	Time 0.311 (0.387)	Data 1.54e-04 (1.10e-03)	Tok/s 66980 (72252)	Loss/tok 2.9381 (3.1127)	LR 2.000e-03
0: TRAIN [5][820/968]	Time 0.686 (0.387)	Data 1.48e-04 (1.09e-03)	Tok/s 85804 (72240)	Loss/tok 3.4575 (3.1132)	LR 2.000e-03
0: TRAIN [5][830/968]	Time 0.423 (0.387)	Data 3.70e-04 (1.08e-03)	Tok/s 78865 (72252)	Loss/tok 3.2048 (3.1136)	LR 2.000e-03
0: TRAIN [5][840/968]	Time 0.312 (0.387)	Data 1.46e-04 (1.06e-03)	Tok/s 67552 (72258)	Loss/tok 2.9102 (3.1135)	LR 2.000e-03
0: TRAIN [5][850/968]	Time 0.424 (0.387)	Data 1.47e-04 (1.05e-03)	Tok/s 79387 (72349)	Loss/tok 3.1262 (3.1147)	LR 2.000e-03
0: TRAIN [5][860/968]	Time 0.311 (0.387)	Data 1.70e-04 (1.04e-03)	Tok/s 65420 (72334)	Loss/tok 2.8991 (3.1144)	LR 2.000e-03
0: TRAIN [5][870/968]	Time 0.688 (0.388)	Data 1.46e-04 (1.03e-03)	Tok/s 87762 (72343)	Loss/tok 3.4902 (3.1153)	LR 2.000e-03
0: TRAIN [5][880/968]	Time 0.424 (0.387)	Data 1.55e-04 (1.02e-03)	Tok/s 79206 (72328)	Loss/tok 3.1668 (3.1152)	LR 2.000e-03
0: TRAIN [5][890/968]	Time 0.204 (0.387)	Data 1.46e-04 (1.01e-03)	Tok/s 51542 (72312)	Loss/tok 2.5050 (3.1146)	LR 2.000e-03
0: TRAIN [5][900/968]	Time 0.544 (0.388)	Data 1.43e-04 (1.00e-03)	Tok/s 85603 (72391)	Loss/tok 3.3729 (3.1164)	LR 2.000e-03
0: TRAIN [5][910/968]	Time 0.207 (0.387)	Data 1.42e-04 (9.95e-04)	Tok/s 51507 (72326)	Loss/tok 2.4789 (3.1166)	LR 2.000e-03
0: TRAIN [5][920/968]	Time 0.313 (0.388)	Data 1.52e-04 (9.86e-04)	Tok/s 66114 (72344)	Loss/tok 2.9389 (3.1166)	LR 2.000e-03
0: TRAIN [5][930/968]	Time 0.423 (0.388)	Data 1.44e-04 (9.77e-04)	Tok/s 78922 (72365)	Loss/tok 3.0693 (3.1170)	LR 2.000e-03
0: TRAIN [5][940/968]	Time 0.313 (0.388)	Data 1.40e-04 (9.69e-04)	Tok/s 66248 (72342)	Loss/tok 2.9140 (3.1170)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][950/968]	Time 0.548 (0.388)	Data 1.85e-04 (9.61e-04)	Tok/s 85069 (72371)	Loss/tok 3.3360 (3.1178)	LR 2.000e-03
0: TRAIN [5][960/968]	Time 0.314 (0.388)	Data 1.74e-04 (9.52e-04)	Tok/s 65948 (72367)	Loss/tok 2.9325 (3.1177)	LR 2.000e-03
:::MLL 1571841357.874 epoch_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 524}}
:::MLL 1571841357.874 eval_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [5][0/3]	Time 0.712 (0.712)	Decoder iters 129.0 (129.0)	Tok/s 23658 (23658)
0: Running moses detokenizer
0: BLEU(score=23.403627459288742, counts=[37297, 18531, 10518, 6277], totals=[67043, 64040, 61037, 58040], precisions=[55.63146040600808, 28.936602123672703, 17.232170650588987, 10.814955203308063], bp=1.0, sys_len=67043, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571841359.850 eval_accuracy: {"value": 23.4, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 535}}
:::MLL 1571841359.850 eval_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 5	Training Loss: 3.1187	Test BLEU: 23.40
0: Performance: Epoch: 5	Training: 578975 Tok/s
0: Finished epoch 5
:::MLL 1571841359.850 block_stop: {"value": null, "metadata": {"first_epoch_num": 6, "file": "train.py", "lineno": 557}}
:::MLL 1571841359.851 block_start: {"value": null, "metadata": {"first_epoch_num": 7, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571841359.851 epoch_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 514}}
0: Starting epoch 6
0: Executing preallocation
0: Sampler for epoch 6 uses seed 914493576
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [6][0/968]	Time 0.958 (0.958)	Data 7.33e-01 (7.33e-01)	Tok/s 11034 (11034)	Loss/tok 2.5487 (2.5487)	LR 2.000e-03
0: TRAIN [6][10/968]	Time 0.312 (0.446)	Data 1.68e-04 (6.67e-02)	Tok/s 65631 (64766)	Loss/tok 2.8712 (3.0370)	LR 2.000e-03
0: TRAIN [6][20/968]	Time 0.311 (0.399)	Data 1.59e-04 (3.50e-02)	Tok/s 65746 (66761)	Loss/tok 2.8439 (3.0091)	LR 2.000e-03
0: TRAIN [6][30/968]	Time 0.424 (0.415)	Data 1.79e-04 (2.38e-02)	Tok/s 80238 (70825)	Loss/tok 3.0262 (3.0424)	LR 2.000e-03
0: TRAIN [6][40/968]	Time 0.422 (0.402)	Data 2.00e-04 (1.80e-02)	Tok/s 79659 (70839)	Loss/tok 3.0298 (3.0215)	LR 2.000e-03
0: TRAIN [6][50/968]	Time 0.311 (0.397)	Data 1.75e-04 (1.45e-02)	Tok/s 65857 (71228)	Loss/tok 2.9067 (3.0245)	LR 2.000e-03
0: TRAIN [6][60/968]	Time 0.311 (0.393)	Data 1.45e-04 (1.22e-02)	Tok/s 66722 (71362)	Loss/tok 2.8545 (3.0206)	LR 2.000e-03
0: TRAIN [6][70/968]	Time 0.205 (0.390)	Data 1.52e-04 (1.05e-02)	Tok/s 51762 (71054)	Loss/tok 2.5409 (3.0294)	LR 2.000e-03
0: TRAIN [6][80/968]	Time 0.311 (0.390)	Data 1.72e-04 (9.21e-03)	Tok/s 66825 (71427)	Loss/tok 2.8994 (3.0332)	LR 2.000e-03
0: TRAIN [6][90/968]	Time 0.312 (0.391)	Data 1.55e-04 (8.22e-03)	Tok/s 66202 (71486)	Loss/tok 2.9685 (3.0411)	LR 2.000e-03
0: TRAIN [6][100/968]	Time 0.423 (0.395)	Data 1.55e-04 (7.42e-03)	Tok/s 78744 (72245)	Loss/tok 2.9641 (3.0466)	LR 2.000e-03
0: TRAIN [6][110/968]	Time 0.311 (0.397)	Data 1.42e-04 (6.76e-03)	Tok/s 66288 (72500)	Loss/tok 2.8635 (3.0528)	LR 2.000e-03
0: TRAIN [6][120/968]	Time 0.312 (0.396)	Data 1.54e-04 (6.22e-03)	Tok/s 66815 (72410)	Loss/tok 2.8354 (3.0568)	LR 2.000e-03
0: TRAIN [6][130/968]	Time 0.313 (0.398)	Data 1.55e-04 (5.75e-03)	Tok/s 65325 (72497)	Loss/tok 2.8778 (3.0644)	LR 2.000e-03
0: TRAIN [6][140/968]	Time 0.311 (0.399)	Data 1.44e-04 (5.36e-03)	Tok/s 66907 (72642)	Loss/tok 2.8388 (3.0639)	LR 2.000e-03
0: TRAIN [6][150/968]	Time 0.205 (0.401)	Data 1.42e-04 (5.01e-03)	Tok/s 51439 (72976)	Loss/tok 2.5747 (3.0706)	LR 2.000e-03
0: TRAIN [6][160/968]	Time 0.423 (0.400)	Data 1.46e-04 (4.71e-03)	Tok/s 79578 (73045)	Loss/tok 3.0926 (3.0680)	LR 2.000e-03
0: TRAIN [6][170/968]	Time 0.425 (0.399)	Data 1.62e-04 (4.45e-03)	Tok/s 78576 (72997)	Loss/tok 3.1007 (3.0677)	LR 2.000e-03
0: TRAIN [6][180/968]	Time 0.310 (0.397)	Data 1.43e-04 (4.21e-03)	Tok/s 66309 (72842)	Loss/tok 2.8697 (3.0663)	LR 2.000e-03
0: TRAIN [6][190/968]	Time 0.204 (0.396)	Data 1.74e-04 (4.00e-03)	Tok/s 51285 (72915)	Loss/tok 2.4865 (3.0650)	LR 2.000e-03
0: TRAIN [6][200/968]	Time 0.547 (0.396)	Data 1.55e-04 (3.81e-03)	Tok/s 84681 (72952)	Loss/tok 3.2968 (3.0662)	LR 2.000e-03
0: TRAIN [6][210/968]	Time 0.421 (0.395)	Data 1.40e-04 (3.63e-03)	Tok/s 79329 (72931)	Loss/tok 3.0639 (3.0631)	LR 2.000e-03
0: TRAIN [6][220/968]	Time 0.546 (0.395)	Data 1.58e-04 (3.48e-03)	Tok/s 85079 (72889)	Loss/tok 3.1930 (3.0619)	LR 2.000e-03
0: TRAIN [6][230/968]	Time 0.544 (0.397)	Data 1.44e-04 (3.33e-03)	Tok/s 85108 (73035)	Loss/tok 3.2621 (3.0673)	LR 2.000e-03
0: TRAIN [6][240/968]	Time 0.426 (0.397)	Data 1.71e-04 (3.20e-03)	Tok/s 78938 (72971)	Loss/tok 3.0339 (3.0713)	LR 2.000e-03
0: TRAIN [6][250/968]	Time 0.313 (0.395)	Data 1.44e-04 (3.08e-03)	Tok/s 65383 (72839)	Loss/tok 2.8593 (3.0687)	LR 2.000e-03
0: TRAIN [6][260/968]	Time 0.684 (0.395)	Data 1.60e-04 (2.97e-03)	Tok/s 87411 (72822)	Loss/tok 3.4104 (3.0698)	LR 2.000e-03
0: TRAIN [6][270/968]	Time 0.312 (0.395)	Data 1.66e-04 (2.87e-03)	Tok/s 66556 (72769)	Loss/tok 2.9106 (3.0721)	LR 2.000e-03
0: TRAIN [6][280/968]	Time 0.424 (0.396)	Data 1.39e-04 (2.77e-03)	Tok/s 79602 (72783)	Loss/tok 3.0175 (3.0739)	LR 2.000e-03
0: TRAIN [6][290/968]	Time 0.543 (0.396)	Data 1.53e-04 (2.68e-03)	Tok/s 85423 (72818)	Loss/tok 3.2285 (3.0740)	LR 2.000e-03
0: TRAIN [6][300/968]	Time 0.545 (0.395)	Data 1.56e-04 (2.60e-03)	Tok/s 85731 (72784)	Loss/tok 3.2593 (3.0747)	LR 2.000e-03
0: TRAIN [6][310/968]	Time 0.310 (0.395)	Data 1.44e-04 (2.52e-03)	Tok/s 66723 (72817)	Loss/tok 2.9032 (3.0734)	LR 2.000e-03
0: TRAIN [6][320/968]	Time 0.423 (0.396)	Data 1.55e-04 (2.44e-03)	Tok/s 79280 (72880)	Loss/tok 3.1082 (3.0775)	LR 2.000e-03
0: TRAIN [6][330/968]	Time 0.313 (0.396)	Data 1.39e-04 (2.37e-03)	Tok/s 65733 (72891)	Loss/tok 2.8737 (3.0786)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [6][340/968]	Time 0.312 (0.395)	Data 1.42e-04 (2.31e-03)	Tok/s 65988 (72803)	Loss/tok 2.9047 (3.0773)	LR 2.000e-03
0: TRAIN [6][350/968]	Time 0.310 (0.395)	Data 1.49e-04 (2.25e-03)	Tok/s 67379 (72829)	Loss/tok 2.8837 (3.0772)	LR 2.000e-03
0: TRAIN [6][360/968]	Time 0.204 (0.395)	Data 1.64e-04 (2.19e-03)	Tok/s 52539 (72802)	Loss/tok 2.5447 (3.0768)	LR 2.000e-03
0: TRAIN [6][370/968]	Time 0.312 (0.394)	Data 1.50e-04 (2.14e-03)	Tok/s 66037 (72738)	Loss/tok 2.8469 (3.0753)	LR 2.000e-03
0: TRAIN [6][380/968]	Time 0.204 (0.392)	Data 1.38e-04 (2.08e-03)	Tok/s 50549 (72566)	Loss/tok 2.5808 (3.0730)	LR 2.000e-03
0: TRAIN [6][390/968]	Time 0.313 (0.391)	Data 1.46e-04 (2.03e-03)	Tok/s 66078 (72517)	Loss/tok 2.8969 (3.0728)	LR 2.000e-03
0: TRAIN [6][400/968]	Time 0.545 (0.393)	Data 1.68e-04 (1.99e-03)	Tok/s 86451 (72651)	Loss/tok 3.2513 (3.0762)	LR 2.000e-03
0: TRAIN [6][410/968]	Time 0.313 (0.394)	Data 1.64e-04 (1.94e-03)	Tok/s 65845 (72633)	Loss/tok 2.8424 (3.0768)	LR 2.000e-03
0: TRAIN [6][420/968]	Time 0.207 (0.392)	Data 1.71e-04 (1.90e-03)	Tok/s 50842 (72557)	Loss/tok 2.5505 (3.0753)	LR 2.000e-03
0: TRAIN [6][430/968]	Time 0.544 (0.392)	Data 1.68e-04 (1.86e-03)	Tok/s 86258 (72588)	Loss/tok 3.1616 (3.0752)	LR 2.000e-03
0: TRAIN [6][440/968]	Time 0.424 (0.393)	Data 1.79e-04 (1.82e-03)	Tok/s 78910 (72659)	Loss/tok 3.0395 (3.0761)	LR 2.000e-03
0: TRAIN [6][450/968]	Time 0.311 (0.393)	Data 1.41e-04 (1.79e-03)	Tok/s 66794 (72697)	Loss/tok 2.8828 (3.0757)	LR 2.000e-03
0: TRAIN [6][460/968]	Time 0.425 (0.393)	Data 1.59e-04 (1.75e-03)	Tok/s 79357 (72716)	Loss/tok 3.1153 (3.0744)	LR 2.000e-03
0: TRAIN [6][470/968]	Time 0.311 (0.392)	Data 2.66e-04 (1.72e-03)	Tok/s 66182 (72730)	Loss/tok 2.8943 (3.0733)	LR 2.000e-03
0: TRAIN [6][480/968]	Time 0.311 (0.392)	Data 1.52e-04 (1.69e-03)	Tok/s 66445 (72718)	Loss/tok 2.9245 (3.0725)	LR 2.000e-03
0: TRAIN [6][490/968]	Time 0.423 (0.392)	Data 1.75e-04 (1.65e-03)	Tok/s 79174 (72662)	Loss/tok 3.0995 (3.0730)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [6][500/968]	Time 0.205 (0.391)	Data 1.61e-04 (1.63e-03)	Tok/s 51828 (72609)	Loss/tok 2.5080 (3.0737)	LR 2.000e-03
0: TRAIN [6][510/968]	Time 0.423 (0.391)	Data 1.48e-04 (1.60e-03)	Tok/s 79840 (72561)	Loss/tok 3.0863 (3.0736)	LR 2.000e-03
0: TRAIN [6][520/968]	Time 0.311 (0.390)	Data 1.39e-04 (1.57e-03)	Tok/s 66604 (72561)	Loss/tok 2.9493 (3.0741)	LR 2.000e-03
0: TRAIN [6][530/968]	Time 0.423 (0.390)	Data 6.30e-04 (1.54e-03)	Tok/s 78853 (72530)	Loss/tok 3.1532 (3.0737)	LR 2.000e-03
0: TRAIN [6][540/968]	Time 0.312 (0.390)	Data 1.47e-04 (1.52e-03)	Tok/s 66035 (72553)	Loss/tok 2.8710 (3.0730)	LR 2.000e-03
0: TRAIN [6][550/968]	Time 0.312 (0.388)	Data 1.55e-04 (1.49e-03)	Tok/s 66357 (72439)	Loss/tok 2.8348 (3.0710)	LR 2.000e-03
0: TRAIN [6][560/968]	Time 0.205 (0.387)	Data 1.50e-04 (1.47e-03)	Tok/s 51545 (72370)	Loss/tok 2.5274 (3.0694)	LR 2.000e-03
0: TRAIN [6][570/968]	Time 0.424 (0.387)	Data 1.67e-04 (1.45e-03)	Tok/s 78496 (72295)	Loss/tok 3.1591 (3.0698)	LR 2.000e-03
0: TRAIN [6][580/968]	Time 0.313 (0.386)	Data 1.43e-04 (1.43e-03)	Tok/s 66255 (72222)	Loss/tok 2.8520 (3.0686)	LR 2.000e-03
0: TRAIN [6][590/968]	Time 0.312 (0.385)	Data 1.51e-04 (1.40e-03)	Tok/s 66383 (72149)	Loss/tok 2.8592 (3.0683)	LR 2.000e-03
0: TRAIN [6][600/968]	Time 0.204 (0.385)	Data 1.50e-04 (1.38e-03)	Tok/s 51706 (72133)	Loss/tok 2.5312 (3.0679)	LR 2.000e-03
0: TRAIN [6][610/968]	Time 0.205 (0.385)	Data 1.54e-04 (1.36e-03)	Tok/s 51686 (72148)	Loss/tok 2.5665 (3.0688)	LR 2.000e-03
0: TRAIN [6][620/968]	Time 0.312 (0.385)	Data 1.52e-04 (1.34e-03)	Tok/s 66964 (72152)	Loss/tok 2.9255 (3.0687)	LR 2.000e-03
0: TRAIN [6][630/968]	Time 0.425 (0.385)	Data 1.39e-04 (1.33e-03)	Tok/s 79280 (72192)	Loss/tok 3.1054 (3.0686)	LR 2.000e-03
0: TRAIN [6][640/968]	Time 0.313 (0.385)	Data 1.38e-04 (1.31e-03)	Tok/s 65482 (72168)	Loss/tok 2.8605 (3.0677)	LR 2.000e-03
0: TRAIN [6][650/968]	Time 0.685 (0.385)	Data 1.75e-04 (1.29e-03)	Tok/s 86817 (72173)	Loss/tok 3.3731 (3.0686)	LR 2.000e-03
0: TRAIN [6][660/968]	Time 0.546 (0.384)	Data 1.47e-04 (1.27e-03)	Tok/s 85464 (72173)	Loss/tok 3.2321 (3.0680)	LR 2.000e-03
0: TRAIN [6][670/968]	Time 0.310 (0.384)	Data 1.55e-04 (1.26e-03)	Tok/s 67269 (72155)	Loss/tok 2.8860 (3.0677)	LR 2.000e-03
0: TRAIN [6][680/968]	Time 0.311 (0.384)	Data 1.40e-04 (1.24e-03)	Tok/s 66099 (72181)	Loss/tok 2.9069 (3.0683)	LR 1.000e-03
0: TRAIN [6][690/968]	Time 0.543 (0.384)	Data 1.49e-04 (1.23e-03)	Tok/s 86277 (72172)	Loss/tok 3.2957 (3.0691)	LR 1.000e-03
0: TRAIN [6][700/968]	Time 0.425 (0.384)	Data 1.44e-04 (1.21e-03)	Tok/s 79833 (72178)	Loss/tok 3.0213 (3.0682)	LR 1.000e-03
0: TRAIN [6][710/968]	Time 0.313 (0.385)	Data 1.40e-04 (1.20e-03)	Tok/s 64945 (72238)	Loss/tok 2.9575 (3.0696)	LR 1.000e-03
0: TRAIN [6][720/968]	Time 0.424 (0.385)	Data 1.67e-04 (1.18e-03)	Tok/s 80072 (72242)	Loss/tok 3.1141 (3.0706)	LR 1.000e-03
0: TRAIN [6][730/968]	Time 0.310 (0.385)	Data 1.61e-04 (1.17e-03)	Tok/s 67497 (72238)	Loss/tok 2.9428 (3.0701)	LR 1.000e-03
0: TRAIN [6][740/968]	Time 0.545 (0.387)	Data 1.42e-04 (1.15e-03)	Tok/s 85786 (72383)	Loss/tok 3.1965 (3.0740)	LR 1.000e-03
0: TRAIN [6][750/968]	Time 0.313 (0.388)	Data 1.63e-04 (1.14e-03)	Tok/s 66373 (72421)	Loss/tok 2.9444 (3.0758)	LR 1.000e-03
0: TRAIN [6][760/968]	Time 0.314 (0.388)	Data 1.45e-04 (1.13e-03)	Tok/s 65384 (72438)	Loss/tok 2.9484 (3.0760)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [6][770/968]	Time 0.427 (0.388)	Data 1.56e-04 (1.12e-03)	Tok/s 78494 (72471)	Loss/tok 3.0346 (3.0759)	LR 1.000e-03
0: TRAIN [6][780/968]	Time 0.544 (0.388)	Data 2.16e-04 (1.10e-03)	Tok/s 85528 (72496)	Loss/tok 3.2369 (3.0768)	LR 1.000e-03
0: TRAIN [6][790/968]	Time 0.426 (0.389)	Data 1.60e-04 (1.09e-03)	Tok/s 79843 (72517)	Loss/tok 3.1048 (3.0772)	LR 1.000e-03
0: TRAIN [6][800/968]	Time 0.425 (0.388)	Data 1.59e-04 (1.08e-03)	Tok/s 79118 (72485)	Loss/tok 3.0990 (3.0762)	LR 1.000e-03
0: TRAIN [6][810/968]	Time 0.424 (0.388)	Data 1.44e-04 (1.07e-03)	Tok/s 78918 (72506)	Loss/tok 3.0900 (3.0765)	LR 1.000e-03
0: TRAIN [6][820/968]	Time 0.313 (0.389)	Data 1.61e-04 (1.06e-03)	Tok/s 66106 (72580)	Loss/tok 2.8528 (3.0783)	LR 1.000e-03
0: TRAIN [6][830/968]	Time 0.545 (0.389)	Data 1.53e-04 (1.05e-03)	Tok/s 84554 (72595)	Loss/tok 3.2348 (3.0781)	LR 1.000e-03
0: TRAIN [6][840/968]	Time 0.424 (0.390)	Data 1.70e-04 (1.04e-03)	Tok/s 79281 (72654)	Loss/tok 3.1374 (3.0786)	LR 1.000e-03
0: TRAIN [6][850/968]	Time 0.547 (0.390)	Data 1.63e-04 (1.03e-03)	Tok/s 85505 (72682)	Loss/tok 3.2659 (3.0794)	LR 1.000e-03
0: TRAIN [6][860/968]	Time 0.546 (0.390)	Data 1.49e-04 (1.02e-03)	Tok/s 85715 (72716)	Loss/tok 3.2486 (3.0789)	LR 1.000e-03
0: TRAIN [6][870/968]	Time 0.312 (0.390)	Data 1.55e-04 (1.01e-03)	Tok/s 66180 (72677)	Loss/tok 2.8870 (3.0779)	LR 1.000e-03
0: TRAIN [6][880/968]	Time 0.205 (0.390)	Data 1.73e-04 (9.97e-04)	Tok/s 51952 (72692)	Loss/tok 2.5316 (3.0786)	LR 1.000e-03
0: TRAIN [6][890/968]	Time 0.205 (0.389)	Data 1.39e-04 (9.87e-04)	Tok/s 52530 (72558)	Loss/tok 2.5824 (3.0774)	LR 1.000e-03
0: TRAIN [6][900/968]	Time 0.545 (0.389)	Data 1.39e-04 (9.78e-04)	Tok/s 86058 (72553)	Loss/tok 3.1972 (3.0767)	LR 1.000e-03
0: TRAIN [6][910/968]	Time 0.424 (0.389)	Data 1.58e-04 (9.69e-04)	Tok/s 79515 (72525)	Loss/tok 3.0563 (3.0765)	LR 1.000e-03
0: TRAIN [6][920/968]	Time 0.310 (0.388)	Data 1.39e-04 (9.60e-04)	Tok/s 66470 (72497)	Loss/tok 2.8771 (3.0757)	LR 1.000e-03
0: TRAIN [6][930/968]	Time 0.545 (0.388)	Data 1.46e-04 (9.52e-04)	Tok/s 85861 (72492)	Loss/tok 3.2019 (3.0764)	LR 1.000e-03
0: TRAIN [6][940/968]	Time 0.429 (0.389)	Data 1.53e-04 (9.43e-04)	Tok/s 78089 (72532)	Loss/tok 3.0664 (3.0761)	LR 1.000e-03
0: TRAIN [6][950/968]	Time 0.424 (0.389)	Data 1.50e-04 (9.35e-04)	Tok/s 80630 (72505)	Loss/tok 3.0436 (3.0760)	LR 1.000e-03
0: TRAIN [6][960/968]	Time 0.425 (0.388)	Data 1.76e-04 (9.27e-04)	Tok/s 79022 (72485)	Loss/tok 3.0106 (3.0753)	LR 1.000e-03
:::MLL 1571841737.935 epoch_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 524}}
:::MLL 1571841737.936 eval_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [6][0/3]	Time 0.641 (0.641)	Decoder iters 102.0 (102.0)	Tok/s 25552 (25552)
0: Running moses detokenizer
0: BLEU(score=24.31161151156899, counts=[37162, 18669, 10656, 6374], totals=[65199, 62196, 59193, 56195], precisions=[56.997806714826915, 30.016399768473857, 18.002128630074502, 11.342646142895275], bp=1.0, sys_len=65199, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1571841739.793 eval_accuracy: {"value": 24.31, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 535}}
:::MLL 1571841739.793 eval_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 6	Training Loss: 3.0725	Test BLEU: 24.31
0: Performance: Epoch: 6	Training: 579707 Tok/s
0: Finished epoch 6
:::MLL 1571841739.794 block_stop: {"value": null, "metadata": {"first_epoch_num": 7, "file": "train.py", "lineno": 557}}
0: Closing preprocessed data file
:::MLL 1571841739.794 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 568}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-10-23 02:42:32 PM
RESULT,RNN_TRANSLATOR,,2704,nvidia,2019-10-23 01:57:28 PM
