Beginning trial 1 of 3
Gathering sys log on dss01
:::MLL 1571836606.228 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1571836606.229 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1571836606.229 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1571836606.230 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1571836606.230 submission_platform: {"value": "1xDSS8440", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1571836606.230 submission_entry: {"value": "{'hardware': 'DSS8440', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '5.0-0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz', 'num_cores': '40', 'num_vcpus': '40', 'accelerator': 'Tesla V100-PCIE-32GB', 'num_accelerators': '8', 'sys_mem_size': '754 GB', 'sys_storage_type': 'SATA SSD', 'sys_storage_size': '1x 447.1G + 1x 931.5G', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27700 Family [ConnectX-4]', 'num_network_cards': '1', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1571836606.231 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1571836606.231 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1571836612.310 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node dss01
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=4710' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=191023081507341412725 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191023081507341412725 ./run_and_time.sh
Run vars: id 191023081507341412725 gpus 8 mparams  --master_port=4710
NCCL_SOCKET_NTHREADS=2
NCCL_NSOCKS_PERTHREAD=8
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=0
STARTING TIMING RUN AT 2019-10-23 01:16:53 PM
+ MATH=amp_fp16
+ [[ 0 -eq 1 ]]
+ LAUNCH_OPT='torch.distributed.launch --nproc_per_node 8  --master_port=4710'
running benchmark
+ echo 'running benchmark'
+ python -m torch.distributed.launch --nproc_per_node 8 --master_port=4710 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1571836615.334 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571836615.334 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571836615.335 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571836615.335 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571836615.335 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571836615.335 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571836615.336 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571836615.336 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 3900573390
dss01:464:464 [0] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:464:464 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:464:464 [0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:464:464 [0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:464:464 [0] NCCL INFO NET/IB : No device found.
dss01:464:464 [0] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
NCCL version 2.4.8+cuda10.1
dss01:465:465 [1] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:465:465 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:469:469 [5] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:469:469 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:470:470 [6] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:470:470 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:466:466 [2] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:466:466 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:471:471 [7] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:471:471 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:468:468 [4] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:468:468 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:467:467 [3] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:467:467 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:469:469 [5] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:465:465 [1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:469:469 [5] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0

dss01:465:465 [1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:469:469 [5] NCCL INFO NET/IB : No device found.
dss01:465:465 [1] NCCL INFO NET/IB : No device found.

dss01:470:470 [6] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:470:470 [6] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:470:470 [6] NCCL INFO NET/IB : No device found.

dss01:466:466 [2] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:466:466 [2] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:466:466 [2] NCCL INFO NET/IB : No device found.

dss01:468:468 [4] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:468:468 [4] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:468:468 [4] NCCL INFO NET/IB : No device found.

dss01:467:467 [3] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:467:467 [3] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:467:467 [3] NCCL INFO NET/IB : No device found.

dss01:471:471 [7] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:471:471 [7] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:471:471 [7] NCCL INFO NET/IB : No device found.
dss01:470:470 [6] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:465:465 [1] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:469:469 [5] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:471:471 [7] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:468:468 [4] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:467:467 [3] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:466:466 [2] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:464:826 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555
dss01:465:827 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555
dss01:470:828 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa
dss01:469:829 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa
dss01:471:830 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa
dss01:466:831 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555
dss01:468:833 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa
dss01:467:832 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555
dss01:464:826 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:465:827 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:466:831 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:467:832 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:469:829 [5] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:468:833 [4] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:471:830 [7] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:470:828 [6] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:464:826 [0] NCCL INFO Channel 00 :    0   1   2   3   4   5   6   7
dss01:464:826 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/IPC
dss01:470:828 [6] NCCL INFO Ring 00 : 6[6] -> 7[7] via P2P/IPC
dss01:466:831 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/IPC
dss01:468:833 [4] NCCL INFO Ring 00 : 4[4] -> 5[5] via P2P/IPC
dss01:465:827 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via direct shared memory
dss01:469:829 [5] NCCL INFO Ring 00 : 5[5] -> 6[6] via direct shared memory
dss01:471:830 [7] NCCL INFO Ring 00 : 7[7] -> 0[0] via direct shared memory
dss01:467:832 [3] NCCL INFO Ring 00 : 3[3] -> 4[4] via direct shared memory
dss01:464:826 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
dss01:465:827 [1] NCCL INFO comm 0x7fff38007590 rank 1 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE
dss01:469:829 [5] NCCL INFO comm 0x7ffe90007590 rank 5 nranks 8 cudaDev 5 nvmlDev 5 - Init COMPLETE
dss01:467:832 [3] NCCL INFO comm 0x7ffe90007590 rank 3 nranks 8 cudaDev 3 nvmlDev 3 - Init COMPLETE
dss01:471:830 [7] NCCL INFO comm 0x7ffe98007590 rank 7 nranks 8 cudaDev 7 nvmlDev 7 - Init COMPLETE
dss01:468:833 [4] NCCL INFO comm 0x7fff5c007590 rank 4 nranks 8 cudaDev 4 nvmlDev 4 - Init COMPLETE
dss01:464:826 [0] NCCL INFO comm 0x7ffe30007590 rank 0 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE
dss01:464:464 [0] NCCL INFO Launch mode Parallel
0: Worker 0 is using worker seed: 351399973
0: Building vocabulary from /data/vocab.bpe.32000
dss01:470:828 [6] NCCL INFO comm 0x7ffe90007590 rank 6 nranks 8 cudaDev 6 nvmlDev 6 - Init COMPLETE
dss01:466:831 [2] NCCL INFO comm 0x7fff54007590 rank 2 nranks 8 cudaDev 2 nvmlDev 2 - Init COMPLETE
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1571836641.712 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1571836644.470 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1571836644.471 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1571836644.471 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1571836645.482 global_batch_size: {"value": 2048, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1571836645.484 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1571836645.484 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1571836645.484 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1571836645.485 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1571836645.485 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1571836645.485 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1571836645.491 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1571836645.512 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571836645.512 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 2187212035
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/1938]	Time 1.158 (1.158)	Data 8.17e-01 (8.17e-01)	Tok/s 20422 (20422)	Loss/tok 10.7639 (10.7639)	LR 2.000e-05
0: TRAIN [0][10/1938]	Time 0.215 (0.334)	Data 3.03e-04 (7.44e-02)	Tok/s 48278 (52325)	Loss/tok 9.6577 (10.2818)	LR 2.518e-05
0: TRAIN [0][20/1938]	Time 0.274 (0.294)	Data 2.00e-04 (3.90e-02)	Tok/s 60875 (52712)	Loss/tok 9.4045 (9.9425)	LR 3.170e-05
0: TRAIN [0][30/1938]	Time 0.216 (0.281)	Data 1.66e-04 (2.65e-02)	Tok/s 47571 (52109)	Loss/tok 8.9929 (9.7232)	LR 3.991e-05
0: TRAIN [0][40/1938]	Time 0.273 (0.271)	Data 1.44e-04 (2.01e-02)	Tok/s 61383 (52034)	Loss/tok 8.8455 (9.5445)	LR 5.024e-05
0: TRAIN [0][50/1938]	Time 0.273 (0.267)	Data 1.18e-04 (1.62e-02)	Tok/s 61544 (52639)	Loss/tok 8.6064 (9.3773)	LR 6.325e-05
0: TRAIN [0][60/1938]	Time 0.273 (0.264)	Data 1.22e-04 (1.35e-02)	Tok/s 61118 (52880)	Loss/tok 8.3984 (9.2254)	LR 7.962e-05
0: TRAIN [0][70/1938]	Time 0.274 (0.263)	Data 1.20e-04 (1.16e-02)	Tok/s 60637 (53107)	Loss/tok 8.1904 (9.0913)	LR 1.002e-04
0: TRAIN [0][80/1938]	Time 0.273 (0.258)	Data 1.57e-04 (1.02e-02)	Tok/s 61818 (52604)	Loss/tok 8.1011 (8.9855)	LR 1.262e-04
0: TRAIN [0][90/1938]	Time 0.403 (0.264)	Data 1.27e-04 (9.12e-03)	Tok/s 72780 (53814)	Loss/tok 8.2830 (8.8669)	LR 1.589e-04
0: TRAIN [0][100/1938]	Time 0.331 (0.264)	Data 1.21e-04 (8.23e-03)	Tok/s 69888 (54325)	Loss/tok 8.1097 (8.7818)	LR 2.000e-04
0: TRAIN [0][110/1938]	Time 0.161 (0.264)	Data 1.61e-04 (7.50e-03)	Tok/s 32948 (54605)	Loss/tok 7.3569 (8.7034)	LR 2.518e-04
0: TRAIN [0][120/1938]	Time 0.214 (0.261)	Data 1.69e-04 (6.90e-03)	Tok/s 47685 (54248)	Loss/tok 7.8021 (8.6566)	LR 3.170e-04
0: TRAIN [0][130/1938]	Time 0.273 (0.263)	Data 1.51e-04 (6.38e-03)	Tok/s 61578 (54531)	Loss/tok 7.9086 (8.5976)	LR 3.991e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][140/1938]	Time 0.159 (0.261)	Data 2.07e-04 (5.94e-03)	Tok/s 33463 (54466)	Loss/tok 7.0525 (8.5484)	LR 4.909e-04
0: TRAIN [0][150/1938]	Time 0.333 (0.262)	Data 1.27e-04 (5.55e-03)	Tok/s 69426 (54689)	Loss/tok 7.9276 (8.5012)	LR 6.181e-04
0: TRAIN [0][160/1938]	Time 0.332 (0.261)	Data 1.22e-04 (5.22e-03)	Tok/s 70018 (54776)	Loss/tok 7.8111 (8.4543)	LR 7.781e-04
0: TRAIN [0][170/1938]	Time 0.273 (0.262)	Data 1.36e-04 (4.92e-03)	Tok/s 61932 (54964)	Loss/tok 7.6928 (8.4060)	LR 9.796e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][180/1938]	Time 0.273 (0.260)	Data 1.13e-04 (4.66e-03)	Tok/s 61616 (54737)	Loss/tok 7.5403 (8.3626)	LR 1.205e-03
0: TRAIN [0][190/1938]	Time 0.214 (0.259)	Data 1.36e-04 (4.42e-03)	Tok/s 47634 (54683)	Loss/tok 7.0747 (8.3150)	LR 1.517e-03
0: TRAIN [0][200/1938]	Time 0.215 (0.260)	Data 1.88e-04 (4.21e-03)	Tok/s 48417 (54853)	Loss/tok 6.8546 (8.2566)	LR 1.910e-03
0: TRAIN [0][210/1938]	Time 0.332 (0.261)	Data 1.17e-04 (4.01e-03)	Tok/s 70712 (55213)	Loss/tok 7.1647 (8.1887)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.332 (0.261)	Data 1.07e-04 (3.84e-03)	Tok/s 69282 (55129)	Loss/tok 7.0487 (8.1361)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.274 (0.262)	Data 1.18e-04 (3.68e-03)	Tok/s 61188 (55357)	Loss/tok 6.7120 (8.0715)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.273 (0.262)	Data 1.34e-04 (3.53e-03)	Tok/s 61289 (55440)	Loss/tok 6.5763 (8.0076)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.273 (0.262)	Data 1.28e-04 (3.40e-03)	Tok/s 62069 (55499)	Loss/tok 6.4466 (7.9448)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.216 (0.262)	Data 1.09e-04 (3.27e-03)	Tok/s 48143 (55518)	Loss/tok 6.0937 (7.8846)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.273 (0.262)	Data 1.26e-04 (3.16e-03)	Tok/s 61384 (55609)	Loss/tok 6.2608 (7.8237)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.402 (0.263)	Data 1.36e-04 (3.05e-03)	Tok/s 73696 (55661)	Loss/tok 6.4600 (7.7608)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.215 (0.262)	Data 1.20e-04 (2.95e-03)	Tok/s 48507 (55553)	Loss/tok 5.8356 (7.7068)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.274 (0.262)	Data 1.12e-04 (2.86e-03)	Tok/s 61571 (55614)	Loss/tok 5.8818 (7.6458)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.273 (0.261)	Data 1.59e-04 (2.77e-03)	Tok/s 61084 (55420)	Loss/tok 5.8658 (7.5983)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.273 (0.261)	Data 1.13e-04 (2.69e-03)	Tok/s 61836 (55457)	Loss/tok 5.7739 (7.5420)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.216 (0.259)	Data 1.40e-04 (2.61e-03)	Tok/s 47127 (55213)	Loss/tok 5.4141 (7.4969)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.160 (0.259)	Data 1.16e-04 (2.54e-03)	Tok/s 32841 (55165)	Loss/tok 4.4570 (7.4449)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.273 (0.259)	Data 1.59e-04 (2.47e-03)	Tok/s 61886 (55161)	Loss/tok 5.5502 (7.3931)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.215 (0.259)	Data 1.13e-04 (2.40e-03)	Tok/s 48745 (55207)	Loss/tok 5.2880 (7.3372)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.215 (0.259)	Data 1.11e-04 (2.34e-03)	Tok/s 46501 (55222)	Loss/tok 4.8909 (7.2847)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.214 (0.258)	Data 1.13e-04 (2.28e-03)	Tok/s 48435 (55174)	Loss/tok 4.9442 (7.2346)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.274 (0.258)	Data 1.26e-04 (2.23e-03)	Tok/s 61512 (55146)	Loss/tok 5.0823 (7.1842)	LR 2.000e-03
0: TRAIN [0][400/1938]	Time 0.274 (0.257)	Data 1.26e-04 (2.18e-03)	Tok/s 61805 (55059)	Loss/tok 4.9605 (7.1376)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.274 (0.257)	Data 1.13e-04 (2.13e-03)	Tok/s 62693 (55059)	Loss/tok 4.8272 (7.0867)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.216 (0.257)	Data 1.40e-04 (2.08e-03)	Tok/s 47336 (54973)	Loss/tok 4.7085 (7.0405)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.215 (0.256)	Data 1.43e-04 (2.04e-03)	Tok/s 48909 (54843)	Loss/tok 4.4918 (6.9999)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.273 (0.256)	Data 1.12e-04 (1.99e-03)	Tok/s 61168 (54869)	Loss/tok 4.8863 (6.9497)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.215 (0.256)	Data 1.31e-04 (1.95e-03)	Tok/s 47374 (54812)	Loss/tok 4.2996 (6.9038)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.216 (0.256)	Data 1.16e-04 (1.91e-03)	Tok/s 48234 (54840)	Loss/tok 4.3448 (6.8541)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.275 (0.257)	Data 1.24e-04 (1.88e-03)	Tok/s 61473 (55005)	Loss/tok 4.6236 (6.7962)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.275 (0.256)	Data 1.18e-04 (1.84e-03)	Tok/s 60440 (54878)	Loss/tok 4.5786 (6.7581)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.274 (0.257)	Data 1.92e-04 (1.81e-03)	Tok/s 60809 (55064)	Loss/tok 4.4853 (6.7014)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.275 (0.257)	Data 1.15e-04 (1.77e-03)	Tok/s 60517 (55070)	Loss/tok 4.5786 (6.6581)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.215 (0.256)	Data 1.17e-04 (1.74e-03)	Tok/s 46911 (54921)	Loss/tok 4.0380 (6.6248)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.160 (0.256)	Data 1.50e-04 (1.71e-03)	Tok/s 33306 (54766)	Loss/tok 3.3994 (6.5915)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.275 (0.256)	Data 1.50e-04 (1.68e-03)	Tok/s 60984 (54760)	Loss/tok 4.5855 (6.5524)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.160 (0.255)	Data 1.24e-04 (1.65e-03)	Tok/s 33294 (54665)	Loss/tok 3.5450 (6.5181)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.274 (0.256)	Data 1.17e-04 (1.62e-03)	Tok/s 60986 (54769)	Loss/tok 4.5009 (6.4742)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.215 (0.256)	Data 1.51e-04 (1.60e-03)	Tok/s 48297 (54833)	Loss/tok 4.1495 (6.4332)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.161 (0.256)	Data 1.42e-04 (1.57e-03)	Tok/s 32980 (54808)	Loss/tok 3.4390 (6.3981)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][580/1938]	Time 0.274 (0.256)	Data 1.71e-04 (1.55e-03)	Tok/s 61019 (54770)	Loss/tok 4.2947 (6.3647)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.216 (0.255)	Data 1.13e-04 (1.52e-03)	Tok/s 47885 (54664)	Loss/tok 4.0222 (6.3357)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.276 (0.255)	Data 1.20e-04 (1.50e-03)	Tok/s 60547 (54771)	Loss/tok 4.2661 (6.2950)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.274 (0.256)	Data 1.28e-04 (1.48e-03)	Tok/s 61379 (54825)	Loss/tok 4.2062 (6.2585)	LR 2.000e-03
0: TRAIN [0][620/1938]	Time 0.217 (0.256)	Data 1.38e-04 (1.46e-03)	Tok/s 48348 (54791)	Loss/tok 4.1236 (6.2284)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.334 (0.255)	Data 1.21e-04 (1.44e-03)	Tok/s 70756 (54681)	Loss/tok 4.5196 (6.2013)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.274 (0.256)	Data 1.30e-04 (1.42e-03)	Tok/s 61243 (54757)	Loss/tok 4.1944 (6.1662)	LR 2.000e-03
0: TRAIN [0][650/1938]	Time 0.407 (0.256)	Data 1.22e-04 (1.40e-03)	Tok/s 74147 (54892)	Loss/tok 4.4008 (6.1283)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.216 (0.256)	Data 1.18e-04 (1.38e-03)	Tok/s 46832 (54902)	Loss/tok 3.9279 (6.0981)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.215 (0.256)	Data 5.52e-04 (1.36e-03)	Tok/s 48235 (54923)	Loss/tok 3.8859 (6.0686)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.216 (0.256)	Data 1.17e-04 (1.34e-03)	Tok/s 46832 (54914)	Loss/tok 3.9842 (6.0412)	LR 2.000e-03
0: TRAIN [0][690/1938]	Time 0.216 (0.256)	Data 1.18e-04 (1.32e-03)	Tok/s 49098 (54847)	Loss/tok 3.8032 (6.0169)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.274 (0.256)	Data 1.13e-04 (1.31e-03)	Tok/s 61911 (54821)	Loss/tok 4.0782 (5.9906)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.216 (0.256)	Data 1.12e-04 (1.29e-03)	Tok/s 48762 (54857)	Loss/tok 3.6871 (5.9620)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.274 (0.256)	Data 1.40e-04 (1.27e-03)	Tok/s 61554 (54872)	Loss/tok 4.0826 (5.9351)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.276 (0.256)	Data 3.26e-04 (1.26e-03)	Tok/s 61148 (54889)	Loss/tok 4.1373 (5.9089)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.216 (0.256)	Data 1.26e-04 (1.24e-03)	Tok/s 47517 (54919)	Loss/tok 3.8217 (5.8825)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.334 (0.256)	Data 1.17e-04 (1.23e-03)	Tok/s 69566 (54922)	Loss/tok 4.3302 (5.8578)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.334 (0.257)	Data 1.35e-04 (1.21e-03)	Tok/s 70168 (54972)	Loss/tok 4.2258 (5.8321)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.407 (0.257)	Data 1.29e-04 (1.20e-03)	Tok/s 73049 (54957)	Loss/tok 4.4287 (5.8090)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.274 (0.256)	Data 1.13e-04 (1.19e-03)	Tok/s 60619 (54921)	Loss/tok 4.0573 (5.7876)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.274 (0.256)	Data 1.30e-04 (1.17e-03)	Tok/s 62141 (54910)	Loss/tok 3.9903 (5.7659)	LR 2.000e-03
0: TRAIN [0][800/1938]	Time 0.275 (0.256)	Data 1.38e-04 (1.16e-03)	Tok/s 61031 (54883)	Loss/tok 3.9342 (5.7453)	LR 2.000e-03
0: TRAIN [0][810/1938]	Time 0.275 (0.256)	Data 1.11e-04 (1.15e-03)	Tok/s 61027 (54917)	Loss/tok 3.9317 (5.7213)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.275 (0.256)	Data 1.19e-04 (1.14e-03)	Tok/s 61872 (54888)	Loss/tok 3.9092 (5.7008)	LR 2.000e-03
0: TRAIN [0][830/1938]	Time 0.216 (0.256)	Data 1.51e-04 (1.12e-03)	Tok/s 47932 (54864)	Loss/tok 3.7453 (5.6812)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.216 (0.256)	Data 1.12e-04 (1.11e-03)	Tok/s 48114 (54805)	Loss/tok 3.6249 (5.6630)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][850/1938]	Time 0.215 (0.256)	Data 1.28e-04 (1.10e-03)	Tok/s 47701 (54868)	Loss/tok 3.7727 (5.6396)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.157 (0.256)	Data 1.18e-04 (1.09e-03)	Tok/s 33860 (54882)	Loss/tok 3.0846 (5.6185)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][870/1938]	Time 0.160 (0.256)	Data 1.27e-04 (1.08e-03)	Tok/s 33013 (54925)	Loss/tok 3.1625 (5.5966)	LR 2.000e-03
0: TRAIN [0][880/1938]	Time 0.334 (0.256)	Data 1.26e-04 (1.07e-03)	Tok/s 69786 (54912)	Loss/tok 4.2473 (5.5786)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.275 (0.257)	Data 1.24e-04 (1.06e-03)	Tok/s 61253 (54957)	Loss/tok 3.8431 (5.5573)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.215 (0.256)	Data 1.17e-04 (1.05e-03)	Tok/s 46644 (54913)	Loss/tok 3.6080 (5.5412)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.335 (0.256)	Data 1.33e-04 (1.04e-03)	Tok/s 70402 (54914)	Loss/tok 4.1497 (5.5232)	LR 2.000e-03
0: TRAIN [0][920/1938]	Time 0.159 (0.256)	Data 1.16e-04 (1.03e-03)	Tok/s 33509 (54801)	Loss/tok 3.0258 (5.5100)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.215 (0.256)	Data 1.75e-04 (1.02e-03)	Tok/s 47782 (54809)	Loss/tok 3.6839 (5.4923)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.215 (0.256)	Data 1.16e-04 (1.01e-03)	Tok/s 47645 (54911)	Loss/tok 3.4416 (5.4699)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.216 (0.256)	Data 1.59e-04 (1.00e-03)	Tok/s 47612 (54951)	Loss/tok 3.4988 (5.4514)	LR 2.000e-03
0: TRAIN [0][960/1938]	Time 0.334 (0.256)	Data 1.29e-04 (9.92e-04)	Tok/s 68810 (54977)	Loss/tok 4.1126 (5.4342)	LR 2.000e-03
0: TRAIN [0][970/1938]	Time 0.274 (0.256)	Data 1.32e-04 (9.83e-04)	Tok/s 61054 (54924)	Loss/tok 3.7413 (5.4198)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.215 (0.256)	Data 1.12e-04 (9.74e-04)	Tok/s 48903 (54923)	Loss/tok 3.6695 (5.4037)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.275 (0.256)	Data 1.63e-04 (9.66e-04)	Tok/s 60565 (54916)	Loss/tok 3.8316 (5.3877)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.334 (0.256)	Data 2.22e-04 (9.58e-04)	Tok/s 69629 (54932)	Loss/tok 4.0270 (5.3715)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.216 (0.256)	Data 1.30e-04 (9.49e-04)	Tok/s 49118 (54922)	Loss/tok 3.5505 (5.3565)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.407 (0.256)	Data 1.51e-04 (9.41e-04)	Tok/s 73339 (54911)	Loss/tok 4.2131 (5.3422)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.216 (0.256)	Data 1.04e-04 (9.33e-04)	Tok/s 47354 (54891)	Loss/tok 3.5735 (5.3283)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.216 (0.256)	Data 1.10e-04 (9.26e-04)	Tok/s 48588 (54882)	Loss/tok 3.5015 (5.3139)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.274 (0.256)	Data 1.09e-04 (9.18e-04)	Tok/s 62234 (54862)	Loss/tok 3.8141 (5.3004)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.216 (0.256)	Data 1.30e-04 (9.11e-04)	Tok/s 47955 (54864)	Loss/tok 3.4444 (5.2867)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.216 (0.255)	Data 1.15e-04 (9.04e-04)	Tok/s 48042 (54822)	Loss/tok 3.5033 (5.2740)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.216 (0.255)	Data 1.15e-04 (8.97e-04)	Tok/s 48386 (54824)	Loss/tok 3.4381 (5.2600)	LR 2.000e-03
0: TRAIN [0][1090/1938]	Time 0.216 (0.255)	Data 1.24e-04 (8.90e-04)	Tok/s 48575 (54816)	Loss/tok 3.5722 (5.2466)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.216 (0.255)	Data 1.19e-04 (8.83e-04)	Tok/s 47423 (54819)	Loss/tok 3.5587 (5.2330)	LR 2.000e-03
0: TRAIN [0][1110/1938]	Time 0.275 (0.256)	Data 1.25e-04 (8.76e-04)	Tok/s 60811 (54849)	Loss/tok 3.9892 (5.2189)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.335 (0.256)	Data 1.25e-04 (8.70e-04)	Tok/s 69485 (54912)	Loss/tok 4.0813 (5.2042)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.274 (0.256)	Data 1.19e-04 (8.63e-04)	Tok/s 61607 (54893)	Loss/tok 3.7680 (5.1925)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.335 (0.256)	Data 1.36e-04 (8.57e-04)	Tok/s 69654 (54962)	Loss/tok 3.9795 (5.1774)	LR 2.000e-03
0: TRAIN [0][1150/1938]	Time 0.275 (0.256)	Data 1.24e-04 (8.50e-04)	Tok/s 60478 (54982)	Loss/tok 3.7423 (5.1647)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.275 (0.256)	Data 1.31e-04 (8.44e-04)	Tok/s 60220 (54977)	Loss/tok 3.7144 (5.1527)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.162 (0.256)	Data 1.65e-04 (8.38e-04)	Tok/s 31915 (54947)	Loss/tok 2.8241 (5.1414)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.215 (0.256)	Data 1.21e-04 (8.32e-04)	Tok/s 48060 (54919)	Loss/tok 3.4816 (5.1304)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.274 (0.256)	Data 1.22e-04 (8.26e-04)	Tok/s 61739 (54936)	Loss/tok 3.6620 (5.1182)	LR 2.000e-03
0: TRAIN [0][1200/1938]	Time 0.334 (0.256)	Data 1.16e-04 (8.20e-04)	Tok/s 68917 (54965)	Loss/tok 3.9693 (5.1057)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.216 (0.256)	Data 1.18e-04 (8.15e-04)	Tok/s 48038 (54921)	Loss/tok 3.3534 (5.0956)	LR 2.000e-03
0: TRAIN [0][1220/1938]	Time 0.334 (0.256)	Data 1.14e-04 (8.09e-04)	Tok/s 68919 (54944)	Loss/tok 3.9403 (5.0836)	LR 2.000e-03
0: TRAIN [0][1230/1938]	Time 0.276 (0.256)	Data 1.40e-04 (8.04e-04)	Tok/s 61503 (54945)	Loss/tok 3.6720 (5.0724)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.275 (0.256)	Data 1.36e-04 (7.98e-04)	Tok/s 61606 (54887)	Loss/tok 3.6937 (5.0630)	LR 2.000e-03
0: TRAIN [0][1250/1938]	Time 0.216 (0.256)	Data 1.27e-04 (7.93e-04)	Tok/s 47755 (54909)	Loss/tok 3.5220 (5.0517)	LR 2.000e-03
0: TRAIN [0][1260/1938]	Time 0.217 (0.256)	Data 1.24e-04 (7.88e-04)	Tok/s 47439 (54916)	Loss/tok 3.4138 (5.0403)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.275 (0.256)	Data 1.24e-04 (7.83e-04)	Tok/s 60651 (54927)	Loss/tok 3.7123 (5.0291)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1280/1938]	Time 0.216 (0.256)	Data 1.48e-04 (7.78e-04)	Tok/s 47839 (54951)	Loss/tok 3.4764 (5.0177)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.216 (0.256)	Data 1.37e-04 (7.73e-04)	Tok/s 48245 (54905)	Loss/tok 3.6312 (5.0087)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.276 (0.256)	Data 1.28e-04 (7.68e-04)	Tok/s 60519 (54916)	Loss/tok 3.6829 (4.9984)	LR 2.000e-03
0: TRAIN [0][1310/1938]	Time 0.162 (0.255)	Data 1.20e-04 (7.63e-04)	Tok/s 32551 (54865)	Loss/tok 2.7183 (4.9898)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.216 (0.255)	Data 1.58e-04 (7.59e-04)	Tok/s 47788 (54837)	Loss/tok 3.4427 (4.9809)	LR 2.000e-03
0: TRAIN [0][1330/1938]	Time 0.216 (0.255)	Data 1.13e-04 (7.54e-04)	Tok/s 48808 (54811)	Loss/tok 3.4017 (4.9718)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.160 (0.255)	Data 1.35e-04 (7.50e-04)	Tok/s 33098 (54814)	Loss/tok 2.8813 (4.9620)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.215 (0.255)	Data 1.43e-04 (7.45e-04)	Tok/s 48437 (54797)	Loss/tok 3.3924 (4.9529)	LR 2.000e-03
0: TRAIN [0][1360/1938]	Time 0.275 (0.255)	Data 1.20e-04 (7.41e-04)	Tok/s 61417 (54798)	Loss/tok 3.7081 (4.9435)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.216 (0.255)	Data 1.43e-04 (7.37e-04)	Tok/s 48077 (54791)	Loss/tok 3.5045 (4.9344)	LR 2.000e-03
0: TRAIN [0][1380/1938]	Time 0.275 (0.255)	Data 1.33e-04 (7.33e-04)	Tok/s 61593 (54796)	Loss/tok 3.7967 (4.9252)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.275 (0.255)	Data 1.33e-04 (7.28e-04)	Tok/s 61145 (54833)	Loss/tok 3.7053 (4.9147)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.275 (0.255)	Data 1.18e-04 (7.24e-04)	Tok/s 60849 (54877)	Loss/tok 3.6354 (4.9046)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.275 (0.255)	Data 1.28e-04 (7.20e-04)	Tok/s 61211 (54896)	Loss/tok 3.6141 (4.8947)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.275 (0.255)	Data 1.49e-04 (7.16e-04)	Tok/s 61656 (54924)	Loss/tok 3.6759 (4.8852)	LR 2.000e-03
0: TRAIN [0][1430/1938]	Time 0.216 (0.255)	Data 1.18e-04 (7.12e-04)	Tok/s 47415 (54919)	Loss/tok 3.5691 (4.8767)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1440/1938]	Time 0.273 (0.255)	Data 1.57e-04 (7.08e-04)	Tok/s 61015 (54923)	Loss/tok 3.7056 (4.8682)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1450/1938]	Time 0.215 (0.255)	Data 1.28e-04 (7.05e-04)	Tok/s 48653 (54947)	Loss/tok 3.4715 (4.8590)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.216 (0.255)	Data 1.47e-04 (7.01e-04)	Tok/s 48450 (54931)	Loss/tok 3.3564 (4.8512)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.274 (0.255)	Data 1.34e-04 (6.97e-04)	Tok/s 61085 (54891)	Loss/tok 3.7605 (4.8441)	LR 2.000e-03
0: TRAIN [0][1480/1938]	Time 0.215 (0.255)	Data 1.17e-04 (6.94e-04)	Tok/s 47626 (54895)	Loss/tok 3.3331 (4.8357)	LR 2.000e-03
0: TRAIN [0][1490/1938]	Time 0.216 (0.255)	Data 1.46e-04 (6.90e-04)	Tok/s 48710 (54863)	Loss/tok 3.4208 (4.8285)	LR 2.000e-03
0: TRAIN [0][1500/1938]	Time 0.215 (0.255)	Data 1.22e-04 (6.86e-04)	Tok/s 48531 (54858)	Loss/tok 3.3752 (4.8208)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.160 (0.255)	Data 1.29e-04 (6.82e-04)	Tok/s 33199 (54837)	Loss/tok 2.8409 (4.8133)	LR 2.000e-03
0: TRAIN [0][1520/1938]	Time 0.215 (0.255)	Data 1.57e-04 (6.79e-04)	Tok/s 48579 (54813)	Loss/tok 3.4836 (4.8061)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.217 (0.255)	Data 1.17e-04 (6.76e-04)	Tok/s 48407 (54821)	Loss/tok 3.3799 (4.7983)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.216 (0.255)	Data 1.17e-04 (6.73e-04)	Tok/s 48147 (54826)	Loss/tok 3.3517 (4.7905)	LR 2.000e-03
0: TRAIN [0][1550/1938]	Time 0.218 (0.255)	Data 1.24e-04 (6.69e-04)	Tok/s 47562 (54810)	Loss/tok 3.3616 (4.7833)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.274 (0.255)	Data 1.31e-04 (6.66e-04)	Tok/s 61363 (54811)	Loss/tok 3.5999 (4.7759)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.275 (0.255)	Data 1.30e-04 (6.63e-04)	Tok/s 61735 (54818)	Loss/tok 3.5897 (4.7680)	LR 2.000e-03
0: TRAIN [0][1580/1938]	Time 0.162 (0.255)	Data 1.16e-04 (6.59e-04)	Tok/s 31789 (54787)	Loss/tok 3.0268 (4.7615)	LR 2.000e-03
0: TRAIN [0][1590/1938]	Time 0.160 (0.255)	Data 1.13e-04 (6.56e-04)	Tok/s 32913 (54803)	Loss/tok 2.8721 (4.7536)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.407 (0.255)	Data 1.12e-04 (6.52e-04)	Tok/s 73358 (54810)	Loss/tok 3.9461 (4.7463)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.216 (0.255)	Data 1.17e-04 (6.49e-04)	Tok/s 46920 (54819)	Loss/tok 3.3887 (4.7391)	LR 2.000e-03
0: TRAIN [0][1620/1938]	Time 0.216 (0.255)	Data 1.17e-04 (6.46e-04)	Tok/s 48785 (54838)	Loss/tok 3.3161 (4.7313)	LR 2.000e-03
0: TRAIN [0][1630/1938]	Time 0.335 (0.255)	Data 1.32e-04 (6.43e-04)	Tok/s 69853 (54832)	Loss/tok 3.8125 (4.7245)	LR 2.000e-03
0: TRAIN [0][1640/1938]	Time 0.216 (0.255)	Data 1.38e-04 (6.40e-04)	Tok/s 47088 (54843)	Loss/tok 3.3600 (4.7178)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.274 (0.255)	Data 1.22e-04 (6.37e-04)	Tok/s 61076 (54841)	Loss/tok 3.6271 (4.7109)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.216 (0.255)	Data 1.66e-04 (6.34e-04)	Tok/s 48806 (54853)	Loss/tok 3.4103 (4.7037)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.216 (0.255)	Data 2.00e-04 (6.31e-04)	Tok/s 48406 (54847)	Loss/tok 3.2879 (4.6971)	LR 2.000e-03
0: TRAIN [0][1680/1938]	Time 0.216 (0.255)	Data 1.30e-04 (6.28e-04)	Tok/s 47427 (54822)	Loss/tok 3.2182 (4.6908)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.274 (0.255)	Data 1.21e-04 (6.25e-04)	Tok/s 62305 (54816)	Loss/tok 3.6174 (4.6843)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.332 (0.255)	Data 1.24e-04 (6.22e-04)	Tok/s 70313 (54809)	Loss/tok 3.7733 (4.6779)	LR 2.000e-03
0: TRAIN [0][1710/1938]	Time 0.275 (0.255)	Data 1.19e-04 (6.19e-04)	Tok/s 60880 (54801)	Loss/tok 3.5859 (4.6716)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.274 (0.255)	Data 1.39e-04 (6.17e-04)	Tok/s 60698 (54811)	Loss/tok 3.5311 (4.6650)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.273 (0.255)	Data 1.17e-04 (6.14e-04)	Tok/s 61064 (54814)	Loss/tok 3.6049 (4.6587)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1740/1938]	Time 0.213 (0.254)	Data 1.34e-04 (6.11e-04)	Tok/s 48276 (54794)	Loss/tok 3.4301 (4.6529)	LR 2.000e-03
0: TRAIN [0][1750/1938]	Time 0.275 (0.255)	Data 1.17e-04 (6.08e-04)	Tok/s 60895 (54797)	Loss/tok 3.6065 (4.6468)	LR 2.000e-03
0: TRAIN [0][1760/1938]	Time 0.274 (0.255)	Data 1.26e-04 (6.06e-04)	Tok/s 62195 (54821)	Loss/tok 3.4818 (4.6398)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.275 (0.255)	Data 1.17e-04 (6.03e-04)	Tok/s 61340 (54825)	Loss/tok 3.5172 (4.6334)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.216 (0.255)	Data 1.29e-04 (6.00e-04)	Tok/s 47753 (54822)	Loss/tok 3.3497 (4.6276)	LR 2.000e-03
0: TRAIN [0][1790/1938]	Time 0.406 (0.255)	Data 1.20e-04 (5.98e-04)	Tok/s 72691 (54824)	Loss/tok 4.0767 (4.6214)	LR 2.000e-03
0: TRAIN [0][1800/1938]	Time 0.274 (0.255)	Data 1.49e-04 (5.95e-04)	Tok/s 60354 (54823)	Loss/tok 3.4751 (4.6153)	LR 2.000e-03
0: TRAIN [0][1810/1938]	Time 0.275 (0.255)	Data 1.42e-04 (5.93e-04)	Tok/s 61114 (54809)	Loss/tok 3.4021 (4.6098)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.216 (0.255)	Data 1.38e-04 (5.91e-04)	Tok/s 47563 (54824)	Loss/tok 3.1728 (4.6039)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.215 (0.255)	Data 1.27e-04 (5.88e-04)	Tok/s 47580 (54861)	Loss/tok 3.2570 (4.5972)	LR 2.000e-03
0: TRAIN [0][1840/1938]	Time 0.216 (0.255)	Data 1.34e-04 (5.86e-04)	Tok/s 47036 (54858)	Loss/tok 3.3259 (4.5920)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.274 (0.255)	Data 1.53e-04 (5.83e-04)	Tok/s 60494 (54844)	Loss/tok 3.5666 (4.5868)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.216 (0.255)	Data 2.36e-04 (5.81e-04)	Tok/s 47463 (54826)	Loss/tok 3.3711 (4.5814)	LR 2.000e-03
0: TRAIN [0][1870/1938]	Time 0.215 (0.255)	Data 1.21e-04 (5.78e-04)	Tok/s 48044 (54843)	Loss/tok 3.2560 (4.5755)	LR 2.000e-03
0: TRAIN [0][1880/1938]	Time 0.215 (0.255)	Data 1.20e-04 (5.76e-04)	Tok/s 47948 (54851)	Loss/tok 3.2593 (4.5699)	LR 2.000e-03
0: TRAIN [0][1890/1938]	Time 0.216 (0.255)	Data 1.29e-04 (5.74e-04)	Tok/s 48008 (54830)	Loss/tok 3.2986 (4.5648)	LR 2.000e-03
0: TRAIN [0][1900/1938]	Time 0.219 (0.255)	Data 1.20e-04 (5.72e-04)	Tok/s 46579 (54814)	Loss/tok 3.2677 (4.5598)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1910/1938]	Time 0.407 (0.255)	Data 1.41e-04 (5.69e-04)	Tok/s 73240 (54812)	Loss/tok 4.0274 (4.5545)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.216 (0.255)	Data 1.40e-04 (5.67e-04)	Tok/s 47865 (54807)	Loss/tok 3.3008 (4.5492)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.275 (0.255)	Data 1.27e-04 (5.65e-04)	Tok/s 61377 (54799)	Loss/tok 3.5722 (4.5439)	LR 2.000e-03
:::MLL 1571837140.272 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1571837140.273 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.714 (0.714)	Decoder iters 149.0 (149.0)	Tok/s 21707 (21707)
0: Running moses detokenizer
0: BLEU(score=19.60354072571181, counts=[33538, 15318, 8159, 4546], totals=[62082, 59079, 56076, 53077], precisions=[54.02209980348571, 25.92799471893566, 14.54989656894215, 8.564915123311415], bp=0.9590774485152627, sys_len=62082, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571837142.179 eval_accuracy: {"value": 19.6, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1571837142.179 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 4.5385	Test BLEU: 19.60
0: Performance: Epoch: 0	Training: 438694 Tok/s
0: Finished epoch 0
:::MLL 1571837142.180 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1571837142.180 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571837142.180 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 1608383879
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [1][0/1938]	Time 0.946 (0.946)	Data 7.18e-01 (7.18e-01)	Tok/s 10844 (10844)	Loss/tok 3.2267 (3.2267)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.275 (0.299)	Data 1.06e-04 (6.54e-02)	Tok/s 60510 (47852)	Loss/tok 3.4544 (3.3531)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.215 (0.276)	Data 1.17e-04 (3.43e-02)	Tok/s 48801 (51513)	Loss/tok 3.1232 (3.3869)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.215 (0.274)	Data 1.04e-04 (2.33e-02)	Tok/s 48170 (53734)	Loss/tok 3.3151 (3.4320)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.273 (0.263)	Data 1.01e-04 (1.76e-02)	Tok/s 60824 (52875)	Loss/tok 3.4514 (3.4123)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.215 (0.260)	Data 1.00e-04 (1.42e-02)	Tok/s 47495 (52548)	Loss/tok 3.1024 (3.4186)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.334 (0.262)	Data 1.05e-04 (1.19e-02)	Tok/s 69725 (53132)	Loss/tok 3.5948 (3.4439)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.275 (0.266)	Data 1.14e-04 (1.02e-02)	Tok/s 61247 (54146)	Loss/tok 3.4424 (3.4647)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.275 (0.263)	Data 1.04e-04 (8.99e-03)	Tok/s 60525 (54038)	Loss/tok 3.4532 (3.4522)	LR 2.000e-03
0: TRAIN [1][90/1938]	Time 0.408 (0.263)	Data 1.14e-04 (8.02e-03)	Tok/s 72366 (54345)	Loss/tok 3.8329 (3.4528)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.215 (0.259)	Data 1.24e-04 (7.24e-03)	Tok/s 47211 (53859)	Loss/tok 3.2366 (3.4463)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.215 (0.258)	Data 1.45e-04 (6.60e-03)	Tok/s 48481 (53723)	Loss/tok 3.2472 (3.4495)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.274 (0.260)	Data 1.21e-04 (6.07e-03)	Tok/s 61268 (54048)	Loss/tok 3.4739 (3.4599)	LR 2.000e-03
0: TRAIN [1][130/1938]	Time 0.217 (0.259)	Data 1.25e-04 (5.62e-03)	Tok/s 48058 (54134)	Loss/tok 3.1965 (3.4571)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.275 (0.261)	Data 1.43e-04 (5.23e-03)	Tok/s 60981 (54605)	Loss/tok 3.4686 (3.4665)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.275 (0.262)	Data 1.53e-04 (4.89e-03)	Tok/s 61038 (54783)	Loss/tok 3.4911 (3.4656)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.275 (0.259)	Data 1.38e-04 (4.60e-03)	Tok/s 62390 (54416)	Loss/tok 3.4239 (3.4584)	LR 2.000e-03
0: TRAIN [1][170/1938]	Time 0.216 (0.258)	Data 1.14e-04 (4.34e-03)	Tok/s 47291 (54304)	Loss/tok 3.3060 (3.4545)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.216 (0.259)	Data 1.17e-04 (4.10e-03)	Tok/s 48347 (54562)	Loss/tok 3.2354 (3.4605)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.160 (0.259)	Data 1.31e-04 (3.90e-03)	Tok/s 32536 (54511)	Loss/tok 2.8084 (3.4588)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.274 (0.259)	Data 1.26e-04 (3.71e-03)	Tok/s 61477 (54607)	Loss/tok 3.4353 (3.4616)	LR 2.000e-03
0: TRAIN [1][210/1938]	Time 0.216 (0.258)	Data 1.18e-04 (3.54e-03)	Tok/s 48929 (54581)	Loss/tok 3.3275 (3.4591)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.216 (0.259)	Data 1.41e-04 (3.39e-03)	Tok/s 48547 (54751)	Loss/tok 3.3270 (3.4583)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.216 (0.257)	Data 1.17e-04 (3.25e-03)	Tok/s 48918 (54543)	Loss/tok 3.3421 (3.4537)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.334 (0.257)	Data 1.35e-04 (3.12e-03)	Tok/s 70393 (54579)	Loss/tok 3.6687 (3.4550)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.161 (0.257)	Data 3.62e-04 (3.00e-03)	Tok/s 31810 (54441)	Loss/tok 2.6997 (3.4545)	LR 2.000e-03
0: TRAIN [1][260/1938]	Time 0.215 (0.256)	Data 1.37e-04 (2.89e-03)	Tok/s 48868 (54262)	Loss/tok 3.1900 (3.4507)	LR 2.000e-03
0: TRAIN [1][270/1938]	Time 0.334 (0.256)	Data 1.59e-04 (2.79e-03)	Tok/s 69580 (54431)	Loss/tok 3.6055 (3.4535)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.161 (0.256)	Data 1.20e-04 (2.70e-03)	Tok/s 33071 (54381)	Loss/tok 2.7827 (3.4526)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.335 (0.256)	Data 2.03e-04 (2.61e-03)	Tok/s 68941 (54402)	Loss/tok 3.6776 (3.4512)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.272 (0.256)	Data 1.43e-04 (2.53e-03)	Tok/s 61540 (54484)	Loss/tok 3.5408 (3.4512)	LR 2.000e-03
0: TRAIN [1][310/1938]	Time 0.276 (0.256)	Data 1.43e-04 (2.45e-03)	Tok/s 60839 (54470)	Loss/tok 3.5117 (3.4513)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.276 (0.256)	Data 1.54e-04 (2.38e-03)	Tok/s 61034 (54525)	Loss/tok 3.4818 (3.4520)	LR 2.000e-03
0: TRAIN [1][330/1938]	Time 0.276 (0.256)	Data 1.68e-04 (2.32e-03)	Tok/s 60725 (54549)	Loss/tok 3.4093 (3.4507)	LR 2.000e-03
0: TRAIN [1][340/1938]	Time 0.273 (0.256)	Data 1.22e-04 (2.25e-03)	Tok/s 61633 (54652)	Loss/tok 3.4597 (3.4504)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.275 (0.256)	Data 1.47e-04 (2.19e-03)	Tok/s 60949 (54622)	Loss/tok 3.5004 (3.4488)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.216 (0.255)	Data 1.23e-04 (2.14e-03)	Tok/s 48158 (54556)	Loss/tok 3.1155 (3.4490)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.216 (0.255)	Data 1.44e-04 (2.08e-03)	Tok/s 48236 (54614)	Loss/tok 3.2402 (3.4476)	LR 2.000e-03
0: TRAIN [1][380/1938]	Time 0.275 (0.256)	Data 1.45e-04 (2.03e-03)	Tok/s 61429 (54696)	Loss/tok 3.4392 (3.4470)	LR 2.000e-03
0: TRAIN [1][390/1938]	Time 0.334 (0.256)	Data 1.61e-04 (1.99e-03)	Tok/s 69397 (54762)	Loss/tok 3.7194 (3.4475)	LR 2.000e-03
0: TRAIN [1][400/1938]	Time 0.161 (0.256)	Data 1.41e-04 (1.94e-03)	Tok/s 32634 (54691)	Loss/tok 2.8134 (3.4475)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.216 (0.256)	Data 1.68e-04 (1.90e-03)	Tok/s 47299 (54810)	Loss/tok 3.2044 (3.4498)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.216 (0.256)	Data 1.23e-04 (1.85e-03)	Tok/s 47849 (54782)	Loss/tok 3.2436 (3.4480)	LR 2.000e-03
0: TRAIN [1][430/1938]	Time 0.215 (0.256)	Data 1.60e-04 (1.82e-03)	Tok/s 48808 (54806)	Loss/tok 3.2008 (3.4495)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.275 (0.256)	Data 1.33e-04 (1.78e-03)	Tok/s 61851 (54819)	Loss/tok 3.3731 (3.4484)	LR 2.000e-03
0: TRAIN [1][450/1938]	Time 0.215 (0.256)	Data 1.58e-04 (1.74e-03)	Tok/s 48608 (54757)	Loss/tok 3.3236 (3.4456)	LR 2.000e-03
0: TRAIN [1][460/1938]	Time 0.275 (0.256)	Data 1.46e-04 (1.71e-03)	Tok/s 60797 (54869)	Loss/tok 3.4451 (3.4466)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.275 (0.256)	Data 1.39e-04 (1.67e-03)	Tok/s 61019 (54762)	Loss/tok 3.3677 (3.4447)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.215 (0.256)	Data 1.37e-04 (1.64e-03)	Tok/s 47840 (54763)	Loss/tok 3.2472 (3.4458)	LR 2.000e-03
0: TRAIN [1][490/1938]	Time 0.216 (0.256)	Data 1.25e-04 (1.61e-03)	Tok/s 47923 (54696)	Loss/tok 3.2973 (3.4451)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.275 (0.256)	Data 1.33e-04 (1.58e-03)	Tok/s 61152 (54777)	Loss/tok 3.4729 (3.4464)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.335 (0.256)	Data 1.27e-04 (1.55e-03)	Tok/s 69352 (54781)	Loss/tok 3.7538 (3.4470)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][520/1938]	Time 0.216 (0.256)	Data 1.21e-04 (1.53e-03)	Tok/s 47790 (54810)	Loss/tok 3.2305 (3.4487)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.335 (0.257)	Data 1.23e-04 (1.50e-03)	Tok/s 68973 (54859)	Loss/tok 3.6571 (3.4510)	LR 2.000e-03
0: TRAIN [1][540/1938]	Time 0.276 (0.257)	Data 1.21e-04 (1.48e-03)	Tok/s 60659 (54838)	Loss/tok 3.4463 (3.4494)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.162 (0.257)	Data 1.22e-04 (1.45e-03)	Tok/s 33121 (54861)	Loss/tok 2.8433 (3.4496)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.216 (0.256)	Data 1.26e-04 (1.43e-03)	Tok/s 47893 (54779)	Loss/tok 3.1699 (3.4473)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.216 (0.256)	Data 1.32e-04 (1.41e-03)	Tok/s 47544 (54744)	Loss/tok 3.0205 (3.4452)	LR 2.000e-03
0: TRAIN [1][580/1938]	Time 0.334 (0.256)	Data 1.49e-04 (1.39e-03)	Tok/s 70076 (54777)	Loss/tok 3.5135 (3.4448)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.335 (0.256)	Data 1.22e-04 (1.36e-03)	Tok/s 68942 (54704)	Loss/tok 3.8259 (3.4450)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.216 (0.256)	Data 1.19e-04 (1.34e-03)	Tok/s 47384 (54709)	Loss/tok 3.2157 (3.4442)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.335 (0.256)	Data 1.22e-04 (1.32e-03)	Tok/s 69632 (54749)	Loss/tok 3.6472 (3.4455)	LR 2.000e-03
0: TRAIN [1][620/1938]	Time 0.216 (0.256)	Data 4.89e-04 (1.31e-03)	Tok/s 48042 (54756)	Loss/tok 3.1706 (3.4459)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.275 (0.255)	Data 1.22e-04 (1.29e-03)	Tok/s 61139 (54648)	Loss/tok 3.3597 (3.4441)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.216 (0.256)	Data 1.43e-04 (1.27e-03)	Tok/s 47790 (54673)	Loss/tok 3.2905 (3.4443)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][650/1938]	Time 0.332 (0.256)	Data 1.56e-04 (1.25e-03)	Tok/s 69626 (54713)	Loss/tok 3.6901 (3.4438)	LR 2.000e-03
0: TRAIN [1][660/1938]	Time 0.216 (0.256)	Data 4.19e-04 (1.24e-03)	Tok/s 47811 (54774)	Loss/tok 3.2656 (3.4441)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.407 (0.256)	Data 1.39e-04 (1.22e-03)	Tok/s 71850 (54699)	Loss/tok 3.8123 (3.4432)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.216 (0.256)	Data 1.57e-04 (1.20e-03)	Tok/s 48195 (54677)	Loss/tok 3.1975 (3.4423)	LR 2.000e-03
0: TRAIN [1][690/1938]	Time 0.216 (0.255)	Data 1.17e-04 (1.19e-03)	Tok/s 47814 (54585)	Loss/tok 3.1736 (3.4406)	LR 2.000e-03
0: TRAIN [1][700/1938]	Time 0.275 (0.255)	Data 4.76e-04 (1.17e-03)	Tok/s 61446 (54593)	Loss/tok 3.4385 (3.4401)	LR 2.000e-03
0: TRAIN [1][710/1938]	Time 0.161 (0.255)	Data 1.28e-04 (1.16e-03)	Tok/s 33026 (54626)	Loss/tok 2.6724 (3.4398)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.275 (0.256)	Data 1.57e-04 (1.15e-03)	Tok/s 61137 (54679)	Loss/tok 3.3850 (3.4405)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.276 (0.255)	Data 1.57e-04 (1.13e-03)	Tok/s 60714 (54684)	Loss/tok 3.4305 (3.4396)	LR 2.000e-03
0: TRAIN [1][740/1938]	Time 0.216 (0.255)	Data 1.43e-04 (1.12e-03)	Tok/s 46548 (54608)	Loss/tok 3.2041 (3.4382)	LR 2.000e-03
0: TRAIN [1][750/1938]	Time 0.216 (0.255)	Data 1.16e-04 (1.11e-03)	Tok/s 48268 (54541)	Loss/tok 3.2047 (3.4366)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.275 (0.255)	Data 1.30e-04 (1.09e-03)	Tok/s 61381 (54481)	Loss/tok 3.3797 (3.4362)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.216 (0.254)	Data 1.28e-04 (1.08e-03)	Tok/s 48625 (54446)	Loss/tok 3.0487 (3.4348)	LR 2.000e-03
0: TRAIN [1][780/1938]	Time 0.274 (0.254)	Data 1.22e-04 (1.07e-03)	Tok/s 61030 (54443)	Loss/tok 3.4305 (3.4335)	LR 2.000e-03
0: TRAIN [1][790/1938]	Time 0.216 (0.254)	Data 1.18e-04 (1.06e-03)	Tok/s 47672 (54399)	Loss/tok 3.1287 (3.4321)	LR 2.000e-03
0: TRAIN [1][800/1938]	Time 0.216 (0.254)	Data 2.35e-04 (1.05e-03)	Tok/s 47802 (54366)	Loss/tok 3.2063 (3.4314)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.408 (0.254)	Data 1.29e-04 (1.04e-03)	Tok/s 72686 (54382)	Loss/tok 3.8351 (3.4315)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.160 (0.254)	Data 1.39e-04 (1.03e-03)	Tok/s 34106 (54355)	Loss/tok 2.8320 (3.4310)	LR 2.000e-03
0: TRAIN [1][830/1938]	Time 0.276 (0.254)	Data 1.54e-04 (1.01e-03)	Tok/s 60762 (54345)	Loss/tok 3.4614 (3.4305)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.163 (0.253)	Data 4.94e-04 (1.00e-03)	Tok/s 32343 (54314)	Loss/tok 2.7760 (3.4300)	LR 2.000e-03
0: TRAIN [1][850/1938]	Time 0.216 (0.254)	Data 4.33e-04 (9.95e-04)	Tok/s 46774 (54349)	Loss/tok 3.2360 (3.4303)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.216 (0.253)	Data 1.56e-04 (9.85e-04)	Tok/s 47508 (54326)	Loss/tok 3.1296 (3.4289)	LR 2.000e-03
0: TRAIN [1][870/1938]	Time 0.336 (0.254)	Data 5.47e-04 (9.76e-04)	Tok/s 70320 (54374)	Loss/tok 3.5808 (3.4300)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.335 (0.254)	Data 1.21e-04 (9.67e-04)	Tok/s 69403 (54430)	Loss/tok 3.5862 (3.4299)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][890/1938]	Time 0.214 (0.254)	Data 1.47e-04 (9.58e-04)	Tok/s 48517 (54434)	Loss/tok 3.2595 (3.4294)	LR 2.000e-03
0: TRAIN [1][900/1938]	Time 0.275 (0.254)	Data 1.48e-04 (9.49e-04)	Tok/s 60871 (54499)	Loss/tok 3.4260 (3.4300)	LR 2.000e-03
0: TRAIN [1][910/1938]	Time 0.335 (0.254)	Data 1.31e-04 (9.40e-04)	Tok/s 69767 (54480)	Loss/tok 3.5583 (3.4293)	LR 2.000e-03
0: TRAIN [1][920/1938]	Time 0.216 (0.254)	Data 1.53e-04 (9.31e-04)	Tok/s 48184 (54500)	Loss/tok 3.2791 (3.4292)	LR 2.000e-03
0: TRAIN [1][930/1938]	Time 0.216 (0.254)	Data 1.35e-04 (9.23e-04)	Tok/s 48480 (54480)	Loss/tok 3.2759 (3.4281)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.216 (0.254)	Data 1.20e-04 (9.14e-04)	Tok/s 48892 (54488)	Loss/tok 3.2318 (3.4283)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.161 (0.254)	Data 3.62e-04 (9.06e-04)	Tok/s 32665 (54436)	Loss/tok 2.7193 (3.4271)	LR 2.000e-03
0: TRAIN [1][960/1938]	Time 0.335 (0.254)	Data 1.41e-04 (8.99e-04)	Tok/s 70788 (54468)	Loss/tok 3.5831 (3.4266)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.275 (0.254)	Data 1.20e-04 (8.91e-04)	Tok/s 60551 (54454)	Loss/tok 3.5929 (3.4264)	LR 2.000e-03
0: TRAIN [1][980/1938]	Time 0.215 (0.254)	Data 1.17e-04 (8.83e-04)	Tok/s 47785 (54470)	Loss/tok 3.2028 (3.4267)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.275 (0.254)	Data 1.21e-04 (8.75e-04)	Tok/s 61905 (54398)	Loss/tok 3.3233 (3.4251)	LR 2.000e-03
0: TRAIN [1][1000/1938]	Time 0.217 (0.253)	Data 1.24e-04 (8.69e-04)	Tok/s 47597 (54376)	Loss/tok 2.9967 (3.4241)	LR 2.000e-03
0: TRAIN [1][1010/1938]	Time 0.335 (0.253)	Data 1.45e-04 (8.61e-04)	Tok/s 70152 (54380)	Loss/tok 3.4127 (3.4232)	LR 2.000e-03
0: TRAIN [1][1020/1938]	Time 0.216 (0.254)	Data 1.36e-04 (8.54e-04)	Tok/s 49085 (54433)	Loss/tok 3.2347 (3.4242)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.334 (0.254)	Data 2.07e-04 (8.48e-04)	Tok/s 69849 (54464)	Loss/tok 3.5072 (3.4235)	LR 2.000e-03
0: TRAIN [1][1040/1938]	Time 0.275 (0.254)	Data 1.15e-04 (8.41e-04)	Tok/s 60650 (54461)	Loss/tok 3.4286 (3.4233)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.275 (0.254)	Data 1.88e-04 (8.35e-04)	Tok/s 60755 (54488)	Loss/tok 3.4061 (3.4233)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.275 (0.254)	Data 2.31e-04 (8.28e-04)	Tok/s 61302 (54538)	Loss/tok 3.3215 (3.4234)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1070/1938]	Time 0.405 (0.254)	Data 1.42e-04 (8.22e-04)	Tok/s 73341 (54531)	Loss/tok 3.7814 (3.4234)	LR 2.000e-03
0: TRAIN [1][1080/1938]	Time 0.215 (0.254)	Data 1.31e-04 (8.15e-04)	Tok/s 47729 (54533)	Loss/tok 3.3158 (3.4230)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.274 (0.254)	Data 1.37e-04 (8.09e-04)	Tok/s 60703 (54573)	Loss/tok 3.4243 (3.4229)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.216 (0.254)	Data 1.24e-04 (8.03e-04)	Tok/s 47439 (54539)	Loss/tok 3.1754 (3.4220)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1110/1938]	Time 0.274 (0.254)	Data 1.53e-04 (7.97e-04)	Tok/s 61567 (54543)	Loss/tok 3.4190 (3.4220)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.216 (0.254)	Data 1.27e-04 (7.91e-04)	Tok/s 47122 (54552)	Loss/tok 3.0613 (3.4221)	LR 2.000e-03
0: TRAIN [1][1130/1938]	Time 0.216 (0.254)	Data 1.22e-04 (7.86e-04)	Tok/s 46754 (54539)	Loss/tok 3.1962 (3.4224)	LR 2.000e-03
0: TRAIN [1][1140/1938]	Time 0.161 (0.254)	Data 1.25e-04 (7.80e-04)	Tok/s 33433 (54550)	Loss/tok 2.6901 (3.4218)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.217 (0.254)	Data 1.16e-04 (7.74e-04)	Tok/s 47849 (54536)	Loss/tok 3.1596 (3.4213)	LR 2.000e-03
0: TRAIN [1][1160/1938]	Time 0.275 (0.254)	Data 1.24e-04 (7.69e-04)	Tok/s 61258 (54542)	Loss/tok 3.2936 (3.4216)	LR 2.000e-03
0: TRAIN [1][1170/1938]	Time 0.407 (0.255)	Data 1.60e-04 (7.63e-04)	Tok/s 73269 (54589)	Loss/tok 3.7517 (3.4227)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.276 (0.255)	Data 1.58e-04 (7.58e-04)	Tok/s 60347 (54593)	Loss/tok 3.3540 (3.4225)	LR 2.000e-03
0: TRAIN [1][1190/1938]	Time 0.216 (0.254)	Data 1.51e-04 (7.53e-04)	Tok/s 46487 (54533)	Loss/tok 3.2626 (3.4212)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.216 (0.255)	Data 1.22e-04 (7.48e-04)	Tok/s 48374 (54599)	Loss/tok 3.2278 (3.4222)	LR 2.000e-03
0: TRAIN [1][1210/1938]	Time 0.216 (0.255)	Data 1.30e-04 (7.43e-04)	Tok/s 48660 (54605)	Loss/tok 3.0901 (3.4219)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.275 (0.255)	Data 1.65e-04 (7.38e-04)	Tok/s 61343 (54609)	Loss/tok 3.3923 (3.4221)	LR 2.000e-03
0: TRAIN [1][1230/1938]	Time 0.275 (0.255)	Data 1.24e-04 (7.33e-04)	Tok/s 60995 (54590)	Loss/tok 3.3581 (3.4214)	LR 2.000e-03
0: TRAIN [1][1240/1938]	Time 0.335 (0.255)	Data 1.24e-04 (7.29e-04)	Tok/s 69097 (54624)	Loss/tok 3.6111 (3.4211)	LR 2.000e-03
0: TRAIN [1][1250/1938]	Time 0.275 (0.255)	Data 1.45e-04 (7.24e-04)	Tok/s 60167 (54616)	Loss/tok 3.2633 (3.4205)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.215 (0.255)	Data 1.38e-04 (7.20e-04)	Tok/s 48205 (54571)	Loss/tok 2.9994 (3.4188)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.276 (0.255)	Data 1.42e-04 (7.15e-04)	Tok/s 62441 (54592)	Loss/tok 3.2404 (3.4179)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.160 (0.255)	Data 1.21e-04 (7.11e-04)	Tok/s 33385 (54580)	Loss/tok 2.6107 (3.4173)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.335 (0.255)	Data 1.41e-04 (7.07e-04)	Tok/s 69738 (54626)	Loss/tok 3.5012 (3.4185)	LR 2.000e-03
0: TRAIN [1][1300/1938]	Time 0.216 (0.255)	Data 1.23e-04 (7.02e-04)	Tok/s 48693 (54642)	Loss/tok 3.1643 (3.4181)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.274 (0.255)	Data 1.19e-04 (6.98e-04)	Tok/s 61441 (54656)	Loss/tok 3.5032 (3.4177)	LR 2.000e-03
0: TRAIN [1][1320/1938]	Time 0.276 (0.255)	Data 1.23e-04 (6.94e-04)	Tok/s 61205 (54678)	Loss/tok 3.3821 (3.4174)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.216 (0.255)	Data 1.32e-04 (6.90e-04)	Tok/s 48152 (54651)	Loss/tok 3.1600 (3.4166)	LR 2.000e-03
0: TRAIN [1][1340/1938]	Time 0.216 (0.255)	Data 1.39e-04 (6.85e-04)	Tok/s 47881 (54681)	Loss/tok 3.2400 (3.4172)	LR 2.000e-03
0: TRAIN [1][1350/1938]	Time 0.216 (0.255)	Data 1.23e-04 (6.81e-04)	Tok/s 48530 (54721)	Loss/tok 3.1301 (3.4174)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.216 (0.255)	Data 1.35e-04 (6.77e-04)	Tok/s 46835 (54733)	Loss/tok 3.1461 (3.4172)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.335 (0.255)	Data 1.26e-04 (6.74e-04)	Tok/s 70167 (54712)	Loss/tok 3.4473 (3.4163)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1380/1938]	Time 0.276 (0.255)	Data 1.61e-04 (6.70e-04)	Tok/s 60861 (54696)	Loss/tok 3.4162 (3.4158)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.217 (0.255)	Data 1.23e-04 (6.66e-04)	Tok/s 47351 (54707)	Loss/tok 3.2182 (3.4153)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.162 (0.255)	Data 2.96e-04 (6.63e-04)	Tok/s 31567 (54679)	Loss/tok 2.7169 (3.4150)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.216 (0.255)	Data 1.54e-04 (6.59e-04)	Tok/s 47297 (54709)	Loss/tok 3.2623 (3.4157)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.335 (0.255)	Data 1.20e-04 (6.55e-04)	Tok/s 70833 (54705)	Loss/tok 3.4867 (3.4151)	LR 2.000e-03
0: TRAIN [1][1430/1938]	Time 0.407 (0.255)	Data 1.17e-04 (6.52e-04)	Tok/s 73684 (54744)	Loss/tok 3.7049 (3.4154)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.335 (0.255)	Data 1.33e-04 (6.48e-04)	Tok/s 69306 (54719)	Loss/tok 3.5158 (3.4144)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1450/1938]	Time 0.216 (0.255)	Data 1.24e-04 (6.45e-04)	Tok/s 47919 (54746)	Loss/tok 3.2378 (3.4143)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.335 (0.255)	Data 1.46e-04 (6.41e-04)	Tok/s 70275 (54762)	Loss/tok 3.4477 (3.4136)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.275 (0.255)	Data 1.22e-04 (6.38e-04)	Tok/s 61259 (54739)	Loss/tok 3.4159 (3.4133)	LR 2.000e-03
0: TRAIN [1][1480/1938]	Time 0.216 (0.255)	Data 1.57e-04 (6.34e-04)	Tok/s 47985 (54710)	Loss/tok 3.2765 (3.4126)	LR 2.000e-03
0: TRAIN [1][1490/1938]	Time 0.406 (0.255)	Data 1.47e-04 (6.31e-04)	Tok/s 73836 (54720)	Loss/tok 3.6071 (3.4126)	LR 2.000e-03
0: TRAIN [1][1500/1938]	Time 0.275 (0.255)	Data 1.31e-04 (6.28e-04)	Tok/s 60522 (54702)	Loss/tok 3.3307 (3.4119)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.338 (0.255)	Data 1.39e-04 (6.24e-04)	Tok/s 68770 (54699)	Loss/tok 3.4438 (3.4116)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.216 (0.255)	Data 1.24e-04 (6.22e-04)	Tok/s 48754 (54685)	Loss/tok 3.0919 (3.4114)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.275 (0.255)	Data 1.13e-04 (6.18e-04)	Tok/s 60929 (54687)	Loss/tok 3.4044 (3.4113)	LR 2.000e-03
0: TRAIN [1][1540/1938]	Time 0.215 (0.255)	Data 1.24e-04 (6.15e-04)	Tok/s 46654 (54640)	Loss/tok 3.1315 (3.4103)	LR 2.000e-03
0: TRAIN [1][1550/1938]	Time 0.274 (0.255)	Data 1.23e-04 (6.12e-04)	Tok/s 61630 (54665)	Loss/tok 3.2883 (3.4102)	LR 2.000e-03
0: TRAIN [1][1560/1938]	Time 0.215 (0.255)	Data 1.23e-04 (6.09e-04)	Tok/s 47616 (54658)	Loss/tok 3.2177 (3.4098)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.215 (0.255)	Data 1.43e-04 (6.06e-04)	Tok/s 47460 (54639)	Loss/tok 3.1043 (3.4089)	LR 2.000e-03
0: TRAIN [1][1580/1938]	Time 0.275 (0.255)	Data 1.17e-04 (6.03e-04)	Tok/s 61690 (54656)	Loss/tok 3.2435 (3.4089)	LR 2.000e-03
0: TRAIN [1][1590/1938]	Time 0.275 (0.255)	Data 1.14e-04 (6.00e-04)	Tok/s 61776 (54645)	Loss/tok 3.3201 (3.4084)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.275 (0.255)	Data 1.18e-04 (5.97e-04)	Tok/s 61859 (54648)	Loss/tok 3.4105 (3.4080)	LR 2.000e-03
0: TRAIN [1][1610/1938]	Time 0.275 (0.255)	Data 1.46e-04 (5.94e-04)	Tok/s 60467 (54668)	Loss/tok 3.5503 (3.4077)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.217 (0.255)	Data 1.17e-04 (5.91e-04)	Tok/s 47071 (54639)	Loss/tok 3.0661 (3.4072)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.216 (0.255)	Data 1.14e-04 (5.89e-04)	Tok/s 48051 (54595)	Loss/tok 3.1109 (3.4060)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.275 (0.255)	Data 2.38e-04 (5.86e-04)	Tok/s 60730 (54585)	Loss/tok 3.3493 (3.4055)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.217 (0.255)	Data 1.28e-04 (5.83e-04)	Tok/s 47299 (54580)	Loss/tok 3.1430 (3.4048)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.408 (0.255)	Data 1.38e-04 (5.81e-04)	Tok/s 72851 (54603)	Loss/tok 3.7159 (3.4051)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.161 (0.255)	Data 1.37e-04 (5.78e-04)	Tok/s 32564 (54582)	Loss/tok 2.7087 (3.4043)	LR 2.000e-03
0: TRAIN [1][1680/1938]	Time 0.216 (0.255)	Data 1.25e-04 (5.76e-04)	Tok/s 47833 (54586)	Loss/tok 2.9834 (3.4040)	LR 2.000e-03
0: TRAIN [1][1690/1938]	Time 0.215 (0.255)	Data 1.26e-04 (5.73e-04)	Tok/s 47428 (54590)	Loss/tok 3.0931 (3.4035)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.275 (0.255)	Data 1.36e-04 (5.71e-04)	Tok/s 60013 (54600)	Loss/tok 3.4204 (3.4031)	LR 2.000e-03
0: TRAIN [1][1710/1938]	Time 0.160 (0.254)	Data 1.19e-04 (5.68e-04)	Tok/s 33148 (54567)	Loss/tok 2.7707 (3.4019)	LR 2.000e-03
0: TRAIN [1][1720/1938]	Time 0.216 (0.254)	Data 1.32e-04 (5.66e-04)	Tok/s 47656 (54551)	Loss/tok 3.0393 (3.4009)	LR 2.000e-03
0: TRAIN [1][1730/1938]	Time 0.216 (0.254)	Data 1.41e-04 (5.63e-04)	Tok/s 47425 (54550)	Loss/tok 3.0704 (3.4000)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1740/1938]	Time 0.215 (0.254)	Data 1.57e-04 (5.61e-04)	Tok/s 48057 (54532)	Loss/tok 3.0962 (3.4001)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.216 (0.254)	Data 1.28e-04 (5.58e-04)	Tok/s 48273 (54548)	Loss/tok 3.1623 (3.3997)	LR 2.000e-03
0: TRAIN [1][1760/1938]	Time 0.215 (0.254)	Data 1.48e-04 (5.56e-04)	Tok/s 48627 (54557)	Loss/tok 3.1158 (3.3996)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.216 (0.254)	Data 1.26e-04 (5.54e-04)	Tok/s 48084 (54552)	Loss/tok 3.0277 (3.3991)	LR 2.000e-03
0: TRAIN [1][1780/1938]	Time 0.274 (0.255)	Data 1.20e-04 (5.51e-04)	Tok/s 61895 (54574)	Loss/tok 3.2646 (3.3994)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.405 (0.255)	Data 1.16e-04 (5.49e-04)	Tok/s 73728 (54618)	Loss/tok 3.7877 (3.3999)	LR 2.000e-03
0: TRAIN [1][1800/1938]	Time 0.216 (0.255)	Data 1.29e-04 (5.47e-04)	Tok/s 48396 (54653)	Loss/tok 3.1576 (3.4000)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.216 (0.255)	Data 1.34e-04 (5.45e-04)	Tok/s 47287 (54657)	Loss/tok 3.1558 (3.3998)	LR 2.000e-03
0: TRAIN [1][1820/1938]	Time 0.214 (0.255)	Data 1.24e-04 (5.42e-04)	Tok/s 48343 (54641)	Loss/tok 3.0722 (3.3992)	LR 2.000e-03
0: TRAIN [1][1830/1938]	Time 0.408 (0.255)	Data 1.31e-04 (5.40e-04)	Tok/s 73032 (54651)	Loss/tok 3.6866 (3.3989)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.336 (0.255)	Data 1.24e-04 (5.38e-04)	Tok/s 68906 (54695)	Loss/tok 3.5679 (3.3993)	LR 2.000e-03
0: TRAIN [1][1850/1938]	Time 0.216 (0.255)	Data 1.23e-04 (5.36e-04)	Tok/s 48462 (54691)	Loss/tok 3.2203 (3.3990)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.216 (0.255)	Data 1.48e-04 (5.34e-04)	Tok/s 47373 (54702)	Loss/tok 3.1817 (3.3987)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.216 (0.255)	Data 1.45e-04 (5.31e-04)	Tok/s 46539 (54720)	Loss/tok 3.0281 (3.3984)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1880/1938]	Time 0.275 (0.256)	Data 1.47e-04 (5.29e-04)	Tok/s 60603 (54742)	Loss/tok 3.3985 (3.3985)	LR 2.000e-03
0: TRAIN [1][1890/1938]	Time 0.216 (0.255)	Data 1.24e-04 (5.27e-04)	Tok/s 48699 (54727)	Loss/tok 3.2353 (3.3983)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.161 (0.255)	Data 1.17e-04 (5.25e-04)	Tok/s 33539 (54696)	Loss/tok 2.7134 (3.3974)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.163 (0.255)	Data 2.55e-04 (5.23e-04)	Tok/s 32161 (54661)	Loss/tok 2.7624 (3.3964)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.334 (0.255)	Data 1.23e-04 (5.21e-04)	Tok/s 69337 (54693)	Loss/tok 3.4984 (3.3966)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1930/1938]	Time 0.161 (0.255)	Data 1.26e-04 (5.19e-04)	Tok/s 32913 (54701)	Loss/tok 2.6770 (3.3965)	LR 2.000e-03
:::MLL 1571837638.065 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1571837638.065 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.758 (0.758)	Decoder iters 149.0 (149.0)	Tok/s 22036 (22036)
0: Running moses detokenizer
0: BLEU(score=21.668518054358998, counts=[36411, 17509, 9654, 5557], totals=[67353, 64350, 61347, 58348], precisions=[54.059952786067434, 27.209013209013207, 15.736710841605946, 9.523891135942963], bp=1.0, sys_len=67353, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571837640.196 eval_accuracy: {"value": 21.67, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1571837640.197 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.3984	Test BLEU: 21.67
0: Performance: Epoch: 1	Training: 437825 Tok/s
0: Finished epoch 1
:::MLL 1571837640.197 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1571837640.197 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571837640.198 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 2128501865
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [2][0/1938]	Time 0.958 (0.958)	Data 7.25e-01 (7.25e-01)	Tok/s 10877 (10877)	Loss/tok 3.0531 (3.0531)	LR 2.000e-03
0: TRAIN [2][10/1938]	Time 0.216 (0.321)	Data 1.05e-04 (6.60e-02)	Tok/s 48272 (52393)	Loss/tok 3.0455 (3.2244)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.275 (0.288)	Data 1.03e-04 (3.46e-02)	Tok/s 61647 (53389)	Loss/tok 3.2421 (3.2269)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.216 (0.282)	Data 1.20e-04 (2.35e-02)	Tok/s 46727 (53000)	Loss/tok 3.1748 (3.2535)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.216 (0.273)	Data 1.24e-04 (1.78e-02)	Tok/s 48149 (53058)	Loss/tok 2.9740 (3.2491)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.335 (0.270)	Data 1.13e-04 (1.43e-02)	Tok/s 69613 (52755)	Loss/tok 3.4299 (3.2650)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.273 (0.270)	Data 1.62e-04 (1.20e-02)	Tok/s 60823 (53437)	Loss/tok 3.2314 (3.2804)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.216 (0.270)	Data 1.28e-04 (1.03e-02)	Tok/s 48705 (54076)	Loss/tok 3.0686 (3.2824)	LR 2.000e-03
0: TRAIN [2][80/1938]	Time 0.275 (0.267)	Data 1.44e-04 (9.09e-03)	Tok/s 61157 (53963)	Loss/tok 3.3244 (3.2727)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.275 (0.266)	Data 1.27e-04 (8.11e-03)	Tok/s 60686 (54214)	Loss/tok 3.3403 (3.2683)	LR 2.000e-03
0: TRAIN [2][100/1938]	Time 0.275 (0.269)	Data 1.38e-04 (7.32e-03)	Tok/s 61152 (54801)	Loss/tok 3.2327 (3.2771)	LR 2.000e-03
0: TRAIN [2][110/1938]	Time 0.216 (0.268)	Data 1.65e-04 (6.67e-03)	Tok/s 47981 (54729)	Loss/tok 3.1025 (3.2800)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.335 (0.270)	Data 1.31e-04 (6.13e-03)	Tok/s 69290 (55402)	Loss/tok 3.5558 (3.2872)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.334 (0.269)	Data 1.40e-04 (5.68e-03)	Tok/s 69672 (55210)	Loss/tok 3.3341 (3.2873)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.215 (0.268)	Data 1.54e-04 (5.29e-03)	Tok/s 47233 (55351)	Loss/tok 3.0684 (3.2858)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.216 (0.267)	Data 1.25e-04 (4.95e-03)	Tok/s 47756 (55233)	Loss/tok 2.9492 (3.2831)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.406 (0.268)	Data 1.42e-04 (4.65e-03)	Tok/s 73240 (55374)	Loss/tok 3.7039 (3.2870)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.274 (0.268)	Data 1.24e-04 (4.38e-03)	Tok/s 61826 (55669)	Loss/tok 3.3031 (3.2887)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.274 (0.270)	Data 1.42e-04 (4.15e-03)	Tok/s 62002 (56093)	Loss/tok 3.2212 (3.2952)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.216 (0.269)	Data 2.38e-04 (3.94e-03)	Tok/s 47547 (55992)	Loss/tok 3.0598 (3.2937)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.276 (0.269)	Data 1.37e-04 (3.75e-03)	Tok/s 61257 (56158)	Loss/tok 3.2324 (3.2919)	LR 2.000e-03
0: TRAIN [2][210/1938]	Time 0.335 (0.270)	Data 1.24e-04 (3.58e-03)	Tok/s 69475 (56376)	Loss/tok 3.4987 (3.2957)	LR 2.000e-03
0: TRAIN [2][220/1938]	Time 0.216 (0.269)	Data 1.26e-04 (3.43e-03)	Tok/s 47579 (56272)	Loss/tok 3.0073 (3.2927)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.273 (0.269)	Data 1.18e-04 (3.28e-03)	Tok/s 62031 (56262)	Loss/tok 3.2515 (3.2936)	LR 2.000e-03
0: TRAIN [2][240/1938]	Time 0.335 (0.269)	Data 1.22e-04 (3.15e-03)	Tok/s 70007 (56295)	Loss/tok 3.5711 (3.2937)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.216 (0.267)	Data 1.27e-04 (3.03e-03)	Tok/s 47142 (55828)	Loss/tok 2.9848 (3.2877)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.217 (0.266)	Data 1.28e-04 (2.92e-03)	Tok/s 46897 (55851)	Loss/tok 3.0099 (3.2865)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.275 (0.265)	Data 1.26e-04 (2.83e-03)	Tok/s 60189 (55630)	Loss/tok 3.3064 (3.2816)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.275 (0.264)	Data 1.39e-04 (2.73e-03)	Tok/s 61478 (55583)	Loss/tok 3.2103 (3.2786)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.216 (0.263)	Data 3.98e-04 (2.64e-03)	Tok/s 48263 (55496)	Loss/tok 2.9995 (3.2749)	LR 2.000e-03
0: TRAIN [2][300/1938]	Time 0.161 (0.263)	Data 1.43e-04 (2.56e-03)	Tok/s 32946 (55508)	Loss/tok 2.6889 (3.2745)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.276 (0.263)	Data 1.58e-04 (2.48e-03)	Tok/s 59858 (55452)	Loss/tok 3.2848 (3.2741)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.215 (0.262)	Data 1.28e-04 (2.41e-03)	Tok/s 48098 (55213)	Loss/tok 3.0440 (3.2703)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.275 (0.262)	Data 1.46e-04 (2.34e-03)	Tok/s 60532 (55301)	Loss/tok 3.2065 (3.2696)	LR 2.000e-03
0: TRAIN [2][340/1938]	Time 0.275 (0.261)	Data 1.19e-04 (2.28e-03)	Tok/s 61086 (55324)	Loss/tok 3.3461 (3.2684)	LR 2.000e-03
0: TRAIN [2][350/1938]	Time 0.334 (0.262)	Data 1.35e-04 (2.22e-03)	Tok/s 69850 (55443)	Loss/tok 3.4973 (3.2691)	LR 2.000e-03
0: TRAIN [2][360/1938]	Time 0.276 (0.262)	Data 1.29e-04 (2.16e-03)	Tok/s 62066 (55445)	Loss/tok 3.3525 (3.2671)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.216 (0.261)	Data 5.77e-04 (2.11e-03)	Tok/s 46998 (55307)	Loss/tok 3.1251 (3.2639)	LR 2.000e-03
0: TRAIN [2][380/1938]	Time 0.160 (0.260)	Data 1.44e-04 (2.06e-03)	Tok/s 33560 (55277)	Loss/tok 2.7024 (3.2641)	LR 2.000e-03
0: TRAIN [2][390/1938]	Time 0.336 (0.260)	Data 1.28e-04 (2.01e-03)	Tok/s 69527 (55163)	Loss/tok 3.4809 (3.2631)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][400/1938]	Time 0.213 (0.260)	Data 1.44e-04 (1.96e-03)	Tok/s 48386 (55188)	Loss/tok 3.0564 (3.2657)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.276 (0.260)	Data 1.46e-04 (1.92e-03)	Tok/s 59951 (55206)	Loss/tok 3.5223 (3.2664)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.215 (0.260)	Data 1.30e-04 (1.88e-03)	Tok/s 48422 (55168)	Loss/tok 3.0557 (3.2651)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.216 (0.260)	Data 1.41e-04 (1.84e-03)	Tok/s 48231 (55194)	Loss/tok 3.1309 (3.2662)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.335 (0.260)	Data 1.28e-04 (1.80e-03)	Tok/s 68987 (55233)	Loss/tok 3.3610 (3.2672)	LR 2.000e-03
0: TRAIN [2][450/1938]	Time 0.216 (0.259)	Data 1.59e-04 (1.76e-03)	Tok/s 47760 (55170)	Loss/tok 3.0974 (3.2669)	LR 2.000e-03
0: TRAIN [2][460/1938]	Time 0.335 (0.259)	Data 1.38e-04 (1.73e-03)	Tok/s 68937 (55172)	Loss/tok 3.5003 (3.2674)	LR 2.000e-03
0: TRAIN [2][470/1938]	Time 0.216 (0.259)	Data 1.18e-04 (1.69e-03)	Tok/s 47697 (55106)	Loss/tok 2.9617 (3.2677)	LR 2.000e-03
0: TRAIN [2][480/1938]	Time 0.216 (0.259)	Data 1.33e-04 (1.66e-03)	Tok/s 47503 (55105)	Loss/tok 3.1024 (3.2671)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.275 (0.259)	Data 1.48e-04 (1.63e-03)	Tok/s 60283 (55063)	Loss/tok 3.3668 (3.2670)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.216 (0.258)	Data 1.51e-04 (1.60e-03)	Tok/s 47514 (55017)	Loss/tok 3.0923 (3.2654)	LR 2.000e-03
0: TRAIN [2][510/1938]	Time 0.219 (0.258)	Data 1.79e-04 (1.57e-03)	Tok/s 46783 (54938)	Loss/tok 3.0566 (3.2635)	LR 2.000e-03
0: TRAIN [2][520/1938]	Time 0.215 (0.258)	Data 1.39e-04 (1.54e-03)	Tok/s 48033 (54929)	Loss/tok 3.1092 (3.2630)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.216 (0.257)	Data 1.47e-04 (1.52e-03)	Tok/s 47332 (54863)	Loss/tok 3.0455 (3.2614)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.335 (0.258)	Data 1.24e-04 (1.49e-03)	Tok/s 69795 (54899)	Loss/tok 3.4621 (3.2623)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.217 (0.257)	Data 1.38e-04 (1.47e-03)	Tok/s 48234 (54876)	Loss/tok 3.0999 (3.2614)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][560/1938]	Time 0.335 (0.257)	Data 2.77e-04 (1.44e-03)	Tok/s 69303 (54872)	Loss/tok 3.5218 (3.2612)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.274 (0.257)	Data 1.40e-04 (1.42e-03)	Tok/s 61406 (54890)	Loss/tok 3.3060 (3.2626)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.216 (0.257)	Data 1.30e-04 (1.40e-03)	Tok/s 48178 (54801)	Loss/tok 3.1189 (3.2610)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.216 (0.257)	Data 1.50e-04 (1.38e-03)	Tok/s 47775 (54900)	Loss/tok 2.9202 (3.2614)	LR 2.000e-03
0: TRAIN [2][600/1938]	Time 0.275 (0.257)	Data 1.19e-04 (1.36e-03)	Tok/s 61255 (54891)	Loss/tok 3.1418 (3.2611)	LR 2.000e-03
0: TRAIN [2][610/1938]	Time 0.335 (0.257)	Data 1.29e-04 (1.34e-03)	Tok/s 69057 (54745)	Loss/tok 3.3813 (3.2597)	LR 2.000e-03
0: TRAIN [2][620/1938]	Time 0.216 (0.257)	Data 1.35e-04 (1.32e-03)	Tok/s 48559 (54816)	Loss/tok 3.0626 (3.2604)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.274 (0.257)	Data 1.30e-04 (1.30e-03)	Tok/s 61117 (54814)	Loss/tok 3.2771 (3.2602)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.216 (0.257)	Data 1.28e-04 (1.28e-03)	Tok/s 47698 (54792)	Loss/tok 3.0442 (3.2601)	LR 2.000e-03
0: TRAIN [2][650/1938]	Time 0.216 (0.257)	Data 1.25e-04 (1.26e-03)	Tok/s 48834 (54822)	Loss/tok 3.0948 (3.2604)	LR 2.000e-03
0: TRAIN [2][660/1938]	Time 0.334 (0.257)	Data 1.18e-04 (1.25e-03)	Tok/s 70610 (54811)	Loss/tok 3.3936 (3.2593)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.216 (0.256)	Data 1.14e-04 (1.23e-03)	Tok/s 47268 (54744)	Loss/tok 3.0499 (3.2578)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.160 (0.256)	Data 1.17e-04 (1.21e-03)	Tok/s 31541 (54750)	Loss/tok 2.5691 (3.2571)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.215 (0.256)	Data 1.36e-04 (1.20e-03)	Tok/s 47492 (54786)	Loss/tok 2.9966 (3.2580)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.335 (0.256)	Data 1.18e-04 (1.18e-03)	Tok/s 70420 (54798)	Loss/tok 3.3974 (3.2579)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.216 (0.256)	Data 1.45e-04 (1.17e-03)	Tok/s 47850 (54731)	Loss/tok 2.9799 (3.2574)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][720/1938]	Time 0.216 (0.256)	Data 1.22e-04 (1.15e-03)	Tok/s 47400 (54788)	Loss/tok 3.1530 (3.2585)	LR 2.000e-03
0: TRAIN [2][730/1938]	Time 0.216 (0.256)	Data 1.23e-04 (1.14e-03)	Tok/s 47343 (54739)	Loss/tok 3.0857 (3.2579)	LR 2.000e-03
0: TRAIN [2][740/1938]	Time 0.276 (0.256)	Data 1.42e-04 (1.13e-03)	Tok/s 61303 (54693)	Loss/tok 3.3687 (3.2580)	LR 2.000e-03
0: TRAIN [2][750/1938]	Time 0.335 (0.257)	Data 1.21e-04 (1.11e-03)	Tok/s 69429 (54795)	Loss/tok 3.4916 (3.2613)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.335 (0.257)	Data 1.36e-04 (1.10e-03)	Tok/s 69289 (54897)	Loss/tok 3.4425 (3.2628)	LR 2.000e-03
0: TRAIN [2][770/1938]	Time 0.335 (0.257)	Data 1.40e-04 (1.09e-03)	Tok/s 70373 (54878)	Loss/tok 3.4088 (3.2626)	LR 2.000e-03
0: TRAIN [2][780/1938]	Time 0.274 (0.257)	Data 1.22e-04 (1.08e-03)	Tok/s 61782 (54881)	Loss/tok 3.1875 (3.2620)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.275 (0.257)	Data 1.33e-04 (1.06e-03)	Tok/s 61750 (54904)	Loss/tok 3.3057 (3.2622)	LR 2.000e-03
0: TRAIN [2][800/1938]	Time 0.217 (0.257)	Data 1.18e-04 (1.05e-03)	Tok/s 47354 (54849)	Loss/tok 3.1046 (3.2611)	LR 2.000e-03
0: TRAIN [2][810/1938]	Time 0.408 (0.257)	Data 1.50e-04 (1.04e-03)	Tok/s 73589 (54823)	Loss/tok 3.5744 (3.2608)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.335 (0.257)	Data 1.55e-04 (1.03e-03)	Tok/s 69116 (54854)	Loss/tok 3.4259 (3.2608)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.215 (0.256)	Data 1.45e-04 (1.02e-03)	Tok/s 48319 (54803)	Loss/tok 3.0095 (3.2601)	LR 2.000e-03
0: TRAIN [2][840/1938]	Time 0.407 (0.256)	Data 1.15e-04 (1.01e-03)	Tok/s 73475 (54783)	Loss/tok 3.5960 (3.2596)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.218 (0.256)	Data 1.31e-04 (9.99e-04)	Tok/s 46389 (54756)	Loss/tok 3.1503 (3.2596)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.275 (0.256)	Data 1.24e-04 (9.88e-04)	Tok/s 61212 (54737)	Loss/tok 3.3830 (3.2594)	LR 2.000e-03
0: TRAIN [2][870/1938]	Time 0.160 (0.255)	Data 1.19e-04 (9.79e-04)	Tok/s 32604 (54613)	Loss/tok 2.6401 (3.2581)	LR 2.000e-03
0: TRAIN [2][880/1938]	Time 0.216 (0.255)	Data 1.59e-04 (9.69e-04)	Tok/s 48620 (54623)	Loss/tok 3.0135 (3.2575)	LR 2.000e-03
0: TRAIN [2][890/1938]	Time 0.335 (0.255)	Data 1.22e-04 (9.60e-04)	Tok/s 69414 (54641)	Loss/tok 3.4691 (3.2575)	LR 2.000e-03
0: TRAIN [2][900/1938]	Time 0.336 (0.255)	Data 1.21e-04 (9.51e-04)	Tok/s 69929 (54647)	Loss/tok 3.4616 (3.2579)	LR 2.000e-03
0: TRAIN [2][910/1938]	Time 0.216 (0.256)	Data 1.23e-04 (9.42e-04)	Tok/s 47949 (54675)	Loss/tok 3.1248 (3.2580)	LR 2.000e-03
0: TRAIN [2][920/1938]	Time 0.275 (0.255)	Data 6.11e-04 (9.34e-04)	Tok/s 61347 (54638)	Loss/tok 3.3479 (3.2574)	LR 2.000e-03
0: TRAIN [2][930/1938]	Time 0.216 (0.255)	Data 2.07e-04 (9.26e-04)	Tok/s 48053 (54632)	Loss/tok 3.1114 (3.2568)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.408 (0.255)	Data 1.54e-04 (9.17e-04)	Tok/s 73181 (54647)	Loss/tok 3.5125 (3.2573)	LR 2.000e-03
0: TRAIN [2][950/1938]	Time 0.335 (0.255)	Data 1.47e-04 (9.09e-04)	Tok/s 69808 (54653)	Loss/tok 3.3648 (3.2576)	LR 2.000e-03
0: TRAIN [2][960/1938]	Time 0.216 (0.256)	Data 1.35e-04 (9.02e-04)	Tok/s 47333 (54677)	Loss/tok 3.0823 (3.2584)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][970/1938]	Time 0.335 (0.256)	Data 1.20e-04 (8.94e-04)	Tok/s 69638 (54699)	Loss/tok 3.4732 (3.2582)	LR 2.000e-03
0: TRAIN [2][980/1938]	Time 0.275 (0.256)	Data 1.33e-04 (8.86e-04)	Tok/s 60984 (54708)	Loss/tok 3.2802 (3.2580)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.216 (0.255)	Data 1.47e-04 (8.78e-04)	Tok/s 48415 (54689)	Loss/tok 3.0723 (3.2578)	LR 2.000e-03
0: TRAIN [2][1000/1938]	Time 0.276 (0.255)	Data 1.77e-04 (8.71e-04)	Tok/s 60198 (54631)	Loss/tok 3.3991 (3.2566)	LR 2.000e-03
0: TRAIN [2][1010/1938]	Time 0.216 (0.255)	Data 1.30e-04 (8.65e-04)	Tok/s 47597 (54655)	Loss/tok 2.9594 (3.2574)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1020/1938]	Time 0.337 (0.255)	Data 1.56e-04 (8.58e-04)	Tok/s 68750 (54638)	Loss/tok 3.4033 (3.2572)	LR 2.000e-03
0: TRAIN [2][1030/1938]	Time 0.215 (0.255)	Data 1.25e-04 (8.51e-04)	Tok/s 48273 (54621)	Loss/tok 3.0290 (3.2577)	LR 2.000e-03
0: TRAIN [2][1040/1938]	Time 0.275 (0.255)	Data 1.40e-04 (8.45e-04)	Tok/s 60910 (54651)	Loss/tok 3.2289 (3.2574)	LR 2.000e-03
0: TRAIN [2][1050/1938]	Time 0.277 (0.255)	Data 1.80e-04 (8.38e-04)	Tok/s 60878 (54680)	Loss/tok 3.1137 (3.2582)	LR 2.000e-03
0: TRAIN [2][1060/1938]	Time 0.275 (0.256)	Data 1.36e-04 (8.32e-04)	Tok/s 61217 (54689)	Loss/tok 3.3165 (3.2581)	LR 2.000e-03
0: TRAIN [2][1070/1938]	Time 0.217 (0.255)	Data 1.35e-04 (8.26e-04)	Tok/s 49393 (54660)	Loss/tok 3.0092 (3.2570)	LR 2.000e-03
0: TRAIN [2][1080/1938]	Time 0.161 (0.255)	Data 1.16e-04 (8.19e-04)	Tok/s 32352 (54591)	Loss/tok 2.6313 (3.2559)	LR 2.000e-03
0: TRAIN [2][1090/1938]	Time 0.216 (0.255)	Data 1.22e-04 (8.13e-04)	Tok/s 48262 (54564)	Loss/tok 2.9008 (3.2560)	LR 2.000e-03
0: TRAIN [2][1100/1938]	Time 0.217 (0.255)	Data 1.25e-04 (8.08e-04)	Tok/s 47403 (54534)	Loss/tok 2.9970 (3.2566)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.216 (0.255)	Data 1.48e-04 (8.02e-04)	Tok/s 47879 (54561)	Loss/tok 3.0788 (3.2579)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.216 (0.255)	Data 1.26e-04 (7.96e-04)	Tok/s 48662 (54559)	Loss/tok 3.1307 (3.2577)	LR 2.000e-03
0: TRAIN [2][1130/1938]	Time 0.216 (0.255)	Data 1.41e-04 (7.90e-04)	Tok/s 47217 (54535)	Loss/tok 3.0093 (3.2578)	LR 2.000e-03
0: TRAIN [2][1140/1938]	Time 0.216 (0.255)	Data 1.38e-04 (7.84e-04)	Tok/s 46829 (54549)	Loss/tok 3.0056 (3.2575)	LR 2.000e-03
0: TRAIN [2][1150/1938]	Time 0.216 (0.255)	Data 1.22e-04 (7.79e-04)	Tok/s 48742 (54561)	Loss/tok 3.0810 (3.2580)	LR 2.000e-03
0: TRAIN [2][1160/1938]	Time 0.335 (0.255)	Data 1.85e-04 (7.74e-04)	Tok/s 69368 (54613)	Loss/tok 3.5060 (3.2590)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.275 (0.255)	Data 1.50e-04 (7.69e-04)	Tok/s 61971 (54584)	Loss/tok 3.3149 (3.2583)	LR 2.000e-03
0: TRAIN [2][1180/1938]	Time 0.408 (0.255)	Data 1.35e-04 (7.63e-04)	Tok/s 73859 (54610)	Loss/tok 3.4750 (3.2583)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1190/1938]	Time 0.215 (0.255)	Data 2.52e-04 (7.58e-04)	Tok/s 48331 (54627)	Loss/tok 2.9913 (3.2587)	LR 2.000e-03
0: TRAIN [2][1200/1938]	Time 0.216 (0.255)	Data 1.30e-04 (7.53e-04)	Tok/s 48264 (54626)	Loss/tok 3.0142 (3.2586)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.276 (0.255)	Data 1.31e-04 (7.48e-04)	Tok/s 59869 (54630)	Loss/tok 3.2147 (3.2583)	LR 2.000e-03
0: TRAIN [2][1220/1938]	Time 0.216 (0.255)	Data 3.14e-04 (7.43e-04)	Tok/s 47884 (54628)	Loss/tok 3.0913 (3.2586)	LR 2.000e-03
0: TRAIN [2][1230/1938]	Time 0.215 (0.256)	Data 1.24e-04 (7.38e-04)	Tok/s 48203 (54652)	Loss/tok 3.0866 (3.2593)	LR 2.000e-03
0: TRAIN [2][1240/1938]	Time 0.216 (0.255)	Data 1.17e-04 (7.34e-04)	Tok/s 48338 (54633)	Loss/tok 2.8834 (3.2586)	LR 2.000e-03
0: TRAIN [2][1250/1938]	Time 0.216 (0.255)	Data 1.58e-04 (7.29e-04)	Tok/s 46699 (54614)	Loss/tok 3.0750 (3.2583)	LR 2.000e-03
0: TRAIN [2][1260/1938]	Time 0.335 (0.255)	Data 1.23e-04 (7.24e-04)	Tok/s 69162 (54623)	Loss/tok 3.4941 (3.2583)	LR 2.000e-03
0: TRAIN [2][1270/1938]	Time 0.408 (0.256)	Data 1.40e-04 (7.20e-04)	Tok/s 72979 (54653)	Loss/tok 3.6380 (3.2592)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.276 (0.256)	Data 1.39e-04 (7.15e-04)	Tok/s 61282 (54674)	Loss/tok 3.2499 (3.2599)	LR 2.000e-03
0: TRAIN [2][1290/1938]	Time 0.216 (0.255)	Data 1.46e-04 (7.11e-04)	Tok/s 46545 (54640)	Loss/tok 3.1242 (3.2589)	LR 2.000e-03
0: TRAIN [2][1300/1938]	Time 0.275 (0.256)	Data 1.39e-04 (7.06e-04)	Tok/s 60470 (54695)	Loss/tok 3.2187 (3.2595)	LR 2.000e-03
0: TRAIN [2][1310/1938]	Time 0.216 (0.256)	Data 1.40e-04 (7.02e-04)	Tok/s 47414 (54677)	Loss/tok 3.0237 (3.2594)	LR 2.000e-03
0: TRAIN [2][1320/1938]	Time 0.275 (0.256)	Data 1.15e-04 (6.97e-04)	Tok/s 60919 (54722)	Loss/tok 3.2405 (3.2607)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1330/1938]	Time 0.336 (0.256)	Data 1.30e-04 (6.94e-04)	Tok/s 69718 (54743)	Loss/tok 3.4338 (3.2617)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.159 (0.256)	Data 1.12e-04 (6.89e-04)	Tok/s 32403 (54726)	Loss/tok 2.6030 (3.2611)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.335 (0.256)	Data 1.14e-04 (6.85e-04)	Tok/s 69745 (54760)	Loss/tok 3.3460 (3.2618)	LR 2.000e-03
0: TRAIN [2][1360/1938]	Time 0.217 (0.256)	Data 1.11e-04 (6.81e-04)	Tok/s 47389 (54728)	Loss/tok 3.1189 (3.2610)	LR 2.000e-03
0: TRAIN [2][1370/1938]	Time 0.275 (0.256)	Data 1.30e-04 (6.77e-04)	Tok/s 60509 (54743)	Loss/tok 3.2255 (3.2607)	LR 2.000e-03
0: TRAIN [2][1380/1938]	Time 0.275 (0.256)	Data 1.29e-04 (6.73e-04)	Tok/s 61888 (54740)	Loss/tok 3.3474 (3.2613)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.215 (0.256)	Data 1.26e-04 (6.69e-04)	Tok/s 47660 (54738)	Loss/tok 2.9459 (3.2606)	LR 2.000e-03
0: TRAIN [2][1400/1938]	Time 0.275 (0.256)	Data 1.18e-04 (6.66e-04)	Tok/s 61503 (54722)	Loss/tok 3.2990 (3.2605)	LR 2.000e-03
0: TRAIN [2][1410/1938]	Time 0.275 (0.256)	Data 1.45e-04 (6.62e-04)	Tok/s 60611 (54724)	Loss/tok 3.3537 (3.2610)	LR 2.000e-03
0: TRAIN [2][1420/1938]	Time 0.217 (0.256)	Data 1.27e-04 (6.59e-04)	Tok/s 46432 (54701)	Loss/tok 2.8876 (3.2600)	LR 2.000e-03
0: TRAIN [2][1430/1938]	Time 0.276 (0.256)	Data 1.22e-04 (6.55e-04)	Tok/s 59905 (54665)	Loss/tok 3.3469 (3.2594)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.217 (0.256)	Data 1.43e-04 (6.51e-04)	Tok/s 49156 (54660)	Loss/tok 3.0274 (3.2600)	LR 2.000e-03
0: TRAIN [2][1450/1938]	Time 0.162 (0.256)	Data 1.23e-04 (6.48e-04)	Tok/s 32052 (54636)	Loss/tok 2.5464 (3.2595)	LR 2.000e-03
0: TRAIN [2][1460/1938]	Time 0.161 (0.256)	Data 1.40e-04 (6.44e-04)	Tok/s 32956 (54651)	Loss/tok 2.6263 (3.2594)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.335 (0.256)	Data 1.32e-04 (6.41e-04)	Tok/s 69185 (54646)	Loss/tok 3.3316 (3.2588)	LR 2.000e-03
0: TRAIN [2][1480/1938]	Time 0.275 (0.256)	Data 1.17e-04 (6.38e-04)	Tok/s 61087 (54648)	Loss/tok 3.3208 (3.2588)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.216 (0.256)	Data 1.30e-04 (6.34e-04)	Tok/s 48679 (54668)	Loss/tok 2.9581 (3.2596)	LR 2.000e-03
0: TRAIN [2][1500/1938]	Time 0.276 (0.256)	Data 1.13e-04 (6.31e-04)	Tok/s 61329 (54681)	Loss/tok 3.2780 (3.2595)	LR 2.000e-03
0: TRAIN [2][1510/1938]	Time 0.215 (0.256)	Data 1.20e-04 (6.27e-04)	Tok/s 49097 (54648)	Loss/tok 3.0391 (3.2589)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.216 (0.256)	Data 1.16e-04 (6.24e-04)	Tok/s 47991 (54639)	Loss/tok 3.0703 (3.2585)	LR 2.000e-03
0: TRAIN [2][1530/1938]	Time 0.408 (0.256)	Data 1.52e-04 (6.21e-04)	Tok/s 72540 (54647)	Loss/tok 3.6569 (3.2588)	LR 2.000e-03
0: TRAIN [2][1540/1938]	Time 0.159 (0.255)	Data 1.52e-04 (6.18e-04)	Tok/s 33394 (54628)	Loss/tok 2.5971 (3.2585)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.216 (0.256)	Data 1.39e-04 (6.15e-04)	Tok/s 49074 (54659)	Loss/tok 3.0506 (3.2587)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1560/1938]	Time 0.275 (0.256)	Data 2.74e-04 (6.13e-04)	Tok/s 60596 (54685)	Loss/tok 3.3626 (3.2598)	LR 2.000e-03
0: TRAIN [2][1570/1938]	Time 0.160 (0.256)	Data 1.28e-04 (6.09e-04)	Tok/s 33306 (54663)	Loss/tok 2.6456 (3.2596)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.275 (0.256)	Data 1.38e-04 (6.06e-04)	Tok/s 61400 (54685)	Loss/tok 3.2254 (3.2596)	LR 2.000e-03
0: TRAIN [2][1590/1938]	Time 0.216 (0.256)	Data 1.55e-04 (6.04e-04)	Tok/s 47971 (54677)	Loss/tok 3.0855 (3.2595)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.275 (0.256)	Data 1.48e-04 (6.01e-04)	Tok/s 60623 (54700)	Loss/tok 3.2063 (3.2600)	LR 2.000e-03
0: TRAIN [2][1610/1938]	Time 0.216 (0.256)	Data 1.47e-04 (5.98e-04)	Tok/s 48001 (54731)	Loss/tok 3.0387 (3.2601)	LR 2.000e-03
0: TRAIN [2][1620/1938]	Time 0.335 (0.256)	Data 1.62e-04 (5.95e-04)	Tok/s 68387 (54736)	Loss/tok 3.4275 (3.2604)	LR 2.000e-03
0: TRAIN [2][1630/1938]	Time 0.275 (0.256)	Data 2.18e-04 (5.93e-04)	Tok/s 61705 (54779)	Loss/tok 3.2259 (3.2613)	LR 2.000e-03
0: TRAIN [2][1640/1938]	Time 0.217 (0.256)	Data 1.21e-04 (5.90e-04)	Tok/s 47821 (54755)	Loss/tok 3.0570 (3.2611)	LR 2.000e-03
0: TRAIN [2][1650/1938]	Time 0.216 (0.256)	Data 1.42e-04 (5.87e-04)	Tok/s 47692 (54752)	Loss/tok 3.0017 (3.2608)	LR 2.000e-03
0: TRAIN [2][1660/1938]	Time 0.216 (0.256)	Data 1.47e-04 (5.85e-04)	Tok/s 47844 (54743)	Loss/tok 3.0334 (3.2605)	LR 2.000e-03
0: TRAIN [2][1670/1938]	Time 0.277 (0.256)	Data 1.30e-04 (5.82e-04)	Tok/s 61021 (54755)	Loss/tok 3.2544 (3.2607)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.217 (0.256)	Data 1.41e-04 (5.79e-04)	Tok/s 47958 (54755)	Loss/tok 2.9955 (3.2607)	LR 2.000e-03
0: TRAIN [2][1690/1938]	Time 0.216 (0.256)	Data 1.26e-04 (5.77e-04)	Tok/s 47167 (54734)	Loss/tok 3.0466 (3.2600)	LR 2.000e-03
0: TRAIN [2][1700/1938]	Time 0.216 (0.256)	Data 1.45e-04 (5.74e-04)	Tok/s 49310 (54701)	Loss/tok 3.0050 (3.2592)	LR 2.000e-03
0: TRAIN [2][1710/1938]	Time 0.160 (0.256)	Data 1.16e-04 (5.72e-04)	Tok/s 33150 (54702)	Loss/tok 2.5517 (3.2594)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1720/1938]	Time 0.272 (0.256)	Data 1.36e-04 (5.69e-04)	Tok/s 62610 (54715)	Loss/tok 3.3086 (3.2599)	LR 2.000e-03
0: TRAIN [2][1730/1938]	Time 0.407 (0.256)	Data 1.40e-04 (5.67e-04)	Tok/s 73552 (54757)	Loss/tok 3.5000 (3.2604)	LR 2.000e-03
0: TRAIN [2][1740/1938]	Time 0.216 (0.256)	Data 1.42e-04 (5.64e-04)	Tok/s 47276 (54747)	Loss/tok 3.0638 (3.2607)	LR 2.000e-03
0: TRAIN [2][1750/1938]	Time 0.216 (0.256)	Data 1.30e-04 (5.62e-04)	Tok/s 47734 (54745)	Loss/tok 3.0207 (3.2604)	LR 2.000e-03
0: TRAIN [2][1760/1938]	Time 0.275 (0.256)	Data 1.14e-04 (5.59e-04)	Tok/s 61523 (54756)	Loss/tok 3.3323 (3.2601)	LR 2.000e-03
0: TRAIN [2][1770/1938]	Time 0.216 (0.256)	Data 1.34e-04 (5.57e-04)	Tok/s 47739 (54765)	Loss/tok 3.0933 (3.2602)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.276 (0.256)	Data 1.18e-04 (5.55e-04)	Tok/s 60891 (54779)	Loss/tok 3.2295 (3.2601)	LR 2.000e-03
0: TRAIN [2][1790/1938]	Time 0.275 (0.256)	Data 1.43e-04 (5.52e-04)	Tok/s 60988 (54803)	Loss/tok 3.1877 (3.2601)	LR 2.000e-03
0: TRAIN [2][1800/1938]	Time 0.275 (0.256)	Data 1.31e-04 (5.50e-04)	Tok/s 62098 (54833)	Loss/tok 3.1591 (3.2604)	LR 2.000e-03
0: TRAIN [2][1810/1938]	Time 0.216 (0.256)	Data 1.30e-04 (5.48e-04)	Tok/s 47908 (54824)	Loss/tok 3.1400 (3.2602)	LR 2.000e-03
0: TRAIN [2][1820/1938]	Time 0.216 (0.256)	Data 1.28e-04 (5.45e-04)	Tok/s 47471 (54815)	Loss/tok 3.0525 (3.2601)	LR 2.000e-03
0: TRAIN [2][1830/1938]	Time 0.216 (0.256)	Data 1.17e-04 (5.43e-04)	Tok/s 47812 (54792)	Loss/tok 3.0239 (3.2595)	LR 2.000e-03
0: TRAIN [2][1840/1938]	Time 0.275 (0.256)	Data 1.46e-04 (5.41e-04)	Tok/s 61681 (54812)	Loss/tok 3.2095 (3.2597)	LR 2.000e-03
0: TRAIN [2][1850/1938]	Time 0.216 (0.256)	Data 1.28e-04 (5.39e-04)	Tok/s 47912 (54794)	Loss/tok 3.0465 (3.2593)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.275 (0.256)	Data 1.25e-04 (5.37e-04)	Tok/s 61109 (54778)	Loss/tok 3.3444 (3.2587)	LR 2.000e-03
0: TRAIN [2][1870/1938]	Time 0.216 (0.256)	Data 1.23e-04 (5.34e-04)	Tok/s 48215 (54762)	Loss/tok 2.9830 (3.2584)	LR 2.000e-03
0: TRAIN [2][1880/1938]	Time 0.276 (0.256)	Data 1.35e-04 (5.33e-04)	Tok/s 60416 (54753)	Loss/tok 3.2881 (3.2579)	LR 2.000e-03
0: TRAIN [2][1890/1938]	Time 0.275 (0.256)	Data 1.20e-04 (5.30e-04)	Tok/s 60574 (54777)	Loss/tok 3.3203 (3.2579)	LR 2.000e-03
0: TRAIN [2][1900/1938]	Time 0.215 (0.256)	Data 1.21e-04 (5.28e-04)	Tok/s 46945 (54770)	Loss/tok 2.9961 (3.2578)	LR 2.000e-03
0: TRAIN [2][1910/1938]	Time 0.215 (0.256)	Data 1.84e-04 (5.26e-04)	Tok/s 47615 (54722)	Loss/tok 2.9712 (3.2571)	LR 2.000e-03
0: TRAIN [2][1920/1938]	Time 0.276 (0.255)	Data 1.34e-04 (5.24e-04)	Tok/s 61136 (54696)	Loss/tok 3.2455 (3.2566)	LR 2.000e-03
0: TRAIN [2][1930/1938]	Time 0.275 (0.255)	Data 1.20e-04 (5.22e-04)	Tok/s 60857 (54687)	Loss/tok 3.2960 (3.2561)	LR 2.000e-03
:::MLL 1571838136.213 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1571838136.213 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.754 (0.754)	Decoder iters 102.0 (102.0)	Tok/s 21860 (21860)
0: Running moses detokenizer
0: BLEU(score=23.31884855110554, counts=[36623, 18098, 10185, 5973], totals=[65364, 62361, 59358, 56361], precisions=[56.02931277155621, 29.021343467872548, 17.15859698776913, 10.597753765901954], bp=1.0, sys_len=65364, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571838138.317 eval_accuracy: {"value": 23.32, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1571838138.318 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.2585	Test BLEU: 23.32
0: Performance: Epoch: 2	Training: 437718 Tok/s
0: Finished epoch 2
:::MLL 1571838138.318 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1571838138.318 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571838138.319 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 3769010536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][0/1938]	Time 0.892 (0.892)	Data 7.08e-01 (7.08e-01)	Tok/s 6011 (6011)	Loss/tok 2.5993 (2.5993)	LR 2.000e-03
0: TRAIN [3][10/1938]	Time 0.336 (0.360)	Data 1.33e-04 (6.45e-02)	Tok/s 69979 (53516)	Loss/tok 3.2835 (3.2868)	LR 2.000e-03
0: TRAIN [3][20/1938]	Time 0.275 (0.306)	Data 1.03e-04 (3.39e-02)	Tok/s 60359 (53401)	Loss/tok 3.1259 (3.1991)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.161 (0.287)	Data 1.13e-04 (2.30e-02)	Tok/s 32794 (53222)	Loss/tok 2.5462 (3.1883)	LR 2.000e-03
0: TRAIN [3][40/1938]	Time 0.161 (0.274)	Data 1.05e-04 (1.74e-02)	Tok/s 32801 (52813)	Loss/tok 2.4503 (3.1613)	LR 2.000e-03
0: TRAIN [3][50/1938]	Time 0.216 (0.276)	Data 1.09e-04 (1.40e-02)	Tok/s 47216 (54139)	Loss/tok 2.9763 (3.1683)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.215 (0.275)	Data 1.26e-04 (1.17e-02)	Tok/s 48460 (54736)	Loss/tok 2.8756 (3.1752)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.275 (0.270)	Data 1.05e-04 (1.01e-02)	Tok/s 61271 (54401)	Loss/tok 3.0537 (3.1674)	LR 2.000e-03
0: TRAIN [3][80/1938]	Time 0.275 (0.273)	Data 1.06e-04 (8.87e-03)	Tok/s 60762 (55243)	Loss/tok 3.1969 (3.1804)	LR 2.000e-03
0: TRAIN [3][90/1938]	Time 0.335 (0.274)	Data 1.14e-04 (7.91e-03)	Tok/s 69053 (55664)	Loss/tok 3.4339 (3.1876)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.216 (0.271)	Data 1.08e-04 (7.14e-03)	Tok/s 47810 (55503)	Loss/tok 3.0333 (3.1841)	LR 2.000e-03
0: TRAIN [3][110/1938]	Time 0.160 (0.268)	Data 3.98e-04 (6.51e-03)	Tok/s 33603 (55076)	Loss/tok 2.5824 (3.1783)	LR 2.000e-03
0: TRAIN [3][120/1938]	Time 0.215 (0.265)	Data 1.18e-04 (5.98e-03)	Tok/s 48075 (54802)	Loss/tok 2.9205 (3.1705)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.275 (0.264)	Data 1.24e-04 (5.53e-03)	Tok/s 61319 (54748)	Loss/tok 3.1413 (3.1644)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.216 (0.262)	Data 1.12e-04 (5.15e-03)	Tok/s 47654 (54486)	Loss/tok 2.9865 (3.1592)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.275 (0.260)	Data 1.15e-04 (4.82e-03)	Tok/s 61080 (54363)	Loss/tok 3.1160 (3.1537)	LR 2.000e-03
0: TRAIN [3][160/1938]	Time 0.215 (0.259)	Data 1.38e-04 (4.53e-03)	Tok/s 47836 (54200)	Loss/tok 3.0248 (3.1489)	LR 2.000e-03
0: TRAIN [3][170/1938]	Time 0.216 (0.260)	Data 1.15e-04 (4.27e-03)	Tok/s 46672 (54528)	Loss/tok 3.0382 (3.1561)	LR 2.000e-03
0: TRAIN [3][180/1938]	Time 0.216 (0.259)	Data 1.03e-04 (4.04e-03)	Tok/s 48368 (54424)	Loss/tok 2.9866 (3.1531)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.160 (0.259)	Data 1.29e-04 (3.84e-03)	Tok/s 32789 (54436)	Loss/tok 2.6914 (3.1533)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.216 (0.259)	Data 1.17e-04 (3.65e-03)	Tok/s 47453 (54525)	Loss/tok 3.0266 (3.1567)	LR 2.000e-03
0: TRAIN [3][210/1938]	Time 0.216 (0.258)	Data 1.30e-04 (3.48e-03)	Tok/s 48128 (54499)	Loss/tok 3.0507 (3.1561)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.216 (0.256)	Data 1.32e-04 (3.33e-03)	Tok/s 48389 (54138)	Loss/tok 3.0316 (3.1511)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.216 (0.256)	Data 1.63e-04 (3.20e-03)	Tok/s 48265 (54134)	Loss/tok 3.0929 (3.1494)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][240/1938]	Time 0.216 (0.255)	Data 1.07e-04 (3.07e-03)	Tok/s 48502 (54064)	Loss/tok 3.0217 (3.1491)	LR 2.000e-03
0: TRAIN [3][250/1938]	Time 0.216 (0.254)	Data 5.33e-04 (2.95e-03)	Tok/s 48259 (53953)	Loss/tok 3.0454 (3.1478)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.216 (0.255)	Data 1.10e-04 (2.85e-03)	Tok/s 48646 (54088)	Loss/tok 3.0110 (3.1530)	LR 2.000e-03
0: TRAIN [3][270/1938]	Time 0.217 (0.255)	Data 1.08e-04 (2.75e-03)	Tok/s 48174 (54032)	Loss/tok 2.9842 (3.1510)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.216 (0.255)	Data 1.26e-04 (2.65e-03)	Tok/s 48258 (54110)	Loss/tok 2.9051 (3.1524)	LR 2.000e-03
0: TRAIN [3][290/1938]	Time 0.216 (0.254)	Data 1.08e-04 (2.57e-03)	Tok/s 48003 (54030)	Loss/tok 3.0221 (3.1499)	LR 2.000e-03
0: TRAIN [3][300/1938]	Time 0.276 (0.254)	Data 1.14e-04 (2.49e-03)	Tok/s 61260 (54032)	Loss/tok 3.1222 (3.1481)	LR 2.000e-03
0: TRAIN [3][310/1938]	Time 0.215 (0.254)	Data 1.10e-04 (2.41e-03)	Tok/s 47965 (54126)	Loss/tok 3.0018 (3.1484)	LR 2.000e-03
0: TRAIN [3][320/1938]	Time 0.215 (0.254)	Data 1.12e-04 (2.34e-03)	Tok/s 48749 (54036)	Loss/tok 2.9240 (3.1491)	LR 2.000e-03
0: TRAIN [3][330/1938]	Time 0.217 (0.255)	Data 1.06e-04 (2.27e-03)	Tok/s 48371 (54171)	Loss/tok 3.0356 (3.1531)	LR 2.000e-03
0: TRAIN [3][340/1938]	Time 0.216 (0.254)	Data 1.18e-04 (2.21e-03)	Tok/s 48404 (54016)	Loss/tok 2.9186 (3.1505)	LR 2.000e-03
0: TRAIN [3][350/1938]	Time 0.275 (0.254)	Data 1.39e-04 (2.15e-03)	Tok/s 60974 (54154)	Loss/tok 3.1562 (3.1546)	LR 2.000e-03
0: TRAIN [3][360/1938]	Time 0.216 (0.254)	Data 1.03e-04 (2.10e-03)	Tok/s 48333 (54161)	Loss/tok 2.9117 (3.1554)	LR 2.000e-03
0: TRAIN [3][370/1938]	Time 0.276 (0.255)	Data 1.29e-04 (2.04e-03)	Tok/s 60402 (54209)	Loss/tok 3.2491 (3.1560)	LR 2.000e-03
0: TRAIN [3][380/1938]	Time 0.275 (0.255)	Data 1.08e-04 (1.99e-03)	Tok/s 61704 (54269)	Loss/tok 3.2467 (3.1596)	LR 2.000e-03
0: TRAIN [3][390/1938]	Time 0.274 (0.255)	Data 2.72e-04 (1.95e-03)	Tok/s 61268 (54251)	Loss/tok 3.1256 (3.1592)	LR 2.000e-03
0: TRAIN [3][400/1938]	Time 0.216 (0.255)	Data 1.24e-04 (1.90e-03)	Tok/s 47401 (54244)	Loss/tok 3.0361 (3.1590)	LR 2.000e-03
0: TRAIN [3][410/1938]	Time 0.216 (0.255)	Data 1.28e-04 (1.86e-03)	Tok/s 47135 (54270)	Loss/tok 3.0403 (3.1606)	LR 2.000e-03
0: TRAIN [3][420/1938]	Time 0.275 (0.254)	Data 1.10e-04 (1.82e-03)	Tok/s 61194 (54200)	Loss/tok 3.1615 (3.1588)	LR 2.000e-03
0: TRAIN [3][430/1938]	Time 0.276 (0.254)	Data 1.05e-04 (1.78e-03)	Tok/s 60564 (54200)	Loss/tok 3.2480 (3.1580)	LR 2.000e-03
0: TRAIN [3][440/1938]	Time 0.216 (0.255)	Data 1.14e-04 (1.74e-03)	Tok/s 47086 (54298)	Loss/tok 3.0688 (3.1600)	LR 2.000e-03
0: TRAIN [3][450/1938]	Time 0.275 (0.254)	Data 1.14e-04 (1.70e-03)	Tok/s 61543 (54258)	Loss/tok 3.1528 (3.1592)	LR 2.000e-03
0: TRAIN [3][460/1938]	Time 0.275 (0.255)	Data 1.09e-04 (1.67e-03)	Tok/s 61982 (54346)	Loss/tok 3.1293 (3.1600)	LR 2.000e-03
0: TRAIN [3][470/1938]	Time 0.217 (0.254)	Data 1.08e-04 (1.64e-03)	Tok/s 47778 (54310)	Loss/tok 2.9115 (3.1588)	LR 2.000e-03
0: TRAIN [3][480/1938]	Time 0.273 (0.255)	Data 1.21e-04 (1.61e-03)	Tok/s 61854 (54372)	Loss/tok 3.1752 (3.1609)	LR 2.000e-03
0: TRAIN [3][490/1938]	Time 0.276 (0.254)	Data 1.43e-04 (1.58e-03)	Tok/s 61596 (54309)	Loss/tok 3.1187 (3.1602)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][500/1938]	Time 0.335 (0.255)	Data 1.13e-04 (1.55e-03)	Tok/s 70223 (54342)	Loss/tok 3.3575 (3.1629)	LR 2.000e-03
0: TRAIN [3][510/1938]	Time 0.335 (0.255)	Data 1.24e-04 (1.52e-03)	Tok/s 69093 (54394)	Loss/tok 3.3548 (3.1635)	LR 2.000e-03
0: TRAIN [3][520/1938]	Time 0.217 (0.254)	Data 1.22e-04 (1.49e-03)	Tok/s 48107 (54330)	Loss/tok 3.0528 (3.1628)	LR 2.000e-03
0: TRAIN [3][530/1938]	Time 0.216 (0.255)	Data 1.21e-04 (1.47e-03)	Tok/s 47471 (54405)	Loss/tok 2.9549 (3.1639)	LR 2.000e-03
0: TRAIN [3][540/1938]	Time 0.275 (0.254)	Data 1.35e-04 (1.44e-03)	Tok/s 60682 (54367)	Loss/tok 3.1704 (3.1632)	LR 2.000e-03
0: TRAIN [3][550/1938]	Time 0.160 (0.254)	Data 1.30e-04 (1.42e-03)	Tok/s 33314 (54263)	Loss/tok 2.5787 (3.1620)	LR 2.000e-03
0: TRAIN [3][560/1938]	Time 0.275 (0.254)	Data 1.53e-04 (1.40e-03)	Tok/s 60471 (54234)	Loss/tok 3.1410 (3.1606)	LR 2.000e-03
0: TRAIN [3][570/1938]	Time 0.216 (0.254)	Data 1.42e-04 (1.37e-03)	Tok/s 48809 (54258)	Loss/tok 2.9264 (3.1620)	LR 2.000e-03
0: TRAIN [3][580/1938]	Time 0.216 (0.253)	Data 5.71e-04 (1.35e-03)	Tok/s 47710 (54202)	Loss/tok 2.9888 (3.1607)	LR 2.000e-03
0: TRAIN [3][590/1938]	Time 0.216 (0.253)	Data 1.09e-04 (1.33e-03)	Tok/s 48507 (54107)	Loss/tok 2.9174 (3.1591)	LR 2.000e-03
0: TRAIN [3][600/1938]	Time 0.160 (0.253)	Data 2.78e-04 (1.31e-03)	Tok/s 33130 (54164)	Loss/tok 2.5139 (3.1609)	LR 2.000e-03
0: TRAIN [3][610/1938]	Time 0.276 (0.253)	Data 1.17e-04 (1.29e-03)	Tok/s 60515 (54188)	Loss/tok 3.3098 (3.1616)	LR 2.000e-03
0: TRAIN [3][620/1938]	Time 0.216 (0.253)	Data 1.01e-04 (1.27e-03)	Tok/s 48118 (54119)	Loss/tok 3.0406 (3.1612)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][630/1938]	Time 0.216 (0.254)	Data 7.31e-04 (1.26e-03)	Tok/s 47937 (54231)	Loss/tok 2.9219 (3.1626)	LR 2.000e-03
0: TRAIN [3][640/1938]	Time 0.276 (0.254)	Data 1.09e-04 (1.24e-03)	Tok/s 60877 (54237)	Loss/tok 3.2121 (3.1629)	LR 2.000e-03
0: TRAIN [3][650/1938]	Time 0.216 (0.254)	Data 1.05e-04 (1.22e-03)	Tok/s 46637 (54276)	Loss/tok 2.9291 (3.1639)	LR 2.000e-03
0: TRAIN [3][660/1938]	Time 0.275 (0.254)	Data 1.06e-04 (1.21e-03)	Tok/s 61737 (54361)	Loss/tok 3.1552 (3.1653)	LR 2.000e-03
0: TRAIN [3][670/1938]	Time 0.217 (0.254)	Data 4.38e-04 (1.19e-03)	Tok/s 47228 (54273)	Loss/tok 2.8815 (3.1641)	LR 2.000e-03
0: TRAIN [3][680/1938]	Time 0.216 (0.254)	Data 1.07e-04 (1.17e-03)	Tok/s 46694 (54318)	Loss/tok 3.0505 (3.1651)	LR 1.000e-03
0: TRAIN [3][690/1938]	Time 0.215 (0.254)	Data 1.05e-04 (1.16e-03)	Tok/s 47506 (54322)	Loss/tok 2.9251 (3.1657)	LR 1.000e-03
0: TRAIN [3][700/1938]	Time 0.275 (0.255)	Data 1.06e-04 (1.14e-03)	Tok/s 61736 (54414)	Loss/tok 3.1907 (3.1669)	LR 1.000e-03
0: TRAIN [3][710/1938]	Time 0.335 (0.255)	Data 1.12e-04 (1.13e-03)	Tok/s 69639 (54544)	Loss/tok 3.4066 (3.1712)	LR 1.000e-03
0: TRAIN [3][720/1938]	Time 0.277 (0.255)	Data 7.73e-04 (1.12e-03)	Tok/s 60333 (54525)	Loss/tok 3.2113 (3.1700)	LR 1.000e-03
0: TRAIN [3][730/1938]	Time 0.276 (0.255)	Data 1.23e-04 (1.10e-03)	Tok/s 60963 (54514)	Loss/tok 3.2461 (3.1701)	LR 1.000e-03
0: TRAIN [3][740/1938]	Time 0.276 (0.255)	Data 1.02e-04 (1.09e-03)	Tok/s 60294 (54558)	Loss/tok 3.2224 (3.1708)	LR 1.000e-03
0: TRAIN [3][750/1938]	Time 0.216 (0.255)	Data 1.07e-04 (1.08e-03)	Tok/s 47254 (54526)	Loss/tok 2.9236 (3.1697)	LR 1.000e-03
0: TRAIN [3][760/1938]	Time 0.161 (0.255)	Data 1.05e-04 (1.06e-03)	Tok/s 32815 (54527)	Loss/tok 2.5438 (3.1694)	LR 1.000e-03
0: TRAIN [3][770/1938]	Time 0.216 (0.255)	Data 1.19e-04 (1.05e-03)	Tok/s 48051 (54531)	Loss/tok 2.9091 (3.1693)	LR 1.000e-03
0: TRAIN [3][780/1938]	Time 0.276 (0.255)	Data 1.29e-04 (1.04e-03)	Tok/s 60233 (54537)	Loss/tok 3.2167 (3.1693)	LR 1.000e-03
0: TRAIN [3][790/1938]	Time 0.335 (0.256)	Data 1.07e-04 (1.03e-03)	Tok/s 69807 (54641)	Loss/tok 3.3018 (3.1710)	LR 1.000e-03
0: TRAIN [3][800/1938]	Time 0.275 (0.256)	Data 1.32e-04 (1.02e-03)	Tok/s 61073 (54678)	Loss/tok 3.1207 (3.1709)	LR 1.000e-03
0: TRAIN [3][810/1938]	Time 0.217 (0.256)	Data 1.24e-04 (1.01e-03)	Tok/s 47850 (54655)	Loss/tok 2.9677 (3.1698)	LR 1.000e-03
0: TRAIN [3][820/1938]	Time 0.216 (0.256)	Data 1.39e-04 (9.97e-04)	Tok/s 47325 (54645)	Loss/tok 2.9385 (3.1691)	LR 1.000e-03
0: TRAIN [3][830/1938]	Time 0.335 (0.256)	Data 1.04e-04 (9.86e-04)	Tok/s 69135 (54690)	Loss/tok 3.2686 (3.1707)	LR 1.000e-03
0: TRAIN [3][840/1938]	Time 0.160 (0.256)	Data 1.02e-04 (9.76e-04)	Tok/s 32530 (54636)	Loss/tok 2.5973 (3.1695)	LR 1.000e-03
0: TRAIN [3][850/1938]	Time 0.216 (0.255)	Data 1.04e-04 (9.66e-04)	Tok/s 47619 (54579)	Loss/tok 2.9904 (3.1686)	LR 1.000e-03
0: TRAIN [3][860/1938]	Time 0.216 (0.256)	Data 1.21e-04 (9.56e-04)	Tok/s 47903 (54593)	Loss/tok 3.0320 (3.1687)	LR 1.000e-03
0: TRAIN [3][870/1938]	Time 0.216 (0.256)	Data 1.08e-04 (9.47e-04)	Tok/s 47308 (54671)	Loss/tok 3.0659 (3.1698)	LR 1.000e-03
0: TRAIN [3][880/1938]	Time 0.216 (0.256)	Data 1.36e-04 (9.37e-04)	Tok/s 48153 (54716)	Loss/tok 2.9073 (3.1698)	LR 1.000e-03
0: TRAIN [3][890/1938]	Time 0.216 (0.256)	Data 1.10e-04 (9.28e-04)	Tok/s 47992 (54723)	Loss/tok 2.8071 (3.1685)	LR 1.000e-03
0: TRAIN [3][900/1938]	Time 0.216 (0.256)	Data 1.11e-04 (9.19e-04)	Tok/s 47246 (54741)	Loss/tok 2.9169 (3.1687)	LR 1.000e-03
0: TRAIN [3][910/1938]	Time 0.216 (0.256)	Data 1.20e-04 (9.10e-04)	Tok/s 46860 (54734)	Loss/tok 2.8202 (3.1676)	LR 1.000e-03
0: TRAIN [3][920/1938]	Time 0.276 (0.256)	Data 3.13e-04 (9.02e-04)	Tok/s 61105 (54746)	Loss/tok 3.0593 (3.1685)	LR 1.000e-03
0: TRAIN [3][930/1938]	Time 0.216 (0.256)	Data 1.47e-04 (8.94e-04)	Tok/s 47887 (54782)	Loss/tok 2.9091 (3.1686)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][940/1938]	Time 0.276 (0.256)	Data 1.22e-04 (8.86e-04)	Tok/s 61399 (54674)	Loss/tok 3.1060 (3.1671)	LR 1.000e-03
0: TRAIN [3][950/1938]	Time 0.276 (0.256)	Data 1.06e-04 (8.78e-04)	Tok/s 60512 (54668)	Loss/tok 3.0236 (3.1663)	LR 1.000e-03
0: TRAIN [3][960/1938]	Time 0.216 (0.256)	Data 1.10e-04 (8.70e-04)	Tok/s 48406 (54667)	Loss/tok 3.0365 (3.1668)	LR 1.000e-03
0: TRAIN [3][970/1938]	Time 0.335 (0.256)	Data 1.07e-04 (8.62e-04)	Tok/s 69269 (54731)	Loss/tok 3.3977 (3.1671)	LR 1.000e-03
0: TRAIN [3][980/1938]	Time 0.409 (0.256)	Data 1.05e-04 (8.55e-04)	Tok/s 73295 (54737)	Loss/tok 3.4796 (3.1676)	LR 1.000e-03
0: TRAIN [3][990/1938]	Time 0.276 (0.256)	Data 1.15e-04 (8.47e-04)	Tok/s 60827 (54716)	Loss/tok 3.1569 (3.1666)	LR 1.000e-03
0: TRAIN [3][1000/1938]	Time 0.161 (0.256)	Data 1.30e-04 (8.40e-04)	Tok/s 31871 (54696)	Loss/tok 2.4754 (3.1655)	LR 1.000e-03
0: TRAIN [3][1010/1938]	Time 0.216 (0.256)	Data 2.24e-04 (8.33e-04)	Tok/s 47533 (54708)	Loss/tok 2.9512 (3.1659)	LR 1.000e-03
0: TRAIN [3][1020/1938]	Time 0.275 (0.257)	Data 1.04e-04 (8.26e-04)	Tok/s 60433 (54798)	Loss/tok 3.1474 (3.1673)	LR 1.000e-03
0: TRAIN [3][1030/1938]	Time 0.336 (0.256)	Data 1.18e-04 (8.19e-04)	Tok/s 68899 (54785)	Loss/tok 3.3545 (3.1664)	LR 1.000e-03
0: TRAIN [3][1040/1938]	Time 0.161 (0.256)	Data 1.54e-04 (8.12e-04)	Tok/s 32705 (54764)	Loss/tok 2.6334 (3.1654)	LR 1.000e-03
0: TRAIN [3][1050/1938]	Time 0.275 (0.256)	Data 1.04e-04 (8.06e-04)	Tok/s 61213 (54742)	Loss/tok 3.1729 (3.1646)	LR 1.000e-03
0: TRAIN [3][1060/1938]	Time 0.216 (0.256)	Data 1.08e-04 (8.00e-04)	Tok/s 48529 (54728)	Loss/tok 2.9729 (3.1640)	LR 1.000e-03
0: TRAIN [3][1070/1938]	Time 0.274 (0.256)	Data 1.27e-04 (7.94e-04)	Tok/s 61262 (54705)	Loss/tok 3.1431 (3.1631)	LR 1.000e-03
0: TRAIN [3][1080/1938]	Time 0.216 (0.256)	Data 1.29e-04 (7.88e-04)	Tok/s 47944 (54722)	Loss/tok 3.0281 (3.1623)	LR 1.000e-03
0: TRAIN [3][1090/1938]	Time 0.216 (0.256)	Data 1.03e-04 (7.82e-04)	Tok/s 47530 (54732)	Loss/tok 3.0119 (3.1621)	LR 1.000e-03
0: TRAIN [3][1100/1938]	Time 0.275 (0.256)	Data 1.08e-04 (7.76e-04)	Tok/s 60183 (54754)	Loss/tok 3.2477 (3.1620)	LR 1.000e-03
0: TRAIN [3][1110/1938]	Time 0.275 (0.256)	Data 1.17e-04 (7.70e-04)	Tok/s 61015 (54727)	Loss/tok 3.1208 (3.1622)	LR 1.000e-03
0: TRAIN [3][1120/1938]	Time 0.216 (0.256)	Data 1.23e-04 (7.64e-04)	Tok/s 47308 (54773)	Loss/tok 2.9263 (3.1627)	LR 1.000e-03
0: TRAIN [3][1130/1938]	Time 0.335 (0.256)	Data 1.11e-04 (7.58e-04)	Tok/s 70234 (54766)	Loss/tok 3.3669 (3.1620)	LR 1.000e-03
0: TRAIN [3][1140/1938]	Time 0.216 (0.256)	Data 1.06e-04 (7.53e-04)	Tok/s 48348 (54737)	Loss/tok 2.9448 (3.1612)	LR 1.000e-03
0: TRAIN [3][1150/1938]	Time 0.215 (0.256)	Data 1.10e-04 (7.47e-04)	Tok/s 48691 (54742)	Loss/tok 2.8518 (3.1606)	LR 1.000e-03
0: TRAIN [3][1160/1938]	Time 0.275 (0.256)	Data 1.08e-04 (7.42e-04)	Tok/s 60428 (54736)	Loss/tok 3.1550 (3.1604)	LR 1.000e-03
0: TRAIN [3][1170/1938]	Time 0.276 (0.256)	Data 1.24e-04 (7.37e-04)	Tok/s 61576 (54782)	Loss/tok 3.0588 (3.1602)	LR 1.000e-03
0: TRAIN [3][1180/1938]	Time 0.217 (0.256)	Data 1.32e-04 (7.32e-04)	Tok/s 47691 (54736)	Loss/tok 2.8594 (3.1594)	LR 1.000e-03
0: TRAIN [3][1190/1938]	Time 0.276 (0.256)	Data 1.19e-04 (7.27e-04)	Tok/s 59762 (54675)	Loss/tok 3.2276 (3.1585)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1200/1938]	Time 0.276 (0.256)	Data 1.17e-04 (7.22e-04)	Tok/s 61022 (54694)	Loss/tok 3.1332 (3.1589)	LR 1.000e-03
0: TRAIN [3][1210/1938]	Time 0.275 (0.256)	Data 1.24e-04 (7.17e-04)	Tok/s 61203 (54710)	Loss/tok 3.1763 (3.1596)	LR 1.000e-03
0: TRAIN [3][1220/1938]	Time 0.276 (0.256)	Data 1.07e-04 (7.12e-04)	Tok/s 61616 (54770)	Loss/tok 3.1646 (3.1607)	LR 1.000e-03
0: TRAIN [3][1230/1938]	Time 0.275 (0.256)	Data 1.36e-04 (7.08e-04)	Tok/s 61401 (54797)	Loss/tok 3.1837 (3.1609)	LR 1.000e-03
0: TRAIN [3][1240/1938]	Time 0.276 (0.256)	Data 1.24e-04 (7.03e-04)	Tok/s 61663 (54822)	Loss/tok 3.1592 (3.1608)	LR 1.000e-03
0: TRAIN [3][1250/1938]	Time 0.160 (0.256)	Data 1.21e-04 (6.99e-04)	Tok/s 32808 (54819)	Loss/tok 2.5286 (3.1609)	LR 1.000e-03
0: TRAIN [3][1260/1938]	Time 0.216 (0.256)	Data 1.08e-04 (6.94e-04)	Tok/s 48734 (54852)	Loss/tok 2.9469 (3.1614)	LR 1.000e-03
0: TRAIN [3][1270/1938]	Time 0.276 (0.257)	Data 1.31e-04 (6.90e-04)	Tok/s 60309 (54876)	Loss/tok 3.2538 (3.1616)	LR 1.000e-03
0: TRAIN [3][1280/1938]	Time 0.216 (0.256)	Data 1.13e-04 (6.85e-04)	Tok/s 47949 (54851)	Loss/tok 2.9632 (3.1608)	LR 1.000e-03
0: TRAIN [3][1290/1938]	Time 0.219 (0.256)	Data 1.09e-04 (6.81e-04)	Tok/s 47165 (54825)	Loss/tok 2.8928 (3.1605)	LR 1.000e-03
0: TRAIN [3][1300/1938]	Time 0.215 (0.256)	Data 1.12e-04 (6.77e-04)	Tok/s 48591 (54849)	Loss/tok 2.9815 (3.1602)	LR 1.000e-03
0: TRAIN [3][1310/1938]	Time 0.335 (0.256)	Data 1.46e-04 (6.73e-04)	Tok/s 69169 (54865)	Loss/tok 3.3642 (3.1606)	LR 1.000e-03
0: TRAIN [3][1320/1938]	Time 0.217 (0.256)	Data 1.08e-04 (6.69e-04)	Tok/s 48075 (54829)	Loss/tok 2.9879 (3.1604)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1330/1938]	Time 0.217 (0.256)	Data 1.38e-04 (6.65e-04)	Tok/s 47095 (54823)	Loss/tok 2.9685 (3.1603)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1340/1938]	Time 0.216 (0.256)	Data 1.02e-04 (6.60e-04)	Tok/s 47425 (54801)	Loss/tok 2.9922 (3.1598)	LR 1.000e-03
0: TRAIN [3][1350/1938]	Time 0.275 (0.256)	Data 1.08e-04 (6.56e-04)	Tok/s 60852 (54808)	Loss/tok 3.0523 (3.1595)	LR 1.000e-03
0: TRAIN [3][1360/1938]	Time 0.277 (0.256)	Data 1.10e-04 (6.53e-04)	Tok/s 60551 (54817)	Loss/tok 3.1491 (3.1592)	LR 1.000e-03
0: TRAIN [3][1370/1938]	Time 0.275 (0.256)	Data 1.10e-04 (6.49e-04)	Tok/s 60370 (54790)	Loss/tok 3.2127 (3.1591)	LR 1.000e-03
0: TRAIN [3][1380/1938]	Time 0.275 (0.256)	Data 1.09e-04 (6.46e-04)	Tok/s 60972 (54794)	Loss/tok 3.1353 (3.1589)	LR 1.000e-03
0: TRAIN [3][1390/1938]	Time 0.161 (0.256)	Data 1.13e-04 (6.42e-04)	Tok/s 32288 (54753)	Loss/tok 2.5971 (3.1580)	LR 1.000e-03
0: TRAIN [3][1400/1938]	Time 0.275 (0.256)	Data 1.06e-04 (6.38e-04)	Tok/s 61350 (54768)	Loss/tok 3.0390 (3.1579)	LR 1.000e-03
0: TRAIN [3][1410/1938]	Time 0.276 (0.256)	Data 1.28e-04 (6.34e-04)	Tok/s 61759 (54750)	Loss/tok 3.1400 (3.1573)	LR 1.000e-03
0: TRAIN [3][1420/1938]	Time 0.216 (0.256)	Data 1.09e-04 (6.31e-04)	Tok/s 48088 (54741)	Loss/tok 2.9612 (3.1567)	LR 1.000e-03
0: TRAIN [3][1430/1938]	Time 0.216 (0.256)	Data 1.10e-04 (6.27e-04)	Tok/s 48088 (54761)	Loss/tok 2.9812 (3.1568)	LR 1.000e-03
0: TRAIN [3][1440/1938]	Time 0.216 (0.256)	Data 1.11e-04 (6.24e-04)	Tok/s 48994 (54769)	Loss/tok 2.9687 (3.1571)	LR 1.000e-03
0: TRAIN [3][1450/1938]	Time 0.160 (0.256)	Data 1.07e-04 (6.20e-04)	Tok/s 32879 (54740)	Loss/tok 2.4759 (3.1569)	LR 1.000e-03
0: TRAIN [3][1460/1938]	Time 0.277 (0.256)	Data 1.26e-04 (6.17e-04)	Tok/s 60611 (54761)	Loss/tok 3.1492 (3.1570)	LR 1.000e-03
0: TRAIN [3][1470/1938]	Time 0.337 (0.256)	Data 1.24e-04 (6.13e-04)	Tok/s 68693 (54740)	Loss/tok 3.3455 (3.1568)	LR 1.000e-03
0: TRAIN [3][1480/1938]	Time 0.277 (0.256)	Data 1.35e-04 (6.10e-04)	Tok/s 61507 (54757)	Loss/tok 3.1179 (3.1569)	LR 1.000e-03
0: TRAIN [3][1490/1938]	Time 0.335 (0.256)	Data 1.22e-04 (6.07e-04)	Tok/s 69833 (54774)	Loss/tok 3.2921 (3.1570)	LR 5.000e-04
0: TRAIN [3][1500/1938]	Time 0.216 (0.256)	Data 1.08e-04 (6.04e-04)	Tok/s 46820 (54771)	Loss/tok 3.0569 (3.1571)	LR 5.000e-04
0: TRAIN [3][1510/1938]	Time 0.276 (0.256)	Data 1.05e-04 (6.01e-04)	Tok/s 62135 (54785)	Loss/tok 3.1626 (3.1570)	LR 5.000e-04
0: TRAIN [3][1520/1938]	Time 0.216 (0.256)	Data 1.30e-04 (5.98e-04)	Tok/s 48167 (54803)	Loss/tok 2.9739 (3.1571)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1530/1938]	Time 0.408 (0.256)	Data 1.13e-04 (5.95e-04)	Tok/s 72803 (54776)	Loss/tok 3.5754 (3.1569)	LR 5.000e-04
0: TRAIN [3][1540/1938]	Time 0.217 (0.256)	Data 1.06e-04 (5.92e-04)	Tok/s 47540 (54773)	Loss/tok 3.0060 (3.1568)	LR 5.000e-04
0: TRAIN [3][1550/1938]	Time 0.216 (0.256)	Data 1.08e-04 (5.89e-04)	Tok/s 47269 (54772)	Loss/tok 2.9240 (3.1565)	LR 5.000e-04
0: TRAIN [3][1560/1938]	Time 0.274 (0.256)	Data 1.19e-04 (5.86e-04)	Tok/s 61241 (54753)	Loss/tok 3.0950 (3.1558)	LR 5.000e-04
0: TRAIN [3][1570/1938]	Time 0.409 (0.256)	Data 1.19e-04 (5.83e-04)	Tok/s 72197 (54748)	Loss/tok 3.4742 (3.1557)	LR 5.000e-04
0: TRAIN [3][1580/1938]	Time 0.162 (0.256)	Data 1.52e-04 (5.80e-04)	Tok/s 32779 (54716)	Loss/tok 2.5466 (3.1551)	LR 5.000e-04
0: TRAIN [3][1590/1938]	Time 0.216 (0.256)	Data 1.37e-04 (5.77e-04)	Tok/s 47819 (54731)	Loss/tok 3.0420 (3.1552)	LR 5.000e-04
0: TRAIN [3][1600/1938]	Time 0.216 (0.256)	Data 1.04e-04 (5.74e-04)	Tok/s 48063 (54740)	Loss/tok 2.8794 (3.1549)	LR 5.000e-04
0: TRAIN [3][1610/1938]	Time 0.275 (0.256)	Data 1.11e-04 (5.71e-04)	Tok/s 60786 (54745)	Loss/tok 3.1592 (3.1548)	LR 5.000e-04
0: TRAIN [3][1620/1938]	Time 0.216 (0.256)	Data 1.23e-04 (5.69e-04)	Tok/s 47392 (54734)	Loss/tok 2.8244 (3.1541)	LR 5.000e-04
0: TRAIN [3][1630/1938]	Time 0.275 (0.256)	Data 1.41e-04 (5.66e-04)	Tok/s 61335 (54774)	Loss/tok 3.1297 (3.1546)	LR 5.000e-04
0: TRAIN [3][1640/1938]	Time 0.161 (0.256)	Data 1.19e-04 (5.63e-04)	Tok/s 32616 (54749)	Loss/tok 2.5994 (3.1541)	LR 5.000e-04
0: TRAIN [3][1650/1938]	Time 0.216 (0.256)	Data 1.23e-04 (5.61e-04)	Tok/s 47215 (54749)	Loss/tok 2.9678 (3.1542)	LR 5.000e-04
0: TRAIN [3][1660/1938]	Time 0.217 (0.256)	Data 1.07e-04 (5.58e-04)	Tok/s 47548 (54744)	Loss/tok 2.9349 (3.1538)	LR 5.000e-04
0: TRAIN [3][1670/1938]	Time 0.276 (0.256)	Data 1.24e-04 (5.56e-04)	Tok/s 61192 (54757)	Loss/tok 3.0801 (3.1539)	LR 5.000e-04
0: TRAIN [3][1680/1938]	Time 0.335 (0.256)	Data 1.08e-04 (5.53e-04)	Tok/s 69967 (54722)	Loss/tok 3.2702 (3.1535)	LR 5.000e-04
0: TRAIN [3][1690/1938]	Time 0.215 (0.256)	Data 1.14e-04 (5.51e-04)	Tok/s 48279 (54720)	Loss/tok 3.0017 (3.1535)	LR 5.000e-04
0: TRAIN [3][1700/1938]	Time 0.275 (0.256)	Data 1.27e-04 (5.48e-04)	Tok/s 61548 (54739)	Loss/tok 3.0601 (3.1533)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1710/1938]	Time 0.216 (0.256)	Data 1.10e-04 (5.46e-04)	Tok/s 46872 (54717)	Loss/tok 2.9591 (3.1531)	LR 5.000e-04
0: TRAIN [3][1720/1938]	Time 0.216 (0.256)	Data 1.06e-04 (5.43e-04)	Tok/s 47361 (54715)	Loss/tok 2.8239 (3.1528)	LR 5.000e-04
0: TRAIN [3][1730/1938]	Time 0.276 (0.256)	Data 5.43e-04 (5.41e-04)	Tok/s 61136 (54739)	Loss/tok 3.0959 (3.1530)	LR 5.000e-04
0: TRAIN [3][1740/1938]	Time 0.335 (0.256)	Data 1.18e-04 (5.39e-04)	Tok/s 70012 (54783)	Loss/tok 3.2560 (3.1536)	LR 5.000e-04
0: TRAIN [3][1750/1938]	Time 0.216 (0.256)	Data 1.31e-04 (5.36e-04)	Tok/s 47568 (54762)	Loss/tok 3.0215 (3.1530)	LR 5.000e-04
0: TRAIN [3][1760/1938]	Time 0.336 (0.256)	Data 1.09e-04 (5.34e-04)	Tok/s 68811 (54761)	Loss/tok 3.2163 (3.1525)	LR 5.000e-04
0: TRAIN [3][1770/1938]	Time 0.216 (0.256)	Data 1.10e-04 (5.32e-04)	Tok/s 46662 (54737)	Loss/tok 2.9830 (3.1521)	LR 5.000e-04
0: TRAIN [3][1780/1938]	Time 0.336 (0.256)	Data 1.08e-04 (5.29e-04)	Tok/s 68569 (54719)	Loss/tok 3.4067 (3.1520)	LR 5.000e-04
0: TRAIN [3][1790/1938]	Time 0.275 (0.256)	Data 1.08e-04 (5.27e-04)	Tok/s 61415 (54721)	Loss/tok 3.1352 (3.1520)	LR 5.000e-04
0: TRAIN [3][1800/1938]	Time 0.217 (0.256)	Data 1.07e-04 (5.25e-04)	Tok/s 47890 (54702)	Loss/tok 3.0180 (3.1515)	LR 5.000e-04
0: TRAIN [3][1810/1938]	Time 0.216 (0.256)	Data 1.24e-04 (5.23e-04)	Tok/s 48082 (54727)	Loss/tok 2.9591 (3.1514)	LR 5.000e-04
0: TRAIN [3][1820/1938]	Time 0.276 (0.256)	Data 1.22e-04 (5.21e-04)	Tok/s 60883 (54708)	Loss/tok 3.1275 (3.1509)	LR 5.000e-04
0: TRAIN [3][1830/1938]	Time 0.215 (0.256)	Data 1.14e-04 (5.19e-04)	Tok/s 49468 (54691)	Loss/tok 3.0733 (3.1503)	LR 5.000e-04
0: TRAIN [3][1840/1938]	Time 0.216 (0.256)	Data 1.09e-04 (5.17e-04)	Tok/s 48193 (54660)	Loss/tok 2.8912 (3.1499)	LR 5.000e-04
0: TRAIN [3][1850/1938]	Time 0.216 (0.255)	Data 1.32e-04 (5.14e-04)	Tok/s 46690 (54654)	Loss/tok 2.9275 (3.1495)	LR 5.000e-04
0: TRAIN [3][1860/1938]	Time 0.276 (0.255)	Data 1.18e-04 (5.12e-04)	Tok/s 61627 (54634)	Loss/tok 3.1833 (3.1490)	LR 5.000e-04
0: TRAIN [3][1870/1938]	Time 0.216 (0.255)	Data 1.07e-04 (5.10e-04)	Tok/s 48289 (54639)	Loss/tok 2.9458 (3.1484)	LR 5.000e-04
0: TRAIN [3][1880/1938]	Time 0.218 (0.255)	Data 1.14e-04 (5.08e-04)	Tok/s 47045 (54639)	Loss/tok 2.9695 (3.1483)	LR 5.000e-04
0: TRAIN [3][1890/1938]	Time 0.274 (0.255)	Data 1.09e-04 (5.06e-04)	Tok/s 60763 (54646)	Loss/tok 3.1422 (3.1481)	LR 5.000e-04
0: TRAIN [3][1900/1938]	Time 0.216 (0.256)	Data 1.08e-04 (5.04e-04)	Tok/s 47159 (54671)	Loss/tok 3.0215 (3.1485)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1910/1938]	Time 0.335 (0.256)	Data 1.36e-04 (5.02e-04)	Tok/s 69479 (54671)	Loss/tok 3.3430 (3.1485)	LR 5.000e-04
0: TRAIN [3][1920/1938]	Time 0.336 (0.256)	Data 1.04e-04 (5.00e-04)	Tok/s 69212 (54678)	Loss/tok 3.4882 (3.1487)	LR 5.000e-04
0: TRAIN [3][1930/1938]	Time 0.161 (0.256)	Data 1.14e-04 (4.99e-04)	Tok/s 32776 (54678)	Loss/tok 2.5084 (3.1490)	LR 5.000e-04
:::MLL 1571838634.809 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 524}}
:::MLL 1571838634.809 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.753 (0.753)	Decoder iters 149.0 (149.0)	Tok/s 21838 (21838)
0: Running moses detokenizer
0: BLEU(score=24.205270174489417, counts=[37300, 18727, 10684, 6337], totals=[65520, 62517, 59514, 56515], precisions=[56.92918192918193, 29.955052225794585, 17.95207850253722, 11.212952313545076], bp=1.0, sys_len=65520, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1571838636.798 eval_accuracy: {"value": 24.21, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 535}}
:::MLL 1571838636.798 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 3	Training Loss: 3.1491	Test BLEU: 24.21
0: Performance: Epoch: 3	Training: 437358 Tok/s
0: Finished epoch 3
:::MLL 1571838636.799 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 557}}
0: Closing preprocessed data file
:::MLL 1571838636.799 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 568}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-10-23 01:50:48 PM
RESULT,RNN_TRANSLATOR,,2035,nvidia,2019-10-23 01:16:53 PM
