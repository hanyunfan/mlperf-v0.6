Beginning trial 1 of 2
Gathering sys log on node009
:::MLL 1581973683.525 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1581973683.525 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1581973683.525 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1581973683.526 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1581973683.526 submission_platform: {"value": "1xDSS8440", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1581973683.527 submission_entry: {"value": "{'hardware': 'DSS8440', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'Ethernet 100 Gb/sec (4X EDR)', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.6-2.1.4', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz', 'num_cores': '40', 'num_vcpus': '40', 'accelerator': 'Tesla V100S-PCIE-32GB', 'num_accelerators': '8', 'sys_mem_size': '754 GB', 'sys_storage_type': 'SATA SSD', 'sys_storage_size': '1x 446.6G + 1x 931.5G', 'cpu_accel_interconnect': 'UPI', 'network_card': '', 'num_network_cards': '0', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1581973683.527 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1581973683.527 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1581973685.205 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node node009
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=5038' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=512 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=200217150644465178236 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_200217150644465178236 ./run_and_time.sh
Run vars: id 200217150644465178236 gpus 8 mparams  --master_port=5038
NCCL_SOCKET_NTHREADS=2
NCCL_NSOCKS_PERTHREAD=8
STARTING TIMING RUN AT 2020-02-17 09:08:05 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=512
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=0
+ MATH=amp_fp16
+ [[ 0 -eq 1 ]]
+ LAUNCH_OPT='torch.distributed.launch --nproc_per_node 8  --master_port=5038'
+ echo 'running benchmark'
running benchmark
+ python -m torch.distributed.launch --nproc_per_node 8 --master_port=5038 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 512 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1581973687.854 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1581973687.854 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1581973687.854 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1581973687.854 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1581973687.854 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1581973687.854 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1581973687.854 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1581973687.855 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=8, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=512, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 2046471938
node009:465:465 [0] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:465:465 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation

node009:465:465 [0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:465:465 [0] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:465:465 [0] NCCL INFO NET/IB : No device found.
node009:465:465 [0] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
NCCL version 2.5.6+cuda10.2
node009:471:471 [6] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:472:472 [7] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:472:472 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node009:471:471 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node009:470:470 [5] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:470:470 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation

node009:472:472 [7] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:471:471 [6] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:472:472 [7] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0

node009:471:471 [6] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:472:472 [7] NCCL INFO NET/IB : No device found.
node009:471:471 [6] NCCL INFO NET/IB : No device found.
node009:471:471 [6] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:472:472 [7] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>

node009:470:470 [5] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:470:470 [5] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:470:470 [5] NCCL INFO NET/IB : No device found.
node009:470:470 [5] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:469:469 [4] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:469:469 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation

node009:469:469 [4] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:469:469 [4] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:469:469 [4] NCCL INFO NET/IB : No device found.
node009:469:469 [4] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:467:467 [2] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:467:467 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation

node009:467:467 [2] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:467:467 [2] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:467:467 [2] NCCL INFO NET/IB : No device found.
node009:467:467 [2] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:468:468 [3] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:468:468 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation

node009:468:468 [3] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:468:468 [3] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:468:468 [3] NCCL INFO NET/IB : No device found.
node009:468:468 [3] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:466:466 [1] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:466:466 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation

node009:466:466 [1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:466:466 [1] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:466:466 [1] NCCL INFO NET/IB : No device found.
node009:466:466 [1] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:465:827 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555
node009:471:828 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa
node009:470:829 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa
node009:472:830 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa
node009:469:831 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa
node009:467:832 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555
node009:468:833 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555
node009:466:834 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555
node009:467:832 [2] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:467:832 [2] NCCL INFO include/net.h:19 -> 2
node009:469:831 [4] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:469:831 [4] NCCL INFO include/net.h:19 -> 2
node009:471:828 [6] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:471:828 [6] NCCL INFO include/net.h:19 -> 2
node009:468:833 [3] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:468:833 [3] NCCL INFO include/net.h:19 -> 2
node009:465:827 [0] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:465:827 [0] NCCL INFO include/net.h:19 -> 2
node009:466:834 [1] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:466:834 [1] NCCL INFO include/net.h:19 -> 2
node009:470:829 [5] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:472:830 [7] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:470:829 [5] NCCL INFO include/net.h:19 -> 2
node009:472:830 [7] NCCL INFO include/net.h:19 -> 2
node009:467:832 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:469:831 [4] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:468:833 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:471:828 [6] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:465:827 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:466:834 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:470:829 [5] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:472:830 [7] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:466:834 [1] NCCL INFO Threads per block : 512/640/256
node009:466:834 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:466:834 [1] NCCL INFO Trees [0] 2/-1/-1->1->0|0->1->2/-1/-1 [1] 2/-1/-1->1->0|0->1->2/-1/-1
node009:465:827 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
node009:465:827 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
node009:465:827 [0] NCCL INFO Threads per block : 512/640/256
node009:468:833 [3] NCCL INFO Threads per block : 512/640/256
node009:467:832 [2] NCCL INFO Threads per block : 512/640/256
node009:468:833 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:467:832 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:468:833 [3] NCCL INFO Trees [0] 4/-1/-1->3->2|2->3->4/-1/-1 [1] 4/-1/-1->3->2|2->3->4/-1/-1
node009:467:832 [2] NCCL INFO Trees [0] 3/-1/-1->2->1|1->2->3/-1/-1 [1] 3/-1/-1->2->1|1->2->3/-1/-1
node009:469:831 [4] NCCL INFO Threads per block : 512/640/256
node009:472:830 [7] NCCL INFO Threads per block : 512/640/256
node009:470:829 [5] NCCL INFO Threads per block : 512/640/256
node009:471:828 [6] NCCL INFO Threads per block : 512/640/256
node009:469:831 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:472:830 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:470:829 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:469:831 [4] NCCL INFO Trees [0] 5/-1/-1->4->3|3->4->5/-1/-1 [1] 5/-1/-1->4->3|3->4->5/-1/-1
node009:471:828 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:472:830 [7] NCCL INFO Trees [0] -1/-1/-1->7->6|6->7->-1/-1/-1 [1] -1/-1/-1->7->6|6->7->-1/-1/-1
node009:470:829 [5] NCCL INFO Trees [0] 6/-1/-1->5->4|4->5->6/-1/-1 [1] 6/-1/-1->5->4|4->5->6/-1/-1
node009:465:827 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:471:828 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1
node009:465:827 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 1/-1/-1->0->-1|-1->0->1/-1/-1
node009:468:833 [3] NCCL INFO Ring 00 : 3[48000] -> 4[89000] via direct shared memory
node009:467:832 [2] NCCL INFO Ring 00 : 2[47000] -> 3[48000] via P2P/IPC
node009:466:834 [1] NCCL INFO Ring 00 : 1[11000] -> 2[47000] via P2P/IPC
node009:472:830 [7] NCCL INFO Ring 00 : 7[c2000] -> 0[10000] via direct shared memory
node009:471:828 [6] NCCL INFO Ring 00 : 6[c1000] -> 7[c2000] via P2P/IPC
node009:470:829 [5] NCCL INFO Ring 00 : 5[8a000] -> 6[c1000] via P2P/IPC
node009:469:831 [4] NCCL INFO Ring 00 : 4[89000] -> 5[8a000] via P2P/IPC
node009:465:827 [0] NCCL INFO Ring 00 : 0[10000] -> 1[11000] via P2P/IPC
node009:467:832 [2] NCCL INFO Ring 00 : 2[47000] -> 1[11000] via P2P/IPC
node009:469:831 [4] NCCL INFO Ring 00 : 4[89000] -> 3[48000] via direct shared memory
node009:472:830 [7] NCCL INFO Ring 00 : 7[c2000] -> 6[c1000] via P2P/IPC
node009:470:829 [5] NCCL INFO Ring 00 : 5[8a000] -> 4[89000] via P2P/IPC
node009:471:828 [6] NCCL INFO Ring 00 : 6[c1000] -> 5[8a000] via P2P/IPC
node009:466:834 [1] NCCL INFO Ring 00 : 1[11000] -> 0[10000] via P2P/IPC
node009:472:830 [7] NCCL INFO Ring 01 : 7[c2000] -> 0[10000] via direct shared memory
node009:470:829 [5] NCCL INFO Ring 01 : 5[8a000] -> 6[c1000] via P2P/IPC
node009:471:828 [6] NCCL INFO Ring 01 : 6[c1000] -> 7[c2000] via P2P/IPC
node009:466:834 [1] NCCL INFO Ring 01 : 1[11000] -> 2[47000] via P2P/IPC
node009:468:833 [3] NCCL INFO Ring 00 : 3[48000] -> 2[47000] via P2P/IPC
node009:467:832 [2] NCCL INFO Ring 01 : 2[47000] -> 3[48000] via P2P/IPC
node009:465:827 [0] NCCL INFO Ring 01 : 0[10000] -> 1[11000] via P2P/IPC
node009:468:833 [3] NCCL INFO Ring 01 : 3[48000] -> 4[89000] via direct shared memory
node009:471:828 [6] NCCL INFO Ring 01 : 6[c1000] -> 5[8a000] via P2P/IPC
node009:472:830 [7] NCCL INFO Ring 01 : 7[c2000] -> 6[c1000] via P2P/IPC
node009:466:834 [1] NCCL INFO Ring 01 : 1[11000] -> 0[10000] via P2P/IPC
node009:467:832 [2] NCCL INFO Ring 01 : 2[47000] -> 1[11000] via P2P/IPC
node009:469:831 [4] NCCL INFO Ring 01 : 4[89000] -> 5[8a000] via P2P/IPC
node009:470:829 [5] NCCL INFO Ring 01 : 5[8a000] -> 4[89000] via P2P/IPC
node009:469:831 [4] NCCL INFO Ring 01 : 4[89000] -> 3[48000] via direct shared memory
node009:472:830 [7] NCCL INFO comm 0x7ffe98007570 rank 7 nranks 8 cudaDev 7 busId c2000 - Init COMPLETE
node009:465:827 [0] NCCL INFO comm 0x7ffe50007570 rank 0 nranks 8 cudaDev 0 busId 10000 - Init COMPLETE
node009:465:465 [0] NCCL INFO Launch mode Parallel
node009:468:833 [3] NCCL INFO Ring 01 : 3[48000] -> 2[47000] via P2P/IPC
node009:466:834 [1] NCCL INFO comm 0x7fff44007570 rank 1 nranks 8 cudaDev 1 busId 11000 - Init COMPLETE
0: Worker 0 is using worker seed: 3731136007
0: Building vocabulary from /data/vocab.bpe.32000
node009:471:828 [6] NCCL INFO comm 0x7ffe90007570 rank 6 nranks 8 cudaDev 6 busId c1000 - Init COMPLETE
node009:470:829 [5] NCCL INFO comm 0x7fff64007570 rank 5 nranks 8 cudaDev 5 busId 8a000 - Init COMPLETE
0: Size of vocabulary: 32320
node009:469:831 [4] NCCL INFO comm 0x7ffe98007570 rank 4 nranks 8 cudaDev 4 busId 89000 - Init COMPLETE
node009:467:832 [2] NCCL INFO comm 0x7ffe88007570 rank 2 nranks 8 cudaDev 2 busId 47000 - Init COMPLETE
node009:468:833 [3] NCCL INFO comm 0x7fff6c007570 rank 3 nranks 8 cudaDev 3 busId 48000 - Init COMPLETE
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1581973697.118 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1581973698.616 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 407}}
:::MLL 1581973698.617 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 409}}
:::MLL 1581973698.617 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 414}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1581973699.647 global_batch_size: {"value": 4096, "metadata": {"file": "train.py", "lineno": 459}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 8, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 8
:::MLL 1581973699.649 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1581973699.649 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1581973699.649 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1581973699.649 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1581973699.650 opt_learning_rate_decay_steps: {"value": 8, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1581973699.650 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1581973699.650 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1581973699.665 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1581973699.666 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 515}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 816466201
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/968]	Time 1.340 (1.340)	Data 9.05e-01 (9.05e-01)	Tok/s 24994 (24994)	Loss/tok 10.6788 (10.6788)	LR 2.000e-05
0: TRAIN [0][10/968]	Time 0.430 (0.482)	Data 7.46e-04 (8.26e-02)	Tok/s 78224 (68911)	Loss/tok 9.7435 (10.2088)	LR 2.518e-05
0: TRAIN [0][20/968]	Time 0.206 (0.451)	Data 2.03e-04 (4.33e-02)	Tok/s 49912 (69112)	Loss/tok 9.0436 (9.8709)	LR 3.170e-05
0: TRAIN [0][30/968]	Time 0.315 (0.441)	Data 1.58e-04 (2.94e-02)	Tok/s 65783 (70772)	Loss/tok 8.9660 (9.6463)	LR 3.991e-05
0: TRAIN [0][40/968]	Time 0.312 (0.431)	Data 1.57e-04 (2.23e-02)	Tok/s 67051 (71137)	Loss/tok 8.6932 (9.4748)	LR 5.024e-05
0: TRAIN [0][50/968]	Time 0.319 (0.428)	Data 1.73e-04 (1.80e-02)	Tok/s 64707 (71951)	Loss/tok 8.5078 (9.3253)	LR 6.325e-05
0: TRAIN [0][60/968]	Time 0.428 (0.418)	Data 1.66e-04 (1.50e-02)	Tok/s 78284 (71417)	Loss/tok 8.4386 (9.2087)	LR 7.962e-05
0: TRAIN [0][70/968]	Time 0.315 (0.407)	Data 1.63e-04 (1.30e-02)	Tok/s 65765 (70793)	Loss/tok 8.1112 (9.0956)	LR 1.002e-04
0: TRAIN [0][80/968]	Time 0.428 (0.405)	Data 1.59e-04 (1.14e-02)	Tok/s 78735 (71080)	Loss/tok 8.1326 (8.9742)	LR 1.262e-04
0: TRAIN [0][90/968]	Time 0.316 (0.402)	Data 1.59e-04 (1.01e-02)	Tok/s 65784 (70952)	Loss/tok 7.8494 (8.8749)	LR 1.589e-04
0: TRAIN [0][100/968]	Time 0.546 (0.399)	Data 1.90e-04 (9.16e-03)	Tok/s 85829 (70964)	Loss/tok 8.1004 (8.7886)	LR 2.000e-04
0: TRAIN [0][110/968]	Time 0.429 (0.402)	Data 2.06e-04 (8.35e-03)	Tok/s 77873 (71421)	Loss/tok 7.9472 (8.7024)	LR 2.518e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][120/968]	Time 0.428 (0.400)	Data 1.77e-04 (7.67e-03)	Tok/s 78560 (71229)	Loss/tok 7.9122 (8.6764)	LR 3.098e-04
0: TRAIN [0][130/968]	Time 0.317 (0.398)	Data 1.48e-04 (7.10e-03)	Tok/s 65605 (71194)	Loss/tok 7.7627 (8.6377)	LR 3.900e-04
0: TRAIN [0][140/968]	Time 0.684 (0.400)	Data 1.81e-04 (6.61e-03)	Tok/s 87760 (71428)	Loss/tok 8.0903 (8.5823)	LR 4.909e-04
0: TRAIN [0][150/968]	Time 0.318 (0.397)	Data 1.88e-04 (6.18e-03)	Tok/s 64473 (71308)	Loss/tok 7.6028 (8.5339)	LR 6.181e-04
0: TRAIN [0][160/968]	Time 0.319 (0.394)	Data 1.91e-04 (5.81e-03)	Tok/s 65027 (71035)	Loss/tok 7.5187 (8.4888)	LR 7.781e-04
0: TRAIN [0][170/968]	Time 0.549 (0.397)	Data 1.57e-04 (5.48e-03)	Tok/s 84938 (71462)	Loss/tok 7.5779 (8.4275)	LR 9.796e-04
0: TRAIN [0][180/968]	Time 0.428 (0.397)	Data 1.82e-04 (5.19e-03)	Tok/s 79293 (71610)	Loss/tok 7.3588 (8.3718)	LR 1.233e-03
0: TRAIN [0][190/968]	Time 0.206 (0.394)	Data 1.51e-04 (4.92e-03)	Tok/s 50794 (71276)	Loss/tok 6.4871 (8.3204)	LR 1.552e-03
0: TRAIN [0][200/968]	Time 0.428 (0.395)	Data 1.84e-04 (4.69e-03)	Tok/s 78826 (71477)	Loss/tok 6.9266 (8.2533)	LR 1.954e-03
0: TRAIN [0][210/968]	Time 0.210 (0.393)	Data 1.60e-04 (4.47e-03)	Tok/s 49363 (71324)	Loss/tok 5.9017 (8.1946)	LR 2.000e-03
0: TRAIN [0][220/968]	Time 0.428 (0.397)	Data 1.70e-04 (4.28e-03)	Tok/s 78938 (71653)	Loss/tok 6.6479 (8.1159)	LR 2.000e-03
0: TRAIN [0][230/968]	Time 0.428 (0.399)	Data 1.56e-04 (4.10e-03)	Tok/s 77886 (71870)	Loss/tok 6.5335 (8.0391)	LR 2.000e-03
0: TRAIN [0][240/968]	Time 0.318 (0.398)	Data 1.46e-04 (3.94e-03)	Tok/s 64641 (71841)	Loss/tok 6.1601 (7.9757)	LR 2.000e-03
0: TRAIN [0][250/968]	Time 0.549 (0.398)	Data 1.63e-04 (3.79e-03)	Tok/s 83946 (71824)	Loss/tok 6.4114 (7.9087)	LR 2.000e-03
0: TRAIN [0][260/968]	Time 0.316 (0.399)	Data 1.56e-04 (3.65e-03)	Tok/s 65666 (71962)	Loss/tok 5.9109 (7.8353)	LR 2.000e-03
0: TRAIN [0][270/968]	Time 0.210 (0.399)	Data 1.67e-04 (3.52e-03)	Tok/s 50265 (71930)	Loss/tok 4.9478 (7.7691)	LR 2.000e-03
0: TRAIN [0][280/968]	Time 0.317 (0.399)	Data 1.66e-04 (3.40e-03)	Tok/s 65233 (71992)	Loss/tok 5.5476 (7.7018)	LR 2.000e-03
0: TRAIN [0][290/968]	Time 0.549 (0.400)	Data 1.96e-04 (3.29e-03)	Tok/s 84005 (72076)	Loss/tok 5.9742 (7.6347)	LR 2.000e-03
0: TRAIN [0][300/968]	Time 0.429 (0.398)	Data 1.73e-04 (3.19e-03)	Tok/s 78146 (71987)	Loss/tok 5.8492 (7.5784)	LR 2.000e-03
0: TRAIN [0][310/968]	Time 0.550 (0.398)	Data 1.70e-04 (3.09e-03)	Tok/s 84291 (72080)	Loss/tok 5.8115 (7.5126)	LR 2.000e-03
0: TRAIN [0][320/968]	Time 0.320 (0.398)	Data 1.89e-04 (3.00e-03)	Tok/s 64021 (72090)	Loss/tok 5.1552 (7.4492)	LR 2.000e-03
0: TRAIN [0][330/968]	Time 0.314 (0.397)	Data 1.62e-04 (2.91e-03)	Tok/s 66146 (71967)	Loss/tok 5.0054 (7.3963)	LR 2.000e-03
0: TRAIN [0][340/968]	Time 0.318 (0.396)	Data 1.63e-04 (2.83e-03)	Tok/s 64202 (71903)	Loss/tok 4.8819 (7.3393)	LR 2.000e-03
0: TRAIN [0][350/968]	Time 0.318 (0.396)	Data 3.55e-04 (2.76e-03)	Tok/s 64746 (71891)	Loss/tok 4.9577 (7.2782)	LR 2.000e-03
0: TRAIN [0][360/968]	Time 0.317 (0.396)	Data 1.65e-04 (2.69e-03)	Tok/s 65613 (71851)	Loss/tok 4.7182 (7.2188)	LR 2.000e-03
0: TRAIN [0][370/968]	Time 0.315 (0.398)	Data 1.53e-04 (2.62e-03)	Tok/s 65521 (72003)	Loss/tok 4.5283 (7.1493)	LR 2.000e-03
0: TRAIN [0][380/968]	Time 0.552 (0.398)	Data 1.91e-04 (2.55e-03)	Tok/s 84369 (72003)	Loss/tok 5.2679 (7.0913)	LR 2.000e-03
0: TRAIN [0][390/968]	Time 0.318 (0.398)	Data 1.84e-04 (2.49e-03)	Tok/s 64699 (72003)	Loss/tok 4.5736 (7.0365)	LR 2.000e-03
0: TRAIN [0][400/968]	Time 0.688 (0.398)	Data 1.66e-04 (2.44e-03)	Tok/s 86812 (71982)	Loss/tok 5.2272 (6.9822)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][410/968]	Time 0.208 (0.399)	Data 1.75e-04 (2.38e-03)	Tok/s 50617 (72025)	Loss/tok 3.6905 (6.9214)	LR 2.000e-03
0: TRAIN [0][420/968]	Time 0.429 (0.399)	Data 1.93e-04 (2.33e-03)	Tok/s 78209 (72066)	Loss/tok 4.6483 (6.8648)	LR 2.000e-03
0: TRAIN [0][430/968]	Time 0.429 (0.401)	Data 1.48e-04 (2.28e-03)	Tok/s 79484 (72162)	Loss/tok 4.5154 (6.8037)	LR 2.000e-03
0: TRAIN [0][440/968]	Time 0.428 (0.400)	Data 1.57e-04 (2.23e-03)	Tok/s 77827 (72153)	Loss/tok 4.5638 (6.7533)	LR 2.000e-03
0: TRAIN [0][450/968]	Time 0.428 (0.400)	Data 1.68e-04 (2.18e-03)	Tok/s 78353 (72076)	Loss/tok 4.5375 (6.7077)	LR 2.000e-03
0: TRAIN [0][460/968]	Time 0.432 (0.400)	Data 1.77e-04 (2.14e-03)	Tok/s 77229 (72147)	Loss/tok 4.4607 (6.6541)	LR 2.000e-03
0: TRAIN [0][470/968]	Time 0.427 (0.401)	Data 1.97e-04 (2.10e-03)	Tok/s 78344 (72279)	Loss/tok 4.3824 (6.5981)	LR 2.000e-03
0: TRAIN [0][480/968]	Time 0.319 (0.401)	Data 1.53e-04 (2.06e-03)	Tok/s 64334 (72246)	Loss/tok 4.0736 (6.5544)	LR 2.000e-03
0: TRAIN [0][490/968]	Time 0.210 (0.399)	Data 1.70e-04 (2.02e-03)	Tok/s 49949 (72065)	Loss/tok 3.2705 (6.5195)	LR 2.000e-03
0: TRAIN [0][500/968]	Time 0.682 (0.399)	Data 1.67e-04 (1.98e-03)	Tok/s 88101 (72055)	Loss/tok 4.7358 (6.4766)	LR 2.000e-03
0: TRAIN [0][510/968]	Time 0.314 (0.398)	Data 1.56e-04 (1.95e-03)	Tok/s 65530 (72009)	Loss/tok 3.9205 (6.4373)	LR 2.000e-03
0: TRAIN [0][520/968]	Time 0.313 (0.398)	Data 1.73e-04 (1.91e-03)	Tok/s 66083 (71977)	Loss/tok 3.9556 (6.3976)	LR 2.000e-03
0: TRAIN [0][530/968]	Time 0.319 (0.397)	Data 1.51e-04 (1.88e-03)	Tok/s 64911 (71962)	Loss/tok 3.8972 (6.3573)	LR 2.000e-03
0: TRAIN [0][540/968]	Time 0.318 (0.397)	Data 1.50e-04 (1.85e-03)	Tok/s 65625 (71910)	Loss/tok 3.9215 (6.3196)	LR 2.000e-03
0: TRAIN [0][550/968]	Time 0.430 (0.396)	Data 1.50e-04 (1.82e-03)	Tok/s 77852 (71811)	Loss/tok 4.1613 (6.2872)	LR 2.000e-03
0: TRAIN [0][560/968]	Time 0.318 (0.395)	Data 1.54e-04 (1.79e-03)	Tok/s 65201 (71778)	Loss/tok 3.8411 (6.2511)	LR 2.000e-03
0: TRAIN [0][570/968]	Time 0.209 (0.396)	Data 1.72e-04 (1.76e-03)	Tok/s 49886 (71769)	Loss/tok 3.1486 (6.2124)	LR 2.000e-03
0: TRAIN [0][580/968]	Time 0.318 (0.395)	Data 1.95e-04 (1.73e-03)	Tok/s 64266 (71713)	Loss/tok 3.7955 (6.1786)	LR 2.000e-03
0: TRAIN [0][590/968]	Time 0.318 (0.395)	Data 1.71e-04 (1.71e-03)	Tok/s 65112 (71703)	Loss/tok 3.7183 (6.1449)	LR 2.000e-03
0: TRAIN [0][600/968]	Time 0.316 (0.396)	Data 1.65e-04 (1.68e-03)	Tok/s 65263 (71739)	Loss/tok 3.6342 (6.1078)	LR 2.000e-03
0: TRAIN [0][610/968]	Time 0.317 (0.396)	Data 1.47e-04 (1.66e-03)	Tok/s 66508 (71782)	Loss/tok 3.7103 (6.0726)	LR 2.000e-03
0: TRAIN [0][620/968]	Time 0.319 (0.395)	Data 1.49e-04 (1.63e-03)	Tok/s 64290 (71773)	Loss/tok 3.7459 (6.0410)	LR 2.000e-03
0: TRAIN [0][630/968]	Time 0.207 (0.395)	Data 2.13e-04 (1.61e-03)	Tok/s 51314 (71745)	Loss/tok 3.2265 (6.0102)	LR 2.000e-03
0: TRAIN [0][640/968]	Time 0.212 (0.395)	Data 1.58e-04 (1.59e-03)	Tok/s 49666 (71724)	Loss/tok 3.1939 (5.9790)	LR 2.000e-03
0: TRAIN [0][650/968]	Time 0.318 (0.394)	Data 1.81e-04 (1.57e-03)	Tok/s 64939 (71659)	Loss/tok 3.7111 (5.9522)	LR 2.000e-03
0: TRAIN [0][660/968]	Time 0.681 (0.394)	Data 1.66e-04 (1.54e-03)	Tok/s 87625 (71615)	Loss/tok 4.3838 (5.9233)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][670/968]	Time 0.319 (0.394)	Data 1.65e-04 (1.52e-03)	Tok/s 64669 (71619)	Loss/tok 3.6953 (5.8944)	LR 2.000e-03
0: TRAIN [0][680/968]	Time 0.550 (0.395)	Data 1.97e-04 (1.50e-03)	Tok/s 85814 (71732)	Loss/tok 4.0894 (5.8592)	LR 2.000e-03
0: TRAIN [0][690/968]	Time 0.321 (0.395)	Data 1.53e-04 (1.48e-03)	Tok/s 64492 (71710)	Loss/tok 3.6217 (5.8329)	LR 2.000e-03
0: TRAIN [0][700/968]	Time 0.318 (0.394)	Data 1.61e-04 (1.47e-03)	Tok/s 65046 (71653)	Loss/tok 3.6328 (5.8080)	LR 2.000e-03
0: TRAIN [0][710/968]	Time 0.433 (0.393)	Data 1.89e-04 (1.45e-03)	Tok/s 78000 (71579)	Loss/tok 3.9228 (5.7847)	LR 2.000e-03
0: TRAIN [0][720/968]	Time 0.323 (0.393)	Data 1.81e-04 (1.43e-03)	Tok/s 63918 (71520)	Loss/tok 3.6092 (5.7618)	LR 2.000e-03
0: TRAIN [0][730/968]	Time 0.429 (0.393)	Data 1.59e-04 (1.41e-03)	Tok/s 78091 (71587)	Loss/tok 3.8833 (5.7326)	LR 2.000e-03
0: TRAIN [0][740/968]	Time 0.210 (0.393)	Data 1.54e-04 (1.40e-03)	Tok/s 49758 (71580)	Loss/tok 3.0917 (5.7082)	LR 2.000e-03
0: TRAIN [0][750/968]	Time 0.685 (0.393)	Data 1.69e-04 (1.38e-03)	Tok/s 88101 (71605)	Loss/tok 4.1588 (5.6821)	LR 2.000e-03
0: TRAIN [0][760/968]	Time 0.317 (0.393)	Data 1.52e-04 (1.36e-03)	Tok/s 65742 (71548)	Loss/tok 3.6309 (5.6612)	LR 2.000e-03
0: TRAIN [0][770/968]	Time 0.546 (0.393)	Data 1.69e-04 (1.35e-03)	Tok/s 85463 (71571)	Loss/tok 4.0733 (5.6363)	LR 2.000e-03
0: TRAIN [0][780/968]	Time 0.430 (0.392)	Data 1.83e-04 (1.33e-03)	Tok/s 78576 (71508)	Loss/tok 3.8919 (5.6162)	LR 2.000e-03
0: TRAIN [0][790/968]	Time 0.208 (0.392)	Data 1.73e-04 (1.32e-03)	Tok/s 50632 (71485)	Loss/tok 3.0060 (5.5950)	LR 2.000e-03
0: TRAIN [0][800/968]	Time 0.318 (0.392)	Data 1.78e-04 (1.30e-03)	Tok/s 65240 (71460)	Loss/tok 3.5271 (5.5739)	LR 2.000e-03
0: TRAIN [0][810/968]	Time 0.319 (0.391)	Data 1.50e-04 (1.29e-03)	Tok/s 65525 (71425)	Loss/tok 3.5419 (5.5542)	LR 2.000e-03
0: TRAIN [0][820/968]	Time 0.550 (0.391)	Data 1.49e-04 (1.28e-03)	Tok/s 84313 (71431)	Loss/tok 4.1128 (5.5317)	LR 2.000e-03
0: TRAIN [0][830/968]	Time 0.321 (0.391)	Data 1.55e-04 (1.26e-03)	Tok/s 63279 (71396)	Loss/tok 3.5864 (5.5125)	LR 2.000e-03
0: TRAIN [0][840/968]	Time 0.550 (0.392)	Data 1.82e-04 (1.25e-03)	Tok/s 85203 (71492)	Loss/tok 3.9534 (5.4869)	LR 2.000e-03
0: TRAIN [0][850/968]	Time 0.550 (0.393)	Data 1.55e-04 (1.24e-03)	Tok/s 84850 (71524)	Loss/tok 3.9119 (5.4648)	LR 2.000e-03
0: TRAIN [0][860/968]	Time 0.208 (0.392)	Data 1.77e-04 (1.22e-03)	Tok/s 50870 (71453)	Loss/tok 3.0114 (5.4479)	LR 2.000e-03
0: TRAIN [0][870/968]	Time 0.549 (0.392)	Data 1.54e-04 (1.21e-03)	Tok/s 85000 (71465)	Loss/tok 3.9925 (5.4284)	LR 2.000e-03
0: TRAIN [0][880/968]	Time 0.429 (0.392)	Data 1.57e-04 (1.20e-03)	Tok/s 77825 (71467)	Loss/tok 3.7406 (5.4092)	LR 2.000e-03
0: TRAIN [0][890/968]	Time 0.430 (0.392)	Data 1.72e-04 (1.19e-03)	Tok/s 78355 (71485)	Loss/tok 3.7598 (5.3893)	LR 2.000e-03
0: TRAIN [0][900/968]	Time 0.431 (0.393)	Data 1.75e-04 (1.18e-03)	Tok/s 77347 (71506)	Loss/tok 3.7179 (5.3699)	LR 2.000e-03
0: TRAIN [0][910/968]	Time 0.547 (0.393)	Data 1.78e-04 (1.17e-03)	Tok/s 85027 (71543)	Loss/tok 3.8782 (5.3502)	LR 2.000e-03
0: TRAIN [0][920/968]	Time 0.546 (0.394)	Data 1.63e-04 (1.16e-03)	Tok/s 85049 (71593)	Loss/tok 3.9668 (5.3300)	LR 2.000e-03
0: TRAIN [0][930/968]	Time 0.319 (0.393)	Data 1.71e-04 (1.15e-03)	Tok/s 64193 (71537)	Loss/tok 3.4387 (5.3151)	LR 2.000e-03
0: TRAIN [0][940/968]	Time 0.433 (0.393)	Data 1.95e-04 (1.13e-03)	Tok/s 76771 (71549)	Loss/tok 3.7321 (5.2972)	LR 2.000e-03
0: TRAIN [0][950/968]	Time 0.428 (0.393)	Data 1.73e-04 (1.12e-03)	Tok/s 78457 (71526)	Loss/tok 3.7282 (5.2815)	LR 2.000e-03
0: TRAIN [0][960/968]	Time 0.319 (0.393)	Data 1.78e-04 (1.11e-03)	Tok/s 64579 (71548)	Loss/tok 3.4474 (5.2642)	LR 2.000e-03
:::MLL 1581974081.195 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 525}}
:::MLL 1581974081.196 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.709 (0.709)	Decoder iters 149.0 (149.0)	Tok/s 22133 (22133)
0: Running moses detokenizer
0: BLEU(score=18.23658475630812, counts=[33016, 14498, 7471, 4010], totals=[63568, 60565, 57562, 54564], precisions=[51.93808205386358, 23.937917939403945, 12.979048677947256, 7.349167949563815], bp=0.9827208728457162, sys_len=63568, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1581974083.239 eval_accuracy: {"value": 18.24, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 536}}
:::MLL 1581974083.240 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 0	Training Loss: 5.2534	Test BLEU: 18.24
0: Performance: Epoch: 0	Training: 572300 Tok/s
0: Finished epoch 0
:::MLL 1581974083.240 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 558}}
:::MLL 1581974083.240 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1581974083.241 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 515}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 3368759426
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][0/968]	Time 1.274 (1.274)	Data 7.09e-01 (7.09e-01)	Tok/s 36653 (36653)	Loss/tok 3.8233 (3.8233)	LR 2.000e-03
0: TRAIN [1][10/968]	Time 0.431 (0.415)	Data 1.87e-04 (6.46e-02)	Tok/s 78002 (63758)	Loss/tok 3.7030 (3.5300)	LR 2.000e-03
0: TRAIN [1][20/968]	Time 0.320 (0.391)	Data 1.48e-04 (3.39e-02)	Tok/s 63917 (66449)	Loss/tok 3.3456 (3.5388)	LR 2.000e-03
0: TRAIN [1][30/968]	Time 0.209 (0.395)	Data 1.76e-04 (2.30e-02)	Tok/s 50545 (67707)	Loss/tok 2.8835 (3.5934)	LR 2.000e-03
0: TRAIN [1][40/968]	Time 0.317 (0.396)	Data 1.72e-04 (1.74e-02)	Tok/s 64879 (68715)	Loss/tok 3.3442 (3.6084)	LR 2.000e-03
0: TRAIN [1][50/968]	Time 0.314 (0.385)	Data 1.86e-04 (1.41e-02)	Tok/s 66132 (68362)	Loss/tok 3.2892 (3.5867)	LR 2.000e-03
0: TRAIN [1][60/968]	Time 0.317 (0.382)	Data 1.66e-04 (1.18e-02)	Tok/s 64476 (68433)	Loss/tok 3.4637 (3.5852)	LR 2.000e-03
0: TRAIN [1][70/968]	Time 0.429 (0.385)	Data 1.81e-04 (1.02e-02)	Tok/s 78227 (69242)	Loss/tok 3.5486 (3.5922)	LR 2.000e-03
0: TRAIN [1][80/968]	Time 0.429 (0.384)	Data 1.72e-04 (8.92e-03)	Tok/s 78220 (69369)	Loss/tok 3.6220 (3.5892)	LR 2.000e-03
0: TRAIN [1][90/968]	Time 0.548 (0.388)	Data 1.90e-04 (7.96e-03)	Tok/s 85567 (69961)	Loss/tok 3.7798 (3.5971)	LR 2.000e-03
0: TRAIN [1][100/968]	Time 0.689 (0.394)	Data 1.85e-04 (7.19e-03)	Tok/s 86931 (70601)	Loss/tok 4.0004 (3.6125)	LR 2.000e-03
0: TRAIN [1][110/968]	Time 0.426 (0.397)	Data 1.60e-04 (6.56e-03)	Tok/s 79124 (70922)	Loss/tok 3.5910 (3.6209)	LR 2.000e-03
0: TRAIN [1][120/968]	Time 0.548 (0.392)	Data 1.60e-04 (6.03e-03)	Tok/s 85159 (70490)	Loss/tok 3.9011 (3.6156)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][130/968]	Time 0.430 (0.393)	Data 1.53e-04 (5.58e-03)	Tok/s 78071 (70790)	Loss/tok 3.6258 (3.6148)	LR 2.000e-03
0: TRAIN [1][140/968]	Time 0.316 (0.394)	Data 1.53e-04 (5.19e-03)	Tok/s 65651 (70629)	Loss/tok 3.3719 (3.6242)	LR 2.000e-03
0: TRAIN [1][150/968]	Time 0.686 (0.399)	Data 1.90e-04 (4.86e-03)	Tok/s 87149 (71026)	Loss/tok 4.0394 (3.6299)	LR 2.000e-03
0: TRAIN [1][160/968]	Time 0.317 (0.396)	Data 1.63e-04 (4.57e-03)	Tok/s 64701 (70800)	Loss/tok 3.3244 (3.6244)	LR 2.000e-03
0: TRAIN [1][170/968]	Time 0.316 (0.394)	Data 1.79e-04 (4.31e-03)	Tok/s 64859 (70840)	Loss/tok 3.3539 (3.6188)	LR 2.000e-03
0: TRAIN [1][180/968]	Time 0.431 (0.398)	Data 1.67e-04 (4.08e-03)	Tok/s 77460 (71100)	Loss/tok 3.5774 (3.6291)	LR 2.000e-03
0: TRAIN [1][190/968]	Time 0.687 (0.396)	Data 1.50e-04 (3.88e-03)	Tok/s 86868 (70862)	Loss/tok 4.0707 (3.6266)	LR 2.000e-03
0: TRAIN [1][200/968]	Time 0.430 (0.395)	Data 1.62e-04 (3.69e-03)	Tok/s 78237 (70928)	Loss/tok 3.5429 (3.6236)	LR 2.000e-03
0: TRAIN [1][210/968]	Time 0.316 (0.395)	Data 1.55e-04 (3.53e-03)	Tok/s 65067 (70953)	Loss/tok 3.3401 (3.6188)	LR 2.000e-03
0: TRAIN [1][220/968]	Time 0.549 (0.397)	Data 1.51e-04 (3.37e-03)	Tok/s 84821 (71120)	Loss/tok 3.8589 (3.6217)	LR 2.000e-03
0: TRAIN [1][230/968]	Time 0.317 (0.396)	Data 1.51e-04 (3.23e-03)	Tok/s 65172 (71109)	Loss/tok 3.3420 (3.6178)	LR 2.000e-03
0: TRAIN [1][240/968]	Time 0.427 (0.396)	Data 1.57e-04 (3.11e-03)	Tok/s 78639 (71198)	Loss/tok 3.5631 (3.6148)	LR 2.000e-03
0: TRAIN [1][250/968]	Time 0.316 (0.395)	Data 1.69e-04 (2.99e-03)	Tok/s 64824 (71176)	Loss/tok 3.2870 (3.6104)	LR 2.000e-03
0: TRAIN [1][260/968]	Time 0.320 (0.392)	Data 1.54e-04 (2.88e-03)	Tok/s 64919 (70887)	Loss/tok 3.3109 (3.6042)	LR 2.000e-03
0: TRAIN [1][270/968]	Time 0.318 (0.392)	Data 1.61e-04 (2.78e-03)	Tok/s 65117 (70955)	Loss/tok 3.3429 (3.6033)	LR 2.000e-03
0: TRAIN [1][280/968]	Time 0.428 (0.392)	Data 1.50e-04 (2.69e-03)	Tok/s 78909 (70947)	Loss/tok 3.5770 (3.6025)	LR 2.000e-03
0: TRAIN [1][290/968]	Time 0.431 (0.393)	Data 1.44e-04 (2.60e-03)	Tok/s 78513 (71022)	Loss/tok 3.5297 (3.6022)	LR 2.000e-03
0: TRAIN [1][300/968]	Time 0.319 (0.392)	Data 1.61e-04 (2.52e-03)	Tok/s 63305 (71012)	Loss/tok 3.3453 (3.5986)	LR 2.000e-03
0: TRAIN [1][310/968]	Time 0.430 (0.392)	Data 1.51e-04 (2.45e-03)	Tok/s 77779 (71065)	Loss/tok 3.6099 (3.5963)	LR 2.000e-03
0: TRAIN [1][320/968]	Time 0.429 (0.394)	Data 1.77e-04 (2.37e-03)	Tok/s 77881 (71165)	Loss/tok 3.5972 (3.5984)	LR 2.000e-03
0: TRAIN [1][330/968]	Time 0.211 (0.394)	Data 1.45e-04 (2.31e-03)	Tok/s 50285 (71212)	Loss/tok 2.8447 (3.5964)	LR 2.000e-03
0: TRAIN [1][340/968]	Time 0.546 (0.394)	Data 1.67e-04 (2.24e-03)	Tok/s 85188 (71213)	Loss/tok 3.7190 (3.5966)	LR 2.000e-03
0: TRAIN [1][350/968]	Time 0.430 (0.393)	Data 1.47e-04 (2.18e-03)	Tok/s 78485 (71142)	Loss/tok 3.4998 (3.5917)	LR 2.000e-03
0: TRAIN [1][360/968]	Time 0.318 (0.393)	Data 1.39e-04 (2.13e-03)	Tok/s 64826 (71143)	Loss/tok 3.2132 (3.5892)	LR 2.000e-03
0: TRAIN [1][370/968]	Time 0.315 (0.393)	Data 1.48e-04 (2.08e-03)	Tok/s 65317 (71188)	Loss/tok 3.4084 (3.5886)	LR 2.000e-03
0: TRAIN [1][380/968]	Time 0.319 (0.392)	Data 1.63e-04 (2.03e-03)	Tok/s 65216 (71108)	Loss/tok 3.2650 (3.5845)	LR 2.000e-03
0: TRAIN [1][390/968]	Time 0.430 (0.392)	Data 1.46e-04 (1.98e-03)	Tok/s 78182 (71130)	Loss/tok 3.5922 (3.5826)	LR 2.000e-03
0: TRAIN [1][400/968]	Time 0.318 (0.391)	Data 1.48e-04 (1.93e-03)	Tok/s 65457 (71128)	Loss/tok 3.2987 (3.5802)	LR 2.000e-03
0: TRAIN [1][410/968]	Time 0.548 (0.390)	Data 1.75e-04 (1.89e-03)	Tok/s 85979 (70991)	Loss/tok 3.7557 (3.5764)	LR 2.000e-03
0: TRAIN [1][420/968]	Time 0.428 (0.390)	Data 1.65e-04 (1.85e-03)	Tok/s 78491 (71069)	Loss/tok 3.4939 (3.5745)	LR 2.000e-03
0: TRAIN [1][430/968]	Time 0.319 (0.390)	Data 1.49e-04 (1.81e-03)	Tok/s 65088 (71002)	Loss/tok 3.2621 (3.5724)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][440/968]	Time 0.431 (0.390)	Data 1.70e-04 (1.77e-03)	Tok/s 77080 (71092)	Loss/tok 3.6073 (3.5718)	LR 2.000e-03
0: TRAIN [1][450/968]	Time 0.317 (0.391)	Data 1.73e-04 (1.74e-03)	Tok/s 65322 (71135)	Loss/tok 3.2691 (3.5714)	LR 2.000e-03
0: TRAIN [1][460/968]	Time 0.429 (0.391)	Data 1.52e-04 (1.70e-03)	Tok/s 78403 (71175)	Loss/tok 3.4819 (3.5696)	LR 2.000e-03
0: TRAIN [1][470/968]	Time 0.317 (0.391)	Data 1.63e-04 (1.67e-03)	Tok/s 64216 (71132)	Loss/tok 3.3315 (3.5690)	LR 2.000e-03
0: TRAIN [1][480/968]	Time 0.427 (0.391)	Data 1.80e-04 (1.64e-03)	Tok/s 78525 (71171)	Loss/tok 3.5336 (3.5667)	LR 2.000e-03
0: TRAIN [1][490/968]	Time 0.320 (0.390)	Data 1.58e-04 (1.61e-03)	Tok/s 65870 (71128)	Loss/tok 3.2759 (3.5638)	LR 2.000e-03
0: TRAIN [1][500/968]	Time 0.319 (0.390)	Data 1.49e-04 (1.58e-03)	Tok/s 63266 (71082)	Loss/tok 3.3503 (3.5610)	LR 2.000e-03
0: TRAIN [1][510/968]	Time 0.321 (0.389)	Data 1.59e-04 (1.55e-03)	Tok/s 64272 (71049)	Loss/tok 3.2598 (3.5586)	LR 2.000e-03
0: TRAIN [1][520/968]	Time 0.430 (0.389)	Data 1.53e-04 (1.53e-03)	Tok/s 77766 (71064)	Loss/tok 3.5290 (3.5566)	LR 2.000e-03
0: TRAIN [1][530/968]	Time 0.550 (0.389)	Data 1.52e-04 (1.50e-03)	Tok/s 85933 (71069)	Loss/tok 3.6728 (3.5551)	LR 2.000e-03
0: TRAIN [1][540/968]	Time 0.428 (0.390)	Data 1.70e-04 (1.48e-03)	Tok/s 78724 (71113)	Loss/tok 3.4727 (3.5551)	LR 2.000e-03
0: TRAIN [1][550/968]	Time 0.318 (0.390)	Data 1.96e-04 (1.45e-03)	Tok/s 65456 (71132)	Loss/tok 3.1304 (3.5539)	LR 2.000e-03
0: TRAIN [1][560/968]	Time 0.320 (0.390)	Data 1.61e-04 (1.43e-03)	Tok/s 64033 (71151)	Loss/tok 3.2462 (3.5520)	LR 2.000e-03
0: TRAIN [1][570/968]	Time 0.429 (0.390)	Data 1.64e-04 (1.41e-03)	Tok/s 77685 (71138)	Loss/tok 3.5009 (3.5508)	LR 2.000e-03
0: TRAIN [1][580/968]	Time 0.316 (0.391)	Data 1.52e-04 (1.39e-03)	Tok/s 64864 (71226)	Loss/tok 3.3129 (3.5514)	LR 2.000e-03
0: TRAIN [1][590/968]	Time 0.428 (0.392)	Data 1.56e-04 (1.36e-03)	Tok/s 78801 (71338)	Loss/tok 3.5116 (3.5509)	LR 2.000e-03
0: TRAIN [1][600/968]	Time 0.429 (0.393)	Data 1.82e-04 (1.35e-03)	Tok/s 77778 (71452)	Loss/tok 3.4359 (3.5509)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][610/968]	Time 0.688 (0.393)	Data 1.64e-04 (1.33e-03)	Tok/s 86039 (71480)	Loss/tok 3.8143 (3.5503)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][620/968]	Time 0.427 (0.394)	Data 1.77e-04 (1.31e-03)	Tok/s 77334 (71572)	Loss/tok 3.4784 (3.5515)	LR 2.000e-03
0: TRAIN [1][630/968]	Time 0.428 (0.395)	Data 1.65e-04 (1.29e-03)	Tok/s 78603 (71589)	Loss/tok 3.4587 (3.5507)	LR 2.000e-03
0: TRAIN [1][640/968]	Time 0.320 (0.395)	Data 1.50e-04 (1.27e-03)	Tok/s 65281 (71588)	Loss/tok 3.2220 (3.5501)	LR 2.000e-03
0: TRAIN [1][650/968]	Time 0.212 (0.395)	Data 1.73e-04 (1.25e-03)	Tok/s 50555 (71552)	Loss/tok 2.8908 (3.5485)	LR 2.000e-03
0: TRAIN [1][660/968]	Time 0.318 (0.395)	Data 1.84e-04 (1.24e-03)	Tok/s 64694 (71605)	Loss/tok 3.2204 (3.5482)	LR 2.000e-03
0: TRAIN [1][670/968]	Time 0.550 (0.395)	Data 1.70e-04 (1.22e-03)	Tok/s 85097 (71545)	Loss/tok 3.7449 (3.5466)	LR 2.000e-03
0: TRAIN [1][680/968]	Time 0.430 (0.395)	Data 1.79e-04 (1.21e-03)	Tok/s 78751 (71587)	Loss/tok 3.4777 (3.5458)	LR 2.000e-03
0: TRAIN [1][690/968]	Time 0.209 (0.394)	Data 1.45e-04 (1.19e-03)	Tok/s 50687 (71489)	Loss/tok 2.7290 (3.5430)	LR 2.000e-03
0: TRAIN [1][700/968]	Time 0.431 (0.394)	Data 1.89e-04 (1.18e-03)	Tok/s 78820 (71488)	Loss/tok 3.3641 (3.5412)	LR 2.000e-03
0: TRAIN [1][710/968]	Time 0.430 (0.393)	Data 1.76e-04 (1.16e-03)	Tok/s 77867 (71496)	Loss/tok 3.3769 (3.5392)	LR 2.000e-03
0: TRAIN [1][720/968]	Time 0.320 (0.393)	Data 1.62e-04 (1.15e-03)	Tok/s 63899 (71436)	Loss/tok 3.1556 (3.5367)	LR 2.000e-03
0: TRAIN [1][730/968]	Time 0.429 (0.393)	Data 1.48e-04 (1.14e-03)	Tok/s 77790 (71435)	Loss/tok 3.4728 (3.5349)	LR 2.000e-03
0: TRAIN [1][740/968]	Time 0.319 (0.392)	Data 1.57e-04 (1.12e-03)	Tok/s 65486 (71374)	Loss/tok 3.1693 (3.5324)	LR 2.000e-03
0: TRAIN [1][750/968]	Time 0.548 (0.392)	Data 1.74e-04 (1.11e-03)	Tok/s 85585 (71379)	Loss/tok 3.5738 (3.5311)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][760/968]	Time 0.550 (0.393)	Data 1.70e-04 (1.10e-03)	Tok/s 84683 (71455)	Loss/tok 3.6644 (3.5317)	LR 2.000e-03
0: TRAIN [1][770/968]	Time 0.319 (0.392)	Data 1.50e-04 (1.08e-03)	Tok/s 64702 (71423)	Loss/tok 3.2398 (3.5293)	LR 2.000e-03
0: TRAIN [1][780/968]	Time 0.427 (0.393)	Data 1.77e-04 (1.07e-03)	Tok/s 78608 (71468)	Loss/tok 3.4954 (3.5288)	LR 2.000e-03
0: TRAIN [1][790/968]	Time 0.428 (0.393)	Data 1.41e-04 (1.06e-03)	Tok/s 78892 (71467)	Loss/tok 3.5057 (3.5273)	LR 2.000e-03
0: TRAIN [1][800/968]	Time 0.318 (0.393)	Data 1.59e-04 (1.05e-03)	Tok/s 64677 (71464)	Loss/tok 3.1909 (3.5255)	LR 2.000e-03
0: TRAIN [1][810/968]	Time 0.319 (0.392)	Data 1.70e-04 (1.04e-03)	Tok/s 64530 (71420)	Loss/tok 3.1744 (3.5233)	LR 2.000e-03
0: TRAIN [1][820/968]	Time 0.551 (0.393)	Data 1.66e-04 (1.03e-03)	Tok/s 85453 (71479)	Loss/tok 3.6000 (3.5230)	LR 2.000e-03
0: TRAIN [1][830/968]	Time 0.549 (0.393)	Data 1.77e-04 (1.02e-03)	Tok/s 85219 (71466)	Loss/tok 3.5686 (3.5212)	LR 2.000e-03
0: TRAIN [1][840/968]	Time 0.318 (0.393)	Data 1.49e-04 (1.01e-03)	Tok/s 64865 (71503)	Loss/tok 3.1499 (3.5203)	LR 2.000e-03
0: TRAIN [1][850/968]	Time 0.430 (0.393)	Data 1.49e-04 (9.98e-04)	Tok/s 78265 (71503)	Loss/tok 3.4528 (3.5199)	LR 2.000e-03
0: TRAIN [1][860/968]	Time 0.318 (0.393)	Data 1.60e-04 (9.89e-04)	Tok/s 65455 (71461)	Loss/tok 3.2280 (3.5182)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][870/968]	Time 0.430 (0.393)	Data 1.51e-04 (9.79e-04)	Tok/s 77506 (71471)	Loss/tok 3.4907 (3.5179)	LR 2.000e-03
0: TRAIN [1][880/968]	Time 0.432 (0.393)	Data 1.75e-04 (9.70e-04)	Tok/s 77620 (71482)	Loss/tok 3.4189 (3.5175)	LR 2.000e-03
0: TRAIN [1][890/968]	Time 0.428 (0.393)	Data 1.76e-04 (9.61e-04)	Tok/s 79432 (71461)	Loss/tok 3.4656 (3.5161)	LR 2.000e-03
0: TRAIN [1][900/968]	Time 0.425 (0.393)	Data 1.72e-04 (9.52e-04)	Tok/s 79404 (71511)	Loss/tok 3.4169 (3.5153)	LR 2.000e-03
0: TRAIN [1][910/968]	Time 0.548 (0.393)	Data 1.48e-04 (9.44e-04)	Tok/s 85424 (71518)	Loss/tok 3.6483 (3.5144)	LR 2.000e-03
0: TRAIN [1][920/968]	Time 0.687 (0.394)	Data 1.70e-04 (9.35e-04)	Tok/s 86649 (71532)	Loss/tok 3.7443 (3.5141)	LR 2.000e-03
0: TRAIN [1][930/968]	Time 0.543 (0.394)	Data 1.94e-04 (9.27e-04)	Tok/s 86040 (71512)	Loss/tok 3.5920 (3.5127)	LR 2.000e-03
0: TRAIN [1][940/968]	Time 0.318 (0.393)	Data 1.56e-04 (9.19e-04)	Tok/s 64830 (71479)	Loss/tok 3.2219 (3.5113)	LR 2.000e-03
0: TRAIN [1][950/968]	Time 0.430 (0.393)	Data 1.65e-04 (9.11e-04)	Tok/s 78339 (71513)	Loss/tok 3.4293 (3.5099)	LR 2.000e-03
0: TRAIN [1][960/968]	Time 0.319 (0.393)	Data 1.69e-04 (9.03e-04)	Tok/s 64606 (71474)	Loss/tok 3.2512 (3.5090)	LR 2.000e-03
:::MLL 1581974464.930 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 525}}
:::MLL 1581974464.930 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.572 (0.572)	Decoder iters 90.0 (90.0)	Tok/s 27518 (27518)
0: Running moses detokenizer
0: BLEU(score=20.998303876281426, counts=[34442, 16300, 8890, 5029], totals=[62287, 59284, 56281, 53284], precisions=[55.29564756690802, 27.494770933135417, 15.795739237042696, 9.438105247353802], bp=0.9623715147868939, sys_len=62287, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1581974466.643 eval_accuracy: {"value": 21.0, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 536}}
:::MLL 1581974466.644 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 1	Training Loss: 3.5077	Test BLEU: 21.00
0: Performance: Epoch: 1	Training: 571740 Tok/s
0: Finished epoch 1
:::MLL 1581974466.644 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 558}}
:::MLL 1581974466.645 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1581974466.645 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 515}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 4088370112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [2][0/968]	Time 1.048 (1.048)	Data 7.12e-01 (7.12e-01)	Tok/s 19717 (19717)	Loss/tok 3.1398 (3.1398)	LR 2.000e-03
0: TRAIN [2][10/968]	Time 0.210 (0.386)	Data 1.47e-04 (6.49e-02)	Tok/s 50534 (59876)	Loss/tok 2.7744 (3.1962)	LR 2.000e-03
0: TRAIN [2][20/968]	Time 0.317 (0.369)	Data 1.59e-04 (3.41e-02)	Tok/s 65657 (64119)	Loss/tok 3.0940 (3.1998)	LR 2.000e-03
0: TRAIN [2][30/968]	Time 0.212 (0.381)	Data 1.45e-04 (2.31e-02)	Tok/s 49200 (65804)	Loss/tok 2.7047 (3.2861)	LR 2.000e-03
0: TRAIN [2][40/968]	Time 0.317 (0.388)	Data 1.81e-04 (1.75e-02)	Tok/s 65470 (67618)	Loss/tok 3.0681 (3.3106)	LR 2.000e-03
0: TRAIN [2][50/968]	Time 0.430 (0.384)	Data 1.68e-04 (1.41e-02)	Tok/s 78533 (67987)	Loss/tok 3.3876 (3.3066)	LR 2.000e-03
0: TRAIN [2][60/968]	Time 0.320 (0.387)	Data 1.57e-04 (1.19e-02)	Tok/s 64355 (68178)	Loss/tok 3.1569 (3.3314)	LR 2.000e-03
0: TRAIN [2][70/968]	Time 0.431 (0.385)	Data 1.42e-04 (1.02e-02)	Tok/s 78075 (68547)	Loss/tok 3.3833 (3.3227)	LR 2.000e-03
0: TRAIN [2][80/968]	Time 0.431 (0.389)	Data 1.47e-04 (8.97e-03)	Tok/s 78497 (69138)	Loss/tok 3.3043 (3.3274)	LR 2.000e-03
0: TRAIN [2][90/968]	Time 0.318 (0.392)	Data 1.69e-04 (8.00e-03)	Tok/s 64492 (69796)	Loss/tok 3.1829 (3.3298)	LR 2.000e-03
0: TRAIN [2][100/968]	Time 0.206 (0.393)	Data 1.68e-04 (7.23e-03)	Tok/s 51043 (70010)	Loss/tok 2.6640 (3.3313)	LR 2.000e-03
0: TRAIN [2][110/968]	Time 0.209 (0.391)	Data 1.70e-04 (6.59e-03)	Tok/s 50932 (69942)	Loss/tok 2.7001 (3.3328)	LR 2.000e-03
0: TRAIN [2][120/968]	Time 0.316 (0.391)	Data 1.81e-04 (6.06e-03)	Tok/s 65112 (70178)	Loss/tok 3.1265 (3.3335)	LR 2.000e-03
0: TRAIN [2][130/968]	Time 0.318 (0.390)	Data 1.46e-04 (5.61e-03)	Tok/s 66227 (70304)	Loss/tok 3.0664 (3.3299)	LR 2.000e-03
0: TRAIN [2][140/968]	Time 0.547 (0.391)	Data 1.53e-04 (5.22e-03)	Tok/s 84997 (70493)	Loss/tok 3.4969 (3.3290)	LR 2.000e-03
0: TRAIN [2][150/968]	Time 0.427 (0.391)	Data 1.43e-04 (4.89e-03)	Tok/s 78131 (70623)	Loss/tok 3.3791 (3.3323)	LR 2.000e-03
0: TRAIN [2][160/968]	Time 0.428 (0.391)	Data 1.91e-04 (4.60e-03)	Tok/s 77939 (70735)	Loss/tok 3.4030 (3.3287)	LR 2.000e-03
0: TRAIN [2][170/968]	Time 0.210 (0.389)	Data 1.96e-04 (4.34e-03)	Tok/s 50562 (70631)	Loss/tok 2.6327 (3.3225)	LR 2.000e-03
0: TRAIN [2][180/968]	Time 0.547 (0.392)	Data 1.71e-04 (4.11e-03)	Tok/s 85036 (70990)	Loss/tok 3.5092 (3.3288)	LR 2.000e-03
0: TRAIN [2][190/968]	Time 0.319 (0.393)	Data 1.83e-04 (3.90e-03)	Tok/s 64488 (71163)	Loss/tok 3.0831 (3.3285)	LR 2.000e-03
0: TRAIN [2][200/968]	Time 0.427 (0.398)	Data 1.55e-04 (3.71e-03)	Tok/s 78519 (71653)	Loss/tok 3.3529 (3.3397)	LR 2.000e-03
0: TRAIN [2][210/968]	Time 0.318 (0.397)	Data 1.51e-04 (3.55e-03)	Tok/s 64638 (71634)	Loss/tok 3.1638 (3.3371)	LR 2.000e-03
0: TRAIN [2][220/968]	Time 0.316 (0.396)	Data 1.71e-04 (3.39e-03)	Tok/s 65548 (71488)	Loss/tok 3.0025 (3.3349)	LR 2.000e-03
0: TRAIN [2][230/968]	Time 0.427 (0.396)	Data 1.86e-04 (3.25e-03)	Tok/s 78318 (71619)	Loss/tok 3.3504 (3.3349)	LR 2.000e-03
0: TRAIN [2][240/968]	Time 0.546 (0.397)	Data 1.83e-04 (3.12e-03)	Tok/s 84168 (71640)	Loss/tok 3.5702 (3.3384)	LR 2.000e-03
0: TRAIN [2][250/968]	Time 0.548 (0.395)	Data 1.58e-04 (3.01e-03)	Tok/s 85164 (71423)	Loss/tok 3.5287 (3.3369)	LR 2.000e-03
0: TRAIN [2][260/968]	Time 0.316 (0.393)	Data 1.87e-04 (2.90e-03)	Tok/s 64744 (71311)	Loss/tok 3.1777 (3.3342)	LR 2.000e-03
0: TRAIN [2][270/968]	Time 0.321 (0.394)	Data 1.53e-04 (2.80e-03)	Tok/s 63760 (71386)	Loss/tok 3.0985 (3.3347)	LR 2.000e-03
0: TRAIN [2][280/968]	Time 0.428 (0.395)	Data 1.69e-04 (2.70e-03)	Tok/s 78367 (71501)	Loss/tok 3.3303 (3.3361)	LR 2.000e-03
0: TRAIN [2][290/968]	Time 0.549 (0.396)	Data 1.65e-04 (2.62e-03)	Tok/s 85745 (71565)	Loss/tok 3.5324 (3.3378)	LR 2.000e-03
0: TRAIN [2][300/968]	Time 0.427 (0.395)	Data 1.56e-04 (2.53e-03)	Tok/s 77837 (71540)	Loss/tok 3.3653 (3.3395)	LR 2.000e-03
0: TRAIN [2][310/968]	Time 0.315 (0.396)	Data 1.82e-04 (2.46e-03)	Tok/s 64547 (71620)	Loss/tok 3.2101 (3.3416)	LR 2.000e-03
0: TRAIN [2][320/968]	Time 0.317 (0.396)	Data 1.52e-04 (2.39e-03)	Tok/s 65394 (71667)	Loss/tok 3.0599 (3.3391)	LR 2.000e-03
0: TRAIN [2][330/968]	Time 0.426 (0.394)	Data 1.49e-04 (2.32e-03)	Tok/s 77841 (71524)	Loss/tok 3.3452 (3.3360)	LR 2.000e-03
0: TRAIN [2][340/968]	Time 0.210 (0.394)	Data 1.73e-04 (2.26e-03)	Tok/s 50245 (71519)	Loss/tok 2.7024 (3.3352)	LR 2.000e-03
0: TRAIN [2][350/968]	Time 0.428 (0.393)	Data 1.70e-04 (2.20e-03)	Tok/s 78791 (71488)	Loss/tok 3.3690 (3.3326)	LR 2.000e-03
0: TRAIN [2][360/968]	Time 0.546 (0.393)	Data 1.76e-04 (2.14e-03)	Tok/s 84713 (71439)	Loss/tok 3.6426 (3.3339)	LR 2.000e-03
0: TRAIN [2][370/968]	Time 0.549 (0.393)	Data 1.68e-04 (2.09e-03)	Tok/s 84821 (71462)	Loss/tok 3.4815 (3.3342)	LR 2.000e-03
0: TRAIN [2][380/968]	Time 0.429 (0.392)	Data 1.45e-04 (2.04e-03)	Tok/s 78173 (71347)	Loss/tok 3.3204 (3.3311)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][390/968]	Time 0.430 (0.392)	Data 1.61e-04 (1.99e-03)	Tok/s 78137 (71372)	Loss/tok 3.2900 (3.3307)	LR 2.000e-03
0: TRAIN [2][400/968]	Time 0.431 (0.392)	Data 1.74e-04 (1.94e-03)	Tok/s 78047 (71306)	Loss/tok 3.3153 (3.3308)	LR 2.000e-03
0: TRAIN [2][410/968]	Time 0.429 (0.391)	Data 1.88e-04 (1.90e-03)	Tok/s 78580 (71283)	Loss/tok 3.3722 (3.3293)	LR 2.000e-03
0: TRAIN [2][420/968]	Time 0.316 (0.391)	Data 1.53e-04 (1.86e-03)	Tok/s 65616 (71288)	Loss/tok 3.0248 (3.3293)	LR 2.000e-03
0: TRAIN [2][430/968]	Time 0.320 (0.391)	Data 1.52e-04 (1.82e-03)	Tok/s 64270 (71267)	Loss/tok 3.0801 (3.3300)	LR 2.000e-03
0: TRAIN [2][440/968]	Time 0.318 (0.393)	Data 1.75e-04 (1.78e-03)	Tok/s 64656 (71379)	Loss/tok 3.0550 (3.3332)	LR 2.000e-03
0: TRAIN [2][450/968]	Time 0.430 (0.394)	Data 1.74e-04 (1.75e-03)	Tok/s 78300 (71487)	Loss/tok 3.2951 (3.3351)	LR 2.000e-03
0: TRAIN [2][460/968]	Time 0.317 (0.394)	Data 1.74e-04 (1.71e-03)	Tok/s 65629 (71476)	Loss/tok 3.1151 (3.3337)	LR 2.000e-03
0: TRAIN [2][470/968]	Time 0.426 (0.395)	Data 1.51e-04 (1.68e-03)	Tok/s 79514 (71579)	Loss/tok 3.3471 (3.3354)	LR 2.000e-03
0: TRAIN [2][480/968]	Time 0.687 (0.397)	Data 1.49e-04 (1.65e-03)	Tok/s 85773 (71764)	Loss/tok 3.7235 (3.3386)	LR 2.000e-03
0: TRAIN [2][490/968]	Time 0.315 (0.396)	Data 1.87e-04 (1.62e-03)	Tok/s 65810 (71699)	Loss/tok 3.0663 (3.3366)	LR 2.000e-03
0: TRAIN [2][500/968]	Time 0.551 (0.398)	Data 1.58e-04 (1.59e-03)	Tok/s 84804 (71853)	Loss/tok 3.5011 (3.3387)	LR 2.000e-03
0: TRAIN [2][510/968]	Time 0.548 (0.399)	Data 1.67e-04 (1.56e-03)	Tok/s 85124 (71991)	Loss/tok 3.4858 (3.3423)	LR 2.000e-03
0: TRAIN [2][520/968]	Time 0.431 (0.398)	Data 1.54e-04 (1.53e-03)	Tok/s 77982 (71930)	Loss/tok 3.3282 (3.3404)	LR 2.000e-03
0: TRAIN [2][530/968]	Time 0.552 (0.399)	Data 1.63e-04 (1.51e-03)	Tok/s 84760 (72036)	Loss/tok 3.4449 (3.3407)	LR 2.000e-03
0: TRAIN [2][540/968]	Time 0.319 (0.399)	Data 1.74e-04 (1.48e-03)	Tok/s 64460 (72076)	Loss/tok 3.0817 (3.3395)	LR 2.000e-03
0: TRAIN [2][550/968]	Time 0.430 (0.398)	Data 1.54e-04 (1.46e-03)	Tok/s 77818 (71908)	Loss/tok 3.3469 (3.3369)	LR 2.000e-03
0: TRAIN [2][560/968]	Time 0.319 (0.397)	Data 1.82e-04 (1.44e-03)	Tok/s 64511 (71838)	Loss/tok 3.1045 (3.3350)	LR 2.000e-03
0: TRAIN [2][570/968]	Time 0.428 (0.396)	Data 1.62e-04 (1.41e-03)	Tok/s 77702 (71820)	Loss/tok 3.2777 (3.3337)	LR 2.000e-03
0: TRAIN [2][580/968]	Time 0.684 (0.397)	Data 1.78e-04 (1.39e-03)	Tok/s 87452 (71854)	Loss/tok 3.6458 (3.3350)	LR 2.000e-03
0: TRAIN [2][590/968]	Time 0.427 (0.396)	Data 1.41e-04 (1.37e-03)	Tok/s 78654 (71806)	Loss/tok 3.4019 (3.3331)	LR 2.000e-03
0: TRAIN [2][600/968]	Time 0.319 (0.396)	Data 1.67e-04 (1.35e-03)	Tok/s 65029 (71744)	Loss/tok 3.0446 (3.3317)	LR 2.000e-03
0: TRAIN [2][610/968]	Time 0.318 (0.395)	Data 1.58e-04 (1.33e-03)	Tok/s 64984 (71731)	Loss/tok 3.0556 (3.3306)	LR 2.000e-03
0: TRAIN [2][620/968]	Time 0.209 (0.396)	Data 1.73e-04 (1.31e-03)	Tok/s 50007 (71733)	Loss/tok 2.6195 (3.3328)	LR 2.000e-03
0: TRAIN [2][630/968]	Time 0.318 (0.395)	Data 1.50e-04 (1.30e-03)	Tok/s 65289 (71693)	Loss/tok 3.0789 (3.3310)	LR 2.000e-03
0: TRAIN [2][640/968]	Time 0.433 (0.396)	Data 1.53e-04 (1.28e-03)	Tok/s 77224 (71738)	Loss/tok 3.3110 (3.3309)	LR 2.000e-03
0: TRAIN [2][650/968]	Time 0.430 (0.395)	Data 1.51e-04 (1.26e-03)	Tok/s 77514 (71733)	Loss/tok 3.3500 (3.3296)	LR 2.000e-03
0: TRAIN [2][660/968]	Time 0.430 (0.395)	Data 1.68e-04 (1.24e-03)	Tok/s 77058 (71740)	Loss/tok 3.3098 (3.3292)	LR 2.000e-03
0: TRAIN [2][670/968]	Time 0.685 (0.397)	Data 1.60e-04 (1.23e-03)	Tok/s 86380 (71833)	Loss/tok 3.5849 (3.3325)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][680/968]	Time 0.429 (0.397)	Data 1.48e-04 (1.21e-03)	Tok/s 78696 (71850)	Loss/tok 3.3257 (3.3324)	LR 2.000e-03
0: TRAIN [2][690/968]	Time 0.427 (0.397)	Data 1.51e-04 (1.20e-03)	Tok/s 78192 (71809)	Loss/tok 3.3441 (3.3310)	LR 2.000e-03
0: TRAIN [2][700/968]	Time 0.430 (0.397)	Data 1.55e-04 (1.18e-03)	Tok/s 77599 (71844)	Loss/tok 3.3264 (3.3299)	LR 2.000e-03
0: TRAIN [2][710/968]	Time 0.317 (0.397)	Data 1.81e-04 (1.17e-03)	Tok/s 65461 (71870)	Loss/tok 3.0586 (3.3293)	LR 2.000e-03
0: TRAIN [2][720/968]	Time 0.432 (0.397)	Data 1.81e-04 (1.15e-03)	Tok/s 77005 (71850)	Loss/tok 3.3327 (3.3294)	LR 2.000e-03
0: TRAIN [2][730/968]	Time 0.431 (0.397)	Data 1.55e-04 (1.14e-03)	Tok/s 78481 (71891)	Loss/tok 3.3104 (3.3288)	LR 2.000e-03
0: TRAIN [2][740/968]	Time 0.551 (0.398)	Data 1.49e-04 (1.13e-03)	Tok/s 84579 (71946)	Loss/tok 3.4591 (3.3309)	LR 2.000e-03
0: TRAIN [2][750/968]	Time 0.319 (0.397)	Data 1.62e-04 (1.12e-03)	Tok/s 65155 (71880)	Loss/tok 3.1203 (3.3293)	LR 2.000e-03
0: TRAIN [2][760/968]	Time 0.431 (0.396)	Data 1.67e-04 (1.10e-03)	Tok/s 79215 (71808)	Loss/tok 3.2849 (3.3278)	LR 2.000e-03
0: TRAIN [2][770/968]	Time 0.429 (0.397)	Data 1.85e-04 (1.09e-03)	Tok/s 78720 (71820)	Loss/tok 3.3157 (3.3287)	LR 2.000e-03
0: TRAIN [2][780/968]	Time 0.319 (0.396)	Data 1.89e-04 (1.08e-03)	Tok/s 64099 (71816)	Loss/tok 3.1007 (3.3285)	LR 2.000e-03
0: TRAIN [2][790/968]	Time 0.319 (0.396)	Data 1.49e-04 (1.07e-03)	Tok/s 65296 (71813)	Loss/tok 3.1035 (3.3275)	LR 2.000e-03
0: TRAIN [2][800/968]	Time 0.318 (0.396)	Data 1.57e-04 (1.06e-03)	Tok/s 65338 (71814)	Loss/tok 3.0745 (3.3263)	LR 2.000e-03
0: TRAIN [2][810/968]	Time 0.319 (0.395)	Data 1.64e-04 (1.04e-03)	Tok/s 64405 (71748)	Loss/tok 3.0469 (3.3249)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][820/968]	Time 0.319 (0.395)	Data 1.74e-04 (1.03e-03)	Tok/s 64336 (71717)	Loss/tok 3.1184 (3.3241)	LR 2.000e-03
0: TRAIN [2][830/968]	Time 0.318 (0.395)	Data 1.65e-04 (1.02e-03)	Tok/s 65274 (71676)	Loss/tok 3.1308 (3.3228)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][840/968]	Time 0.209 (0.395)	Data 1.87e-04 (1.01e-03)	Tok/s 50251 (71685)	Loss/tok 2.6977 (3.3239)	LR 2.000e-03
0: TRAIN [2][850/968]	Time 0.554 (0.395)	Data 1.62e-04 (1.00e-03)	Tok/s 84074 (71694)	Loss/tok 3.4748 (3.3236)	LR 2.000e-03
0: TRAIN [2][860/968]	Time 0.317 (0.394)	Data 1.58e-04 (9.94e-04)	Tok/s 66010 (71633)	Loss/tok 3.1343 (3.3225)	LR 2.000e-03
0: TRAIN [2][870/968]	Time 0.320 (0.394)	Data 1.81e-04 (9.84e-04)	Tok/s 64494 (71584)	Loss/tok 3.0355 (3.3209)	LR 2.000e-03
0: TRAIN [2][880/968]	Time 0.322 (0.393)	Data 1.48e-04 (9.75e-04)	Tok/s 64745 (71565)	Loss/tok 3.0853 (3.3204)	LR 2.000e-03
0: TRAIN [2][890/968]	Time 0.428 (0.393)	Data 1.58e-04 (9.66e-04)	Tok/s 78518 (71545)	Loss/tok 3.2453 (3.3196)	LR 2.000e-03
0: TRAIN [2][900/968]	Time 0.212 (0.393)	Data 1.65e-04 (9.57e-04)	Tok/s 48973 (71554)	Loss/tok 2.6732 (3.3193)	LR 2.000e-03
0: TRAIN [2][910/968]	Time 0.318 (0.393)	Data 1.56e-04 (9.48e-04)	Tok/s 65343 (71554)	Loss/tok 3.1120 (3.3184)	LR 2.000e-03
0: TRAIN [2][920/968]	Time 0.427 (0.393)	Data 1.90e-04 (9.40e-04)	Tok/s 77876 (71549)	Loss/tok 3.3149 (3.3174)	LR 2.000e-03
0: TRAIN [2][930/968]	Time 0.686 (0.393)	Data 1.78e-04 (9.31e-04)	Tok/s 87381 (71508)	Loss/tok 3.6165 (3.3171)	LR 2.000e-03
0: TRAIN [2][940/968]	Time 0.209 (0.393)	Data 1.78e-04 (9.23e-04)	Tok/s 50392 (71478)	Loss/tok 2.5515 (3.3165)	LR 2.000e-03
0: TRAIN [2][950/968]	Time 0.551 (0.393)	Data 1.66e-04 (9.15e-04)	Tok/s 84548 (71516)	Loss/tok 3.4050 (3.3170)	LR 2.000e-03
0: TRAIN [2][960/968]	Time 0.432 (0.393)	Data 1.49e-04 (9.08e-04)	Tok/s 78181 (71541)	Loss/tok 3.3167 (3.3175)	LR 2.000e-03
:::MLL 1581974848.306 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 525}}
:::MLL 1581974848.306 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.607 (0.607)	Decoder iters 104.0 (104.0)	Tok/s 26210 (26210)
0: Running moses detokenizer
0: BLEU(score=22.726360904955683, counts=[35888, 17522, 9759, 5647], totals=[64033, 61030, 58028, 55031], precisions=[56.04610122905377, 28.71047026052761, 16.81774315847522, 10.261488978939143], bp=0.9900085522545491, sys_len=64033, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1581974850.072 eval_accuracy: {"value": 22.73, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 536}}
:::MLL 1581974850.072 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 2	Training Loss: 3.3146	Test BLEU: 22.73
0: Performance: Epoch: 2	Training: 571918 Tok/s
0: Finished epoch 2
:::MLL 1581974850.073 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 558}}
:::MLL 1581974850.073 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1581974850.073 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 515}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 1534678926
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][0/968]	Time 1.439 (1.439)	Data 7.23e-01 (7.23e-01)	Tok/s 41375 (41375)	Loss/tok 3.5951 (3.5951)	LR 2.000e-03
0: TRAIN [3][10/968]	Time 0.429 (0.532)	Data 1.90e-04 (6.59e-02)	Tok/s 77541 (69541)	Loss/tok 3.2394 (3.3769)	LR 2.000e-03
0: TRAIN [3][20/968]	Time 0.318 (0.451)	Data 1.74e-04 (3.46e-02)	Tok/s 64013 (69569)	Loss/tok 3.0191 (3.2799)	LR 2.000e-03
0: TRAIN [3][30/968]	Time 0.208 (0.442)	Data 1.60e-04 (2.35e-02)	Tok/s 50374 (71049)	Loss/tok 2.6881 (3.2769)	LR 2.000e-03
0: TRAIN [3][40/968]	Time 0.548 (0.429)	Data 1.60e-04 (1.78e-02)	Tok/s 85474 (71253)	Loss/tok 3.3373 (3.2569)	LR 2.000e-03
0: TRAIN [3][50/968]	Time 0.429 (0.418)	Data 1.87e-04 (1.43e-02)	Tok/s 77146 (71162)	Loss/tok 3.2051 (3.2386)	LR 2.000e-03
0: TRAIN [3][60/968]	Time 0.317 (0.413)	Data 2.47e-04 (1.20e-02)	Tok/s 64649 (71042)	Loss/tok 3.0423 (3.2375)	LR 2.000e-03
0: TRAIN [3][70/968]	Time 0.321 (0.412)	Data 1.97e-04 (1.04e-02)	Tok/s 63986 (71181)	Loss/tok 3.0726 (3.2404)	LR 2.000e-03
0: TRAIN [3][80/968]	Time 0.318 (0.414)	Data 1.71e-04 (9.10e-03)	Tok/s 64529 (71174)	Loss/tok 3.0070 (3.2580)	LR 2.000e-03
0: TRAIN [3][90/968]	Time 0.685 (0.416)	Data 1.67e-04 (8.11e-03)	Tok/s 86902 (71400)	Loss/tok 3.5944 (3.2645)	LR 2.000e-03
0: TRAIN [3][100/968]	Time 0.429 (0.412)	Data 1.66e-04 (7.33e-03)	Tok/s 79025 (71406)	Loss/tok 3.1628 (3.2541)	LR 2.000e-03
0: TRAIN [3][110/968]	Time 0.553 (0.422)	Data 1.58e-04 (6.68e-03)	Tok/s 83920 (72256)	Loss/tok 3.3702 (3.2714)	LR 2.000e-03
0: TRAIN [3][120/968]	Time 0.428 (0.420)	Data 1.54e-04 (6.14e-03)	Tok/s 78096 (72330)	Loss/tok 3.2222 (3.2664)	LR 2.000e-03
0: TRAIN [3][130/968]	Time 0.428 (0.417)	Data 1.65e-04 (5.69e-03)	Tok/s 78388 (72260)	Loss/tok 3.1802 (3.2595)	LR 2.000e-03
0: TRAIN [3][140/968]	Time 0.211 (0.414)	Data 1.51e-04 (5.30e-03)	Tok/s 49607 (72164)	Loss/tok 2.5688 (3.2528)	LR 2.000e-03
0: TRAIN [3][150/968]	Time 0.430 (0.409)	Data 1.66e-04 (4.96e-03)	Tok/s 78264 (71851)	Loss/tok 3.1624 (3.2446)	LR 2.000e-03
0: TRAIN [3][160/968]	Time 0.431 (0.413)	Data 1.57e-04 (4.66e-03)	Tok/s 77910 (72182)	Loss/tok 3.2466 (3.2524)	LR 2.000e-03
0: TRAIN [3][170/968]	Time 0.208 (0.411)	Data 1.57e-04 (4.40e-03)	Tok/s 49761 (72048)	Loss/tok 2.6039 (3.2488)	LR 2.000e-03
0: TRAIN [3][180/968]	Time 0.320 (0.412)	Data 1.54e-04 (4.16e-03)	Tok/s 63773 (72185)	Loss/tok 3.0415 (3.2528)	LR 2.000e-03
0: TRAIN [3][190/968]	Time 0.551 (0.413)	Data 1.80e-04 (3.95e-03)	Tok/s 85069 (72359)	Loss/tok 3.3783 (3.2526)	LR 2.000e-03
0: TRAIN [3][200/968]	Time 0.431 (0.415)	Data 1.58e-04 (3.77e-03)	Tok/s 77754 (72686)	Loss/tok 3.1788 (3.2517)	LR 2.000e-03
0: TRAIN [3][210/968]	Time 0.317 (0.415)	Data 1.75e-04 (3.59e-03)	Tok/s 65265 (72714)	Loss/tok 3.0399 (3.2540)	LR 2.000e-03
0: TRAIN [3][220/968]	Time 0.548 (0.416)	Data 1.83e-04 (3.44e-03)	Tok/s 84795 (72873)	Loss/tok 3.3914 (3.2539)	LR 2.000e-03
0: TRAIN [3][230/968]	Time 0.551 (0.417)	Data 1.56e-04 (3.30e-03)	Tok/s 85120 (72995)	Loss/tok 3.3952 (3.2569)	LR 2.000e-03
0: TRAIN [3][240/968]	Time 0.319 (0.416)	Data 1.80e-04 (3.17e-03)	Tok/s 65420 (72885)	Loss/tok 2.9926 (3.2564)	LR 2.000e-03
0: TRAIN [3][250/968]	Time 0.316 (0.416)	Data 2.21e-04 (3.05e-03)	Tok/s 65071 (72979)	Loss/tok 3.0280 (3.2556)	LR 2.000e-03
0: TRAIN [3][260/968]	Time 0.319 (0.415)	Data 1.73e-04 (2.94e-03)	Tok/s 65077 (72870)	Loss/tok 2.9812 (3.2547)	LR 2.000e-03
0: TRAIN [3][270/968]	Time 0.430 (0.417)	Data 1.54e-04 (2.84e-03)	Tok/s 78149 (73017)	Loss/tok 3.1921 (3.2575)	LR 2.000e-03
0: TRAIN [3][280/968]	Time 0.318 (0.414)	Data 1.50e-04 (2.74e-03)	Tok/s 63741 (72821)	Loss/tok 2.9655 (3.2529)	LR 2.000e-03
0: TRAIN [3][290/968]	Time 0.318 (0.412)	Data 1.77e-04 (2.65e-03)	Tok/s 64802 (72604)	Loss/tok 3.0127 (3.2493)	LR 2.000e-03
0: TRAIN [3][300/968]	Time 0.550 (0.412)	Data 1.49e-04 (2.57e-03)	Tok/s 85370 (72684)	Loss/tok 3.3724 (3.2491)	LR 2.000e-03
0: TRAIN [3][310/968]	Time 0.318 (0.412)	Data 1.77e-04 (2.49e-03)	Tok/s 63401 (72781)	Loss/tok 3.0077 (3.2481)	LR 2.000e-03
0: TRAIN [3][320/968]	Time 0.430 (0.411)	Data 1.53e-04 (2.42e-03)	Tok/s 78178 (72627)	Loss/tok 3.3188 (3.2482)	LR 2.000e-03
0: TRAIN [3][330/968]	Time 0.428 (0.411)	Data 1.72e-04 (2.35e-03)	Tok/s 78583 (72673)	Loss/tok 3.1707 (3.2498)	LR 2.000e-03
0: TRAIN [3][340/968]	Time 0.316 (0.410)	Data 2.00e-04 (2.29e-03)	Tok/s 65829 (72628)	Loss/tok 3.0039 (3.2478)	LR 2.000e-03
0: TRAIN [3][350/968]	Time 0.213 (0.409)	Data 1.57e-04 (2.23e-03)	Tok/s 49548 (72561)	Loss/tok 2.6276 (3.2455)	LR 2.000e-03
0: TRAIN [3][360/968]	Time 0.429 (0.408)	Data 1.69e-04 (2.17e-03)	Tok/s 79166 (72415)	Loss/tok 3.2271 (3.2443)	LR 2.000e-03
0: TRAIN [3][370/968]	Time 0.210 (0.405)	Data 1.58e-04 (2.12e-03)	Tok/s 49964 (72190)	Loss/tok 2.5972 (3.2409)	LR 2.000e-03
0: TRAIN [3][380/968]	Time 0.318 (0.403)	Data 1.69e-04 (2.07e-03)	Tok/s 65290 (72000)	Loss/tok 3.0347 (3.2377)	LR 2.000e-03
0: TRAIN [3][390/968]	Time 0.428 (0.402)	Data 2.08e-04 (2.02e-03)	Tok/s 77580 (71928)	Loss/tok 3.2399 (3.2359)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][400/968]	Time 0.316 (0.402)	Data 1.75e-04 (1.97e-03)	Tok/s 66343 (71925)	Loss/tok 3.0213 (3.2359)	LR 2.000e-03
0: TRAIN [3][410/968]	Time 0.316 (0.402)	Data 1.55e-04 (1.93e-03)	Tok/s 65463 (71914)	Loss/tok 3.0479 (3.2357)	LR 2.000e-03
0: TRAIN [3][420/968]	Time 0.550 (0.401)	Data 1.68e-04 (1.89e-03)	Tok/s 84959 (71863)	Loss/tok 3.4088 (3.2343)	LR 2.000e-03
0: TRAIN [3][430/968]	Time 0.431 (0.401)	Data 1.53e-04 (1.85e-03)	Tok/s 77936 (71871)	Loss/tok 3.2350 (3.2330)	LR 2.000e-03
0: TRAIN [3][440/968]	Time 0.428 (0.400)	Data 1.47e-04 (1.81e-03)	Tok/s 79124 (71816)	Loss/tok 3.1312 (3.2314)	LR 2.000e-03
0: TRAIN [3][450/968]	Time 0.429 (0.401)	Data 1.97e-04 (1.77e-03)	Tok/s 78620 (71889)	Loss/tok 3.3089 (3.2331)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][460/968]	Time 0.429 (0.401)	Data 1.60e-04 (1.74e-03)	Tok/s 78288 (71945)	Loss/tok 3.2717 (3.2342)	LR 2.000e-03
0: TRAIN [3][470/968]	Time 0.434 (0.401)	Data 1.73e-04 (1.70e-03)	Tok/s 76717 (71975)	Loss/tok 3.2354 (3.2349)	LR 2.000e-03
0: TRAIN [3][480/968]	Time 0.316 (0.401)	Data 1.59e-04 (1.67e-03)	Tok/s 65426 (71923)	Loss/tok 2.9991 (3.2346)	LR 2.000e-03
0: TRAIN [3][490/968]	Time 0.429 (0.400)	Data 1.68e-04 (1.64e-03)	Tok/s 77896 (71916)	Loss/tok 3.1997 (3.2330)	LR 2.000e-03
0: TRAIN [3][500/968]	Time 0.209 (0.400)	Data 1.80e-04 (1.61e-03)	Tok/s 50476 (71884)	Loss/tok 2.5535 (3.2324)	LR 2.000e-03
0: TRAIN [3][510/968]	Time 0.320 (0.400)	Data 1.59e-04 (1.58e-03)	Tok/s 65618 (71930)	Loss/tok 2.9832 (3.2328)	LR 2.000e-03
0: TRAIN [3][520/968]	Time 0.319 (0.399)	Data 1.59e-04 (1.56e-03)	Tok/s 65208 (71801)	Loss/tok 3.0172 (3.2307)	LR 2.000e-03
0: TRAIN [3][530/968]	Time 0.319 (0.399)	Data 1.57e-04 (1.53e-03)	Tok/s 65770 (71829)	Loss/tok 3.0110 (3.2314)	LR 2.000e-03
0: TRAIN [3][540/968]	Time 0.319 (0.399)	Data 1.75e-04 (1.50e-03)	Tok/s 64750 (71833)	Loss/tok 3.0186 (3.2309)	LR 2.000e-03
0: TRAIN [3][550/968]	Time 0.433 (0.400)	Data 1.47e-04 (1.48e-03)	Tok/s 77443 (71940)	Loss/tok 3.2113 (3.2336)	LR 2.000e-03
0: TRAIN [3][560/968]	Time 0.434 (0.401)	Data 1.83e-04 (1.46e-03)	Tok/s 76628 (71982)	Loss/tok 3.2478 (3.2340)	LR 2.000e-03
0: TRAIN [3][570/968]	Time 0.319 (0.400)	Data 1.53e-04 (1.43e-03)	Tok/s 64016 (71898)	Loss/tok 3.0468 (3.2331)	LR 2.000e-03
0: TRAIN [3][580/968]	Time 0.316 (0.400)	Data 1.48e-04 (1.41e-03)	Tok/s 65375 (71914)	Loss/tok 3.0333 (3.2331)	LR 2.000e-03
0: TRAIN [3][590/968]	Time 0.319 (0.399)	Data 1.78e-04 (1.39e-03)	Tok/s 64298 (71854)	Loss/tok 3.0477 (3.2318)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][600/968]	Time 0.434 (0.400)	Data 1.76e-04 (1.37e-03)	Tok/s 77995 (71871)	Loss/tok 3.1623 (3.2340)	LR 2.000e-03
0: TRAIN [3][610/968]	Time 0.428 (0.400)	Data 1.55e-04 (1.35e-03)	Tok/s 78913 (71932)	Loss/tok 3.2215 (3.2347)	LR 2.000e-03
0: TRAIN [3][620/968]	Time 0.316 (0.400)	Data 1.70e-04 (1.33e-03)	Tok/s 64816 (71934)	Loss/tok 3.0339 (3.2338)	LR 2.000e-03
0: TRAIN [3][630/968]	Time 0.315 (0.399)	Data 1.80e-04 (1.31e-03)	Tok/s 66232 (71846)	Loss/tok 3.0134 (3.2317)	LR 2.000e-03
0: TRAIN [3][640/968]	Time 0.318 (0.398)	Data 1.69e-04 (1.30e-03)	Tok/s 64464 (71777)	Loss/tok 2.9652 (3.2297)	LR 2.000e-03
0: TRAIN [3][650/968]	Time 0.688 (0.399)	Data 1.78e-04 (1.28e-03)	Tok/s 86610 (71852)	Loss/tok 3.6518 (3.2320)	LR 2.000e-03
0: TRAIN [3][660/968]	Time 0.427 (0.400)	Data 1.76e-04 (1.26e-03)	Tok/s 78944 (71899)	Loss/tok 3.2732 (3.2333)	LR 2.000e-03
0: TRAIN [3][670/968]	Time 0.320 (0.401)	Data 1.49e-04 (1.25e-03)	Tok/s 64518 (72007)	Loss/tok 2.9973 (3.2346)	LR 2.000e-03
0: TRAIN [3][680/968]	Time 0.210 (0.400)	Data 1.54e-04 (1.23e-03)	Tok/s 50531 (71955)	Loss/tok 2.6436 (3.2338)	LR 2.000e-03
0: TRAIN [3][690/968]	Time 0.319 (0.400)	Data 1.69e-04 (1.21e-03)	Tok/s 63689 (71919)	Loss/tok 3.0397 (3.2326)	LR 2.000e-03
0: TRAIN [3][700/968]	Time 0.548 (0.399)	Data 1.63e-04 (1.20e-03)	Tok/s 84706 (71915)	Loss/tok 3.4038 (3.2323)	LR 2.000e-03
0: TRAIN [3][710/968]	Time 0.547 (0.400)	Data 1.66e-04 (1.19e-03)	Tok/s 85107 (71949)	Loss/tok 3.3664 (3.2326)	LR 2.000e-03
0: TRAIN [3][720/968]	Time 0.316 (0.399)	Data 1.76e-04 (1.17e-03)	Tok/s 65639 (71889)	Loss/tok 2.9606 (3.2309)	LR 2.000e-03
0: TRAIN [3][730/968]	Time 0.319 (0.399)	Data 1.75e-04 (1.16e-03)	Tok/s 64838 (71901)	Loss/tok 2.9353 (3.2306)	LR 2.000e-03
0: TRAIN [3][740/968]	Time 0.318 (0.399)	Data 1.75e-04 (1.14e-03)	Tok/s 64748 (71924)	Loss/tok 2.9755 (3.2295)	LR 2.000e-03
0: TRAIN [3][750/968]	Time 0.686 (0.399)	Data 1.68e-04 (1.13e-03)	Tok/s 87291 (71866)	Loss/tok 3.5997 (3.2301)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][760/968]	Time 0.318 (0.398)	Data 1.42e-04 (1.12e-03)	Tok/s 64273 (71799)	Loss/tok 3.0074 (3.2295)	LR 2.000e-03
0: TRAIN [3][770/968]	Time 0.317 (0.397)	Data 1.57e-04 (1.11e-03)	Tok/s 64815 (71736)	Loss/tok 2.9448 (3.2279)	LR 2.000e-03
0: TRAIN [3][780/968]	Time 0.316 (0.397)	Data 1.47e-04 (1.09e-03)	Tok/s 65697 (71693)	Loss/tok 3.0174 (3.2275)	LR 2.000e-03
0: TRAIN [3][790/968]	Time 0.212 (0.397)	Data 1.82e-04 (1.08e-03)	Tok/s 49441 (71655)	Loss/tok 2.6013 (3.2267)	LR 2.000e-03
0: TRAIN [3][800/968]	Time 0.430 (0.397)	Data 1.82e-04 (1.07e-03)	Tok/s 78675 (71689)	Loss/tok 3.1798 (3.2263)	LR 2.000e-03
0: TRAIN [3][810/968]	Time 0.207 (0.396)	Data 1.55e-04 (1.06e-03)	Tok/s 51801 (71613)	Loss/tok 2.5646 (3.2250)	LR 2.000e-03
0: TRAIN [3][820/968]	Time 0.317 (0.395)	Data 1.65e-04 (1.05e-03)	Tok/s 65307 (71574)	Loss/tok 2.9623 (3.2236)	LR 2.000e-03
0: TRAIN [3][830/968]	Time 0.317 (0.395)	Data 1.63e-04 (1.04e-03)	Tok/s 66011 (71568)	Loss/tok 3.0021 (3.2230)	LR 2.000e-03
0: TRAIN [3][840/968]	Time 0.207 (0.394)	Data 1.48e-04 (1.03e-03)	Tok/s 51374 (71508)	Loss/tok 2.5814 (3.2220)	LR 2.000e-03
0: TRAIN [3][850/968]	Time 0.317 (0.394)	Data 1.76e-04 (1.02e-03)	Tok/s 65696 (71487)	Loss/tok 3.0168 (3.2214)	LR 2.000e-03
0: TRAIN [3][860/968]	Time 0.210 (0.394)	Data 1.56e-04 (1.01e-03)	Tok/s 49549 (71490)	Loss/tok 2.6272 (3.2217)	LR 2.000e-03
0: TRAIN [3][870/968]	Time 0.317 (0.394)	Data 1.85e-04 (9.97e-04)	Tok/s 64481 (71456)	Loss/tok 2.9403 (3.2215)	LR 2.000e-03
0: TRAIN [3][880/968]	Time 0.431 (0.393)	Data 2.07e-04 (9.88e-04)	Tok/s 78079 (71388)	Loss/tok 3.2386 (3.2210)	LR 2.000e-03
0: TRAIN [3][890/968]	Time 0.430 (0.394)	Data 1.72e-04 (9.79e-04)	Tok/s 78150 (71417)	Loss/tok 3.2564 (3.2219)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][900/968]	Time 0.687 (0.394)	Data 1.56e-04 (9.70e-04)	Tok/s 86803 (71450)	Loss/tok 3.5181 (3.2231)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][910/968]	Time 0.320 (0.394)	Data 1.66e-04 (9.61e-04)	Tok/s 64041 (71468)	Loss/tok 2.9657 (3.2238)	LR 2.000e-03
0: TRAIN [3][920/968]	Time 0.319 (0.394)	Data 1.76e-04 (9.52e-04)	Tok/s 64553 (71457)	Loss/tok 3.0217 (3.2234)	LR 2.000e-03
0: TRAIN [3][930/968]	Time 0.319 (0.394)	Data 1.49e-04 (9.44e-04)	Tok/s 65267 (71460)	Loss/tok 3.0085 (3.2230)	LR 2.000e-03
0: TRAIN [3][940/968]	Time 0.318 (0.394)	Data 1.69e-04 (9.36e-04)	Tok/s 64275 (71443)	Loss/tok 3.0897 (3.2222)	LR 2.000e-03
0: TRAIN [3][950/968]	Time 0.428 (0.394)	Data 1.54e-04 (9.28e-04)	Tok/s 78395 (71423)	Loss/tok 3.1340 (3.2215)	LR 2.000e-03
0: TRAIN [3][960/968]	Time 0.548 (0.394)	Data 1.75e-04 (9.20e-04)	Tok/s 85045 (71438)	Loss/tok 3.4163 (3.2221)	LR 2.000e-03
:::MLL 1581975232.059 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 525}}
:::MLL 1581975232.059 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.658 (0.658)	Decoder iters 116.0 (116.0)	Tok/s 24927 (24927)
0: Running moses detokenizer
0: BLEU(score=23.286955001699276, counts=[36295, 17864, 10041, 5891], totals=[64691, 61688, 58686, 55688], precisions=[56.10517691796386, 28.958630527817405, 17.109702484408547, 10.578580663697744], bp=1.0, sys_len=64691, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1581975233.861 eval_accuracy: {"value": 23.29, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 536}}
:::MLL 1581975233.861 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 3	Training Loss: 3.2214	Test BLEU: 23.29
0: Performance: Epoch: 3	Training: 571459 Tok/s
0: Finished epoch 3
:::MLL 1581975233.862 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 558}}
:::MLL 1581975233.862 block_start: {"value": null, "metadata": {"first_epoch_num": 5, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1581975233.862 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 515}}
0: Starting epoch 4
0: Executing preallocation
0: Sampler for epoch 4 uses seed 2978146364
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [4][0/968]	Time 1.146 (1.146)	Data 6.89e-01 (6.89e-01)	Tok/s 29382 (29382)	Loss/tok 3.1185 (3.1185)	LR 2.000e-03
0: TRAIN [4][10/968]	Time 0.548 (0.533)	Data 1.82e-04 (6.28e-02)	Tok/s 85398 (72573)	Loss/tok 3.3423 (3.2373)	LR 2.000e-03
0: TRAIN [4][20/968]	Time 0.432 (0.476)	Data 1.87e-04 (3.30e-02)	Tok/s 77114 (72547)	Loss/tok 3.1787 (3.2137)	LR 2.000e-03
0: TRAIN [4][30/968]	Time 0.429 (0.436)	Data 1.59e-04 (2.24e-02)	Tok/s 78277 (70890)	Loss/tok 3.1003 (3.1862)	LR 2.000e-03
0: TRAIN [4][40/968]	Time 0.432 (0.422)	Data 1.61e-04 (1.70e-02)	Tok/s 77565 (70514)	Loss/tok 3.1906 (3.1708)	LR 2.000e-03
0: TRAIN [4][50/968]	Time 0.313 (0.417)	Data 1.73e-04 (1.37e-02)	Tok/s 66103 (70980)	Loss/tok 2.9699 (3.1642)	LR 2.000e-03
0: TRAIN [4][60/968]	Time 0.685 (0.414)	Data 1.80e-04 (1.15e-02)	Tok/s 87244 (71295)	Loss/tok 3.5247 (3.1610)	LR 2.000e-03
0: TRAIN [4][70/968]	Time 0.428 (0.410)	Data 1.54e-04 (9.87e-03)	Tok/s 78499 (71594)	Loss/tok 3.1345 (3.1530)	LR 2.000e-03
0: TRAIN [4][80/968]	Time 0.317 (0.397)	Data 1.49e-04 (8.67e-03)	Tok/s 65399 (70601)	Loss/tok 2.9209 (3.1359)	LR 2.000e-03
0: TRAIN [4][90/968]	Time 0.429 (0.402)	Data 1.71e-04 (7.74e-03)	Tok/s 77710 (71460)	Loss/tok 3.1493 (3.1429)	LR 2.000e-03
0: TRAIN [4][100/968]	Time 0.318 (0.397)	Data 1.65e-04 (6.99e-03)	Tok/s 64298 (71123)	Loss/tok 2.9572 (3.1364)	LR 2.000e-03
0: TRAIN [4][110/968]	Time 0.318 (0.392)	Data 1.56e-04 (6.38e-03)	Tok/s 65353 (70722)	Loss/tok 2.9484 (3.1286)	LR 2.000e-03
0: TRAIN [4][120/968]	Time 0.687 (0.400)	Data 1.61e-04 (5.86e-03)	Tok/s 86843 (71333)	Loss/tok 3.5651 (3.1479)	LR 2.000e-03
0: TRAIN [4][130/968]	Time 0.316 (0.395)	Data 1.82e-04 (5.43e-03)	Tok/s 65154 (71052)	Loss/tok 2.9827 (3.1416)	LR 2.000e-03
0: TRAIN [4][140/968]	Time 0.434 (0.393)	Data 1.75e-04 (5.06e-03)	Tok/s 77886 (70948)	Loss/tok 3.1954 (3.1380)	LR 2.000e-03
0: TRAIN [4][150/968]	Time 0.316 (0.390)	Data 1.96e-04 (4.73e-03)	Tok/s 65072 (70713)	Loss/tok 3.0004 (3.1360)	LR 2.000e-03
0: TRAIN [4][160/968]	Time 0.431 (0.392)	Data 1.66e-04 (4.45e-03)	Tok/s 77801 (70893)	Loss/tok 3.1529 (3.1378)	LR 2.000e-03
0: TRAIN [4][170/968]	Time 0.691 (0.393)	Data 1.71e-04 (4.20e-03)	Tok/s 86376 (71061)	Loss/tok 3.5407 (3.1437)	LR 2.000e-03
0: TRAIN [4][180/968]	Time 0.318 (0.399)	Data 1.90e-04 (3.98e-03)	Tok/s 64041 (71524)	Loss/tok 2.9821 (3.1555)	LR 2.000e-03
0: TRAIN [4][190/968]	Time 0.319 (0.400)	Data 1.69e-04 (3.78e-03)	Tok/s 64646 (71626)	Loss/tok 2.9368 (3.1557)	LR 2.000e-03
0: TRAIN [4][200/968]	Time 0.430 (0.401)	Data 1.71e-04 (3.60e-03)	Tok/s 78272 (71816)	Loss/tok 3.0836 (3.1553)	LR 2.000e-03
0: TRAIN [4][210/968]	Time 0.319 (0.400)	Data 1.60e-04 (3.44e-03)	Tok/s 64383 (71873)	Loss/tok 2.9456 (3.1528)	LR 2.000e-03
0: TRAIN [4][220/968]	Time 0.430 (0.400)	Data 1.69e-04 (3.29e-03)	Tok/s 77646 (71866)	Loss/tok 3.2362 (3.1538)	LR 2.000e-03
0: TRAIN [4][230/968]	Time 0.320 (0.399)	Data 1.85e-04 (3.15e-03)	Tok/s 64471 (71738)	Loss/tok 2.9811 (3.1537)	LR 2.000e-03
0: TRAIN [4][240/968]	Time 0.211 (0.396)	Data 1.68e-04 (3.03e-03)	Tok/s 50202 (71474)	Loss/tok 2.5625 (3.1501)	LR 2.000e-03
0: TRAIN [4][250/968]	Time 0.317 (0.394)	Data 1.58e-04 (2.92e-03)	Tok/s 64582 (71349)	Loss/tok 2.9732 (3.1467)	LR 2.000e-03
0: TRAIN [4][260/968]	Time 0.549 (0.396)	Data 1.52e-04 (2.81e-03)	Tok/s 84484 (71485)	Loss/tok 3.3403 (3.1494)	LR 2.000e-03
0: TRAIN [4][270/968]	Time 0.320 (0.397)	Data 2.06e-04 (2.71e-03)	Tok/s 65817 (71530)	Loss/tok 2.9859 (3.1513)	LR 2.000e-03
0: TRAIN [4][280/968]	Time 0.687 (0.397)	Data 1.69e-04 (2.63e-03)	Tok/s 86425 (71555)	Loss/tok 3.6781 (3.1544)	LR 2.000e-03
0: TRAIN [4][290/968]	Time 0.317 (0.396)	Data 2.19e-04 (2.54e-03)	Tok/s 65277 (71488)	Loss/tok 2.9810 (3.1531)	LR 2.000e-03
0: TRAIN [4][300/968]	Time 0.209 (0.396)	Data 1.83e-04 (2.46e-03)	Tok/s 50352 (71542)	Loss/tok 2.6189 (3.1536)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [4][310/968]	Time 0.210 (0.395)	Data 2.00e-04 (2.39e-03)	Tok/s 49477 (71418)	Loss/tok 2.5296 (3.1514)	LR 2.000e-03
0: TRAIN [4][320/968]	Time 0.318 (0.394)	Data 1.81e-04 (2.32e-03)	Tok/s 64999 (71299)	Loss/tok 2.9846 (3.1499)	LR 2.000e-03
0: TRAIN [4][330/968]	Time 0.550 (0.394)	Data 1.73e-04 (2.26e-03)	Tok/s 85555 (71298)	Loss/tok 3.2884 (3.1495)	LR 2.000e-03
0: TRAIN [4][340/968]	Time 0.550 (0.395)	Data 2.14e-04 (2.20e-03)	Tok/s 84905 (71412)	Loss/tok 3.3402 (3.1520)	LR 2.000e-03
0: TRAIN [4][350/968]	Time 0.432 (0.395)	Data 1.72e-04 (2.14e-03)	Tok/s 77964 (71516)	Loss/tok 3.0891 (3.1522)	LR 2.000e-03
0: TRAIN [4][360/968]	Time 0.316 (0.394)	Data 2.15e-04 (2.09e-03)	Tok/s 66006 (71464)	Loss/tok 2.9677 (3.1510)	LR 2.000e-03
0: TRAIN [4][370/968]	Time 0.550 (0.396)	Data 1.93e-04 (2.03e-03)	Tok/s 84539 (71569)	Loss/tok 3.3382 (3.1546)	LR 2.000e-03
0: TRAIN [4][380/968]	Time 0.548 (0.395)	Data 1.96e-04 (1.99e-03)	Tok/s 84773 (71527)	Loss/tok 3.3386 (3.1554)	LR 2.000e-03
0: TRAIN [4][390/968]	Time 0.684 (0.396)	Data 2.04e-04 (1.94e-03)	Tok/s 87603 (71543)	Loss/tok 3.5298 (3.1579)	LR 2.000e-03
0: TRAIN [4][400/968]	Time 0.430 (0.395)	Data 1.96e-04 (1.90e-03)	Tok/s 77804 (71510)	Loss/tok 3.1507 (3.1567)	LR 2.000e-03
0: TRAIN [4][410/968]	Time 0.318 (0.394)	Data 1.77e-04 (1.86e-03)	Tok/s 65370 (71413)	Loss/tok 2.9341 (3.1538)	LR 2.000e-03
0: TRAIN [4][420/968]	Time 0.428 (0.393)	Data 2.00e-04 (1.82e-03)	Tok/s 79078 (71332)	Loss/tok 3.2161 (3.1520)	LR 2.000e-03
0: TRAIN [4][430/968]	Time 0.318 (0.393)	Data 1.80e-04 (1.78e-03)	Tok/s 65814 (71402)	Loss/tok 2.9235 (3.1518)	LR 2.000e-03
0: TRAIN [4][440/968]	Time 0.431 (0.393)	Data 2.27e-04 (1.74e-03)	Tok/s 77339 (71383)	Loss/tok 3.1773 (3.1515)	LR 2.000e-03
0: TRAIN [4][450/968]	Time 0.317 (0.393)	Data 2.09e-04 (1.71e-03)	Tok/s 66471 (71335)	Loss/tok 2.9321 (3.1530)	LR 2.000e-03
0: TRAIN [4][460/968]	Time 0.430 (0.395)	Data 2.11e-04 (1.68e-03)	Tok/s 77855 (71513)	Loss/tok 3.1721 (3.1577)	LR 2.000e-03
0: TRAIN [4][470/968]	Time 0.318 (0.394)	Data 2.25e-04 (1.64e-03)	Tok/s 64596 (71440)	Loss/tok 3.0035 (3.1563)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [4][480/968]	Time 0.320 (0.393)	Data 1.86e-04 (1.61e-03)	Tok/s 64036 (71375)	Loss/tok 2.9304 (3.1550)	LR 2.000e-03
0: TRAIN [4][490/968]	Time 0.319 (0.393)	Data 2.16e-04 (1.59e-03)	Tok/s 65721 (71344)	Loss/tok 3.0536 (3.1546)	LR 2.000e-03
0: TRAIN [4][500/968]	Time 0.430 (0.393)	Data 1.86e-04 (1.56e-03)	Tok/s 78966 (71393)	Loss/tok 3.1010 (3.1568)	LR 2.000e-03
0: TRAIN [4][510/968]	Time 0.424 (0.394)	Data 1.98e-04 (1.53e-03)	Tok/s 79062 (71421)	Loss/tok 3.1978 (3.1566)	LR 2.000e-03
0: TRAIN [4][520/968]	Time 0.690 (0.395)	Data 2.36e-04 (1.51e-03)	Tok/s 85525 (71520)	Loss/tok 3.5711 (3.1609)	LR 2.000e-03
0: TRAIN [4][530/968]	Time 0.209 (0.394)	Data 1.95e-04 (1.48e-03)	Tok/s 50802 (71453)	Loss/tok 2.5474 (3.1597)	LR 2.000e-03
0: TRAIN [4][540/968]	Time 0.551 (0.394)	Data 1.77e-04 (1.46e-03)	Tok/s 84589 (71468)	Loss/tok 3.3584 (3.1602)	LR 2.000e-03
0: TRAIN [4][550/968]	Time 0.432 (0.394)	Data 1.83e-04 (1.43e-03)	Tok/s 76138 (71475)	Loss/tok 3.1559 (3.1597)	LR 2.000e-03
0: TRAIN [4][560/968]	Time 0.319 (0.395)	Data 1.78e-04 (1.41e-03)	Tok/s 64204 (71558)	Loss/tok 2.9952 (3.1609)	LR 2.000e-03
0: TRAIN [4][570/968]	Time 0.319 (0.394)	Data 1.83e-04 (1.39e-03)	Tok/s 65074 (71490)	Loss/tok 2.9889 (3.1598)	LR 2.000e-03
0: TRAIN [4][580/968]	Time 0.317 (0.394)	Data 1.84e-04 (1.37e-03)	Tok/s 65196 (71458)	Loss/tok 2.9783 (3.1583)	LR 2.000e-03
0: TRAIN [4][590/968]	Time 0.210 (0.392)	Data 2.21e-04 (1.35e-03)	Tok/s 49154 (71342)	Loss/tok 2.5625 (3.1561)	LR 2.000e-03
0: TRAIN [4][600/968]	Time 0.552 (0.393)	Data 2.01e-04 (1.33e-03)	Tok/s 84507 (71406)	Loss/tok 3.2665 (3.1566)	LR 2.000e-03
0: TRAIN [4][610/968]	Time 0.432 (0.393)	Data 2.00e-04 (1.31e-03)	Tok/s 77433 (71452)	Loss/tok 3.2307 (3.1574)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [4][620/968]	Time 0.319 (0.393)	Data 2.20e-04 (1.29e-03)	Tok/s 64261 (71377)	Loss/tok 2.9063 (3.1568)	LR 2.000e-03
0: TRAIN [4][630/968]	Time 0.433 (0.392)	Data 1.97e-04 (1.28e-03)	Tok/s 77019 (71285)	Loss/tok 3.1999 (3.1550)	LR 2.000e-03
0: TRAIN [4][640/968]	Time 0.429 (0.391)	Data 2.04e-04 (1.26e-03)	Tok/s 78112 (71201)	Loss/tok 3.2088 (3.1534)	LR 2.000e-03
0: TRAIN [4][650/968]	Time 0.429 (0.392)	Data 1.81e-04 (1.24e-03)	Tok/s 77463 (71267)	Loss/tok 3.1733 (3.1550)	LR 2.000e-03
0: TRAIN [4][660/968]	Time 0.545 (0.392)	Data 2.03e-04 (1.23e-03)	Tok/s 86361 (71319)	Loss/tok 3.4021 (3.1566)	LR 2.000e-03
0: TRAIN [4][670/968]	Time 0.317 (0.392)	Data 1.74e-04 (1.21e-03)	Tok/s 65178 (71275)	Loss/tok 2.9261 (3.1557)	LR 2.000e-03
0: TRAIN [4][680/968]	Time 0.317 (0.393)	Data 2.05e-04 (1.20e-03)	Tok/s 65077 (71337)	Loss/tok 2.9161 (3.1580)	LR 2.000e-03
0: TRAIN [4][690/968]	Time 0.212 (0.392)	Data 1.90e-04 (1.18e-03)	Tok/s 49461 (71288)	Loss/tok 2.5798 (3.1578)	LR 2.000e-03
0: TRAIN [4][700/968]	Time 0.547 (0.392)	Data 1.93e-04 (1.17e-03)	Tok/s 85338 (71286)	Loss/tok 3.4046 (3.1581)	LR 2.000e-03
0: TRAIN [4][710/968]	Time 0.318 (0.393)	Data 1.74e-04 (1.15e-03)	Tok/s 64834 (71354)	Loss/tok 2.9691 (3.1606)	LR 2.000e-03
0: TRAIN [4][720/968]	Time 0.320 (0.392)	Data 1.91e-04 (1.14e-03)	Tok/s 65092 (71306)	Loss/tok 2.9119 (3.1602)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [4][730/968]	Time 0.428 (0.393)	Data 1.94e-04 (1.13e-03)	Tok/s 77225 (71376)	Loss/tok 3.1241 (3.1616)	LR 2.000e-03
0: TRAIN [4][740/968]	Time 0.430 (0.393)	Data 2.00e-04 (1.12e-03)	Tok/s 78486 (71391)	Loss/tok 3.1309 (3.1616)	LR 2.000e-03
0: TRAIN [4][750/968]	Time 0.433 (0.393)	Data 2.10e-04 (1.10e-03)	Tok/s 77515 (71334)	Loss/tok 3.2216 (3.1602)	LR 2.000e-03
0: TRAIN [4][760/968]	Time 0.321 (0.392)	Data 2.05e-04 (1.09e-03)	Tok/s 64908 (71289)	Loss/tok 2.9258 (3.1592)	LR 2.000e-03
0: TRAIN [4][770/968]	Time 0.549 (0.392)	Data 1.90e-04 (1.08e-03)	Tok/s 84626 (71293)	Loss/tok 3.3003 (3.1587)	LR 2.000e-03
0: TRAIN [4][780/968]	Time 0.317 (0.392)	Data 1.81e-04 (1.07e-03)	Tok/s 65613 (71274)	Loss/tok 2.9327 (3.1582)	LR 2.000e-03
0: TRAIN [4][790/968]	Time 0.435 (0.393)	Data 2.02e-04 (1.06e-03)	Tok/s 76343 (71328)	Loss/tok 3.1371 (3.1598)	LR 2.000e-03
0: TRAIN [4][800/968]	Time 0.428 (0.392)	Data 1.96e-04 (1.05e-03)	Tok/s 78410 (71299)	Loss/tok 3.1902 (3.1587)	LR 2.000e-03
0: TRAIN [4][810/968]	Time 0.317 (0.392)	Data 1.96e-04 (1.04e-03)	Tok/s 66492 (71259)	Loss/tok 2.9055 (3.1583)	LR 2.000e-03
0: TRAIN [4][820/968]	Time 0.214 (0.392)	Data 2.32e-04 (1.03e-03)	Tok/s 49551 (71246)	Loss/tok 2.5299 (3.1588)	LR 2.000e-03
0: TRAIN [4][830/968]	Time 0.429 (0.393)	Data 1.91e-04 (1.02e-03)	Tok/s 78627 (71327)	Loss/tok 3.1605 (3.1615)	LR 2.000e-03
0: TRAIN [4][840/968]	Time 0.317 (0.393)	Data 2.19e-04 (1.01e-03)	Tok/s 64540 (71363)	Loss/tok 2.9955 (3.1611)	LR 2.000e-03
0: TRAIN [4][850/968]	Time 0.316 (0.393)	Data 1.69e-04 (9.97e-04)	Tok/s 66291 (71394)	Loss/tok 3.0212 (3.1610)	LR 2.000e-03
0: TRAIN [4][860/968]	Time 0.319 (0.393)	Data 2.07e-04 (9.87e-04)	Tok/s 64272 (71387)	Loss/tok 2.9617 (3.1615)	LR 2.000e-03
0: TRAIN [4][870/968]	Time 0.429 (0.393)	Data 1.89e-04 (9.78e-04)	Tok/s 77434 (71348)	Loss/tok 3.1497 (3.1615)	LR 2.000e-03
0: TRAIN [4][880/968]	Time 0.429 (0.393)	Data 1.86e-04 (9.69e-04)	Tok/s 78649 (71401)	Loss/tok 3.2473 (3.1623)	LR 2.000e-03
0: TRAIN [4][890/968]	Time 0.318 (0.393)	Data 1.68e-04 (9.61e-04)	Tok/s 64377 (71395)	Loss/tok 3.0566 (3.1617)	LR 2.000e-03
0: TRAIN [4][900/968]	Time 0.686 (0.394)	Data 1.86e-04 (9.52e-04)	Tok/s 86583 (71439)	Loss/tok 3.4138 (3.1629)	LR 2.000e-03
0: TRAIN [4][910/968]	Time 0.208 (0.393)	Data 1.85e-04 (9.44e-04)	Tok/s 51213 (71399)	Loss/tok 2.4926 (3.1620)	LR 2.000e-03
0: TRAIN [4][920/968]	Time 0.317 (0.393)	Data 2.11e-04 (9.36e-04)	Tok/s 65250 (71414)	Loss/tok 2.9055 (3.1626)	LR 2.000e-03
0: TRAIN [4][930/968]	Time 0.317 (0.393)	Data 1.70e-04 (9.28e-04)	Tok/s 65066 (71410)	Loss/tok 3.0403 (3.1625)	LR 2.000e-03
0: TRAIN [4][940/968]	Time 0.429 (0.393)	Data 1.91e-04 (9.20e-04)	Tok/s 78991 (71355)	Loss/tok 3.1429 (3.1612)	LR 2.000e-03
0: TRAIN [4][950/968]	Time 0.317 (0.393)	Data 1.99e-04 (9.12e-04)	Tok/s 65141 (71363)	Loss/tok 3.0343 (3.1611)	LR 2.000e-03
0: TRAIN [4][960/968]	Time 0.683 (0.393)	Data 1.71e-04 (9.05e-04)	Tok/s 87332 (71420)	Loss/tok 3.4809 (3.1627)	LR 2.000e-03
:::MLL 1581975615.850 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 525}}
:::MLL 1581975615.850 eval_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [4][0/3]	Time 0.599 (0.599)	Decoder iters 101.0 (101.0)	Tok/s 26841 (26841)
0: Running moses detokenizer
0: BLEU(score=23.788229825974096, counts=[36256, 18144, 10316, 6132], totals=[64108, 61105, 58103, 55106], precisions=[56.55456417295813, 29.69315113329515, 17.75467703905134, 11.127644902551447], bp=0.9911790858999818, sys_len=64108, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1581975617.613 eval_accuracy: {"value": 23.79, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 536}}
:::MLL 1581975617.614 eval_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 4	Training Loss: 3.1609	Test BLEU: 23.79
0: Performance: Epoch: 4	Training: 571338 Tok/s
0: Finished epoch 4
:::MLL 1581975617.614 block_stop: {"value": null, "metadata": {"first_epoch_num": 5, "file": "train.py", "lineno": 558}}
:::MLL 1581975617.615 block_start: {"value": null, "metadata": {"first_epoch_num": 6, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1581975617.615 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 515}}
0: Starting epoch 5
0: Executing preallocation
0: Sampler for epoch 5 uses seed 3952903265
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [5][0/968]	Time 1.051 (1.051)	Data 7.16e-01 (7.16e-01)	Tok/s 19819 (19819)	Loss/tok 2.8867 (2.8867)	LR 2.000e-03
0: TRAIN [5][10/968]	Time 0.549 (0.458)	Data 1.62e-04 (6.53e-02)	Tok/s 85165 (68099)	Loss/tok 3.1854 (3.0484)	LR 2.000e-03
0: TRAIN [5][20/968]	Time 0.320 (0.397)	Data 1.97e-04 (3.43e-02)	Tok/s 65032 (67255)	Loss/tok 2.9041 (3.0006)	LR 2.000e-03
0: TRAIN [5][30/968]	Time 0.320 (0.401)	Data 2.01e-04 (2.33e-02)	Tok/s 64561 (69333)	Loss/tok 2.9490 (3.0412)	LR 2.000e-03
0: TRAIN [5][40/968]	Time 0.544 (0.413)	Data 2.50e-04 (1.77e-02)	Tok/s 85104 (70701)	Loss/tok 3.2578 (3.0889)	LR 2.000e-03
0: TRAIN [5][50/968]	Time 0.692 (0.413)	Data 1.48e-04 (1.42e-02)	Tok/s 86111 (70990)	Loss/tok 3.4682 (3.0945)	LR 2.000e-03
0: TRAIN [5][60/968]	Time 0.317 (0.397)	Data 1.83e-04 (1.19e-02)	Tok/s 65374 (69927)	Loss/tok 2.9359 (3.0750)	LR 2.000e-03
0: TRAIN [5][70/968]	Time 0.689 (0.396)	Data 1.58e-04 (1.03e-02)	Tok/s 86242 (70110)	Loss/tok 3.3837 (3.0732)	LR 2.000e-03
0: TRAIN [5][80/968]	Time 0.317 (0.397)	Data 1.80e-04 (9.02e-03)	Tok/s 64921 (70231)	Loss/tok 2.9291 (3.0807)	LR 2.000e-03
0: TRAIN [5][90/968]	Time 0.429 (0.389)	Data 1.77e-04 (8.05e-03)	Tok/s 79177 (69727)	Loss/tok 3.1627 (3.0725)	LR 2.000e-03
0: TRAIN [5][100/968]	Time 0.318 (0.383)	Data 1.82e-04 (7.27e-03)	Tok/s 65314 (69315)	Loss/tok 2.8698 (3.0648)	LR 2.000e-03
0: TRAIN [5][110/968]	Time 0.427 (0.383)	Data 1.54e-04 (6.63e-03)	Tok/s 78486 (69450)	Loss/tok 3.1445 (3.0626)	LR 2.000e-03
0: TRAIN [5][120/968]	Time 0.316 (0.383)	Data 1.66e-04 (6.09e-03)	Tok/s 65749 (69740)	Loss/tok 2.8803 (3.0620)	LR 2.000e-03
0: TRAIN [5][130/968]	Time 0.429 (0.386)	Data 1.62e-04 (5.64e-03)	Tok/s 78967 (70111)	Loss/tok 3.1784 (3.0699)	LR 2.000e-03
0: TRAIN [5][140/968]	Time 0.428 (0.388)	Data 1.78e-04 (5.25e-03)	Tok/s 78251 (70368)	Loss/tok 3.1464 (3.0747)	LR 2.000e-03
0: TRAIN [5][150/968]	Time 0.319 (0.388)	Data 1.49e-04 (4.91e-03)	Tok/s 64451 (70490)	Loss/tok 2.8892 (3.0744)	LR 2.000e-03
0: TRAIN [5][160/968]	Time 0.211 (0.385)	Data 1.67e-04 (4.62e-03)	Tok/s 50249 (70279)	Loss/tok 2.5840 (3.0698)	LR 2.000e-03
0: TRAIN [5][170/968]	Time 0.317 (0.387)	Data 1.51e-04 (4.36e-03)	Tok/s 65511 (70523)	Loss/tok 2.8455 (3.0724)	LR 2.000e-03
0: TRAIN [5][180/968]	Time 0.550 (0.389)	Data 1.80e-04 (4.13e-03)	Tok/s 84133 (70861)	Loss/tok 3.3222 (3.0759)	LR 2.000e-03
0: TRAIN [5][190/968]	Time 0.316 (0.388)	Data 1.54e-04 (3.92e-03)	Tok/s 65803 (70834)	Loss/tok 2.8711 (3.0735)	LR 2.000e-03
0: TRAIN [5][200/968]	Time 0.429 (0.388)	Data 1.48e-04 (3.73e-03)	Tok/s 77641 (70850)	Loss/tok 3.1089 (3.0750)	LR 2.000e-03
0: TRAIN [5][210/968]	Time 0.209 (0.387)	Data 1.68e-04 (3.56e-03)	Tok/s 51734 (70742)	Loss/tok 2.4898 (3.0758)	LR 2.000e-03
0: TRAIN [5][220/968]	Time 0.547 (0.388)	Data 1.68e-04 (3.41e-03)	Tok/s 84995 (70875)	Loss/tok 3.3214 (3.0785)	LR 2.000e-03
0: TRAIN [5][230/968]	Time 0.429 (0.386)	Data 1.60e-04 (3.27e-03)	Tok/s 78131 (70710)	Loss/tok 3.1289 (3.0750)	LR 2.000e-03
0: TRAIN [5][240/968]	Time 0.318 (0.386)	Data 1.64e-04 (3.14e-03)	Tok/s 65445 (70606)	Loss/tok 2.9147 (3.0758)	LR 2.000e-03
0: TRAIN [5][250/968]	Time 0.316 (0.386)	Data 1.73e-04 (3.02e-03)	Tok/s 64808 (70681)	Loss/tok 2.9711 (3.0751)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][260/968]	Time 0.687 (0.388)	Data 1.72e-04 (2.91e-03)	Tok/s 86385 (70827)	Loss/tok 3.5275 (3.0834)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][270/968]	Time 0.687 (0.388)	Data 1.69e-04 (2.81e-03)	Tok/s 86035 (70814)	Loss/tok 3.5316 (3.0861)	LR 2.000e-03
0: TRAIN [5][280/968]	Time 0.318 (0.387)	Data 1.68e-04 (2.72e-03)	Tok/s 64930 (70734)	Loss/tok 2.9707 (3.0835)	LR 2.000e-03
0: TRAIN [5][290/968]	Time 0.317 (0.389)	Data 1.57e-04 (2.63e-03)	Tok/s 65057 (70887)	Loss/tok 2.9231 (3.0895)	LR 2.000e-03
0: TRAIN [5][300/968]	Time 0.688 (0.390)	Data 1.79e-04 (2.55e-03)	Tok/s 86695 (70949)	Loss/tok 3.5199 (3.0927)	LR 2.000e-03
0: TRAIN [5][310/968]	Time 0.428 (0.390)	Data 1.54e-04 (2.47e-03)	Tok/s 78619 (71006)	Loss/tok 3.1196 (3.0939)	LR 2.000e-03
0: TRAIN [5][320/968]	Time 0.318 (0.390)	Data 1.56e-04 (2.40e-03)	Tok/s 65229 (70988)	Loss/tok 2.9265 (3.0942)	LR 2.000e-03
0: TRAIN [5][330/968]	Time 0.431 (0.391)	Data 1.60e-04 (2.33e-03)	Tok/s 77326 (71051)	Loss/tok 3.0960 (3.0974)	LR 2.000e-03
0: TRAIN [5][340/968]	Time 0.318 (0.392)	Data 1.65e-04 (2.27e-03)	Tok/s 65466 (71138)	Loss/tok 2.9008 (3.0987)	LR 2.000e-03
0: TRAIN [5][350/968]	Time 0.430 (0.390)	Data 1.62e-04 (2.21e-03)	Tok/s 78936 (71051)	Loss/tok 3.0870 (3.0954)	LR 2.000e-03
0: TRAIN [5][360/968]	Time 0.426 (0.391)	Data 1.56e-04 (2.15e-03)	Tok/s 79403 (71126)	Loss/tok 3.1293 (3.0968)	LR 2.000e-03
0: TRAIN [5][370/968]	Time 0.552 (0.392)	Data 1.90e-04 (2.10e-03)	Tok/s 85861 (71264)	Loss/tok 3.2478 (3.1000)	LR 2.000e-03
0: TRAIN [5][380/968]	Time 0.321 (0.392)	Data 1.54e-04 (2.05e-03)	Tok/s 64840 (71257)	Loss/tok 2.8772 (3.1010)	LR 2.000e-03
0: TRAIN [5][390/968]	Time 0.430 (0.394)	Data 1.75e-04 (2.00e-03)	Tok/s 77796 (71415)	Loss/tok 3.1321 (3.1041)	LR 2.000e-03
0: TRAIN [5][400/968]	Time 0.552 (0.394)	Data 1.61e-04 (1.96e-03)	Tok/s 85296 (71497)	Loss/tok 3.2809 (3.1047)	LR 2.000e-03
0: TRAIN [5][410/968]	Time 0.320 (0.395)	Data 1.57e-04 (1.91e-03)	Tok/s 64098 (71593)	Loss/tok 3.0198 (3.1052)	LR 2.000e-03
0: TRAIN [5][420/968]	Time 0.316 (0.395)	Data 1.72e-04 (1.87e-03)	Tok/s 65305 (71597)	Loss/tok 2.8864 (3.1056)	LR 2.000e-03
0: TRAIN [5][430/968]	Time 0.321 (0.395)	Data 1.50e-04 (1.83e-03)	Tok/s 64127 (71577)	Loss/tok 2.9276 (3.1053)	LR 2.000e-03
0: TRAIN [5][440/968]	Time 0.431 (0.395)	Data 1.57e-04 (1.79e-03)	Tok/s 78421 (71646)	Loss/tok 3.1288 (3.1065)	LR 2.000e-03
0: TRAIN [5][450/968]	Time 0.549 (0.396)	Data 1.86e-04 (1.76e-03)	Tok/s 84830 (71666)	Loss/tok 3.2795 (3.1101)	LR 2.000e-03
0: TRAIN [5][460/968]	Time 0.317 (0.397)	Data 1.63e-04 (1.72e-03)	Tok/s 64011 (71720)	Loss/tok 2.9550 (3.1121)	LR 2.000e-03
0: TRAIN [5][470/968]	Time 0.551 (0.396)	Data 1.69e-04 (1.69e-03)	Tok/s 84544 (71698)	Loss/tok 3.3546 (3.1115)	LR 2.000e-03
0: TRAIN [5][480/968]	Time 0.430 (0.396)	Data 1.43e-04 (1.66e-03)	Tok/s 78524 (71715)	Loss/tok 3.1770 (3.1118)	LR 2.000e-03
0: TRAIN [5][490/968]	Time 0.432 (0.396)	Data 1.74e-04 (1.63e-03)	Tok/s 78422 (71637)	Loss/tok 3.0678 (3.1106)	LR 2.000e-03
0: TRAIN [5][500/968]	Time 0.688 (0.396)	Data 1.66e-04 (1.60e-03)	Tok/s 86503 (71651)	Loss/tok 3.4679 (3.1119)	LR 2.000e-03
0: TRAIN [5][510/968]	Time 0.551 (0.395)	Data 1.89e-04 (1.57e-03)	Tok/s 84722 (71586)	Loss/tok 3.3083 (3.1116)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [5][520/968]	Time 0.321 (0.395)	Data 1.62e-04 (1.54e-03)	Tok/s 64618 (71570)	Loss/tok 2.9350 (3.1110)	LR 2.000e-03
0: TRAIN [5][530/968]	Time 0.551 (0.395)	Data 1.59e-04 (1.52e-03)	Tok/s 84801 (71587)	Loss/tok 3.2776 (3.1115)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][540/968]	Time 0.427 (0.395)	Data 1.78e-04 (1.49e-03)	Tok/s 78713 (71576)	Loss/tok 3.1101 (3.1112)	LR 2.000e-03
0: TRAIN [5][550/968]	Time 0.317 (0.394)	Data 1.74e-04 (1.47e-03)	Tok/s 65589 (71500)	Loss/tok 3.0115 (3.1096)	LR 2.000e-03
0: TRAIN [5][560/968]	Time 0.430 (0.394)	Data 1.56e-04 (1.44e-03)	Tok/s 77718 (71496)	Loss/tok 3.1439 (3.1094)	LR 2.000e-03
0: TRAIN [5][570/968]	Time 0.429 (0.395)	Data 1.64e-04 (1.42e-03)	Tok/s 78087 (71587)	Loss/tok 3.0072 (3.1110)	LR 2.000e-03
0: TRAIN [5][580/968]	Time 0.430 (0.395)	Data 1.53e-04 (1.40e-03)	Tok/s 77680 (71641)	Loss/tok 3.1410 (3.1128)	LR 2.000e-03
0: TRAIN [5][590/968]	Time 0.430 (0.395)	Data 1.56e-04 (1.38e-03)	Tok/s 77970 (71603)	Loss/tok 3.1109 (3.1118)	LR 2.000e-03
0: TRAIN [5][600/968]	Time 0.427 (0.395)	Data 1.68e-04 (1.36e-03)	Tok/s 78531 (71612)	Loss/tok 3.0886 (3.1113)	LR 2.000e-03
0: TRAIN [5][610/968]	Time 0.318 (0.395)	Data 1.65e-04 (1.34e-03)	Tok/s 64170 (71599)	Loss/tok 2.9175 (3.1125)	LR 2.000e-03
0: TRAIN [5][620/968]	Time 0.686 (0.395)	Data 1.62e-04 (1.32e-03)	Tok/s 86326 (71604)	Loss/tok 3.5298 (3.1137)	LR 2.000e-03
0: TRAIN [5][630/968]	Time 0.318 (0.394)	Data 1.47e-04 (1.30e-03)	Tok/s 64860 (71529)	Loss/tok 2.9188 (3.1126)	LR 2.000e-03
0: TRAIN [5][640/968]	Time 0.431 (0.394)	Data 1.87e-04 (1.28e-03)	Tok/s 77271 (71511)	Loss/tok 3.1311 (3.1122)	LR 2.000e-03
0: TRAIN [5][650/968]	Time 0.549 (0.395)	Data 2.06e-04 (1.27e-03)	Tok/s 85311 (71535)	Loss/tok 3.3351 (3.1147)	LR 2.000e-03
0: TRAIN [5][660/968]	Time 0.318 (0.394)	Data 1.61e-04 (1.25e-03)	Tok/s 64369 (71490)	Loss/tok 2.9661 (3.1145)	LR 2.000e-03
0: TRAIN [5][670/968]	Time 0.552 (0.395)	Data 1.63e-04 (1.23e-03)	Tok/s 85118 (71538)	Loss/tok 3.2515 (3.1160)	LR 2.000e-03
0: TRAIN [5][680/968]	Time 0.430 (0.395)	Data 1.81e-04 (1.22e-03)	Tok/s 77738 (71545)	Loss/tok 3.1465 (3.1156)	LR 2.000e-03
0: TRAIN [5][690/968]	Time 0.430 (0.395)	Data 1.90e-04 (1.20e-03)	Tok/s 78272 (71557)	Loss/tok 3.0767 (3.1151)	LR 2.000e-03
0: TRAIN [5][700/968]	Time 0.431 (0.394)	Data 1.48e-04 (1.19e-03)	Tok/s 78101 (71467)	Loss/tok 3.1090 (3.1141)	LR 2.000e-03
0: TRAIN [5][710/968]	Time 0.547 (0.395)	Data 1.57e-04 (1.17e-03)	Tok/s 86460 (71557)	Loss/tok 3.1915 (3.1168)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][720/968]	Time 0.431 (0.396)	Data 1.50e-04 (1.16e-03)	Tok/s 77650 (71601)	Loss/tok 3.1171 (3.1186)	LR 2.000e-03
0: TRAIN [5][730/968]	Time 0.322 (0.396)	Data 1.65e-04 (1.15e-03)	Tok/s 64007 (71609)	Loss/tok 2.8997 (3.1185)	LR 2.000e-03
0: TRAIN [5][740/968]	Time 0.428 (0.395)	Data 1.62e-04 (1.13e-03)	Tok/s 78523 (71587)	Loss/tok 3.1092 (3.1174)	LR 2.000e-03
0: TRAIN [5][750/968]	Time 0.548 (0.395)	Data 1.46e-04 (1.12e-03)	Tok/s 85084 (71578)	Loss/tok 3.2608 (3.1178)	LR 2.000e-03
0: TRAIN [5][760/968]	Time 0.318 (0.395)	Data 1.66e-04 (1.11e-03)	Tok/s 65394 (71544)	Loss/tok 2.9594 (3.1170)	LR 2.000e-03
0: TRAIN [5][770/968]	Time 0.429 (0.395)	Data 1.61e-04 (1.10e-03)	Tok/s 78067 (71529)	Loss/tok 3.1918 (3.1176)	LR 2.000e-03
0: TRAIN [5][780/968]	Time 0.317 (0.395)	Data 1.83e-04 (1.08e-03)	Tok/s 65627 (71561)	Loss/tok 2.8643 (3.1183)	LR 2.000e-03
0: TRAIN [5][790/968]	Time 0.211 (0.395)	Data 1.72e-04 (1.07e-03)	Tok/s 50480 (71586)	Loss/tok 2.5115 (3.1193)	LR 2.000e-03
0: TRAIN [5][800/968]	Time 0.320 (0.396)	Data 1.78e-04 (1.06e-03)	Tok/s 64260 (71618)	Loss/tok 2.9641 (3.1200)	LR 2.000e-03
0: TRAIN [5][810/968]	Time 0.552 (0.396)	Data 1.51e-04 (1.05e-03)	Tok/s 84125 (71642)	Loss/tok 3.3199 (3.1204)	LR 2.000e-03
0: TRAIN [5][820/968]	Time 0.427 (0.396)	Data 1.94e-04 (1.04e-03)	Tok/s 78866 (71633)	Loss/tok 3.1156 (3.1203)	LR 2.000e-03
0: TRAIN [5][830/968]	Time 0.320 (0.396)	Data 1.71e-04 (1.03e-03)	Tok/s 62932 (71659)	Loss/tok 2.9451 (3.1209)	LR 2.000e-03
0: TRAIN [5][840/968]	Time 0.548 (0.396)	Data 1.49e-04 (1.02e-03)	Tok/s 85862 (71645)	Loss/tok 3.3015 (3.1204)	LR 2.000e-03
0: TRAIN [5][850/968]	Time 0.318 (0.396)	Data 1.65e-04 (1.01e-03)	Tok/s 65024 (71649)	Loss/tok 2.9405 (3.1205)	LR 2.000e-03
0: TRAIN [5][860/968]	Time 0.319 (0.396)	Data 1.72e-04 (9.99e-04)	Tok/s 65492 (71649)	Loss/tok 2.9209 (3.1211)	LR 2.000e-03
0: TRAIN [5][870/968]	Time 0.318 (0.396)	Data 1.53e-04 (9.90e-04)	Tok/s 64681 (71649)	Loss/tok 2.9032 (3.1216)	LR 2.000e-03
0: TRAIN [5][880/968]	Time 0.317 (0.396)	Data 1.67e-04 (9.80e-04)	Tok/s 65524 (71617)	Loss/tok 2.9730 (3.1218)	LR 2.000e-03
0: TRAIN [5][890/968]	Time 0.319 (0.395)	Data 1.73e-04 (9.71e-04)	Tok/s 64701 (71568)	Loss/tok 2.9412 (3.1215)	LR 2.000e-03
0: TRAIN [5][900/968]	Time 0.429 (0.395)	Data 1.62e-04 (9.62e-04)	Tok/s 77289 (71541)	Loss/tok 3.1272 (3.1209)	LR 2.000e-03
0: TRAIN [5][910/968]	Time 0.550 (0.395)	Data 1.47e-04 (9.53e-04)	Tok/s 84279 (71548)	Loss/tok 3.2654 (3.1208)	LR 2.000e-03
0: TRAIN [5][920/968]	Time 0.210 (0.394)	Data 1.50e-04 (9.45e-04)	Tok/s 50259 (71496)	Loss/tok 2.5472 (3.1200)	LR 2.000e-03
0: TRAIN [5][930/968]	Time 0.551 (0.395)	Data 1.66e-04 (9.36e-04)	Tok/s 85330 (71528)	Loss/tok 3.2965 (3.1201)	LR 2.000e-03
0: TRAIN [5][940/968]	Time 0.211 (0.393)	Data 1.71e-04 (9.28e-04)	Tok/s 49699 (71393)	Loss/tok 2.5594 (3.1184)	LR 2.000e-03
0: TRAIN [5][950/968]	Time 0.552 (0.393)	Data 1.60e-04 (9.20e-04)	Tok/s 84119 (71397)	Loss/tok 3.4119 (3.1182)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][960/968]	Time 0.317 (0.393)	Data 1.64e-04 (9.13e-04)	Tok/s 65342 (71391)	Loss/tok 2.9275 (3.1188)	LR 2.000e-03
:::MLL 1581975999.725 epoch_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 525}}
:::MLL 1581975999.725 eval_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [5][0/3]	Time 0.692 (0.692)	Decoder iters 133.0 (133.0)	Tok/s 23858 (23858)
0: Running moses detokenizer
0: BLEU(score=23.683628533837826, counts=[36890, 18386, 10434, 6172], totals=[65636, 62633, 59630, 56633], precisions=[56.20391248704979, 29.355132278511327, 17.497903739728326, 10.898239542316317], bp=1.0, sys_len=65636, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1581976001.620 eval_accuracy: {"value": 23.68, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 536}}
:::MLL 1581976001.620 eval_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 5	Training Loss: 3.1191	Test BLEU: 23.68
0: Performance: Epoch: 5	Training: 571201 Tok/s
0: Finished epoch 5
:::MLL 1581976001.621 block_stop: {"value": null, "metadata": {"first_epoch_num": 6, "file": "train.py", "lineno": 558}}
:::MLL 1581976001.621 block_start: {"value": null, "metadata": {"first_epoch_num": 7, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1581976001.621 epoch_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 515}}
0: Starting epoch 6
0: Executing preallocation
0: Sampler for epoch 6 uses seed 4144341636
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [6][0/968]	Time 1.151 (1.151)	Data 7.00e-01 (7.00e-01)	Tok/s 29310 (29310)	Loss/tok 3.1055 (3.1055)	LR 2.000e-03
0: TRAIN [6][10/968]	Time 0.431 (0.447)	Data 1.77e-04 (6.38e-02)	Tok/s 78242 (66144)	Loss/tok 2.9958 (3.0144)	LR 2.000e-03
0: TRAIN [6][20/968]	Time 0.432 (0.432)	Data 1.54e-04 (3.35e-02)	Tok/s 78230 (69310)	Loss/tok 2.9761 (3.0421)	LR 2.000e-03
0: TRAIN [6][30/968]	Time 0.317 (0.434)	Data 1.80e-04 (2.27e-02)	Tok/s 64850 (70697)	Loss/tok 2.8781 (3.0721)	LR 2.000e-03
0: TRAIN [6][40/968]	Time 0.318 (0.423)	Data 1.55e-04 (1.72e-02)	Tok/s 63900 (70756)	Loss/tok 2.8777 (3.0679)	LR 2.000e-03
0: TRAIN [6][50/968]	Time 0.315 (0.420)	Data 1.66e-04 (1.39e-02)	Tok/s 65425 (71256)	Loss/tok 2.8495 (3.0744)	LR 2.000e-03
0: TRAIN [6][60/968]	Time 0.549 (0.421)	Data 1.58e-04 (1.16e-02)	Tok/s 85202 (71741)	Loss/tok 3.2523 (3.0767)	LR 2.000e-03
0: TRAIN [6][70/968]	Time 0.318 (0.413)	Data 1.60e-04 (1.00e-02)	Tok/s 66034 (71394)	Loss/tok 2.8527 (3.0665)	LR 2.000e-03
0: TRAIN [6][80/968]	Time 0.210 (0.408)	Data 1.66e-04 (8.81e-03)	Tok/s 49938 (71223)	Loss/tok 2.4946 (3.0623)	LR 2.000e-03
0: TRAIN [6][90/968]	Time 0.318 (0.412)	Data 1.48e-04 (7.86e-03)	Tok/s 64712 (71822)	Loss/tok 2.8654 (3.0693)	LR 2.000e-03
0: TRAIN [6][100/968]	Time 0.209 (0.403)	Data 1.76e-04 (7.10e-03)	Tok/s 50008 (71088)	Loss/tok 2.5318 (3.0579)	LR 2.000e-03
0: TRAIN [6][110/968]	Time 0.429 (0.407)	Data 1.76e-04 (6.47e-03)	Tok/s 76708 (71491)	Loss/tok 3.1117 (3.0668)	LR 2.000e-03
0: TRAIN [6][120/968]	Time 0.428 (0.404)	Data 1.52e-04 (5.95e-03)	Tok/s 78888 (71241)	Loss/tok 3.1497 (3.0651)	LR 2.000e-03
0: TRAIN [6][130/968]	Time 0.427 (0.406)	Data 1.58e-04 (5.51e-03)	Tok/s 80055 (71559)	Loss/tok 2.9706 (3.0694)	LR 2.000e-03
0: TRAIN [6][140/968]	Time 0.319 (0.401)	Data 1.81e-04 (5.13e-03)	Tok/s 65880 (71225)	Loss/tok 2.9388 (3.0633)	LR 2.000e-03
0: TRAIN [6][150/968]	Time 0.319 (0.403)	Data 1.73e-04 (4.80e-03)	Tok/s 65453 (71413)	Loss/tok 2.8268 (3.0667)	LR 2.000e-03
0: TRAIN [6][160/968]	Time 0.552 (0.402)	Data 1.59e-04 (4.51e-03)	Tok/s 85404 (71431)	Loss/tok 3.1503 (3.0668)	LR 2.000e-03
0: TRAIN [6][170/968]	Time 0.318 (0.403)	Data 1.78e-04 (4.26e-03)	Tok/s 64686 (71599)	Loss/tok 2.9062 (3.0664)	LR 2.000e-03
0: TRAIN [6][180/968]	Time 0.431 (0.403)	Data 1.51e-04 (4.03e-03)	Tok/s 77356 (71722)	Loss/tok 3.0701 (3.0678)	LR 2.000e-03
0: TRAIN [6][190/968]	Time 0.318 (0.403)	Data 1.46e-04 (3.83e-03)	Tok/s 64931 (71860)	Loss/tok 2.9328 (3.0677)	LR 2.000e-03
0: TRAIN [6][200/968]	Time 0.427 (0.402)	Data 1.79e-04 (3.65e-03)	Tok/s 78461 (71808)	Loss/tok 3.0462 (3.0656)	LR 2.000e-03
0: TRAIN [6][210/968]	Time 0.552 (0.401)	Data 1.44e-04 (3.48e-03)	Tok/s 84498 (71765)	Loss/tok 3.2627 (3.0639)	LR 2.000e-03
0: TRAIN [6][220/968]	Time 0.317 (0.401)	Data 1.47e-04 (3.33e-03)	Tok/s 66110 (71845)	Loss/tok 2.9320 (3.0661)	LR 2.000e-03
0: TRAIN [6][230/968]	Time 0.317 (0.402)	Data 1.65e-04 (3.19e-03)	Tok/s 64926 (71904)	Loss/tok 2.8645 (3.0690)	LR 2.000e-03
0: TRAIN [6][240/968]	Time 0.686 (0.402)	Data 1.72e-04 (3.07e-03)	Tok/s 86577 (71906)	Loss/tok 3.3744 (3.0692)	LR 2.000e-03
0: TRAIN [6][250/968]	Time 0.319 (0.402)	Data 1.73e-04 (2.95e-03)	Tok/s 64712 (71950)	Loss/tok 2.8657 (3.0694)	LR 2.000e-03
0: TRAIN [6][260/968]	Time 0.431 (0.401)	Data 1.63e-04 (2.84e-03)	Tok/s 77978 (71961)	Loss/tok 3.1433 (3.0680)	LR 2.000e-03
0: TRAIN [6][270/968]	Time 0.431 (0.400)	Data 1.69e-04 (2.74e-03)	Tok/s 77830 (71906)	Loss/tok 3.0916 (3.0671)	LR 2.000e-03
0: TRAIN [6][280/968]	Time 0.547 (0.401)	Data 1.49e-04 (2.65e-03)	Tok/s 85375 (72073)	Loss/tok 3.2466 (3.0684)	LR 2.000e-03
0: TRAIN [6][290/968]	Time 0.316 (0.398)	Data 1.76e-04 (2.57e-03)	Tok/s 65224 (71772)	Loss/tok 2.8678 (3.0649)	LR 2.000e-03
0: TRAIN [6][300/968]	Time 0.429 (0.399)	Data 1.86e-04 (2.49e-03)	Tok/s 79178 (71852)	Loss/tok 3.0381 (3.0651)	LR 2.000e-03
0: TRAIN [6][310/968]	Time 0.689 (0.398)	Data 1.55e-04 (2.41e-03)	Tok/s 86700 (71784)	Loss/tok 3.3802 (3.0660)	LR 2.000e-03
0: TRAIN [6][320/968]	Time 0.318 (0.396)	Data 1.44e-04 (2.34e-03)	Tok/s 64444 (71599)	Loss/tok 2.9623 (3.0640)	LR 2.000e-03
0: TRAIN [6][330/968]	Time 0.318 (0.395)	Data 1.57e-04 (2.28e-03)	Tok/s 64937 (71560)	Loss/tok 2.9023 (3.0628)	LR 2.000e-03
0: TRAIN [6][340/968]	Time 0.315 (0.397)	Data 1.51e-04 (2.21e-03)	Tok/s 65514 (71621)	Loss/tok 2.9363 (3.0694)	LR 2.000e-03
0: TRAIN [6][350/968]	Time 0.430 (0.399)	Data 1.39e-04 (2.16e-03)	Tok/s 78551 (71724)	Loss/tok 3.0529 (3.0731)	LR 2.000e-03
0: TRAIN [6][360/968]	Time 0.210 (0.397)	Data 1.45e-04 (2.10e-03)	Tok/s 50725 (71628)	Loss/tok 2.4926 (3.0713)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [6][370/968]	Time 0.319 (0.399)	Data 1.74e-04 (2.05e-03)	Tok/s 65306 (71800)	Loss/tok 2.8270 (3.0749)	LR 2.000e-03
0: TRAIN [6][380/968]	Time 0.321 (0.397)	Data 1.67e-04 (2.00e-03)	Tok/s 64077 (71606)	Loss/tok 2.9196 (3.0726)	LR 2.000e-03
0: TRAIN [6][390/968]	Time 0.209 (0.397)	Data 1.82e-04 (1.95e-03)	Tok/s 50087 (71656)	Loss/tok 2.5832 (3.0727)	LR 2.000e-03
0: TRAIN [6][400/968]	Time 0.428 (0.396)	Data 1.63e-04 (1.91e-03)	Tok/s 78163 (71614)	Loss/tok 2.9939 (3.0713)	LR 2.000e-03
0: TRAIN [6][410/968]	Time 0.207 (0.396)	Data 1.80e-04 (1.86e-03)	Tok/s 51332 (71552)	Loss/tok 2.5991 (3.0714)	LR 2.000e-03
0: TRAIN [6][420/968]	Time 0.429 (0.396)	Data 1.60e-04 (1.82e-03)	Tok/s 78198 (71649)	Loss/tok 3.0902 (3.0728)	LR 2.000e-03
0: TRAIN [6][430/968]	Time 0.553 (0.396)	Data 1.64e-04 (1.78e-03)	Tok/s 84561 (71641)	Loss/tok 3.2762 (3.0732)	LR 2.000e-03
0: TRAIN [6][440/968]	Time 0.211 (0.396)	Data 1.68e-04 (1.75e-03)	Tok/s 50411 (71589)	Loss/tok 2.5936 (3.0727)	LR 2.000e-03
0: TRAIN [6][450/968]	Time 0.319 (0.398)	Data 1.85e-04 (1.72e-03)	Tok/s 65406 (71728)	Loss/tok 2.8656 (3.0772)	LR 2.000e-03
0: TRAIN [6][460/968]	Time 0.686 (0.399)	Data 1.73e-04 (1.68e-03)	Tok/s 86712 (71805)	Loss/tok 3.4699 (3.0799)	LR 2.000e-03
0: TRAIN [6][470/968]	Time 0.686 (0.398)	Data 1.77e-04 (1.65e-03)	Tok/s 87022 (71669)	Loss/tok 3.4334 (3.0791)	LR 2.000e-03
0: TRAIN [6][480/968]	Time 0.321 (0.398)	Data 1.76e-04 (1.62e-03)	Tok/s 64636 (71688)	Loss/tok 2.9466 (3.0797)	LR 2.000e-03
0: TRAIN [6][490/968]	Time 0.431 (0.398)	Data 1.91e-04 (1.59e-03)	Tok/s 77993 (71722)	Loss/tok 3.1277 (3.0795)	LR 2.000e-03
0: TRAIN [6][500/968]	Time 0.431 (0.398)	Data 1.79e-04 (1.56e-03)	Tok/s 78436 (71781)	Loss/tok 3.0562 (3.0793)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [6][510/968]	Time 0.213 (0.397)	Data 1.55e-04 (1.53e-03)	Tok/s 49244 (71648)	Loss/tok 2.5493 (3.0784)	LR 2.000e-03
0: TRAIN [6][520/968]	Time 0.212 (0.396)	Data 1.65e-04 (1.51e-03)	Tok/s 50117 (71581)	Loss/tok 2.4851 (3.0771)	LR 2.000e-03
0: TRAIN [6][530/968]	Time 0.320 (0.397)	Data 1.50e-04 (1.48e-03)	Tok/s 63966 (71638)	Loss/tok 2.8293 (3.0797)	LR 2.000e-03
0: TRAIN [6][540/968]	Time 0.319 (0.396)	Data 1.64e-04 (1.46e-03)	Tok/s 64293 (71612)	Loss/tok 2.8598 (3.0783)	LR 2.000e-03
0: TRAIN [6][550/968]	Time 0.320 (0.397)	Data 1.48e-04 (1.43e-03)	Tok/s 65538 (71697)	Loss/tok 3.0106 (3.0803)	LR 2.000e-03
0: TRAIN [6][560/968]	Time 0.317 (0.397)	Data 1.64e-04 (1.41e-03)	Tok/s 65268 (71681)	Loss/tok 2.9033 (3.0798)	LR 2.000e-03
0: TRAIN [6][570/968]	Time 0.319 (0.397)	Data 1.46e-04 (1.39e-03)	Tok/s 65391 (71670)	Loss/tok 2.9903 (3.0794)	LR 2.000e-03
0: TRAIN [6][580/968]	Time 0.320 (0.397)	Data 1.76e-04 (1.37e-03)	Tok/s 64326 (71681)	Loss/tok 2.8507 (3.0804)	LR 2.000e-03
0: TRAIN [6][590/968]	Time 0.688 (0.397)	Data 1.50e-04 (1.35e-03)	Tok/s 86246 (71657)	Loss/tok 3.5266 (3.0818)	LR 2.000e-03
0: TRAIN [6][600/968]	Time 0.320 (0.397)	Data 1.66e-04 (1.33e-03)	Tok/s 64892 (71648)	Loss/tok 2.9424 (3.0838)	LR 2.000e-03
0: TRAIN [6][610/968]	Time 0.321 (0.397)	Data 1.67e-04 (1.31e-03)	Tok/s 64628 (71635)	Loss/tok 2.8543 (3.0833)	LR 2.000e-03
0: TRAIN [6][620/968]	Time 0.317 (0.398)	Data 1.50e-04 (1.29e-03)	Tok/s 65494 (71728)	Loss/tok 2.8632 (3.0861)	LR 2.000e-03
0: TRAIN [6][630/968]	Time 0.428 (0.400)	Data 1.55e-04 (1.27e-03)	Tok/s 78434 (71873)	Loss/tok 3.1129 (3.0900)	LR 2.000e-03
0: TRAIN [6][640/968]	Time 0.550 (0.402)	Data 1.61e-04 (1.26e-03)	Tok/s 84730 (72001)	Loss/tok 3.2387 (3.0929)	LR 2.000e-03
0: TRAIN [6][650/968]	Time 0.317 (0.401)	Data 1.54e-04 (1.24e-03)	Tok/s 65890 (71946)	Loss/tok 2.8770 (3.0916)	LR 2.000e-03
0: TRAIN [6][660/968]	Time 0.322 (0.401)	Data 1.57e-04 (1.22e-03)	Tok/s 63124 (71929)	Loss/tok 2.8374 (3.0904)	LR 2.000e-03
0: TRAIN [6][670/968]	Time 0.429 (0.400)	Data 1.63e-04 (1.21e-03)	Tok/s 78162 (71894)	Loss/tok 3.1450 (3.0902)	LR 2.000e-03
0: TRAIN [6][680/968]	Time 0.429 (0.401)	Data 1.55e-04 (1.19e-03)	Tok/s 78570 (71912)	Loss/tok 3.1017 (3.0909)	LR 1.000e-03
0: TRAIN [6][690/968]	Time 0.431 (0.400)	Data 1.68e-04 (1.18e-03)	Tok/s 78250 (71852)	Loss/tok 3.1109 (3.0894)	LR 1.000e-03
0: TRAIN [6][700/968]	Time 0.214 (0.399)	Data 1.57e-04 (1.16e-03)	Tok/s 50136 (71822)	Loss/tok 2.5428 (3.0884)	LR 1.000e-03
0: TRAIN [6][710/968]	Time 0.319 (0.399)	Data 1.44e-04 (1.15e-03)	Tok/s 64726 (71802)	Loss/tok 2.8712 (3.0870)	LR 1.000e-03
0: TRAIN [6][720/968]	Time 0.551 (0.400)	Data 1.56e-04 (1.14e-03)	Tok/s 85350 (71887)	Loss/tok 3.2048 (3.0894)	LR 1.000e-03
0: TRAIN [6][730/968]	Time 0.319 (0.399)	Data 1.52e-04 (1.12e-03)	Tok/s 63953 (71802)	Loss/tok 2.8530 (3.0880)	LR 1.000e-03
0: TRAIN [6][740/968]	Time 0.430 (0.399)	Data 1.73e-04 (1.11e-03)	Tok/s 77144 (71871)	Loss/tok 3.1584 (3.0885)	LR 1.000e-03
0: TRAIN [6][750/968]	Time 0.687 (0.400)	Data 1.78e-04 (1.10e-03)	Tok/s 86223 (71898)	Loss/tok 3.4633 (3.0899)	LR 1.000e-03
0: TRAIN [6][760/968]	Time 0.317 (0.399)	Data 1.56e-04 (1.08e-03)	Tok/s 64854 (71783)	Loss/tok 2.8923 (3.0889)	LR 1.000e-03
0: TRAIN [6][770/968]	Time 0.315 (0.398)	Data 1.70e-04 (1.07e-03)	Tok/s 66686 (71729)	Loss/tok 2.9441 (3.0873)	LR 1.000e-03
0: TRAIN [6][780/968]	Time 0.212 (0.398)	Data 1.68e-04 (1.06e-03)	Tok/s 50379 (71657)	Loss/tok 2.4085 (3.0869)	LR 1.000e-03
0: TRAIN [6][790/968]	Time 0.318 (0.397)	Data 1.53e-04 (1.05e-03)	Tok/s 64620 (71638)	Loss/tok 2.8677 (3.0864)	LR 1.000e-03
0: TRAIN [6][800/968]	Time 0.325 (0.397)	Data 1.54e-04 (1.04e-03)	Tok/s 63871 (71632)	Loss/tok 2.8388 (3.0860)	LR 1.000e-03
0: TRAIN [6][810/968]	Time 0.324 (0.397)	Data 1.58e-04 (1.03e-03)	Tok/s 62966 (71609)	Loss/tok 2.8355 (3.0848)	LR 1.000e-03
0: TRAIN [6][820/968]	Time 0.210 (0.396)	Data 1.50e-04 (1.02e-03)	Tok/s 49963 (71515)	Loss/tok 2.4917 (3.0834)	LR 1.000e-03
0: TRAIN [6][830/968]	Time 0.210 (0.395)	Data 1.53e-04 (1.01e-03)	Tok/s 50329 (71456)	Loss/tok 2.5077 (3.0820)	LR 1.000e-03
0: TRAIN [6][840/968]	Time 0.317 (0.396)	Data 1.70e-04 (9.97e-04)	Tok/s 65848 (71516)	Loss/tok 2.8632 (3.0821)	LR 1.000e-03
0: TRAIN [6][850/968]	Time 0.320 (0.395)	Data 1.82e-04 (9.87e-04)	Tok/s 64903 (71457)	Loss/tok 2.8811 (3.0810)	LR 1.000e-03
0: TRAIN [6][860/968]	Time 0.431 (0.395)	Data 1.85e-04 (9.78e-04)	Tok/s 77528 (71440)	Loss/tok 3.0451 (3.0811)	LR 1.000e-03
0: TRAIN [6][870/968]	Time 0.318 (0.395)	Data 1.53e-04 (9.69e-04)	Tok/s 65058 (71439)	Loss/tok 2.8747 (3.0806)	LR 1.000e-03
0: TRAIN [6][880/968]	Time 0.316 (0.395)	Data 1.74e-04 (9.59e-04)	Tok/s 64834 (71462)	Loss/tok 2.8251 (3.0801)	LR 1.000e-03
0: TRAIN [6][890/968]	Time 0.318 (0.395)	Data 1.79e-04 (9.50e-04)	Tok/s 64649 (71484)	Loss/tok 2.8895 (3.0806)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][900/968]	Time 0.317 (0.394)	Data 1.53e-04 (9.42e-04)	Tok/s 64913 (71376)	Loss/tok 2.8420 (3.0789)	LR 1.000e-03
0: TRAIN [6][910/968]	Time 0.319 (0.394)	Data 1.55e-04 (9.33e-04)	Tok/s 63964 (71395)	Loss/tok 2.9509 (3.0786)	LR 1.000e-03
0: TRAIN [6][920/968]	Time 0.554 (0.394)	Data 1.53e-04 (9.25e-04)	Tok/s 84321 (71344)	Loss/tok 3.1488 (3.0775)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [6][930/968]	Time 0.320 (0.394)	Data 1.68e-04 (9.17e-04)	Tok/s 64261 (71353)	Loss/tok 2.8360 (3.0777)	LR 1.000e-03
0: TRAIN [6][940/968]	Time 0.551 (0.394)	Data 1.69e-04 (9.09e-04)	Tok/s 85210 (71394)	Loss/tok 3.1902 (3.0774)	LR 1.000e-03
0: TRAIN [6][950/968]	Time 0.316 (0.394)	Data 1.57e-04 (9.01e-04)	Tok/s 65084 (71402)	Loss/tok 2.8305 (3.0770)	LR 1.000e-03
0: TRAIN [6][960/968]	Time 0.431 (0.394)	Data 1.50e-04 (8.93e-04)	Tok/s 77821 (71394)	Loss/tok 3.0997 (3.0771)	LR 1.000e-03
:::MLL 1581976383.935 epoch_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 525}}
:::MLL 1581976383.936 eval_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [6][0/3]	Time 0.603 (0.603)	Decoder iters 97.0 (97.0)	Tok/s 27157 (27157)
0: Running moses detokenizer
0: BLEU(score=24.362669261074938, counts=[37198, 18733, 10711, 6411], totals=[65304, 62301, 59298, 56300], precisions=[56.96128874188411, 30.06853822571066, 18.06300381125839, 11.38721136767318], bp=1.0, sys_len=65304, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1581976385.685 eval_accuracy: {"value": 24.36, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 536}}
:::MLL 1581976385.685 eval_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 6	Training Loss: 3.0757	Test BLEU: 24.36
0: Performance: Epoch: 6	Training: 570892 Tok/s
0: Finished epoch 6
:::MLL 1581976385.686 block_stop: {"value": null, "metadata": {"first_epoch_num": 7, "file": "train.py", "lineno": 558}}
0: Closing preprocessed data file
:::MLL 1581976385.686 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 569}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-02-17 09:53:10 PM
RESULT,RNN_TRANSLATOR,,2705,nvidia,2020-02-17 09:08:05 PM
