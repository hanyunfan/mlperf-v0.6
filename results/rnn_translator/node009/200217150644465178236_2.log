Beginning trial 2 of 2
Gathering sys log on node009
:::MLL 1581976390.714 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1581976390.715 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1581976390.715 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1581976390.715 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1581976390.716 submission_platform: {"value": "1xDSS8440", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1581976390.716 submission_entry: {"value": "{'hardware': 'DSS8440', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'Ethernet 100 Gb/sec (4X EDR)', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.6-2.1.4', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz', 'num_cores': '40', 'num_vcpus': '40', 'accelerator': 'Tesla V100S-PCIE-32GB', 'num_accelerators': '8', 'sys_mem_size': '754 GB', 'sys_storage_type': 'SATA SSD', 'sys_storage_size': '1x 446.6G + 1x 931.5G', 'cpu_accel_interconnect': 'UPI', 'network_card': '', 'num_network_cards': '0', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1581976390.717 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1581976390.717 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1581976392.755 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node node009
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=5038' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=512 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=200217150644465178236 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_200217150644465178236 ./run_and_time.sh
Run vars: id 200217150644465178236 gpus 8 mparams  --master_port=5038
NCCL_SOCKET_NTHREADS=2
NCCL_NSOCKS_PERTHREAD=8
STARTING TIMING RUN AT 2020-02-17 09:53:13 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=512
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=0
+ MATH=amp_fp16
+ [[ 0 -eq 1 ]]
+ LAUNCH_OPT='torch.distributed.launch --nproc_per_node 8  --master_port=5038'
+ echo 'running benchmark'
running benchmark
+ python -m torch.distributed.launch --nproc_per_node 8 --master_port=5038 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 512 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1581976395.819 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1581976395.819 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1581976395.819 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1581976395.819 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1581976395.820 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1581976395.820 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1581976395.820 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1581976395.823 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=8, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=512, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 209663566
node009:1793:1793 [0] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1793:1793 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation

node009:1793:1793 [0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:1793:1793 [0] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:1793:1793 [0] NCCL INFO NET/IB : No device found.
node009:1793:1793 [0] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
NCCL version 2.5.6+cuda10.2
node009:1795:1795 [2] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1797:1797 [4] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1800:1800 [7] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1799:1799 [6] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1796:1796 [3] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1794:1794 [1] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1795:1795 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node009:1800:1800 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node009:1797:1797 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node009:1799:1799 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node009:1796:1796 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node009:1794:1794 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node009:1798:1798 [5] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1798:1798 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation

node009:1795:1795 [2] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:1800:1800 [7] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:1799:1799 [6] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:1797:1797 [4] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:1795:1795 [2] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0

node009:1800:1800 [7] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0

node009:1799:1799 [6] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:1795:1795 [2] NCCL INFO NET/IB : No device found.

node009:1797:1797 [4] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:1800:1800 [7] NCCL INFO NET/IB : No device found.
node009:1799:1799 [6] NCCL INFO NET/IB : No device found.
node009:1797:1797 [4] NCCL INFO NET/IB : No device found.

node009:1796:1796 [3] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:1794:1794 [1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:1796:1796 [3] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0

node009:1794:1794 [1] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:1796:1796 [3] NCCL INFO NET/IB : No device found.
node009:1794:1794 [1] NCCL INFO NET/IB : No device found.

node009:1798:1798 [5] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:1798:1798 [5] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:1798:1798 [5] NCCL INFO NET/IB : No device found.
node009:1799:1799 [6] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1800:1800 [7] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1795:1795 [2] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1794:1794 [1] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1797:1797 [4] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1796:1796 [3] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1798:1798 [5] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1793:2155 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555
node009:1794:2156 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555
node009:1798:2157 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa
node009:1796:2158 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555
node009:1799:2160 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa
node009:1800:2161 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa
node009:1797:2162 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa
node009:1795:2159 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555
node009:1796:2158 [3] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:1796:2158 [3] NCCL INFO include/net.h:19 -> 2
node009:1795:2159 [2] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:1795:2159 [2] NCCL INFO include/net.h:19 -> 2
node009:1794:2156 [1] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:1794:2156 [1] NCCL INFO include/net.h:19 -> 2
node009:1796:2158 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:1800:2161 [7] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:1800:2161 [7] NCCL INFO include/net.h:19 -> 2
node009:1793:2155 [0] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:1793:2155 [0] NCCL INFO include/net.h:19 -> 2
node009:1798:2157 [5] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:1799:2160 [6] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:1797:2162 [4] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:1798:2157 [5] NCCL INFO include/net.h:19 -> 2
node009:1799:2160 [6] NCCL INFO include/net.h:19 -> 2
node009:1797:2162 [4] NCCL INFO include/net.h:19 -> 2
node009:1795:2159 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:1794:2156 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:1798:2157 [5] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:1800:2161 [7] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:1793:2155 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:1799:2160 [6] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:1797:2162 [4] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:1793:2155 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
node009:1794:2156 [1] NCCL INFO Threads per block : 512/640/256
node009:1795:2159 [2] NCCL INFO Threads per block : 512/640/256
node009:1793:2155 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
node009:1794:2156 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:1795:2159 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:1796:2158 [3] NCCL INFO Threads per block : 512/640/256
node009:1793:2155 [0] NCCL INFO Threads per block : 512/640/256
node009:1794:2156 [1] NCCL INFO Trees [0] 2/-1/-1->1->0|0->1->2/-1/-1 [1] 2/-1/-1->1->0|0->1->2/-1/-1
node009:1795:2159 [2] NCCL INFO Trees [0] 3/-1/-1->2->1|1->2->3/-1/-1 [1] 3/-1/-1->2->1|1->2->3/-1/-1
node009:1796:2158 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:1796:2158 [3] NCCL INFO Trees [0] 4/-1/-1->3->2|2->3->4/-1/-1 [1] 4/-1/-1->3->2|2->3->4/-1/-1
node009:1793:2155 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:1793:2155 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 1/-1/-1->0->-1|-1->0->1/-1/-1
node009:1798:2157 [5] NCCL INFO Threads per block : 512/640/256
node009:1797:2162 [4] NCCL INFO Threads per block : 512/640/256
node009:1798:2157 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:1797:2162 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:1798:2157 [5] NCCL INFO Trees [0] 6/-1/-1->5->4|4->5->6/-1/-1 [1] 6/-1/-1->5->4|4->5->6/-1/-1
node009:1797:2162 [4] NCCL INFO Trees [0] 5/-1/-1->4->3|3->4->5/-1/-1 [1] 5/-1/-1->4->3|3->4->5/-1/-1
node009:1799:2160 [6] NCCL INFO Threads per block : 512/640/256
node009:1799:2160 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:1799:2160 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1
node009:1800:2161 [7] NCCL INFO Threads per block : 512/640/256
node009:1800:2161 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:1800:2161 [7] NCCL INFO Trees [0] -1/-1/-1->7->6|6->7->-1/-1/-1 [1] -1/-1/-1->7->6|6->7->-1/-1/-1
node009:1795:2159 [2] NCCL INFO Ring 00 : 2[47000] -> 3[48000] via P2P/IPC
node009:1794:2156 [1] NCCL INFO Ring 00 : 1[11000] -> 2[47000] via P2P/IPC
node009:1796:2158 [3] NCCL INFO Ring 00 : 3[48000] -> 4[89000] via direct shared memory
node009:1798:2157 [5] NCCL INFO Ring 00 : 5[8a000] -> 6[c1000] via P2P/IPC
node009:1799:2160 [6] NCCL INFO Ring 00 : 6[c1000] -> 7[c2000] via P2P/IPC
node009:1800:2161 [7] NCCL INFO Ring 00 : 7[c2000] -> 0[10000] via direct shared memory
node009:1793:2155 [0] NCCL INFO Ring 00 : 0[10000] -> 1[11000] via P2P/IPC
node009:1797:2162 [4] NCCL INFO Ring 00 : 4[89000] -> 5[8a000] via P2P/IPC
node009:1795:2159 [2] NCCL INFO Ring 00 : 2[47000] -> 1[11000] via P2P/IPC
node009:1800:2161 [7] NCCL INFO Ring 00 : 7[c2000] -> 6[c1000] via P2P/IPC
node009:1794:2156 [1] NCCL INFO Ring 00 : 1[11000] -> 0[10000] via P2P/IPC
node009:1799:2160 [6] NCCL INFO Ring 00 : 6[c1000] -> 5[8a000] via P2P/IPC
node009:1798:2157 [5] NCCL INFO Ring 00 : 5[8a000] -> 4[89000] via P2P/IPC
node009:1797:2162 [4] NCCL INFO Ring 00 : 4[89000] -> 3[48000] via direct shared memory
node009:1796:2158 [3] NCCL INFO Ring 00 : 3[48000] -> 2[47000] via P2P/IPC
node009:1794:2156 [1] NCCL INFO Ring 01 : 1[11000] -> 2[47000] via P2P/IPC
node009:1800:2161 [7] NCCL INFO Ring 01 : 7[c2000] -> 0[10000] via direct shared memory
node009:1799:2160 [6] NCCL INFO Ring 01 : 6[c1000] -> 7[c2000] via P2P/IPC
node009:1793:2155 [0] NCCL INFO Ring 01 : 0[10000] -> 1[11000] via P2P/IPC
node009:1795:2159 [2] NCCL INFO Ring 01 : 2[47000] -> 3[48000] via P2P/IPC
node009:1798:2157 [5] NCCL INFO Ring 01 : 5[8a000] -> 6[c1000] via P2P/IPC
node009:1796:2158 [3] NCCL INFO Ring 01 : 3[48000] -> 4[89000] via direct shared memory
node009:1800:2161 [7] NCCL INFO Ring 01 : 7[c2000] -> 6[c1000] via P2P/IPC
node009:1794:2156 [1] NCCL INFO Ring 01 : 1[11000] -> 0[10000] via P2P/IPC
node009:1797:2162 [4] NCCL INFO Ring 01 : 4[89000] -> 5[8a000] via P2P/IPC
node009:1799:2160 [6] NCCL INFO Ring 01 : 6[c1000] -> 5[8a000] via P2P/IPC
node009:1795:2159 [2] NCCL INFO Ring 01 : 2[47000] -> 1[11000] via P2P/IPC
node009:1798:2157 [5] NCCL INFO Ring 01 : 5[8a000] -> 4[89000] via P2P/IPC
node009:1797:2162 [4] NCCL INFO Ring 01 : 4[89000] -> 3[48000] via direct shared memory
node009:1796:2158 [3] NCCL INFO Ring 01 : 3[48000] -> 2[47000] via P2P/IPC
node009:1800:2161 [7] NCCL INFO comm 0x7ffe98007570 rank 7 nranks 8 cudaDev 7 busId c2000 - Init COMPLETE
node009:1793:2155 [0] NCCL INFO comm 0x7ffe38007570 rank 0 nranks 8 cudaDev 0 busId 10000 - Init COMPLETE
node009:1793:1793 [0] NCCL INFO Launch mode Parallel
node009:1794:2156 [1] NCCL INFO comm 0x7fff50007570 rank 1 nranks 8 cudaDev 1 busId 11000 - Init COMPLETE
0: Worker 0 is using worker seed: 1099170562
0: Building vocabulary from /data/vocab.bpe.32000
node009:1799:2160 [6] NCCL INFO comm 0x7ffe98007570 rank 6 nranks 8 cudaDev 6 busId c1000 - Init COMPLETE
node009:1798:2157 [5] NCCL INFO comm 0x7ffe90007570 rank 5 nranks 8 cudaDev 5 busId 8a000 - Init COMPLETE
0: Size of vocabulary: 32320
node009:1795:2159 [2] NCCL INFO comm 0x7fff38007570 rank 2 nranks 8 cudaDev 2 busId 47000 - Init COMPLETE
node009:1797:2162 [4] NCCL INFO comm 0x7ffe98007570 rank 4 nranks 8 cudaDev 4 busId 89000 - Init COMPLETE
node009:1796:2158 [3] NCCL INFO comm 0x7fff54007570 rank 3 nranks 8 cudaDev 3 busId 48000 - Init COMPLETE
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1581976405.493 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1581976406.973 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 407}}
:::MLL 1581976406.974 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 409}}
:::MLL 1581976406.974 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 414}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1581976408.030 global_batch_size: {"value": 4096, "metadata": {"file": "train.py", "lineno": 459}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 8, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 8
:::MLL 1581976408.033 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1581976408.034 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1581976408.034 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1581976408.034 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1581976408.034 opt_learning_rate_decay_steps: {"value": 8, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1581976408.035 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1581976408.035 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1581976408.035 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1581976408.036 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 515}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 901113374
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/968]	Time 1.117 (1.117)	Data 9.04e-01 (9.04e-01)	Tok/s 9378 (9378)	Loss/tok 10.4700 (10.4700)	LR 2.000e-05
0: TRAIN [0][10/968]	Time 0.555 (0.529)	Data 2.10e-04 (8.25e-02)	Tok/s 84116 (69154)	Loss/tok 9.7350 (10.1568)	LR 2.518e-05
0: TRAIN [0][20/968]	Time 0.429 (0.450)	Data 1.52e-04 (4.33e-02)	Tok/s 78598 (69384)	Loss/tok 9.3097 (9.8656)	LR 3.170e-05
0: TRAIN [0][30/968]	Time 0.317 (0.422)	Data 2.10e-04 (2.94e-02)	Tok/s 65870 (69463)	Loss/tok 8.9103 (9.6492)	LR 3.991e-05
0: TRAIN [0][40/968]	Time 0.428 (0.413)	Data 1.56e-04 (2.23e-02)	Tok/s 78094 (70162)	Loss/tok 8.7912 (9.4628)	LR 5.024e-05
0: TRAIN [0][50/968]	Time 0.210 (0.399)	Data 1.62e-04 (1.79e-02)	Tok/s 49558 (69670)	Loss/tok 8.1364 (9.3193)	LR 6.325e-05
0: TRAIN [0][60/968]	Time 0.318 (0.397)	Data 1.99e-04 (1.50e-02)	Tok/s 65847 (70109)	Loss/tok 8.1980 (9.1684)	LR 7.962e-05
0: TRAIN [0][70/968]	Time 0.429 (0.397)	Data 1.50e-04 (1.29e-02)	Tok/s 78658 (70577)	Loss/tok 8.1739 (9.0308)	LR 1.002e-04
0: TRAIN [0][80/968]	Time 0.319 (0.397)	Data 2.06e-04 (1.14e-02)	Tok/s 66021 (70881)	Loss/tok 7.8923 (8.9166)	LR 1.262e-04
0: TRAIN [0][90/968]	Time 0.314 (0.391)	Data 1.59e-04 (1.01e-02)	Tok/s 66222 (70398)	Loss/tok 7.8163 (8.8248)	LR 1.589e-04
0: TRAIN [0][100/968]	Time 0.686 (0.393)	Data 1.75e-04 (9.15e-03)	Tok/s 87287 (70602)	Loss/tok 8.2583 (8.7343)	LR 2.000e-04
0: TRAIN [0][110/968]	Time 0.429 (0.390)	Data 1.65e-04 (8.34e-03)	Tok/s 79126 (70575)	Loss/tok 7.9543 (8.6625)	LR 2.518e-04
0: TRAIN [0][120/968]	Time 0.319 (0.385)	Data 1.59e-04 (7.67e-03)	Tok/s 63844 (70152)	Loss/tok 7.6849 (8.6042)	LR 3.170e-04
0: TRAIN [0][130/968]	Time 0.549 (0.383)	Data 1.51e-04 (7.10e-03)	Tok/s 85739 (70016)	Loss/tok 7.9919 (8.5471)	LR 3.991e-04
0: TRAIN [0][140/968]	Time 0.316 (0.382)	Data 1.70e-04 (6.61e-03)	Tok/s 65778 (69956)	Loss/tok 7.5839 (8.4907)	LR 5.024e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][150/968]	Time 0.211 (0.379)	Data 1.70e-04 (6.18e-03)	Tok/s 48957 (69580)	Loss/tok 6.8087 (8.4458)	LR 6.181e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][160/968]	Time 0.429 (0.382)	Data 1.70e-04 (5.81e-03)	Tok/s 78996 (69768)	Loss/tok 7.7843 (8.4003)	LR 7.604e-04
0: TRAIN [0][170/968]	Time 0.318 (0.386)	Data 1.74e-04 (5.48e-03)	Tok/s 65755 (70049)	Loss/tok 7.4433 (8.3497)	LR 9.573e-04
0: TRAIN [0][180/968]	Time 0.550 (0.388)	Data 1.83e-04 (5.19e-03)	Tok/s 85186 (70255)	Loss/tok 7.5873 (8.2991)	LR 1.205e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
0: TRAIN [0][190/968]	Time 0.430 (0.386)	Data 1.71e-04 (4.92e-03)	Tok/s 77191 (70079)	Loss/tok 7.3910 (8.2554)	LR 1.483e-03
0: TRAIN [0][200/968]	Time 0.318 (0.386)	Data 1.80e-04 (4.69e-03)	Tok/s 64571 (70089)	Loss/tok 6.9607 (8.2006)	LR 1.867e-03
0: TRAIN [0][210/968]	Time 0.320 (0.385)	Data 1.76e-04 (4.47e-03)	Tok/s 64084 (69970)	Loss/tok 6.8542 (8.1457)	LR 2.000e-03
0: TRAIN [0][220/968]	Time 0.319 (0.383)	Data 1.57e-04 (4.28e-03)	Tok/s 64281 (69814)	Loss/tok 6.5802 (8.0918)	LR 2.000e-03
0: TRAIN [0][230/968]	Time 0.546 (0.384)	Data 1.61e-04 (4.10e-03)	Tok/s 84882 (69896)	Loss/tok 6.7569 (8.0273)	LR 2.000e-03
0: TRAIN [0][240/968]	Time 0.317 (0.384)	Data 1.57e-04 (3.94e-03)	Tok/s 65293 (69980)	Loss/tok 6.2354 (7.9592)	LR 2.000e-03
0: TRAIN [0][250/968]	Time 0.427 (0.388)	Data 1.64e-04 (3.79e-03)	Tok/s 78701 (70293)	Loss/tok 6.3646 (7.8793)	LR 2.000e-03
0: TRAIN [0][260/968]	Time 0.321 (0.390)	Data 1.54e-04 (3.65e-03)	Tok/s 64122 (70478)	Loss/tok 5.8891 (7.8042)	LR 2.000e-03
0: TRAIN [0][270/968]	Time 0.316 (0.390)	Data 1.90e-04 (3.52e-03)	Tok/s 66808 (70546)	Loss/tok 5.7401 (7.7395)	LR 2.000e-03
0: TRAIN [0][280/968]	Time 0.317 (0.390)	Data 1.65e-04 (3.40e-03)	Tok/s 65207 (70538)	Loss/tok 5.6409 (7.6772)	LR 2.000e-03
0: TRAIN [0][290/968]	Time 0.211 (0.389)	Data 1.55e-04 (3.29e-03)	Tok/s 49416 (70539)	Loss/tok 4.7499 (7.6161)	LR 2.000e-03
0: TRAIN [0][300/968]	Time 0.311 (0.388)	Data 1.70e-04 (3.19e-03)	Tok/s 66105 (70536)	Loss/tok 5.4153 (7.5570)	LR 2.000e-03
0: TRAIN [0][310/968]	Time 0.316 (0.387)	Data 1.58e-04 (3.09e-03)	Tok/s 66103 (70453)	Loss/tok 5.3107 (7.5010)	LR 2.000e-03
0: TRAIN [0][320/968]	Time 0.318 (0.388)	Data 1.88e-04 (3.00e-03)	Tok/s 64696 (70616)	Loss/tok 5.2369 (7.4332)	LR 2.000e-03
0: TRAIN [0][330/968]	Time 0.318 (0.388)	Data 1.66e-04 (2.91e-03)	Tok/s 65149 (70685)	Loss/tok 5.1143 (7.3709)	LR 2.000e-03
0: TRAIN [0][340/968]	Time 0.210 (0.387)	Data 1.73e-04 (2.83e-03)	Tok/s 50479 (70543)	Loss/tok 4.2673 (7.3171)	LR 2.000e-03
0: TRAIN [0][350/968]	Time 0.318 (0.388)	Data 2.05e-04 (2.76e-03)	Tok/s 65059 (70663)	Loss/tok 4.8931 (7.2505)	LR 2.000e-03
0: TRAIN [0][360/968]	Time 0.209 (0.387)	Data 1.80e-04 (2.69e-03)	Tok/s 50756 (70574)	Loss/tok 3.9236 (7.1980)	LR 2.000e-03
0: TRAIN [0][370/968]	Time 0.431 (0.388)	Data 1.50e-04 (2.62e-03)	Tok/s 78233 (70681)	Loss/tok 5.0764 (7.1351)	LR 2.000e-03
0: TRAIN [0][380/968]	Time 0.321 (0.388)	Data 2.04e-04 (2.55e-03)	Tok/s 63186 (70699)	Loss/tok 4.6972 (7.0785)	LR 2.000e-03
0: TRAIN [0][390/968]	Time 0.318 (0.388)	Data 1.70e-04 (2.49e-03)	Tok/s 64025 (70723)	Loss/tok 4.5225 (7.0209)	LR 2.000e-03
0: TRAIN [0][400/968]	Time 0.684 (0.389)	Data 1.76e-04 (2.44e-03)	Tok/s 86869 (70808)	Loss/tok 5.2403 (6.9587)	LR 2.000e-03
0: TRAIN [0][410/968]	Time 0.427 (0.389)	Data 1.61e-04 (2.38e-03)	Tok/s 79055 (70836)	Loss/tok 4.7068 (6.9015)	LR 2.000e-03
0: TRAIN [0][420/968]	Time 0.427 (0.389)	Data 1.58e-04 (2.33e-03)	Tok/s 78739 (70872)	Loss/tok 4.7757 (6.8478)	LR 2.000e-03
0: TRAIN [0][430/968]	Time 0.317 (0.389)	Data 1.54e-04 (2.28e-03)	Tok/s 64978 (70864)	Loss/tok 4.2989 (6.7972)	LR 2.000e-03
0: TRAIN [0][440/968]	Time 0.318 (0.389)	Data 1.81e-04 (2.23e-03)	Tok/s 64778 (70837)	Loss/tok 4.3528 (6.7461)	LR 2.000e-03
0: TRAIN [0][450/968]	Time 0.318 (0.390)	Data 1.50e-04 (2.18e-03)	Tok/s 65936 (70907)	Loss/tok 4.1419 (6.6924)	LR 2.000e-03
0: TRAIN [0][460/968]	Time 0.552 (0.391)	Data 2.09e-04 (2.14e-03)	Tok/s 83841 (71028)	Loss/tok 4.7173 (6.6354)	LR 2.000e-03
0: TRAIN [0][470/968]	Time 0.548 (0.391)	Data 1.49e-04 (2.10e-03)	Tok/s 84636 (71012)	Loss/tok 4.6400 (6.5892)	LR 2.000e-03
0: TRAIN [0][480/968]	Time 0.429 (0.391)	Data 1.74e-04 (2.06e-03)	Tok/s 77738 (71063)	Loss/tok 4.3566 (6.5404)	LR 2.000e-03
0: TRAIN [0][490/968]	Time 0.316 (0.390)	Data 1.72e-04 (2.02e-03)	Tok/s 66365 (71016)	Loss/tok 4.0401 (6.4992)	LR 2.000e-03
0: TRAIN [0][500/968]	Time 0.549 (0.392)	Data 1.79e-04 (1.98e-03)	Tok/s 84328 (71172)	Loss/tok 4.5915 (6.4441)	LR 2.000e-03
0: TRAIN [0][510/968]	Time 0.427 (0.392)	Data 1.68e-04 (1.95e-03)	Tok/s 77941 (71156)	Loss/tok 4.3453 (6.4026)	LR 2.000e-03
0: TRAIN [0][520/968]	Time 0.427 (0.392)	Data 1.83e-04 (1.91e-03)	Tok/s 79009 (71156)	Loss/tok 4.2872 (6.3614)	LR 2.000e-03
0: TRAIN [0][530/968]	Time 0.428 (0.393)	Data 1.96e-04 (1.88e-03)	Tok/s 78504 (71235)	Loss/tok 4.1021 (6.3163)	LR 2.000e-03
0: TRAIN [0][540/968]	Time 0.318 (0.393)	Data 1.54e-04 (1.85e-03)	Tok/s 64948 (71254)	Loss/tok 3.8504 (6.2766)	LR 2.000e-03
0: TRAIN [0][550/968]	Time 0.322 (0.391)	Data 1.60e-04 (1.82e-03)	Tok/s 63408 (71128)	Loss/tok 3.8334 (6.2453)	LR 2.000e-03
0: TRAIN [0][560/968]	Time 0.320 (0.390)	Data 1.80e-04 (1.79e-03)	Tok/s 63133 (71002)	Loss/tok 3.7967 (6.2149)	LR 2.000e-03
0: TRAIN [0][570/968]	Time 0.432 (0.390)	Data 1.59e-04 (1.76e-03)	Tok/s 77308 (71048)	Loss/tok 4.2167 (6.1763)	LR 2.000e-03
0: TRAIN [0][580/968]	Time 0.434 (0.390)	Data 1.62e-04 (1.73e-03)	Tok/s 77506 (71040)	Loss/tok 4.0801 (6.1417)	LR 2.000e-03
0: TRAIN [0][590/968]	Time 0.321 (0.390)	Data 1.71e-04 (1.71e-03)	Tok/s 64710 (71043)	Loss/tok 3.6890 (6.1065)	LR 2.000e-03
0: TRAIN [0][600/968]	Time 0.321 (0.391)	Data 1.67e-04 (1.68e-03)	Tok/s 64923 (71087)	Loss/tok 3.8538 (6.0693)	LR 2.000e-03
0: TRAIN [0][610/968]	Time 0.318 (0.391)	Data 1.57e-04 (1.66e-03)	Tok/s 64189 (71137)	Loss/tok 3.6752 (6.0336)	LR 2.000e-03
0: TRAIN [0][620/968]	Time 0.212 (0.391)	Data 1.60e-04 (1.63e-03)	Tok/s 49279 (71099)	Loss/tok 3.1249 (6.0024)	LR 2.000e-03
0: TRAIN [0][630/968]	Time 0.687 (0.392)	Data 1.68e-04 (1.61e-03)	Tok/s 85568 (71138)	Loss/tok 4.5344 (5.9677)	LR 2.000e-03
0: TRAIN [0][640/968]	Time 0.547 (0.393)	Data 1.99e-04 (1.59e-03)	Tok/s 84866 (71244)	Loss/tok 4.1712 (5.9304)	LR 2.000e-03
0: TRAIN [0][650/968]	Time 0.430 (0.393)	Data 1.79e-04 (1.57e-03)	Tok/s 78567 (71282)	Loss/tok 4.0426 (5.8991)	LR 2.000e-03
0: TRAIN [0][660/968]	Time 0.317 (0.393)	Data 1.51e-04 (1.54e-03)	Tok/s 64918 (71276)	Loss/tok 3.7593 (5.8705)	LR 2.000e-03
0: TRAIN [0][670/968]	Time 0.319 (0.393)	Data 1.84e-04 (1.52e-03)	Tok/s 64843 (71235)	Loss/tok 3.6970 (5.8436)	LR 2.000e-03
0: TRAIN [0][680/968]	Time 0.550 (0.392)	Data 1.59e-04 (1.50e-03)	Tok/s 85302 (71203)	Loss/tok 4.2486 (5.8176)	LR 2.000e-03
0: TRAIN [0][690/968]	Time 0.317 (0.392)	Data 1.59e-04 (1.49e-03)	Tok/s 64251 (71188)	Loss/tok 3.6297 (5.7912)	LR 2.000e-03
0: TRAIN [0][700/968]	Time 0.428 (0.391)	Data 1.89e-04 (1.47e-03)	Tok/s 78884 (71138)	Loss/tok 3.8954 (5.7674)	LR 2.000e-03
0: TRAIN [0][710/968]	Time 0.210 (0.390)	Data 1.45e-04 (1.45e-03)	Tok/s 50828 (71065)	Loss/tok 3.0624 (5.7450)	LR 2.000e-03
0: TRAIN [0][720/968]	Time 0.317 (0.391)	Data 1.66e-04 (1.43e-03)	Tok/s 64394 (71081)	Loss/tok 3.6288 (5.7176)	LR 2.000e-03
0: TRAIN [0][730/968]	Time 0.318 (0.391)	Data 1.93e-04 (1.41e-03)	Tok/s 65594 (71075)	Loss/tok 3.6920 (5.6930)	LR 2.000e-03
0: TRAIN [0][740/968]	Time 0.430 (0.390)	Data 1.55e-04 (1.40e-03)	Tok/s 78129 (71032)	Loss/tok 3.8618 (5.6702)	LR 2.000e-03
0: TRAIN [0][750/968]	Time 0.318 (0.391)	Data 1.72e-04 (1.38e-03)	Tok/s 64016 (71109)	Loss/tok 3.6226 (5.6407)	LR 2.000e-03
0: TRAIN [0][760/968]	Time 0.318 (0.391)	Data 1.77e-04 (1.36e-03)	Tok/s 65855 (71103)	Loss/tok 3.5708 (5.6171)	LR 2.000e-03
0: TRAIN [0][770/968]	Time 0.430 (0.391)	Data 1.75e-04 (1.35e-03)	Tok/s 78706 (71160)	Loss/tok 3.7109 (5.5916)	LR 2.000e-03
0: TRAIN [0][780/968]	Time 0.317 (0.391)	Data 1.53e-04 (1.33e-03)	Tok/s 64576 (71152)	Loss/tok 3.5187 (5.5694)	LR 2.000e-03
0: TRAIN [0][790/968]	Time 0.432 (0.392)	Data 1.63e-04 (1.32e-03)	Tok/s 77872 (71188)	Loss/tok 3.8917 (5.5457)	LR 2.000e-03
0: TRAIN [0][800/968]	Time 0.317 (0.391)	Data 1.67e-04 (1.30e-03)	Tok/s 64312 (71156)	Loss/tok 3.6086 (5.5254)	LR 2.000e-03
0: TRAIN [0][810/968]	Time 0.320 (0.392)	Data 1.55e-04 (1.29e-03)	Tok/s 65279 (71215)	Loss/tok 3.4447 (5.5010)	LR 2.000e-03
0: TRAIN [0][820/968]	Time 0.427 (0.392)	Data 1.73e-04 (1.28e-03)	Tok/s 78875 (71263)	Loss/tok 3.8697 (5.4786)	LR 2.000e-03
0: TRAIN [0][830/968]	Time 0.430 (0.392)	Data 1.74e-04 (1.26e-03)	Tok/s 77935 (71265)	Loss/tok 3.8037 (5.4582)	LR 2.000e-03
0: TRAIN [0][840/968]	Time 0.428 (0.393)	Data 1.68e-04 (1.25e-03)	Tok/s 78074 (71288)	Loss/tok 3.9031 (5.4372)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][850/968]	Time 0.551 (0.393)	Data 1.65e-04 (1.24e-03)	Tok/s 84605 (71335)	Loss/tok 3.9884 (5.4152)	LR 2.000e-03
0: TRAIN [0][860/968]	Time 0.316 (0.393)	Data 1.63e-04 (1.23e-03)	Tok/s 64665 (71323)	Loss/tok 3.5726 (5.3967)	LR 2.000e-03
0: TRAIN [0][870/968]	Time 0.319 (0.393)	Data 1.60e-04 (1.21e-03)	Tok/s 64490 (71388)	Loss/tok 3.4152 (5.3750)	LR 2.000e-03
0: TRAIN [0][880/968]	Time 0.686 (0.393)	Data 1.75e-04 (1.20e-03)	Tok/s 87625 (71324)	Loss/tok 4.1404 (5.3590)	LR 2.000e-03
0: TRAIN [0][890/968]	Time 0.317 (0.392)	Data 1.57e-04 (1.19e-03)	Tok/s 63842 (71277)	Loss/tok 3.5283 (5.3430)	LR 2.000e-03
0: TRAIN [0][900/968]	Time 0.317 (0.392)	Data 1.77e-04 (1.18e-03)	Tok/s 64881 (71295)	Loss/tok 3.5136 (5.3242)	LR 2.000e-03
0: TRAIN [0][910/968]	Time 0.547 (0.393)	Data 1.84e-04 (1.17e-03)	Tok/s 85444 (71342)	Loss/tok 3.9642 (5.3048)	LR 2.000e-03
0: TRAIN [0][920/968]	Time 0.315 (0.393)	Data 1.60e-04 (1.16e-03)	Tok/s 66059 (71395)	Loss/tok 3.4567 (5.2854)	LR 2.000e-03
0: TRAIN [0][930/968]	Time 0.319 (0.392)	Data 1.80e-04 (1.15e-03)	Tok/s 64917 (71351)	Loss/tok 3.4022 (5.2704)	LR 2.000e-03
0: TRAIN [0][940/968]	Time 0.549 (0.393)	Data 1.69e-04 (1.14e-03)	Tok/s 84872 (71385)	Loss/tok 3.8495 (5.2524)	LR 2.000e-03
0: TRAIN [0][950/968]	Time 0.316 (0.393)	Data 1.52e-04 (1.13e-03)	Tok/s 65874 (71460)	Loss/tok 3.4669 (5.2326)	LR 2.000e-03
0: TRAIN [0][960/968]	Time 0.317 (0.394)	Data 1.75e-04 (1.12e-03)	Tok/s 65736 (71481)	Loss/tok 3.4992 (5.2157)	LR 2.000e-03
:::MLL 1581976790.032 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 525}}
:::MLL 1581976790.033 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.734 (0.734)	Decoder iters 149.0 (149.0)	Tok/s 22549 (22549)
0: Running moses detokenizer
0: BLEU(score=18.20614115558212, counts=[33877, 14970, 7722, 4119], totals=[66495, 63492, 60489, 57490], precisions=[50.94668772088127, 23.577773577773577, 12.76595744680851, 7.16472429987824], bp=1.0, sys_len=66495, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1581976791.967 eval_accuracy: {"value": 18.21, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 536}}
:::MLL 1581976791.967 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 0	Training Loss: 5.2056	Test BLEU: 18.21
0: Performance: Epoch: 0	Training: 571662 Tok/s
0: Finished epoch 0
:::MLL 1581976791.968 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 558}}
:::MLL 1581976791.968 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1581976791.968 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 515}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 2367554368
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [1][0/968]	Time 1.155 (1.155)	Data 7.02e-01 (7.02e-01)	Tok/s 29322 (29322)	Loss/tok 3.6254 (3.6254)	LR 2.000e-03
0: TRAIN [1][10/968]	Time 0.428 (0.473)	Data 1.77e-04 (6.40e-02)	Tok/s 78692 (66194)	Loss/tok 3.6465 (3.7339)	LR 2.000e-03
0: TRAIN [1][20/968]	Time 0.319 (0.459)	Data 2.51e-04 (3.36e-02)	Tok/s 65819 (71506)	Loss/tok 3.4214 (3.7022)	LR 2.000e-03
0: TRAIN [1][30/968]	Time 0.430 (0.459)	Data 3.31e-04 (2.29e-02)	Tok/s 78520 (73303)	Loss/tok 3.5648 (3.7194)	LR 2.000e-03
0: TRAIN [1][40/968]	Time 0.545 (0.435)	Data 2.28e-04 (1.73e-02)	Tok/s 85290 (72416)	Loss/tok 3.8922 (3.6843)	LR 2.000e-03
0: TRAIN [1][50/968]	Time 0.547 (0.431)	Data 1.76e-04 (1.40e-02)	Tok/s 86055 (72540)	Loss/tok 3.7534 (3.6786)	LR 2.000e-03
0: TRAIN [1][60/968]	Time 0.685 (0.437)	Data 1.67e-04 (1.17e-02)	Tok/s 86714 (73323)	Loss/tok 4.0182 (3.6932)	LR 2.000e-03
0: TRAIN [1][70/968]	Time 0.319 (0.425)	Data 2.00e-04 (1.01e-02)	Tok/s 64434 (72636)	Loss/tok 3.3287 (3.6694)	LR 2.000e-03
0: TRAIN [1][80/968]	Time 0.430 (0.424)	Data 2.59e-04 (8.87e-03)	Tok/s 78718 (72555)	Loss/tok 3.5805 (3.6667)	LR 2.000e-03
0: TRAIN [1][90/968]	Time 0.433 (0.422)	Data 1.76e-04 (7.91e-03)	Tok/s 77305 (72444)	Loss/tok 3.5862 (3.6661)	LR 2.000e-03
0: TRAIN [1][100/968]	Time 0.431 (0.418)	Data 1.80e-04 (7.15e-03)	Tok/s 77975 (72429)	Loss/tok 3.6302 (3.6552)	LR 2.000e-03
0: TRAIN [1][110/968]	Time 0.547 (0.418)	Data 1.95e-04 (6.52e-03)	Tok/s 85252 (72659)	Loss/tok 3.8024 (3.6521)	LR 2.000e-03
0: TRAIN [1][120/968]	Time 0.549 (0.412)	Data 1.87e-04 (6.00e-03)	Tok/s 84869 (72188)	Loss/tok 3.7125 (3.6376)	LR 2.000e-03
0: TRAIN [1][130/968]	Time 0.430 (0.412)	Data 2.42e-04 (5.55e-03)	Tok/s 78677 (72237)	Loss/tok 3.6988 (3.6414)	LR 2.000e-03
0: TRAIN [1][140/968]	Time 0.319 (0.409)	Data 1.83e-04 (5.17e-03)	Tok/s 64821 (72083)	Loss/tok 3.3248 (3.6351)	LR 2.000e-03
0: TRAIN [1][150/968]	Time 0.427 (0.410)	Data 2.21e-04 (4.84e-03)	Tok/s 79037 (72323)	Loss/tok 3.4800 (3.6316)	LR 2.000e-03
0: TRAIN [1][160/968]	Time 0.318 (0.408)	Data 1.72e-04 (4.56e-03)	Tok/s 65518 (72216)	Loss/tok 3.3751 (3.6269)	LR 2.000e-03
0: TRAIN [1][170/968]	Time 0.210 (0.404)	Data 1.81e-04 (4.30e-03)	Tok/s 50090 (71932)	Loss/tok 2.8598 (3.6177)	LR 2.000e-03
0: TRAIN [1][180/968]	Time 0.433 (0.403)	Data 1.69e-04 (4.07e-03)	Tok/s 77432 (71840)	Loss/tok 3.5730 (3.6157)	LR 2.000e-03
0: TRAIN [1][190/968]	Time 0.316 (0.401)	Data 2.10e-04 (3.87e-03)	Tok/s 65638 (71588)	Loss/tok 3.2701 (3.6104)	LR 2.000e-03
0: TRAIN [1][200/968]	Time 0.428 (0.402)	Data 1.79e-04 (3.69e-03)	Tok/s 78746 (71853)	Loss/tok 3.5255 (3.6083)	LR 2.000e-03
0: TRAIN [1][210/968]	Time 0.432 (0.402)	Data 2.20e-04 (3.52e-03)	Tok/s 78960 (71886)	Loss/tok 3.4864 (3.6052)	LR 2.000e-03
0: TRAIN [1][220/968]	Time 0.321 (0.402)	Data 1.91e-04 (3.37e-03)	Tok/s 64677 (72017)	Loss/tok 3.2361 (3.6015)	LR 2.000e-03
0: TRAIN [1][230/968]	Time 0.318 (0.403)	Data 1.65e-04 (3.23e-03)	Tok/s 64635 (71984)	Loss/tok 3.2405 (3.6052)	LR 2.000e-03
0: TRAIN [1][240/968]	Time 0.430 (0.404)	Data 2.00e-04 (3.11e-03)	Tok/s 78471 (72066)	Loss/tok 3.6037 (3.6051)	LR 2.000e-03
0: TRAIN [1][250/968]	Time 0.319 (0.403)	Data 1.71e-04 (2.99e-03)	Tok/s 64897 (71905)	Loss/tok 3.2791 (3.6018)	LR 2.000e-03
0: TRAIN [1][260/968]	Time 0.321 (0.402)	Data 1.90e-04 (2.88e-03)	Tok/s 63793 (71910)	Loss/tok 3.3342 (3.5976)	LR 2.000e-03
0: TRAIN [1][270/968]	Time 0.320 (0.402)	Data 1.85e-04 (2.78e-03)	Tok/s 64511 (71886)	Loss/tok 3.3743 (3.5946)	LR 2.000e-03
0: TRAIN [1][280/968]	Time 0.548 (0.400)	Data 1.81e-04 (2.69e-03)	Tok/s 85438 (71805)	Loss/tok 3.7435 (3.5907)	LR 2.000e-03
0: TRAIN [1][290/968]	Time 0.319 (0.400)	Data 1.43e-04 (2.60e-03)	Tok/s 64285 (71783)	Loss/tok 3.3932 (3.5893)	LR 2.000e-03
0: TRAIN [1][300/968]	Time 0.430 (0.398)	Data 1.51e-04 (2.52e-03)	Tok/s 77876 (71630)	Loss/tok 3.5331 (3.5856)	LR 2.000e-03
0: TRAIN [1][310/968]	Time 0.547 (0.398)	Data 1.76e-04 (2.45e-03)	Tok/s 84720 (71613)	Loss/tok 3.7455 (3.5856)	LR 2.000e-03
0: TRAIN [1][320/968]	Time 0.430 (0.397)	Data 2.09e-04 (2.38e-03)	Tok/s 78133 (71518)	Loss/tok 3.5443 (3.5814)	LR 2.000e-03
0: TRAIN [1][330/968]	Time 0.210 (0.395)	Data 1.49e-04 (2.31e-03)	Tok/s 50174 (71398)	Loss/tok 2.7570 (3.5767)	LR 2.000e-03
0: TRAIN [1][340/968]	Time 0.429 (0.396)	Data 1.54e-04 (2.25e-03)	Tok/s 78412 (71458)	Loss/tok 3.5117 (3.5750)	LR 2.000e-03
0: TRAIN [1][350/968]	Time 0.545 (0.396)	Data 1.55e-04 (2.19e-03)	Tok/s 85932 (71577)	Loss/tok 3.6835 (3.5738)	LR 2.000e-03
0: TRAIN [1][360/968]	Time 0.317 (0.397)	Data 1.96e-04 (2.13e-03)	Tok/s 64453 (71579)	Loss/tok 3.3278 (3.5726)	LR 2.000e-03
0: TRAIN [1][370/968]	Time 0.430 (0.396)	Data 1.86e-04 (2.08e-03)	Tok/s 77321 (71554)	Loss/tok 3.5761 (3.5698)	LR 2.000e-03
0: TRAIN [1][380/968]	Time 0.314 (0.397)	Data 1.79e-04 (2.03e-03)	Tok/s 65355 (71627)	Loss/tok 3.2988 (3.5713)	LR 2.000e-03
0: TRAIN [1][390/968]	Time 0.318 (0.396)	Data 1.68e-04 (1.98e-03)	Tok/s 63790 (71584)	Loss/tok 3.3209 (3.5674)	LR 2.000e-03
0: TRAIN [1][400/968]	Time 0.427 (0.396)	Data 1.91e-04 (1.94e-03)	Tok/s 78342 (71624)	Loss/tok 3.4635 (3.5651)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][410/968]	Time 0.552 (0.396)	Data 1.68e-04 (1.89e-03)	Tok/s 84453 (71596)	Loss/tok 3.7204 (3.5655)	LR 2.000e-03
0: TRAIN [1][420/968]	Time 0.429 (0.396)	Data 1.69e-04 (1.85e-03)	Tok/s 78518 (71580)	Loss/tok 3.4673 (3.5631)	LR 2.000e-03
0: TRAIN [1][430/968]	Time 0.431 (0.395)	Data 1.69e-04 (1.81e-03)	Tok/s 77571 (71540)	Loss/tok 3.4921 (3.5608)	LR 2.000e-03
0: TRAIN [1][440/968]	Time 0.433 (0.393)	Data 1.55e-04 (1.77e-03)	Tok/s 77347 (71345)	Loss/tok 3.5605 (3.5562)	LR 2.000e-03
0: TRAIN [1][450/968]	Time 0.687 (0.395)	Data 1.64e-04 (1.74e-03)	Tok/s 86606 (71483)	Loss/tok 3.8924 (3.5570)	LR 2.000e-03
0: TRAIN [1][460/968]	Time 0.211 (0.393)	Data 1.70e-04 (1.70e-03)	Tok/s 49843 (71296)	Loss/tok 2.7794 (3.5527)	LR 2.000e-03
0: TRAIN [1][470/968]	Time 0.212 (0.393)	Data 1.55e-04 (1.67e-03)	Tok/s 50364 (71292)	Loss/tok 2.8399 (3.5518)	LR 2.000e-03
0: TRAIN [1][480/968]	Time 0.430 (0.393)	Data 1.48e-04 (1.64e-03)	Tok/s 77254 (71287)	Loss/tok 3.5526 (3.5504)	LR 2.000e-03
0: TRAIN [1][490/968]	Time 0.550 (0.394)	Data 1.81e-04 (1.61e-03)	Tok/s 84821 (71447)	Loss/tok 3.6224 (3.5511)	LR 2.000e-03
0: TRAIN [1][500/968]	Time 0.432 (0.395)	Data 1.91e-04 (1.58e-03)	Tok/s 78169 (71522)	Loss/tok 3.4956 (3.5491)	LR 2.000e-03
0: TRAIN [1][510/968]	Time 0.548 (0.394)	Data 1.46e-04 (1.55e-03)	Tok/s 84703 (71455)	Loss/tok 3.6954 (3.5460)	LR 2.000e-03
0: TRAIN [1][520/968]	Time 0.317 (0.393)	Data 1.54e-04 (1.53e-03)	Tok/s 65288 (71368)	Loss/tok 3.2370 (3.5430)	LR 2.000e-03
0: TRAIN [1][530/968]	Time 0.321 (0.393)	Data 1.56e-04 (1.50e-03)	Tok/s 64421 (71386)	Loss/tok 3.2556 (3.5406)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][540/968]	Time 0.431 (0.393)	Data 1.57e-04 (1.48e-03)	Tok/s 78581 (71424)	Loss/tok 3.4827 (3.5412)	LR 2.000e-03
0: TRAIN [1][550/968]	Time 0.428 (0.394)	Data 1.79e-04 (1.45e-03)	Tok/s 78725 (71449)	Loss/tok 3.5321 (3.5411)	LR 2.000e-03
0: TRAIN [1][560/968]	Time 0.319 (0.393)	Data 1.90e-04 (1.43e-03)	Tok/s 64957 (71363)	Loss/tok 3.1791 (3.5383)	LR 2.000e-03
0: TRAIN [1][570/968]	Time 0.317 (0.392)	Data 1.72e-04 (1.41e-03)	Tok/s 63984 (71353)	Loss/tok 3.3115 (3.5366)	LR 2.000e-03
0: TRAIN [1][580/968]	Time 0.433 (0.393)	Data 1.58e-04 (1.39e-03)	Tok/s 76778 (71392)	Loss/tok 3.5173 (3.5365)	LR 2.000e-03
0: TRAIN [1][590/968]	Time 0.318 (0.393)	Data 1.67e-04 (1.37e-03)	Tok/s 64256 (71429)	Loss/tok 3.2929 (3.5364)	LR 2.000e-03
0: TRAIN [1][600/968]	Time 0.317 (0.395)	Data 1.56e-04 (1.35e-03)	Tok/s 64626 (71512)	Loss/tok 3.2850 (3.5377)	LR 2.000e-03
0: TRAIN [1][610/968]	Time 0.317 (0.394)	Data 1.70e-04 (1.33e-03)	Tok/s 65238 (71446)	Loss/tok 3.2546 (3.5364)	LR 2.000e-03
0: TRAIN [1][620/968]	Time 0.319 (0.393)	Data 1.75e-04 (1.31e-03)	Tok/s 65910 (71316)	Loss/tok 3.3444 (3.5339)	LR 2.000e-03
0: TRAIN [1][630/968]	Time 0.431 (0.393)	Data 1.61e-04 (1.29e-03)	Tok/s 78000 (71299)	Loss/tok 3.4402 (3.5333)	LR 2.000e-03
0: TRAIN [1][640/968]	Time 0.432 (0.392)	Data 1.46e-04 (1.27e-03)	Tok/s 76962 (71282)	Loss/tok 3.4404 (3.5313)	LR 2.000e-03
0: TRAIN [1][650/968]	Time 0.431 (0.393)	Data 1.67e-04 (1.26e-03)	Tok/s 77522 (71359)	Loss/tok 3.4849 (3.5323)	LR 2.000e-03
0: TRAIN [1][660/968]	Time 0.320 (0.393)	Data 1.65e-04 (1.24e-03)	Tok/s 64376 (71320)	Loss/tok 3.1822 (3.5298)	LR 2.000e-03
0: TRAIN [1][670/968]	Time 0.428 (0.393)	Data 1.74e-04 (1.22e-03)	Tok/s 78157 (71330)	Loss/tok 3.4969 (3.5282)	LR 2.000e-03
0: TRAIN [1][680/968]	Time 0.213 (0.392)	Data 1.72e-04 (1.21e-03)	Tok/s 50065 (71238)	Loss/tok 2.7579 (3.5257)	LR 2.000e-03
0: TRAIN [1][690/968]	Time 0.319 (0.391)	Data 1.51e-04 (1.19e-03)	Tok/s 64715 (71096)	Loss/tok 3.2416 (3.5227)	LR 2.000e-03
0: TRAIN [1][700/968]	Time 0.317 (0.390)	Data 1.56e-04 (1.18e-03)	Tok/s 64620 (71075)	Loss/tok 3.1672 (3.5204)	LR 2.000e-03
0: TRAIN [1][710/968]	Time 0.317 (0.389)	Data 1.58e-04 (1.16e-03)	Tok/s 65868 (71005)	Loss/tok 3.2351 (3.5177)	LR 2.000e-03
0: TRAIN [1][720/968]	Time 0.686 (0.389)	Data 1.85e-04 (1.15e-03)	Tok/s 86507 (70994)	Loss/tok 3.8170 (3.5167)	LR 2.000e-03
0: TRAIN [1][730/968]	Time 0.318 (0.389)	Data 1.65e-04 (1.14e-03)	Tok/s 65604 (70993)	Loss/tok 3.2595 (3.5154)	LR 2.000e-03
0: TRAIN [1][740/968]	Time 0.690 (0.391)	Data 1.71e-04 (1.12e-03)	Tok/s 86263 (71070)	Loss/tok 3.8370 (3.5180)	LR 2.000e-03
0: TRAIN [1][750/968]	Time 0.432 (0.391)	Data 1.50e-04 (1.11e-03)	Tok/s 78014 (71063)	Loss/tok 3.4248 (3.5172)	LR 2.000e-03
0: TRAIN [1][760/968]	Time 0.429 (0.390)	Data 1.75e-04 (1.10e-03)	Tok/s 77666 (71036)	Loss/tok 3.5017 (3.5153)	LR 2.000e-03
0: TRAIN [1][770/968]	Time 0.433 (0.390)	Data 1.96e-04 (1.09e-03)	Tok/s 77517 (71034)	Loss/tok 3.5083 (3.5136)	LR 2.000e-03
0: TRAIN [1][780/968]	Time 0.430 (0.390)	Data 1.74e-04 (1.07e-03)	Tok/s 78717 (71018)	Loss/tok 3.4696 (3.5122)	LR 2.000e-03
0: TRAIN [1][790/968]	Time 0.209 (0.390)	Data 1.55e-04 (1.06e-03)	Tok/s 51436 (71052)	Loss/tok 2.7150 (3.5117)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][800/968]	Time 0.318 (0.391)	Data 1.70e-04 (1.05e-03)	Tok/s 64412 (71101)	Loss/tok 3.1615 (3.5106)	LR 2.000e-03
0: TRAIN [1][810/968]	Time 0.429 (0.391)	Data 1.54e-04 (1.04e-03)	Tok/s 78579 (71082)	Loss/tok 3.4804 (3.5090)	LR 2.000e-03
0: TRAIN [1][820/968]	Time 0.318 (0.391)	Data 1.91e-04 (1.03e-03)	Tok/s 63889 (71122)	Loss/tok 3.2038 (3.5079)	LR 2.000e-03
0: TRAIN [1][830/968]	Time 0.551 (0.392)	Data 1.72e-04 (1.02e-03)	Tok/s 84985 (71198)	Loss/tok 3.6029 (3.5080)	LR 2.000e-03
0: TRAIN [1][840/968]	Time 0.321 (0.392)	Data 1.51e-04 (1.01e-03)	Tok/s 63993 (71248)	Loss/tok 3.2581 (3.5075)	LR 2.000e-03
0: TRAIN [1][850/968]	Time 0.687 (0.392)	Data 1.98e-04 (1.00e-03)	Tok/s 86417 (71211)	Loss/tok 3.8366 (3.5062)	LR 2.000e-03
0: TRAIN [1][860/968]	Time 0.697 (0.392)	Data 1.81e-04 (9.90e-04)	Tok/s 85462 (71245)	Loss/tok 3.7596 (3.5064)	LR 2.000e-03
0: TRAIN [1][870/968]	Time 0.210 (0.392)	Data 1.77e-04 (9.81e-04)	Tok/s 50845 (71196)	Loss/tok 2.7627 (3.5044)	LR 2.000e-03
0: TRAIN [1][880/968]	Time 0.320 (0.392)	Data 1.69e-04 (9.71e-04)	Tok/s 64533 (71217)	Loss/tok 3.1645 (3.5042)	LR 2.000e-03
0: TRAIN [1][890/968]	Time 0.426 (0.392)	Data 1.79e-04 (9.63e-04)	Tok/s 77692 (71213)	Loss/tok 3.4632 (3.5034)	LR 2.000e-03
0: TRAIN [1][900/968]	Time 0.552 (0.392)	Data 1.88e-04 (9.54e-04)	Tok/s 84009 (71196)	Loss/tok 3.6361 (3.5021)	LR 2.000e-03
0: TRAIN [1][910/968]	Time 0.317 (0.392)	Data 1.75e-04 (9.45e-04)	Tok/s 65085 (71164)	Loss/tok 3.1945 (3.5010)	LR 2.000e-03
0: TRAIN [1][920/968]	Time 0.553 (0.392)	Data 1.61e-04 (9.37e-04)	Tok/s 83852 (71201)	Loss/tok 3.5369 (3.5005)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][930/968]	Time 0.319 (0.392)	Data 1.45e-04 (9.28e-04)	Tok/s 64659 (71222)	Loss/tok 3.1652 (3.4993)	LR 2.000e-03
0: TRAIN [1][940/968]	Time 0.551 (0.392)	Data 1.52e-04 (9.20e-04)	Tok/s 84331 (71209)	Loss/tok 3.5951 (3.4985)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][950/968]	Time 0.428 (0.393)	Data 1.71e-04 (9.12e-04)	Tok/s 78354 (71253)	Loss/tok 3.4053 (3.4990)	LR 2.000e-03
0: TRAIN [1][960/968]	Time 0.208 (0.393)	Data 1.99e-04 (9.05e-04)	Tok/s 50237 (71278)	Loss/tok 2.7043 (3.4991)	LR 2.000e-03
:::MLL 1581977174.366 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 525}}
:::MLL 1581977174.367 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.703 (0.703)	Decoder iters 149.0 (149.0)	Tok/s 22760 (22760)
0: Running moses detokenizer
0: BLEU(score=20.916448263979436, counts=[34485, 16258, 8833, 4994], totals=[62557, 59554, 56553, 53557], precisions=[55.12572533849129, 27.299593646102696, 15.618976888936043, 9.324644770991654], bp=0.9666941628759782, sys_len=62557, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1581977176.213 eval_accuracy: {"value": 20.92, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 536}}
:::MLL 1581977176.214 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 1	Training Loss: 3.4999	Test BLEU: 20.92
0: Performance: Epoch: 1	Training: 570744 Tok/s
0: Finished epoch 1
:::MLL 1581977176.214 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 558}}
:::MLL 1581977176.214 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1581977176.215 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 515}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 1364041519
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [2][0/968]	Time 0.926 (0.926)	Data 7.17e-01 (7.17e-01)	Tok/s 11563 (11563)	Loss/tok 2.7188 (2.7188)	LR 2.000e-03
0: TRAIN [2][10/968]	Time 0.209 (0.430)	Data 1.77e-04 (6.54e-02)	Tok/s 49961 (63421)	Loss/tok 2.5904 (3.3060)	LR 2.000e-03
0: TRAIN [2][20/968]	Time 0.431 (0.400)	Data 1.77e-04 (3.43e-02)	Tok/s 77733 (65549)	Loss/tok 3.3198 (3.3165)	LR 2.000e-03
0: TRAIN [2][30/968]	Time 0.320 (0.408)	Data 1.74e-04 (2.33e-02)	Tok/s 64893 (68163)	Loss/tok 3.1673 (3.3385)	LR 2.000e-03
0: TRAIN [2][40/968]	Time 0.430 (0.408)	Data 1.78e-04 (1.77e-02)	Tok/s 77984 (69634)	Loss/tok 3.3012 (3.3353)	LR 2.000e-03
0: TRAIN [2][50/968]	Time 0.319 (0.400)	Data 1.54e-04 (1.42e-02)	Tok/s 65028 (69564)	Loss/tok 3.0545 (3.3215)	LR 2.000e-03
0: TRAIN [2][60/968]	Time 0.318 (0.408)	Data 1.77e-04 (1.19e-02)	Tok/s 64321 (70141)	Loss/tok 3.0579 (3.3567)	LR 2.000e-03
0: TRAIN [2][70/968]	Time 0.321 (0.404)	Data 1.57e-04 (1.03e-02)	Tok/s 65162 (70279)	Loss/tok 3.0934 (3.3454)	LR 2.000e-03
0: TRAIN [2][80/968]	Time 0.429 (0.404)	Data 1.59e-04 (9.02e-03)	Tok/s 78608 (70785)	Loss/tok 3.3204 (3.3427)	LR 2.000e-03
0: TRAIN [2][90/968]	Time 0.320 (0.402)	Data 1.57e-04 (8.05e-03)	Tok/s 64846 (70587)	Loss/tok 3.1739 (3.3430)	LR 2.000e-03
0: TRAIN [2][100/968]	Time 0.318 (0.405)	Data 1.63e-04 (7.27e-03)	Tok/s 64973 (71160)	Loss/tok 3.0546 (3.3463)	LR 2.000e-03
0: TRAIN [2][110/968]	Time 0.430 (0.408)	Data 1.65e-04 (6.63e-03)	Tok/s 78801 (71626)	Loss/tok 3.3334 (3.3535)	LR 2.000e-03
0: TRAIN [2][120/968]	Time 0.211 (0.404)	Data 1.64e-04 (6.10e-03)	Tok/s 50447 (71328)	Loss/tok 2.7065 (3.3469)	LR 2.000e-03
0: TRAIN [2][130/968]	Time 0.432 (0.407)	Data 1.96e-04 (5.64e-03)	Tok/s 77715 (71594)	Loss/tok 3.3687 (3.3563)	LR 2.000e-03
0: TRAIN [2][140/968]	Time 0.431 (0.405)	Data 1.90e-04 (5.26e-03)	Tok/s 78229 (71545)	Loss/tok 3.3878 (3.3506)	LR 2.000e-03
0: TRAIN [2][150/968]	Time 0.321 (0.401)	Data 1.64e-04 (4.92e-03)	Tok/s 64203 (71292)	Loss/tok 3.0609 (3.3443)	LR 2.000e-03
0: TRAIN [2][160/968]	Time 0.319 (0.404)	Data 1.61e-04 (4.63e-03)	Tok/s 65235 (71465)	Loss/tok 3.1105 (3.3519)	LR 2.000e-03
0: TRAIN [2][170/968]	Time 0.430 (0.402)	Data 1.77e-04 (4.37e-03)	Tok/s 77311 (71382)	Loss/tok 3.3405 (3.3459)	LR 2.000e-03
0: TRAIN [2][180/968]	Time 0.430 (0.403)	Data 1.68e-04 (4.13e-03)	Tok/s 77934 (71567)	Loss/tok 3.3501 (3.3475)	LR 2.000e-03
0: TRAIN [2][190/968]	Time 0.315 (0.404)	Data 1.89e-04 (3.93e-03)	Tok/s 65962 (71661)	Loss/tok 3.1138 (3.3477)	LR 2.000e-03
0: TRAIN [2][200/968]	Time 0.318 (0.406)	Data 1.79e-04 (3.74e-03)	Tok/s 65074 (71789)	Loss/tok 3.0880 (3.3548)	LR 2.000e-03
0: TRAIN [2][210/968]	Time 0.430 (0.401)	Data 1.82e-04 (3.57e-03)	Tok/s 78167 (71302)	Loss/tok 3.3629 (3.3472)	LR 2.000e-03
0: TRAIN [2][220/968]	Time 0.692 (0.401)	Data 1.66e-04 (3.42e-03)	Tok/s 85910 (71320)	Loss/tok 3.7244 (3.3488)	LR 2.000e-03
0: TRAIN [2][230/968]	Time 0.434 (0.400)	Data 2.07e-04 (3.28e-03)	Tok/s 77805 (71308)	Loss/tok 3.3100 (3.3457)	LR 2.000e-03
0: TRAIN [2][240/968]	Time 0.695 (0.401)	Data 1.58e-04 (3.15e-03)	Tok/s 85695 (71254)	Loss/tok 3.6657 (3.3471)	LR 2.000e-03
0: TRAIN [2][250/968]	Time 0.318 (0.401)	Data 1.90e-04 (3.03e-03)	Tok/s 64783 (71305)	Loss/tok 3.0368 (3.3460)	LR 2.000e-03
0: TRAIN [2][260/968]	Time 0.318 (0.401)	Data 1.75e-04 (2.92e-03)	Tok/s 66114 (71410)	Loss/tok 3.1533 (3.3448)	LR 2.000e-03
0: TRAIN [2][270/968]	Time 0.551 (0.402)	Data 1.87e-04 (2.82e-03)	Tok/s 84605 (71472)	Loss/tok 3.5284 (3.3492)	LR 2.000e-03
0: TRAIN [2][280/968]	Time 0.317 (0.403)	Data 1.96e-04 (2.73e-03)	Tok/s 66019 (71494)	Loss/tok 3.1355 (3.3504)	LR 2.000e-03
0: TRAIN [2][290/968]	Time 0.208 (0.402)	Data 1.83e-04 (2.64e-03)	Tok/s 50014 (71372)	Loss/tok 2.7114 (3.3509)	LR 2.000e-03
0: TRAIN [2][300/968]	Time 0.319 (0.403)	Data 2.04e-04 (2.56e-03)	Tok/s 64640 (71535)	Loss/tok 3.0913 (3.3534)	LR 2.000e-03
0: TRAIN [2][310/968]	Time 0.320 (0.403)	Data 2.20e-04 (2.48e-03)	Tok/s 65000 (71549)	Loss/tok 3.0862 (3.3521)	LR 2.000e-03
0: TRAIN [2][320/968]	Time 0.430 (0.401)	Data 1.85e-04 (2.41e-03)	Tok/s 78488 (71389)	Loss/tok 3.3412 (3.3487)	LR 2.000e-03
0: TRAIN [2][330/968]	Time 0.319 (0.401)	Data 2.29e-04 (2.34e-03)	Tok/s 64186 (71349)	Loss/tok 3.1590 (3.3490)	LR 2.000e-03
0: TRAIN [2][340/968]	Time 0.429 (0.400)	Data 2.09e-04 (2.28e-03)	Tok/s 78371 (71300)	Loss/tok 3.3162 (3.3476)	LR 2.000e-03
0: TRAIN [2][350/968]	Time 0.429 (0.399)	Data 1.71e-04 (2.22e-03)	Tok/s 78129 (71260)	Loss/tok 3.3136 (3.3439)	LR 2.000e-03
0: TRAIN [2][360/968]	Time 0.434 (0.401)	Data 2.03e-04 (2.17e-03)	Tok/s 77598 (71445)	Loss/tok 3.2839 (3.3480)	LR 2.000e-03
0: TRAIN [2][370/968]	Time 0.551 (0.404)	Data 1.88e-04 (2.11e-03)	Tok/s 85023 (71663)	Loss/tok 3.5177 (3.3520)	LR 2.000e-03
0: TRAIN [2][380/968]	Time 0.318 (0.403)	Data 1.91e-04 (2.06e-03)	Tok/s 65397 (71672)	Loss/tok 3.0870 (3.3502)	LR 2.000e-03
0: TRAIN [2][390/968]	Time 0.428 (0.403)	Data 2.04e-04 (2.01e-03)	Tok/s 78043 (71696)	Loss/tok 3.4227 (3.3499)	LR 2.000e-03
0: TRAIN [2][400/968]	Time 0.430 (0.402)	Data 2.09e-04 (1.97e-03)	Tok/s 78659 (71625)	Loss/tok 3.2876 (3.3463)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][410/968]	Time 0.214 (0.402)	Data 1.70e-04 (1.93e-03)	Tok/s 49698 (71588)	Loss/tok 2.6906 (3.3458)	LR 2.000e-03
0: TRAIN [2][420/968]	Time 0.431 (0.401)	Data 1.74e-04 (1.88e-03)	Tok/s 78695 (71494)	Loss/tok 3.2505 (3.3437)	LR 2.000e-03
0: TRAIN [2][430/968]	Time 0.320 (0.401)	Data 1.77e-04 (1.84e-03)	Tok/s 64491 (71556)	Loss/tok 3.0916 (3.3442)	LR 2.000e-03
0: TRAIN [2][440/968]	Time 0.320 (0.401)	Data 1.73e-04 (1.81e-03)	Tok/s 64744 (71512)	Loss/tok 3.0804 (3.3428)	LR 2.000e-03
0: TRAIN [2][450/968]	Time 0.317 (0.399)	Data 1.82e-04 (1.77e-03)	Tok/s 64995 (71367)	Loss/tok 3.0325 (3.3403)	LR 2.000e-03
0: TRAIN [2][460/968]	Time 0.552 (0.401)	Data 1.76e-04 (1.74e-03)	Tok/s 84250 (71558)	Loss/tok 3.4957 (3.3437)	LR 2.000e-03
0: TRAIN [2][470/968]	Time 0.319 (0.401)	Data 1.84e-04 (1.70e-03)	Tok/s 64895 (71511)	Loss/tok 3.0817 (3.3413)	LR 2.000e-03
0: TRAIN [2][480/968]	Time 0.432 (0.400)	Data 1.88e-04 (1.67e-03)	Tok/s 78107 (71500)	Loss/tok 3.3418 (3.3395)	LR 2.000e-03
0: TRAIN [2][490/968]	Time 0.433 (0.400)	Data 1.67e-04 (1.64e-03)	Tok/s 77567 (71508)	Loss/tok 3.3337 (3.3383)	LR 2.000e-03
0: TRAIN [2][500/968]	Time 0.319 (0.400)	Data 1.84e-04 (1.61e-03)	Tok/s 64376 (71449)	Loss/tok 3.0690 (3.3392)	LR 2.000e-03
0: TRAIN [2][510/968]	Time 0.430 (0.400)	Data 2.21e-04 (1.59e-03)	Tok/s 78295 (71467)	Loss/tok 3.2952 (3.3388)	LR 2.000e-03
0: TRAIN [2][520/968]	Time 0.433 (0.399)	Data 1.78e-04 (1.56e-03)	Tok/s 78550 (71378)	Loss/tok 3.2751 (3.3366)	LR 2.000e-03
0: TRAIN [2][530/968]	Time 0.429 (0.399)	Data 2.33e-04 (1.53e-03)	Tok/s 77475 (71407)	Loss/tok 3.3343 (3.3375)	LR 2.000e-03
0: TRAIN [2][540/968]	Time 0.549 (0.399)	Data 2.16e-04 (1.51e-03)	Tok/s 84845 (71461)	Loss/tok 3.5259 (3.3373)	LR 2.000e-03
0: TRAIN [2][550/968]	Time 0.434 (0.399)	Data 2.04e-04 (1.48e-03)	Tok/s 77803 (71453)	Loss/tok 3.3375 (3.3369)	LR 2.000e-03
0: TRAIN [2][560/968]	Time 0.323 (0.399)	Data 2.02e-04 (1.46e-03)	Tok/s 63374 (71430)	Loss/tok 3.1248 (3.3357)	LR 2.000e-03
0: TRAIN [2][570/968]	Time 0.434 (0.399)	Data 1.86e-04 (1.44e-03)	Tok/s 78027 (71527)	Loss/tok 3.2895 (3.3354)	LR 2.000e-03
0: TRAIN [2][580/968]	Time 0.318 (0.398)	Data 1.92e-04 (1.42e-03)	Tok/s 64655 (71396)	Loss/tok 3.1281 (3.3330)	LR 2.000e-03
0: TRAIN [2][590/968]	Time 0.208 (0.397)	Data 1.86e-04 (1.40e-03)	Tok/s 50879 (71315)	Loss/tok 2.6352 (3.3336)	LR 2.000e-03
0: TRAIN [2][600/968]	Time 0.546 (0.398)	Data 1.78e-04 (1.38e-03)	Tok/s 85979 (71374)	Loss/tok 3.4390 (3.3327)	LR 2.000e-03
0: TRAIN [2][610/968]	Time 0.432 (0.398)	Data 2.01e-04 (1.36e-03)	Tok/s 78542 (71418)	Loss/tok 3.3064 (3.3325)	LR 2.000e-03
0: TRAIN [2][620/968]	Time 0.318 (0.398)	Data 2.15e-04 (1.34e-03)	Tok/s 65607 (71442)	Loss/tok 3.0938 (3.3326)	LR 2.000e-03
0: TRAIN [2][630/968]	Time 0.551 (0.398)	Data 1.69e-04 (1.32e-03)	Tok/s 84528 (71409)	Loss/tok 3.5033 (3.3315)	LR 2.000e-03
0: TRAIN [2][640/968]	Time 0.549 (0.398)	Data 1.95e-04 (1.30e-03)	Tok/s 85267 (71471)	Loss/tok 3.5558 (3.3320)	LR 2.000e-03
0: TRAIN [2][650/968]	Time 0.549 (0.398)	Data 2.12e-04 (1.29e-03)	Tok/s 84915 (71499)	Loss/tok 3.4511 (3.3308)	LR 2.000e-03
0: TRAIN [2][660/968]	Time 0.319 (0.399)	Data 1.97e-04 (1.27e-03)	Tok/s 66066 (71545)	Loss/tok 3.0188 (3.3315)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][670/968]	Time 0.313 (0.399)	Data 2.06e-04 (1.25e-03)	Tok/s 66917 (71565)	Loss/tok 2.9934 (3.3315)	LR 2.000e-03
0: TRAIN [2][680/968]	Time 0.320 (0.398)	Data 2.07e-04 (1.24e-03)	Tok/s 65196 (71529)	Loss/tok 3.0834 (3.3300)	LR 2.000e-03
0: TRAIN [2][690/968]	Time 0.552 (0.398)	Data 2.01e-04 (1.22e-03)	Tok/s 84539 (71466)	Loss/tok 3.4998 (3.3292)	LR 2.000e-03
0: TRAIN [2][700/968]	Time 0.319 (0.397)	Data 1.81e-04 (1.21e-03)	Tok/s 65536 (71434)	Loss/tok 3.1116 (3.3282)	LR 2.000e-03
0: TRAIN [2][710/968]	Time 0.210 (0.397)	Data 2.05e-04 (1.19e-03)	Tok/s 51230 (71391)	Loss/tok 2.6640 (3.3267)	LR 2.000e-03
0: TRAIN [2][720/968]	Time 0.319 (0.397)	Data 2.14e-04 (1.18e-03)	Tok/s 63774 (71412)	Loss/tok 3.0900 (3.3268)	LR 2.000e-03
0: TRAIN [2][730/968]	Time 0.431 (0.396)	Data 1.89e-04 (1.17e-03)	Tok/s 76795 (71389)	Loss/tok 3.3154 (3.3253)	LR 2.000e-03
0: TRAIN [2][740/968]	Time 0.320 (0.396)	Data 1.92e-04 (1.15e-03)	Tok/s 65384 (71407)	Loss/tok 3.1432 (3.3247)	LR 2.000e-03
0: TRAIN [2][750/968]	Time 0.208 (0.396)	Data 2.12e-04 (1.14e-03)	Tok/s 51854 (71364)	Loss/tok 2.6217 (3.3235)	LR 2.000e-03
0: TRAIN [2][760/968]	Time 0.427 (0.396)	Data 1.91e-04 (1.13e-03)	Tok/s 78398 (71392)	Loss/tok 3.3196 (3.3239)	LR 2.000e-03
0: TRAIN [2][770/968]	Time 0.318 (0.396)	Data 2.30e-04 (1.12e-03)	Tok/s 64229 (71362)	Loss/tok 3.1023 (3.3244)	LR 2.000e-03
0: TRAIN [2][780/968]	Time 0.322 (0.396)	Data 1.83e-04 (1.10e-03)	Tok/s 64813 (71368)	Loss/tok 3.0639 (3.3236)	LR 2.000e-03
0: TRAIN [2][790/968]	Time 0.432 (0.396)	Data 1.96e-04 (1.09e-03)	Tok/s 77821 (71370)	Loss/tok 3.2351 (3.3227)	LR 2.000e-03
0: TRAIN [2][800/968]	Time 0.434 (0.396)	Data 1.71e-04 (1.08e-03)	Tok/s 76387 (71329)	Loss/tok 3.2862 (3.3213)	LR 2.000e-03
0: TRAIN [2][810/968]	Time 0.320 (0.395)	Data 2.15e-04 (1.07e-03)	Tok/s 64764 (71310)	Loss/tok 3.0975 (3.3205)	LR 2.000e-03
0: TRAIN [2][820/968]	Time 0.320 (0.395)	Data 2.13e-04 (1.06e-03)	Tok/s 64221 (71321)	Loss/tok 3.1399 (3.3200)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][830/968]	Time 0.431 (0.395)	Data 1.84e-04 (1.05e-03)	Tok/s 77556 (71349)	Loss/tok 3.3155 (3.3197)	LR 2.000e-03
0: TRAIN [2][840/968]	Time 0.432 (0.396)	Data 1.80e-04 (1.04e-03)	Tok/s 77914 (71356)	Loss/tok 3.3333 (3.3194)	LR 2.000e-03
0: TRAIN [2][850/968]	Time 0.430 (0.396)	Data 1.94e-04 (1.03e-03)	Tok/s 77737 (71368)	Loss/tok 3.3584 (3.3186)	LR 2.000e-03
0: TRAIN [2][860/968]	Time 0.321 (0.395)	Data 1.78e-04 (1.02e-03)	Tok/s 64970 (71320)	Loss/tok 3.1239 (3.3171)	LR 2.000e-03
0: TRAIN [2][870/968]	Time 0.430 (0.395)	Data 2.03e-04 (1.01e-03)	Tok/s 78179 (71347)	Loss/tok 3.2197 (3.3164)	LR 2.000e-03
0: TRAIN [2][880/968]	Time 0.431 (0.395)	Data 2.12e-04 (1.00e-03)	Tok/s 78125 (71346)	Loss/tok 3.2436 (3.3155)	LR 2.000e-03
0: TRAIN [2][890/968]	Time 0.320 (0.395)	Data 1.75e-04 (9.92e-04)	Tok/s 63105 (71332)	Loss/tok 3.0733 (3.3153)	LR 2.000e-03
0: TRAIN [2][900/968]	Time 0.432 (0.394)	Data 1.87e-04 (9.83e-04)	Tok/s 77737 (71281)	Loss/tok 3.3094 (3.3142)	LR 2.000e-03
0: TRAIN [2][910/968]	Time 0.319 (0.394)	Data 1.79e-04 (9.75e-04)	Tok/s 64872 (71262)	Loss/tok 3.0905 (3.3150)	LR 2.000e-03
0: TRAIN [2][920/968]	Time 0.554 (0.395)	Data 2.17e-04 (9.66e-04)	Tok/s 84238 (71294)	Loss/tok 3.4810 (3.3156)	LR 2.000e-03
0: TRAIN [2][930/968]	Time 0.317 (0.394)	Data 2.03e-04 (9.58e-04)	Tok/s 65712 (71222)	Loss/tok 3.0713 (3.3138)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][940/968]	Time 0.545 (0.394)	Data 1.86e-04 (9.50e-04)	Tok/s 85724 (71251)	Loss/tok 3.3315 (3.3142)	LR 2.000e-03
0: TRAIN [2][950/968]	Time 0.319 (0.394)	Data 2.00e-04 (9.42e-04)	Tok/s 64981 (71248)	Loss/tok 3.0076 (3.3138)	LR 2.000e-03
0: TRAIN [2][960/968]	Time 0.315 (0.394)	Data 2.04e-04 (9.34e-04)	Tok/s 65288 (71240)	Loss/tok 3.0986 (3.3131)	LR 2.000e-03
:::MLL 1581977559.033 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 525}}
:::MLL 1581977559.034 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.737 (0.737)	Decoder iters 149.0 (149.0)	Tok/s 22901 (22901)
0: Running moses detokenizer
0: BLEU(score=22.44838013295941, counts=[36501, 17770, 9917, 5763], totals=[66406, 63403, 60400, 57402], precisions=[54.96641869710568, 28.027064965380188, 16.41887417218543, 10.039719870387792], bp=1.0, sys_len=66406, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1581977560.945 eval_accuracy: {"value": 22.45, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 536}}
:::MLL 1581977560.946 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 2	Training Loss: 3.3125	Test BLEU: 22.45
0: Performance: Epoch: 2	Training: 570178 Tok/s
0: Finished epoch 2
:::MLL 1581977560.946 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 558}}
:::MLL 1581977560.946 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1581977560.947 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 515}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 1772733759
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [3][0/968]	Time 1.147 (1.147)	Data 7.05e-01 (7.05e-01)	Tok/s 29377 (29377)	Loss/tok 3.2285 (3.2285)	LR 2.000e-03
0: TRAIN [3][10/968]	Time 0.429 (0.444)	Data 1.48e-04 (6.43e-02)	Tok/s 79550 (67264)	Loss/tok 3.1284 (3.1414)	LR 2.000e-03
0: TRAIN [3][20/968]	Time 0.430 (0.412)	Data 1.85e-04 (3.38e-02)	Tok/s 77946 (69035)	Loss/tok 3.2212 (3.1236)	LR 2.000e-03
0: TRAIN [3][30/968]	Time 0.319 (0.417)	Data 2.13e-04 (2.29e-02)	Tok/s 64059 (70520)	Loss/tok 2.9556 (3.1679)	LR 2.000e-03
0: TRAIN [3][40/968]	Time 0.319 (0.391)	Data 1.94e-04 (1.74e-02)	Tok/s 65481 (68789)	Loss/tok 2.9533 (3.1347)	LR 2.000e-03
0: TRAIN [3][50/968]	Time 0.426 (0.400)	Data 2.21e-04 (1.40e-02)	Tok/s 79229 (69648)	Loss/tok 3.1762 (3.1741)	LR 2.000e-03
0: TRAIN [3][60/968]	Time 0.318 (0.391)	Data 1.79e-04 (1.18e-02)	Tok/s 65615 (69276)	Loss/tok 2.9867 (3.1608)	LR 2.000e-03
0: TRAIN [3][70/968]	Time 0.318 (0.386)	Data 1.87e-04 (1.01e-02)	Tok/s 64934 (68934)	Loss/tok 2.9527 (3.1632)	LR 2.000e-03
0: TRAIN [3][80/968]	Time 0.209 (0.384)	Data 2.04e-04 (8.90e-03)	Tok/s 51202 (69269)	Loss/tok 2.6154 (3.1619)	LR 2.000e-03
0: TRAIN [3][90/968]	Time 0.429 (0.376)	Data 1.80e-04 (7.94e-03)	Tok/s 77892 (68635)	Loss/tok 3.2140 (3.1486)	LR 2.000e-03
0: TRAIN [3][100/968]	Time 0.319 (0.379)	Data 2.08e-04 (7.17e-03)	Tok/s 64122 (69134)	Loss/tok 3.0114 (3.1596)	LR 2.000e-03
0: TRAIN [3][110/968]	Time 0.428 (0.375)	Data 1.76e-04 (6.54e-03)	Tok/s 78903 (68700)	Loss/tok 3.2081 (3.1561)	LR 2.000e-03
0: TRAIN [3][120/968]	Time 0.319 (0.384)	Data 1.91e-04 (6.02e-03)	Tok/s 65536 (69507)	Loss/tok 3.0547 (3.1776)	LR 2.000e-03
0: TRAIN [3][130/968]	Time 0.688 (0.391)	Data 1.78e-04 (5.57e-03)	Tok/s 86230 (70085)	Loss/tok 3.6696 (3.1942)	LR 2.000e-03
0: TRAIN [3][140/968]	Time 0.688 (0.391)	Data 1.94e-04 (5.19e-03)	Tok/s 85493 (70033)	Loss/tok 3.7013 (3.1995)	LR 2.000e-03
0: TRAIN [3][150/968]	Time 0.690 (0.391)	Data 2.12e-04 (4.86e-03)	Tok/s 86358 (70019)	Loss/tok 3.6281 (3.2042)	LR 2.000e-03
0: TRAIN [3][160/968]	Time 0.320 (0.390)	Data 2.07e-04 (4.57e-03)	Tok/s 64384 (70060)	Loss/tok 2.9676 (3.2008)	LR 2.000e-03
0: TRAIN [3][170/968]	Time 0.316 (0.389)	Data 2.14e-04 (4.32e-03)	Tok/s 64738 (70065)	Loss/tok 3.0136 (3.2015)	LR 2.000e-03
0: TRAIN [3][180/968]	Time 0.543 (0.389)	Data 1.97e-04 (4.09e-03)	Tok/s 86488 (70186)	Loss/tok 3.4081 (3.2005)	LR 2.000e-03
0: TRAIN [3][190/968]	Time 0.430 (0.387)	Data 1.83e-04 (3.88e-03)	Tok/s 78184 (70079)	Loss/tok 3.2385 (3.1967)	LR 2.000e-03
0: TRAIN [3][200/968]	Time 0.319 (0.388)	Data 1.72e-04 (3.70e-03)	Tok/s 65227 (70154)	Loss/tok 3.0133 (3.1974)	LR 2.000e-03
0: TRAIN [3][210/968]	Time 0.430 (0.387)	Data 1.72e-04 (3.53e-03)	Tok/s 77921 (70105)	Loss/tok 3.2852 (3.1970)	LR 2.000e-03
0: TRAIN [3][220/968]	Time 0.431 (0.387)	Data 1.91e-04 (3.38e-03)	Tok/s 78165 (70173)	Loss/tok 3.2128 (3.1988)	LR 2.000e-03
0: TRAIN [3][230/968]	Time 0.693 (0.389)	Data 1.76e-04 (3.24e-03)	Tok/s 86637 (70328)	Loss/tok 3.4864 (3.2027)	LR 2.000e-03
0: TRAIN [3][240/968]	Time 0.317 (0.387)	Data 1.75e-04 (3.12e-03)	Tok/s 64614 (70138)	Loss/tok 2.9437 (3.2005)	LR 2.000e-03
0: TRAIN [3][250/968]	Time 0.555 (0.389)	Data 2.02e-04 (3.00e-03)	Tok/s 84541 (70339)	Loss/tok 3.3700 (3.2040)	LR 2.000e-03
0: TRAIN [3][260/968]	Time 0.322 (0.388)	Data 1.80e-04 (2.89e-03)	Tok/s 64515 (70375)	Loss/tok 3.0951 (3.2016)	LR 2.000e-03
0: TRAIN [3][270/968]	Time 0.432 (0.389)	Data 2.07e-04 (2.79e-03)	Tok/s 77683 (70479)	Loss/tok 3.2384 (3.2032)	LR 2.000e-03
0: TRAIN [3][280/968]	Time 0.319 (0.390)	Data 1.68e-04 (2.70e-03)	Tok/s 64758 (70531)	Loss/tok 3.0115 (3.2041)	LR 2.000e-03
0: TRAIN [3][290/968]	Time 0.430 (0.389)	Data 1.76e-04 (2.61e-03)	Tok/s 77793 (70475)	Loss/tok 3.2922 (3.2031)	LR 2.000e-03
0: TRAIN [3][300/968]	Time 0.210 (0.392)	Data 1.96e-04 (2.53e-03)	Tok/s 50415 (70678)	Loss/tok 2.6917 (3.2090)	LR 2.000e-03
0: TRAIN [3][310/968]	Time 0.549 (0.394)	Data 2.14e-04 (2.46e-03)	Tok/s 84971 (70889)	Loss/tok 3.4337 (3.2137)	LR 2.000e-03
0: TRAIN [3][320/968]	Time 0.318 (0.393)	Data 1.73e-04 (2.39e-03)	Tok/s 64975 (70857)	Loss/tok 2.9994 (3.2124)	LR 2.000e-03
0: TRAIN [3][330/968]	Time 0.545 (0.392)	Data 1.94e-04 (2.32e-03)	Tok/s 85612 (70793)	Loss/tok 3.4442 (3.2131)	LR 2.000e-03
0: TRAIN [3][340/968]	Time 0.430 (0.394)	Data 1.95e-04 (2.26e-03)	Tok/s 79146 (70896)	Loss/tok 3.3160 (3.2167)	LR 2.000e-03
0: TRAIN [3][350/968]	Time 0.545 (0.396)	Data 2.08e-04 (2.20e-03)	Tok/s 86046 (71040)	Loss/tok 3.3366 (3.2211)	LR 2.000e-03
0: TRAIN [3][360/968]	Time 0.430 (0.396)	Data 1.76e-04 (2.14e-03)	Tok/s 77806 (71128)	Loss/tok 3.1668 (3.2212)	LR 2.000e-03
0: TRAIN [3][370/968]	Time 0.429 (0.396)	Data 1.73e-04 (2.09e-03)	Tok/s 78390 (71175)	Loss/tok 3.2975 (3.2218)	LR 2.000e-03
0: TRAIN [3][380/968]	Time 0.689 (0.396)	Data 1.99e-04 (2.04e-03)	Tok/s 86486 (71171)	Loss/tok 3.5828 (3.2219)	LR 2.000e-03
0: TRAIN [3][390/968]	Time 0.434 (0.396)	Data 1.98e-04 (1.99e-03)	Tok/s 78105 (71183)	Loss/tok 3.2456 (3.2204)	LR 2.000e-03
0: TRAIN [3][400/968]	Time 0.315 (0.395)	Data 2.32e-04 (1.95e-03)	Tok/s 65504 (71136)	Loss/tok 3.0621 (3.2190)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][410/968]	Time 0.319 (0.395)	Data 1.96e-04 (1.91e-03)	Tok/s 64693 (71120)	Loss/tok 3.0890 (3.2190)	LR 2.000e-03
0: TRAIN [3][420/968]	Time 0.432 (0.395)	Data 1.69e-04 (1.87e-03)	Tok/s 77581 (71163)	Loss/tok 3.3172 (3.2199)	LR 2.000e-03
0: TRAIN [3][430/968]	Time 0.208 (0.395)	Data 1.94e-04 (1.83e-03)	Tok/s 51053 (71063)	Loss/tok 2.5846 (3.2208)	LR 2.000e-03
0: TRAIN [3][440/968]	Time 0.320 (0.395)	Data 1.71e-04 (1.79e-03)	Tok/s 65539 (71124)	Loss/tok 3.0369 (3.2217)	LR 2.000e-03
0: TRAIN [3][450/968]	Time 0.319 (0.395)	Data 1.90e-04 (1.75e-03)	Tok/s 64392 (71129)	Loss/tok 3.0318 (3.2213)	LR 2.000e-03
0: TRAIN [3][460/968]	Time 0.431 (0.396)	Data 1.76e-04 (1.72e-03)	Tok/s 78063 (71217)	Loss/tok 3.1975 (3.2234)	LR 2.000e-03
0: TRAIN [3][470/968]	Time 0.315 (0.395)	Data 1.90e-04 (1.69e-03)	Tok/s 65420 (71154)	Loss/tok 3.0212 (3.2220)	LR 2.000e-03
0: TRAIN [3][480/968]	Time 0.546 (0.396)	Data 2.11e-04 (1.66e-03)	Tok/s 85888 (71271)	Loss/tok 3.3855 (3.2253)	LR 2.000e-03
0: TRAIN [3][490/968]	Time 0.319 (0.397)	Data 2.09e-04 (1.63e-03)	Tok/s 64942 (71332)	Loss/tok 3.0086 (3.2264)	LR 2.000e-03
0: TRAIN [3][500/968]	Time 0.545 (0.397)	Data 2.01e-04 (1.60e-03)	Tok/s 85681 (71333)	Loss/tok 3.4411 (3.2256)	LR 2.000e-03
0: TRAIN [3][510/968]	Time 0.319 (0.395)	Data 1.78e-04 (1.57e-03)	Tok/s 63834 (71232)	Loss/tok 2.9848 (3.2232)	LR 2.000e-03
0: TRAIN [3][520/968]	Time 0.428 (0.395)	Data 1.92e-04 (1.54e-03)	Tok/s 78818 (71168)	Loss/tok 3.1866 (3.2217)	LR 2.000e-03
0: TRAIN [3][530/968]	Time 0.317 (0.396)	Data 2.04e-04 (1.52e-03)	Tok/s 65380 (71269)	Loss/tok 2.9715 (3.2232)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][540/968]	Time 0.315 (0.396)	Data 2.02e-04 (1.50e-03)	Tok/s 66404 (71238)	Loss/tok 3.0225 (3.2245)	LR 2.000e-03
0: TRAIN [3][550/968]	Time 0.318 (0.395)	Data 2.06e-04 (1.47e-03)	Tok/s 64836 (71231)	Loss/tok 2.9505 (3.2229)	LR 2.000e-03
0: TRAIN [3][560/968]	Time 0.317 (0.395)	Data 1.73e-04 (1.45e-03)	Tok/s 64588 (71184)	Loss/tok 3.0222 (3.2228)	LR 2.000e-03
0: TRAIN [3][570/968]	Time 0.315 (0.395)	Data 2.10e-04 (1.43e-03)	Tok/s 66072 (71233)	Loss/tok 3.0551 (3.2233)	LR 2.000e-03
0: TRAIN [3][580/968]	Time 0.316 (0.395)	Data 1.97e-04 (1.41e-03)	Tok/s 65638 (71253)	Loss/tok 3.0529 (3.2223)	LR 2.000e-03
0: TRAIN [3][590/968]	Time 0.208 (0.395)	Data 2.02e-04 (1.39e-03)	Tok/s 50688 (71296)	Loss/tok 2.6222 (3.2237)	LR 2.000e-03
0: TRAIN [3][600/968]	Time 0.546 (0.396)	Data 2.30e-04 (1.37e-03)	Tok/s 85522 (71347)	Loss/tok 3.3032 (3.2235)	LR 2.000e-03
0: TRAIN [3][610/968]	Time 0.318 (0.395)	Data 1.93e-04 (1.35e-03)	Tok/s 64781 (71319)	Loss/tok 3.0593 (3.2229)	LR 2.000e-03
0: TRAIN [3][620/968]	Time 0.320 (0.395)	Data 1.77e-04 (1.33e-03)	Tok/s 63999 (71298)	Loss/tok 3.0179 (3.2229)	LR 2.000e-03
0: TRAIN [3][630/968]	Time 0.429 (0.396)	Data 1.76e-04 (1.31e-03)	Tok/s 78069 (71329)	Loss/tok 3.2721 (3.2233)	LR 2.000e-03
0: TRAIN [3][640/968]	Time 0.431 (0.395)	Data 2.33e-04 (1.29e-03)	Tok/s 78654 (71270)	Loss/tok 3.1964 (3.2227)	LR 2.000e-03
0: TRAIN [3][650/968]	Time 0.431 (0.395)	Data 1.70e-04 (1.28e-03)	Tok/s 77134 (71320)	Loss/tok 3.2941 (3.2230)	LR 2.000e-03
0: TRAIN [3][660/968]	Time 0.315 (0.395)	Data 1.93e-04 (1.26e-03)	Tok/s 65596 (71319)	Loss/tok 3.0436 (3.2226)	LR 2.000e-03
0: TRAIN [3][670/968]	Time 0.316 (0.395)	Data 2.01e-04 (1.24e-03)	Tok/s 64914 (71324)	Loss/tok 2.9544 (3.2213)	LR 2.000e-03
0: TRAIN [3][680/968]	Time 0.550 (0.395)	Data 1.82e-04 (1.23e-03)	Tok/s 84681 (71286)	Loss/tok 3.4259 (3.2208)	LR 2.000e-03
0: TRAIN [3][690/968]	Time 0.320 (0.395)	Data 1.90e-04 (1.21e-03)	Tok/s 66000 (71319)	Loss/tok 3.0006 (3.2204)	LR 2.000e-03
0: TRAIN [3][700/968]	Time 0.436 (0.395)	Data 1.66e-04 (1.20e-03)	Tok/s 76403 (71371)	Loss/tok 3.2472 (3.2212)	LR 2.000e-03
0: TRAIN [3][710/968]	Time 0.679 (0.396)	Data 1.80e-04 (1.18e-03)	Tok/s 86995 (71386)	Loss/tok 3.5748 (3.2225)	LR 2.000e-03
0: TRAIN [3][720/968]	Time 0.548 (0.396)	Data 1.77e-04 (1.17e-03)	Tok/s 85257 (71385)	Loss/tok 3.3826 (3.2221)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][730/968]	Time 0.427 (0.395)	Data 1.99e-04 (1.16e-03)	Tok/s 78698 (71364)	Loss/tok 3.2323 (3.2218)	LR 2.000e-03
0: TRAIN [3][740/968]	Time 0.432 (0.395)	Data 1.79e-04 (1.14e-03)	Tok/s 77626 (71381)	Loss/tok 3.2181 (3.2216)	LR 2.000e-03
0: TRAIN [3][750/968]	Time 0.550 (0.396)	Data 2.23e-04 (1.13e-03)	Tok/s 84259 (71450)	Loss/tok 3.4803 (3.2229)	LR 2.000e-03
0: TRAIN [3][760/968]	Time 0.318 (0.395)	Data 2.01e-04 (1.12e-03)	Tok/s 64464 (71413)	Loss/tok 3.0181 (3.2218)	LR 2.000e-03
0: TRAIN [3][770/968]	Time 0.432 (0.395)	Data 1.74e-04 (1.11e-03)	Tok/s 77542 (71388)	Loss/tok 3.1971 (3.2212)	LR 2.000e-03
0: TRAIN [3][780/968]	Time 0.320 (0.395)	Data 2.35e-04 (1.10e-03)	Tok/s 65424 (71353)	Loss/tok 3.0103 (3.2206)	LR 2.000e-03
0: TRAIN [3][790/968]	Time 0.550 (0.394)	Data 2.11e-04 (1.08e-03)	Tok/s 84219 (71325)	Loss/tok 3.3690 (3.2197)	LR 2.000e-03
0: TRAIN [3][800/968]	Time 0.552 (0.394)	Data 1.81e-04 (1.07e-03)	Tok/s 83918 (71341)	Loss/tok 3.4803 (3.2197)	LR 2.000e-03
0: TRAIN [3][810/968]	Time 0.429 (0.394)	Data 1.69e-04 (1.06e-03)	Tok/s 78310 (71348)	Loss/tok 3.2585 (3.2191)	LR 2.000e-03
0: TRAIN [3][820/968]	Time 0.686 (0.395)	Data 1.91e-04 (1.05e-03)	Tok/s 86337 (71365)	Loss/tok 3.6310 (3.2205)	LR 2.000e-03
0: TRAIN [3][830/968]	Time 0.315 (0.395)	Data 2.24e-04 (1.04e-03)	Tok/s 64475 (71373)	Loss/tok 2.9821 (3.2195)	LR 2.000e-03
0: TRAIN [3][840/968]	Time 0.318 (0.393)	Data 1.75e-04 (1.03e-03)	Tok/s 64927 (71228)	Loss/tok 2.9610 (3.2173)	LR 2.000e-03
0: TRAIN [3][850/968]	Time 0.320 (0.393)	Data 1.69e-04 (1.02e-03)	Tok/s 64075 (71209)	Loss/tok 3.0557 (3.2178)	LR 2.000e-03
0: TRAIN [3][860/968]	Time 0.320 (0.393)	Data 2.07e-04 (1.01e-03)	Tok/s 63688 (71218)	Loss/tok 2.9958 (3.2172)	LR 2.000e-03
0: TRAIN [3][870/968]	Time 0.207 (0.393)	Data 1.76e-04 (1.00e-03)	Tok/s 50880 (71220)	Loss/tok 2.6123 (3.2169)	LR 2.000e-03
0: TRAIN [3][880/968]	Time 0.432 (0.393)	Data 2.00e-04 (9.93e-04)	Tok/s 77537 (71242)	Loss/tok 3.1087 (3.2164)	LR 2.000e-03
0: TRAIN [3][890/968]	Time 0.321 (0.393)	Data 1.87e-04 (9.84e-04)	Tok/s 64698 (71202)	Loss/tok 3.0244 (3.2159)	LR 2.000e-03
0: TRAIN [3][900/968]	Time 0.689 (0.393)	Data 2.04e-04 (9.75e-04)	Tok/s 85851 (71265)	Loss/tok 3.5597 (3.2169)	LR 2.000e-03
0: TRAIN [3][910/968]	Time 0.317 (0.394)	Data 1.91e-04 (9.66e-04)	Tok/s 65833 (71301)	Loss/tok 3.0452 (3.2176)	LR 2.000e-03
0: TRAIN [3][920/968]	Time 0.433 (0.394)	Data 1.95e-04 (9.58e-04)	Tok/s 77176 (71354)	Loss/tok 3.1956 (3.2181)	LR 2.000e-03
0: TRAIN [3][930/968]	Time 0.430 (0.394)	Data 1.75e-04 (9.50e-04)	Tok/s 78844 (71326)	Loss/tok 3.2135 (3.2172)	LR 2.000e-03
0: TRAIN [3][940/968]	Time 0.320 (0.394)	Data 1.87e-04 (9.42e-04)	Tok/s 64792 (71332)	Loss/tok 2.9939 (3.2166)	LR 2.000e-03
0: TRAIN [3][950/968]	Time 0.320 (0.394)	Data 2.26e-04 (9.34e-04)	Tok/s 64964 (71378)	Loss/tok 2.9780 (3.2176)	LR 2.000e-03
0: TRAIN [3][960/968]	Time 0.322 (0.394)	Data 2.00e-04 (9.26e-04)	Tok/s 63876 (71374)	Loss/tok 2.9971 (3.2178)	LR 2.000e-03
:::MLL 1581977943.608 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 525}}
:::MLL 1581977943.609 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.727 (0.727)	Decoder iters 149.0 (149.0)	Tok/s 23049 (23049)
0: Running moses detokenizer
0: BLEU(score=22.34278268005915, counts=[36593, 17821, 9961, 5793], totals=[66931, 63928, 60925, 57926], precisions=[54.67272265467422, 27.876673757977724, 16.34961017644645, 10.000690536201361], bp=1.0, sys_len=66931, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1581977945.659 eval_accuracy: {"value": 22.34, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 536}}
:::MLL 1581977945.660 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 3	Training Loss: 3.2205	Test BLEU: 22.34
0: Performance: Epoch: 3	Training: 570390 Tok/s
0: Finished epoch 3
:::MLL 1581977945.660 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 558}}
:::MLL 1581977945.660 block_start: {"value": null, "metadata": {"first_epoch_num": 5, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1581977945.661 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 515}}
0: Starting epoch 4
0: Executing preallocation
0: Sampler for epoch 4 uses seed 2734740518
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [4][0/968]	Time 1.034 (1.034)	Data 7.14e-01 (7.14e-01)	Tok/s 19889 (19889)	Loss/tok 2.9941 (2.9941)	LR 2.000e-03
0: TRAIN [4][10/968]	Time 0.431 (0.426)	Data 1.64e-04 (6.50e-02)	Tok/s 77717 (64771)	Loss/tok 3.1183 (3.0591)	LR 2.000e-03
0: TRAIN [4][20/968]	Time 0.431 (0.386)	Data 1.61e-04 (3.41e-02)	Tok/s 77886 (65993)	Loss/tok 3.1786 (3.0359)	LR 2.000e-03
0: TRAIN [4][30/968]	Time 0.215 (0.412)	Data 1.81e-04 (2.32e-02)	Tok/s 48607 (69087)	Loss/tok 2.5485 (3.1299)	LR 2.000e-03
0: TRAIN [4][40/968]	Time 0.429 (0.406)	Data 1.90e-04 (1.76e-02)	Tok/s 78182 (69864)	Loss/tok 3.1130 (3.1226)	LR 2.000e-03
0: TRAIN [4][50/968]	Time 0.551 (0.398)	Data 1.54e-04 (1.42e-02)	Tok/s 84961 (69393)	Loss/tok 3.2896 (3.1243)	LR 2.000e-03
0: TRAIN [4][60/968]	Time 0.316 (0.406)	Data 1.61e-04 (1.19e-02)	Tok/s 65498 (70295)	Loss/tok 2.9331 (3.1451)	LR 2.000e-03
0: TRAIN [4][70/968]	Time 0.319 (0.404)	Data 1.44e-04 (1.02e-02)	Tok/s 64553 (70362)	Loss/tok 2.9661 (3.1394)	LR 2.000e-03
0: TRAIN [4][80/968]	Time 0.547 (0.406)	Data 1.69e-04 (8.98e-03)	Tok/s 85681 (71029)	Loss/tok 3.2086 (3.1372)	LR 2.000e-03
0: TRAIN [4][90/968]	Time 0.314 (0.400)	Data 1.65e-04 (8.01e-03)	Tok/s 65093 (70751)	Loss/tok 2.9215 (3.1292)	LR 2.000e-03
0: TRAIN [4][100/968]	Time 0.319 (0.400)	Data 1.67e-04 (7.23e-03)	Tok/s 65053 (70791)	Loss/tok 2.8900 (3.1308)	LR 2.000e-03
0: TRAIN [4][110/968]	Time 0.212 (0.399)	Data 1.58e-04 (6.59e-03)	Tok/s 49433 (70727)	Loss/tok 2.5394 (3.1315)	LR 2.000e-03
0: TRAIN [4][120/968]	Time 0.548 (0.405)	Data 1.85e-04 (6.06e-03)	Tok/s 85001 (71310)	Loss/tok 3.2447 (3.1434)	LR 2.000e-03
0: TRAIN [4][130/968]	Time 0.320 (0.407)	Data 1.57e-04 (5.61e-03)	Tok/s 64553 (71476)	Loss/tok 2.9422 (3.1525)	LR 2.000e-03
0: TRAIN [4][140/968]	Time 0.321 (0.408)	Data 1.54e-04 (5.23e-03)	Tok/s 64611 (71481)	Loss/tok 2.9079 (3.1558)	LR 2.000e-03
0: TRAIN [4][150/968]	Time 0.317 (0.405)	Data 1.51e-04 (4.89e-03)	Tok/s 65749 (71244)	Loss/tok 3.0033 (3.1528)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [4][160/968]	Time 0.316 (0.406)	Data 1.60e-04 (4.60e-03)	Tok/s 64964 (71221)	Loss/tok 2.9098 (3.1614)	LR 2.000e-03
0: TRAIN [4][170/968]	Time 0.429 (0.404)	Data 1.91e-04 (4.34e-03)	Tok/s 78400 (71061)	Loss/tok 3.1870 (3.1609)	LR 2.000e-03
0: TRAIN [4][180/968]	Time 0.688 (0.401)	Data 1.45e-04 (4.11e-03)	Tok/s 86628 (70819)	Loss/tok 3.4960 (3.1596)	LR 2.000e-03
0: TRAIN [4][190/968]	Time 0.319 (0.399)	Data 1.48e-04 (3.90e-03)	Tok/s 64545 (70733)	Loss/tok 2.9755 (3.1577)	LR 2.000e-03
0: TRAIN [4][200/968]	Time 0.427 (0.406)	Data 1.56e-04 (3.72e-03)	Tok/s 77836 (71305)	Loss/tok 3.1332 (3.1716)	LR 2.000e-03
0: TRAIN [4][210/968]	Time 0.318 (0.406)	Data 1.65e-04 (3.55e-03)	Tok/s 64621 (71439)	Loss/tok 2.9767 (3.1709)	LR 2.000e-03
0: TRAIN [4][220/968]	Time 0.551 (0.407)	Data 1.70e-04 (3.39e-03)	Tok/s 85569 (71462)	Loss/tok 3.2586 (3.1710)	LR 2.000e-03
0: TRAIN [4][230/968]	Time 0.431 (0.406)	Data 1.72e-04 (3.26e-03)	Tok/s 78280 (71529)	Loss/tok 3.2069 (3.1697)	LR 2.000e-03
0: TRAIN [4][240/968]	Time 0.318 (0.406)	Data 2.00e-04 (3.13e-03)	Tok/s 64284 (71474)	Loss/tok 3.0206 (3.1728)	LR 2.000e-03
0: TRAIN [4][250/968]	Time 0.319 (0.404)	Data 1.67e-04 (3.01e-03)	Tok/s 65353 (71372)	Loss/tok 2.9919 (3.1686)	LR 2.000e-03
0: TRAIN [4][260/968]	Time 0.550 (0.402)	Data 1.58e-04 (2.90e-03)	Tok/s 84359 (71239)	Loss/tok 3.3800 (3.1659)	LR 2.000e-03
0: TRAIN [4][270/968]	Time 0.431 (0.402)	Data 1.53e-04 (2.80e-03)	Tok/s 78080 (71290)	Loss/tok 3.2206 (3.1642)	LR 2.000e-03
0: TRAIN [4][280/968]	Time 0.428 (0.403)	Data 1.63e-04 (2.71e-03)	Tok/s 77993 (71508)	Loss/tok 3.1577 (3.1659)	LR 2.000e-03
0: TRAIN [4][290/968]	Time 0.317 (0.402)	Data 2.06e-04 (2.62e-03)	Tok/s 66130 (71423)	Loss/tok 2.9867 (3.1632)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [4][300/968]	Time 0.688 (0.402)	Data 1.45e-04 (2.54e-03)	Tok/s 86351 (71418)	Loss/tok 3.5223 (3.1658)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [4][310/968]	Time 0.435 (0.400)	Data 1.58e-04 (2.46e-03)	Tok/s 77226 (71260)	Loss/tok 3.1408 (3.1630)	LR 2.000e-03
0: TRAIN [4][320/968]	Time 0.318 (0.399)	Data 1.70e-04 (2.39e-03)	Tok/s 64725 (71185)	Loss/tok 2.9601 (3.1596)	LR 2.000e-03
0: TRAIN [4][330/968]	Time 0.320 (0.398)	Data 1.81e-04 (2.33e-03)	Tok/s 64562 (71119)	Loss/tok 2.9232 (3.1578)	LR 2.000e-03
0: TRAIN [4][340/968]	Time 0.550 (0.398)	Data 2.02e-04 (2.26e-03)	Tok/s 84883 (71205)	Loss/tok 3.2763 (3.1574)	LR 2.000e-03
0: TRAIN [4][350/968]	Time 0.319 (0.399)	Data 2.05e-04 (2.20e-03)	Tok/s 65389 (71280)	Loss/tok 2.9712 (3.1576)	LR 2.000e-03
0: TRAIN [4][360/968]	Time 0.317 (0.397)	Data 1.93e-04 (2.15e-03)	Tok/s 64574 (71118)	Loss/tok 2.9644 (3.1541)	LR 2.000e-03
0: TRAIN [4][370/968]	Time 0.430 (0.397)	Data 2.33e-04 (2.10e-03)	Tok/s 77733 (71116)	Loss/tok 3.1862 (3.1537)	LR 2.000e-03
0: TRAIN [4][380/968]	Time 0.429 (0.397)	Data 1.94e-04 (2.05e-03)	Tok/s 78679 (71220)	Loss/tok 3.1515 (3.1532)	LR 2.000e-03
0: TRAIN [4][390/968]	Time 0.322 (0.397)	Data 1.97e-04 (2.00e-03)	Tok/s 64425 (71293)	Loss/tok 2.9232 (3.1536)	LR 2.000e-03
0: TRAIN [4][400/968]	Time 0.430 (0.398)	Data 1.79e-04 (1.95e-03)	Tok/s 78802 (71419)	Loss/tok 3.0893 (3.1535)	LR 2.000e-03
0: TRAIN [4][410/968]	Time 0.316 (0.396)	Data 2.21e-04 (1.91e-03)	Tok/s 65919 (71299)	Loss/tok 3.0057 (3.1516)	LR 2.000e-03
0: TRAIN [4][420/968]	Time 0.321 (0.395)	Data 1.94e-04 (1.87e-03)	Tok/s 64894 (71212)	Loss/tok 2.9756 (3.1496)	LR 2.000e-03
0: TRAIN [4][430/968]	Time 0.431 (0.397)	Data 1.80e-04 (1.83e-03)	Tok/s 78378 (71348)	Loss/tok 3.1279 (3.1550)	LR 2.000e-03
0: TRAIN [4][440/968]	Time 0.210 (0.397)	Data 1.95e-04 (1.79e-03)	Tok/s 50502 (71318)	Loss/tok 2.5608 (3.1547)	LR 2.000e-03
0: TRAIN [4][450/968]	Time 0.431 (0.396)	Data 1.77e-04 (1.76e-03)	Tok/s 77184 (71265)	Loss/tok 3.2418 (3.1538)	LR 2.000e-03
0: TRAIN [4][460/968]	Time 0.322 (0.395)	Data 1.98e-04 (1.72e-03)	Tok/s 64193 (71202)	Loss/tok 3.0773 (3.1521)	LR 2.000e-03
0: TRAIN [4][470/968]	Time 0.211 (0.394)	Data 1.70e-04 (1.69e-03)	Tok/s 49641 (71113)	Loss/tok 2.5827 (3.1517)	LR 2.000e-03
0: TRAIN [4][480/968]	Time 0.549 (0.394)	Data 1.82e-04 (1.66e-03)	Tok/s 84962 (71097)	Loss/tok 3.3218 (3.1518)	LR 2.000e-03
0: TRAIN [4][490/968]	Time 0.321 (0.393)	Data 1.92e-04 (1.63e-03)	Tok/s 63338 (71020)	Loss/tok 3.0027 (3.1499)	LR 2.000e-03
0: TRAIN [4][500/968]	Time 0.547 (0.393)	Data 1.84e-04 (1.60e-03)	Tok/s 85007 (70940)	Loss/tok 3.2498 (3.1501)	LR 2.000e-03
0: TRAIN [4][510/968]	Time 0.432 (0.392)	Data 2.01e-04 (1.57e-03)	Tok/s 77731 (70934)	Loss/tok 3.2060 (3.1497)	LR 2.000e-03
0: TRAIN [4][520/968]	Time 0.316 (0.393)	Data 1.83e-04 (1.55e-03)	Tok/s 65490 (71034)	Loss/tok 3.0108 (3.1512)	LR 2.000e-03
0: TRAIN [4][530/968]	Time 0.319 (0.393)	Data 1.85e-04 (1.52e-03)	Tok/s 64950 (71055)	Loss/tok 3.0228 (3.1506)	LR 2.000e-03
0: TRAIN [4][540/968]	Time 0.320 (0.394)	Data 2.02e-04 (1.50e-03)	Tok/s 65250 (71079)	Loss/tok 2.9752 (3.1521)	LR 2.000e-03
0: TRAIN [4][550/968]	Time 0.209 (0.394)	Data 1.84e-04 (1.47e-03)	Tok/s 50080 (71114)	Loss/tok 2.6682 (3.1527)	LR 2.000e-03
0: TRAIN [4][560/968]	Time 0.429 (0.394)	Data 1.97e-04 (1.45e-03)	Tok/s 78196 (71107)	Loss/tok 3.1663 (3.1527)	LR 2.000e-03
0: TRAIN [4][570/968]	Time 0.683 (0.393)	Data 1.92e-04 (1.43e-03)	Tok/s 87086 (71053)	Loss/tok 3.5235 (3.1522)	LR 2.000e-03
0: TRAIN [4][580/968]	Time 0.554 (0.394)	Data 2.06e-04 (1.41e-03)	Tok/s 85012 (71110)	Loss/tok 3.2960 (3.1521)	LR 2.000e-03
0: TRAIN [4][590/968]	Time 0.211 (0.393)	Data 1.75e-04 (1.39e-03)	Tok/s 50011 (71104)	Loss/tok 2.6577 (3.1514)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [4][600/968]	Time 0.556 (0.394)	Data 1.93e-04 (1.37e-03)	Tok/s 83954 (71178)	Loss/tok 3.3924 (3.1523)	LR 2.000e-03
0: TRAIN [4][610/968]	Time 0.430 (0.394)	Data 1.82e-04 (1.35e-03)	Tok/s 78605 (71174)	Loss/tok 3.2314 (3.1516)	LR 2.000e-03
0: TRAIN [4][620/968]	Time 0.429 (0.394)	Data 1.89e-04 (1.33e-03)	Tok/s 78555 (71193)	Loss/tok 3.1075 (3.1514)	LR 2.000e-03
0: TRAIN [4][630/968]	Time 0.318 (0.393)	Data 1.64e-04 (1.31e-03)	Tok/s 65363 (71151)	Loss/tok 2.9938 (3.1512)	LR 2.000e-03
0: TRAIN [4][640/968]	Time 0.687 (0.394)	Data 1.83e-04 (1.29e-03)	Tok/s 87772 (71185)	Loss/tok 3.4660 (3.1519)	LR 2.000e-03
0: TRAIN [4][650/968]	Time 0.319 (0.394)	Data 1.92e-04 (1.28e-03)	Tok/s 64732 (71190)	Loss/tok 2.9816 (3.1523)	LR 2.000e-03
0: TRAIN [4][660/968]	Time 0.318 (0.393)	Data 2.02e-04 (1.26e-03)	Tok/s 65752 (71162)	Loss/tok 3.0379 (3.1519)	LR 2.000e-03
0: TRAIN [4][670/968]	Time 0.214 (0.393)	Data 1.96e-04 (1.24e-03)	Tok/s 49291 (71143)	Loss/tok 2.5125 (3.1523)	LR 2.000e-03
0: TRAIN [4][680/968]	Time 0.319 (0.393)	Data 1.83e-04 (1.23e-03)	Tok/s 64108 (71097)	Loss/tok 3.0165 (3.1511)	LR 2.000e-03
0: TRAIN [4][690/968]	Time 0.318 (0.392)	Data 2.01e-04 (1.21e-03)	Tok/s 65575 (71036)	Loss/tok 2.9294 (3.1495)	LR 2.000e-03
0: TRAIN [4][700/968]	Time 0.552 (0.391)	Data 2.09e-04 (1.20e-03)	Tok/s 84515 (70957)	Loss/tok 3.3132 (3.1486)	LR 2.000e-03
0: TRAIN [4][710/968]	Time 0.316 (0.392)	Data 1.87e-04 (1.18e-03)	Tok/s 65804 (71041)	Loss/tok 3.0085 (3.1509)	LR 2.000e-03
0: TRAIN [4][720/968]	Time 0.428 (0.392)	Data 2.14e-04 (1.17e-03)	Tok/s 78956 (71044)	Loss/tok 3.0910 (3.1508)	LR 2.000e-03
0: TRAIN [4][730/968]	Time 0.553 (0.392)	Data 1.91e-04 (1.16e-03)	Tok/s 83742 (71003)	Loss/tok 3.3719 (3.1508)	LR 2.000e-03
0: TRAIN [4][740/968]	Time 0.550 (0.392)	Data 1.83e-04 (1.14e-03)	Tok/s 84320 (71070)	Loss/tok 3.3035 (3.1514)	LR 2.000e-03
0: TRAIN [4][750/968]	Time 0.545 (0.392)	Data 1.75e-04 (1.13e-03)	Tok/s 85458 (71087)	Loss/tok 3.3154 (3.1510)	LR 2.000e-03
0: TRAIN [4][760/968]	Time 0.320 (0.392)	Data 1.84e-04 (1.12e-03)	Tok/s 64245 (71026)	Loss/tok 2.9180 (3.1507)	LR 2.000e-03
0: TRAIN [4][770/968]	Time 0.429 (0.392)	Data 1.78e-04 (1.11e-03)	Tok/s 78344 (71000)	Loss/tok 3.2254 (3.1504)	LR 2.000e-03
0: TRAIN [4][780/968]	Time 0.322 (0.391)	Data 1.90e-04 (1.10e-03)	Tok/s 64528 (70974)	Loss/tok 2.9501 (3.1497)	LR 2.000e-03
0: TRAIN [4][790/968]	Time 0.322 (0.392)	Data 1.97e-04 (1.08e-03)	Tok/s 63265 (71029)	Loss/tok 2.9924 (3.1513)	LR 2.000e-03
0: TRAIN [4][800/968]	Time 0.320 (0.392)	Data 2.30e-04 (1.07e-03)	Tok/s 64366 (71016)	Loss/tok 2.9023 (3.1506)	LR 2.000e-03
0: TRAIN [4][810/968]	Time 0.430 (0.392)	Data 1.72e-04 (1.06e-03)	Tok/s 78871 (71013)	Loss/tok 3.2129 (3.1512)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [4][820/968]	Time 0.550 (0.392)	Data 1.94e-04 (1.05e-03)	Tok/s 84493 (71066)	Loss/tok 3.2819 (3.1519)	LR 2.000e-03
0: TRAIN [4][830/968]	Time 0.318 (0.393)	Data 2.08e-04 (1.04e-03)	Tok/s 65695 (71122)	Loss/tok 3.0614 (3.1536)	LR 2.000e-03
0: TRAIN [4][840/968]	Time 0.432 (0.394)	Data 1.82e-04 (1.03e-03)	Tok/s 78088 (71201)	Loss/tok 3.1263 (3.1547)	LR 2.000e-03
0: TRAIN [4][850/968]	Time 0.320 (0.394)	Data 1.77e-04 (1.02e-03)	Tok/s 65897 (71226)	Loss/tok 2.9314 (3.1553)	LR 2.000e-03
0: TRAIN [4][860/968]	Time 0.548 (0.394)	Data 2.08e-04 (1.01e-03)	Tok/s 85122 (71216)	Loss/tok 3.2793 (3.1555)	LR 2.000e-03
0: TRAIN [4][870/968]	Time 0.324 (0.394)	Data 1.66e-04 (1.00e-03)	Tok/s 63822 (71184)	Loss/tok 2.9272 (3.1555)	LR 2.000e-03
0: TRAIN [4][880/968]	Time 0.314 (0.394)	Data 2.30e-04 (9.94e-04)	Tok/s 65798 (71190)	Loss/tok 2.9799 (3.1553)	LR 2.000e-03
0: TRAIN [4][890/968]	Time 0.550 (0.394)	Data 1.76e-04 (9.85e-04)	Tok/s 85506 (71240)	Loss/tok 3.3208 (3.1557)	LR 2.000e-03
0: TRAIN [4][900/968]	Time 0.323 (0.394)	Data 1.88e-04 (9.76e-04)	Tok/s 63986 (71215)	Loss/tok 2.9575 (3.1551)	LR 2.000e-03
0: TRAIN [4][910/968]	Time 0.555 (0.394)	Data 1.73e-04 (9.67e-04)	Tok/s 84147 (71243)	Loss/tok 3.4167 (3.1559)	LR 2.000e-03
0: TRAIN [4][920/968]	Time 0.210 (0.393)	Data 1.91e-04 (9.59e-04)	Tok/s 48802 (71136)	Loss/tok 2.5164 (3.1544)	LR 2.000e-03
0: TRAIN [4][930/968]	Time 0.550 (0.394)	Data 1.70e-04 (9.51e-04)	Tok/s 85765 (71188)	Loss/tok 3.3252 (3.1553)	LR 2.000e-03
0: TRAIN [4][940/968]	Time 0.690 (0.394)	Data 2.07e-04 (9.43e-04)	Tok/s 86071 (71245)	Loss/tok 3.4796 (3.1574)	LR 2.000e-03
0: TRAIN [4][950/968]	Time 0.318 (0.394)	Data 1.87e-04 (9.35e-04)	Tok/s 65002 (71182)	Loss/tok 2.9575 (3.1564)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [4][960/968]	Time 0.429 (0.394)	Data 1.93e-04 (9.27e-04)	Tok/s 77813 (71228)	Loss/tok 3.2271 (3.1580)	LR 2.000e-03
:::MLL 1581978328.685 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 525}}
:::MLL 1581978328.686 eval_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [4][0/3]	Time 0.613 (0.613)	Decoder iters 102.0 (102.0)	Tok/s 26479 (26479)
0: Running moses detokenizer
0: BLEU(score=23.100320592925605, counts=[36500, 17957, 10107, 5951], totals=[65594, 62591, 59588, 56589], precisions=[55.645333414641584, 28.689428192551645, 16.961468752097737, 10.516178055805899], bp=1.0, sys_len=65594, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1581978330.520 eval_accuracy: {"value": 23.1, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 536}}
:::MLL 1581978330.520 eval_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 4	Training Loss: 3.1599	Test BLEU: 23.10
0: Performance: Epoch: 4	Training: 569775 Tok/s
0: Finished epoch 4
:::MLL 1581978330.521 block_stop: {"value": null, "metadata": {"first_epoch_num": 5, "file": "train.py", "lineno": 558}}
:::MLL 1581978330.521 block_start: {"value": null, "metadata": {"first_epoch_num": 6, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1581978330.521 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 515}}
0: Starting epoch 5
0: Executing preallocation
0: Sampler for epoch 5 uses seed 3532599105
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [5][0/968]	Time 1.278 (1.278)	Data 7.08e-01 (7.08e-01)	Tok/s 36562 (36562)	Loss/tok 3.2682 (3.2682)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [5][10/968]	Time 0.320 (0.478)	Data 1.62e-04 (6.46e-02)	Tok/s 63968 (69583)	Loss/tok 2.8001 (3.1008)	LR 2.000e-03
0: TRAIN [5][20/968]	Time 0.553 (0.440)	Data 1.89e-04 (3.39e-02)	Tok/s 84053 (71058)	Loss/tok 3.2554 (3.0830)	LR 2.000e-03
0: TRAIN [5][30/968]	Time 0.552 (0.441)	Data 1.62e-04 (2.30e-02)	Tok/s 85157 (73060)	Loss/tok 3.1474 (3.0925)	LR 2.000e-03
0: TRAIN [5][40/968]	Time 0.429 (0.430)	Data 1.74e-04 (1.75e-02)	Tok/s 78816 (73196)	Loss/tok 3.0681 (3.0863)	LR 2.000e-03
0: TRAIN [5][50/968]	Time 0.314 (0.425)	Data 1.87e-04 (1.41e-02)	Tok/s 65462 (73016)	Loss/tok 2.8581 (3.0874)	LR 2.000e-03
0: TRAIN [5][60/968]	Time 0.322 (0.420)	Data 1.58e-04 (1.18e-02)	Tok/s 64669 (73113)	Loss/tok 2.8058 (3.0828)	LR 2.000e-03
0: TRAIN [5][70/968]	Time 0.549 (0.417)	Data 1.78e-04 (1.02e-02)	Tok/s 85010 (73058)	Loss/tok 3.2114 (3.0845)	LR 2.000e-03
0: TRAIN [5][80/968]	Time 0.547 (0.422)	Data 1.81e-04 (8.93e-03)	Tok/s 85297 (73482)	Loss/tok 3.2949 (3.0993)	LR 2.000e-03
0: TRAIN [5][90/968]	Time 0.316 (0.415)	Data 1.73e-04 (7.96e-03)	Tok/s 65085 (72899)	Loss/tok 2.9126 (3.0926)	LR 2.000e-03
0: TRAIN [5][100/968]	Time 0.428 (0.412)	Data 1.58e-04 (7.19e-03)	Tok/s 78383 (72852)	Loss/tok 3.0579 (3.0875)	LR 2.000e-03
0: TRAIN [5][110/968]	Time 0.431 (0.406)	Data 1.85e-04 (6.56e-03)	Tok/s 77258 (72490)	Loss/tok 3.1202 (3.0812)	LR 2.000e-03
0: TRAIN [5][120/968]	Time 0.319 (0.404)	Data 2.02e-04 (6.03e-03)	Tok/s 64972 (72210)	Loss/tok 2.9358 (3.0816)	LR 2.000e-03
0: TRAIN [5][130/968]	Time 0.316 (0.401)	Data 1.66e-04 (5.59e-03)	Tok/s 65103 (72013)	Loss/tok 2.9255 (3.0781)	LR 2.000e-03
0: TRAIN [5][140/968]	Time 0.319 (0.398)	Data 1.88e-04 (5.20e-03)	Tok/s 64759 (71767)	Loss/tok 2.9205 (3.0732)	LR 2.000e-03
0: TRAIN [5][150/968]	Time 0.430 (0.395)	Data 1.76e-04 (4.87e-03)	Tok/s 78931 (71644)	Loss/tok 3.0594 (3.0702)	LR 2.000e-03
0: TRAIN [5][160/968]	Time 0.320 (0.398)	Data 1.92e-04 (4.58e-03)	Tok/s 64014 (71793)	Loss/tok 2.8271 (3.0770)	LR 2.000e-03
0: TRAIN [5][170/968]	Time 0.319 (0.394)	Data 1.89e-04 (4.32e-03)	Tok/s 64210 (71424)	Loss/tok 2.9642 (3.0715)	LR 2.000e-03
0: TRAIN [5][180/968]	Time 0.690 (0.396)	Data 1.56e-04 (4.09e-03)	Tok/s 85822 (71578)	Loss/tok 3.4157 (3.0793)	LR 2.000e-03
0: TRAIN [5][190/968]	Time 0.317 (0.395)	Data 1.78e-04 (3.89e-03)	Tok/s 65653 (71403)	Loss/tok 2.9330 (3.0790)	LR 2.000e-03
0: TRAIN [5][200/968]	Time 0.318 (0.393)	Data 1.92e-04 (3.70e-03)	Tok/s 64760 (71226)	Loss/tok 3.0045 (3.0768)	LR 2.000e-03
0: TRAIN [5][210/968]	Time 0.690 (0.394)	Data 1.81e-04 (3.54e-03)	Tok/s 86909 (71265)	Loss/tok 3.5243 (3.0823)	LR 2.000e-03
0: TRAIN [5][220/968]	Time 0.429 (0.392)	Data 1.55e-04 (3.38e-03)	Tok/s 78843 (71219)	Loss/tok 3.0706 (3.0792)	LR 2.000e-03
0: TRAIN [5][230/968]	Time 0.211 (0.391)	Data 1.58e-04 (3.25e-03)	Tok/s 49987 (71100)	Loss/tok 2.5599 (3.0780)	LR 2.000e-03
0: TRAIN [5][240/968]	Time 0.320 (0.390)	Data 1.77e-04 (3.12e-03)	Tok/s 64021 (70895)	Loss/tok 2.9533 (3.0814)	LR 2.000e-03
0: TRAIN [5][250/968]	Time 0.322 (0.389)	Data 1.74e-04 (3.00e-03)	Tok/s 64113 (70789)	Loss/tok 3.0011 (3.0811)	LR 2.000e-03
0: TRAIN [5][260/968]	Time 0.319 (0.391)	Data 1.62e-04 (2.89e-03)	Tok/s 64193 (70970)	Loss/tok 2.8542 (3.0849)	LR 2.000e-03
0: TRAIN [5][270/968]	Time 0.319 (0.391)	Data 1.92e-04 (2.79e-03)	Tok/s 64045 (70910)	Loss/tok 2.9075 (3.0842)	LR 2.000e-03
0: TRAIN [5][280/968]	Time 0.208 (0.391)	Data 1.93e-04 (2.70e-03)	Tok/s 50683 (70887)	Loss/tok 2.5812 (3.0881)	LR 2.000e-03
0: TRAIN [5][290/968]	Time 0.430 (0.392)	Data 1.98e-04 (2.61e-03)	Tok/s 78438 (71025)	Loss/tok 3.1480 (3.0888)	LR 2.000e-03
0: TRAIN [5][300/968]	Time 0.320 (0.392)	Data 2.20e-04 (2.53e-03)	Tok/s 64077 (71020)	Loss/tok 2.8890 (3.0894)	LR 2.000e-03
0: TRAIN [5][310/968]	Time 0.318 (0.392)	Data 1.90e-04 (2.46e-03)	Tok/s 65611 (71078)	Loss/tok 2.9225 (3.0918)	LR 2.000e-03
0: TRAIN [5][320/968]	Time 0.430 (0.392)	Data 2.17e-04 (2.39e-03)	Tok/s 78399 (71106)	Loss/tok 3.0188 (3.0910)	LR 2.000e-03
0: TRAIN [5][330/968]	Time 0.318 (0.392)	Data 1.85e-04 (2.32e-03)	Tok/s 65184 (71055)	Loss/tok 2.9645 (3.0912)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][340/968]	Time 0.317 (0.391)	Data 2.09e-04 (2.26e-03)	Tok/s 64833 (70988)	Loss/tok 2.8961 (3.0921)	LR 2.000e-03
0: TRAIN [5][350/968]	Time 0.428 (0.392)	Data 1.77e-04 (2.20e-03)	Tok/s 78538 (71058)	Loss/tok 3.0915 (3.0931)	LR 2.000e-03
0: TRAIN [5][360/968]	Time 0.320 (0.391)	Data 1.95e-04 (2.14e-03)	Tok/s 65076 (70975)	Loss/tok 2.9105 (3.0911)	LR 2.000e-03
0: TRAIN [5][370/968]	Time 0.689 (0.390)	Data 1.96e-04 (2.09e-03)	Tok/s 87846 (70813)	Loss/tok 3.4512 (3.0924)	LR 2.000e-03
0: TRAIN [5][380/968]	Time 0.318 (0.391)	Data 2.24e-04 (2.04e-03)	Tok/s 64551 (70888)	Loss/tok 2.9110 (3.0940)	LR 2.000e-03
0: TRAIN [5][390/968]	Time 0.549 (0.392)	Data 1.84e-04 (2.00e-03)	Tok/s 85204 (71021)	Loss/tok 3.2489 (3.0967)	LR 2.000e-03
0: TRAIN [5][400/968]	Time 0.320 (0.393)	Data 2.19e-04 (1.95e-03)	Tok/s 63040 (71135)	Loss/tok 2.8996 (3.0984)	LR 2.000e-03
0: TRAIN [5][410/968]	Time 0.318 (0.393)	Data 2.21e-04 (1.91e-03)	Tok/s 65291 (71104)	Loss/tok 2.8981 (3.1011)	LR 2.000e-03
0: TRAIN [5][420/968]	Time 0.320 (0.392)	Data 2.01e-04 (1.87e-03)	Tok/s 65081 (70999)	Loss/tok 2.8719 (3.0990)	LR 2.000e-03
0: TRAIN [5][430/968]	Time 0.214 (0.393)	Data 1.96e-04 (1.83e-03)	Tok/s 49066 (71046)	Loss/tok 2.5073 (3.1008)	LR 2.000e-03
0: TRAIN [5][440/968]	Time 0.320 (0.393)	Data 1.86e-04 (1.80e-03)	Tok/s 65206 (71052)	Loss/tok 2.9461 (3.1009)	LR 2.000e-03
0: TRAIN [5][450/968]	Time 0.319 (0.392)	Data 2.24e-04 (1.76e-03)	Tok/s 64234 (70975)	Loss/tok 2.9109 (3.0991)	LR 2.000e-03
0: TRAIN [5][460/968]	Time 0.319 (0.392)	Data 2.20e-04 (1.73e-03)	Tok/s 64836 (70959)	Loss/tok 2.9106 (3.0996)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][470/968]	Time 0.691 (0.392)	Data 1.78e-04 (1.69e-03)	Tok/s 86277 (70998)	Loss/tok 3.4665 (3.1012)	LR 2.000e-03
0: TRAIN [5][480/968]	Time 0.554 (0.393)	Data 2.21e-04 (1.66e-03)	Tok/s 83971 (71047)	Loss/tok 3.2948 (3.1037)	LR 2.000e-03
0: TRAIN [5][490/968]	Time 0.431 (0.393)	Data 1.80e-04 (1.63e-03)	Tok/s 77768 (71084)	Loss/tok 3.1634 (3.1045)	LR 2.000e-03
0: TRAIN [5][500/968]	Time 0.430 (0.394)	Data 2.14e-04 (1.61e-03)	Tok/s 78051 (71121)	Loss/tok 3.1136 (3.1063)	LR 2.000e-03
0: TRAIN [5][510/968]	Time 0.316 (0.392)	Data 2.00e-04 (1.58e-03)	Tok/s 64839 (70911)	Loss/tok 2.9275 (3.1032)	LR 2.000e-03
0: TRAIN [5][520/968]	Time 0.549 (0.392)	Data 1.84e-04 (1.55e-03)	Tok/s 84851 (70957)	Loss/tok 3.2630 (3.1043)	LR 2.000e-03
0: TRAIN [5][530/968]	Time 0.685 (0.392)	Data 2.24e-04 (1.53e-03)	Tok/s 86807 (70940)	Loss/tok 3.4554 (3.1045)	LR 2.000e-03
0: TRAIN [5][540/968]	Time 0.319 (0.393)	Data 1.73e-04 (1.50e-03)	Tok/s 65230 (71015)	Loss/tok 2.9925 (3.1047)	LR 2.000e-03
0: TRAIN [5][550/968]	Time 0.318 (0.393)	Data 1.97e-04 (1.48e-03)	Tok/s 64106 (70984)	Loss/tok 2.9088 (3.1058)	LR 2.000e-03
0: TRAIN [5][560/968]	Time 0.210 (0.393)	Data 1.94e-04 (1.45e-03)	Tok/s 50212 (71013)	Loss/tok 2.5647 (3.1086)	LR 2.000e-03
0: TRAIN [5][570/968]	Time 0.317 (0.395)	Data 2.01e-04 (1.43e-03)	Tok/s 64718 (71149)	Loss/tok 2.9239 (3.1113)	LR 2.000e-03
0: TRAIN [5][580/968]	Time 0.428 (0.394)	Data 2.07e-04 (1.41e-03)	Tok/s 79242 (71155)	Loss/tok 3.0622 (3.1099)	LR 2.000e-03
0: TRAIN [5][590/968]	Time 0.428 (0.394)	Data 2.01e-04 (1.39e-03)	Tok/s 78979 (71113)	Loss/tok 3.1332 (3.1090)	LR 2.000e-03
0: TRAIN [5][600/968]	Time 0.321 (0.394)	Data 2.07e-04 (1.37e-03)	Tok/s 64251 (71095)	Loss/tok 2.9307 (3.1092)	LR 2.000e-03
0: TRAIN [5][610/968]	Time 0.549 (0.394)	Data 2.05e-04 (1.35e-03)	Tok/s 84477 (71166)	Loss/tok 3.3086 (3.1112)	LR 2.000e-03
0: TRAIN [5][620/968]	Time 0.430 (0.394)	Data 1.78e-04 (1.33e-03)	Tok/s 77357 (71133)	Loss/tok 3.1365 (3.1105)	LR 2.000e-03
0: TRAIN [5][630/968]	Time 0.319 (0.394)	Data 2.49e-04 (1.31e-03)	Tok/s 65290 (71083)	Loss/tok 2.8885 (3.1104)	LR 2.000e-03
0: TRAIN [5][640/968]	Time 0.322 (0.392)	Data 2.06e-04 (1.30e-03)	Tok/s 63320 (70979)	Loss/tok 2.8929 (3.1085)	LR 2.000e-03
0: TRAIN [5][650/968]	Time 0.690 (0.393)	Data 1.96e-04 (1.28e-03)	Tok/s 85887 (71048)	Loss/tok 3.5406 (3.1106)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][660/968]	Time 0.551 (0.394)	Data 2.09e-04 (1.26e-03)	Tok/s 84513 (71088)	Loss/tok 3.3329 (3.1120)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][670/968]	Time 0.317 (0.394)	Data 1.86e-04 (1.25e-03)	Tok/s 64800 (71083)	Loss/tok 2.8699 (3.1114)	LR 2.000e-03
0: TRAIN [5][680/968]	Time 0.550 (0.394)	Data 1.98e-04 (1.23e-03)	Tok/s 84708 (71105)	Loss/tok 3.3772 (3.1117)	LR 2.000e-03
0: TRAIN [5][690/968]	Time 0.320 (0.393)	Data 1.96e-04 (1.22e-03)	Tok/s 65111 (71075)	Loss/tok 2.9353 (3.1116)	LR 2.000e-03
0: TRAIN [5][700/968]	Time 0.318 (0.394)	Data 2.13e-04 (1.20e-03)	Tok/s 64822 (71139)	Loss/tok 2.8969 (3.1143)	LR 2.000e-03
0: TRAIN [5][710/968]	Time 0.429 (0.395)	Data 1.70e-04 (1.19e-03)	Tok/s 77930 (71145)	Loss/tok 3.1116 (3.1160)	LR 2.000e-03
0: TRAIN [5][720/968]	Time 0.429 (0.395)	Data 1.73e-04 (1.17e-03)	Tok/s 78120 (71108)	Loss/tok 3.1836 (3.1162)	LR 2.000e-03
0: TRAIN [5][730/968]	Time 0.321 (0.394)	Data 2.01e-04 (1.16e-03)	Tok/s 63621 (71032)	Loss/tok 2.8970 (3.1148)	LR 2.000e-03
0: TRAIN [5][740/968]	Time 0.426 (0.394)	Data 1.85e-04 (1.15e-03)	Tok/s 78380 (71039)	Loss/tok 3.1770 (3.1141)	LR 2.000e-03
0: TRAIN [5][750/968]	Time 0.430 (0.393)	Data 1.98e-04 (1.14e-03)	Tok/s 78391 (71031)	Loss/tok 3.0860 (3.1136)	LR 2.000e-03
0: TRAIN [5][760/968]	Time 0.317 (0.393)	Data 1.90e-04 (1.12e-03)	Tok/s 64612 (71046)	Loss/tok 2.9377 (3.1134)	LR 2.000e-03
0: TRAIN [5][770/968]	Time 0.430 (0.394)	Data 1.83e-04 (1.11e-03)	Tok/s 78586 (71112)	Loss/tok 3.1548 (3.1134)	LR 2.000e-03
0: TRAIN [5][780/968]	Time 0.684 (0.394)	Data 2.27e-04 (1.10e-03)	Tok/s 86259 (71153)	Loss/tok 3.4902 (3.1164)	LR 2.000e-03
0: TRAIN [5][790/968]	Time 0.552 (0.394)	Data 1.83e-04 (1.09e-03)	Tok/s 85196 (71176)	Loss/tok 3.2624 (3.1160)	LR 2.000e-03
0: TRAIN [5][800/968]	Time 0.212 (0.394)	Data 2.02e-04 (1.08e-03)	Tok/s 49992 (71125)	Loss/tok 2.6186 (3.1151)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][810/968]	Time 0.429 (0.394)	Data 2.20e-04 (1.07e-03)	Tok/s 77860 (71119)	Loss/tok 3.0653 (3.1153)	LR 2.000e-03
0: TRAIN [5][820/968]	Time 0.553 (0.394)	Data 1.70e-04 (1.06e-03)	Tok/s 84537 (71114)	Loss/tok 3.2877 (3.1147)	LR 2.000e-03
0: TRAIN [5][830/968]	Time 0.316 (0.393)	Data 1.90e-04 (1.04e-03)	Tok/s 65093 (71114)	Loss/tok 2.9981 (3.1147)	LR 2.000e-03
0: TRAIN [5][840/968]	Time 0.314 (0.393)	Data 2.20e-04 (1.03e-03)	Tok/s 66167 (71120)	Loss/tok 2.9197 (3.1142)	LR 2.000e-03
0: TRAIN [5][850/968]	Time 0.431 (0.393)	Data 2.10e-04 (1.02e-03)	Tok/s 77982 (71124)	Loss/tok 3.1387 (3.1146)	LR 2.000e-03
0: TRAIN [5][860/968]	Time 0.323 (0.393)	Data 2.12e-04 (1.02e-03)	Tok/s 63655 (71093)	Loss/tok 2.9367 (3.1142)	LR 2.000e-03
0: TRAIN [5][870/968]	Time 0.318 (0.393)	Data 2.04e-04 (1.01e-03)	Tok/s 65756 (71132)	Loss/tok 2.9850 (3.1146)	LR 2.000e-03
0: TRAIN [5][880/968]	Time 0.317 (0.393)	Data 2.03e-04 (9.97e-04)	Tok/s 65928 (71147)	Loss/tok 2.9091 (3.1148)	LR 2.000e-03
0: TRAIN [5][890/968]	Time 0.685 (0.394)	Data 1.75e-04 (9.88e-04)	Tok/s 86428 (71175)	Loss/tok 3.4729 (3.1160)	LR 2.000e-03
0: TRAIN [5][900/968]	Time 0.213 (0.394)	Data 2.01e-04 (9.79e-04)	Tok/s 50115 (71160)	Loss/tok 2.5586 (3.1155)	LR 2.000e-03
0: TRAIN [5][910/968]	Time 0.318 (0.393)	Data 2.12e-04 (9.70e-04)	Tok/s 65473 (71143)	Loss/tok 2.9136 (3.1155)	LR 2.000e-03
0: TRAIN [5][920/968]	Time 0.431 (0.394)	Data 1.75e-04 (9.62e-04)	Tok/s 77858 (71185)	Loss/tok 3.2153 (3.1166)	LR 2.000e-03
0: TRAIN [5][930/968]	Time 0.320 (0.394)	Data 2.14e-04 (9.53e-04)	Tok/s 63560 (71193)	Loss/tok 2.8537 (3.1163)	LR 2.000e-03
0: TRAIN [5][940/968]	Time 0.314 (0.394)	Data 1.82e-04 (9.45e-04)	Tok/s 66051 (71204)	Loss/tok 2.9917 (3.1160)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [5][950/968]	Time 0.212 (0.395)	Data 1.95e-04 (9.37e-04)	Tok/s 49937 (71245)	Loss/tok 2.5571 (3.1186)	LR 2.000e-03
0: TRAIN [5][960/968]	Time 0.430 (0.394)	Data 2.22e-04 (9.30e-04)	Tok/s 78608 (71254)	Loss/tok 3.1355 (3.1183)	LR 2.000e-03
:::MLL 1581978713.341 epoch_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 525}}
:::MLL 1581978713.342 eval_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [5][0/3]	Time 0.710 (0.710)	Decoder iters 149.0 (149.0)	Tok/s 22628 (22628)
0: Running moses detokenizer
0: BLEU(score=23.855924299040424, counts=[36568, 18199, 10344, 6126], totals=[64550, 61547, 58544, 55546], precisions=[56.650658404337726, 29.569272263473444, 17.668761956818802, 11.02869693587297], bp=0.9980499286516358, sys_len=64550, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1581978715.190 eval_accuracy: {"value": 23.86, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 536}}
:::MLL 1581978715.190 eval_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 5	Training Loss: 3.1170	Test BLEU: 23.86
0: Performance: Epoch: 5	Training: 570102 Tok/s
0: Finished epoch 5
:::MLL 1581978715.191 block_stop: {"value": null, "metadata": {"first_epoch_num": 6, "file": "train.py", "lineno": 558}}
:::MLL 1581978715.191 block_start: {"value": null, "metadata": {"first_epoch_num": 7, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1581978715.191 epoch_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 515}}
0: Starting epoch 6
0: Executing preallocation
0: Sampler for epoch 6 uses seed 3214089207
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [6][0/968]	Time 1.154 (1.154)	Data 6.89e-01 (6.89e-01)	Tok/s 29135 (29135)	Loss/tok 3.0472 (3.0472)	LR 2.000e-03
0: TRAIN [6][10/968]	Time 0.428 (0.426)	Data 1.77e-04 (6.28e-02)	Tok/s 78571 (64719)	Loss/tok 3.0143 (2.9440)	LR 2.000e-03
0: TRAIN [6][20/968]	Time 0.316 (0.415)	Data 1.75e-04 (3.30e-02)	Tok/s 65692 (67489)	Loss/tok 2.8474 (3.0277)	LR 2.000e-03
0: TRAIN [6][30/968]	Time 0.317 (0.387)	Data 1.57e-04 (2.24e-02)	Tok/s 65100 (67022)	Loss/tok 2.8922 (2.9970)	LR 2.000e-03
0: TRAIN [6][40/968]	Time 0.316 (0.376)	Data 1.75e-04 (1.70e-02)	Tok/s 65864 (67135)	Loss/tok 2.8746 (2.9827)	LR 2.000e-03
0: TRAIN [6][50/968]	Time 0.320 (0.380)	Data 1.62e-04 (1.37e-02)	Tok/s 63977 (68386)	Loss/tok 2.8305 (2.9947)	LR 2.000e-03
0: TRAIN [6][60/968]	Time 0.552 (0.380)	Data 1.47e-04 (1.15e-02)	Tok/s 84583 (68503)	Loss/tok 3.1263 (2.9982)	LR 2.000e-03
0: TRAIN [6][70/968]	Time 0.432 (0.384)	Data 1.68e-04 (9.87e-03)	Tok/s 77033 (69363)	Loss/tok 2.9926 (3.0051)	LR 2.000e-03
0: TRAIN [6][80/968]	Time 0.319 (0.384)	Data 1.70e-04 (8.67e-03)	Tok/s 64703 (69610)	Loss/tok 2.8995 (3.0104)	LR 2.000e-03
0: TRAIN [6][90/968]	Time 0.429 (0.380)	Data 1.48e-04 (7.74e-03)	Tok/s 78333 (69534)	Loss/tok 3.0033 (3.0057)	LR 2.000e-03
0: TRAIN [6][100/968]	Time 0.317 (0.383)	Data 1.73e-04 (6.99e-03)	Tok/s 64938 (69950)	Loss/tok 2.8433 (3.0133)	LR 2.000e-03
0: TRAIN [6][110/968]	Time 0.321 (0.384)	Data 1.65e-04 (6.37e-03)	Tok/s 64504 (70020)	Loss/tok 2.9573 (3.0160)	LR 2.000e-03
0: TRAIN [6][120/968]	Time 0.549 (0.385)	Data 1.81e-04 (5.86e-03)	Tok/s 85663 (70018)	Loss/tok 3.2983 (3.0236)	LR 2.000e-03
0: TRAIN [6][130/968]	Time 0.320 (0.378)	Data 1.74e-04 (5.43e-03)	Tok/s 64594 (69385)	Loss/tok 2.8589 (3.0157)	LR 2.000e-03
0: TRAIN [6][140/968]	Time 0.211 (0.374)	Data 1.74e-04 (5.05e-03)	Tok/s 49589 (69018)	Loss/tok 2.5104 (3.0105)	LR 2.000e-03
0: TRAIN [6][150/968]	Time 0.549 (0.374)	Data 1.71e-04 (4.73e-03)	Tok/s 85090 (69098)	Loss/tok 3.2333 (3.0120)	LR 2.000e-03
0: TRAIN [6][160/968]	Time 0.554 (0.376)	Data 1.67e-04 (4.45e-03)	Tok/s 84119 (69293)	Loss/tok 3.2248 (3.0151)	LR 2.000e-03
0: TRAIN [6][170/968]	Time 0.549 (0.381)	Data 1.69e-04 (4.20e-03)	Tok/s 85681 (69624)	Loss/tok 3.2158 (3.0313)	LR 2.000e-03
0: TRAIN [6][180/968]	Time 0.551 (0.380)	Data 1.80e-04 (3.97e-03)	Tok/s 84375 (69671)	Loss/tok 3.3051 (3.0300)	LR 2.000e-03
0: TRAIN [6][190/968]	Time 0.550 (0.381)	Data 1.67e-04 (3.77e-03)	Tok/s 84327 (69752)	Loss/tok 3.3560 (3.0314)	LR 2.000e-03
0: TRAIN [6][200/968]	Time 0.320 (0.383)	Data 1.86e-04 (3.60e-03)	Tok/s 64307 (69818)	Loss/tok 2.9350 (3.0368)	LR 2.000e-03
0: TRAIN [6][210/968]	Time 0.318 (0.385)	Data 1.81e-04 (3.43e-03)	Tok/s 64653 (69946)	Loss/tok 2.9673 (3.0436)	LR 2.000e-03
0: TRAIN [6][220/968]	Time 0.432 (0.381)	Data 1.66e-04 (3.28e-03)	Tok/s 78760 (69638)	Loss/tok 3.0760 (3.0386)	LR 2.000e-03
0: TRAIN [6][230/968]	Time 0.549 (0.382)	Data 1.65e-04 (3.15e-03)	Tok/s 84747 (69717)	Loss/tok 3.2521 (3.0397)	LR 2.000e-03
0: TRAIN [6][240/968]	Time 0.318 (0.382)	Data 1.45e-04 (3.03e-03)	Tok/s 64946 (69819)	Loss/tok 2.8999 (3.0391)	LR 2.000e-03
0: TRAIN [6][250/968]	Time 0.429 (0.385)	Data 1.62e-04 (2.91e-03)	Tok/s 78734 (70053)	Loss/tok 3.1473 (3.0456)	LR 2.000e-03
0: TRAIN [6][260/968]	Time 0.430 (0.385)	Data 1.63e-04 (2.81e-03)	Tok/s 77324 (70145)	Loss/tok 3.1128 (3.0464)	LR 2.000e-03
0: TRAIN [6][270/968]	Time 0.429 (0.386)	Data 1.62e-04 (2.71e-03)	Tok/s 78715 (70232)	Loss/tok 3.1468 (3.0493)	LR 2.000e-03
0: TRAIN [6][280/968]	Time 0.319 (0.387)	Data 1.57e-04 (2.62e-03)	Tok/s 65157 (70389)	Loss/tok 2.9093 (3.0494)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [6][290/968]	Time 0.428 (0.389)	Data 1.68e-04 (2.53e-03)	Tok/s 78478 (70566)	Loss/tok 3.0897 (3.0539)	LR 2.000e-03
0: TRAIN [6][300/968]	Time 0.551 (0.390)	Data 1.44e-04 (2.46e-03)	Tok/s 84940 (70643)	Loss/tok 3.2210 (3.0554)	LR 2.000e-03
0: TRAIN [6][310/968]	Time 0.213 (0.391)	Data 1.73e-04 (2.38e-03)	Tok/s 50568 (70696)	Loss/tok 2.5740 (3.0574)	LR 2.000e-03
0: TRAIN [6][320/968]	Time 0.313 (0.392)	Data 1.66e-04 (2.31e-03)	Tok/s 66207 (70787)	Loss/tok 2.9183 (3.0609)	LR 2.000e-03
0: TRAIN [6][330/968]	Time 0.318 (0.392)	Data 1.62e-04 (2.25e-03)	Tok/s 64502 (70822)	Loss/tok 2.9231 (3.0634)	LR 2.000e-03
0: TRAIN [6][340/968]	Time 0.211 (0.390)	Data 1.52e-04 (2.19e-03)	Tok/s 50454 (70640)	Loss/tok 2.5606 (3.0612)	LR 2.000e-03
0: TRAIN [6][350/968]	Time 0.432 (0.390)	Data 1.45e-04 (2.13e-03)	Tok/s 77687 (70610)	Loss/tok 3.1227 (3.0617)	LR 2.000e-03
0: TRAIN [6][360/968]	Time 0.319 (0.390)	Data 1.74e-04 (2.07e-03)	Tok/s 64838 (70633)	Loss/tok 2.9263 (3.0631)	LR 2.000e-03
0: TRAIN [6][370/968]	Time 0.318 (0.391)	Data 1.46e-04 (2.02e-03)	Tok/s 65640 (70727)	Loss/tok 2.8920 (3.0665)	LR 2.000e-03
0: TRAIN [6][380/968]	Time 0.321 (0.392)	Data 1.88e-04 (1.97e-03)	Tok/s 63355 (70752)	Loss/tok 2.8651 (3.0700)	LR 2.000e-03
0: TRAIN [6][390/968]	Time 0.688 (0.393)	Data 2.00e-04 (1.93e-03)	Tok/s 86826 (70751)	Loss/tok 3.4213 (3.0711)	LR 2.000e-03
0: TRAIN [6][400/968]	Time 0.549 (0.393)	Data 1.85e-04 (1.88e-03)	Tok/s 84861 (70866)	Loss/tok 3.2596 (3.0720)	LR 2.000e-03
0: TRAIN [6][410/968]	Time 0.214 (0.392)	Data 1.51e-04 (1.84e-03)	Tok/s 49089 (70761)	Loss/tok 2.5063 (3.0712)	LR 2.000e-03
0: TRAIN [6][420/968]	Time 0.432 (0.394)	Data 1.51e-04 (1.80e-03)	Tok/s 77611 (70865)	Loss/tok 3.0189 (3.0734)	LR 2.000e-03
0: TRAIN [6][430/968]	Time 0.214 (0.393)	Data 1.73e-04 (1.77e-03)	Tok/s 50071 (70847)	Loss/tok 2.5403 (3.0731)	LR 2.000e-03
0: TRAIN [6][440/968]	Time 0.322 (0.394)	Data 1.54e-04 (1.73e-03)	Tok/s 64950 (70912)	Loss/tok 2.9005 (3.0729)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [6][450/968]	Time 0.695 (0.395)	Data 2.12e-04 (1.70e-03)	Tok/s 86185 (70951)	Loss/tok 3.3526 (3.0759)	LR 2.000e-03
0: TRAIN [6][460/968]	Time 0.319 (0.394)	Data 1.53e-04 (1.66e-03)	Tok/s 64529 (70938)	Loss/tok 2.9200 (3.0752)	LR 2.000e-03
0: TRAIN [6][470/968]	Time 0.430 (0.394)	Data 1.68e-04 (1.63e-03)	Tok/s 78309 (70960)	Loss/tok 3.1015 (3.0746)	LR 2.000e-03
0: TRAIN [6][480/968]	Time 0.212 (0.394)	Data 1.92e-04 (1.60e-03)	Tok/s 49919 (70952)	Loss/tok 2.4432 (3.0734)	LR 2.000e-03
0: TRAIN [6][490/968]	Time 0.316 (0.393)	Data 1.80e-04 (1.57e-03)	Tok/s 65922 (70940)	Loss/tok 2.9134 (3.0718)	LR 2.000e-03
0: TRAIN [6][500/968]	Time 0.430 (0.392)	Data 1.74e-04 (1.54e-03)	Tok/s 78345 (70898)	Loss/tok 3.0951 (3.0706)	LR 2.000e-03
0: TRAIN [6][510/968]	Time 0.433 (0.391)	Data 1.82e-04 (1.52e-03)	Tok/s 77551 (70853)	Loss/tok 2.9907 (3.0688)	LR 2.000e-03
0: TRAIN [6][520/968]	Time 0.318 (0.391)	Data 1.66e-04 (1.49e-03)	Tok/s 65165 (70812)	Loss/tok 2.9114 (3.0689)	LR 2.000e-03
0: TRAIN [6][530/968]	Time 0.210 (0.390)	Data 1.90e-04 (1.47e-03)	Tok/s 50092 (70734)	Loss/tok 2.5499 (3.0677)	LR 2.000e-03
0: TRAIN [6][540/968]	Time 0.432 (0.390)	Data 1.70e-04 (1.44e-03)	Tok/s 77841 (70728)	Loss/tok 3.0470 (3.0683)	LR 2.000e-03
0: TRAIN [6][550/968]	Time 0.431 (0.390)	Data 1.62e-04 (1.42e-03)	Tok/s 77990 (70700)	Loss/tok 3.0618 (3.0686)	LR 2.000e-03
0: TRAIN [6][560/968]	Time 0.317 (0.390)	Data 1.51e-04 (1.40e-03)	Tok/s 64451 (70713)	Loss/tok 2.9791 (3.0697)	LR 2.000e-03
0: TRAIN [6][570/968]	Time 0.210 (0.390)	Data 1.72e-04 (1.37e-03)	Tok/s 51063 (70714)	Loss/tok 2.5395 (3.0701)	LR 2.000e-03
0: TRAIN [6][580/968]	Time 0.320 (0.390)	Data 1.72e-04 (1.35e-03)	Tok/s 64760 (70679)	Loss/tok 2.8818 (3.0686)	LR 2.000e-03
0: TRAIN [6][590/968]	Time 0.554 (0.389)	Data 1.61e-04 (1.33e-03)	Tok/s 83723 (70672)	Loss/tok 3.2570 (3.0680)	LR 2.000e-03
0: TRAIN [6][600/968]	Time 0.318 (0.388)	Data 1.60e-04 (1.31e-03)	Tok/s 64824 (70547)	Loss/tok 2.9162 (3.0658)	LR 2.000e-03
0: TRAIN [6][610/968]	Time 0.318 (0.387)	Data 1.70e-04 (1.30e-03)	Tok/s 65261 (70472)	Loss/tok 2.8508 (3.0641)	LR 2.000e-03
0: TRAIN [6][620/968]	Time 0.319 (0.387)	Data 1.52e-04 (1.28e-03)	Tok/s 65580 (70518)	Loss/tok 2.8751 (3.0639)	LR 2.000e-03
0: TRAIN [6][630/968]	Time 0.211 (0.388)	Data 1.71e-04 (1.26e-03)	Tok/s 50534 (70514)	Loss/tok 2.5436 (3.0652)	LR 2.000e-03
0: TRAIN [6][640/968]	Time 0.314 (0.387)	Data 1.86e-04 (1.24e-03)	Tok/s 65321 (70523)	Loss/tok 2.8858 (3.0653)	LR 2.000e-03
0: TRAIN [6][650/968]	Time 0.432 (0.388)	Data 1.71e-04 (1.23e-03)	Tok/s 78461 (70592)	Loss/tok 3.1249 (3.0657)	LR 2.000e-03
0: TRAIN [6][660/968]	Time 0.317 (0.387)	Data 1.52e-04 (1.21e-03)	Tok/s 65191 (70581)	Loss/tok 2.8216 (3.0649)	LR 2.000e-03
0: TRAIN [6][670/968]	Time 0.317 (0.387)	Data 1.62e-04 (1.20e-03)	Tok/s 65497 (70595)	Loss/tok 2.8088 (3.0647)	LR 2.000e-03
0: TRAIN [6][680/968]	Time 0.316 (0.388)	Data 1.67e-04 (1.18e-03)	Tok/s 65409 (70679)	Loss/tok 2.9594 (3.0662)	LR 1.000e-03
0: TRAIN [6][690/968]	Time 0.550 (0.389)	Data 1.50e-04 (1.17e-03)	Tok/s 84605 (70767)	Loss/tok 3.2437 (3.0686)	LR 1.000e-03
0: TRAIN [6][700/968]	Time 0.431 (0.390)	Data 1.75e-04 (1.15e-03)	Tok/s 77052 (70822)	Loss/tok 3.0686 (3.0705)	LR 1.000e-03
0: TRAIN [6][710/968]	Time 0.321 (0.390)	Data 1.90e-04 (1.14e-03)	Tok/s 64812 (70849)	Loss/tok 2.8703 (3.0712)	LR 1.000e-03
0: TRAIN [6][720/968]	Time 0.428 (0.390)	Data 1.59e-04 (1.12e-03)	Tok/s 78317 (70836)	Loss/tok 3.0685 (3.0708)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [6][730/968]	Time 0.432 (0.391)	Data 1.57e-04 (1.11e-03)	Tok/s 78438 (70872)	Loss/tok 3.0146 (3.0719)	LR 1.000e-03
0: TRAIN [6][740/968]	Time 0.209 (0.391)	Data 1.65e-04 (1.10e-03)	Tok/s 49967 (70872)	Loss/tok 2.5498 (3.0717)	LR 1.000e-03
0: TRAIN [6][750/968]	Time 0.318 (0.391)	Data 1.49e-04 (1.09e-03)	Tok/s 65617 (70898)	Loss/tok 2.8085 (3.0710)	LR 1.000e-03
0: TRAIN [6][760/968]	Time 0.551 (0.392)	Data 1.54e-04 (1.07e-03)	Tok/s 84132 (71000)	Loss/tok 3.2608 (3.0731)	LR 1.000e-03
0: TRAIN [6][770/968]	Time 0.435 (0.392)	Data 1.94e-04 (1.06e-03)	Tok/s 77272 (70986)	Loss/tok 2.9758 (3.0727)	LR 1.000e-03
0: TRAIN [6][780/968]	Time 0.552 (0.392)	Data 1.54e-04 (1.05e-03)	Tok/s 85807 (71020)	Loss/tok 3.1929 (3.0728)	LR 1.000e-03
0: TRAIN [6][790/968]	Time 0.319 (0.391)	Data 1.58e-04 (1.04e-03)	Tok/s 64567 (70992)	Loss/tok 2.8975 (3.0721)	LR 1.000e-03
0: TRAIN [6][800/968]	Time 0.555 (0.392)	Data 1.69e-04 (1.03e-03)	Tok/s 84045 (71075)	Loss/tok 3.2363 (3.0731)	LR 1.000e-03
0: TRAIN [6][810/968]	Time 0.427 (0.393)	Data 1.47e-04 (1.02e-03)	Tok/s 79289 (71120)	Loss/tok 2.9918 (3.0733)	LR 1.000e-03
0: TRAIN [6][820/968]	Time 0.317 (0.392)	Data 1.75e-04 (1.01e-03)	Tok/s 64978 (71083)	Loss/tok 2.8151 (3.0727)	LR 1.000e-03
0: TRAIN [6][830/968]	Time 0.316 (0.392)	Data 1.54e-04 (9.97e-04)	Tok/s 65188 (71080)	Loss/tok 2.8920 (3.0725)	LR 1.000e-03
0: TRAIN [6][840/968]	Time 0.316 (0.392)	Data 1.63e-04 (9.87e-04)	Tok/s 66307 (71042)	Loss/tok 2.8305 (3.0724)	LR 1.000e-03
0: TRAIN [6][850/968]	Time 0.553 (0.393)	Data 1.61e-04 (9.78e-04)	Tok/s 84513 (71117)	Loss/tok 3.2752 (3.0745)	LR 1.000e-03
0: TRAIN [6][860/968]	Time 0.430 (0.394)	Data 1.47e-04 (9.68e-04)	Tok/s 78147 (71165)	Loss/tok 3.0905 (3.0758)	LR 1.000e-03
0: TRAIN [6][870/968]	Time 0.320 (0.394)	Data 1.49e-04 (9.59e-04)	Tok/s 64085 (71193)	Loss/tok 2.8151 (3.0757)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [6][880/968]	Time 0.320 (0.394)	Data 1.53e-04 (9.50e-04)	Tok/s 64912 (71164)	Loss/tok 2.8441 (3.0756)	LR 1.000e-03
0: TRAIN [6][890/968]	Time 0.432 (0.394)	Data 1.51e-04 (9.41e-04)	Tok/s 77461 (71199)	Loss/tok 3.0613 (3.0755)	LR 1.000e-03
0: TRAIN [6][900/968]	Time 0.437 (0.394)	Data 1.75e-04 (9.32e-04)	Tok/s 76699 (71166)	Loss/tok 3.0528 (3.0753)	LR 1.000e-03
0: TRAIN [6][910/968]	Time 0.320 (0.395)	Data 1.62e-04 (9.24e-04)	Tok/s 63903 (71229)	Loss/tok 2.8977 (3.0762)	LR 1.000e-03
0: TRAIN [6][920/968]	Time 0.432 (0.394)	Data 1.60e-04 (9.16e-04)	Tok/s 77254 (71218)	Loss/tok 3.0536 (3.0757)	LR 1.000e-03
0: TRAIN [6][930/968]	Time 0.321 (0.395)	Data 1.83e-04 (9.08e-04)	Tok/s 63987 (71237)	Loss/tok 2.8502 (3.0756)	LR 1.000e-03
0: TRAIN [6][940/968]	Time 0.434 (0.395)	Data 1.56e-04 (9.00e-04)	Tok/s 78058 (71260)	Loss/tok 3.0993 (3.0758)	LR 1.000e-03
0: TRAIN [6][950/968]	Time 0.549 (0.395)	Data 1.92e-04 (8.92e-04)	Tok/s 85155 (71245)	Loss/tok 3.2694 (3.0756)	LR 1.000e-03
0: TRAIN [6][960/968]	Time 0.428 (0.394)	Data 1.90e-04 (8.85e-04)	Tok/s 79147 (71223)	Loss/tok 3.0558 (3.0746)	LR 1.000e-03
:::MLL 1581979098.169 epoch_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 525}}
:::MLL 1581979098.169 eval_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [6][0/3]	Time 0.602 (0.602)	Decoder iters 98.0 (98.0)	Tok/s 27032 (27032)
0: Running moses detokenizer
0: BLEU(score=24.199406070086624, counts=[37219, 18670, 10655, 6305], totals=[65337, 62334, 59331, 56333], precisions=[56.964660146624425, 29.951551320306734, 17.958571404493433, 11.192373919372304], bp=1.0, sys_len=65337, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1581979099.960 eval_accuracy: {"value": 24.2, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 536}}
:::MLL 1581979099.960 eval_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 6	Training Loss: 3.0737	Test BLEU: 24.20
0: Performance: Epoch: 6	Training: 569914 Tok/s
0: Finished epoch 6
:::MLL 1581979099.960 block_stop: {"value": null, "metadata": {"first_epoch_num": 7, "file": "train.py", "lineno": 558}}
0: Closing preprocessed data file
:::MLL 1581979099.961 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 569}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-02-17 10:38:24 PM
RESULT,RNN_TRANSLATOR,,2711,nvidia,2020-02-17 09:53:13 PM
