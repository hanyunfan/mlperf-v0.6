Beginning trial 2 of 2
Gathering sys log on node009
:::MLL 1582040903.000 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1582040903.000 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1582040903.001 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1582040903.001 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1582040903.002 submission_platform: {"value": "1xDSS8440", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1582040903.002 submission_entry: {"value": "{'hardware': 'DSS8440', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'Ethernet 100 Gb/sec (4X EDR)', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.6-2.1.4', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz', 'num_cores': '40', 'num_vcpus': '40', 'accelerator': 'Tesla V100S-PCIE-32GB', 'num_accelerators': '8', 'sys_mem_size': '754 GB', 'sys_storage_type': 'SATA SSD', 'sys_storage_size': '1x 446.6G + 1x 931.5G', 'cpu_accel_interconnect': 'UPI', 'network_card': '', 'num_network_cards': '0', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1582040903.002 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1582040903.003 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1582040905.119 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node node009
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=4670' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=200218085543228087240 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_200218085543228087240 ./run_and_time.sh
Run vars: id 200218085543228087240 gpus 8 mparams  --master_port=4670
NCCL_SOCKET_NTHREADS=2
NCCL_NSOCKS_PERTHREAD=8
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-02-18 03:48:25 PM
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=0
+ MATH=amp_fp16
+ [[ 0 -eq 1 ]]
running benchmark
+ LAUNCH_OPT='torch.distributed.launch --nproc_per_node 8  --master_port=4670'
+ echo 'running benchmark'
+ python -m torch.distributed.launch --nproc_per_node 8 --master_port=4670 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1582040907.588 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1582040907.605 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1582040907.627 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1582040907.627 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1582040907.628 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1582040907.630 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1582040907.635 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1582040907.648 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=8, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 523187844
node009:1625:1625 [0] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1625:1625 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation

node009:1625:1625 [0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:1625:1625 [0] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:1625:1625 [0] NCCL INFO NET/IB : No device found.
node009:1625:1625 [0] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
NCCL version 2.5.6+cuda10.2
node009:1632:1632 [7] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1628:1628 [3] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1632:1632 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node009:1628:1628 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node009:1629:1629 [4] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1629:1629 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node009:1626:1626 [1] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1630:1630 [5] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1626:1626 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node009:1630:1630 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation

node009:1629:1629 [4] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:1629:1629 [4] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:1629:1629 [4] NCCL INFO NET/IB : No device found.

node009:1628:1628 [3] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:1632:1632 [7] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:1628:1628 [3] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0

node009:1632:1632 [7] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:1628:1628 [3] NCCL INFO NET/IB : No device found.
node009:1632:1632 [7] NCCL INFO NET/IB : No device found.

node009:1630:1630 [5] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:1630:1630 [5] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:1630:1630 [5] NCCL INFO NET/IB : No device found.

node009:1626:1626 [1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:1626:1626 [1] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:1626:1626 [1] NCCL INFO NET/IB : No device found.
node009:1632:1632 [7] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1628:1628 [3] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1629:1629 [4] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1631:1631 [6] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1631:1631 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node009:1626:1626 [1] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1630:1630 [5] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>

node009:1631:1631 [6] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:1631:1631 [6] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:1631:1631 [6] NCCL INFO NET/IB : No device found.
node009:1631:1631 [6] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1627:1627 [2] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1627:1627 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation

node009:1627:1627 [2] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:1627:1627 [2] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:1627:1627 [2] NCCL INFO NET/IB : No device found.
node009:1627:1627 [2] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:1625:1987 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555
node009:1628:1988 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555
node009:1632:1990 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa
node009:1629:1989 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa
node009:1631:1993 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa
node009:1626:1992 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555
node009:1630:1991 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa
node009:1627:1994 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555
node009:1626:1992 [1] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:1626:1992 [1] NCCL INFO include/net.h:19 -> 2
node009:1627:1994 [2] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:1627:1994 [2] NCCL INFO include/net.h:19 -> 2
node009:1625:1987 [0] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:1625:1987 [0] NCCL INFO include/net.h:19 -> 2
node009:1626:1992 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:1628:1988 [3] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:1628:1988 [3] NCCL INFO include/net.h:19 -> 2
node009:1631:1993 [6] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:1631:1993 [6] NCCL INFO include/net.h:19 -> 2
node009:1630:1991 [5] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:1630:1991 [5] NCCL INFO include/net.h:19 -> 2
node009:1629:1989 [4] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:1632:1990 [7] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:1629:1989 [4] NCCL INFO include/net.h:19 -> 2
node009:1632:1990 [7] NCCL INFO include/net.h:19 -> 2
node009:1627:1994 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:1625:1987 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:1628:1988 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:1631:1993 [6] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:1630:1991 [5] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:1632:1990 [7] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:1629:1989 [4] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:1631:1993 [6] NCCL INFO Threads per block : 512/640/256
node009:1630:1991 [5] NCCL INFO Threads per block : 512/640/256
node009:1631:1993 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:1630:1991 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:1629:1989 [4] NCCL INFO Threads per block : 512/640/256
node009:1631:1993 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1
node009:1630:1991 [5] NCCL INFO Trees [0] 6/-1/-1->5->4|4->5->6/-1/-1 [1] 6/-1/-1->5->4|4->5->6/-1/-1
node009:1629:1989 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:1628:1988 [3] NCCL INFO Threads per block : 512/640/256
node009:1629:1989 [4] NCCL INFO Trees [0] 5/-1/-1->4->3|3->4->5/-1/-1 [1] 5/-1/-1->4->3|3->4->5/-1/-1
node009:1628:1988 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:1628:1988 [3] NCCL INFO Trees [0] 4/-1/-1->3->2|2->3->4/-1/-1 [1] 4/-1/-1->3->2|2->3->4/-1/-1
node009:1627:1994 [2] NCCL INFO Threads per block : 512/640/256
node009:1627:1994 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:1625:1987 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
node009:1627:1994 [2] NCCL INFO Trees [0] 3/-1/-1->2->1|1->2->3/-1/-1 [1] 3/-1/-1->2->1|1->2->3/-1/-1
node009:1625:1987 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
node009:1625:1987 [0] NCCL INFO Threads per block : 512/640/256
node009:1626:1992 [1] NCCL INFO Threads per block : 512/640/256
node009:1626:1992 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:1626:1992 [1] NCCL INFO Trees [0] 2/-1/-1->1->0|0->1->2/-1/-1 [1] 2/-1/-1->1->0|0->1->2/-1/-1
node009:1625:1987 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:1625:1987 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 1/-1/-1->0->-1|-1->0->1/-1/-1
node009:1632:1990 [7] NCCL INFO Threads per block : 512/640/256
node009:1632:1990 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:1632:1990 [7] NCCL INFO Trees [0] -1/-1/-1->7->6|6->7->-1/-1/-1 [1] -1/-1/-1->7->6|6->7->-1/-1/-1
node009:1631:1993 [6] NCCL INFO Ring 00 : 6[c1000] -> 7[c2000] via P2P/IPC
node009:1630:1991 [5] NCCL INFO Ring 00 : 5[8a000] -> 6[c1000] via P2P/IPC
node009:1626:1992 [1] NCCL INFO Ring 00 : 1[11000] -> 2[47000] via P2P/IPC
node009:1632:1990 [7] NCCL INFO Ring 00 : 7[c2000] -> 0[10000] via direct shared memory
node009:1627:1994 [2] NCCL INFO Ring 00 : 2[47000] -> 3[48000] via P2P/IPC
node009:1628:1988 [3] NCCL INFO Ring 00 : 3[48000] -> 4[89000] via direct shared memory
node009:1625:1987 [0] NCCL INFO Ring 00 : 0[10000] -> 1[11000] via P2P/IPC
node009:1629:1989 [4] NCCL INFO Ring 00 : 4[89000] -> 5[8a000] via P2P/IPC
node009:1631:1993 [6] NCCL INFO Ring 00 : 6[c1000] -> 5[8a000] via P2P/IPC
node009:1629:1989 [4] NCCL INFO Ring 00 : 4[89000] -> 3[48000] via direct shared memory
node009:1630:1991 [5] NCCL INFO Ring 00 : 5[8a000] -> 4[89000] via P2P/IPC
node009:1632:1990 [7] NCCL INFO Ring 00 : 7[c2000] -> 6[c1000] via P2P/IPC
node009:1627:1994 [2] NCCL INFO Ring 00 : 2[47000] -> 1[11000] via P2P/IPC
node009:1626:1992 [1] NCCL INFO Ring 00 : 1[11000] -> 0[10000] via P2P/IPC
node009:1632:1990 [7] NCCL INFO Ring 01 : 7[c2000] -> 0[10000] via direct shared memory
node009:1631:1993 [6] NCCL INFO Ring 01 : 6[c1000] -> 7[c2000] via P2P/IPC
node009:1630:1991 [5] NCCL INFO Ring 01 : 5[8a000] -> 6[c1000] via P2P/IPC
node009:1628:1988 [3] NCCL INFO Ring 00 : 3[48000] -> 2[47000] via P2P/IPC
node009:1626:1992 [1] NCCL INFO Ring 01 : 1[11000] -> 2[47000] via P2P/IPC
node009:1625:1987 [0] NCCL INFO Ring 01 : 0[10000] -> 1[11000] via P2P/IPC
node009:1627:1994 [2] NCCL INFO Ring 01 : 2[47000] -> 3[48000] via P2P/IPC
node009:1631:1993 [6] NCCL INFO Ring 01 : 6[c1000] -> 5[8a000] via P2P/IPC
node009:1628:1988 [3] NCCL INFO Ring 01 : 3[48000] -> 4[89000] via direct shared memory
node009:1632:1990 [7] NCCL INFO Ring 01 : 7[c2000] -> 6[c1000] via P2P/IPC
node009:1626:1992 [1] NCCL INFO Ring 01 : 1[11000] -> 0[10000] via P2P/IPC
node009:1629:1989 [4] NCCL INFO Ring 01 : 4[89000] -> 5[8a000] via P2P/IPC
node009:1627:1994 [2] NCCL INFO Ring 01 : 2[47000] -> 1[11000] via P2P/IPC
node009:1630:1991 [5] NCCL INFO Ring 01 : 5[8a000] -> 4[89000] via P2P/IPC
node009:1629:1989 [4] NCCL INFO Ring 01 : 4[89000] -> 3[48000] via direct shared memory
node009:1628:1988 [3] NCCL INFO Ring 01 : 3[48000] -> 2[47000] via P2P/IPC
node009:1632:1990 [7] NCCL INFO comm 0x7ffe98007570 rank 7 nranks 8 cudaDev 7 busId c2000 - Init COMPLETE
node009:1625:1987 [0] NCCL INFO comm 0x7ffe30007570 rank 0 nranks 8 cudaDev 0 busId 10000 - Init COMPLETE
node009:1625:1625 [0] NCCL INFO Launch mode Parallel
node009:1626:1992 [1] NCCL INFO comm 0x7fff58007570 rank 1 nranks 8 cudaDev 1 busId 11000 - Init COMPLETE
0: Worker 0 is using worker seed: 3931567652
0: Building vocabulary from /data/vocab.bpe.32000
node009:1631:1993 [6] NCCL INFO comm 0x7ffe98007570 rank 6 nranks 8 cudaDev 6 busId c1000 - Init COMPLETE
0: Size of vocabulary: 32320
node009:1630:1991 [5] NCCL INFO comm 0x7ffe98007570 rank 5 nranks 8 cudaDev 5 busId 8a000 - Init COMPLETE
node009:1627:1994 [2] NCCL INFO comm 0x7ffed0007570 rank 2 nranks 8 cudaDev 2 busId 47000 - Init COMPLETE
node009:1629:1989 [4] NCCL INFO comm 0x7fff7c007570 rank 4 nranks 8 cudaDev 4 busId 89000 - Init COMPLETE
node009:1628:1988 [3] NCCL INFO comm 0x7ffe98007570 rank 3 nranks 8 cudaDev 3 busId 48000 - Init COMPLETE
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1582040916.307 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1582040917.531 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 407}}
:::MLL 1582040917.531 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 409}}
:::MLL 1582040917.532 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 414}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1582040918.568 global_batch_size: {"value": 2048, "metadata": {"file": "train.py", "lineno": 459}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 8, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 8
:::MLL 1582040918.578 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1582040918.578 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1582040918.579 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1582040918.579 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1582040918.579 opt_learning_rate_decay_steps: {"value": 8, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1582040918.580 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1582040918.580 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1582040918.580 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1582040918.581 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 515}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 1540497006
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/1938]	Time 1.057 (1.057)	Data 7.77e-01 (7.77e-01)	Tok/s 16045 (16045)	Loss/tok 10.6388 (10.6388)	LR 2.000e-05
0: TRAIN [0][10/1938]	Time 0.221 (0.313)	Data 9.85e-05 (7.08e-02)	Tok/s 46418 (47530)	Loss/tok 9.6695 (10.2017)	LR 2.518e-05
0: TRAIN [0][20/1938]	Time 0.282 (0.294)	Data 1.18e-04 (3.72e-02)	Tok/s 60193 (50823)	Loss/tok 9.4067 (9.8554)	LR 3.170e-05
0: TRAIN [0][30/1938]	Time 0.280 (0.286)	Data 2.42e-04 (2.52e-02)	Tok/s 60346 (52381)	Loss/tok 9.1488 (9.6409)	LR 3.991e-05
0: TRAIN [0][40/1938]	Time 0.280 (0.283)	Data 1.26e-04 (1.91e-02)	Tok/s 61857 (53565)	Loss/tok 8.8524 (9.4623)	LR 5.024e-05
0: TRAIN [0][50/1938]	Time 0.220 (0.285)	Data 1.04e-04 (1.54e-02)	Tok/s 47181 (54162)	Loss/tok 8.4831 (9.3038)	LR 6.325e-05
0: TRAIN [0][60/1938]	Time 0.222 (0.277)	Data 1.29e-04 (1.29e-02)	Tok/s 46094 (53465)	Loss/tok 8.3128 (9.1858)	LR 7.962e-05
0: TRAIN [0][70/1938]	Time 0.220 (0.274)	Data 1.31e-04 (1.11e-02)	Tok/s 46681 (53365)	Loss/tok 8.0898 (9.0614)	LR 1.002e-04
0: TRAIN [0][80/1938]	Time 0.215 (0.272)	Data 1.25e-04 (9.72e-03)	Tok/s 48392 (53460)	Loss/tok 7.9728 (8.9489)	LR 1.262e-04
0: TRAIN [0][90/1938]	Time 0.221 (0.273)	Data 1.10e-04 (8.67e-03)	Tok/s 46831 (53688)	Loss/tok 7.9511 (8.8484)	LR 1.589e-04
0: TRAIN [0][100/1938]	Time 0.219 (0.270)	Data 1.09e-04 (7.82e-03)	Tok/s 47033 (53643)	Loss/tok 7.8465 (8.7680)	LR 2.000e-04
0: TRAIN [0][110/1938]	Time 0.220 (0.269)	Data 9.70e-05 (7.13e-03)	Tok/s 47248 (53635)	Loss/tok 7.8607 (8.6936)	LR 2.518e-04
0: TRAIN [0][120/1938]	Time 0.218 (0.267)	Data 1.01e-04 (6.55e-03)	Tok/s 46965 (53520)	Loss/tok 7.7083 (8.6318)	LR 3.170e-04
0: TRAIN [0][130/1938]	Time 0.279 (0.267)	Data 9.66e-05 (6.06e-03)	Tok/s 60741 (53773)	Loss/tok 7.8557 (8.5707)	LR 3.991e-04
0: TRAIN [0][140/1938]	Time 0.218 (0.266)	Data 1.18e-04 (5.63e-03)	Tok/s 47488 (53639)	Loss/tok 7.6555 (8.5192)	LR 5.024e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][150/1938]	Time 0.170 (0.265)	Data 1.16e-04 (5.27e-03)	Tok/s 31407 (53400)	Loss/tok 7.0635 (8.4770)	LR 6.040e-04
0: TRAIN [0][160/1938]	Time 0.218 (0.264)	Data 1.78e-04 (4.95e-03)	Tok/s 47585 (53204)	Loss/tok 7.4813 (8.4340)	LR 7.604e-04
0: TRAIN [0][170/1938]	Time 0.340 (0.264)	Data 1.01e-04 (4.67e-03)	Tok/s 69035 (53361)	Loss/tok 7.8492 (8.3887)	LR 9.573e-04
0: TRAIN [0][180/1938]	Time 0.221 (0.265)	Data 1.34e-04 (4.42e-03)	Tok/s 47806 (53658)	Loss/tok 7.2522 (8.3367)	LR 1.205e-03
0: TRAIN [0][190/1938]	Time 0.280 (0.265)	Data 1.11e-04 (4.19e-03)	Tok/s 59787 (53815)	Loss/tok 7.3654 (8.2817)	LR 1.517e-03
0: TRAIN [0][200/1938]	Time 0.342 (0.266)	Data 1.02e-04 (3.99e-03)	Tok/s 67673 (53983)	Loss/tok 7.3293 (8.2250)	LR 1.910e-03
0: TRAIN [0][210/1938]	Time 0.344 (0.267)	Data 1.06e-04 (3.80e-03)	Tok/s 68770 (54251)	Loss/tok 7.0055 (8.1593)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.340 (0.265)	Data 1.16e-04 (3.64e-03)	Tok/s 68642 (54017)	Loss/tok 7.0171 (8.1098)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.220 (0.264)	Data 1.40e-04 (3.48e-03)	Tok/s 46582 (53807)	Loss/tok 6.4588 (8.0578)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.278 (0.264)	Data 1.18e-04 (3.34e-03)	Tok/s 60118 (53869)	Loss/tok 6.6250 (7.9957)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.165 (0.264)	Data 1.22e-04 (3.22e-03)	Tok/s 31767 (53828)	Loss/tok 5.6068 (7.9368)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.277 (0.264)	Data 1.44e-04 (3.10e-03)	Tok/s 60488 (53855)	Loss/tok 6.3132 (7.8750)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.282 (0.264)	Data 1.32e-04 (2.99e-03)	Tok/s 59453 (53924)	Loss/tok 6.1454 (7.8087)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.281 (0.264)	Data 1.35e-04 (2.89e-03)	Tok/s 60472 (53906)	Loss/tok 6.0491 (7.7502)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.219 (0.264)	Data 1.08e-04 (2.79e-03)	Tok/s 47238 (53965)	Loss/tok 5.6493 (7.6877)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.413 (0.264)	Data 1.10e-04 (2.70e-03)	Tok/s 71264 (53883)	Loss/tok 6.2482 (7.6312)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.283 (0.263)	Data 1.25e-04 (2.62e-03)	Tok/s 58919 (53857)	Loss/tok 5.7917 (7.5747)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.168 (0.264)	Data 1.22e-04 (2.54e-03)	Tok/s 31209 (53950)	Loss/tok 4.7624 (7.5110)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.219 (0.263)	Data 1.05e-04 (2.47e-03)	Tok/s 47165 (53817)	Loss/tok 5.3768 (7.4630)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.340 (0.263)	Data 1.39e-04 (2.40e-03)	Tok/s 68650 (53868)	Loss/tok 5.7494 (7.4056)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.344 (0.263)	Data 1.09e-04 (2.33e-03)	Tok/s 68046 (53920)	Loss/tok 5.6504 (7.3482)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.218 (0.263)	Data 2.72e-04 (2.27e-03)	Tok/s 48200 (53798)	Loss/tok 5.1066 (7.3015)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.219 (0.263)	Data 1.12e-04 (2.21e-03)	Tok/s 45728 (53864)	Loss/tok 5.0168 (7.2459)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.166 (0.263)	Data 1.04e-04 (2.16e-03)	Tok/s 31896 (53905)	Loss/tok 4.0908 (7.1915)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.166 (0.262)	Data 1.10e-04 (2.11e-03)	Tok/s 32049 (53826)	Loss/tok 4.0239 (7.1446)	LR 2.000e-03
0: TRAIN [0][400/1938]	Time 0.217 (0.262)	Data 1.18e-04 (2.06e-03)	Tok/s 46759 (53777)	Loss/tok 4.6991 (7.0948)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.220 (0.261)	Data 1.00e-04 (2.01e-03)	Tok/s 46841 (53708)	Loss/tok 4.6813 (7.0485)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.163 (0.262)	Data 1.09e-04 (1.97e-03)	Tok/s 32413 (53757)	Loss/tok 3.8479 (6.9943)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.343 (0.262)	Data 9.68e-05 (1.92e-03)	Tok/s 67647 (53776)	Loss/tok 5.2484 (6.9435)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.282 (0.263)	Data 1.17e-04 (1.88e-03)	Tok/s 58499 (53937)	Loss/tok 4.7510 (6.8849)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.165 (0.262)	Data 1.24e-04 (1.84e-03)	Tok/s 31842 (53818)	Loss/tok 3.7620 (6.8431)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.221 (0.261)	Data 1.21e-04 (1.80e-03)	Tok/s 46325 (53751)	Loss/tok 4.2719 (6.8010)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.345 (0.262)	Data 1.32e-04 (1.77e-03)	Tok/s 67527 (53789)	Loss/tok 4.8987 (6.7520)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.284 (0.261)	Data 1.26e-04 (1.73e-03)	Tok/s 59349 (53750)	Loss/tok 4.4524 (6.7095)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.278 (0.261)	Data 1.71e-04 (1.70e-03)	Tok/s 60042 (53664)	Loss/tok 4.5804 (6.6694)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.221 (0.260)	Data 1.14e-04 (1.67e-03)	Tok/s 46590 (53534)	Loss/tok 4.2095 (6.6325)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.168 (0.261)	Data 1.25e-04 (1.64e-03)	Tok/s 31601 (53529)	Loss/tok 3.5112 (6.5908)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.340 (0.261)	Data 1.20e-04 (1.61e-03)	Tok/s 68655 (53547)	Loss/tok 4.6643 (6.5500)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.283 (0.261)	Data 1.17e-04 (1.58e-03)	Tok/s 58366 (53641)	Loss/tok 4.3080 (6.5033)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][540/1938]	Time 0.220 (0.261)	Data 1.47e-04 (1.56e-03)	Tok/s 47223 (53713)	Loss/tok 4.2083 (6.4604)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.166 (0.261)	Data 1.45e-04 (1.53e-03)	Tok/s 31457 (53628)	Loss/tok 3.3938 (6.4272)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.224 (0.261)	Data 1.57e-04 (1.50e-03)	Tok/s 46197 (53592)	Loss/tok 4.0387 (6.3927)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.165 (0.261)	Data 1.23e-04 (1.48e-03)	Tok/s 31937 (53612)	Loss/tok 3.3420 (6.3552)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.220 (0.261)	Data 1.28e-04 (1.46e-03)	Tok/s 46812 (53662)	Loss/tok 3.9945 (6.3166)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.339 (0.261)	Data 1.40e-04 (1.43e-03)	Tok/s 68820 (53657)	Loss/tok 4.5685 (6.2813)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.283 (0.261)	Data 1.13e-04 (1.41e-03)	Tok/s 59768 (53755)	Loss/tok 4.1881 (6.2424)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.168 (0.261)	Data 1.11e-04 (1.39e-03)	Tok/s 30656 (53674)	Loss/tok 3.3811 (6.2121)	LR 2.000e-03
0: TRAIN [0][620/1938]	Time 0.341 (0.261)	Data 1.07e-04 (1.37e-03)	Tok/s 68828 (53680)	Loss/tok 4.4694 (6.1798)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.220 (0.261)	Data 1.12e-04 (1.35e-03)	Tok/s 46837 (53600)	Loss/tok 3.8269 (6.1523)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.339 (0.261)	Data 1.35e-04 (1.33e-03)	Tok/s 68241 (53609)	Loss/tok 4.4580 (6.1217)	LR 2.000e-03
0: TRAIN [0][650/1938]	Time 0.221 (0.260)	Data 1.33e-04 (1.31e-03)	Tok/s 47281 (53541)	Loss/tok 3.9435 (6.0954)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.280 (0.261)	Data 1.37e-04 (1.30e-03)	Tok/s 59793 (53583)	Loss/tok 4.1375 (6.0636)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.222 (0.261)	Data 1.22e-04 (1.28e-03)	Tok/s 46014 (53608)	Loss/tok 3.8321 (6.0338)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.165 (0.261)	Data 1.05e-04 (1.26e-03)	Tok/s 31684 (53590)	Loss/tok 3.2160 (6.0063)	LR 2.000e-03
0: TRAIN [0][690/1938]	Time 0.222 (0.260)	Data 1.19e-04 (1.24e-03)	Tok/s 45573 (53535)	Loss/tok 3.9235 (5.9815)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.343 (0.261)	Data 1.04e-04 (1.23e-03)	Tok/s 67843 (53586)	Loss/tok 4.3247 (5.9519)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.222 (0.261)	Data 1.04e-04 (1.21e-03)	Tok/s 46880 (53615)	Loss/tok 3.7813 (5.9238)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.220 (0.261)	Data 1.18e-04 (1.20e-03)	Tok/s 46522 (53607)	Loss/tok 3.6849 (5.8988)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.283 (0.260)	Data 1.06e-04 (1.18e-03)	Tok/s 59314 (53563)	Loss/tok 3.9609 (5.8755)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.220 (0.260)	Data 1.09e-04 (1.17e-03)	Tok/s 46814 (53511)	Loss/tok 3.6717 (5.8521)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.282 (0.260)	Data 9.23e-05 (1.15e-03)	Tok/s 59330 (53561)	Loss/tok 4.0369 (5.8259)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.340 (0.260)	Data 1.16e-04 (1.14e-03)	Tok/s 69274 (53532)	Loss/tok 4.3009 (5.8037)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.166 (0.260)	Data 1.09e-04 (1.13e-03)	Tok/s 31754 (53478)	Loss/tok 3.1246 (5.7827)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.221 (0.260)	Data 1.04e-04 (1.11e-03)	Tok/s 46223 (53509)	Loss/tok 3.7099 (5.7586)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.277 (0.260)	Data 1.02e-04 (1.10e-03)	Tok/s 60924 (53562)	Loss/tok 3.8883 (5.7330)	LR 2.000e-03
0: TRAIN [0][800/1938]	Time 0.221 (0.260)	Data 1.10e-04 (1.09e-03)	Tok/s 45766 (53511)	Loss/tok 3.7391 (5.7136)	LR 2.000e-03
0: TRAIN [0][810/1938]	Time 0.166 (0.260)	Data 1.30e-04 (1.08e-03)	Tok/s 32149 (53509)	Loss/tok 3.1902 (5.6918)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.281 (0.260)	Data 1.08e-04 (1.07e-03)	Tok/s 60212 (53510)	Loss/tok 3.9605 (5.6709)	LR 2.000e-03
0: TRAIN [0][830/1938]	Time 0.418 (0.261)	Data 1.24e-04 (1.05e-03)	Tok/s 71801 (53541)	Loss/tok 4.3518 (5.6485)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.282 (0.260)	Data 1.29e-04 (1.04e-03)	Tok/s 59516 (53490)	Loss/tok 3.8673 (5.6304)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.220 (0.260)	Data 1.26e-04 (1.03e-03)	Tok/s 46608 (53469)	Loss/tok 3.8053 (5.6117)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.283 (0.260)	Data 1.13e-04 (1.02e-03)	Tok/s 59869 (53530)	Loss/tok 3.8444 (5.5897)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.343 (0.261)	Data 1.16e-04 (1.01e-03)	Tok/s 68347 (53582)	Loss/tok 4.1640 (5.5686)	LR 2.000e-03
0: TRAIN [0][880/1938]	Time 0.280 (0.261)	Data 1.16e-04 (1.00e-03)	Tok/s 59542 (53589)	Loss/tok 3.9982 (5.5496)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.284 (0.261)	Data 1.24e-04 (9.95e-04)	Tok/s 59369 (53597)	Loss/tok 3.8513 (5.5309)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.221 (0.260)	Data 9.99e-05 (9.85e-04)	Tok/s 46173 (53488)	Loss/tok 3.6352 (5.5167)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.343 (0.260)	Data 1.03e-04 (9.76e-04)	Tok/s 67609 (53517)	Loss/tok 4.1491 (5.4976)	LR 2.000e-03
0: TRAIN [0][920/1938]	Time 0.221 (0.260)	Data 9.70e-05 (9.66e-04)	Tok/s 45964 (53514)	Loss/tok 3.6646 (5.4801)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.219 (0.260)	Data 1.04e-04 (9.57e-04)	Tok/s 46415 (53447)	Loss/tok 3.6320 (5.4654)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.221 (0.260)	Data 1.20e-04 (9.48e-04)	Tok/s 47159 (53504)	Loss/tok 3.4852 (5.4452)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.223 (0.260)	Data 1.31e-04 (9.39e-04)	Tok/s 46889 (53462)	Loss/tok 3.4731 (5.4301)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][960/1938]	Time 0.218 (0.260)	Data 1.11e-04 (9.31e-04)	Tok/s 47700 (53492)	Loss/tok 3.5279 (5.4120)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][970/1938]	Time 0.282 (0.260)	Data 1.16e-04 (9.22e-04)	Tok/s 59710 (53470)	Loss/tok 3.8827 (5.3969)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.220 (0.260)	Data 1.20e-04 (9.14e-04)	Tok/s 47499 (53520)	Loss/tok 3.4666 (5.3792)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.222 (0.260)	Data 1.06e-04 (9.06e-04)	Tok/s 47244 (53484)	Loss/tok 3.5910 (5.3650)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.221 (0.260)	Data 1.32e-04 (8.98e-04)	Tok/s 45974 (53470)	Loss/tok 3.6179 (5.3503)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.165 (0.260)	Data 1.06e-04 (8.90e-04)	Tok/s 31900 (53441)	Loss/tok 2.9932 (5.3363)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.220 (0.260)	Data 1.07e-04 (8.83e-04)	Tok/s 47218 (53467)	Loss/tok 3.5814 (5.3211)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.220 (0.260)	Data 1.21e-04 (8.75e-04)	Tok/s 46232 (53455)	Loss/tok 3.4012 (5.3068)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.221 (0.260)	Data 9.85e-05 (8.68e-04)	Tok/s 47336 (53474)	Loss/tok 3.5190 (5.2912)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.282 (0.260)	Data 1.07e-04 (8.61e-04)	Tok/s 59794 (53508)	Loss/tok 3.8113 (5.2759)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.165 (0.260)	Data 1.12e-04 (8.54e-04)	Tok/s 31636 (53488)	Loss/tok 3.0055 (5.2630)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.219 (0.260)	Data 1.47e-04 (8.47e-04)	Tok/s 48241 (53468)	Loss/tok 3.5049 (5.2500)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.220 (0.260)	Data 9.97e-05 (8.41e-04)	Tok/s 46251 (53427)	Loss/tok 3.4639 (5.2377)	LR 2.000e-03
0: TRAIN [0][1090/1938]	Time 0.166 (0.260)	Data 1.11e-04 (8.34e-04)	Tok/s 31889 (53448)	Loss/tok 3.0277 (5.2236)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.166 (0.260)	Data 1.05e-04 (8.28e-04)	Tok/s 31867 (53410)	Loss/tok 3.0023 (5.2116)	LR 2.000e-03
0: TRAIN [0][1110/1938]	Time 0.283 (0.260)	Data 1.35e-04 (8.22e-04)	Tok/s 59017 (53423)	Loss/tok 3.8356 (5.1980)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.166 (0.260)	Data 1.19e-04 (8.15e-04)	Tok/s 32221 (53360)	Loss/tok 2.8235 (5.1872)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.277 (0.259)	Data 1.08e-04 (8.09e-04)	Tok/s 61250 (53344)	Loss/tok 3.7202 (5.1755)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.343 (0.260)	Data 1.30e-04 (8.03e-04)	Tok/s 68116 (53405)	Loss/tok 3.9575 (5.1606)	LR 2.000e-03
0: TRAIN [0][1150/1938]	Time 0.219 (0.260)	Data 9.35e-05 (7.97e-04)	Tok/s 47641 (53374)	Loss/tok 3.5496 (5.1492)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.342 (0.260)	Data 1.32e-04 (7.91e-04)	Tok/s 69212 (53377)	Loss/tok 3.9635 (5.1367)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.280 (0.260)	Data 1.17e-04 (7.86e-04)	Tok/s 60760 (53432)	Loss/tok 3.7375 (5.1232)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.221 (0.260)	Data 1.22e-04 (7.80e-04)	Tok/s 46696 (53382)	Loss/tok 3.6480 (5.1133)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.220 (0.260)	Data 1.06e-04 (7.75e-04)	Tok/s 47614 (53430)	Loss/tok 3.5604 (5.0998)	LR 2.000e-03
0: TRAIN [0][1200/1938]	Time 0.342 (0.260)	Data 1.14e-04 (7.69e-04)	Tok/s 68053 (53456)	Loss/tok 3.9726 (5.0875)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.280 (0.260)	Data 1.37e-04 (7.64e-04)	Tok/s 59673 (53508)	Loss/tok 3.7157 (5.0745)	LR 2.000e-03
0: TRAIN [0][1220/1938]	Time 0.280 (0.260)	Data 1.04e-04 (7.58e-04)	Tok/s 59852 (53509)	Loss/tok 3.6169 (5.0634)	LR 2.000e-03
0: TRAIN [0][1230/1938]	Time 0.221 (0.260)	Data 1.11e-04 (7.53e-04)	Tok/s 46621 (53510)	Loss/tok 3.5444 (5.0520)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.221 (0.260)	Data 1.09e-04 (7.48e-04)	Tok/s 45377 (53507)	Loss/tok 3.5334 (5.0410)	LR 2.000e-03
0: TRAIN [0][1250/1938]	Time 0.283 (0.260)	Data 9.73e-05 (7.43e-04)	Tok/s 58856 (53517)	Loss/tok 3.7350 (5.0303)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1260/1938]	Time 0.221 (0.260)	Data 1.03e-04 (7.38e-04)	Tok/s 46789 (53516)	Loss/tok 3.3787 (5.0195)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.222 (0.260)	Data 9.68e-05 (7.33e-04)	Tok/s 46136 (53515)	Loss/tok 3.5160 (5.0092)	LR 2.000e-03
0: TRAIN [0][1280/1938]	Time 0.339 (0.260)	Data 1.11e-04 (7.28e-04)	Tok/s 69045 (53525)	Loss/tok 4.0283 (4.9986)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.169 (0.260)	Data 1.25e-04 (7.24e-04)	Tok/s 32241 (53463)	Loss/tok 2.9718 (4.9903)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.223 (0.260)	Data 1.27e-04 (7.19e-04)	Tok/s 46367 (53438)	Loss/tok 3.4276 (4.9808)	LR 2.000e-03
0: TRAIN [0][1310/1938]	Time 0.167 (0.260)	Data 1.03e-04 (7.14e-04)	Tok/s 31665 (53400)	Loss/tok 2.8842 (4.9719)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.219 (0.260)	Data 1.03e-04 (7.10e-04)	Tok/s 47156 (53362)	Loss/tok 3.4796 (4.9627)	LR 2.000e-03
0: TRAIN [0][1330/1938]	Time 0.219 (0.259)	Data 1.12e-04 (7.05e-04)	Tok/s 47093 (53322)	Loss/tok 3.3877 (4.9539)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.220 (0.259)	Data 1.05e-04 (7.01e-04)	Tok/s 46694 (53297)	Loss/tok 3.5998 (4.9453)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.167 (0.259)	Data 9.18e-05 (6.97e-04)	Tok/s 31215 (53289)	Loss/tok 2.8080 (4.9360)	LR 2.000e-03
0: TRAIN [0][1360/1938]	Time 0.283 (0.259)	Data 1.09e-04 (6.92e-04)	Tok/s 59382 (53312)	Loss/tok 3.5267 (4.9253)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.164 (0.259)	Data 1.24e-04 (6.88e-04)	Tok/s 32221 (53306)	Loss/tok 2.8692 (4.9161)	LR 2.000e-03
0: TRAIN [0][1380/1938]	Time 0.279 (0.259)	Data 1.20e-04 (6.84e-04)	Tok/s 60212 (53301)	Loss/tok 3.7393 (4.9071)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.222 (0.259)	Data 1.16e-04 (6.80e-04)	Tok/s 46961 (53328)	Loss/tok 3.4045 (4.8970)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.287 (0.259)	Data 9.94e-05 (6.76e-04)	Tok/s 58257 (53317)	Loss/tok 3.7119 (4.8887)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.222 (0.260)	Data 1.04e-04 (6.72e-04)	Tok/s 46315 (53362)	Loss/tok 3.3675 (4.8784)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.416 (0.260)	Data 1.09e-04 (6.68e-04)	Tok/s 71713 (53414)	Loss/tok 3.9821 (4.8681)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1430/1938]	Time 0.283 (0.260)	Data 1.31e-04 (6.64e-04)	Tok/s 59139 (53429)	Loss/tok 3.6863 (4.8591)	LR 2.000e-03
0: TRAIN [0][1440/1938]	Time 0.342 (0.260)	Data 1.25e-04 (6.60e-04)	Tok/s 68716 (53423)	Loss/tok 3.8130 (4.8507)	LR 2.000e-03
0: TRAIN [0][1450/1938]	Time 0.414 (0.260)	Data 1.02e-04 (6.56e-04)	Tok/s 72719 (53460)	Loss/tok 4.0906 (4.8413)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.281 (0.260)	Data 9.70e-05 (6.53e-04)	Tok/s 60336 (53472)	Loss/tok 3.6271 (4.8327)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.283 (0.260)	Data 1.16e-04 (6.49e-04)	Tok/s 59760 (53442)	Loss/tok 3.6748 (4.8253)	LR 2.000e-03
0: TRAIN [0][1480/1938]	Time 0.222 (0.260)	Data 1.28e-04 (6.45e-04)	Tok/s 46850 (53461)	Loss/tok 3.4128 (4.8166)	LR 2.000e-03
0: TRAIN [0][1490/1938]	Time 0.281 (0.260)	Data 1.08e-04 (6.42e-04)	Tok/s 59429 (53484)	Loss/tok 3.7139 (4.8083)	LR 2.000e-03
0: TRAIN [0][1500/1938]	Time 0.278 (0.260)	Data 9.99e-05 (6.38e-04)	Tok/s 60430 (53478)	Loss/tok 3.5649 (4.8005)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.167 (0.260)	Data 1.13e-04 (6.35e-04)	Tok/s 31477 (53450)	Loss/tok 2.8745 (4.7934)	LR 2.000e-03
0: TRAIN [0][1520/1938]	Time 0.342 (0.260)	Data 1.78e-04 (6.31e-04)	Tok/s 68433 (53458)	Loss/tok 3.6735 (4.7850)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.282 (0.260)	Data 1.04e-04 (6.28e-04)	Tok/s 59434 (53448)	Loss/tok 3.6405 (4.7777)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.283 (0.260)	Data 1.32e-04 (6.25e-04)	Tok/s 59047 (53459)	Loss/tok 3.8182 (4.7699)	LR 2.000e-03
0: TRAIN [0][1550/1938]	Time 0.224 (0.260)	Data 1.36e-04 (6.22e-04)	Tok/s 45784 (53480)	Loss/tok 3.3183 (4.7617)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.219 (0.260)	Data 1.10e-04 (6.18e-04)	Tok/s 47217 (53469)	Loss/tok 3.2737 (4.7544)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.278 (0.260)	Data 1.23e-04 (6.15e-04)	Tok/s 60894 (53462)	Loss/tok 3.5837 (4.7475)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1580/1938]	Time 0.279 (0.260)	Data 1.36e-04 (6.12e-04)	Tok/s 60396 (53458)	Loss/tok 3.5941 (4.7404)	LR 2.000e-03
0: TRAIN [0][1590/1938]	Time 0.343 (0.260)	Data 1.13e-04 (6.09e-04)	Tok/s 67784 (53469)	Loss/tok 3.8565 (4.7331)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1600/1938]	Time 0.218 (0.260)	Data 1.22e-04 (6.06e-04)	Tok/s 45726 (53473)	Loss/tok 3.4408 (4.7265)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.283 (0.260)	Data 1.71e-04 (6.03e-04)	Tok/s 58840 (53498)	Loss/tok 3.5359 (4.7190)	LR 2.000e-03
0: TRAIN [0][1620/1938]	Time 0.219 (0.260)	Data 1.00e-04 (6.00e-04)	Tok/s 47554 (53502)	Loss/tok 3.4858 (4.7120)	LR 2.000e-03
0: TRAIN [0][1630/1938]	Time 0.217 (0.260)	Data 1.05e-04 (5.97e-04)	Tok/s 47736 (53497)	Loss/tok 3.3249 (4.7056)	LR 2.000e-03
0: TRAIN [0][1640/1938]	Time 0.281 (0.261)	Data 1.18e-04 (5.94e-04)	Tok/s 59884 (53538)	Loss/tok 3.6017 (4.6981)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.343 (0.261)	Data 1.20e-04 (5.91e-04)	Tok/s 68985 (53543)	Loss/tok 3.8159 (4.6910)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.221 (0.261)	Data 1.34e-04 (5.88e-04)	Tok/s 47274 (53583)	Loss/tok 3.3826 (4.6836)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.278 (0.261)	Data 1.23e-04 (5.86e-04)	Tok/s 59812 (53575)	Loss/tok 3.5403 (4.6772)	LR 2.000e-03
0: TRAIN [0][1680/1938]	Time 0.283 (0.261)	Data 1.19e-04 (5.83e-04)	Tok/s 61039 (53573)	Loss/tok 3.4780 (4.6705)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.283 (0.261)	Data 1.38e-04 (5.80e-04)	Tok/s 58955 (53582)	Loss/tok 3.5839 (4.6639)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.221 (0.261)	Data 1.15e-04 (5.77e-04)	Tok/s 47228 (53616)	Loss/tok 3.2992 (4.6568)	LR 2.000e-03
0: TRAIN [0][1710/1938]	Time 0.283 (0.261)	Data 1.01e-04 (5.75e-04)	Tok/s 58950 (53594)	Loss/tok 3.5396 (4.6509)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.167 (0.261)	Data 9.82e-05 (5.72e-04)	Tok/s 31571 (53571)	Loss/tok 2.7556 (4.6450)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.220 (0.261)	Data 1.26e-04 (5.69e-04)	Tok/s 47593 (53553)	Loss/tok 3.3665 (4.6391)	LR 2.000e-03
0: TRAIN [0][1740/1938]	Time 0.221 (0.261)	Data 1.24e-04 (5.67e-04)	Tok/s 47204 (53539)	Loss/tok 3.3903 (4.6333)	LR 2.000e-03
0: TRAIN [0][1750/1938]	Time 0.219 (0.261)	Data 1.10e-04 (5.64e-04)	Tok/s 47495 (53551)	Loss/tok 3.4746 (4.6270)	LR 2.000e-03
0: TRAIN [0][1760/1938]	Time 0.221 (0.261)	Data 1.27e-04 (5.61e-04)	Tok/s 46646 (53551)	Loss/tok 3.3420 (4.6209)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.414 (0.261)	Data 1.45e-04 (5.59e-04)	Tok/s 72873 (53560)	Loss/tok 3.9065 (4.6145)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.220 (0.261)	Data 1.05e-04 (5.56e-04)	Tok/s 46663 (53555)	Loss/tok 3.2373 (4.6086)	LR 2.000e-03
0: TRAIN [0][1790/1938]	Time 0.341 (0.261)	Data 1.00e-04 (5.54e-04)	Tok/s 68633 (53554)	Loss/tok 3.8054 (4.6029)	LR 2.000e-03
0: TRAIN [0][1800/1938]	Time 0.414 (0.261)	Data 9.54e-05 (5.51e-04)	Tok/s 72794 (53577)	Loss/tok 4.0124 (4.5967)	LR 2.000e-03
0: TRAIN [0][1810/1938]	Time 0.281 (0.261)	Data 1.16e-04 (5.49e-04)	Tok/s 58784 (53571)	Loss/tok 3.5665 (4.5912)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.281 (0.261)	Data 1.03e-04 (5.47e-04)	Tok/s 59706 (53596)	Loss/tok 3.5479 (4.5850)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.221 (0.261)	Data 9.39e-05 (5.44e-04)	Tok/s 46432 (53594)	Loss/tok 3.3021 (4.5796)	LR 2.000e-03
0: TRAIN [0][1840/1938]	Time 0.165 (0.261)	Data 1.05e-04 (5.42e-04)	Tok/s 32212 (53587)	Loss/tok 2.8686 (4.5740)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.221 (0.261)	Data 9.54e-05 (5.40e-04)	Tok/s 46853 (53598)	Loss/tok 3.2634 (4.5682)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.415 (0.261)	Data 9.54e-05 (5.37e-04)	Tok/s 71839 (53627)	Loss/tok 3.9578 (4.5620)	LR 2.000e-03
0: TRAIN [0][1870/1938]	Time 0.220 (0.261)	Data 1.15e-04 (5.35e-04)	Tok/s 46365 (53621)	Loss/tok 3.3006 (4.5567)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1880/1938]	Time 0.222 (0.261)	Data 1.91e-04 (5.33e-04)	Tok/s 47467 (53594)	Loss/tok 3.3213 (4.5520)	LR 2.000e-03
0: TRAIN [0][1890/1938]	Time 0.223 (0.261)	Data 1.05e-04 (5.31e-04)	Tok/s 46605 (53601)	Loss/tok 3.2444 (4.5464)	LR 2.000e-03
0: TRAIN [0][1900/1938]	Time 0.341 (0.261)	Data 1.32e-04 (5.28e-04)	Tok/s 68995 (53599)	Loss/tok 3.6089 (4.5411)	LR 2.000e-03
0: TRAIN [0][1910/1938]	Time 0.220 (0.261)	Data 1.03e-04 (5.26e-04)	Tok/s 46203 (53583)	Loss/tok 3.1914 (4.5363)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.222 (0.261)	Data 1.50e-04 (5.24e-04)	Tok/s 45939 (53554)	Loss/tok 3.2959 (4.5317)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.278 (0.261)	Data 9.99e-05 (5.22e-04)	Tok/s 60532 (53558)	Loss/tok 3.6993 (4.5268)	LR 2.000e-03
:::MLL 1582041424.782 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 525}}
:::MLL 1582041424.782 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.697 (0.697)	Decoder iters 149.0 (149.0)	Tok/s 22397 (22397)
0: Running moses detokenizer
0: BLEU(score=19.953376134987163, counts=[34104, 15636, 8335, 4628], totals=[63792, 60789, 57786, 54787], precisions=[53.46124905944319, 25.721758870848344, 14.423908905271173, 8.447259386350776], bp=0.9862380366892585, sys_len=63792, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1582041426.911 eval_accuracy: {"value": 19.95, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 536}}
:::MLL 1582041426.912 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 0	Training Loss: 4.5224	Test BLEU: 19.95
0: Performance: Epoch: 0	Training: 428600 Tok/s
0: Finished epoch 0
:::MLL 1582041426.912 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 558}}
:::MLL 1582041426.913 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1582041426.913 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 515}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 985637513
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][0/1938]	Time 1.047 (1.047)	Data 6.95e-01 (6.95e-01)	Tok/s 22477 (22477)	Loss/tok 3.6391 (3.6391)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.278 (0.335)	Data 1.25e-04 (6.33e-02)	Tok/s 60785 (51553)	Loss/tok 3.4704 (3.4952)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.220 (0.289)	Data 1.09e-04 (3.32e-02)	Tok/s 47672 (50993)	Loss/tok 3.2109 (3.4315)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.166 (0.277)	Data 1.03e-04 (2.25e-02)	Tok/s 31411 (51402)	Loss/tok 2.7099 (3.4239)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.344 (0.271)	Data 1.04e-04 (1.71e-02)	Tok/s 68485 (51680)	Loss/tok 3.4950 (3.4142)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.164 (0.264)	Data 1.02e-04 (1.38e-02)	Tok/s 31942 (50932)	Loss/tok 2.7612 (3.4133)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.219 (0.262)	Data 1.09e-04 (1.15e-02)	Tok/s 46817 (51304)	Loss/tok 3.1680 (3.4113)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.218 (0.263)	Data 1.22e-04 (9.91e-03)	Tok/s 47550 (51848)	Loss/tok 3.2375 (3.4250)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.221 (0.261)	Data 1.32e-04 (8.70e-03)	Tok/s 47022 (51797)	Loss/tok 3.2750 (3.4184)	LR 2.000e-03
0: TRAIN [1][90/1938]	Time 0.222 (0.261)	Data 1.10e-04 (7.76e-03)	Tok/s 46675 (52027)	Loss/tok 3.1943 (3.4276)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.219 (0.259)	Data 1.08e-04 (7.00e-03)	Tok/s 47569 (51784)	Loss/tok 3.2593 (3.4241)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.219 (0.257)	Data 1.03e-04 (6.38e-03)	Tok/s 47415 (51504)	Loss/tok 3.2157 (3.4161)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.283 (0.259)	Data 9.92e-05 (5.86e-03)	Tok/s 60069 (51928)	Loss/tok 3.4810 (3.4325)	LR 2.000e-03
0: TRAIN [1][130/1938]	Time 0.221 (0.260)	Data 1.09e-04 (5.42e-03)	Tok/s 47433 (52223)	Loss/tok 3.2936 (3.4417)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.222 (0.259)	Data 9.85e-05 (5.04e-03)	Tok/s 47308 (52060)	Loss/tok 3.1829 (3.4372)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.279 (0.260)	Data 1.03e-04 (4.72e-03)	Tok/s 60580 (52392)	Loss/tok 3.4004 (3.4382)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.165 (0.258)	Data 1.10e-04 (4.43e-03)	Tok/s 32208 (52197)	Loss/tok 2.7674 (3.4312)	LR 2.000e-03
0: TRAIN [1][170/1938]	Time 0.283 (0.260)	Data 1.12e-04 (4.18e-03)	Tok/s 59942 (52411)	Loss/tok 3.5096 (3.4413)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.415 (0.260)	Data 1.10e-04 (3.96e-03)	Tok/s 71331 (52439)	Loss/tok 3.9185 (3.4458)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.219 (0.260)	Data 1.07e-04 (3.75e-03)	Tok/s 46513 (52405)	Loss/tok 3.2290 (3.4455)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.284 (0.259)	Data 1.20e-04 (3.57e-03)	Tok/s 59440 (52322)	Loss/tok 3.4944 (3.4411)	LR 2.000e-03
0: TRAIN [1][210/1938]	Time 0.222 (0.258)	Data 1.09e-04 (3.41e-03)	Tok/s 47347 (52235)	Loss/tok 3.2203 (3.4352)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.222 (0.258)	Data 1.08e-04 (3.26e-03)	Tok/s 46383 (52358)	Loss/tok 3.2469 (3.4356)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.343 (0.258)	Data 9.51e-05 (3.12e-03)	Tok/s 67161 (52458)	Loss/tok 3.7189 (3.4385)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.345 (0.258)	Data 9.58e-05 (3.00e-03)	Tok/s 68312 (52568)	Loss/tok 3.6549 (3.4386)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.344 (0.258)	Data 1.07e-04 (2.88e-03)	Tok/s 67171 (52506)	Loss/tok 3.6889 (3.4371)	LR 2.000e-03
0: TRAIN [1][260/1938]	Time 0.220 (0.257)	Data 1.02e-04 (2.78e-03)	Tok/s 46240 (52360)	Loss/tok 3.2046 (3.4340)	LR 2.000e-03
0: TRAIN [1][270/1938]	Time 0.222 (0.257)	Data 1.02e-04 (2.68e-03)	Tok/s 47098 (52420)	Loss/tok 3.2122 (3.4357)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.219 (0.259)	Data 1.12e-04 (2.59e-03)	Tok/s 46730 (52594)	Loss/tok 3.1630 (3.4406)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.412 (0.259)	Data 1.28e-04 (2.50e-03)	Tok/s 72260 (52728)	Loss/tok 3.8388 (3.4474)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.418 (0.260)	Data 1.26e-04 (2.42e-03)	Tok/s 70365 (52844)	Loss/tok 3.8504 (3.4490)	LR 2.000e-03
0: TRAIN [1][310/1938]	Time 0.221 (0.260)	Data 1.19e-04 (2.35e-03)	Tok/s 46655 (52857)	Loss/tok 3.1813 (3.4482)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.283 (0.260)	Data 1.13e-04 (2.28e-03)	Tok/s 59605 (52929)	Loss/tok 3.4354 (3.4466)	LR 2.000e-03
0: TRAIN [1][330/1938]	Time 0.223 (0.260)	Data 1.39e-04 (2.21e-03)	Tok/s 45040 (52866)	Loss/tok 3.1602 (3.4468)	LR 2.000e-03
0: TRAIN [1][340/1938]	Time 0.168 (0.261)	Data 1.20e-04 (2.15e-03)	Tok/s 31966 (53004)	Loss/tok 2.8801 (3.4493)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.285 (0.261)	Data 1.15e-04 (2.09e-03)	Tok/s 59285 (53132)	Loss/tok 3.4933 (3.4533)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.343 (0.261)	Data 1.30e-04 (2.04e-03)	Tok/s 68690 (53129)	Loss/tok 3.6963 (3.4561)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.283 (0.261)	Data 1.21e-04 (1.99e-03)	Tok/s 59166 (53009)	Loss/tok 3.3589 (3.4546)	LR 2.000e-03
0: TRAIN [1][380/1938]	Time 0.342 (0.261)	Data 1.22e-04 (1.94e-03)	Tok/s 68263 (53126)	Loss/tok 3.5983 (3.4564)	LR 2.000e-03
0: TRAIN [1][390/1938]	Time 0.344 (0.261)	Data 1.27e-04 (1.89e-03)	Tok/s 66815 (53045)	Loss/tok 3.6307 (3.4552)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][400/1938]	Time 0.222 (0.261)	Data 1.18e-04 (1.85e-03)	Tok/s 47117 (53137)	Loss/tok 3.2102 (3.4578)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.344 (0.261)	Data 1.35e-04 (1.81e-03)	Tok/s 68600 (53063)	Loss/tok 3.5782 (3.4568)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.219 (0.261)	Data 1.01e-04 (1.77e-03)	Tok/s 46760 (53026)	Loss/tok 3.4136 (3.4568)	LR 2.000e-03
0: TRAIN [1][430/1938]	Time 0.221 (0.261)	Data 1.12e-04 (1.73e-03)	Tok/s 46712 (53018)	Loss/tok 3.1675 (3.4550)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.343 (0.261)	Data 1.39e-04 (1.69e-03)	Tok/s 67499 (53024)	Loss/tok 3.6517 (3.4557)	LR 2.000e-03
0: TRAIN [1][450/1938]	Time 0.218 (0.260)	Data 1.50e-04 (1.66e-03)	Tok/s 47317 (52911)	Loss/tok 3.2099 (3.4520)	LR 2.000e-03
0: TRAIN [1][460/1938]	Time 0.220 (0.260)	Data 1.26e-04 (1.62e-03)	Tok/s 48022 (52912)	Loss/tok 3.3192 (3.4508)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.342 (0.260)	Data 1.17e-04 (1.59e-03)	Tok/s 68560 (52981)	Loss/tok 3.6164 (3.4510)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.219 (0.260)	Data 1.24e-04 (1.56e-03)	Tok/s 46584 (52966)	Loss/tok 3.1707 (3.4485)	LR 2.000e-03
0: TRAIN [1][490/1938]	Time 0.220 (0.259)	Data 1.18e-04 (1.53e-03)	Tok/s 47435 (52892)	Loss/tok 3.2848 (3.4461)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.221 (0.259)	Data 1.17e-04 (1.50e-03)	Tok/s 46533 (52868)	Loss/tok 3.2392 (3.4447)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.279 (0.259)	Data 1.03e-04 (1.48e-03)	Tok/s 61059 (52859)	Loss/tok 3.4380 (3.4432)	LR 2.000e-03
0: TRAIN [1][520/1938]	Time 0.218 (0.260)	Data 1.22e-04 (1.45e-03)	Tok/s 47008 (53058)	Loss/tok 3.2219 (3.4489)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.342 (0.259)	Data 1.19e-04 (1.43e-03)	Tok/s 67801 (52924)	Loss/tok 3.6863 (3.4467)	LR 2.000e-03
0: TRAIN [1][540/1938]	Time 0.221 (0.259)	Data 1.49e-04 (1.40e-03)	Tok/s 46129 (52884)	Loss/tok 3.1287 (3.4451)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][550/1938]	Time 0.341 (0.259)	Data 1.27e-04 (1.38e-03)	Tok/s 68009 (52924)	Loss/tok 3.6957 (3.4449)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.281 (0.259)	Data 1.20e-04 (1.36e-03)	Tok/s 59868 (53016)	Loss/tok 3.4820 (3.4462)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.416 (0.260)	Data 1.18e-04 (1.33e-03)	Tok/s 72348 (53074)	Loss/tok 3.8282 (3.4480)	LR 2.000e-03
0: TRAIN [1][580/1938]	Time 0.283 (0.260)	Data 1.45e-04 (1.31e-03)	Tok/s 59487 (53173)	Loss/tok 3.4087 (3.4488)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.220 (0.260)	Data 1.36e-04 (1.29e-03)	Tok/s 46276 (53164)	Loss/tok 3.2147 (3.4495)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.216 (0.261)	Data 1.12e-04 (1.27e-03)	Tok/s 47272 (53191)	Loss/tok 3.3561 (3.4496)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.219 (0.260)	Data 1.09e-04 (1.26e-03)	Tok/s 47305 (53143)	Loss/tok 3.1765 (3.4478)	LR 2.000e-03
0: TRAIN [1][620/1938]	Time 0.283 (0.260)	Data 1.52e-04 (1.24e-03)	Tok/s 59351 (53177)	Loss/tok 3.4145 (3.4468)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.166 (0.260)	Data 1.15e-04 (1.22e-03)	Tok/s 31861 (53206)	Loss/tok 2.6034 (3.4469)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.280 (0.260)	Data 1.03e-04 (1.20e-03)	Tok/s 59769 (53138)	Loss/tok 3.5378 (3.4451)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.218 (0.260)	Data 1.22e-04 (1.19e-03)	Tok/s 46871 (53155)	Loss/tok 3.1354 (3.4450)	LR 2.000e-03
0: TRAIN [1][660/1938]	Time 0.219 (0.260)	Data 1.26e-04 (1.17e-03)	Tok/s 46989 (53253)	Loss/tok 3.1667 (3.4449)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.281 (0.261)	Data 1.24e-04 (1.15e-03)	Tok/s 60658 (53335)	Loss/tok 3.4891 (3.4461)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.165 (0.261)	Data 1.41e-04 (1.14e-03)	Tok/s 31857 (53296)	Loss/tok 2.7379 (3.4447)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][690/1938]	Time 0.418 (0.261)	Data 1.02e-04 (1.12e-03)	Tok/s 71416 (53363)	Loss/tok 3.7952 (3.4461)	LR 2.000e-03
0: TRAIN [1][700/1938]	Time 0.415 (0.261)	Data 1.55e-04 (1.11e-03)	Tok/s 70646 (53360)	Loss/tok 3.7361 (3.4449)	LR 2.000e-03
0: TRAIN [1][710/1938]	Time 0.340 (0.261)	Data 1.04e-04 (1.10e-03)	Tok/s 68431 (53378)	Loss/tok 3.5814 (3.4454)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.220 (0.262)	Data 1.00e-04 (1.08e-03)	Tok/s 46834 (53422)	Loss/tok 3.2022 (3.4462)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.279 (0.261)	Data 1.18e-04 (1.07e-03)	Tok/s 60154 (53425)	Loss/tok 3.4832 (3.4449)	LR 2.000e-03
0: TRAIN [1][740/1938]	Time 0.223 (0.262)	Data 1.56e-04 (1.06e-03)	Tok/s 46075 (53478)	Loss/tok 3.2242 (3.4457)	LR 2.000e-03
0: TRAIN [1][750/1938]	Time 0.222 (0.261)	Data 1.21e-04 (1.04e-03)	Tok/s 46285 (53449)	Loss/tok 3.1568 (3.4446)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.221 (0.262)	Data 1.14e-04 (1.03e-03)	Tok/s 46856 (53472)	Loss/tok 3.0882 (3.4441)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.341 (0.262)	Data 1.17e-04 (1.02e-03)	Tok/s 67945 (53491)	Loss/tok 3.6509 (3.4436)	LR 2.000e-03
0: TRAIN [1][780/1938]	Time 0.285 (0.262)	Data 1.02e-04 (1.01e-03)	Tok/s 58066 (53505)	Loss/tok 3.4109 (3.4430)	LR 2.000e-03
0: TRAIN [1][790/1938]	Time 0.219 (0.261)	Data 1.20e-04 (9.96e-04)	Tok/s 46783 (53485)	Loss/tok 3.2318 (3.4417)	LR 2.000e-03
0: TRAIN [1][800/1938]	Time 0.283 (0.261)	Data 1.22e-04 (9.85e-04)	Tok/s 58169 (53449)	Loss/tok 3.4315 (3.4401)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][810/1938]	Time 0.417 (0.261)	Data 1.22e-04 (9.75e-04)	Tok/s 71512 (53505)	Loss/tok 3.8119 (3.4412)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.221 (0.261)	Data 1.06e-04 (9.64e-04)	Tok/s 46939 (53495)	Loss/tok 3.1553 (3.4413)	LR 2.000e-03
0: TRAIN [1][830/1938]	Time 0.343 (0.261)	Data 9.73e-05 (9.54e-04)	Tok/s 67478 (53490)	Loss/tok 3.7307 (3.4408)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.280 (0.261)	Data 1.11e-04 (9.44e-04)	Tok/s 59834 (53472)	Loss/tok 3.3586 (3.4391)	LR 2.000e-03
0: TRAIN [1][850/1938]	Time 0.163 (0.261)	Data 1.22e-04 (9.34e-04)	Tok/s 32325 (53410)	Loss/tok 2.6900 (3.4385)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.282 (0.261)	Data 9.99e-05 (9.25e-04)	Tok/s 58857 (53478)	Loss/tok 3.4344 (3.4386)	LR 2.000e-03
0: TRAIN [1][870/1938]	Time 0.413 (0.262)	Data 1.05e-04 (9.16e-04)	Tok/s 71660 (53523)	Loss/tok 3.7912 (3.4389)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.222 (0.262)	Data 1.05e-04 (9.07e-04)	Tok/s 46286 (53547)	Loss/tok 3.1528 (3.4385)	LR 2.000e-03
0: TRAIN [1][890/1938]	Time 0.221 (0.261)	Data 1.32e-04 (8.98e-04)	Tok/s 45679 (53489)	Loss/tok 3.2085 (3.4373)	LR 2.000e-03
0: TRAIN [1][900/1938]	Time 0.221 (0.261)	Data 1.02e-04 (8.89e-04)	Tok/s 45697 (53514)	Loss/tok 3.0925 (3.4367)	LR 2.000e-03
0: TRAIN [1][910/1938]	Time 0.222 (0.261)	Data 1.02e-04 (8.81e-04)	Tok/s 46541 (53450)	Loss/tok 3.2637 (3.4351)	LR 2.000e-03
0: TRAIN [1][920/1938]	Time 0.284 (0.261)	Data 1.29e-04 (8.72e-04)	Tok/s 58385 (53417)	Loss/tok 3.4473 (3.4346)	LR 2.000e-03
0: TRAIN [1][930/1938]	Time 0.284 (0.261)	Data 1.39e-04 (8.64e-04)	Tok/s 59829 (53420)	Loss/tok 3.3392 (3.4341)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.280 (0.261)	Data 1.15e-04 (8.56e-04)	Tok/s 60598 (53400)	Loss/tok 3.4236 (3.4327)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.347 (0.261)	Data 1.22e-04 (8.49e-04)	Tok/s 66962 (53434)	Loss/tok 3.7229 (3.4329)	LR 2.000e-03
0: TRAIN [1][960/1938]	Time 0.222 (0.261)	Data 1.22e-04 (8.41e-04)	Tok/s 46397 (53424)	Loss/tok 3.2309 (3.4324)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.347 (0.261)	Data 1.26e-04 (8.34e-04)	Tok/s 66486 (53451)	Loss/tok 3.6447 (3.4320)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][980/1938]	Time 0.345 (0.261)	Data 1.13e-04 (8.26e-04)	Tok/s 68588 (53476)	Loss/tok 3.5555 (3.4324)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.282 (0.261)	Data 1.18e-04 (8.19e-04)	Tok/s 59450 (53500)	Loss/tok 3.3427 (3.4316)	LR 2.000e-03
0: TRAIN [1][1000/1938]	Time 0.284 (0.261)	Data 1.19e-04 (8.12e-04)	Tok/s 59530 (53473)	Loss/tok 3.3310 (3.4307)	LR 2.000e-03
0: TRAIN [1][1010/1938]	Time 0.282 (0.261)	Data 1.19e-04 (8.05e-04)	Tok/s 60302 (53478)	Loss/tok 3.5134 (3.4307)	LR 2.000e-03
0: TRAIN [1][1020/1938]	Time 0.411 (0.261)	Data 1.21e-04 (7.99e-04)	Tok/s 72198 (53475)	Loss/tok 3.8207 (3.4304)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.219 (0.261)	Data 1.30e-04 (7.92e-04)	Tok/s 47064 (53504)	Loss/tok 3.2129 (3.4303)	LR 2.000e-03
0: TRAIN [1][1040/1938]	Time 0.223 (0.261)	Data 1.30e-04 (7.86e-04)	Tok/s 46102 (53524)	Loss/tok 3.0483 (3.4297)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.282 (0.261)	Data 1.14e-04 (7.79e-04)	Tok/s 59797 (53546)	Loss/tok 3.4374 (3.4294)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.224 (0.261)	Data 1.04e-04 (7.73e-04)	Tok/s 46427 (53551)	Loss/tok 3.1414 (3.4301)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.222 (0.262)	Data 1.42e-04 (7.67e-04)	Tok/s 46349 (53578)	Loss/tok 3.1559 (3.4295)	LR 2.000e-03
0: TRAIN [1][1080/1938]	Time 0.282 (0.262)	Data 1.27e-04 (7.61e-04)	Tok/s 60214 (53634)	Loss/tok 3.4432 (3.4296)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.343 (0.262)	Data 1.01e-04 (7.55e-04)	Tok/s 68965 (53679)	Loss/tok 3.5319 (3.4306)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.279 (0.262)	Data 1.02e-04 (7.49e-04)	Tok/s 59867 (53689)	Loss/tok 3.3757 (3.4300)	LR 2.000e-03
0: TRAIN [1][1110/1938]	Time 0.281 (0.262)	Data 1.20e-04 (7.44e-04)	Tok/s 58880 (53682)	Loss/tok 3.4336 (3.4296)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.342 (0.262)	Data 1.07e-04 (7.38e-04)	Tok/s 66981 (53716)	Loss/tok 3.6647 (3.4292)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1130/1938]	Time 0.341 (0.262)	Data 1.06e-04 (7.33e-04)	Tok/s 67852 (53735)	Loss/tok 3.6736 (3.4293)	LR 2.000e-03
0: TRAIN [1][1140/1938]	Time 0.221 (0.263)	Data 1.05e-04 (7.27e-04)	Tok/s 46187 (53775)	Loss/tok 3.1149 (3.4291)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.283 (0.262)	Data 1.15e-04 (7.22e-04)	Tok/s 58884 (53762)	Loss/tok 3.3051 (3.4280)	LR 2.000e-03
0: TRAIN [1][1160/1938]	Time 0.281 (0.262)	Data 9.87e-05 (7.17e-04)	Tok/s 59263 (53782)	Loss/tok 3.4319 (3.4278)	LR 2.000e-03
0: TRAIN [1][1170/1938]	Time 0.342 (0.262)	Data 1.12e-04 (7.11e-04)	Tok/s 67799 (53753)	Loss/tok 3.6313 (3.4277)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.221 (0.262)	Data 1.31e-04 (7.06e-04)	Tok/s 46305 (53765)	Loss/tok 3.2188 (3.4273)	LR 2.000e-03
0: TRAIN [1][1190/1938]	Time 0.221 (0.262)	Data 1.05e-04 (7.01e-04)	Tok/s 47986 (53748)	Loss/tok 3.1721 (3.4262)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.282 (0.262)	Data 1.26e-04 (6.96e-04)	Tok/s 59968 (53757)	Loss/tok 3.3556 (3.4257)	LR 2.000e-03
0: TRAIN [1][1210/1938]	Time 0.165 (0.262)	Data 1.13e-04 (6.92e-04)	Tok/s 32625 (53703)	Loss/tok 2.6308 (3.4245)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.219 (0.262)	Data 1.21e-04 (6.87e-04)	Tok/s 48151 (53722)	Loss/tok 3.1203 (3.4246)	LR 2.000e-03
0: TRAIN [1][1230/1938]	Time 0.282 (0.262)	Data 1.27e-04 (6.82e-04)	Tok/s 59562 (53746)	Loss/tok 3.4143 (3.4250)	LR 2.000e-03
0: TRAIN [1][1240/1938]	Time 0.221 (0.262)	Data 9.80e-05 (6.77e-04)	Tok/s 46626 (53676)	Loss/tok 3.1307 (3.4236)	LR 2.000e-03
0: TRAIN [1][1250/1938]	Time 0.279 (0.262)	Data 9.89e-05 (6.73e-04)	Tok/s 59928 (53679)	Loss/tok 3.3720 (3.4233)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.223 (0.262)	Data 1.27e-04 (6.68e-04)	Tok/s 47335 (53657)	Loss/tok 3.0555 (3.4220)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.285 (0.262)	Data 1.22e-04 (6.64e-04)	Tok/s 58548 (53714)	Loss/tok 3.2724 (3.4229)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.342 (0.263)	Data 1.26e-04 (6.60e-04)	Tok/s 68496 (53764)	Loss/tok 3.4678 (3.4250)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.282 (0.262)	Data 1.03e-04 (6.56e-04)	Tok/s 59222 (53729)	Loss/tok 3.4351 (3.4238)	LR 2.000e-03
0: TRAIN [1][1300/1938]	Time 0.280 (0.262)	Data 1.16e-04 (6.52e-04)	Tok/s 59391 (53681)	Loss/tok 3.4087 (3.4227)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1310/1938]	Time 0.339 (0.262)	Data 1.25e-04 (6.48e-04)	Tok/s 69197 (53699)	Loss/tok 3.6175 (3.4234)	LR 2.000e-03
0: TRAIN [1][1320/1938]	Time 0.284 (0.262)	Data 1.10e-04 (6.44e-04)	Tok/s 58298 (53738)	Loss/tok 3.5234 (3.4244)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.221 (0.262)	Data 1.03e-04 (6.40e-04)	Tok/s 46911 (53735)	Loss/tok 3.1620 (3.4234)	LR 2.000e-03
0: TRAIN [1][1340/1938]	Time 0.283 (0.262)	Data 1.44e-04 (6.36e-04)	Tok/s 59274 (53717)	Loss/tok 3.4423 (3.4225)	LR 2.000e-03
0: TRAIN [1][1350/1938]	Time 0.222 (0.262)	Data 1.15e-04 (6.32e-04)	Tok/s 47006 (53725)	Loss/tok 3.0028 (3.4223)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.220 (0.262)	Data 1.01e-04 (6.28e-04)	Tok/s 46879 (53716)	Loss/tok 3.1166 (3.4219)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.343 (0.262)	Data 1.31e-04 (6.24e-04)	Tok/s 67730 (53701)	Loss/tok 3.5193 (3.4213)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.342 (0.262)	Data 1.28e-04 (6.21e-04)	Tok/s 67751 (53717)	Loss/tok 3.5356 (3.4216)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.217 (0.262)	Data 1.12e-04 (6.17e-04)	Tok/s 46835 (53685)	Loss/tok 3.0865 (3.4204)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.220 (0.262)	Data 1.10e-04 (6.13e-04)	Tok/s 46731 (53696)	Loss/tok 3.1334 (3.4197)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.218 (0.262)	Data 1.40e-04 (6.10e-04)	Tok/s 47593 (53692)	Loss/tok 3.1725 (3.4193)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.220 (0.262)	Data 1.16e-04 (6.06e-04)	Tok/s 46898 (53645)	Loss/tok 3.0804 (3.4182)	LR 2.000e-03
0: TRAIN [1][1430/1938]	Time 0.282 (0.262)	Data 1.12e-04 (6.03e-04)	Tok/s 58918 (53643)	Loss/tok 3.3859 (3.4175)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.279 (0.262)	Data 9.94e-05 (6.00e-04)	Tok/s 59468 (53627)	Loss/tok 3.4347 (3.4166)	LR 2.000e-03
0: TRAIN [1][1450/1938]	Time 0.220 (0.261)	Data 1.15e-04 (5.96e-04)	Tok/s 46790 (53599)	Loss/tok 3.2679 (3.4156)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.283 (0.261)	Data 1.23e-04 (5.93e-04)	Tok/s 60088 (53601)	Loss/tok 3.3423 (3.4150)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.220 (0.261)	Data 9.49e-05 (5.90e-04)	Tok/s 46749 (53561)	Loss/tok 2.9912 (3.4137)	LR 2.000e-03
0: TRAIN [1][1480/1938]	Time 0.220 (0.261)	Data 1.21e-04 (5.86e-04)	Tok/s 45855 (53552)	Loss/tok 3.1215 (3.4129)	LR 2.000e-03
0: TRAIN [1][1490/1938]	Time 0.165 (0.261)	Data 1.05e-04 (5.83e-04)	Tok/s 32809 (53568)	Loss/tok 2.7646 (3.4131)	LR 2.000e-03
0: TRAIN [1][1500/1938]	Time 0.166 (0.261)	Data 1.13e-04 (5.80e-04)	Tok/s 31728 (53525)	Loss/tok 2.6993 (3.4120)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.222 (0.261)	Data 1.43e-04 (5.77e-04)	Tok/s 45384 (53486)	Loss/tok 3.1171 (3.4110)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.165 (0.261)	Data 1.15e-04 (5.74e-04)	Tok/s 31927 (53481)	Loss/tok 2.6262 (3.4101)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.220 (0.261)	Data 1.03e-04 (5.71e-04)	Tok/s 47653 (53439)	Loss/tok 3.2410 (3.4091)	LR 2.000e-03
0: TRAIN [1][1540/1938]	Time 0.220 (0.260)	Data 1.40e-04 (5.68e-04)	Tok/s 48158 (53397)	Loss/tok 3.2268 (3.4080)	LR 2.000e-03
0: TRAIN [1][1550/1938]	Time 0.222 (0.260)	Data 1.25e-04 (5.65e-04)	Tok/s 45829 (53387)	Loss/tok 3.1502 (3.4076)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1560/1938]	Time 0.339 (0.260)	Data 1.28e-04 (5.62e-04)	Tok/s 69317 (53426)	Loss/tok 3.5002 (3.4077)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.342 (0.260)	Data 1.02e-04 (5.59e-04)	Tok/s 67901 (53422)	Loss/tok 3.5744 (3.4074)	LR 2.000e-03
0: TRAIN [1][1580/1938]	Time 0.282 (0.260)	Data 1.25e-04 (5.56e-04)	Tok/s 60251 (53433)	Loss/tok 3.3228 (3.4068)	LR 2.000e-03
0: TRAIN [1][1590/1938]	Time 0.229 (0.260)	Data 1.25e-04 (5.54e-04)	Tok/s 43832 (53443)	Loss/tok 3.1345 (3.4063)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.220 (0.260)	Data 1.15e-04 (5.51e-04)	Tok/s 45724 (53423)	Loss/tok 3.2077 (3.4055)	LR 2.000e-03
0: TRAIN [1][1610/1938]	Time 0.220 (0.260)	Data 1.29e-04 (5.48e-04)	Tok/s 47361 (53414)	Loss/tok 3.1429 (3.4049)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.283 (0.260)	Data 1.26e-04 (5.46e-04)	Tok/s 59419 (53408)	Loss/tok 3.3124 (3.4042)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.166 (0.260)	Data 1.02e-04 (5.43e-04)	Tok/s 32135 (53411)	Loss/tok 2.7198 (3.4043)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.343 (0.260)	Data 1.11e-04 (5.40e-04)	Tok/s 67949 (53405)	Loss/tok 3.5528 (3.4039)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.342 (0.260)	Data 1.13e-04 (5.38e-04)	Tok/s 68285 (53424)	Loss/tok 3.4903 (3.4037)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.222 (0.260)	Data 9.78e-05 (5.35e-04)	Tok/s 46694 (53444)	Loss/tok 3.0533 (3.4035)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.221 (0.260)	Data 9.70e-05 (5.33e-04)	Tok/s 46741 (53416)	Loss/tok 3.2131 (3.4025)	LR 2.000e-03
0: TRAIN [1][1680/1938]	Time 0.220 (0.260)	Data 9.70e-05 (5.30e-04)	Tok/s 47315 (53418)	Loss/tok 3.1511 (3.4027)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1690/1938]	Time 0.342 (0.260)	Data 1.08e-04 (5.28e-04)	Tok/s 68936 (53434)	Loss/tok 3.5547 (3.4030)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.163 (0.261)	Data 1.15e-04 (5.25e-04)	Tok/s 32605 (53427)	Loss/tok 2.7254 (3.4032)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1710/1938]	Time 0.222 (0.261)	Data 1.05e-04 (5.23e-04)	Tok/s 46281 (53448)	Loss/tok 3.1600 (3.4033)	LR 2.000e-03
0: TRAIN [1][1720/1938]	Time 0.284 (0.261)	Data 1.02e-04 (5.20e-04)	Tok/s 59762 (53475)	Loss/tok 3.4254 (3.4036)	LR 2.000e-03
0: TRAIN [1][1730/1938]	Time 0.281 (0.261)	Data 1.22e-04 (5.18e-04)	Tok/s 59922 (53473)	Loss/tok 3.4419 (3.4032)	LR 2.000e-03
0: TRAIN [1][1740/1938]	Time 0.219 (0.261)	Data 1.02e-04 (5.16e-04)	Tok/s 46813 (53480)	Loss/tok 3.1643 (3.4026)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.163 (0.261)	Data 1.23e-04 (5.13e-04)	Tok/s 32789 (53467)	Loss/tok 2.6390 (3.4027)	LR 2.000e-03
0: TRAIN [1][1760/1938]	Time 0.223 (0.261)	Data 1.02e-04 (5.11e-04)	Tok/s 46642 (53492)	Loss/tok 3.2320 (3.4027)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.218 (0.261)	Data 1.09e-04 (5.09e-04)	Tok/s 46922 (53460)	Loss/tok 3.1356 (3.4024)	LR 2.000e-03
0: TRAIN [1][1780/1938]	Time 0.280 (0.261)	Data 1.03e-04 (5.07e-04)	Tok/s 60343 (53494)	Loss/tok 3.4230 (3.4022)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.279 (0.261)	Data 9.80e-05 (5.05e-04)	Tok/s 60615 (53477)	Loss/tok 3.3595 (3.4019)	LR 2.000e-03
0: TRAIN [1][1800/1938]	Time 0.343 (0.261)	Data 1.12e-04 (5.02e-04)	Tok/s 66687 (53488)	Loss/tok 3.4955 (3.4016)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.284 (0.261)	Data 1.12e-04 (5.00e-04)	Tok/s 59338 (53486)	Loss/tok 3.2521 (3.4013)	LR 2.000e-03
0: TRAIN [1][1820/1938]	Time 0.221 (0.261)	Data 1.13e-04 (4.98e-04)	Tok/s 46346 (53468)	Loss/tok 3.1771 (3.4006)	LR 2.000e-03
0: TRAIN [1][1830/1938]	Time 0.283 (0.261)	Data 9.99e-05 (4.96e-04)	Tok/s 59746 (53493)	Loss/tok 3.3606 (3.4004)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.220 (0.261)	Data 1.09e-04 (4.94e-04)	Tok/s 47900 (53503)	Loss/tok 3.0182 (3.4002)	LR 2.000e-03
0: TRAIN [1][1850/1938]	Time 0.283 (0.261)	Data 1.16e-04 (4.92e-04)	Tok/s 59371 (53518)	Loss/tok 3.3011 (3.3996)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.418 (0.261)	Data 1.16e-04 (4.90e-04)	Tok/s 70572 (53508)	Loss/tok 3.8888 (3.3994)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.219 (0.261)	Data 1.12e-04 (4.88e-04)	Tok/s 46805 (53540)	Loss/tok 3.1329 (3.3993)	LR 2.000e-03
0: TRAIN [1][1880/1938]	Time 0.217 (0.261)	Data 1.12e-04 (4.86e-04)	Tok/s 48036 (53517)	Loss/tok 3.2285 (3.3985)	LR 2.000e-03
0: TRAIN [1][1890/1938]	Time 0.220 (0.261)	Data 1.07e-04 (4.84e-04)	Tok/s 46633 (53491)	Loss/tok 3.0414 (3.3977)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.165 (0.261)	Data 1.14e-04 (4.82e-04)	Tok/s 32051 (53489)	Loss/tok 2.7274 (3.3972)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.282 (0.261)	Data 1.05e-04 (4.80e-04)	Tok/s 59831 (53502)	Loss/tok 3.3820 (3.3974)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.279 (0.261)	Data 1.16e-04 (4.78e-04)	Tok/s 60187 (53502)	Loss/tok 3.3013 (3.3980)	LR 2.000e-03
0: TRAIN [1][1930/1938]	Time 0.281 (0.261)	Data 1.11e-04 (4.76e-04)	Tok/s 60385 (53490)	Loss/tok 3.3383 (3.3975)	LR 2.000e-03
:::MLL 1582041933.666 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 525}}
:::MLL 1582041933.667 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.608 (0.608)	Decoder iters 98.0 (98.0)	Tok/s 27212 (27212)
0: Running moses detokenizer
0: BLEU(score=21.91932322099207, counts=[36105, 17386, 9584, 5494], totals=[66109, 63106, 60103, 57104], precisions=[54.614349029632876, 27.550470636706493, 15.945959436301017, 9.621042308769963], bp=1.0, sys_len=66109, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1582041935.508 eval_accuracy: {"value": 21.92, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 536}}
:::MLL 1582041935.508 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 1	Training Loss: 3.3982	Test BLEU: 21.92
0: Performance: Epoch: 1	Training: 428099 Tok/s
0: Finished epoch 1
:::MLL 1582041935.509 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 558}}
:::MLL 1582041935.509 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1582041935.510 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 515}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 2818748122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][0/1938]	Time 0.923 (0.923)	Data 6.96e-01 (6.96e-01)	Tok/s 11036 (11036)	Loss/tok 3.0764 (3.0764)	LR 2.000e-03
0: TRAIN [2][10/1938]	Time 0.284 (0.352)	Data 1.06e-04 (6.34e-02)	Tok/s 58769 (55746)	Loss/tok 3.2978 (3.3035)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.282 (0.320)	Data 1.18e-04 (3.33e-02)	Tok/s 59582 (56360)	Loss/tok 3.2767 (3.2996)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.221 (0.310)	Data 1.13e-04 (2.26e-02)	Tok/s 46873 (57075)	Loss/tok 2.9724 (3.3034)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.219 (0.297)	Data 1.12e-04 (1.71e-02)	Tok/s 46720 (56416)	Loss/tok 3.1064 (3.2866)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.221 (0.290)	Data 1.33e-04 (1.38e-02)	Tok/s 47239 (56092)	Loss/tok 3.0967 (3.2806)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.282 (0.287)	Data 1.26e-04 (1.15e-02)	Tok/s 60192 (56183)	Loss/tok 3.1816 (3.2739)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.220 (0.283)	Data 1.26e-04 (9.92e-03)	Tok/s 46153 (55590)	Loss/tok 3.1211 (3.2798)	LR 2.000e-03
0: TRAIN [2][80/1938]	Time 0.280 (0.281)	Data 1.21e-04 (8.71e-03)	Tok/s 59628 (55360)	Loss/tok 3.2653 (3.2780)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.280 (0.280)	Data 1.36e-04 (7.77e-03)	Tok/s 59424 (55415)	Loss/tok 3.2631 (3.2799)	LR 2.000e-03
0: TRAIN [2][100/1938]	Time 0.220 (0.279)	Data 1.27e-04 (7.01e-03)	Tok/s 47557 (55470)	Loss/tok 3.0521 (3.2817)	LR 2.000e-03
0: TRAIN [2][110/1938]	Time 0.220 (0.277)	Data 1.30e-04 (6.39e-03)	Tok/s 48082 (55356)	Loss/tok 2.9940 (3.2766)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.220 (0.275)	Data 1.18e-04 (5.88e-03)	Tok/s 47301 (55067)	Loss/tok 3.0036 (3.2716)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.165 (0.274)	Data 1.13e-04 (5.44e-03)	Tok/s 31587 (54895)	Loss/tok 2.5849 (3.2692)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][140/1938]	Time 0.415 (0.274)	Data 1.29e-04 (5.06e-03)	Tok/s 70844 (54820)	Loss/tok 3.7252 (3.2822)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.279 (0.274)	Data 1.25e-04 (4.73e-03)	Tok/s 60300 (54794)	Loss/tok 3.2830 (3.2832)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.282 (0.274)	Data 1.37e-04 (4.45e-03)	Tok/s 59001 (54980)	Loss/tok 3.2692 (3.2827)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.343 (0.272)	Data 1.22e-04 (4.20e-03)	Tok/s 68390 (54757)	Loss/tok 3.4610 (3.2787)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.414 (0.273)	Data 1.15e-04 (3.97e-03)	Tok/s 71867 (54921)	Loss/tok 3.7475 (3.2848)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.221 (0.272)	Data 1.29e-04 (3.77e-03)	Tok/s 47552 (54745)	Loss/tok 3.1085 (3.2814)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.220 (0.273)	Data 1.44e-04 (3.59e-03)	Tok/s 47116 (55023)	Loss/tok 3.0338 (3.2902)	LR 2.000e-03
0: TRAIN [2][210/1938]	Time 0.169 (0.273)	Data 1.48e-04 (3.43e-03)	Tok/s 31495 (54939)	Loss/tok 2.7586 (3.2882)	LR 2.000e-03
0: TRAIN [2][220/1938]	Time 0.165 (0.272)	Data 1.50e-04 (3.28e-03)	Tok/s 31890 (54855)	Loss/tok 2.6253 (3.2866)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.285 (0.272)	Data 1.37e-04 (3.14e-03)	Tok/s 59371 (54713)	Loss/tok 3.2422 (3.2882)	LR 2.000e-03
0: TRAIN [2][240/1938]	Time 0.221 (0.269)	Data 1.41e-04 (3.02e-03)	Tok/s 46698 (54275)	Loss/tok 3.0278 (3.2812)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.218 (0.268)	Data 1.25e-04 (2.90e-03)	Tok/s 48031 (54269)	Loss/tok 2.9938 (3.2780)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.280 (0.269)	Data 1.54e-04 (2.80e-03)	Tok/s 59943 (54328)	Loss/tok 3.2418 (3.2793)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.279 (0.269)	Data 1.58e-04 (2.70e-03)	Tok/s 60135 (54294)	Loss/tok 3.2285 (3.2827)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.224 (0.268)	Data 1.24e-04 (2.61e-03)	Tok/s 45665 (54131)	Loss/tok 3.0766 (3.2795)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.283 (0.268)	Data 1.24e-04 (2.52e-03)	Tok/s 59131 (54141)	Loss/tok 3.2493 (3.2791)	LR 2.000e-03
0: TRAIN [2][300/1938]	Time 0.284 (0.268)	Data 1.25e-04 (2.44e-03)	Tok/s 59544 (54216)	Loss/tok 3.1638 (3.2784)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.340 (0.268)	Data 1.26e-04 (2.37e-03)	Tok/s 68068 (54246)	Loss/tok 3.5332 (3.2803)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.344 (0.268)	Data 2.51e-04 (2.30e-03)	Tok/s 67779 (54269)	Loss/tok 3.4174 (3.2819)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.221 (0.267)	Data 1.27e-04 (2.23e-03)	Tok/s 46713 (54116)	Loss/tok 3.1332 (3.2807)	LR 2.000e-03
0: TRAIN [2][340/1938]	Time 0.283 (0.267)	Data 1.27e-04 (2.17e-03)	Tok/s 59080 (54112)	Loss/tok 3.2473 (3.2791)	LR 2.000e-03
0: TRAIN [2][350/1938]	Time 0.221 (0.267)	Data 1.56e-04 (2.11e-03)	Tok/s 47797 (54210)	Loss/tok 3.0352 (3.2791)	LR 2.000e-03
0: TRAIN [2][360/1938]	Time 0.279 (0.268)	Data 1.38e-04 (2.06e-03)	Tok/s 60597 (54259)	Loss/tok 3.2636 (3.2815)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.416 (0.268)	Data 1.31e-04 (2.01e-03)	Tok/s 71504 (54258)	Loss/tok 3.6792 (3.2819)	LR 2.000e-03
0: TRAIN [2][380/1938]	Time 0.281 (0.268)	Data 1.10e-04 (1.96e-03)	Tok/s 60164 (54341)	Loss/tok 3.1208 (3.2835)	LR 2.000e-03
0: TRAIN [2][390/1938]	Time 0.221 (0.268)	Data 1.34e-04 (1.91e-03)	Tok/s 45630 (54284)	Loss/tok 3.1679 (3.2830)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.223 (0.267)	Data 1.45e-04 (1.87e-03)	Tok/s 46205 (54237)	Loss/tok 3.1260 (3.2807)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.168 (0.266)	Data 1.26e-04 (1.82e-03)	Tok/s 32095 (54111)	Loss/tok 2.7637 (3.2777)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.342 (0.266)	Data 1.15e-04 (1.78e-03)	Tok/s 68181 (54079)	Loss/tok 3.4427 (3.2777)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.283 (0.266)	Data 1.25e-04 (1.75e-03)	Tok/s 59306 (54107)	Loss/tok 3.2145 (3.2785)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.282 (0.267)	Data 1.08e-04 (1.71e-03)	Tok/s 59882 (54242)	Loss/tok 3.3557 (3.2827)	LR 2.000e-03
0: TRAIN [2][450/1938]	Time 0.279 (0.267)	Data 1.31e-04 (1.67e-03)	Tok/s 59840 (54292)	Loss/tok 3.3622 (3.2819)	LR 2.000e-03
0: TRAIN [2][460/1938]	Time 0.217 (0.267)	Data 1.33e-04 (1.64e-03)	Tok/s 46709 (54177)	Loss/tok 3.1508 (3.2809)	LR 2.000e-03
0: TRAIN [2][470/1938]	Time 0.219 (0.266)	Data 1.21e-04 (1.61e-03)	Tok/s 46984 (54078)	Loss/tok 3.1690 (3.2785)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][480/1938]	Time 0.224 (0.266)	Data 1.18e-04 (1.58e-03)	Tok/s 45333 (54112)	Loss/tok 2.9496 (3.2780)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.344 (0.266)	Data 1.42e-04 (1.55e-03)	Tok/s 68286 (54109)	Loss/tok 3.5031 (3.2779)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.221 (0.266)	Data 1.21e-04 (1.52e-03)	Tok/s 46870 (54079)	Loss/tok 3.0351 (3.2763)	LR 2.000e-03
0: TRAIN [2][510/1938]	Time 0.222 (0.266)	Data 1.24e-04 (1.49e-03)	Tok/s 45158 (54156)	Loss/tok 3.1318 (3.2781)	LR 2.000e-03
0: TRAIN [2][520/1938]	Time 0.282 (0.266)	Data 1.49e-04 (1.47e-03)	Tok/s 60093 (54121)	Loss/tok 3.3026 (3.2764)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.222 (0.266)	Data 1.23e-04 (1.44e-03)	Tok/s 47097 (54184)	Loss/tok 3.0419 (3.2768)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.220 (0.266)	Data 1.40e-04 (1.42e-03)	Tok/s 47176 (54185)	Loss/tok 3.0931 (3.2766)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.218 (0.266)	Data 1.22e-04 (1.39e-03)	Tok/s 47676 (54190)	Loss/tok 2.9959 (3.2774)	LR 2.000e-03
0: TRAIN [2][560/1938]	Time 0.220 (0.266)	Data 1.50e-04 (1.37e-03)	Tok/s 47115 (54111)	Loss/tok 3.0776 (3.2759)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.285 (0.266)	Data 1.14e-04 (1.35e-03)	Tok/s 58151 (54107)	Loss/tok 3.2257 (3.2755)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.222 (0.265)	Data 1.51e-04 (1.33e-03)	Tok/s 46183 (54079)	Loss/tok 3.1255 (3.2760)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.219 (0.265)	Data 1.20e-04 (1.31e-03)	Tok/s 47168 (53984)	Loss/tok 3.1858 (3.2736)	LR 2.000e-03
0: TRAIN [2][600/1938]	Time 0.222 (0.264)	Data 1.24e-04 (1.29e-03)	Tok/s 46576 (53899)	Loss/tok 3.0009 (3.2718)	LR 2.000e-03
0: TRAIN [2][610/1938]	Time 0.166 (0.264)	Data 1.46e-04 (1.27e-03)	Tok/s 32293 (53902)	Loss/tok 2.6791 (3.2727)	LR 2.000e-03
0: TRAIN [2][620/1938]	Time 0.278 (0.264)	Data 1.63e-04 (1.25e-03)	Tok/s 59941 (53884)	Loss/tok 3.2151 (3.2711)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.342 (0.265)	Data 1.19e-04 (1.23e-03)	Tok/s 68098 (53977)	Loss/tok 3.3104 (3.2723)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.166 (0.264)	Data 1.68e-04 (1.22e-03)	Tok/s 32032 (53896)	Loss/tok 2.5935 (3.2715)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][650/1938]	Time 0.340 (0.264)	Data 1.14e-04 (1.20e-03)	Tok/s 68273 (53895)	Loss/tok 3.3322 (3.2712)	LR 2.000e-03
0: TRAIN [2][660/1938]	Time 0.344 (0.264)	Data 1.49e-04 (1.18e-03)	Tok/s 67402 (53899)	Loss/tok 3.4493 (3.2722)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.220 (0.264)	Data 1.25e-04 (1.17e-03)	Tok/s 46157 (53861)	Loss/tok 3.0189 (3.2710)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.220 (0.264)	Data 1.41e-04 (1.15e-03)	Tok/s 46673 (53832)	Loss/tok 3.0922 (3.2708)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.280 (0.264)	Data 1.44e-04 (1.14e-03)	Tok/s 59663 (53839)	Loss/tok 3.2979 (3.2711)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.284 (0.264)	Data 1.39e-04 (1.12e-03)	Tok/s 59397 (53818)	Loss/tok 3.2381 (3.2703)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.282 (0.264)	Data 1.48e-04 (1.11e-03)	Tok/s 59858 (53823)	Loss/tok 3.2432 (3.2697)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.283 (0.264)	Data 1.16e-04 (1.10e-03)	Tok/s 59116 (53850)	Loss/tok 3.2258 (3.2701)	LR 2.000e-03
0: TRAIN [2][730/1938]	Time 0.282 (0.264)	Data 1.28e-04 (1.08e-03)	Tok/s 59874 (53916)	Loss/tok 3.2951 (3.2714)	LR 2.000e-03
0: TRAIN [2][740/1938]	Time 0.220 (0.264)	Data 1.29e-04 (1.07e-03)	Tok/s 47164 (53906)	Loss/tok 3.0842 (3.2710)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][750/1938]	Time 0.341 (0.264)	Data 1.19e-04 (1.06e-03)	Tok/s 68822 (53920)	Loss/tok 3.4436 (3.2716)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][760/1938]	Time 0.343 (0.264)	Data 1.23e-04 (1.05e-03)	Tok/s 67350 (53990)	Loss/tok 3.5146 (3.2734)	LR 2.000e-03
0: TRAIN [2][770/1938]	Time 0.339 (0.264)	Data 1.30e-04 (1.03e-03)	Tok/s 69068 (53924)	Loss/tok 3.4455 (3.2720)	LR 2.000e-03
0: TRAIN [2][780/1938]	Time 0.221 (0.263)	Data 1.33e-04 (1.02e-03)	Tok/s 46941 (53834)	Loss/tok 3.2190 (3.2707)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.165 (0.263)	Data 1.15e-04 (1.01e-03)	Tok/s 31050 (53792)	Loss/tok 2.5309 (3.2696)	LR 2.000e-03
0: TRAIN [2][800/1938]	Time 0.222 (0.263)	Data 1.44e-04 (1.00e-03)	Tok/s 47734 (53823)	Loss/tok 3.0191 (3.2694)	LR 2.000e-03
0: TRAIN [2][810/1938]	Time 0.280 (0.263)	Data 1.22e-04 (9.89e-04)	Tok/s 60592 (53767)	Loss/tok 3.3029 (3.2684)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.280 (0.263)	Data 1.45e-04 (9.78e-04)	Tok/s 59899 (53794)	Loss/tok 3.3068 (3.2703)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.219 (0.264)	Data 1.27e-04 (9.68e-04)	Tok/s 48185 (53854)	Loss/tok 2.9836 (3.2710)	LR 2.000e-03
0: TRAIN [2][840/1938]	Time 0.280 (0.263)	Data 1.34e-04 (9.58e-04)	Tok/s 59725 (53847)	Loss/tok 3.2158 (3.2709)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.342 (0.264)	Data 1.37e-04 (9.49e-04)	Tok/s 67858 (53858)	Loss/tok 3.5173 (3.2715)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.220 (0.263)	Data 1.22e-04 (9.39e-04)	Tok/s 47345 (53821)	Loss/tok 3.0618 (3.2708)	LR 2.000e-03
0: TRAIN [2][870/1938]	Time 0.285 (0.263)	Data 1.20e-04 (9.30e-04)	Tok/s 59627 (53838)	Loss/tok 3.2592 (3.2705)	LR 2.000e-03
0: TRAIN [2][880/1938]	Time 0.167 (0.263)	Data 1.26e-04 (9.21e-04)	Tok/s 31165 (53807)	Loss/tok 2.6666 (3.2697)	LR 2.000e-03
0: TRAIN [2][890/1938]	Time 0.344 (0.263)	Data 1.38e-04 (9.12e-04)	Tok/s 68454 (53808)	Loss/tok 3.3834 (3.2700)	LR 2.000e-03
0: TRAIN [2][900/1938]	Time 0.343 (0.263)	Data 1.23e-04 (9.03e-04)	Tok/s 68585 (53817)	Loss/tok 3.3778 (3.2694)	LR 2.000e-03
0: TRAIN [2][910/1938]	Time 0.218 (0.263)	Data 1.27e-04 (8.94e-04)	Tok/s 47353 (53805)	Loss/tok 3.0304 (3.2692)	LR 2.000e-03
0: TRAIN [2][920/1938]	Time 0.222 (0.263)	Data 1.11e-04 (8.86e-04)	Tok/s 46727 (53846)	Loss/tok 3.0238 (3.2704)	LR 2.000e-03
0: TRAIN [2][930/1938]	Time 0.222 (0.263)	Data 1.32e-04 (8.78e-04)	Tok/s 46497 (53828)	Loss/tok 3.0347 (3.2700)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.279 (0.263)	Data 1.38e-04 (8.70e-04)	Tok/s 60504 (53821)	Loss/tok 3.2503 (3.2703)	LR 2.000e-03
0: TRAIN [2][950/1938]	Time 0.167 (0.263)	Data 1.24e-04 (8.62e-04)	Tok/s 31846 (53804)	Loss/tok 2.5908 (3.2699)	LR 2.000e-03
0: TRAIN [2][960/1938]	Time 0.281 (0.264)	Data 1.35e-04 (8.55e-04)	Tok/s 59848 (53884)	Loss/tok 3.2709 (3.2712)	LR 2.000e-03
0: TRAIN [2][970/1938]	Time 0.220 (0.263)	Data 1.24e-04 (8.47e-04)	Tok/s 47026 (53872)	Loss/tok 3.1512 (3.2706)	LR 2.000e-03
0: TRAIN [2][980/1938]	Time 0.223 (0.264)	Data 1.24e-04 (8.40e-04)	Tok/s 46594 (53877)	Loss/tok 3.1467 (3.2712)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.283 (0.264)	Data 1.61e-04 (8.33e-04)	Tok/s 58814 (53923)	Loss/tok 3.3672 (3.2723)	LR 2.000e-03
0: TRAIN [2][1000/1938]	Time 0.168 (0.264)	Data 1.51e-04 (8.26e-04)	Tok/s 31090 (53949)	Loss/tok 2.5953 (3.2731)	LR 2.000e-03
0: TRAIN [2][1010/1938]	Time 0.286 (0.264)	Data 1.05e-04 (8.19e-04)	Tok/s 58880 (53962)	Loss/tok 3.2594 (3.2743)	LR 2.000e-03
0: TRAIN [2][1020/1938]	Time 0.284 (0.265)	Data 1.21e-04 (8.12e-04)	Tok/s 59074 (54017)	Loss/tok 3.3377 (3.2750)	LR 2.000e-03
0: TRAIN [2][1030/1938]	Time 0.220 (0.265)	Data 1.45e-04 (8.05e-04)	Tok/s 45978 (54042)	Loss/tok 3.1178 (3.2759)	LR 2.000e-03
0: TRAIN [2][1040/1938]	Time 0.284 (0.264)	Data 1.10e-04 (7.99e-04)	Tok/s 59900 (54011)	Loss/tok 3.2464 (3.2747)	LR 2.000e-03
0: TRAIN [2][1050/1938]	Time 0.221 (0.264)	Data 1.20e-04 (7.92e-04)	Tok/s 47284 (53982)	Loss/tok 3.0724 (3.2742)	LR 2.000e-03
0: TRAIN [2][1060/1938]	Time 0.221 (0.264)	Data 1.20e-04 (7.86e-04)	Tok/s 46879 (53967)	Loss/tok 3.0058 (3.2735)	LR 2.000e-03
0: TRAIN [2][1070/1938]	Time 0.279 (0.264)	Data 1.46e-04 (7.80e-04)	Tok/s 60426 (54002)	Loss/tok 3.2518 (3.2739)	LR 2.000e-03
0: TRAIN [2][1080/1938]	Time 0.217 (0.264)	Data 1.15e-04 (7.74e-04)	Tok/s 47457 (53953)	Loss/tok 3.1687 (3.2738)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1090/1938]	Time 0.283 (0.264)	Data 1.24e-04 (7.68e-04)	Tok/s 59415 (53918)	Loss/tok 3.2760 (3.2736)	LR 2.000e-03
0: TRAIN [2][1100/1938]	Time 0.282 (0.264)	Data 1.02e-04 (7.62e-04)	Tok/s 60127 (53931)	Loss/tok 3.2542 (3.2732)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.415 (0.264)	Data 1.22e-04 (7.56e-04)	Tok/s 71655 (53980)	Loss/tok 3.5930 (3.2738)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.282 (0.265)	Data 1.24e-04 (7.51e-04)	Tok/s 58889 (54033)	Loss/tok 3.2994 (3.2748)	LR 2.000e-03
0: TRAIN [2][1130/1938]	Time 0.281 (0.264)	Data 1.23e-04 (7.45e-04)	Tok/s 60454 (54010)	Loss/tok 3.1806 (3.2739)	LR 2.000e-03
0: TRAIN [2][1140/1938]	Time 0.219 (0.264)	Data 1.39e-04 (7.40e-04)	Tok/s 47011 (54010)	Loss/tok 3.0889 (3.2737)	LR 2.000e-03
0: TRAIN [2][1150/1938]	Time 0.281 (0.264)	Data 1.29e-04 (7.34e-04)	Tok/s 59166 (53996)	Loss/tok 3.2102 (3.2731)	LR 2.000e-03
0: TRAIN [2][1160/1938]	Time 0.278 (0.265)	Data 1.18e-04 (7.29e-04)	Tok/s 61243 (54034)	Loss/tok 3.2287 (3.2743)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.283 (0.265)	Data 1.37e-04 (7.24e-04)	Tok/s 59396 (54041)	Loss/tok 3.1870 (3.2740)	LR 2.000e-03
0: TRAIN [2][1180/1938]	Time 0.416 (0.265)	Data 1.31e-04 (7.19e-04)	Tok/s 71483 (54039)	Loss/tok 3.5694 (3.2745)	LR 2.000e-03
0: TRAIN [2][1190/1938]	Time 0.165 (0.265)	Data 1.33e-04 (7.14e-04)	Tok/s 31533 (54041)	Loss/tok 2.8290 (3.2742)	LR 2.000e-03
0: TRAIN [2][1200/1938]	Time 0.220 (0.264)	Data 1.43e-04 (7.09e-04)	Tok/s 47615 (53937)	Loss/tok 2.9243 (3.2727)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.220 (0.264)	Data 1.14e-04 (7.04e-04)	Tok/s 45717 (53928)	Loss/tok 3.0949 (3.2726)	LR 2.000e-03
0: TRAIN [2][1220/1938]	Time 0.282 (0.264)	Data 1.20e-04 (7.00e-04)	Tok/s 60690 (53977)	Loss/tok 3.3801 (3.2728)	LR 2.000e-03
0: TRAIN [2][1230/1938]	Time 0.282 (0.265)	Data 1.33e-04 (6.95e-04)	Tok/s 59673 (54031)	Loss/tok 3.3091 (3.2734)	LR 2.000e-03
0: TRAIN [2][1240/1938]	Time 0.220 (0.264)	Data 1.15e-04 (6.91e-04)	Tok/s 46900 (54010)	Loss/tok 3.0005 (3.2727)	LR 2.000e-03
0: TRAIN [2][1250/1938]	Time 0.283 (0.264)	Data 1.50e-04 (6.86e-04)	Tok/s 60178 (54025)	Loss/tok 3.2870 (3.2721)	LR 2.000e-03
0: TRAIN [2][1260/1938]	Time 0.221 (0.264)	Data 1.41e-04 (6.81e-04)	Tok/s 45885 (54002)	Loss/tok 2.9694 (3.2713)	LR 2.000e-03
0: TRAIN [2][1270/1938]	Time 0.279 (0.264)	Data 1.18e-04 (6.77e-04)	Tok/s 60216 (53983)	Loss/tok 3.2727 (3.2706)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.344 (0.264)	Data 1.05e-04 (6.73e-04)	Tok/s 68067 (53999)	Loss/tok 3.3903 (3.2710)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1290/1938]	Time 0.282 (0.264)	Data 1.18e-04 (6.69e-04)	Tok/s 59617 (53978)	Loss/tok 3.1709 (3.2714)	LR 2.000e-03
0: TRAIN [2][1300/1938]	Time 0.340 (0.264)	Data 1.31e-04 (6.64e-04)	Tok/s 69130 (53936)	Loss/tok 3.4760 (3.2707)	LR 2.000e-03
0: TRAIN [2][1310/1938]	Time 0.220 (0.264)	Data 1.09e-04 (6.60e-04)	Tok/s 46545 (53925)	Loss/tok 3.0364 (3.2705)	LR 2.000e-03
0: TRAIN [2][1320/1938]	Time 0.279 (0.264)	Data 1.15e-04 (6.56e-04)	Tok/s 61169 (53944)	Loss/tok 3.1966 (3.2703)	LR 2.000e-03
0: TRAIN [2][1330/1938]	Time 0.219 (0.264)	Data 1.13e-04 (6.52e-04)	Tok/s 47265 (53953)	Loss/tok 3.0157 (3.2700)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.222 (0.264)	Data 1.24e-04 (6.48e-04)	Tok/s 46528 (53921)	Loss/tok 3.2126 (3.2694)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.221 (0.264)	Data 1.38e-04 (6.45e-04)	Tok/s 47266 (53932)	Loss/tok 3.0222 (3.2698)	LR 2.000e-03
0: TRAIN [2][1360/1938]	Time 0.164 (0.264)	Data 1.16e-04 (6.41e-04)	Tok/s 32339 (53935)	Loss/tok 2.6585 (3.2694)	LR 2.000e-03
0: TRAIN [2][1370/1938]	Time 0.219 (0.264)	Data 1.14e-04 (6.37e-04)	Tok/s 46874 (53938)	Loss/tok 3.1173 (3.2692)	LR 2.000e-03
0: TRAIN [2][1380/1938]	Time 0.277 (0.264)	Data 1.14e-04 (6.33e-04)	Tok/s 61299 (53934)	Loss/tok 3.2572 (3.2686)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.278 (0.263)	Data 1.23e-04 (6.30e-04)	Tok/s 60666 (53889)	Loss/tok 3.1529 (3.2677)	LR 2.000e-03
0: TRAIN [2][1400/1938]	Time 0.283 (0.263)	Data 1.41e-04 (6.26e-04)	Tok/s 59820 (53887)	Loss/tok 3.2361 (3.2676)	LR 2.000e-03
0: TRAIN [2][1410/1938]	Time 0.222 (0.263)	Data 1.24e-04 (6.23e-04)	Tok/s 47072 (53884)	Loss/tok 3.0256 (3.2671)	LR 2.000e-03
0: TRAIN [2][1420/1938]	Time 0.168 (0.263)	Data 1.47e-04 (6.19e-04)	Tok/s 30820 (53878)	Loss/tok 2.6671 (3.2669)	LR 2.000e-03
0: TRAIN [2][1430/1938]	Time 0.280 (0.263)	Data 1.24e-04 (6.16e-04)	Tok/s 60258 (53840)	Loss/tok 3.2609 (3.2662)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.220 (0.263)	Data 1.30e-04 (6.13e-04)	Tok/s 46951 (53851)	Loss/tok 3.0298 (3.2660)	LR 2.000e-03
0: TRAIN [2][1450/1938]	Time 0.221 (0.263)	Data 1.29e-04 (6.09e-04)	Tok/s 46312 (53792)	Loss/tok 3.0666 (3.2649)	LR 2.000e-03
0: TRAIN [2][1460/1938]	Time 0.280 (0.263)	Data 1.22e-04 (6.06e-04)	Tok/s 59857 (53784)	Loss/tok 3.2054 (3.2644)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.281 (0.262)	Data 1.16e-04 (6.03e-04)	Tok/s 60041 (53743)	Loss/tok 3.3200 (3.2636)	LR 2.000e-03
0: TRAIN [2][1480/1938]	Time 0.220 (0.262)	Data 1.32e-04 (6.00e-04)	Tok/s 46497 (53729)	Loss/tok 3.0628 (3.2638)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.417 (0.263)	Data 1.43e-04 (5.96e-04)	Tok/s 72273 (53754)	Loss/tok 3.4282 (3.2641)	LR 2.000e-03
0: TRAIN [2][1500/1938]	Time 0.219 (0.262)	Data 1.30e-04 (5.93e-04)	Tok/s 47874 (53717)	Loss/tok 3.0993 (3.2631)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1510/1938]	Time 0.283 (0.262)	Data 1.21e-04 (5.90e-04)	Tok/s 59284 (53722)	Loss/tok 3.3038 (3.2632)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.284 (0.263)	Data 1.29e-04 (5.87e-04)	Tok/s 59283 (53773)	Loss/tok 3.2587 (3.2639)	LR 2.000e-03
0: TRAIN [2][1530/1938]	Time 0.342 (0.263)	Data 1.32e-04 (5.84e-04)	Tok/s 67884 (53780)	Loss/tok 3.3666 (3.2636)	LR 2.000e-03
0: TRAIN [2][1540/1938]	Time 0.219 (0.263)	Data 1.33e-04 (5.81e-04)	Tok/s 47189 (53759)	Loss/tok 3.0087 (3.2633)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.165 (0.262)	Data 1.24e-04 (5.78e-04)	Tok/s 31638 (53711)	Loss/tok 2.6686 (3.2625)	LR 2.000e-03
0: TRAIN [2][1560/1938]	Time 0.282 (0.262)	Data 1.20e-04 (5.75e-04)	Tok/s 59547 (53707)	Loss/tok 3.2820 (3.2623)	LR 2.000e-03
0: TRAIN [2][1570/1938]	Time 0.281 (0.262)	Data 1.18e-04 (5.72e-04)	Tok/s 59432 (53712)	Loss/tok 3.2785 (3.2623)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.220 (0.262)	Data 1.29e-04 (5.70e-04)	Tok/s 47533 (53697)	Loss/tok 2.9320 (3.2617)	LR 2.000e-03
0: TRAIN [2][1590/1938]	Time 0.219 (0.262)	Data 1.24e-04 (5.67e-04)	Tok/s 47517 (53709)	Loss/tok 2.9918 (3.2615)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.221 (0.262)	Data 1.22e-04 (5.64e-04)	Tok/s 45989 (53684)	Loss/tok 2.9351 (3.2609)	LR 2.000e-03
0: TRAIN [2][1610/1938]	Time 0.281 (0.262)	Data 1.41e-04 (5.61e-04)	Tok/s 59136 (53682)	Loss/tok 3.2278 (3.2607)	LR 2.000e-03
0: TRAIN [2][1620/1938]	Time 0.343 (0.262)	Data 1.20e-04 (5.59e-04)	Tok/s 67525 (53694)	Loss/tok 3.4819 (3.2611)	LR 2.000e-03
0: TRAIN [2][1630/1938]	Time 0.221 (0.262)	Data 1.30e-04 (5.56e-04)	Tok/s 46994 (53691)	Loss/tok 2.9440 (3.2613)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1640/1938]	Time 0.413 (0.262)	Data 1.14e-04 (5.53e-04)	Tok/s 72007 (53710)	Loss/tok 3.7287 (3.2624)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1650/1938]	Time 0.283 (0.262)	Data 1.33e-04 (5.51e-04)	Tok/s 59128 (53713)	Loss/tok 3.2809 (3.2624)	LR 2.000e-03
0: TRAIN [2][1660/1938]	Time 0.281 (0.262)	Data 1.33e-04 (5.48e-04)	Tok/s 59428 (53685)	Loss/tok 3.2334 (3.2622)	LR 2.000e-03
0: TRAIN [2][1670/1938]	Time 0.416 (0.262)	Data 1.16e-04 (5.46e-04)	Tok/s 71680 (53713)	Loss/tok 3.6637 (3.2631)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.341 (0.263)	Data 1.20e-04 (5.43e-04)	Tok/s 68747 (53722)	Loss/tok 3.4821 (3.2632)	LR 2.000e-03
0: TRAIN [2][1690/1938]	Time 0.222 (0.262)	Data 1.08e-04 (5.41e-04)	Tok/s 46521 (53695)	Loss/tok 3.0554 (3.2630)	LR 2.000e-03
0: TRAIN [2][1700/1938]	Time 0.282 (0.262)	Data 1.27e-04 (5.38e-04)	Tok/s 59599 (53683)	Loss/tok 3.1354 (3.2626)	LR 2.000e-03
0: TRAIN [2][1710/1938]	Time 0.280 (0.262)	Data 1.03e-04 (5.36e-04)	Tok/s 60260 (53668)	Loss/tok 3.2992 (3.2625)	LR 2.000e-03
0: TRAIN [2][1720/1938]	Time 0.281 (0.262)	Data 1.05e-04 (5.33e-04)	Tok/s 60655 (53692)	Loss/tok 3.2636 (3.2631)	LR 2.000e-03
0: TRAIN [2][1730/1938]	Time 0.222 (0.262)	Data 9.54e-05 (5.31e-04)	Tok/s 46558 (53681)	Loss/tok 3.1440 (3.2626)	LR 2.000e-03
0: TRAIN [2][1740/1938]	Time 0.221 (0.262)	Data 1.13e-04 (5.28e-04)	Tok/s 45909 (53667)	Loss/tok 3.0337 (3.2622)	LR 2.000e-03
0: TRAIN [2][1750/1938]	Time 0.219 (0.262)	Data 1.16e-04 (5.26e-04)	Tok/s 47612 (53654)	Loss/tok 2.9564 (3.2617)	LR 2.000e-03
0: TRAIN [2][1760/1938]	Time 0.217 (0.262)	Data 1.38e-04 (5.24e-04)	Tok/s 47699 (53648)	Loss/tok 3.0730 (3.2614)	LR 2.000e-03
0: TRAIN [2][1770/1938]	Time 0.218 (0.262)	Data 1.27e-04 (5.21e-04)	Tok/s 46699 (53608)	Loss/tok 3.0830 (3.2607)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.341 (0.262)	Data 1.15e-04 (5.19e-04)	Tok/s 68484 (53610)	Loss/tok 3.4247 (3.2605)	LR 2.000e-03
0: TRAIN [2][1790/1938]	Time 0.218 (0.262)	Data 1.09e-04 (5.17e-04)	Tok/s 46083 (53606)	Loss/tok 3.0849 (3.2604)	LR 2.000e-03
0: TRAIN [2][1800/1938]	Time 0.344 (0.262)	Data 1.13e-04 (5.15e-04)	Tok/s 68628 (53605)	Loss/tok 3.2793 (3.2603)	LR 2.000e-03
0: TRAIN [2][1810/1938]	Time 0.217 (0.262)	Data 1.34e-04 (5.12e-04)	Tok/s 48627 (53565)	Loss/tok 3.0598 (3.2594)	LR 2.000e-03
0: TRAIN [2][1820/1938]	Time 0.221 (0.261)	Data 1.23e-04 (5.10e-04)	Tok/s 47334 (53556)	Loss/tok 3.0205 (3.2589)	LR 2.000e-03
0: TRAIN [2][1830/1938]	Time 0.414 (0.262)	Data 1.27e-04 (5.08e-04)	Tok/s 72723 (53579)	Loss/tok 3.5019 (3.2594)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1840/1938]	Time 0.221 (0.262)	Data 1.15e-04 (5.06e-04)	Tok/s 45941 (53566)	Loss/tok 3.0048 (3.2591)	LR 2.000e-03
0: TRAIN [2][1850/1938]	Time 0.216 (0.261)	Data 1.07e-04 (5.04e-04)	Tok/s 47991 (53536)	Loss/tok 3.0098 (3.2585)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.280 (0.261)	Data 1.08e-04 (5.02e-04)	Tok/s 59111 (53549)	Loss/tok 3.2632 (3.2583)	LR 2.000e-03
0: TRAIN [2][1870/1938]	Time 0.283 (0.261)	Data 1.09e-04 (5.00e-04)	Tok/s 58381 (53529)	Loss/tok 3.2911 (3.2577)	LR 2.000e-03
0: TRAIN [2][1880/1938]	Time 0.340 (0.261)	Data 1.01e-04 (4.98e-04)	Tok/s 68039 (53504)	Loss/tok 3.4049 (3.2572)	LR 2.000e-03
0: TRAIN [2][1890/1938]	Time 0.281 (0.261)	Data 1.14e-04 (4.96e-04)	Tok/s 58710 (53483)	Loss/tok 3.3505 (3.2568)	LR 2.000e-03
0: TRAIN [2][1900/1938]	Time 0.216 (0.261)	Data 1.14e-04 (4.94e-04)	Tok/s 47733 (53478)	Loss/tok 3.0369 (3.2571)	LR 2.000e-03
0: TRAIN [2][1910/1938]	Time 0.343 (0.261)	Data 1.22e-04 (4.92e-04)	Tok/s 67668 (53493)	Loss/tok 3.4320 (3.2574)	LR 2.000e-03
0: TRAIN [2][1920/1938]	Time 0.221 (0.261)	Data 9.78e-05 (4.90e-04)	Tok/s 46831 (53490)	Loss/tok 2.9762 (3.2575)	LR 2.000e-03
0: TRAIN [2][1930/1938]	Time 0.220 (0.261)	Data 1.16e-04 (4.88e-04)	Tok/s 46152 (53484)	Loss/tok 3.0584 (3.2572)	LR 2.000e-03
:::MLL 1582042442.235 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 525}}
:::MLL 1582042442.235 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.684 (0.684)	Decoder iters 136.0 (136.0)	Tok/s 23985 (23985)
0: Running moses detokenizer
0: BLEU(score=23.04945631775902, counts=[36489, 17930, 10004, 5832], totals=[65238, 62235, 59232, 56234], precisions=[55.93212544835832, 28.81015505744356, 16.88951917882226, 10.370949959099477], bp=1.0, sys_len=65238, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1582042444.101 eval_accuracy: {"value": 23.05, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 536}}
:::MLL 1582042444.102 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 2	Training Loss: 3.2589	Test BLEU: 23.05
0: Performance: Epoch: 2	Training: 428144 Tok/s
0: Finished epoch 2
:::MLL 1582042444.102 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 558}}
:::MLL 1582042444.103 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1582042444.103 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 515}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 1954871563
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][0/1938]	Time 1.051 (1.051)	Data 7.14e-01 (7.14e-01)	Tok/s 22458 (22458)	Loss/tok 3.3255 (3.3255)	LR 2.000e-03
0: TRAIN [3][10/1938]	Time 0.222 (0.360)	Data 2.02e-04 (6.50e-02)	Tok/s 46708 (53305)	Loss/tok 3.0741 (3.2873)	LR 2.000e-03
0: TRAIN [3][20/1938]	Time 0.417 (0.330)	Data 9.97e-05 (3.41e-02)	Tok/s 72073 (56077)	Loss/tok 3.5748 (3.2856)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.281 (0.301)	Data 1.12e-04 (2.31e-02)	Tok/s 59677 (54209)	Loss/tok 3.1312 (3.2343)	LR 2.000e-03
0: TRAIN [3][40/1938]	Time 0.222 (0.291)	Data 1.24e-04 (1.75e-02)	Tok/s 46535 (54024)	Loss/tok 3.0263 (3.2140)	LR 2.000e-03
0: TRAIN [3][50/1938]	Time 0.219 (0.279)	Data 1.43e-04 (1.41e-02)	Tok/s 46514 (53036)	Loss/tok 2.9084 (3.1867)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.284 (0.278)	Data 1.32e-04 (1.18e-02)	Tok/s 58665 (53493)	Loss/tok 3.1765 (3.1895)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.281 (0.280)	Data 1.18e-04 (1.02e-02)	Tok/s 59476 (54382)	Loss/tok 3.0565 (3.2053)	LR 2.000e-03
0: TRAIN [3][80/1938]	Time 0.342 (0.277)	Data 1.26e-04 (8.94e-03)	Tok/s 69220 (54214)	Loss/tok 3.2882 (3.1913)	LR 2.000e-03
0: TRAIN [3][90/1938]	Time 0.280 (0.273)	Data 1.26e-04 (7.97e-03)	Tok/s 59970 (53904)	Loss/tok 3.1317 (3.1799)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.413 (0.273)	Data 1.17e-04 (7.20e-03)	Tok/s 72272 (54106)	Loss/tok 3.5218 (3.1855)	LR 2.000e-03
0: TRAIN [3][110/1938]	Time 0.414 (0.273)	Data 1.32e-04 (6.56e-03)	Tok/s 71369 (54101)	Loss/tok 3.5814 (3.1867)	LR 2.000e-03
0: TRAIN [3][120/1938]	Time 0.339 (0.270)	Data 1.26e-04 (6.03e-03)	Tok/s 70007 (53904)	Loss/tok 3.2966 (3.1807)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.221 (0.269)	Data 1.22e-04 (5.58e-03)	Tok/s 46902 (53984)	Loss/tok 2.9466 (3.1792)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.415 (0.270)	Data 1.22e-04 (5.19e-03)	Tok/s 71522 (54112)	Loss/tok 3.6000 (3.1866)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.218 (0.271)	Data 1.36e-04 (4.86e-03)	Tok/s 46989 (54026)	Loss/tok 2.9376 (3.1956)	LR 2.000e-03
0: TRAIN [3][160/1938]	Time 0.344 (0.270)	Data 1.27e-04 (4.56e-03)	Tok/s 68043 (54150)	Loss/tok 3.4400 (3.1953)	LR 2.000e-03
0: TRAIN [3][170/1938]	Time 0.221 (0.270)	Data 1.42e-04 (4.30e-03)	Tok/s 47263 (54073)	Loss/tok 2.9742 (3.1948)	LR 2.000e-03
0: TRAIN [3][180/1938]	Time 0.221 (0.269)	Data 1.10e-04 (4.07e-03)	Tok/s 46299 (54059)	Loss/tok 2.9882 (3.1921)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.221 (0.269)	Data 1.35e-04 (3.87e-03)	Tok/s 46812 (54011)	Loss/tok 2.9565 (3.1948)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.220 (0.269)	Data 1.31e-04 (3.68e-03)	Tok/s 47117 (54095)	Loss/tok 3.0180 (3.1942)	LR 2.000e-03
0: TRAIN [3][210/1938]	Time 0.217 (0.266)	Data 1.27e-04 (3.51e-03)	Tok/s 47292 (53675)	Loss/tok 3.0297 (3.1868)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.418 (0.267)	Data 1.59e-04 (3.36e-03)	Tok/s 72068 (53712)	Loss/tok 3.5546 (3.1895)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.221 (0.267)	Data 1.95e-04 (3.22e-03)	Tok/s 47425 (53723)	Loss/tok 3.0118 (3.1887)	LR 2.000e-03
0: TRAIN [3][240/1938]	Time 0.217 (0.266)	Data 1.59e-04 (3.09e-03)	Tok/s 47215 (53632)	Loss/tok 3.0701 (3.1874)	LR 2.000e-03
0: TRAIN [3][250/1938]	Time 0.278 (0.266)	Data 1.52e-04 (2.97e-03)	Tok/s 60466 (53756)	Loss/tok 3.1620 (3.1876)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.283 (0.266)	Data 1.41e-04 (2.87e-03)	Tok/s 58663 (53739)	Loss/tok 3.1386 (3.1862)	LR 2.000e-03
0: TRAIN [3][270/1938]	Time 0.219 (0.266)	Data 1.35e-04 (2.76e-03)	Tok/s 46583 (53862)	Loss/tok 2.9938 (3.1872)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.283 (0.265)	Data 1.47e-04 (2.67e-03)	Tok/s 59174 (53651)	Loss/tok 3.2285 (3.1830)	LR 2.000e-03
0: TRAIN [3][290/1938]	Time 0.220 (0.264)	Data 1.47e-04 (2.59e-03)	Tok/s 47739 (53474)	Loss/tok 2.9026 (3.1799)	LR 2.000e-03
0: TRAIN [3][300/1938]	Time 0.282 (0.263)	Data 1.36e-04 (2.50e-03)	Tok/s 60354 (53364)	Loss/tok 3.1427 (3.1775)	LR 2.000e-03
0: TRAIN [3][310/1938]	Time 0.342 (0.263)	Data 1.24e-04 (2.43e-03)	Tok/s 67645 (53332)	Loss/tok 3.3574 (3.1768)	LR 2.000e-03
0: TRAIN [3][320/1938]	Time 0.217 (0.263)	Data 1.19e-04 (2.36e-03)	Tok/s 48117 (53405)	Loss/tok 2.9607 (3.1798)	LR 2.000e-03
0: TRAIN [3][330/1938]	Time 0.221 (0.263)	Data 1.48e-04 (2.29e-03)	Tok/s 46253 (53364)	Loss/tok 2.9769 (3.1824)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][340/1938]	Time 0.281 (0.264)	Data 1.38e-04 (2.23e-03)	Tok/s 59892 (53486)	Loss/tok 3.1852 (3.1850)	LR 2.000e-03
0: TRAIN [3][350/1938]	Time 0.221 (0.265)	Data 1.49e-04 (2.17e-03)	Tok/s 47825 (53568)	Loss/tok 2.9887 (3.1906)	LR 2.000e-03
0: TRAIN [3][360/1938]	Time 0.165 (0.265)	Data 1.54e-04 (2.11e-03)	Tok/s 31769 (53503)	Loss/tok 2.5775 (3.1928)	LR 2.000e-03
0: TRAIN [3][370/1938]	Time 0.221 (0.265)	Data 1.38e-04 (2.06e-03)	Tok/s 46742 (53485)	Loss/tok 3.0048 (3.1935)	LR 2.000e-03
0: TRAIN [3][380/1938]	Time 0.221 (0.264)	Data 1.35e-04 (2.01e-03)	Tok/s 46593 (53388)	Loss/tok 3.0037 (3.1915)	LR 2.000e-03
0: TRAIN [3][390/1938]	Time 0.282 (0.265)	Data 1.40e-04 (1.96e-03)	Tok/s 58824 (53502)	Loss/tok 3.1791 (3.1950)	LR 2.000e-03
0: TRAIN [3][400/1938]	Time 0.344 (0.265)	Data 1.61e-04 (1.91e-03)	Tok/s 67446 (53531)	Loss/tok 3.4685 (3.1954)	LR 2.000e-03
0: TRAIN [3][410/1938]	Time 0.281 (0.265)	Data 1.38e-04 (1.87e-03)	Tok/s 59672 (53520)	Loss/tok 3.1551 (3.1948)	LR 2.000e-03
0: TRAIN [3][420/1938]	Time 0.221 (0.264)	Data 1.26e-04 (1.83e-03)	Tok/s 47729 (53505)	Loss/tok 2.9621 (3.1948)	LR 2.000e-03
0: TRAIN [3][430/1938]	Time 0.281 (0.265)	Data 1.20e-04 (1.79e-03)	Tok/s 60205 (53579)	Loss/tok 3.1669 (3.1933)	LR 2.000e-03
0: TRAIN [3][440/1938]	Time 0.284 (0.265)	Data 1.29e-04 (1.75e-03)	Tok/s 58824 (53643)	Loss/tok 3.1451 (3.1940)	LR 2.000e-03
0: TRAIN [3][450/1938]	Time 0.417 (0.265)	Data 1.54e-04 (1.72e-03)	Tok/s 71622 (53679)	Loss/tok 3.4857 (3.1954)	LR 2.000e-03
0: TRAIN [3][460/1938]	Time 0.220 (0.265)	Data 1.46e-04 (1.68e-03)	Tok/s 47800 (53649)	Loss/tok 3.0727 (3.1954)	LR 2.000e-03
0: TRAIN [3][470/1938]	Time 0.220 (0.265)	Data 1.28e-04 (1.65e-03)	Tok/s 46039 (53634)	Loss/tok 3.0002 (3.1953)	LR 2.000e-03
0: TRAIN [3][480/1938]	Time 0.219 (0.265)	Data 1.20e-04 (1.62e-03)	Tok/s 47560 (53569)	Loss/tok 2.9794 (3.1948)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][490/1938]	Time 0.219 (0.265)	Data 1.53e-04 (1.59e-03)	Tok/s 47021 (53670)	Loss/tok 3.0749 (3.1975)	LR 2.000e-03
0: TRAIN [3][500/1938]	Time 0.164 (0.264)	Data 1.15e-04 (1.56e-03)	Tok/s 32350 (53518)	Loss/tok 2.5640 (3.1952)	LR 2.000e-03
0: TRAIN [3][510/1938]	Time 0.346 (0.264)	Data 1.19e-04 (1.53e-03)	Tok/s 67870 (53581)	Loss/tok 3.3651 (3.1959)	LR 2.000e-03
0: TRAIN [3][520/1938]	Time 0.282 (0.264)	Data 1.32e-04 (1.50e-03)	Tok/s 60161 (53594)	Loss/tok 3.1335 (3.1959)	LR 2.000e-03
0: TRAIN [3][530/1938]	Time 0.221 (0.264)	Data 1.53e-04 (1.48e-03)	Tok/s 46698 (53544)	Loss/tok 3.0718 (3.1951)	LR 2.000e-03
0: TRAIN [3][540/1938]	Time 0.219 (0.264)	Data 1.20e-04 (1.45e-03)	Tok/s 47281 (53565)	Loss/tok 3.0110 (3.1943)	LR 2.000e-03
0: TRAIN [3][550/1938]	Time 0.166 (0.264)	Data 1.40e-04 (1.43e-03)	Tok/s 31610 (53480)	Loss/tok 2.5863 (3.1939)	LR 2.000e-03
0: TRAIN [3][560/1938]	Time 0.340 (0.264)	Data 1.40e-04 (1.41e-03)	Tok/s 68442 (53474)	Loss/tok 3.3723 (3.1939)	LR 2.000e-03
0: TRAIN [3][570/1938]	Time 0.221 (0.263)	Data 1.15e-04 (1.38e-03)	Tok/s 47515 (53440)	Loss/tok 3.0057 (3.1927)	LR 2.000e-03
0: TRAIN [3][580/1938]	Time 0.221 (0.262)	Data 1.24e-04 (1.36e-03)	Tok/s 47504 (53321)	Loss/tok 2.9346 (3.1907)	LR 2.000e-03
0: TRAIN [3][590/1938]	Time 0.284 (0.263)	Data 1.15e-04 (1.34e-03)	Tok/s 58596 (53387)	Loss/tok 3.2348 (3.1905)	LR 2.000e-03
0: TRAIN [3][600/1938]	Time 0.167 (0.263)	Data 1.24e-04 (1.32e-03)	Tok/s 31293 (53422)	Loss/tok 2.6863 (3.1915)	LR 2.000e-03
0: TRAIN [3][610/1938]	Time 0.220 (0.263)	Data 1.25e-04 (1.30e-03)	Tok/s 46933 (53480)	Loss/tok 3.0107 (3.1927)	LR 2.000e-03
0: TRAIN [3][620/1938]	Time 0.282 (0.263)	Data 1.41e-04 (1.28e-03)	Tok/s 59664 (53403)	Loss/tok 3.1536 (3.1918)	LR 2.000e-03
0: TRAIN [3][630/1938]	Time 0.283 (0.263)	Data 1.61e-04 (1.26e-03)	Tok/s 59002 (53466)	Loss/tok 3.1622 (3.1935)	LR 2.000e-03
0: TRAIN [3][640/1938]	Time 0.221 (0.263)	Data 1.24e-04 (1.25e-03)	Tok/s 46331 (53495)	Loss/tok 3.0826 (3.1943)	LR 2.000e-03
0: TRAIN [3][650/1938]	Time 0.220 (0.263)	Data 1.29e-04 (1.23e-03)	Tok/s 47596 (53464)	Loss/tok 2.9904 (3.1926)	LR 2.000e-03
0: TRAIN [3][660/1938]	Time 0.218 (0.263)	Data 1.14e-04 (1.21e-03)	Tok/s 47615 (53406)	Loss/tok 2.9672 (3.1915)	LR 2.000e-03
0: TRAIN [3][670/1938]	Time 0.219 (0.262)	Data 1.31e-04 (1.20e-03)	Tok/s 47147 (53372)	Loss/tok 2.9277 (3.1903)	LR 1.000e-03
0: TRAIN [3][680/1938]	Time 0.218 (0.262)	Data 1.17e-04 (1.18e-03)	Tok/s 47377 (53367)	Loss/tok 2.9765 (3.1898)	LR 1.000e-03
0: TRAIN [3][690/1938]	Time 0.282 (0.263)	Data 1.30e-04 (1.17e-03)	Tok/s 59770 (53429)	Loss/tok 3.1046 (3.1910)	LR 1.000e-03
0: TRAIN [3][700/1938]	Time 0.220 (0.263)	Data 1.23e-04 (1.15e-03)	Tok/s 45988 (53477)	Loss/tok 3.0850 (3.1914)	LR 1.000e-03
0: TRAIN [3][710/1938]	Time 0.280 (0.263)	Data 1.18e-04 (1.14e-03)	Tok/s 59854 (53481)	Loss/tok 3.1575 (3.1904)	LR 1.000e-03
0: TRAIN [3][720/1938]	Time 0.284 (0.263)	Data 1.36e-04 (1.12e-03)	Tok/s 59509 (53494)	Loss/tok 3.2258 (3.1894)	LR 1.000e-03
0: TRAIN [3][730/1938]	Time 0.281 (0.262)	Data 1.46e-04 (1.11e-03)	Tok/s 59676 (53426)	Loss/tok 3.1822 (3.1881)	LR 1.000e-03
0: TRAIN [3][740/1938]	Time 0.223 (0.262)	Data 1.17e-04 (1.10e-03)	Tok/s 46679 (53463)	Loss/tok 2.8990 (3.1877)	LR 1.000e-03
0: TRAIN [3][750/1938]	Time 0.166 (0.263)	Data 1.20e-04 (1.08e-03)	Tok/s 32224 (53510)	Loss/tok 2.6618 (3.1881)	LR 1.000e-03
0: TRAIN [3][760/1938]	Time 0.279 (0.263)	Data 1.21e-04 (1.07e-03)	Tok/s 60817 (53586)	Loss/tok 3.2908 (3.1884)	LR 1.000e-03
0: TRAIN [3][770/1938]	Time 0.283 (0.263)	Data 1.13e-04 (1.06e-03)	Tok/s 59471 (53633)	Loss/tok 3.0837 (3.1879)	LR 1.000e-03
0: TRAIN [3][780/1938]	Time 0.165 (0.263)	Data 1.11e-04 (1.05e-03)	Tok/s 32300 (53617)	Loss/tok 2.5939 (3.1879)	LR 1.000e-03
0: TRAIN [3][790/1938]	Time 0.223 (0.263)	Data 1.13e-04 (1.04e-03)	Tok/s 47044 (53636)	Loss/tok 2.9750 (3.1870)	LR 1.000e-03
0: TRAIN [3][800/1938]	Time 0.282 (0.263)	Data 1.43e-04 (1.02e-03)	Tok/s 60366 (53670)	Loss/tok 3.1196 (3.1873)	LR 1.000e-03
0: TRAIN [3][810/1938]	Time 0.343 (0.263)	Data 1.19e-04 (1.01e-03)	Tok/s 68221 (53722)	Loss/tok 3.2251 (3.1876)	LR 1.000e-03
0: TRAIN [3][820/1938]	Time 0.280 (0.263)	Data 1.71e-04 (1.00e-03)	Tok/s 59240 (53724)	Loss/tok 3.2279 (3.1871)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][830/1938]	Time 0.281 (0.264)	Data 1.19e-04 (9.92e-04)	Tok/s 59967 (53757)	Loss/tok 3.1038 (3.1892)	LR 1.000e-03
0: TRAIN [3][840/1938]	Time 0.282 (0.264)	Data 1.41e-04 (9.82e-04)	Tok/s 59185 (53815)	Loss/tok 3.2464 (3.1907)	LR 1.000e-03
0: TRAIN [3][850/1938]	Time 0.223 (0.264)	Data 1.33e-04 (9.72e-04)	Tok/s 47165 (53827)	Loss/tok 2.9492 (3.1905)	LR 1.000e-03
0: TRAIN [3][860/1938]	Time 0.286 (0.264)	Data 1.29e-04 (9.62e-04)	Tok/s 59279 (53798)	Loss/tok 3.1857 (3.1891)	LR 1.000e-03
0: TRAIN [3][870/1938]	Time 0.220 (0.264)	Data 1.77e-04 (9.53e-04)	Tok/s 46533 (53775)	Loss/tok 2.8884 (3.1885)	LR 1.000e-03
0: TRAIN [3][880/1938]	Time 0.220 (0.263)	Data 1.47e-04 (9.43e-04)	Tok/s 47102 (53662)	Loss/tok 2.9328 (3.1866)	LR 1.000e-03
0: TRAIN [3][890/1938]	Time 0.281 (0.264)	Data 1.23e-04 (9.34e-04)	Tok/s 59557 (53718)	Loss/tok 3.2226 (3.1874)	LR 1.000e-03
0: TRAIN [3][900/1938]	Time 0.168 (0.264)	Data 1.20e-04 (9.25e-04)	Tok/s 31964 (53698)	Loss/tok 2.5181 (3.1864)	LR 1.000e-03
0: TRAIN [3][910/1938]	Time 0.220 (0.264)	Data 1.54e-04 (9.16e-04)	Tok/s 47345 (53711)	Loss/tok 3.0158 (3.1860)	LR 1.000e-03
0: TRAIN [3][920/1938]	Time 0.341 (0.264)	Data 1.14e-04 (9.08e-04)	Tok/s 68039 (53762)	Loss/tok 3.2930 (3.1857)	LR 1.000e-03
0: TRAIN [3][930/1938]	Time 0.345 (0.264)	Data 1.41e-04 (9.00e-04)	Tok/s 68075 (53790)	Loss/tok 3.2669 (3.1858)	LR 1.000e-03
0: TRAIN [3][940/1938]	Time 0.343 (0.264)	Data 1.22e-04 (8.91e-04)	Tok/s 67905 (53791)	Loss/tok 3.2728 (3.1849)	LR 1.000e-03
0: TRAIN [3][950/1938]	Time 0.221 (0.264)	Data 1.23e-04 (8.83e-04)	Tok/s 46331 (53800)	Loss/tok 2.9440 (3.1851)	LR 1.000e-03
0: TRAIN [3][960/1938]	Time 0.222 (0.264)	Data 1.35e-04 (8.75e-04)	Tok/s 47304 (53795)	Loss/tok 2.9339 (3.1845)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][970/1938]	Time 0.284 (0.264)	Data 1.31e-04 (8.68e-04)	Tok/s 59684 (53806)	Loss/tok 3.1192 (3.1838)	LR 1.000e-03
0: TRAIN [3][980/1938]	Time 0.342 (0.264)	Data 1.50e-04 (8.60e-04)	Tok/s 69457 (53785)	Loss/tok 3.1161 (3.1827)	LR 1.000e-03
0: TRAIN [3][990/1938]	Time 0.282 (0.263)	Data 1.30e-04 (8.53e-04)	Tok/s 59396 (53779)	Loss/tok 3.1536 (3.1820)	LR 1.000e-03
0: TRAIN [3][1000/1938]	Time 0.221 (0.264)	Data 1.40e-04 (8.46e-04)	Tok/s 46576 (53808)	Loss/tok 3.0493 (3.1827)	LR 1.000e-03
0: TRAIN [3][1010/1938]	Time 0.221 (0.263)	Data 1.25e-04 (8.39e-04)	Tok/s 46712 (53738)	Loss/tok 2.8742 (3.1812)	LR 1.000e-03
0: TRAIN [3][1020/1938]	Time 0.221 (0.263)	Data 1.41e-04 (8.32e-04)	Tok/s 47586 (53727)	Loss/tok 2.9702 (3.1802)	LR 1.000e-03
0: TRAIN [3][1030/1938]	Time 0.343 (0.263)	Data 1.31e-04 (8.25e-04)	Tok/s 67911 (53792)	Loss/tok 3.3287 (3.1810)	LR 1.000e-03
0: TRAIN [3][1040/1938]	Time 0.165 (0.263)	Data 1.31e-04 (8.18e-04)	Tok/s 32568 (53758)	Loss/tok 2.5663 (3.1799)	LR 1.000e-03
0: TRAIN [3][1050/1938]	Time 0.220 (0.263)	Data 1.19e-04 (8.12e-04)	Tok/s 47513 (53749)	Loss/tok 2.9500 (3.1792)	LR 1.000e-03
0: TRAIN [3][1060/1938]	Time 0.283 (0.263)	Data 1.29e-04 (8.05e-04)	Tok/s 58757 (53761)	Loss/tok 3.1237 (3.1785)	LR 1.000e-03
0: TRAIN [3][1070/1938]	Time 0.221 (0.263)	Data 1.19e-04 (7.99e-04)	Tok/s 46411 (53755)	Loss/tok 2.9054 (3.1774)	LR 1.000e-03
0: TRAIN [3][1080/1938]	Time 0.223 (0.263)	Data 1.22e-04 (7.93e-04)	Tok/s 46694 (53792)	Loss/tok 2.9588 (3.1775)	LR 1.000e-03
0: TRAIN [3][1090/1938]	Time 0.343 (0.263)	Data 1.33e-04 (7.87e-04)	Tok/s 68284 (53771)	Loss/tok 3.2969 (3.1776)	LR 1.000e-03
0: TRAIN [3][1100/1938]	Time 0.278 (0.263)	Data 1.38e-04 (7.81e-04)	Tok/s 60945 (53767)	Loss/tok 3.2174 (3.1772)	LR 1.000e-03
0: TRAIN [3][1110/1938]	Time 0.166 (0.263)	Data 1.35e-04 (7.75e-04)	Tok/s 32274 (53774)	Loss/tok 2.5329 (3.1771)	LR 1.000e-03
0: TRAIN [3][1120/1938]	Time 0.282 (0.263)	Data 1.20e-04 (7.69e-04)	Tok/s 59888 (53764)	Loss/tok 3.2176 (3.1766)	LR 1.000e-03
0: TRAIN [3][1130/1938]	Time 0.221 (0.263)	Data 1.67e-04 (7.64e-04)	Tok/s 47161 (53754)	Loss/tok 2.9054 (3.1761)	LR 1.000e-03
0: TRAIN [3][1140/1938]	Time 0.281 (0.263)	Data 1.38e-04 (7.58e-04)	Tok/s 58758 (53805)	Loss/tok 3.2289 (3.1764)	LR 1.000e-03
0: TRAIN [3][1150/1938]	Time 0.220 (0.263)	Data 1.24e-04 (7.53e-04)	Tok/s 47015 (53776)	Loss/tok 2.8920 (3.1752)	LR 1.000e-03
0: TRAIN [3][1160/1938]	Time 0.219 (0.263)	Data 1.52e-04 (7.48e-04)	Tok/s 48220 (53758)	Loss/tok 2.9256 (3.1747)	LR 1.000e-03
0: TRAIN [3][1170/1938]	Time 0.285 (0.263)	Data 1.47e-04 (7.43e-04)	Tok/s 58243 (53754)	Loss/tok 3.1664 (3.1752)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1180/1938]	Time 0.414 (0.263)	Data 1.54e-04 (7.38e-04)	Tok/s 70825 (53719)	Loss/tok 3.5845 (3.1749)	LR 1.000e-03
0: TRAIN [3][1190/1938]	Time 0.282 (0.263)	Data 1.33e-04 (7.33e-04)	Tok/s 60062 (53722)	Loss/tok 3.1581 (3.1745)	LR 1.000e-03
0: TRAIN [3][1200/1938]	Time 0.282 (0.263)	Data 1.53e-04 (7.28e-04)	Tok/s 59644 (53725)	Loss/tok 3.1826 (3.1739)	LR 1.000e-03
0: TRAIN [3][1210/1938]	Time 0.344 (0.263)	Data 1.44e-04 (7.23e-04)	Tok/s 67448 (53672)	Loss/tok 3.2035 (3.1729)	LR 1.000e-03
0: TRAIN [3][1220/1938]	Time 0.219 (0.262)	Data 1.34e-04 (7.18e-04)	Tok/s 46425 (53653)	Loss/tok 2.9853 (3.1719)	LR 1.000e-03
0: TRAIN [3][1230/1938]	Time 0.280 (0.262)	Data 1.18e-04 (7.13e-04)	Tok/s 59953 (53625)	Loss/tok 3.0724 (3.1716)	LR 1.000e-03
0: TRAIN [3][1240/1938]	Time 0.344 (0.262)	Data 1.76e-04 (7.09e-04)	Tok/s 68144 (53658)	Loss/tok 3.3536 (3.1717)	LR 1.000e-03
0: TRAIN [3][1250/1938]	Time 0.221 (0.262)	Data 1.20e-04 (7.04e-04)	Tok/s 46604 (53653)	Loss/tok 2.9359 (3.1714)	LR 1.000e-03
0: TRAIN [3][1260/1938]	Time 0.343 (0.263)	Data 1.30e-04 (7.00e-04)	Tok/s 68400 (53671)	Loss/tok 3.2777 (3.1718)	LR 1.000e-03
0: TRAIN [3][1270/1938]	Time 0.280 (0.263)	Data 1.30e-04 (6.95e-04)	Tok/s 60267 (53685)	Loss/tok 3.0642 (3.1714)	LR 1.000e-03
0: TRAIN [3][1280/1938]	Time 0.219 (0.262)	Data 1.44e-04 (6.91e-04)	Tok/s 46214 (53671)	Loss/tok 2.9411 (3.1710)	LR 1.000e-03
0: TRAIN [3][1290/1938]	Time 0.278 (0.262)	Data 1.23e-04 (6.87e-04)	Tok/s 60304 (53632)	Loss/tok 3.1402 (3.1701)	LR 1.000e-03
0: TRAIN [3][1300/1938]	Time 0.221 (0.262)	Data 1.54e-04 (6.82e-04)	Tok/s 46736 (53626)	Loss/tok 2.9115 (3.1694)	LR 1.000e-03
0: TRAIN [3][1310/1938]	Time 0.221 (0.262)	Data 1.26e-04 (6.78e-04)	Tok/s 46527 (53663)	Loss/tok 2.9821 (3.1702)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1320/1938]	Time 0.282 (0.263)	Data 1.41e-04 (6.74e-04)	Tok/s 59414 (53678)	Loss/tok 3.1386 (3.1705)	LR 1.000e-03
0: TRAIN [3][1330/1938]	Time 0.220 (0.262)	Data 1.17e-04 (6.70e-04)	Tok/s 46304 (53621)	Loss/tok 3.0258 (3.1695)	LR 1.000e-03
0: TRAIN [3][1340/1938]	Time 0.223 (0.262)	Data 1.10e-04 (6.66e-04)	Tok/s 46947 (53618)	Loss/tok 2.9239 (3.1685)	LR 1.000e-03
0: TRAIN [3][1350/1938]	Time 0.220 (0.262)	Data 1.31e-04 (6.62e-04)	Tok/s 46668 (53634)	Loss/tok 3.0649 (3.1685)	LR 1.000e-03
0: TRAIN [3][1360/1938]	Time 0.218 (0.262)	Data 1.27e-04 (6.58e-04)	Tok/s 47590 (53611)	Loss/tok 2.9681 (3.1677)	LR 1.000e-03
0: TRAIN [3][1370/1938]	Time 0.220 (0.262)	Data 1.33e-04 (6.54e-04)	Tok/s 46950 (53577)	Loss/tok 2.8626 (3.1668)	LR 1.000e-03
0: TRAIN [3][1380/1938]	Time 0.343 (0.262)	Data 1.64e-04 (6.50e-04)	Tok/s 68016 (53606)	Loss/tok 3.3925 (3.1671)	LR 1.000e-03
0: TRAIN [3][1390/1938]	Time 0.343 (0.262)	Data 1.37e-04 (6.47e-04)	Tok/s 68379 (53622)	Loss/tok 3.2689 (3.1673)	LR 1.000e-03
0: TRAIN [3][1400/1938]	Time 0.168 (0.262)	Data 1.31e-04 (6.43e-04)	Tok/s 31041 (53604)	Loss/tok 2.5614 (3.1669)	LR 1.000e-03
0: TRAIN [3][1410/1938]	Time 0.221 (0.262)	Data 1.14e-04 (6.39e-04)	Tok/s 45362 (53605)	Loss/tok 3.0173 (3.1667)	LR 1.000e-03
0: TRAIN [3][1420/1938]	Time 0.412 (0.262)	Data 1.43e-04 (6.36e-04)	Tok/s 71414 (53656)	Loss/tok 3.5202 (3.1678)	LR 1.000e-03
0: TRAIN [3][1430/1938]	Time 0.167 (0.262)	Data 1.33e-04 (6.32e-04)	Tok/s 31865 (53609)	Loss/tok 2.4072 (3.1668)	LR 1.000e-03
0: TRAIN [3][1440/1938]	Time 0.224 (0.262)	Data 1.20e-04 (6.29e-04)	Tok/s 46575 (53640)	Loss/tok 3.0124 (3.1668)	LR 1.000e-03
0: TRAIN [3][1450/1938]	Time 0.220 (0.262)	Data 1.23e-04 (6.25e-04)	Tok/s 46743 (53635)	Loss/tok 2.9372 (3.1664)	LR 1.000e-03
0: TRAIN [3][1460/1938]	Time 0.280 (0.262)	Data 1.28e-04 (6.22e-04)	Tok/s 59971 (53594)	Loss/tok 3.0842 (3.1656)	LR 1.000e-03
0: TRAIN [3][1470/1938]	Time 0.280 (0.262)	Data 1.19e-04 (6.19e-04)	Tok/s 60438 (53581)	Loss/tok 3.0247 (3.1653)	LR 1.000e-03
0: TRAIN [3][1480/1938]	Time 0.222 (0.262)	Data 1.52e-04 (6.15e-04)	Tok/s 46706 (53531)	Loss/tok 2.9381 (3.1643)	LR 1.000e-03
0: TRAIN [3][1490/1938]	Time 0.281 (0.262)	Data 1.39e-04 (6.12e-04)	Tok/s 60103 (53534)	Loss/tok 3.1133 (3.1648)	LR 5.000e-04
0: TRAIN [3][1500/1938]	Time 0.222 (0.262)	Data 1.33e-04 (6.09e-04)	Tok/s 46106 (53527)	Loss/tok 2.8828 (3.1647)	LR 5.000e-04
0: TRAIN [3][1510/1938]	Time 0.280 (0.262)	Data 1.48e-04 (6.06e-04)	Tok/s 59547 (53542)	Loss/tok 3.1820 (3.1652)	LR 5.000e-04
0: TRAIN [3][1520/1938]	Time 0.285 (0.262)	Data 1.25e-04 (6.03e-04)	Tok/s 58469 (53570)	Loss/tok 3.0682 (3.1649)	LR 5.000e-04
0: TRAIN [3][1530/1938]	Time 0.283 (0.262)	Data 1.29e-04 (6.00e-04)	Tok/s 59216 (53541)	Loss/tok 3.1829 (3.1641)	LR 5.000e-04
0: TRAIN [3][1540/1938]	Time 0.282 (0.262)	Data 1.14e-04 (5.96e-04)	Tok/s 60060 (53571)	Loss/tok 3.1087 (3.1636)	LR 5.000e-04
0: TRAIN [3][1550/1938]	Time 0.284 (0.262)	Data 1.19e-04 (5.93e-04)	Tok/s 59585 (53560)	Loss/tok 3.1208 (3.1633)	LR 5.000e-04
0: TRAIN [3][1560/1938]	Time 0.222 (0.262)	Data 2.04e-04 (5.90e-04)	Tok/s 46554 (53523)	Loss/tok 2.9216 (3.1628)	LR 5.000e-04
0: TRAIN [3][1570/1938]	Time 0.280 (0.262)	Data 1.27e-04 (5.87e-04)	Tok/s 59919 (53523)	Loss/tok 3.1584 (3.1625)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1580/1938]	Time 0.165 (0.262)	Data 1.13e-04 (5.84e-04)	Tok/s 31204 (53498)	Loss/tok 2.4769 (3.1621)	LR 5.000e-04
0: TRAIN [3][1590/1938]	Time 0.222 (0.262)	Data 1.01e-04 (5.81e-04)	Tok/s 45466 (53513)	Loss/tok 2.8237 (3.1622)	LR 5.000e-04
0: TRAIN [3][1600/1938]	Time 0.166 (0.262)	Data 1.06e-04 (5.78e-04)	Tok/s 31340 (53497)	Loss/tok 2.4177 (3.1617)	LR 5.000e-04
0: TRAIN [3][1610/1938]	Time 0.280 (0.262)	Data 1.04e-04 (5.76e-04)	Tok/s 60668 (53504)	Loss/tok 3.1953 (3.1615)	LR 5.000e-04
0: TRAIN [3][1620/1938]	Time 0.347 (0.262)	Data 1.06e-04 (5.73e-04)	Tok/s 66581 (53509)	Loss/tok 3.3846 (3.1614)	LR 5.000e-04
0: TRAIN [3][1630/1938]	Time 0.220 (0.262)	Data 1.02e-04 (5.70e-04)	Tok/s 46966 (53505)	Loss/tok 2.8891 (3.1617)	LR 5.000e-04
0: TRAIN [3][1640/1938]	Time 0.279 (0.262)	Data 1.06e-04 (5.67e-04)	Tok/s 59831 (53526)	Loss/tok 3.0848 (3.1616)	LR 5.000e-04
0: TRAIN [3][1650/1938]	Time 0.278 (0.262)	Data 1.07e-04 (5.64e-04)	Tok/s 59186 (53527)	Loss/tok 3.1912 (3.1615)	LR 5.000e-04
0: TRAIN [3][1660/1938]	Time 0.343 (0.262)	Data 1.25e-04 (5.62e-04)	Tok/s 67035 (53510)	Loss/tok 3.4220 (3.1612)	LR 5.000e-04
0: TRAIN [3][1670/1938]	Time 0.281 (0.262)	Data 1.46e-04 (5.59e-04)	Tok/s 60036 (53496)	Loss/tok 3.0865 (3.1605)	LR 5.000e-04
0: TRAIN [3][1680/1938]	Time 0.342 (0.262)	Data 9.92e-05 (5.57e-04)	Tok/s 68190 (53480)	Loss/tok 3.2873 (3.1601)	LR 5.000e-04
0: TRAIN [3][1690/1938]	Time 0.219 (0.262)	Data 1.35e-04 (5.54e-04)	Tok/s 47754 (53487)	Loss/tok 2.8542 (3.1602)	LR 5.000e-04
0: TRAIN [3][1700/1938]	Time 0.218 (0.262)	Data 9.63e-05 (5.51e-04)	Tok/s 48138 (53512)	Loss/tok 3.0314 (3.1608)	LR 5.000e-04
0: TRAIN [3][1710/1938]	Time 0.281 (0.262)	Data 9.87e-05 (5.49e-04)	Tok/s 59680 (53515)	Loss/tok 3.0944 (3.1602)	LR 5.000e-04
0: TRAIN [3][1720/1938]	Time 0.222 (0.262)	Data 1.41e-04 (5.46e-04)	Tok/s 46801 (53495)	Loss/tok 2.9445 (3.1595)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1730/1938]	Time 0.167 (0.262)	Data 9.73e-05 (5.44e-04)	Tok/s 31353 (53469)	Loss/tok 2.4889 (3.1594)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1740/1938]	Time 0.219 (0.261)	Data 1.17e-04 (5.42e-04)	Tok/s 46686 (53437)	Loss/tok 3.0088 (3.1589)	LR 5.000e-04
0: TRAIN [3][1750/1938]	Time 0.344 (0.262)	Data 1.00e-04 (5.39e-04)	Tok/s 67790 (53468)	Loss/tok 3.4191 (3.1590)	LR 5.000e-04
0: TRAIN [3][1760/1938]	Time 0.278 (0.262)	Data 1.36e-04 (5.37e-04)	Tok/s 61113 (53485)	Loss/tok 3.1530 (3.1589)	LR 5.000e-04
0: TRAIN [3][1770/1938]	Time 0.282 (0.262)	Data 1.84e-04 (5.35e-04)	Tok/s 59255 (53482)	Loss/tok 3.2254 (3.1585)	LR 5.000e-04
0: TRAIN [3][1780/1938]	Time 0.341 (0.262)	Data 1.72e-04 (5.32e-04)	Tok/s 67667 (53504)	Loss/tok 3.4159 (3.1586)	LR 5.000e-04
0: TRAIN [3][1790/1938]	Time 0.280 (0.262)	Data 1.01e-04 (5.30e-04)	Tok/s 60157 (53484)	Loss/tok 3.1594 (3.1580)	LR 5.000e-04
0: TRAIN [3][1800/1938]	Time 0.224 (0.261)	Data 1.23e-04 (5.28e-04)	Tok/s 46387 (53477)	Loss/tok 2.9372 (3.1576)	LR 5.000e-04
0: TRAIN [3][1810/1938]	Time 0.281 (0.261)	Data 1.39e-04 (5.25e-04)	Tok/s 60565 (53466)	Loss/tok 3.0890 (3.1571)	LR 5.000e-04
0: TRAIN [3][1820/1938]	Time 0.220 (0.261)	Data 1.41e-04 (5.23e-04)	Tok/s 47283 (53467)	Loss/tok 2.8952 (3.1567)	LR 5.000e-04
0: TRAIN [3][1830/1938]	Time 0.343 (0.261)	Data 1.59e-04 (5.21e-04)	Tok/s 68114 (53489)	Loss/tok 3.2638 (3.1570)	LR 5.000e-04
0: TRAIN [3][1840/1938]	Time 0.220 (0.261)	Data 1.13e-04 (5.19e-04)	Tok/s 46691 (53471)	Loss/tok 2.9270 (3.1565)	LR 5.000e-04
0: TRAIN [3][1850/1938]	Time 0.167 (0.261)	Data 1.10e-04 (5.17e-04)	Tok/s 31785 (53466)	Loss/tok 2.5690 (3.1558)	LR 5.000e-04
0: TRAIN [3][1860/1938]	Time 0.345 (0.261)	Data 1.08e-04 (5.14e-04)	Tok/s 67209 (53455)	Loss/tok 3.3374 (3.1553)	LR 5.000e-04
0: TRAIN [3][1870/1938]	Time 0.342 (0.261)	Data 1.10e-04 (5.12e-04)	Tok/s 67963 (53444)	Loss/tok 3.3826 (3.1547)	LR 5.000e-04
0: TRAIN [3][1880/1938]	Time 0.283 (0.261)	Data 1.04e-04 (5.10e-04)	Tok/s 59575 (53457)	Loss/tok 3.1022 (3.1549)	LR 5.000e-04
0: TRAIN [3][1890/1938]	Time 0.284 (0.261)	Data 1.06e-04 (5.08e-04)	Tok/s 59043 (53433)	Loss/tok 3.1699 (3.1542)	LR 5.000e-04
0: TRAIN [3][1900/1938]	Time 0.222 (0.261)	Data 1.29e-04 (5.06e-04)	Tok/s 46045 (53427)	Loss/tok 2.9318 (3.1539)	LR 5.000e-04
0: TRAIN [3][1910/1938]	Time 0.418 (0.261)	Data 1.12e-04 (5.04e-04)	Tok/s 70307 (53444)	Loss/tok 3.5000 (3.1542)	LR 5.000e-04
0: TRAIN [3][1920/1938]	Time 0.345 (0.261)	Data 1.09e-04 (5.02e-04)	Tok/s 68306 (53444)	Loss/tok 3.2605 (3.1539)	LR 5.000e-04
0: TRAIN [3][1930/1938]	Time 0.412 (0.261)	Data 1.11e-04 (5.00e-04)	Tok/s 71708 (53478)	Loss/tok 3.4571 (3.1541)	LR 5.000e-04
:::MLL 1582042951.187 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 525}}
:::MLL 1582042951.188 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.614 (0.614)	Decoder iters 101.0 (101.0)	Tok/s 26813 (26813)
0: Running moses detokenizer
0: BLEU(score=24.33178092194391, counts=[37184, 18707, 10712, 6398], totals=[65324, 62321, 59318, 56323], precisions=[56.9224174882126, 30.01716917251007, 18.058599413331535, 11.3594801413277], bp=1.0, sys_len=65324, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1582042952.989 eval_accuracy: {"value": 24.33, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 536}}
:::MLL 1582042952.989 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 3	Training Loss: 3.1500	Test BLEU: 24.33
0: Performance: Epoch: 3	Training: 427800 Tok/s
0: Finished epoch 3
:::MLL 1582042952.990 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 558}}
0: Closing preprocessed data file
:::MLL 1582042952.990 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 569}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-02-18 04:22:37 PM
RESULT,RNN_TRANSLATOR,,2052,nvidia,2020-02-18 03:48:25 PM
