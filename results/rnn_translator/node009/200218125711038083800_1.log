Beginning trial 1 of 2
Gathering sys log on node009
:::MLL 1582052311.453 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1582052311.453 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1582052311.454 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1582052311.454 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1582052311.454 submission_platform: {"value": "1xDSS8440", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1582052311.455 submission_entry: {"value": "{'hardware': 'DSS8440', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'Ethernet 100 Gb/sec (4X EDR)', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.6-2.1.4', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz', 'num_cores': '40', 'num_vcpus': '40', 'accelerator': 'Tesla V100S-PCIE-32GB', 'num_accelerators': '8', 'sys_mem_size': '754 GB', 'sys_storage_type': 'SATA SSD', 'sys_storage_size': '1x 446.6G + 1x 931.5G', 'cpu_accel_interconnect': 'UPI', 'network_card': '', 'num_network_cards': '0', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1582052311.455 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1582052311.456 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1582052313.493 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node node009
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=4923' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=200218125711038083800 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_200218125711038083800 ./run_and_time.sh
Run vars: id 200218125711038083800 gpus 8 mparams  --master_port=4923
STARTING TIMING RUN AT 2020-02-18 06:58:33 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=0
+ MATH=amp_fp16
+ [[ 0 -eq 1 ]]
+ LAUNCH_OPT='torch.distributed.launch --nproc_per_node 8  --master_port=4923'
running benchmark
+ echo 'running benchmark'
+ python -m torch.distributed.launch --nproc_per_node 8 --master_port=4923 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1582052316.028 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1582052316.031 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1582052316.041 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1582052316.043 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1582052316.043 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1582052316.048 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1582052316.049 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
:::MLL 1582052316.060 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 288}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=8, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 757854565
node009:465:465 [0] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:465:465 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation

node009:465:465 [0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:465:465 [0] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:465:465 [0] NCCL INFO NET/IB : No device found.
node009:465:465 [0] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
NCCL version 2.5.6+cuda10.2
node009:466:466 [1] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:466:466 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node009:472:472 [7] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:472:472 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node009:470:470 [5] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:470:470 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node009:471:471 [6] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:471:471 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node009:468:468 [3] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:468:468 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node009:467:467 [2] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:467:467 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
node009:469:469 [4] NCCL INFO Bootstrap : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:469:469 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation

node009:466:466 [1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:466:466 [1] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:466:466 [1] NCCL INFO NET/IB : No device found.

node009:472:472 [7] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:472:472 [7] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:472:472 [7] NCCL INFO NET/IB : No device found.

node009:470:470 [5] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:470:470 [5] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:470:470 [5] NCCL INFO NET/IB : No device found.

node009:471:471 [6] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:471:471 [6] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:471:471 [6] NCCL INFO NET/IB : No device found.

node009:468:468 [3] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:468:468 [3] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:468:468 [3] NCCL INFO NET/IB : No device found.

node009:467:467 [2] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:467:467 [2] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:467:467 [2] NCCL INFO NET/IB : No device found.

node009:469:469 [4] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

node009:469:469 [4] transport/net_ib.cc:120 NCCL WARN NET/IB : Unable to open device mlx5_0
node009:469:469 [4] NCCL INFO NET/IB : No device found.
node009:466:466 [1] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:472:472 [7] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:471:471 [6] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:470:470 [5] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:469:469 [4] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:467:467 [2] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:468:468 [3] NCCL INFO NET/Socket : Using [0]eth3:10.141.0.9<0> [1]cali2db325eb964:fe80::ecee:eeff:feee:eeee%cali2db325eb964<0>
node009:465:827 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555
node009:467:828 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555
node009:466:831 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555
node009:470:833 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa
node009:469:830 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa
node009:471:829 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa
node009:472:832 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa
node009:468:834 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555
node009:469:830 [4] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:469:830 [4] NCCL INFO include/net.h:19 -> 2
node009:468:834 [3] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:468:834 [3] NCCL INFO include/net.h:19 -> 2
node009:466:831 [1] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:466:831 [1] NCCL INFO include/net.h:19 -> 2
node009:465:827 [0] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:465:827 [0] NCCL INFO include/net.h:19 -> 2
node009:471:829 [6] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:471:829 [6] NCCL INFO include/net.h:19 -> 2
node009:472:832 [7] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:472:832 [7] NCCL INFO include/net.h:19 -> 2
node009:467:828 [2] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:467:828 [2] NCCL INFO include/net.h:19 -> 2
node009:470:833 [5] NCCL INFO Could not find real path of /sys/class/net/cali2db325eb964/device
node009:470:833 [5] NCCL INFO include/net.h:19 -> 2
node009:469:830 [4] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:468:834 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:466:831 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:465:827 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:471:829 [6] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:472:832 [7] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:467:828 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:470:833 [5] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
node009:467:828 [2] NCCL INFO Threads per block : 512/640/256
node009:467:828 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:467:828 [2] NCCL INFO Trees [0] 3/-1/-1->2->1|1->2->3/-1/-1 [1] 3/-1/-1->2->1|1->2->3/-1/-1
node009:466:831 [1] NCCL INFO Threads per block : 512/640/256
node009:468:834 [3] NCCL INFO Threads per block : 512/640/256
node009:466:831 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:468:834 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:471:829 [6] NCCL INFO Threads per block : 512/640/256
node009:472:832 [7] NCCL INFO Threads per block : 512/640/256
node009:466:831 [1] NCCL INFO Trees [0] 2/-1/-1->1->0|0->1->2/-1/-1 [1] 2/-1/-1->1->0|0->1->2/-1/-1
node009:469:830 [4] NCCL INFO Threads per block : 512/640/256
node009:468:834 [3] NCCL INFO Trees [0] 4/-1/-1->3->2|2->3->4/-1/-1 [1] 4/-1/-1->3->2|2->3->4/-1/-1
node009:465:827 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
node009:470:833 [5] NCCL INFO Threads per block : 512/640/256
node009:471:829 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:472:832 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:469:830 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:465:827 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
node009:471:829 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1
node009:472:832 [7] NCCL INFO Trees [0] -1/-1/-1->7->6|6->7->-1/-1/-1 [1] -1/-1/-1->7->6|6->7->-1/-1/-1
node009:469:830 [4] NCCL INFO Trees [0] 5/-1/-1->4->3|3->4->5/-1/-1 [1] 5/-1/-1->4->3|3->4->5/-1/-1
node009:470:833 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:465:827 [0] NCCL INFO Threads per block : 512/640/256
node009:470:833 [5] NCCL INFO Trees [0] 6/-1/-1->5->4|4->5->6/-1/-1 [1] 6/-1/-1->5->4|4->5->6/-1/-1
node009:465:827 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64
node009:465:827 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 1/-1/-1->0->-1|-1->0->1/-1/-1
node009:466:831 [1] NCCL INFO Ring 00 : 1[11000] -> 2[47000] via P2P/IPC
node009:468:834 [3] NCCL INFO Ring 00 : 3[48000] -> 4[89000] via direct shared memory
node009:467:828 [2] NCCL INFO Ring 00 : 2[47000] -> 3[48000] via P2P/IPC
node009:471:829 [6] NCCL INFO Ring 00 : 6[c1000] -> 7[c2000] via P2P/IPC
node009:470:833 [5] NCCL INFO Ring 00 : 5[8a000] -> 6[c1000] via P2P/IPC
node009:472:832 [7] NCCL INFO Ring 00 : 7[c2000] -> 0[10000] via direct shared memory
node009:469:830 [4] NCCL INFO Ring 00 : 4[89000] -> 5[8a000] via P2P/IPC
node009:465:827 [0] NCCL INFO Ring 00 : 0[10000] -> 1[11000] via P2P/IPC
node009:467:828 [2] NCCL INFO Ring 00 : 2[47000] -> 1[11000] via P2P/IPC
node009:472:832 [7] NCCL INFO Ring 00 : 7[c2000] -> 6[c1000] via P2P/IPC
node009:471:829 [6] NCCL INFO Ring 00 : 6[c1000] -> 5[8a000] via P2P/IPC
node009:470:833 [5] NCCL INFO Ring 00 : 5[8a000] -> 4[89000] via P2P/IPC
node009:466:831 [1] NCCL INFO Ring 00 : 1[11000] -> 0[10000] via P2P/IPC
node009:469:830 [4] NCCL INFO Ring 00 : 4[89000] -> 3[48000] via direct shared memory
node009:472:832 [7] NCCL INFO Ring 01 : 7[c2000] -> 0[10000] via direct shared memory
node009:471:829 [6] NCCL INFO Ring 01 : 6[c1000] -> 7[c2000] via P2P/IPC
node009:470:833 [5] NCCL INFO Ring 01 : 5[8a000] -> 6[c1000] via P2P/IPC
node009:466:831 [1] NCCL INFO Ring 01 : 1[11000] -> 2[47000] via P2P/IPC
node009:468:834 [3] NCCL INFO Ring 00 : 3[48000] -> 2[47000] via P2P/IPC
node009:467:828 [2] NCCL INFO Ring 01 : 2[47000] -> 3[48000] via P2P/IPC
node009:468:834 [3] NCCL INFO Ring 01 : 3[48000] -> 4[89000] via direct shared memory
node009:471:829 [6] NCCL INFO Ring 01 : 6[c1000] -> 5[8a000] via P2P/IPC
node009:465:827 [0] NCCL INFO Ring 01 : 0[10000] -> 1[11000] via P2P/IPC
node009:472:832 [7] NCCL INFO Ring 01 : 7[c2000] -> 6[c1000] via P2P/IPC
node009:466:831 [1] NCCL INFO Ring 01 : 1[11000] -> 0[10000] via P2P/IPC
node009:467:828 [2] NCCL INFO Ring 01 : 2[47000] -> 1[11000] via P2P/IPC
node009:469:830 [4] NCCL INFO Ring 01 : 4[89000] -> 5[8a000] via P2P/IPC
node009:470:833 [5] NCCL INFO Ring 01 : 5[8a000] -> 4[89000] via P2P/IPC
node009:469:830 [4] NCCL INFO Ring 01 : 4[89000] -> 3[48000] via direct shared memory
node009:472:832 [7] NCCL INFO comm 0x7fff60007570 rank 7 nranks 8 cudaDev 7 busId c2000 - Init COMPLETE
node009:468:834 [3] NCCL INFO Ring 01 : 3[48000] -> 2[47000] via P2P/IPC
node009:465:827 [0] NCCL INFO comm 0x7ffe40007570 rank 0 nranks 8 cudaDev 0 busId 10000 - Init COMPLETE
node009:465:465 [0] NCCL INFO Launch mode Parallel
node009:466:831 [1] NCCL INFO comm 0x7fff68007570 rank 1 nranks 8 cudaDev 1 busId 11000 - Init COMPLETE
0: Worker 0 is using worker seed: 4263654728
0: Building vocabulary from /data/vocab.bpe.32000
node009:471:829 [6] NCCL INFO comm 0x7ffe94007570 rank 6 nranks 8 cudaDev 6 busId c1000 - Init COMPLETE
node009:470:833 [5] NCCL INFO comm 0x7fff54007570 rank 5 nranks 8 cudaDev 5 busId 8a000 - Init COMPLETE
0: Size of vocabulary: 32320
node009:469:830 [4] NCCL INFO comm 0x7ffe94007570 rank 4 nranks 8 cudaDev 4 busId 89000 - Init COMPLETE
node009:467:828 [2] NCCL INFO comm 0x7ffe70007570 rank 2 nranks 8 cudaDev 2 busId 47000 - Init COMPLETE
node009:468:834 [3] NCCL INFO comm 0x7fff5c007570 rank 3 nranks 8 cudaDev 3 busId 48000 - Init COMPLETE
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1582052325.412 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1582052326.579 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 407}}
:::MLL 1582052326.580 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 409}}
:::MLL 1582052326.580 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 414}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1582052327.610 global_batch_size: {"value": 2048, "metadata": {"file": "train.py", "lineno": 459}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 8, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 8
:::MLL 1582052327.612 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1582052327.612 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1582052327.612 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1582052327.613 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1582052327.613 opt_learning_rate_decay_steps: {"value": 8, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1582052327.613 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1582052327.614 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1582052327.620 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1582052327.620 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 515}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 3728763025
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/1938]	Time 1.199 (1.199)	Data 7.88e-01 (7.88e-01)	Tok/s 24959 (24959)	Loss/tok 10.7102 (10.7102)	LR 2.000e-05
0: TRAIN [0][10/1938]	Time 0.338 (0.349)	Data 1.43e-04 (7.18e-02)	Tok/s 68564 (51352)	Loss/tok 9.8013 (10.2405)	LR 2.518e-05
0: TRAIN [0][20/1938]	Time 0.282 (0.309)	Data 1.31e-04 (3.77e-02)	Tok/s 59816 (52671)	Loss/tok 9.3323 (9.9033)	LR 3.170e-05
0: TRAIN [0][30/1938]	Time 0.167 (0.288)	Data 1.47e-04 (2.56e-02)	Tok/s 31914 (52159)	Loss/tok 8.8037 (9.6855)	LR 3.991e-05
0: TRAIN [0][40/1938]	Time 0.165 (0.275)	Data 1.56e-04 (1.94e-02)	Tok/s 31919 (51471)	Loss/tok 8.4293 (9.5250)	LR 5.024e-05
0: TRAIN [0][50/1938]	Time 0.219 (0.274)	Data 1.33e-04 (1.56e-02)	Tok/s 47554 (52310)	Loss/tok 8.5210 (9.3524)	LR 6.325e-05
0: TRAIN [0][60/1938]	Time 0.222 (0.268)	Data 1.16e-04 (1.31e-02)	Tok/s 47069 (51961)	Loss/tok 8.2622 (9.2217)	LR 7.962e-05
0: TRAIN [0][70/1938]	Time 0.167 (0.265)	Data 2.02e-04 (1.12e-02)	Tok/s 32049 (51630)	Loss/tok 7.5854 (9.0961)	LR 1.002e-04
0: TRAIN [0][80/1938]	Time 0.218 (0.269)	Data 1.89e-04 (9.89e-03)	Tok/s 47715 (52556)	Loss/tok 7.9539 (8.9571)	LR 1.262e-04
0: TRAIN [0][90/1938]	Time 0.220 (0.267)	Data 1.97e-04 (8.82e-03)	Tok/s 45907 (52410)	Loss/tok 7.8044 (8.8619)	LR 1.589e-04
0: TRAIN [0][100/1938]	Time 0.342 (0.266)	Data 1.87e-04 (7.96e-03)	Tok/s 67593 (52610)	Loss/tok 8.0530 (8.7665)	LR 2.000e-04
0: TRAIN [0][110/1938]	Time 0.221 (0.264)	Data 1.32e-04 (7.26e-03)	Tok/s 45926 (52521)	Loss/tok 7.7929 (8.6948)	LR 2.518e-04
0: TRAIN [0][120/1938]	Time 0.340 (0.266)	Data 1.42e-04 (6.67e-03)	Tok/s 67732 (53066)	Loss/tok 8.0870 (8.6265)	LR 3.170e-04
0: TRAIN [0][130/1938]	Time 0.220 (0.263)	Data 1.33e-04 (6.17e-03)	Tok/s 46698 (52682)	Loss/tok 7.7065 (8.5741)	LR 3.991e-04
0: TRAIN [0][140/1938]	Time 0.340 (0.263)	Data 1.21e-04 (5.74e-03)	Tok/s 67790 (52824)	Loss/tok 7.9540 (8.5196)	LR 5.024e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][150/1938]	Time 0.277 (0.263)	Data 1.20e-04 (5.37e-03)	Tok/s 60939 (53120)	Loss/tok 7.9335 (8.4668)	LR 6.181e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][160/1938]	Time 0.282 (0.264)	Data 1.28e-04 (5.05e-03)	Tok/s 59611 (53394)	Loss/tok 7.6157 (8.4191)	LR 7.604e-04
0: TRAIN [0][170/1938]	Time 0.219 (0.264)	Data 1.32e-04 (4.76e-03)	Tok/s 47695 (53526)	Loss/tok 7.4903 (8.3706)	LR 9.573e-04
0: TRAIN [0][180/1938]	Time 0.343 (0.265)	Data 1.15e-04 (4.50e-03)	Tok/s 66725 (53598)	Loss/tok 7.7060 (8.3204)	LR 1.205e-03
0: TRAIN [0][190/1938]	Time 0.339 (0.266)	Data 1.19e-04 (4.28e-03)	Tok/s 68659 (53792)	Loss/tok 7.5606 (8.2669)	LR 1.517e-03
0: TRAIN [0][200/1938]	Time 0.278 (0.268)	Data 1.13e-04 (4.07e-03)	Tok/s 61451 (54178)	Loss/tok 7.0293 (8.2006)	LR 1.910e-03
0: TRAIN [0][210/1938]	Time 0.278 (0.268)	Data 1.07e-04 (3.88e-03)	Tok/s 59767 (54223)	Loss/tok 6.8485 (8.1422)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.282 (0.268)	Data 1.29e-04 (3.71e-03)	Tok/s 60170 (54276)	Loss/tok 6.7118 (8.0814)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.221 (0.268)	Data 1.43e-04 (3.56e-03)	Tok/s 46191 (54391)	Loss/tok 6.4604 (8.0188)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.339 (0.269)	Data 1.37e-04 (3.42e-03)	Tok/s 68988 (54582)	Loss/tok 6.6378 (7.9511)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.283 (0.270)	Data 1.27e-04 (3.29e-03)	Tok/s 59347 (54637)	Loss/tok 6.4442 (7.8860)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.217 (0.268)	Data 1.25e-04 (3.17e-03)	Tok/s 47405 (54437)	Loss/tok 5.9651 (7.8352)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.279 (0.267)	Data 1.70e-04 (3.05e-03)	Tok/s 59598 (54228)	Loss/tok 6.2868 (7.7843)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.280 (0.267)	Data 1.41e-04 (2.95e-03)	Tok/s 60600 (54205)	Loss/tok 6.0935 (7.7256)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.220 (0.267)	Data 1.39e-04 (2.85e-03)	Tok/s 46593 (54203)	Loss/tok 5.7172 (7.6676)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.281 (0.266)	Data 1.58e-04 (2.76e-03)	Tok/s 59842 (54175)	Loss/tok 6.0288 (7.6107)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.221 (0.266)	Data 1.21e-04 (2.68e-03)	Tok/s 46366 (54149)	Loss/tok 5.5231 (7.5569)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.341 (0.266)	Data 1.44e-04 (2.60e-03)	Tok/s 67727 (54097)	Loss/tok 6.0107 (7.5045)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.219 (0.266)	Data 1.41e-04 (2.52e-03)	Tok/s 46909 (54137)	Loss/tok 5.2961 (7.4482)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.164 (0.266)	Data 1.17e-04 (2.45e-03)	Tok/s 32893 (54192)	Loss/tok 4.5372 (7.3907)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.165 (0.265)	Data 1.30e-04 (2.39e-03)	Tok/s 32921 (54025)	Loss/tok 4.2985 (7.3427)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.280 (0.265)	Data 1.34e-04 (2.33e-03)	Tok/s 59280 (54085)	Loss/tok 5.2943 (7.2880)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.222 (0.264)	Data 1.71e-04 (2.27e-03)	Tok/s 46486 (53922)	Loss/tok 4.9438 (7.2437)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.281 (0.264)	Data 1.21e-04 (2.21e-03)	Tok/s 59260 (53875)	Loss/tok 5.1798 (7.1926)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.221 (0.263)	Data 1.33e-04 (2.16e-03)	Tok/s 47161 (53822)	Loss/tok 4.8334 (7.1451)	LR 2.000e-03
0: TRAIN [0][400/1938]	Time 0.221 (0.263)	Data 1.18e-04 (2.11e-03)	Tok/s 46945 (53885)	Loss/tok 4.6309 (7.0895)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.279 (0.263)	Data 1.31e-04 (2.06e-03)	Tok/s 60777 (53931)	Loss/tok 4.9596 (7.0370)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.220 (0.263)	Data 1.77e-04 (2.01e-03)	Tok/s 47896 (53900)	Loss/tok 4.6496 (6.9878)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.222 (0.263)	Data 1.74e-04 (1.97e-03)	Tok/s 45869 (53955)	Loss/tok 4.4511 (6.9372)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.276 (0.263)	Data 1.75e-04 (1.93e-03)	Tok/s 60204 (53930)	Loss/tok 4.7834 (6.8901)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.222 (0.263)	Data 9.92e-05 (1.89e-03)	Tok/s 45741 (53833)	Loss/tok 4.4706 (6.8460)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.283 (0.263)	Data 1.34e-04 (1.85e-03)	Tok/s 59901 (53921)	Loss/tok 4.7387 (6.7945)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.165 (0.263)	Data 1.25e-04 (1.82e-03)	Tok/s 32134 (53853)	Loss/tok 3.6106 (6.7507)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.220 (0.263)	Data 1.25e-04 (1.78e-03)	Tok/s 46033 (53802)	Loss/tok 4.2514 (6.7085)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.220 (0.262)	Data 1.13e-04 (1.75e-03)	Tok/s 47120 (53705)	Loss/tok 4.2901 (6.6706)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.164 (0.262)	Data 1.12e-04 (1.72e-03)	Tok/s 31966 (53626)	Loss/tok 3.5314 (6.6319)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.344 (0.262)	Data 1.17e-04 (1.69e-03)	Tok/s 69361 (53667)	Loss/tok 4.6218 (6.5877)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.165 (0.262)	Data 1.15e-04 (1.66e-03)	Tok/s 31476 (53652)	Loss/tok 3.4887 (6.5483)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.219 (0.261)	Data 1.15e-04 (1.63e-03)	Tok/s 46948 (53486)	Loss/tok 4.0547 (6.5174)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.412 (0.261)	Data 1.22e-04 (1.60e-03)	Tok/s 72730 (53522)	Loss/tok 4.8137 (6.4760)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.281 (0.260)	Data 1.31e-04 (1.57e-03)	Tok/s 60194 (53461)	Loss/tok 4.2749 (6.4407)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.221 (0.260)	Data 1.34e-04 (1.55e-03)	Tok/s 46560 (53472)	Loss/tok 4.0172 (6.4025)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.223 (0.260)	Data 1.22e-04 (1.52e-03)	Tok/s 46290 (53439)	Loss/tok 4.0865 (6.3681)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.219 (0.260)	Data 1.36e-04 (1.50e-03)	Tok/s 47176 (53332)	Loss/tok 4.0432 (6.3379)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][590/1938]	Time 0.169 (0.260)	Data 1.37e-04 (1.48e-03)	Tok/s 31138 (53307)	Loss/tok 3.2214 (6.3042)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.281 (0.260)	Data 1.08e-04 (1.45e-03)	Tok/s 60248 (53292)	Loss/tok 4.1826 (6.2706)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.281 (0.259)	Data 1.10e-04 (1.43e-03)	Tok/s 59451 (53290)	Loss/tok 4.2344 (6.2375)	LR 2.000e-03
0: TRAIN [0][620/1938]	Time 0.282 (0.260)	Data 1.45e-04 (1.41e-03)	Tok/s 60045 (53339)	Loss/tok 4.2162 (6.2020)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.222 (0.260)	Data 1.01e-04 (1.39e-03)	Tok/s 45830 (53378)	Loss/tok 3.7325 (6.1682)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.218 (0.259)	Data 1.20e-04 (1.37e-03)	Tok/s 47666 (53307)	Loss/tok 3.9887 (6.1408)	LR 2.000e-03
0: TRAIN [0][650/1938]	Time 0.223 (0.260)	Data 1.25e-04 (1.35e-03)	Tok/s 45764 (53356)	Loss/tok 3.8980 (6.1084)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.281 (0.260)	Data 1.31e-04 (1.33e-03)	Tok/s 58927 (53368)	Loss/tok 4.1546 (6.0772)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.222 (0.260)	Data 1.08e-04 (1.31e-03)	Tok/s 47521 (53405)	Loss/tok 3.9825 (6.0468)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.221 (0.260)	Data 1.18e-04 (1.30e-03)	Tok/s 46261 (53480)	Loss/tok 3.8244 (6.0140)	LR 2.000e-03
0: TRAIN [0][690/1938]	Time 0.221 (0.260)	Data 1.40e-04 (1.28e-03)	Tok/s 46818 (53473)	Loss/tok 3.9349 (5.9863)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.169 (0.260)	Data 1.18e-04 (1.26e-03)	Tok/s 31049 (53467)	Loss/tok 3.2505 (5.9593)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.280 (0.261)	Data 1.07e-04 (1.25e-03)	Tok/s 59619 (53533)	Loss/tok 4.1769 (5.9300)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.221 (0.261)	Data 1.35e-04 (1.23e-03)	Tok/s 46366 (53550)	Loss/tok 3.8290 (5.9035)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.222 (0.261)	Data 1.18e-04 (1.22e-03)	Tok/s 47971 (53547)	Loss/tok 3.6648 (5.8781)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.221 (0.261)	Data 1.24e-04 (1.20e-03)	Tok/s 47852 (53529)	Loss/tok 3.7597 (5.8540)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.221 (0.261)	Data 1.34e-04 (1.19e-03)	Tok/s 46885 (53481)	Loss/tok 3.8309 (5.8313)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.283 (0.261)	Data 1.24e-04 (1.17e-03)	Tok/s 59414 (53484)	Loss/tok 4.0372 (5.8078)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.167 (0.261)	Data 1.08e-04 (1.16e-03)	Tok/s 30956 (53503)	Loss/tok 3.1921 (5.7833)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.344 (0.261)	Data 1.32e-04 (1.15e-03)	Tok/s 66420 (53517)	Loss/tok 4.2494 (5.7599)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.280 (0.261)	Data 1.12e-04 (1.13e-03)	Tok/s 60113 (53586)	Loss/tok 4.0384 (5.7343)	LR 2.000e-03
0: TRAIN [0][800/1938]	Time 0.222 (0.261)	Data 1.22e-04 (1.12e-03)	Tok/s 46401 (53553)	Loss/tok 3.7699 (5.7141)	LR 2.000e-03
0: TRAIN [0][810/1938]	Time 0.280 (0.261)	Data 1.31e-04 (1.11e-03)	Tok/s 60628 (53568)	Loss/tok 3.9484 (5.6926)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.220 (0.261)	Data 1.18e-04 (1.10e-03)	Tok/s 47543 (53551)	Loss/tok 3.6758 (5.6726)	LR 2.000e-03
0: TRAIN [0][830/1938]	Time 0.222 (0.261)	Data 1.14e-04 (1.09e-03)	Tok/s 46603 (53553)	Loss/tok 3.7602 (5.6518)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.220 (0.261)	Data 1.45e-04 (1.07e-03)	Tok/s 45490 (53545)	Loss/tok 3.6005 (5.6328)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.282 (0.261)	Data 1.33e-04 (1.06e-03)	Tok/s 58474 (53549)	Loss/tok 3.9600 (5.6128)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.164 (0.260)	Data 1.29e-04 (1.05e-03)	Tok/s 32593 (53468)	Loss/tok 3.1652 (5.5965)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.280 (0.260)	Data 1.30e-04 (1.04e-03)	Tok/s 60605 (53411)	Loss/tok 3.9123 (5.5797)	LR 2.000e-03
0: TRAIN [0][880/1938]	Time 0.284 (0.260)	Data 1.22e-04 (1.03e-03)	Tok/s 58927 (53447)	Loss/tok 3.8861 (5.5589)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.341 (0.261)	Data 1.42e-04 (1.02e-03)	Tok/s 69032 (53492)	Loss/tok 4.1122 (5.5385)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.220 (0.261)	Data 1.25e-04 (1.01e-03)	Tok/s 46155 (53485)	Loss/tok 3.7074 (5.5205)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.222 (0.260)	Data 1.14e-04 (1.00e-03)	Tok/s 47052 (53474)	Loss/tok 3.7313 (5.5032)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][920/1938]	Time 0.222 (0.260)	Data 1.39e-04 (9.92e-04)	Tok/s 47003 (53451)	Loss/tok 3.7891 (5.4870)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.221 (0.260)	Data 1.37e-04 (9.83e-04)	Tok/s 46474 (53448)	Loss/tok 3.5639 (5.4700)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.221 (0.260)	Data 1.21e-04 (9.74e-04)	Tok/s 46946 (53467)	Loss/tok 3.5966 (5.4515)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.166 (0.260)	Data 1.22e-04 (9.65e-04)	Tok/s 32180 (53427)	Loss/tok 2.9589 (5.4365)	LR 2.000e-03
0: TRAIN [0][960/1938]	Time 0.345 (0.261)	Data 1.40e-04 (9.56e-04)	Tok/s 69240 (53494)	Loss/tok 3.9891 (5.4175)	LR 2.000e-03
0: TRAIN [0][970/1938]	Time 0.343 (0.261)	Data 1.23e-04 (9.47e-04)	Tok/s 68209 (53488)	Loss/tok 4.0470 (5.4015)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.414 (0.261)	Data 1.18e-04 (9.39e-04)	Tok/s 72418 (53557)	Loss/tok 4.3309 (5.3824)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.413 (0.261)	Data 1.30e-04 (9.31e-04)	Tok/s 72822 (53605)	Loss/tok 4.2283 (5.3646)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.344 (0.262)	Data 1.43e-04 (9.23e-04)	Tok/s 67569 (53635)	Loss/tok 4.0978 (5.3484)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.221 (0.261)	Data 1.22e-04 (9.15e-04)	Tok/s 46600 (53627)	Loss/tok 3.5650 (5.3336)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.218 (0.261)	Data 1.18e-04 (9.07e-04)	Tok/s 46921 (53627)	Loss/tok 3.6067 (5.3181)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.282 (0.261)	Data 1.32e-04 (9.00e-04)	Tok/s 59309 (53636)	Loss/tok 3.9171 (5.3027)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.222 (0.261)	Data 1.53e-04 (8.92e-04)	Tok/s 46463 (53621)	Loss/tok 3.5181 (5.2888)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.166 (0.261)	Data 1.28e-04 (8.85e-04)	Tok/s 31833 (53576)	Loss/tok 3.0102 (5.2765)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.221 (0.261)	Data 1.25e-04 (8.78e-04)	Tok/s 46679 (53586)	Loss/tok 3.6677 (5.2624)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1070/1938]	Time 0.278 (0.261)	Data 1.51e-04 (8.71e-04)	Tok/s 60644 (53567)	Loss/tok 3.7616 (5.2493)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.280 (0.261)	Data 1.43e-04 (8.64e-04)	Tok/s 59151 (53589)	Loss/tok 3.9109 (5.2345)	LR 2.000e-03
0: TRAIN [0][1090/1938]	Time 0.220 (0.261)	Data 1.39e-04 (8.57e-04)	Tok/s 46214 (53613)	Loss/tok 3.5191 (5.2200)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.279 (0.261)	Data 1.26e-04 (8.50e-04)	Tok/s 60753 (53575)	Loss/tok 3.7060 (5.2082)	LR 2.000e-03
0: TRAIN [0][1110/1938]	Time 0.222 (0.261)	Data 1.31e-04 (8.44e-04)	Tok/s 46182 (53570)	Loss/tok 3.5412 (5.1955)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.219 (0.261)	Data 1.51e-04 (8.38e-04)	Tok/s 47967 (53536)	Loss/tok 3.5049 (5.1838)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.219 (0.261)	Data 1.20e-04 (8.31e-04)	Tok/s 46641 (53503)	Loss/tok 3.4406 (5.1722)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.225 (0.261)	Data 1.26e-04 (8.25e-04)	Tok/s 45670 (53470)	Loss/tok 3.5045 (5.1612)	LR 2.000e-03
0: TRAIN [0][1150/1938]	Time 0.280 (0.260)	Data 1.31e-04 (8.19e-04)	Tok/s 59582 (53443)	Loss/tok 3.8523 (5.1497)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.283 (0.260)	Data 1.32e-04 (8.13e-04)	Tok/s 58922 (53444)	Loss/tok 3.6504 (5.1373)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.283 (0.261)	Data 1.32e-04 (8.07e-04)	Tok/s 59415 (53510)	Loss/tok 3.7423 (5.1227)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.343 (0.261)	Data 1.18e-04 (8.02e-04)	Tok/s 68330 (53502)	Loss/tok 3.9530 (5.1112)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.220 (0.261)	Data 1.24e-04 (7.96e-04)	Tok/s 47124 (53534)	Loss/tok 3.4978 (5.0983)	LR 2.000e-03
0: TRAIN [0][1200/1938]	Time 0.221 (0.261)	Data 1.36e-04 (7.91e-04)	Tok/s 46267 (53486)	Loss/tok 3.5341 (5.0882)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.343 (0.261)	Data 1.17e-04 (7.85e-04)	Tok/s 67655 (53468)	Loss/tok 3.9749 (5.0777)	LR 2.000e-03
0: TRAIN [0][1220/1938]	Time 0.280 (0.261)	Data 1.19e-04 (7.80e-04)	Tok/s 60445 (53473)	Loss/tok 3.6949 (5.0662)	LR 2.000e-03
0: TRAIN [0][1230/1938]	Time 0.282 (0.260)	Data 1.17e-04 (7.74e-04)	Tok/s 60141 (53453)	Loss/tok 3.6210 (5.0557)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.166 (0.261)	Data 1.24e-04 (7.69e-04)	Tok/s 32065 (53461)	Loss/tok 2.9530 (5.0445)	LR 2.000e-03
0: TRAIN [0][1250/1938]	Time 0.219 (0.261)	Data 1.12e-04 (7.64e-04)	Tok/s 46887 (53468)	Loss/tok 3.3195 (5.0333)	LR 2.000e-03
0: TRAIN [0][1260/1938]	Time 0.221 (0.261)	Data 1.21e-04 (7.59e-04)	Tok/s 47215 (53503)	Loss/tok 3.3887 (5.0213)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.282 (0.261)	Data 1.28e-04 (7.54e-04)	Tok/s 59108 (53562)	Loss/tok 3.7394 (5.0093)	LR 2.000e-03
0: TRAIN [0][1280/1938]	Time 0.219 (0.261)	Data 1.12e-04 (7.49e-04)	Tok/s 46642 (53555)	Loss/tok 3.4938 (4.9992)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.222 (0.261)	Data 1.28e-04 (7.44e-04)	Tok/s 45725 (53567)	Loss/tok 3.4545 (4.9886)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.222 (0.261)	Data 1.30e-04 (7.39e-04)	Tok/s 46372 (53543)	Loss/tok 3.5220 (4.9795)	LR 2.000e-03
0: TRAIN [0][1310/1938]	Time 0.280 (0.261)	Data 1.47e-04 (7.35e-04)	Tok/s 59486 (53565)	Loss/tok 3.6866 (4.9691)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.278 (0.261)	Data 1.28e-04 (7.30e-04)	Tok/s 59902 (53590)	Loss/tok 3.8134 (4.9590)	LR 2.000e-03
0: TRAIN [0][1330/1938]	Time 0.223 (0.261)	Data 1.54e-04 (7.25e-04)	Tok/s 46295 (53571)	Loss/tok 3.5570 (4.9499)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.282 (0.261)	Data 1.09e-04 (7.21e-04)	Tok/s 59181 (53575)	Loss/tok 3.5561 (4.9400)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.282 (0.261)	Data 1.25e-04 (7.17e-04)	Tok/s 59601 (53629)	Loss/tok 3.7634 (4.9292)	LR 2.000e-03
0: TRAIN [0][1360/1938]	Time 0.219 (0.261)	Data 1.18e-04 (7.12e-04)	Tok/s 47091 (53614)	Loss/tok 3.3049 (4.9203)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.278 (0.261)	Data 1.27e-04 (7.08e-04)	Tok/s 60589 (53642)	Loss/tok 3.7325 (4.9101)	LR 2.000e-03
0: TRAIN [0][1380/1938]	Time 0.166 (0.261)	Data 1.17e-04 (7.04e-04)	Tok/s 32270 (53623)	Loss/tok 2.9347 (4.9014)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.281 (0.261)	Data 1.16e-04 (7.00e-04)	Tok/s 59573 (53589)	Loss/tok 3.7224 (4.8933)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.220 (0.261)	Data 1.16e-04 (6.95e-04)	Tok/s 46965 (53588)	Loss/tok 3.4406 (4.8843)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.342 (0.261)	Data 1.24e-04 (6.91e-04)	Tok/s 67299 (53555)	Loss/tok 3.9895 (4.8768)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.279 (0.261)	Data 1.29e-04 (6.87e-04)	Tok/s 60453 (53554)	Loss/tok 3.5991 (4.8680)	LR 2.000e-03
0: TRAIN [0][1430/1938]	Time 0.220 (0.261)	Data 1.34e-04 (6.83e-04)	Tok/s 46990 (53580)	Loss/tok 3.3055 (4.8587)	LR 2.000e-03
0: TRAIN [0][1440/1938]	Time 0.279 (0.261)	Data 1.25e-04 (6.79e-04)	Tok/s 59873 (53585)	Loss/tok 3.7200 (4.8502)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1450/1938]	Time 0.344 (0.261)	Data 1.43e-04 (6.76e-04)	Tok/s 67510 (53608)	Loss/tok 3.8594 (4.8412)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.219 (0.261)	Data 1.27e-04 (6.72e-04)	Tok/s 47792 (53583)	Loss/tok 3.3709 (4.8335)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.221 (0.261)	Data 1.14e-04 (6.68e-04)	Tok/s 46279 (53579)	Loss/tok 3.3401 (4.8251)	LR 2.000e-03
0: TRAIN [0][1480/1938]	Time 0.220 (0.261)	Data 1.12e-04 (6.65e-04)	Tok/s 46531 (53559)	Loss/tok 3.4299 (4.8175)	LR 2.000e-03
0: TRAIN [0][1490/1938]	Time 0.344 (0.261)	Data 1.14e-04 (6.61e-04)	Tok/s 67562 (53591)	Loss/tok 3.7874 (4.8082)	LR 2.000e-03
0: TRAIN [0][1500/1938]	Time 0.344 (0.261)	Data 1.24e-04 (6.57e-04)	Tok/s 66695 (53618)	Loss/tok 3.9518 (4.7992)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.342 (0.261)	Data 1.17e-04 (6.54e-04)	Tok/s 67407 (53622)	Loss/tok 3.8934 (4.7915)	LR 2.000e-03
0: TRAIN [0][1520/1938]	Time 0.221 (0.261)	Data 1.18e-04 (6.50e-04)	Tok/s 47366 (53611)	Loss/tok 3.4497 (4.7843)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.280 (0.261)	Data 1.24e-04 (6.47e-04)	Tok/s 59588 (53578)	Loss/tok 3.5676 (4.7774)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.280 (0.261)	Data 1.43e-04 (6.44e-04)	Tok/s 60195 (53593)	Loss/tok 3.5671 (4.7690)	LR 2.000e-03
0: TRAIN [0][1550/1938]	Time 0.345 (0.261)	Data 1.21e-04 (6.40e-04)	Tok/s 68816 (53618)	Loss/tok 3.7560 (4.7610)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.167 (0.261)	Data 1.22e-04 (6.37e-04)	Tok/s 31952 (53600)	Loss/tok 2.9197 (4.7544)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.220 (0.261)	Data 1.24e-04 (6.34e-04)	Tok/s 47982 (53609)	Loss/tok 3.2363 (4.7470)	LR 2.000e-03
0: TRAIN [0][1580/1938]	Time 0.342 (0.261)	Data 1.30e-04 (6.31e-04)	Tok/s 67407 (53630)	Loss/tok 3.8903 (4.7394)	LR 2.000e-03
0: TRAIN [0][1590/1938]	Time 0.415 (0.261)	Data 1.24e-04 (6.27e-04)	Tok/s 71081 (53623)	Loss/tok 4.0242 (4.7327)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.280 (0.261)	Data 1.48e-04 (6.24e-04)	Tok/s 59832 (53649)	Loss/tok 3.6095 (4.7247)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.223 (0.261)	Data 1.40e-04 (6.21e-04)	Tok/s 45467 (53627)	Loss/tok 3.3961 (4.7182)	LR 2.000e-03
0: TRAIN [0][1620/1938]	Time 0.283 (0.261)	Data 1.27e-04 (6.18e-04)	Tok/s 59283 (53643)	Loss/tok 3.5265 (4.7107)	LR 2.000e-03
0: TRAIN [0][1630/1938]	Time 0.221 (0.261)	Data 1.16e-04 (6.15e-04)	Tok/s 46310 (53643)	Loss/tok 3.3658 (4.7038)	LR 2.000e-03
0: TRAIN [0][1640/1938]	Time 0.221 (0.261)	Data 1.14e-04 (6.12e-04)	Tok/s 46399 (53636)	Loss/tok 3.3942 (4.6972)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.342 (0.261)	Data 1.20e-04 (6.09e-04)	Tok/s 67900 (53663)	Loss/tok 3.7931 (4.6903)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.285 (0.261)	Data 1.29e-04 (6.07e-04)	Tok/s 58062 (53667)	Loss/tok 3.6676 (4.6835)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.343 (0.261)	Data 1.18e-04 (6.04e-04)	Tok/s 67954 (53649)	Loss/tok 3.7813 (4.6774)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1680/1938]	Time 0.167 (0.261)	Data 1.17e-04 (6.01e-04)	Tok/s 31205 (53637)	Loss/tok 2.8448 (4.6710)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.224 (0.261)	Data 1.41e-04 (5.98e-04)	Tok/s 45527 (53629)	Loss/tok 3.2618 (4.6650)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.165 (0.261)	Data 1.20e-04 (5.95e-04)	Tok/s 32235 (53623)	Loss/tok 2.9199 (4.6586)	LR 2.000e-03
0: TRAIN [0][1710/1938]	Time 0.223 (0.261)	Data 1.17e-04 (5.92e-04)	Tok/s 45651 (53600)	Loss/tok 3.2441 (4.6526)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.164 (0.261)	Data 1.29e-04 (5.90e-04)	Tok/s 31917 (53594)	Loss/tok 2.9679 (4.6465)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.342 (0.261)	Data 1.28e-04 (5.87e-04)	Tok/s 68129 (53631)	Loss/tok 3.7180 (4.6392)	LR 2.000e-03
0: TRAIN [0][1740/1938]	Time 0.220 (0.261)	Data 1.26e-04 (5.84e-04)	Tok/s 46435 (53631)	Loss/tok 3.4949 (4.6330)	LR 2.000e-03
0: TRAIN [0][1750/1938]	Time 0.282 (0.262)	Data 1.19e-04 (5.82e-04)	Tok/s 59166 (53656)	Loss/tok 3.5604 (4.6264)	LR 2.000e-03
0: TRAIN [0][1760/1938]	Time 0.220 (0.261)	Data 1.31e-04 (5.79e-04)	Tok/s 47999 (53616)	Loss/tok 3.3514 (4.6213)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.281 (0.261)	Data 1.25e-04 (5.77e-04)	Tok/s 59733 (53603)	Loss/tok 3.4810 (4.6155)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.220 (0.261)	Data 1.19e-04 (5.75e-04)	Tok/s 47384 (53588)	Loss/tok 3.2971 (4.6099)	LR 2.000e-03
0: TRAIN [0][1790/1938]	Time 0.222 (0.261)	Data 1.24e-04 (5.72e-04)	Tok/s 46495 (53596)	Loss/tok 3.3412 (4.6039)	LR 2.000e-03
0: TRAIN [0][1800/1938]	Time 0.220 (0.261)	Data 1.30e-04 (5.70e-04)	Tok/s 46891 (53620)	Loss/tok 3.1780 (4.5974)	LR 2.000e-03
0: TRAIN [0][1810/1938]	Time 0.341 (0.262)	Data 1.41e-04 (5.67e-04)	Tok/s 69832 (53660)	Loss/tok 3.7140 (4.5908)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.279 (0.261)	Data 1.78e-04 (5.65e-04)	Tok/s 60049 (53654)	Loss/tok 3.6253 (4.5851)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.221 (0.261)	Data 1.38e-04 (5.62e-04)	Tok/s 46383 (53654)	Loss/tok 3.3382 (4.5797)	LR 2.000e-03
0: TRAIN [0][1840/1938]	Time 0.275 (0.261)	Data 1.20e-04 (5.60e-04)	Tok/s 61195 (53634)	Loss/tok 3.6373 (4.5745)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.223 (0.261)	Data 1.14e-04 (5.58e-04)	Tok/s 46117 (53626)	Loss/tok 3.3111 (4.5691)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.280 (0.261)	Data 1.18e-04 (5.55e-04)	Tok/s 60499 (53602)	Loss/tok 3.6112 (4.5642)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1870/1938]	Time 0.222 (0.261)	Data 1.36e-04 (5.53e-04)	Tok/s 47339 (53602)	Loss/tok 3.4007 (4.5589)	LR 2.000e-03
0: TRAIN [0][1880/1938]	Time 0.219 (0.261)	Data 2.56e-04 (5.51e-04)	Tok/s 46421 (53599)	Loss/tok 3.3674 (4.5536)	LR 2.000e-03
0: TRAIN [0][1890/1938]	Time 0.284 (0.261)	Data 1.16e-04 (5.49e-04)	Tok/s 59556 (53573)	Loss/tok 3.3949 (4.5486)	LR 2.000e-03
0: TRAIN [0][1900/1938]	Time 0.283 (0.261)	Data 1.20e-04 (5.46e-04)	Tok/s 59406 (53567)	Loss/tok 3.6198 (4.5433)	LR 2.000e-03
0: TRAIN [0][1910/1938]	Time 0.220 (0.261)	Data 1.23e-04 (5.44e-04)	Tok/s 46903 (53569)	Loss/tok 3.4639 (4.5380)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.282 (0.261)	Data 1.22e-04 (5.42e-04)	Tok/s 60418 (53584)	Loss/tok 3.4619 (4.5323)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.167 (0.261)	Data 1.13e-04 (5.40e-04)	Tok/s 31914 (53553)	Loss/tok 2.9698 (4.5276)	LR 2.000e-03
:::MLL 1582052833.954 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 525}}
:::MLL 1582052833.955 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.745 (0.745)	Decoder iters 149.0 (149.0)	Tok/s 22677 (22677)
0: Running moses detokenizer
0: BLEU(score=19.537198482181154, counts=[35109, 16064, 8531, 4696], totals=[67347, 64344, 61341, 58341], precisions=[52.13149806227449, 24.965808777819223, 13.907500692848176, 8.049227815772785], bp=1.0, sys_len=67347, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1582052835.932 eval_accuracy: {"value": 19.54, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 536}}
:::MLL 1582052835.932 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 0	Training Loss: 4.5230	Test BLEU: 19.54
0: Performance: Epoch: 0	Training: 428481 Tok/s
0: Finished epoch 0
:::MLL 1582052835.933 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 558}}
:::MLL 1582052835.933 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1582052835.934 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 515}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 3048488785
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][0/1938]	Time 0.932 (0.932)	Data 6.82e-01 (6.82e-01)	Tok/s 11124 (11124)	Loss/tok 3.2115 (3.2115)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.281 (0.318)	Data 9.39e-05 (6.21e-02)	Tok/s 60540 (50207)	Loss/tok 3.4789 (3.4473)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.221 (0.302)	Data 9.44e-05 (3.26e-02)	Tok/s 46609 (53121)	Loss/tok 3.1648 (3.5074)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.282 (0.290)	Data 9.82e-05 (2.21e-02)	Tok/s 60251 (53414)	Loss/tok 3.4644 (3.4955)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.165 (0.281)	Data 9.87e-05 (1.67e-02)	Tok/s 31437 (53051)	Loss/tok 2.7003 (3.4850)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.278 (0.273)	Data 1.37e-04 (1.35e-02)	Tok/s 59826 (52511)	Loss/tok 3.4917 (3.4602)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.282 (0.271)	Data 1.02e-04 (1.13e-02)	Tok/s 59541 (52710)	Loss/tok 3.4763 (3.4699)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.416 (0.276)	Data 1.11e-04 (9.72e-03)	Tok/s 71268 (53760)	Loss/tok 3.8633 (3.4899)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.344 (0.271)	Data 1.06e-04 (8.54e-03)	Tok/s 67463 (53251)	Loss/tok 3.6164 (3.4803)	LR 2.000e-03
0: TRAIN [1][90/1938]	Time 0.282 (0.270)	Data 1.13e-04 (7.62e-03)	Tok/s 58821 (53210)	Loss/tok 3.3788 (3.4728)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.219 (0.270)	Data 1.02e-04 (6.88e-03)	Tok/s 47564 (53168)	Loss/tok 3.2974 (3.4808)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.220 (0.267)	Data 1.09e-04 (6.27e-03)	Tok/s 47082 (52892)	Loss/tok 3.3403 (3.4708)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.219 (0.265)	Data 1.01e-04 (5.76e-03)	Tok/s 46027 (52617)	Loss/tok 3.2052 (3.4693)	LR 2.000e-03
0: TRAIN [1][130/1938]	Time 0.219 (0.264)	Data 1.07e-04 (5.33e-03)	Tok/s 46700 (52609)	Loss/tok 3.3328 (3.4660)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.218 (0.261)	Data 9.82e-05 (4.96e-03)	Tok/s 47306 (52123)	Loss/tok 3.2771 (3.4555)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.345 (0.260)	Data 6.59e-04 (4.64e-03)	Tok/s 67438 (51963)	Loss/tok 3.6181 (3.4505)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.220 (0.259)	Data 1.20e-04 (4.36e-03)	Tok/s 47371 (51900)	Loss/tok 3.1563 (3.4460)	LR 2.000e-03
0: TRAIN [1][170/1938]	Time 0.281 (0.258)	Data 1.22e-04 (4.11e-03)	Tok/s 59837 (51820)	Loss/tok 3.3685 (3.4387)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.280 (0.258)	Data 1.03e-04 (3.89e-03)	Tok/s 60416 (51990)	Loss/tok 3.4346 (3.4415)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.221 (0.259)	Data 1.04e-04 (3.69e-03)	Tok/s 46167 (52183)	Loss/tok 3.2364 (3.4434)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.282 (0.259)	Data 1.27e-04 (3.51e-03)	Tok/s 59327 (52241)	Loss/tok 3.4286 (3.4423)	LR 2.000e-03
0: TRAIN [1][210/1938]	Time 0.345 (0.260)	Data 1.05e-04 (3.35e-03)	Tok/s 66481 (52537)	Loss/tok 3.6598 (3.4443)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.165 (0.259)	Data 9.78e-05 (3.21e-03)	Tok/s 31575 (52467)	Loss/tok 2.6706 (3.4418)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.291 (0.259)	Data 1.13e-04 (3.07e-03)	Tok/s 57879 (52338)	Loss/tok 3.4455 (3.4405)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.231 (0.259)	Data 9.80e-05 (2.95e-03)	Tok/s 43736 (52202)	Loss/tok 3.3229 (3.4427)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.283 (0.260)	Data 1.32e-04 (2.84e-03)	Tok/s 59058 (52391)	Loss/tok 3.5076 (3.4468)	LR 2.000e-03
0: TRAIN [1][260/1938]	Time 0.281 (0.260)	Data 9.70e-05 (2.73e-03)	Tok/s 59836 (52469)	Loss/tok 3.6278 (3.4488)	LR 2.000e-03
0: TRAIN [1][270/1938]	Time 0.280 (0.260)	Data 1.45e-04 (2.64e-03)	Tok/s 61075 (52523)	Loss/tok 3.3758 (3.4475)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.220 (0.261)	Data 1.27e-04 (2.55e-03)	Tok/s 46744 (52665)	Loss/tok 3.1309 (3.4492)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.279 (0.261)	Data 1.19e-04 (2.46e-03)	Tok/s 60673 (52701)	Loss/tok 3.5070 (3.4502)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.284 (0.262)	Data 1.19e-04 (2.39e-03)	Tok/s 58595 (52804)	Loss/tok 3.5708 (3.4532)	LR 2.000e-03
0: TRAIN [1][310/1938]	Time 0.222 (0.261)	Data 1.17e-04 (2.31e-03)	Tok/s 47888 (52807)	Loss/tok 3.1838 (3.4514)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.167 (0.260)	Data 1.20e-04 (2.25e-03)	Tok/s 32045 (52642)	Loss/tok 2.7629 (3.4482)	LR 2.000e-03
0: TRAIN [1][330/1938]	Time 0.224 (0.260)	Data 1.22e-04 (2.18e-03)	Tok/s 45408 (52587)	Loss/tok 3.2891 (3.4465)	LR 2.000e-03
0: TRAIN [1][340/1938]	Time 0.166 (0.259)	Data 1.17e-04 (2.12e-03)	Tok/s 32007 (52526)	Loss/tok 2.6946 (3.4447)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.282 (0.259)	Data 1.08e-04 (2.06e-03)	Tok/s 58397 (52542)	Loss/tok 3.3925 (3.4446)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][360/1938]	Time 0.223 (0.260)	Data 1.19e-04 (2.01e-03)	Tok/s 46767 (52713)	Loss/tok 3.2786 (3.4490)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.342 (0.261)	Data 1.12e-04 (1.96e-03)	Tok/s 69267 (52801)	Loss/tok 3.7210 (3.4511)	LR 2.000e-03
0: TRAIN [1][380/1938]	Time 0.412 (0.262)	Data 1.22e-04 (1.91e-03)	Tok/s 72600 (52935)	Loss/tok 3.8899 (3.4571)	LR 2.000e-03
0: TRAIN [1][390/1938]	Time 0.220 (0.263)	Data 1.27e-04 (1.87e-03)	Tok/s 47709 (53120)	Loss/tok 3.1088 (3.4603)	LR 2.000e-03
0: TRAIN [1][400/1938]	Time 0.222 (0.263)	Data 1.26e-04 (1.82e-03)	Tok/s 46029 (53165)	Loss/tok 3.2367 (3.4601)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.285 (0.264)	Data 1.31e-04 (1.78e-03)	Tok/s 58398 (53266)	Loss/tok 3.4734 (3.4626)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.281 (0.264)	Data 1.39e-04 (1.74e-03)	Tok/s 59413 (53364)	Loss/tok 3.4090 (3.4626)	LR 2.000e-03
0: TRAIN [1][430/1938]	Time 0.223 (0.264)	Data 1.70e-04 (1.70e-03)	Tok/s 46335 (53310)	Loss/tok 3.2684 (3.4599)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.281 (0.264)	Data 1.20e-04 (1.67e-03)	Tok/s 58418 (53311)	Loss/tok 3.3929 (3.4606)	LR 2.000e-03
0: TRAIN [1][450/1938]	Time 0.221 (0.264)	Data 1.21e-04 (1.63e-03)	Tok/s 45687 (53316)	Loss/tok 3.1721 (3.4588)	LR 2.000e-03
0: TRAIN [1][460/1938]	Time 0.218 (0.263)	Data 1.26e-04 (1.60e-03)	Tok/s 49077 (53244)	Loss/tok 3.2121 (3.4567)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.222 (0.263)	Data 1.24e-04 (1.57e-03)	Tok/s 46356 (53201)	Loss/tok 3.1577 (3.4544)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.281 (0.263)	Data 1.33e-04 (1.54e-03)	Tok/s 60051 (53215)	Loss/tok 3.4340 (3.4534)	LR 2.000e-03
0: TRAIN [1][490/1938]	Time 0.280 (0.262)	Data 1.26e-04 (1.51e-03)	Tok/s 60986 (53114)	Loss/tok 3.3496 (3.4501)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.279 (0.262)	Data 1.45e-04 (1.48e-03)	Tok/s 61474 (53143)	Loss/tok 3.4037 (3.4489)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.280 (0.263)	Data 1.29e-04 (1.46e-03)	Tok/s 59569 (53279)	Loss/tok 3.3852 (3.4520)	LR 2.000e-03
0: TRAIN [1][520/1938]	Time 0.283 (0.262)	Data 1.26e-04 (1.43e-03)	Tok/s 59217 (53213)	Loss/tok 3.4256 (3.4506)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.220 (0.262)	Data 1.04e-04 (1.41e-03)	Tok/s 47678 (53203)	Loss/tok 3.1545 (3.4490)	LR 2.000e-03
0: TRAIN [1][540/1938]	Time 0.283 (0.262)	Data 1.01e-04 (1.38e-03)	Tok/s 59293 (53285)	Loss/tok 3.3668 (3.4483)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.222 (0.262)	Data 1.03e-04 (1.36e-03)	Tok/s 47075 (53339)	Loss/tok 3.1468 (3.4473)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.284 (0.262)	Data 1.22e-04 (1.34e-03)	Tok/s 59991 (53284)	Loss/tok 3.3688 (3.4460)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.219 (0.262)	Data 1.03e-04 (1.32e-03)	Tok/s 47642 (53255)	Loss/tok 3.2036 (3.4468)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][580/1938]	Time 0.219 (0.262)	Data 9.27e-05 (1.30e-03)	Tok/s 47241 (53231)	Loss/tok 3.0719 (3.4477)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.278 (0.262)	Data 1.01e-04 (1.28e-03)	Tok/s 60780 (53277)	Loss/tok 3.3855 (3.4474)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.279 (0.262)	Data 1.01e-04 (1.26e-03)	Tok/s 59965 (53246)	Loss/tok 3.5134 (3.4476)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.221 (0.262)	Data 1.05e-04 (1.24e-03)	Tok/s 47096 (53275)	Loss/tok 3.2422 (3.4468)	LR 2.000e-03
0: TRAIN [1][620/1938]	Time 0.218 (0.262)	Data 1.02e-04 (1.22e-03)	Tok/s 47581 (53288)	Loss/tok 3.1393 (3.4455)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.220 (0.262)	Data 1.15e-04 (1.20e-03)	Tok/s 46615 (53256)	Loss/tok 3.1080 (3.4448)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.279 (0.262)	Data 1.20e-04 (1.18e-03)	Tok/s 59231 (53227)	Loss/tok 3.4801 (3.4452)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.281 (0.262)	Data 9.63e-05 (1.17e-03)	Tok/s 59577 (53232)	Loss/tok 3.3908 (3.4442)	LR 2.000e-03
0: TRAIN [1][660/1938]	Time 0.284 (0.262)	Data 9.89e-05 (1.15e-03)	Tok/s 59942 (53298)	Loss/tok 3.4173 (3.4451)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.284 (0.262)	Data 1.16e-04 (1.14e-03)	Tok/s 59572 (53327)	Loss/tok 3.3975 (3.4441)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.220 (0.262)	Data 9.70e-05 (1.12e-03)	Tok/s 46572 (53377)	Loss/tok 3.2561 (3.4440)	LR 2.000e-03
0: TRAIN [1][690/1938]	Time 0.278 (0.262)	Data 9.56e-05 (1.11e-03)	Tok/s 60393 (53370)	Loss/tok 3.4617 (3.4426)	LR 2.000e-03
0: TRAIN [1][700/1938]	Time 0.221 (0.261)	Data 1.09e-04 (1.09e-03)	Tok/s 46387 (53233)	Loss/tok 3.1217 (3.4401)	LR 2.000e-03
0: TRAIN [1][710/1938]	Time 0.285 (0.261)	Data 9.56e-05 (1.08e-03)	Tok/s 59635 (53239)	Loss/tok 3.5222 (3.4391)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.280 (0.261)	Data 1.08e-04 (1.06e-03)	Tok/s 60657 (53214)	Loss/tok 3.4203 (3.4379)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.220 (0.261)	Data 1.02e-04 (1.05e-03)	Tok/s 47341 (53140)	Loss/tok 3.2780 (3.4361)	LR 2.000e-03
0: TRAIN [1][740/1938]	Time 0.220 (0.261)	Data 1.27e-04 (1.04e-03)	Tok/s 47248 (53122)	Loss/tok 3.1682 (3.4349)	LR 2.000e-03
0: TRAIN [1][750/1938]	Time 0.419 (0.261)	Data 1.08e-04 (1.03e-03)	Tok/s 70138 (53208)	Loss/tok 3.9183 (3.4369)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.280 (0.262)	Data 9.37e-05 (1.01e-03)	Tok/s 59591 (53303)	Loss/tok 3.4138 (3.4381)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.224 (0.262)	Data 1.20e-04 (1.00e-03)	Tok/s 45929 (53366)	Loss/tok 3.1993 (3.4403)	LR 2.000e-03
0: TRAIN [1][780/1938]	Time 0.221 (0.262)	Data 9.61e-05 (9.91e-04)	Tok/s 46848 (53368)	Loss/tok 3.2214 (3.4395)	LR 2.000e-03
0: TRAIN [1][790/1938]	Time 0.283 (0.262)	Data 9.75e-05 (9.80e-04)	Tok/s 59699 (53374)	Loss/tok 3.3752 (3.4382)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][800/1938]	Time 0.221 (0.262)	Data 1.04e-04 (9.69e-04)	Tok/s 46069 (53430)	Loss/tok 3.2258 (3.4401)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.277 (0.262)	Data 1.17e-04 (9.58e-04)	Tok/s 60542 (53462)	Loss/tok 3.4520 (3.4397)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.282 (0.262)	Data 1.08e-04 (9.48e-04)	Tok/s 59651 (53428)	Loss/tok 3.4020 (3.4390)	LR 2.000e-03
0: TRAIN [1][830/1938]	Time 0.221 (0.262)	Data 9.87e-05 (9.38e-04)	Tok/s 45897 (53363)	Loss/tok 3.1042 (3.4369)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.166 (0.262)	Data 1.16e-04 (9.28e-04)	Tok/s 31616 (53314)	Loss/tok 2.6141 (3.4356)	LR 2.000e-03
0: TRAIN [1][850/1938]	Time 0.342 (0.262)	Data 1.09e-04 (9.18e-04)	Tok/s 67365 (53372)	Loss/tok 3.6697 (3.4356)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.341 (0.262)	Data 1.12e-04 (9.09e-04)	Tok/s 69256 (53383)	Loss/tok 3.4881 (3.4345)	LR 2.000e-03
0: TRAIN [1][870/1938]	Time 0.415 (0.262)	Data 9.75e-05 (9.00e-04)	Tok/s 72104 (53421)	Loss/tok 3.8176 (3.4354)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.220 (0.262)	Data 9.99e-05 (8.91e-04)	Tok/s 47374 (53405)	Loss/tok 3.2166 (3.4346)	LR 2.000e-03
0: TRAIN [1][890/1938]	Time 0.279 (0.262)	Data 1.03e-04 (8.82e-04)	Tok/s 60659 (53437)	Loss/tok 3.3980 (3.4342)	LR 2.000e-03
0: TRAIN [1][900/1938]	Time 0.279 (0.262)	Data 1.21e-04 (8.74e-04)	Tok/s 60746 (53428)	Loss/tok 3.3380 (3.4337)	LR 2.000e-03
0: TRAIN [1][910/1938]	Time 0.282 (0.262)	Data 9.35e-05 (8.65e-04)	Tok/s 59788 (53509)	Loss/tok 3.3936 (3.4342)	LR 2.000e-03
0: TRAIN [1][920/1938]	Time 0.167 (0.262)	Data 1.15e-04 (8.57e-04)	Tok/s 31681 (53516)	Loss/tok 2.6648 (3.4338)	LR 2.000e-03
0: TRAIN [1][930/1938]	Time 0.220 (0.263)	Data 1.23e-04 (8.49e-04)	Tok/s 46594 (53580)	Loss/tok 3.1413 (3.4349)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.416 (0.263)	Data 1.12e-04 (8.41e-04)	Tok/s 70718 (53633)	Loss/tok 3.7701 (3.4354)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.280 (0.263)	Data 1.07e-04 (8.33e-04)	Tok/s 59988 (53595)	Loss/tok 3.4256 (3.4343)	LR 2.000e-03
0: TRAIN [1][960/1938]	Time 0.165 (0.262)	Data 1.12e-04 (8.26e-04)	Tok/s 31962 (53503)	Loss/tok 2.6460 (3.4327)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.222 (0.262)	Data 1.04e-04 (8.18e-04)	Tok/s 45530 (53505)	Loss/tok 3.0812 (3.4318)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][980/1938]	Time 0.342 (0.263)	Data 9.23e-05 (8.11e-04)	Tok/s 67794 (53543)	Loss/tok 3.5569 (3.4328)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.280 (0.263)	Data 1.01e-04 (8.04e-04)	Tok/s 60742 (53547)	Loss/tok 3.2466 (3.4321)	LR 2.000e-03
0: TRAIN [1][1000/1938]	Time 0.220 (0.263)	Data 1.05e-04 (7.97e-04)	Tok/s 45787 (53585)	Loss/tok 3.2343 (3.4324)	LR 2.000e-03
0: TRAIN [1][1010/1938]	Time 0.223 (0.263)	Data 1.15e-04 (7.90e-04)	Tok/s 46099 (53596)	Loss/tok 3.2125 (3.4315)	LR 2.000e-03
0: TRAIN [1][1020/1938]	Time 0.219 (0.263)	Data 1.09e-04 (7.83e-04)	Tok/s 47109 (53608)	Loss/tok 3.1638 (3.4331)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.285 (0.263)	Data 9.75e-05 (7.77e-04)	Tok/s 57560 (53607)	Loss/tok 3.4381 (3.4330)	LR 2.000e-03
0: TRAIN [1][1040/1938]	Time 0.223 (0.263)	Data 1.30e-04 (7.70e-04)	Tok/s 46854 (53603)	Loss/tok 3.1444 (3.4319)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.220 (0.263)	Data 9.78e-05 (7.64e-04)	Tok/s 47425 (53612)	Loss/tok 3.0904 (3.4318)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.219 (0.263)	Data 1.03e-04 (7.58e-04)	Tok/s 47187 (53591)	Loss/tok 3.1236 (3.4315)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.281 (0.263)	Data 1.01e-04 (7.52e-04)	Tok/s 59480 (53551)	Loss/tok 3.2921 (3.4302)	LR 2.000e-03
0: TRAIN [1][1080/1938]	Time 0.285 (0.263)	Data 1.41e-04 (7.46e-04)	Tok/s 59659 (53532)	Loss/tok 3.5261 (3.4294)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.219 (0.262)	Data 9.61e-05 (7.40e-04)	Tok/s 46773 (53498)	Loss/tok 3.1082 (3.4282)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.219 (0.262)	Data 9.92e-05 (7.34e-04)	Tok/s 46932 (53430)	Loss/tok 3.0533 (3.4270)	LR 2.000e-03
0: TRAIN [1][1110/1938]	Time 0.282 (0.262)	Data 1.05e-04 (7.28e-04)	Tok/s 59676 (53467)	Loss/tok 3.4484 (3.4273)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.413 (0.263)	Data 1.18e-04 (7.23e-04)	Tok/s 72003 (53490)	Loss/tok 3.7366 (3.4284)	LR 2.000e-03
0: TRAIN [1][1130/1938]	Time 0.219 (0.263)	Data 1.02e-04 (7.17e-04)	Tok/s 47134 (53501)	Loss/tok 3.1779 (3.4282)	LR 2.000e-03
0: TRAIN [1][1140/1938]	Time 0.222 (0.263)	Data 1.04e-04 (7.12e-04)	Tok/s 46306 (53507)	Loss/tok 3.1724 (3.4283)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.280 (0.262)	Data 1.19e-04 (7.07e-04)	Tok/s 60248 (53484)	Loss/tok 3.4068 (3.4272)	LR 2.000e-03
0: TRAIN [1][1160/1938]	Time 0.223 (0.262)	Data 9.82e-05 (7.02e-04)	Tok/s 44421 (53451)	Loss/tok 3.2310 (3.4264)	LR 2.000e-03
0: TRAIN [1][1170/1938]	Time 0.224 (0.262)	Data 1.18e-04 (6.97e-04)	Tok/s 45697 (53486)	Loss/tok 3.2091 (3.4266)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.284 (0.262)	Data 1.15e-04 (6.92e-04)	Tok/s 58971 (53469)	Loss/tok 3.2112 (3.4253)	LR 2.000e-03
0: TRAIN [1][1190/1938]	Time 0.341 (0.262)	Data 1.01e-04 (6.87e-04)	Tok/s 68228 (53502)	Loss/tok 3.5207 (3.4255)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.281 (0.263)	Data 1.04e-04 (6.82e-04)	Tok/s 59553 (53548)	Loss/tok 3.3938 (3.4258)	LR 2.000e-03
0: TRAIN [1][1210/1938]	Time 0.284 (0.263)	Data 1.08e-04 (6.77e-04)	Tok/s 59344 (53568)	Loss/tok 3.3304 (3.4255)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.279 (0.263)	Data 9.80e-05 (6.73e-04)	Tok/s 60507 (53598)	Loss/tok 3.4077 (3.4258)	LR 2.000e-03
0: TRAIN [1][1230/1938]	Time 0.221 (0.263)	Data 9.44e-05 (6.68e-04)	Tok/s 47496 (53538)	Loss/tok 3.1653 (3.4251)	LR 2.000e-03
0: TRAIN [1][1240/1938]	Time 0.221 (0.262)	Data 9.73e-05 (6.63e-04)	Tok/s 47809 (53520)	Loss/tok 3.1358 (3.4244)	LR 2.000e-03
0: TRAIN [1][1250/1938]	Time 0.166 (0.262)	Data 1.42e-04 (6.59e-04)	Tok/s 31446 (53459)	Loss/tok 2.7823 (3.4233)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.223 (0.262)	Data 1.42e-04 (6.55e-04)	Tok/s 45754 (53464)	Loss/tok 3.0841 (3.4231)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.223 (0.262)	Data 1.14e-04 (6.51e-04)	Tok/s 46836 (53463)	Loss/tok 3.1168 (3.4222)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1280/1938]	Time 0.341 (0.262)	Data 9.80e-05 (6.46e-04)	Tok/s 68142 (53482)	Loss/tok 3.5444 (3.4225)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1290/1938]	Time 0.281 (0.263)	Data 9.49e-05 (6.42e-04)	Tok/s 60048 (53534)	Loss/tok 3.3011 (3.4232)	LR 2.000e-03
0: TRAIN [1][1300/1938]	Time 0.413 (0.263)	Data 1.07e-04 (6.38e-04)	Tok/s 71934 (53521)	Loss/tok 3.7136 (3.4226)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.282 (0.262)	Data 1.51e-04 (6.34e-04)	Tok/s 59629 (53485)	Loss/tok 3.2858 (3.4213)	LR 2.000e-03
0: TRAIN [1][1320/1938]	Time 0.223 (0.262)	Data 9.63e-05 (6.30e-04)	Tok/s 46982 (53456)	Loss/tok 3.2780 (3.4207)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.278 (0.262)	Data 9.73e-05 (6.26e-04)	Tok/s 60495 (53429)	Loss/tok 3.3655 (3.4201)	LR 2.000e-03
0: TRAIN [1][1340/1938]	Time 0.344 (0.262)	Data 1.23e-04 (6.22e-04)	Tok/s 67097 (53439)	Loss/tok 3.5563 (3.4196)	LR 2.000e-03
0: TRAIN [1][1350/1938]	Time 0.219 (0.262)	Data 1.41e-04 (6.18e-04)	Tok/s 46251 (53417)	Loss/tok 3.1931 (3.4186)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.279 (0.262)	Data 1.07e-04 (6.15e-04)	Tok/s 59420 (53411)	Loss/tok 3.4635 (3.4178)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.223 (0.262)	Data 1.02e-04 (6.11e-04)	Tok/s 45814 (53397)	Loss/tok 3.0582 (3.4170)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.285 (0.262)	Data 1.29e-04 (6.07e-04)	Tok/s 59091 (53397)	Loss/tok 3.3014 (3.4165)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.345 (0.262)	Data 1.31e-04 (6.04e-04)	Tok/s 68583 (53392)	Loss/tok 3.5225 (3.4160)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.284 (0.262)	Data 1.28e-04 (6.00e-04)	Tok/s 58672 (53400)	Loss/tok 3.3810 (3.4157)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.280 (0.262)	Data 1.24e-04 (5.97e-04)	Tok/s 60778 (53398)	Loss/tok 3.2473 (3.4149)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.281 (0.262)	Data 1.18e-04 (5.94e-04)	Tok/s 60186 (53406)	Loss/tok 3.3965 (3.4145)	LR 2.000e-03
0: TRAIN [1][1430/1938]	Time 0.284 (0.262)	Data 1.03e-04 (5.90e-04)	Tok/s 59476 (53401)	Loss/tok 3.2937 (3.4136)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.220 (0.262)	Data 9.89e-05 (5.87e-04)	Tok/s 46374 (53409)	Loss/tok 3.1129 (3.4131)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1450/1938]	Time 0.417 (0.262)	Data 9.56e-05 (5.84e-04)	Tok/s 71177 (53441)	Loss/tok 3.7283 (3.4135)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1460/1938]	Time 0.347 (0.262)	Data 1.44e-04 (5.80e-04)	Tok/s 67545 (53449)	Loss/tok 3.5845 (3.4135)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.230 (0.262)	Data 1.54e-04 (5.77e-04)	Tok/s 44745 (53456)	Loss/tok 3.1023 (3.4138)	LR 2.000e-03
0: TRAIN [1][1480/1938]	Time 0.228 (0.262)	Data 1.54e-04 (5.74e-04)	Tok/s 44979 (53471)	Loss/tok 3.2175 (3.4136)	LR 2.000e-03
0: TRAIN [1][1490/1938]	Time 0.286 (0.262)	Data 1.58e-04 (5.72e-04)	Tok/s 58818 (53476)	Loss/tok 3.2972 (3.4133)	LR 2.000e-03
0: TRAIN [1][1500/1938]	Time 0.345 (0.262)	Data 1.45e-04 (5.69e-04)	Tok/s 67989 (53514)	Loss/tok 3.5090 (3.4132)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.219 (0.263)	Data 1.44e-04 (5.66e-04)	Tok/s 47652 (53542)	Loss/tok 3.1205 (3.4135)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.284 (0.262)	Data 1.13e-04 (5.63e-04)	Tok/s 58583 (53531)	Loss/tok 3.3408 (3.4125)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.218 (0.262)	Data 1.33e-04 (5.60e-04)	Tok/s 46604 (53479)	Loss/tok 3.2627 (3.4112)	LR 2.000e-03
0: TRAIN [1][1540/1938]	Time 0.282 (0.262)	Data 1.29e-04 (5.57e-04)	Tok/s 59276 (53457)	Loss/tok 3.4083 (3.4108)	LR 2.000e-03
0: TRAIN [1][1550/1938]	Time 0.414 (0.262)	Data 1.21e-04 (5.54e-04)	Tok/s 71669 (53460)	Loss/tok 3.6492 (3.4103)	LR 2.000e-03
0: TRAIN [1][1560/1938]	Time 0.342 (0.262)	Data 1.21e-04 (5.51e-04)	Tok/s 68207 (53421)	Loss/tok 3.6289 (3.4095)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.218 (0.262)	Data 1.25e-04 (5.49e-04)	Tok/s 47127 (53391)	Loss/tok 3.1563 (3.4087)	LR 2.000e-03
0: TRAIN [1][1580/1938]	Time 0.280 (0.262)	Data 1.17e-04 (5.46e-04)	Tok/s 59773 (53346)	Loss/tok 3.3805 (3.4078)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1590/1938]	Time 0.222 (0.262)	Data 9.92e-05 (5.43e-04)	Tok/s 47395 (53347)	Loss/tok 3.0748 (3.4074)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.224 (0.261)	Data 1.12e-04 (5.41e-04)	Tok/s 45436 (53336)	Loss/tok 3.1831 (3.4064)	LR 2.000e-03
0: TRAIN [1][1610/1938]	Time 0.281 (0.261)	Data 1.26e-04 (5.38e-04)	Tok/s 59802 (53334)	Loss/tok 3.3885 (3.4066)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.416 (0.262)	Data 1.44e-04 (5.36e-04)	Tok/s 71188 (53351)	Loss/tok 3.6422 (3.4063)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.224 (0.262)	Data 1.27e-04 (5.33e-04)	Tok/s 45658 (53360)	Loss/tok 3.1425 (3.4060)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.220 (0.261)	Data 1.53e-04 (5.31e-04)	Tok/s 47463 (53352)	Loss/tok 3.1117 (3.4051)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.221 (0.262)	Data 1.40e-04 (5.28e-04)	Tok/s 46992 (53387)	Loss/tok 3.1414 (3.4049)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.167 (0.262)	Data 1.02e-04 (5.26e-04)	Tok/s 31268 (53385)	Loss/tok 2.7249 (3.4043)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.217 (0.261)	Data 1.18e-04 (5.23e-04)	Tok/s 47492 (53340)	Loss/tok 3.0836 (3.4033)	LR 2.000e-03
0: TRAIN [1][1680/1938]	Time 0.221 (0.261)	Data 9.70e-05 (5.21e-04)	Tok/s 46179 (53313)	Loss/tok 3.0863 (3.4029)	LR 2.000e-03
0: TRAIN [1][1690/1938]	Time 0.281 (0.261)	Data 9.97e-05 (5.18e-04)	Tok/s 59595 (53299)	Loss/tok 3.2981 (3.4023)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.221 (0.261)	Data 1.27e-04 (5.16e-04)	Tok/s 47512 (53307)	Loss/tok 3.2363 (3.4023)	LR 2.000e-03
0: TRAIN [1][1710/1938]	Time 0.343 (0.261)	Data 1.58e-04 (5.14e-04)	Tok/s 67849 (53317)	Loss/tok 3.5447 (3.4018)	LR 2.000e-03
0: TRAIN [1][1720/1938]	Time 0.283 (0.261)	Data 1.03e-04 (5.11e-04)	Tok/s 58416 (53317)	Loss/tok 3.3715 (3.4014)	LR 2.000e-03
0: TRAIN [1][1730/1938]	Time 0.223 (0.261)	Data 1.12e-04 (5.09e-04)	Tok/s 46140 (53313)	Loss/tok 3.1397 (3.4007)	LR 2.000e-03
0: TRAIN [1][1740/1938]	Time 0.221 (0.261)	Data 1.25e-04 (5.07e-04)	Tok/s 46208 (53342)	Loss/tok 3.1178 (3.4005)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1750/1938]	Time 0.416 (0.261)	Data 1.25e-04 (5.05e-04)	Tok/s 69793 (53360)	Loss/tok 3.7460 (3.4004)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1760/1938]	Time 0.344 (0.261)	Data 1.24e-04 (5.03e-04)	Tok/s 68178 (53351)	Loss/tok 3.4857 (3.4003)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.167 (0.261)	Data 1.09e-04 (5.01e-04)	Tok/s 31071 (53345)	Loss/tok 2.5743 (3.3998)	LR 2.000e-03
0: TRAIN [1][1780/1938]	Time 0.282 (0.261)	Data 1.46e-04 (4.98e-04)	Tok/s 59207 (53366)	Loss/tok 3.4169 (3.3995)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.345 (0.261)	Data 9.68e-05 (4.96e-04)	Tok/s 67886 (53359)	Loss/tok 3.5080 (3.3990)	LR 2.000e-03
0: TRAIN [1][1800/1938]	Time 0.166 (0.261)	Data 9.75e-05 (4.94e-04)	Tok/s 31688 (53341)	Loss/tok 2.7132 (3.3983)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.284 (0.261)	Data 9.63e-05 (4.92e-04)	Tok/s 60070 (53340)	Loss/tok 3.3176 (3.3978)	LR 2.000e-03
0: TRAIN [1][1820/1938]	Time 0.163 (0.261)	Data 1.18e-04 (4.90e-04)	Tok/s 32604 (53335)	Loss/tok 2.6774 (3.3975)	LR 2.000e-03
0: TRAIN [1][1830/1938]	Time 0.223 (0.261)	Data 1.43e-04 (4.88e-04)	Tok/s 46605 (53358)	Loss/tok 3.0480 (3.3976)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.222 (0.261)	Data 1.16e-04 (4.86e-04)	Tok/s 46023 (53371)	Loss/tok 3.1889 (3.3973)	LR 2.000e-03
0: TRAIN [1][1850/1938]	Time 0.345 (0.261)	Data 1.49e-04 (4.84e-04)	Tok/s 66963 (53382)	Loss/tok 3.5667 (3.3974)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.281 (0.261)	Data 1.46e-04 (4.82e-04)	Tok/s 59912 (53346)	Loss/tok 3.3245 (3.3965)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.220 (0.261)	Data 1.30e-04 (4.80e-04)	Tok/s 46807 (53349)	Loss/tok 3.0560 (3.3965)	LR 2.000e-03
0: TRAIN [1][1880/1938]	Time 0.350 (0.261)	Data 1.49e-04 (4.78e-04)	Tok/s 65881 (53348)	Loss/tok 3.7122 (3.3966)	LR 2.000e-03
0: TRAIN [1][1890/1938]	Time 0.169 (0.262)	Data 1.49e-04 (4.77e-04)	Tok/s 31555 (53369)	Loss/tok 2.6871 (3.3965)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.282 (0.262)	Data 1.46e-04 (4.75e-04)	Tok/s 60550 (53343)	Loss/tok 3.2727 (3.3957)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.282 (0.262)	Data 1.28e-04 (4.73e-04)	Tok/s 59792 (53362)	Loss/tok 3.2282 (3.3957)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.165 (0.262)	Data 9.63e-05 (4.71e-04)	Tok/s 31987 (53364)	Loss/tok 2.7177 (3.3952)	LR 2.000e-03
0: TRAIN [1][1930/1938]	Time 0.281 (0.262)	Data 1.01e-04 (4.69e-04)	Tok/s 60799 (53376)	Loss/tok 3.2505 (3.3950)	LR 2.000e-03
:::MLL 1582053343.867 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 525}}
:::MLL 1582053343.868 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.712 (0.712)	Decoder iters 149.0 (149.0)	Tok/s 23054 (23054)
0: Running moses detokenizer
0: BLEU(score=22.02136370607071, counts=[36276, 17527, 9653, 5552], totals=[66292, 63289, 60286, 57287], precisions=[54.7215350268509, 27.693596043546272, 16.012009421756296, 9.691553057412676], bp=1.0, sys_len=66292, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1582053345.741 eval_accuracy: {"value": 22.02, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 536}}
:::MLL 1582053345.741 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 1	Training Loss: 3.3965	Test BLEU: 22.02
0: Performance: Epoch: 1	Training: 427152 Tok/s
0: Finished epoch 1
:::MLL 1582053345.742 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 558}}
:::MLL 1582053345.742 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1582053345.743 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 515}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 1225622457
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][0/1938]	Time 0.930 (0.930)	Data 7.13e-01 (7.13e-01)	Tok/s 10841 (10841)	Loss/tok 3.0789 (3.0789)	LR 2.000e-03
0: TRAIN [2][10/1938]	Time 0.344 (0.353)	Data 1.14e-04 (6.49e-02)	Tok/s 68011 (55330)	Loss/tok 3.4486 (3.3151)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.220 (0.304)	Data 1.02e-04 (3.41e-02)	Tok/s 47444 (54397)	Loss/tok 3.1284 (3.2521)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.284 (0.288)	Data 1.18e-04 (2.31e-02)	Tok/s 58292 (53780)	Loss/tok 3.3037 (3.2422)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.222 (0.279)	Data 1.27e-04 (1.75e-02)	Tok/s 46205 (53446)	Loss/tok 3.0106 (3.2381)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.280 (0.277)	Data 1.70e-04 (1.41e-02)	Tok/s 60519 (53764)	Loss/tok 3.3100 (3.2548)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.167 (0.273)	Data 1.30e-04 (1.18e-02)	Tok/s 30557 (53494)	Loss/tok 2.7760 (3.2493)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.342 (0.272)	Data 1.25e-04 (1.02e-02)	Tok/s 69712 (53630)	Loss/tok 3.2839 (3.2486)	LR 2.000e-03
0: TRAIN [2][80/1938]	Time 0.280 (0.272)	Data 1.45e-04 (8.93e-03)	Tok/s 60402 (53782)	Loss/tok 3.2247 (3.2503)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.221 (0.270)	Data 1.25e-04 (7.96e-03)	Tok/s 47238 (53584)	Loss/tok 3.0794 (3.2477)	LR 2.000e-03
0: TRAIN [2][100/1938]	Time 0.221 (0.267)	Data 1.22e-04 (7.19e-03)	Tok/s 47822 (53270)	Loss/tok 3.0117 (3.2479)	LR 2.000e-03
0: TRAIN [2][110/1938]	Time 0.341 (0.271)	Data 1.17e-04 (6.55e-03)	Tok/s 67745 (53857)	Loss/tok 3.4643 (3.2608)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.166 (0.269)	Data 1.66e-04 (6.02e-03)	Tok/s 31059 (53674)	Loss/tok 2.6006 (3.2570)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.282 (0.266)	Data 1.17e-04 (5.57e-03)	Tok/s 58733 (53420)	Loss/tok 3.1320 (3.2504)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.221 (0.265)	Data 1.21e-04 (5.19e-03)	Tok/s 46946 (53369)	Loss/tok 3.0988 (3.2469)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.346 (0.265)	Data 1.14e-04 (4.85e-03)	Tok/s 68335 (53408)	Loss/tok 3.3554 (3.2485)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.220 (0.266)	Data 1.18e-04 (4.56e-03)	Tok/s 46489 (53582)	Loss/tok 3.0174 (3.2469)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.220 (0.265)	Data 1.42e-04 (4.30e-03)	Tok/s 48076 (53474)	Loss/tok 3.1259 (3.2468)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.344 (0.266)	Data 1.18e-04 (4.07e-03)	Tok/s 68680 (53610)	Loss/tok 3.4476 (3.2522)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.220 (0.264)	Data 1.19e-04 (3.86e-03)	Tok/s 46495 (53442)	Loss/tok 2.9366 (3.2480)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.338 (0.264)	Data 1.34e-04 (3.68e-03)	Tok/s 68794 (53488)	Loss/tok 3.4122 (3.2481)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][210/1938]	Time 0.412 (0.265)	Data 1.26e-04 (3.51e-03)	Tok/s 72714 (53564)	Loss/tok 3.6184 (3.2500)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][220/1938]	Time 0.283 (0.266)	Data 1.50e-04 (3.35e-03)	Tok/s 58946 (53811)	Loss/tok 3.2120 (3.2536)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.222 (0.266)	Data 1.23e-04 (3.22e-03)	Tok/s 45966 (53783)	Loss/tok 3.0734 (3.2534)	LR 2.000e-03
0: TRAIN [2][240/1938]	Time 0.343 (0.266)	Data 1.14e-04 (3.09e-03)	Tok/s 68576 (53824)	Loss/tok 3.3613 (3.2527)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.221 (0.265)	Data 1.28e-04 (2.97e-03)	Tok/s 47256 (53717)	Loss/tok 3.1325 (3.2512)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.167 (0.264)	Data 1.38e-04 (2.86e-03)	Tok/s 32231 (53650)	Loss/tok 2.6578 (3.2489)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.219 (0.264)	Data 1.17e-04 (2.76e-03)	Tok/s 46766 (53637)	Loss/tok 3.1718 (3.2477)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.220 (0.264)	Data 1.54e-04 (2.67e-03)	Tok/s 46705 (53643)	Loss/tok 3.1699 (3.2471)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.167 (0.264)	Data 1.30e-04 (2.58e-03)	Tok/s 30594 (53666)	Loss/tok 2.6709 (3.2507)	LR 2.000e-03
0: TRAIN [2][300/1938]	Time 0.222 (0.264)	Data 1.29e-04 (2.50e-03)	Tok/s 47004 (53694)	Loss/tok 3.1563 (3.2514)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.223 (0.264)	Data 1.34e-04 (2.42e-03)	Tok/s 46070 (53652)	Loss/tok 3.0530 (3.2522)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.219 (0.263)	Data 1.25e-04 (2.35e-03)	Tok/s 48191 (53537)	Loss/tok 2.9357 (3.2522)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.221 (0.263)	Data 1.27e-04 (2.28e-03)	Tok/s 46908 (53440)	Loss/tok 2.9836 (3.2492)	LR 2.000e-03
0: TRAIN [2][340/1938]	Time 0.282 (0.263)	Data 1.25e-04 (2.22e-03)	Tok/s 59238 (53474)	Loss/tok 3.1838 (3.2504)	LR 2.000e-03
0: TRAIN [2][350/1938]	Time 0.222 (0.263)	Data 1.30e-04 (2.16e-03)	Tok/s 47084 (53527)	Loss/tok 2.9535 (3.2549)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][360/1938]	Time 0.340 (0.263)	Data 1.26e-04 (2.10e-03)	Tok/s 68849 (53511)	Loss/tok 3.4429 (3.2549)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.281 (0.263)	Data 1.26e-04 (2.05e-03)	Tok/s 58490 (53525)	Loss/tok 3.2749 (3.2554)	LR 2.000e-03
0: TRAIN [2][380/1938]	Time 0.343 (0.264)	Data 1.15e-04 (2.00e-03)	Tok/s 68034 (53654)	Loss/tok 3.3464 (3.2572)	LR 2.000e-03
0: TRAIN [2][390/1938]	Time 0.284 (0.263)	Data 1.37e-04 (1.95e-03)	Tok/s 59013 (53519)	Loss/tok 3.2556 (3.2567)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.280 (0.264)	Data 1.31e-04 (1.91e-03)	Tok/s 60149 (53525)	Loss/tok 3.2119 (3.2572)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.167 (0.262)	Data 1.15e-04 (1.86e-03)	Tok/s 31353 (53306)	Loss/tok 2.5565 (3.2545)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.283 (0.262)	Data 1.36e-04 (1.82e-03)	Tok/s 59049 (53314)	Loss/tok 3.3044 (3.2554)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.282 (0.262)	Data 1.41e-04 (1.78e-03)	Tok/s 59644 (53229)	Loss/tok 3.1982 (3.2552)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.166 (0.262)	Data 1.22e-04 (1.74e-03)	Tok/s 31143 (53237)	Loss/tok 2.6616 (3.2558)	LR 2.000e-03
0: TRAIN [2][450/1938]	Time 0.343 (0.262)	Data 1.25e-04 (1.71e-03)	Tok/s 66903 (53228)	Loss/tok 3.4334 (3.2579)	LR 2.000e-03
0: TRAIN [2][460/1938]	Time 0.219 (0.262)	Data 1.31e-04 (1.67e-03)	Tok/s 47828 (53232)	Loss/tok 2.9527 (3.2575)	LR 2.000e-03
0: TRAIN [2][470/1938]	Time 0.219 (0.262)	Data 1.43e-04 (1.64e-03)	Tok/s 47401 (53311)	Loss/tok 3.0167 (3.2591)	LR 2.000e-03
0: TRAIN [2][480/1938]	Time 0.279 (0.262)	Data 1.33e-04 (1.61e-03)	Tok/s 60207 (53275)	Loss/tok 3.2845 (3.2586)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.284 (0.263)	Data 1.15e-04 (1.58e-03)	Tok/s 59113 (53371)	Loss/tok 3.2614 (3.2601)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.280 (0.262)	Data 1.31e-04 (1.55e-03)	Tok/s 60371 (53344)	Loss/tok 3.2817 (3.2595)	LR 2.000e-03
0: TRAIN [2][510/1938]	Time 0.345 (0.263)	Data 1.41e-04 (1.52e-03)	Tok/s 68276 (53428)	Loss/tok 3.4412 (3.2618)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][520/1938]	Time 0.344 (0.263)	Data 1.16e-04 (1.50e-03)	Tok/s 67308 (53508)	Loss/tok 3.5517 (3.2632)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.417 (0.263)	Data 1.41e-04 (1.47e-03)	Tok/s 71261 (53515)	Loss/tok 3.6629 (3.2639)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.284 (0.264)	Data 1.13e-04 (1.45e-03)	Tok/s 58419 (53599)	Loss/tok 3.1957 (3.2644)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.280 (0.264)	Data 1.53e-04 (1.42e-03)	Tok/s 59153 (53613)	Loss/tok 3.4020 (3.2642)	LR 2.000e-03
0: TRAIN [2][560/1938]	Time 0.279 (0.264)	Data 1.15e-04 (1.40e-03)	Tok/s 59292 (53688)	Loss/tok 3.3375 (3.2654)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.417 (0.265)	Data 1.44e-04 (1.38e-03)	Tok/s 72092 (53780)	Loss/tok 3.5754 (3.2673)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.278 (0.265)	Data 1.13e-04 (1.36e-03)	Tok/s 60630 (53785)	Loss/tok 3.3981 (3.2703)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.221 (0.265)	Data 1.25e-04 (1.33e-03)	Tok/s 47140 (53757)	Loss/tok 2.9561 (3.2689)	LR 2.000e-03
0: TRAIN [2][600/1938]	Time 0.165 (0.264)	Data 1.20e-04 (1.31e-03)	Tok/s 31844 (53645)	Loss/tok 2.6485 (3.2672)	LR 2.000e-03
0: TRAIN [2][610/1938]	Time 0.282 (0.264)	Data 1.24e-04 (1.30e-03)	Tok/s 60855 (53590)	Loss/tok 3.2273 (3.2657)	LR 2.000e-03
0: TRAIN [2][620/1938]	Time 0.219 (0.264)	Data 1.45e-04 (1.28e-03)	Tok/s 47472 (53587)	Loss/tok 3.0433 (3.2651)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.278 (0.264)	Data 1.16e-04 (1.26e-03)	Tok/s 60614 (53622)	Loss/tok 3.2137 (3.2647)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.220 (0.263)	Data 1.28e-04 (1.24e-03)	Tok/s 47943 (53545)	Loss/tok 3.0498 (3.2632)	LR 2.000e-03
0: TRAIN [2][650/1938]	Time 0.343 (0.263)	Data 1.18e-04 (1.22e-03)	Tok/s 67015 (53562)	Loss/tok 3.4584 (3.2631)	LR 2.000e-03
0: TRAIN [2][660/1938]	Time 0.282 (0.264)	Data 1.60e-04 (1.21e-03)	Tok/s 58915 (53602)	Loss/tok 3.1319 (3.2636)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.223 (0.263)	Data 1.09e-04 (1.19e-03)	Tok/s 45748 (53498)	Loss/tok 3.0232 (3.2615)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.280 (0.263)	Data 1.18e-04 (1.18e-03)	Tok/s 59548 (53505)	Loss/tok 3.2221 (3.2628)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.220 (0.263)	Data 1.39e-04 (1.16e-03)	Tok/s 47288 (53534)	Loss/tok 3.0781 (3.2621)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.220 (0.263)	Data 1.21e-04 (1.15e-03)	Tok/s 47268 (53549)	Loss/tok 3.1566 (3.2621)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.166 (0.263)	Data 1.22e-04 (1.13e-03)	Tok/s 31512 (53535)	Loss/tok 2.5811 (3.2615)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.222 (0.263)	Data 1.23e-04 (1.12e-03)	Tok/s 46678 (53525)	Loss/tok 3.0394 (3.2610)	LR 2.000e-03
0: TRAIN [2][730/1938]	Time 0.164 (0.262)	Data 1.29e-04 (1.10e-03)	Tok/s 32577 (53445)	Loss/tok 2.6139 (3.2594)	LR 2.000e-03
0: TRAIN [2][740/1938]	Time 0.342 (0.262)	Data 1.55e-04 (1.09e-03)	Tok/s 68092 (53485)	Loss/tok 3.4370 (3.2603)	LR 2.000e-03
0: TRAIN [2][750/1938]	Time 0.282 (0.263)	Data 1.28e-04 (1.08e-03)	Tok/s 58803 (53544)	Loss/tok 3.3908 (3.2613)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.217 (0.263)	Data 1.27e-04 (1.07e-03)	Tok/s 47586 (53581)	Loss/tok 2.9970 (3.2613)	LR 2.000e-03
0: TRAIN [2][770/1938]	Time 0.283 (0.263)	Data 1.06e-04 (1.05e-03)	Tok/s 59279 (53550)	Loss/tok 3.2895 (3.2604)	LR 2.000e-03
0: TRAIN [2][780/1938]	Time 0.222 (0.262)	Data 1.72e-04 (1.04e-03)	Tok/s 46631 (53506)	Loss/tok 3.1402 (3.2593)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.221 (0.262)	Data 1.15e-04 (1.03e-03)	Tok/s 45981 (53478)	Loss/tok 3.0545 (3.2585)	LR 2.000e-03
0: TRAIN [2][800/1938]	Time 0.218 (0.262)	Data 1.23e-04 (1.02e-03)	Tok/s 47512 (53495)	Loss/tok 3.0723 (3.2589)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][810/1938]	Time 0.414 (0.262)	Data 1.20e-04 (1.01e-03)	Tok/s 71524 (53497)	Loss/tok 3.7445 (3.2605)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.221 (0.262)	Data 1.42e-04 (9.97e-04)	Tok/s 46710 (53491)	Loss/tok 3.1164 (3.2598)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.165 (0.262)	Data 1.47e-04 (9.86e-04)	Tok/s 31370 (53531)	Loss/tok 2.6685 (3.2615)	LR 2.000e-03
0: TRAIN [2][840/1938]	Time 0.222 (0.263)	Data 1.31e-04 (9.76e-04)	Tok/s 47299 (53581)	Loss/tok 3.0217 (3.2632)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.342 (0.263)	Data 1.25e-04 (9.66e-04)	Tok/s 68814 (53635)	Loss/tok 3.4451 (3.2640)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.222 (0.263)	Data 1.24e-04 (9.57e-04)	Tok/s 46479 (53673)	Loss/tok 3.1310 (3.2647)	LR 2.000e-03
0: TRAIN [2][870/1938]	Time 0.280 (0.263)	Data 1.34e-04 (9.47e-04)	Tok/s 60029 (53638)	Loss/tok 3.2049 (3.2633)	LR 2.000e-03
0: TRAIN [2][880/1938]	Time 0.165 (0.263)	Data 1.62e-04 (9.38e-04)	Tok/s 32824 (53563)	Loss/tok 2.6544 (3.2621)	LR 2.000e-03
0: TRAIN [2][890/1938]	Time 0.282 (0.263)	Data 1.21e-04 (9.29e-04)	Tok/s 59145 (53560)	Loss/tok 3.2663 (3.2621)	LR 2.000e-03
0: TRAIN [2][900/1938]	Time 0.216 (0.262)	Data 1.13e-04 (9.20e-04)	Tok/s 47957 (53465)	Loss/tok 3.0604 (3.2608)	LR 2.000e-03
0: TRAIN [2][910/1938]	Time 0.222 (0.262)	Data 1.16e-04 (9.12e-04)	Tok/s 46179 (53419)	Loss/tok 3.0559 (3.2603)	LR 2.000e-03
0: TRAIN [2][920/1938]	Time 0.226 (0.262)	Data 1.63e-04 (9.03e-04)	Tok/s 46079 (53423)	Loss/tok 2.9552 (3.2599)	LR 2.000e-03
0: TRAIN [2][930/1938]	Time 0.233 (0.262)	Data 1.74e-04 (8.95e-04)	Tok/s 43640 (53343)	Loss/tok 3.0630 (3.2594)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.221 (0.262)	Data 1.67e-04 (8.88e-04)	Tok/s 47545 (53334)	Loss/tok 3.0493 (3.2591)	LR 2.000e-03
0: TRAIN [2][950/1938]	Time 0.418 (0.262)	Data 1.71e-04 (8.80e-04)	Tok/s 71322 (53316)	Loss/tok 3.6188 (3.2594)	LR 2.000e-03
0: TRAIN [2][960/1938]	Time 0.222 (0.261)	Data 1.57e-04 (8.73e-04)	Tok/s 47203 (53272)	Loss/tok 3.1097 (3.2584)	LR 2.000e-03
0: TRAIN [2][970/1938]	Time 0.286 (0.261)	Data 1.52e-04 (8.65e-04)	Tok/s 58236 (53301)	Loss/tok 3.3198 (3.2586)	LR 2.000e-03
0: TRAIN [2][980/1938]	Time 0.282 (0.262)	Data 1.53e-04 (8.58e-04)	Tok/s 59922 (53343)	Loss/tok 3.3283 (3.2597)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.220 (0.262)	Data 1.50e-04 (8.51e-04)	Tok/s 46797 (53382)	Loss/tok 3.0526 (3.2605)	LR 2.000e-03
0: TRAIN [2][1000/1938]	Time 0.222 (0.262)	Data 1.36e-04 (8.44e-04)	Tok/s 45616 (53382)	Loss/tok 2.9785 (3.2605)	LR 2.000e-03
0: TRAIN [2][1010/1938]	Time 0.345 (0.262)	Data 1.61e-04 (8.38e-04)	Tok/s 67039 (53366)	Loss/tok 3.4348 (3.2597)	LR 2.000e-03
0: TRAIN [2][1020/1938]	Time 0.221 (0.262)	Data 1.54e-04 (8.31e-04)	Tok/s 46751 (53406)	Loss/tok 2.9985 (3.2596)	LR 2.000e-03
0: TRAIN [2][1030/1938]	Time 0.283 (0.262)	Data 1.71e-04 (8.24e-04)	Tok/s 59085 (53402)	Loss/tok 3.3120 (3.2592)	LR 2.000e-03
0: TRAIN [2][1040/1938]	Time 0.415 (0.262)	Data 1.77e-04 (8.18e-04)	Tok/s 71635 (53387)	Loss/tok 3.6558 (3.2590)	LR 2.000e-03
0: TRAIN [2][1050/1938]	Time 0.416 (0.262)	Data 1.56e-04 (8.12e-04)	Tok/s 71347 (53424)	Loss/tok 3.5458 (3.2597)	LR 2.000e-03
0: TRAIN [2][1060/1938]	Time 0.341 (0.262)	Data 1.62e-04 (8.05e-04)	Tok/s 67793 (53461)	Loss/tok 3.3823 (3.2594)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1070/1938]	Time 0.342 (0.262)	Data 1.65e-04 (7.99e-04)	Tok/s 67613 (53475)	Loss/tok 3.4387 (3.2597)	LR 2.000e-03
0: TRAIN [2][1080/1938]	Time 0.219 (0.262)	Data 1.21e-04 (7.93e-04)	Tok/s 47448 (53420)	Loss/tok 3.1304 (3.2593)	LR 2.000e-03
0: TRAIN [2][1090/1938]	Time 0.221 (0.262)	Data 1.52e-04 (7.87e-04)	Tok/s 46918 (53409)	Loss/tok 3.1111 (3.2591)	LR 2.000e-03
0: TRAIN [2][1100/1938]	Time 0.220 (0.262)	Data 1.21e-04 (7.81e-04)	Tok/s 46352 (53407)	Loss/tok 2.9625 (3.2586)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.283 (0.262)	Data 1.73e-04 (7.76e-04)	Tok/s 59723 (53464)	Loss/tok 3.1692 (3.2599)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.278 (0.262)	Data 1.18e-04 (7.70e-04)	Tok/s 60894 (53483)	Loss/tok 3.2798 (3.2594)	LR 2.000e-03
0: TRAIN [2][1130/1938]	Time 0.347 (0.262)	Data 1.17e-04 (7.64e-04)	Tok/s 66940 (53517)	Loss/tok 3.4315 (3.2603)	LR 2.000e-03
0: TRAIN [2][1140/1938]	Time 0.220 (0.262)	Data 1.89e-04 (7.59e-04)	Tok/s 46435 (53496)	Loss/tok 3.0491 (3.2596)	LR 2.000e-03
0: TRAIN [2][1150/1938]	Time 0.222 (0.262)	Data 1.42e-04 (7.53e-04)	Tok/s 47483 (53513)	Loss/tok 3.0471 (3.2597)	LR 2.000e-03
0: TRAIN [2][1160/1938]	Time 0.167 (0.262)	Data 1.38e-04 (7.48e-04)	Tok/s 31817 (53492)	Loss/tok 2.7741 (3.2591)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.169 (0.262)	Data 1.57e-04 (7.43e-04)	Tok/s 31924 (53445)	Loss/tok 2.5325 (3.2586)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1180/1938]	Time 0.414 (0.262)	Data 1.44e-04 (7.38e-04)	Tok/s 72114 (53475)	Loss/tok 3.5493 (3.2599)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1190/1938]	Time 0.345 (0.262)	Data 1.48e-04 (7.33e-04)	Tok/s 67327 (53490)	Loss/tok 3.4592 (3.2602)	LR 2.000e-03
0: TRAIN [2][1200/1938]	Time 0.166 (0.262)	Data 1.23e-04 (7.28e-04)	Tok/s 31853 (53486)	Loss/tok 2.5921 (3.2604)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.279 (0.262)	Data 1.11e-04 (7.23e-04)	Tok/s 60179 (53468)	Loss/tok 3.2019 (3.2599)	LR 2.000e-03
0: TRAIN [2][1220/1938]	Time 0.281 (0.262)	Data 1.22e-04 (7.18e-04)	Tok/s 59833 (53457)	Loss/tok 3.1360 (3.2591)	LR 2.000e-03
0: TRAIN [2][1230/1938]	Time 0.283 (0.262)	Data 1.54e-04 (7.13e-04)	Tok/s 59590 (53499)	Loss/tok 3.2499 (3.2594)	LR 2.000e-03
0: TRAIN [2][1240/1938]	Time 0.223 (0.262)	Data 1.54e-04 (7.09e-04)	Tok/s 45997 (53520)	Loss/tok 3.0736 (3.2600)	LR 2.000e-03
0: TRAIN [2][1250/1938]	Time 0.420 (0.262)	Data 1.72e-04 (7.04e-04)	Tok/s 70492 (53535)	Loss/tok 3.5646 (3.2606)	LR 2.000e-03
0: TRAIN [2][1260/1938]	Time 0.345 (0.263)	Data 1.38e-04 (7.00e-04)	Tok/s 67790 (53535)	Loss/tok 3.4126 (3.2605)	LR 2.000e-03
0: TRAIN [2][1270/1938]	Time 0.278 (0.263)	Data 1.43e-04 (6.95e-04)	Tok/s 60432 (53540)	Loss/tok 3.2396 (3.2602)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.343 (0.262)	Data 1.61e-04 (6.91e-04)	Tok/s 68340 (53513)	Loss/tok 3.4858 (3.2597)	LR 2.000e-03
0: TRAIN [2][1290/1938]	Time 0.285 (0.262)	Data 1.31e-04 (6.87e-04)	Tok/s 59255 (53524)	Loss/tok 3.1905 (3.2598)	LR 2.000e-03
0: TRAIN [2][1300/1938]	Time 0.284 (0.263)	Data 1.71e-04 (6.83e-04)	Tok/s 58413 (53552)	Loss/tok 3.2653 (3.2603)	LR 2.000e-03
0: TRAIN [2][1310/1938]	Time 0.282 (0.263)	Data 1.76e-04 (6.79e-04)	Tok/s 59109 (53586)	Loss/tok 3.2931 (3.2613)	LR 2.000e-03
0: TRAIN [2][1320/1938]	Time 0.163 (0.263)	Data 1.33e-04 (6.75e-04)	Tok/s 32972 (53568)	Loss/tok 2.6881 (3.2607)	LR 2.000e-03
0: TRAIN [2][1330/1938]	Time 0.237 (0.263)	Data 1.68e-04 (6.71e-04)	Tok/s 44246 (53525)	Loss/tok 3.1275 (3.2599)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.289 (0.263)	Data 1.64e-04 (6.67e-04)	Tok/s 58686 (53521)	Loss/tok 3.2786 (3.2596)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.341 (0.263)	Data 1.75e-04 (6.64e-04)	Tok/s 68422 (53504)	Loss/tok 3.4284 (3.2596)	LR 2.000e-03
0: TRAIN [2][1360/1938]	Time 0.221 (0.263)	Data 1.19e-04 (6.60e-04)	Tok/s 46526 (53546)	Loss/tok 3.1305 (3.2606)	LR 2.000e-03
0: TRAIN [2][1370/1938]	Time 0.166 (0.263)	Data 1.29e-04 (6.56e-04)	Tok/s 32104 (53503)	Loss/tok 2.6539 (3.2599)	LR 2.000e-03
0: TRAIN [2][1380/1938]	Time 0.341 (0.263)	Data 1.24e-04 (6.52e-04)	Tok/s 68115 (53515)	Loss/tok 3.5275 (3.2598)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.220 (0.263)	Data 1.32e-04 (6.48e-04)	Tok/s 46263 (53509)	Loss/tok 3.0704 (3.2594)	LR 2.000e-03
0: TRAIN [2][1400/1938]	Time 0.220 (0.263)	Data 1.21e-04 (6.45e-04)	Tok/s 46681 (53509)	Loss/tok 3.1203 (3.2600)	LR 2.000e-03
0: TRAIN [2][1410/1938]	Time 0.219 (0.263)	Data 1.34e-04 (6.41e-04)	Tok/s 47554 (53486)	Loss/tok 3.0662 (3.2602)	LR 2.000e-03
0: TRAIN [2][1420/1938]	Time 0.281 (0.263)	Data 1.34e-04 (6.38e-04)	Tok/s 60062 (53513)	Loss/tok 3.3133 (3.2605)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1430/1938]	Time 0.283 (0.263)	Data 1.53e-04 (6.34e-04)	Tok/s 60191 (53508)	Loss/tok 3.2560 (3.2603)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.285 (0.263)	Data 1.11e-04 (6.31e-04)	Tok/s 58886 (53519)	Loss/tok 3.3447 (3.2605)	LR 2.000e-03
0: TRAIN [2][1450/1938]	Time 0.343 (0.263)	Data 1.12e-04 (6.27e-04)	Tok/s 68232 (53500)	Loss/tok 3.4016 (3.2600)	LR 2.000e-03
0: TRAIN [2][1460/1938]	Time 0.222 (0.263)	Data 1.48e-04 (6.24e-04)	Tok/s 45964 (53498)	Loss/tok 3.0562 (3.2602)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.280 (0.263)	Data 1.39e-04 (6.20e-04)	Tok/s 59661 (53470)	Loss/tok 3.2932 (3.2596)	LR 2.000e-03
0: TRAIN [2][1480/1938]	Time 0.221 (0.263)	Data 1.36e-04 (6.17e-04)	Tok/s 46092 (53499)	Loss/tok 2.9547 (3.2603)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.282 (0.263)	Data 1.17e-04 (6.14e-04)	Tok/s 59030 (53522)	Loss/tok 3.1435 (3.2600)	LR 2.000e-03
0: TRAIN [2][1500/1938]	Time 0.280 (0.263)	Data 1.24e-04 (6.10e-04)	Tok/s 59339 (53541)	Loss/tok 3.3815 (3.2604)	LR 2.000e-03
0: TRAIN [2][1510/1938]	Time 0.221 (0.263)	Data 1.39e-04 (6.07e-04)	Tok/s 46563 (53540)	Loss/tok 3.0170 (3.2605)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.168 (0.263)	Data 1.24e-04 (6.04e-04)	Tok/s 31749 (53495)	Loss/tok 2.5535 (3.2596)	LR 2.000e-03
0: TRAIN [2][1530/1938]	Time 0.221 (0.262)	Data 1.36e-04 (6.01e-04)	Tok/s 46207 (53474)	Loss/tok 3.0392 (3.2589)	LR 2.000e-03
0: TRAIN [2][1540/1938]	Time 0.225 (0.262)	Data 1.28e-04 (5.98e-04)	Tok/s 45300 (53453)	Loss/tok 3.0469 (3.2588)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.283 (0.263)	Data 1.33e-04 (5.95e-04)	Tok/s 59820 (53461)	Loss/tok 3.2259 (3.2592)	LR 2.000e-03
0: TRAIN [2][1560/1938]	Time 0.165 (0.263)	Data 1.12e-04 (5.92e-04)	Tok/s 31443 (53451)	Loss/tok 2.6271 (3.2594)	LR 2.000e-03
0: TRAIN [2][1570/1938]	Time 0.281 (0.262)	Data 1.16e-04 (5.89e-04)	Tok/s 60036 (53457)	Loss/tok 3.3485 (3.2591)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.221 (0.262)	Data 1.22e-04 (5.86e-04)	Tok/s 46672 (53409)	Loss/tok 3.0871 (3.2584)	LR 2.000e-03
0: TRAIN [2][1590/1938]	Time 0.167 (0.262)	Data 1.54e-04 (5.83e-04)	Tok/s 32014 (53412)	Loss/tok 2.5644 (3.2584)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.220 (0.262)	Data 1.28e-04 (5.80e-04)	Tok/s 47659 (53395)	Loss/tok 3.0503 (3.2577)	LR 2.000e-03
0: TRAIN [2][1610/1938]	Time 0.218 (0.262)	Data 1.35e-04 (5.77e-04)	Tok/s 47771 (53392)	Loss/tok 3.1360 (3.2573)	LR 2.000e-03
0: TRAIN [2][1620/1938]	Time 0.282 (0.262)	Data 1.31e-04 (5.75e-04)	Tok/s 59221 (53400)	Loss/tok 3.3402 (3.2572)	LR 2.000e-03
0: TRAIN [2][1630/1938]	Time 0.221 (0.262)	Data 1.26e-04 (5.72e-04)	Tok/s 46787 (53430)	Loss/tok 3.0718 (3.2575)	LR 2.000e-03
0: TRAIN [2][1640/1938]	Time 0.344 (0.262)	Data 1.18e-04 (5.69e-04)	Tok/s 68013 (53430)	Loss/tok 3.4245 (3.2574)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1650/1938]	Time 0.281 (0.262)	Data 1.32e-04 (5.66e-04)	Tok/s 59896 (53432)	Loss/tok 3.1564 (3.2575)	LR 2.000e-03
0: TRAIN [2][1660/1938]	Time 0.166 (0.262)	Data 1.15e-04 (5.64e-04)	Tok/s 32423 (53428)	Loss/tok 2.6312 (3.2574)	LR 2.000e-03
0: TRAIN [2][1670/1938]	Time 0.222 (0.262)	Data 1.46e-04 (5.61e-04)	Tok/s 47336 (53455)	Loss/tok 3.1272 (3.2576)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.280 (0.262)	Data 1.58e-04 (5.59e-04)	Tok/s 59986 (53452)	Loss/tok 3.2541 (3.2572)	LR 2.000e-03
0: TRAIN [2][1690/1938]	Time 0.222 (0.262)	Data 1.31e-04 (5.56e-04)	Tok/s 46950 (53419)	Loss/tok 3.0750 (3.2564)	LR 2.000e-03
0: TRAIN [2][1700/1938]	Time 0.167 (0.262)	Data 1.20e-04 (5.54e-04)	Tok/s 31828 (53391)	Loss/tok 2.7547 (3.2558)	LR 2.000e-03
0: TRAIN [2][1710/1938]	Time 0.281 (0.262)	Data 1.49e-04 (5.51e-04)	Tok/s 59610 (53394)	Loss/tok 3.3315 (3.2558)	LR 2.000e-03
0: TRAIN [2][1720/1938]	Time 0.283 (0.262)	Data 1.29e-04 (5.49e-04)	Tok/s 59579 (53361)	Loss/tok 3.3323 (3.2551)	LR 2.000e-03
0: TRAIN [2][1730/1938]	Time 0.344 (0.262)	Data 1.26e-04 (5.46e-04)	Tok/s 67790 (53367)	Loss/tok 3.3589 (3.2551)	LR 2.000e-03
0: TRAIN [2][1740/1938]	Time 0.281 (0.262)	Data 1.46e-04 (5.44e-04)	Tok/s 59639 (53366)	Loss/tok 3.2406 (3.2545)	LR 2.000e-03
0: TRAIN [2][1750/1938]	Time 0.277 (0.262)	Data 1.29e-04 (5.42e-04)	Tok/s 59882 (53359)	Loss/tok 3.3017 (3.2543)	LR 2.000e-03
0: TRAIN [2][1760/1938]	Time 0.279 (0.261)	Data 1.35e-04 (5.39e-04)	Tok/s 60319 (53345)	Loss/tok 3.3447 (3.2541)	LR 2.000e-03
0: TRAIN [2][1770/1938]	Time 0.221 (0.261)	Data 1.52e-04 (5.37e-04)	Tok/s 47152 (53346)	Loss/tok 3.0304 (3.2538)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.417 (0.262)	Data 1.39e-04 (5.35e-04)	Tok/s 72297 (53402)	Loss/tok 3.5318 (3.2553)	LR 2.000e-03
0: TRAIN [2][1790/1938]	Time 0.222 (0.262)	Data 1.14e-04 (5.32e-04)	Tok/s 46914 (53382)	Loss/tok 3.1519 (3.2549)	LR 2.000e-03
0: TRAIN [2][1800/1938]	Time 0.283 (0.262)	Data 1.13e-04 (5.30e-04)	Tok/s 58943 (53384)	Loss/tok 3.3313 (3.2547)	LR 2.000e-03
0: TRAIN [2][1810/1938]	Time 0.309 (0.262)	Data 1.67e-04 (5.28e-04)	Tok/s 53862 (53364)	Loss/tok 3.2710 (3.2541)	LR 2.000e-03
0: TRAIN [2][1820/1938]	Time 0.279 (0.261)	Data 1.79e-04 (5.26e-04)	Tok/s 60741 (53339)	Loss/tok 3.3149 (3.2536)	LR 2.000e-03
0: TRAIN [2][1830/1938]	Time 0.223 (0.262)	Data 1.75e-04 (5.24e-04)	Tok/s 46288 (53349)	Loss/tok 3.0944 (3.2535)	LR 2.000e-03
0: TRAIN [2][1840/1938]	Time 0.282 (0.262)	Data 1.65e-04 (5.22e-04)	Tok/s 58911 (53357)	Loss/tok 3.2153 (3.2535)	LR 2.000e-03
0: TRAIN [2][1850/1938]	Time 0.224 (0.262)	Data 1.38e-04 (5.21e-04)	Tok/s 46395 (53344)	Loss/tok 3.0841 (3.2534)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.167 (0.261)	Data 1.62e-04 (5.19e-04)	Tok/s 31197 (53324)	Loss/tok 2.6993 (3.2534)	LR 2.000e-03
0: TRAIN [2][1870/1938]	Time 0.280 (0.262)	Data 1.46e-04 (5.17e-04)	Tok/s 59851 (53341)	Loss/tok 3.3806 (3.2536)	LR 2.000e-03
0: TRAIN [2][1880/1938]	Time 0.221 (0.262)	Data 1.67e-04 (5.15e-04)	Tok/s 46537 (53335)	Loss/tok 3.1028 (3.2534)	LR 2.000e-03
0: TRAIN [2][1890/1938]	Time 0.414 (0.262)	Data 1.56e-04 (5.13e-04)	Tok/s 71851 (53353)	Loss/tok 3.6567 (3.2538)	LR 2.000e-03
0: TRAIN [2][1900/1938]	Time 0.222 (0.262)	Data 1.73e-04 (5.11e-04)	Tok/s 46817 (53364)	Loss/tok 3.0152 (3.2544)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1910/1938]	Time 0.222 (0.262)	Data 1.68e-04 (5.09e-04)	Tok/s 47668 (53388)	Loss/tok 2.9862 (3.2549)	LR 2.000e-03
0: TRAIN [2][1920/1938]	Time 0.222 (0.262)	Data 1.73e-04 (5.07e-04)	Tok/s 46726 (53384)	Loss/tok 3.1359 (3.2545)	LR 2.000e-03
0: TRAIN [2][1930/1938]	Time 0.167 (0.262)	Data 1.37e-04 (5.05e-04)	Tok/s 32294 (53368)	Loss/tok 2.6209 (3.2544)	LR 2.000e-03
:::MLL 1582053853.830 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 525}}
:::MLL 1582053853.831 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.675 (0.675)	Decoder iters 126.0 (126.0)	Tok/s 24191 (24191)
0: Running moses detokenizer
0: BLEU(score=22.891628575073515, counts=[36322, 17909, 10056, 5885], totals=[65785, 62782, 59779, 56780], precisions=[55.213194497225814, 28.52569207734701, 16.8219608892755, 10.364564987671715], bp=1.0, sys_len=65785, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1582053855.693 eval_accuracy: {"value": 22.89, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 536}}
:::MLL 1582053855.694 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 2	Training Loss: 3.2552	Test BLEU: 22.89
0: Performance: Epoch: 2	Training: 427033 Tok/s
0: Finished epoch 2
:::MLL 1582053855.694 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 558}}
:::MLL 1582053855.695 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 512}}
:::MLL 1582053855.695 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 515}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 684599158
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][0/1938]	Time 1.033 (1.033)	Data 7.54e-01 (7.54e-01)	Tok/s 16326 (16326)	Loss/tok 3.0356 (3.0356)	LR 2.000e-03
0: TRAIN [3][10/1938]	Time 0.221 (0.336)	Data 1.23e-04 (6.87e-02)	Tok/s 47135 (49744)	Loss/tok 3.0259 (3.1693)	LR 2.000e-03
0: TRAIN [3][20/1938]	Time 0.222 (0.293)	Data 1.33e-04 (3.60e-02)	Tok/s 45948 (50145)	Loss/tok 2.9774 (3.1489)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.285 (0.282)	Data 1.34e-04 (2.45e-02)	Tok/s 57834 (51421)	Loss/tok 3.2098 (3.1283)	LR 2.000e-03
0: TRAIN [3][40/1938]	Time 0.222 (0.279)	Data 1.35e-04 (1.85e-02)	Tok/s 46554 (52278)	Loss/tok 2.8853 (3.1428)	LR 2.000e-03
0: TRAIN [3][50/1938]	Time 0.222 (0.275)	Data 1.33e-04 (1.49e-02)	Tok/s 46167 (52401)	Loss/tok 3.0793 (3.1529)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.166 (0.270)	Data 1.32e-04 (1.25e-02)	Tok/s 31971 (51760)	Loss/tok 2.5563 (3.1546)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.420 (0.272)	Data 1.80e-04 (1.08e-02)	Tok/s 70061 (52267)	Loss/tok 3.4426 (3.1686)	LR 2.000e-03
0: TRAIN [3][80/1938]	Time 0.343 (0.271)	Data 1.39e-04 (9.44e-03)	Tok/s 67883 (52577)	Loss/tok 3.2244 (3.1633)	LR 2.000e-03
0: TRAIN [3][90/1938]	Time 0.414 (0.271)	Data 1.53e-04 (8.43e-03)	Tok/s 71971 (52769)	Loss/tok 3.4683 (3.1690)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.279 (0.271)	Data 1.36e-04 (7.61e-03)	Tok/s 60242 (53087)	Loss/tok 3.1232 (3.1675)	LR 2.000e-03
0: TRAIN [3][110/1938]	Time 0.284 (0.271)	Data 1.16e-04 (6.94e-03)	Tok/s 59044 (53371)	Loss/tok 3.2473 (3.1723)	LR 2.000e-03
0: TRAIN [3][120/1938]	Time 0.221 (0.274)	Data 1.25e-04 (6.37e-03)	Tok/s 45398 (53915)	Loss/tok 2.9550 (3.1817)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.343 (0.273)	Data 1.24e-04 (5.90e-03)	Tok/s 68395 (53855)	Loss/tok 3.2506 (3.1754)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.341 (0.271)	Data 1.38e-04 (5.49e-03)	Tok/s 69064 (53767)	Loss/tok 3.2624 (3.1693)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.222 (0.271)	Data 1.18e-04 (5.14e-03)	Tok/s 46966 (53793)	Loss/tok 2.9852 (3.1670)	LR 2.000e-03
0: TRAIN [3][160/1938]	Time 0.283 (0.271)	Data 1.36e-04 (4.82e-03)	Tok/s 57932 (53917)	Loss/tok 3.2504 (3.1657)	LR 2.000e-03
0: TRAIN [3][170/1938]	Time 0.347 (0.272)	Data 1.41e-04 (4.55e-03)	Tok/s 67157 (54099)	Loss/tok 3.3155 (3.1708)	LR 2.000e-03
0: TRAIN [3][180/1938]	Time 0.219 (0.272)	Data 1.34e-04 (4.31e-03)	Tok/s 47145 (54154)	Loss/tok 3.0407 (3.1753)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.341 (0.271)	Data 1.12e-04 (4.09e-03)	Tok/s 69195 (54129)	Loss/tok 3.2927 (3.1738)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.165 (0.269)	Data 1.18e-04 (3.89e-03)	Tok/s 31528 (53837)	Loss/tok 2.5774 (3.1707)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][210/1938]	Time 0.285 (0.269)	Data 1.21e-04 (3.71e-03)	Tok/s 58699 (53726)	Loss/tok 3.1470 (3.1696)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.220 (0.268)	Data 1.73e-04 (3.55e-03)	Tok/s 47331 (53601)	Loss/tok 2.9864 (3.1668)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.349 (0.267)	Data 1.55e-04 (3.40e-03)	Tok/s 66547 (53577)	Loss/tok 3.4051 (3.1667)	LR 2.000e-03
0: TRAIN [3][240/1938]	Time 0.281 (0.266)	Data 1.39e-04 (3.27e-03)	Tok/s 59484 (53355)	Loss/tok 3.2290 (3.1630)	LR 2.000e-03
0: TRAIN [3][250/1938]	Time 0.168 (0.266)	Data 1.18e-04 (3.14e-03)	Tok/s 30560 (53289)	Loss/tok 2.6661 (3.1635)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.239 (0.267)	Data 1.48e-04 (3.03e-03)	Tok/s 43230 (53339)	Loss/tok 3.0359 (3.1668)	LR 2.000e-03
0: TRAIN [3][270/1938]	Time 0.285 (0.267)	Data 1.50e-04 (2.92e-03)	Tok/s 58973 (53355)	Loss/tok 3.1028 (3.1672)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.297 (0.268)	Data 1.57e-04 (2.82e-03)	Tok/s 56596 (53350)	Loss/tok 3.2412 (3.1696)	LR 2.000e-03
0: TRAIN [3][290/1938]	Time 0.232 (0.268)	Data 1.83e-04 (2.73e-03)	Tok/s 44469 (53279)	Loss/tok 3.0348 (3.1680)	LR 2.000e-03
0: TRAIN [3][300/1938]	Time 0.292 (0.269)	Data 1.48e-04 (2.64e-03)	Tok/s 57455 (53323)	Loss/tok 3.1024 (3.1695)	LR 2.000e-03
0: TRAIN [3][310/1938]	Time 0.292 (0.269)	Data 1.44e-04 (2.56e-03)	Tok/s 57571 (53201)	Loss/tok 3.1723 (3.1704)	LR 2.000e-03
0: TRAIN [3][320/1938]	Time 0.281 (0.268)	Data 1.41e-04 (2.49e-03)	Tok/s 59242 (53089)	Loss/tok 3.2306 (3.1689)	LR 2.000e-03
0: TRAIN [3][330/1938]	Time 0.220 (0.268)	Data 1.42e-04 (2.42e-03)	Tok/s 45962 (53160)	Loss/tok 2.8823 (3.1675)	LR 2.000e-03
0: TRAIN [3][340/1938]	Time 0.223 (0.267)	Data 1.50e-04 (2.35e-03)	Tok/s 46686 (53132)	Loss/tok 2.9574 (3.1656)	LR 2.000e-03
0: TRAIN [3][350/1938]	Time 0.284 (0.267)	Data 1.24e-04 (2.29e-03)	Tok/s 59011 (53212)	Loss/tok 3.1363 (3.1659)	LR 2.000e-03
0: TRAIN [3][360/1938]	Time 0.281 (0.267)	Data 1.45e-04 (2.23e-03)	Tok/s 59940 (53228)	Loss/tok 3.1434 (3.1669)	LR 2.000e-03
0: TRAIN [3][370/1938]	Time 0.279 (0.267)	Data 1.44e-04 (2.17e-03)	Tok/s 60408 (53269)	Loss/tok 3.2198 (3.1678)	LR 2.000e-03
0: TRAIN [3][380/1938]	Time 0.166 (0.267)	Data 1.25e-04 (2.12e-03)	Tok/s 31925 (53168)	Loss/tok 2.6802 (3.1669)	LR 2.000e-03
0: TRAIN [3][390/1938]	Time 0.168 (0.266)	Data 1.51e-04 (2.07e-03)	Tok/s 30949 (53077)	Loss/tok 2.5753 (3.1659)	LR 2.000e-03
0: TRAIN [3][400/1938]	Time 0.220 (0.265)	Data 1.27e-04 (2.02e-03)	Tok/s 47095 (52961)	Loss/tok 2.9888 (3.1667)	LR 2.000e-03
0: TRAIN [3][410/1938]	Time 0.221 (0.266)	Data 1.60e-04 (1.97e-03)	Tok/s 46869 (53064)	Loss/tok 3.0210 (3.1709)	LR 2.000e-03
0: TRAIN [3][420/1938]	Time 0.281 (0.265)	Data 1.17e-04 (1.93e-03)	Tok/s 59936 (52937)	Loss/tok 3.2003 (3.1699)	LR 2.000e-03
0: TRAIN [3][430/1938]	Time 0.222 (0.265)	Data 1.49e-04 (1.89e-03)	Tok/s 46190 (52873)	Loss/tok 3.0630 (3.1687)	LR 2.000e-03
0: TRAIN [3][440/1938]	Time 0.283 (0.264)	Data 1.03e-04 (1.85e-03)	Tok/s 58969 (52882)	Loss/tok 3.2313 (3.1686)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][450/1938]	Time 0.342 (0.264)	Data 1.04e-04 (1.81e-03)	Tok/s 68203 (52878)	Loss/tok 3.4234 (3.1695)	LR 2.000e-03
0: TRAIN [3][460/1938]	Time 0.218 (0.263)	Data 1.09e-04 (1.77e-03)	Tok/s 47137 (52705)	Loss/tok 2.9570 (3.1666)	LR 2.000e-03
0: TRAIN [3][470/1938]	Time 0.415 (0.264)	Data 1.36e-04 (1.74e-03)	Tok/s 71347 (52795)	Loss/tok 3.4209 (3.1699)	LR 2.000e-03
0: TRAIN [3][480/1938]	Time 0.281 (0.264)	Data 1.03e-04 (1.70e-03)	Tok/s 59482 (52863)	Loss/tok 3.2764 (3.1708)	LR 2.000e-03
0: TRAIN [3][490/1938]	Time 0.168 (0.264)	Data 1.25e-04 (1.67e-03)	Tok/s 31545 (52851)	Loss/tok 2.5478 (3.1717)	LR 2.000e-03
0: TRAIN [3][500/1938]	Time 0.416 (0.265)	Data 1.22e-04 (1.64e-03)	Tok/s 72553 (52965)	Loss/tok 3.3285 (3.1751)	LR 2.000e-03
0: TRAIN [3][510/1938]	Time 0.220 (0.265)	Data 1.09e-04 (1.61e-03)	Tok/s 46052 (53005)	Loss/tok 2.9474 (3.1753)	LR 2.000e-03
0: TRAIN [3][520/1938]	Time 0.281 (0.264)	Data 1.04e-04 (1.58e-03)	Tok/s 60014 (52935)	Loss/tok 3.1234 (3.1733)	LR 2.000e-03
0: TRAIN [3][530/1938]	Time 0.222 (0.264)	Data 1.03e-04 (1.55e-03)	Tok/s 46031 (52878)	Loss/tok 2.9421 (3.1722)	LR 2.000e-03
0: TRAIN [3][540/1938]	Time 0.220 (0.263)	Data 1.02e-04 (1.53e-03)	Tok/s 47128 (52863)	Loss/tok 3.1005 (3.1722)	LR 2.000e-03
0: TRAIN [3][550/1938]	Time 0.341 (0.263)	Data 1.37e-04 (1.50e-03)	Tok/s 69131 (52884)	Loss/tok 3.3330 (3.1717)	LR 2.000e-03
0: TRAIN [3][560/1938]	Time 0.279 (0.263)	Data 1.28e-04 (1.48e-03)	Tok/s 61024 (52906)	Loss/tok 3.2418 (3.1725)	LR 2.000e-03
0: TRAIN [3][570/1938]	Time 0.221 (0.263)	Data 1.09e-04 (1.45e-03)	Tok/s 47918 (52933)	Loss/tok 2.9843 (3.1715)	LR 2.000e-03
0: TRAIN [3][580/1938]	Time 0.282 (0.263)	Data 1.11e-04 (1.43e-03)	Tok/s 59199 (52934)	Loss/tok 3.1562 (3.1701)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][590/1938]	Time 0.220 (0.262)	Data 1.31e-04 (1.41e-03)	Tok/s 47317 (52825)	Loss/tok 2.9576 (3.1689)	LR 2.000e-03
0: TRAIN [3][600/1938]	Time 0.220 (0.263)	Data 1.08e-04 (1.38e-03)	Tok/s 46974 (52892)	Loss/tok 2.9650 (3.1698)	LR 2.000e-03
0: TRAIN [3][610/1938]	Time 0.281 (0.263)	Data 1.16e-04 (1.36e-03)	Tok/s 60606 (52926)	Loss/tok 3.1649 (3.1702)	LR 2.000e-03
0: TRAIN [3][620/1938]	Time 0.279 (0.263)	Data 1.06e-04 (1.34e-03)	Tok/s 59925 (52941)	Loss/tok 3.1466 (3.1699)	LR 2.000e-03
0: TRAIN [3][630/1938]	Time 0.415 (0.263)	Data 1.08e-04 (1.32e-03)	Tok/s 70526 (53078)	Loss/tok 3.5288 (3.1726)	LR 2.000e-03
0: TRAIN [3][640/1938]	Time 0.280 (0.263)	Data 1.04e-04 (1.31e-03)	Tok/s 59789 (53105)	Loss/tok 3.0890 (3.1720)	LR 2.000e-03
0: TRAIN [3][650/1938]	Time 0.222 (0.264)	Data 1.03e-04 (1.29e-03)	Tok/s 46558 (53158)	Loss/tok 2.9952 (3.1747)	LR 2.000e-03
0: TRAIN [3][660/1938]	Time 0.342 (0.264)	Data 1.54e-04 (1.27e-03)	Tok/s 68736 (53212)	Loss/tok 3.3319 (3.1767)	LR 2.000e-03
0: TRAIN [3][670/1938]	Time 0.342 (0.264)	Data 1.26e-04 (1.25e-03)	Tok/s 68333 (53273)	Loss/tok 3.3204 (3.1763)	LR 1.000e-03
0: TRAIN [3][680/1938]	Time 0.345 (0.265)	Data 1.45e-04 (1.24e-03)	Tok/s 67367 (53357)	Loss/tok 3.3478 (3.1773)	LR 1.000e-03
0: TRAIN [3][690/1938]	Time 0.280 (0.265)	Data 1.01e-04 (1.22e-03)	Tok/s 59067 (53428)	Loss/tok 3.0800 (3.1784)	LR 1.000e-03
0: TRAIN [3][700/1938]	Time 0.283 (0.265)	Data 1.04e-04 (1.20e-03)	Tok/s 59394 (53489)	Loss/tok 3.1364 (3.1793)	LR 1.000e-03
0: TRAIN [3][710/1938]	Time 0.223 (0.265)	Data 1.12e-04 (1.19e-03)	Tok/s 46640 (53530)	Loss/tok 2.9335 (3.1803)	LR 1.000e-03
0: TRAIN [3][720/1938]	Time 0.342 (0.266)	Data 1.03e-04 (1.17e-03)	Tok/s 67970 (53593)	Loss/tok 3.3319 (3.1803)	LR 1.000e-03
0: TRAIN [3][730/1938]	Time 0.224 (0.265)	Data 1.23e-04 (1.16e-03)	Tok/s 46515 (53532)	Loss/tok 3.0068 (3.1793)	LR 1.000e-03
0: TRAIN [3][740/1938]	Time 0.166 (0.265)	Data 1.06e-04 (1.14e-03)	Tok/s 31898 (53444)	Loss/tok 2.5255 (3.1778)	LR 1.000e-03
0: TRAIN [3][750/1938]	Time 0.285 (0.265)	Data 1.25e-04 (1.13e-03)	Tok/s 58858 (53434)	Loss/tok 3.1538 (3.1776)	LR 1.000e-03
0: TRAIN [3][760/1938]	Time 0.221 (0.265)	Data 1.27e-04 (1.12e-03)	Tok/s 47105 (53504)	Loss/tok 2.9719 (3.1784)	LR 1.000e-03
0: TRAIN [3][770/1938]	Time 0.282 (0.265)	Data 1.26e-04 (1.10e-03)	Tok/s 59669 (53475)	Loss/tok 3.3092 (3.1774)	LR 1.000e-03
0: TRAIN [3][780/1938]	Time 0.283 (0.265)	Data 1.08e-04 (1.09e-03)	Tok/s 59441 (53523)	Loss/tok 3.1708 (3.1779)	LR 1.000e-03
0: TRAIN [3][790/1938]	Time 0.221 (0.265)	Data 1.06e-04 (1.08e-03)	Tok/s 45247 (53429)	Loss/tok 2.9906 (3.1766)	LR 1.000e-03
0: TRAIN [3][800/1938]	Time 0.223 (0.264)	Data 1.13e-04 (1.07e-03)	Tok/s 46184 (53419)	Loss/tok 2.9477 (3.1756)	LR 1.000e-03
0: TRAIN [3][810/1938]	Time 0.283 (0.264)	Data 1.19e-04 (1.06e-03)	Tok/s 59043 (53377)	Loss/tok 3.0999 (3.1745)	LR 1.000e-03
0: TRAIN [3][820/1938]	Time 0.279 (0.264)	Data 1.13e-04 (1.04e-03)	Tok/s 59787 (53379)	Loss/tok 3.1853 (3.1739)	LR 1.000e-03
0: TRAIN [3][830/1938]	Time 0.222 (0.264)	Data 1.02e-04 (1.03e-03)	Tok/s 46594 (53388)	Loss/tok 3.0147 (3.1742)	LR 1.000e-03
0: TRAIN [3][840/1938]	Time 0.166 (0.264)	Data 1.10e-04 (1.02e-03)	Tok/s 31705 (53417)	Loss/tok 2.4809 (3.1740)	LR 1.000e-03
0: TRAIN [3][850/1938]	Time 0.280 (0.264)	Data 1.06e-04 (1.01e-03)	Tok/s 59780 (53417)	Loss/tok 3.0837 (3.1738)	LR 1.000e-03
0: TRAIN [3][860/1938]	Time 0.220 (0.264)	Data 1.04e-04 (1.00e-03)	Tok/s 47846 (53449)	Loss/tok 3.0038 (3.1734)	LR 1.000e-03
0: TRAIN [3][870/1938]	Time 0.278 (0.264)	Data 1.07e-04 (9.90e-04)	Tok/s 60085 (53414)	Loss/tok 3.1163 (3.1722)	LR 1.000e-03
0: TRAIN [3][880/1938]	Time 0.340 (0.264)	Data 1.06e-04 (9.80e-04)	Tok/s 68331 (53443)	Loss/tok 3.3639 (3.1729)	LR 1.000e-03
0: TRAIN [3][890/1938]	Time 0.225 (0.264)	Data 9.94e-05 (9.71e-04)	Tok/s 46181 (53442)	Loss/tok 2.9909 (3.1734)	LR 1.000e-03
0: TRAIN [3][900/1938]	Time 0.283 (0.264)	Data 1.05e-04 (9.61e-04)	Tok/s 59046 (53445)	Loss/tok 3.1929 (3.1728)	LR 1.000e-03
0: TRAIN [3][910/1938]	Time 0.220 (0.264)	Data 1.02e-04 (9.52e-04)	Tok/s 46572 (53415)	Loss/tok 2.9888 (3.1715)	LR 1.000e-03
0: TRAIN [3][920/1938]	Time 0.221 (0.264)	Data 1.20e-04 (9.43e-04)	Tok/s 47207 (53404)	Loss/tok 2.8752 (3.1714)	LR 1.000e-03
0: TRAIN [3][930/1938]	Time 0.415 (0.264)	Data 1.18e-04 (9.34e-04)	Tok/s 71332 (53368)	Loss/tok 3.5370 (3.1708)	LR 1.000e-03
0: TRAIN [3][940/1938]	Time 0.221 (0.263)	Data 1.07e-04 (9.25e-04)	Tok/s 46832 (53353)	Loss/tok 2.9697 (3.1701)	LR 1.000e-03
0: TRAIN [3][950/1938]	Time 0.223 (0.263)	Data 1.04e-04 (9.16e-04)	Tok/s 46440 (53349)	Loss/tok 2.7965 (3.1698)	LR 1.000e-03
0: TRAIN [3][960/1938]	Time 0.221 (0.263)	Data 1.35e-04 (9.08e-04)	Tok/s 46227 (53273)	Loss/tok 2.9265 (3.1684)	LR 1.000e-03
0: TRAIN [3][970/1938]	Time 0.221 (0.263)	Data 1.30e-04 (9.00e-04)	Tok/s 46675 (53237)	Loss/tok 3.0193 (3.1681)	LR 1.000e-03
0: TRAIN [3][980/1938]	Time 0.218 (0.263)	Data 1.20e-04 (8.92e-04)	Tok/s 46963 (53202)	Loss/tok 2.8830 (3.1675)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][990/1938]	Time 0.221 (0.262)	Data 9.66e-05 (8.84e-04)	Tok/s 46630 (53150)	Loss/tok 2.9345 (3.1660)	LR 1.000e-03
0: TRAIN [3][1000/1938]	Time 0.224 (0.263)	Data 1.17e-04 (8.76e-04)	Tok/s 47203 (53195)	Loss/tok 2.9967 (3.1669)	LR 1.000e-03
0: TRAIN [3][1010/1938]	Time 0.283 (0.262)	Data 1.05e-04 (8.69e-04)	Tok/s 59518 (53169)	Loss/tok 3.0305 (3.1658)	LR 1.000e-03
0: TRAIN [3][1020/1938]	Time 0.282 (0.262)	Data 1.26e-04 (8.61e-04)	Tok/s 60104 (53198)	Loss/tok 3.2278 (3.1663)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1030/1938]	Time 0.165 (0.262)	Data 1.09e-04 (8.54e-04)	Tok/s 32637 (53155)	Loss/tok 2.5514 (3.1664)	LR 1.000e-03
0: TRAIN [3][1040/1938]	Time 0.285 (0.262)	Data 1.04e-04 (8.47e-04)	Tok/s 58698 (53111)	Loss/tok 3.1824 (3.1656)	LR 1.000e-03
0: TRAIN [3][1050/1938]	Time 0.283 (0.262)	Data 1.12e-04 (8.40e-04)	Tok/s 60008 (53167)	Loss/tok 3.1399 (3.1670)	LR 1.000e-03
0: TRAIN [3][1060/1938]	Time 0.221 (0.262)	Data 1.10e-04 (8.33e-04)	Tok/s 46019 (53151)	Loss/tok 2.9702 (3.1660)	LR 1.000e-03
0: TRAIN [3][1070/1938]	Time 0.168 (0.262)	Data 1.18e-04 (8.26e-04)	Tok/s 31390 (53123)	Loss/tok 2.5781 (3.1651)	LR 1.000e-03
0: TRAIN [3][1080/1938]	Time 0.168 (0.262)	Data 1.02e-04 (8.20e-04)	Tok/s 32045 (53108)	Loss/tok 2.6439 (3.1646)	LR 1.000e-03
0: TRAIN [3][1090/1938]	Time 0.283 (0.262)	Data 1.03e-04 (8.13e-04)	Tok/s 58951 (53118)	Loss/tok 3.1373 (3.1639)	LR 1.000e-03
0: TRAIN [3][1100/1938]	Time 0.221 (0.262)	Data 1.05e-04 (8.07e-04)	Tok/s 46460 (53157)	Loss/tok 3.0089 (3.1636)	LR 1.000e-03
0: TRAIN [3][1110/1938]	Time 0.281 (0.262)	Data 1.08e-04 (8.01e-04)	Tok/s 59884 (53181)	Loss/tok 3.0497 (3.1632)	LR 1.000e-03
0: TRAIN [3][1120/1938]	Time 0.344 (0.262)	Data 1.09e-04 (7.95e-04)	Tok/s 67657 (53196)	Loss/tok 3.2746 (3.1630)	LR 1.000e-03
0: TRAIN [3][1130/1938]	Time 0.220 (0.262)	Data 1.03e-04 (7.88e-04)	Tok/s 46980 (53206)	Loss/tok 3.0870 (3.1629)	LR 1.000e-03
0: TRAIN [3][1140/1938]	Time 0.343 (0.262)	Data 1.04e-04 (7.83e-04)	Tok/s 67310 (53208)	Loss/tok 3.3524 (3.1633)	LR 1.000e-03
0: TRAIN [3][1150/1938]	Time 0.166 (0.262)	Data 1.13e-04 (7.77e-04)	Tok/s 31872 (53202)	Loss/tok 2.5938 (3.1627)	LR 1.000e-03
0: TRAIN [3][1160/1938]	Time 0.218 (0.262)	Data 1.12e-04 (7.71e-04)	Tok/s 45759 (53200)	Loss/tok 3.0350 (3.1627)	LR 1.000e-03
0: TRAIN [3][1170/1938]	Time 0.343 (0.262)	Data 1.28e-04 (7.65e-04)	Tok/s 67488 (53226)	Loss/tok 3.4438 (3.1630)	LR 1.000e-03
0: TRAIN [3][1180/1938]	Time 0.221 (0.262)	Data 1.39e-04 (7.60e-04)	Tok/s 46523 (53253)	Loss/tok 2.8797 (3.1636)	LR 1.000e-03
0: TRAIN [3][1190/1938]	Time 0.222 (0.262)	Data 1.38e-04 (7.54e-04)	Tok/s 46551 (53287)	Loss/tok 3.0385 (3.1634)	LR 1.000e-03
0: TRAIN [3][1200/1938]	Time 0.166 (0.262)	Data 1.05e-04 (7.49e-04)	Tok/s 31617 (53266)	Loss/tok 2.6316 (3.1633)	LR 1.000e-03
0: TRAIN [3][1210/1938]	Time 0.415 (0.263)	Data 1.04e-04 (7.44e-04)	Tok/s 72272 (53273)	Loss/tok 3.5468 (3.1638)	LR 1.000e-03
0: TRAIN [3][1220/1938]	Time 0.219 (0.262)	Data 1.04e-04 (7.39e-04)	Tok/s 47030 (53257)	Loss/tok 2.9379 (3.1629)	LR 1.000e-03
0: TRAIN [3][1230/1938]	Time 0.343 (0.263)	Data 1.01e-04 (7.34e-04)	Tok/s 67124 (53287)	Loss/tok 3.3337 (3.1632)	LR 1.000e-03
0: TRAIN [3][1240/1938]	Time 0.284 (0.263)	Data 1.36e-04 (7.29e-04)	Tok/s 58078 (53294)	Loss/tok 3.1430 (3.1630)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1250/1938]	Time 0.342 (0.263)	Data 1.08e-04 (7.24e-04)	Tok/s 68504 (53283)	Loss/tok 3.2474 (3.1635)	LR 1.000e-03
0: TRAIN [3][1260/1938]	Time 0.344 (0.263)	Data 1.11e-04 (7.19e-04)	Tok/s 67185 (53362)	Loss/tok 3.3586 (3.1650)	LR 1.000e-03
0: TRAIN [3][1270/1938]	Time 0.284 (0.263)	Data 1.22e-04 (7.14e-04)	Tok/s 59858 (53317)	Loss/tok 3.1926 (3.1643)	LR 1.000e-03
0: TRAIN [3][1280/1938]	Time 0.283 (0.263)	Data 1.11e-04 (7.09e-04)	Tok/s 57850 (53313)	Loss/tok 3.2431 (3.1637)	LR 1.000e-03
0: TRAIN [3][1290/1938]	Time 0.342 (0.263)	Data 1.12e-04 (7.05e-04)	Tok/s 67061 (53305)	Loss/tok 3.4500 (3.1632)	LR 1.000e-03
0: TRAIN [3][1300/1938]	Time 0.222 (0.263)	Data 1.01e-04 (7.00e-04)	Tok/s 46838 (53320)	Loss/tok 2.9777 (3.1628)	LR 1.000e-03
0: TRAIN [3][1310/1938]	Time 0.220 (0.263)	Data 1.21e-04 (6.96e-04)	Tok/s 47822 (53314)	Loss/tok 2.8732 (3.1621)	LR 1.000e-03
0: TRAIN [3][1320/1938]	Time 0.280 (0.263)	Data 1.10e-04 (6.91e-04)	Tok/s 59660 (53311)	Loss/tok 3.2273 (3.1617)	LR 1.000e-03
0: TRAIN [3][1330/1938]	Time 0.224 (0.262)	Data 1.13e-04 (6.87e-04)	Tok/s 46592 (53270)	Loss/tok 3.0101 (3.1609)	LR 1.000e-03
0: TRAIN [3][1340/1938]	Time 0.222 (0.262)	Data 1.02e-04 (6.83e-04)	Tok/s 44985 (53267)	Loss/tok 2.9729 (3.1605)	LR 1.000e-03
0: TRAIN [3][1350/1938]	Time 0.219 (0.262)	Data 1.36e-04 (6.78e-04)	Tok/s 46242 (53217)	Loss/tok 2.9023 (3.1595)	LR 1.000e-03
0: TRAIN [3][1360/1938]	Time 0.343 (0.262)	Data 1.06e-04 (6.74e-04)	Tok/s 67482 (53210)	Loss/tok 3.2521 (3.1590)	LR 1.000e-03
0: TRAIN [3][1370/1938]	Time 0.341 (0.262)	Data 1.52e-04 (6.70e-04)	Tok/s 68845 (53208)	Loss/tok 3.2571 (3.1584)	LR 1.000e-03
0: TRAIN [3][1380/1938]	Time 0.221 (0.262)	Data 1.14e-04 (6.66e-04)	Tok/s 47035 (53273)	Loss/tok 3.0103 (3.1593)	LR 1.000e-03
0: TRAIN [3][1390/1938]	Time 0.281 (0.262)	Data 1.27e-04 (6.62e-04)	Tok/s 60391 (53281)	Loss/tok 3.1169 (3.1589)	LR 1.000e-03
0: TRAIN [3][1400/1938]	Time 0.280 (0.262)	Data 1.10e-04 (6.58e-04)	Tok/s 60007 (53302)	Loss/tok 3.1267 (3.1588)	LR 1.000e-03
0: TRAIN [3][1410/1938]	Time 0.222 (0.262)	Data 1.00e-04 (6.54e-04)	Tok/s 46521 (53328)	Loss/tok 2.9656 (3.1588)	LR 1.000e-03
0: TRAIN [3][1420/1938]	Time 0.166 (0.262)	Data 1.20e-04 (6.51e-04)	Tok/s 31580 (53276)	Loss/tok 2.5917 (3.1581)	LR 1.000e-03
0: TRAIN [3][1430/1938]	Time 0.415 (0.262)	Data 1.11e-04 (6.47e-04)	Tok/s 72510 (53272)	Loss/tok 3.3530 (3.1578)	LR 1.000e-03
0: TRAIN [3][1440/1938]	Time 0.279 (0.262)	Data 1.07e-04 (6.43e-04)	Tok/s 60402 (53265)	Loss/tok 3.1140 (3.1573)	LR 1.000e-03
0: TRAIN [3][1450/1938]	Time 0.342 (0.262)	Data 1.15e-04 (6.39e-04)	Tok/s 67968 (53282)	Loss/tok 3.2486 (3.1570)	LR 1.000e-03
0: TRAIN [3][1460/1938]	Time 0.415 (0.262)	Data 9.75e-05 (6.36e-04)	Tok/s 70370 (53280)	Loss/tok 3.5826 (3.1580)	LR 1.000e-03
0: TRAIN [3][1470/1938]	Time 0.223 (0.262)	Data 1.04e-04 (6.32e-04)	Tok/s 46558 (53271)	Loss/tok 3.0092 (3.1577)	LR 1.000e-03
0: TRAIN [3][1480/1938]	Time 0.167 (0.262)	Data 1.14e-04 (6.29e-04)	Tok/s 31843 (53228)	Loss/tok 2.6646 (3.1572)	LR 5.000e-04
0: TRAIN [3][1490/1938]	Time 0.222 (0.262)	Data 1.05e-04 (6.25e-04)	Tok/s 45699 (53257)	Loss/tok 3.0026 (3.1570)	LR 5.000e-04
0: TRAIN [3][1500/1938]	Time 0.343 (0.262)	Data 1.13e-04 (6.22e-04)	Tok/s 68737 (53285)	Loss/tok 3.3310 (3.1576)	LR 5.000e-04
0: TRAIN [3][1510/1938]	Time 0.282 (0.262)	Data 1.07e-04 (6.18e-04)	Tok/s 59955 (53269)	Loss/tok 3.0746 (3.1573)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1520/1938]	Time 0.417 (0.262)	Data 1.18e-04 (6.15e-04)	Tok/s 70442 (53314)	Loss/tok 3.5268 (3.1583)	LR 5.000e-04
0: TRAIN [3][1530/1938]	Time 0.283 (0.262)	Data 1.34e-04 (6.12e-04)	Tok/s 59963 (53291)	Loss/tok 3.1959 (3.1579)	LR 5.000e-04
0: TRAIN [3][1540/1938]	Time 0.283 (0.263)	Data 1.26e-04 (6.09e-04)	Tok/s 59966 (53342)	Loss/tok 3.0663 (3.1586)	LR 5.000e-04
0: TRAIN [3][1550/1938]	Time 0.221 (0.262)	Data 1.06e-04 (6.05e-04)	Tok/s 47190 (53289)	Loss/tok 2.9157 (3.1576)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1560/1938]	Time 0.224 (0.262)	Data 1.01e-04 (6.02e-04)	Tok/s 47025 (53307)	Loss/tok 2.8777 (3.1572)	LR 5.000e-04
0: TRAIN [3][1570/1938]	Time 0.221 (0.262)	Data 1.42e-04 (5.99e-04)	Tok/s 46261 (53314)	Loss/tok 2.9231 (3.1571)	LR 5.000e-04
0: TRAIN [3][1580/1938]	Time 0.220 (0.262)	Data 1.21e-04 (5.96e-04)	Tok/s 47166 (53279)	Loss/tok 3.0092 (3.1562)	LR 5.000e-04
0: TRAIN [3][1590/1938]	Time 0.343 (0.262)	Data 1.10e-04 (5.93e-04)	Tok/s 66771 (53309)	Loss/tok 3.3083 (3.1568)	LR 5.000e-04
0: TRAIN [3][1600/1938]	Time 0.344 (0.262)	Data 1.07e-04 (5.90e-04)	Tok/s 68060 (53320)	Loss/tok 3.2401 (3.1564)	LR 5.000e-04
0: TRAIN [3][1610/1938]	Time 0.284 (0.262)	Data 1.04e-04 (5.87e-04)	Tok/s 59343 (53339)	Loss/tok 3.0714 (3.1569)	LR 5.000e-04
0: TRAIN [3][1620/1938]	Time 0.219 (0.262)	Data 1.02e-04 (5.84e-04)	Tok/s 46984 (53363)	Loss/tok 2.9348 (3.1567)	LR 5.000e-04
0: TRAIN [3][1630/1938]	Time 0.220 (0.262)	Data 1.18e-04 (5.81e-04)	Tok/s 46151 (53346)	Loss/tok 2.8572 (3.1565)	LR 5.000e-04
0: TRAIN [3][1640/1938]	Time 0.221 (0.262)	Data 1.03e-04 (5.78e-04)	Tok/s 46218 (53331)	Loss/tok 2.9394 (3.1559)	LR 5.000e-04
0: TRAIN [3][1650/1938]	Time 0.220 (0.262)	Data 1.04e-04 (5.76e-04)	Tok/s 47500 (53344)	Loss/tok 2.9143 (3.1559)	LR 5.000e-04
0: TRAIN [3][1660/1938]	Time 0.281 (0.262)	Data 1.63e-04 (5.73e-04)	Tok/s 60135 (53358)	Loss/tok 2.9345 (3.1553)	LR 5.000e-04
0: TRAIN [3][1670/1938]	Time 0.281 (0.263)	Data 1.11e-04 (5.70e-04)	Tok/s 59372 (53386)	Loss/tok 3.0686 (3.1552)	LR 5.000e-04
0: TRAIN [3][1680/1938]	Time 0.281 (0.263)	Data 1.03e-04 (5.67e-04)	Tok/s 59475 (53388)	Loss/tok 3.1571 (3.1551)	LR 5.000e-04
0: TRAIN [3][1690/1938]	Time 0.343 (0.263)	Data 1.47e-04 (5.65e-04)	Tok/s 68762 (53409)	Loss/tok 3.2075 (3.1550)	LR 5.000e-04
0: TRAIN [3][1700/1938]	Time 0.285 (0.263)	Data 1.50e-04 (5.62e-04)	Tok/s 58626 (53416)	Loss/tok 3.0118 (3.1545)	LR 5.000e-04
0: TRAIN [3][1710/1938]	Time 0.343 (0.263)	Data 1.40e-04 (5.60e-04)	Tok/s 67994 (53418)	Loss/tok 3.2352 (3.1542)	LR 5.000e-04
0: TRAIN [3][1720/1938]	Time 0.225 (0.263)	Data 1.60e-04 (5.58e-04)	Tok/s 46557 (53420)	Loss/tok 2.8039 (3.1540)	LR 5.000e-04
0: TRAIN [3][1730/1938]	Time 0.284 (0.263)	Data 1.36e-04 (5.55e-04)	Tok/s 58350 (53408)	Loss/tok 3.2525 (3.1535)	LR 5.000e-04
0: TRAIN [3][1740/1938]	Time 0.166 (0.263)	Data 1.35e-04 (5.53e-04)	Tok/s 31651 (53385)	Loss/tok 2.5640 (3.1531)	LR 5.000e-04
0: TRAIN [3][1750/1938]	Time 0.342 (0.263)	Data 1.37e-04 (5.50e-04)	Tok/s 67787 (53400)	Loss/tok 3.2674 (3.1530)	LR 5.000e-04
0: TRAIN [3][1760/1938]	Time 0.283 (0.262)	Data 1.26e-04 (5.48e-04)	Tok/s 60065 (53367)	Loss/tok 3.1695 (3.1523)	LR 5.000e-04
0: TRAIN [3][1770/1938]	Time 0.220 (0.262)	Data 1.29e-04 (5.46e-04)	Tok/s 46498 (53339)	Loss/tok 2.9755 (3.1516)	LR 5.000e-04
0: TRAIN [3][1780/1938]	Time 0.218 (0.262)	Data 1.18e-04 (5.43e-04)	Tok/s 47485 (53322)	Loss/tok 2.8560 (3.1509)	LR 5.000e-04
0: TRAIN [3][1790/1938]	Time 0.223 (0.262)	Data 1.30e-04 (5.41e-04)	Tok/s 45646 (53333)	Loss/tok 2.9610 (3.1505)	LR 5.000e-04
0: TRAIN [3][1800/1938]	Time 0.164 (0.262)	Data 1.40e-04 (5.39e-04)	Tok/s 32423 (53317)	Loss/tok 2.5730 (3.1498)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1810/1938]	Time 0.219 (0.262)	Data 1.23e-04 (5.37e-04)	Tok/s 47200 (53323)	Loss/tok 2.8417 (3.1495)	LR 5.000e-04
0: TRAIN [3][1820/1938]	Time 0.284 (0.262)	Data 1.27e-04 (5.34e-04)	Tok/s 59532 (53334)	Loss/tok 3.1009 (3.1495)	LR 5.000e-04
0: TRAIN [3][1830/1938]	Time 0.345 (0.262)	Data 1.23e-04 (5.32e-04)	Tok/s 68281 (53331)	Loss/tok 3.2989 (3.1491)	LR 5.000e-04
0: TRAIN [3][1840/1938]	Time 0.341 (0.262)	Data 1.43e-04 (5.30e-04)	Tok/s 68733 (53317)	Loss/tok 3.1819 (3.1486)	LR 5.000e-04
0: TRAIN [3][1850/1938]	Time 0.280 (0.262)	Data 1.44e-04 (5.28e-04)	Tok/s 59467 (53325)	Loss/tok 3.1017 (3.1480)	LR 5.000e-04
0: TRAIN [3][1860/1938]	Time 0.283 (0.262)	Data 1.28e-04 (5.26e-04)	Tok/s 59178 (53324)	Loss/tok 3.2456 (3.1481)	LR 5.000e-04
0: TRAIN [3][1870/1938]	Time 0.218 (0.262)	Data 1.15e-04 (5.24e-04)	Tok/s 47345 (53321)	Loss/tok 2.9966 (3.1484)	LR 5.000e-04
0: TRAIN [3][1880/1938]	Time 0.222 (0.262)	Data 1.31e-04 (5.21e-04)	Tok/s 46275 (53349)	Loss/tok 3.0119 (3.1487)	LR 5.000e-04
0: TRAIN [3][1890/1938]	Time 0.282 (0.262)	Data 1.40e-04 (5.19e-04)	Tok/s 58895 (53351)	Loss/tok 3.1538 (3.1483)	LR 5.000e-04
0: TRAIN [3][1900/1938]	Time 0.283 (0.262)	Data 1.23e-04 (5.17e-04)	Tok/s 60050 (53348)	Loss/tok 3.1777 (3.1479)	LR 5.000e-04
0: TRAIN [3][1910/1938]	Time 0.276 (0.262)	Data 1.30e-04 (5.15e-04)	Tok/s 61149 (53342)	Loss/tok 3.1179 (3.1475)	LR 5.000e-04
0: TRAIN [3][1920/1938]	Time 0.218 (0.262)	Data 1.25e-04 (5.13e-04)	Tok/s 46050 (53336)	Loss/tok 2.8274 (3.1471)	LR 5.000e-04
0: TRAIN [3][1930/1938]	Time 0.342 (0.262)	Data 1.28e-04 (5.11e-04)	Tok/s 68934 (53361)	Loss/tok 3.3205 (3.1472)	LR 5.000e-04
:::MLL 1582054364.092 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 525}}
:::MLL 1582054364.093 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 530}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.621 (0.621)	Decoder iters 102.0 (102.0)	Tok/s 26455 (26455)
0: Running moses detokenizer
0: BLEU(score=24.395776247272007, counts=[37292, 18812, 10788, 6486], totals=[65609, 62606, 59603, 56606], precisions=[56.8397628374156, 30.048238188033096, 18.099760079190645, 11.458149312793697], bp=1.0, sys_len=65609, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1582054365.906 eval_accuracy: {"value": 24.4, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 536}}
:::MLL 1582054365.907 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 539}}
0: Summary: Epoch: 3	Training Loss: 3.1464	Test BLEU: 24.40
0: Performance: Epoch: 3	Training: 426790 Tok/s
0: Finished epoch 3
:::MLL 1582054365.907 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 558}}
0: Closing preprocessed data file
:::MLL 1582054365.908 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 569}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-02-18 07:32:50 PM
RESULT,RNN_TRANSLATOR,,2057,nvidia,2020-02-18 06:58:33 PM
