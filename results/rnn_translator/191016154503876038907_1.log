Beginning trial 1 of 1
Gathering sys log on dss01
:::MLL 1571258802.853 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1571258802.854 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1571258802.854 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1571258802.855 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1571258802.855 submission_platform: {"value": "1xDSS8440", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1571258802.856 submission_entry: {"value": "{'hardware': 'DSS8440', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '5.0-0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz', 'num_cores': '40', 'num_vcpus': '40', 'accelerator': 'Tesla V100-PCIE-32GB', 'num_accelerators': '8', 'sys_mem_size': '754 GB', 'sys_storage_type': 'SATA SSD', 'sys_storage_size': '1x 447.1G + 1x 931.5G', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27700 Family [ConnectX-4]', 'num_network_cards': '1', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1571258802.856 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1571258802.857 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1571258808.205 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node dss01
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=5120' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=128 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=1600 -e TARGET=24.0 -e NUMEPOCHS=6 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=191016154503876038907 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191016154503876038907 ./run_and_time.sh
Run vars: id 191016154503876038907 gpus 8 mparams  --master_port=5120
NCCL_SOCKET_NTHREADS=2
NCCL_NSOCKS_PERTHREAD=8
STARTING TIMING RUN AT 2019-10-16 08:46:48 PM
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=128
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=1600
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=6
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=0
+ MATH=amp_fp16
+ [[ 0 -eq 1 ]]
+ LAUNCH_OPT='torch.distributed.launch --nproc_per_node 8  --master_port=5120'
+ echo 'running benchmark'
+ python -m torch.distributed.launch --nproc_per_node 8 --master_port=5120 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 6 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 128 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 1600 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1571258810.995 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571258810.995 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571258811.002 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571258811.004 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571258811.005 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571258811.008 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571258811.009 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1571258811.009 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=1600, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=6, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=128, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 149062076
dss01:465:465 [0] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:465:465 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:465:465 [0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:465:465 [0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:465:465 [0] NCCL INFO NET/IB : No device found.
dss01:465:465 [0] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
NCCL version 2.4.8+cuda10.1
dss01:467:467 [2] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:467:467 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:470:470 [5] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:470:470 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:472:472 [7] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:472:472 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:466:466 [1] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:466:466 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:469:469 [4] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:469:469 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:468:468 [3] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:468:468 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:471:471 [6] NCCL INFO Bootstrap : Using [0]eth0:10.141.0.200<0>
dss01:471:471 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:470:470 [5] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:467:467 [2] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:467:467 [2] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0

dss01:470:470 [5] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:467:467 [2] NCCL INFO NET/IB : No device found.
dss01:470:470 [5] NCCL INFO NET/IB : No device found.

dss01:469:469 [4] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:469:469 [4] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:469:469 [4] NCCL INFO NET/IB : No device found.

dss01:468:468 [3] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:471:471 [6] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:468:468 [3] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:468:468 [3] NCCL INFO NET/IB : No device found.

dss01:471:471 [6] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0

dss01:466:466 [1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
dss01:471:471 [6] NCCL INFO NET/IB : No device found.

dss01:472:472 [7] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:466:466 [1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0

dss01:472:472 [7] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:466:466 [1] NCCL INFO NET/IB : No device found.
dss01:472:472 [7] NCCL INFO NET/IB : No device found.
dss01:467:467 [2] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:468:468 [3] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:466:466 [1] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:472:472 [7] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:470:470 [5] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:471:471 [6] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:469:469 [4] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:465:827 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555
dss01:467:828 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555
dss01:470:829 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa
dss01:472:830 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa
dss01:466:831 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555
dss01:468:832 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555
dss01:471:833 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa
dss01:469:834 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa
dss01:471:833 [6] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:467:828 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:468:832 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:466:831 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:472:830 [7] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:465:827 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:469:834 [4] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:470:829 [5] NCCL INFO NCCL_P2P_LEVEL set by environment to 3.
dss01:465:827 [0] NCCL INFO Channel 00 :    0   1   2   3   4   5   6   7
dss01:465:827 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/IPC
dss01:467:828 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/IPC
dss01:469:834 [4] NCCL INFO Ring 00 : 4[4] -> 5[5] via P2P/IPC
dss01:471:833 [6] NCCL INFO Ring 00 : 6[6] -> 7[7] via P2P/IPC
dss01:472:830 [7] NCCL INFO Ring 00 : 7[7] -> 0[0] via direct shared memory
dss01:466:831 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via direct shared memory
dss01:468:832 [3] NCCL INFO Ring 00 : 3[3] -> 4[4] via direct shared memory
dss01:470:829 [5] NCCL INFO Ring 00 : 5[5] -> 6[6] via direct shared memory
dss01:465:827 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
dss01:472:830 [7] NCCL INFO comm 0x7ffe98007590 rank 7 nranks 8 cudaDev 7 nvmlDev 7 - Init COMPLETE
dss01:468:832 [3] NCCL INFO comm 0x7fff68007590 rank 3 nranks 8 cudaDev 3 nvmlDev 3 - Init COMPLETE
dss01:466:831 [1] NCCL INFO comm 0x7fff48007590 rank 1 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE
dss01:470:829 [5] NCCL INFO comm 0x7ffe98007590 rank 5 nranks 8 cudaDev 5 nvmlDev 5 - Init COMPLETE
dss01:465:827 [0] NCCL INFO comm 0x7ffe90007590 rank 0 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE
dss01:465:465 [0] NCCL INFO Launch mode Parallel
0: Worker 0 is using worker seed: 2082264190
0: Building vocabulary from /data/vocab.bpe.32000
dss01:467:828 [2] NCCL INFO comm 0x7fff40007590 rank 2 nranks 8 cudaDev 2 nvmlDev 2 - Init COMPLETE
0: Size of vocabulary: 32320
dss01:469:834 [4] NCCL INFO comm 0x7fff58007590 rank 4 nranks 8 cudaDev 4 nvmlDev 4 - Init COMPLETE
dss01:471:833 [6] NCCL INFO comm 0x7ffe98007590 rank 6 nranks 8 cudaDev 6 nvmlDev 6 - Init COMPLETE
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1571258835.160 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1571258837.751 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1571258837.752 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1571258837.752 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1571258838.736 global_batch_size: {"value": 1024, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 1600, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 1600
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1571258838.737 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1571258838.738 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1571258838.738 opt_learning_rate_decay_interval: {"value": 1600, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1571258838.738 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1571258838.738 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1571258838.739 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1571258838.739 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1571258838.823 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571258838.824 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 4141917608
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/3880]	Time 0.907 (0.907)	Data 7.28e-01 (7.28e-01)	Tok/s 5648 (5648)	Loss/tok 10.6792 (10.6792)	LR 2.000e-05
0: TRAIN [0][10/3880]	Time 0.190 (0.258)	Data 1.46e-04 (6.62e-02)	Tok/s 43949 (37054)	Loss/tok 9.8323 (10.2833)	LR 2.518e-05
0: TRAIN [0][20/3880]	Time 0.191 (0.232)	Data 1.13e-04 (3.48e-02)	Tok/s 44239 (40974)	Loss/tok 9.3455 (9.9165)	LR 3.170e-05
0: TRAIN [0][30/3880]	Time 0.163 (0.219)	Data 9.44e-05 (2.36e-02)	Tok/s 31440 (41622)	Loss/tok 9.0317 (9.7043)	LR 3.991e-05
0: TRAIN [0][40/3880]	Time 0.220 (0.212)	Data 1.05e-04 (1.79e-02)	Tok/s 52496 (41714)	Loss/tok 8.9036 (9.5346)	LR 5.024e-05
0: TRAIN [0][50/3880]	Time 0.163 (0.206)	Data 2.20e-04 (1.44e-02)	Tok/s 31615 (40997)	Loss/tok 8.4537 (9.3969)	LR 6.325e-05
0: TRAIN [0][60/3880]	Time 0.162 (0.202)	Data 1.38e-04 (1.21e-02)	Tok/s 32945 (40700)	Loss/tok 8.2442 (9.2582)	LR 7.962e-05
0: TRAIN [0][70/3880]	Time 0.221 (0.202)	Data 1.27e-04 (1.04e-02)	Tok/s 52674 (41167)	Loss/tok 8.2722 (9.1072)	LR 1.002e-04
0: TRAIN [0][80/3880]	Time 0.220 (0.200)	Data 9.68e-05 (9.10e-03)	Tok/s 52311 (40925)	Loss/tok 8.1638 (8.9946)	LR 1.262e-04
0: TRAIN [0][90/3880]	Time 0.191 (0.200)	Data 1.01e-04 (8.11e-03)	Tok/s 43275 (41283)	Loss/tok 7.9758 (8.8870)	LR 1.589e-04
0: TRAIN [0][100/3880]	Time 0.194 (0.199)	Data 1.14e-04 (7.32e-03)	Tok/s 43697 (41092)	Loss/tok 8.0264 (8.8034)	LR 2.000e-04
0: TRAIN [0][110/3880]	Time 0.220 (0.199)	Data 1.06e-04 (6.67e-03)	Tok/s 52856 (41536)	Loss/tok 7.9515 (8.7216)	LR 2.518e-04
0: TRAIN [0][120/3880]	Time 0.162 (0.197)	Data 1.17e-04 (6.13e-03)	Tok/s 31998 (41136)	Loss/tok 7.7046 (8.6617)	LR 3.170e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][130/3880]	Time 0.163 (0.196)	Data 1.08e-04 (5.67e-03)	Tok/s 32503 (40911)	Loss/tok 7.5689 (8.6071)	LR 3.900e-04
0: TRAIN [0][140/3880]	Time 0.221 (0.195)	Data 1.39e-04 (5.28e-03)	Tok/s 53158 (40914)	Loss/tok 7.8767 (8.5519)	LR 4.909e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][150/3880]	Time 0.190 (0.194)	Data 1.17e-04 (4.93e-03)	Tok/s 43722 (40848)	Loss/tok 7.8653 (8.5013)	LR 6.040e-04
0: TRAIN [0][160/3880]	Time 0.220 (0.194)	Data 1.12e-04 (4.63e-03)	Tok/s 52825 (41075)	Loss/tok 7.8059 (8.4549)	LR 7.604e-04
0: TRAIN [0][170/3880]	Time 0.162 (0.193)	Data 1.03e-04 (4.37e-03)	Tok/s 31091 (40872)	Loss/tok 7.5311 (8.4128)	LR 9.573e-04
0: TRAIN [0][180/3880]	Time 0.162 (0.192)	Data 1.50e-04 (4.14e-03)	Tok/s 31756 (40495)	Loss/tok 7.3367 (8.3740)	LR 1.205e-03
0: TRAIN [0][190/3880]	Time 0.190 (0.191)	Data 9.58e-05 (3.93e-03)	Tok/s 44150 (40417)	Loss/tok 7.2577 (8.3268)	LR 1.517e-03
0: TRAIN [0][200/3880]	Time 0.256 (0.191)	Data 1.07e-04 (3.74e-03)	Tok/s 57577 (40452)	Loss/tok 7.3911 (8.2718)	LR 1.910e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
0: TRAIN [0][210/3880]	Time 0.216 (0.191)	Data 1.13e-04 (3.57e-03)	Tok/s 54063 (40332)	Loss/tok 7.6129 (8.2281)	LR 2.000e-03
0: TRAIN [0][220/3880]	Time 0.162 (0.191)	Data 1.17e-04 (3.41e-03)	Tok/s 31404 (40521)	Loss/tok 6.8299 (8.1721)	LR 2.000e-03
0: TRAIN [0][230/3880]	Time 0.256 (0.191)	Data 1.17e-04 (3.27e-03)	Tok/s 58788 (40604)	Loss/tok 6.9117 (8.1128)	LR 2.000e-03
0: TRAIN [0][240/3880]	Time 0.220 (0.191)	Data 1.46e-04 (3.14e-03)	Tok/s 52890 (40467)	Loss/tok 6.9649 (8.0637)	LR 2.000e-03
0: TRAIN [0][250/3880]	Time 0.162 (0.190)	Data 1.14e-04 (3.02e-03)	Tok/s 31877 (40300)	Loss/tok 6.4845 (8.0162)	LR 2.000e-03
0: TRAIN [0][260/3880]	Time 0.136 (0.189)	Data 1.28e-04 (2.91e-03)	Tok/s 18760 (40090)	Loss/tok 5.5173 (7.9683)	LR 2.000e-03
0: TRAIN [0][270/3880]	Time 0.220 (0.189)	Data 1.18e-04 (2.81e-03)	Tok/s 52178 (40069)	Loss/tok 6.6786 (7.9146)	LR 2.000e-03
0: TRAIN [0][280/3880]	Time 0.190 (0.189)	Data 1.15e-04 (2.71e-03)	Tok/s 43654 (40046)	Loss/tok 6.5604 (7.8609)	LR 2.000e-03
0: TRAIN [0][290/3880]	Time 0.220 (0.189)	Data 1.13e-04 (2.62e-03)	Tok/s 52516 (40193)	Loss/tok 6.4895 (7.8071)	LR 2.000e-03
0: TRAIN [0][300/3880]	Time 0.220 (0.189)	Data 1.06e-04 (2.54e-03)	Tok/s 54111 (40195)	Loss/tok 6.2567 (7.7538)	LR 2.000e-03
0: TRAIN [0][310/3880]	Time 0.162 (0.189)	Data 1.58e-04 (2.47e-03)	Tok/s 33199 (40290)	Loss/tok 5.7250 (7.6963)	LR 2.000e-03
0: TRAIN [0][320/3880]	Time 0.191 (0.189)	Data 1.13e-04 (2.39e-03)	Tok/s 44415 (40281)	Loss/tok 6.0400 (7.6472)	LR 2.000e-03
0: TRAIN [0][330/3880]	Time 0.162 (0.188)	Data 9.08e-05 (2.32e-03)	Tok/s 30905 (40170)	Loss/tok 5.6708 (7.6037)	LR 2.000e-03
0: TRAIN [0][340/3880]	Time 0.165 (0.188)	Data 1.12e-04 (2.26e-03)	Tok/s 30157 (39995)	Loss/tok 5.6246 (7.5601)	LR 2.000e-03
0: TRAIN [0][350/3880]	Time 0.136 (0.188)	Data 9.08e-05 (2.20e-03)	Tok/s 19472 (39935)	Loss/tok 4.7660 (7.5147)	LR 2.000e-03
0: TRAIN [0][360/3880]	Time 0.191 (0.187)	Data 2.01e-04 (2.14e-03)	Tok/s 43375 (39815)	Loss/tok 5.7440 (7.4714)	LR 2.000e-03
0: TRAIN [0][370/3880]	Time 0.220 (0.188)	Data 1.18e-04 (2.09e-03)	Tok/s 52804 (39977)	Loss/tok 5.8430 (7.4145)	LR 2.000e-03
0: TRAIN [0][380/3880]	Time 0.191 (0.188)	Data 3.66e-04 (2.04e-03)	Tok/s 44374 (40064)	Loss/tok 5.5621 (7.3630)	LR 2.000e-03
0: TRAIN [0][390/3880]	Time 0.191 (0.188)	Data 8.96e-05 (1.99e-03)	Tok/s 44166 (39980)	Loss/tok 5.3957 (7.3202)	LR 2.000e-03
0: TRAIN [0][400/3880]	Time 0.162 (0.187)	Data 1.12e-04 (1.94e-03)	Tok/s 32231 (39876)	Loss/tok 5.1156 (7.2783)	LR 2.000e-03
0: TRAIN [0][410/3880]	Time 0.162 (0.187)	Data 1.05e-04 (1.90e-03)	Tok/s 32305 (39698)	Loss/tok 5.1028 (7.2417)	LR 2.000e-03
0: TRAIN [0][420/3880]	Time 0.190 (0.187)	Data 9.66e-05 (1.86e-03)	Tok/s 44305 (39710)	Loss/tok 5.2328 (7.1950)	LR 2.000e-03
0: TRAIN [0][430/3880]	Time 0.162 (0.187)	Data 1.09e-04 (1.82e-03)	Tok/s 32489 (39806)	Loss/tok 4.9854 (7.1452)	LR 2.000e-03
0: TRAIN [0][440/3880]	Time 0.137 (0.187)	Data 1.09e-04 (1.78e-03)	Tok/s 19387 (39717)	Loss/tok 3.9563 (7.1057)	LR 2.000e-03
0: TRAIN [0][450/3880]	Time 0.255 (0.187)	Data 1.04e-04 (1.74e-03)	Tok/s 57822 (39700)	Loss/tok 5.5847 (7.0642)	LR 2.000e-03
0: TRAIN [0][460/3880]	Time 0.191 (0.187)	Data 9.80e-05 (1.71e-03)	Tok/s 43635 (39829)	Loss/tok 4.9709 (7.0125)	LR 2.000e-03
0: TRAIN [0][470/3880]	Time 0.192 (0.187)	Data 6.53e-04 (1.67e-03)	Tok/s 43831 (39794)	Loss/tok 5.0018 (6.9731)	LR 2.000e-03
0: TRAIN [0][480/3880]	Time 0.162 (0.186)	Data 9.13e-05 (1.64e-03)	Tok/s 31502 (39723)	Loss/tok 4.5923 (6.9352)	LR 2.000e-03
0: TRAIN [0][490/3880]	Time 0.192 (0.186)	Data 1.74e-04 (1.61e-03)	Tok/s 44574 (39701)	Loss/tok 4.8789 (6.8955)	LR 2.000e-03
0: TRAIN [0][500/3880]	Time 0.256 (0.186)	Data 1.18e-04 (1.58e-03)	Tok/s 59111 (39620)	Loss/tok 5.2171 (6.8591)	LR 2.000e-03
0: TRAIN [0][510/3880]	Time 0.136 (0.186)	Data 1.10e-04 (1.55e-03)	Tok/s 19296 (39598)	Loss/tok 3.5362 (6.8218)	LR 2.000e-03
0: TRAIN [0][520/3880]	Time 0.192 (0.186)	Data 1.06e-04 (1.53e-03)	Tok/s 43738 (39606)	Loss/tok 4.7635 (6.7829)	LR 2.000e-03
0: TRAIN [0][530/3880]	Time 0.162 (0.186)	Data 1.11e-04 (1.50e-03)	Tok/s 32464 (39593)	Loss/tok 4.5134 (6.7465)	LR 2.000e-03
0: TRAIN [0][540/3880]	Time 0.190 (0.186)	Data 1.13e-04 (1.48e-03)	Tok/s 44457 (39541)	Loss/tok 4.6923 (6.7117)	LR 2.000e-03
0: TRAIN [0][550/3880]	Time 0.220 (0.186)	Data 1.21e-04 (1.45e-03)	Tok/s 53542 (39567)	Loss/tok 4.8081 (6.6735)	LR 2.000e-03
0: TRAIN [0][560/3880]	Time 0.137 (0.185)	Data 1.06e-04 (1.43e-03)	Tok/s 18820 (39466)	Loss/tok 3.6527 (6.6434)	LR 2.000e-03
0: TRAIN [0][570/3880]	Time 0.221 (0.185)	Data 4.95e-04 (1.41e-03)	Tok/s 53171 (39473)	Loss/tok 4.8617 (6.6086)	LR 2.000e-03
0: TRAIN [0][580/3880]	Time 0.162 (0.185)	Data 1.09e-04 (1.38e-03)	Tok/s 32703 (39452)	Loss/tok 4.3550 (6.5754)	LR 2.000e-03
0: TRAIN [0][590/3880]	Time 0.162 (0.185)	Data 1.01e-04 (1.36e-03)	Tok/s 32096 (39478)	Loss/tok 4.1987 (6.5414)	LR 2.000e-03
0: TRAIN [0][600/3880]	Time 0.162 (0.185)	Data 9.87e-05 (1.34e-03)	Tok/s 32571 (39430)	Loss/tok 4.4292 (6.5110)	LR 2.000e-03
0: TRAIN [0][610/3880]	Time 0.162 (0.185)	Data 1.11e-04 (1.32e-03)	Tok/s 31660 (39456)	Loss/tok 4.4458 (6.4766)	LR 2.000e-03
0: TRAIN [0][620/3880]	Time 0.162 (0.185)	Data 1.28e-04 (1.30e-03)	Tok/s 32615 (39406)	Loss/tok 4.1698 (6.4475)	LR 2.000e-03
0: TRAIN [0][630/3880]	Time 0.191 (0.185)	Data 1.21e-04 (1.28e-03)	Tok/s 44735 (39454)	Loss/tok 4.5466 (6.4149)	LR 2.000e-03
0: TRAIN [0][640/3880]	Time 0.162 (0.185)	Data 1.29e-04 (1.27e-03)	Tok/s 31772 (39449)	Loss/tok 4.3631 (6.3852)	LR 2.000e-03
0: TRAIN [0][650/3880]	Time 0.162 (0.184)	Data 1.12e-04 (1.25e-03)	Tok/s 30966 (39272)	Loss/tok 4.2446 (6.3642)	LR 2.000e-03
0: TRAIN [0][660/3880]	Time 0.191 (0.184)	Data 1.01e-04 (1.23e-03)	Tok/s 44186 (39270)	Loss/tok 4.6179 (6.3365)	LR 2.000e-03
0: TRAIN [0][670/3880]	Time 0.257 (0.185)	Data 1.07e-04 (1.22e-03)	Tok/s 57688 (39270)	Loss/tok 4.7779 (6.3083)	LR 2.000e-03
0: TRAIN [0][680/3880]	Time 0.162 (0.184)	Data 1.03e-04 (1.20e-03)	Tok/s 32590 (39229)	Loss/tok 3.9507 (6.2822)	LR 2.000e-03
0: TRAIN [0][690/3880]	Time 0.191 (0.184)	Data 1.01e-04 (1.18e-03)	Tok/s 44254 (39214)	Loss/tok 4.3960 (6.2555)	LR 2.000e-03
0: TRAIN [0][700/3880]	Time 0.191 (0.184)	Data 1.13e-04 (1.17e-03)	Tok/s 43752 (39210)	Loss/tok 4.2861 (6.2282)	LR 2.000e-03
0: TRAIN [0][710/3880]	Time 0.162 (0.184)	Data 5.60e-04 (1.15e-03)	Tok/s 31618 (39192)	Loss/tok 4.0616 (6.2028)	LR 2.000e-03
0: TRAIN [0][720/3880]	Time 0.257 (0.184)	Data 1.04e-04 (1.14e-03)	Tok/s 58002 (39209)	Loss/tok 4.7966 (6.1750)	LR 2.000e-03
0: TRAIN [0][730/3880]	Time 0.162 (0.184)	Data 8.65e-05 (1.13e-03)	Tok/s 31261 (39261)	Loss/tok 4.0653 (6.1461)	LR 2.000e-03
0: TRAIN [0][740/3880]	Time 0.256 (0.185)	Data 5.84e-04 (1.11e-03)	Tok/s 59722 (39323)	Loss/tok 4.6619 (6.1164)	LR 2.000e-03
0: TRAIN [0][750/3880]	Time 0.191 (0.185)	Data 9.49e-05 (1.10e-03)	Tok/s 43761 (39339)	Loss/tok 4.2354 (6.0911)	LR 2.000e-03
0: TRAIN [0][760/3880]	Time 0.257 (0.185)	Data 1.19e-04 (1.09e-03)	Tok/s 57058 (39381)	Loss/tok 4.6011 (6.0648)	LR 2.000e-03
0: TRAIN [0][770/3880]	Time 0.162 (0.184)	Data 1.07e-04 (1.07e-03)	Tok/s 31543 (39299)	Loss/tok 4.1227 (6.0452)	LR 2.000e-03
0: TRAIN [0][780/3880]	Time 0.257 (0.184)	Data 1.10e-04 (1.06e-03)	Tok/s 57928 (39312)	Loss/tok 4.6400 (6.0216)	LR 2.000e-03
0: TRAIN [0][790/3880]	Time 0.221 (0.185)	Data 8.68e-05 (1.05e-03)	Tok/s 52135 (39328)	Loss/tok 4.4221 (5.9976)	LR 2.000e-03
0: TRAIN [0][800/3880]	Time 0.191 (0.184)	Data 9.04e-05 (1.04e-03)	Tok/s 44075 (39303)	Loss/tok 4.1991 (5.9770)	LR 2.000e-03
0: TRAIN [0][810/3880]	Time 0.137 (0.184)	Data 1.17e-04 (1.03e-03)	Tok/s 19778 (39320)	Loss/tok 3.1994 (5.9531)	LR 2.000e-03
0: TRAIN [0][820/3880]	Time 0.162 (0.184)	Data 1.06e-04 (1.01e-03)	Tok/s 32166 (39297)	Loss/tok 3.7121 (5.9330)	LR 2.000e-03
0: TRAIN [0][830/3880]	Time 0.162 (0.184)	Data 1.03e-04 (1.00e-03)	Tok/s 31324 (39291)	Loss/tok 3.9877 (5.9119)	LR 2.000e-03
0: TRAIN [0][840/3880]	Time 0.191 (0.184)	Data 1.03e-04 (9.93e-04)	Tok/s 44206 (39319)	Loss/tok 4.1880 (5.8905)	LR 2.000e-03
0: TRAIN [0][850/3880]	Time 0.163 (0.184)	Data 1.07e-04 (9.83e-04)	Tok/s 31318 (39328)	Loss/tok 4.1370 (5.8696)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][860/3880]	Time 0.191 (0.185)	Data 9.73e-05 (9.73e-04)	Tok/s 44003 (39376)	Loss/tok 4.1664 (5.8481)	LR 2.000e-03
0: TRAIN [0][870/3880]	Time 0.221 (0.185)	Data 8.68e-05 (9.63e-04)	Tok/s 52314 (39363)	Loss/tok 4.3838 (5.8289)	LR 2.000e-03
0: TRAIN [0][880/3880]	Time 0.162 (0.185)	Data 1.03e-04 (9.53e-04)	Tok/s 30875 (39380)	Loss/tok 3.9204 (5.8087)	LR 2.000e-03
0: TRAIN [0][890/3880]	Time 0.257 (0.185)	Data 1.03e-04 (9.44e-04)	Tok/s 57798 (39337)	Loss/tok 4.5145 (5.7918)	LR 2.000e-03
0: TRAIN [0][900/3880]	Time 0.162 (0.185)	Data 9.61e-05 (9.34e-04)	Tok/s 31461 (39334)	Loss/tok 3.8062 (5.7733)	LR 2.000e-03
0: TRAIN [0][910/3880]	Time 0.191 (0.185)	Data 8.87e-05 (9.26e-04)	Tok/s 42581 (39384)	Loss/tok 4.1746 (5.7524)	LR 2.000e-03
0: TRAIN [0][920/3880]	Time 0.191 (0.185)	Data 1.04e-04 (9.17e-04)	Tok/s 44577 (39412)	Loss/tok 4.1484 (5.7330)	LR 2.000e-03
0: TRAIN [0][930/3880]	Time 0.162 (0.185)	Data 1.16e-04 (9.09e-04)	Tok/s 32724 (39439)	Loss/tok 3.6754 (5.7139)	LR 2.000e-03
0: TRAIN [0][940/3880]	Time 0.191 (0.185)	Data 1.02e-04 (9.01e-04)	Tok/s 44856 (39466)	Loss/tok 3.9551 (5.6949)	LR 2.000e-03
0: TRAIN [0][950/3880]	Time 0.191 (0.185)	Data 9.73e-05 (8.92e-04)	Tok/s 44388 (39450)	Loss/tok 4.0316 (5.6783)	LR 2.000e-03
0: TRAIN [0][960/3880]	Time 0.162 (0.185)	Data 9.61e-05 (8.84e-04)	Tok/s 30560 (39382)	Loss/tok 3.6063 (5.6641)	LR 2.000e-03
0: TRAIN [0][970/3880]	Time 0.191 (0.184)	Data 1.03e-04 (8.76e-04)	Tok/s 44635 (39329)	Loss/tok 4.0329 (5.6501)	LR 2.000e-03
0: TRAIN [0][980/3880]	Time 0.257 (0.185)	Data 8.99e-05 (8.69e-04)	Tok/s 58083 (39358)	Loss/tok 4.3409 (5.6326)	LR 2.000e-03
0: TRAIN [0][990/3880]	Time 0.163 (0.184)	Data 1.05e-04 (8.61e-04)	Tok/s 31092 (39291)	Loss/tok 3.7084 (5.6188)	LR 2.000e-03
0: TRAIN [0][1000/3880]	Time 0.191 (0.184)	Data 1.01e-04 (8.53e-04)	Tok/s 43849 (39328)	Loss/tok 4.0773 (5.6015)	LR 2.000e-03
0: TRAIN [0][1010/3880]	Time 0.191 (0.185)	Data 6.01e-04 (8.46e-04)	Tok/s 44317 (39364)	Loss/tok 3.9092 (5.5840)	LR 2.000e-03
0: TRAIN [0][1020/3880]	Time 0.222 (0.185)	Data 1.05e-04 (8.40e-04)	Tok/s 52381 (39364)	Loss/tok 4.1660 (5.5686)	LR 2.000e-03
0: TRAIN [0][1030/3880]	Time 0.221 (0.184)	Data 1.32e-04 (8.33e-04)	Tok/s 52396 (39363)	Loss/tok 4.1496 (5.5535)	LR 2.000e-03
0: TRAIN [0][1040/3880]	Time 0.162 (0.184)	Data 1.04e-04 (8.26e-04)	Tok/s 32896 (39335)	Loss/tok 3.8079 (5.5398)	LR 2.000e-03
0: TRAIN [0][1050/3880]	Time 0.191 (0.184)	Data 1.06e-04 (8.19e-04)	Tok/s 44451 (39263)	Loss/tok 3.8883 (5.5280)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1060/3880]	Time 0.191 (0.184)	Data 1.14e-04 (8.13e-04)	Tok/s 44676 (39307)	Loss/tok 3.8524 (5.5120)	LR 2.000e-03
0: TRAIN [0][1070/3880]	Time 0.136 (0.184)	Data 3.11e-04 (8.06e-04)	Tok/s 19423 (39283)	Loss/tok 3.1341 (5.4987)	LR 2.000e-03
0: TRAIN [0][1080/3880]	Time 0.191 (0.184)	Data 1.35e-04 (8.00e-04)	Tok/s 43273 (39230)	Loss/tok 3.9654 (5.4872)	LR 2.000e-03
0: TRAIN [0][1090/3880]	Time 0.162 (0.184)	Data 1.04e-04 (7.93e-04)	Tok/s 32374 (39196)	Loss/tok 3.7289 (5.4749)	LR 2.000e-03
0: TRAIN [0][1100/3880]	Time 0.136 (0.184)	Data 8.96e-05 (7.87e-04)	Tok/s 19444 (39150)	Loss/tok 3.1137 (5.4634)	LR 2.000e-03
0: TRAIN [0][1110/3880]	Time 0.221 (0.184)	Data 8.99e-05 (7.81e-04)	Tok/s 52911 (39156)	Loss/tok 4.3376 (5.4495)	LR 2.000e-03
0: TRAIN [0][1120/3880]	Time 0.191 (0.184)	Data 9.16e-05 (7.75e-04)	Tok/s 44709 (39215)	Loss/tok 4.0390 (5.4343)	LR 2.000e-03
0: TRAIN [0][1130/3880]	Time 0.191 (0.184)	Data 8.99e-05 (7.69e-04)	Tok/s 43958 (39234)	Loss/tok 3.8693 (5.4207)	LR 2.000e-03
0: TRAIN [0][1140/3880]	Time 0.162 (0.184)	Data 1.21e-04 (7.63e-04)	Tok/s 30819 (39259)	Loss/tok 3.7049 (5.4069)	LR 2.000e-03
0: TRAIN [0][1150/3880]	Time 0.137 (0.184)	Data 9.18e-05 (7.58e-04)	Tok/s 18965 (39246)	Loss/tok 3.1709 (5.3944)	LR 2.000e-03
0: TRAIN [0][1160/3880]	Time 0.162 (0.184)	Data 1.24e-04 (7.52e-04)	Tok/s 32185 (39246)	Loss/tok 3.6971 (5.3820)	LR 2.000e-03
0: TRAIN [0][1170/3880]	Time 0.191 (0.184)	Data 9.06e-05 (7.47e-04)	Tok/s 44204 (39276)	Loss/tok 4.0065 (5.3687)	LR 2.000e-03
0: TRAIN [0][1180/3880]	Time 0.162 (0.184)	Data 1.04e-04 (7.42e-04)	Tok/s 32371 (39225)	Loss/tok 3.7442 (5.3587)	LR 2.000e-03
0: TRAIN [0][1190/3880]	Time 0.191 (0.184)	Data 1.11e-04 (7.36e-04)	Tok/s 43591 (39228)	Loss/tok 3.8859 (5.3466)	LR 2.000e-03
0: TRAIN [0][1200/3880]	Time 0.191 (0.184)	Data 1.13e-04 (7.31e-04)	Tok/s 43799 (39283)	Loss/tok 3.8584 (5.3321)	LR 2.000e-03
0: TRAIN [0][1210/3880]	Time 0.162 (0.184)	Data 1.18e-04 (7.26e-04)	Tok/s 32311 (39275)	Loss/tok 3.7693 (5.3205)	LR 2.000e-03
0: TRAIN [0][1220/3880]	Time 0.162 (0.184)	Data 8.73e-05 (7.21e-04)	Tok/s 32090 (39214)	Loss/tok 3.5050 (5.3108)	LR 2.000e-03
0: TRAIN [0][1230/3880]	Time 0.162 (0.184)	Data 1.10e-04 (7.16e-04)	Tok/s 31691 (39197)	Loss/tok 3.6249 (5.3002)	LR 2.000e-03
0: TRAIN [0][1240/3880]	Time 0.191 (0.184)	Data 9.11e-05 (7.11e-04)	Tok/s 44048 (39228)	Loss/tok 3.9017 (5.2879)	LR 2.000e-03
0: TRAIN [0][1250/3880]	Time 0.162 (0.184)	Data 1.01e-04 (7.06e-04)	Tok/s 31729 (39190)	Loss/tok 3.5978 (5.2780)	LR 2.000e-03
0: TRAIN [0][1260/3880]	Time 0.162 (0.184)	Data 9.42e-05 (7.01e-04)	Tok/s 30753 (39189)	Loss/tok 3.4265 (5.2668)	LR 2.000e-03
0: TRAIN [0][1270/3880]	Time 0.191 (0.184)	Data 1.04e-04 (6.97e-04)	Tok/s 43578 (39172)	Loss/tok 3.9140 (5.2563)	LR 2.000e-03
0: TRAIN [0][1280/3880]	Time 0.162 (0.184)	Data 8.77e-05 (6.92e-04)	Tok/s 31240 (39189)	Loss/tok 3.7557 (5.2447)	LR 2.000e-03
0: TRAIN [0][1290/3880]	Time 0.191 (0.184)	Data 1.03e-04 (6.88e-04)	Tok/s 43354 (39167)	Loss/tok 3.9624 (5.2346)	LR 2.000e-03
0: TRAIN [0][1300/3880]	Time 0.162 (0.184)	Data 1.05e-04 (6.83e-04)	Tok/s 31972 (39143)	Loss/tok 3.6708 (5.2257)	LR 2.000e-03
0: TRAIN [0][1310/3880]	Time 0.191 (0.184)	Data 1.12e-04 (6.79e-04)	Tok/s 44273 (39162)	Loss/tok 3.9063 (5.2151)	LR 2.000e-03
0: TRAIN [0][1320/3880]	Time 0.191 (0.184)	Data 1.27e-04 (6.74e-04)	Tok/s 44116 (39136)	Loss/tok 3.8190 (5.2055)	LR 2.000e-03
0: TRAIN [0][1330/3880]	Time 0.162 (0.184)	Data 1.18e-04 (6.70e-04)	Tok/s 32014 (39130)	Loss/tok 3.6416 (5.1957)	LR 2.000e-03
0: TRAIN [0][1340/3880]	Time 0.221 (0.184)	Data 1.22e-04 (6.66e-04)	Tok/s 53175 (39145)	Loss/tok 3.9841 (5.1851)	LR 2.000e-03
0: TRAIN [0][1350/3880]	Time 0.162 (0.184)	Data 1.39e-04 (6.62e-04)	Tok/s 32038 (39158)	Loss/tok 3.6755 (5.1747)	LR 2.000e-03
0: TRAIN [0][1360/3880]	Time 0.191 (0.184)	Data 1.04e-04 (6.58e-04)	Tok/s 44649 (39189)	Loss/tok 3.8809 (5.1640)	LR 2.000e-03
0: TRAIN [0][1370/3880]	Time 0.191 (0.184)	Data 8.92e-05 (6.54e-04)	Tok/s 43561 (39200)	Loss/tok 3.8549 (5.1536)	LR 2.000e-03
0: TRAIN [0][1380/3880]	Time 0.162 (0.184)	Data 1.19e-04 (6.50e-04)	Tok/s 31239 (39202)	Loss/tok 3.4881 (5.1437)	LR 2.000e-03
0: TRAIN [0][1390/3880]	Time 0.162 (0.184)	Data 1.06e-04 (6.47e-04)	Tok/s 31526 (39196)	Loss/tok 3.6638 (5.1344)	LR 2.000e-03
0: TRAIN [0][1400/3880]	Time 0.162 (0.184)	Data 9.39e-05 (6.43e-04)	Tok/s 32565 (39183)	Loss/tok 3.6532 (5.1257)	LR 2.000e-03
0: TRAIN [0][1410/3880]	Time 0.191 (0.184)	Data 4.29e-04 (6.39e-04)	Tok/s 43661 (39189)	Loss/tok 3.8350 (5.1162)	LR 2.000e-03
0: TRAIN [0][1420/3880]	Time 0.191 (0.184)	Data 1.01e-04 (6.35e-04)	Tok/s 44749 (39147)	Loss/tok 3.9641 (5.1085)	LR 2.000e-03
0: TRAIN [0][1430/3880]	Time 0.191 (0.184)	Data 1.16e-04 (6.32e-04)	Tok/s 44361 (39144)	Loss/tok 3.7648 (5.0995)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1440/3880]	Time 0.162 (0.184)	Data 1.05e-04 (6.28e-04)	Tok/s 31145 (39161)	Loss/tok 3.7176 (5.0904)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1450/3880]	Time 0.221 (0.184)	Data 8.92e-05 (6.24e-04)	Tok/s 52286 (39139)	Loss/tok 4.1499 (5.0825)	LR 2.000e-03
0: TRAIN [0][1460/3880]	Time 0.191 (0.183)	Data 1.03e-04 (6.21e-04)	Tok/s 45004 (39098)	Loss/tok 3.7376 (5.0753)	LR 2.000e-03
0: TRAIN [0][1470/3880]	Time 0.221 (0.183)	Data 9.47e-05 (6.17e-04)	Tok/s 54356 (39108)	Loss/tok 4.0525 (5.0663)	LR 2.000e-03
0: TRAIN [0][1480/3880]	Time 0.191 (0.183)	Data 1.03e-04 (6.14e-04)	Tok/s 44021 (39110)	Loss/tok 3.6134 (5.0581)	LR 2.000e-03
0: TRAIN [0][1490/3880]	Time 0.191 (0.183)	Data 9.89e-05 (6.10e-04)	Tok/s 44734 (39109)	Loss/tok 3.7152 (5.0499)	LR 2.000e-03
0: TRAIN [0][1500/3880]	Time 0.221 (0.183)	Data 1.08e-04 (6.07e-04)	Tok/s 52294 (39110)	Loss/tok 3.8607 (5.0417)	LR 2.000e-03
0: TRAIN [0][1510/3880]	Time 0.136 (0.183)	Data 8.80e-05 (6.04e-04)	Tok/s 19254 (39105)	Loss/tok 3.0795 (5.0337)	LR 2.000e-03
0: TRAIN [0][1520/3880]	Time 0.162 (0.183)	Data 1.01e-04 (6.00e-04)	Tok/s 31994 (39091)	Loss/tok 3.5902 (5.0256)	LR 2.000e-03
0: TRAIN [0][1530/3880]	Time 0.191 (0.183)	Data 1.06e-04 (5.97e-04)	Tok/s 44532 (39057)	Loss/tok 3.7448 (5.0186)	LR 2.000e-03
0: TRAIN [0][1540/3880]	Time 0.136 (0.183)	Data 1.10e-04 (5.94e-04)	Tok/s 18670 (39049)	Loss/tok 2.8742 (5.0104)	LR 2.000e-03
0: TRAIN [0][1550/3880]	Time 0.191 (0.183)	Data 1.09e-04 (5.91e-04)	Tok/s 44252 (39054)	Loss/tok 3.8000 (5.0028)	LR 2.000e-03
0: TRAIN [0][1560/3880]	Time 0.191 (0.183)	Data 9.37e-05 (5.88e-04)	Tok/s 44412 (39059)	Loss/tok 3.7491 (4.9946)	LR 2.000e-03
0: TRAIN [0][1570/3880]	Time 0.136 (0.183)	Data 9.49e-05 (5.85e-04)	Tok/s 18899 (39052)	Loss/tok 2.8289 (4.9873)	LR 2.000e-03
0: TRAIN [0][1580/3880]	Time 0.162 (0.183)	Data 1.19e-04 (5.82e-04)	Tok/s 32051 (39038)	Loss/tok 3.5380 (4.9801)	LR 2.000e-03
0: TRAIN [0][1590/3880]	Time 0.162 (0.183)	Data 1.17e-04 (5.79e-04)	Tok/s 30686 (39036)	Loss/tok 3.6459 (4.9729)	LR 2.000e-03
0: TRAIN [0][1600/3880]	Time 0.163 (0.183)	Data 1.08e-04 (5.77e-04)	Tok/s 32130 (39029)	Loss/tok 3.4057 (4.9654)	LR 2.000e-03
0: TRAIN [0][1610/3880]	Time 0.191 (0.183)	Data 9.44e-05 (5.74e-04)	Tok/s 44214 (39006)	Loss/tok 3.7453 (4.9588)	LR 2.000e-03
0: TRAIN [0][1620/3880]	Time 0.162 (0.183)	Data 1.18e-04 (5.71e-04)	Tok/s 31922 (38989)	Loss/tok 3.5196 (4.9518)	LR 2.000e-03
0: TRAIN [0][1630/3880]	Time 0.137 (0.183)	Data 1.30e-04 (5.68e-04)	Tok/s 19124 (38985)	Loss/tok 2.8344 (4.9444)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1640/3880]	Time 0.191 (0.183)	Data 9.32e-05 (5.66e-04)	Tok/s 44340 (38998)	Loss/tok 3.8247 (4.9372)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1650/3880]	Time 0.162 (0.183)	Data 9.30e-05 (5.63e-04)	Tok/s 32330 (39001)	Loss/tok 3.6715 (4.9298)	LR 2.000e-03
0: TRAIN [0][1660/3880]	Time 0.162 (0.183)	Data 1.05e-04 (5.60e-04)	Tok/s 31829 (38980)	Loss/tok 3.3436 (4.9235)	LR 2.000e-03
0: TRAIN [0][1670/3880]	Time 0.191 (0.183)	Data 1.11e-04 (5.58e-04)	Tok/s 44228 (38978)	Loss/tok 3.6081 (4.9168)	LR 2.000e-03
0: TRAIN [0][1680/3880]	Time 0.191 (0.183)	Data 1.02e-04 (5.55e-04)	Tok/s 43986 (38963)	Loss/tok 3.6589 (4.9102)	LR 2.000e-03
0: TRAIN [0][1690/3880]	Time 0.220 (0.183)	Data 1.19e-04 (5.53e-04)	Tok/s 52704 (38947)	Loss/tok 4.0908 (4.9038)	LR 2.000e-03
0: TRAIN [0][1700/3880]	Time 0.191 (0.183)	Data 8.92e-05 (5.50e-04)	Tok/s 45215 (38942)	Loss/tok 3.6630 (4.8972)	LR 2.000e-03
0: TRAIN [0][1710/3880]	Time 0.192 (0.183)	Data 1.01e-04 (5.48e-04)	Tok/s 43581 (38936)	Loss/tok 3.7117 (4.8904)	LR 2.000e-03
0: TRAIN [0][1720/3880]	Time 0.162 (0.183)	Data 1.19e-04 (5.45e-04)	Tok/s 31790 (38909)	Loss/tok 3.5681 (4.8845)	LR 2.000e-03
0: TRAIN [0][1730/3880]	Time 0.163 (0.183)	Data 1.22e-04 (5.43e-04)	Tok/s 30900 (38874)	Loss/tok 3.5176 (4.8788)	LR 2.000e-03
0: TRAIN [0][1740/3880]	Time 0.191 (0.183)	Data 1.30e-04 (5.40e-04)	Tok/s 43102 (38847)	Loss/tok 3.7598 (4.8728)	LR 2.000e-03
0: TRAIN [0][1750/3880]	Time 0.137 (0.183)	Data 1.46e-04 (5.38e-04)	Tok/s 19888 (38838)	Loss/tok 2.9868 (4.8664)	LR 2.000e-03
0: TRAIN [0][1760/3880]	Time 0.136 (0.182)	Data 1.12e-04 (5.35e-04)	Tok/s 19570 (38822)	Loss/tok 2.8564 (4.8606)	LR 2.000e-03
0: TRAIN [0][1770/3880]	Time 0.162 (0.183)	Data 9.13e-05 (5.33e-04)	Tok/s 30775 (38828)	Loss/tok 3.4696 (4.8543)	LR 2.000e-03
0: TRAIN [0][1780/3880]	Time 0.163 (0.182)	Data 1.35e-04 (5.31e-04)	Tok/s 32119 (38811)	Loss/tok 3.4239 (4.8483)	LR 2.000e-03
0: TRAIN [0][1790/3880]	Time 0.192 (0.182)	Data 1.31e-04 (5.28e-04)	Tok/s 44094 (38804)	Loss/tok 3.7398 (4.8418)	LR 2.000e-03
0: TRAIN [0][1800/3880]	Time 0.191 (0.182)	Data 1.15e-04 (5.26e-04)	Tok/s 43843 (38795)	Loss/tok 3.6806 (4.8357)	LR 2.000e-03
0: TRAIN [0][1810/3880]	Time 0.163 (0.182)	Data 1.29e-04 (5.24e-04)	Tok/s 32313 (38812)	Loss/tok 3.3978 (4.8292)	LR 2.000e-03
0: TRAIN [0][1820/3880]	Time 0.221 (0.182)	Data 1.66e-04 (5.22e-04)	Tok/s 52389 (38809)	Loss/tok 3.9893 (4.8233)	LR 2.000e-03
0: TRAIN [0][1830/3880]	Time 0.162 (0.182)	Data 9.54e-05 (5.20e-04)	Tok/s 31404 (38803)	Loss/tok 3.4595 (4.8176)	LR 2.000e-03
0: TRAIN [0][1840/3880]	Time 0.162 (0.182)	Data 1.18e-04 (5.17e-04)	Tok/s 32415 (38786)	Loss/tok 3.5794 (4.8122)	LR 2.000e-03
0: TRAIN [0][1850/3880]	Time 0.136 (0.182)	Data 1.14e-04 (5.15e-04)	Tok/s 19670 (38780)	Loss/tok 2.9376 (4.8066)	LR 2.000e-03
0: TRAIN [0][1860/3880]	Time 0.162 (0.183)	Data 1.24e-04 (5.13e-04)	Tok/s 32300 (38818)	Loss/tok 3.2984 (4.7997)	LR 2.000e-03
0: TRAIN [0][1870/3880]	Time 0.162 (0.182)	Data 1.12e-04 (5.11e-04)	Tok/s 32528 (38793)	Loss/tok 3.5374 (4.7944)	LR 2.000e-03
0: TRAIN [0][1880/3880]	Time 0.191 (0.182)	Data 9.54e-05 (5.09e-04)	Tok/s 44811 (38765)	Loss/tok 3.5351 (4.7891)	LR 2.000e-03
0: TRAIN [0][1890/3880]	Time 0.191 (0.182)	Data 1.17e-04 (5.07e-04)	Tok/s 43493 (38780)	Loss/tok 3.7755 (4.7830)	LR 2.000e-03
0: TRAIN [0][1900/3880]	Time 0.136 (0.182)	Data 1.05e-04 (5.05e-04)	Tok/s 19256 (38767)	Loss/tok 2.9381 (4.7778)	LR 2.000e-03
0: TRAIN [0][1910/3880]	Time 0.221 (0.182)	Data 8.73e-05 (5.03e-04)	Tok/s 53198 (38770)	Loss/tok 3.7532 (4.7723)	LR 2.000e-03
0: TRAIN [0][1920/3880]	Time 0.191 (0.182)	Data 1.24e-04 (5.01e-04)	Tok/s 44271 (38746)	Loss/tok 3.6457 (4.7671)	LR 2.000e-03
0: TRAIN [0][1930/3880]	Time 0.191 (0.182)	Data 1.29e-04 (4.99e-04)	Tok/s 44330 (38728)	Loss/tok 3.6857 (4.7618)	LR 2.000e-03
0: TRAIN [0][1940/3880]	Time 0.162 (0.182)	Data 1.18e-04 (4.97e-04)	Tok/s 31763 (38734)	Loss/tok 3.4325 (4.7561)	LR 2.000e-03
0: TRAIN [0][1950/3880]	Time 0.191 (0.182)	Data 1.17e-04 (4.95e-04)	Tok/s 44100 (38741)	Loss/tok 3.5530 (4.7503)	LR 2.000e-03
0: TRAIN [0][1960/3880]	Time 0.221 (0.182)	Data 1.03e-04 (4.93e-04)	Tok/s 52738 (38760)	Loss/tok 3.8402 (4.7445)	LR 2.000e-03
0: TRAIN [0][1970/3880]	Time 0.137 (0.182)	Data 9.42e-05 (4.92e-04)	Tok/s 19835 (38743)	Loss/tok 2.8528 (4.7398)	LR 2.000e-03
0: TRAIN [0][1980/3880]	Time 0.163 (0.182)	Data 1.18e-04 (4.90e-04)	Tok/s 31770 (38749)	Loss/tok 3.6050 (4.7344)	LR 2.000e-03
0: TRAIN [0][1990/3880]	Time 0.191 (0.182)	Data 1.24e-04 (4.88e-04)	Tok/s 44210 (38744)	Loss/tok 3.7435 (4.7293)	LR 2.000e-03
0: TRAIN [0][2000/3880]	Time 0.162 (0.182)	Data 1.06e-04 (4.86e-04)	Tok/s 31764 (38753)	Loss/tok 3.6034 (4.7240)	LR 2.000e-03
0: TRAIN [0][2010/3880]	Time 0.162 (0.182)	Data 9.35e-05 (4.85e-04)	Tok/s 32220 (38733)	Loss/tok 3.3430 (4.7193)	LR 2.000e-03
0: TRAIN [0][2020/3880]	Time 0.162 (0.182)	Data 8.87e-05 (4.83e-04)	Tok/s 32112 (38719)	Loss/tok 3.3752 (4.7144)	LR 2.000e-03
0: TRAIN [0][2030/3880]	Time 0.137 (0.182)	Data 1.23e-04 (4.81e-04)	Tok/s 19349 (38701)	Loss/tok 2.8075 (4.7096)	LR 2.000e-03
0: TRAIN [0][2040/3880]	Time 0.191 (0.182)	Data 1.30e-04 (4.79e-04)	Tok/s 42915 (38726)	Loss/tok 3.6449 (4.7042)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2050/3880]	Time 0.221 (0.182)	Data 1.16e-04 (4.78e-04)	Tok/s 53296 (38759)	Loss/tok 3.9631 (4.6985)	LR 2.000e-03
0: TRAIN [0][2060/3880]	Time 0.136 (0.182)	Data 8.87e-05 (4.76e-04)	Tok/s 19710 (38727)	Loss/tok 2.8412 (4.6940)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][2070/3880]	Time 0.221 (0.182)	Data 9.32e-05 (4.74e-04)	Tok/s 53510 (38746)	Loss/tok 3.6891 (4.6887)	LR 2.000e-03
0: TRAIN [0][2080/3880]	Time 0.137 (0.182)	Data 1.15e-04 (4.72e-04)	Tok/s 19337 (38733)	Loss/tok 2.8116 (4.6840)	LR 2.000e-03
0: TRAIN [0][2090/3880]	Time 0.163 (0.182)	Data 9.18e-05 (4.71e-04)	Tok/s 31338 (38746)	Loss/tok 3.3998 (4.6790)	LR 2.000e-03
0: TRAIN [0][2100/3880]	Time 0.162 (0.182)	Data 1.19e-04 (4.69e-04)	Tok/s 31500 (38756)	Loss/tok 3.3251 (4.6738)	LR 2.000e-03
0: TRAIN [0][2110/3880]	Time 0.162 (0.182)	Data 1.19e-04 (4.67e-04)	Tok/s 31816 (38761)	Loss/tok 3.6039 (4.6689)	LR 2.000e-03
0: TRAIN [0][2120/3880]	Time 0.136 (0.182)	Data 1.09e-04 (4.66e-04)	Tok/s 19037 (38765)	Loss/tok 2.8305 (4.6642)	LR 2.000e-03
0: TRAIN [0][2130/3880]	Time 0.191 (0.182)	Data 1.32e-04 (4.64e-04)	Tok/s 44616 (38781)	Loss/tok 3.4389 (4.6589)	LR 2.000e-03
0: TRAIN [0][2140/3880]	Time 0.162 (0.182)	Data 1.33e-04 (4.63e-04)	Tok/s 32599 (38776)	Loss/tok 3.3648 (4.6545)	LR 2.000e-03
0: TRAIN [0][2150/3880]	Time 0.162 (0.182)	Data 8.75e-05 (4.61e-04)	Tok/s 31725 (38777)	Loss/tok 3.2115 (4.6501)	LR 2.000e-03
0: TRAIN [0][2160/3880]	Time 0.191 (0.182)	Data 1.03e-04 (4.59e-04)	Tok/s 43512 (38765)	Loss/tok 3.5578 (4.6457)	LR 2.000e-03
0: TRAIN [0][2170/3880]	Time 0.136 (0.182)	Data 1.02e-04 (4.58e-04)	Tok/s 19591 (38748)	Loss/tok 2.8297 (4.6417)	LR 2.000e-03
0: TRAIN [0][2180/3880]	Time 0.191 (0.182)	Data 1.14e-04 (4.56e-04)	Tok/s 43659 (38720)	Loss/tok 3.7049 (4.6376)	LR 2.000e-03
0: TRAIN [0][2190/3880]	Time 0.162 (0.182)	Data 1.03e-04 (4.55e-04)	Tok/s 31897 (38715)	Loss/tok 3.4416 (4.6333)	LR 2.000e-03
0: TRAIN [0][2200/3880]	Time 0.221 (0.182)	Data 1.59e-04 (4.53e-04)	Tok/s 53186 (38720)	Loss/tok 3.7591 (4.6288)	LR 2.000e-03
0: TRAIN [0][2210/3880]	Time 0.162 (0.182)	Data 1.03e-04 (4.52e-04)	Tok/s 32129 (38717)	Loss/tok 3.3125 (4.6243)	LR 2.000e-03
0: TRAIN [0][2220/3880]	Time 0.162 (0.182)	Data 1.18e-04 (4.50e-04)	Tok/s 31976 (38706)	Loss/tok 3.3625 (4.6198)	LR 2.000e-03
0: TRAIN [0][2230/3880]	Time 0.162 (0.182)	Data 1.04e-04 (4.49e-04)	Tok/s 32082 (38711)	Loss/tok 3.4215 (4.6153)	LR 2.000e-03
0: TRAIN [0][2240/3880]	Time 0.162 (0.182)	Data 1.14e-04 (4.47e-04)	Tok/s 32319 (38697)	Loss/tok 3.4268 (4.6113)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][2250/3880]	Time 0.191 (0.182)	Data 1.00e-04 (4.46e-04)	Tok/s 43853 (38697)	Loss/tok 3.6034 (4.6070)	LR 2.000e-03
0: TRAIN [0][2260/3880]	Time 0.221 (0.182)	Data 1.17e-04 (4.45e-04)	Tok/s 53073 (38726)	Loss/tok 3.7776 (4.6021)	LR 2.000e-03
0: TRAIN [0][2270/3880]	Time 0.162 (0.182)	Data 1.09e-04 (4.43e-04)	Tok/s 31819 (38739)	Loss/tok 3.5471 (4.5975)	LR 2.000e-03
0: TRAIN [0][2280/3880]	Time 0.221 (0.182)	Data 1.04e-04 (4.42e-04)	Tok/s 53247 (38718)	Loss/tok 3.7897 (4.5936)	LR 2.000e-03
0: TRAIN [0][2290/3880]	Time 0.221 (0.182)	Data 1.02e-04 (4.40e-04)	Tok/s 52701 (38722)	Loss/tok 3.7986 (4.5894)	LR 2.000e-03
0: TRAIN [0][2300/3880]	Time 0.221 (0.182)	Data 1.02e-04 (4.39e-04)	Tok/s 53530 (38736)	Loss/tok 3.6934 (4.5849)	LR 2.000e-03
0: TRAIN [0][2310/3880]	Time 0.221 (0.182)	Data 1.01e-04 (4.37e-04)	Tok/s 52677 (38756)	Loss/tok 3.8238 (4.5804)	LR 2.000e-03
0: TRAIN [0][2320/3880]	Time 0.136 (0.182)	Data 1.16e-04 (4.36e-04)	Tok/s 19461 (38730)	Loss/tok 2.7688 (4.5768)	LR 2.000e-03
0: TRAIN [0][2330/3880]	Time 0.162 (0.182)	Data 1.17e-04 (4.35e-04)	Tok/s 32318 (38735)	Loss/tok 3.2623 (4.5726)	LR 2.000e-03
0: TRAIN [0][2340/3880]	Time 0.162 (0.182)	Data 1.09e-04 (4.33e-04)	Tok/s 32200 (38749)	Loss/tok 3.3958 (4.5682)	LR 2.000e-03
0: TRAIN [0][2350/3880]	Time 0.191 (0.182)	Data 1.22e-04 (4.32e-04)	Tok/s 44712 (38740)	Loss/tok 3.5471 (4.5644)	LR 2.000e-03
0: TRAIN [0][2360/3880]	Time 0.265 (0.182)	Data 1.21e-04 (4.31e-04)	Tok/s 56318 (38749)	Loss/tok 4.1755 (4.5607)	LR 2.000e-03
0: TRAIN [0][2370/3880]	Time 0.136 (0.182)	Data 1.26e-04 (4.30e-04)	Tok/s 19084 (38763)	Loss/tok 2.9664 (4.5567)	LR 2.000e-03
0: TRAIN [0][2380/3880]	Time 0.257 (0.182)	Data 1.03e-04 (4.28e-04)	Tok/s 56483 (38781)	Loss/tok 4.1613 (4.5524)	LR 2.000e-03
0: TRAIN [0][2390/3880]	Time 0.136 (0.182)	Data 8.65e-05 (4.27e-04)	Tok/s 19227 (38776)	Loss/tok 2.8500 (4.5488)	LR 2.000e-03
0: TRAIN [0][2400/3880]	Time 0.136 (0.182)	Data 1.02e-04 (4.26e-04)	Tok/s 19827 (38741)	Loss/tok 2.8389 (4.5455)	LR 2.000e-03
0: TRAIN [0][2410/3880]	Time 0.221 (0.182)	Data 1.03e-04 (4.24e-04)	Tok/s 51708 (38743)	Loss/tok 3.9334 (4.5417)	LR 2.000e-03
0: TRAIN [0][2420/3880]	Time 0.191 (0.182)	Data 1.12e-04 (4.23e-04)	Tok/s 43104 (38760)	Loss/tok 3.6217 (4.5377)	LR 2.000e-03
0: TRAIN [0][2430/3880]	Time 0.162 (0.182)	Data 8.94e-05 (4.22e-04)	Tok/s 31277 (38746)	Loss/tok 3.4490 (4.5341)	LR 2.000e-03
0: TRAIN [0][2440/3880]	Time 0.138 (0.182)	Data 8.89e-05 (4.21e-04)	Tok/s 19438 (38732)	Loss/tok 3.1606 (4.5304)	LR 2.000e-03
0: TRAIN [0][2450/3880]	Time 0.162 (0.182)	Data 1.13e-04 (4.20e-04)	Tok/s 31300 (38727)	Loss/tok 3.3655 (4.5268)	LR 2.000e-03
0: TRAIN [0][2460/3880]	Time 0.162 (0.182)	Data 1.02e-04 (4.19e-04)	Tok/s 31465 (38743)	Loss/tok 3.3231 (4.5230)	LR 2.000e-03
0: TRAIN [0][2470/3880]	Time 0.191 (0.182)	Data 1.01e-04 (4.17e-04)	Tok/s 44005 (38738)	Loss/tok 3.6194 (4.5193)	LR 2.000e-03
0: TRAIN [0][2480/3880]	Time 0.162 (0.182)	Data 2.14e-04 (4.16e-04)	Tok/s 31588 (38725)	Loss/tok 3.4771 (4.5158)	LR 2.000e-03
0: TRAIN [0][2490/3880]	Time 0.162 (0.182)	Data 8.99e-05 (4.15e-04)	Tok/s 32524 (38726)	Loss/tok 3.2527 (4.5118)	LR 2.000e-03
0: TRAIN [0][2500/3880]	Time 0.221 (0.182)	Data 9.70e-05 (4.14e-04)	Tok/s 52679 (38742)	Loss/tok 3.6841 (4.5077)	LR 2.000e-03
0: TRAIN [0][2510/3880]	Time 0.162 (0.182)	Data 1.44e-04 (4.13e-04)	Tok/s 31296 (38750)	Loss/tok 3.3101 (4.5039)	LR 2.000e-03
0: TRAIN [0][2520/3880]	Time 0.191 (0.182)	Data 9.70e-05 (4.11e-04)	Tok/s 43933 (38771)	Loss/tok 3.5875 (4.4998)	LR 2.000e-03
0: TRAIN [0][2530/3880]	Time 0.191 (0.182)	Data 1.15e-04 (4.10e-04)	Tok/s 44036 (38784)	Loss/tok 3.5555 (4.4960)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2540/3880]	Time 0.191 (0.182)	Data 1.05e-04 (4.09e-04)	Tok/s 43758 (38794)	Loss/tok 3.5993 (4.4925)	LR 2.000e-03
0: TRAIN [0][2550/3880]	Time 0.162 (0.182)	Data 9.49e-05 (4.08e-04)	Tok/s 30414 (38797)	Loss/tok 3.3868 (4.4890)	LR 2.000e-03
0: TRAIN [0][2560/3880]	Time 0.163 (0.182)	Data 1.06e-04 (4.07e-04)	Tok/s 30220 (38787)	Loss/tok 3.3915 (4.4858)	LR 2.000e-03
0: TRAIN [0][2570/3880]	Time 0.136 (0.182)	Data 1.07e-04 (4.06e-04)	Tok/s 19630 (38764)	Loss/tok 2.8521 (4.4827)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][2580/3880]	Time 0.158 (0.182)	Data 1.11e-04 (4.05e-04)	Tok/s 32825 (38755)	Loss/tok 3.2182 (4.4795)	LR 2.000e-03
0: TRAIN [0][2590/3880]	Time 0.162 (0.182)	Data 1.06e-04 (4.03e-04)	Tok/s 32572 (38757)	Loss/tok 3.3617 (4.4761)	LR 2.000e-03
0: TRAIN [0][2600/3880]	Time 0.191 (0.182)	Data 9.44e-05 (4.02e-04)	Tok/s 44660 (38748)	Loss/tok 3.6421 (4.4731)	LR 2.000e-03
0: TRAIN [0][2610/3880]	Time 0.191 (0.182)	Data 8.96e-05 (4.01e-04)	Tok/s 43337 (38743)	Loss/tok 3.5372 (4.4697)	LR 2.000e-03
0: TRAIN [0][2620/3880]	Time 0.162 (0.182)	Data 1.09e-04 (4.00e-04)	Tok/s 31411 (38740)	Loss/tok 3.4326 (4.4663)	LR 2.000e-03
0: TRAIN [0][2630/3880]	Time 0.191 (0.182)	Data 1.03e-04 (3.99e-04)	Tok/s 43297 (38749)	Loss/tok 3.6439 (4.4633)	LR 2.000e-03
0: TRAIN [0][2640/3880]	Time 0.163 (0.182)	Data 9.11e-05 (3.98e-04)	Tok/s 30611 (38733)	Loss/tok 3.3045 (4.4602)	LR 2.000e-03
0: TRAIN [0][2650/3880]	Time 0.191 (0.182)	Data 9.97e-05 (3.97e-04)	Tok/s 43318 (38732)	Loss/tok 3.5594 (4.4569)	LR 2.000e-03
0: TRAIN [0][2660/3880]	Time 0.162 (0.182)	Data 8.85e-05 (3.96e-04)	Tok/s 31374 (38738)	Loss/tok 3.4563 (4.4535)	LR 2.000e-03
0: TRAIN [0][2670/3880]	Time 0.162 (0.182)	Data 1.03e-04 (3.95e-04)	Tok/s 31714 (38725)	Loss/tok 3.3917 (4.4505)	LR 2.000e-03
0: TRAIN [0][2680/3880]	Time 0.257 (0.182)	Data 1.03e-04 (3.94e-04)	Tok/s 57744 (38724)	Loss/tok 4.0601 (4.4472)	LR 2.000e-03
0: TRAIN [0][2690/3880]	Time 0.162 (0.182)	Data 3.27e-04 (3.93e-04)	Tok/s 31297 (38733)	Loss/tok 3.2609 (4.4441)	LR 2.000e-03
0: TRAIN [0][2700/3880]	Time 0.162 (0.182)	Data 1.03e-04 (3.92e-04)	Tok/s 32539 (38744)	Loss/tok 3.4414 (4.4407)	LR 2.000e-03
0: TRAIN [0][2710/3880]	Time 0.191 (0.182)	Data 9.23e-05 (3.90e-04)	Tok/s 45407 (38754)	Loss/tok 3.5451 (4.4375)	LR 2.000e-03
0: TRAIN [0][2720/3880]	Time 0.163 (0.182)	Data 9.01e-05 (3.89e-04)	Tok/s 31546 (38753)	Loss/tok 3.2890 (4.4342)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][2730/3880]	Time 0.220 (0.182)	Data 1.09e-04 (3.88e-04)	Tok/s 52806 (38775)	Loss/tok 3.8420 (4.4306)	LR 2.000e-03
0: TRAIN [0][2740/3880]	Time 0.162 (0.182)	Data 1.27e-04 (3.87e-04)	Tok/s 31567 (38787)	Loss/tok 3.5221 (4.4273)	LR 2.000e-03
0: TRAIN [0][2750/3880]	Time 0.162 (0.182)	Data 1.08e-04 (3.86e-04)	Tok/s 31770 (38791)	Loss/tok 3.3393 (4.4237)	LR 2.000e-03
0: TRAIN [0][2760/3880]	Time 0.137 (0.182)	Data 9.51e-05 (3.85e-04)	Tok/s 18951 (38782)	Loss/tok 2.8909 (4.4209)	LR 2.000e-03
0: TRAIN [0][2770/3880]	Time 0.162 (0.182)	Data 9.92e-05 (3.84e-04)	Tok/s 30943 (38764)	Loss/tok 3.2914 (4.4180)	LR 2.000e-03
0: TRAIN [0][2780/3880]	Time 0.162 (0.182)	Data 1.48e-04 (3.83e-04)	Tok/s 32211 (38771)	Loss/tok 3.4247 (4.4150)	LR 2.000e-03
0: TRAIN [0][2790/3880]	Time 0.162 (0.182)	Data 8.61e-05 (3.83e-04)	Tok/s 32873 (38764)	Loss/tok 3.3580 (4.4122)	LR 2.000e-03
0: TRAIN [0][2800/3880]	Time 0.191 (0.182)	Data 1.09e-04 (3.82e-04)	Tok/s 44151 (38754)	Loss/tok 3.4181 (4.4092)	LR 2.000e-03
0: TRAIN [0][2810/3880]	Time 0.162 (0.182)	Data 9.01e-05 (3.81e-04)	Tok/s 31684 (38732)	Loss/tok 3.4440 (4.4065)	LR 2.000e-03
0: TRAIN [0][2820/3880]	Time 0.191 (0.182)	Data 1.22e-04 (3.80e-04)	Tok/s 44647 (38719)	Loss/tok 3.4914 (4.4039)	LR 2.000e-03
0: TRAIN [0][2830/3880]	Time 0.136 (0.182)	Data 1.03e-04 (3.79e-04)	Tok/s 19512 (38710)	Loss/tok 2.8237 (4.4011)	LR 2.000e-03
0: TRAIN [0][2840/3880]	Time 0.162 (0.182)	Data 1.15e-04 (3.78e-04)	Tok/s 32563 (38715)	Loss/tok 3.2669 (4.3977)	LR 2.000e-03
0: TRAIN [0][2850/3880]	Time 0.163 (0.182)	Data 1.01e-04 (3.77e-04)	Tok/s 32465 (38708)	Loss/tok 3.2473 (4.3948)	LR 2.000e-03
0: TRAIN [0][2860/3880]	Time 0.222 (0.182)	Data 1.44e-04 (3.76e-04)	Tok/s 51648 (38715)	Loss/tok 3.7061 (4.3918)	LR 2.000e-03
0: TRAIN [0][2870/3880]	Time 0.162 (0.182)	Data 1.23e-04 (3.75e-04)	Tok/s 31546 (38708)	Loss/tok 3.2892 (4.3891)	LR 2.000e-03
0: TRAIN [0][2880/3880]	Time 0.193 (0.182)	Data 1.28e-04 (3.74e-04)	Tok/s 43453 (38710)	Loss/tok 3.4671 (4.3862)	LR 2.000e-03
0: TRAIN [0][2890/3880]	Time 0.162 (0.182)	Data 1.08e-04 (3.73e-04)	Tok/s 32437 (38718)	Loss/tok 3.2594 (4.3831)	LR 2.000e-03
0: TRAIN [0][2900/3880]	Time 0.163 (0.182)	Data 5.43e-04 (3.73e-04)	Tok/s 31194 (38689)	Loss/tok 3.4575 (4.3808)	LR 2.000e-03
0: TRAIN [0][2910/3880]	Time 0.191 (0.182)	Data 1.03e-04 (3.72e-04)	Tok/s 43763 (38674)	Loss/tok 3.6236 (4.3783)	LR 2.000e-03
0: TRAIN [0][2920/3880]	Time 0.221 (0.182)	Data 1.01e-04 (3.71e-04)	Tok/s 52501 (38669)	Loss/tok 3.7154 (4.3755)	LR 2.000e-03
0: TRAIN [0][2930/3880]	Time 0.191 (0.182)	Data 1.02e-04 (3.70e-04)	Tok/s 44164 (38674)	Loss/tok 3.4731 (4.3728)	LR 2.000e-03
0: TRAIN [0][2940/3880]	Time 0.162 (0.182)	Data 1.22e-04 (3.69e-04)	Tok/s 31438 (38674)	Loss/tok 3.1800 (4.3699)	LR 2.000e-03
0: TRAIN [0][2950/3880]	Time 0.136 (0.182)	Data 4.91e-04 (3.68e-04)	Tok/s 19563 (38659)	Loss/tok 3.0409 (4.3671)	LR 2.000e-03
0: TRAIN [0][2960/3880]	Time 0.162 (0.182)	Data 8.89e-05 (3.67e-04)	Tok/s 31188 (38640)	Loss/tok 3.4806 (4.3647)	LR 2.000e-03
0: TRAIN [0][2970/3880]	Time 0.221 (0.182)	Data 1.07e-04 (3.67e-04)	Tok/s 53206 (38666)	Loss/tok 3.8070 (4.3618)	LR 2.000e-03
0: TRAIN [0][2980/3880]	Time 0.162 (0.182)	Data 1.02e-04 (3.66e-04)	Tok/s 31324 (38655)	Loss/tok 3.2534 (4.3591)	LR 2.000e-03
0: TRAIN [0][2990/3880]	Time 0.191 (0.182)	Data 1.13e-04 (3.65e-04)	Tok/s 43944 (38656)	Loss/tok 3.5418 (4.3564)	LR 2.000e-03
0: TRAIN [0][3000/3880]	Time 0.191 (0.182)	Data 1.13e-04 (3.64e-04)	Tok/s 45185 (38649)	Loss/tok 3.4318 (4.3536)	LR 2.000e-03
0: TRAIN [0][3010/3880]	Time 0.162 (0.182)	Data 1.38e-04 (3.63e-04)	Tok/s 31700 (38645)	Loss/tok 3.2981 (4.3510)	LR 2.000e-03
0: TRAIN [0][3020/3880]	Time 0.162 (0.182)	Data 1.21e-04 (3.62e-04)	Tok/s 31965 (38637)	Loss/tok 3.4123 (4.3484)	LR 2.000e-03
0: TRAIN [0][3030/3880]	Time 0.137 (0.182)	Data 1.18e-04 (3.62e-04)	Tok/s 19228 (38633)	Loss/tok 2.9214 (4.3458)	LR 2.000e-03
0: TRAIN [0][3040/3880]	Time 0.220 (0.182)	Data 1.29e-04 (3.61e-04)	Tok/s 52820 (38633)	Loss/tok 3.7445 (4.3431)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][3050/3880]	Time 0.162 (0.182)	Data 9.78e-05 (3.60e-04)	Tok/s 31904 (38628)	Loss/tok 3.3853 (4.3406)	LR 2.000e-03
0: TRAIN [0][3060/3880]	Time 0.191 (0.182)	Data 1.12e-04 (3.59e-04)	Tok/s 43734 (38654)	Loss/tok 3.6331 (4.3377)	LR 2.000e-03
0: TRAIN [0][3070/3880]	Time 0.137 (0.182)	Data 1.01e-04 (3.58e-04)	Tok/s 19321 (38658)	Loss/tok 2.7324 (4.3349)	LR 2.000e-03
0: TRAIN [0][3080/3880]	Time 0.162 (0.182)	Data 1.13e-04 (3.58e-04)	Tok/s 32229 (38672)	Loss/tok 3.4363 (4.3323)	LR 2.000e-03
0: TRAIN [0][3090/3880]	Time 0.162 (0.182)	Data 1.11e-04 (3.57e-04)	Tok/s 31616 (38660)	Loss/tok 3.2609 (4.3299)	LR 2.000e-03
0: TRAIN [0][3100/3880]	Time 0.162 (0.182)	Data 1.02e-04 (3.56e-04)	Tok/s 32425 (38663)	Loss/tok 3.2594 (4.3274)	LR 2.000e-03
0: TRAIN [0][3110/3880]	Time 0.136 (0.182)	Data 1.20e-04 (3.56e-04)	Tok/s 18807 (38654)	Loss/tok 2.7515 (4.3248)	LR 2.000e-03
0: TRAIN [0][3120/3880]	Time 0.191 (0.182)	Data 1.12e-04 (3.55e-04)	Tok/s 43489 (38658)	Loss/tok 3.5495 (4.3224)	LR 2.000e-03
0: TRAIN [0][3130/3880]	Time 0.220 (0.182)	Data 1.28e-04 (3.54e-04)	Tok/s 51805 (38678)	Loss/tok 3.8448 (4.3199)	LR 2.000e-03
0: TRAIN [0][3140/3880]	Time 0.191 (0.182)	Data 9.82e-05 (3.53e-04)	Tok/s 44317 (38681)	Loss/tok 3.3814 (4.3173)	LR 2.000e-03
0: TRAIN [0][3150/3880]	Time 0.162 (0.182)	Data 8.65e-05 (3.52e-04)	Tok/s 31236 (38680)	Loss/tok 3.4436 (4.3147)	LR 2.000e-03
0: TRAIN [0][3160/3880]	Time 0.162 (0.182)	Data 8.89e-05 (3.52e-04)	Tok/s 32146 (38675)	Loss/tok 3.2732 (4.3124)	LR 2.000e-03
0: TRAIN [0][3170/3880]	Time 0.137 (0.182)	Data 8.65e-05 (3.51e-04)	Tok/s 18862 (38677)	Loss/tok 2.7592 (4.3099)	LR 2.000e-03
0: TRAIN [0][3180/3880]	Time 0.257 (0.182)	Data 5.10e-04 (3.50e-04)	Tok/s 57561 (38663)	Loss/tok 3.9266 (4.3078)	LR 2.000e-03
0: TRAIN [0][3190/3880]	Time 0.162 (0.182)	Data 8.61e-05 (3.49e-04)	Tok/s 32412 (38656)	Loss/tok 3.3281 (4.3054)	LR 2.000e-03
0: TRAIN [0][3200/3880]	Time 0.162 (0.182)	Data 1.00e-04 (3.49e-04)	Tok/s 31188 (38646)	Loss/tok 3.1540 (4.3030)	LR 2.000e-03
0: TRAIN [0][3210/3880]	Time 0.162 (0.182)	Data 9.30e-05 (3.48e-04)	Tok/s 31745 (38634)	Loss/tok 3.2911 (4.3009)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][3220/3880]	Time 0.222 (0.182)	Data 1.04e-04 (3.47e-04)	Tok/s 52124 (38638)	Loss/tok 3.7831 (4.2984)	LR 2.000e-03
0: TRAIN [0][3230/3880]	Time 0.162 (0.182)	Data 1.16e-04 (3.47e-04)	Tok/s 32019 (38629)	Loss/tok 3.2199 (4.2961)	LR 2.000e-03
0: TRAIN [0][3240/3880]	Time 0.162 (0.182)	Data 9.97e-05 (3.46e-04)	Tok/s 30930 (38622)	Loss/tok 3.2380 (4.2938)	LR 2.000e-03
0: TRAIN [0][3250/3880]	Time 0.162 (0.182)	Data 1.01e-04 (3.45e-04)	Tok/s 32986 (38606)	Loss/tok 3.3093 (4.2916)	LR 2.000e-03
0: TRAIN [0][3260/3880]	Time 0.162 (0.182)	Data 1.04e-04 (3.44e-04)	Tok/s 31602 (38577)	Loss/tok 3.2924 (4.2896)	LR 2.000e-03
0: TRAIN [0][3270/3880]	Time 0.191 (0.182)	Data 1.06e-04 (3.44e-04)	Tok/s 43523 (38578)	Loss/tok 3.4024 (4.2873)	LR 2.000e-03
0: TRAIN [0][3280/3880]	Time 0.221 (0.182)	Data 1.05e-04 (3.43e-04)	Tok/s 52873 (38586)	Loss/tok 3.7867 (4.2848)	LR 2.000e-03
0: TRAIN [0][3290/3880]	Time 0.191 (0.182)	Data 1.12e-04 (3.42e-04)	Tok/s 43573 (38597)	Loss/tok 3.4753 (4.2826)	LR 2.000e-03
0: TRAIN [0][3300/3880]	Time 0.191 (0.182)	Data 1.27e-04 (3.42e-04)	Tok/s 43924 (38601)	Loss/tok 3.4729 (4.2803)	LR 2.000e-03
0: TRAIN [0][3310/3880]	Time 0.136 (0.182)	Data 1.13e-04 (3.41e-04)	Tok/s 19762 (38587)	Loss/tok 2.9657 (4.2782)	LR 2.000e-03
0: TRAIN [0][3320/3880]	Time 0.136 (0.182)	Data 1.03e-04 (3.40e-04)	Tok/s 19393 (38580)	Loss/tok 2.7420 (4.2760)	LR 2.000e-03
0: TRAIN [0][3330/3880]	Time 0.136 (0.182)	Data 1.02e-04 (3.40e-04)	Tok/s 19717 (38559)	Loss/tok 2.8496 (4.2741)	LR 2.000e-03
0: TRAIN [0][3340/3880]	Time 0.163 (0.182)	Data 8.56e-05 (3.39e-04)	Tok/s 32001 (38538)	Loss/tok 3.3821 (4.2720)	LR 2.000e-03
0: TRAIN [0][3350/3880]	Time 0.162 (0.182)	Data 4.66e-04 (3.38e-04)	Tok/s 32421 (38522)	Loss/tok 3.4193 (4.2699)	LR 2.000e-03
0: TRAIN [0][3360/3880]	Time 0.257 (0.182)	Data 1.01e-04 (3.38e-04)	Tok/s 57506 (38523)	Loss/tok 3.9042 (4.2676)	LR 2.000e-03
0: TRAIN [0][3370/3880]	Time 0.257 (0.182)	Data 1.36e-04 (3.37e-04)	Tok/s 56489 (38531)	Loss/tok 4.0407 (4.2655)	LR 2.000e-03
0: TRAIN [0][3380/3880]	Time 0.162 (0.182)	Data 9.92e-05 (3.37e-04)	Tok/s 32239 (38530)	Loss/tok 3.2353 (4.2630)	LR 2.000e-03
0: TRAIN [0][3390/3880]	Time 0.191 (0.182)	Data 1.29e-04 (3.36e-04)	Tok/s 44138 (38523)	Loss/tok 3.5349 (4.2609)	LR 2.000e-03
0: TRAIN [0][3400/3880]	Time 0.221 (0.182)	Data 1.12e-04 (3.35e-04)	Tok/s 53683 (38525)	Loss/tok 3.4705 (4.2585)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][3410/3880]	Time 0.162 (0.181)	Data 1.01e-04 (3.35e-04)	Tok/s 30872 (38513)	Loss/tok 3.4180 (4.2566)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][3420/3880]	Time 0.162 (0.181)	Data 9.82e-05 (3.34e-04)	Tok/s 31901 (38502)	Loss/tok 3.3417 (4.2546)	LR 2.000e-03
0: TRAIN [0][3430/3880]	Time 0.162 (0.181)	Data 1.04e-04 (3.33e-04)	Tok/s 32505 (38495)	Loss/tok 3.2954 (4.2524)	LR 2.000e-03
0: TRAIN [0][3440/3880]	Time 0.257 (0.181)	Data 3.92e-04 (3.33e-04)	Tok/s 57898 (38509)	Loss/tok 3.8083 (4.2500)	LR 2.000e-03
0: TRAIN [0][3450/3880]	Time 0.163 (0.181)	Data 1.01e-04 (3.32e-04)	Tok/s 31864 (38507)	Loss/tok 3.1814 (4.2478)	LR 2.000e-03
0: TRAIN [0][3460/3880]	Time 0.136 (0.181)	Data 8.61e-05 (3.31e-04)	Tok/s 19119 (38498)	Loss/tok 2.8654 (4.2457)	LR 2.000e-03
0: TRAIN [0][3470/3880]	Time 0.221 (0.181)	Data 1.04e-04 (3.31e-04)	Tok/s 53411 (38502)	Loss/tok 3.7644 (4.2436)	LR 2.000e-03
0: TRAIN [0][3480/3880]	Time 0.191 (0.181)	Data 1.44e-04 (3.30e-04)	Tok/s 44698 (38503)	Loss/tok 3.4328 (4.2415)	LR 2.000e-03
0: TRAIN [0][3490/3880]	Time 0.162 (0.181)	Data 8.73e-05 (3.29e-04)	Tok/s 32101 (38511)	Loss/tok 3.2780 (4.2393)	LR 2.000e-03
0: TRAIN [0][3500/3880]	Time 0.162 (0.181)	Data 1.18e-04 (3.29e-04)	Tok/s 32589 (38517)	Loss/tok 3.3015 (4.2372)	LR 2.000e-03
0: TRAIN [0][3510/3880]	Time 0.191 (0.182)	Data 1.12e-04 (3.28e-04)	Tok/s 44362 (38527)	Loss/tok 3.5559 (4.2350)	LR 2.000e-03
0: TRAIN [0][3520/3880]	Time 0.221 (0.182)	Data 1.22e-04 (3.28e-04)	Tok/s 52163 (38527)	Loss/tok 3.8196 (4.2328)	LR 2.000e-03
0: TRAIN [0][3530/3880]	Time 0.256 (0.182)	Data 1.24e-04 (3.27e-04)	Tok/s 57814 (38547)	Loss/tok 3.8683 (4.2305)	LR 2.000e-03
0: TRAIN [0][3540/3880]	Time 0.191 (0.182)	Data 1.08e-04 (3.26e-04)	Tok/s 44336 (38542)	Loss/tok 3.4946 (4.2285)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][3550/3880]	Time 0.256 (0.182)	Data 1.21e-04 (3.26e-04)	Tok/s 57325 (38556)	Loss/tok 3.9386 (4.2264)	LR 2.000e-03
0: TRAIN [0][3560/3880]	Time 0.190 (0.182)	Data 1.41e-04 (3.25e-04)	Tok/s 43726 (38560)	Loss/tok 3.5988 (4.2243)	LR 2.000e-03
0: TRAIN [0][3570/3880]	Time 0.191 (0.182)	Data 1.20e-04 (3.25e-04)	Tok/s 43827 (38562)	Loss/tok 3.3963 (4.2222)	LR 2.000e-03
0: TRAIN [0][3580/3880]	Time 0.191 (0.182)	Data 1.86e-04 (3.24e-04)	Tok/s 44590 (38557)	Loss/tok 3.4337 (4.2201)	LR 2.000e-03
0: TRAIN [0][3590/3880]	Time 0.162 (0.182)	Data 8.73e-05 (3.23e-04)	Tok/s 32039 (38555)	Loss/tok 3.3916 (4.2181)	LR 2.000e-03
0: TRAIN [0][3600/3880]	Time 0.191 (0.182)	Data 3.46e-04 (3.23e-04)	Tok/s 45019 (38561)	Loss/tok 3.5136 (4.2160)	LR 2.000e-03
0: TRAIN [0][3610/3880]	Time 0.162 (0.182)	Data 1.33e-04 (3.22e-04)	Tok/s 32221 (38577)	Loss/tok 3.1554 (4.2137)	LR 2.000e-03
0: TRAIN [0][3620/3880]	Time 0.162 (0.182)	Data 1.13e-04 (3.22e-04)	Tok/s 31358 (38585)	Loss/tok 3.3319 (4.2114)	LR 2.000e-03
0: TRAIN [0][3630/3880]	Time 0.162 (0.182)	Data 9.99e-05 (3.21e-04)	Tok/s 31450 (38565)	Loss/tok 3.3216 (4.2097)	LR 2.000e-03
0: TRAIN [0][3640/3880]	Time 0.191 (0.182)	Data 1.20e-04 (3.21e-04)	Tok/s 43925 (38545)	Loss/tok 3.6437 (4.2080)	LR 2.000e-03
0: TRAIN [0][3650/3880]	Time 0.162 (0.182)	Data 1.22e-04 (3.20e-04)	Tok/s 32742 (38553)	Loss/tok 3.3139 (4.2058)	LR 2.000e-03
0: TRAIN [0][3660/3880]	Time 0.162 (0.182)	Data 1.05e-04 (3.20e-04)	Tok/s 31901 (38563)	Loss/tok 3.1814 (4.2037)	LR 2.000e-03
0: TRAIN [0][3670/3880]	Time 0.162 (0.182)	Data 1.33e-04 (3.19e-04)	Tok/s 31477 (38567)	Loss/tok 3.2410 (4.2015)	LR 2.000e-03
0: TRAIN [0][3680/3880]	Time 0.192 (0.182)	Data 1.01e-04 (3.19e-04)	Tok/s 43758 (38561)	Loss/tok 3.5637 (4.1997)	LR 2.000e-03
0: TRAIN [0][3690/3880]	Time 0.162 (0.182)	Data 1.18e-04 (3.18e-04)	Tok/s 32223 (38538)	Loss/tok 3.4493 (4.1980)	LR 2.000e-03
0: TRAIN [0][3700/3880]	Time 0.191 (0.182)	Data 1.39e-04 (3.18e-04)	Tok/s 43980 (38527)	Loss/tok 3.4801 (4.1961)	LR 2.000e-03
0: TRAIN [0][3710/3880]	Time 0.191 (0.182)	Data 9.32e-05 (3.17e-04)	Tok/s 43393 (38544)	Loss/tok 3.4392 (4.1941)	LR 2.000e-03
0: TRAIN [0][3720/3880]	Time 0.136 (0.182)	Data 8.87e-05 (3.16e-04)	Tok/s 18986 (38538)	Loss/tok 2.6337 (4.1921)	LR 2.000e-03
0: TRAIN [0][3730/3880]	Time 0.192 (0.182)	Data 1.08e-04 (3.16e-04)	Tok/s 43734 (38560)	Loss/tok 3.3083 (4.1900)	LR 2.000e-03
0: TRAIN [0][3740/3880]	Time 0.163 (0.182)	Data 1.12e-04 (3.15e-04)	Tok/s 32465 (38553)	Loss/tok 3.2303 (4.1880)	LR 2.000e-03
0: TRAIN [0][3750/3880]	Time 0.163 (0.182)	Data 8.73e-05 (3.15e-04)	Tok/s 32037 (38559)	Loss/tok 3.1807 (4.1860)	LR 2.000e-03
0: TRAIN [0][3760/3880]	Time 0.191 (0.182)	Data 1.21e-04 (3.14e-04)	Tok/s 43257 (38577)	Loss/tok 3.3930 (4.1839)	LR 2.000e-03
0: TRAIN [0][3770/3880]	Time 0.163 (0.182)	Data 1.23e-04 (3.14e-04)	Tok/s 31988 (38580)	Loss/tok 3.2381 (4.1823)	LR 2.000e-03
0: TRAIN [0][3780/3880]	Time 0.162 (0.182)	Data 9.18e-05 (3.13e-04)	Tok/s 31899 (38577)	Loss/tok 3.0513 (4.1804)	LR 2.000e-03
0: TRAIN [0][3790/3880]	Time 0.191 (0.182)	Data 1.22e-04 (3.13e-04)	Tok/s 43870 (38577)	Loss/tok 3.3682 (4.1785)	LR 2.000e-03
0: TRAIN [0][3800/3880]	Time 0.162 (0.182)	Data 1.18e-04 (3.12e-04)	Tok/s 31776 (38588)	Loss/tok 3.3096 (4.1766)	LR 2.000e-03
0: TRAIN [0][3810/3880]	Time 0.221 (0.182)	Data 1.05e-04 (3.12e-04)	Tok/s 51087 (38594)	Loss/tok 3.8263 (4.1749)	LR 2.000e-03
0: TRAIN [0][3820/3880]	Time 0.136 (0.182)	Data 1.02e-04 (3.11e-04)	Tok/s 18411 (38581)	Loss/tok 2.6278 (4.1731)	LR 2.000e-03
0: TRAIN [0][3830/3880]	Time 0.191 (0.182)	Data 1.33e-04 (3.11e-04)	Tok/s 43817 (38602)	Loss/tok 3.5625 (4.1712)	LR 2.000e-03
0: TRAIN [0][3840/3880]	Time 0.221 (0.182)	Data 1.15e-04 (3.10e-04)	Tok/s 53233 (38583)	Loss/tok 3.6396 (4.1694)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][3850/3880]	Time 0.191 (0.182)	Data 1.24e-04 (3.10e-04)	Tok/s 43412 (38583)	Loss/tok 3.4398 (4.1676)	LR 2.000e-03
0: TRAIN [0][3860/3880]	Time 0.191 (0.182)	Data 1.01e-04 (3.09e-04)	Tok/s 43653 (38576)	Loss/tok 3.4499 (4.1660)	LR 2.000e-03
0: TRAIN [0][3870/3880]	Time 0.191 (0.182)	Data 1.11e-04 (3.09e-04)	Tok/s 44168 (38568)	Loss/tok 3.4806 (4.1641)	LR 2.000e-03
:::MLL 1571259544.221 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1571259544.221 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.616 (0.616)	Decoder iters 95.0 (95.0)	Tok/s 25984 (25984)
0: Running moses detokenizer
0: BLEU(score=20.81003548899085, counts=[34816, 16256, 8751, 4915], totals=[63836, 60833, 57831, 54835], precisions=[54.53975813020866, 26.7223382045929, 15.132022617627225, 8.963253396553297], bp=0.9869274782308313, sys_len=63836, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571259546.049 eval_accuracy: {"value": 20.81, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1571259546.050 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 4.1635	Test BLEU: 20.81
0: Performance: Epoch: 0	Training: 308649 Tok/s
0: Finished epoch 0
:::MLL 1571259546.050 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1571259546.050 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571259546.051 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 3594379051
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][0/3880]	Time 0.939 (0.939)	Data 6.92e-01 (6.92e-01)	Tok/s 12039 (12039)	Loss/tok 3.8154 (3.8154)	LR 2.000e-03
0: TRAIN [1][10/3880]	Time 0.139 (0.244)	Data 7.68e-05 (6.30e-02)	Tok/s 18786 (34304)	Loss/tok 2.6043 (3.3614)	LR 2.000e-03
0: TRAIN [1][20/3880]	Time 0.162 (0.209)	Data 9.35e-05 (3.30e-02)	Tok/s 32115 (34769)	Loss/tok 3.0296 (3.3197)	LR 2.000e-03
0: TRAIN [1][30/3880]	Time 0.256 (0.199)	Data 9.37e-05 (2.24e-02)	Tok/s 59466 (35497)	Loss/tok 3.8883 (3.3578)	LR 2.000e-03
0: TRAIN [1][40/3880]	Time 0.137 (0.194)	Data 9.42e-05 (1.70e-02)	Tok/s 19390 (35770)	Loss/tok 2.5814 (3.3812)	LR 2.000e-03
0: TRAIN [1][50/3880]	Time 0.162 (0.190)	Data 1.07e-04 (1.37e-02)	Tok/s 32225 (35693)	Loss/tok 3.2016 (3.3796)	LR 2.000e-03
0: TRAIN [1][60/3880]	Time 0.255 (0.190)	Data 1.34e-04 (1.14e-02)	Tok/s 59267 (36815)	Loss/tok 3.6546 (3.3878)	LR 2.000e-03
0: TRAIN [1][70/3880]	Time 0.162 (0.189)	Data 7.70e-05 (9.84e-03)	Tok/s 32106 (36825)	Loss/tok 3.2673 (3.3922)	LR 2.000e-03
0: TRAIN [1][80/3880]	Time 0.162 (0.188)	Data 7.77e-05 (8.64e-03)	Tok/s 32884 (37017)	Loss/tok 3.0512 (3.3976)	LR 2.000e-03
0: TRAIN [1][90/3880]	Time 0.162 (0.186)	Data 8.23e-05 (7.70e-03)	Tok/s 31992 (36655)	Loss/tok 3.2659 (3.3906)	LR 2.000e-03
0: TRAIN [1][100/3880]	Time 0.221 (0.186)	Data 8.13e-05 (6.94e-03)	Tok/s 53267 (37252)	Loss/tok 3.5514 (3.3999)	LR 2.000e-03
0: TRAIN [1][110/3880]	Time 0.191 (0.187)	Data 7.94e-05 (6.33e-03)	Tok/s 44090 (37654)	Loss/tok 3.4140 (3.4116)	LR 2.000e-03
0: TRAIN [1][120/3880]	Time 0.162 (0.185)	Data 9.99e-05 (5.81e-03)	Tok/s 32657 (37440)	Loss/tok 3.0737 (3.4025)	LR 2.000e-03
0: TRAIN [1][130/3880]	Time 0.162 (0.185)	Data 9.56e-05 (5.38e-03)	Tok/s 32348 (37489)	Loss/tok 3.3023 (3.4036)	LR 2.000e-03
0: TRAIN [1][140/3880]	Time 0.191 (0.184)	Data 2.18e-04 (5.00e-03)	Tok/s 44226 (37424)	Loss/tok 3.4873 (3.3990)	LR 2.000e-03
0: TRAIN [1][150/3880]	Time 0.162 (0.184)	Data 1.91e-04 (4.68e-03)	Tok/s 32611 (37575)	Loss/tok 3.0491 (3.3983)	LR 2.000e-03
0: TRAIN [1][160/3880]	Time 0.162 (0.184)	Data 1.06e-04 (4.40e-03)	Tok/s 31363 (37559)	Loss/tok 3.2354 (3.3939)	LR 2.000e-03
0: TRAIN [1][170/3880]	Time 0.162 (0.183)	Data 7.72e-05 (4.15e-03)	Tok/s 31508 (37465)	Loss/tok 3.0513 (3.3864)	LR 2.000e-03
0: TRAIN [1][180/3880]	Time 0.162 (0.183)	Data 8.39e-05 (3.92e-03)	Tok/s 32083 (37593)	Loss/tok 3.1905 (3.3915)	LR 2.000e-03
0: TRAIN [1][190/3880]	Time 0.191 (0.184)	Data 8.46e-05 (3.72e-03)	Tok/s 44360 (37890)	Loss/tok 3.3905 (3.3970)	LR 2.000e-03
0: TRAIN [1][200/3880]	Time 0.162 (0.183)	Data 9.75e-05 (3.54e-03)	Tok/s 31469 (37717)	Loss/tok 3.2204 (3.3895)	LR 2.000e-03
0: TRAIN [1][210/3880]	Time 0.162 (0.183)	Data 9.63e-05 (3.38e-03)	Tok/s 32157 (37762)	Loss/tok 3.2627 (3.3884)	LR 2.000e-03
0: TRAIN [1][220/3880]	Time 0.191 (0.183)	Data 8.94e-05 (3.23e-03)	Tok/s 44403 (37905)	Loss/tok 3.3201 (3.3950)	LR 2.000e-03
0: TRAIN [1][230/3880]	Time 0.191 (0.183)	Data 8.11e-05 (3.09e-03)	Tok/s 43955 (37854)	Loss/tok 3.4248 (3.3952)	LR 2.000e-03
0: TRAIN [1][240/3880]	Time 0.191 (0.183)	Data 9.99e-05 (2.98e-03)	Tok/s 44358 (37945)	Loss/tok 3.5253 (3.3971)	LR 2.000e-03
0: TRAIN [1][250/3880]	Time 0.162 (0.183)	Data 9.66e-05 (2.86e-03)	Tok/s 31324 (37789)	Loss/tok 3.2243 (3.3936)	LR 2.000e-03
0: TRAIN [1][260/3880]	Time 0.162 (0.183)	Data 9.85e-05 (2.76e-03)	Tok/s 32234 (38077)	Loss/tok 3.1858 (3.3999)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][270/3880]	Time 0.162 (0.183)	Data 1.06e-04 (2.66e-03)	Tok/s 32282 (37883)	Loss/tok 3.1927 (3.3945)	LR 2.000e-03
0: TRAIN [1][280/3880]	Time 0.163 (0.182)	Data 8.23e-05 (2.57e-03)	Tok/s 31044 (37857)	Loss/tok 3.1917 (3.3921)	LR 2.000e-03
0: TRAIN [1][290/3880]	Time 0.162 (0.182)	Data 8.13e-05 (2.48e-03)	Tok/s 31625 (37920)	Loss/tok 2.9810 (3.3922)	LR 2.000e-03
0: TRAIN [1][300/3880]	Time 0.221 (0.182)	Data 8.30e-05 (2.40e-03)	Tok/s 52982 (37876)	Loss/tok 3.5513 (3.3925)	LR 2.000e-03
0: TRAIN [1][310/3880]	Time 0.162 (0.182)	Data 9.66e-05 (2.33e-03)	Tok/s 31890 (37876)	Loss/tok 3.3170 (3.3904)	LR 2.000e-03
0: TRAIN [1][320/3880]	Time 0.256 (0.183)	Data 4.18e-04 (2.26e-03)	Tok/s 58041 (38044)	Loss/tok 3.8145 (3.3963)	LR 2.000e-03
0: TRAIN [1][330/3880]	Time 0.257 (0.183)	Data 1.14e-04 (2.19e-03)	Tok/s 57904 (38213)	Loss/tok 3.7758 (3.4006)	LR 2.000e-03
0: TRAIN [1][340/3880]	Time 0.191 (0.183)	Data 1.48e-04 (2.13e-03)	Tok/s 43610 (38246)	Loss/tok 3.3709 (3.4001)	LR 2.000e-03
0: TRAIN [1][350/3880]	Time 0.191 (0.183)	Data 9.16e-05 (2.07e-03)	Tok/s 43855 (38350)	Loss/tok 3.3251 (3.4013)	LR 2.000e-03
0: TRAIN [1][360/3880]	Time 0.221 (0.183)	Data 8.08e-05 (2.02e-03)	Tok/s 52923 (38377)	Loss/tok 3.5440 (3.4007)	LR 2.000e-03
0: TRAIN [1][370/3880]	Time 0.191 (0.183)	Data 8.49e-05 (1.97e-03)	Tok/s 43485 (38387)	Loss/tok 3.5186 (3.3990)	LR 2.000e-03
0: TRAIN [1][380/3880]	Time 0.191 (0.184)	Data 9.66e-05 (1.92e-03)	Tok/s 44222 (38539)	Loss/tok 3.4348 (3.4029)	LR 2.000e-03
0: TRAIN [1][390/3880]	Time 0.162 (0.183)	Data 9.23e-05 (1.87e-03)	Tok/s 32132 (38536)	Loss/tok 3.2108 (3.4028)	LR 2.000e-03
0: TRAIN [1][400/3880]	Time 0.163 (0.183)	Data 8.68e-05 (1.83e-03)	Tok/s 31209 (38509)	Loss/tok 3.1575 (3.4040)	LR 2.000e-03
0: TRAIN [1][410/3880]	Time 0.162 (0.183)	Data 9.54e-05 (1.79e-03)	Tok/s 32159 (38445)	Loss/tok 3.2221 (3.4020)	LR 2.000e-03
0: TRAIN [1][420/3880]	Time 0.164 (0.183)	Data 1.57e-04 (1.75e-03)	Tok/s 32005 (38481)	Loss/tok 3.1957 (3.4008)	LR 2.000e-03
0: TRAIN [1][430/3880]	Time 0.191 (0.183)	Data 1.06e-04 (1.71e-03)	Tok/s 42905 (38565)	Loss/tok 3.3794 (3.4008)	LR 2.000e-03
0: TRAIN [1][440/3880]	Time 0.162 (0.183)	Data 8.03e-05 (1.67e-03)	Tok/s 32741 (38507)	Loss/tok 3.1470 (3.4005)	LR 2.000e-03
0: TRAIN [1][450/3880]	Time 0.191 (0.183)	Data 9.56e-05 (1.64e-03)	Tok/s 44638 (38570)	Loss/tok 3.3438 (3.3991)	LR 2.000e-03
0: TRAIN [1][460/3880]	Time 0.137 (0.183)	Data 3.99e-04 (1.60e-03)	Tok/s 19707 (38489)	Loss/tok 2.8328 (3.3969)	LR 2.000e-03
0: TRAIN [1][470/3880]	Time 0.191 (0.182)	Data 7.80e-05 (1.57e-03)	Tok/s 44194 (38401)	Loss/tok 3.4459 (3.3936)	LR 2.000e-03
0: TRAIN [1][480/3880]	Time 0.136 (0.183)	Data 9.49e-05 (1.54e-03)	Tok/s 19662 (38457)	Loss/tok 2.6722 (3.3949)	LR 2.000e-03
0: TRAIN [1][490/3880]	Time 0.137 (0.183)	Data 8.15e-05 (1.51e-03)	Tok/s 18955 (38538)	Loss/tok 2.8862 (3.3961)	LR 2.000e-03
0: TRAIN [1][500/3880]	Time 0.162 (0.183)	Data 7.96e-05 (1.48e-03)	Tok/s 30441 (38547)	Loss/tok 3.1110 (3.3981)	LR 2.000e-03
0: TRAIN [1][510/3880]	Time 0.191 (0.183)	Data 9.61e-05 (1.46e-03)	Tok/s 44118 (38431)	Loss/tok 3.4555 (3.3956)	LR 2.000e-03
0: TRAIN [1][520/3880]	Time 0.162 (0.182)	Data 9.37e-05 (1.43e-03)	Tok/s 31495 (38356)	Loss/tok 3.0006 (3.3932)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][530/3880]	Time 0.191 (0.182)	Data 7.58e-05 (1.40e-03)	Tok/s 43007 (38386)	Loss/tok 3.3696 (3.3943)	LR 2.000e-03
0: TRAIN [1][540/3880]	Time 0.221 (0.182)	Data 8.11e-05 (1.38e-03)	Tok/s 52848 (38329)	Loss/tok 3.6544 (3.3930)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][550/3880]	Time 0.216 (0.182)	Data 8.99e-05 (1.36e-03)	Tok/s 53763 (38416)	Loss/tok 3.6175 (3.3960)	LR 2.000e-03
0: TRAIN [1][560/3880]	Time 0.191 (0.182)	Data 1.03e-04 (1.33e-03)	Tok/s 43929 (38452)	Loss/tok 3.4938 (3.3975)	LR 2.000e-03
0: TRAIN [1][570/3880]	Time 0.162 (0.183)	Data 3.79e-04 (1.31e-03)	Tok/s 32229 (38473)	Loss/tok 3.1402 (3.3989)	LR 2.000e-03
0: TRAIN [1][580/3880]	Time 0.162 (0.182)	Data 9.49e-05 (1.29e-03)	Tok/s 32500 (38407)	Loss/tok 3.1741 (3.3968)	LR 2.000e-03
0: TRAIN [1][590/3880]	Time 0.162 (0.182)	Data 1.03e-04 (1.27e-03)	Tok/s 30977 (38400)	Loss/tok 2.9765 (3.3962)	LR 2.000e-03
0: TRAIN [1][600/3880]	Time 0.162 (0.182)	Data 1.20e-04 (1.25e-03)	Tok/s 31407 (38353)	Loss/tok 3.0569 (3.3954)	LR 2.000e-03
0: TRAIN [1][610/3880]	Time 0.162 (0.182)	Data 1.05e-04 (1.23e-03)	Tok/s 32179 (38408)	Loss/tok 3.1170 (3.3973)	LR 2.000e-03
0: TRAIN [1][620/3880]	Time 0.137 (0.182)	Data 9.06e-05 (1.22e-03)	Tok/s 19898 (38371)	Loss/tok 2.6073 (3.3961)	LR 2.000e-03
0: TRAIN [1][630/3880]	Time 0.221 (0.182)	Data 1.68e-04 (1.20e-03)	Tok/s 53500 (38397)	Loss/tok 3.6127 (3.3971)	LR 2.000e-03
0: TRAIN [1][640/3880]	Time 0.257 (0.182)	Data 8.99e-05 (1.18e-03)	Tok/s 57887 (38446)	Loss/tok 3.6557 (3.3990)	LR 2.000e-03
0: TRAIN [1][650/3880]	Time 0.192 (0.183)	Data 8.18e-05 (1.16e-03)	Tok/s 44359 (38489)	Loss/tok 3.2849 (3.3989)	LR 2.000e-03
0: TRAIN [1][660/3880]	Time 0.136 (0.182)	Data 9.78e-05 (1.15e-03)	Tok/s 19644 (38482)	Loss/tok 2.8299 (3.3987)	LR 2.000e-03
0: TRAIN [1][670/3880]	Time 0.191 (0.183)	Data 8.11e-05 (1.13e-03)	Tok/s 43548 (38518)	Loss/tok 3.3706 (3.3996)	LR 2.000e-03
0: TRAIN [1][680/3880]	Time 0.191 (0.183)	Data 8.06e-05 (1.12e-03)	Tok/s 44236 (38574)	Loss/tok 3.3477 (3.4006)	LR 2.000e-03
0: TRAIN [1][690/3880]	Time 0.162 (0.183)	Data 9.23e-05 (1.10e-03)	Tok/s 32211 (38549)	Loss/tok 3.1861 (3.3998)	LR 2.000e-03
0: TRAIN [1][700/3880]	Time 0.220 (0.183)	Data 1.21e-04 (1.09e-03)	Tok/s 53120 (38574)	Loss/tok 3.4923 (3.4004)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][710/3880]	Time 0.221 (0.183)	Data 8.01e-05 (1.07e-03)	Tok/s 52947 (38563)	Loss/tok 3.4367 (3.4004)	LR 2.000e-03
0: TRAIN [1][720/3880]	Time 0.221 (0.183)	Data 9.56e-05 (1.06e-03)	Tok/s 53087 (38562)	Loss/tok 3.6776 (3.4011)	LR 2.000e-03
0: TRAIN [1][730/3880]	Time 0.162 (0.183)	Data 8.54e-05 (1.05e-03)	Tok/s 31206 (38600)	Loss/tok 2.9607 (3.4011)	LR 2.000e-03
0: TRAIN [1][740/3880]	Time 0.162 (0.183)	Data 9.63e-05 (1.04e-03)	Tok/s 31869 (38617)	Loss/tok 2.9519 (3.4009)	LR 2.000e-03
0: TRAIN [1][750/3880]	Time 0.257 (0.183)	Data 9.25e-05 (1.02e-03)	Tok/s 56290 (38634)	Loss/tok 3.8582 (3.4020)	LR 2.000e-03
0: TRAIN [1][760/3880]	Time 0.162 (0.183)	Data 7.89e-05 (1.01e-03)	Tok/s 32561 (38546)	Loss/tok 3.0509 (3.3996)	LR 2.000e-03
0: TRAIN [1][770/3880]	Time 0.162 (0.183)	Data 9.87e-05 (9.99e-04)	Tok/s 31775 (38564)	Loss/tok 3.1617 (3.3994)	LR 2.000e-03
0: TRAIN [1][780/3880]	Time 0.191 (0.183)	Data 8.06e-05 (9.88e-04)	Tok/s 45151 (38563)	Loss/tok 3.2336 (3.3983)	LR 2.000e-03
0: TRAIN [1][790/3880]	Time 0.162 (0.183)	Data 1.06e-04 (9.76e-04)	Tok/s 32067 (38581)	Loss/tok 3.0475 (3.3978)	LR 2.000e-03
0: TRAIN [1][800/3880]	Time 0.221 (0.183)	Data 1.18e-04 (9.65e-04)	Tok/s 52553 (38620)	Loss/tok 3.6245 (3.3985)	LR 2.000e-03
0: TRAIN [1][810/3880]	Time 0.162 (0.183)	Data 1.04e-04 (9.55e-04)	Tok/s 31864 (38602)	Loss/tok 3.1700 (3.3983)	LR 2.000e-03
0: TRAIN [1][820/3880]	Time 0.191 (0.183)	Data 9.35e-05 (9.44e-04)	Tok/s 43474 (38588)	Loss/tok 3.4255 (3.3975)	LR 2.000e-03
0: TRAIN [1][830/3880]	Time 0.162 (0.182)	Data 8.18e-05 (9.35e-04)	Tok/s 32623 (38562)	Loss/tok 3.1293 (3.3961)	LR 2.000e-03
0: TRAIN [1][840/3880]	Time 0.221 (0.183)	Data 9.20e-05 (9.24e-04)	Tok/s 52153 (38633)	Loss/tok 3.5068 (3.3964)	LR 2.000e-03
0: TRAIN [1][850/3880]	Time 0.136 (0.183)	Data 1.03e-04 (9.15e-04)	Tok/s 19457 (38656)	Loss/tok 2.7581 (3.3966)	LR 2.000e-03
0: TRAIN [1][860/3880]	Time 0.162 (0.182)	Data 9.66e-05 (9.05e-04)	Tok/s 31262 (38592)	Loss/tok 3.1881 (3.3949)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][870/3880]	Time 0.162 (0.182)	Data 8.80e-05 (8.96e-04)	Tok/s 32641 (38508)	Loss/tok 3.1731 (3.3944)	LR 2.000e-03
0: TRAIN [1][880/3880]	Time 0.191 (0.182)	Data 4.99e-04 (8.88e-04)	Tok/s 44202 (38459)	Loss/tok 3.2527 (3.3931)	LR 2.000e-03
0: TRAIN [1][890/3880]	Time 0.221 (0.182)	Data 9.56e-05 (8.79e-04)	Tok/s 52165 (38527)	Loss/tok 3.5958 (3.3955)	LR 2.000e-03
0: TRAIN [1][900/3880]	Time 0.191 (0.182)	Data 8.89e-05 (8.70e-04)	Tok/s 44763 (38545)	Loss/tok 3.3225 (3.3948)	LR 2.000e-03
0: TRAIN [1][910/3880]	Time 0.191 (0.182)	Data 1.09e-04 (8.61e-04)	Tok/s 45170 (38566)	Loss/tok 3.3194 (3.3960)	LR 2.000e-03
0: TRAIN [1][920/3880]	Time 0.162 (0.182)	Data 7.92e-05 (8.53e-04)	Tok/s 31680 (38568)	Loss/tok 3.1380 (3.3967)	LR 2.000e-03
0: TRAIN [1][930/3880]	Time 0.191 (0.182)	Data 8.11e-05 (8.45e-04)	Tok/s 43807 (38520)	Loss/tok 3.4142 (3.3955)	LR 2.000e-03
0: TRAIN [1][940/3880]	Time 0.162 (0.182)	Data 1.03e-04 (8.37e-04)	Tok/s 31519 (38523)	Loss/tok 3.2389 (3.3960)	LR 2.000e-03
0: TRAIN [1][950/3880]	Time 0.192 (0.182)	Data 8.87e-05 (8.29e-04)	Tok/s 43121 (38578)	Loss/tok 3.3544 (3.3965)	LR 2.000e-03
0: TRAIN [1][960/3880]	Time 0.163 (0.182)	Data 8.20e-05 (8.21e-04)	Tok/s 31872 (38579)	Loss/tok 3.2146 (3.3959)	LR 2.000e-03
0: TRAIN [1][970/3880]	Time 0.162 (0.182)	Data 8.06e-05 (8.14e-04)	Tok/s 31472 (38595)	Loss/tok 3.2528 (3.3964)	LR 2.000e-03
0: TRAIN [1][980/3880]	Time 0.191 (0.182)	Data 7.72e-05 (8.06e-04)	Tok/s 43923 (38559)	Loss/tok 3.3137 (3.3954)	LR 2.000e-03
0: TRAIN [1][990/3880]	Time 0.221 (0.182)	Data 9.25e-05 (7.99e-04)	Tok/s 52430 (38600)	Loss/tok 3.5561 (3.3958)	LR 2.000e-03
0: TRAIN [1][1000/3880]	Time 0.162 (0.182)	Data 8.15e-05 (7.92e-04)	Tok/s 31632 (38619)	Loss/tok 3.3712 (3.3957)	LR 2.000e-03
0: TRAIN [1][1010/3880]	Time 0.136 (0.182)	Data 9.13e-05 (7.85e-04)	Tok/s 19512 (38623)	Loss/tok 2.8159 (3.3954)	LR 2.000e-03
0: TRAIN [1][1020/3880]	Time 0.191 (0.182)	Data 9.42e-05 (7.78e-04)	Tok/s 44385 (38601)	Loss/tok 3.3878 (3.3949)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1030/3880]	Time 0.162 (0.182)	Data 7.84e-05 (7.71e-04)	Tok/s 31273 (38594)	Loss/tok 3.0742 (3.3947)	LR 2.000e-03
0: TRAIN [1][1040/3880]	Time 0.162 (0.182)	Data 9.30e-05 (7.65e-04)	Tok/s 31529 (38587)	Loss/tok 3.1893 (3.3941)	LR 2.000e-03
0: TRAIN [1][1050/3880]	Time 0.162 (0.182)	Data 7.44e-05 (7.59e-04)	Tok/s 31677 (38544)	Loss/tok 3.2607 (3.3937)	LR 2.000e-03
0: TRAIN [1][1060/3880]	Time 0.221 (0.182)	Data 7.89e-05 (7.52e-04)	Tok/s 53004 (38488)	Loss/tok 3.5507 (3.3931)	LR 2.000e-03
0: TRAIN [1][1070/3880]	Time 0.162 (0.182)	Data 8.25e-05 (7.46e-04)	Tok/s 31938 (38512)	Loss/tok 3.2117 (3.3929)	LR 2.000e-03
0: TRAIN [1][1080/3880]	Time 0.162 (0.182)	Data 9.70e-05 (7.40e-04)	Tok/s 31763 (38528)	Loss/tok 3.1951 (3.3926)	LR 2.000e-03
0: TRAIN [1][1090/3880]	Time 0.191 (0.182)	Data 8.44e-05 (7.34e-04)	Tok/s 43719 (38509)	Loss/tok 3.3162 (3.3921)	LR 2.000e-03
0: TRAIN [1][1100/3880]	Time 0.162 (0.182)	Data 1.13e-04 (7.28e-04)	Tok/s 31314 (38537)	Loss/tok 3.0824 (3.3926)	LR 2.000e-03
0: TRAIN [1][1110/3880]	Time 0.191 (0.182)	Data 1.08e-04 (7.23e-04)	Tok/s 44642 (38588)	Loss/tok 3.3133 (3.3934)	LR 2.000e-03
0: TRAIN [1][1120/3880]	Time 0.162 (0.182)	Data 8.30e-05 (7.17e-04)	Tok/s 31991 (38597)	Loss/tok 3.0378 (3.3932)	LR 2.000e-03
0: TRAIN [1][1130/3880]	Time 0.162 (0.182)	Data 7.65e-05 (7.11e-04)	Tok/s 31659 (38567)	Loss/tok 3.2013 (3.3924)	LR 2.000e-03
0: TRAIN [1][1140/3880]	Time 0.162 (0.182)	Data 7.87e-05 (7.06e-04)	Tok/s 32029 (38538)	Loss/tok 3.0783 (3.3917)	LR 2.000e-03
0: TRAIN [1][1150/3880]	Time 0.162 (0.182)	Data 7.96e-05 (7.01e-04)	Tok/s 31040 (38538)	Loss/tok 3.1104 (3.3910)	LR 2.000e-03
0: TRAIN [1][1160/3880]	Time 0.163 (0.182)	Data 8.96e-05 (6.95e-04)	Tok/s 31282 (38531)	Loss/tok 3.1510 (3.3905)	LR 2.000e-03
0: TRAIN [1][1170/3880]	Time 0.162 (0.182)	Data 7.80e-05 (6.91e-04)	Tok/s 31323 (38532)	Loss/tok 3.0227 (3.3901)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1180/3880]	Time 0.136 (0.182)	Data 9.37e-05 (6.85e-04)	Tok/s 19059 (38511)	Loss/tok 2.8336 (3.3900)	LR 2.000e-03
0: TRAIN [1][1190/3880]	Time 0.191 (0.182)	Data 7.75e-05 (6.80e-04)	Tok/s 42647 (38503)	Loss/tok 3.4096 (3.3893)	LR 2.000e-03
0: TRAIN [1][1200/3880]	Time 0.162 (0.182)	Data 7.77e-05 (6.76e-04)	Tok/s 32135 (38474)	Loss/tok 3.0635 (3.3881)	LR 2.000e-03
0: TRAIN [1][1210/3880]	Time 0.162 (0.182)	Data 7.58e-05 (6.71e-04)	Tok/s 31564 (38445)	Loss/tok 3.0239 (3.3872)	LR 2.000e-03
0: TRAIN [1][1220/3880]	Time 0.162 (0.182)	Data 9.11e-05 (6.66e-04)	Tok/s 32777 (38423)	Loss/tok 3.1652 (3.3869)	LR 2.000e-03
0: TRAIN [1][1230/3880]	Time 0.162 (0.181)	Data 8.20e-05 (6.61e-04)	Tok/s 32209 (38369)	Loss/tok 3.1729 (3.3858)	LR 2.000e-03
0: TRAIN [1][1240/3880]	Time 0.191 (0.181)	Data 7.51e-05 (6.57e-04)	Tok/s 43353 (38375)	Loss/tok 3.3485 (3.3867)	LR 2.000e-03
0: TRAIN [1][1250/3880]	Time 0.191 (0.181)	Data 1.14e-04 (6.52e-04)	Tok/s 45512 (38362)	Loss/tok 3.2774 (3.3861)	LR 2.000e-03
0: TRAIN [1][1260/3880]	Time 0.162 (0.181)	Data 9.32e-05 (6.48e-04)	Tok/s 31463 (38319)	Loss/tok 3.2597 (3.3850)	LR 2.000e-03
0: TRAIN [1][1270/3880]	Time 0.221 (0.181)	Data 9.32e-05 (6.43e-04)	Tok/s 52624 (38328)	Loss/tok 3.6230 (3.3854)	LR 2.000e-03
0: TRAIN [1][1280/3880]	Time 0.163 (0.181)	Data 8.77e-05 (6.39e-04)	Tok/s 31625 (38292)	Loss/tok 3.0234 (3.3844)	LR 2.000e-03
0: TRAIN [1][1290/3880]	Time 0.162 (0.181)	Data 9.49e-05 (6.35e-04)	Tok/s 32616 (38305)	Loss/tok 3.1751 (3.3840)	LR 2.000e-03
0: TRAIN [1][1300/3880]	Time 0.162 (0.181)	Data 7.92e-05 (6.31e-04)	Tok/s 32110 (38319)	Loss/tok 2.9789 (3.3842)	LR 2.000e-03
0: TRAIN [1][1310/3880]	Time 0.191 (0.181)	Data 7.99e-05 (6.27e-04)	Tok/s 44303 (38362)	Loss/tok 3.3773 (3.3847)	LR 2.000e-03
0: TRAIN [1][1320/3880]	Time 0.191 (0.181)	Data 2.82e-04 (6.23e-04)	Tok/s 42715 (38407)	Loss/tok 3.3855 (3.3854)	LR 2.000e-03
0: TRAIN [1][1330/3880]	Time 0.221 (0.181)	Data 7.99e-05 (6.19e-04)	Tok/s 52007 (38372)	Loss/tok 3.6169 (3.3849)	LR 2.000e-03
0: TRAIN [1][1340/3880]	Time 0.162 (0.181)	Data 9.16e-05 (6.15e-04)	Tok/s 31897 (38360)	Loss/tok 3.2640 (3.3840)	LR 2.000e-03
0: TRAIN [1][1350/3880]	Time 0.191 (0.181)	Data 7.68e-05 (6.11e-04)	Tok/s 44168 (38359)	Loss/tok 3.4277 (3.3843)	LR 2.000e-03
0: TRAIN [1][1360/3880]	Time 0.162 (0.181)	Data 8.54e-05 (6.07e-04)	Tok/s 32356 (38358)	Loss/tok 3.0965 (3.3837)	LR 2.000e-03
0: TRAIN [1][1370/3880]	Time 0.137 (0.181)	Data 7.82e-05 (6.03e-04)	Tok/s 18615 (38350)	Loss/tok 2.6354 (3.3833)	LR 2.000e-03
0: TRAIN [1][1380/3880]	Time 0.162 (0.181)	Data 8.70e-05 (6.00e-04)	Tok/s 32468 (38320)	Loss/tok 3.2134 (3.3825)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1390/3880]	Time 0.162 (0.181)	Data 9.66e-05 (5.96e-04)	Tok/s 32094 (38305)	Loss/tok 3.2657 (3.3819)	LR 2.000e-03
0: TRAIN [1][1400/3880]	Time 0.257 (0.181)	Data 9.27e-05 (5.93e-04)	Tok/s 56640 (38276)	Loss/tok 3.8574 (3.3816)	LR 2.000e-03
0: TRAIN [1][1410/3880]	Time 0.136 (0.181)	Data 9.20e-05 (5.89e-04)	Tok/s 19122 (38254)	Loss/tok 2.7052 (3.3812)	LR 2.000e-03
0: TRAIN [1][1420/3880]	Time 0.192 (0.181)	Data 8.03e-05 (5.86e-04)	Tok/s 43906 (38259)	Loss/tok 3.3760 (3.3811)	LR 2.000e-03
0: TRAIN [1][1430/3880]	Time 0.162 (0.181)	Data 8.92e-05 (5.82e-04)	Tok/s 31904 (38251)	Loss/tok 3.2113 (3.3802)	LR 2.000e-03
0: TRAIN [1][1440/3880]	Time 0.191 (0.181)	Data 9.47e-05 (5.79e-04)	Tok/s 42927 (38237)	Loss/tok 3.4627 (3.3798)	LR 2.000e-03
0: TRAIN [1][1450/3880]	Time 0.136 (0.181)	Data 9.27e-05 (5.76e-04)	Tok/s 19545 (38231)	Loss/tok 2.6158 (3.3796)	LR 2.000e-03
0: TRAIN [1][1460/3880]	Time 0.138 (0.181)	Data 1.18e-04 (5.72e-04)	Tok/s 19298 (38249)	Loss/tok 2.7444 (3.3803)	LR 2.000e-03
0: TRAIN [1][1470/3880]	Time 0.221 (0.181)	Data 8.08e-05 (5.69e-04)	Tok/s 51905 (38277)	Loss/tok 3.6595 (3.3806)	LR 2.000e-03
0: TRAIN [1][1480/3880]	Time 0.191 (0.181)	Data 7.96e-05 (5.66e-04)	Tok/s 43533 (38292)	Loss/tok 3.2666 (3.3807)	LR 2.000e-03
0: TRAIN [1][1490/3880]	Time 0.191 (0.181)	Data 9.13e-05 (5.63e-04)	Tok/s 43705 (38278)	Loss/tok 3.5522 (3.3802)	LR 2.000e-03
0: TRAIN [1][1500/3880]	Time 0.192 (0.181)	Data 9.13e-05 (5.60e-04)	Tok/s 44315 (38261)	Loss/tok 3.2984 (3.3796)	LR 2.000e-03
0: TRAIN [1][1510/3880]	Time 0.221 (0.181)	Data 1.13e-04 (5.57e-04)	Tok/s 52472 (38280)	Loss/tok 3.5084 (3.3799)	LR 2.000e-03
0: TRAIN [1][1520/3880]	Time 0.162 (0.181)	Data 9.37e-05 (5.54e-04)	Tok/s 31718 (38272)	Loss/tok 3.3704 (3.3798)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1530/3880]	Time 0.191 (0.181)	Data 8.13e-05 (5.51e-04)	Tok/s 43267 (38307)	Loss/tok 3.4699 (3.3805)	LR 2.000e-03
0: TRAIN [1][1540/3880]	Time 0.162 (0.181)	Data 8.25e-05 (5.48e-04)	Tok/s 31586 (38311)	Loss/tok 3.1187 (3.3809)	LR 2.000e-03
0: TRAIN [1][1550/3880]	Time 0.162 (0.181)	Data 7.80e-05 (5.45e-04)	Tok/s 31706 (38316)	Loss/tok 3.1455 (3.3805)	LR 2.000e-03
0: TRAIN [1][1560/3880]	Time 0.162 (0.181)	Data 9.61e-05 (5.42e-04)	Tok/s 31462 (38307)	Loss/tok 3.0677 (3.3802)	LR 2.000e-03
0: TRAIN [1][1570/3880]	Time 0.136 (0.181)	Data 9.04e-05 (5.39e-04)	Tok/s 19061 (38297)	Loss/tok 2.7763 (3.3806)	LR 2.000e-03
0: TRAIN [1][1580/3880]	Time 0.221 (0.181)	Data 9.11e-05 (5.36e-04)	Tok/s 52037 (38332)	Loss/tok 3.6964 (3.3819)	LR 2.000e-03
0: TRAIN [1][1590/3880]	Time 0.256 (0.181)	Data 7.89e-05 (5.33e-04)	Tok/s 58524 (38324)	Loss/tok 3.6882 (3.3818)	LR 2.000e-03
0: TRAIN [1][1600/3880]	Time 0.221 (0.181)	Data 7.80e-05 (5.31e-04)	Tok/s 52238 (38334)	Loss/tok 3.3704 (3.3815)	LR 2.000e-03
0: TRAIN [1][1610/3880]	Time 0.191 (0.181)	Data 7.80e-05 (5.28e-04)	Tok/s 43692 (38345)	Loss/tok 3.3787 (3.3818)	LR 2.000e-03
0: TRAIN [1][1620/3880]	Time 0.257 (0.181)	Data 8.49e-05 (5.25e-04)	Tok/s 58116 (38390)	Loss/tok 3.6942 (3.3835)	LR 2.000e-03
0: TRAIN [1][1630/3880]	Time 0.221 (0.181)	Data 8.68e-05 (5.22e-04)	Tok/s 53691 (38386)	Loss/tok 3.5073 (3.3830)	LR 2.000e-03
0: TRAIN [1][1640/3880]	Time 0.162 (0.181)	Data 3.58e-04 (5.20e-04)	Tok/s 31793 (38363)	Loss/tok 3.1122 (3.3827)	LR 2.000e-03
0: TRAIN [1][1650/3880]	Time 0.191 (0.181)	Data 7.92e-05 (5.17e-04)	Tok/s 43846 (38352)	Loss/tok 3.3804 (3.3821)	LR 2.000e-03
0: TRAIN [1][1660/3880]	Time 0.191 (0.181)	Data 8.42e-05 (5.15e-04)	Tok/s 43588 (38399)	Loss/tok 3.3589 (3.3828)	LR 2.000e-03
0: TRAIN [1][1670/3880]	Time 0.191 (0.181)	Data 8.99e-05 (5.13e-04)	Tok/s 43465 (38397)	Loss/tok 3.3573 (3.3824)	LR 2.000e-03
0: TRAIN [1][1680/3880]	Time 0.162 (0.181)	Data 9.54e-05 (5.10e-04)	Tok/s 32206 (38405)	Loss/tok 3.1485 (3.3828)	LR 2.000e-03
0: TRAIN [1][1690/3880]	Time 0.163 (0.181)	Data 8.92e-05 (5.08e-04)	Tok/s 30772 (38400)	Loss/tok 3.0861 (3.3826)	LR 2.000e-03
0: TRAIN [1][1700/3880]	Time 0.162 (0.181)	Data 9.70e-05 (5.05e-04)	Tok/s 32113 (38375)	Loss/tok 3.1958 (3.3821)	LR 2.000e-03
0: TRAIN [1][1710/3880]	Time 0.191 (0.181)	Data 1.13e-04 (5.03e-04)	Tok/s 44034 (38359)	Loss/tok 3.3799 (3.3818)	LR 2.000e-03
0: TRAIN [1][1720/3880]	Time 0.164 (0.181)	Data 1.65e-04 (5.00e-04)	Tok/s 32193 (38388)	Loss/tok 3.1158 (3.3815)	LR 2.000e-03
0: TRAIN [1][1730/3880]	Time 0.191 (0.181)	Data 9.44e-05 (4.98e-04)	Tok/s 44166 (38410)	Loss/tok 3.4754 (3.3814)	LR 2.000e-03
0: TRAIN [1][1740/3880]	Time 0.191 (0.181)	Data 8.63e-05 (4.96e-04)	Tok/s 44374 (38400)	Loss/tok 3.2689 (3.3809)	LR 2.000e-03
0: TRAIN [1][1750/3880]	Time 0.162 (0.181)	Data 7.68e-05 (4.94e-04)	Tok/s 32142 (38404)	Loss/tok 3.1367 (3.3807)	LR 2.000e-03
0: TRAIN [1][1760/3880]	Time 0.256 (0.181)	Data 7.72e-05 (4.91e-04)	Tok/s 58987 (38414)	Loss/tok 3.5926 (3.3806)	LR 2.000e-03
0: TRAIN [1][1770/3880]	Time 0.136 (0.181)	Data 7.53e-05 (4.89e-04)	Tok/s 19128 (38439)	Loss/tok 2.6926 (3.3809)	LR 2.000e-03
0: TRAIN [1][1780/3880]	Time 0.162 (0.181)	Data 8.94e-05 (4.87e-04)	Tok/s 31257 (38448)	Loss/tok 3.0633 (3.3812)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1790/3880]	Time 0.257 (0.182)	Data 8.77e-05 (4.85e-04)	Tok/s 57080 (38464)	Loss/tok 3.7611 (3.3814)	LR 2.000e-03
0: TRAIN [1][1800/3880]	Time 0.162 (0.181)	Data 9.27e-05 (4.82e-04)	Tok/s 30650 (38444)	Loss/tok 3.1649 (3.3816)	LR 2.000e-03
0: TRAIN [1][1810/3880]	Time 0.191 (0.181)	Data 8.92e-05 (4.81e-04)	Tok/s 44327 (38452)	Loss/tok 3.4396 (3.3816)	LR 2.000e-03
0: TRAIN [1][1820/3880]	Time 0.162 (0.181)	Data 9.01e-05 (4.78e-04)	Tok/s 32478 (38445)	Loss/tok 3.0594 (3.3810)	LR 2.000e-03
0: TRAIN [1][1830/3880]	Time 0.162 (0.182)	Data 9.20e-05 (4.76e-04)	Tok/s 32006 (38478)	Loss/tok 3.1369 (3.3817)	LR 2.000e-03
0: TRAIN [1][1840/3880]	Time 0.162 (0.182)	Data 9.99e-05 (4.74e-04)	Tok/s 31393 (38480)	Loss/tok 3.1254 (3.3817)	LR 2.000e-03
0: TRAIN [1][1850/3880]	Time 0.191 (0.182)	Data 8.01e-05 (4.72e-04)	Tok/s 44472 (38510)	Loss/tok 3.3297 (3.3823)	LR 2.000e-03
0: TRAIN [1][1860/3880]	Time 0.162 (0.182)	Data 9.37e-05 (4.70e-04)	Tok/s 32550 (38468)	Loss/tok 3.2704 (3.3818)	LR 2.000e-03
0: TRAIN [1][1870/3880]	Time 0.137 (0.182)	Data 8.49e-05 (4.68e-04)	Tok/s 19247 (38460)	Loss/tok 2.6804 (3.3815)	LR 2.000e-03
0: TRAIN [1][1880/3880]	Time 0.221 (0.182)	Data 7.89e-05 (4.66e-04)	Tok/s 52794 (38492)	Loss/tok 3.6481 (3.3828)	LR 2.000e-03
0: TRAIN [1][1890/3880]	Time 0.191 (0.182)	Data 7.75e-05 (4.64e-04)	Tok/s 43823 (38502)	Loss/tok 3.3853 (3.3828)	LR 2.000e-03
0: TRAIN [1][1900/3880]	Time 0.137 (0.182)	Data 7.63e-05 (4.62e-04)	Tok/s 19037 (38512)	Loss/tok 2.7093 (3.3831)	LR 2.000e-03
0: TRAIN [1][1910/3880]	Time 0.162 (0.182)	Data 9.42e-05 (4.60e-04)	Tok/s 31292 (38512)	Loss/tok 3.3653 (3.3830)	LR 2.000e-03
0: TRAIN [1][1920/3880]	Time 0.191 (0.182)	Data 9.30e-05 (4.58e-04)	Tok/s 44124 (38514)	Loss/tok 3.3817 (3.3831)	LR 2.000e-03
0: TRAIN [1][1930/3880]	Time 0.221 (0.182)	Data 9.47e-05 (4.56e-04)	Tok/s 53961 (38530)	Loss/tok 3.4359 (3.3830)	LR 2.000e-03
0: TRAIN [1][1940/3880]	Time 0.191 (0.182)	Data 7.92e-05 (4.55e-04)	Tok/s 42983 (38543)	Loss/tok 3.2882 (3.3828)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1950/3880]	Time 0.159 (0.182)	Data 8.37e-05 (4.53e-04)	Tok/s 32739 (38549)	Loss/tok 3.1549 (3.3831)	LR 2.000e-03
0: TRAIN [1][1960/3880]	Time 0.191 (0.182)	Data 7.89e-05 (4.51e-04)	Tok/s 44284 (38551)	Loss/tok 3.4128 (3.3830)	LR 2.000e-03
0: TRAIN [1][1970/3880]	Time 0.191 (0.182)	Data 8.99e-05 (4.49e-04)	Tok/s 43065 (38557)	Loss/tok 3.4259 (3.3826)	LR 2.000e-03
0: TRAIN [1][1980/3880]	Time 0.191 (0.182)	Data 8.75e-05 (4.47e-04)	Tok/s 43752 (38579)	Loss/tok 3.4997 (3.3829)	LR 2.000e-03
0: TRAIN [1][1990/3880]	Time 0.220 (0.182)	Data 7.89e-05 (4.45e-04)	Tok/s 51636 (38586)	Loss/tok 3.5777 (3.3831)	LR 2.000e-03
0: TRAIN [1][2000/3880]	Time 0.162 (0.182)	Data 9.16e-05 (4.43e-04)	Tok/s 31028 (38573)	Loss/tok 3.2820 (3.3829)	LR 2.000e-03
0: TRAIN [1][2010/3880]	Time 0.136 (0.182)	Data 9.54e-05 (4.42e-04)	Tok/s 19256 (38556)	Loss/tok 2.7822 (3.3824)	LR 2.000e-03
0: TRAIN [1][2020/3880]	Time 0.162 (0.182)	Data 7.89e-05 (4.40e-04)	Tok/s 30898 (38527)	Loss/tok 3.0806 (3.3818)	LR 2.000e-03
0: TRAIN [1][2030/3880]	Time 0.162 (0.182)	Data 7.77e-05 (4.38e-04)	Tok/s 32221 (38523)	Loss/tok 3.1104 (3.3814)	LR 2.000e-03
0: TRAIN [1][2040/3880]	Time 0.162 (0.182)	Data 1.27e-04 (4.37e-04)	Tok/s 31660 (38550)	Loss/tok 3.0379 (3.3818)	LR 2.000e-03
0: TRAIN [1][2050/3880]	Time 0.162 (0.182)	Data 7.96e-05 (4.35e-04)	Tok/s 32375 (38546)	Loss/tok 3.1626 (3.3814)	LR 2.000e-03
0: TRAIN [1][2060/3880]	Time 0.162 (0.182)	Data 9.27e-05 (4.33e-04)	Tok/s 31616 (38522)	Loss/tok 3.1314 (3.3806)	LR 2.000e-03
0: TRAIN [1][2070/3880]	Time 0.162 (0.182)	Data 7.96e-05 (4.32e-04)	Tok/s 31317 (38551)	Loss/tok 3.1406 (3.3814)	LR 2.000e-03
0: TRAIN [1][2080/3880]	Time 0.162 (0.182)	Data 4.62e-04 (4.30e-04)	Tok/s 32035 (38530)	Loss/tok 3.1270 (3.3812)	LR 2.000e-03
0: TRAIN [1][2090/3880]	Time 0.162 (0.182)	Data 8.27e-05 (4.28e-04)	Tok/s 31104 (38518)	Loss/tok 3.1818 (3.3808)	LR 2.000e-03
0: TRAIN [1][2100/3880]	Time 0.191 (0.182)	Data 9.51e-05 (4.27e-04)	Tok/s 43453 (38516)	Loss/tok 3.4937 (3.3807)	LR 2.000e-03
0: TRAIN [1][2110/3880]	Time 0.163 (0.182)	Data 8.01e-05 (4.25e-04)	Tok/s 31700 (38512)	Loss/tok 3.1995 (3.3803)	LR 2.000e-03
0: TRAIN [1][2120/3880]	Time 0.221 (0.182)	Data 8.08e-05 (4.24e-04)	Tok/s 54016 (38541)	Loss/tok 3.5305 (3.3805)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][2130/3880]	Time 0.220 (0.182)	Data 8.54e-05 (4.22e-04)	Tok/s 53434 (38556)	Loss/tok 3.4462 (3.3805)	LR 2.000e-03
0: TRAIN [1][2140/3880]	Time 0.162 (0.182)	Data 9.87e-05 (4.20e-04)	Tok/s 32165 (38525)	Loss/tok 3.2874 (3.3799)	LR 2.000e-03
0: TRAIN [1][2150/3880]	Time 0.221 (0.182)	Data 8.25e-05 (4.19e-04)	Tok/s 53309 (38529)	Loss/tok 3.5723 (3.3798)	LR 2.000e-03
0: TRAIN [1][2160/3880]	Time 0.162 (0.182)	Data 9.63e-05 (4.17e-04)	Tok/s 31637 (38547)	Loss/tok 3.3442 (3.3804)	LR 2.000e-03
0: TRAIN [1][2170/3880]	Time 0.191 (0.182)	Data 9.58e-05 (4.16e-04)	Tok/s 43661 (38553)	Loss/tok 3.4706 (3.3802)	LR 2.000e-03
0: TRAIN [1][2180/3880]	Time 0.221 (0.182)	Data 8.61e-05 (4.14e-04)	Tok/s 52382 (38549)	Loss/tok 3.4509 (3.3800)	LR 2.000e-03
0: TRAIN [1][2190/3880]	Time 0.221 (0.182)	Data 7.94e-05 (4.13e-04)	Tok/s 53310 (38538)	Loss/tok 3.4681 (3.3796)	LR 2.000e-03
0: TRAIN [1][2200/3880]	Time 0.137 (0.182)	Data 8.37e-05 (4.11e-04)	Tok/s 19941 (38528)	Loss/tok 2.8537 (3.3793)	LR 2.000e-03
0: TRAIN [1][2210/3880]	Time 0.162 (0.182)	Data 7.99e-05 (4.10e-04)	Tok/s 32095 (38518)	Loss/tok 3.2403 (3.3789)	LR 2.000e-03
0: TRAIN [1][2220/3880]	Time 0.191 (0.182)	Data 8.96e-05 (4.09e-04)	Tok/s 43617 (38526)	Loss/tok 3.3398 (3.3791)	LR 2.000e-03
0: TRAIN [1][2230/3880]	Time 0.191 (0.182)	Data 9.20e-05 (4.07e-04)	Tok/s 43611 (38541)	Loss/tok 3.3805 (3.3789)	LR 2.000e-03
0: TRAIN [1][2240/3880]	Time 0.191 (0.182)	Data 9.42e-05 (4.06e-04)	Tok/s 43512 (38573)	Loss/tok 3.4388 (3.3795)	LR 2.000e-03
0: TRAIN [1][2250/3880]	Time 0.162 (0.182)	Data 7.96e-05 (4.05e-04)	Tok/s 32310 (38559)	Loss/tok 3.1133 (3.3791)	LR 2.000e-03
0: TRAIN [1][2260/3880]	Time 0.191 (0.182)	Data 9.51e-05 (4.03e-04)	Tok/s 43661 (38550)	Loss/tok 3.2603 (3.3788)	LR 2.000e-03
0: TRAIN [1][2270/3880]	Time 0.191 (0.182)	Data 7.99e-05 (4.02e-04)	Tok/s 44290 (38536)	Loss/tok 3.4528 (3.3783)	LR 2.000e-03
0: TRAIN [1][2280/3880]	Time 0.191 (0.182)	Data 8.87e-05 (4.00e-04)	Tok/s 43875 (38522)	Loss/tok 3.3121 (3.3777)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][2290/3880]	Time 0.191 (0.182)	Data 8.11e-05 (3.99e-04)	Tok/s 42663 (38535)	Loss/tok 3.4454 (3.3778)	LR 2.000e-03
0: TRAIN [1][2300/3880]	Time 0.162 (0.182)	Data 9.27e-05 (3.98e-04)	Tok/s 32319 (38539)	Loss/tok 3.0770 (3.3775)	LR 2.000e-03
0: TRAIN [1][2310/3880]	Time 0.162 (0.182)	Data 9.51e-05 (3.97e-04)	Tok/s 31662 (38520)	Loss/tok 3.2924 (3.3768)	LR 2.000e-03
0: TRAIN [1][2320/3880]	Time 0.162 (0.182)	Data 9.23e-05 (3.95e-04)	Tok/s 32427 (38514)	Loss/tok 3.0419 (3.3762)	LR 2.000e-03
0: TRAIN [1][2330/3880]	Time 0.163 (0.182)	Data 8.20e-05 (3.94e-04)	Tok/s 32543 (38519)	Loss/tok 3.1787 (3.3761)	LR 2.000e-03
0: TRAIN [1][2340/3880]	Time 0.191 (0.182)	Data 9.42e-05 (3.93e-04)	Tok/s 43723 (38523)	Loss/tok 3.2397 (3.3761)	LR 2.000e-03
0: TRAIN [1][2350/3880]	Time 0.136 (0.182)	Data 7.89e-05 (3.91e-04)	Tok/s 19680 (38510)	Loss/tok 2.7867 (3.3760)	LR 2.000e-03
0: TRAIN [1][2360/3880]	Time 0.162 (0.181)	Data 7.49e-05 (3.90e-04)	Tok/s 31238 (38494)	Loss/tok 3.1635 (3.3757)	LR 2.000e-03
0: TRAIN [1][2370/3880]	Time 0.162 (0.182)	Data 7.65e-05 (3.89e-04)	Tok/s 32588 (38508)	Loss/tok 3.1190 (3.3758)	LR 2.000e-03
0: TRAIN [1][2380/3880]	Time 0.162 (0.181)	Data 7.92e-05 (3.88e-04)	Tok/s 32045 (38490)	Loss/tok 3.1850 (3.3754)	LR 2.000e-03
0: TRAIN [1][2390/3880]	Time 0.162 (0.181)	Data 1.05e-04 (3.87e-04)	Tok/s 31817 (38477)	Loss/tok 3.0007 (3.3748)	LR 2.000e-03
0: TRAIN [1][2400/3880]	Time 0.191 (0.181)	Data 9.37e-05 (3.85e-04)	Tok/s 44613 (38483)	Loss/tok 3.2826 (3.3745)	LR 2.000e-03
0: TRAIN [1][2410/3880]	Time 0.162 (0.181)	Data 9.11e-05 (3.84e-04)	Tok/s 31266 (38491)	Loss/tok 3.1710 (3.3748)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][2420/3880]	Time 0.162 (0.182)	Data 8.92e-05 (3.83e-04)	Tok/s 32166 (38511)	Loss/tok 3.0541 (3.3750)	LR 2.000e-03
0: TRAIN [1][2430/3880]	Time 0.162 (0.182)	Data 1.18e-04 (3.82e-04)	Tok/s 31476 (38513)	Loss/tok 3.2164 (3.3747)	LR 2.000e-03
0: TRAIN [1][2440/3880]	Time 0.162 (0.182)	Data 9.61e-05 (3.80e-04)	Tok/s 31018 (38510)	Loss/tok 3.1410 (3.3744)	LR 2.000e-03
0: TRAIN [1][2450/3880]	Time 0.191 (0.182)	Data 9.70e-05 (3.79e-04)	Tok/s 43886 (38523)	Loss/tok 3.4059 (3.3747)	LR 2.000e-03
0: TRAIN [1][2460/3880]	Time 0.257 (0.182)	Data 9.39e-05 (3.78e-04)	Tok/s 57898 (38545)	Loss/tok 3.6976 (3.3752)	LR 2.000e-03
0: TRAIN [1][2470/3880]	Time 0.162 (0.182)	Data 9.80e-05 (3.77e-04)	Tok/s 31422 (38564)	Loss/tok 3.1990 (3.3757)	LR 2.000e-03
0: TRAIN [1][2480/3880]	Time 0.191 (0.182)	Data 8.15e-05 (3.76e-04)	Tok/s 43246 (38593)	Loss/tok 3.3113 (3.3760)	LR 2.000e-03
0: TRAIN [1][2490/3880]	Time 0.162 (0.182)	Data 9.35e-05 (3.75e-04)	Tok/s 32701 (38608)	Loss/tok 3.2402 (3.3758)	LR 2.000e-03
0: TRAIN [1][2500/3880]	Time 0.191 (0.182)	Data 7.94e-05 (3.74e-04)	Tok/s 43940 (38612)	Loss/tok 3.3018 (3.3756)	LR 2.000e-03
0: TRAIN [1][2510/3880]	Time 0.221 (0.182)	Data 8.08e-05 (3.73e-04)	Tok/s 52113 (38605)	Loss/tok 3.5896 (3.3755)	LR 2.000e-03
0: TRAIN [1][2520/3880]	Time 0.162 (0.182)	Data 8.73e-05 (3.72e-04)	Tok/s 32037 (38607)	Loss/tok 3.2173 (3.3752)	LR 2.000e-03
0: TRAIN [1][2530/3880]	Time 0.191 (0.182)	Data 7.75e-05 (3.71e-04)	Tok/s 43616 (38603)	Loss/tok 3.2574 (3.3754)	LR 2.000e-03
0: TRAIN [1][2540/3880]	Time 0.258 (0.182)	Data 1.01e-04 (3.70e-04)	Tok/s 58665 (38610)	Loss/tok 3.7263 (3.3754)	LR 2.000e-03
0: TRAIN [1][2550/3880]	Time 0.136 (0.182)	Data 9.75e-05 (3.69e-04)	Tok/s 19480 (38591)	Loss/tok 2.5929 (3.3750)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][2560/3880]	Time 0.257 (0.182)	Data 8.06e-05 (3.68e-04)	Tok/s 58369 (38599)	Loss/tok 3.6142 (3.3750)	LR 2.000e-03
0: TRAIN [1][2570/3880]	Time 0.192 (0.182)	Data 8.30e-05 (3.66e-04)	Tok/s 43361 (38576)	Loss/tok 3.1173 (3.3744)	LR 2.000e-03
0: TRAIN [1][2580/3880]	Time 0.136 (0.182)	Data 9.66e-05 (3.65e-04)	Tok/s 19224 (38566)	Loss/tok 2.6510 (3.3742)	LR 2.000e-03
0: TRAIN [1][2590/3880]	Time 0.163 (0.182)	Data 8.25e-05 (3.64e-04)	Tok/s 31901 (38566)	Loss/tok 3.0749 (3.3743)	LR 2.000e-03
0: TRAIN [1][2600/3880]	Time 0.162 (0.182)	Data 9.80e-05 (3.63e-04)	Tok/s 31119 (38560)	Loss/tok 3.1529 (3.3741)	LR 2.000e-03
0: TRAIN [1][2610/3880]	Time 0.221 (0.182)	Data 1.01e-04 (3.62e-04)	Tok/s 52374 (38571)	Loss/tok 3.6397 (3.3743)	LR 1.000e-03
0: TRAIN [1][2620/3880]	Time 0.191 (0.182)	Data 8.49e-05 (3.61e-04)	Tok/s 43614 (38568)	Loss/tok 3.3211 (3.3739)	LR 1.000e-03
0: TRAIN [1][2630/3880]	Time 0.162 (0.182)	Data 8.08e-05 (3.60e-04)	Tok/s 31745 (38583)	Loss/tok 3.0058 (3.3741)	LR 1.000e-03
0: TRAIN [1][2640/3880]	Time 0.162 (0.182)	Data 8.75e-05 (3.59e-04)	Tok/s 31364 (38571)	Loss/tok 3.1152 (3.3735)	LR 1.000e-03
0: TRAIN [1][2650/3880]	Time 0.257 (0.182)	Data 8.20e-05 (3.58e-04)	Tok/s 58307 (38565)	Loss/tok 3.6460 (3.3734)	LR 1.000e-03
0: TRAIN [1][2660/3880]	Time 0.191 (0.182)	Data 9.89e-05 (3.57e-04)	Tok/s 43591 (38562)	Loss/tok 3.3780 (3.3729)	LR 1.000e-03
0: TRAIN [1][2670/3880]	Time 0.191 (0.182)	Data 9.47e-05 (3.57e-04)	Tok/s 44068 (38570)	Loss/tok 3.4117 (3.3728)	LR 1.000e-03
0: TRAIN [1][2680/3880]	Time 0.191 (0.182)	Data 7.70e-05 (3.56e-04)	Tok/s 45085 (38594)	Loss/tok 3.2788 (3.3729)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][2690/3880]	Time 0.163 (0.182)	Data 9.39e-05 (3.55e-04)	Tok/s 32138 (38580)	Loss/tok 3.0394 (3.3725)	LR 1.000e-03
0: TRAIN [1][2700/3880]	Time 0.162 (0.182)	Data 2.47e-04 (3.54e-04)	Tok/s 32038 (38583)	Loss/tok 3.0451 (3.3724)	LR 1.000e-03
0: TRAIN [1][2710/3880]	Time 0.162 (0.182)	Data 8.03e-05 (3.53e-04)	Tok/s 31383 (38557)	Loss/tok 3.0775 (3.3717)	LR 1.000e-03
0: TRAIN [1][2720/3880]	Time 0.221 (0.182)	Data 9.56e-05 (3.52e-04)	Tok/s 52259 (38540)	Loss/tok 3.5597 (3.3712)	LR 1.000e-03
0: TRAIN [1][2730/3880]	Time 0.191 (0.182)	Data 9.68e-05 (3.51e-04)	Tok/s 44345 (38556)	Loss/tok 3.2090 (3.3714)	LR 1.000e-03
0: TRAIN [1][2740/3880]	Time 0.138 (0.182)	Data 9.58e-05 (3.50e-04)	Tok/s 18640 (38564)	Loss/tok 2.7848 (3.3711)	LR 1.000e-03
0: TRAIN [1][2750/3880]	Time 0.162 (0.182)	Data 8.08e-05 (3.49e-04)	Tok/s 32885 (38552)	Loss/tok 3.1962 (3.3707)	LR 1.000e-03
0: TRAIN [1][2760/3880]	Time 0.163 (0.182)	Data 9.82e-05 (3.48e-04)	Tok/s 32023 (38558)	Loss/tok 3.0622 (3.3708)	LR 1.000e-03
0: TRAIN [1][2770/3880]	Time 0.191 (0.182)	Data 9.80e-05 (3.47e-04)	Tok/s 44255 (38563)	Loss/tok 3.2891 (3.3709)	LR 1.000e-03
0: TRAIN [1][2780/3880]	Time 0.257 (0.182)	Data 3.96e-04 (3.46e-04)	Tok/s 58188 (38566)	Loss/tok 3.6242 (3.3711)	LR 1.000e-03
0: TRAIN [1][2790/3880]	Time 0.191 (0.182)	Data 7.92e-05 (3.46e-04)	Tok/s 43752 (38555)	Loss/tok 3.3513 (3.3707)	LR 1.000e-03
0: TRAIN [1][2800/3880]	Time 0.191 (0.182)	Data 1.01e-04 (3.45e-04)	Tok/s 43985 (38572)	Loss/tok 3.2069 (3.3705)	LR 1.000e-03
0: TRAIN [1][2810/3880]	Time 0.162 (0.182)	Data 9.13e-05 (3.44e-04)	Tok/s 30810 (38560)	Loss/tok 3.0052 (3.3698)	LR 1.000e-03
0: TRAIN [1][2820/3880]	Time 0.137 (0.182)	Data 7.99e-05 (3.43e-04)	Tok/s 19665 (38551)	Loss/tok 2.6802 (3.3696)	LR 1.000e-03
0: TRAIN [1][2830/3880]	Time 0.191 (0.182)	Data 9.35e-05 (3.42e-04)	Tok/s 43073 (38571)	Loss/tok 3.4276 (3.3696)	LR 1.000e-03
0: TRAIN [1][2840/3880]	Time 0.162 (0.182)	Data 1.15e-04 (3.41e-04)	Tok/s 31470 (38573)	Loss/tok 3.0287 (3.3694)	LR 1.000e-03
0: TRAIN [1][2850/3880]	Time 0.136 (0.182)	Data 8.06e-05 (3.40e-04)	Tok/s 19119 (38575)	Loss/tok 2.7099 (3.3691)	LR 1.000e-03
0: TRAIN [1][2860/3880]	Time 0.162 (0.182)	Data 1.07e-04 (3.40e-04)	Tok/s 31161 (38573)	Loss/tok 2.9778 (3.3687)	LR 1.000e-03
0: TRAIN [1][2870/3880]	Time 0.162 (0.182)	Data 8.08e-05 (3.39e-04)	Tok/s 32067 (38574)	Loss/tok 3.0685 (3.3685)	LR 1.000e-03
0: TRAIN [1][2880/3880]	Time 0.162 (0.182)	Data 7.94e-05 (3.38e-04)	Tok/s 31333 (38557)	Loss/tok 3.3320 (3.3681)	LR 1.000e-03
0: TRAIN [1][2890/3880]	Time 0.162 (0.182)	Data 9.54e-05 (3.37e-04)	Tok/s 32192 (38545)	Loss/tok 3.0921 (3.3677)	LR 1.000e-03
0: TRAIN [1][2900/3880]	Time 0.257 (0.182)	Data 9.56e-05 (3.36e-04)	Tok/s 57970 (38550)	Loss/tok 3.5829 (3.3676)	LR 1.000e-03
0: TRAIN [1][2910/3880]	Time 0.162 (0.182)	Data 9.30e-05 (3.35e-04)	Tok/s 32203 (38524)	Loss/tok 3.1129 (3.3669)	LR 1.000e-03
0: TRAIN [1][2920/3880]	Time 0.162 (0.182)	Data 9.35e-05 (3.35e-04)	Tok/s 31144 (38497)	Loss/tok 2.9920 (3.3662)	LR 1.000e-03
0: TRAIN [1][2930/3880]	Time 0.191 (0.182)	Data 1.14e-04 (3.34e-04)	Tok/s 43980 (38496)	Loss/tok 3.1265 (3.3658)	LR 1.000e-03
0: TRAIN [1][2940/3880]	Time 0.162 (0.182)	Data 8.49e-05 (3.33e-04)	Tok/s 31746 (38511)	Loss/tok 3.2307 (3.3658)	LR 1.000e-03
0: TRAIN [1][2950/3880]	Time 0.162 (0.182)	Data 1.02e-04 (3.32e-04)	Tok/s 32338 (38501)	Loss/tok 3.0711 (3.3652)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2960/3880]	Time 0.191 (0.181)	Data 1.02e-04 (3.32e-04)	Tok/s 44393 (38493)	Loss/tok 3.4183 (3.3648)	LR 1.000e-03
0: TRAIN [1][2970/3880]	Time 0.222 (0.181)	Data 7.68e-05 (3.31e-04)	Tok/s 53052 (38489)	Loss/tok 3.5533 (3.3645)	LR 1.000e-03
0: TRAIN [1][2980/3880]	Time 0.162 (0.181)	Data 7.75e-05 (3.30e-04)	Tok/s 32721 (38491)	Loss/tok 3.0663 (3.3641)	LR 1.000e-03
0: TRAIN [1][2990/3880]	Time 0.162 (0.181)	Data 7.87e-05 (3.29e-04)	Tok/s 32725 (38476)	Loss/tok 3.0566 (3.3637)	LR 1.000e-03
0: TRAIN [1][3000/3880]	Time 0.162 (0.181)	Data 9.54e-05 (3.28e-04)	Tok/s 32683 (38469)	Loss/tok 3.1067 (3.3634)	LR 1.000e-03
0: TRAIN [1][3010/3880]	Time 0.192 (0.181)	Data 7.60e-04 (3.28e-04)	Tok/s 44663 (38455)	Loss/tok 3.1489 (3.3628)	LR 1.000e-03
0: TRAIN [1][3020/3880]	Time 0.221 (0.181)	Data 8.34e-05 (3.27e-04)	Tok/s 53188 (38460)	Loss/tok 3.5246 (3.3628)	LR 1.000e-03
0: TRAIN [1][3030/3880]	Time 0.191 (0.181)	Data 8.06e-05 (3.26e-04)	Tok/s 43786 (38457)	Loss/tok 3.2060 (3.3622)	LR 1.000e-03
0: TRAIN [1][3040/3880]	Time 0.163 (0.181)	Data 7.53e-05 (3.26e-04)	Tok/s 32166 (38443)	Loss/tok 3.1070 (3.3616)	LR 1.000e-03
0: TRAIN [1][3050/3880]	Time 0.162 (0.181)	Data 8.39e-05 (3.25e-04)	Tok/s 31785 (38448)	Loss/tok 3.2706 (3.3613)	LR 1.000e-03
0: TRAIN [1][3060/3880]	Time 0.162 (0.181)	Data 1.09e-04 (3.24e-04)	Tok/s 31776 (38455)	Loss/tok 3.0903 (3.3610)	LR 1.000e-03
0: TRAIN [1][3070/3880]	Time 0.162 (0.181)	Data 9.82e-05 (3.23e-04)	Tok/s 32052 (38462)	Loss/tok 3.1370 (3.3610)	LR 1.000e-03
0: TRAIN [1][3080/3880]	Time 0.191 (0.181)	Data 9.58e-05 (3.23e-04)	Tok/s 43817 (38471)	Loss/tok 3.1972 (3.3610)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][3090/3880]	Time 0.191 (0.181)	Data 8.01e-05 (3.22e-04)	Tok/s 43730 (38464)	Loss/tok 3.3095 (3.3606)	LR 1.000e-03
0: TRAIN [1][3100/3880]	Time 0.191 (0.181)	Data 9.08e-05 (3.21e-04)	Tok/s 44349 (38482)	Loss/tok 3.3963 (3.3607)	LR 1.000e-03
0: TRAIN [1][3110/3880]	Time 0.162 (0.181)	Data 9.94e-05 (3.20e-04)	Tok/s 32219 (38493)	Loss/tok 2.9593 (3.3606)	LR 1.000e-03
0: TRAIN [1][3120/3880]	Time 0.162 (0.181)	Data 8.20e-05 (3.20e-04)	Tok/s 31601 (38493)	Loss/tok 3.0371 (3.3603)	LR 1.000e-03
0: TRAIN [1][3130/3880]	Time 0.191 (0.181)	Data 8.32e-05 (3.19e-04)	Tok/s 44012 (38487)	Loss/tok 3.2899 (3.3599)	LR 1.000e-03
0: TRAIN [1][3140/3880]	Time 0.162 (0.181)	Data 8.03e-05 (3.18e-04)	Tok/s 33250 (38496)	Loss/tok 2.9468 (3.3595)	LR 1.000e-03
0: TRAIN [1][3150/3880]	Time 0.220 (0.181)	Data 8.11e-05 (3.18e-04)	Tok/s 53051 (38491)	Loss/tok 3.3885 (3.3591)	LR 1.000e-03
0: TRAIN [1][3160/3880]	Time 0.137 (0.181)	Data 8.70e-05 (3.17e-04)	Tok/s 19289 (38462)	Loss/tok 2.7691 (3.3586)	LR 1.000e-03
0: TRAIN [1][3170/3880]	Time 0.191 (0.181)	Data 8.15e-05 (3.16e-04)	Tok/s 43356 (38465)	Loss/tok 3.1759 (3.3582)	LR 1.000e-03
0: TRAIN [1][3180/3880]	Time 0.191 (0.181)	Data 9.49e-05 (3.16e-04)	Tok/s 43812 (38461)	Loss/tok 3.3772 (3.3578)	LR 1.000e-03
0: TRAIN [1][3190/3880]	Time 0.191 (0.181)	Data 9.56e-05 (3.15e-04)	Tok/s 44257 (38459)	Loss/tok 3.1649 (3.3576)	LR 1.000e-03
0: TRAIN [1][3200/3880]	Time 0.163 (0.181)	Data 8.80e-05 (3.14e-04)	Tok/s 31609 (38452)	Loss/tok 3.0115 (3.3572)	LR 1.000e-03
0: TRAIN [1][3210/3880]	Time 0.162 (0.181)	Data 9.89e-05 (3.14e-04)	Tok/s 31248 (38456)	Loss/tok 3.0483 (3.3572)	LR 1.000e-03
0: TRAIN [1][3220/3880]	Time 0.191 (0.181)	Data 8.87e-05 (3.13e-04)	Tok/s 44681 (38469)	Loss/tok 2.9932 (3.3569)	LR 1.000e-03
0: TRAIN [1][3230/3880]	Time 0.162 (0.181)	Data 8.49e-05 (3.12e-04)	Tok/s 31624 (38474)	Loss/tok 2.7873 (3.3568)	LR 1.000e-03
0: TRAIN [1][3240/3880]	Time 0.192 (0.181)	Data 9.54e-05 (3.12e-04)	Tok/s 43223 (38471)	Loss/tok 3.3064 (3.3563)	LR 1.000e-03
0: TRAIN [1][3250/3880]	Time 0.191 (0.181)	Data 9.44e-05 (3.11e-04)	Tok/s 43942 (38459)	Loss/tok 3.2642 (3.3562)	LR 1.000e-03
0: TRAIN [1][3260/3880]	Time 0.162 (0.181)	Data 9.63e-05 (3.10e-04)	Tok/s 32287 (38442)	Loss/tok 3.2478 (3.3556)	LR 1.000e-03
0: TRAIN [1][3270/3880]	Time 0.257 (0.181)	Data 9.66e-05 (3.10e-04)	Tok/s 59290 (38464)	Loss/tok 3.5389 (3.3557)	LR 1.000e-03
0: TRAIN [1][3280/3880]	Time 0.162 (0.181)	Data 7.96e-05 (3.09e-04)	Tok/s 33129 (38460)	Loss/tok 2.9986 (3.3555)	LR 1.000e-03
0: TRAIN [1][3290/3880]	Time 0.137 (0.181)	Data 6.70e-04 (3.08e-04)	Tok/s 18863 (38456)	Loss/tok 2.7044 (3.3552)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][3300/3880]	Time 0.256 (0.181)	Data 7.99e-05 (3.08e-04)	Tok/s 57849 (38470)	Loss/tok 3.7134 (3.3553)	LR 1.000e-03
0: TRAIN [1][3310/3880]	Time 0.137 (0.181)	Data 8.03e-05 (3.07e-04)	Tok/s 18803 (38445)	Loss/tok 2.7463 (3.3548)	LR 1.000e-03
0: TRAIN [1][3320/3880]	Time 0.162 (0.181)	Data 8.06e-05 (3.06e-04)	Tok/s 33622 (38447)	Loss/tok 3.1407 (3.3549)	LR 1.000e-03
0: TRAIN [1][3330/3880]	Time 0.162 (0.181)	Data 8.94e-05 (3.06e-04)	Tok/s 31756 (38453)	Loss/tok 2.8964 (3.3545)	LR 1.000e-03
0: TRAIN [1][3340/3880]	Time 0.136 (0.181)	Data 9.32e-05 (3.05e-04)	Tok/s 19664 (38451)	Loss/tok 2.6509 (3.3541)	LR 1.000e-03
0: TRAIN [1][3350/3880]	Time 0.136 (0.181)	Data 9.58e-05 (3.05e-04)	Tok/s 18489 (38442)	Loss/tok 2.5856 (3.3537)	LR 1.000e-03
0: TRAIN [1][3360/3880]	Time 0.136 (0.181)	Data 9.35e-05 (3.04e-04)	Tok/s 19356 (38434)	Loss/tok 2.7364 (3.3535)	LR 1.000e-03
0: TRAIN [1][3370/3880]	Time 0.162 (0.181)	Data 8.37e-05 (3.03e-04)	Tok/s 32507 (38440)	Loss/tok 3.2754 (3.3532)	LR 1.000e-03
0: TRAIN [1][3380/3880]	Time 0.221 (0.181)	Data 8.01e-05 (3.03e-04)	Tok/s 52762 (38463)	Loss/tok 3.4968 (3.3533)	LR 1.000e-03
0: TRAIN [1][3390/3880]	Time 0.136 (0.181)	Data 9.73e-05 (3.02e-04)	Tok/s 19345 (38461)	Loss/tok 2.6475 (3.3531)	LR 1.000e-03
0: TRAIN [1][3400/3880]	Time 0.162 (0.181)	Data 8.11e-05 (3.01e-04)	Tok/s 30822 (38474)	Loss/tok 3.2167 (3.3532)	LR 1.000e-03
0: TRAIN [1][3410/3880]	Time 0.221 (0.181)	Data 8.15e-05 (3.01e-04)	Tok/s 52541 (38485)	Loss/tok 3.4532 (3.3531)	LR 1.000e-03
0: TRAIN [1][3420/3880]	Time 0.191 (0.181)	Data 8.06e-05 (3.00e-04)	Tok/s 44973 (38489)	Loss/tok 3.3700 (3.3528)	LR 1.000e-03
0: TRAIN [1][3430/3880]	Time 0.162 (0.181)	Data 7.75e-05 (3.00e-04)	Tok/s 30734 (38466)	Loss/tok 3.0602 (3.3523)	LR 1.000e-03
0: TRAIN [1][3440/3880]	Time 0.162 (0.181)	Data 7.77e-05 (2.99e-04)	Tok/s 31893 (38457)	Loss/tok 3.1999 (3.3518)	LR 1.000e-03
0: TRAIN [1][3450/3880]	Time 0.191 (0.181)	Data 9.56e-05 (2.98e-04)	Tok/s 43725 (38459)	Loss/tok 3.2530 (3.3517)	LR 1.000e-03
0: TRAIN [1][3460/3880]	Time 0.191 (0.181)	Data 7.63e-05 (2.98e-04)	Tok/s 43950 (38458)	Loss/tok 3.1593 (3.3513)	LR 1.000e-03
0: TRAIN [1][3470/3880]	Time 0.258 (0.181)	Data 9.35e-05 (2.97e-04)	Tok/s 57915 (38476)	Loss/tok 3.5201 (3.3514)	LR 1.000e-03
0: TRAIN [1][3480/3880]	Time 0.162 (0.181)	Data 9.85e-05 (2.97e-04)	Tok/s 32087 (38484)	Loss/tok 3.0596 (3.3514)	LR 1.000e-03
0: TRAIN [1][3490/3880]	Time 0.221 (0.181)	Data 7.99e-05 (2.96e-04)	Tok/s 52697 (38497)	Loss/tok 3.3112 (3.3512)	LR 1.000e-03
0: TRAIN [1][3500/3880]	Time 0.162 (0.181)	Data 7.70e-05 (2.95e-04)	Tok/s 31973 (38500)	Loss/tok 2.9444 (3.3510)	LR 1.000e-03
0: TRAIN [1][3510/3880]	Time 0.191 (0.181)	Data 8.01e-05 (2.95e-04)	Tok/s 43892 (38499)	Loss/tok 3.1667 (3.3506)	LR 1.000e-03
0: TRAIN [1][3520/3880]	Time 0.191 (0.181)	Data 8.58e-05 (2.94e-04)	Tok/s 42994 (38497)	Loss/tok 3.1451 (3.3505)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][3530/3880]	Time 0.162 (0.181)	Data 7.89e-05 (2.94e-04)	Tok/s 32190 (38497)	Loss/tok 3.1112 (3.3501)	LR 1.000e-03
0: TRAIN [1][3540/3880]	Time 0.221 (0.181)	Data 8.18e-05 (2.93e-04)	Tok/s 53293 (38505)	Loss/tok 3.4519 (3.3500)	LR 1.000e-03
0: TRAIN [1][3550/3880]	Time 0.136 (0.181)	Data 7.89e-05 (2.93e-04)	Tok/s 19502 (38496)	Loss/tok 2.5076 (3.3497)	LR 1.000e-03
0: TRAIN [1][3560/3880]	Time 0.221 (0.181)	Data 7.77e-05 (2.92e-04)	Tok/s 52860 (38483)	Loss/tok 3.3719 (3.3492)	LR 1.000e-03
0: TRAIN [1][3570/3880]	Time 0.221 (0.181)	Data 9.30e-05 (2.92e-04)	Tok/s 52785 (38492)	Loss/tok 3.4079 (3.3490)	LR 1.000e-03
0: TRAIN [1][3580/3880]	Time 0.163 (0.181)	Data 7.92e-05 (2.91e-04)	Tok/s 31159 (38487)	Loss/tok 3.0593 (3.3489)	LR 1.000e-03
0: TRAIN [1][3590/3880]	Time 0.163 (0.181)	Data 7.84e-05 (2.90e-04)	Tok/s 31713 (38500)	Loss/tok 3.0199 (3.3486)	LR 1.000e-03
0: TRAIN [1][3600/3880]	Time 0.191 (0.182)	Data 1.01e-04 (2.90e-04)	Tok/s 44139 (38513)	Loss/tok 3.2403 (3.3485)	LR 1.000e-03
0: TRAIN [1][3610/3880]	Time 0.136 (0.182)	Data 8.18e-05 (2.89e-04)	Tok/s 18908 (38510)	Loss/tok 2.7304 (3.3483)	LR 1.000e-03
0: TRAIN [1][3620/3880]	Time 0.162 (0.182)	Data 9.27e-05 (2.89e-04)	Tok/s 32190 (38511)	Loss/tok 3.0397 (3.3482)	LR 1.000e-03
0: TRAIN [1][3630/3880]	Time 0.221 (0.182)	Data 9.32e-05 (2.88e-04)	Tok/s 52620 (38513)	Loss/tok 3.4192 (3.3480)	LR 1.000e-03
0: TRAIN [1][3640/3880]	Time 0.221 (0.182)	Data 9.49e-05 (2.88e-04)	Tok/s 53142 (38538)	Loss/tok 3.4653 (3.3484)	LR 1.000e-03
0: TRAIN [1][3650/3880]	Time 0.257 (0.182)	Data 9.35e-05 (2.87e-04)	Tok/s 58248 (38543)	Loss/tok 3.4499 (3.3481)	LR 1.000e-03
0: TRAIN [1][3660/3880]	Time 0.137 (0.182)	Data 8.46e-05 (2.87e-04)	Tok/s 19501 (38544)	Loss/tok 2.7326 (3.3479)	LR 1.000e-03
0: TRAIN [1][3670/3880]	Time 0.162 (0.182)	Data 9.70e-05 (2.86e-04)	Tok/s 32138 (38550)	Loss/tok 3.2777 (3.3478)	LR 1.000e-03
0: TRAIN [1][3680/3880]	Time 0.163 (0.182)	Data 9.89e-05 (2.86e-04)	Tok/s 31125 (38547)	Loss/tok 3.1255 (3.3474)	LR 1.000e-03
0: TRAIN [1][3690/3880]	Time 0.191 (0.182)	Data 9.20e-05 (2.85e-04)	Tok/s 43823 (38556)	Loss/tok 3.4204 (3.3476)	LR 1.000e-03
0: TRAIN [1][3700/3880]	Time 0.191 (0.182)	Data 8.18e-05 (2.85e-04)	Tok/s 43383 (38578)	Loss/tok 3.4023 (3.3475)	LR 1.000e-03
0: TRAIN [1][3710/3880]	Time 0.221 (0.182)	Data 9.35e-05 (2.84e-04)	Tok/s 52767 (38565)	Loss/tok 3.3516 (3.3471)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][3720/3880]	Time 0.162 (0.182)	Data 9.16e-05 (2.83e-04)	Tok/s 32479 (38574)	Loss/tok 3.0081 (3.3471)	LR 1.000e-03
0: TRAIN [1][3730/3880]	Time 0.221 (0.182)	Data 9.44e-05 (2.83e-04)	Tok/s 53020 (38558)	Loss/tok 3.3418 (3.3468)	LR 1.000e-03
0: TRAIN [1][3740/3880]	Time 0.221 (0.182)	Data 8.23e-05 (2.82e-04)	Tok/s 52987 (38555)	Loss/tok 3.3506 (3.3465)	LR 1.000e-03
0: TRAIN [1][3750/3880]	Time 0.136 (0.182)	Data 1.00e-04 (2.82e-04)	Tok/s 18616 (38548)	Loss/tok 2.6473 (3.3463)	LR 1.000e-03
0: TRAIN [1][3760/3880]	Time 0.191 (0.182)	Data 8.08e-05 (2.81e-04)	Tok/s 44288 (38550)	Loss/tok 3.2801 (3.3461)	LR 1.000e-03
0: TRAIN [1][3770/3880]	Time 0.191 (0.182)	Data 1.09e-04 (2.81e-04)	Tok/s 43635 (38563)	Loss/tok 3.3047 (3.3459)	LR 1.000e-03
0: TRAIN [1][3780/3880]	Time 0.162 (0.182)	Data 1.10e-04 (2.80e-04)	Tok/s 31316 (38558)	Loss/tok 3.0767 (3.3455)	LR 1.000e-03
0: TRAIN [1][3790/3880]	Time 0.163 (0.182)	Data 1.03e-04 (2.80e-04)	Tok/s 31280 (38560)	Loss/tok 3.0885 (3.3453)	LR 1.000e-03
0: TRAIN [1][3800/3880]	Time 0.191 (0.182)	Data 9.78e-05 (2.79e-04)	Tok/s 44626 (38551)	Loss/tok 3.3666 (3.3449)	LR 1.000e-03
0: TRAIN [1][3810/3880]	Time 0.137 (0.182)	Data 1.14e-04 (2.79e-04)	Tok/s 19287 (38559)	Loss/tok 2.6329 (3.3448)	LR 1.000e-03
0: TRAIN [1][3820/3880]	Time 0.257 (0.182)	Data 1.62e-04 (2.79e-04)	Tok/s 58473 (38561)	Loss/tok 3.5996 (3.3449)	LR 1.000e-03
0: TRAIN [1][3830/3880]	Time 0.162 (0.182)	Data 8.37e-05 (2.78e-04)	Tok/s 31756 (38570)	Loss/tok 3.1609 (3.3449)	LR 1.000e-03
0: TRAIN [1][3840/3880]	Time 0.191 (0.182)	Data 8.49e-05 (2.78e-04)	Tok/s 43664 (38569)	Loss/tok 3.2718 (3.3447)	LR 1.000e-03
0: TRAIN [1][3850/3880]	Time 0.221 (0.182)	Data 1.01e-04 (2.77e-04)	Tok/s 52479 (38583)	Loss/tok 3.5587 (3.3447)	LR 1.000e-03
0: TRAIN [1][3860/3880]	Time 0.191 (0.182)	Data 8.20e-05 (2.77e-04)	Tok/s 43680 (38583)	Loss/tok 3.3465 (3.3444)	LR 1.000e-03
0: TRAIN [1][3870/3880]	Time 0.162 (0.182)	Data 8.46e-05 (2.76e-04)	Tok/s 30600 (38575)	Loss/tok 3.1351 (3.3440)	LR 1.000e-03
:::MLL 1571260251.818 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1571260251.818 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.672 (0.672)	Decoder iters 101.0 (101.0)	Tok/s 24510 (24510)
0: Running moses detokenizer
0: BLEU(score=22.99350627204071, counts=[36693, 18051, 10115, 5909], totals=[65941, 62938, 59935, 56937], precisions=[55.64519798001243, 28.680606310972703, 16.876616334362225, 10.378137239404957], bp=1.0, sys_len=65941, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1571260253.696 eval_accuracy: {"value": 22.99, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1571260253.696 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.3477	Test BLEU: 22.99
0: Performance: Epoch: 1	Training: 308514 Tok/s
0: Finished epoch 1
:::MLL 1571260253.697 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1571260253.697 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1571260253.697 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 590158374
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][0/3880]	Time 0.904 (0.904)	Data 6.93e-01 (6.93e-01)	Tok/s 5750 (5750)	Loss/tok 2.9087 (2.9087)	LR 1.000e-03
0: TRAIN [2][10/3880]	Time 0.259 (0.273)	Data 1.31e-04 (6.31e-02)	Tok/s 58572 (42918)	Loss/tok 3.4166 (3.2238)	LR 1.000e-03
0: TRAIN [2][20/3880]	Time 0.162 (0.230)	Data 1.09e-04 (3.31e-02)	Tok/s 30429 (41344)	Loss/tok 3.1953 (3.1920)	LR 1.000e-03
0: TRAIN [2][30/3880]	Time 0.162 (0.215)	Data 1.06e-04 (2.25e-02)	Tok/s 32154 (40734)	Loss/tok 2.9996 (3.1959)	LR 1.000e-03
0: TRAIN [2][40/3880]	Time 0.136 (0.206)	Data 1.58e-04 (1.70e-02)	Tok/s 20364 (39410)	Loss/tok 2.6688 (3.1849)	LR 1.000e-03
0: TRAIN [2][50/3880]	Time 0.191 (0.202)	Data 9.49e-05 (1.37e-02)	Tok/s 44258 (39750)	Loss/tok 3.1995 (3.1771)	LR 1.000e-03
0: TRAIN [2][60/3880]	Time 0.220 (0.198)	Data 1.29e-04 (1.15e-02)	Tok/s 51577 (39579)	Loss/tok 3.4934 (3.1740)	LR 1.000e-03
0: TRAIN [2][70/3880]	Time 0.162 (0.195)	Data 1.02e-04 (9.88e-03)	Tok/s 31913 (39253)	Loss/tok 2.8779 (3.1733)	LR 1.000e-03
0: TRAIN [2][80/3880]	Time 0.162 (0.194)	Data 9.97e-05 (8.68e-03)	Tok/s 31013 (39131)	Loss/tok 2.9656 (3.1800)	LR 1.000e-03
0: TRAIN [2][90/3880]	Time 0.221 (0.190)	Data 9.25e-05 (7.74e-03)	Tok/s 52128 (38126)	Loss/tok 3.4445 (3.1675)	LR 1.000e-03
0: TRAIN [2][100/3880]	Time 0.191 (0.190)	Data 1.09e-04 (6.99e-03)	Tok/s 44606 (38391)	Loss/tok 3.1388 (3.1701)	LR 1.000e-03
0: TRAIN [2][110/3880]	Time 0.162 (0.189)	Data 8.94e-05 (6.37e-03)	Tok/s 32126 (38288)	Loss/tok 3.0032 (3.1693)	LR 1.000e-03
0: TRAIN [2][120/3880]	Time 0.162 (0.188)	Data 1.90e-04 (5.86e-03)	Tok/s 32022 (38042)	Loss/tok 3.2997 (3.1735)	LR 1.000e-03
0: TRAIN [2][130/3880]	Time 0.163 (0.188)	Data 9.47e-05 (5.42e-03)	Tok/s 32684 (38189)	Loss/tok 2.8772 (3.1763)	LR 1.000e-03
0: TRAIN [2][140/3880]	Time 0.191 (0.189)	Data 1.24e-04 (5.04e-03)	Tok/s 43980 (38645)	Loss/tok 3.2785 (3.1879)	LR 1.000e-03
0: TRAIN [2][150/3880]	Time 0.191 (0.188)	Data 1.06e-04 (4.71e-03)	Tok/s 44780 (38642)	Loss/tok 3.2170 (3.1832)	LR 1.000e-03
0: TRAIN [2][160/3880]	Time 0.162 (0.188)	Data 1.24e-04 (4.43e-03)	Tok/s 32948 (38634)	Loss/tok 2.9229 (3.1803)	LR 1.000e-03
0: TRAIN [2][170/3880]	Time 0.191 (0.187)	Data 1.29e-04 (4.18e-03)	Tok/s 44351 (38669)	Loss/tok 3.0381 (3.1800)	LR 1.000e-03
0: TRAIN [2][180/3880]	Time 0.163 (0.187)	Data 1.03e-04 (3.96e-03)	Tok/s 31841 (38412)	Loss/tok 2.8788 (3.1800)	LR 1.000e-03
0: TRAIN [2][190/3880]	Time 0.191 (0.187)	Data 1.02e-04 (3.75e-03)	Tok/s 43871 (38457)	Loss/tok 2.9798 (3.1829)	LR 1.000e-03
0: TRAIN [2][200/3880]	Time 0.162 (0.186)	Data 1.14e-04 (3.57e-03)	Tok/s 31802 (38445)	Loss/tok 3.1300 (3.1813)	LR 1.000e-03
0: TRAIN [2][210/3880]	Time 0.162 (0.186)	Data 1.05e-04 (3.41e-03)	Tok/s 31922 (38374)	Loss/tok 3.0596 (3.1817)	LR 1.000e-03
0: TRAIN [2][220/3880]	Time 0.162 (0.186)	Data 5.90e-04 (3.26e-03)	Tok/s 32175 (38380)	Loss/tok 3.0858 (3.1796)	LR 1.000e-03
0: TRAIN [2][230/3880]	Time 0.162 (0.185)	Data 9.01e-05 (3.13e-03)	Tok/s 31968 (38295)	Loss/tok 2.8727 (3.1751)	LR 1.000e-03
0: TRAIN [2][240/3880]	Time 0.191 (0.186)	Data 1.13e-04 (3.00e-03)	Tok/s 44994 (38588)	Loss/tok 3.2145 (3.1784)	LR 1.000e-03
0: TRAIN [2][250/3880]	Time 0.221 (0.186)	Data 1.10e-04 (2.89e-03)	Tok/s 53517 (38630)	Loss/tok 3.1959 (3.1772)	LR 1.000e-03
0: TRAIN [2][260/3880]	Time 0.162 (0.185)	Data 1.03e-04 (2.78e-03)	Tok/s 32358 (38670)	Loss/tok 2.8895 (3.1785)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][270/3880]	Time 0.221 (0.186)	Data 1.16e-04 (2.68e-03)	Tok/s 52876 (38743)	Loss/tok 3.4610 (3.1821)	LR 1.000e-03
0: TRAIN [2][280/3880]	Time 0.221 (0.186)	Data 1.07e-04 (2.59e-03)	Tok/s 52905 (38763)	Loss/tok 3.3628 (3.1821)	LR 1.000e-03
0: TRAIN [2][290/3880]	Time 0.220 (0.186)	Data 1.22e-04 (2.51e-03)	Tok/s 53004 (38975)	Loss/tok 3.3554 (3.1860)	LR 1.000e-03
0: TRAIN [2][300/3880]	Time 0.191 (0.187)	Data 1.07e-04 (2.43e-03)	Tok/s 42887 (39209)	Loss/tok 3.2347 (3.1918)	LR 1.000e-03
0: TRAIN [2][310/3880]	Time 0.162 (0.186)	Data 1.38e-04 (2.35e-03)	Tok/s 32361 (39082)	Loss/tok 3.0653 (3.1884)	LR 1.000e-03
0: TRAIN [2][320/3880]	Time 0.162 (0.186)	Data 9.23e-05 (2.28e-03)	Tok/s 31744 (39128)	Loss/tok 2.9056 (3.1875)	LR 1.000e-03
0: TRAIN [2][330/3880]	Time 0.222 (0.186)	Data 1.16e-04 (2.22e-03)	Tok/s 53186 (39124)	Loss/tok 3.5408 (3.1879)	LR 1.000e-03
0: TRAIN [2][340/3880]	Time 0.191 (0.186)	Data 1.20e-04 (2.16e-03)	Tok/s 43571 (39045)	Loss/tok 3.3103 (3.1858)	LR 5.000e-04
0: TRAIN [2][350/3880]	Time 0.256 (0.186)	Data 1.14e-04 (2.10e-03)	Tok/s 58340 (39041)	Loss/tok 3.4516 (3.1864)	LR 5.000e-04
0: TRAIN [2][360/3880]	Time 0.222 (0.186)	Data 1.13e-04 (2.05e-03)	Tok/s 53600 (39042)	Loss/tok 3.3545 (3.1877)	LR 5.000e-04
0: TRAIN [2][370/3880]	Time 0.162 (0.186)	Data 1.32e-04 (1.99e-03)	Tok/s 32234 (39088)	Loss/tok 2.9581 (3.1891)	LR 5.000e-04
0: TRAIN [2][380/3880]	Time 0.257 (0.185)	Data 3.11e-04 (1.95e-03)	Tok/s 57709 (38995)	Loss/tok 3.6671 (3.1895)	LR 5.000e-04
0: TRAIN [2][390/3880]	Time 0.162 (0.185)	Data 1.02e-04 (1.90e-03)	Tok/s 31471 (38958)	Loss/tok 2.9165 (3.1886)	LR 5.000e-04
0: TRAIN [2][400/3880]	Time 0.136 (0.185)	Data 1.21e-04 (1.86e-03)	Tok/s 19498 (38934)	Loss/tok 2.6148 (3.1878)	LR 5.000e-04
0: TRAIN [2][410/3880]	Time 0.221 (0.185)	Data 9.30e-05 (1.81e-03)	Tok/s 52722 (38962)	Loss/tok 3.1776 (3.1877)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][420/3880]	Time 0.222 (0.185)	Data 6.28e-04 (1.78e-03)	Tok/s 53409 (39028)	Loss/tok 3.3692 (3.1921)	LR 5.000e-04
0: TRAIN [2][430/3880]	Time 0.191 (0.185)	Data 9.08e-05 (1.74e-03)	Tok/s 43055 (38974)	Loss/tok 3.2276 (3.1944)	LR 5.000e-04
0: TRAIN [2][440/3880]	Time 0.162 (0.185)	Data 1.32e-04 (1.70e-03)	Tok/s 31559 (39027)	Loss/tok 3.1165 (3.1951)	LR 5.000e-04
0: TRAIN [2][450/3880]	Time 0.163 (0.185)	Data 9.54e-05 (1.67e-03)	Tok/s 31601 (38907)	Loss/tok 3.1211 (3.1937)	LR 5.000e-04
0: TRAIN [2][460/3880]	Time 0.191 (0.185)	Data 1.20e-04 (1.63e-03)	Tok/s 44427 (39014)	Loss/tok 3.0180 (3.1934)	LR 5.000e-04
0: TRAIN [2][470/3880]	Time 0.191 (0.185)	Data 8.92e-05 (1.60e-03)	Tok/s 44003 (39008)	Loss/tok 3.1955 (3.1943)	LR 5.000e-04
0: TRAIN [2][480/3880]	Time 0.191 (0.185)	Data 1.37e-04 (1.57e-03)	Tok/s 44577 (39054)	Loss/tok 3.1222 (3.1933)	LR 5.000e-04
0: TRAIN [2][490/3880]	Time 0.191 (0.185)	Data 9.11e-05 (1.54e-03)	Tok/s 43459 (38942)	Loss/tok 3.0867 (3.1907)	LR 5.000e-04
0: TRAIN [2][500/3880]	Time 0.191 (0.185)	Data 9.32e-05 (1.51e-03)	Tok/s 43877 (39026)	Loss/tok 3.3306 (3.1912)	LR 5.000e-04
0: TRAIN [2][510/3880]	Time 0.162 (0.185)	Data 1.30e-04 (1.48e-03)	Tok/s 31025 (38962)	Loss/tok 2.9059 (3.1906)	LR 5.000e-04
0: TRAIN [2][520/3880]	Time 0.162 (0.185)	Data 1.22e-04 (1.46e-03)	Tok/s 31468 (38979)	Loss/tok 3.0051 (3.1895)	LR 5.000e-04
0: TRAIN [2][530/3880]	Time 0.162 (0.185)	Data 1.02e-04 (1.43e-03)	Tok/s 30946 (38939)	Loss/tok 2.8891 (3.1899)	LR 5.000e-04
0: TRAIN [2][540/3880]	Time 0.191 (0.185)	Data 1.35e-04 (1.41e-03)	Tok/s 44084 (38984)	Loss/tok 3.1302 (3.1897)	LR 5.000e-04
0: TRAIN [2][550/3880]	Time 0.162 (0.185)	Data 1.07e-04 (1.38e-03)	Tok/s 31512 (39040)	Loss/tok 2.9391 (3.1887)	LR 5.000e-04
0: TRAIN [2][560/3880]	Time 0.162 (0.185)	Data 1.09e-04 (1.36e-03)	Tok/s 32215 (39062)	Loss/tok 3.2210 (3.1874)	LR 5.000e-04
0: TRAIN [2][570/3880]	Time 0.162 (0.185)	Data 1.02e-04 (1.34e-03)	Tok/s 31345 (39078)	Loss/tok 2.9313 (3.1858)	LR 5.000e-04
0: TRAIN [2][580/3880]	Time 0.221 (0.185)	Data 8.87e-05 (1.32e-03)	Tok/s 52197 (39143)	Loss/tok 3.2263 (3.1869)	LR 5.000e-04
0: TRAIN [2][590/3880]	Time 0.256 (0.185)	Data 9.08e-05 (1.30e-03)	Tok/s 58815 (39123)	Loss/tok 3.3836 (3.1861)	LR 5.000e-04
0: TRAIN [2][600/3880]	Time 0.162 (0.185)	Data 1.10e-04 (1.28e-03)	Tok/s 32666 (39111)	Loss/tok 2.9727 (3.1856)	LR 5.000e-04
0: TRAIN [2][610/3880]	Time 0.221 (0.185)	Data 9.08e-05 (1.26e-03)	Tok/s 52835 (39128)	Loss/tok 3.3789 (3.1853)	LR 5.000e-04
0: TRAIN [2][620/3880]	Time 0.162 (0.185)	Data 1.14e-04 (1.24e-03)	Tok/s 31993 (39142)	Loss/tok 3.0383 (3.1853)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][630/3880]	Time 0.191 (0.185)	Data 1.24e-04 (1.22e-03)	Tok/s 43722 (39086)	Loss/tok 3.2647 (3.1851)	LR 5.000e-04
0: TRAIN [2][640/3880]	Time 0.191 (0.185)	Data 1.17e-04 (1.21e-03)	Tok/s 43886 (39097)	Loss/tok 3.1476 (3.1841)	LR 5.000e-04
0: TRAIN [2][650/3880]	Time 0.162 (0.185)	Data 1.09e-04 (1.19e-03)	Tok/s 31805 (39037)	Loss/tok 2.9236 (3.1831)	LR 5.000e-04
0: TRAIN [2][660/3880]	Time 0.191 (0.185)	Data 1.78e-04 (1.17e-03)	Tok/s 44166 (39174)	Loss/tok 2.9506 (3.1841)	LR 5.000e-04
0: TRAIN [2][670/3880]	Time 0.162 (0.185)	Data 1.01e-04 (1.16e-03)	Tok/s 32908 (39103)	Loss/tok 2.8819 (3.1823)	LR 5.000e-04
0: TRAIN [2][680/3880]	Time 0.191 (0.184)	Data 8.80e-05 (1.14e-03)	Tok/s 43820 (39085)	Loss/tok 3.2271 (3.1810)	LR 5.000e-04
0: TRAIN [2][690/3880]	Time 0.137 (0.185)	Data 9.78e-05 (1.13e-03)	Tok/s 19833 (39114)	Loss/tok 2.5777 (3.1831)	LR 5.000e-04
0: TRAIN [2][700/3880]	Time 0.257 (0.185)	Data 1.05e-04 (1.11e-03)	Tok/s 57774 (39165)	Loss/tok 3.5527 (3.1848)	LR 5.000e-04
0: TRAIN [2][710/3880]	Time 0.221 (0.185)	Data 1.08e-04 (1.10e-03)	Tok/s 53280 (39215)	Loss/tok 3.1755 (3.1855)	LR 5.000e-04
0: TRAIN [2][720/3880]	Time 0.221 (0.185)	Data 1.04e-04 (1.08e-03)	Tok/s 52825 (39210)	Loss/tok 3.2834 (3.1854)	LR 5.000e-04
0: TRAIN [2][730/3880]	Time 0.162 (0.185)	Data 1.08e-04 (1.07e-03)	Tok/s 30567 (39199)	Loss/tok 3.1024 (3.1849)	LR 5.000e-04
0: TRAIN [2][740/3880]	Time 0.162 (0.185)	Data 1.20e-04 (1.06e-03)	Tok/s 32260 (39190)	Loss/tok 3.0696 (3.1843)	LR 5.000e-04
0: TRAIN [2][750/3880]	Time 0.221 (0.185)	Data 1.00e-04 (1.05e-03)	Tok/s 52731 (39182)	Loss/tok 3.2959 (3.1833)	LR 5.000e-04
0: TRAIN [2][760/3880]	Time 0.192 (0.185)	Data 1.05e-04 (1.03e-03)	Tok/s 43847 (39205)	Loss/tok 3.1820 (3.1831)	LR 5.000e-04
0: TRAIN [2][770/3880]	Time 0.136 (0.185)	Data 1.02e-04 (1.02e-03)	Tok/s 19650 (39146)	Loss/tok 2.4919 (3.1816)	LR 5.000e-04
0: TRAIN [2][780/3880]	Time 0.191 (0.184)	Data 1.80e-04 (1.01e-03)	Tok/s 43482 (39127)	Loss/tok 3.1216 (3.1800)	LR 5.000e-04
0: TRAIN [2][790/3880]	Time 0.162 (0.184)	Data 1.05e-04 (9.98e-04)	Tok/s 32716 (39098)	Loss/tok 3.0022 (3.1794)	LR 5.000e-04
0: TRAIN [2][800/3880]	Time 0.162 (0.184)	Data 1.17e-04 (9.87e-04)	Tok/s 31769 (39040)	Loss/tok 2.8819 (3.1786)	LR 5.000e-04
0: TRAIN [2][810/3880]	Time 0.136 (0.184)	Data 1.07e-04 (9.77e-04)	Tok/s 18552 (39020)	Loss/tok 2.5154 (3.1778)	LR 5.000e-04
0: TRAIN [2][820/3880]	Time 0.191 (0.184)	Data 1.27e-04 (9.66e-04)	Tok/s 43853 (39038)	Loss/tok 3.2621 (3.1783)	LR 5.000e-04
0: TRAIN [2][830/3880]	Time 0.191 (0.184)	Data 1.20e-04 (9.56e-04)	Tok/s 43792 (39059)	Loss/tok 3.1023 (3.1780)	LR 5.000e-04
0: TRAIN [2][840/3880]	Time 0.191 (0.184)	Data 1.20e-04 (9.46e-04)	Tok/s 44441 (38990)	Loss/tok 3.0152 (3.1763)	LR 5.000e-04
0: TRAIN [2][850/3880]	Time 0.162 (0.184)	Data 1.14e-04 (9.36e-04)	Tok/s 31979 (39007)	Loss/tok 3.0239 (3.1771)	LR 5.000e-04
0: TRAIN [2][860/3880]	Time 0.162 (0.184)	Data 1.21e-04 (9.27e-04)	Tok/s 31372 (39095)	Loss/tok 2.9547 (3.1780)	LR 5.000e-04
0: TRAIN [2][870/3880]	Time 0.162 (0.184)	Data 1.03e-04 (9.18e-04)	Tok/s 32235 (39049)	Loss/tok 2.9401 (3.1771)	LR 5.000e-04
0: TRAIN [2][880/3880]	Time 0.162 (0.184)	Data 9.37e-05 (9.09e-04)	Tok/s 31939 (39020)	Loss/tok 2.9389 (3.1763)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][890/3880]	Time 0.221 (0.184)	Data 1.31e-04 (9.00e-04)	Tok/s 52601 (39027)	Loss/tok 3.3301 (3.1758)	LR 5.000e-04
0: TRAIN [2][900/3880]	Time 0.162 (0.184)	Data 1.38e-04 (8.92e-04)	Tok/s 32269 (39008)	Loss/tok 2.9657 (3.1753)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][910/3880]	Time 0.163 (0.184)	Data 9.68e-05 (8.83e-04)	Tok/s 31159 (39036)	Loss/tok 3.0110 (3.1758)	LR 5.000e-04
0: TRAIN [2][920/3880]	Time 0.257 (0.184)	Data 1.18e-04 (8.75e-04)	Tok/s 57072 (39075)	Loss/tok 3.5809 (3.1763)	LR 5.000e-04
0: TRAIN [2][930/3880]	Time 0.163 (0.184)	Data 9.51e-05 (8.66e-04)	Tok/s 31769 (39064)	Loss/tok 2.9980 (3.1764)	LR 5.000e-04
0: TRAIN [2][940/3880]	Time 0.162 (0.184)	Data 1.02e-04 (8.58e-04)	Tok/s 32033 (39038)	Loss/tok 3.0119 (3.1755)	LR 5.000e-04
0: TRAIN [2][950/3880]	Time 0.191 (0.184)	Data 9.08e-05 (8.50e-04)	Tok/s 43594 (39073)	Loss/tok 3.2263 (3.1758)	LR 5.000e-04
0: TRAIN [2][960/3880]	Time 0.221 (0.184)	Data 1.09e-04 (8.42e-04)	Tok/s 52800 (39076)	Loss/tok 3.5280 (3.1767)	LR 5.000e-04
0: TRAIN [2][970/3880]	Time 0.191 (0.184)	Data 9.32e-05 (8.35e-04)	Tok/s 43937 (39040)	Loss/tok 3.3312 (3.1758)	LR 5.000e-04
0: TRAIN [2][980/3880]	Time 0.221 (0.184)	Data 9.11e-05 (8.28e-04)	Tok/s 53560 (39076)	Loss/tok 3.2210 (3.1760)	LR 5.000e-04
0: TRAIN [2][990/3880]	Time 0.191 (0.184)	Data 9.70e-05 (8.20e-04)	Tok/s 43197 (39027)	Loss/tok 3.3519 (3.1750)	LR 5.000e-04
0: TRAIN [2][1000/3880]	Time 0.192 (0.184)	Data 1.01e-04 (8.13e-04)	Tok/s 42999 (39049)	Loss/tok 3.1651 (3.1752)	LR 5.000e-04
0: TRAIN [2][1010/3880]	Time 0.163 (0.184)	Data 1.03e-04 (8.06e-04)	Tok/s 31692 (39008)	Loss/tok 3.0398 (3.1744)	LR 5.000e-04
0: TRAIN [2][1020/3880]	Time 0.162 (0.184)	Data 9.56e-05 (7.99e-04)	Tok/s 32148 (38968)	Loss/tok 3.1042 (3.1733)	LR 5.000e-04
0: TRAIN [2][1030/3880]	Time 0.191 (0.184)	Data 1.18e-04 (7.93e-04)	Tok/s 44219 (38969)	Loss/tok 3.1075 (3.1729)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1040/3880]	Time 0.137 (0.184)	Data 1.21e-04 (7.86e-04)	Tok/s 19595 (38950)	Loss/tok 2.6128 (3.1735)	LR 5.000e-04
0: TRAIN [2][1050/3880]	Time 0.257 (0.184)	Data 1.12e-04 (7.80e-04)	Tok/s 57211 (38996)	Loss/tok 3.5776 (3.1747)	LR 5.000e-04
0: TRAIN [2][1060/3880]	Time 0.191 (0.184)	Data 1.13e-04 (7.74e-04)	Tok/s 43985 (39000)	Loss/tok 3.1595 (3.1747)	LR 5.000e-04
0: TRAIN [2][1070/3880]	Time 0.162 (0.184)	Data 1.29e-04 (7.67e-04)	Tok/s 31801 (39007)	Loss/tok 2.8469 (3.1740)	LR 5.000e-04
0: TRAIN [2][1080/3880]	Time 0.163 (0.184)	Data 1.25e-04 (7.61e-04)	Tok/s 31178 (39021)	Loss/tok 2.9575 (3.1743)	LR 5.000e-04
0: TRAIN [2][1090/3880]	Time 0.136 (0.184)	Data 1.17e-04 (7.55e-04)	Tok/s 19431 (39026)	Loss/tok 2.5534 (3.1739)	LR 5.000e-04
0: TRAIN [2][1100/3880]	Time 0.191 (0.184)	Data 1.10e-04 (7.50e-04)	Tok/s 44601 (39034)	Loss/tok 3.0785 (3.1736)	LR 5.000e-04
0: TRAIN [2][1110/3880]	Time 0.162 (0.184)	Data 1.20e-04 (7.44e-04)	Tok/s 32004 (39062)	Loss/tok 2.9630 (3.1740)	LR 5.000e-04
0: TRAIN [2][1120/3880]	Time 0.162 (0.184)	Data 9.97e-05 (7.38e-04)	Tok/s 32558 (39047)	Loss/tok 3.0529 (3.1735)	LR 5.000e-04
0: TRAIN [2][1130/3880]	Time 0.162 (0.184)	Data 1.15e-04 (7.33e-04)	Tok/s 31554 (39008)	Loss/tok 2.9593 (3.1728)	LR 5.000e-04
0: TRAIN [2][1140/3880]	Time 0.163 (0.184)	Data 3.83e-04 (7.27e-04)	Tok/s 32586 (38995)	Loss/tok 2.8567 (3.1725)	LR 5.000e-04
0: TRAIN [2][1150/3880]	Time 0.162 (0.184)	Data 9.61e-05 (7.22e-04)	Tok/s 31026 (38993)	Loss/tok 2.9196 (3.1721)	LR 5.000e-04
0: TRAIN [2][1160/3880]	Time 0.162 (0.184)	Data 8.70e-05 (7.16e-04)	Tok/s 31979 (38969)	Loss/tok 2.9798 (3.1716)	LR 5.000e-04
0: TRAIN [2][1170/3880]	Time 0.191 (0.184)	Data 2.22e-04 (7.12e-04)	Tok/s 42808 (38978)	Loss/tok 3.2125 (3.1715)	LR 5.000e-04
0: TRAIN [2][1180/3880]	Time 0.162 (0.184)	Data 1.15e-04 (7.06e-04)	Tok/s 32345 (39008)	Loss/tok 2.9743 (3.1718)	LR 5.000e-04
0: TRAIN [2][1190/3880]	Time 0.136 (0.184)	Data 1.18e-04 (7.01e-04)	Tok/s 18850 (39015)	Loss/tok 2.5484 (3.1714)	LR 5.000e-04
0: TRAIN [2][1200/3880]	Time 0.191 (0.183)	Data 1.17e-04 (6.97e-04)	Tok/s 43746 (38993)	Loss/tok 3.2250 (3.1706)	LR 5.000e-04
0: TRAIN [2][1210/3880]	Time 0.191 (0.184)	Data 1.03e-04 (6.92e-04)	Tok/s 44038 (39010)	Loss/tok 3.2361 (3.1707)	LR 5.000e-04
0: TRAIN [2][1220/3880]	Time 0.191 (0.184)	Data 3.72e-04 (6.87e-04)	Tok/s 43404 (39005)	Loss/tok 3.1542 (3.1709)	LR 5.000e-04
0: TRAIN [2][1230/3880]	Time 0.221 (0.184)	Data 1.12e-04 (6.83e-04)	Tok/s 52539 (39010)	Loss/tok 3.3233 (3.1702)	LR 5.000e-04
0: TRAIN [2][1240/3880]	Time 0.191 (0.184)	Data 1.16e-04 (6.78e-04)	Tok/s 44103 (39010)	Loss/tok 3.1573 (3.1696)	LR 5.000e-04
0: TRAIN [2][1250/3880]	Time 0.191 (0.183)	Data 1.06e-04 (6.74e-04)	Tok/s 44431 (38989)	Loss/tok 3.0112 (3.1688)	LR 5.000e-04
0: TRAIN [2][1260/3880]	Time 0.191 (0.183)	Data 3.41e-04 (6.69e-04)	Tok/s 44167 (38985)	Loss/tok 3.2071 (3.1693)	LR 5.000e-04
0: TRAIN [2][1270/3880]	Time 0.191 (0.183)	Data 1.06e-04 (6.65e-04)	Tok/s 43843 (38936)	Loss/tok 3.0542 (3.1683)	LR 5.000e-04
0: TRAIN [2][1280/3880]	Time 0.162 (0.183)	Data 1.34e-04 (6.61e-04)	Tok/s 31596 (38919)	Loss/tok 2.9736 (3.1679)	LR 5.000e-04
0: TRAIN [2][1290/3880]	Time 0.162 (0.183)	Data 9.42e-05 (6.57e-04)	Tok/s 31718 (38894)	Loss/tok 2.8169 (3.1671)	LR 5.000e-04
0: TRAIN [2][1300/3880]	Time 0.191 (0.183)	Data 9.68e-05 (6.52e-04)	Tok/s 43552 (38916)	Loss/tok 3.1226 (3.1666)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1310/3880]	Time 0.221 (0.183)	Data 1.07e-04 (6.49e-04)	Tok/s 54334 (38954)	Loss/tok 3.3212 (3.1678)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1320/3880]	Time 0.162 (0.183)	Data 1.09e-04 (6.45e-04)	Tok/s 31145 (38956)	Loss/tok 3.0561 (3.1680)	LR 5.000e-04
0: TRAIN [2][1330/3880]	Time 0.191 (0.183)	Data 1.06e-04 (6.41e-04)	Tok/s 43254 (38968)	Loss/tok 3.1690 (3.1677)	LR 5.000e-04
0: TRAIN [2][1340/3880]	Time 0.221 (0.183)	Data 1.16e-04 (6.37e-04)	Tok/s 53875 (38960)	Loss/tok 3.2869 (3.1672)	LR 5.000e-04
0: TRAIN [2][1350/3880]	Time 0.137 (0.183)	Data 1.26e-04 (6.33e-04)	Tok/s 19332 (38944)	Loss/tok 2.6682 (3.1671)	LR 5.000e-04
0: TRAIN [2][1360/3880]	Time 0.221 (0.183)	Data 1.12e-04 (6.29e-04)	Tok/s 54021 (38949)	Loss/tok 3.2330 (3.1670)	LR 5.000e-04
0: TRAIN [2][1370/3880]	Time 0.162 (0.183)	Data 9.97e-05 (6.25e-04)	Tok/s 31237 (38961)	Loss/tok 2.9600 (3.1673)	LR 5.000e-04
0: TRAIN [2][1380/3880]	Time 0.191 (0.183)	Data 9.56e-05 (6.22e-04)	Tok/s 43298 (38945)	Loss/tok 3.1416 (3.1672)	LR 5.000e-04
0: TRAIN [2][1390/3880]	Time 0.191 (0.183)	Data 1.86e-04 (6.18e-04)	Tok/s 44422 (38984)	Loss/tok 3.0347 (3.1679)	LR 5.000e-04
0: TRAIN [2][1400/3880]	Time 0.136 (0.183)	Data 1.09e-04 (6.14e-04)	Tok/s 19334 (38942)	Loss/tok 2.5437 (3.1669)	LR 5.000e-04
0: TRAIN [2][1410/3880]	Time 0.137 (0.183)	Data 1.02e-04 (6.11e-04)	Tok/s 19163 (38942)	Loss/tok 2.5967 (3.1670)	LR 5.000e-04
0: TRAIN [2][1420/3880]	Time 0.222 (0.183)	Data 1.03e-04 (6.07e-04)	Tok/s 53092 (38948)	Loss/tok 3.4910 (3.1669)	LR 5.000e-04
0: TRAIN [2][1430/3880]	Time 0.162 (0.183)	Data 9.37e-05 (6.04e-04)	Tok/s 31084 (38956)	Loss/tok 2.8740 (3.1669)	LR 5.000e-04
0: TRAIN [2][1440/3880]	Time 0.137 (0.183)	Data 8.96e-05 (6.00e-04)	Tok/s 19286 (38951)	Loss/tok 2.4784 (3.1671)	LR 5.000e-04
0: TRAIN [2][1450/3880]	Time 0.191 (0.183)	Data 1.28e-04 (5.97e-04)	Tok/s 44312 (38978)	Loss/tok 3.1119 (3.1671)	LR 5.000e-04
0: TRAIN [2][1460/3880]	Time 0.191 (0.183)	Data 1.24e-04 (5.94e-04)	Tok/s 42889 (38932)	Loss/tok 3.2045 (3.1665)	LR 5.000e-04
0: TRAIN [2][1470/3880]	Time 0.221 (0.183)	Data 1.29e-04 (5.90e-04)	Tok/s 52806 (38943)	Loss/tok 3.4065 (3.1664)	LR 5.000e-04
0: TRAIN [2][1480/3880]	Time 0.136 (0.183)	Data 1.23e-04 (5.87e-04)	Tok/s 19640 (38913)	Loss/tok 2.5641 (3.1661)	LR 5.000e-04
0: TRAIN [2][1490/3880]	Time 0.220 (0.183)	Data 1.31e-04 (5.84e-04)	Tok/s 52191 (38914)	Loss/tok 3.2115 (3.1659)	LR 5.000e-04
0: TRAIN [2][1500/3880]	Time 0.221 (0.183)	Data 1.87e-04 (5.81e-04)	Tok/s 52875 (38890)	Loss/tok 3.3000 (3.1654)	LR 5.000e-04
0: TRAIN [2][1510/3880]	Time 0.221 (0.183)	Data 9.61e-05 (5.78e-04)	Tok/s 52183 (38896)	Loss/tok 3.4191 (3.1652)	LR 5.000e-04
0: TRAIN [2][1520/3880]	Time 0.162 (0.183)	Data 1.23e-04 (5.75e-04)	Tok/s 31985 (38863)	Loss/tok 2.9878 (3.1646)	LR 5.000e-04
0: TRAIN [2][1530/3880]	Time 0.191 (0.183)	Data 1.11e-04 (5.72e-04)	Tok/s 44258 (38891)	Loss/tok 3.1697 (3.1649)	LR 5.000e-04
0: TRAIN [2][1540/3880]	Time 0.191 (0.183)	Data 4.06e-04 (5.69e-04)	Tok/s 43382 (38898)	Loss/tok 3.0961 (3.1650)	LR 5.000e-04
0: TRAIN [2][1550/3880]	Time 0.162 (0.183)	Data 1.18e-04 (5.66e-04)	Tok/s 31676 (38867)	Loss/tok 3.0386 (3.1648)	LR 5.000e-04
0: TRAIN [2][1560/3880]	Time 0.162 (0.183)	Data 3.34e-04 (5.64e-04)	Tok/s 32317 (38836)	Loss/tok 2.9011 (3.1639)	LR 5.000e-04
0: TRAIN [2][1570/3880]	Time 0.162 (0.183)	Data 1.15e-04 (5.61e-04)	Tok/s 32700 (38816)	Loss/tok 2.9538 (3.1633)	LR 5.000e-04
0: TRAIN [2][1580/3880]	Time 0.191 (0.183)	Data 1.23e-04 (5.58e-04)	Tok/s 44457 (38825)	Loss/tok 3.0840 (3.1631)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1590/3880]	Time 0.137 (0.183)	Data 9.23e-05 (5.55e-04)	Tok/s 18979 (38810)	Loss/tok 2.5622 (3.1627)	LR 5.000e-04
0: TRAIN [2][1600/3880]	Time 0.220 (0.183)	Data 1.28e-04 (5.53e-04)	Tok/s 53062 (38807)	Loss/tok 3.1913 (3.1623)	LR 5.000e-04
0: TRAIN [2][1610/3880]	Time 0.162 (0.183)	Data 1.17e-04 (5.50e-04)	Tok/s 31858 (38778)	Loss/tok 2.9457 (3.1617)	LR 5.000e-04
0: TRAIN [2][1620/3880]	Time 0.191 (0.183)	Data 1.22e-04 (5.48e-04)	Tok/s 43324 (38787)	Loss/tok 3.0056 (3.1621)	LR 5.000e-04
0: TRAIN [2][1630/3880]	Time 0.163 (0.183)	Data 9.08e-05 (5.45e-04)	Tok/s 31750 (38775)	Loss/tok 2.9144 (3.1619)	LR 5.000e-04
0: TRAIN [2][1640/3880]	Time 0.220 (0.183)	Data 1.33e-04 (5.42e-04)	Tok/s 52923 (38798)	Loss/tok 3.3151 (3.1622)	LR 5.000e-04
0: TRAIN [2][1650/3880]	Time 0.191 (0.183)	Data 1.18e-04 (5.40e-04)	Tok/s 44786 (38798)	Loss/tok 3.0782 (3.1619)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1660/3880]	Time 0.191 (0.183)	Data 1.21e-04 (5.37e-04)	Tok/s 44040 (38800)	Loss/tok 2.9428 (3.1618)	LR 5.000e-04
0: TRAIN [2][1670/3880]	Time 0.191 (0.183)	Data 9.23e-05 (5.35e-04)	Tok/s 43467 (38771)	Loss/tok 3.1744 (3.1612)	LR 5.000e-04
0: TRAIN [2][1680/3880]	Time 0.162 (0.182)	Data 1.28e-04 (5.32e-04)	Tok/s 31136 (38744)	Loss/tok 2.9251 (3.1608)	LR 5.000e-04
0: TRAIN [2][1690/3880]	Time 0.191 (0.183)	Data 1.12e-04 (5.30e-04)	Tok/s 44215 (38758)	Loss/tok 3.1584 (3.1613)	LR 5.000e-04
0: TRAIN [2][1700/3880]	Time 0.192 (0.182)	Data 9.23e-05 (5.27e-04)	Tok/s 43157 (38731)	Loss/tok 3.1616 (3.1608)	LR 5.000e-04
0: TRAIN [2][1710/3880]	Time 0.163 (0.183)	Data 9.99e-05 (5.25e-04)	Tok/s 30884 (38753)	Loss/tok 2.9655 (3.1612)	LR 5.000e-04
0: TRAIN [2][1720/3880]	Time 0.191 (0.182)	Data 2.32e-04 (5.23e-04)	Tok/s 44161 (38740)	Loss/tok 3.2100 (3.1608)	LR 5.000e-04
0: TRAIN [2][1730/3880]	Time 0.191 (0.183)	Data 1.38e-04 (5.21e-04)	Tok/s 43025 (38752)	Loss/tok 3.2278 (3.1604)	LR 5.000e-04
0: TRAIN [2][1740/3880]	Time 0.136 (0.182)	Data 1.45e-04 (5.19e-04)	Tok/s 19307 (38726)	Loss/tok 2.6794 (3.1600)	LR 5.000e-04
0: TRAIN [2][1750/3880]	Time 0.191 (0.183)	Data 1.57e-04 (5.16e-04)	Tok/s 43171 (38773)	Loss/tok 3.2653 (3.1612)	LR 5.000e-04
0: TRAIN [2][1760/3880]	Time 0.221 (0.183)	Data 1.23e-04 (5.14e-04)	Tok/s 52791 (38794)	Loss/tok 3.6035 (3.1621)	LR 5.000e-04
0: TRAIN [2][1770/3880]	Time 0.191 (0.183)	Data 1.04e-04 (5.12e-04)	Tok/s 43873 (38809)	Loss/tok 3.1457 (3.1622)	LR 5.000e-04
0: TRAIN [2][1780/3880]	Time 0.163 (0.183)	Data 2.59e-04 (5.10e-04)	Tok/s 32148 (38850)	Loss/tok 2.9282 (3.1633)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1790/3880]	Time 0.191 (0.183)	Data 1.29e-04 (5.08e-04)	Tok/s 43759 (38856)	Loss/tok 3.2738 (3.1631)	LR 5.000e-04
0: TRAIN [2][1800/3880]	Time 0.191 (0.183)	Data 1.42e-04 (5.06e-04)	Tok/s 44376 (38846)	Loss/tok 3.0862 (3.1627)	LR 5.000e-04
0: TRAIN [2][1810/3880]	Time 0.137 (0.183)	Data 1.25e-04 (5.04e-04)	Tok/s 19467 (38845)	Loss/tok 2.5888 (3.1624)	LR 5.000e-04
0: TRAIN [2][1820/3880]	Time 0.162 (0.183)	Data 1.32e-04 (5.02e-04)	Tok/s 31735 (38817)	Loss/tok 2.9513 (3.1617)	LR 5.000e-04
0: TRAIN [2][1830/3880]	Time 0.162 (0.183)	Data 1.20e-04 (5.00e-04)	Tok/s 30894 (38833)	Loss/tok 3.0103 (3.1618)	LR 5.000e-04
0: TRAIN [2][1840/3880]	Time 0.221 (0.183)	Data 1.13e-04 (4.98e-04)	Tok/s 52995 (38822)	Loss/tok 3.2387 (3.1617)	LR 5.000e-04
0: TRAIN [2][1850/3880]	Time 0.162 (0.183)	Data 9.70e-05 (4.96e-04)	Tok/s 31616 (38803)	Loss/tok 2.9200 (3.1616)	LR 5.000e-04
0: TRAIN [2][1860/3880]	Time 0.162 (0.183)	Data 9.85e-05 (4.94e-04)	Tok/s 31575 (38799)	Loss/tok 2.8850 (3.1615)	LR 5.000e-04
0: TRAIN [2][1870/3880]	Time 0.191 (0.183)	Data 3.57e-04 (4.92e-04)	Tok/s 43684 (38799)	Loss/tok 3.2902 (3.1619)	LR 5.000e-04
0: TRAIN [2][1880/3880]	Time 0.136 (0.183)	Data 1.30e-04 (4.90e-04)	Tok/s 19239 (38789)	Loss/tok 2.5230 (3.1617)	LR 5.000e-04
0: TRAIN [2][1890/3880]	Time 0.162 (0.183)	Data 1.39e-04 (4.88e-04)	Tok/s 31354 (38758)	Loss/tok 2.8245 (3.1614)	LR 5.000e-04
0: TRAIN [2][1900/3880]	Time 0.221 (0.183)	Data 1.30e-04 (4.86e-04)	Tok/s 52310 (38762)	Loss/tok 3.1547 (3.1610)	LR 5.000e-04
0: TRAIN [2][1910/3880]	Time 0.191 (0.183)	Data 1.23e-04 (4.85e-04)	Tok/s 43987 (38783)	Loss/tok 3.1259 (3.1619)	LR 5.000e-04
0: TRAIN [2][1920/3880]	Time 0.162 (0.183)	Data 1.43e-04 (4.83e-04)	Tok/s 31548 (38773)	Loss/tok 2.8933 (3.1614)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1930/3880]	Time 0.163 (0.183)	Data 1.44e-04 (4.81e-04)	Tok/s 31812 (38781)	Loss/tok 2.8394 (3.1615)	LR 5.000e-04
0: TRAIN [2][1940/3880]	Time 0.162 (0.183)	Data 1.29e-04 (4.79e-04)	Tok/s 32823 (38785)	Loss/tok 2.9537 (3.1614)	LR 5.000e-04
0: TRAIN [2][1950/3880]	Time 0.163 (0.183)	Data 1.19e-04 (4.77e-04)	Tok/s 31879 (38808)	Loss/tok 2.9015 (3.1614)	LR 2.500e-04
0: TRAIN [2][1960/3880]	Time 0.162 (0.183)	Data 1.28e-04 (4.75e-04)	Tok/s 31229 (38771)	Loss/tok 3.1458 (3.1609)	LR 2.500e-04
0: TRAIN [2][1970/3880]	Time 0.163 (0.183)	Data 1.20e-04 (4.74e-04)	Tok/s 31564 (38784)	Loss/tok 2.9005 (3.1607)	LR 2.500e-04
0: TRAIN [2][1980/3880]	Time 0.191 (0.183)	Data 1.20e-04 (4.72e-04)	Tok/s 44147 (38776)	Loss/tok 3.1848 (3.1606)	LR 2.500e-04
0: TRAIN [2][1990/3880]	Time 0.162 (0.183)	Data 1.12e-04 (4.70e-04)	Tok/s 31448 (38770)	Loss/tok 2.8683 (3.1604)	LR 2.500e-04
0: TRAIN [2][2000/3880]	Time 0.256 (0.183)	Data 1.32e-04 (4.68e-04)	Tok/s 58780 (38792)	Loss/tok 3.3581 (3.1606)	LR 2.500e-04
0: TRAIN [2][2010/3880]	Time 0.191 (0.183)	Data 1.13e-04 (4.67e-04)	Tok/s 44511 (38802)	Loss/tok 3.0834 (3.1607)	LR 2.500e-04
0: TRAIN [2][2020/3880]	Time 0.191 (0.183)	Data 9.11e-05 (4.65e-04)	Tok/s 44337 (38767)	Loss/tok 3.0657 (3.1600)	LR 2.500e-04
0: TRAIN [2][2030/3880]	Time 0.162 (0.182)	Data 1.06e-04 (4.63e-04)	Tok/s 31406 (38736)	Loss/tok 2.8489 (3.1594)	LR 2.500e-04
0: TRAIN [2][2040/3880]	Time 0.162 (0.183)	Data 1.06e-04 (4.62e-04)	Tok/s 33289 (38760)	Loss/tok 2.9036 (3.1600)	LR 2.500e-04
0: TRAIN [2][2050/3880]	Time 0.136 (0.183)	Data 9.23e-05 (4.60e-04)	Tok/s 18961 (38753)	Loss/tok 2.5269 (3.1596)	LR 2.500e-04
0: TRAIN [2][2060/3880]	Time 0.136 (0.182)	Data 9.35e-05 (4.59e-04)	Tok/s 19156 (38751)	Loss/tok 2.5604 (3.1595)	LR 2.500e-04
0: TRAIN [2][2070/3880]	Time 0.162 (0.183)	Data 1.29e-04 (4.57e-04)	Tok/s 32333 (38757)	Loss/tok 2.9020 (3.1600)	LR 2.500e-04
0: TRAIN [2][2080/3880]	Time 0.162 (0.182)	Data 1.09e-04 (4.55e-04)	Tok/s 31573 (38739)	Loss/tok 2.9172 (3.1596)	LR 2.500e-04
0: TRAIN [2][2090/3880]	Time 0.162 (0.182)	Data 1.22e-04 (4.54e-04)	Tok/s 32071 (38716)	Loss/tok 2.9228 (3.1592)	LR 2.500e-04
0: TRAIN [2][2100/3880]	Time 0.162 (0.182)	Data 1.23e-04 (4.52e-04)	Tok/s 31425 (38722)	Loss/tok 2.9340 (3.1589)	LR 2.500e-04
0: TRAIN [2][2110/3880]	Time 0.162 (0.182)	Data 1.28e-04 (4.50e-04)	Tok/s 32261 (38749)	Loss/tok 2.8655 (3.1595)	LR 2.500e-04
0: TRAIN [2][2120/3880]	Time 0.136 (0.182)	Data 1.03e-04 (4.49e-04)	Tok/s 18707 (38743)	Loss/tok 2.3766 (3.1597)	LR 2.500e-04
0: TRAIN [2][2130/3880]	Time 0.136 (0.182)	Data 1.21e-04 (4.47e-04)	Tok/s 19011 (38732)	Loss/tok 2.5543 (3.1590)	LR 2.500e-04
0: TRAIN [2][2140/3880]	Time 0.191 (0.182)	Data 1.19e-04 (4.46e-04)	Tok/s 44421 (38727)	Loss/tok 3.0752 (3.1585)	LR 2.500e-04
0: TRAIN [2][2150/3880]	Time 0.191 (0.182)	Data 1.03e-04 (4.44e-04)	Tok/s 44170 (38722)	Loss/tok 3.1593 (3.1583)	LR 2.500e-04
0: TRAIN [2][2160/3880]	Time 0.162 (0.182)	Data 1.19e-04 (4.43e-04)	Tok/s 30861 (38729)	Loss/tok 3.0683 (3.1584)	LR 2.500e-04
0: TRAIN [2][2170/3880]	Time 0.221 (0.182)	Data 1.08e-04 (4.41e-04)	Tok/s 52925 (38727)	Loss/tok 3.4255 (3.1581)	LR 2.500e-04
0: TRAIN [2][2180/3880]	Time 0.222 (0.182)	Data 1.20e-04 (4.40e-04)	Tok/s 52291 (38719)	Loss/tok 3.4320 (3.1580)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2190/3880]	Time 0.158 (0.182)	Data 1.06e-04 (4.39e-04)	Tok/s 33503 (38725)	Loss/tok 2.9588 (3.1581)	LR 2.500e-04
0: TRAIN [2][2200/3880]	Time 0.136 (0.182)	Data 1.14e-04 (4.37e-04)	Tok/s 19159 (38677)	Loss/tok 2.5364 (3.1574)	LR 2.500e-04
0: TRAIN [2][2210/3880]	Time 0.191 (0.182)	Data 6.12e-04 (4.36e-04)	Tok/s 44484 (38700)	Loss/tok 2.9834 (3.1575)	LR 2.500e-04
0: TRAIN [2][2220/3880]	Time 0.221 (0.182)	Data 1.30e-04 (4.35e-04)	Tok/s 52930 (38713)	Loss/tok 3.1465 (3.1578)	LR 2.500e-04
0: TRAIN [2][2230/3880]	Time 0.191 (0.182)	Data 1.02e-04 (4.33e-04)	Tok/s 44393 (38699)	Loss/tok 3.2244 (3.1573)	LR 2.500e-04
0: TRAIN [2][2240/3880]	Time 0.162 (0.182)	Data 1.07e-04 (4.32e-04)	Tok/s 31613 (38701)	Loss/tok 2.9468 (3.1570)	LR 2.500e-04
0: TRAIN [2][2250/3880]	Time 0.191 (0.182)	Data 1.18e-04 (4.30e-04)	Tok/s 43913 (38691)	Loss/tok 3.1072 (3.1567)	LR 2.500e-04
0: TRAIN [2][2260/3880]	Time 0.191 (0.182)	Data 9.25e-05 (4.29e-04)	Tok/s 43869 (38705)	Loss/tok 3.1964 (3.1570)	LR 2.500e-04
0: TRAIN [2][2270/3880]	Time 0.162 (0.182)	Data 1.40e-04 (4.28e-04)	Tok/s 31169 (38700)	Loss/tok 2.9956 (3.1566)	LR 2.500e-04
0: TRAIN [2][2280/3880]	Time 0.191 (0.182)	Data 1.05e-04 (4.26e-04)	Tok/s 43554 (38706)	Loss/tok 3.1776 (3.1563)	LR 2.500e-04
0: TRAIN [2][2290/3880]	Time 0.136 (0.182)	Data 3.63e-04 (4.25e-04)	Tok/s 18749 (38696)	Loss/tok 2.6076 (3.1562)	LR 2.500e-04
0: TRAIN [2][2300/3880]	Time 0.162 (0.182)	Data 1.29e-04 (4.24e-04)	Tok/s 32611 (38666)	Loss/tok 2.8363 (3.1556)	LR 2.500e-04
0: TRAIN [2][2310/3880]	Time 0.256 (0.182)	Data 1.18e-04 (4.22e-04)	Tok/s 58068 (38685)	Loss/tok 3.5702 (3.1559)	LR 2.500e-04
0: TRAIN [2][2320/3880]	Time 0.191 (0.182)	Data 1.13e-04 (4.21e-04)	Tok/s 43217 (38692)	Loss/tok 3.3109 (3.1560)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2330/3880]	Time 0.191 (0.182)	Data 1.24e-04 (4.20e-04)	Tok/s 44131 (38703)	Loss/tok 3.2364 (3.1562)	LR 2.500e-04
0: TRAIN [2][2340/3880]	Time 0.191 (0.182)	Data 4.17e-04 (4.19e-04)	Tok/s 43942 (38705)	Loss/tok 3.0690 (3.1557)	LR 2.500e-04
0: TRAIN [2][2350/3880]	Time 0.162 (0.182)	Data 9.85e-05 (4.18e-04)	Tok/s 32763 (38696)	Loss/tok 2.9584 (3.1553)	LR 2.500e-04
0: TRAIN [2][2360/3880]	Time 0.258 (0.182)	Data 1.44e-04 (4.17e-04)	Tok/s 58917 (38691)	Loss/tok 3.3486 (3.1551)	LR 2.500e-04
0: TRAIN [2][2370/3880]	Time 0.162 (0.182)	Data 1.16e-04 (4.15e-04)	Tok/s 31488 (38682)	Loss/tok 2.8653 (3.1547)	LR 2.500e-04
0: TRAIN [2][2380/3880]	Time 0.221 (0.182)	Data 1.10e-04 (4.14e-04)	Tok/s 52851 (38701)	Loss/tok 3.2447 (3.1556)	LR 2.500e-04
0: TRAIN [2][2390/3880]	Time 0.221 (0.182)	Data 1.10e-04 (4.13e-04)	Tok/s 52840 (38705)	Loss/tok 3.2769 (3.1555)	LR 2.500e-04
0: TRAIN [2][2400/3880]	Time 0.162 (0.182)	Data 1.07e-04 (4.12e-04)	Tok/s 33428 (38684)	Loss/tok 2.9488 (3.1552)	LR 2.500e-04
0: TRAIN [2][2410/3880]	Time 0.162 (0.182)	Data 8.82e-05 (4.10e-04)	Tok/s 31927 (38661)	Loss/tok 2.7648 (3.1546)	LR 2.500e-04
0: TRAIN [2][2420/3880]	Time 0.191 (0.182)	Data 1.05e-04 (4.09e-04)	Tok/s 44149 (38638)	Loss/tok 3.1622 (3.1540)	LR 2.500e-04
0: TRAIN [2][2430/3880]	Time 0.191 (0.182)	Data 1.21e-04 (4.08e-04)	Tok/s 44548 (38648)	Loss/tok 3.1460 (3.1540)	LR 2.500e-04
0: TRAIN [2][2440/3880]	Time 0.162 (0.182)	Data 1.04e-04 (4.07e-04)	Tok/s 31244 (38650)	Loss/tok 3.1044 (3.1540)	LR 2.500e-04
0: TRAIN [2][2450/3880]	Time 0.136 (0.182)	Data 1.05e-04 (4.05e-04)	Tok/s 19097 (38662)	Loss/tok 2.6259 (3.1540)	LR 2.500e-04
0: TRAIN [2][2460/3880]	Time 0.221 (0.182)	Data 8.94e-05 (4.04e-04)	Tok/s 53000 (38667)	Loss/tok 3.3161 (3.1538)	LR 2.500e-04
0: TRAIN [2][2470/3880]	Time 0.191 (0.182)	Data 1.15e-04 (4.03e-04)	Tok/s 44449 (38659)	Loss/tok 3.0506 (3.1534)	LR 2.500e-04
0: TRAIN [2][2480/3880]	Time 0.163 (0.182)	Data 1.23e-04 (4.02e-04)	Tok/s 31976 (38647)	Loss/tok 2.8696 (3.1531)	LR 2.500e-04
0: TRAIN [2][2490/3880]	Time 0.162 (0.182)	Data 1.13e-04 (4.01e-04)	Tok/s 32019 (38634)	Loss/tok 2.8459 (3.1526)	LR 2.500e-04
0: TRAIN [2][2500/3880]	Time 0.162 (0.182)	Data 1.33e-04 (4.00e-04)	Tok/s 33247 (38621)	Loss/tok 2.9131 (3.1524)	LR 2.500e-04
0: TRAIN [2][2510/3880]	Time 0.191 (0.182)	Data 1.34e-04 (3.99e-04)	Tok/s 43941 (38621)	Loss/tok 3.2126 (3.1523)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2520/3880]	Time 0.191 (0.182)	Data 1.27e-04 (3.97e-04)	Tok/s 42993 (38627)	Loss/tok 3.1254 (3.1520)	LR 2.500e-04
0: TRAIN [2][2530/3880]	Time 0.221 (0.182)	Data 1.21e-04 (3.96e-04)	Tok/s 52788 (38608)	Loss/tok 3.3738 (3.1516)	LR 2.500e-04
0: TRAIN [2][2540/3880]	Time 0.221 (0.182)	Data 1.00e-04 (3.95e-04)	Tok/s 53439 (38594)	Loss/tok 3.2081 (3.1514)	LR 2.500e-04
0: TRAIN [2][2550/3880]	Time 0.137 (0.182)	Data 1.25e-04 (3.94e-04)	Tok/s 20071 (38577)	Loss/tok 2.6318 (3.1510)	LR 2.500e-04
0: TRAIN [2][2560/3880]	Time 0.136 (0.182)	Data 1.32e-04 (3.93e-04)	Tok/s 19420 (38574)	Loss/tok 2.5126 (3.1510)	LR 2.500e-04
0: TRAIN [2][2570/3880]	Time 0.162 (0.182)	Data 1.19e-04 (3.92e-04)	Tok/s 32418 (38580)	Loss/tok 2.8843 (3.1510)	LR 2.500e-04
0: TRAIN [2][2580/3880]	Time 0.162 (0.182)	Data 1.09e-04 (3.91e-04)	Tok/s 31671 (38564)	Loss/tok 2.9212 (3.1507)	LR 2.500e-04
0: TRAIN [2][2590/3880]	Time 0.258 (0.182)	Data 9.99e-05 (3.90e-04)	Tok/s 56717 (38572)	Loss/tok 3.5954 (3.1509)	LR 2.500e-04
0: TRAIN [2][2600/3880]	Time 0.191 (0.182)	Data 8.77e-05 (3.89e-04)	Tok/s 43781 (38560)	Loss/tok 3.1865 (3.1507)	LR 2.500e-04
0: TRAIN [2][2610/3880]	Time 0.162 (0.182)	Data 9.39e-05 (3.88e-04)	Tok/s 31886 (38553)	Loss/tok 2.7688 (3.1505)	LR 2.500e-04
0: TRAIN [2][2620/3880]	Time 0.137 (0.182)	Data 1.09e-04 (3.87e-04)	Tok/s 20025 (38546)	Loss/tok 2.7161 (3.1502)	LR 2.500e-04
0: TRAIN [2][2630/3880]	Time 0.190 (0.182)	Data 9.54e-05 (3.86e-04)	Tok/s 44273 (38547)	Loss/tok 3.1399 (3.1500)	LR 2.500e-04
0: TRAIN [2][2640/3880]	Time 0.162 (0.182)	Data 1.49e-04 (3.85e-04)	Tok/s 32153 (38531)	Loss/tok 2.9672 (3.1496)	LR 2.500e-04
0: TRAIN [2][2650/3880]	Time 0.191 (0.182)	Data 1.24e-04 (3.84e-04)	Tok/s 44446 (38527)	Loss/tok 3.1457 (3.1494)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2660/3880]	Time 0.257 (0.182)	Data 1.28e-04 (3.83e-04)	Tok/s 57975 (38530)	Loss/tok 3.5192 (3.1496)	LR 2.500e-04
0: TRAIN [2][2670/3880]	Time 0.162 (0.182)	Data 1.52e-04 (3.82e-04)	Tok/s 31796 (38539)	Loss/tok 2.9122 (3.1500)	LR 2.500e-04
0: TRAIN [2][2680/3880]	Time 0.162 (0.182)	Data 1.52e-04 (3.81e-04)	Tok/s 31721 (38550)	Loss/tok 2.9022 (3.1499)	LR 2.500e-04
0: TRAIN [2][2690/3880]	Time 0.191 (0.182)	Data 1.31e-04 (3.80e-04)	Tok/s 44698 (38574)	Loss/tok 3.0415 (3.1502)	LR 2.500e-04
0: TRAIN [2][2700/3880]	Time 0.191 (0.182)	Data 1.45e-04 (3.80e-04)	Tok/s 45031 (38585)	Loss/tok 3.2257 (3.1504)	LR 2.500e-04
0: TRAIN [2][2710/3880]	Time 0.191 (0.182)	Data 1.22e-04 (3.79e-04)	Tok/s 43973 (38603)	Loss/tok 3.2058 (3.1505)	LR 2.500e-04
0: TRAIN [2][2720/3880]	Time 0.163 (0.182)	Data 1.21e-04 (3.78e-04)	Tok/s 31824 (38604)	Loss/tok 2.9150 (3.1504)	LR 2.500e-04
0: TRAIN [2][2730/3880]	Time 0.191 (0.182)	Data 1.12e-04 (3.77e-04)	Tok/s 44781 (38618)	Loss/tok 3.0282 (3.1505)	LR 2.500e-04
0: TRAIN [2][2740/3880]	Time 0.163 (0.182)	Data 1.23e-04 (3.76e-04)	Tok/s 31488 (38608)	Loss/tok 3.0165 (3.1503)	LR 2.500e-04
0: TRAIN [2][2750/3880]	Time 0.191 (0.182)	Data 1.21e-04 (3.75e-04)	Tok/s 44204 (38592)	Loss/tok 2.9410 (3.1498)	LR 2.500e-04
0: TRAIN [2][2760/3880]	Time 0.162 (0.182)	Data 1.02e-04 (3.74e-04)	Tok/s 32380 (38596)	Loss/tok 2.9210 (3.1495)	LR 2.500e-04
0: TRAIN [2][2770/3880]	Time 0.191 (0.182)	Data 1.10e-04 (3.73e-04)	Tok/s 43745 (38602)	Loss/tok 3.0928 (3.1497)	LR 2.500e-04
0: TRAIN [2][2780/3880]	Time 0.191 (0.182)	Data 1.12e-04 (3.72e-04)	Tok/s 44031 (38612)	Loss/tok 3.2622 (3.1499)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2790/3880]	Time 0.186 (0.182)	Data 8.89e-05 (3.71e-04)	Tok/s 45899 (38629)	Loss/tok 2.9018 (3.1504)	LR 2.500e-04
0: TRAIN [2][2800/3880]	Time 0.221 (0.182)	Data 1.23e-04 (3.70e-04)	Tok/s 52055 (38629)	Loss/tok 3.3414 (3.1505)	LR 2.500e-04
0: TRAIN [2][2810/3880]	Time 0.163 (0.182)	Data 1.20e-04 (3.69e-04)	Tok/s 32404 (38621)	Loss/tok 3.1493 (3.1505)	LR 2.500e-04
0: TRAIN [2][2820/3880]	Time 0.162 (0.182)	Data 1.04e-04 (3.69e-04)	Tok/s 32029 (38614)	Loss/tok 2.9396 (3.1503)	LR 2.500e-04
0: TRAIN [2][2830/3880]	Time 0.191 (0.182)	Data 1.16e-04 (3.68e-04)	Tok/s 43759 (38624)	Loss/tok 3.0766 (3.1504)	LR 2.500e-04
0: TRAIN [2][2840/3880]	Time 0.221 (0.182)	Data 1.05e-04 (3.67e-04)	Tok/s 53389 (38631)	Loss/tok 3.2775 (3.1504)	LR 2.500e-04
0: TRAIN [2][2850/3880]	Time 0.257 (0.182)	Data 1.15e-04 (3.66e-04)	Tok/s 58441 (38633)	Loss/tok 3.3311 (3.1508)	LR 2.500e-04
0: TRAIN [2][2860/3880]	Time 0.162 (0.182)	Data 1.09e-04 (3.65e-04)	Tok/s 32402 (38613)	Loss/tok 3.0631 (3.1503)	LR 2.500e-04
0: TRAIN [2][2870/3880]	Time 0.162 (0.182)	Data 9.61e-05 (3.64e-04)	Tok/s 31612 (38611)	Loss/tok 3.0503 (3.1503)	LR 2.500e-04
0: TRAIN [2][2880/3880]	Time 0.164 (0.182)	Data 1.04e-04 (3.64e-04)	Tok/s 32456 (38614)	Loss/tok 2.8480 (3.1502)	LR 2.500e-04
0: TRAIN [2][2890/3880]	Time 0.162 (0.182)	Data 1.20e-04 (3.63e-04)	Tok/s 31763 (38619)	Loss/tok 2.7963 (3.1502)	LR 2.500e-04
0: TRAIN [2][2900/3880]	Time 0.162 (0.182)	Data 1.18e-04 (3.62e-04)	Tok/s 32697 (38620)	Loss/tok 3.0112 (3.1502)	LR 2.500e-04
0: TRAIN [2][2910/3880]	Time 0.191 (0.182)	Data 1.02e-04 (3.61e-04)	Tok/s 42256 (38635)	Loss/tok 3.2477 (3.1504)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2920/3880]	Time 0.221 (0.182)	Data 9.23e-05 (3.60e-04)	Tok/s 53867 (38666)	Loss/tok 3.3181 (3.1514)	LR 2.500e-04
0: TRAIN [2][2930/3880]	Time 0.137 (0.182)	Data 1.07e-04 (3.59e-04)	Tok/s 19770 (38651)	Loss/tok 2.5469 (3.1511)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][2940/3880]	Time 0.162 (0.182)	Data 1.06e-04 (3.59e-04)	Tok/s 32697 (38651)	Loss/tok 2.9956 (3.1511)	LR 2.500e-04
0: TRAIN [2][2950/3880]	Time 0.191 (0.182)	Data 1.08e-04 (3.58e-04)	Tok/s 44096 (38657)	Loss/tok 3.1953 (3.1511)	LR 2.500e-04
0: TRAIN [2][2960/3880]	Time 0.192 (0.182)	Data 1.03e-04 (3.57e-04)	Tok/s 43732 (38638)	Loss/tok 3.3711 (3.1509)	LR 2.500e-04
0: TRAIN [2][2970/3880]	Time 0.137 (0.182)	Data 1.33e-04 (3.56e-04)	Tok/s 19026 (38629)	Loss/tok 2.5980 (3.1507)	LR 2.500e-04
0: TRAIN [2][2980/3880]	Time 0.221 (0.182)	Data 1.31e-04 (3.56e-04)	Tok/s 52190 (38647)	Loss/tok 3.4468 (3.1512)	LR 2.500e-04
0: TRAIN [2][2990/3880]	Time 0.221 (0.182)	Data 1.20e-04 (3.55e-04)	Tok/s 51895 (38650)	Loss/tok 3.4072 (3.1512)	LR 2.500e-04
0: TRAIN [2][3000/3880]	Time 0.162 (0.182)	Data 1.26e-04 (3.54e-04)	Tok/s 31693 (38639)	Loss/tok 2.8772 (3.1508)	LR 2.500e-04
0: TRAIN [2][3010/3880]	Time 0.162 (0.182)	Data 1.07e-04 (3.53e-04)	Tok/s 31994 (38635)	Loss/tok 2.9940 (3.1507)	LR 2.500e-04
0: TRAIN [2][3020/3880]	Time 0.162 (0.182)	Data 1.15e-04 (3.52e-04)	Tok/s 32185 (38625)	Loss/tok 2.9490 (3.1502)	LR 2.500e-04
0: TRAIN [2][3030/3880]	Time 0.162 (0.182)	Data 1.09e-04 (3.52e-04)	Tok/s 32301 (38601)	Loss/tok 2.9492 (3.1498)	LR 2.500e-04
0: TRAIN [2][3040/3880]	Time 0.137 (0.182)	Data 1.04e-04 (3.51e-04)	Tok/s 18956 (38587)	Loss/tok 2.5002 (3.1495)	LR 2.500e-04
0: TRAIN [2][3050/3880]	Time 0.221 (0.182)	Data 1.18e-04 (3.50e-04)	Tok/s 52463 (38580)	Loss/tok 3.2448 (3.1493)	LR 2.500e-04
0: TRAIN [2][3060/3880]	Time 0.162 (0.182)	Data 1.05e-04 (3.50e-04)	Tok/s 31588 (38580)	Loss/tok 2.9024 (3.1492)	LR 2.500e-04
0: TRAIN [2][3070/3880]	Time 0.191 (0.182)	Data 1.34e-04 (3.49e-04)	Tok/s 44101 (38585)	Loss/tok 3.1677 (3.1491)	LR 2.500e-04
0: TRAIN [2][3080/3880]	Time 0.191 (0.182)	Data 1.21e-04 (3.48e-04)	Tok/s 43732 (38589)	Loss/tok 3.0802 (3.1491)	LR 2.500e-04
0: TRAIN [2][3090/3880]	Time 0.257 (0.182)	Data 9.49e-05 (3.47e-04)	Tok/s 58925 (38603)	Loss/tok 3.5285 (3.1494)	LR 2.500e-04
0: TRAIN [2][3100/3880]	Time 0.257 (0.182)	Data 9.35e-05 (3.47e-04)	Tok/s 55556 (38599)	Loss/tok 3.4809 (3.1494)	LR 2.500e-04
0: TRAIN [2][3110/3880]	Time 0.191 (0.182)	Data 1.04e-04 (3.46e-04)	Tok/s 44936 (38595)	Loss/tok 3.0358 (3.1492)	LR 2.500e-04
0: TRAIN [2][3120/3880]	Time 0.162 (0.182)	Data 9.61e-05 (3.45e-04)	Tok/s 31924 (38601)	Loss/tok 2.9120 (3.1491)	LR 2.500e-04
0: TRAIN [2][3130/3880]	Time 0.163 (0.182)	Data 1.11e-04 (3.44e-04)	Tok/s 31536 (38597)	Loss/tok 2.9554 (3.1490)	LR 2.500e-04
0: TRAIN [2][3140/3880]	Time 0.221 (0.182)	Data 9.23e-05 (3.44e-04)	Tok/s 51657 (38603)	Loss/tok 3.4444 (3.1490)	LR 2.500e-04
0: TRAIN [2][3150/3880]	Time 0.191 (0.182)	Data 1.21e-04 (3.43e-04)	Tok/s 43112 (38586)	Loss/tok 3.2403 (3.1487)	LR 2.500e-04
0: TRAIN [2][3160/3880]	Time 0.191 (0.182)	Data 1.04e-04 (3.42e-04)	Tok/s 43907 (38582)	Loss/tok 2.9684 (3.1484)	LR 2.500e-04
0: TRAIN [2][3170/3880]	Time 0.162 (0.182)	Data 9.23e-05 (3.41e-04)	Tok/s 32169 (38568)	Loss/tok 2.9623 (3.1482)	LR 2.500e-04
0: TRAIN [2][3180/3880]	Time 0.256 (0.182)	Data 1.09e-04 (3.41e-04)	Tok/s 58586 (38572)	Loss/tok 3.3658 (3.1483)	LR 2.500e-04
0: TRAIN [2][3190/3880]	Time 0.162 (0.182)	Data 1.09e-04 (3.40e-04)	Tok/s 31779 (38579)	Loss/tok 3.1117 (3.1483)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3200/3880]	Time 0.133 (0.182)	Data 1.19e-04 (3.39e-04)	Tok/s 19869 (38567)	Loss/tok 2.5824 (3.1482)	LR 2.500e-04
0: TRAIN [2][3210/3880]	Time 0.162 (0.182)	Data 1.07e-04 (3.39e-04)	Tok/s 31214 (38562)	Loss/tok 2.9039 (3.1480)	LR 2.500e-04
0: TRAIN [2][3220/3880]	Time 0.162 (0.182)	Data 1.33e-04 (3.38e-04)	Tok/s 31825 (38558)	Loss/tok 2.8902 (3.1478)	LR 2.500e-04
0: TRAIN [2][3230/3880]	Time 0.191 (0.182)	Data 1.03e-04 (3.37e-04)	Tok/s 44137 (38563)	Loss/tok 2.9734 (3.1479)	LR 2.500e-04
0: TRAIN [2][3240/3880]	Time 0.162 (0.182)	Data 1.06e-04 (3.37e-04)	Tok/s 31643 (38552)	Loss/tok 2.9884 (3.1477)	LR 2.500e-04
0: TRAIN [2][3250/3880]	Time 0.162 (0.182)	Data 1.40e-04 (3.36e-04)	Tok/s 31333 (38563)	Loss/tok 3.0034 (3.1481)	LR 2.500e-04
0: TRAIN [2][3260/3880]	Time 0.162 (0.182)	Data 1.20e-04 (3.35e-04)	Tok/s 32203 (38549)	Loss/tok 2.8812 (3.1477)	LR 2.500e-04
0: TRAIN [2][3270/3880]	Time 0.162 (0.182)	Data 1.03e-04 (3.35e-04)	Tok/s 31768 (38554)	Loss/tok 2.9271 (3.1478)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][3280/3880]	Time 0.221 (0.182)	Data 9.68e-05 (3.34e-04)	Tok/s 52916 (38565)	Loss/tok 3.2779 (3.1479)	LR 2.500e-04
0: TRAIN [2][3290/3880]	Time 0.191 (0.182)	Data 1.11e-04 (3.33e-04)	Tok/s 44515 (38587)	Loss/tok 3.1752 (3.1481)	LR 2.500e-04
0: TRAIN [2][3300/3880]	Time 0.191 (0.182)	Data 1.34e-04 (3.33e-04)	Tok/s 43560 (38580)	Loss/tok 3.2966 (3.1479)	LR 2.500e-04
0: TRAIN [2][3310/3880]	Time 0.256 (0.182)	Data 1.13e-04 (3.32e-04)	Tok/s 58071 (38594)	Loss/tok 3.5726 (3.1483)	LR 2.500e-04
0: TRAIN [2][3320/3880]	Time 0.162 (0.182)	Data 1.32e-04 (3.32e-04)	Tok/s 32335 (38606)	Loss/tok 2.9776 (3.1484)	LR 2.500e-04
0: TRAIN [2][3330/3880]	Time 0.162 (0.182)	Data 1.32e-04 (3.31e-04)	Tok/s 31549 (38607)	Loss/tok 2.8967 (3.1484)	LR 2.500e-04
0: TRAIN [2][3340/3880]	Time 0.191 (0.182)	Data 1.28e-04 (3.30e-04)	Tok/s 43321 (38594)	Loss/tok 3.1635 (3.1480)	LR 2.500e-04
0: TRAIN [2][3350/3880]	Time 0.191 (0.182)	Data 1.06e-04 (3.30e-04)	Tok/s 43953 (38588)	Loss/tok 3.0602 (3.1476)	LR 2.500e-04
0: TRAIN [2][3360/3880]	Time 0.191 (0.182)	Data 1.47e-04 (3.29e-04)	Tok/s 43778 (38578)	Loss/tok 3.0219 (3.1474)	LR 2.500e-04
0: TRAIN [2][3370/3880]	Time 0.191 (0.182)	Data 1.11e-04 (3.29e-04)	Tok/s 44617 (38567)	Loss/tok 3.1007 (3.1473)	LR 2.500e-04
0: TRAIN [2][3380/3880]	Time 0.221 (0.182)	Data 1.22e-04 (3.28e-04)	Tok/s 52838 (38577)	Loss/tok 3.2900 (3.1473)	LR 2.500e-04
0: TRAIN [2][3390/3880]	Time 0.162 (0.182)	Data 1.09e-04 (3.28e-04)	Tok/s 31922 (38572)	Loss/tok 2.9042 (3.1471)	LR 2.500e-04
0: TRAIN [2][3400/3880]	Time 0.137 (0.182)	Data 1.14e-04 (3.27e-04)	Tok/s 19019 (38567)	Loss/tok 2.6767 (3.1472)	LR 2.500e-04
0: TRAIN [2][3410/3880]	Time 0.191 (0.182)	Data 1.29e-04 (3.26e-04)	Tok/s 44789 (38582)	Loss/tok 3.2267 (3.1476)	LR 2.500e-04
0: TRAIN [2][3420/3880]	Time 0.191 (0.182)	Data 1.08e-04 (3.26e-04)	Tok/s 43184 (38586)	Loss/tok 3.0460 (3.1475)	LR 2.500e-04
0: TRAIN [2][3430/3880]	Time 0.162 (0.182)	Data 1.23e-04 (3.25e-04)	Tok/s 30956 (38582)	Loss/tok 3.0162 (3.1476)	LR 2.500e-04
0: TRAIN [2][3440/3880]	Time 0.191 (0.182)	Data 9.89e-05 (3.25e-04)	Tok/s 44848 (38594)	Loss/tok 3.1874 (3.1477)	LR 2.500e-04
0: TRAIN [2][3450/3880]	Time 0.163 (0.182)	Data 9.23e-05 (3.24e-04)	Tok/s 32824 (38595)	Loss/tok 2.9926 (3.1478)	LR 2.500e-04
0: TRAIN [2][3460/3880]	Time 0.191 (0.182)	Data 1.24e-04 (3.24e-04)	Tok/s 44320 (38609)	Loss/tok 3.1204 (3.1481)	LR 2.500e-04
0: TRAIN [2][3470/3880]	Time 0.221 (0.182)	Data 1.25e-04 (3.23e-04)	Tok/s 53149 (38618)	Loss/tok 3.3659 (3.1484)	LR 2.500e-04
0: TRAIN [2][3480/3880]	Time 0.163 (0.182)	Data 1.01e-04 (3.23e-04)	Tok/s 31991 (38629)	Loss/tok 2.9075 (3.1486)	LR 2.500e-04
0: TRAIN [2][3490/3880]	Time 0.136 (0.182)	Data 1.00e-04 (3.22e-04)	Tok/s 18194 (38628)	Loss/tok 2.5105 (3.1485)	LR 2.500e-04
0: TRAIN [2][3500/3880]	Time 0.136 (0.182)	Data 2.61e-04 (3.22e-04)	Tok/s 19474 (38608)	Loss/tok 2.5787 (3.1481)	LR 2.500e-04
0: TRAIN [2][3510/3880]	Time 0.191 (0.182)	Data 9.56e-05 (3.21e-04)	Tok/s 43567 (38634)	Loss/tok 3.2102 (3.1486)	LR 2.500e-04
0: TRAIN [2][3520/3880]	Time 0.137 (0.182)	Data 1.14e-04 (3.20e-04)	Tok/s 18762 (38629)	Loss/tok 2.5028 (3.1486)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][3530/3880]	Time 0.221 (0.182)	Data 1.14e-04 (3.20e-04)	Tok/s 51771 (38637)	Loss/tok 3.5219 (3.1487)	LR 2.500e-04
0: TRAIN [2][3540/3880]	Time 0.191 (0.182)	Data 1.17e-04 (3.19e-04)	Tok/s 43617 (38635)	Loss/tok 3.1661 (3.1486)	LR 2.500e-04
0: TRAIN [2][3550/3880]	Time 0.257 (0.182)	Data 4.36e-04 (3.19e-04)	Tok/s 57806 (38624)	Loss/tok 3.5159 (3.1484)	LR 2.500e-04
0: TRAIN [2][3560/3880]	Time 0.191 (0.182)	Data 1.04e-04 (3.18e-04)	Tok/s 44481 (38640)	Loss/tok 3.0259 (3.1486)	LR 1.250e-04
0: TRAIN [2][3570/3880]	Time 0.162 (0.182)	Data 1.03e-04 (3.18e-04)	Tok/s 31718 (38644)	Loss/tok 3.0316 (3.1486)	LR 1.250e-04
0: TRAIN [2][3580/3880]	Time 0.163 (0.182)	Data 1.22e-04 (3.17e-04)	Tok/s 31153 (38653)	Loss/tok 2.9436 (3.1485)	LR 1.250e-04
0: TRAIN [2][3590/3880]	Time 0.162 (0.182)	Data 1.36e-04 (3.17e-04)	Tok/s 30982 (38653)	Loss/tok 2.8614 (3.1485)	LR 1.250e-04
0: TRAIN [2][3600/3880]	Time 0.162 (0.182)	Data 1.21e-04 (3.16e-04)	Tok/s 32044 (38654)	Loss/tok 2.9325 (3.1485)	LR 1.250e-04
0: TRAIN [2][3610/3880]	Time 0.136 (0.182)	Data 1.10e-04 (3.16e-04)	Tok/s 19199 (38655)	Loss/tok 2.4969 (3.1485)	LR 1.250e-04
0: TRAIN [2][3620/3880]	Time 0.162 (0.182)	Data 1.11e-04 (3.15e-04)	Tok/s 32090 (38641)	Loss/tok 2.9821 (3.1484)	LR 1.250e-04
0: TRAIN [2][3630/3880]	Time 0.162 (0.182)	Data 1.14e-04 (3.14e-04)	Tok/s 31239 (38641)	Loss/tok 2.9340 (3.1483)	LR 1.250e-04
0: TRAIN [2][3640/3880]	Time 0.257 (0.182)	Data 1.54e-04 (3.14e-04)	Tok/s 58541 (38632)	Loss/tok 3.4808 (3.1482)	LR 1.250e-04
0: TRAIN [2][3650/3880]	Time 0.221 (0.182)	Data 1.10e-04 (3.13e-04)	Tok/s 54039 (38628)	Loss/tok 3.0660 (3.1479)	LR 1.250e-04
0: TRAIN [2][3660/3880]	Time 0.162 (0.182)	Data 1.20e-04 (3.13e-04)	Tok/s 32048 (38627)	Loss/tok 2.8422 (3.1479)	LR 1.250e-04
0: TRAIN [2][3670/3880]	Time 0.221 (0.182)	Data 1.13e-04 (3.12e-04)	Tok/s 53327 (38629)	Loss/tok 3.4229 (3.1480)	LR 1.250e-04
0: TRAIN [2][3680/3880]	Time 0.221 (0.182)	Data 1.15e-04 (3.12e-04)	Tok/s 52589 (38623)	Loss/tok 3.4512 (3.1480)	LR 1.250e-04
0: TRAIN [2][3690/3880]	Time 0.163 (0.182)	Data 1.11e-04 (3.11e-04)	Tok/s 32396 (38611)	Loss/tok 2.9439 (3.1477)	LR 1.250e-04
0: TRAIN [2][3700/3880]	Time 0.162 (0.182)	Data 1.19e-04 (3.11e-04)	Tok/s 31748 (38593)	Loss/tok 2.7946 (3.1473)	LR 1.250e-04
0: TRAIN [2][3710/3880]	Time 0.162 (0.182)	Data 1.05e-04 (3.10e-04)	Tok/s 31790 (38580)	Loss/tok 2.8328 (3.1470)	LR 1.250e-04
0: TRAIN [2][3720/3880]	Time 0.162 (0.182)	Data 1.10e-04 (3.10e-04)	Tok/s 32029 (38578)	Loss/tok 2.9402 (3.1468)	LR 1.250e-04
0: TRAIN [2][3730/3880]	Time 0.163 (0.182)	Data 1.40e-04 (3.09e-04)	Tok/s 32195 (38563)	Loss/tok 3.0042 (3.1465)	LR 1.250e-04
0: TRAIN [2][3740/3880]	Time 0.256 (0.182)	Data 2.40e-04 (3.09e-04)	Tok/s 59001 (38571)	Loss/tok 3.5146 (3.1467)	LR 1.250e-04
0: TRAIN [2][3750/3880]	Time 0.192 (0.182)	Data 1.20e-04 (3.08e-04)	Tok/s 43273 (38571)	Loss/tok 3.2801 (3.1467)	LR 1.250e-04
0: TRAIN [2][3760/3880]	Time 0.191 (0.182)	Data 1.47e-04 (3.08e-04)	Tok/s 44111 (38570)	Loss/tok 3.1882 (3.1465)	LR 1.250e-04
0: TRAIN [2][3770/3880]	Time 0.191 (0.182)	Data 1.24e-04 (3.07e-04)	Tok/s 43735 (38571)	Loss/tok 3.1876 (3.1465)	LR 1.250e-04
0: TRAIN [2][3780/3880]	Time 0.137 (0.182)	Data 1.21e-04 (3.07e-04)	Tok/s 20099 (38572)	Loss/tok 2.5616 (3.1466)	LR 1.250e-04
0: TRAIN [2][3790/3880]	Time 0.162 (0.182)	Data 1.06e-04 (3.06e-04)	Tok/s 32342 (38576)	Loss/tok 3.0082 (3.1468)	LR 1.250e-04
0: TRAIN [2][3800/3880]	Time 0.162 (0.182)	Data 9.01e-05 (3.06e-04)	Tok/s 31573 (38572)	Loss/tok 3.0318 (3.1467)	LR 1.250e-04
0: TRAIN [2][3810/3880]	Time 0.221 (0.182)	Data 1.14e-04 (3.05e-04)	Tok/s 51892 (38572)	Loss/tok 3.4000 (3.1466)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3820/3880]	Time 0.257 (0.182)	Data 1.03e-04 (3.05e-04)	Tok/s 57397 (38578)	Loss/tok 3.4319 (3.1468)	LR 1.250e-04
0: TRAIN [2][3830/3880]	Time 0.221 (0.182)	Data 8.56e-05 (3.04e-04)	Tok/s 51841 (38578)	Loss/tok 3.2984 (3.1466)	LR 1.250e-04
0: TRAIN [2][3840/3880]	Time 0.191 (0.182)	Data 2.75e-04 (3.04e-04)	Tok/s 43656 (38579)	Loss/tok 3.2661 (3.1467)	LR 1.250e-04
0: TRAIN [2][3850/3880]	Time 0.136 (0.182)	Data 1.19e-04 (3.03e-04)	Tok/s 19003 (38568)	Loss/tok 2.5266 (3.1464)	LR 1.250e-04
0: TRAIN [2][3860/3880]	Time 0.162 (0.182)	Data 1.06e-04 (3.03e-04)	Tok/s 31019 (38567)	Loss/tok 3.0513 (3.1465)	LR 1.250e-04
0: TRAIN [2][3870/3880]	Time 0.162 (0.182)	Data 1.11e-04 (3.02e-04)	Tok/s 32429 (38566)	Loss/tok 2.9191 (3.1463)	LR 1.250e-04
:::MLL 1571260959.602 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1571260959.602 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.662 (0.662)	Decoder iters 110.0 (110.0)	Tok/s 24820 (24820)
0: Running moses detokenizer
0: BLEU(score=24.15302871686577, counts=[37042, 18568, 10616, 6317], totals=[65271, 62268, 59265, 56268], precisions=[56.751083942332734, 29.819489946682083, 17.9127647009196, 11.226629700717993], bp=1.0, sys_len=65271, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1571260961.468 eval_accuracy: {"value": 24.15, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1571260961.468 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.1467	Test BLEU: 24.15
0: Performance: Epoch: 2	Training: 308466 Tok/s
0: Finished epoch 2
:::MLL 1571260961.469 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
0: Closing preprocessed data file
:::MLL 1571260961.469 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 568}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-10-16 09:22:52 PM
RESULT,RNN_TRANSLATOR,,2164,nvidia,2019-10-16 08:46:48 PM
