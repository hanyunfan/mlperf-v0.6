Beginning trial 2 of 2
Gathering sys log on dss01
:::MLL 1570028642.173 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1570028642.174 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1570028642.174 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1570028642.174 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1570028642.175 submission_platform: {"value": "1xDSS8440", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1570028642.175 submission_entry: {"value": "{'hardware': 'DSS8440', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '5.0-0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz', 'num_cores': '40', 'num_vcpus': '40', 'accelerator': 'Tesla V100-PCIE-32GB', 'num_accelerators': '8', 'sys_mem_size': '754 GB', 'sys_storage_type': 'SATA SSD', 'sys_storage_size': '1x 447.1G + 1x 931.5G', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27700 Family [ConnectX-4]', 'num_network_cards': '1', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1570028642.176 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1570028642.176 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1570028646.810 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node dss01
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=4259' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=191002092209022722196 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191002092209022722196 ./run_and_time.sh
Run vars: id 191002092209022722196 gpus 8 mparams  --master_port=4259
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=0
+ MATH=amp_fp16
STARTING TIMING RUN AT 2019-10-02 03:04:07 PM
+ [[ 0 -eq 1 ]]
+ LAUNCH_OPT='torch.distributed.launch --nproc_per_node 8  --master_port=4259'
running benchmark
+ echo 'running benchmark'
+ python -m torch.distributed.launch --nproc_per_node 8 --master_port=4259 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1570028649.513 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570028649.517 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570028649.522 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570028649.522 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570028649.527 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570028649.531 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570028649.533 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1570028649.534 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 4016777925
dss01:1616:1616 [0] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:1616:1616 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:1616:1616 [0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:1616:1616 [0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:1616:1616 [0] NCCL INFO NET/IB : No device found.
NCCL version 2.4.6+cuda10.1
dss01:1620:1620 [4] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:1620:1620 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:1619:1619 [3] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:1619:1619 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:1622:1622 [6] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:1622:1622 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:1623:1623 [7] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:1623:1623 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:1617:1617 [1] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:1617:1617 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:1621:1621 [5] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:1621:1621 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
dss01:1618:1618 [2] NCCL INFO NET/Socket : Using [0]eth0:10.141.0.200<0>
dss01:1618:1618 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

dss01:1620:1620 [4] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:1620:1620 [4] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:1620:1620 [4] NCCL INFO NET/IB : No device found.

dss01:1619:1619 [3] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:1622:1622 [6] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:1619:1619 [3] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0

dss01:1622:1622 [6] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:1619:1619 [3] NCCL INFO NET/IB : No device found.
dss01:1622:1622 [6] NCCL INFO NET/IB : No device found.

dss01:1621:1621 [5] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:1621:1621 [5] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:1621:1621 [5] NCCL INFO NET/IB : No device found.

dss01:1617:1617 [1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:1618:1618 [2] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:1617:1617 [1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0

dss01:1618:1618 [2] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:1617:1617 [1] NCCL INFO NET/IB : No device found.
dss01:1618:1618 [2] NCCL INFO NET/IB : No device found.

dss01:1623:1623 [7] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed

dss01:1623:1623 [7] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
dss01:1623:1623 [7] NCCL INFO NET/IB : No device found.
dss01:1616:1978 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555
dss01:1620:1979 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa
dss01:1619:1980 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555
dss01:1622:1981 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa
dss01:1618:1982 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555
dss01:1621:1983 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa
dss01:1617:1984 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555
dss01:1623:1985 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa
dss01:1618:1982 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to 1.
dss01:1621:1983 [5] NCCL INFO NCCL_P2P_LEVEL set by environment to 1.
dss01:1617:1984 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to 1.
dss01:1622:1981 [6] NCCL INFO NCCL_P2P_LEVEL set by environment to 1.
dss01:1619:1980 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to 1.
dss01:1620:1979 [4] NCCL INFO NCCL_P2P_LEVEL set by environment to 1.
dss01:1623:1985 [7] NCCL INFO NCCL_P2P_LEVEL set by environment to 1.
dss01:1616:1978 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to 1.
dss01:1616:1978 [0] NCCL INFO Channel 00 :    0   1   2   3   4   5   6   7
dss01:1616:1978 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/IPC
dss01:1618:1982 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/IPC
dss01:1622:1981 [6] NCCL INFO Ring 00 : 6[6] -> 7[7] via P2P/IPC
dss01:1620:1979 [4] NCCL INFO Ring 00 : 4[4] -> 5[5] via P2P/IPC
dss01:1617:1984 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via direct shared memory
dss01:1619:1980 [3] NCCL INFO Ring 00 : 3[3] -> 4[4] via direct shared memory
dss01:1621:1983 [5] NCCL INFO Ring 00 : 5[5] -> 6[6] via direct shared memory
dss01:1623:1985 [7] NCCL INFO Ring 00 : 7[7] -> 0[0] via direct shared memory
dss01:1616:1978 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
dss01:1623:1985 [7] NCCL INFO comm 0x7ffe98007590 rank 7 nranks 8 cudaDev 7 nvmlDev 7 - Init COMPLETE
dss01:1617:1984 [1] NCCL INFO comm 0x7fff68007590 rank 1 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE
dss01:1619:1980 [3] NCCL INFO comm 0x7fff54007590 rank 3 nranks 8 cudaDev 3 nvmlDev 3 - Init COMPLETE
dss01:1621:1983 [5] NCCL INFO comm 0x7fff5c007590 rank 5 nranks 8 cudaDev 5 nvmlDev 5 - Init COMPLETE
dss01:1616:1978 [0] NCCL INFO comm 0x7ffe38007590 rank 0 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE
dss01:1616:1616 [0] NCCL INFO Launch mode Parallel
0: Worker 0 is using worker seed: 2608780779
0: Building vocabulary from /data/vocab.bpe.32000
dss01:1620:1979 [4] NCCL INFO comm 0x7fff58007590 rank 4 nranks 8 cudaDev 4 nvmlDev 4 - Init COMPLETE
dss01:1618:1982 [2] NCCL INFO comm 0x7ffe70007590 rank 2 nranks 8 cudaDev 2 nvmlDev 2 - Init COMPLETE
dss01:1622:1981 [6] NCCL INFO comm 0x7ffe98007590 rank 6 nranks 8 cudaDev 6 nvmlDev 6 - Init COMPLETE
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1570028672.511 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1570028675.156 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1570028675.157 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1570028675.157 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1570028676.201 global_batch_size: {"value": 2048, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1570028676.202 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1570028676.203 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1570028676.203 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1570028676.203 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1570028676.203 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1570028676.204 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1570028676.204 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1570028676.204 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1570028676.205 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 2227653730
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/1938]	Time 1.068 (1.068)	Data 7.99e-01 (7.99e-01)	Tok/s 15737 (15737)	Loss/tok 10.6538 (10.6538)	LR 2.000e-05
0: TRAIN [0][10/1938]	Time 0.336 (0.384)	Data 1.35e-04 (7.28e-02)	Tok/s 70769 (59788)	Loss/tok 9.7070 (10.1760)	LR 2.518e-05
0: TRAIN [0][20/1938]	Time 0.212 (0.323)	Data 1.49e-04 (3.82e-02)	Tok/s 48958 (58370)	Loss/tok 9.2732 (9.8984)	LR 3.170e-05
0: TRAIN [0][30/1938]	Time 0.272 (0.292)	Data 1.74e-04 (2.59e-02)	Tok/s 61388 (55310)	Loss/tok 9.1267 (9.7260)	LR 3.991e-05
0: TRAIN [0][40/1938]	Time 0.408 (0.284)	Data 1.61e-04 (1.97e-02)	Tok/s 72552 (55726)	Loss/tok 8.9806 (9.5448)	LR 5.024e-05
0: TRAIN [0][50/1938]	Time 0.274 (0.275)	Data 1.28e-04 (1.58e-02)	Tok/s 61930 (55294)	Loss/tok 8.5505 (9.3949)	LR 6.325e-05
0: TRAIN [0][60/1938]	Time 0.274 (0.269)	Data 1.26e-04 (1.33e-02)	Tok/s 62116 (55004)	Loss/tok 8.3442 (9.2569)	LR 7.962e-05
0: TRAIN [0][70/1938]	Time 0.336 (0.271)	Data 2.01e-04 (1.14e-02)	Tok/s 69558 (55609)	Loss/tok 8.2828 (9.1034)	LR 1.002e-04
0: TRAIN [0][80/1938]	Time 0.212 (0.270)	Data 1.30e-04 (1.00e-02)	Tok/s 48767 (56139)	Loss/tok 7.9716 (8.9745)	LR 1.262e-04
0: TRAIN [0][90/1938]	Time 0.212 (0.271)	Data 2.37e-04 (8.96e-03)	Tok/s 48367 (56440)	Loss/tok 7.7816 (8.8649)	LR 1.589e-04
0: TRAIN [0][100/1938]	Time 0.212 (0.269)	Data 1.72e-04 (8.09e-03)	Tok/s 49345 (56253)	Loss/tok 7.7555 (8.7808)	LR 2.000e-04
0: TRAIN [0][110/1938]	Time 0.336 (0.268)	Data 1.94e-04 (7.37e-03)	Tok/s 70172 (56154)	Loss/tok 8.0801 (8.7081)	LR 2.518e-04
0: TRAIN [0][120/1938]	Time 0.156 (0.266)	Data 1.16e-04 (6.78e-03)	Tok/s 34146 (55919)	Loss/tok 7.2906 (8.6439)	LR 3.170e-04
0: TRAIN [0][130/1938]	Time 0.275 (0.264)	Data 1.39e-04 (6.27e-03)	Tok/s 60774 (55629)	Loss/tok 7.8753 (8.5902)	LR 3.991e-04
0: TRAIN [0][140/1938]	Time 0.212 (0.261)	Data 1.42e-04 (5.84e-03)	Tok/s 48978 (55191)	Loss/tok 7.6136 (8.5414)	LR 5.024e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][150/1938]	Time 0.274 (0.259)	Data 1.33e-04 (5.46e-03)	Tok/s 61388 (54985)	Loss/tok 8.1592 (8.5218)	LR 6.181e-04
0: TRAIN [0][160/1938]	Time 0.210 (0.259)	Data 1.49e-04 (5.13e-03)	Tok/s 48864 (55043)	Loss/tok 7.4137 (8.4727)	LR 7.781e-04
0: TRAIN [0][170/1938]	Time 0.213 (0.259)	Data 1.16e-04 (4.84e-03)	Tok/s 48528 (55289)	Loss/tok 7.3798 (8.4199)	LR 9.796e-04
0: TRAIN [0][180/1938]	Time 0.409 (0.260)	Data 1.20e-04 (4.58e-03)	Tok/s 72233 (55391)	Loss/tok 7.6087 (8.3653)	LR 1.233e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][190/1938]	Time 0.212 (0.259)	Data 1.97e-04 (4.35e-03)	Tok/s 49275 (55413)	Loss/tok 7.1061 (8.3111)	LR 1.517e-03
0: TRAIN [0][200/1938]	Time 0.212 (0.258)	Data 1.34e-04 (4.14e-03)	Tok/s 49864 (55316)	Loss/tok 6.8258 (8.2562)	LR 1.910e-03
0: TRAIN [0][210/1938]	Time 0.273 (0.257)	Data 1.36e-04 (3.95e-03)	Tok/s 61886 (55230)	Loss/tok 7.0504 (8.2030)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.158 (0.256)	Data 1.43e-04 (3.78e-03)	Tok/s 33283 (55068)	Loss/tok 5.8733 (8.1440)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.272 (0.255)	Data 1.13e-04 (3.62e-03)	Tok/s 61760 (54943)	Loss/tok 6.6342 (8.0870)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.409 (0.256)	Data 1.11e-04 (3.47e-03)	Tok/s 73075 (55177)	Loss/tok 6.8395 (8.0156)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.273 (0.257)	Data 1.32e-04 (3.34e-03)	Tok/s 62067 (55387)	Loss/tok 6.5333 (7.9449)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.335 (0.257)	Data 1.36e-04 (3.22e-03)	Tok/s 69339 (55406)	Loss/tok 6.4857 (7.8829)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.274 (0.256)	Data 1.21e-04 (3.10e-03)	Tok/s 61109 (55345)	Loss/tok 6.2412 (7.8235)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.212 (0.254)	Data 1.27e-04 (3.00e-03)	Tok/s 48706 (55043)	Loss/tok 5.8596 (7.7765)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.270 (0.254)	Data 1.62e-04 (2.90e-03)	Tok/s 62348 (55016)	Loss/tok 5.9793 (7.7175)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.211 (0.254)	Data 1.25e-04 (2.81e-03)	Tok/s 48088 (54967)	Loss/tok 5.6197 (7.6583)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.274 (0.254)	Data 1.08e-04 (2.72e-03)	Tok/s 61543 (55071)	Loss/tok 5.8613 (7.5955)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.213 (0.254)	Data 1.41e-04 (2.64e-03)	Tok/s 47652 (55024)	Loss/tok 5.4650 (7.5426)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.273 (0.254)	Data 1.36e-04 (2.57e-03)	Tok/s 61596 (55026)	Loss/tok 5.5845 (7.4859)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.275 (0.254)	Data 1.26e-04 (2.49e-03)	Tok/s 61495 (55102)	Loss/tok 5.4611 (7.4255)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.273 (0.254)	Data 1.37e-04 (2.43e-03)	Tok/s 61808 (55127)	Loss/tok 5.5145 (7.3674)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.212 (0.254)	Data 1.56e-04 (2.36e-03)	Tok/s 49186 (55151)	Loss/tok 4.9860 (7.3105)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.274 (0.254)	Data 1.27e-04 (2.30e-03)	Tok/s 61846 (55155)	Loss/tok 5.3065 (7.2559)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.212 (0.253)	Data 1.70e-04 (2.25e-03)	Tok/s 47870 (55075)	Loss/tok 4.9125 (7.2062)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.213 (0.252)	Data 1.04e-04 (2.20e-03)	Tok/s 47664 (54896)	Loss/tok 4.8374 (7.1635)	LR 2.000e-03
0: TRAIN [0][400/1938]	Time 0.213 (0.252)	Data 1.65e-04 (2.14e-03)	Tok/s 48674 (54881)	Loss/tok 4.7593 (7.1122)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.275 (0.253)	Data 1.41e-04 (2.10e-03)	Tok/s 61700 (54970)	Loss/tok 4.8944 (7.0553)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.213 (0.253)	Data 3.54e-04 (2.05e-03)	Tok/s 48525 (54956)	Loss/tok 4.6188 (7.0047)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.212 (0.253)	Data 1.40e-04 (2.01e-03)	Tok/s 48531 (55017)	Loss/tok 4.3325 (6.9512)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.337 (0.253)	Data 1.32e-04 (1.96e-03)	Tok/s 69299 (55009)	Loss/tok 5.0556 (6.9019)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.213 (0.253)	Data 1.45e-04 (1.92e-03)	Tok/s 49331 (55050)	Loss/tok 4.3807 (6.8504)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.276 (0.253)	Data 1.23e-04 (1.89e-03)	Tok/s 60865 (55096)	Loss/tok 4.7180 (6.8013)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.156 (0.253)	Data 1.24e-04 (1.85e-03)	Tok/s 34217 (55040)	Loss/tok 3.6452 (6.7581)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.213 (0.253)	Data 1.16e-04 (1.81e-03)	Tok/s 47747 (54995)	Loss/tok 4.1732 (6.7143)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.214 (0.253)	Data 1.47e-04 (1.78e-03)	Tok/s 48875 (54973)	Loss/tok 4.3054 (6.6723)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.338 (0.252)	Data 1.32e-04 (1.75e-03)	Tok/s 68762 (54916)	Loss/tok 4.8342 (6.6325)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.274 (0.252)	Data 1.33e-04 (1.72e-03)	Tok/s 61805 (54930)	Loss/tok 4.6004 (6.5900)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.276 (0.253)	Data 1.36e-04 (1.69e-03)	Tok/s 61388 (54995)	Loss/tok 4.3163 (6.5429)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.408 (0.253)	Data 1.42e-04 (1.66e-03)	Tok/s 72845 (54945)	Loss/tok 4.9089 (6.5056)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.337 (0.253)	Data 1.53e-04 (1.63e-03)	Tok/s 69502 (55073)	Loss/tok 4.6690 (6.4580)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.213 (0.253)	Data 1.32e-04 (1.60e-03)	Tok/s 49235 (55018)	Loss/tok 4.0212 (6.4229)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.272 (0.253)	Data 1.17e-04 (1.58e-03)	Tok/s 61479 (55009)	Loss/tok 4.2840 (6.3868)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.274 (0.253)	Data 1.59e-04 (1.55e-03)	Tok/s 61241 (55093)	Loss/tok 4.2710 (6.3442)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.275 (0.254)	Data 1.35e-04 (1.53e-03)	Tok/s 60734 (55100)	Loss/tok 4.3435 (6.3079)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.213 (0.254)	Data 1.40e-04 (1.51e-03)	Tok/s 48156 (55156)	Loss/tok 3.9607 (6.2673)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.213 (0.254)	Data 1.10e-04 (1.48e-03)	Tok/s 48012 (55221)	Loss/tok 3.9677 (6.2291)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.336 (0.254)	Data 1.46e-04 (1.46e-03)	Tok/s 70006 (55145)	Loss/tok 4.5305 (6.2005)	LR 2.000e-03
0: TRAIN [0][620/1938]	Time 0.212 (0.254)	Data 1.38e-04 (1.44e-03)	Tok/s 48809 (55179)	Loss/tok 3.9158 (6.1659)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.276 (0.254)	Data 1.30e-03 (1.42e-03)	Tok/s 61253 (55177)	Loss/tok 4.1332 (6.1340)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.274 (0.255)	Data 1.36e-04 (1.40e-03)	Tok/s 60723 (55304)	Loss/tok 4.2516 (6.0968)	LR 2.000e-03
0: TRAIN [0][650/1938]	Time 0.213 (0.255)	Data 1.30e-04 (1.38e-03)	Tok/s 48612 (55335)	Loss/tok 3.9618 (6.0646)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.276 (0.255)	Data 1.51e-04 (1.36e-03)	Tok/s 60222 (55359)	Loss/tok 4.1144 (6.0347)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.336 (0.255)	Data 1.17e-04 (1.35e-03)	Tok/s 69582 (55319)	Loss/tok 4.3242 (6.0075)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.274 (0.255)	Data 1.14e-04 (1.33e-03)	Tok/s 61127 (55317)	Loss/tok 4.1984 (5.9792)	LR 2.000e-03
0: TRAIN [0][690/1938]	Time 0.275 (0.255)	Data 1.24e-04 (1.31e-03)	Tok/s 60602 (55245)	Loss/tok 4.0371 (5.9552)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.213 (0.255)	Data 1.11e-04 (1.29e-03)	Tok/s 48385 (55223)	Loss/tok 3.8013 (5.9301)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.277 (0.255)	Data 1.09e-04 (1.28e-03)	Tok/s 61154 (55283)	Loss/tok 3.9192 (5.9006)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][720/1938]	Time 0.275 (0.255)	Data 1.08e-04 (1.26e-03)	Tok/s 60880 (55336)	Loss/tok 4.0374 (5.8728)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.214 (0.255)	Data 1.53e-04 (1.25e-03)	Tok/s 48524 (55333)	Loss/tok 3.7537 (5.8481)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.156 (0.255)	Data 1.16e-04 (1.23e-03)	Tok/s 33692 (55271)	Loss/tok 3.1649 (5.8266)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.277 (0.255)	Data 1.65e-04 (1.22e-03)	Tok/s 60250 (55300)	Loss/tok 3.9608 (5.8010)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.212 (0.255)	Data 1.52e-04 (1.20e-03)	Tok/s 49019 (55267)	Loss/tok 3.6604 (5.7794)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.275 (0.255)	Data 1.46e-04 (1.19e-03)	Tok/s 61398 (55234)	Loss/tok 4.0036 (5.7573)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.213 (0.255)	Data 1.12e-04 (1.17e-03)	Tok/s 48383 (55237)	Loss/tok 3.7568 (5.7343)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.213 (0.255)	Data 1.41e-04 (1.16e-03)	Tok/s 48500 (55267)	Loss/tok 3.7364 (5.7110)	LR 2.000e-03
0: TRAIN [0][800/1938]	Time 0.212 (0.255)	Data 1.21e-04 (1.15e-03)	Tok/s 48503 (55215)	Loss/tok 3.8345 (5.6919)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][810/1938]	Time 0.272 (0.255)	Data 1.21e-04 (1.14e-03)	Tok/s 62212 (55197)	Loss/tok 3.8790 (5.6709)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.212 (0.255)	Data 1.18e-04 (1.12e-03)	Tok/s 48735 (55203)	Loss/tok 3.6088 (5.6498)	LR 2.000e-03
0: TRAIN [0][830/1938]	Time 0.156 (0.254)	Data 1.18e-04 (1.11e-03)	Tok/s 33600 (55140)	Loss/tok 3.0996 (5.6320)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.411 (0.254)	Data 1.33e-04 (1.10e-03)	Tok/s 73511 (55140)	Loss/tok 4.3610 (5.6117)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.213 (0.255)	Data 5.41e-04 (1.09e-03)	Tok/s 49301 (55132)	Loss/tok 3.6600 (5.5924)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.212 (0.255)	Data 1.24e-04 (1.08e-03)	Tok/s 48671 (55185)	Loss/tok 3.5988 (5.5710)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.274 (0.255)	Data 1.13e-04 (1.07e-03)	Tok/s 62186 (55134)	Loss/tok 3.9452 (5.5537)	LR 2.000e-03
0: TRAIN [0][880/1938]	Time 0.212 (0.254)	Data 1.61e-04 (1.06e-03)	Tok/s 49528 (55109)	Loss/tok 3.7383 (5.5360)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.275 (0.255)	Data 1.65e-04 (1.05e-03)	Tok/s 60154 (55163)	Loss/tok 4.0503 (5.5156)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.157 (0.254)	Data 1.32e-04 (1.04e-03)	Tok/s 33913 (55117)	Loss/tok 3.0152 (5.4992)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.277 (0.254)	Data 1.19e-04 (1.03e-03)	Tok/s 60193 (55120)	Loss/tok 4.0316 (5.4817)	LR 2.000e-03
0: TRAIN [0][920/1938]	Time 0.212 (0.254)	Data 1.22e-04 (1.02e-03)	Tok/s 48168 (55036)	Loss/tok 3.5082 (5.4673)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.276 (0.254)	Data 1.23e-04 (1.01e-03)	Tok/s 61402 (55015)	Loss/tok 3.9113 (5.4514)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.212 (0.254)	Data 1.37e-04 (1.00e-03)	Tok/s 48247 (54949)	Loss/tok 3.6232 (5.4373)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.213 (0.254)	Data 1.19e-04 (9.91e-04)	Tok/s 47875 (54950)	Loss/tok 3.6587 (5.4209)	LR 2.000e-03
0: TRAIN [0][960/1938]	Time 0.213 (0.254)	Data 1.42e-04 (9.82e-04)	Tok/s 48346 (54984)	Loss/tok 3.5425 (5.4035)	LR 2.000e-03
0: TRAIN [0][970/1938]	Time 0.274 (0.254)	Data 1.18e-04 (9.73e-04)	Tok/s 61273 (54962)	Loss/tok 3.6885 (5.3880)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.277 (0.254)	Data 3.76e-04 (9.65e-04)	Tok/s 60789 (54989)	Loss/tok 3.8349 (5.3711)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.213 (0.254)	Data 1.42e-04 (9.56e-04)	Tok/s 47051 (54981)	Loss/tok 3.6231 (5.3558)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.337 (0.254)	Data 1.18e-04 (9.48e-04)	Tok/s 69601 (55012)	Loss/tok 4.0034 (5.3395)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.213 (0.254)	Data 2.03e-04 (9.40e-04)	Tok/s 49275 (54987)	Loss/tok 3.5716 (5.3254)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.213 (0.253)	Data 2.47e-04 (9.32e-04)	Tok/s 49248 (54941)	Loss/tok 3.7134 (5.3128)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.214 (0.254)	Data 1.35e-04 (9.25e-04)	Tok/s 47804 (54982)	Loss/tok 3.5626 (5.2964)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.273 (0.254)	Data 1.40e-04 (9.17e-04)	Tok/s 61809 (54978)	Loss/tok 3.7660 (5.2819)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.336 (0.254)	Data 1.34e-04 (9.10e-04)	Tok/s 69158 (54956)	Loss/tok 3.9830 (5.2687)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.336 (0.254)	Data 1.42e-04 (9.02e-04)	Tok/s 68857 (54977)	Loss/tok 4.0682 (5.2541)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.214 (0.254)	Data 1.24e-04 (8.96e-04)	Tok/s 48078 (54991)	Loss/tok 3.5326 (5.2399)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.336 (0.254)	Data 1.35e-04 (8.89e-04)	Tok/s 69540 (54964)	Loss/tok 4.1061 (5.2275)	LR 2.000e-03
0: TRAIN [0][1090/1938]	Time 0.213 (0.254)	Data 1.13e-04 (8.82e-04)	Tok/s 48489 (54977)	Loss/tok 3.6143 (5.2137)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.159 (0.254)	Data 1.19e-04 (8.75e-04)	Tok/s 33292 (54946)	Loss/tok 3.0294 (5.2014)	LR 2.000e-03
0: TRAIN [0][1110/1938]	Time 0.275 (0.254)	Data 1.13e-04 (8.68e-04)	Tok/s 60469 (54958)	Loss/tok 3.7017 (5.1878)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.214 (0.253)	Data 1.20e-04 (8.62e-04)	Tok/s 48730 (54862)	Loss/tok 3.5761 (5.1784)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.213 (0.253)	Data 1.25e-04 (8.55e-04)	Tok/s 47771 (54856)	Loss/tok 3.5568 (5.1657)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.214 (0.253)	Data 1.32e-04 (8.49e-04)	Tok/s 48075 (54828)	Loss/tok 3.5548 (5.1541)	LR 2.000e-03
0: TRAIN [0][1150/1938]	Time 0.156 (0.253)	Data 1.43e-04 (8.42e-04)	Tok/s 33277 (54788)	Loss/tok 2.9558 (5.1432)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.213 (0.253)	Data 1.16e-04 (8.36e-04)	Tok/s 47992 (54761)	Loss/tok 3.3890 (5.1318)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.337 (0.253)	Data 1.20e-04 (8.30e-04)	Tok/s 69356 (54778)	Loss/tok 4.0776 (5.1194)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.213 (0.253)	Data 1.16e-04 (8.24e-04)	Tok/s 49889 (54805)	Loss/tok 3.5763 (5.1066)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.278 (0.253)	Data 1.22e-04 (8.19e-04)	Tok/s 60886 (54863)	Loss/tok 3.5968 (5.0928)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1200/1938]	Time 0.276 (0.253)	Data 1.33e-04 (8.13e-04)	Tok/s 61805 (54868)	Loss/tok 3.6558 (5.0810)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1210/1938]	Time 0.413 (0.253)	Data 1.20e-04 (8.07e-04)	Tok/s 72812 (54841)	Loss/tok 4.0520 (5.0706)	LR 2.000e-03
0: TRAIN [0][1220/1938]	Time 0.277 (0.253)	Data 1.23e-04 (8.02e-04)	Tok/s 60866 (54850)	Loss/tok 3.7016 (5.0595)	LR 2.000e-03
0: TRAIN [0][1230/1938]	Time 0.274 (0.253)	Data 1.26e-04 (7.96e-04)	Tok/s 61172 (54872)	Loss/tok 3.8200 (5.0474)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.337 (0.253)	Data 1.21e-04 (7.91e-04)	Tok/s 69550 (54911)	Loss/tok 3.7296 (5.0347)	LR 2.000e-03
0: TRAIN [0][1250/1938]	Time 0.212 (0.253)	Data 1.11e-04 (7.85e-04)	Tok/s 49333 (54900)	Loss/tok 3.4932 (5.0245)	LR 2.000e-03
0: TRAIN [0][1260/1938]	Time 0.337 (0.253)	Data 1.21e-04 (7.80e-04)	Tok/s 69631 (54943)	Loss/tok 3.8748 (5.0124)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.338 (0.253)	Data 1.49e-04 (7.75e-04)	Tok/s 69379 (54933)	Loss/tok 3.9049 (5.0025)	LR 2.000e-03
0: TRAIN [0][1280/1938]	Time 0.213 (0.253)	Data 1.49e-04 (7.70e-04)	Tok/s 49586 (54906)	Loss/tok 3.5226 (4.9934)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.159 (0.253)	Data 1.19e-04 (7.65e-04)	Tok/s 33588 (54942)	Loss/tok 2.9655 (4.9825)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.275 (0.253)	Data 1.19e-04 (7.60e-04)	Tok/s 61665 (54939)	Loss/tok 3.5964 (4.9723)	LR 2.000e-03
0: TRAIN [0][1310/1938]	Time 0.213 (0.254)	Data 1.18e-04 (7.55e-04)	Tok/s 49415 (54968)	Loss/tok 3.5159 (4.9614)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.273 (0.254)	Data 1.44e-04 (7.51e-04)	Tok/s 62240 (54988)	Loss/tok 3.5813 (4.9510)	LR 2.000e-03
0: TRAIN [0][1330/1938]	Time 0.277 (0.253)	Data 1.37e-04 (7.46e-04)	Tok/s 60498 (54941)	Loss/tok 3.5749 (4.9425)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.213 (0.253)	Data 3.97e-04 (7.42e-04)	Tok/s 49645 (54919)	Loss/tok 3.4169 (4.9335)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.158 (0.253)	Data 1.48e-04 (7.38e-04)	Tok/s 33062 (54917)	Loss/tok 2.8974 (4.9241)	LR 2.000e-03
0: TRAIN [0][1360/1938]	Time 0.412 (0.253)	Data 1.28e-04 (7.33e-04)	Tok/s 72101 (54936)	Loss/tok 4.0277 (4.9143)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.213 (0.253)	Data 1.43e-04 (7.29e-04)	Tok/s 49196 (54965)	Loss/tok 3.4073 (4.9043)	LR 2.000e-03
0: TRAIN [0][1380/1938]	Time 0.275 (0.253)	Data 1.28e-04 (7.25e-04)	Tok/s 60957 (54950)	Loss/tok 3.6840 (4.8958)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.275 (0.253)	Data 3.62e-04 (7.21e-04)	Tok/s 60255 (54965)	Loss/tok 3.6900 (4.8864)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.213 (0.254)	Data 1.44e-04 (7.17e-04)	Tok/s 49526 (55008)	Loss/tok 3.4451 (4.8764)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.336 (0.254)	Data 1.35e-04 (7.12e-04)	Tok/s 68102 (55049)	Loss/tok 3.9487 (4.8670)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.213 (0.254)	Data 1.52e-04 (7.08e-04)	Tok/s 48036 (55059)	Loss/tok 3.4397 (4.8581)	LR 2.000e-03
0: TRAIN [0][1430/1938]	Time 0.213 (0.254)	Data 1.27e-04 (7.05e-04)	Tok/s 48720 (55045)	Loss/tok 3.2855 (4.8497)	LR 2.000e-03
0: TRAIN [0][1440/1938]	Time 0.276 (0.254)	Data 1.29e-04 (7.01e-04)	Tok/s 60580 (55047)	Loss/tok 3.5603 (4.8410)	LR 2.000e-03
0: TRAIN [0][1450/1938]	Time 0.337 (0.254)	Data 1.50e-04 (6.97e-04)	Tok/s 69398 (55034)	Loss/tok 3.7778 (4.8328)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.214 (0.254)	Data 1.25e-04 (6.93e-04)	Tok/s 48826 (55039)	Loss/tok 3.3724 (4.8242)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.214 (0.254)	Data 1.15e-04 (6.89e-04)	Tok/s 48404 (55029)	Loss/tok 3.3577 (4.8160)	LR 2.000e-03
0: TRAIN [0][1480/1938]	Time 0.214 (0.254)	Data 1.40e-04 (6.85e-04)	Tok/s 48484 (55025)	Loss/tok 3.4708 (4.8082)	LR 2.000e-03
0: TRAIN [0][1490/1938]	Time 0.274 (0.254)	Data 1.14e-04 (6.82e-04)	Tok/s 60878 (55039)	Loss/tok 3.6155 (4.8000)	LR 2.000e-03
0: TRAIN [0][1500/1938]	Time 0.276 (0.254)	Data 1.10e-04 (6.78e-04)	Tok/s 61117 (55032)	Loss/tok 3.4686 (4.7922)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.213 (0.254)	Data 1.41e-04 (6.75e-04)	Tok/s 49035 (55037)	Loss/tok 3.2584 (4.7843)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1520/1938]	Time 0.157 (0.254)	Data 1.32e-04 (6.71e-04)	Tok/s 32978 (55040)	Loss/tok 2.7776 (4.7767)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.213 (0.254)	Data 1.16e-04 (6.68e-04)	Tok/s 47783 (55069)	Loss/tok 3.3988 (4.7683)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.212 (0.254)	Data 1.38e-04 (6.64e-04)	Tok/s 47654 (55038)	Loss/tok 3.2037 (4.7614)	LR 2.000e-03
0: TRAIN [0][1550/1938]	Time 0.215 (0.254)	Data 1.21e-04 (6.61e-04)	Tok/s 48195 (55055)	Loss/tok 3.2617 (4.7535)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.214 (0.254)	Data 1.10e-04 (6.58e-04)	Tok/s 47912 (55072)	Loss/tok 3.2420 (4.7456)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.213 (0.254)	Data 1.26e-04 (6.54e-04)	Tok/s 48632 (55057)	Loss/tok 3.3224 (4.7386)	LR 2.000e-03
0: TRAIN [0][1580/1938]	Time 0.274 (0.254)	Data 1.68e-04 (6.51e-04)	Tok/s 61021 (55049)	Loss/tok 3.6487 (4.7317)	LR 2.000e-03
0: TRAIN [0][1590/1938]	Time 0.213 (0.254)	Data 1.35e-04 (6.48e-04)	Tok/s 47801 (55039)	Loss/tok 3.3780 (4.7247)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.214 (0.254)	Data 1.34e-04 (6.45e-04)	Tok/s 47875 (55028)	Loss/tok 3.4294 (4.7181)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.275 (0.254)	Data 1.48e-04 (6.41e-04)	Tok/s 61124 (55010)	Loss/tok 3.6899 (4.7116)	LR 2.000e-03
0: TRAIN [0][1620/1938]	Time 0.213 (0.254)	Data 1.31e-04 (6.38e-04)	Tok/s 49478 (55023)	Loss/tok 3.2882 (4.7045)	LR 2.000e-03
0: TRAIN [0][1630/1938]	Time 0.213 (0.254)	Data 1.23e-04 (6.35e-04)	Tok/s 47808 (55011)	Loss/tok 3.4092 (4.6982)	LR 2.000e-03
0: TRAIN [0][1640/1938]	Time 0.155 (0.254)	Data 1.15e-04 (6.32e-04)	Tok/s 34267 (55004)	Loss/tok 2.9190 (4.6917)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.213 (0.254)	Data 1.43e-04 (6.29e-04)	Tok/s 48279 (54990)	Loss/tok 3.2613 (4.6849)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.214 (0.254)	Data 1.42e-04 (6.26e-04)	Tok/s 47273 (54996)	Loss/tok 3.3607 (4.6780)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.411 (0.254)	Data 1.15e-04 (6.23e-04)	Tok/s 72825 (55007)	Loss/tok 3.8772 (4.6709)	LR 2.000e-03
0: TRAIN [0][1680/1938]	Time 0.155 (0.254)	Data 1.31e-04 (6.20e-04)	Tok/s 34051 (54981)	Loss/tok 2.9303 (4.6651)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1690/1938]	Time 0.274 (0.254)	Data 1.30e-04 (6.18e-04)	Tok/s 60946 (55013)	Loss/tok 3.6095 (4.6581)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.337 (0.254)	Data 1.30e-04 (6.15e-04)	Tok/s 68555 (55019)	Loss/tok 3.7700 (4.6515)	LR 2.000e-03
0: TRAIN [0][1710/1938]	Time 0.214 (0.254)	Data 1.38e-04 (6.12e-04)	Tok/s 48639 (55026)	Loss/tok 3.3309 (4.6448)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.336 (0.254)	Data 1.61e-04 (6.09e-04)	Tok/s 69142 (55022)	Loss/tok 3.8260 (4.6388)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.275 (0.254)	Data 4.11e-04 (6.07e-04)	Tok/s 61082 (55002)	Loss/tok 3.6336 (4.6328)	LR 2.000e-03
0: TRAIN [0][1740/1938]	Time 0.213 (0.254)	Data 1.45e-04 (6.04e-04)	Tok/s 48545 (55012)	Loss/tok 3.3557 (4.6263)	LR 2.000e-03
0: TRAIN [0][1750/1938]	Time 0.274 (0.254)	Data 1.43e-04 (6.02e-04)	Tok/s 60212 (55017)	Loss/tok 3.6000 (4.6202)	LR 2.000e-03
0: TRAIN [0][1760/1938]	Time 0.336 (0.254)	Data 1.19e-04 (5.99e-04)	Tok/s 69609 (55038)	Loss/tok 3.8352 (4.6137)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.275 (0.254)	Data 1.24e-04 (5.96e-04)	Tok/s 61925 (55029)	Loss/tok 3.6361 (4.6079)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.276 (0.254)	Data 1.32e-04 (5.94e-04)	Tok/s 60859 (55037)	Loss/tok 3.5371 (4.6020)	LR 2.000e-03
0: TRAIN [0][1790/1938]	Time 0.214 (0.254)	Data 1.38e-04 (5.91e-04)	Tok/s 48329 (55056)	Loss/tok 3.2495 (4.5959)	LR 2.000e-03
0: TRAIN [0][1800/1938]	Time 0.408 (0.254)	Data 1.34e-04 (5.89e-04)	Tok/s 73587 (55073)	Loss/tok 3.7495 (4.5895)	LR 2.000e-03
0: TRAIN [0][1810/1938]	Time 0.213 (0.254)	Data 1.34e-04 (5.86e-04)	Tok/s 48131 (55071)	Loss/tok 3.3890 (4.5836)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.412 (0.254)	Data 1.30e-04 (5.84e-04)	Tok/s 72684 (55075)	Loss/tok 3.9631 (4.5780)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.274 (0.254)	Data 1.31e-04 (5.81e-04)	Tok/s 61197 (55075)	Loss/tok 3.5552 (4.5723)	LR 2.000e-03
0: TRAIN [0][1840/1938]	Time 0.212 (0.254)	Data 1.26e-04 (5.79e-04)	Tok/s 49199 (55035)	Loss/tok 3.3021 (4.5674)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.156 (0.254)	Data 1.11e-04 (5.76e-04)	Tok/s 33242 (55029)	Loss/tok 2.8038 (4.5619)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.212 (0.254)	Data 1.48e-04 (5.74e-04)	Tok/s 48033 (55024)	Loss/tok 3.2784 (4.5566)	LR 2.000e-03
0: TRAIN [0][1870/1938]	Time 0.276 (0.254)	Data 1.18e-04 (5.71e-04)	Tok/s 61244 (55028)	Loss/tok 3.5351 (4.5510)	LR 2.000e-03
0: TRAIN [0][1880/1938]	Time 0.213 (0.254)	Data 1.33e-04 (5.69e-04)	Tok/s 47777 (55029)	Loss/tok 3.3916 (4.5456)	LR 2.000e-03
0: TRAIN [0][1890/1938]	Time 0.276 (0.254)	Data 1.31e-04 (5.67e-04)	Tok/s 60758 (55029)	Loss/tok 3.4422 (4.5405)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1900/1938]	Time 0.214 (0.254)	Data 1.52e-04 (5.65e-04)	Tok/s 47274 (55041)	Loss/tok 3.3411 (4.5350)	LR 2.000e-03
0: TRAIN [0][1910/1938]	Time 0.156 (0.254)	Data 1.14e-04 (5.63e-04)	Tok/s 33796 (55040)	Loss/tok 2.9096 (4.5299)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.276 (0.254)	Data 1.20e-04 (5.60e-04)	Tok/s 61271 (55046)	Loss/tok 3.5311 (4.5244)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.276 (0.254)	Data 1.16e-04 (5.58e-04)	Tok/s 61405 (55041)	Loss/tok 3.4838 (4.5191)	LR 2.000e-03
:::MLL 1570029169.422 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1570029169.422 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.753 (0.753)	Decoder iters 149.0 (149.0)	Tok/s 21495 (21495)
0: Running moses detokenizer
0: BLEU(score=19.97161846727586, counts=[34602, 15837, 8433, 4718], totals=[65440, 62437, 59434, 56434], precisions=[52.87591687041565, 25.364767685827314, 14.188848134064676, 8.360208385016126], bp=1.0, sys_len=65440, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1570029171.614 eval_accuracy: {"value": 19.97, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1570029171.615 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 4.5156	Test BLEU: 19.97
0: Performance: Epoch: 0	Training: 440185 Tok/s
0: Finished epoch 0
:::MLL 1570029171.615 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1570029171.615 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1570029171.616 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 2049654140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][0/1938]	Time 1.037 (1.037)	Data 7.02e-01 (7.02e-01)	Tok/s 22235 (22235)	Loss/tok 3.6844 (3.6844)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.275 (0.326)	Data 1.16e-04 (6.40e-02)	Tok/s 60927 (49769)	Loss/tok 3.5053 (3.5670)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.214 (0.273)	Data 1.13e-04 (3.36e-02)	Tok/s 48831 (48844)	Loss/tok 3.2472 (3.4484)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.212 (0.268)	Data 1.04e-04 (2.28e-02)	Tok/s 46901 (50894)	Loss/tok 3.3231 (3.4566)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.337 (0.273)	Data 1.21e-04 (1.72e-02)	Tok/s 68809 (53184)	Loss/tok 3.5990 (3.4814)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.213 (0.268)	Data 1.24e-04 (1.39e-02)	Tok/s 48538 (53373)	Loss/tok 3.3338 (3.4664)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.213 (0.265)	Data 1.02e-04 (1.16e-02)	Tok/s 49396 (53669)	Loss/tok 3.2288 (3.4592)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.338 (0.265)	Data 1.10e-04 (1.00e-02)	Tok/s 69179 (54218)	Loss/tok 3.6846 (3.4689)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.213 (0.265)	Data 1.07e-04 (8.79e-03)	Tok/s 48429 (54406)	Loss/tok 3.2081 (3.4685)	LR 2.000e-03
0: TRAIN [1][90/1938]	Time 0.337 (0.262)	Data 9.99e-05 (7.84e-03)	Tok/s 70015 (54090)	Loss/tok 3.5385 (3.4603)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.276 (0.261)	Data 9.89e-05 (7.07e-03)	Tok/s 60403 (54167)	Loss/tok 3.5502 (3.4560)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.275 (0.259)	Data 1.06e-04 (6.45e-03)	Tok/s 60962 (54212)	Loss/tok 3.5024 (3.4501)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.273 (0.258)	Data 1.08e-04 (5.92e-03)	Tok/s 61421 (54042)	Loss/tok 3.4322 (3.4484)	LR 2.000e-03
0: TRAIN [1][130/1938]	Time 0.275 (0.258)	Data 5.46e-04 (5.48e-03)	Tok/s 61510 (54272)	Loss/tok 3.4900 (3.4509)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.213 (0.258)	Data 9.47e-05 (5.10e-03)	Tok/s 48550 (54423)	Loss/tok 3.1890 (3.4528)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.212 (0.259)	Data 9.49e-05 (4.77e-03)	Tok/s 48587 (54719)	Loss/tok 3.1590 (3.4544)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.277 (0.257)	Data 1.23e-04 (4.49e-03)	Tok/s 60682 (54483)	Loss/tok 3.4367 (3.4453)	LR 2.000e-03
0: TRAIN [1][170/1938]	Time 0.275 (0.256)	Data 1.22e-04 (4.23e-03)	Tok/s 60666 (54302)	Loss/tok 3.5805 (3.4463)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.212 (0.255)	Data 1.01e-04 (4.00e-03)	Tok/s 48345 (54208)	Loss/tok 3.2401 (3.4434)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.274 (0.255)	Data 1.18e-04 (3.80e-03)	Tok/s 61698 (54111)	Loss/tok 3.4329 (3.4434)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.213 (0.255)	Data 1.05e-04 (3.62e-03)	Tok/s 48248 (54305)	Loss/tok 3.2386 (3.4443)	LR 2.000e-03
0: TRAIN [1][210/1938]	Time 0.275 (0.255)	Data 1.53e-04 (3.45e-03)	Tok/s 61990 (54218)	Loss/tok 3.4876 (3.4452)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.213 (0.255)	Data 1.10e-04 (3.30e-03)	Tok/s 48354 (54212)	Loss/tok 3.2036 (3.4470)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.213 (0.257)	Data 1.19e-04 (3.16e-03)	Tok/s 49562 (54537)	Loss/tok 3.2961 (3.4553)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.338 (0.257)	Data 1.29e-04 (3.04e-03)	Tok/s 69013 (54621)	Loss/tok 3.6360 (3.4542)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.276 (0.257)	Data 1.02e-04 (2.92e-03)	Tok/s 61119 (54706)	Loss/tok 3.3892 (3.4529)	LR 2.000e-03
0: TRAIN [1][260/1938]	Time 0.276 (0.257)	Data 9.89e-05 (2.82e-03)	Tok/s 61645 (54776)	Loss/tok 3.4713 (3.4517)	LR 2.000e-03
0: TRAIN [1][270/1938]	Time 0.276 (0.255)	Data 1.03e-04 (2.72e-03)	Tok/s 61549 (54464)	Loss/tok 3.4904 (3.4463)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.277 (0.256)	Data 1.08e-04 (2.63e-03)	Tok/s 60703 (54512)	Loss/tok 3.3807 (3.4469)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.213 (0.255)	Data 1.02e-04 (2.54e-03)	Tok/s 47691 (54488)	Loss/tok 3.2286 (3.4453)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.157 (0.254)	Data 9.61e-05 (2.46e-03)	Tok/s 33096 (54209)	Loss/tok 2.7501 (3.4408)	LR 2.000e-03
0: TRAIN [1][310/1938]	Time 0.275 (0.254)	Data 9.73e-05 (2.38e-03)	Tok/s 61075 (54259)	Loss/tok 3.5412 (3.4397)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.276 (0.253)	Data 9.85e-05 (2.31e-03)	Tok/s 61600 (54214)	Loss/tok 3.4268 (3.4373)	LR 2.000e-03
0: TRAIN [1][330/1938]	Time 0.158 (0.252)	Data 9.66e-05 (2.25e-03)	Tok/s 32982 (54122)	Loss/tok 2.6469 (3.4349)	LR 2.000e-03
0: TRAIN [1][340/1938]	Time 0.156 (0.251)	Data 1.36e-04 (2.18e-03)	Tok/s 33903 (53929)	Loss/tok 2.8960 (3.4309)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.275 (0.252)	Data 1.19e-04 (2.12e-03)	Tok/s 60452 (54093)	Loss/tok 3.3477 (3.4347)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.338 (0.253)	Data 9.75e-05 (2.07e-03)	Tok/s 68692 (54188)	Loss/tok 3.7741 (3.4368)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.337 (0.253)	Data 9.82e-05 (2.02e-03)	Tok/s 69938 (54290)	Loss/tok 3.5427 (3.4370)	LR 2.000e-03
0: TRAIN [1][380/1938]	Time 0.276 (0.253)	Data 9.94e-05 (1.97e-03)	Tok/s 60926 (54305)	Loss/tok 3.5399 (3.4385)	LR 2.000e-03
0: TRAIN [1][390/1938]	Time 0.213 (0.253)	Data 1.17e-04 (1.92e-03)	Tok/s 49249 (54164)	Loss/tok 3.3058 (3.4359)	LR 2.000e-03
0: TRAIN [1][400/1938]	Time 0.214 (0.253)	Data 1.00e-04 (1.87e-03)	Tok/s 48269 (54291)	Loss/tok 3.2277 (3.4361)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.157 (0.253)	Data 9.92e-05 (1.83e-03)	Tok/s 33573 (54228)	Loss/tok 2.6578 (3.4354)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.214 (0.252)	Data 1.20e-04 (1.79e-03)	Tok/s 48405 (54013)	Loss/tok 3.1688 (3.4311)	LR 2.000e-03
0: TRAIN [1][430/1938]	Time 0.213 (0.251)	Data 1.20e-04 (1.75e-03)	Tok/s 48409 (54016)	Loss/tok 3.2165 (3.4295)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][440/1938]	Time 0.213 (0.251)	Data 1.05e-04 (1.71e-03)	Tok/s 48263 (54019)	Loss/tok 3.3186 (3.4275)	LR 2.000e-03
0: TRAIN [1][450/1938]	Time 0.338 (0.252)	Data 1.02e-04 (1.68e-03)	Tok/s 68932 (54093)	Loss/tok 3.5741 (3.4271)	LR 2.000e-03
0: TRAIN [1][460/1938]	Time 0.276 (0.251)	Data 1.08e-04 (1.65e-03)	Tok/s 60723 (53954)	Loss/tok 3.5200 (3.4250)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.156 (0.251)	Data 9.94e-05 (1.61e-03)	Tok/s 33500 (53938)	Loss/tok 2.8299 (3.4244)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.213 (0.250)	Data 1.02e-04 (1.58e-03)	Tok/s 48095 (53870)	Loss/tok 3.1155 (3.4223)	LR 2.000e-03
0: TRAIN [1][490/1938]	Time 0.411 (0.251)	Data 1.19e-04 (1.55e-03)	Tok/s 71461 (53951)	Loss/tok 3.9507 (3.4242)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.275 (0.251)	Data 9.80e-05 (1.52e-03)	Tok/s 61542 (54038)	Loss/tok 3.4544 (3.4255)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.213 (0.251)	Data 1.01e-04 (1.50e-03)	Tok/s 48643 (54107)	Loss/tok 3.1571 (3.4250)	LR 2.000e-03
0: TRAIN [1][520/1938]	Time 0.336 (0.252)	Data 1.01e-04 (1.47e-03)	Tok/s 69697 (54196)	Loss/tok 3.6091 (3.4274)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.214 (0.251)	Data 1.25e-04 (1.44e-03)	Tok/s 48980 (54146)	Loss/tok 3.3132 (3.4257)	LR 2.000e-03
0: TRAIN [1][540/1938]	Time 0.212 (0.252)	Data 1.29e-04 (1.42e-03)	Tok/s 48407 (54180)	Loss/tok 3.1533 (3.4268)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.213 (0.252)	Data 1.29e-04 (1.40e-03)	Tok/s 47679 (54296)	Loss/tok 3.2391 (3.4296)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.213 (0.253)	Data 1.04e-04 (1.37e-03)	Tok/s 48262 (54349)	Loss/tok 3.2804 (3.4315)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][570/1938]	Time 0.158 (0.252)	Data 1.01e-04 (1.35e-03)	Tok/s 33276 (54322)	Loss/tok 2.6688 (3.4316)	LR 2.000e-03
0: TRAIN [1][580/1938]	Time 0.213 (0.252)	Data 1.15e-04 (1.33e-03)	Tok/s 49595 (54295)	Loss/tok 3.1519 (3.4303)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.213 (0.252)	Data 9.54e-05 (1.31e-03)	Tok/s 48995 (54278)	Loss/tok 3.2389 (3.4297)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.335 (0.252)	Data 1.02e-04 (1.29e-03)	Tok/s 69490 (54239)	Loss/tok 3.5934 (3.4302)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.213 (0.252)	Data 9.75e-05 (1.27e-03)	Tok/s 49006 (54165)	Loss/tok 3.2695 (3.4287)	LR 2.000e-03
0: TRAIN [1][620/1938]	Time 0.214 (0.251)	Data 9.97e-05 (1.25e-03)	Tok/s 47493 (54167)	Loss/tok 3.1069 (3.4275)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.214 (0.251)	Data 9.92e-05 (1.23e-03)	Tok/s 48926 (54163)	Loss/tok 3.1349 (3.4273)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.157 (0.251)	Data 1.40e-04 (1.22e-03)	Tok/s 33506 (54122)	Loss/tok 2.8349 (3.4263)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.213 (0.251)	Data 1.03e-04 (1.20e-03)	Tok/s 48133 (54116)	Loss/tok 3.1124 (3.4249)	LR 2.000e-03
0: TRAIN [1][660/1938]	Time 0.157 (0.251)	Data 9.92e-05 (1.18e-03)	Tok/s 34156 (54027)	Loss/tok 2.8289 (3.4236)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.213 (0.251)	Data 1.24e-04 (1.17e-03)	Tok/s 47883 (54051)	Loss/tok 3.1216 (3.4228)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.156 (0.251)	Data 9.61e-05 (1.15e-03)	Tok/s 33555 (54090)	Loss/tok 2.7239 (3.4240)	LR 2.000e-03
0: TRAIN [1][690/1938]	Time 0.275 (0.251)	Data 9.75e-05 (1.14e-03)	Tok/s 61751 (54016)	Loss/tok 3.3389 (3.4219)	LR 2.000e-03
0: TRAIN [1][700/1938]	Time 0.213 (0.250)	Data 1.34e-04 (1.12e-03)	Tok/s 47880 (54007)	Loss/tok 3.3086 (3.4219)	LR 2.000e-03
0: TRAIN [1][710/1938]	Time 0.276 (0.251)	Data 1.03e-04 (1.11e-03)	Tok/s 60542 (54037)	Loss/tok 3.4277 (3.4216)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.277 (0.251)	Data 9.78e-05 (1.10e-03)	Tok/s 60610 (54045)	Loss/tok 3.4505 (3.4214)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.276 (0.251)	Data 1.01e-04 (1.08e-03)	Tok/s 61187 (54076)	Loss/tok 3.4513 (3.4208)	LR 2.000e-03
0: TRAIN [1][740/1938]	Time 0.338 (0.251)	Data 9.94e-05 (1.07e-03)	Tok/s 68903 (54180)	Loss/tok 3.6865 (3.4240)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][750/1938]	Time 0.213 (0.252)	Data 1.02e-04 (1.06e-03)	Tok/s 47905 (54208)	Loss/tok 3.3144 (3.4255)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.213 (0.252)	Data 1.03e-04 (1.04e-03)	Tok/s 47134 (54239)	Loss/tok 3.2023 (3.4250)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.213 (0.252)	Data 9.82e-05 (1.03e-03)	Tok/s 48247 (54224)	Loss/tok 3.2119 (3.4237)	LR 2.000e-03
0: TRAIN [1][780/1938]	Time 0.276 (0.252)	Data 9.56e-05 (1.02e-03)	Tok/s 60574 (54309)	Loss/tok 3.4280 (3.4254)	LR 2.000e-03
0: TRAIN [1][790/1938]	Time 0.213 (0.252)	Data 1.04e-04 (1.01e-03)	Tok/s 48633 (54321)	Loss/tok 3.2962 (3.4245)	LR 2.000e-03
0: TRAIN [1][800/1938]	Time 0.275 (0.252)	Data 9.97e-05 (9.97e-04)	Tok/s 62075 (54374)	Loss/tok 3.3756 (3.4238)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.213 (0.252)	Data 1.31e-04 (9.86e-04)	Tok/s 48106 (54350)	Loss/tok 3.1544 (3.4241)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.214 (0.252)	Data 1.66e-04 (9.75e-04)	Tok/s 49167 (54410)	Loss/tok 3.1716 (3.4249)	LR 2.000e-03
0: TRAIN [1][830/1938]	Time 0.277 (0.252)	Data 1.00e-04 (9.65e-04)	Tok/s 61390 (54448)	Loss/tok 3.4080 (3.4239)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.213 (0.253)	Data 1.14e-04 (9.55e-04)	Tok/s 48947 (54485)	Loss/tok 3.1827 (3.4251)	LR 2.000e-03
0: TRAIN [1][850/1938]	Time 0.213 (0.253)	Data 1.00e-04 (9.45e-04)	Tok/s 49377 (54471)	Loss/tok 3.2266 (3.4238)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.337 (0.253)	Data 1.03e-04 (9.36e-04)	Tok/s 68116 (54584)	Loss/tok 3.6507 (3.4252)	LR 2.000e-03
0: TRAIN [1][870/1938]	Time 0.213 (0.253)	Data 9.63e-05 (9.27e-04)	Tok/s 48880 (54585)	Loss/tok 3.2655 (3.4246)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][880/1938]	Time 0.275 (0.253)	Data 1.25e-04 (9.18e-04)	Tok/s 60133 (54624)	Loss/tok 3.2799 (3.4248)	LR 2.000e-03
0: TRAIN [1][890/1938]	Time 0.213 (0.253)	Data 9.89e-05 (9.08e-04)	Tok/s 48835 (54591)	Loss/tok 3.2012 (3.4237)	LR 2.000e-03
0: TRAIN [1][900/1938]	Time 0.273 (0.253)	Data 9.70e-05 (9.00e-04)	Tok/s 62135 (54611)	Loss/tok 3.3123 (3.4232)	LR 2.000e-03
0: TRAIN [1][910/1938]	Time 0.214 (0.253)	Data 6.02e-04 (8.91e-04)	Tok/s 48841 (54613)	Loss/tok 3.2446 (3.4220)	LR 2.000e-03
0: TRAIN [1][920/1938]	Time 0.213 (0.253)	Data 1.02e-04 (8.83e-04)	Tok/s 49126 (54598)	Loss/tok 3.1447 (3.4215)	LR 2.000e-03
0: TRAIN [1][930/1938]	Time 0.275 (0.253)	Data 9.97e-05 (8.74e-04)	Tok/s 60469 (54551)	Loss/tok 3.4192 (3.4202)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.276 (0.253)	Data 9.97e-05 (8.67e-04)	Tok/s 61371 (54572)	Loss/tok 3.3535 (3.4203)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.213 (0.253)	Data 1.10e-04 (8.59e-04)	Tok/s 47182 (54557)	Loss/tok 3.2186 (3.4194)	LR 2.000e-03
0: TRAIN [1][960/1938]	Time 0.214 (0.253)	Data 1.04e-04 (8.51e-04)	Tok/s 47934 (54572)	Loss/tok 3.0558 (3.4196)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.337 (0.253)	Data 9.97e-05 (8.44e-04)	Tok/s 70603 (54528)	Loss/tok 3.4007 (3.4183)	LR 2.000e-03
0: TRAIN [1][980/1938]	Time 0.213 (0.253)	Data 9.97e-05 (8.36e-04)	Tok/s 48993 (54554)	Loss/tok 3.1508 (3.4189)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.339 (0.253)	Data 1.40e-04 (8.29e-04)	Tok/s 68902 (54524)	Loss/tok 3.5148 (3.4195)	LR 2.000e-03
0: TRAIN [1][1000/1938]	Time 0.215 (0.253)	Data 1.23e-04 (8.22e-04)	Tok/s 47501 (54555)	Loss/tok 3.2036 (3.4201)	LR 2.000e-03
0: TRAIN [1][1010/1938]	Time 0.413 (0.253)	Data 1.01e-04 (8.15e-04)	Tok/s 71344 (54563)	Loss/tok 3.8782 (3.4204)	LR 2.000e-03
0: TRAIN [1][1020/1938]	Time 0.338 (0.253)	Data 1.10e-04 (8.08e-04)	Tok/s 69584 (54599)	Loss/tok 3.4079 (3.4202)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.338 (0.253)	Data 1.10e-04 (8.02e-04)	Tok/s 69270 (54620)	Loss/tok 3.5020 (3.4205)	LR 2.000e-03
0: TRAIN [1][1040/1938]	Time 0.274 (0.253)	Data 1.00e-04 (7.95e-04)	Tok/s 62766 (54651)	Loss/tok 3.2514 (3.4198)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.337 (0.253)	Data 9.99e-05 (7.89e-04)	Tok/s 69353 (54627)	Loss/tok 3.6368 (3.4190)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.338 (0.254)	Data 9.97e-05 (7.82e-04)	Tok/s 67403 (54646)	Loss/tok 3.7661 (3.4200)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.213 (0.254)	Data 1.00e-04 (7.76e-04)	Tok/s 48638 (54663)	Loss/tok 3.1922 (3.4197)	LR 2.000e-03
0: TRAIN [1][1080/1938]	Time 0.214 (0.253)	Data 1.06e-04 (7.70e-04)	Tok/s 47749 (54629)	Loss/tok 3.1479 (3.4186)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.276 (0.253)	Data 1.17e-04 (7.64e-04)	Tok/s 61243 (54648)	Loss/tok 3.3524 (3.4180)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.412 (0.254)	Data 1.18e-04 (7.58e-04)	Tok/s 73063 (54714)	Loss/tok 3.7112 (3.4190)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1110/1938]	Time 0.213 (0.254)	Data 1.16e-04 (7.52e-04)	Tok/s 48073 (54722)	Loss/tok 3.2045 (3.4183)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.212 (0.253)	Data 1.00e-04 (7.46e-04)	Tok/s 48282 (54700)	Loss/tok 3.2299 (3.4171)	LR 2.000e-03
0: TRAIN [1][1130/1938]	Time 0.213 (0.253)	Data 9.87e-05 (7.41e-04)	Tok/s 49994 (54700)	Loss/tok 3.1635 (3.4161)	LR 2.000e-03
0: TRAIN [1][1140/1938]	Time 0.413 (0.254)	Data 1.05e-04 (7.35e-04)	Tok/s 71373 (54796)	Loss/tok 3.5829 (3.4174)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.213 (0.254)	Data 3.56e-04 (7.30e-04)	Tok/s 47830 (54826)	Loss/tok 3.0738 (3.4173)	LR 2.000e-03
0: TRAIN [1][1160/1938]	Time 0.338 (0.254)	Data 1.02e-04 (7.25e-04)	Tok/s 68598 (54860)	Loss/tok 3.6223 (3.4173)	LR 2.000e-03
0: TRAIN [1][1170/1938]	Time 0.158 (0.254)	Data 1.19e-04 (7.19e-04)	Tok/s 33938 (54850)	Loss/tok 2.7023 (3.4171)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.337 (0.254)	Data 1.04e-04 (7.14e-04)	Tok/s 69473 (54866)	Loss/tok 3.5964 (3.4170)	LR 2.000e-03
0: TRAIN [1][1190/1938]	Time 0.156 (0.254)	Data 1.23e-04 (7.09e-04)	Tok/s 33829 (54855)	Loss/tok 2.6053 (3.4168)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.337 (0.254)	Data 1.00e-04 (7.04e-04)	Tok/s 70129 (54869)	Loss/tok 3.5589 (3.4166)	LR 2.000e-03
0: TRAIN [1][1210/1938]	Time 0.337 (0.254)	Data 1.26e-04 (6.99e-04)	Tok/s 68774 (54884)	Loss/tok 3.6829 (3.4166)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.213 (0.254)	Data 1.19e-04 (6.95e-04)	Tok/s 48097 (54856)	Loss/tok 3.1558 (3.4169)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1230/1938]	Time 0.215 (0.254)	Data 1.01e-04 (6.90e-04)	Tok/s 47853 (54873)	Loss/tok 3.1425 (3.4168)	LR 2.000e-03
0: TRAIN [1][1240/1938]	Time 0.214 (0.254)	Data 1.03e-04 (6.85e-04)	Tok/s 49129 (54834)	Loss/tok 3.0964 (3.4158)	LR 2.000e-03
0: TRAIN [1][1250/1938]	Time 0.213 (0.254)	Data 9.97e-05 (6.81e-04)	Tok/s 49590 (54830)	Loss/tok 3.1881 (3.4150)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.157 (0.254)	Data 1.01e-04 (6.76e-04)	Tok/s 33413 (54806)	Loss/tok 2.7167 (3.4143)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.214 (0.254)	Data 1.31e-04 (6.72e-04)	Tok/s 48430 (54813)	Loss/tok 3.2456 (3.4135)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.275 (0.254)	Data 9.87e-05 (6.68e-04)	Tok/s 60759 (54821)	Loss/tok 3.3339 (3.4132)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.156 (0.254)	Data 1.05e-04 (6.64e-04)	Tok/s 33471 (54809)	Loss/tok 2.6237 (3.4133)	LR 2.000e-03
0: TRAIN [1][1300/1938]	Time 0.213 (0.254)	Data 1.02e-04 (6.60e-04)	Tok/s 48036 (54861)	Loss/tok 3.3035 (3.4150)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.212 (0.255)	Data 1.03e-04 (6.56e-04)	Tok/s 48390 (54876)	Loss/tok 3.1643 (3.4151)	LR 2.000e-03
0: TRAIN [1][1320/1938]	Time 0.214 (0.255)	Data 1.55e-04 (6.53e-04)	Tok/s 49147 (54890)	Loss/tok 3.1453 (3.4146)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.213 (0.255)	Data 1.17e-04 (6.49e-04)	Tok/s 47126 (54902)	Loss/tok 3.0992 (3.4140)	LR 2.000e-03
0: TRAIN [1][1340/1938]	Time 0.213 (0.255)	Data 1.02e-04 (6.45e-04)	Tok/s 48789 (54930)	Loss/tok 3.2933 (3.4142)	LR 2.000e-03
0: TRAIN [1][1350/1938]	Time 0.213 (0.255)	Data 1.71e-04 (6.41e-04)	Tok/s 48037 (54958)	Loss/tok 3.1286 (3.4145)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.275 (0.255)	Data 1.03e-04 (6.37e-04)	Tok/s 61930 (54967)	Loss/tok 3.3420 (3.4141)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.339 (0.255)	Data 1.03e-04 (6.33e-04)	Tok/s 69125 (54985)	Loss/tok 3.5825 (3.4146)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.213 (0.255)	Data 9.89e-05 (6.30e-04)	Tok/s 48461 (54977)	Loss/tok 3.1892 (3.4147)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.157 (0.255)	Data 1.34e-04 (6.26e-04)	Tok/s 33588 (54966)	Loss/tok 2.7810 (3.4142)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.336 (0.255)	Data 1.03e-04 (6.23e-04)	Tok/s 68884 (54984)	Loss/tok 3.6007 (3.4143)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.277 (0.255)	Data 1.00e-04 (6.19e-04)	Tok/s 60936 (55015)	Loss/tok 3.3842 (3.4150)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.213 (0.255)	Data 4.84e-04 (6.16e-04)	Tok/s 47570 (54993)	Loss/tok 3.2306 (3.4144)	LR 2.000e-03
0: TRAIN [1][1430/1938]	Time 0.277 (0.255)	Data 1.76e-04 (6.13e-04)	Tok/s 60961 (55000)	Loss/tok 3.4238 (3.4139)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.337 (0.255)	Data 1.32e-04 (6.10e-04)	Tok/s 69246 (54993)	Loss/tok 3.6211 (3.4133)	LR 2.000e-03
0: TRAIN [1][1450/1938]	Time 0.412 (0.255)	Data 1.03e-04 (6.06e-04)	Tok/s 72153 (55001)	Loss/tok 3.7378 (3.4133)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.274 (0.255)	Data 1.05e-04 (6.03e-04)	Tok/s 61145 (55037)	Loss/tok 3.3863 (3.4130)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.213 (0.255)	Data 1.01e-04 (6.00e-04)	Tok/s 48239 (55009)	Loss/tok 3.1237 (3.4120)	LR 2.000e-03
0: TRAIN [1][1480/1938]	Time 0.276 (0.255)	Data 9.97e-05 (5.96e-04)	Tok/s 60954 (54998)	Loss/tok 3.4336 (3.4112)	LR 2.000e-03
0: TRAIN [1][1490/1938]	Time 0.411 (0.255)	Data 1.50e-04 (5.93e-04)	Tok/s 73092 (54992)	Loss/tok 3.7095 (3.4109)	LR 2.000e-03
0: TRAIN [1][1500/1938]	Time 0.213 (0.255)	Data 9.66e-05 (5.90e-04)	Tok/s 48956 (55013)	Loss/tok 3.1907 (3.4108)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.275 (0.255)	Data 1.08e-04 (5.87e-04)	Tok/s 61680 (55020)	Loss/tok 3.4002 (3.4103)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.277 (0.255)	Data 1.04e-04 (5.84e-04)	Tok/s 59701 (55045)	Loss/tok 3.4185 (3.4100)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.213 (0.255)	Data 1.19e-04 (5.81e-04)	Tok/s 48592 (55057)	Loss/tok 3.2199 (3.4094)	LR 2.000e-03
0: TRAIN [1][1540/1938]	Time 0.213 (0.255)	Data 1.08e-04 (5.78e-04)	Tok/s 46934 (55033)	Loss/tok 3.0803 (3.4086)	LR 2.000e-03
0: TRAIN [1][1550/1938]	Time 0.213 (0.255)	Data 1.06e-04 (5.75e-04)	Tok/s 48888 (55021)	Loss/tok 3.0563 (3.4079)	LR 2.000e-03
0: TRAIN [1][1560/1938]	Time 0.277 (0.255)	Data 1.06e-04 (5.72e-04)	Tok/s 60798 (55046)	Loss/tok 3.3703 (3.4080)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.157 (0.255)	Data 1.22e-04 (5.70e-04)	Tok/s 33297 (55004)	Loss/tok 2.7878 (3.4070)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1580/1938]	Time 0.339 (0.255)	Data 1.06e-04 (5.67e-04)	Tok/s 68298 (55059)	Loss/tok 3.5695 (3.4075)	LR 2.000e-03
0: TRAIN [1][1590/1938]	Time 0.411 (0.255)	Data 1.00e-04 (5.64e-04)	Tok/s 73015 (55036)	Loss/tok 3.5946 (3.4069)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.276 (0.255)	Data 1.03e-04 (5.61e-04)	Tok/s 59927 (55074)	Loss/tok 3.3553 (3.4070)	LR 2.000e-03
0: TRAIN [1][1610/1938]	Time 0.277 (0.255)	Data 1.02e-04 (5.59e-04)	Tok/s 60783 (55047)	Loss/tok 3.4205 (3.4060)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.213 (0.255)	Data 5.27e-04 (5.56e-04)	Tok/s 47960 (55026)	Loss/tok 3.1285 (3.4055)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.213 (0.255)	Data 1.01e-04 (5.53e-04)	Tok/s 48124 (54993)	Loss/tok 3.1997 (3.4044)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.411 (0.255)	Data 1.17e-04 (5.51e-04)	Tok/s 73111 (54986)	Loss/tok 3.6771 (3.4045)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.337 (0.255)	Data 9.82e-05 (5.48e-04)	Tok/s 68544 (54985)	Loss/tok 3.6393 (3.4042)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.214 (0.255)	Data 2.81e-04 (5.46e-04)	Tok/s 47533 (54987)	Loss/tok 3.2191 (3.4038)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.279 (0.255)	Data 1.21e-04 (5.43e-04)	Tok/s 59371 (55009)	Loss/tok 3.3100 (3.4040)	LR 2.000e-03
0: TRAIN [1][1680/1938]	Time 0.277 (0.255)	Data 1.18e-04 (5.41e-04)	Tok/s 60857 (54999)	Loss/tok 3.3957 (3.4033)	LR 2.000e-03
0: TRAIN [1][1690/1938]	Time 0.338 (0.255)	Data 1.03e-04 (5.38e-04)	Tok/s 68189 (54996)	Loss/tok 3.4538 (3.4029)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.338 (0.255)	Data 1.67e-04 (5.36e-04)	Tok/s 69180 (54976)	Loss/tok 3.5461 (3.4023)	LR 2.000e-03
0: TRAIN [1][1710/1938]	Time 0.213 (0.255)	Data 9.99e-05 (5.33e-04)	Tok/s 48007 (54975)	Loss/tok 3.2202 (3.4015)	LR 2.000e-03
0: TRAIN [1][1720/1938]	Time 0.273 (0.255)	Data 1.13e-04 (5.31e-04)	Tok/s 62142 (54960)	Loss/tok 3.2979 (3.4006)	LR 2.000e-03
0: TRAIN [1][1730/1938]	Time 0.277 (0.255)	Data 1.04e-04 (5.29e-04)	Tok/s 59918 (54995)	Loss/tok 3.4036 (3.4007)	LR 2.000e-03
0: TRAIN [1][1740/1938]	Time 0.214 (0.255)	Data 1.02e-04 (5.26e-04)	Tok/s 47910 (54965)	Loss/tok 3.2214 (3.4001)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.337 (0.255)	Data 1.37e-04 (5.24e-04)	Tok/s 69095 (54972)	Loss/tok 3.3773 (3.3999)	LR 2.000e-03
0: TRAIN [1][1760/1938]	Time 0.276 (0.255)	Data 1.54e-04 (5.21e-04)	Tok/s 60832 (54944)	Loss/tok 3.4138 (3.3992)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1770/1938]	Time 0.213 (0.255)	Data 1.21e-04 (5.19e-04)	Tok/s 48458 (54940)	Loss/tok 3.2339 (3.3991)	LR 2.000e-03
0: TRAIN [1][1780/1938]	Time 0.213 (0.254)	Data 1.15e-04 (5.17e-04)	Tok/s 48464 (54884)	Loss/tok 3.1963 (3.3980)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1790/1938]	Time 0.275 (0.254)	Data 9.99e-05 (5.15e-04)	Tok/s 61123 (54902)	Loss/tok 3.3486 (3.3980)	LR 2.000e-03
0: TRAIN [1][1800/1938]	Time 0.276 (0.254)	Data 1.11e-04 (5.13e-04)	Tok/s 60333 (54900)	Loss/tok 3.4354 (3.3974)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.215 (0.255)	Data 1.20e-04 (5.10e-04)	Tok/s 47892 (54932)	Loss/tok 3.1977 (3.3976)	LR 2.000e-03
0: TRAIN [1][1820/1938]	Time 0.213 (0.255)	Data 1.05e-04 (5.08e-04)	Tok/s 47806 (54930)	Loss/tok 3.1270 (3.3977)	LR 2.000e-03
0: TRAIN [1][1830/1938]	Time 0.213 (0.255)	Data 1.00e-04 (5.06e-04)	Tok/s 48970 (54912)	Loss/tok 3.0471 (3.3969)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.214 (0.254)	Data 1.10e-04 (5.04e-04)	Tok/s 47182 (54900)	Loss/tok 3.0966 (3.3967)	LR 2.000e-03
0: TRAIN [1][1850/1938]	Time 0.213 (0.254)	Data 9.87e-05 (5.02e-04)	Tok/s 48659 (54881)	Loss/tok 3.0148 (3.3961)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.276 (0.254)	Data 1.01e-04 (5.00e-04)	Tok/s 61199 (54853)	Loss/tok 3.3291 (3.3955)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.275 (0.254)	Data 6.15e-04 (4.98e-04)	Tok/s 61555 (54851)	Loss/tok 3.3489 (3.3950)	LR 2.000e-03
0: TRAIN [1][1880/1938]	Time 0.276 (0.254)	Data 4.54e-04 (4.97e-04)	Tok/s 61598 (54838)	Loss/tok 3.2702 (3.3947)	LR 2.000e-03
0: TRAIN [1][1890/1938]	Time 0.275 (0.254)	Data 1.08e-04 (4.94e-04)	Tok/s 61784 (54866)	Loss/tok 3.3287 (3.3950)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.277 (0.254)	Data 1.49e-04 (4.93e-04)	Tok/s 60785 (54883)	Loss/tok 3.3374 (3.3946)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.275 (0.254)	Data 1.20e-04 (4.91e-04)	Tok/s 60924 (54890)	Loss/tok 3.3608 (3.3950)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.277 (0.255)	Data 1.03e-04 (4.89e-04)	Tok/s 61308 (54915)	Loss/tok 3.3277 (3.3951)	LR 2.000e-03
0: TRAIN [1][1930/1938]	Time 0.214 (0.255)	Data 1.02e-04 (4.87e-04)	Tok/s 48194 (54913)	Loss/tok 3.1211 (3.3951)	LR 2.000e-03
:::MLL 1570029665.786 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1570029665.787 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.757 (0.757)	Decoder iters 149.0 (149.0)	Tok/s 22047 (22047)
0: Running moses detokenizer
0: BLEU(score=22.007279172397944, counts=[36331, 17601, 9748, 5605], totals=[66718, 63715, 60712, 57714], precisions=[54.45456998111454, 27.624578199795966, 16.05613387798129, 9.711681740998717], bp=1.0, sys_len=66718, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1570029667.801 eval_accuracy: {"value": 22.01, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1570029667.801 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.3943	Test BLEU: 22.01
0: Performance: Epoch: 1	Training: 439289 Tok/s
0: Finished epoch 1
:::MLL 1570029667.801 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1570029667.802 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1570029667.802 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 1422287287
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][0/1938]	Time 0.960 (0.960)	Data 7.22e-01 (7.22e-01)	Tok/s 10740 (10740)	Loss/tok 3.0908 (3.0908)	LR 2.000e-03
0: TRAIN [2][10/1938]	Time 0.213 (0.332)	Data 9.73e-05 (6.58e-02)	Tok/s 48646 (54692)	Loss/tok 3.0501 (3.2570)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.213 (0.291)	Data 1.05e-04 (3.45e-02)	Tok/s 48356 (53704)	Loss/tok 3.0942 (3.2510)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.213 (0.272)	Data 9.61e-05 (2.34e-02)	Tok/s 49750 (53217)	Loss/tok 3.1071 (3.2120)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.277 (0.267)	Data 8.87e-05 (1.77e-02)	Tok/s 59701 (53544)	Loss/tok 3.2699 (3.2272)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.212 (0.261)	Data 9.37e-05 (1.43e-02)	Tok/s 48253 (53257)	Loss/tok 3.1343 (3.2285)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.155 (0.259)	Data 9.85e-05 (1.20e-02)	Tok/s 34570 (53439)	Loss/tok 2.7617 (3.2185)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.409 (0.255)	Data 1.17e-04 (1.03e-02)	Tok/s 73145 (53054)	Loss/tok 3.5024 (3.2125)	LR 2.000e-03
0: TRAIN [2][80/1938]	Time 0.337 (0.256)	Data 1.37e-04 (9.03e-03)	Tok/s 69525 (53466)	Loss/tok 3.4546 (3.2238)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.214 (0.256)	Data 1.29e-04 (8.05e-03)	Tok/s 48819 (53590)	Loss/tok 3.0995 (3.2336)	LR 2.000e-03
0: TRAIN [2][100/1938]	Time 0.275 (0.260)	Data 1.36e-04 (7.27e-03)	Tok/s 60683 (54429)	Loss/tok 3.1799 (3.2486)	LR 2.000e-03
0: TRAIN [2][110/1938]	Time 0.337 (0.258)	Data 1.17e-04 (6.63e-03)	Tok/s 69631 (54280)	Loss/tok 3.4280 (3.2433)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.213 (0.260)	Data 1.47e-04 (6.09e-03)	Tok/s 48252 (54656)	Loss/tok 3.1798 (3.2495)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.214 (0.263)	Data 1.17e-04 (5.64e-03)	Tok/s 49424 (55142)	Loss/tok 3.0069 (3.2620)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.276 (0.261)	Data 1.16e-04 (5.25e-03)	Tok/s 60738 (54917)	Loss/tok 3.1691 (3.2593)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.338 (0.262)	Data 1.32e-04 (4.91e-03)	Tok/s 69366 (55138)	Loss/tok 3.3998 (3.2636)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.214 (0.262)	Data 1.10e-04 (4.62e-03)	Tok/s 49771 (55255)	Loss/tok 3.1243 (3.2650)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.213 (0.260)	Data 1.24e-04 (4.35e-03)	Tok/s 49380 (55029)	Loss/tok 3.0737 (3.2634)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.213 (0.261)	Data 1.16e-04 (4.12e-03)	Tok/s 49161 (55095)	Loss/tok 2.9269 (3.2639)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.213 (0.262)	Data 1.21e-04 (3.91e-03)	Tok/s 49072 (55253)	Loss/tok 2.9775 (3.2655)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.337 (0.261)	Data 1.16e-04 (3.72e-03)	Tok/s 68465 (55245)	Loss/tok 3.3841 (3.2662)	LR 2.000e-03
0: TRAIN [2][210/1938]	Time 0.275 (0.262)	Data 1.08e-04 (3.55e-03)	Tok/s 60796 (55217)	Loss/tok 3.2672 (3.2673)	LR 2.000e-03
0: TRAIN [2][220/1938]	Time 0.277 (0.262)	Data 1.23e-04 (3.40e-03)	Tok/s 60265 (55274)	Loss/tok 3.3222 (3.2704)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.336 (0.261)	Data 1.23e-04 (3.26e-03)	Tok/s 69091 (55221)	Loss/tok 3.4387 (3.2685)	LR 2.000e-03
0: TRAIN [2][240/1938]	Time 0.337 (0.262)	Data 1.44e-04 (3.13e-03)	Tok/s 69791 (55293)	Loss/tok 3.3835 (3.2710)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.213 (0.261)	Data 1.19e-04 (3.01e-03)	Tok/s 48139 (55195)	Loss/tok 3.0554 (3.2681)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.214 (0.261)	Data 1.18e-04 (2.90e-03)	Tok/s 47755 (55245)	Loss/tok 2.9825 (3.2661)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.215 (0.260)	Data 1.38e-04 (2.79e-03)	Tok/s 48599 (55216)	Loss/tok 3.1250 (3.2643)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.275 (0.260)	Data 1.22e-04 (2.70e-03)	Tok/s 60008 (55143)	Loss/tok 3.2846 (3.2608)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.214 (0.260)	Data 1.14e-04 (2.61e-03)	Tok/s 49083 (55138)	Loss/tok 3.0058 (3.2621)	LR 2.000e-03
0: TRAIN [2][300/1938]	Time 0.214 (0.259)	Data 1.15e-04 (2.53e-03)	Tok/s 48705 (55075)	Loss/tok 3.0104 (3.2598)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.277 (0.258)	Data 2.53e-04 (2.45e-03)	Tok/s 61402 (54964)	Loss/tok 3.2188 (3.2584)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][320/1938]	Time 0.276 (0.259)	Data 1.12e-04 (2.38e-03)	Tok/s 60249 (55095)	Loss/tok 3.2667 (3.2614)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.158 (0.258)	Data 1.26e-04 (2.31e-03)	Tok/s 33187 (55075)	Loss/tok 2.6291 (3.2623)	LR 2.000e-03
0: TRAIN [2][340/1938]	Time 0.213 (0.258)	Data 1.27e-04 (2.25e-03)	Tok/s 48780 (55038)	Loss/tok 3.0778 (3.2612)	LR 2.000e-03
0: TRAIN [2][350/1938]	Time 0.214 (0.258)	Data 1.11e-04 (2.19e-03)	Tok/s 48229 (55055)	Loss/tok 3.0576 (3.2626)	LR 2.000e-03
0: TRAIN [2][360/1938]	Time 0.276 (0.257)	Data 1.09e-04 (2.13e-03)	Tok/s 61379 (54896)	Loss/tok 3.2517 (3.2589)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.277 (0.257)	Data 1.17e-04 (2.08e-03)	Tok/s 60100 (54901)	Loss/tok 3.3202 (3.2591)	LR 2.000e-03
0: TRAIN [2][380/1938]	Time 0.159 (0.256)	Data 1.45e-04 (2.03e-03)	Tok/s 32708 (54782)	Loss/tok 2.5728 (3.2560)	LR 2.000e-03
0: TRAIN [2][390/1938]	Time 0.213 (0.256)	Data 5.26e-04 (1.98e-03)	Tok/s 49205 (54847)	Loss/tok 3.1004 (3.2567)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.213 (0.256)	Data 1.15e-04 (1.93e-03)	Tok/s 48918 (54835)	Loss/tok 3.1274 (3.2565)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.213 (0.257)	Data 1.25e-04 (1.89e-03)	Tok/s 47429 (54925)	Loss/tok 3.0844 (3.2570)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.274 (0.257)	Data 1.39e-04 (1.85e-03)	Tok/s 60689 (54953)	Loss/tok 3.2605 (3.2592)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.412 (0.257)	Data 1.39e-04 (1.81e-03)	Tok/s 73673 (55028)	Loss/tok 3.4636 (3.2606)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.276 (0.257)	Data 1.18e-04 (1.77e-03)	Tok/s 60957 (54959)	Loss/tok 3.2097 (3.2596)	LR 2.000e-03
0: TRAIN [2][450/1938]	Time 0.275 (0.257)	Data 1.16e-04 (1.73e-03)	Tok/s 61921 (54991)	Loss/tok 3.2070 (3.2593)	LR 2.000e-03
0: TRAIN [2][460/1938]	Time 0.277 (0.256)	Data 1.25e-04 (1.70e-03)	Tok/s 61021 (54819)	Loss/tok 3.2172 (3.2569)	LR 2.000e-03
0: TRAIN [2][470/1938]	Time 0.275 (0.256)	Data 1.27e-04 (1.67e-03)	Tok/s 60978 (54808)	Loss/tok 3.2369 (3.2556)	LR 2.000e-03
0: TRAIN [2][480/1938]	Time 0.277 (0.255)	Data 1.13e-04 (1.64e-03)	Tok/s 61554 (54736)	Loss/tok 3.2507 (3.2548)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.213 (0.255)	Data 1.28e-04 (1.61e-03)	Tok/s 48162 (54689)	Loss/tok 3.0327 (3.2532)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.214 (0.255)	Data 1.27e-04 (1.58e-03)	Tok/s 48234 (54657)	Loss/tok 3.0391 (3.2525)	LR 2.000e-03
0: TRAIN [2][510/1938]	Time 0.276 (0.255)	Data 1.21e-04 (1.55e-03)	Tok/s 60508 (54757)	Loss/tok 3.3940 (3.2552)	LR 2.000e-03
0: TRAIN [2][520/1938]	Time 0.215 (0.255)	Data 1.35e-04 (1.52e-03)	Tok/s 47804 (54711)	Loss/tok 3.1698 (3.2541)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][530/1938]	Time 0.337 (0.255)	Data 1.44e-04 (1.49e-03)	Tok/s 68011 (54777)	Loss/tok 3.4767 (3.2552)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.337 (0.256)	Data 1.02e-04 (1.47e-03)	Tok/s 68981 (54839)	Loss/tok 3.3099 (3.2558)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.276 (0.256)	Data 4.25e-04 (1.44e-03)	Tok/s 61196 (54880)	Loss/tok 3.3165 (3.2577)	LR 2.000e-03
0: TRAIN [2][560/1938]	Time 0.214 (0.256)	Data 1.24e-04 (1.42e-03)	Tok/s 48242 (54902)	Loss/tok 3.1159 (3.2577)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.214 (0.255)	Data 1.15e-04 (1.40e-03)	Tok/s 47437 (54803)	Loss/tok 3.0075 (3.2551)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.337 (0.256)	Data 1.31e-04 (1.38e-03)	Tok/s 69341 (54935)	Loss/tok 3.3502 (3.2579)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.159 (0.256)	Data 1.24e-04 (1.36e-03)	Tok/s 33380 (54871)	Loss/tok 2.6900 (3.2580)	LR 2.000e-03
0: TRAIN [2][600/1938]	Time 0.213 (0.256)	Data 1.36e-04 (1.34e-03)	Tok/s 47962 (54915)	Loss/tok 3.0703 (3.2591)	LR 2.000e-03
0: TRAIN [2][610/1938]	Time 0.156 (0.255)	Data 1.35e-04 (1.32e-03)	Tok/s 34527 (54734)	Loss/tok 2.6096 (3.2565)	LR 2.000e-03
0: TRAIN [2][620/1938]	Time 0.276 (0.255)	Data 1.12e-04 (1.30e-03)	Tok/s 60880 (54685)	Loss/tok 3.2880 (3.2558)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.214 (0.255)	Data 3.75e-04 (1.28e-03)	Tok/s 48433 (54749)	Loss/tok 3.0785 (3.2566)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.278 (0.255)	Data 5.91e-04 (1.26e-03)	Tok/s 60651 (54760)	Loss/tok 3.2788 (3.2559)	LR 2.000e-03
0: TRAIN [2][650/1938]	Time 0.213 (0.255)	Data 1.13e-04 (1.25e-03)	Tok/s 48556 (54788)	Loss/tok 3.0486 (3.2549)	LR 2.000e-03
0: TRAIN [2][660/1938]	Time 0.337 (0.255)	Data 1.34e-04 (1.23e-03)	Tok/s 69351 (54796)	Loss/tok 3.3931 (3.2540)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.275 (0.255)	Data 1.31e-04 (1.21e-03)	Tok/s 61699 (54814)	Loss/tok 3.2196 (3.2545)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.339 (0.255)	Data 1.31e-04 (1.20e-03)	Tok/s 68832 (54845)	Loss/tok 3.4589 (3.2559)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.338 (0.255)	Data 1.14e-04 (1.18e-03)	Tok/s 68932 (54787)	Loss/tok 3.4745 (3.2556)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][700/1938]	Time 0.336 (0.255)	Data 1.19e-04 (1.17e-03)	Tok/s 69148 (54721)	Loss/tok 3.4604 (3.2544)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.214 (0.255)	Data 1.37e-04 (1.15e-03)	Tok/s 48371 (54775)	Loss/tok 3.0122 (3.2544)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.337 (0.255)	Data 1.16e-04 (1.14e-03)	Tok/s 68827 (54788)	Loss/tok 3.5780 (3.2557)	LR 2.000e-03
0: TRAIN [2][730/1938]	Time 0.214 (0.255)	Data 1.36e-04 (1.13e-03)	Tok/s 47522 (54747)	Loss/tok 3.1537 (3.2546)	LR 2.000e-03
0: TRAIN [2][740/1938]	Time 0.214 (0.254)	Data 1.22e-04 (1.11e-03)	Tok/s 47214 (54692)	Loss/tok 2.9844 (3.2532)	LR 2.000e-03
0: TRAIN [2][750/1938]	Time 0.276 (0.255)	Data 1.32e-04 (1.10e-03)	Tok/s 61112 (54794)	Loss/tok 3.1673 (3.2550)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.214 (0.255)	Data 1.18e-04 (1.09e-03)	Tok/s 48965 (54737)	Loss/tok 3.0624 (3.2545)	LR 2.000e-03
0: TRAIN [2][770/1938]	Time 0.276 (0.255)	Data 1.32e-04 (1.07e-03)	Tok/s 61016 (54787)	Loss/tok 3.2179 (3.2557)	LR 2.000e-03
0: TRAIN [2][780/1938]	Time 0.213 (0.255)	Data 1.21e-04 (1.06e-03)	Tok/s 47874 (54712)	Loss/tok 3.0897 (3.2550)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.214 (0.255)	Data 1.29e-04 (1.05e-03)	Tok/s 49572 (54707)	Loss/tok 3.0869 (3.2556)	LR 2.000e-03
0: TRAIN [2][800/1938]	Time 0.275 (0.254)	Data 1.43e-04 (1.04e-03)	Tok/s 60916 (54658)	Loss/tok 3.2225 (3.2542)	LR 2.000e-03
0: TRAIN [2][810/1938]	Time 0.213 (0.254)	Data 1.29e-04 (1.03e-03)	Tok/s 47521 (54663)	Loss/tok 2.9746 (3.2553)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.276 (0.255)	Data 1.49e-04 (1.02e-03)	Tok/s 60503 (54675)	Loss/tok 3.3100 (3.2561)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.275 (0.255)	Data 1.23e-04 (1.01e-03)	Tok/s 60721 (54733)	Loss/tok 3.2865 (3.2562)	LR 2.000e-03
0: TRAIN [2][840/1938]	Time 0.213 (0.255)	Data 1.30e-04 (9.95e-04)	Tok/s 47580 (54705)	Loss/tok 3.0785 (3.2561)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.213 (0.254)	Data 1.11e-04 (9.85e-04)	Tok/s 47084 (54667)	Loss/tok 3.1197 (3.2550)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.337 (0.255)	Data 1.12e-04 (9.75e-04)	Tok/s 69661 (54691)	Loss/tok 3.5127 (3.2554)	LR 2.000e-03
0: TRAIN [2][870/1938]	Time 0.157 (0.255)	Data 1.32e-04 (9.65e-04)	Tok/s 33221 (54710)	Loss/tok 2.6100 (3.2563)	LR 2.000e-03
0: TRAIN [2][880/1938]	Time 0.157 (0.255)	Data 1.11e-04 (9.56e-04)	Tok/s 34139 (54690)	Loss/tok 2.6434 (3.2561)	LR 2.000e-03
0: TRAIN [2][890/1938]	Time 0.157 (0.255)	Data 1.13e-04 (9.47e-04)	Tok/s 33518 (54716)	Loss/tok 2.7166 (3.2568)	LR 2.000e-03
0: TRAIN [2][900/1938]	Time 0.275 (0.255)	Data 1.11e-04 (9.38e-04)	Tok/s 61103 (54697)	Loss/tok 3.3311 (3.2563)	LR 2.000e-03
0: TRAIN [2][910/1938]	Time 0.411 (0.255)	Data 1.31e-04 (9.29e-04)	Tok/s 72312 (54681)	Loss/tok 3.5439 (3.2560)	LR 2.000e-03
0: TRAIN [2][920/1938]	Time 0.277 (0.255)	Data 1.25e-04 (9.21e-04)	Tok/s 60265 (54696)	Loss/tok 3.3381 (3.2555)	LR 2.000e-03
0: TRAIN [2][930/1938]	Time 0.214 (0.255)	Data 1.21e-04 (9.13e-04)	Tok/s 47195 (54700)	Loss/tok 2.9720 (3.2550)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.273 (0.255)	Data 1.32e-04 (9.05e-04)	Tok/s 62409 (54681)	Loss/tok 3.2409 (3.2556)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][950/1938]	Time 0.157 (0.255)	Data 1.27e-04 (8.97e-04)	Tok/s 34725 (54671)	Loss/tok 2.5474 (3.2561)	LR 2.000e-03
0: TRAIN [2][960/1938]	Time 0.214 (0.254)	Data 1.24e-04 (8.89e-04)	Tok/s 49071 (54651)	Loss/tok 3.0817 (3.2554)	LR 2.000e-03
0: TRAIN [2][970/1938]	Time 0.338 (0.255)	Data 1.48e-04 (8.82e-04)	Tok/s 69000 (54679)	Loss/tok 3.3810 (3.2564)	LR 2.000e-03
0: TRAIN [2][980/1938]	Time 0.158 (0.254)	Data 1.26e-04 (8.74e-04)	Tok/s 34002 (54657)	Loss/tok 2.7171 (3.2556)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.339 (0.254)	Data 9.87e-05 (8.66e-04)	Tok/s 68868 (54653)	Loss/tok 3.4817 (3.2556)	LR 2.000e-03
0: TRAIN [2][1000/1938]	Time 0.337 (0.254)	Data 1.24e-04 (8.58e-04)	Tok/s 69275 (54659)	Loss/tok 3.5557 (3.2556)	LR 2.000e-03
0: TRAIN [2][1010/1938]	Time 0.214 (0.254)	Data 1.06e-04 (8.51e-04)	Tok/s 47971 (54669)	Loss/tok 3.0998 (3.2558)	LR 2.000e-03
0: TRAIN [2][1020/1938]	Time 0.277 (0.254)	Data 1.23e-04 (8.44e-04)	Tok/s 60261 (54669)	Loss/tok 3.1755 (3.2563)	LR 2.000e-03
0: TRAIN [2][1030/1938]	Time 0.159 (0.254)	Data 1.20e-04 (8.37e-04)	Tok/s 33636 (54647)	Loss/tok 2.6357 (3.2565)	LR 2.000e-03
0: TRAIN [2][1040/1938]	Time 0.213 (0.255)	Data 1.30e-04 (8.31e-04)	Tok/s 48053 (54669)	Loss/tok 3.0928 (3.2571)	LR 2.000e-03
0: TRAIN [2][1050/1938]	Time 0.275 (0.254)	Data 1.35e-04 (8.24e-04)	Tok/s 61572 (54659)	Loss/tok 3.1920 (3.2560)	LR 2.000e-03
0: TRAIN [2][1060/1938]	Time 0.214 (0.254)	Data 1.43e-04 (8.18e-04)	Tok/s 48021 (54666)	Loss/tok 3.0328 (3.2556)	LR 2.000e-03
0: TRAIN [2][1070/1938]	Time 0.275 (0.254)	Data 1.26e-04 (8.11e-04)	Tok/s 61692 (54644)	Loss/tok 3.2530 (3.2549)	LR 2.000e-03
0: TRAIN [2][1080/1938]	Time 0.214 (0.254)	Data 1.34e-04 (8.05e-04)	Tok/s 48381 (54665)	Loss/tok 2.9840 (3.2553)	LR 2.000e-03
0: TRAIN [2][1090/1938]	Time 0.276 (0.254)	Data 1.23e-04 (8.00e-04)	Tok/s 60977 (54656)	Loss/tok 3.2378 (3.2556)	LR 2.000e-03
0: TRAIN [2][1100/1938]	Time 0.341 (0.254)	Data 1.32e-04 (7.94e-04)	Tok/s 68388 (54646)	Loss/tok 3.4396 (3.2555)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.277 (0.254)	Data 1.25e-04 (7.88e-04)	Tok/s 62082 (54607)	Loss/tok 3.1659 (3.2551)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.214 (0.254)	Data 5.65e-04 (7.82e-04)	Tok/s 48964 (54595)	Loss/tok 2.9856 (3.2551)	LR 2.000e-03
0: TRAIN [2][1130/1938]	Time 0.215 (0.254)	Data 1.32e-04 (7.76e-04)	Tok/s 48435 (54570)	Loss/tok 2.9521 (3.2543)	LR 2.000e-03
0: TRAIN [2][1140/1938]	Time 0.214 (0.254)	Data 1.29e-04 (7.71e-04)	Tok/s 48071 (54609)	Loss/tok 3.0673 (3.2541)	LR 2.000e-03
0: TRAIN [2][1150/1938]	Time 0.213 (0.254)	Data 1.41e-04 (7.65e-04)	Tok/s 48414 (54583)	Loss/tok 3.0844 (3.2533)	LR 2.000e-03
0: TRAIN [2][1160/1938]	Time 0.275 (0.254)	Data 1.39e-04 (7.60e-04)	Tok/s 61909 (54643)	Loss/tok 3.2112 (3.2544)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.160 (0.254)	Data 1.13e-04 (7.55e-04)	Tok/s 32954 (54641)	Loss/tok 2.6948 (3.2542)	LR 2.000e-03
0: TRAIN [2][1180/1938]	Time 0.275 (0.254)	Data 1.23e-04 (7.50e-04)	Tok/s 61525 (54676)	Loss/tok 3.1701 (3.2547)	LR 2.000e-03
0: TRAIN [2][1190/1938]	Time 0.213 (0.254)	Data 1.28e-04 (7.45e-04)	Tok/s 49127 (54684)	Loss/tok 3.0554 (3.2551)	LR 2.000e-03
0: TRAIN [2][1200/1938]	Time 0.275 (0.255)	Data 1.13e-04 (7.40e-04)	Tok/s 61346 (54727)	Loss/tok 3.1572 (3.2557)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.159 (0.254)	Data 1.29e-04 (7.35e-04)	Tok/s 33011 (54690)	Loss/tok 2.6463 (3.2554)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1220/1938]	Time 0.213 (0.254)	Data 1.36e-04 (7.30e-04)	Tok/s 48822 (54681)	Loss/tok 3.0981 (3.2553)	LR 2.000e-03
0: TRAIN [2][1230/1938]	Time 0.157 (0.254)	Data 1.27e-04 (7.25e-04)	Tok/s 33298 (54625)	Loss/tok 2.6833 (3.2541)	LR 2.000e-03
0: TRAIN [2][1240/1938]	Time 0.157 (0.254)	Data 1.51e-04 (7.20e-04)	Tok/s 33310 (54629)	Loss/tok 2.5788 (3.2543)	LR 2.000e-03
0: TRAIN [2][1250/1938]	Time 0.276 (0.254)	Data 1.41e-04 (7.15e-04)	Tok/s 60059 (54623)	Loss/tok 3.2926 (3.2548)	LR 2.000e-03
0: TRAIN [2][1260/1938]	Time 0.339 (0.255)	Data 1.56e-04 (7.11e-04)	Tok/s 68829 (54683)	Loss/tok 3.4896 (3.2567)	LR 2.000e-03
0: TRAIN [2][1270/1938]	Time 0.277 (0.255)	Data 1.12e-04 (7.06e-04)	Tok/s 60047 (54714)	Loss/tok 3.2420 (3.2574)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.213 (0.255)	Data 1.29e-04 (7.02e-04)	Tok/s 47653 (54744)	Loss/tok 2.8692 (3.2573)	LR 2.000e-03
0: TRAIN [2][1290/1938]	Time 0.276 (0.255)	Data 1.34e-04 (6.97e-04)	Tok/s 61604 (54769)	Loss/tok 3.1672 (3.2581)	LR 2.000e-03
0: TRAIN [2][1300/1938]	Time 0.275 (0.255)	Data 1.27e-04 (6.93e-04)	Tok/s 60773 (54760)	Loss/tok 3.2993 (3.2581)	LR 2.000e-03
0: TRAIN [2][1310/1938]	Time 0.275 (0.255)	Data 1.29e-04 (6.89e-04)	Tok/s 60570 (54814)	Loss/tok 3.3275 (3.2591)	LR 2.000e-03
0: TRAIN [2][1320/1938]	Time 0.271 (0.255)	Data 1.38e-04 (6.84e-04)	Tok/s 61560 (54822)	Loss/tok 3.2180 (3.2587)	LR 2.000e-03
0: TRAIN [2][1330/1938]	Time 0.337 (0.255)	Data 1.28e-04 (6.80e-04)	Tok/s 68579 (54822)	Loss/tok 3.4243 (3.2583)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.273 (0.256)	Data 1.28e-04 (6.76e-04)	Tok/s 61747 (54847)	Loss/tok 3.2841 (3.2588)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.215 (0.256)	Data 1.13e-04 (6.72e-04)	Tok/s 47575 (54829)	Loss/tok 3.0126 (3.2582)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1360/1938]	Time 0.212 (0.255)	Data 1.28e-04 (6.68e-04)	Tok/s 48912 (54806)	Loss/tok 2.9317 (3.2576)	LR 2.000e-03
0: TRAIN [2][1370/1938]	Time 0.410 (0.256)	Data 1.32e-04 (6.64e-04)	Tok/s 72446 (54844)	Loss/tok 3.5891 (3.2581)	LR 2.000e-03
0: TRAIN [2][1380/1938]	Time 0.338 (0.256)	Data 1.30e-04 (6.60e-04)	Tok/s 69642 (54870)	Loss/tok 3.3276 (3.2583)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.412 (0.256)	Data 1.09e-04 (6.57e-04)	Tok/s 71976 (54865)	Loss/tok 3.6000 (3.2581)	LR 2.000e-03
0: TRAIN [2][1400/1938]	Time 0.409 (0.255)	Data 1.34e-04 (6.53e-04)	Tok/s 71669 (54822)	Loss/tok 3.7034 (3.2579)	LR 2.000e-03
0: TRAIN [2][1410/1938]	Time 0.276 (0.256)	Data 1.33e-04 (6.49e-04)	Tok/s 61703 (54852)	Loss/tok 3.1454 (3.2582)	LR 2.000e-03
0: TRAIN [2][1420/1938]	Time 0.276 (0.256)	Data 1.24e-04 (6.46e-04)	Tok/s 61382 (54881)	Loss/tok 3.2088 (3.2583)	LR 2.000e-03
0: TRAIN [2][1430/1938]	Time 0.213 (0.256)	Data 1.37e-04 (6.42e-04)	Tok/s 48774 (54873)	Loss/tok 3.1299 (3.2578)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.276 (0.256)	Data 1.13e-04 (6.39e-04)	Tok/s 61221 (54904)	Loss/tok 3.2594 (3.2577)	LR 2.000e-03
0: TRAIN [2][1450/1938]	Time 0.214 (0.256)	Data 1.35e-04 (6.35e-04)	Tok/s 49311 (54888)	Loss/tok 3.2406 (3.2575)	LR 2.000e-03
0: TRAIN [2][1460/1938]	Time 0.213 (0.256)	Data 1.11e-04 (6.32e-04)	Tok/s 48272 (54874)	Loss/tok 3.0987 (3.2573)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.338 (0.255)	Data 1.08e-04 (6.29e-04)	Tok/s 68098 (54865)	Loss/tok 3.3384 (3.2569)	LR 2.000e-03
0: TRAIN [2][1480/1938]	Time 0.277 (0.255)	Data 1.38e-04 (6.25e-04)	Tok/s 61029 (54856)	Loss/tok 3.3216 (3.2568)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.157 (0.255)	Data 1.17e-04 (6.22e-04)	Tok/s 33424 (54837)	Loss/tok 2.6302 (3.2569)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1500/1938]	Time 0.337 (0.255)	Data 1.00e-04 (6.19e-04)	Tok/s 69735 (54811)	Loss/tok 3.3733 (3.2565)	LR 2.000e-03
0: TRAIN [2][1510/1938]	Time 0.277 (0.255)	Data 9.94e-05 (6.16e-04)	Tok/s 60852 (54827)	Loss/tok 3.2010 (3.2565)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.156 (0.255)	Data 1.15e-04 (6.12e-04)	Tok/s 33820 (54799)	Loss/tok 2.5330 (3.2557)	LR 2.000e-03
0: TRAIN [2][1530/1938]	Time 0.215 (0.255)	Data 1.32e-04 (6.09e-04)	Tok/s 48156 (54787)	Loss/tok 3.1211 (3.2551)	LR 2.000e-03
0: TRAIN [2][1540/1938]	Time 0.158 (0.255)	Data 1.36e-04 (6.06e-04)	Tok/s 33270 (54809)	Loss/tok 2.6666 (3.2556)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.337 (0.255)	Data 4.27e-04 (6.03e-04)	Tok/s 69344 (54819)	Loss/tok 3.3466 (3.2554)	LR 2.000e-03
0: TRAIN [2][1560/1938]	Time 0.214 (0.255)	Data 1.34e-04 (6.00e-04)	Tok/s 48881 (54808)	Loss/tok 2.9529 (3.2554)	LR 2.000e-03
0: TRAIN [2][1570/1938]	Time 0.275 (0.255)	Data 1.32e-04 (5.97e-04)	Tok/s 60520 (54826)	Loss/tok 3.2483 (3.2559)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.213 (0.255)	Data 1.50e-04 (5.94e-04)	Tok/s 48460 (54838)	Loss/tok 3.0230 (3.2559)	LR 2.000e-03
0: TRAIN [2][1590/1938]	Time 0.214 (0.255)	Data 1.30e-04 (5.91e-04)	Tok/s 48794 (54813)	Loss/tok 2.9392 (3.2548)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.158 (0.255)	Data 3.42e-04 (5.88e-04)	Tok/s 33453 (54818)	Loss/tok 2.6970 (3.2549)	LR 2.000e-03
0: TRAIN [2][1610/1938]	Time 0.213 (0.255)	Data 1.29e-04 (5.86e-04)	Tok/s 48004 (54852)	Loss/tok 2.9325 (3.2553)	LR 2.000e-03
0: TRAIN [2][1620/1938]	Time 0.276 (0.255)	Data 1.30e-04 (5.83e-04)	Tok/s 60795 (54863)	Loss/tok 3.1357 (3.2553)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][1630/1938]	Time 0.275 (0.255)	Data 1.14e-04 (5.80e-04)	Tok/s 61274 (54872)	Loss/tok 3.2633 (3.2557)	LR 2.000e-03
0: TRAIN [2][1640/1938]	Time 0.274 (0.255)	Data 1.42e-04 (5.78e-04)	Tok/s 60505 (54875)	Loss/tok 3.2919 (3.2554)	LR 2.000e-03
0: TRAIN [2][1650/1938]	Time 0.213 (0.255)	Data 1.41e-04 (5.75e-04)	Tok/s 48667 (54843)	Loss/tok 3.0037 (3.2546)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1660/1938]	Time 0.214 (0.255)	Data 1.27e-04 (5.73e-04)	Tok/s 48659 (54867)	Loss/tok 3.0211 (3.2552)	LR 2.000e-03
0: TRAIN [2][1670/1938]	Time 0.213 (0.255)	Data 1.14e-04 (5.70e-04)	Tok/s 48424 (54854)	Loss/tok 2.9491 (3.2550)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.213 (0.255)	Data 1.08e-04 (5.67e-04)	Tok/s 48742 (54848)	Loss/tok 3.1496 (3.2547)	LR 2.000e-03
0: TRAIN [2][1690/1938]	Time 0.277 (0.255)	Data 9.80e-05 (5.65e-04)	Tok/s 60362 (54865)	Loss/tok 3.2732 (3.2550)	LR 2.000e-03
0: TRAIN [2][1700/1938]	Time 0.274 (0.255)	Data 9.75e-05 (5.62e-04)	Tok/s 60620 (54864)	Loss/tok 3.2129 (3.2546)	LR 2.000e-03
0: TRAIN [2][1710/1938]	Time 0.277 (0.255)	Data 1.11e-04 (5.59e-04)	Tok/s 60615 (54855)	Loss/tok 3.2821 (3.2543)	LR 2.000e-03
0: TRAIN [2][1720/1938]	Time 0.277 (0.255)	Data 6.92e-04 (5.57e-04)	Tok/s 60693 (54872)	Loss/tok 3.3263 (3.2545)	LR 2.000e-03
0: TRAIN [2][1730/1938]	Time 0.213 (0.255)	Data 1.29e-04 (5.55e-04)	Tok/s 48641 (54893)	Loss/tok 3.1517 (3.2549)	LR 2.000e-03
0: TRAIN [2][1740/1938]	Time 0.276 (0.255)	Data 1.33e-04 (5.53e-04)	Tok/s 60537 (54913)	Loss/tok 3.2839 (3.2548)	LR 2.000e-03
0: TRAIN [2][1750/1938]	Time 0.213 (0.256)	Data 1.45e-04 (5.50e-04)	Tok/s 49361 (54931)	Loss/tok 3.0247 (3.2547)	LR 2.000e-03
0: TRAIN [2][1760/1938]	Time 0.275 (0.255)	Data 3.73e-04 (5.48e-04)	Tok/s 61115 (54906)	Loss/tok 3.1934 (3.2541)	LR 2.000e-03
0: TRAIN [2][1770/1938]	Time 0.275 (0.255)	Data 1.23e-04 (5.46e-04)	Tok/s 61004 (54904)	Loss/tok 3.1587 (3.2537)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.213 (0.255)	Data 1.41e-04 (5.43e-04)	Tok/s 48527 (54904)	Loss/tok 3.0945 (3.2538)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][1790/1938]	Time 0.214 (0.255)	Data 1.35e-04 (5.41e-04)	Tok/s 47953 (54870)	Loss/tok 3.0359 (3.2537)	LR 2.000e-03
0: TRAIN [2][1800/1938]	Time 0.213 (0.255)	Data 1.10e-04 (5.39e-04)	Tok/s 48875 (54867)	Loss/tok 2.9307 (3.2534)	LR 2.000e-03
0: TRAIN [2][1810/1938]	Time 0.337 (0.255)	Data 1.29e-04 (5.37e-04)	Tok/s 69290 (54865)	Loss/tok 3.4119 (3.2534)	LR 2.000e-03
0: TRAIN [2][1820/1938]	Time 0.273 (0.255)	Data 1.29e-04 (5.34e-04)	Tok/s 61842 (54860)	Loss/tok 3.2832 (3.2535)	LR 2.000e-03
0: TRAIN [2][1830/1938]	Time 0.214 (0.255)	Data 1.29e-04 (5.32e-04)	Tok/s 48577 (54876)	Loss/tok 3.1478 (3.2537)	LR 2.000e-03
0: TRAIN [2][1840/1938]	Time 0.213 (0.255)	Data 1.18e-04 (5.30e-04)	Tok/s 49516 (54889)	Loss/tok 2.9730 (3.2535)	LR 2.000e-03
0: TRAIN [2][1850/1938]	Time 0.277 (0.255)	Data 1.39e-04 (5.28e-04)	Tok/s 60238 (54876)	Loss/tok 3.2582 (3.2534)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.275 (0.255)	Data 1.54e-04 (5.26e-04)	Tok/s 61257 (54867)	Loss/tok 3.2116 (3.2530)	LR 2.000e-03
0: TRAIN [2][1870/1938]	Time 0.214 (0.255)	Data 1.26e-04 (5.24e-04)	Tok/s 46962 (54853)	Loss/tok 3.0821 (3.2527)	LR 2.000e-03
0: TRAIN [2][1880/1938]	Time 0.340 (0.255)	Data 1.38e-04 (5.22e-04)	Tok/s 69561 (54854)	Loss/tok 3.2731 (3.2523)	LR 2.000e-03
0: TRAIN [2][1890/1938]	Time 0.213 (0.255)	Data 1.28e-04 (5.20e-04)	Tok/s 48102 (54846)	Loss/tok 2.9494 (3.2517)	LR 2.000e-03
0: TRAIN [2][1900/1938]	Time 0.275 (0.255)	Data 1.18e-04 (5.17e-04)	Tok/s 60901 (54866)	Loss/tok 3.3021 (3.2522)	LR 2.000e-03
0: TRAIN [2][1910/1938]	Time 0.213 (0.255)	Data 1.44e-04 (5.15e-04)	Tok/s 47597 (54858)	Loss/tok 3.1964 (3.2517)	LR 2.000e-03
0: TRAIN [2][1920/1938]	Time 0.275 (0.255)	Data 4.67e-04 (5.14e-04)	Tok/s 61284 (54855)	Loss/tok 3.2218 (3.2514)	LR 2.000e-03
0: TRAIN [2][1930/1938]	Time 0.337 (0.255)	Data 1.25e-04 (5.12e-04)	Tok/s 68999 (54856)	Loss/tok 3.4384 (3.2517)	LR 2.000e-03
:::MLL 1570030162.626 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1570030162.626 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.744 (0.744)	Decoder iters 149.0 (149.0)	Tok/s 21947 (21947)
0: Running moses detokenizer
0: BLEU(score=23.090723235580246, counts=[36353, 17867, 10026, 5854], totals=[65110, 62107, 59104, 56107], precisions=[55.83320534480111, 28.768093773648705, 16.963318895506227, 10.433635731726879], bp=1.0, sys_len=65110, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1570030164.552 eval_accuracy: {"value": 23.09, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1570030164.553 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.2544	Test BLEU: 23.09
0: Performance: Epoch: 2	Training: 438851 Tok/s
0: Finished epoch 2
:::MLL 1570030164.553 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1570030164.554 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1570030164.554 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 2968761614
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][0/1938]	Time 0.938 (0.938)	Data 7.14e-01 (7.14e-01)	Tok/s 10946 (10946)	Loss/tok 2.9155 (2.9155)	LR 2.000e-03
0: TRAIN [3][10/1938]	Time 0.213 (0.332)	Data 1.01e-04 (6.50e-02)	Tok/s 48715 (52782)	Loss/tok 2.9731 (3.1634)	LR 2.000e-03
0: TRAIN [3][20/1938]	Time 0.337 (0.288)	Data 9.44e-05 (3.41e-02)	Tok/s 68606 (52549)	Loss/tok 3.4344 (3.1485)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.274 (0.280)	Data 1.02e-04 (2.32e-02)	Tok/s 60218 (53777)	Loss/tok 3.1957 (3.1593)	LR 2.000e-03
0: TRAIN [3][40/1938]	Time 0.277 (0.276)	Data 1.02e-04 (1.76e-02)	Tok/s 60978 (54619)	Loss/tok 3.0590 (3.1560)	LR 2.000e-03
0: TRAIN [3][50/1938]	Time 0.213 (0.273)	Data 9.51e-05 (1.41e-02)	Tok/s 49083 (54604)	Loss/tok 3.0697 (3.1650)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.215 (0.266)	Data 1.31e-04 (1.18e-02)	Tok/s 49056 (54122)	Loss/tok 2.9902 (3.1495)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.273 (0.264)	Data 9.78e-05 (1.02e-02)	Tok/s 61957 (54144)	Loss/tok 3.0985 (3.1426)	LR 2.000e-03
0: TRAIN [3][80/1938]	Time 0.215 (0.261)	Data 1.04e-04 (8.94e-03)	Tok/s 47234 (53887)	Loss/tok 2.9398 (3.1359)	LR 2.000e-03
0: TRAIN [3][90/1938]	Time 0.213 (0.258)	Data 1.01e-04 (7.97e-03)	Tok/s 49370 (53705)	Loss/tok 2.9009 (3.1270)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.275 (0.261)	Data 9.58e-05 (7.19e-03)	Tok/s 60614 (54344)	Loss/tok 3.2432 (3.1489)	LR 2.000e-03
0: TRAIN [3][110/1938]	Time 0.275 (0.262)	Data 9.78e-05 (6.55e-03)	Tok/s 61428 (54676)	Loss/tok 3.1142 (3.1543)	LR 2.000e-03
0: TRAIN [3][120/1938]	Time 0.213 (0.260)	Data 9.94e-05 (6.02e-03)	Tok/s 49142 (54482)	Loss/tok 3.0139 (3.1547)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.277 (0.258)	Data 9.87e-05 (5.57e-03)	Tok/s 60751 (54175)	Loss/tok 3.0176 (3.1508)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.275 (0.256)	Data 9.78e-05 (5.18e-03)	Tok/s 61903 (53839)	Loss/tok 3.0445 (3.1505)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.273 (0.256)	Data 9.68e-05 (4.85e-03)	Tok/s 62150 (53717)	Loss/tok 3.1567 (3.1549)	LR 2.000e-03
0: TRAIN [3][160/1938]	Time 0.276 (0.255)	Data 1.34e-04 (4.55e-03)	Tok/s 60710 (53843)	Loss/tok 3.1202 (3.1508)	LR 2.000e-03
0: TRAIN [3][170/1938]	Time 0.276 (0.255)	Data 1.35e-04 (4.29e-03)	Tok/s 60288 (53927)	Loss/tok 3.1997 (3.1514)	LR 2.000e-03
0: TRAIN [3][180/1938]	Time 0.339 (0.257)	Data 1.21e-04 (4.06e-03)	Tok/s 68730 (54236)	Loss/tok 3.2791 (3.1573)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.336 (0.256)	Data 1.21e-04 (3.86e-03)	Tok/s 69312 (54210)	Loss/tok 3.3869 (3.1555)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.275 (0.256)	Data 1.64e-04 (3.68e-03)	Tok/s 61990 (54286)	Loss/tok 3.1253 (3.1551)	LR 2.000e-03
0: TRAIN [3][210/1938]	Time 0.213 (0.255)	Data 1.13e-04 (3.51e-03)	Tok/s 47837 (54205)	Loss/tok 3.0043 (3.1529)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.213 (0.255)	Data 1.19e-04 (3.36e-03)	Tok/s 47244 (54172)	Loss/tok 3.0298 (3.1516)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.214 (0.254)	Data 1.12e-04 (3.22e-03)	Tok/s 48279 (54167)	Loss/tok 3.0077 (3.1505)	LR 2.000e-03
0: TRAIN [3][240/1938]	Time 0.410 (0.256)	Data 1.11e-04 (3.09e-03)	Tok/s 72375 (54436)	Loss/tok 3.5516 (3.1578)	LR 2.000e-03
0: TRAIN [3][250/1938]	Time 0.157 (0.255)	Data 1.35e-04 (2.97e-03)	Tok/s 34196 (54338)	Loss/tok 2.6379 (3.1565)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.214 (0.256)	Data 1.11e-04 (2.86e-03)	Tok/s 47721 (54473)	Loss/tok 2.9446 (3.1556)	LR 2.000e-03
0: TRAIN [3][270/1938]	Time 0.157 (0.255)	Data 1.14e-04 (2.76e-03)	Tok/s 34443 (54397)	Loss/tok 2.6514 (3.1551)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.272 (0.255)	Data 1.35e-04 (2.67e-03)	Tok/s 62470 (54297)	Loss/tok 3.1028 (3.1536)	LR 2.000e-03
0: TRAIN [3][290/1938]	Time 0.213 (0.254)	Data 1.52e-04 (2.58e-03)	Tok/s 47427 (54261)	Loss/tok 2.9842 (3.1522)	LR 2.000e-03
0: TRAIN [3][300/1938]	Time 0.214 (0.254)	Data 1.43e-04 (2.50e-03)	Tok/s 48219 (54183)	Loss/tok 3.0240 (3.1502)	LR 2.000e-03
0: TRAIN [3][310/1938]	Time 0.276 (0.254)	Data 1.18e-04 (2.42e-03)	Tok/s 60808 (54249)	Loss/tok 3.1558 (3.1514)	LR 2.000e-03
0: TRAIN [3][320/1938]	Time 0.213 (0.254)	Data 1.20e-04 (2.35e-03)	Tok/s 49171 (54420)	Loss/tok 3.0445 (3.1534)	LR 2.000e-03
0: TRAIN [3][330/1938]	Time 0.336 (0.256)	Data 1.62e-04 (2.28e-03)	Tok/s 69815 (54641)	Loss/tok 3.3156 (3.1579)	LR 2.000e-03
0: TRAIN [3][340/1938]	Time 0.275 (0.256)	Data 1.09e-04 (2.22e-03)	Tok/s 61878 (54727)	Loss/tok 3.1182 (3.1585)	LR 2.000e-03
0: TRAIN [3][350/1938]	Time 0.213 (0.256)	Data 1.14e-04 (2.16e-03)	Tok/s 48858 (54704)	Loss/tok 2.9192 (3.1569)	LR 2.000e-03
0: TRAIN [3][360/1938]	Time 0.278 (0.256)	Data 1.43e-04 (2.11e-03)	Tok/s 61103 (54753)	Loss/tok 3.1082 (3.1563)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][370/1938]	Time 0.214 (0.256)	Data 1.11e-04 (2.05e-03)	Tok/s 48409 (54761)	Loss/tok 2.9826 (3.1593)	LR 2.000e-03
0: TRAIN [3][380/1938]	Time 0.213 (0.256)	Data 1.13e-04 (2.00e-03)	Tok/s 47706 (54795)	Loss/tok 2.9889 (3.1600)	LR 2.000e-03
0: TRAIN [3][390/1938]	Time 0.277 (0.256)	Data 1.31e-04 (1.95e-03)	Tok/s 61336 (54797)	Loss/tok 3.2202 (3.1599)	LR 2.000e-03
0: TRAIN [3][400/1938]	Time 0.274 (0.256)	Data 1.43e-04 (1.91e-03)	Tok/s 60636 (54783)	Loss/tok 3.1042 (3.1584)	LR 2.000e-03
0: TRAIN [3][410/1938]	Time 0.339 (0.256)	Data 1.38e-04 (1.86e-03)	Tok/s 68649 (54838)	Loss/tok 3.4205 (3.1613)	LR 2.000e-03
0: TRAIN [3][420/1938]	Time 0.279 (0.256)	Data 1.22e-04 (1.82e-03)	Tok/s 59680 (54781)	Loss/tok 3.1724 (3.1602)	LR 2.000e-03
0: TRAIN [3][430/1938]	Time 0.278 (0.256)	Data 1.12e-04 (1.78e-03)	Tok/s 60752 (54758)	Loss/tok 3.2156 (3.1606)	LR 2.000e-03
0: TRAIN [3][440/1938]	Time 0.277 (0.255)	Data 1.86e-04 (1.75e-03)	Tok/s 60212 (54670)	Loss/tok 3.1266 (3.1597)	LR 2.000e-03
0: TRAIN [3][450/1938]	Time 0.411 (0.256)	Data 6.00e-04 (1.72e-03)	Tok/s 72815 (54745)	Loss/tok 3.4183 (3.1618)	LR 2.000e-03
0: TRAIN [3][460/1938]	Time 0.278 (0.256)	Data 1.42e-04 (1.68e-03)	Tok/s 59566 (54831)	Loss/tok 3.1272 (3.1623)	LR 2.000e-03
0: TRAIN [3][470/1938]	Time 0.338 (0.256)	Data 1.21e-04 (1.65e-03)	Tok/s 68245 (54784)	Loss/tok 3.4481 (3.1621)	LR 2.000e-03
0: TRAIN [3][480/1938]	Time 0.339 (0.256)	Data 1.37e-04 (1.62e-03)	Tok/s 68443 (54850)	Loss/tok 3.3561 (3.1633)	LR 2.000e-03
0: TRAIN [3][490/1938]	Time 0.438 (0.257)	Data 1.18e-04 (1.59e-03)	Tok/s 67850 (54888)	Loss/tok 3.5724 (3.1671)	LR 2.000e-03
0: TRAIN [3][500/1938]	Time 0.338 (0.256)	Data 1.26e-04 (1.56e-03)	Tok/s 68880 (54842)	Loss/tok 3.4387 (3.1669)	LR 2.000e-03
0: TRAIN [3][510/1938]	Time 0.274 (0.255)	Data 1.51e-04 (1.53e-03)	Tok/s 61299 (54710)	Loss/tok 3.1603 (3.1646)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][520/1938]	Time 0.213 (0.256)	Data 1.43e-04 (1.50e-03)	Tok/s 47422 (54743)	Loss/tok 3.0204 (3.1655)	LR 2.000e-03
0: TRAIN [3][530/1938]	Time 0.337 (0.256)	Data 1.29e-04 (1.48e-03)	Tok/s 69650 (54848)	Loss/tok 3.3410 (3.1692)	LR 2.000e-03
0: TRAIN [3][540/1938]	Time 0.214 (0.256)	Data 1.16e-04 (1.46e-03)	Tok/s 47905 (54847)	Loss/tok 3.0286 (3.1687)	LR 2.000e-03
0: TRAIN [3][550/1938]	Time 0.158 (0.256)	Data 1.38e-04 (1.43e-03)	Tok/s 33515 (54845)	Loss/tok 2.5888 (3.1688)	LR 2.000e-03
0: TRAIN [3][560/1938]	Time 0.276 (0.256)	Data 1.45e-04 (1.41e-03)	Tok/s 61631 (54860)	Loss/tok 3.1361 (3.1684)	LR 2.000e-03
0: TRAIN [3][570/1938]	Time 0.276 (0.256)	Data 1.49e-04 (1.39e-03)	Tok/s 61844 (54886)	Loss/tok 3.1488 (3.1696)	LR 2.000e-03
0: TRAIN [3][580/1938]	Time 0.339 (0.256)	Data 1.45e-04 (1.36e-03)	Tok/s 68655 (54887)	Loss/tok 3.3894 (3.1713)	LR 2.000e-03
0: TRAIN [3][590/1938]	Time 0.277 (0.256)	Data 1.17e-04 (1.34e-03)	Tok/s 60116 (54912)	Loss/tok 3.1242 (3.1733)	LR 2.000e-03
0: TRAIN [3][600/1938]	Time 0.276 (0.257)	Data 1.57e-04 (1.32e-03)	Tok/s 60683 (54971)	Loss/tok 3.1655 (3.1740)	LR 2.000e-03
0: TRAIN [3][610/1938]	Time 0.212 (0.256)	Data 1.17e-04 (1.30e-03)	Tok/s 49021 (54862)	Loss/tok 2.9805 (3.1736)	LR 2.000e-03
0: TRAIN [3][620/1938]	Time 0.338 (0.257)	Data 1.34e-04 (1.29e-03)	Tok/s 68801 (54946)	Loss/tok 3.4268 (3.1749)	LR 2.000e-03
0: TRAIN [3][630/1938]	Time 0.157 (0.257)	Data 1.12e-04 (1.27e-03)	Tok/s 33786 (54973)	Loss/tok 2.6125 (3.1767)	LR 2.000e-03
0: TRAIN [3][640/1938]	Time 0.214 (0.257)	Data 3.43e-04 (1.25e-03)	Tok/s 46414 (55016)	Loss/tok 3.0080 (3.1778)	LR 2.000e-03
0: TRAIN [3][650/1938]	Time 0.276 (0.257)	Data 1.31e-04 (1.23e-03)	Tok/s 61174 (54929)	Loss/tok 3.1563 (3.1761)	LR 2.000e-03
0: TRAIN [3][660/1938]	Time 0.276 (0.256)	Data 1.30e-04 (1.22e-03)	Tok/s 61072 (54899)	Loss/tok 3.2311 (3.1753)	LR 2.000e-03
0: TRAIN [3][670/1938]	Time 0.337 (0.256)	Data 1.30e-04 (1.20e-03)	Tok/s 69118 (54932)	Loss/tok 3.3256 (3.1755)	LR 1.000e-03
0: TRAIN [3][680/1938]	Time 0.274 (0.256)	Data 1.18e-04 (1.19e-03)	Tok/s 61388 (54937)	Loss/tok 3.0620 (3.1748)	LR 1.000e-03
0: TRAIN [3][690/1938]	Time 0.214 (0.256)	Data 1.19e-04 (1.17e-03)	Tok/s 48415 (54901)	Loss/tok 3.1132 (3.1747)	LR 1.000e-03
0: TRAIN [3][700/1938]	Time 0.213 (0.256)	Data 1.20e-04 (1.16e-03)	Tok/s 48549 (54878)	Loss/tok 3.0339 (3.1739)	LR 1.000e-03
0: TRAIN [3][710/1938]	Time 0.275 (0.256)	Data 1.28e-04 (1.14e-03)	Tok/s 61329 (54890)	Loss/tok 3.1317 (3.1730)	LR 1.000e-03
0: TRAIN [3][720/1938]	Time 0.213 (0.256)	Data 1.32e-04 (1.13e-03)	Tok/s 48036 (54914)	Loss/tok 3.0487 (3.1726)	LR 1.000e-03
0: TRAIN [3][730/1938]	Time 0.213 (0.256)	Data 1.49e-04 (1.11e-03)	Tok/s 47956 (54921)	Loss/tok 3.0891 (3.1719)	LR 1.000e-03
0: TRAIN [3][740/1938]	Time 0.213 (0.256)	Data 1.10e-04 (1.10e-03)	Tok/s 48536 (54959)	Loss/tok 3.0901 (3.1728)	LR 1.000e-03
0: TRAIN [3][750/1938]	Time 0.213 (0.256)	Data 1.25e-04 (1.09e-03)	Tok/s 48445 (54985)	Loss/tok 2.9534 (3.1727)	LR 1.000e-03
0: TRAIN [3][760/1938]	Time 0.213 (0.256)	Data 1.21e-04 (1.08e-03)	Tok/s 47526 (54986)	Loss/tok 3.0160 (3.1719)	LR 1.000e-03
0: TRAIN [3][770/1938]	Time 0.213 (0.256)	Data 1.14e-04 (1.06e-03)	Tok/s 49570 (54974)	Loss/tok 2.8871 (3.1713)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][780/1938]	Time 0.276 (0.256)	Data 1.49e-04 (1.05e-03)	Tok/s 61127 (55011)	Loss/tok 3.1046 (3.1712)	LR 1.000e-03
0: TRAIN [3][790/1938]	Time 0.213 (0.256)	Data 1.25e-04 (1.04e-03)	Tok/s 48335 (55007)	Loss/tok 2.9679 (3.1706)	LR 1.000e-03
0: TRAIN [3][800/1938]	Time 0.157 (0.256)	Data 1.23e-04 (1.03e-03)	Tok/s 33185 (54948)	Loss/tok 2.4950 (3.1696)	LR 1.000e-03
0: TRAIN [3][810/1938]	Time 0.157 (0.256)	Data 1.27e-04 (1.02e-03)	Tok/s 32946 (54887)	Loss/tok 2.5084 (3.1683)	LR 1.000e-03
0: TRAIN [3][820/1938]	Time 0.213 (0.256)	Data 1.56e-04 (1.01e-03)	Tok/s 48466 (54900)	Loss/tok 3.0248 (3.1685)	LR 1.000e-03
0: TRAIN [3][830/1938]	Time 0.277 (0.256)	Data 1.47e-04 (9.96e-04)	Tok/s 60510 (54963)	Loss/tok 3.1781 (3.1682)	LR 1.000e-03
0: TRAIN [3][840/1938]	Time 0.213 (0.256)	Data 1.19e-04 (9.86e-04)	Tok/s 48035 (54964)	Loss/tok 2.9388 (3.1678)	LR 1.000e-03
0: TRAIN [3][850/1938]	Time 0.213 (0.256)	Data 1.17e-04 (9.77e-04)	Tok/s 48746 (54915)	Loss/tok 2.8629 (3.1673)	LR 1.000e-03
0: TRAIN [3][860/1938]	Time 0.276 (0.256)	Data 1.14e-04 (9.67e-04)	Tok/s 61204 (54924)	Loss/tok 3.1464 (3.1672)	LR 1.000e-03
0: TRAIN [3][870/1938]	Time 0.412 (0.256)	Data 1.32e-04 (9.58e-04)	Tok/s 72699 (54909)	Loss/tok 3.5481 (3.1671)	LR 1.000e-03
0: TRAIN [3][880/1938]	Time 0.213 (0.255)	Data 1.16e-04 (9.49e-04)	Tok/s 48530 (54880)	Loss/tok 2.8580 (3.1659)	LR 1.000e-03
0: TRAIN [3][890/1938]	Time 0.337 (0.256)	Data 1.28e-04 (9.39e-04)	Tok/s 69934 (54943)	Loss/tok 3.2951 (3.1664)	LR 1.000e-03
0: TRAIN [3][900/1938]	Time 0.213 (0.256)	Data 1.11e-04 (9.30e-04)	Tok/s 49148 (54926)	Loss/tok 2.9410 (3.1655)	LR 1.000e-03
0: TRAIN [3][910/1938]	Time 0.212 (0.256)	Data 1.25e-04 (9.22e-04)	Tok/s 48502 (54973)	Loss/tok 2.9246 (3.1658)	LR 1.000e-03
0: TRAIN [3][920/1938]	Time 0.213 (0.256)	Data 1.26e-04 (9.13e-04)	Tok/s 48351 (54976)	Loss/tok 2.8956 (3.1662)	LR 1.000e-03
0: TRAIN [3][930/1938]	Time 0.214 (0.256)	Data 1.27e-04 (9.04e-04)	Tok/s 49110 (54945)	Loss/tok 2.9881 (3.1659)	LR 1.000e-03
0: TRAIN [3][940/1938]	Time 0.213 (0.256)	Data 1.13e-04 (8.96e-04)	Tok/s 48448 (54937)	Loss/tok 2.9394 (3.1650)	LR 1.000e-03
0: TRAIN [3][950/1938]	Time 0.413 (0.256)	Data 1.12e-04 (8.88e-04)	Tok/s 71047 (55007)	Loss/tok 3.5120 (3.1668)	LR 1.000e-03
0: TRAIN [3][960/1938]	Time 0.214 (0.256)	Data 1.13e-04 (8.80e-04)	Tok/s 48510 (54935)	Loss/tok 2.9139 (3.1654)	LR 1.000e-03
0: TRAIN [3][970/1938]	Time 0.338 (0.255)	Data 3.20e-04 (8.73e-04)	Tok/s 68500 (54868)	Loss/tok 3.3085 (3.1645)	LR 1.000e-03
0: TRAIN [3][980/1938]	Time 0.213 (0.255)	Data 1.16e-04 (8.65e-04)	Tok/s 49142 (54854)	Loss/tok 2.8927 (3.1636)	LR 1.000e-03
0: TRAIN [3][990/1938]	Time 0.271 (0.255)	Data 1.26e-04 (8.58e-04)	Tok/s 62420 (54876)	Loss/tok 3.1521 (3.1635)	LR 1.000e-03
0: TRAIN [3][1000/1938]	Time 0.275 (0.255)	Data 1.35e-04 (8.51e-04)	Tok/s 61807 (54881)	Loss/tok 3.1593 (3.1633)	LR 1.000e-03
0: TRAIN [3][1010/1938]	Time 0.413 (0.255)	Data 1.38e-04 (8.44e-04)	Tok/s 71743 (54876)	Loss/tok 3.4390 (3.1629)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1020/1938]	Time 0.213 (0.256)	Data 1.21e-04 (8.37e-04)	Tok/s 47661 (54912)	Loss/tok 2.9654 (3.1641)	LR 1.000e-03
0: TRAIN [3][1030/1938]	Time 0.213 (0.255)	Data 1.30e-04 (8.30e-04)	Tok/s 48352 (54870)	Loss/tok 2.8848 (3.1630)	LR 1.000e-03
0: TRAIN [3][1040/1938]	Time 0.213 (0.255)	Data 1.21e-04 (8.23e-04)	Tok/s 49427 (54862)	Loss/tok 2.8577 (3.1629)	LR 1.000e-03
0: TRAIN [3][1050/1938]	Time 0.157 (0.256)	Data 1.46e-04 (8.17e-04)	Tok/s 34903 (54920)	Loss/tok 2.5935 (3.1644)	LR 1.000e-03
0: TRAIN [3][1060/1938]	Time 0.339 (0.256)	Data 1.10e-04 (8.11e-04)	Tok/s 68585 (54915)	Loss/tok 3.4099 (3.1646)	LR 1.000e-03
0: TRAIN [3][1070/1938]	Time 0.157 (0.255)	Data 1.54e-04 (8.05e-04)	Tok/s 33164 (54859)	Loss/tok 2.4895 (3.1633)	LR 1.000e-03
0: TRAIN [3][1080/1938]	Time 0.411 (0.255)	Data 1.18e-04 (7.98e-04)	Tok/s 73067 (54836)	Loss/tok 3.4264 (3.1631)	LR 1.000e-03
0: TRAIN [3][1090/1938]	Time 0.213 (0.255)	Data 1.30e-04 (7.93e-04)	Tok/s 49226 (54772)	Loss/tok 3.0208 (3.1620)	LR 1.000e-03
0: TRAIN [3][1100/1938]	Time 0.213 (0.255)	Data 1.32e-04 (7.87e-04)	Tok/s 48448 (54775)	Loss/tok 2.9404 (3.1616)	LR 1.000e-03
0: TRAIN [3][1110/1938]	Time 0.274 (0.255)	Data 1.37e-04 (7.81e-04)	Tok/s 60904 (54807)	Loss/tok 3.1739 (3.1617)	LR 1.000e-03
0: TRAIN [3][1120/1938]	Time 0.275 (0.255)	Data 1.21e-04 (7.76e-04)	Tok/s 61245 (54818)	Loss/tok 3.1949 (3.1617)	LR 1.000e-03
0: TRAIN [3][1130/1938]	Time 0.214 (0.255)	Data 1.24e-04 (7.70e-04)	Tok/s 48650 (54810)	Loss/tok 2.8763 (3.1611)	LR 1.000e-03
0: TRAIN [3][1140/1938]	Time 0.274 (0.255)	Data 1.36e-04 (7.64e-04)	Tok/s 60976 (54863)	Loss/tok 3.1056 (3.1620)	LR 1.000e-03
0: TRAIN [3][1150/1938]	Time 0.213 (0.255)	Data 1.39e-04 (7.59e-04)	Tok/s 48592 (54862)	Loss/tok 3.0195 (3.1614)	LR 1.000e-03
0: TRAIN [3][1160/1938]	Time 0.274 (0.255)	Data 1.32e-04 (7.53e-04)	Tok/s 60932 (54889)	Loss/tok 3.1822 (3.1618)	LR 1.000e-03
0: TRAIN [3][1170/1938]	Time 0.158 (0.255)	Data 1.31e-04 (7.48e-04)	Tok/s 33170 (54880)	Loss/tok 2.5374 (3.1611)	LR 1.000e-03
0: TRAIN [3][1180/1938]	Time 0.214 (0.255)	Data 1.39e-04 (7.43e-04)	Tok/s 48753 (54901)	Loss/tok 2.9557 (3.1609)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1190/1938]	Time 0.412 (0.255)	Data 1.31e-04 (7.38e-04)	Tok/s 72499 (54909)	Loss/tok 3.5812 (3.1616)	LR 1.000e-03
0: TRAIN [3][1200/1938]	Time 0.213 (0.256)	Data 1.45e-04 (7.33e-04)	Tok/s 49267 (54955)	Loss/tok 2.9316 (3.1624)	LR 1.000e-03
0: TRAIN [3][1210/1938]	Time 0.214 (0.256)	Data 1.18e-04 (7.28e-04)	Tok/s 49182 (54934)	Loss/tok 2.9842 (3.1613)	LR 1.000e-03
0: TRAIN [3][1220/1938]	Time 0.277 (0.255)	Data 1.50e-04 (7.23e-04)	Tok/s 61100 (54878)	Loss/tok 3.1347 (3.1601)	LR 1.000e-03
0: TRAIN [3][1230/1938]	Time 0.273 (0.255)	Data 1.35e-04 (7.19e-04)	Tok/s 60350 (54872)	Loss/tok 3.2067 (3.1597)	LR 1.000e-03
0: TRAIN [3][1240/1938]	Time 0.213 (0.255)	Data 1.28e-04 (7.14e-04)	Tok/s 48162 (54857)	Loss/tok 2.9350 (3.1595)	LR 1.000e-03
0: TRAIN [3][1250/1938]	Time 0.213 (0.255)	Data 1.24e-04 (7.09e-04)	Tok/s 49027 (54893)	Loss/tok 2.8849 (3.1593)	LR 1.000e-03
0: TRAIN [3][1260/1938]	Time 0.213 (0.255)	Data 1.15e-04 (7.05e-04)	Tok/s 48764 (54845)	Loss/tok 2.9285 (3.1585)	LR 1.000e-03
0: TRAIN [3][1270/1938]	Time 0.337 (0.255)	Data 1.33e-04 (7.00e-04)	Tok/s 69577 (54816)	Loss/tok 3.2775 (3.1579)	LR 1.000e-03
0: TRAIN [3][1280/1938]	Time 0.274 (0.255)	Data 1.47e-04 (6.96e-04)	Tok/s 61409 (54808)	Loss/tok 3.1803 (3.1579)	LR 1.000e-03
0: TRAIN [3][1290/1938]	Time 0.277 (0.255)	Data 1.22e-04 (6.91e-04)	Tok/s 61289 (54806)	Loss/tok 3.1055 (3.1572)	LR 1.000e-03
0: TRAIN [3][1300/1938]	Time 0.213 (0.255)	Data 1.42e-04 (6.87e-04)	Tok/s 47937 (54831)	Loss/tok 2.9698 (3.1580)	LR 1.000e-03
0: TRAIN [3][1310/1938]	Time 0.277 (0.255)	Data 1.32e-04 (6.83e-04)	Tok/s 60815 (54900)	Loss/tok 3.2437 (3.1589)	LR 1.000e-03
0: TRAIN [3][1320/1938]	Time 0.276 (0.255)	Data 1.35e-04 (6.79e-04)	Tok/s 61897 (54938)	Loss/tok 3.0875 (3.1590)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1330/1938]	Time 0.158 (0.255)	Data 1.43e-04 (6.75e-04)	Tok/s 33422 (54933)	Loss/tok 2.5839 (3.1590)	LR 1.000e-03
0: TRAIN [3][1340/1938]	Time 0.276 (0.255)	Data 1.43e-04 (6.71e-04)	Tok/s 60677 (54921)	Loss/tok 3.0823 (3.1584)	LR 1.000e-03
0: TRAIN [3][1350/1938]	Time 0.213 (0.255)	Data 1.21e-04 (6.67e-04)	Tok/s 48517 (54933)	Loss/tok 2.8622 (3.1583)	LR 1.000e-03
0: TRAIN [3][1360/1938]	Time 0.215 (0.255)	Data 2.65e-04 (6.63e-04)	Tok/s 48780 (54908)	Loss/tok 2.9998 (3.1577)	LR 1.000e-03
0: TRAIN [3][1370/1938]	Time 0.214 (0.255)	Data 1.35e-04 (6.59e-04)	Tok/s 47884 (54907)	Loss/tok 2.9832 (3.1577)	LR 1.000e-03
0: TRAIN [3][1380/1938]	Time 0.276 (0.255)	Data 1.38e-04 (6.56e-04)	Tok/s 60780 (54942)	Loss/tok 3.0904 (3.1578)	LR 1.000e-03
0: TRAIN [3][1390/1938]	Time 0.276 (0.256)	Data 1.16e-04 (6.52e-04)	Tok/s 60521 (54971)	Loss/tok 3.1722 (3.1591)	LR 1.000e-03
0: TRAIN [3][1400/1938]	Time 0.214 (0.256)	Data 1.44e-04 (6.48e-04)	Tok/s 48449 (54971)	Loss/tok 2.9333 (3.1586)	LR 1.000e-03
0: TRAIN [3][1410/1938]	Time 0.214 (0.255)	Data 1.19e-04 (6.45e-04)	Tok/s 47876 (54954)	Loss/tok 2.8649 (3.1579)	LR 1.000e-03
0: TRAIN [3][1420/1938]	Time 0.338 (0.255)	Data 1.38e-04 (6.41e-04)	Tok/s 68016 (54957)	Loss/tok 3.3369 (3.1575)	LR 1.000e-03
0: TRAIN [3][1430/1938]	Time 0.411 (0.256)	Data 1.47e-04 (6.38e-04)	Tok/s 72322 (54974)	Loss/tok 3.4740 (3.1580)	LR 1.000e-03
0: TRAIN [3][1440/1938]	Time 0.214 (0.256)	Data 1.33e-04 (6.35e-04)	Tok/s 47970 (54993)	Loss/tok 2.8889 (3.1580)	LR 1.000e-03
0: TRAIN [3][1450/1938]	Time 0.213 (0.256)	Data 1.43e-04 (6.31e-04)	Tok/s 47709 (54963)	Loss/tok 2.9321 (3.1572)	LR 1.000e-03
0: TRAIN [3][1460/1938]	Time 0.412 (0.256)	Data 1.32e-04 (6.28e-04)	Tok/s 73039 (54983)	Loss/tok 3.4795 (3.1577)	LR 1.000e-03
0: TRAIN [3][1470/1938]	Time 0.214 (0.256)	Data 1.27e-04 (6.25e-04)	Tok/s 48491 (54960)	Loss/tok 2.9739 (3.1570)	LR 1.000e-03
0: TRAIN [3][1480/1938]	Time 0.338 (0.255)	Data 1.18e-04 (6.21e-04)	Tok/s 69179 (54916)	Loss/tok 3.3474 (3.1564)	LR 1.000e-03
0: TRAIN [3][1490/1938]	Time 0.157 (0.255)	Data 1.17e-04 (6.19e-04)	Tok/s 32665 (54929)	Loss/tok 2.4851 (3.1571)	LR 5.000e-04
0: TRAIN [3][1500/1938]	Time 0.213 (0.255)	Data 1.38e-04 (6.15e-04)	Tok/s 49286 (54931)	Loss/tok 2.9727 (3.1569)	LR 5.000e-04
0: TRAIN [3][1510/1938]	Time 0.157 (0.255)	Data 1.16e-04 (6.12e-04)	Tok/s 33260 (54928)	Loss/tok 2.5700 (3.1567)	LR 5.000e-04
0: TRAIN [3][1520/1938]	Time 0.213 (0.255)	Data 1.37e-04 (6.09e-04)	Tok/s 49132 (54921)	Loss/tok 2.9005 (3.1563)	LR 5.000e-04
0: TRAIN [3][1530/1938]	Time 0.213 (0.255)	Data 1.14e-04 (6.06e-04)	Tok/s 48299 (54912)	Loss/tok 2.8588 (3.1559)	LR 5.000e-04
0: TRAIN [3][1540/1938]	Time 0.214 (0.255)	Data 1.25e-04 (6.03e-04)	Tok/s 48870 (54887)	Loss/tok 2.8844 (3.1555)	LR 5.000e-04
0: TRAIN [3][1550/1938]	Time 0.277 (0.255)	Data 1.32e-04 (6.00e-04)	Tok/s 61158 (54898)	Loss/tok 3.1581 (3.1552)	LR 5.000e-04
0: TRAIN [3][1560/1938]	Time 0.159 (0.255)	Data 1.31e-04 (5.98e-04)	Tok/s 33342 (54899)	Loss/tok 2.5218 (3.1553)	LR 5.000e-04
0: TRAIN [3][1570/1938]	Time 0.337 (0.255)	Data 1.33e-04 (5.95e-04)	Tok/s 68327 (54902)	Loss/tok 3.4133 (3.1558)	LR 5.000e-04
0: TRAIN [3][1580/1938]	Time 0.158 (0.255)	Data 1.46e-04 (5.93e-04)	Tok/s 33521 (54858)	Loss/tok 2.5323 (3.1549)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1590/1938]	Time 0.277 (0.255)	Data 1.35e-04 (5.90e-04)	Tok/s 60485 (54904)	Loss/tok 3.0954 (3.1556)	LR 5.000e-04
0: TRAIN [3][1600/1938]	Time 0.277 (0.255)	Data 1.22e-04 (5.87e-04)	Tok/s 60813 (54918)	Loss/tok 3.0788 (3.1551)	LR 5.000e-04
0: TRAIN [3][1610/1938]	Time 0.158 (0.256)	Data 1.50e-04 (5.84e-04)	Tok/s 32936 (54927)	Loss/tok 2.4882 (3.1552)	LR 5.000e-04
0: TRAIN [3][1620/1938]	Time 0.213 (0.255)	Data 1.23e-04 (5.81e-04)	Tok/s 47168 (54929)	Loss/tok 2.9825 (3.1544)	LR 5.000e-04
0: TRAIN [3][1630/1938]	Time 0.276 (0.256)	Data 1.35e-04 (5.79e-04)	Tok/s 60281 (54944)	Loss/tok 3.0466 (3.1548)	LR 5.000e-04
0: TRAIN [3][1640/1938]	Time 0.338 (0.256)	Data 1.17e-04 (5.76e-04)	Tok/s 68184 (54949)	Loss/tok 3.3813 (3.1551)	LR 5.000e-04
0: TRAIN [3][1650/1938]	Time 0.214 (0.256)	Data 1.18e-04 (5.74e-04)	Tok/s 47703 (54956)	Loss/tok 2.9858 (3.1549)	LR 5.000e-04
0: TRAIN [3][1660/1938]	Time 0.338 (0.256)	Data 1.17e-04 (5.71e-04)	Tok/s 68515 (54979)	Loss/tok 3.2568 (3.1546)	LR 5.000e-04
0: TRAIN [3][1670/1938]	Time 0.213 (0.256)	Data 1.27e-04 (5.68e-04)	Tok/s 48851 (54980)	Loss/tok 3.0015 (3.1543)	LR 5.000e-04
0: TRAIN [3][1680/1938]	Time 0.276 (0.256)	Data 1.41e-04 (5.66e-04)	Tok/s 61110 (54971)	Loss/tok 3.0867 (3.1545)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][1690/1938]	Time 0.414 (0.256)	Data 1.40e-04 (5.63e-04)	Tok/s 71859 (54976)	Loss/tok 3.4688 (3.1545)	LR 5.000e-04
0: TRAIN [3][1700/1938]	Time 0.213 (0.256)	Data 1.12e-04 (5.61e-04)	Tok/s 48668 (54966)	Loss/tok 3.0822 (3.1540)	LR 5.000e-04
0: TRAIN [3][1710/1938]	Time 0.213 (0.256)	Data 4.47e-04 (5.59e-04)	Tok/s 48344 (54950)	Loss/tok 3.0098 (3.1536)	LR 5.000e-04
0: TRAIN [3][1720/1938]	Time 0.338 (0.256)	Data 1.16e-04 (5.56e-04)	Tok/s 68922 (54948)	Loss/tok 3.3032 (3.1534)	LR 5.000e-04
0: TRAIN [3][1730/1938]	Time 0.214 (0.255)	Data 1.33e-04 (5.54e-04)	Tok/s 48437 (54902)	Loss/tok 2.8625 (3.1525)	LR 5.000e-04
0: TRAIN [3][1740/1938]	Time 0.213 (0.255)	Data 1.34e-04 (5.51e-04)	Tok/s 48405 (54884)	Loss/tok 3.0322 (3.1519)	LR 5.000e-04
0: TRAIN [3][1750/1938]	Time 0.212 (0.255)	Data 1.14e-04 (5.49e-04)	Tok/s 48680 (54899)	Loss/tok 2.8385 (3.1518)	LR 5.000e-04
0: TRAIN [3][1760/1938]	Time 0.276 (0.255)	Data 1.18e-04 (5.47e-04)	Tok/s 59614 (54910)	Loss/tok 3.1831 (3.1512)	LR 5.000e-04
0: TRAIN [3][1770/1938]	Time 0.337 (0.255)	Data 1.20e-04 (5.44e-04)	Tok/s 68435 (54903)	Loss/tok 3.2692 (3.1508)	LR 5.000e-04
0: TRAIN [3][1780/1938]	Time 0.337 (0.255)	Data 1.35e-04 (5.42e-04)	Tok/s 69492 (54925)	Loss/tok 3.3974 (3.1510)	LR 5.000e-04
0: TRAIN [3][1790/1938]	Time 0.213 (0.255)	Data 1.10e-04 (5.40e-04)	Tok/s 48435 (54926)	Loss/tok 2.9538 (3.1508)	LR 5.000e-04
0: TRAIN [3][1800/1938]	Time 0.213 (0.255)	Data 1.13e-04 (5.37e-04)	Tok/s 48356 (54933)	Loss/tok 2.9502 (3.1506)	LR 5.000e-04
0: TRAIN [3][1810/1938]	Time 0.214 (0.255)	Data 1.19e-04 (5.35e-04)	Tok/s 48923 (54950)	Loss/tok 2.9135 (3.1509)	LR 5.000e-04
0: TRAIN [3][1820/1938]	Time 0.213 (0.256)	Data 1.18e-04 (5.33e-04)	Tok/s 48331 (54962)	Loss/tok 2.9776 (3.1506)	LR 5.000e-04
0: TRAIN [3][1830/1938]	Time 0.213 (0.255)	Data 1.32e-04 (5.31e-04)	Tok/s 47876 (54951)	Loss/tok 3.0031 (3.1500)	LR 5.000e-04
0: TRAIN [3][1840/1938]	Time 0.214 (0.255)	Data 1.17e-04 (5.29e-04)	Tok/s 48931 (54938)	Loss/tok 2.9252 (3.1497)	LR 5.000e-04
0: TRAIN [3][1850/1938]	Time 0.212 (0.255)	Data 1.19e-04 (5.26e-04)	Tok/s 48803 (54933)	Loss/tok 2.8299 (3.1495)	LR 5.000e-04
0: TRAIN [3][1860/1938]	Time 0.157 (0.255)	Data 1.18e-04 (5.24e-04)	Tok/s 34241 (54912)	Loss/tok 2.6360 (3.1490)	LR 5.000e-04
0: TRAIN [3][1870/1938]	Time 0.276 (0.255)	Data 1.34e-04 (5.22e-04)	Tok/s 60517 (54911)	Loss/tok 3.1035 (3.1487)	LR 5.000e-04
0: TRAIN [3][1880/1938]	Time 0.276 (0.255)	Data 1.20e-04 (5.20e-04)	Tok/s 60938 (54901)	Loss/tok 3.1472 (3.1480)	LR 5.000e-04
0: TRAIN [3][1890/1938]	Time 0.277 (0.255)	Data 6.27e-04 (5.19e-04)	Tok/s 59852 (54885)	Loss/tok 3.0905 (3.1477)	LR 5.000e-04
0: TRAIN [3][1900/1938]	Time 0.214 (0.255)	Data 1.23e-04 (5.17e-04)	Tok/s 47965 (54885)	Loss/tok 2.9217 (3.1474)	LR 5.000e-04
0: TRAIN [3][1910/1938]	Time 0.337 (0.255)	Data 1.37e-04 (5.15e-04)	Tok/s 69256 (54875)	Loss/tok 3.3526 (3.1474)	LR 5.000e-04
0: TRAIN [3][1920/1938]	Time 0.213 (0.255)	Data 1.38e-04 (5.13e-04)	Tok/s 48707 (54863)	Loss/tok 2.9754 (3.1471)	LR 5.000e-04
0: TRAIN [3][1930/1938]	Time 0.276 (0.255)	Data 1.55e-04 (5.11e-04)	Tok/s 62478 (54844)	Loss/tok 3.0751 (3.1470)	LR 5.000e-04
:::MLL 1570030659.380 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 524}}
:::MLL 1570030659.381 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.657 (0.657)	Decoder iters 111.0 (111.0)	Tok/s 25071 (25071)
0: Running moses detokenizer
0: BLEU(score=24.277550280071896, counts=[37300, 18760, 10751, 6414], totals=[65644, 62641, 59638, 56640], precisions=[56.82164401925538, 29.948436327644835, 18.027096817465374, 11.32415254237288], bp=1.0, sys_len=65644, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1570030661.241 eval_accuracy: {"value": 24.28, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 535}}
:::MLL 1570030661.241 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 3	Training Loss: 3.1461	Test BLEU: 24.28
0: Performance: Epoch: 3	Training: 438871 Tok/s
0: Finished epoch 3
:::MLL 1570030661.242 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 557}}
0: Closing preprocessed data file
:::MLL 1570030661.242 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 568}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-10-02 03:37:52 PM
RESULT,RNN_TRANSLATOR,,2025,nvidia,2019-10-02 03:04:07 PM
