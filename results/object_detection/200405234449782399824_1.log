Beginning trial 1 of 2
Gathering sys log on node001
:::MLL 1586148550.706 submission_benchmark: {"value": "maskrcnn", "metadata": {"file": "mlperf_logger.py", "lineno": 213}}
:::MLL 1586148550.708 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_logger.py", "lineno": 218}}
WARNING: Log validation: Key "submission_division" is not in known maskrcnn keys.
:::MLL 1586148550.710 submission_division: {"value": "closed", "metadata": {"file": "mlperf_logger.py", "lineno": 222}}
:::MLL 1586148550.712 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_logger.py", "lineno": 226}}
:::MLL 1586148550.713 submission_platform: {"value": "1xPowerEdge R7525", "metadata": {"file": "mlperf_logger.py", "lineno": 230}}
:::MLL 1586148550.715 submission_entry: {"value": "{'hardware': 'PowerEdge R7525', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': ' ', 'os': '\\\\S / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.0-0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x AMD EPYC 7502 32-Core Processor', 'num_cores': '64', 'num_vcpus': '64', 'accelerator': 'Tesla V100-PCIE-32GB', 'num_accelerators': '3', 'sys_mem_size': '251 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '1x 931.5G', 'cpu_accel_interconnect': 'QPI', 'network_card': '', 'num_network_cards': '0', 'notes': ''}\"}", "metadata": {"file": "mlperf_logger.py", "lineno": 234}}
:::MLL 1586148550.717 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_logger.py", "lineno": 238}}
:::MLL 1586148550.719 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_logger.py", "lineno": 242}}
Clearing caches
:::MLL 1586148551.573 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node node001
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e 'MULTI_NODE= --master_port=4426' -e SLURM_JOB_ID=200405234449782399824 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_200405234449782399824 ./run_and_time.sh
+ DGXSYSTEM=DGX1
+ [[ -f config_DGX1.sh ]]
+ source config_DGX1.sh
++ EXTRA_PARAMS=
++ EXTRA_CONFIG=("SOLVER.BASE_LR" "0.03" "SOLVER.MAX_ITER" "160000" "SOLVER.WARMUP_FACTOR" "0.000096" "SOLVER.WARMUP_ITERS" "625" "SOLVER.WARMUP_METHOD" "mlperf_linear" "SOLVER.STEPS" "(48000, 64000)" "SOLVER.IMS_PER_BATCH" "39" "TEST.IMS_PER_BATCH" "3" "MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN" "6000" "NHWC" "True")
++ DGXNNODES=1
+++ sed 's/^config_//'
+++ sed 's/\.sh$//'
++++ readlink -f config_DGX1.sh
+++ basename /workspace/object_detection/config_DGX1.sh
++ DGXSYSTEM=DGX1
++ WALLTIME=14:00:00
++ DGXNGPU=3
++ DGXSOCKETCORES=16
++ DGXNSOCKET=2
++ DGXHT=1
++ DGXIBDEVICES=
++ BIND_LAUNCH=0
+ SLURM_NTASKS_PER_NODE=3
+ SLURM_JOB_ID=200405234449782399824
+ MULTI_NODE=' --master_port=4426'
+ echo 'Run vars: id 200405234449782399824 gpus 3 mparams  --master_port=4426'
+ set -e
Run vars: id 200405234449782399824 gpus 3 mparams  --master_port=4426
++ date +%s
+ start=1586148551
++ date '+%Y-%m-%d %r'
+ start_fmt='2020-04-06 04:49:11 AM'
+ echo 'STARTING TIMING RUN AT 2020-04-06 04:49:11 AM'
STARTING TIMING RUN AT 2020-04-06 04:49:11 AM
+ set -x
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ '[' '!' -f /coco ']'
+ ln -sf /data/coco2017 /coco
++ ls /data
coco2017 ilsvrc12_passthrough torchvision
+ echo coco2017 ilsvrc12_passthrough torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 16 --no_hyperthreads --no_membind --nproc_per_node 3 --master_port=4426 tools/train_mlperf.py --config-file configs/e2e_mask_rcnn_R_50_FPN_1x.yaml DTYPE float16 PATHS_CATALOG maskrcnn_benchmark/config/paths_catalog_dbcluster.py MODEL.WEIGHT /coco/models/R-50.pkl DISABLE_REDUCED_LOGGING True SOLVER.BASE_LR 0.03 SOLVER.MAX_ITER 160000 SOLVER.WARMUP_FACTOR 0.000096 SOLVER.WARMUP_ITERS 625 SOLVER.WARMUP_METHOD mlperf_linear SOLVER.STEPS '(48000, 64000)' SOLVER.IMS_PER_BATCH 39 TEST.IMS_PER_BATCH 3 MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN 6000 NHWC True
:::MLL 1586148553.768 init_start: {"value": null, "metadata": {"file": "tools/train_mlperf.py", "lineno": 262}}
:::MLL 1586148553.770 init_start: {"value": null, "metadata": {"file": "tools/train_mlperf.py", "lineno": 262}}
:::MLL 1586148553.770 init_start: {"value": null, "metadata": {"file": "tools/train_mlperf.py", "lineno": 262}}
2020-04-06 04:49:19,594 maskrcnn_benchmark INFO: Using 3 GPUs
2020-04-06 04:49:19,594 maskrcnn_benchmark INFO: Namespace(config_file='configs/e2e_mask_rcnn_R_50_FPN_1x.yaml', distributed=True, local_rank=0, opts=['DTYPE', 'float16', 'PATHS_CATALOG', 'maskrcnn_benchmark/config/paths_catalog_dbcluster.py', 'MODEL.WEIGHT', '/coco/models/R-50.pkl', 'DISABLE_REDUCED_LOGGING', 'True', 'SOLVER.BASE_LR', '0.03', 'SOLVER.MAX_ITER', '160000', 'SOLVER.WARMUP_FACTOR', '0.000096', 'SOLVER.WARMUP_ITERS', '625', 'SOLVER.WARMUP_METHOD', 'mlperf_linear', 'SOLVER.STEPS', '(48000, 64000)', 'SOLVER.IMS_PER_BATCH', '39', 'TEST.IMS_PER_BATCH', '3', 'MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN', '6000', 'NHWC', 'True'], seed=2507167744)
2020-04-06 04:49:19,595 maskrcnn_benchmark INFO: Worker 0: Setting seed 4202064271
2020-04-06 04:49:19,595 maskrcnn_benchmark INFO: Collecting env info (might take some time)
2020-04-06 04:49:23,321 maskrcnn_benchmark INFO: 
PyTorch version: 1.1.0a0+828a6a3
Is debug build: No
CUDA used to build PyTorch: 10.1.163

OS: Ubuntu 16.04.6 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.1.163
GPU models and configuration: 
GPU 0: Tesla V100-PCIE-32GB
GPU 1: Tesla V100-PCIE-32GB
GPU 2: Tesla V100-PCIE-32GB

Nvidia driver version: 440.33.01
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.0

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.16.3
[pip] torch==1.1.0a0+828a6a3
[pip] torchtext==0.4.0
[pip] torchvision==0.2.1
[conda] magma-cuda100             2.1.0                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] torch                     1.1.0a0+828a6a3          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.2.1                    pypi_0    pypi
        Pillow (5.3.0.post1)
2020-04-06 04:49:23,321 maskrcnn_benchmark INFO: Loaded configuration file configs/e2e_mask_rcnn_R_50_FPN_1x.yaml
2020-04-06 04:49:23,322 maskrcnn_benchmark INFO: 
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/MSRA/R-50"
  BACKBONE:
    CONV_BODY: "R-50-FPN"
    OUT_CHANNELS: 256
  RPN:
    USE_FPN: True
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    PRE_NMS_TOP_N_TRAIN: 2000
    PRE_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
  ROI_HEADS:
    USE_FPN: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
  ROI_MASK_HEAD:
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    FEATURE_EXTRACTOR: "MaskRCNNFPNFeatureExtractor"
    PREDICTOR: "MaskRCNNC4Predictor"
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 2
    RESOLUTION: 28
    SHARE_BOX_FEATURE_EXTRACTOR: False
  MASK_ON: True
DATASETS:
  TRAIN: ("coco_2017_train",)
  TEST: ("coco_2017_val",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BASE_LR: 0.02
  WEIGHT_DECAY: 0.0001
  STEPS: (60000, 80000)
  MAX_ITER: 90000

2020-04-06 04:49:23,322 maskrcnn_benchmark INFO: Running with config:
AMP_VERBOSE: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 4
  SIZE_DIVISIBILITY: 32
DATASETS:
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
DISABLE_REDUCED_LOGGING: True
DTYPE: float16
INPUT:
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (800,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  TO_BGR255: True
MLPERF:
  MIN_BBOX_MAP: 0.377
  MIN_SEGM_MAP: 0.339
MODEL:
  BACKBONE:
    CONV_BODY: R-50-FPN
    FREEZE_CONV_BODY_AT: 2
    OUT_CHANNELS: 256
    USE_GN: False
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  RESNETS:
    NUM_GROUPS: 1
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 64
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 81
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.5
    DETECTIONS_PER_IMG: 100
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.5
    POSITIVE_FRACTION: 0.25
    SCORE_THRESH: 0.05
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: MaskRCNNFPNFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 28
    SHARE_BOX_FEATURE_EXTRACTOR: False
    USE_GN: False
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 6000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 2000
    PRE_NMS_TOP_N_TEST: 1000
    PRE_NMS_TOP_N_TRAIN: 2000
    RPN_HEAD: SingleConvRPNHead
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  WEIGHT: /coco/models/R-50.pkl
NHWC: True
OUTPUT_DIR: .
PATHS_CATALOG: maskrcnn_benchmark/config/paths_catalog_dbcluster.py
PER_EPOCH_EVAL: True
SAVE_CHECKPOINTS: False
SOLVER:
  BASE_LR: 0.03
  BIAS_LR_FACTOR: 2
  CHECKPOINT_PERIOD: 2500
  GAMMA: 0.1
  IMS_PER_BATCH: 39
  MAX_ITER: 160000
  MOMENTUM: 0.9
  STEPS: (48000, 64000)
  WARMUP_FACTOR: 9.6e-05
  WARMUP_ITERS: 625
  WARMUP_METHOD: mlperf_linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0
TEST:
  DETECTIONS_PER_IMG: 100
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 3
:::MLL 1586148563.326 global_batch_size: {"value": 39, "metadata": {"file": "tools/train_mlperf.py", "lineno": 147}}
:::MLL 1586148563.327 num_image_candidates: {"value": 6000, "metadata": {"file": "tools/train_mlperf.py", "lineno": 148}}
:::MLL 1586148564.690 opt_base_learning_rate: {"value": 0.03, "metadata": {"file": "tools/train_mlperf.py", "lineno": 157}}
:::MLL 1586148564.691 opt_learning_rate_warmup_steps: {"value": 625, "metadata": {"file": "tools/train_mlperf.py", "lineno": 158}}
:::MLL 1586148564.691 opt_learning_rate_warmup_factor: {"value": 9.6e-05, "metadata": {"file": "tools/train_mlperf.py", "lineno": 159}}
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
2020-04-06 04:49:24,709 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from /coco/models/R-50.pkl
2020-04-06 04:49:24,933 maskrcnn_benchmark.utils.c2_model_loading INFO: Remapping C2 weights
2020-04-06 04:49:24,934 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: conv1_b              mapped name: conv1.bias
2020-04-06 04:49:24,934 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: conv1_w              mapped name: conv1.weight
2020-04-06 04:49:24,934 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: fc1000_b             mapped name: fc1000.bias
2020-04-06 04:49:24,934 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: fc1000_w             mapped name: fc1000.weight
2020-04-06 04:49:24,934 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch1_b     mapped name: layer1.0.downsample.0.bias
2020-04-06 04:49:24,934 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch1_bn_b  mapped name: layer1.0.downsample.1.bias
2020-04-06 04:49:24,934 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch1_bn_s  mapped name: layer1.0.downsample.1.weight
2020-04-06 04:49:24,935 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch1_w     mapped name: layer1.0.downsample.0.weight
2020-04-06 04:49:24,935 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2a_b    mapped name: layer1.0.conv1.bias
2020-04-06 04:49:24,935 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2a_bn_b mapped name: layer1.0.bn1.bias
2020-04-06 04:49:24,935 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2a_bn_s mapped name: layer1.0.bn1.weight
2020-04-06 04:49:24,935 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2a_w    mapped name: layer1.0.conv1.weight
2020-04-06 04:49:24,935 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2b_b    mapped name: layer1.0.conv2.bias
2020-04-06 04:49:24,935 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2b_bn_b mapped name: layer1.0.bn2.bias
2020-04-06 04:49:24,935 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2b_bn_s mapped name: layer1.0.bn2.weight
2020-04-06 04:49:24,935 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2b_w    mapped name: layer1.0.conv2.weight
2020-04-06 04:49:24,935 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2c_b    mapped name: layer1.0.conv3.bias
2020-04-06 04:49:24,935 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2c_bn_b mapped name: layer1.0.bn3.bias
2020-04-06 04:49:24,935 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2c_bn_s mapped name: layer1.0.bn3.weight
2020-04-06 04:49:24,935 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2c_w    mapped name: layer1.0.conv3.weight
2020-04-06 04:49:24,935 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2a_b    mapped name: layer1.1.conv1.bias
2020-04-06 04:49:24,936 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2a_bn_b mapped name: layer1.1.bn1.bias
2020-04-06 04:49:24,936 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2a_bn_s mapped name: layer1.1.bn1.weight
2020-04-06 04:49:24,936 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2a_w    mapped name: layer1.1.conv1.weight
2020-04-06 04:49:24,936 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2b_b    mapped name: layer1.1.conv2.bias
2020-04-06 04:49:24,936 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2b_bn_b mapped name: layer1.1.bn2.bias
2020-04-06 04:49:24,936 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2b_bn_s mapped name: layer1.1.bn2.weight
2020-04-06 04:49:24,936 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2b_w    mapped name: layer1.1.conv2.weight
2020-04-06 04:49:24,936 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2c_b    mapped name: layer1.1.conv3.bias
2020-04-06 04:49:24,936 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2c_bn_b mapped name: layer1.1.bn3.bias
2020-04-06 04:49:24,936 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2c_bn_s mapped name: layer1.1.bn3.weight
2020-04-06 04:49:24,936 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2c_w    mapped name: layer1.1.conv3.weight
2020-04-06 04:49:24,936 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2a_b    mapped name: layer1.2.conv1.bias
2020-04-06 04:49:24,936 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2a_bn_b mapped name: layer1.2.bn1.bias
2020-04-06 04:49:24,937 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2a_bn_s mapped name: layer1.2.bn1.weight
2020-04-06 04:49:24,937 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2a_w    mapped name: layer1.2.conv1.weight
2020-04-06 04:49:24,937 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2b_b    mapped name: layer1.2.conv2.bias
2020-04-06 04:49:24,937 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2b_bn_b mapped name: layer1.2.bn2.bias
2020-04-06 04:49:24,937 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2b_bn_s mapped name: layer1.2.bn2.weight
2020-04-06 04:49:24,937 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2b_w    mapped name: layer1.2.conv2.weight
2020-04-06 04:49:24,937 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2c_b    mapped name: layer1.2.conv3.bias
2020-04-06 04:49:24,937 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2c_bn_b mapped name: layer1.2.bn3.bias
2020-04-06 04:49:24,937 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2c_bn_s mapped name: layer1.2.bn3.weight
2020-04-06 04:49:24,937 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2c_w    mapped name: layer1.2.conv3.weight
2020-04-06 04:49:24,937 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch1_b     mapped name: layer2.0.downsample.0.bias
2020-04-06 04:49:24,937 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch1_bn_b  mapped name: layer2.0.downsample.1.bias
2020-04-06 04:49:24,937 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch1_bn_s  mapped name: layer2.0.downsample.1.weight
2020-04-06 04:49:24,937 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch1_w     mapped name: layer2.0.downsample.0.weight
2020-04-06 04:49:24,938 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2a_b    mapped name: layer2.0.conv1.bias
2020-04-06 04:49:24,938 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2a_bn_b mapped name: layer2.0.bn1.bias
2020-04-06 04:49:24,938 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2a_bn_s mapped name: layer2.0.bn1.weight
2020-04-06 04:49:24,938 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2a_w    mapped name: layer2.0.conv1.weight
2020-04-06 04:49:24,938 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2b_b    mapped name: layer2.0.conv2.bias
2020-04-06 04:49:24,938 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2b_bn_b mapped name: layer2.0.bn2.bias
2020-04-06 04:49:24,938 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2b_bn_s mapped name: layer2.0.bn2.weight
2020-04-06 04:49:24,938 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2b_w    mapped name: layer2.0.conv2.weight
2020-04-06 04:49:24,938 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2c_b    mapped name: layer2.0.conv3.bias
2020-04-06 04:49:24,938 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2c_bn_b mapped name: layer2.0.bn3.bias
2020-04-06 04:49:24,938 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2c_bn_s mapped name: layer2.0.bn3.weight
2020-04-06 04:49:24,938 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2c_w    mapped name: layer2.0.conv3.weight
2020-04-06 04:49:24,938 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2a_b    mapped name: layer2.1.conv1.bias
2020-04-06 04:49:24,938 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2a_bn_b mapped name: layer2.1.bn1.bias
2020-04-06 04:49:24,939 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2a_bn_s mapped name: layer2.1.bn1.weight
2020-04-06 04:49:24,939 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2a_w    mapped name: layer2.1.conv1.weight
2020-04-06 04:49:24,939 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2b_b    mapped name: layer2.1.conv2.bias
2020-04-06 04:49:24,939 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2b_bn_b mapped name: layer2.1.bn2.bias
2020-04-06 04:49:24,939 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2b_bn_s mapped name: layer2.1.bn2.weight
2020-04-06 04:49:24,939 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2b_w    mapped name: layer2.1.conv2.weight
2020-04-06 04:49:24,939 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2c_b    mapped name: layer2.1.conv3.bias
2020-04-06 04:49:24,939 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2c_bn_b mapped name: layer2.1.bn3.bias
2020-04-06 04:49:24,939 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2c_bn_s mapped name: layer2.1.bn3.weight
2020-04-06 04:49:24,939 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2c_w    mapped name: layer2.1.conv3.weight
2020-04-06 04:49:24,939 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2a_b    mapped name: layer2.2.conv1.bias
2020-04-06 04:49:24,939 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2a_bn_b mapped name: layer2.2.bn1.bias
2020-04-06 04:49:24,939 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2a_bn_s mapped name: layer2.2.bn1.weight
2020-04-06 04:49:24,940 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2a_w    mapped name: layer2.2.conv1.weight
2020-04-06 04:49:24,940 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2b_b    mapped name: layer2.2.conv2.bias
2020-04-06 04:49:24,940 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2b_bn_b mapped name: layer2.2.bn2.bias
2020-04-06 04:49:24,940 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2b_bn_s mapped name: layer2.2.bn2.weight
2020-04-06 04:49:24,940 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2b_w    mapped name: layer2.2.conv2.weight
2020-04-06 04:49:24,940 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2c_b    mapped name: layer2.2.conv3.bias
2020-04-06 04:49:24,940 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2c_bn_b mapped name: layer2.2.bn3.bias
2020-04-06 04:49:24,940 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2c_bn_s mapped name: layer2.2.bn3.weight
2020-04-06 04:49:24,940 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2c_w    mapped name: layer2.2.conv3.weight
2020-04-06 04:49:24,940 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2a_b    mapped name: layer2.3.conv1.bias
2020-04-06 04:49:24,940 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2a_bn_b mapped name: layer2.3.bn1.bias
2020-04-06 04:49:24,940 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2a_bn_s mapped name: layer2.3.bn1.weight
2020-04-06 04:49:24,940 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2a_w    mapped name: layer2.3.conv1.weight
2020-04-06 04:49:24,941 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2b_b    mapped name: layer2.3.conv2.bias
2020-04-06 04:49:24,941 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2b_bn_b mapped name: layer2.3.bn2.bias
2020-04-06 04:49:24,941 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2b_bn_s mapped name: layer2.3.bn2.weight
2020-04-06 04:49:24,941 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2b_w    mapped name: layer2.3.conv2.weight
2020-04-06 04:49:24,941 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2c_b    mapped name: layer2.3.conv3.bias
2020-04-06 04:49:24,941 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2c_bn_b mapped name: layer2.3.bn3.bias
2020-04-06 04:49:24,941 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2c_bn_s mapped name: layer2.3.bn3.weight
2020-04-06 04:49:24,941 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2c_w    mapped name: layer2.3.conv3.weight
2020-04-06 04:49:24,941 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch1_b     mapped name: layer3.0.downsample.0.bias
2020-04-06 04:49:24,941 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch1_bn_b  mapped name: layer3.0.downsample.1.bias
2020-04-06 04:49:24,941 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch1_bn_s  mapped name: layer3.0.downsample.1.weight
2020-04-06 04:49:24,941 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch1_w     mapped name: layer3.0.downsample.0.weight
2020-04-06 04:49:24,941 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2a_b    mapped name: layer3.0.conv1.bias
2020-04-06 04:49:24,941 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2a_bn_b mapped name: layer3.0.bn1.bias
2020-04-06 04:49:24,942 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2a_bn_s mapped name: layer3.0.bn1.weight
2020-04-06 04:49:24,942 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2a_w    mapped name: layer3.0.conv1.weight
2020-04-06 04:49:24,942 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2b_b    mapped name: layer3.0.conv2.bias
2020-04-06 04:49:24,942 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2b_bn_b mapped name: layer3.0.bn2.bias
2020-04-06 04:49:24,942 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2b_bn_s mapped name: layer3.0.bn2.weight
2020-04-06 04:49:24,942 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2b_w    mapped name: layer3.0.conv2.weight
2020-04-06 04:49:24,942 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2c_b    mapped name: layer3.0.conv3.bias
2020-04-06 04:49:24,942 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2c_bn_b mapped name: layer3.0.bn3.bias
2020-04-06 04:49:24,942 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2c_bn_s mapped name: layer3.0.bn3.weight
2020-04-06 04:49:24,942 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2c_w    mapped name: layer3.0.conv3.weight
2020-04-06 04:49:24,942 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2a_b    mapped name: layer3.1.conv1.bias
2020-04-06 04:49:24,942 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2a_bn_b mapped name: layer3.1.bn1.bias
2020-04-06 04:49:24,942 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2a_bn_s mapped name: layer3.1.bn1.weight
2020-04-06 04:49:24,943 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2a_w    mapped name: layer3.1.conv1.weight
2020-04-06 04:49:24,943 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2b_b    mapped name: layer3.1.conv2.bias
2020-04-06 04:49:24,943 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2b_bn_b mapped name: layer3.1.bn2.bias
2020-04-06 04:49:24,943 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2b_bn_s mapped name: layer3.1.bn2.weight
2020-04-06 04:49:24,943 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2b_w    mapped name: layer3.1.conv2.weight
2020-04-06 04:49:24,943 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2c_b    mapped name: layer3.1.conv3.bias
2020-04-06 04:49:24,943 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2c_bn_b mapped name: layer3.1.bn3.bias
2020-04-06 04:49:24,943 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2c_bn_s mapped name: layer3.1.bn3.weight
2020-04-06 04:49:24,943 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2c_w    mapped name: layer3.1.conv3.weight
2020-04-06 04:49:24,943 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2a_b    mapped name: layer3.2.conv1.bias
2020-04-06 04:49:24,943 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2a_bn_b mapped name: layer3.2.bn1.bias
2020-04-06 04:49:24,943 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2a_bn_s mapped name: layer3.2.bn1.weight
2020-04-06 04:49:24,943 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2a_w    mapped name: layer3.2.conv1.weight
2020-04-06 04:49:24,944 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2b_b    mapped name: layer3.2.conv2.bias
2020-04-06 04:49:24,944 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2b_bn_b mapped name: layer3.2.bn2.bias
2020-04-06 04:49:24,944 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2b_bn_s mapped name: layer3.2.bn2.weight
2020-04-06 04:49:24,944 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2b_w    mapped name: layer3.2.conv2.weight
2020-04-06 04:49:24,944 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2c_b    mapped name: layer3.2.conv3.bias
2020-04-06 04:49:24,944 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2c_bn_b mapped name: layer3.2.bn3.bias
2020-04-06 04:49:24,944 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2c_bn_s mapped name: layer3.2.bn3.weight
2020-04-06 04:49:24,944 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2c_w    mapped name: layer3.2.conv3.weight
2020-04-06 04:49:24,944 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2a_b    mapped name: layer3.3.conv1.bias
2020-04-06 04:49:24,944 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2a_bn_b mapped name: layer3.3.bn1.bias
2020-04-06 04:49:24,944 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2a_bn_s mapped name: layer3.3.bn1.weight
2020-04-06 04:49:24,944 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2a_w    mapped name: layer3.3.conv1.weight
2020-04-06 04:49:24,944 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2b_b    mapped name: layer3.3.conv2.bias
2020-04-06 04:49:24,944 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2b_bn_b mapped name: layer3.3.bn2.bias
2020-04-06 04:49:24,945 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2b_bn_s mapped name: layer3.3.bn2.weight
2020-04-06 04:49:24,945 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2b_w    mapped name: layer3.3.conv2.weight
2020-04-06 04:49:24,945 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2c_b    mapped name: layer3.3.conv3.bias
2020-04-06 04:49:24,945 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2c_bn_b mapped name: layer3.3.bn3.bias
2020-04-06 04:49:24,945 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2c_bn_s mapped name: layer3.3.bn3.weight
2020-04-06 04:49:24,945 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2c_w    mapped name: layer3.3.conv3.weight
2020-04-06 04:49:24,945 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2a_b    mapped name: layer3.4.conv1.bias
2020-04-06 04:49:24,945 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2a_bn_b mapped name: layer3.4.bn1.bias
2020-04-06 04:49:24,945 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2a_bn_s mapped name: layer3.4.bn1.weight
2020-04-06 04:49:24,945 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2a_w    mapped name: layer3.4.conv1.weight
2020-04-06 04:49:24,945 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2b_b    mapped name: layer3.4.conv2.bias
2020-04-06 04:49:24,945 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2b_bn_b mapped name: layer3.4.bn2.bias
2020-04-06 04:49:24,945 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2b_bn_s mapped name: layer3.4.bn2.weight
2020-04-06 04:49:24,946 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2b_w    mapped name: layer3.4.conv2.weight
2020-04-06 04:49:24,946 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2c_b    mapped name: layer3.4.conv3.bias
2020-04-06 04:49:24,946 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2c_bn_b mapped name: layer3.4.bn3.bias
2020-04-06 04:49:24,946 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2c_bn_s mapped name: layer3.4.bn3.weight
2020-04-06 04:49:24,946 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2c_w    mapped name: layer3.4.conv3.weight
2020-04-06 04:49:24,946 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2a_b    mapped name: layer3.5.conv1.bias
2020-04-06 04:49:24,946 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2a_bn_b mapped name: layer3.5.bn1.bias
2020-04-06 04:49:24,946 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2a_bn_s mapped name: layer3.5.bn1.weight
2020-04-06 04:49:24,946 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2a_w    mapped name: layer3.5.conv1.weight
2020-04-06 04:49:24,946 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2b_b    mapped name: layer3.5.conv2.bias
2020-04-06 04:49:24,946 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2b_bn_b mapped name: layer3.5.bn2.bias
2020-04-06 04:49:24,946 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2b_bn_s mapped name: layer3.5.bn2.weight
2020-04-06 04:49:24,946 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2b_w    mapped name: layer3.5.conv2.weight
2020-04-06 04:49:24,946 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2c_b    mapped name: layer3.5.conv3.bias
2020-04-06 04:49:24,947 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2c_bn_b mapped name: layer3.5.bn3.bias
2020-04-06 04:49:24,947 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2c_bn_s mapped name: layer3.5.bn3.weight
2020-04-06 04:49:24,947 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2c_w    mapped name: layer3.5.conv3.weight
2020-04-06 04:49:24,947 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch1_b     mapped name: layer4.0.downsample.0.bias
2020-04-06 04:49:24,947 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch1_bn_b  mapped name: layer4.0.downsample.1.bias
2020-04-06 04:49:24,947 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch1_bn_s  mapped name: layer4.0.downsample.1.weight
2020-04-06 04:49:24,947 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch1_w     mapped name: layer4.0.downsample.0.weight
2020-04-06 04:49:24,947 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2a_b    mapped name: layer4.0.conv1.bias
2020-04-06 04:49:24,947 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2a_bn_b mapped name: layer4.0.bn1.bias
2020-04-06 04:49:24,947 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2a_bn_s mapped name: layer4.0.bn1.weight
2020-04-06 04:49:24,947 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2a_w    mapped name: layer4.0.conv1.weight
2020-04-06 04:49:24,947 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2b_b    mapped name: layer4.0.conv2.bias
2020-04-06 04:49:24,947 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2b_bn_b mapped name: layer4.0.bn2.bias
2020-04-06 04:49:24,948 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2b_bn_s mapped name: layer4.0.bn2.weight
2020-04-06 04:49:24,948 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2b_w    mapped name: layer4.0.conv2.weight
2020-04-06 04:49:24,948 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2c_b    mapped name: layer4.0.conv3.bias
2020-04-06 04:49:24,948 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2c_bn_b mapped name: layer4.0.bn3.bias
2020-04-06 04:49:24,948 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2c_bn_s mapped name: layer4.0.bn3.weight
2020-04-06 04:49:24,948 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2c_w    mapped name: layer4.0.conv3.weight
2020-04-06 04:49:24,948 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2a_b    mapped name: layer4.1.conv1.bias
2020-04-06 04:49:24,948 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2a_bn_b mapped name: layer4.1.bn1.bias
2020-04-06 04:49:24,948 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2a_bn_s mapped name: layer4.1.bn1.weight
2020-04-06 04:49:24,948 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2a_w    mapped name: layer4.1.conv1.weight
2020-04-06 04:49:24,948 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2b_b    mapped name: layer4.1.conv2.bias
2020-04-06 04:49:24,948 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2b_bn_b mapped name: layer4.1.bn2.bias
2020-04-06 04:49:24,948 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2b_bn_s mapped name: layer4.1.bn2.weight
2020-04-06 04:49:24,948 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2b_w    mapped name: layer4.1.conv2.weight
2020-04-06 04:49:24,949 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2c_b    mapped name: layer4.1.conv3.bias
2020-04-06 04:49:24,949 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2c_bn_b mapped name: layer4.1.bn3.bias
2020-04-06 04:49:24,949 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2c_bn_s mapped name: layer4.1.bn3.weight
2020-04-06 04:49:24,949 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2c_w    mapped name: layer4.1.conv3.weight
2020-04-06 04:49:24,949 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2a_b    mapped name: layer4.2.conv1.bias
2020-04-06 04:49:24,949 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2a_bn_b mapped name: layer4.2.bn1.bias
2020-04-06 04:49:24,949 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2a_bn_s mapped name: layer4.2.bn1.weight
2020-04-06 04:49:24,949 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2a_w    mapped name: layer4.2.conv1.weight
2020-04-06 04:49:24,949 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2b_b    mapped name: layer4.2.conv2.bias
2020-04-06 04:49:24,949 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2b_bn_b mapped name: layer4.2.bn2.bias
2020-04-06 04:49:24,949 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2b_bn_s mapped name: layer4.2.bn2.weight
2020-04-06 04:49:24,949 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2b_w    mapped name: layer4.2.conv2.weight
2020-04-06 04:49:24,949 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2c_b    mapped name: layer4.2.conv3.bias
2020-04-06 04:49:24,949 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2c_bn_b mapped name: layer4.2.bn3.bias
2020-04-06 04:49:24,950 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2c_bn_s mapped name: layer4.2.bn3.weight
2020-04-06 04:49:24,950 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2c_w    mapped name: layer4.2.conv3.weight
2020-04-06 04:49:24,950 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res_conv1_bn_b       mapped name: bn1.bias
2020-04-06 04:49:24,950 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res_conv1_bn_s       mapped name: bn1.weight
2020-04-06 04:49:24,973 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.bn1.bias                   loaded from layer1.0.bn1.bias            of shape (64,)
2020-04-06 04:49:24,973 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.bn1.weight                 loaded from layer1.0.bn1.weight          of shape (64,)
2020-04-06 04:49:24,973 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.bn2.bias                   loaded from layer1.0.bn2.bias            of shape (64,)
2020-04-06 04:49:24,973 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.bn2.weight                 loaded from layer1.0.bn2.weight          of shape (64,)
2020-04-06 04:49:24,973 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.bn3.bias                   loaded from layer1.0.bn3.bias            of shape (256,)
2020-04-06 04:49:24,974 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.bn3.weight                 loaded from layer1.0.bn3.weight          of shape (256,)
2020-04-06 04:49:24,974 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.conv1.weight               loaded from layer1.0.conv1.weight        of shape (64, 64, 1, 1)
2020-04-06 04:49:24,974 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.conv2.weight               loaded from layer1.0.conv2.weight        of shape (64, 64, 3, 3)
2020-04-06 04:49:24,974 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.conv3.weight               loaded from layer1.0.conv3.weight        of shape (256, 64, 1, 1)
2020-04-06 04:49:24,974 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.downsample.0.weight        loaded from layer1.0.downsample.0.weight of shape (256, 64, 1, 1)
2020-04-06 04:49:24,974 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.downsample.1.bias          loaded from layer1.0.downsample.1.bias   of shape (256,)
2020-04-06 04:49:24,974 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.downsample.1.weight        loaded from layer1.0.downsample.1.weight of shape (256,)
2020-04-06 04:49:24,974 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.1.bn1.bias                   loaded from layer1.1.bn1.bias            of shape (64,)
2020-04-06 04:49:24,974 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.1.bn1.weight                 loaded from layer1.1.bn1.weight          of shape (64,)
2020-04-06 04:49:24,974 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.1.bn2.bias                   loaded from layer1.1.bn2.bias            of shape (64,)
2020-04-06 04:49:24,974 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.1.bn2.weight                 loaded from layer1.1.bn2.weight          of shape (64,)
2020-04-06 04:49:24,975 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.1.bn3.bias                   loaded from layer1.1.bn3.bias            of shape (256,)
2020-04-06 04:49:24,975 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.1.bn3.weight                 loaded from layer1.1.bn3.weight          of shape (256,)
2020-04-06 04:49:24,975 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.1.conv1.weight               loaded from layer1.1.conv1.weight        of shape (64, 256, 1, 1)
2020-04-06 04:49:24,975 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.1.conv2.weight               loaded from layer1.1.conv2.weight        of shape (64, 64, 3, 3)
2020-04-06 04:49:24,975 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.1.conv3.weight               loaded from layer1.1.conv3.weight        of shape (256, 64, 1, 1)
2020-04-06 04:49:24,975 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.2.bn1.bias                   loaded from layer1.2.bn1.bias            of shape (64,)
2020-04-06 04:49:24,975 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.2.bn1.weight                 loaded from layer1.2.bn1.weight          of shape (64,)
2020-04-06 04:49:24,975 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.2.bn2.bias                   loaded from layer1.2.bn2.bias            of shape (64,)
2020-04-06 04:49:24,975 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.2.bn2.weight                 loaded from layer1.2.bn2.weight          of shape (64,)
2020-04-06 04:49:24,975 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.2.bn3.bias                   loaded from layer1.2.bn3.bias            of shape (256,)
2020-04-06 04:49:24,975 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.2.bn3.weight                 loaded from layer1.2.bn3.weight          of shape (256,)
2020-04-06 04:49:24,976 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.2.conv1.weight               loaded from layer1.2.conv1.weight        of shape (64, 256, 1, 1)
2020-04-06 04:49:24,976 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.2.conv2.weight               loaded from layer1.2.conv2.weight        of shape (64, 64, 3, 3)
2020-04-06 04:49:24,976 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.2.conv3.weight               loaded from layer1.2.conv3.weight        of shape (256, 64, 1, 1)
2020-04-06 04:49:24,976 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.bn1.bias                   loaded from layer2.0.bn1.bias            of shape (128,)
2020-04-06 04:49:24,976 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.bn1.weight                 loaded from layer2.0.bn1.weight          of shape (128,)
2020-04-06 04:49:24,976 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.bn2.bias                   loaded from layer2.0.bn2.bias            of shape (128,)
2020-04-06 04:49:24,976 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.bn2.weight                 loaded from layer2.0.bn2.weight          of shape (128,)
2020-04-06 04:49:24,976 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.bn3.bias                   loaded from layer2.0.bn3.bias            of shape (512,)
2020-04-06 04:49:24,976 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.bn3.weight                 loaded from layer2.0.bn3.weight          of shape (512,)
2020-04-06 04:49:24,976 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.conv1.weight               loaded from layer2.0.conv1.weight        of shape (128, 256, 1, 1)
2020-04-06 04:49:24,977 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.conv2.weight               loaded from layer2.0.conv2.weight        of shape (128, 128, 3, 3)
2020-04-06 04:49:24,977 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.conv3.weight               loaded from layer2.0.conv3.weight        of shape (512, 128, 1, 1)
2020-04-06 04:49:24,977 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.downsample.0.weight        loaded from layer2.0.downsample.0.weight of shape (512, 256, 1, 1)
2020-04-06 04:49:24,977 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.downsample.1.bias          loaded from layer2.0.downsample.1.bias   of shape (512,)
2020-04-06 04:49:24,977 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.downsample.1.weight        loaded from layer2.0.downsample.1.weight of shape (512,)
2020-04-06 04:49:24,977 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.1.bn1.bias                   loaded from layer2.1.bn1.bias            of shape (128,)
2020-04-06 04:49:24,977 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.1.bn1.weight                 loaded from layer2.1.bn1.weight          of shape (128,)
2020-04-06 04:49:24,977 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.1.bn2.bias                   loaded from layer2.1.bn2.bias            of shape (128,)
2020-04-06 04:49:24,977 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.1.bn2.weight                 loaded from layer2.1.bn2.weight          of shape (128,)
2020-04-06 04:49:24,978 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.1.bn3.bias                   loaded from layer2.1.bn3.bias            of shape (512,)
2020-04-06 04:49:24,978 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.1.bn3.weight                 loaded from layer2.1.bn3.weight          of shape (512,)
2020-04-06 04:49:24,978 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.1.conv1.weight               loaded from layer2.1.conv1.weight        of shape (128, 512, 1, 1)
2020-04-06 04:49:24,978 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.1.conv2.weight               loaded from layer2.1.conv2.weight        of shape (128, 128, 3, 3)
2020-04-06 04:49:24,978 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.1.conv3.weight               loaded from layer2.1.conv3.weight        of shape (512, 128, 1, 1)
2020-04-06 04:49:24,978 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.2.bn1.bias                   loaded from layer2.2.bn1.bias            of shape (128,)
2020-04-06 04:49:24,978 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.2.bn1.weight                 loaded from layer2.2.bn1.weight          of shape (128,)
2020-04-06 04:49:24,978 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.2.bn2.bias                   loaded from layer2.2.bn2.bias            of shape (128,)
2020-04-06 04:49:24,979 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.2.bn2.weight                 loaded from layer2.2.bn2.weight          of shape (128,)
2020-04-06 04:49:24,979 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.2.bn3.bias                   loaded from layer2.2.bn3.bias            of shape (512,)
2020-04-06 04:49:24,979 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.2.bn3.weight                 loaded from layer2.2.bn3.weight          of shape (512,)
2020-04-06 04:49:24,979 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.2.conv1.weight               loaded from layer2.2.conv1.weight        of shape (128, 512, 1, 1)
2020-04-06 04:49:24,979 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.2.conv2.weight               loaded from layer2.2.conv2.weight        of shape (128, 128, 3, 3)
2020-04-06 04:49:24,979 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.2.conv3.weight               loaded from layer2.2.conv3.weight        of shape (512, 128, 1, 1)
2020-04-06 04:49:24,979 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.3.bn1.bias                   loaded from layer2.3.bn1.bias            of shape (128,)
2020-04-06 04:49:24,979 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.3.bn1.weight                 loaded from layer2.3.bn1.weight          of shape (128,)
2020-04-06 04:49:24,980 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.3.bn2.bias                   loaded from layer2.3.bn2.bias            of shape (128,)
2020-04-06 04:49:24,980 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.3.bn2.weight                 loaded from layer2.3.bn2.weight          of shape (128,)
2020-04-06 04:49:24,980 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.3.bn3.bias                   loaded from layer2.3.bn3.bias            of shape (512,)
2020-04-06 04:49:24,980 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.3.bn3.weight                 loaded from layer2.3.bn3.weight          of shape (512,)
2020-04-06 04:49:24,980 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.3.conv1.weight               loaded from layer2.3.conv1.weight        of shape (128, 512, 1, 1)
2020-04-06 04:49:24,980 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.3.conv2.weight               loaded from layer2.3.conv2.weight        of shape (128, 128, 3, 3)
2020-04-06 04:49:24,980 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.3.conv3.weight               loaded from layer2.3.conv3.weight        of shape (512, 128, 1, 1)
2020-04-06 04:49:24,981 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.bn1.bias                   loaded from layer3.0.bn1.bias            of shape (256,)
2020-04-06 04:49:24,981 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.bn1.weight                 loaded from layer3.0.bn1.weight          of shape (256,)
2020-04-06 04:49:24,981 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.bn2.bias                   loaded from layer3.0.bn2.bias            of shape (256,)
2020-04-06 04:49:24,981 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.bn2.weight                 loaded from layer3.0.bn2.weight          of shape (256,)
2020-04-06 04:49:24,981 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.bn3.bias                   loaded from layer3.0.bn3.bias            of shape (1024,)
2020-04-06 04:49:24,981 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.bn3.weight                 loaded from layer3.0.bn3.weight          of shape (1024,)
2020-04-06 04:49:24,981 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.conv1.weight               loaded from layer3.0.conv1.weight        of shape (256, 512, 1, 1)
2020-04-06 04:49:24,982 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.conv2.weight               loaded from layer3.0.conv2.weight        of shape (256, 256, 3, 3)
2020-04-06 04:49:24,982 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.conv3.weight               loaded from layer3.0.conv3.weight        of shape (1024, 256, 1, 1)
2020-04-06 04:49:24,982 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.downsample.0.weight        loaded from layer3.0.downsample.0.weight of shape (1024, 512, 1, 1)
2020-04-06 04:49:24,983 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.downsample.1.bias          loaded from layer3.0.downsample.1.bias   of shape (1024,)
2020-04-06 04:49:24,983 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.downsample.1.weight        loaded from layer3.0.downsample.1.weight of shape (1024,)
2020-04-06 04:49:24,983 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.1.bn1.bias                   loaded from layer3.1.bn1.bias            of shape (256,)
2020-04-06 04:49:24,983 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.1.bn1.weight                 loaded from layer3.1.bn1.weight          of shape (256,)
2020-04-06 04:49:24,983 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.1.bn2.bias                   loaded from layer3.1.bn2.bias            of shape (256,)
2020-04-06 04:49:24,983 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.1.bn2.weight                 loaded from layer3.1.bn2.weight          of shape (256,)
2020-04-06 04:49:24,983 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.1.bn3.bias                   loaded from layer3.1.bn3.bias            of shape (1024,)
2020-04-06 04:49:24,983 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.1.bn3.weight                 loaded from layer3.1.bn3.weight          of shape (1024,)
2020-04-06 04:49:24,983 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.1.conv1.weight               loaded from layer3.1.conv1.weight        of shape (256, 1024, 1, 1)
2020-04-06 04:49:24,984 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.1.conv2.weight               loaded from layer3.1.conv2.weight        of shape (256, 256, 3, 3)
2020-04-06 04:49:24,985 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.1.conv3.weight               loaded from layer3.1.conv3.weight        of shape (1024, 256, 1, 1)
2020-04-06 04:49:24,985 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.2.bn1.bias                   loaded from layer3.2.bn1.bias            of shape (256,)
2020-04-06 04:49:24,985 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.2.bn1.weight                 loaded from layer3.2.bn1.weight          of shape (256,)
2020-04-06 04:49:24,985 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.2.bn2.bias                   loaded from layer3.2.bn2.bias            of shape (256,)
2020-04-06 04:49:24,985 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.2.bn2.weight                 loaded from layer3.2.bn2.weight          of shape (256,)
2020-04-06 04:49:24,985 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.2.bn3.bias                   loaded from layer3.2.bn3.bias            of shape (1024,)
2020-04-06 04:49:24,985 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.2.bn3.weight                 loaded from layer3.2.bn3.weight          of shape (1024,)
2020-04-06 04:49:24,985 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.2.conv1.weight               loaded from layer3.2.conv1.weight        of shape (256, 1024, 1, 1)
2020-04-06 04:49:24,986 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.2.conv2.weight               loaded from layer3.2.conv2.weight        of shape (256, 256, 3, 3)
2020-04-06 04:49:24,987 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.2.conv3.weight               loaded from layer3.2.conv3.weight        of shape (1024, 256, 1, 1)
2020-04-06 04:49:24,987 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.3.bn1.bias                   loaded from layer3.3.bn1.bias            of shape (256,)
2020-04-06 04:49:24,987 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.3.bn1.weight                 loaded from layer3.3.bn1.weight          of shape (256,)
2020-04-06 04:49:24,987 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.3.bn2.bias                   loaded from layer3.3.bn2.bias            of shape (256,)
2020-04-06 04:49:24,987 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.3.bn2.weight                 loaded from layer3.3.bn2.weight          of shape (256,)
2020-04-06 04:49:24,987 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.3.bn3.bias                   loaded from layer3.3.bn3.bias            of shape (1024,)
2020-04-06 04:49:24,987 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.3.bn3.weight                 loaded from layer3.3.bn3.weight          of shape (1024,)
2020-04-06 04:49:24,987 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.3.conv1.weight               loaded from layer3.3.conv1.weight        of shape (256, 1024, 1, 1)
2020-04-06 04:49:24,988 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.3.conv2.weight               loaded from layer3.3.conv2.weight        of shape (256, 256, 3, 3)
2020-04-06 04:49:24,988 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.3.conv3.weight               loaded from layer3.3.conv3.weight        of shape (1024, 256, 1, 1)
2020-04-06 04:49:24,989 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.4.bn1.bias                   loaded from layer3.4.bn1.bias            of shape (256,)
2020-04-06 04:49:24,989 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.4.bn1.weight                 loaded from layer3.4.bn1.weight          of shape (256,)
2020-04-06 04:49:24,989 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.4.bn2.bias                   loaded from layer3.4.bn2.bias            of shape (256,)
2020-04-06 04:49:24,989 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.4.bn2.weight                 loaded from layer3.4.bn2.weight          of shape (256,)
2020-04-06 04:49:24,989 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.4.bn3.bias                   loaded from layer3.4.bn3.bias            of shape (1024,)
2020-04-06 04:49:24,989 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.4.bn3.weight                 loaded from layer3.4.bn3.weight          of shape (1024,)
2020-04-06 04:49:24,989 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.4.conv1.weight               loaded from layer3.4.conv1.weight        of shape (256, 1024, 1, 1)
2020-04-06 04:49:24,990 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.4.conv2.weight               loaded from layer3.4.conv2.weight        of shape (256, 256, 3, 3)
2020-04-06 04:49:24,990 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.4.conv3.weight               loaded from layer3.4.conv3.weight        of shape (1024, 256, 1, 1)
2020-04-06 04:49:24,991 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.5.bn1.bias                   loaded from layer3.5.bn1.bias            of shape (256,)
2020-04-06 04:49:24,991 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.5.bn1.weight                 loaded from layer3.5.bn1.weight          of shape (256,)
2020-04-06 04:49:24,991 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.5.bn2.bias                   loaded from layer3.5.bn2.bias            of shape (256,)
2020-04-06 04:49:24,991 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.5.bn2.weight                 loaded from layer3.5.bn2.weight          of shape (256,)
2020-04-06 04:49:24,991 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.5.bn3.bias                   loaded from layer3.5.bn3.bias            of shape (1024,)
2020-04-06 04:49:24,991 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.5.bn3.weight                 loaded from layer3.5.bn3.weight          of shape (1024,)
2020-04-06 04:49:24,991 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.5.conv1.weight               loaded from layer3.5.conv1.weight        of shape (256, 1024, 1, 1)
2020-04-06 04:49:24,992 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.5.conv2.weight               loaded from layer3.5.conv2.weight        of shape (256, 256, 3, 3)
2020-04-06 04:49:24,992 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.5.conv3.weight               loaded from layer3.5.conv3.weight        of shape (1024, 256, 1, 1)
2020-04-06 04:49:24,993 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.bn1.bias                   loaded from layer4.0.bn1.bias            of shape (512,)
2020-04-06 04:49:24,993 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.bn1.weight                 loaded from layer4.0.bn1.weight          of shape (512,)
2020-04-06 04:49:24,993 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.bn2.bias                   loaded from layer4.0.bn2.bias            of shape (512,)
2020-04-06 04:49:24,993 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.bn2.weight                 loaded from layer4.0.bn2.weight          of shape (512,)
2020-04-06 04:49:24,993 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.bn3.bias                   loaded from layer4.0.bn3.bias            of shape (2048,)
2020-04-06 04:49:24,993 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.bn3.weight                 loaded from layer4.0.bn3.weight          of shape (2048,)
2020-04-06 04:49:24,993 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.conv1.weight               loaded from layer4.0.conv1.weight        of shape (512, 1024, 1, 1)
2020-04-06 04:49:24,998 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.conv2.weight               loaded from layer4.0.conv2.weight        of shape (512, 512, 3, 3)
2020-04-06 04:49:24,998 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.conv3.weight               loaded from layer4.0.conv3.weight        of shape (2048, 512, 1, 1)
2020-04-06 04:49:24,998 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.downsample.0.weight        loaded from layer4.0.downsample.0.weight of shape (2048, 1024, 1, 1)
2020-04-06 04:49:24,998 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.downsample.1.bias          loaded from layer4.0.downsample.1.bias   of shape (2048,)
2020-04-06 04:49:24,998 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.downsample.1.weight        loaded from layer4.0.downsample.1.weight of shape (2048,)
2020-04-06 04:49:24,998 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.1.bn1.bias                   loaded from layer4.1.bn1.bias            of shape (512,)
2020-04-06 04:49:24,999 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.1.bn1.weight                 loaded from layer4.1.bn1.weight          of shape (512,)
2020-04-06 04:49:24,999 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.1.bn2.bias                   loaded from layer4.1.bn2.bias            of shape (512,)
2020-04-06 04:49:24,999 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.1.bn2.weight                 loaded from layer4.1.bn2.weight          of shape (512,)
2020-04-06 04:49:24,999 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.1.bn3.bias                   loaded from layer4.1.bn3.bias            of shape (2048,)
2020-04-06 04:49:24,999 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.1.bn3.weight                 loaded from layer4.1.bn3.weight          of shape (2048,)
2020-04-06 04:49:24,999 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.1.conv1.weight               loaded from layer4.1.conv1.weight        of shape (512, 2048, 1, 1)
2020-04-06 04:49:25,005 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.1.conv2.weight               loaded from layer4.1.conv2.weight        of shape (512, 512, 3, 3)
2020-04-06 04:49:25,005 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.1.conv3.weight               loaded from layer4.1.conv3.weight        of shape (2048, 512, 1, 1)
2020-04-06 04:49:25,005 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.2.bn1.bias                   loaded from layer4.2.bn1.bias            of shape (512,)
2020-04-06 04:49:25,005 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.2.bn1.weight                 loaded from layer4.2.bn1.weight          of shape (512,)
2020-04-06 04:49:25,005 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.2.bn2.bias                   loaded from layer4.2.bn2.bias            of shape (512,)
2020-04-06 04:49:25,005 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.2.bn2.weight                 loaded from layer4.2.bn2.weight          of shape (512,)
2020-04-06 04:49:25,005 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.2.bn3.bias                   loaded from layer4.2.bn3.bias            of shape (2048,)
2020-04-06 04:49:25,005 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.2.bn3.weight                 loaded from layer4.2.bn3.weight          of shape (2048,)
2020-04-06 04:49:25,006 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.2.conv1.weight               loaded from layer4.2.conv1.weight        of shape (512, 2048, 1, 1)
2020-04-06 04:49:25,012 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.2.conv2.weight               loaded from layer4.2.conv2.weight        of shape (512, 512, 3, 3)
2020-04-06 04:49:25,012 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.2.conv3.weight               loaded from layer4.2.conv3.weight        of shape (2048, 512, 1, 1)
2020-04-06 04:49:25,012 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.stem.bn1.bias                       loaded from bn1.bias                     of shape (64,)
2020-04-06 04:49:25,012 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.stem.bn1.weight                     loaded from bn1.weight                   of shape (64,)
2020-04-06 04:49:25,013 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.stem.conv1.weight                   loaded from conv1.weight                 of shape (64, 3, 7, 7)
:::MLL 1586148565.057 init_stop: {"value": null, "metadata": {"file": "tools/train_mlperf.py", "lineno": 197}}
:::MLL 1586148565.057 run_start: {"value": null, "metadata": {"file": "tools/train_mlperf.py", "lineno": 198}}
2020-04-06 04:49:25,057 maskrcnn_benchmark.data.build WARNING: When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=14.28s)
creating index...
Done (t=14.24s)
creating index...
Done (t=14.84s)
creating index...
index created!
index created!
index created!
2020-04-06 04:49:42,762 maskrcnn_benchmark.trainer INFO: Start training
:::MLL 1586148585.068 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "tools/train_mlperf.py", "lineno": 129}}
:::MLL 1586148585.068 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "tools/train_mlperf.py", "lineno": 130}}
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.5
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.5
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.5
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.25
2020-04-06 04:50:02,524 maskrcnn_benchmark.trainer INFO: eta: 1 day, 19:54:15  iter: 20  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (1.1567)  loss_objectness: 0.6931 (0.7154)  loss_rpn_box_reg: 0.1247 (0.1450)  time: 0.6463 (0.9880)  data: 0.0018 (0.1174)  lr: -0.028176  max mem: 23836
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.03125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.03125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.03125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.015625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.015625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.015625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0078125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0078125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0078125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.00390625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.00390625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.00390625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.001953125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.001953125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.001953125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0009765625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0009765625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0009765625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.00048828125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.00048828125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.00048828125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.000244140625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.000244140625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.000244140625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0001220703125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0001220703125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0001220703125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.103515625e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.103515625e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.103515625e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0517578125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0517578125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0517578125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.52587890625e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.52587890625e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.52587890625e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.62939453125e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.62939453125e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.62939453125e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.814697265625e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.814697265625e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.814697265625e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9073486328125e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9073486328125e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9073486328125e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.5367431640625e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.5367431640625e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.5367431640625e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.76837158203125e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.76837158203125e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.76837158203125e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.384185791015625e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.384185791015625e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.384185791015625e-07
2020-04-06 04:50:14,936 maskrcnn_benchmark.trainer INFO: eta: 1 day, 11:44:13  iter: 40  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.9249)  loss_objectness: 0.6931 (0.7042)  loss_rpn_box_reg: 0.1234 (0.1494)  time: 0.6075 (0.8043)  data: 0.0017 (0.0596)  lr: -0.026256  max mem: 23836
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1920928955078125e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1920928955078125e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1920928955078125e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.960464477539063e-08
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.960464477539063e-08
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.960464477539063e-08
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9802322387695312e-08
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9802322387695312e-08
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9802322387695312e-08
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4901161193847656e-08
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4901161193847656e-08
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4901161193847656e-08
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.450580596923828e-09
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.450580596923828e-09
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.450580596923828e-09
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.725290298461914e-09
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.725290298461914e-09
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.725290298461914e-09
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.862645149230957e-09
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.862645149230957e-09
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.862645149230957e-09
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.313225746154785e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.313225746154785e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.313225746154785e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.656612873077393e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.656612873077393e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.656612873077393e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3283064365386963e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3283064365386963e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3283064365386963e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1641532182693481e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1641532182693481e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1641532182693481e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.820766091346741e-11
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.820766091346741e-11
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.820766091346741e-11
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9103830456733704e-11
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9103830456733704e-11
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9103830456733704e-11
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4551915228366852e-11
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4551915228366852e-11
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4551915228366852e-11
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.275957614183426e-12
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.275957614183426e-12
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.275957614183426e-12
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.637978807091713e-12
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.637978807091713e-12
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.637978807091713e-12
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8189894035458565e-12
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8189894035458565e-12
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8189894035458565e-12
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.094947017729282e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.094947017729282e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.094947017729282e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.547473508864641e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.547473508864641e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.547473508864641e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2737367544323206e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2737367544323206e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2737367544323206e-13
2020-04-06 04:50:27,484 maskrcnn_benchmark.trainer INFO: eta: 1 day, 9:06:45  iter: 60  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.8477)  loss_objectness: 0.6931 (0.7005)  loss_rpn_box_reg: 0.0890 (0.1387)  time: 0.6106 (0.7453)  data: 0.0021 (0.0405)  lr: -0.024336  max mem: 23836
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1368683772161603e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1368683772161603e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1368683772161603e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.684341886080802e-14
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.684341886080802e-14
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.684341886080802e-14
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.842170943040401e-14
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.842170943040401e-14
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.842170943040401e-14
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4210854715202004e-14
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4210854715202004e-14
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4210854715202004e-14
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.105427357601002e-15
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.105427357601002e-15
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.105427357601002e-15
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.552713678800501e-15
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.552713678800501e-15
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.552713678800501e-15
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7763568394002505e-15
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7763568394002505e-15
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7763568394002505e-15
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.881784197001252e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.881784197001252e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.881784197001252e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.440892098500626e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.440892098500626e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.440892098500626e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.220446049250313e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.220446049250313e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.220446049250313e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1102230246251565e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1102230246251565e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1102230246251565e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.551115123125783e-17
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.551115123125783e-17
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.551115123125783e-17
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7755575615628914e-17
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7755575615628914e-17
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7755575615628914e-17
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3877787807814457e-17
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3877787807814457e-17
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3877787807814457e-17
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.938893903907228e-18
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.938893903907228e-18
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.938893903907228e-18
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.469446951953614e-18
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.469446951953614e-18
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.469446951953614e-18
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.734723475976807e-18
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.734723475976807e-18
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.734723475976807e-18
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.673617379884035e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.673617379884035e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.673617379884035e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.336808689942018e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.336808689942018e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.336808689942018e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.168404344971009e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.168404344971009e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.168404344971009e-19
2020-04-06 04:50:39,670 maskrcnn_benchmark.trainer INFO: eta: 1 day, 7:35:54  iter: 80  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.8090)  loss_objectness: 0.6931 (0.6987)  loss_rpn_box_reg: 0.1139 (0.1333)  time: 0.6028 (0.7113)  data: 0.0017 (0.0308)  lr: -0.022416  max mem: 23836
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0842021724855044e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0842021724855044e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0842021724855044e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.421010862427522e-20
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.421010862427522e-20
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.421010862427522e-20
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.710505431213761e-20
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.710505431213761e-20
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.710505431213761e-20
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3552527156068805e-20
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3552527156068805e-20
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3552527156068805e-20
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.776263578034403e-21
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.776263578034403e-21
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.776263578034403e-21
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3881317890172014e-21
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3881317890172014e-21
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3881317890172014e-21
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6940658945086007e-21
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6940658945086007e-21
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6940658945086007e-21
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.470329472543003e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.470329472543003e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.470329472543003e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.235164736271502e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.235164736271502e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.235164736271502e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.117582368135751e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.117582368135751e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.117582368135751e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0587911840678754e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0587911840678754e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0587911840678754e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.293955920339377e-23
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.293955920339377e-23
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.293955920339377e-23
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6469779601696886e-23
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6469779601696886e-23
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6469779601696886e-23
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3234889800848443e-23
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3234889800848443e-23
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3234889800848443e-23
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.617444900424222e-24
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.617444900424222e-24
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.617444900424222e-24
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.308722450212111e-24
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.308722450212111e-24
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.308722450212111e-24
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6543612251060553e-24
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6543612251060553e-24
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6543612251060553e-24
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.271806125530277e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.271806125530277e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.271806125530277e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1359030627651384e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1359030627651384e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1359030627651384e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0679515313825692e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0679515313825692e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0679515313825692e-25
2020-04-06 04:50:51,939 maskrcnn_benchmark.trainer INFO: eta: 1 day, 6:43:30  iter: 100  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7859)  loss_objectness: 0.6931 (0.6975)  loss_rpn_box_reg: 0.1041 (0.1311)  time: 0.6065 (0.6917)  data: 0.0017 (0.0251)  lr: -0.020496  max mem: 23836
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0339757656912846e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0339757656912846e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0339757656912846e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.169878828456423e-26
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.169878828456423e-26
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.169878828456423e-26
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5849394142282115e-26
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5849394142282115e-26
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5849394142282115e-26
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2924697071141057e-26
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2924697071141057e-26
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2924697071141057e-26
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.462348535570529e-27
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.462348535570529e-27
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.462348535570529e-27
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2311742677852644e-27
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2311742677852644e-27
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2311742677852644e-27
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6155871338926322e-27
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6155871338926322e-27
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6155871338926322e-27
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.077935669463161e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.077935669463161e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.077935669463161e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0389678347315804e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0389678347315804e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0389678347315804e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0194839173657902e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0194839173657902e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0194839173657902e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0097419586828951e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0097419586828951e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0097419586828951e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.048709793414476e-29
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.048709793414476e-29
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.048709793414476e-29
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.524354896707238e-29
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.524354896707238e-29
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.524354896707238e-29
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.262177448353619e-29
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.262177448353619e-29
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.262177448353619e-29
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.310887241768095e-30
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.310887241768095e-30
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.310887241768095e-30
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1554436208840472e-30
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1554436208840472e-30
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1554436208840472e-30
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5777218104420236e-30
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5777218104420236e-30
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5777218104420236e-30
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.888609052210118e-31
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.888609052210118e-31
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.888609052210118e-31
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.944304526105059e-31
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.944304526105059e-31
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.944304526105059e-31
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9721522630525295e-31
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9721522630525295e-31
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9721522630525295e-31
2020-04-06 04:51:03,938 maskrcnn_benchmark.trainer INFO: eta: 1 day, 6:02:29  iter: 120  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7704)  loss_objectness: 0.6931 (0.6968)  loss_rpn_box_reg: 0.1094 (0.1315)  time: 0.6030 (0.6764)  data: 0.0017 (0.0212)  lr: -0.018576  max mem: 23836
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.860761315262648e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.860761315262648e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.860761315262648e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.930380657631324e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.930380657631324e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.930380657631324e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.465190328815662e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.465190328815662e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.465190328815662e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.232595164407831e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.232595164407831e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.232595164407831e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.162975822039155e-33
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.162975822039155e-33
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.162975822039155e-33
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0814879110195774e-33
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0814879110195774e-33
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0814879110195774e-33
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5407439555097887e-33
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5407439555097887e-33
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5407439555097887e-33
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.703719777548943e-34
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.703719777548943e-34
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.703719777548943e-34
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.851859888774472e-34
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.851859888774472e-34
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.851859888774472e-34
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.925929944387236e-34
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.925929944387236e-34
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.925929944387236e-34
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.62964972193618e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.62964972193618e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.62964972193618e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.81482486096809e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.81482486096809e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.81482486096809e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.407412430484045e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.407412430484045e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.407412430484045e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2037062152420224e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2037062152420224e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2037062152420224e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.018531076210112e-36
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.018531076210112e-36
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.018531076210112e-36
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.009265538105056e-36
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.009265538105056e-36
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.009265538105056e-36
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.504632769052528e-36
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.504632769052528e-36
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.504632769052528e-36
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.52316384526264e-37
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.52316384526264e-37
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.52316384526264e-37
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.76158192263132e-37
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.76158192263132e-37
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.76158192263132e-37
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.88079096131566e-37
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.88079096131566e-37
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.88079096131566e-37
2020-04-06 04:51:15,871 maskrcnn_benchmark.trainer INFO: eta: 1 day, 5:31:53  iter: 140  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7594)  loss_objectness: 0.6931 (0.6963)  loss_rpn_box_reg: 0.0708 (0.1284)  time: 0.5978 (0.6650)  data: 0.0016 (0.0184)  lr: -0.016656  max mem: 23836
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.4039548065783e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.4039548065783e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.4039548065783e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.70197740328915e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.70197740328915e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.70197740328915e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.350988701644575e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.350988701644575e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.350988701644575e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1754943508222875e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1754943508222875e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1754943508222875e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.877471754111438e-39
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.877471754111438e-39
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.877471754111438e-39
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.938735877055719e-39
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.938735877055719e-39
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.938735877055719e-39
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4693679385278594e-39
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4693679385278594e-39
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4693679385278594e-39
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.346839692639297e-40
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.346839692639297e-40
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.346839692639297e-40
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6734198463196485e-40
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6734198463196485e-40
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6734198463196485e-40
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8367099231598242e-40
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8367099231598242e-40
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8367099231598242e-40
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.183549615799121e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.183549615799121e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.183549615799121e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.591774807899561e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.591774807899561e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.591774807899561e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2958874039497803e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2958874039497803e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2958874039497803e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1479437019748901e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1479437019748901e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1479437019748901e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.739718509874451e-42
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.739718509874451e-42
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.739718509874451e-42
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8698592549372254e-42
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8698592549372254e-42
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8698592549372254e-42
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4349296274686127e-42
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4349296274686127e-42
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4349296274686127e-42
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.174648137343064e-43
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.174648137343064e-43
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.174648137343064e-43
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.587324068671532e-43
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.587324068671532e-43
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.587324068671532e-43
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.793662034335766e-43
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.793662034335766e-43
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.793662034335766e-43
2020-04-06 04:51:27,895 maskrcnn_benchmark.trainer INFO: eta: 1 day, 5:10:25  iter: 160  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7511)  loss_objectness: 0.6931 (0.6959)  loss_rpn_box_reg: 0.0995 (0.1274)  time: 0.6009 (0.6571)  data: 0.0017 (0.0163)  lr: -0.014736  max mem: 23836
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.96831017167883e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.96831017167883e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.96831017167883e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.484155085839415e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.484155085839415e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.484155085839415e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2420775429197073e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2420775429197073e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2420775429197073e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1210387714598537e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1210387714598537e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1210387714598537e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.605193857299268e-45
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.605193857299268e-45
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.605193857299268e-45
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.802596928649634e-45
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.802596928649634e-45
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.802596928649634e-45
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.401298464324817e-45
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.401298464324817e-45
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.401298464324817e-45
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.006492321624085e-46
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.006492321624085e-46
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.006492321624085e-46
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.503246160812043e-46
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.503246160812043e-46
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.503246160812043e-46
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7516230804060213e-46
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7516230804060213e-46
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7516230804060213e-46
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.758115402030107e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.758115402030107e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.758115402030107e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.3790577010150533e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.3790577010150533e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.3790577010150533e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1895288505075267e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1895288505075267e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1895288505075267e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0947644252537633e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0947644252537633e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0947644252537633e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.473822126268817e-48
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.473822126268817e-48
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.473822126268817e-48
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7369110631344083e-48
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7369110631344083e-48
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7369110631344083e-48
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3684555315672042e-48
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3684555315672042e-48
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3684555315672042e-48
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.842277657836021e-49
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.842277657836021e-49
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.842277657836021e-49
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4211388289180104e-49
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4211388289180104e-49
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4211388289180104e-49
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7105694144590052e-49
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7105694144590052e-49
2020-04-06 04:51:40,103 maskrcnn_benchmark.trainer INFO: eta: 1 day, 4:56:23  iter: 180  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7447)  loss_objectness: 0.6931 (0.6956)  loss_rpn_box_reg: 0.1129 (0.1289)  time: 0.6089 (0.6519)  data: 0.0017 (0.0147)  lr: -0.012816  max mem: 23836
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7105694144590052e-49
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.552847072295026e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.552847072295026e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.552847072295026e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.276423536147513e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.276423536147513e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.276423536147513e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1382117680737565e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1382117680737565e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1382117680737565e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0691058840368783e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0691058840368783e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0691058840368783e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.345529420184391e-51
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.345529420184391e-51
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.345529420184391e-51
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6727647100921956e-51
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6727647100921956e-51
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6727647100921956e-51
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3363823550460978e-51
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3363823550460978e-51
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3363823550460978e-51
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.681911775230489e-52
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.681911775230489e-52
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.681911775230489e-52
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3409558876152446e-52
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3409558876152446e-52
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3409558876152446e-52
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6704779438076223e-52
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6704779438076223e-52
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6704779438076223e-52
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.352389719038111e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.352389719038111e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.352389719038111e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.176194859519056e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.176194859519056e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.176194859519056e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.088097429759528e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.088097429759528e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.088097429759528e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.044048714879764e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.044048714879764e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.044048714879764e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.22024357439882e-54
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.22024357439882e-54
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.22024357439882e-54
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.61012178719941e-54
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.61012178719941e-54
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.61012178719941e-54
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.305060893599705e-54
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.305060893599705e-54
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.305060893599705e-54
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.525304467998525e-55
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.525304467998525e-55
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.525304467998525e-55
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2626522339992623e-55
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2626522339992623e-55
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2626522339992623e-55
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6313261169996311e-55
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6313261169996311e-55
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6313261169996311e-55
2020-04-06 04:51:52,223 maskrcnn_benchmark.trainer INFO: eta: 1 day, 4:43:56  iter: 200  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7395)  loss_objectness: 0.6931 (0.6953)  loss_rpn_box_reg: 0.1279 (0.1299)  time: 0.6022 (0.6473)  data: 0.0018 (0.0134)  lr: -0.010896  max mem: 23838
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.156630584998156e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.156630584998156e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.156630584998156e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.078315292499078e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.078315292499078e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.078315292499078e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.039157646249539e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.039157646249539e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.039157646249539e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0195788231247695e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0195788231247695e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0195788231247695e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.0978941156238473e-57
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.0978941156238473e-57
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.0978941156238473e-57
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5489470578119236e-57
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5489470578119236e-57
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5489470578119236e-57
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2744735289059618e-57
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2744735289059618e-57
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2744735289059618e-57
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.372367644529809e-58
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.372367644529809e-58
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.372367644529809e-58
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1861838222649046e-58
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1861838222649046e-58
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1861838222649046e-58
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5930919111324523e-58
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5930919111324523e-58
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5930919111324523e-58
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.965459555662261e-59
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.965459555662261e-59
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.965459555662261e-59
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.982729777831131e-59
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.982729777831131e-59
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.982729777831131e-59
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9913648889155653e-59
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9913648889155653e-59
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9913648889155653e-59
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.956824444577827e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.956824444577827e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.956824444577827e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.9784122222889134e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.9784122222889134e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.9784122222889134e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4892061111444567e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4892061111444567e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4892061111444567e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2446030555722283e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2446030555722283e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2446030555722283e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.223015277861142e-61
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.223015277861142e-61
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.223015277861142e-61
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.111507638930571e-61
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.111507638930571e-61
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.111507638930571e-61
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5557538194652854e-61
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5557538194652854e-61
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5557538194652854e-61
2020-04-06 04:52:04,200 maskrcnn_benchmark.trainer INFO: eta: 1 day, 4:32:00  iter: 220  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7353)  loss_objectness: 0.6931 (0.6951)  loss_rpn_box_reg: 0.1248 (0.1292)  time: 0.5995 (0.6429)  data: 0.0016 (0.0124)  lr: -0.008976  max mem: 23838
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.778769097326427e-62
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.778769097326427e-62
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.778769097326427e-62
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8893845486632136e-62
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8893845486632136e-62
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8893845486632136e-62
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9446922743316068e-62
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9446922743316068e-62
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9446922743316068e-62
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.723461371658034e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.723461371658034e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.723461371658034e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.861730685829017e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.861730685829017e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.861730685829017e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4308653429145085e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4308653429145085e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4308653429145085e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2154326714572542e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2154326714572542e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2154326714572542e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.077163357286271e-64
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.077163357286271e-64
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.077163357286271e-64
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0385816786431356e-64
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0385816786431356e-64
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0385816786431356e-64
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5192908393215678e-64
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5192908393215678e-64
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5192908393215678e-64
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.596454196607839e-65
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.596454196607839e-65
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.596454196607839e-65
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7982270983039195e-65
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7982270983039195e-65
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7982270983039195e-65
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8991135491519597e-65
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8991135491519597e-65
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8991135491519597e-65
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.495567745759799e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.495567745759799e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.495567745759799e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.7477838728798994e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.7477838728798994e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.7477838728798994e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3738919364399497e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3738919364399497e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3738919364399497e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1869459682199748e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1869459682199748e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1869459682199748e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.934729841099874e-67
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.934729841099874e-67
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.934729841099874e-67
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.967364920549937e-67
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.967364920549937e-67
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.967364920549937e-67
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4836824602749686e-67
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4836824602749686e-67
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4836824602749686e-67
2020-04-06 04:52:16,329 maskrcnn_benchmark.trainer INFO: eta: 1 day, 4:23:42  iter: 240  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7318)  loss_objectness: 0.6931 (0.6949)  loss_rpn_box_reg: 0.1300 (0.1292)  time: 0.6018 (0.6398)  data: 0.0016 (0.0115)  lr: -0.007056  max mem: 23838
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.418412301374843e-68
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.418412301374843e-68
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.418412301374843e-68
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7092061506874214e-68
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7092061506874214e-68
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7092061506874214e-68
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8546030753437107e-68
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8546030753437107e-68
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8546030753437107e-68
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.273015376718553e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.273015376718553e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.273015376718553e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.636507688359277e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.636507688359277e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.636507688359277e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3182538441796384e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3182538441796384e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3182538441796384e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1591269220898192e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1591269220898192e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1591269220898192e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.795634610449096e-70
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.795634610449096e-70
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.795634610449096e-70
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.897817305224548e-70
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.897817305224548e-70
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.897817305224548e-70
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.448908652612274e-70
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.448908652612274e-70
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.448908652612274e-70
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.24454326306137e-71
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.24454326306137e-71
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.24454326306137e-71
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.622271631530685e-71
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.622271631530685e-71
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.622271631530685e-71
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8111358157653425e-71
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8111358157653425e-71
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8111358157653425e-71
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.055679078826712e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.055679078826712e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.055679078826712e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.527839539413356e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.527839539413356e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.527839539413356e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.263919769706678e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.263919769706678e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.263919769706678e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.131959884853339e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.131959884853339e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.131959884853339e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.659799424266695e-73
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.659799424266695e-73
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.659799424266695e-73
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8298997121333476e-73
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8298997121333476e-73
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8298997121333476e-73
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4149498560666738e-73
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4149498560666738e-73
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4149498560666738e-73
2020-04-06 04:52:28,654 maskrcnn_benchmark.trainer INFO: eta: 1 day, 4:18:40  iter: 260  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7288)  loss_objectness: 0.6931 (0.6948)  loss_rpn_box_reg: 0.1104 (0.1281)  time: 0.6086 (0.6380)  data: 0.0017 (0.0107)  lr: -0.005136  max mem: 23838
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.074749280333369e-74
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.074749280333369e-74
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.074749280333369e-74
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5373746401666845e-74
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5373746401666845e-74
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5373746401666845e-74
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7686873200833423e-74
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7686873200833423e-74
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7686873200833423e-74
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.843436600416711e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.843436600416711e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.843436600416711e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.421718300208356e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.421718300208356e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.421718300208356e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.210859150104178e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.210859150104178e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.210859150104178e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.105429575052089e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.105429575052089e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.105429575052089e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.527147875260445e-76
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.527147875260445e-76
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.527147875260445e-76
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7635739376302223e-76
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7635739376302223e-76
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7635739376302223e-76
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3817869688151111e-76
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3817869688151111e-76
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3817869688151111e-76
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.908934844075556e-77
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.908934844075556e-77
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.908934844075556e-77
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.454467422037778e-77
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.454467422037778e-77
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.454467422037778e-77
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.727233711018889e-77
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.727233711018889e-77
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.727233711018889e-77
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.636168555094445e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.636168555094445e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.636168555094445e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.3180842775472223e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.3180842775472223e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.3180842775472223e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1590421387736112e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1590421387736112e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1590421387736112e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0795210693868056e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0795210693868056e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0795210693868056e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.397605346934028e-79
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.397605346934028e-79
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.397605346934028e-79
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.698802673467014e-79
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.698802673467014e-79
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.698802673467014e-79
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.349401336733507e-79
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.349401336733507e-79
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.349401336733507e-79
2020-04-06 04:52:40,766 maskrcnn_benchmark.trainer INFO: eta: 1 day, 4:12:17  iter: 280  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7263)  loss_objectness: 0.6931 (0.6947)  loss_rpn_box_reg: 0.0961 (0.1278)  time: 0.6029 (0.6357)  data: 0.0018 (0.0101)  lr: -0.003216  max mem: 23841
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.747006683667535e-80
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.747006683667535e-80
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.747006683667535e-80
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3735033418337674e-80
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3735033418337674e-80
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3735033418337674e-80
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6867516709168837e-80
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6867516709168837e-80
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6867516709168837e-80
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.433758354584419e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.433758354584419e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.433758354584419e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.2168791772922093e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.2168791772922093e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.2168791772922093e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1084395886461046e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1084395886461046e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1084395886461046e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0542197943230523e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0542197943230523e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0542197943230523e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.271098971615262e-82
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.271098971615262e-82
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.271098971615262e-82
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.635549485807631e-82
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.635549485807631e-82
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.635549485807631e-82
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3177747429038154e-82
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3177747429038154e-82
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3177747429038154e-82
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.588873714519077e-83
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.588873714519077e-83
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.588873714519077e-83
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2944368572595385e-83
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2944368572595385e-83
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2944368572595385e-83
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6472184286297693e-83
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6472184286297693e-83
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6472184286297693e-83
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.236092143148846e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.236092143148846e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.236092143148846e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.118046071574423e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.118046071574423e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.118046071574423e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0590230357872116e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0590230357872116e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0590230357872116e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0295115178936058e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0295115178936058e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0295115178936058e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.147557589468029e-85
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.147557589468029e-85
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.147557589468029e-85
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5737787947340145e-85
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5737787947340145e-85
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5737787947340145e-85
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2868893973670072e-85
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2868893973670072e-85
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2868893973670072e-85
2020-04-06 04:52:52,869 maskrcnn_benchmark.trainer INFO: eta: 1 day, 4:06:38  iter: 300  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7240)  loss_objectness: 0.6931 (0.6946)  loss_rpn_box_reg: 0.1162 (0.1274)  time: 0.6004 (0.6337)  data: 0.0018 (0.0096)  lr: -0.001296  max mem: 23841
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.434446986835036e-86
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.434446986835036e-86
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.434446986835036e-86
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.217223493417518e-86
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.217223493417518e-86
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.217223493417518e-86
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.608611746708759e-86
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.608611746708759e-86
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.608611746708759e-86
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.043058733543795e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.043058733543795e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.043058733543795e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.021529366771898e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.021529366771898e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.021529366771898e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.010764683385949e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.010764683385949e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.010764683385949e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0053823416929744e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0053823416929744e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0053823416929744e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.026911708464872e-88
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.026911708464872e-88
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.026911708464872e-88
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.513455854232436e-88
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.513455854232436e-88
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.513455854232436e-88
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.256727927116218e-88
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.256727927116218e-88
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.256727927116218e-88
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.28363963558109e-89
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.28363963558109e-89
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.28363963558109e-89
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.141819817790545e-89
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.141819817790545e-89
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.141819817790545e-89
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5709099088952725e-89
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5709099088952725e-89
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5709099088952725e-89
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.854549544476363e-90
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.854549544476363e-90
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.854549544476363e-90
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.9272747722381812e-90
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.9272747722381812e-90
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.9272747722381812e-90
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9636373861190906e-90
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9636373861190906e-90
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9636373861190906e-90
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.818186930595453e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.818186930595453e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.818186930595453e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.909093465297727e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.909093465297727e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.909093465297727e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4545467326488633e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4545467326488633e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4545467326488633e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2272733663244316e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2272733663244316e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2272733663244316e-91
2020-04-06 04:53:05,059 maskrcnn_benchmark.trainer INFO: eta: 1 day, 4:02:24  iter: 320  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7221)  loss_objectness: 0.6931 (0.6945)  loss_rpn_box_reg: 0.1031 (0.1264)  time: 0.6056 (0.6322)  data: 0.0017 (0.0091)  lr: 0.000624  max mem: 23841
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.136366831622158e-92
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.136366831622158e-92
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.136366831622158e-92
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.068183415811079e-92
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.068183415811079e-92
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.068183415811079e-92
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5340917079055395e-92
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5340917079055395e-92
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5340917079055395e-92
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.670458539527698e-93
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.670458539527698e-93
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.670458539527698e-93
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.835229269763849e-93
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.835229269763849e-93
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.835229269763849e-93
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9176146348819244e-93
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9176146348819244e-93
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9176146348819244e-93
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.588073174409622e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.588073174409622e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.588073174409622e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.794036587204811e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.794036587204811e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.794036587204811e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3970182936024055e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3970182936024055e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3970182936024055e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1985091468012028e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1985091468012028e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1985091468012028e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.992545734006014e-95
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.992545734006014e-95
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.992545734006014e-95
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.996272867003007e-95
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.996272867003007e-95
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.996272867003007e-95
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4981364335015035e-95
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4981364335015035e-95
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4981364335015035e-95
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.490682167507517e-96
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.490682167507517e-96
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.490682167507517e-96
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.745341083753759e-96
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.745341083753759e-96
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.745341083753759e-96
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8726705418768793e-96
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8726705418768793e-96
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8726705418768793e-96
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.363352709384397e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.363352709384397e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.363352709384397e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.6816763546921983e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.6816763546921983e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.6816763546921983e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3408381773460992e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3408381773460992e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3408381773460992e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1704190886730496e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1704190886730496e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1704190886730496e-97
2020-04-06 04:53:17,188 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:58:10  iter: 340  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7204)  loss_objectness: 0.6931 (0.6944)  loss_rpn_box_reg: 0.1281 (0.1268)  time: 0.6048 (0.6307)  data: 0.0018 (0.0087)  lr: 0.002544  max mem: 23841
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.852095443365248e-98
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.852095443365248e-98
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.852095443365248e-98
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.926047721682624e-98
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.926047721682624e-98
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.926047721682624e-98
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.463023860841312e-98
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.463023860841312e-98
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.463023860841312e-98
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.31511930420656e-99
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.31511930420656e-99
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.31511930420656e-99
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.65755965210328e-99
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.65755965210328e-99
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.65755965210328e-99
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.82877982605164e-99
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.82877982605164e-99
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.82877982605164e-99
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.1438991302582e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.1438991302582e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.1438991302582e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5719495651291e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5719495651291e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5719495651291e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.28597478256455e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.28597478256455e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.28597478256455e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.142987391282275e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.142987391282275e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.142987391282275e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.714936956411375e-101
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.714936956411375e-101
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.714936956411375e-101
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8574684782056875e-101
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8574684782056875e-101
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8574684782056875e-101
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4287342391028437e-101
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4287342391028437e-101
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4287342391028437e-101
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.143671195514219e-102
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.143671195514219e-102
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.143671195514219e-102
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5718355977571093e-102
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5718355977571093e-102
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5718355977571093e-102
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7859177988785547e-102
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7859177988785547e-102
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7859177988785547e-102
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.929588994392773e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.929588994392773e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.929588994392773e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.464794497196387e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.464794497196387e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.464794497196387e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2323972485981933e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2323972485981933e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2323972485981933e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1161986242990967e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1161986242990967e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1161986242990967e-103
2020-04-06 04:53:29,380 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:54:51  iter: 360  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7189)  loss_objectness: 0.6931 (0.6943)  loss_rpn_box_reg: 0.1212 (0.1266)  time: 0.6055 (0.6295)  data: 0.0019 (0.0083)  lr: 0.004464  max mem: 23841
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.5809931214954833e-104
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.5809931214954833e-104
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.5809931214954833e-104
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7904965607477417e-104
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7904965607477417e-104
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7904965607477417e-104
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3952482803738708e-104
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3952482803738708e-104
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3952482803738708e-104
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.976241401869354e-105
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.976241401869354e-105
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.976241401869354e-105
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.488120700934677e-105
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.488120700934677e-105
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.488120700934677e-105
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7440603504673385e-105
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7440603504673385e-105
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7440603504673385e-105
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.720301752336693e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.720301752336693e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.720301752336693e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.3601508761683463e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.3601508761683463e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.3601508761683463e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1800754380841732e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1800754380841732e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1800754380841732e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0900377190420866e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0900377190420866e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0900377190420866e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.450188595210433e-107
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.450188595210433e-107
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.450188595210433e-107
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7250942976052165e-107
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7250942976052165e-107
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7250942976052165e-107
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3625471488026082e-107
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3625471488026082e-107
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3625471488026082e-107
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.812735744013041e-108
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.812735744013041e-108
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.812735744013041e-108
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4063678720065206e-108
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4063678720065206e-108
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4063678720065206e-108
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7031839360032603e-108
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7031839360032603e-108
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7031839360032603e-108
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.515919680016301e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.515919680016301e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.515919680016301e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.257959840008151e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.257959840008151e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.257959840008151e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1289799200040754e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1289799200040754e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1289799200040754e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0644899600020377e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0644899600020377e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0644899600020377e-109
2020-04-06 04:53:41,474 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:51:10  iter: 380  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7175)  loss_objectness: 0.6931 (0.6943)  loss_rpn_box_reg: 0.1262 (0.1270)  time: 0.6026 (0.6282)  data: 0.0018 (0.0080)  lr: 0.006384  max mem: 23841
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.3224498000101884e-110
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.3224498000101884e-110
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.3224498000101884e-110
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6612249000050942e-110
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6612249000050942e-110
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6612249000050942e-110
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3306124500025471e-110
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3306124500025471e-110
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3306124500025471e-110
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.653062250012736e-111
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.653062250012736e-111
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.653062250012736e-111
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.326531125006368e-111
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.326531125006368e-111
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.326531125006368e-111
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.663265562503184e-111
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.663265562503184e-111
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.663265562503184e-111
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.31632781251592e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.31632781251592e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.31632781251592e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.15816390625796e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.15816390625796e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.15816390625796e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.07908195312898e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.07908195312898e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.07908195312898e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.03954097656449e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.03954097656449e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.03954097656449e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.19770488282245e-113
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.19770488282245e-113
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.19770488282245e-113
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.598852441411225e-113
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.598852441411225e-113
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.598852441411225e-113
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2994262207056124e-113
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2994262207056124e-113
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2994262207056124e-113
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.497131103528062e-114
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.497131103528062e-114
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.497131103528062e-114
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.248565551764031e-114
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.248565551764031e-114
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.248565551764031e-114
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6242827758820155e-114
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6242827758820155e-114
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6242827758820155e-114
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.121413879410078e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.121413879410078e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.121413879410078e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.060706939705039e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.060706939705039e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.060706939705039e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0303534698525194e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0303534698525194e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0303534698525194e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0151767349262597e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0151767349262597e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0151767349262597e-115
2020-04-06 04:53:53,435 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:46:57  iter: 400  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7163)  loss_objectness: 0.6931 (0.6942)  loss_rpn_box_reg: 0.1000 (0.1262)  time: 0.6010 (0.6267)  data: 0.0017 (0.0076)  lr: 0.008304  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.075883674631299e-116
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.075883674631299e-116
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.075883674631299e-116
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5379418373156492e-116
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5379418373156492e-116
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5379418373156492e-116
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2689709186578246e-116
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2689709186578246e-116
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2689709186578246e-116
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.344854593289123e-117
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.344854593289123e-117
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.344854593289123e-117
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1724272966445615e-117
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1724272966445615e-117
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1724272966445615e-117
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5862136483222808e-117
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5862136483222808e-117
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5862136483222808e-117
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.931068241611404e-118
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.931068241611404e-118
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.931068241611404e-118
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.965534120805702e-118
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.965534120805702e-118
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.965534120805702e-118
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.982767060402851e-118
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.982767060402851e-118
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.982767060402851e-118
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.913835302014255e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.913835302014255e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.913835302014255e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.9569176510071274e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.9569176510071274e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.9569176510071274e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4784588255035637e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4784588255035637e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4784588255035637e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2392294127517818e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2392294127517818e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2392294127517818e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.196147063758909e-120
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.196147063758909e-120
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.196147063758909e-120
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0980735318794546e-120
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0980735318794546e-120
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0980735318794546e-120
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5490367659397273e-120
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5490367659397273e-120
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5490367659397273e-120
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.745183829698637e-121
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.745183829698637e-121
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.745183829698637e-121
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8725919148493183e-121
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8725919148493183e-121
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8725919148493183e-121
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9362959574246591e-121
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9362959574246591e-121
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9362959574246591e-121
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.681479787123296e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.681479787123296e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.681479787123296e-122
2020-04-06 04:54:05,577 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:44:16  iter: 420  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7152)  loss_objectness: 0.6931 (0.6941)  loss_rpn_box_reg: 0.1321 (0.1268)  time: 0.6005 (0.6257)  data: 0.0017 (0.0074)  lr: 0.010224  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.840739893561648e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.840739893561648e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.840739893561648e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.420369946780824e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.420369946780824e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.420369946780824e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.210184973390412e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.210184973390412e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.210184973390412e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.05092486695206e-123
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.05092486695206e-123
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.05092486695206e-123
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.02546243347603e-123
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.02546243347603e-123
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.02546243347603e-123
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.512731216738015e-123
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.512731216738015e-123
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.512731216738015e-123
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.563656083690075e-124
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.563656083690075e-124
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.563656083690075e-124
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7818280418450374e-124
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7818280418450374e-124
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7818280418450374e-124
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8909140209225187e-124
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8909140209225187e-124
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8909140209225187e-124
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.454570104612593e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.454570104612593e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.454570104612593e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.727285052306297e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.727285052306297e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.727285052306297e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3636425261531484e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3636425261531484e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3636425261531484e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1818212630765742e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1818212630765742e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1818212630765742e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.909106315382871e-126
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.909106315382871e-126
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.909106315382871e-126
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9545531576914354e-126
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9545531576914354e-126
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9545531576914354e-126
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4772765788457177e-126
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4772765788457177e-126
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4772765788457177e-126
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.386382894228589e-127
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.386382894228589e-127
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.386382894228589e-127
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6931914471142943e-127
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6931914471142943e-127
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6931914471142943e-127
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8465957235571472e-127
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8465957235571472e-127
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8465957235571472e-127
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.232978617785736e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.232978617785736e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.232978617785736e-128
2020-04-06 04:54:17,705 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:41:43  iter: 440  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7142)  loss_objectness: 0.6931 (0.6941)  loss_rpn_box_reg: 0.0904 (0.1257)  time: 0.5988 (0.6249)  data: 0.0017 (0.0071)  lr: 0.012144  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.616489308892868e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.616489308892868e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.616489308892868e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.308244654446434e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.308244654446434e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.308244654446434e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.154122327223217e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.154122327223217e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.154122327223217e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.770611636116085e-129
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.770611636116085e-129
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.770611636116085e-129
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8853058180580424e-129
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8853058180580424e-129
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8853058180580424e-129
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4426529090290212e-129
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4426529090290212e-129
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4426529090290212e-129
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.213264545145106e-130
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.213264545145106e-130
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.213264545145106e-130
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.606632272572553e-130
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.606632272572553e-130
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.606632272572553e-130
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8033161362862765e-130
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8033161362862765e-130
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8033161362862765e-130
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.016580681431383e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.016580681431383e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.016580681431383e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5082903407156913e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5082903407156913e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5082903407156913e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2541451703578456e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2541451703578456e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2541451703578456e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1270725851789228e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1270725851789228e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1270725851789228e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.635362925894614e-132
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.635362925894614e-132
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.635362925894614e-132
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.817681462947307e-132
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.817681462947307e-132
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.817681462947307e-132
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4088407314736535e-132
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4088407314736535e-132
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4088407314736535e-132
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.044203657368268e-133
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.044203657368268e-133
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.044203657368268e-133
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.522101828684134e-133
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.522101828684134e-133
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.522101828684134e-133
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.761050914342067e-133
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.761050914342067e-133
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.761050914342067e-133
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.805254571710335e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.805254571710335e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.805254571710335e-134
2020-04-06 04:54:29,757 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:38:56  iter: 460  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7133)  loss_objectness: 0.6931 (0.6941)  loss_rpn_box_reg: 0.1023 (0.1251)  time: 0.6019 (0.6239)  data: 0.0018 (0.0069)  lr: 0.014064  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.4026272858551673e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.4026272858551673e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.4026272858551673e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2013136429275836e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2013136429275836e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2013136429275836e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1006568214637918e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1006568214637918e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1006568214637918e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.503284107318959e-135
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.503284107318959e-135
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.503284107318959e-135
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7516420536594796e-135
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7516420536594796e-135
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7516420536594796e-135
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3758210268297398e-135
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3758210268297398e-135
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3758210268297398e-135
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.879105134148699e-136
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.879105134148699e-136
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.879105134148699e-136
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4395525670743494e-136
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4395525670743494e-136
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4395525670743494e-136
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7197762835371747e-136
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7197762835371747e-136
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7197762835371747e-136
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.598881417685874e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.598881417685874e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.598881417685874e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.299440708842937e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.299440708842937e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.299440708842937e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1497203544214684e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1497203544214684e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1497203544214684e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0748601772107342e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0748601772107342e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0748601772107342e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.374300886053671e-138
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.374300886053671e-138
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.374300886053671e-138
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6871504430268355e-138
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6871504430268355e-138
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6871504430268355e-138
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3435752215134178e-138
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3435752215134178e-138
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3435752215134178e-138
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.717876107567089e-139
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.717876107567089e-139
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.717876107567089e-139
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3589380537835444e-139
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3589380537835444e-139
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3589380537835444e-139
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6794690268917722e-139
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6794690268917722e-139
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6794690268917722e-139
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.397345134458861e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.397345134458861e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.397345134458861e-140
2020-04-06 04:54:42,118 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:38:05  iter: 480  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7125)  loss_objectness: 0.6931 (0.6940)  loss_rpn_box_reg: 0.1504 (0.1263)  time: 0.6083 (0.6237)  data: 0.0019 (0.0067)  lr: 0.015984  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1986725672294305e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1986725672294305e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1986725672294305e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0993362836147152e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0993362836147152e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0993362836147152e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0496681418073576e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0496681418073576e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0496681418073576e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.248340709036788e-141
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.248340709036788e-141
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.248340709036788e-141
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.624170354518394e-141
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.624170354518394e-141
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.624170354518394e-141
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.312085177259197e-141
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.312085177259197e-141
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.312085177259197e-141
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.560425886295985e-142
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.560425886295985e-142
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.560425886295985e-142
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2802129431479926e-142
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2802129431479926e-142
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2802129431479926e-142
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6401064715739963e-142
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6401064715739963e-142
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6401064715739963e-142
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.200532357869981e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.200532357869981e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.200532357869981e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.100266178934991e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.100266178934991e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.100266178934991e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0501330894674953e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0501330894674953e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0501330894674953e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0250665447337477e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0250665447337477e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0250665447337477e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.1253327236687384e-144
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.1253327236687384e-144
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.1253327236687384e-144
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5626663618343692e-144
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5626663618343692e-144
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5626663618343692e-144
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2813331809171846e-144
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2813331809171846e-144
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2813331809171846e-144
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.406665904585923e-145
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.406665904585923e-145
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.406665904585923e-145
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2033329522929615e-145
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2033329522929615e-145
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2033329522929615e-145
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6016664761464807e-145
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6016664761464807e-145
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6016664761464807e-145
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.008332380732404e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.008332380732404e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.008332380732404e-146
2020-04-06 04:54:54,431 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:37:01  iter: 500  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7117)  loss_objectness: 0.6931 (0.6940)  loss_rpn_box_reg: 0.1255 (0.1267)  time: 0.6072 (0.6233)  data: 0.0017 (0.0065)  lr: 0.017904  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.004166190366202e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.004166190366202e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.004166190366202e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.002083095183101e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.002083095183101e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.002083095183101e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0010415475915505e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0010415475915505e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0010415475915505e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.0052077379577523e-147
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.0052077379577523e-147
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.0052077379577523e-147
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5026038689788762e-147
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5026038689788762e-147
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5026038689788762e-147
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2513019344894381e-147
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2513019344894381e-147
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2513019344894381e-147
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.256509672447191e-148
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.256509672447191e-148
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.256509672447191e-148
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1282548362235952e-148
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1282548362235952e-148
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1282548362235952e-148
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5641274181117976e-148
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5641274181117976e-148
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5641274181117976e-148
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.820637090558988e-149
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.820637090558988e-149
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.820637090558988e-149
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.910318545279494e-149
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.910318545279494e-149
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.910318545279494e-149
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.955159272639747e-149
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.955159272639747e-149
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.955159272639747e-149
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.775796363198735e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.775796363198735e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.775796363198735e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.887898181599368e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.887898181599368e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.887898181599368e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.443949090799684e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.443949090799684e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.443949090799684e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.221974545399842e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.221974545399842e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.221974545399842e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.10987272699921e-151
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.10987272699921e-151
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.10987272699921e-151
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.054936363499605e-151
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.054936363499605e-151
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.054936363499605e-151
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5274681817498023e-151
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5274681817498023e-151
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5274681817498023e-151
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.637340908749012e-152
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.637340908749012e-152
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.637340908749012e-152
2020-04-06 04:55:06,572 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:35:09  iter: 520  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7110)  loss_objectness: 0.6931 (0.6939)  loss_rpn_box_reg: 0.0954 (0.1266)  time: 0.6052 (0.6227)  data: 0.0017 (0.0063)  lr: 0.019824  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.818670454374506e-152
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.818670454374506e-152
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.818670454374506e-152
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.909335227187253e-152
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.909335227187253e-152
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.909335227187253e-152
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.546676135936265e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.546676135936265e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.546676135936265e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.7733380679681323e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.7733380679681323e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.7733380679681323e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3866690339840662e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3866690339840662e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3866690339840662e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1933345169920331e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1933345169920331e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1933345169920331e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.966672584960166e-154
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.966672584960166e-154
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.966672584960166e-154
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.983336292480083e-154
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.983336292480083e-154
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.983336292480083e-154
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4916681462400413e-154
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4916681462400413e-154
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4916681462400413e-154
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.458340731200207e-155
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.458340731200207e-155
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.458340731200207e-155
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7291703656001034e-155
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7291703656001034e-155
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7291703656001034e-155
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8645851828000517e-155
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8645851828000517e-155
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8645851828000517e-155
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.322925914000258e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.322925914000258e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.322925914000258e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.661462957000129e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.661462957000129e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.661462957000129e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3307314785000646e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3307314785000646e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3307314785000646e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1653657392500323e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1653657392500323e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1653657392500323e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.826828696250162e-157
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.826828696250162e-157
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.826828696250162e-157
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.913414348125081e-157
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.913414348125081e-157
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.913414348125081e-157
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4567071740625404e-157
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4567071740625404e-157
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4567071740625404e-157
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.283535870312702e-158
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.283535870312702e-158
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.283535870312702e-158
2020-04-06 04:55:18,593 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:32:48  iter: 540  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7103)  loss_objectness: 0.6931 (0.6939)  loss_rpn_box_reg: 0.1184 (0.1263)  time: 0.6004 (0.6219)  data: 0.0017 (0.0061)  lr: 0.021744  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.641767935156351e-158
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.641767935156351e-158
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.641767935156351e-158
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8208839675781755e-158
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8208839675781755e-158
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8208839675781755e-158
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.104419837890877e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.104419837890877e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.104419837890877e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.552209918945439e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.552209918945439e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.552209918945439e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2761049594727193e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2761049594727193e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2761049594727193e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1380524797363597e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1380524797363597e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1380524797363597e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.6902623986817984e-160
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.6902623986817984e-160
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.6902623986817984e-160
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8451311993408992e-160
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8451311993408992e-160
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8451311993408992e-160
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4225655996704496e-160
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4225655996704496e-160
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4225655996704496e-160
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.112827998352248e-161
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.112827998352248e-161
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.112827998352248e-161
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.556413999176124e-161
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.556413999176124e-161
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.556413999176124e-161
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.778206999588062e-161
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.778206999588062e-161
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.778206999588062e-161
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.89103499794031e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.89103499794031e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.89103499794031e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.445517498970155e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.445517498970155e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.445517498970155e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2227587494850775e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2227587494850775e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2227587494850775e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1113793747425387e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1113793747425387e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1113793747425387e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.556896873712694e-163
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.556896873712694e-163
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.556896873712694e-163
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.778448436856347e-163
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.778448436856347e-163
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.778448436856347e-163
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3892242184281734e-163
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3892242184281734e-163
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3892242184281734e-163
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.946121092140867e-164
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.946121092140867e-164
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.946121092140867e-164
2020-04-06 04:55:30,859 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:31:47  iter: 560  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7097)  loss_objectness: 0.6931 (0.6939)  loss_rpn_box_reg: 0.1018 (0.1258)  time: 0.6049 (0.6216)  data: 0.0017 (0.0060)  lr: 0.023664  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4730605460704336e-164
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4730605460704336e-164
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4730605460704336e-164
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7365302730352168e-164
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7365302730352168e-164
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7365302730352168e-164
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.682651365176084e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.682651365176084e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.682651365176084e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.341325682588042e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.341325682588042e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.341325682588042e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.170662841294021e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.170662841294021e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.170662841294021e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0853314206470105e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0853314206470105e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0853314206470105e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.426657103235053e-166
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.426657103235053e-166
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.426657103235053e-166
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7133285516175262e-166
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7133285516175262e-166
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7133285516175262e-166
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3566642758087631e-166
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3566642758087631e-166
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3566642758087631e-166
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.783321379043816e-167
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.783321379043816e-167
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.783321379043816e-167
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.391660689521908e-167
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.391660689521908e-167
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.391660689521908e-167
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.695830344760954e-167
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.695830344760954e-167
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.695830344760954e-167
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.47915172380477e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.47915172380477e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.47915172380477e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.239575861902385e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.239575861902385e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.239575861902385e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1197879309511924e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1197879309511924e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1197879309511924e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0598939654755962e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0598939654755962e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0598939654755962e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.299469827377981e-169
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.299469827377981e-169
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.299469827377981e-169
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6497349136889905e-169
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6497349136889905e-169
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6497349136889905e-169
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3248674568444952e-169
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3248674568444952e-169
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3248674568444952e-169
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.624337284222476e-170
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.624337284222476e-170
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.624337284222476e-170
2020-04-06 04:55:42,879 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:29:41  iter: 580  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7091)  loss_objectness: 0.6931 (0.6939)  loss_rpn_box_reg: 0.0917 (0.1249)  time: 0.6009 (0.6209)  data: 0.0016 (0.0058)  lr: 0.025584  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.312168642111238e-170
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.312168642111238e-170
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.312168642111238e-170
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.656084321055619e-170
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.656084321055619e-170
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.656084321055619e-170
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.280421605278095e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.280421605278095e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.280421605278095e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.140210802639048e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.140210802639048e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.140210802639048e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.070105401319524e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.070105401319524e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.070105401319524e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.035052700659762e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.035052700659762e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.035052700659762e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.17526350329881e-172
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.17526350329881e-172
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.17526350329881e-172
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.587631751649405e-172
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.587631751649405e-172
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.587631751649405e-172
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2938158758247024e-172
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2938158758247024e-172
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2938158758247024e-172
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.469079379123512e-173
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.469079379123512e-173
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.469079379123512e-173
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.234539689561756e-173
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.234539689561756e-173
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.234539689561756e-173
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.617269844780878e-173
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.617269844780878e-173
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.617269844780878e-173
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.08634922390439e-174
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.08634922390439e-174
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.08634922390439e-174
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.043174611952195e-174
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.043174611952195e-174
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.043174611952195e-174
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0215873059760975e-174
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0215873059760975e-174
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0215873059760975e-174
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0107936529880487e-174
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0107936529880487e-174
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0107936529880487e-174
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.053968264940244e-175
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.053968264940244e-175
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.053968264940244e-175
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.526984132470122e-175
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.526984132470122e-175
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.526984132470122e-175
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.263492066235061e-175
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.263492066235061e-175
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.263492066235061e-175
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.317460331175305e-176
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.317460331175305e-176
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.317460331175305e-176
2020-04-06 04:55:54,843 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:27:28  iter: 600  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7086)  loss_objectness: 0.6931 (0.6938)  loss_rpn_box_reg: 0.1168 (0.1250)  time: 0.6000 (0.6201)  data: 0.0017 (0.0057)  lr: 0.027504  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1587301655876523e-176
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1587301655876523e-176
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1587301655876523e-176
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5793650827938261e-176
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5793650827938261e-176
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5793650827938261e-176
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.896825413969131e-177
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.896825413969131e-177
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.896825413969131e-177
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.9484127069845653e-177
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.9484127069845653e-177
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.9484127069845653e-177
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9742063534922827e-177
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9742063534922827e-177
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9742063534922827e-177
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.871031767461413e-178
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.871031767461413e-178
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.871031767461413e-178
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.935515883730707e-178
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.935515883730707e-178
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.935515883730707e-178
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4677579418653533e-178
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4677579418653533e-178
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4677579418653533e-178
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2338789709326767e-178
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2338789709326767e-178
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2338789709326767e-178
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.169394854663383e-179
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.169394854663383e-179
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.169394854663383e-179
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.084697427331692e-179
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.084697427331692e-179
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.084697427331692e-179
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.542348713665846e-179
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.542348713665846e-179
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.542348713665846e-179
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.71174356832923e-180
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.71174356832923e-180
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.71174356832923e-180
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.855871784164615e-180
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.855871784164615e-180
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.855871784164615e-180
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9279358920823073e-180
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9279358920823073e-180
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9279358920823073e-180
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.639679460411536e-181
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.639679460411536e-181
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.639679460411536e-181
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.819839730205768e-181
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.819839730205768e-181
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.819839730205768e-181
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.409919865102884e-181
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.409919865102884e-181
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.409919865102884e-181
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.204959932551442e-181
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.204959932551442e-181
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.204959932551442e-181
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.02479966275721e-182
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.02479966275721e-182
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.02479966275721e-182
2020-04-06 04:56:07,020 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:26:18  iter: 620  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7081)  loss_objectness: 0.6931 (0.6938)  loss_rpn_box_reg: 0.1000 (0.1249)  time: 0.6031 (0.6198)  data: 0.0018 (0.0056)  lr: 0.029424  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.012399831378605e-182
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.012399831378605e-182
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.012399831378605e-182
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5061999156893026e-182
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5061999156893026e-182
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5061999156893026e-182
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.530999578446513e-183
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.530999578446513e-183
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.530999578446513e-183
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7654997892232564e-183
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7654997892232564e-183
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7654997892232564e-183
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8827498946116282e-183
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8827498946116282e-183
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8827498946116282e-183
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.413749473058141e-184
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.413749473058141e-184
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.413749473058141e-184
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.706874736529071e-184
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.706874736529071e-184
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.706874736529071e-184
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3534373682645353e-184
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3534373682645353e-184
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3534373682645353e-184
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1767186841322676e-184
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1767186841322676e-184
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1767186841322676e-184
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.883593420661338e-185
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.883593420661338e-185
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.883593420661338e-185
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.941796710330669e-185
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.941796710330669e-185
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.941796710330669e-185
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4708983551653345e-185
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4708983551653345e-185
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4708983551653345e-185
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.354491775826673e-186
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.354491775826673e-186
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.354491775826673e-186
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6772458879133364e-186
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6772458879133364e-186
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6772458879133364e-186
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8386229439566682e-186
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8386229439566682e-186
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8386229439566682e-186
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.193114719783341e-187
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.193114719783341e-187
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.193114719783341e-187
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5965573598916705e-187
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5965573598916705e-187
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5965573598916705e-187
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2982786799458352e-187
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2982786799458352e-187
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2982786799458352e-187
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1491393399729176e-187
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1491393399729176e-187
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1491393399729176e-187
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.745696699864588e-188
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.745696699864588e-188
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.745696699864588e-188
2020-04-06 04:56:19,043 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:24:33  iter: 640  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7076)  loss_objectness: 0.6931 (0.6938)  loss_rpn_box_reg: 0.1344 (0.1254)  time: 0.5999 (0.6192)  data: 0.0017 (0.0055)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.872848349932294e-188
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.872848349932294e-188
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.872848349932294e-188
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.436424174966147e-188
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.436424174966147e-188
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.436424174966147e-188
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.182120874830735e-189
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.182120874830735e-189
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.182120874830735e-189
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5910604374153675e-189
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5910604374153675e-189
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5910604374153675e-189
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7955302187076838e-189
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7955302187076838e-189
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7955302187076838e-189
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.977651093538419e-190
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.977651093538419e-190
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.977651093538419e-190
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.4888255467692094e-190
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.4888255467692094e-190
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.4888255467692094e-190
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2444127733846047e-190
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2444127733846047e-190
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2444127733846047e-190
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1222063866923024e-190
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1222063866923024e-190
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1222063866923024e-190
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.611031933461512e-191
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.611031933461512e-191
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.611031933461512e-191
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.805515966730756e-191
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.805515966730756e-191
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.805515966730756e-191
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.402757983365378e-191
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.402757983365378e-191
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.402757983365378e-191
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.01378991682689e-192
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.01378991682689e-192
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.01378991682689e-192
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.506894958413445e-192
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.506894958413445e-192
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.506894958413445e-192
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7534474792067224e-192
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7534474792067224e-192
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7534474792067224e-192
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.767237396033612e-193
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.767237396033612e-193
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.767237396033612e-193
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.383618698016806e-193
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.383618698016806e-193
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.383618698016806e-193
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.191809349008403e-193
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.191809349008403e-193
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.191809349008403e-193
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0959046745042015e-193
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0959046745042015e-193
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0959046745042015e-193
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.479523372521008e-194
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.479523372521008e-194
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.479523372521008e-194
2020-04-06 04:56:31,188 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:23:23  iter: 660  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7072)  loss_objectness: 0.6931 (0.6938)  loss_rpn_box_reg: 0.1065 (0.1253)  time: 0.6035 (0.6188)  data: 0.0018 (0.0053)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.739761686260504e-194
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.739761686260504e-194
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.739761686260504e-194
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.369880843130252e-194
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.369880843130252e-194
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.369880843130252e-194
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.84940421565126e-195
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.84940421565126e-195
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.84940421565126e-195
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.42470210782563e-195
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.42470210782563e-195
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.42470210782563e-195
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.712351053912815e-195
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.712351053912815e-195
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.712351053912815e-195
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.561755269564074e-196
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.561755269564074e-196
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.561755269564074e-196
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.280877634782037e-196
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.280877634782037e-196
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.280877634782037e-196
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1404388173910186e-196
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1404388173910186e-196
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1404388173910186e-196
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0702194086955093e-196
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0702194086955093e-196
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0702194086955093e-196
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.351097043477547e-197
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.351097043477547e-197
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.351097043477547e-197
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6755485217387732e-197
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6755485217387732e-197
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6755485217387732e-197
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3377742608693866e-197
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3377742608693866e-197
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3377742608693866e-197
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.688871304346933e-198
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.688871304346933e-198
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.688871304346933e-198
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3444356521734666e-198
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3444356521734666e-198
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3444356521734666e-198
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6722178260867333e-198
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6722178260867333e-198
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6722178260867333e-198
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.361089130433666e-199
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.361089130433666e-199
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.361089130433666e-199
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.180544565216833e-199
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.180544565216833e-199
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.180544565216833e-199
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0902722826084166e-199
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0902722826084166e-199
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0902722826084166e-199
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0451361413042083e-199
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0451361413042083e-199
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0451361413042083e-199
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.225680706521042e-200
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.225680706521042e-200
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.225680706521042e-200
2020-04-06 04:56:43,179 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:21:40  iter: 680  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7068)  loss_objectness: 0.6931 (0.6937)  loss_rpn_box_reg: 0.1235 (0.1259)  time: 0.6024 (0.6183)  data: 0.0017 (0.0052)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.612840353260521e-200
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.612840353260521e-200
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.612840353260521e-200
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3064201766302604e-200
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3064201766302604e-200
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3064201766302604e-200
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.532100883151302e-201
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.532100883151302e-201
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.532100883151302e-201
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.266050441575651e-201
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.266050441575651e-201
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.266050441575651e-201
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6330252207878255e-201
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6330252207878255e-201
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6330252207878255e-201
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.165126103939127e-202
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.165126103939127e-202
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.165126103939127e-202
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.082563051969564e-202
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.082563051969564e-202
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.082563051969564e-202
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.041281525984782e-202
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.041281525984782e-202
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.041281525984782e-202
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.020640762992391e-202
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.020640762992391e-202
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.020640762992391e-202
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.103203814961955e-203
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.103203814961955e-203
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.103203814961955e-203
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5516019074809773e-203
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5516019074809773e-203
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5516019074809773e-203
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2758009537404886e-203
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2758009537404886e-203
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2758009537404886e-203
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.379004768702443e-204
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.379004768702443e-204
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.379004768702443e-204
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1895023843512216e-204
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1895023843512216e-204
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1895023843512216e-204
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5947511921756108e-204
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5947511921756108e-204
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5947511921756108e-204
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.973755960878054e-205
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.973755960878054e-205
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.973755960878054e-205
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.986877980439027e-205
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.986877980439027e-205
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.986877980439027e-205
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9934389902195135e-205
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9934389902195135e-205
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9934389902195135e-205
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.967194951097568e-206
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.967194951097568e-206
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.967194951097568e-206
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.983597475548784e-206
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.983597475548784e-206
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.983597475548784e-206
2020-04-06 04:56:55,162 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:20:01  iter: 700  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7064)  loss_objectness: 0.6931 (0.6937)  loss_rpn_box_reg: 0.1157 (0.1261)  time: 0.5987 (0.6177)  data: 0.0017 (0.0051)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.491798737774392e-206
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.491798737774392e-206
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.491798737774392e-206
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.245899368887196e-206
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.245899368887196e-206
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.245899368887196e-206
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.22949684443598e-207
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.22949684443598e-207
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.22949684443598e-207
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.11474842221799e-207
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.11474842221799e-207
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.11474842221799e-207
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.557374211108995e-207
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.557374211108995e-207
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.557374211108995e-207
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.786871055544975e-208
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.786871055544975e-208
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.786871055544975e-208
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8934355277724873e-208
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8934355277724873e-208
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8934355277724873e-208
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9467177638862437e-208
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9467177638862437e-208
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9467177638862437e-208
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.733588819431218e-209
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.733588819431218e-209
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.733588819431218e-209
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.866794409715609e-209
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.866794409715609e-209
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.866794409715609e-209
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4333972048578046e-209
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4333972048578046e-209
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4333972048578046e-209
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2166986024289023e-209
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2166986024289023e-209
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2166986024289023e-209
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.083493012144512e-210
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.083493012144512e-210
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.083493012144512e-210
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.041746506072256e-210
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.041746506072256e-210
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.041746506072256e-210
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.520873253036128e-210
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.520873253036128e-210
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.520873253036128e-210
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.60436626518064e-211
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.60436626518064e-211
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.60436626518064e-211
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.80218313259032e-211
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.80218313259032e-211
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.80218313259032e-211
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.90109156629516e-211
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.90109156629516e-211
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.90109156629516e-211
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.5054578314758e-212
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.5054578314758e-212
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.5054578314758e-212
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.7527289157379e-212
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.7527289157379e-212
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.7527289157379e-212
2020-04-06 04:57:07,340 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:19:09  iter: 720  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7060)  loss_objectness: 0.6931 (0.6937)  loss_rpn_box_reg: 0.1038 (0.1262)  time: 0.6040 (0.6175)  data: 0.0018 (0.0050)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.37636445786895e-212
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.37636445786895e-212
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.37636445786895e-212
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.188182228934475e-212
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.188182228934475e-212
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.188182228934475e-212
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.940911144672375e-213
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.940911144672375e-213
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.940911144672375e-213
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9704555723361872e-213
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9704555723361872e-213
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9704555723361872e-213
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4852277861680936e-213
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4852277861680936e-213
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4852277861680936e-213
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.426138930840468e-214
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.426138930840468e-214
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.426138930840468e-214
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.713069465420234e-214
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.713069465420234e-214
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.713069465420234e-214
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.856534732710117e-214
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.856534732710117e-214
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.856534732710117e-214
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.282673663550585e-215
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.282673663550585e-215
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.282673663550585e-215
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.641336831775293e-215
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.641336831775293e-215
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.641336831775293e-215
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3206684158876463e-215
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3206684158876463e-215
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3206684158876463e-215
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1603342079438231e-215
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1603342079438231e-215
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1603342079438231e-215
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.801671039719116e-216
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.801671039719116e-216
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.801671039719116e-216
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.900835519859558e-216
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.900835519859558e-216
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.900835519859558e-216
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.450417759929779e-216
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.450417759929779e-216
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.450417759929779e-216
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.252088799648895e-217
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.252088799648895e-217
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.252088799648895e-217
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6260443998244473e-217
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6260443998244473e-217
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6260443998244473e-217
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8130221999122236e-217
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8130221999122236e-217
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8130221999122236e-217
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.065110999561118e-218
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.065110999561118e-218
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.065110999561118e-218
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.532555499780559e-218
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.532555499780559e-218
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.532555499780559e-218
2020-04-06 04:57:19,336 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:17:41  iter: 740  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7057)  loss_objectness: 0.6931 (0.6937)  loss_rpn_box_reg: 0.0965 (0.1255)  time: 0.5999 (0.6170)  data: 0.0016 (0.0050)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2662777498902796e-218
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2662777498902796e-218
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2662777498902796e-218
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1331388749451398e-218
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1331388749451398e-218
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1331388749451398e-218
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.665694374725699e-219
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.665694374725699e-219
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.665694374725699e-219
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8328471873628494e-219
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8328471873628494e-219
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8328471873628494e-219
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4164235936814247e-219
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4164235936814247e-219
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4164235936814247e-219
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.082117968407124e-220
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.082117968407124e-220
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.082117968407124e-220
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.541058984203562e-220
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.541058984203562e-220
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.541058984203562e-220
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.770529492101781e-220
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.770529492101781e-220
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.770529492101781e-220
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.852647460508905e-221
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.852647460508905e-221
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.852647460508905e-221
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.4263237302544523e-221
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.4263237302544523e-221
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.4263237302544523e-221
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2131618651272261e-221
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2131618651272261e-221
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2131618651272261e-221
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1065809325636131e-221
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1065809325636131e-221
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1065809325636131e-221
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.5329046628180653e-222
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.5329046628180653e-222
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.5329046628180653e-222
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7664523314090327e-222
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7664523314090327e-222
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7664523314090327e-222
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3832261657045163e-222
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3832261657045163e-222
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3832261657045163e-222
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.916130828522582e-223
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.916130828522582e-223
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.916130828522582e-223
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.458065414261291e-223
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.458065414261291e-223
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.458065414261291e-223
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7290327071306454e-223
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7290327071306454e-223
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7290327071306454e-223
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.645163535653227e-224
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.645163535653227e-224
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.645163535653227e-224
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.322581767826614e-224
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.322581767826614e-224
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.322581767826614e-224
2020-04-06 04:57:31,406 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:16:32  iter: 760  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7053)  loss_objectness: 0.6931 (0.6937)  loss_rpn_box_reg: 0.0989 (0.1249)  time: 0.6004 (0.6166)  data: 0.0017 (0.0049)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.161290883913307e-224
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.161290883913307e-224
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.161290883913307e-224
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0806454419566534e-224
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0806454419566534e-224
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0806454419566534e-224
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.403227209783267e-225
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.403227209783267e-225
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.403227209783267e-225
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7016136048916335e-225
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7016136048916335e-225
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7016136048916335e-225
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3508068024458167e-225
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3508068024458167e-225
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3508068024458167e-225
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.754034012229084e-226
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.754034012229084e-226
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.754034012229084e-226
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.377017006114542e-226
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.377017006114542e-226
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.377017006114542e-226
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.688508503057271e-226
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.688508503057271e-226
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.688508503057271e-226
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.442542515286355e-227
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.442542515286355e-227
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.442542515286355e-227
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.2212712576431773e-227
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.2212712576431773e-227
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.2212712576431773e-227
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1106356288215886e-227
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1106356288215886e-227
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1106356288215886e-227
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0553178144107943e-227
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0553178144107943e-227
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0553178144107943e-227
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.276589072053972e-228
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.276589072053972e-228
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.276589072053972e-228
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.638294536026986e-228
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.638294536026986e-228
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.638294536026986e-228
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.319147268013493e-228
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.319147268013493e-228
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.319147268013493e-228
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.595736340067465e-229
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.595736340067465e-229
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.595736340067465e-229
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2978681700337323e-229
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2978681700337323e-229
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2978681700337323e-229
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6489340850168661e-229
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6489340850168661e-229
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6489340850168661e-229
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.244670425084331e-230
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.244670425084331e-230
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.244670425084331e-230
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1223352125421653e-230
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1223352125421653e-230
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1223352125421653e-230
2020-04-06 04:57:43,478 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:15:27  iter: 780  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7050)  loss_objectness: 0.6931 (0.6937)  loss_rpn_box_reg: 0.1006 (0.1250)  time: 0.5991 (0.6163)  data: 0.0016 (0.0048)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0611676062710827e-230
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0611676062710827e-230
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0611676062710827e-230
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0305838031355413e-230
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0305838031355413e-230
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0305838031355413e-230
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.152919015677707e-231
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.152919015677707e-231
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.152919015677707e-231
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5764595078388533e-231
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5764595078388533e-231
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5764595078388533e-231
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2882297539194267e-231
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2882297539194267e-231
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2882297539194267e-231
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.441148769597133e-232
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.441148769597133e-232
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.441148769597133e-232
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.220574384798567e-232
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.220574384798567e-232
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.220574384798567e-232
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6102871923992833e-232
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6102871923992833e-232
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6102871923992833e-232
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.051435961996417e-233
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.051435961996417e-233
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.051435961996417e-233
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0257179809982083e-233
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0257179809982083e-233
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0257179809982083e-233
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0128589904991042e-233
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0128589904991042e-233
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0128589904991042e-233
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0064294952495521e-233
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0064294952495521e-233
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0064294952495521e-233
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.0321474762477604e-234
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.0321474762477604e-234
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.0321474762477604e-234
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5160737381238802e-234
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5160737381238802e-234
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5160737381238802e-234
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2580368690619401e-234
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2580368690619401e-234
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2580368690619401e-234
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.290184345309701e-235
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.290184345309701e-235
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.290184345309701e-235
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1450921726548502e-235
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1450921726548502e-235
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1450921726548502e-235
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5725460863274251e-235
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5725460863274251e-235
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5725460863274251e-235
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.862730431637126e-236
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.862730431637126e-236
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.862730431637126e-236
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.931365215818563e-236
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.931365215818563e-236
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.931365215818563e-236
2020-04-06 04:57:55,683 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:14:50  iter: 800  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7047)  loss_objectness: 0.6931 (0.6936)  loss_rpn_box_reg: 0.1245 (0.1248)  time: 0.6010 (0.6161)  data: 0.0017 (0.0047)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9656826079092814e-236
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9656826079092814e-236
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9656826079092814e-236
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.828413039546407e-237
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.828413039546407e-237
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.828413039546407e-237
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.914206519773204e-237
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.914206519773204e-237
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.914206519773204e-237
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.457103259886602e-237
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.457103259886602e-237
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.457103259886602e-237
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.228551629943301e-237
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.228551629943301e-237
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.228551629943301e-237
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.142758149716505e-238
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.142758149716505e-238
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.142758149716505e-238
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0713790748582522e-238
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0713790748582522e-238
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0713790748582522e-238
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5356895374291261e-238
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5356895374291261e-238
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5356895374291261e-238
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.678447687145631e-239
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.678447687145631e-239
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.678447687145631e-239
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8392238435728152e-239
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8392238435728152e-239
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8392238435728152e-239
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9196119217864076e-239
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9196119217864076e-239
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9196119217864076e-239
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.598059608932038e-240
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.598059608932038e-240
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.598059608932038e-240
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.799029804466019e-240
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.799029804466019e-240
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.799029804466019e-240
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3995149022330095e-240
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3995149022330095e-240
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3995149022330095e-240
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1997574511165048e-240
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1997574511165048e-240
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1997574511165048e-240
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.998787255582524e-241
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.998787255582524e-241
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.998787255582524e-241
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.999393627791262e-241
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.999393627791262e-241
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.999393627791262e-241
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.499696813895631e-241
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.499696813895631e-241
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.499696813895631e-241
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.498484069478155e-242
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.498484069478155e-242
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.498484069478155e-242
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7492420347390774e-242
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7492420347390774e-242
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7492420347390774e-242
2020-04-06 04:58:07,822 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:14:02  iter: 820  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7045)  loss_objectness: 0.6931 (0.6936)  loss_rpn_box_reg: 0.1144 (0.1246)  time: 0.6034 (0.6159)  data: 0.0016 (0.0046)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8746210173695387e-242
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8746210173695387e-242
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8746210173695387e-242
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.373105086847693e-243
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.373105086847693e-243
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.373105086847693e-243
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.686552543423847e-243
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.686552543423847e-243
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.686552543423847e-243
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3432762717119234e-243
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3432762717119234e-243
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3432762717119234e-243
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1716381358559617e-243
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1716381358559617e-243
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1716381358559617e-243
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.858190679279809e-244
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.858190679279809e-244
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.858190679279809e-244
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9290953396399042e-244
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9290953396399042e-244
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9290953396399042e-244
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4645476698199521e-244
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4645476698199521e-244
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4645476698199521e-244
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.322738349099761e-245
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.322738349099761e-245
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.322738349099761e-245
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6613691745498803e-245
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6613691745498803e-245
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6613691745498803e-245
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8306845872749401e-245
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8306845872749401e-245
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8306845872749401e-245
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.153422936374701e-246
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.153422936374701e-246
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.153422936374701e-246
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5767114681873503e-246
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5767114681873503e-246
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5767114681873503e-246
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2883557340936752e-246
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2883557340936752e-246
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2883557340936752e-246
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1441778670468376e-246
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1441778670468376e-246
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1441778670468376e-246
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.720889335234188e-247
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.720889335234188e-247
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.720889335234188e-247
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.860444667617094e-247
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.860444667617094e-247
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.860444667617094e-247
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.430222333808547e-247
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.430222333808547e-247
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.430222333808547e-247
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.151111669042735e-248
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.151111669042735e-248
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.151111669042735e-248
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5755558345213674e-248
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5755558345213674e-248
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5755558345213674e-248
2020-04-06 04:58:19,948 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:13:13  iter: 840  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7042)  loss_objectness: 0.6931 (0.6936)  loss_rpn_box_reg: 0.1200 (0.1244)  time: 0.6018 (0.6157)  data: 0.0017 (0.0046)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7877779172606837e-248
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7877779172606837e-248
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7877779172606837e-248
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.938889586303419e-249
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.938889586303419e-249
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.938889586303419e-249
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.4694447931517093e-249
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.4694447931517093e-249
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.4694447931517093e-249
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2347223965758547e-249
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2347223965758547e-249
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2347223965758547e-249
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1173611982879273e-249
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1173611982879273e-249
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1173611982879273e-249
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.586805991439637e-250
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.586805991439637e-250
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.586805991439637e-250
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7934029957198183e-250
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7934029957198183e-250
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7934029957198183e-250
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3967014978599092e-250
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3967014978599092e-250
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3967014978599092e-250
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.983507489299546e-251
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.983507489299546e-251
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.983507489299546e-251
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.491753744649773e-251
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.491753744649773e-251
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.491753744649773e-251
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7458768723248864e-251
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7458768723248864e-251
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7458768723248864e-251
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.729384361624432e-252
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.729384361624432e-252
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.729384361624432e-252
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.364692180812216e-252
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.364692180812216e-252
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.364692180812216e-252
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.182346090406108e-252
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.182346090406108e-252
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.182346090406108e-252
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.091173045203054e-252
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.091173045203054e-252
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.091173045203054e-252
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.45586522601527e-253
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.45586522601527e-253
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.45586522601527e-253
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.727932613007635e-253
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.727932613007635e-253
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.727932613007635e-253
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3639663065038175e-253
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3639663065038175e-253
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3639663065038175e-253
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.819831532519088e-254
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.819831532519088e-254
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.819831532519088e-254
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.409915766259544e-254
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.409915766259544e-254
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.409915766259544e-254
2020-04-06 04:58:32,005 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:12:14  iter: 860  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7039)  loss_objectness: 0.6931 (0.6936)  loss_rpn_box_reg: 0.1061 (0.1243)  time: 0.5983 (0.6154)  data: 0.0017 (0.0045)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.704957883129772e-254
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.704957883129772e-254
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.704957883129772e-254
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.52478941564886e-255
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.52478941564886e-255
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.52478941564886e-255
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.26239470782443e-255
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.26239470782443e-255
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.26239470782443e-255
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.131197353912215e-255
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.131197353912215e-255
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.131197353912215e-255
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0655986769561075e-255
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0655986769561075e-255
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0655986769561075e-255
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.327993384780537e-256
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.327993384780537e-256
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.327993384780537e-256
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6639966923902686e-256
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6639966923902686e-256
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6639966923902686e-256
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3319983461951343e-256
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3319983461951343e-256
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3319983461951343e-256
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.659991730975672e-257
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.659991730975672e-257
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.659991730975672e-257
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.329995865487836e-257
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.329995865487836e-257
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.329995865487836e-257
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.664997932743918e-257
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.664997932743918e-257
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.664997932743918e-257
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.32498966371959e-258
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.32498966371959e-258
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.32498966371959e-258
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.162494831859795e-258
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.162494831859795e-258
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.162494831859795e-258
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0812474159298974e-258
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0812474159298974e-258
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0812474159298974e-258
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0406237079649487e-258
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0406237079649487e-258
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0406237079649487e-258
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.2031185398247434e-259
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.2031185398247434e-259
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.2031185398247434e-259
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6015592699123717e-259
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6015592699123717e-259
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6015592699123717e-259
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3007796349561859e-259
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3007796349561859e-259
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3007796349561859e-259
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.503898174780929e-260
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.503898174780929e-260
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.503898174780929e-260
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2519490873904646e-260
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2519490873904646e-260
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2519490873904646e-260
2020-04-06 04:58:43,927 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:10:51  iter: 880  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7037)  loss_objectness: 0.6931 (0.6936)  loss_rpn_box_reg: 0.1072 (0.1239)  time: 0.6004 (0.6150)  data: 0.0016 (0.0044)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6259745436952323e-260
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6259745436952323e-260
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6259745436952323e-260
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.129872718476162e-261
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.129872718476162e-261
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.129872718476162e-261
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.064936359238081e-261
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.064936359238081e-261
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.064936359238081e-261
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0324681796190404e-261
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0324681796190404e-261
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0324681796190404e-261
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0162340898095202e-261
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0162340898095202e-261
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0162340898095202e-261
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.081170449047601e-262
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.081170449047601e-262
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.081170449047601e-262
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5405852245238005e-262
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5405852245238005e-262
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5405852245238005e-262
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2702926122619002e-262
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2702926122619002e-262
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2702926122619002e-262
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.351463061309501e-263
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.351463061309501e-263
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.351463061309501e-263
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1757315306547506e-263
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1757315306547506e-263
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1757315306547506e-263
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5878657653273753e-263
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5878657653273753e-263
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5878657653273753e-263
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.939328826636877e-264
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.939328826636877e-264
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.939328826636877e-264
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.9696644133184383e-264
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.9696644133184383e-264
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.9696644133184383e-264
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9848322066592191e-264
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9848322066592191e-264
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9848322066592191e-264
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.924161033296096e-265
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.924161033296096e-265
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.924161033296096e-265
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.962080516648048e-265
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.962080516648048e-265
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.962080516648048e-265
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.481040258324024e-265
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.481040258324024e-265
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.481040258324024e-265
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.240520129162012e-265
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.240520129162012e-265
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.240520129162012e-265
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.20260064581006e-266
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.20260064581006e-266
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.20260064581006e-266
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.10130032290503e-266
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.10130032290503e-266
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.10130032290503e-266
2020-04-06 04:58:56,065 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:10:11  iter: 900  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7034)  loss_objectness: 0.6931 (0.6936)  loss_rpn_box_reg: 0.0963 (0.1236)  time: 0.5992 (0.6148)  data: 0.0016 (0.0044)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.550650161452515e-266
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.550650161452515e-266
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.550650161452515e-266
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.753250807262575e-267
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.753250807262575e-267
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.753250807262575e-267
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8766254036312874e-267
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8766254036312874e-267
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8766254036312874e-267
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9383127018156437e-267
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9383127018156437e-267
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9383127018156437e-267
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.691563509078218e-268
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.691563509078218e-268
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.691563509078218e-268
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.845781754539109e-268
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.845781754539109e-268
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.845781754539109e-268
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4228908772695546e-268
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4228908772695546e-268
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4228908772695546e-268
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2114454386347773e-268
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2114454386347773e-268
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2114454386347773e-268
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.057227193173887e-269
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.057227193173887e-269
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.057227193173887e-269
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0286135965869433e-269
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0286135965869433e-269
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0286135965869433e-269
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5143067982934716e-269
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5143067982934716e-269
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5143067982934716e-269
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.571533991467358e-270
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.571533991467358e-270
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.571533991467358e-270
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.785766995733679e-270
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.785766995733679e-270
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.785766995733679e-270
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8928834978668395e-270
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8928834978668395e-270
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8928834978668395e-270
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.464417489334198e-271
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.464417489334198e-271
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.464417489334198e-271
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.732208744667099e-271
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.732208744667099e-271
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.732208744667099e-271
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3661043723335494e-271
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3661043723335494e-271
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3661043723335494e-271
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1830521861667747e-271
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1830521861667747e-271
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1830521861667747e-271
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.915260930833874e-272
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.915260930833874e-272
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.915260930833874e-272
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.957630465416937e-272
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.957630465416937e-272
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.957630465416937e-272
2020-04-06 04:59:08,266 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:09:42  iter: 920  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7032)  loss_objectness: 0.6931 (0.6936)  loss_rpn_box_reg: 0.1245 (0.1239)  time: 0.6045 (0.6147)  data: 0.0016 (0.0043)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4788152327084684e-272
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4788152327084684e-272
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4788152327084684e-272
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.394076163542342e-273
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.394076163542342e-273
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.394076163542342e-273
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.697038081771171e-273
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.697038081771171e-273
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.697038081771171e-273
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8485190408855855e-273
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8485190408855855e-273
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8485190408855855e-273
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.242595204427927e-274
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.242595204427927e-274
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.242595204427927e-274
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.621297602213964e-274
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.621297602213964e-274
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.621297602213964e-274
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.310648801106982e-274
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.310648801106982e-274
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.310648801106982e-274
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.155324400553491e-274
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.155324400553491e-274
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.155324400553491e-274
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.776622002767455e-275
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.776622002767455e-275
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.776622002767455e-275
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8883110013837273e-275
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8883110013837273e-275
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8883110013837273e-275
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4441555006918637e-275
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4441555006918637e-275
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4441555006918637e-275
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.220777503459318e-276
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.220777503459318e-276
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.220777503459318e-276
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.610388751729659e-276
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.610388751729659e-276
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.610388751729659e-276
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8051943758648296e-276
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8051943758648296e-276
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8051943758648296e-276
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.025971879324148e-277
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.025971879324148e-277
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.025971879324148e-277
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.512985939662074e-277
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.512985939662074e-277
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.512985939662074e-277
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.256492969831037e-277
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.256492969831037e-277
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.256492969831037e-277
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1282464849155185e-277
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1282464849155185e-277
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1282464849155185e-277
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.641232424577593e-278
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.641232424577593e-278
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.641232424577593e-278
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8206162122887962e-278
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8206162122887962e-278
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8206162122887962e-278
2020-04-06 04:59:20,505 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:09:21  iter: 940  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7030)  loss_objectness: 0.6931 (0.6936)  loss_rpn_box_reg: 0.1074 (0.1234)  time: 0.6067 (0.6146)  data: 0.0018 (0.0043)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4103081061443981e-278
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4103081061443981e-278
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4103081061443981e-278
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.051540530721991e-279
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.051540530721991e-279
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.051540530721991e-279
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5257702653609953e-279
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5257702653609953e-279
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5257702653609953e-279
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7628851326804976e-279
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7628851326804976e-279
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7628851326804976e-279
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.814425663402488e-280
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.814425663402488e-280
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.814425663402488e-280
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.407212831701244e-280
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.407212831701244e-280
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.407212831701244e-280
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.203606415850622e-280
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.203606415850622e-280
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.203606415850622e-280
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.101803207925311e-280
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.101803207925311e-280
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.101803207925311e-280
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.509016039626555e-281
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.509016039626555e-281
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.509016039626555e-281
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7545080198132776e-281
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7545080198132776e-281
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7545080198132776e-281
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3772540099066388e-281
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3772540099066388e-281
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3772540099066388e-281
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.886270049533194e-282
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.886270049533194e-282
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.886270049533194e-282
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.443135024766597e-282
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.443135024766597e-282
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.443135024766597e-282
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7215675123832985e-282
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7215675123832985e-282
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7215675123832985e-282
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.607837561916492e-283
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.607837561916492e-283
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.607837561916492e-283
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.303918780958246e-283
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.303918780958246e-283
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.303918780958246e-283
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.151959390479123e-283
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.151959390479123e-283
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.151959390479123e-283
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0759796952395615e-283
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0759796952395615e-283
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0759796952395615e-283
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.379898476197808e-284
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.379898476197808e-284
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.379898476197808e-284
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.689949238098904e-284
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.689949238098904e-284
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.689949238098904e-284
2020-04-06 04:59:32,559 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:08:29  iter: 960  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7028)  loss_objectness: 0.6931 (0.6935)  loss_rpn_box_reg: 0.1145 (0.1240)  time: 0.6037 (0.6144)  data: 0.0017 (0.0042)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.344974619049452e-284
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.344974619049452e-284
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.344974619049452e-284
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.72487309524726e-285
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.72487309524726e-285
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.72487309524726e-285
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.36243654762363e-285
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.36243654762363e-285
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.36243654762363e-285
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.681218273811815e-285
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.681218273811815e-285
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.681218273811815e-285
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.406091369059075e-286
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.406091369059075e-286
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.406091369059075e-286
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.2030456845295373e-286
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.2030456845295373e-286
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.2030456845295373e-286
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1015228422647686e-286
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1015228422647686e-286
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1015228422647686e-286
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0507614211323843e-286
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0507614211323843e-286
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0507614211323843e-286
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.253807105661922e-287
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.253807105661922e-287
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.253807105661922e-287
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.626903552830961e-287
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.626903552830961e-287
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.626903552830961e-287
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3134517764154804e-287
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3134517764154804e-287
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3134517764154804e-287
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.567258882077402e-288
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.567258882077402e-288
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.567258882077402e-288
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.283629441038701e-288
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.283629441038701e-288
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.283629441038701e-288
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6418147205193505e-288
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6418147205193505e-288
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6418147205193505e-288
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.209073602596753e-289
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.209073602596753e-289
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.209073602596753e-289
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1045368012983762e-289
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1045368012983762e-289
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1045368012983762e-289
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0522684006491881e-289
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0522684006491881e-289
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0522684006491881e-289
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0261342003245941e-289
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0261342003245941e-289
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0261342003245941e-289
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.1306710016229703e-290
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.1306710016229703e-290
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.1306710016229703e-290
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5653355008114852e-290
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5653355008114852e-290
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5653355008114852e-290
2020-04-06 04:59:44,581 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:07:33  iter: 980  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7026)  loss_objectness: 0.6931 (0.6935)  loss_rpn_box_reg: 0.1085 (0.1238)  time: 0.6023 (0.6141)  data: 0.0017 (0.0042)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2826677504057426e-290
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2826677504057426e-290
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2826677504057426e-290
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.413338752028713e-291
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.413338752028713e-291
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.413338752028713e-291
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2066693760143564e-291
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2066693760143564e-291
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2066693760143564e-291
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6033346880071782e-291
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6033346880071782e-291
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6033346880071782e-291
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.016673440035891e-292
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.016673440035891e-292
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.016673440035891e-292
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.008336720017946e-292
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.008336720017946e-292
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.008336720017946e-292
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.004168360008973e-292
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.004168360008973e-292
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.004168360008973e-292
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0020841800044864e-292
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0020841800044864e-292
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0020841800044864e-292
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.010420900022432e-293
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.010420900022432e-293
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.010420900022432e-293
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.505210450011216e-293
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.505210450011216e-293
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.505210450011216e-293
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.252605225005608e-293
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.252605225005608e-293
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.252605225005608e-293
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.26302612502804e-294
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.26302612502804e-294
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.26302612502804e-294
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.13151306251402e-294
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.13151306251402e-294
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.13151306251402e-294
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.56575653125701e-294
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.56575653125701e-294
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.56575653125701e-294
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.82878265628505e-295
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.82878265628505e-295
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.82878265628505e-295
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.914391328142525e-295
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.914391328142525e-295
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.914391328142525e-295
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9571956640712625e-295
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9571956640712625e-295
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9571956640712625e-295
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.785978320356312e-296
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.785978320356312e-296
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.785978320356312e-296
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.892989160178156e-296
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.892989160178156e-296
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.892989160178156e-296
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.446494580089078e-296
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.446494580089078e-296
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.446494580089078e-296
2020-04-06 04:59:56,593 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:06:38  iter: 1000  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7024)  loss_objectness: 0.6931 (0.6935)  loss_rpn_box_reg: 0.1098 (0.1237)  time: 0.6024 (0.6138)  data: 0.0016 (0.0041)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.223247290044539e-296
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.223247290044539e-296
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.223247290044539e-296
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.116236450222695e-297
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.116236450222695e-297
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.116236450222695e-297
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0581182251113476e-297
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0581182251113476e-297
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0581182251113476e-297
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5290591125556738e-297
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5290591125556738e-297
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5290591125556738e-297
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.645295562778369e-298
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.645295562778369e-298
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.645295562778369e-298
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8226477813891845e-298
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8226477813891845e-298
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8226477813891845e-298
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9113238906945923e-298
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9113238906945923e-298
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9113238906945923e-298
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.556619453472961e-299
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.556619453472961e-299
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.556619453472961e-299
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.778309726736481e-299
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.778309726736481e-299
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.778309726736481e-299
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3891548633682403e-299
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3891548633682403e-299
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3891548633682403e-299
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1945774316841202e-299
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1945774316841202e-299
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1945774316841202e-299
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.972887158420601e-300
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.972887158420601e-300
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.972887158420601e-300
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9864435792103004e-300
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9864435792103004e-300
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9864435792103004e-300
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4932217896051502e-300
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4932217896051502e-300
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4932217896051502e-300
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.466108948025751e-301
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.466108948025751e-301
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.466108948025751e-301
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7330544740128755e-301
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7330544740128755e-301
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7330544740128755e-301
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8665272370064378e-301
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8665272370064378e-301
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8665272370064378e-301
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.332636185032189e-302
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.332636185032189e-302
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.332636185032189e-302
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.6663180925160944e-302
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.6663180925160944e-302
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.6663180925160944e-302
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3331590462580472e-302
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3331590462580472e-302
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3331590462580472e-302
2020-04-06 05:00:08,588 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:05:42  iter: 1020  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7022)  loss_objectness: 0.6931 (0.6935)  loss_rpn_box_reg: 0.1335 (0.1240)  time: 0.6016 (0.6136)  data: 0.0017 (0.0041)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1665795231290236e-302
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1665795231290236e-302
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1665795231290236e-302
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.832897615645118e-303
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.832897615645118e-303
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.832897615645118e-303
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.916448807822559e-303
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.916448807822559e-303
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.916448807822559e-303
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4582244039112795e-303
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4582244039112795e-303
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4582244039112795e-303
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.291122019556398e-304
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.291122019556398e-304
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.291122019556398e-304
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.645561009778199e-304
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.645561009778199e-304
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.645561009778199e-304
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8227805048890994e-304
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8227805048890994e-304
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8227805048890994e-304
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.113902524445497e-305
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.113902524445497e-305
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.113902524445497e-305
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5569512622227484e-305
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5569512622227484e-305
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5569512622227484e-305
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2784756311113742e-305
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2784756311113742e-305
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2784756311113742e-305
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1392378155556871e-305
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1392378155556871e-305
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1392378155556871e-305
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.696189077778436e-306
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.696189077778436e-306
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.696189077778436e-306
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.848094538889218e-306
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.848094538889218e-306
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.848094538889218e-306
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.424047269444609e-306
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.424047269444609e-306
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.424047269444609e-306
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.120236347223045e-307
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.120236347223045e-307
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.120236347223045e-307
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5601181736115222e-307
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5601181736115222e-307
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5601181736115222e-307
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7800590868057611e-307
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7800590868057611e-307
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7800590868057611e-307
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.900295434028806e-308
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.900295434028806e-308
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.900295434028806e-308
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.450147717014403e-308
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.450147717014403e-308
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.450147717014403e-308
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2250738585072014e-308
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2250738585072014e-308
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2250738585072014e-308
2020-04-06 05:00:20,616 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:04:53  iter: 1040  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7021)  loss_objectness: 0.6931 (0.6935)  loss_rpn_box_reg: 0.1145 (0.1240)  time: 0.6017 (0.6133)  data: 0.0016 (0.0040)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1125369292536007e-308
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1125369292536007e-308
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1125369292536007e-308
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.562684646268003e-309
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.562684646268003e-309
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.562684646268003e-309
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.781342323134e-309
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.781342323134e-309
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.781342323134e-309
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.390671161567e-309
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.390671161567e-309
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.390671161567e-309
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.953355807835e-310
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.953355807835e-310
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.953355807835e-310
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4766779039175e-310
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4766779039175e-310
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4766779039175e-310
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.73833895195875e-310
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.73833895195875e-310
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.73833895195875e-310
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.691694759794e-311
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.691694759794e-311
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.691694759794e-311
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.345847379897e-311
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.345847379897e-311
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.345847379897e-311
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1729236899484e-311
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1729236899484e-311
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1729236899484e-311
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.086461844974e-311
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.086461844974e-311
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.086461844974e-311
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.43230922487e-312
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.43230922487e-312
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.43230922487e-312
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.716154612436e-312
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.716154612436e-312
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.716154612436e-312
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.35807730622e-312
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.35807730622e-312
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.35807730622e-312
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.7903865311e-313
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.7903865311e-313
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.7903865311e-313
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.39519326554e-313
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.39519326554e-313
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.39519326554e-313
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.69759663277e-313
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.69759663277e-313
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.69759663277e-313
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.487983164e-314
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.487983164e-314
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.487983164e-314
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.243991582e-314
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.243991582e-314
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.243991582e-314
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.121995791e-314
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.121995791e-314
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.121995791e-314
2020-04-06 05:00:32,569 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:03:53  iter: 1060  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7019)  loss_objectness: 0.6931 (0.6935)  loss_rpn_box_reg: 0.0939 (0.1236)  time: 0.5982 (0.6130)  data: 0.0016 (0.0040)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0609978955e-314
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0609978955e-314
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0609978955e-314
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.304989477e-315
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.304989477e-315
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.304989477e-315
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.65249474e-315
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.65249474e-315
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.65249474e-315
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.32624737e-315
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.32624737e-315
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.32624737e-315
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.63123685e-316
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.63123685e-316
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.63123685e-316
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3156184e-316
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3156184e-316
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3156184e-316
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6578092e-316
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6578092e-316
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6578092e-316
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.289046e-317
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.289046e-317
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.289046e-317
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.144523e-317
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.144523e-317
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.144523e-317
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0722615e-317
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0722615e-317
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0722615e-317
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.036131e-317
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.036131e-317
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.036131e-317
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.180654e-318
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.180654e-318
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.180654e-318
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.590327e-318
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.590327e-318
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.590327e-318
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.295163e-318
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.295163e-318
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.295163e-318
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.4758e-319
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.4758e-319
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.4758e-319
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2379e-319
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2379e-319
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2379e-319
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.61895e-319
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.61895e-319
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.61895e-319
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.095e-320
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.095e-320
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.095e-320
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0474e-320
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0474e-320
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0474e-320
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0237e-320
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0237e-320
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0237e-320
2020-04-06 05:00:44,731 maskrcnn_benchmark.trainer INFO: eta: 1 day, 3:03:27  iter: 1080  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 0.6931 (0.7017)  loss_objectness: 0.6931 (0.6935)  loss_rpn_box_reg: 0.0882 (0.1233)  time: 0.6008 (0.6129)  data: 0.0017 (0.0040)  lr: 0.030000  max mem: 23842
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.012e-320
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.012e-320
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.012e-320
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.06e-321
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.06e-321
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.06e-321
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.53e-321
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.53e-321
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.53e-321
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.265e-321
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.265e-321
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.265e-321
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.3e-322
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.3e-322
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.3e-322
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.16e-322
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.16e-322
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.16e-322
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6e-322
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6e-322
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6e-322
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8e-323
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8e-323
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8e-323
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4e-323
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4e-323
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4e-323
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2e-323
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2e-323
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2e-323
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1e-323
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1e-323
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1e-323
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5e-324
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5e-324
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5e-324
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0
Traceback (most recent call last):
  File "tools/train_mlperf.py", line 365, in <module>
Traceback (most recent call last):
  File "tools/train_mlperf.py", line 365, in <module>
    main()
  File "tools/train_mlperf.py", line 354, in main
    model, success = train(cfg, args.local_rank, args.distributed, random_number_generator)
  File "tools/train_mlperf.py", line 238, in train
    per_iter_end_callback_fn=per_iter_callback_fn,
  File "/workspace/object_detection/maskrcnn_benchmark/engine/trainer.py", line 128, in do_train
    main()
  File "tools/train_mlperf.py", line 354, in main
    scaled_losses.backward()
  File "/opt/conda/lib/python3.6/contextlib.py", line 88, in __exit__
    model, success = train(cfg, args.local_rank, args.distributed, random_number_generator)
  File "tools/train_mlperf.py", line 238, in train
    per_iter_end_callback_fn=per_iter_callback_fn,
  File "/workspace/object_detection/maskrcnn_benchmark/engine/trainer.py", line 128, in do_train
    next(self.gen)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/handle.py", line 113, in scale_loss
    scaled_losses.backward()
  File "/opt/conda/lib/python3.6/contextlib.py", line 88, in __exit__
    next(self.gen)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/handle.py", line 113, in scale_loss
    optimizer._post_amp_backward(loss_scaler)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/_process_optimizer.py", line 329, in post_backward_no_master_weights_FusedSGD
    optimizer._post_amp_backward(loss_scaler)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/_process_optimizer.py", line 329, in post_backward_no_master_weights_FusedSGD
    post_backward_no_master_weights(self, scaler)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/_process_optimizer.py", line 231, in post_backward_no_master_weights
    post_backward_no_master_weights(self, scaler)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/_process_optimizer.py", line 231, in post_backward_no_master_weights
    post_backward_models_are_masters(scaler, params, stashed_grads)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/_process_optimizer.py", line 118, in post_backward_models_are_masters
    post_backward_models_are_masters(scaler, params, stashed_grads)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/_process_optimizer.py", line 118, in post_backward_models_are_masters
    grads_needing_unscale_with_stash)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/scaler.py", line 167, in unscale_with_stashed
    grads_needing_unscale_with_stash)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/scaler.py", line 167, in unscale_with_stashed
    1./scale,
ZeroDivisionError: float division by zero
    1./scale,
ZeroDivisionError: float division by zero
Traceback (most recent call last):
  File "tools/train_mlperf.py", line 365, in <module>
    main()
  File "tools/train_mlperf.py", line 354, in main
    model, success = train(cfg, args.local_rank, args.distributed, random_number_generator)
  File "tools/train_mlperf.py", line 238, in train
    per_iter_end_callback_fn=per_iter_callback_fn,
  File "/workspace/object_detection/maskrcnn_benchmark/engine/trainer.py", line 128, in do_train
    scaled_losses.backward()
  File "/opt/conda/lib/python3.6/contextlib.py", line 88, in __exit__
    next(self.gen)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/handle.py", line 113, in scale_loss
    optimizer._post_amp_backward(loss_scaler)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/_process_optimizer.py", line 329, in post_backward_no_master_weights_FusedSGD
    post_backward_no_master_weights(self, scaler)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/_process_optimizer.py", line 231, in post_backward_no_master_weights
    post_backward_models_are_masters(scaler, params, stashed_grads)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/_process_optimizer.py", line 118, in post_backward_models_are_masters
    grads_needing_unscale_with_stash)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/scaler.py", line 167, in unscale_with_stashed
    1./scale,
ZeroDivisionError: float division by zero
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-04-06 05:01:03 AM
RESULT,OBJECT_DETECTION,,712,nvidia,2020-04-06 04:49:11 AM
