Beginning trial 3 of 3
Run vars: id 191021161658023210160
Gathering sys log on dss01
:::MLL 1571693129.618 submission_benchmark: {"value": "resnet", "metadata": {"lineno": 226, "file": "mlperf_log_utils.py"}}
:::MLL 1571693129.619 submission_org: {"value": "NVIDIA", "metadata": {"lineno": 231, "file": "mlperf_log_utils.py"}}
WARNING: Log validation: Key "submission_division" is not in known resnet keys.
:::MLL 1571693129.620 submission_division: {"value": "closed", "metadata": {"lineno": 235, "file": "mlperf_log_utils.py"}}
:::MLL 1571693129.620 submission_status: {"value": "onprem", "metadata": {"lineno": 239, "file": "mlperf_log_utils.py"}}
:::MLL 1571693129.621 submission_platform: {"value": "1xDSS8440", "metadata": {"lineno": 243, "file": "mlperf_log_utils.py"}}
:::MLL 1571693129.622 submission_entry: {"value": "{'power': 'N/A', 'nodes': \"{'cpu_accel_interconnect': 'UPI', 'num_accelerators': '8', 'accelerator': 'Tesla V100-PCIE-32GB', 'num_network_cards': '1', 'sys_mem_size': '754 GB', 'notes': '', 'num_vcpus': '40', 'network_card': 'Mellanox Technologies MT27700 Family [ConnectX-4]', 'sys_storage_type': 'SATA SSD', 'cpu': '2x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz', 'num_nodes': '1', 'num_cores': '40', 'sys_storage_size': '1x 447.1G + 1x 931.5G'}\", 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'hardware': 'DSS8440', 'notes': 'N/A', 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'framework': 'MXNet NVIDIA Release 19.06', 'libraries': \"{'cuda_driver_version': '418.67', 'dali_version': '0.10.0', 'cublas_version': '10.2.0.168', 'trt_version': '5.1.5.0', 'container_base': 'Ubuntu-16.04', 'nccl_version': '2.4.7', 'cuda_version': '10.1.168', 'cudnn_version': '7.6.0.64', 'mofed_version': '5.0-0', 'openmpi_version': '3.1.3'}\", 'os': '\\\\S / '}", "metadata": {"lineno": 247, "file": "mlperf_log_utils.py"}}
:::MLL 1571693129.622 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"lineno": 251, "file": "mlperf_log_utils.py"}}
:::MLL 1571693129.623 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"lineno": 255, "file": "mlperf_log_utils.py"}}
Clearing cache on dss01
:::MLL 1571693150.289 cache_clear: {"metadata": {"file": "<string>", "lineno": 1}, "value": true}

Launching user script on master node:
[1,0]<stdout>:STARTING TIMING RUN AT 2019-10-21 09:25:50 PM
[1,0]<stdout>:running benchmark
[1,1]<stdout>:STARTING TIMING RUN AT 2019-10-21 09:25:50 PM
[1,1]<stdout>:running benchmark
[1,2]<stdout>:STARTING TIMING RUN AT 2019-10-21 09:25:50 PM
[1,2]<stdout>:running benchmark
[1,4]<stdout>:STARTING TIMING RUN AT 2019-10-21 09:25:50 PM
[1,4]<stdout>:running benchmark
[1,3]<stdout>:STARTING TIMING RUN AT 2019-10-21 09:25:50 PM
[1,3]<stdout>:running benchmark
[1,5]<stdout>:STARTING TIMING RUN AT 2019-10-21 09:25:50 PM
[1,5]<stdout>:running benchmark
[1,6]<stdout>:STARTING TIMING RUN AT 2019-10-21 09:25:50 PM
[1,6]<stdout>:running benchmark
[1,7]<stdout>:STARTING TIMING RUN AT 2019-10-21 09:25:50 PM
[1,7]<stdout>:running benchmark
[1,7]<stderr>:[21:25:51] src/operator/nn/cudnn/cudnn_algoreg.cc:53: cuDNN library mismatch: linked-against version 7604 != compiled-against version 7600
[1,1]<stderr>:[21:25:51] src/operator/nn/cudnn/cudnn_algoreg.cc:53: cuDNN library mismatch: linked-against version 7604 != compiled-against version 7600
[1,2]<stderr>:[21:25:51] src/operator/nn/cudnn/cudnn_algoreg.cc:53: cuDNN library mismatch: linked-against version 7604 != compiled-against version 7600
[1,3]<stderr>:[21:25:51] src/operator/nn/cudnn/cudnn_algoreg.cc:53: cuDNN library mismatch: linked-against version 7604 != compiled-against version 7600
[1,4]<stderr>:[21:25:51] src/operator/nn/cudnn/cudnn_algoreg.cc:53: cuDNN library mismatch: linked-against version 7604 != compiled-against version 7600
[1,5]<stderr>:[21:25:51] src/operator/nn/cudnn/cudnn_algoreg.cc:53: cuDNN library mismatch: linked-against version 7604 != compiled-against version 7600
[1,6]<stderr>:[21:25:51] src/operator/nn/cudnn/cudnn_algoreg.cc:53: cuDNN library mismatch: linked-against version 7604 != compiled-against version 7600
[1,0]<stderr>:[21:25:51] src/operator/nn/cudnn/cudnn_algoreg.cc:53: cuDNN library mismatch: linked-against version 7604 != compiled-against version 7600
--------------------------------------------------------------------------
WARNING: One or more nonexistent OpenFabrics devices/ports were
specified:

  Host:                 dss01
  MCA parameter:        mca_btl_if_include
  Nonexistent entities: mlx5_2

These entities will be ignored.  You can disable this warning by
setting the btl_openib_warn_nonexistent_if MCA parameter to 0.
--------------------------------------------------------------------------
[1,0]<stdout>::::MLL 1571693159.730 init_start: {"value": null, "metadata": {"file": "train_imagenet.py", "lineno": 83}}
[1,1]<stdout>::::MLL 1571693159.730 init_start: {"metadata": {"lineno": 83, "file": "train_imagenet.py"}, "value": null}
[1,4]<stdout>::::MLL 1571693159.730 init_start: {"value": null, "metadata": {"lineno": 83, "file": "train_imagenet.py"}}
[1,2]<stdout>::::MLL 1571693159.730 init_start: {"value": null, "metadata": {"lineno": 83, "file": "train_imagenet.py"}}
[1,3]<stdout>::::MLL 1571693159.730 init_start: {"value": null, "metadata": {"lineno": 83, "file": "train_imagenet.py"}}
[1,5]<stdout>::::MLL 1571693159.730 init_start: {"value": null, "metadata": {"file": "train_imagenet.py", "lineno": 83}}
[1,6]<stdout>::::MLL 1571693159.730 init_start: {"metadata": {"lineno": 83, "file": "train_imagenet.py"}, "value": null}
[1,7]<stdout>::::MLL 1571693159.730 init_start: {"metadata": {"file": "train_imagenet.py", "lineno": 83}, "value": null}
[dss01:02143] 7 more processes have sent help message help-mpi-btl-openib.txt / nonexistent port
[dss01:02143] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[1,7]<stdout>:WARNING: `nvJPEGDecoderRandomCrop` is now deprecated. Use `ImageDecoderRandomCrop` instead
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_initial_shape" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693172.564 model_hp_initial_shape: {"value": [4, 224, 224], "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 266}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693172.566 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693172.567 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693172.568 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693172.569 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693172.570 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693172.571 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693172.572 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693172.573 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693172.574 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693172.575 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693172.576 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693172.577 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693172.578 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693172.579 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693172.580 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693172.582 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_final_shape" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693172.583 model_hp_final_shape: {"value": 1000, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 309}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_loss_fn" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693172.583 model_hp_loss_fn: {"value": "categorical_cross_entropy", "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 320}}
[1,0]<stdout>::::MLL 1571693172.586 model_bn_span: {"value": 272, "metadata": {"file": "common/dali.py", "lineno": 229}}
[1,4]<stdout>:WARNING: `nvJPEGDecoderRandomCrop` is now deprecated. Use `ImageDecoderRandomCrop` instead
[1,1]<stdout>:WARNING: `nvJPEGDecoderRandomCrop` is now deprecated. Use `ImageDecoderRandomCrop` instead
[1,2]<stdout>:WARNING: `nvJPEGDecoderRandomCrop` is now deprecated. Use `ImageDecoderRandomCrop` instead
[1,6]<stdout>:WARNING: `nvJPEGDecoderRandomCrop` is now deprecated. Use `ImageDecoderRandomCrop` instead
[1,3]<stdout>:WARNING: `nvJPEGDecoderRandomCrop` is now deprecated. Use `ImageDecoderRandomCrop` instead
[1,0]<stdout>:WARNING: `nvJPEGDecoderRandomCrop` is now deprecated. Use `ImageDecoderRandomCrop` instead
[1,5]<stdout>:WARNING: `nvJPEGDecoderRandomCrop` is now deprecated. Use `ImageDecoderRandomCrop` instead
[1,7]<stdout>:WARNING: `nvJPEGDecoder` is now deprecated. Use `ImageDecoder` instead
[1,4]<stdout>:WARNING: `nvJPEGDecoder` is now deprecated. Use `ImageDecoder` instead
[1,1]<stdout>:WARNING: `nvJPEGDecoder` is now deprecated. Use `ImageDecoder` instead
[1,2]<stdout>:WARNING: `nvJPEGDecoder` is now deprecated. Use `ImageDecoder` instead
[1,6]<stdout>:WARNING: `nvJPEGDecoder` is now deprecated. Use `ImageDecoder` instead
[1,3]<stdout>:WARNING: `nvJPEGDecoder` is now deprecated. Use `ImageDecoder` instead
[1,0]<stdout>:WARNING: `nvJPEGDecoder` is now deprecated. Use `ImageDecoder` instead
[1,5]<stdout>:WARNING: `nvJPEGDecoder` is now deprecated. Use `ImageDecoder` instead
[1,0]<stdout>::::MLL 1571693181.574 init_stop: {"value": null, "metadata": {"file": "train_imagenet.py", "lineno": 187}}
[1,0]<stdout>::::MLL 1571693181.574 run_start: {"value": null, "metadata": {"file": "train_imagenet.py", "lineno": 190}}
[1,0]<stderr>:2019-10-21 21:26:21,574 Node[0] start with arguments Namespace(accuracy_threshold=0.759, batch_size=272, batchnorm_eps=1e-05, batchnorm_layout='NHWC', batchnorm_mom=0.9, bn_gamma_init0=False, conv_algo=1, conv_layout='NHWC', custom_bn_off=0, dali_cache_size=0, dali_nvjpeg_memory_padding=256, dali_prefetch_queue=8, dali_roi_decode=True, dali_threads=3, data_train='/data/train.rec', data_train_idx='/data/train.idx', data_val='/data/val.rec', data_val_idx='/data/val.idx', disp_batches=20, dtype='float16', epoch_size=0, eval_offset=3, eval_period=4, force_tensor_core=1, fuse_bn_add_relu=1, fuse_bn_relu=1, gpus='0', image_shape='4,224,224', initializer='default', input_layout='NHWC', kv_store='horovod', label_smoothing=0.1, lars_eps=0, lars_eta=0.001, lazy_init_sanity=False, log='', logging_dir='logs', lr=6.0, lr_factor=0.1, lr_step_epochs='pow2', max_random_area=1.0, max_random_aspect_ratio=1.3333333333333333, min_random_area=0.05, min_random_aspect_ratio=0.75, model_prefix=None, mom=0.9, network='resnet-v1b-normconv-fl', num_classes=1000, num_epochs=72, num_examples=1281167, num_layers=50, optimizer='sgdwfastlars', pooling_layout='NHWC', profile_server_suffix='', profile_worker_suffix='', resize=256, save_period=1, seed=6226, separ_val=False, top_k=0, use_dali=True, verbose=0, warmup_epochs=5, warmup_strategy='linear', wd=0.0002)
[1,6]<stderr>:2019-10-21 21:26:21,574 Node[6] start with arguments Namespace(accuracy_threshold=0.759, batch_size=272, batchnorm_eps=1e-05, batchnorm_layout='NHWC', batchnorm_mom=0.9, bn_gamma_init0=False, conv_algo=1, conv_layout='NHWC', custom_bn_off=0, dali_cache_size=0, dali_nvjpeg_memory_padding=256, dali_prefetch_queue=8, dali_roi_decode=True, dali_threads=3, data_train='/data/train.rec', data_train_idx='/data/train.idx', data_val='/data/val.rec', data_val_idx='/data/val.idx', disp_batches=20, dtype='float16', epoch_size=0, eval_offset=3, eval_period=4, force_tensor_core=1, fuse_bn_add_relu=1, fuse_bn_relu=1, gpus='6', image_shape='4,224,224', initializer='default', input_layout='NHWC', kv_store='horovod', label_smoothing=0.1, lars_eps=0, lars_eta=0.001, lazy_init_sanity=False, log='', logging_dir='logs', lr=6.0, lr_factor=0.1, lr_step_epochs='pow2', max_random_area=1.0, max_random_aspect_ratio=1.3333333333333333, min_random_area=0.05, min_random_aspect_ratio=0.75, model_prefix=None, mom=0.9, network='resnet-v1b-normconv-fl', num_classes=1000, num_epochs=72, num_examples=1281167, num_layers=50, optimizer='sgdwfastlars', pooling_layout='NHWC', profile_server_suffix='', profile_worker_suffix='', resize=256, save_period=1, seed=59173, separ_val=False, top_k=0, use_dali=True, verbose=0, warmup_epochs=5, warmup_strategy='linear', wd=0.0002)
[1,7]<stderr>:2019-10-21 21:26:21,574 Node[7] start with arguments Namespace(accuracy_threshold=0.759, batch_size=272, batchnorm_eps=1e-05, batchnorm_layout='NHWC', batchnorm_mom=0.9, bn_gamma_init0=False, conv_algo=1, conv_layout='NHWC', custom_bn_off=0, dali_cache_size=0, dali_nvjpeg_memory_padding=256, dali_prefetch_queue=8, dali_roi_decode=True, dali_threads=3, data_train='/data/train.rec', data_train_idx='/data/train.idx', data_val='/data/val.rec', data_val_idx='/data/val.idx', disp_batches=20, dtype='float16', epoch_size=0, eval_offset=3, eval_period=4, force_tensor_core=1, fuse_bn_add_relu=1, fuse_bn_relu=1, gpus='7', image_shape='4,224,224', initializer='default', input_layout='NHWC', kv_store='horovod', label_smoothing=0.1, lars_eps=0, lars_eta=0.001, lazy_init_sanity=False, log='', logging_dir='logs', lr=6.0, lr_factor=0.1, lr_step_epochs='pow2', max_random_area=1.0, max_random_aspect_ratio=1.3333333333333333, min_random_area=0.05, min_random_aspect_ratio=0.75, model_prefix=None, mom=0.9, network='resnet-v1b-normconv-fl', num_classes=1000, num_epochs=72, num_examples=1281167, num_layers=50, optimizer='sgdwfastlars', pooling_layout='NHWC', profile_server_suffix='', profile_worker_suffix='', resize=256, save_period=1, seed=6035, separ_val=False, top_k=0, use_dali=True, verbose=0, warmup_epochs=5, warmup_strategy='linear', wd=0.0002)
[1,1]<stderr>:2019-10-21 21:26:21,574 Node[1] start with arguments Namespace(accuracy_threshold=0.759, batch_size=272, batchnorm_eps=1e-05, batchnorm_layout='NHWC', batchnorm_mom=0.9, bn_gamma_init0=False, conv_algo=1, conv_layout='NHWC', custom_bn_off=0, dali_cache_size=0, dali_nvjpeg_memory_padding=256, dali_prefetch_queue=8, dali_roi_decode=True, dali_threads=3, data_train='/data/train.rec', data_train_idx='/data/train.idx', data_val='/data/val.rec', data_val_idx='/data/val.idx', disp_batches=20, dtype='float16', epoch_size=0, eval_offset=3, eval_period=4, force_tensor_core=1, fuse_bn_add_relu=1, fuse_bn_relu=1, gpus='1', image_shape='4,224,224', initializer='default', input_layout='NHWC', kv_store='horovod', label_smoothing=0.1, lars_eps=0, lars_eta=0.001, lazy_init_sanity=False, log='', logging_dir='logs', lr=6.0, lr_factor=0.1, lr_step_epochs='pow2', max_random_area=1.0, max_random_aspect_ratio=1.3333333333333333, min_random_area=0.05, min_random_aspect_ratio=0.75, model_prefix=None, mom=0.9, network='resnet-v1b-normconv-fl', num_classes=1000, num_epochs=72, num_examples=1281167, num_layers=50, optimizer='sgdwfastlars', pooling_layout='NHWC', profile_server_suffix='', profile_worker_suffix='', resize=256, save_period=1, seed=5281, separ_val=False, top_k=0, use_dali=True, verbose=0, warmup_epochs=5, warmup_strategy='linear', wd=0.0002)
[1,2]<stderr>:2019-10-21 21:26:21,574 Node[2] start with arguments Namespace(accuracy_threshold=0.759, batch_size=272, batchnorm_eps=1e-05, batchnorm_layout='NHWC', batchnorm_mom=0.9, bn_gamma_init0=False, conv_algo=1, conv_layout='NHWC', custom_bn_off=0, dali_cache_size=0, dali_nvjpeg_memory_padding=256, dali_prefetch_queue=8, dali_roi_decode=True, dali_threads=3, data_train='/data/train.rec', data_train_idx='/data/train.idx', data_val='/data/val.rec', data_val_idx='/data/val.idx', disp_batches=20, dtype='float16', epoch_size=0, eval_offset=3, eval_period=4, force_tensor_core=1, fuse_bn_add_relu=1, fuse_bn_relu=1, gpus='2', image_shape='4,224,224', initializer='default', input_layout='NHWC', kv_store='horovod', label_smoothing=0.1, lars_eps=0, lars_eta=0.001, lazy_init_sanity=False, log='', logging_dir='logs', lr=6.0, lr_factor=0.1, lr_step_epochs='pow2', max_random_area=1.0, max_random_aspect_ratio=1.3333333333333333, min_random_area=0.05, min_random_aspect_ratio=0.75, model_prefix=None, mom=0.9, network='resnet-v1b-normconv-fl', num_classes=1000, num_epochs=72, num_examples=1281167, num_layers=50, optimizer='sgdwfastlars', pooling_layout='NHWC', profile_server_suffix='', profile_worker_suffix='', resize=256, save_period=1, seed=25488, separ_val=False, top_k=0, use_dali=True, verbose=0, warmup_epochs=5, warmup_strategy='linear', wd=0.0002)
[1,3]<stderr>:2019-10-21 21:26:21,574 Node[3] start with arguments Namespace(accuracy_threshold=0.759, batch_size=272, batchnorm_eps=1e-05, batchnorm_layout='NHWC', batchnorm_mom=0.9, bn_gamma_init0=False, conv_algo=1, conv_layout='NHWC', custom_bn_off=0, dali_cache_size=0, dali_nvjpeg_memory_padding=256, dali_prefetch_queue=8, dali_roi_decode=True, dali_threads=3, data_train='/data/train.rec', data_train_idx='/data/train.idx', data_val='/data/val.rec', data_val_idx='/data/val.idx', disp_batches=20, dtype='float16', epoch_size=0, eval_offset=3, eval_period=4, force_tensor_core=1, fuse_bn_add_relu=1, fuse_bn_relu=1, gpus='3', image_shape='4,224,224', initializer='default', input_layout='NHWC', kv_store='horovod', label_smoothing=0.1, lars_eps=0, lars_eta=0.001, lazy_init_sanity=False, log='', logging_dir='logs', lr=6.0, lr_factor=0.1, lr_step_epochs='pow2', max_random_area=1.0, max_random_aspect_ratio=1.3333333333333333, min_random_area=0.05, min_random_aspect_ratio=0.75, model_prefix=None, mom=0.9, network='resnet-v1b-normconv-fl', num_classes=1000, num_epochs=72, num_examples=1281167, num_layers=50, optimizer='sgdwfastlars', pooling_layout='NHWC', profile_server_suffix='', profile_worker_suffix='', resize=256, save_period=1, seed=22527, separ_val=False, top_k=0, use_dali=True, verbose=0, warmup_epochs=5, warmup_strategy='linear', wd=0.0002)
[1,4]<stderr>:2019-10-21 21:26:21,574 Node[4] start with arguments Namespace(accuracy_threshold=0.759, batch_size=272, batchnorm_eps=1e-05, batchnorm_layout='NHWC', batchnorm_mom=0.9, bn_gamma_init0=False, conv_algo=1, conv_layout='NHWC', custom_bn_off=0, dali_cache_size=0, dali_nvjpeg_memory_padding=256, dali_prefetch_queue=8, dali_roi_decode=True, dali_threads=3, data_train='/data/train.rec', data_train_idx='/data/train.idx', data_val='/data/val.rec', data_val_idx='/data/val.idx', disp_batches=20, dtype='float16', epoch_size=0, eval_offset=3, eval_period=4, force_tensor_core=1, fuse_bn_add_relu=1, fuse_bn_relu=1, gpus='4', image_shape='4,224,224', initializer='default', input_layout='NHWC', kv_store='horovod', label_smoothing=0.1, lars_eps=0, lars_eta=0.001, lazy_init_sanity=False, log='', logging_dir='logs', lr=6.0, lr_factor=0.1, lr_step_epochs='pow2', max_random_area=1.0, max_random_aspect_ratio=1.3333333333333333, min_random_area=0.05, min_random_aspect_ratio=0.75, model_prefix=None, mom=0.9, network='resnet-v1b-normconv-fl', num_classes=1000, num_epochs=72, num_examples=1281167, num_layers=50, optimizer='sgdwfastlars', pooling_layout='NHWC', profile_server_suffix='', profile_worker_suffix='', resize=256, save_period=1, seed=64941, separ_val=False, top_k=0, use_dali=True, verbose=0, warmup_epochs=5, warmup_strategy='linear', wd=0.0002)
[1,5]<stderr>:2019-10-21 21:26:21,574 Node[5] start with arguments Namespace(accuracy_threshold=0.759, batch_size=272, batchnorm_eps=1e-05, batchnorm_layout='NHWC', batchnorm_mom=0.9, bn_gamma_init0=False, conv_algo=1, conv_layout='NHWC', custom_bn_off=0, dali_cache_size=0, dali_nvjpeg_memory_padding=256, dali_prefetch_queue=8, dali_roi_decode=True, dali_threads=3, data_train='/data/train.rec', data_train_idx='/data/train.idx', data_val='/data/val.rec', data_val_idx='/data/val.idx', disp_batches=20, dtype='float16', epoch_size=0, eval_offset=3, eval_period=4, force_tensor_core=1, fuse_bn_add_relu=1, fuse_bn_relu=1, gpus='5', image_shape='4,224,224', initializer='default', input_layout='NHWC', kv_store='horovod', label_smoothing=0.1, lars_eps=0, lars_eta=0.001, lazy_init_sanity=False, log='', logging_dir='logs', lr=6.0, lr_factor=0.1, lr_step_epochs='pow2', max_random_area=1.0, max_random_aspect_ratio=1.3333333333333333, min_random_area=0.05, min_random_aspect_ratio=0.75, model_prefix=None, mom=0.9, network='resnet-v1b-normconv-fl', num_classes=1000, num_epochs=72, num_examples=1281167, num_layers=50, optimizer='sgdwfastlars', pooling_layout='NHWC', profile_server_suffix='', profile_worker_suffix='', resize=256, save_period=1, seed=55622, separ_val=False, top_k=0, use_dali=True, verbose=0, warmup_epochs=5, warmup_strategy='linear', wd=0.0002)
[1,4]<stderr>:2019-10-21 21:26:24,148 Node[4] Already bound, ignoring bind()
[1,7]<stderr>:2019-10-21 21:26:24,301 Node[7] Already bound, ignoring bind()
[1,6]<stderr>:2019-10-21 21:26:24,367 Node[6] Already bound, ignoring bind()
[1,1]<stderr>:2019-10-21 21:26:24,456 Node[1] Already bound, ignoring bind()
[1,5]<stderr>:2019-10-21 21:26:24,537 Node[5] Already bound, ignoring bind()
[1,0]<stdout>::::MLL 1571693184.543 opt_base_learning_rate: {"value": 6.0, "metadata": {"file": "common/fit.py", "lineno": 651}}
[1,0]<stdout>::::MLL 1571693184.543 opt_learning_rate_warmup_epochs: {"value": 5, "metadata": {"file": "common/fit.py", "lineno": 652}}
[1,0]<stdout>::::MLL 1571693184.543 lars_opt_learning_rate_decay_steps: {"value": 39463, "metadata": {"file": "common/fit.py", "lineno": 662}}
[1,0]<stdout>::::MLL 1571693184.544 lars_opt_learning_rate_decay_poly_power: {"value": 2, "metadata": {"file": "common/fit.py", "lineno": 698}}
[1,0]<stdout>::::MLL 1571693184.544 lars_opt_end_learning_rate: {"value": 0.0001, "metadata": {"file": "common/fit.py", "lineno": 699}}
[1,0]<stdout>::::MLL 1571693184.544 opt_name: {"value": "lars", "metadata": {"file": "common/fit.py", "lineno": 1075}}
[1,0]<stdout>::::MLL 1571693184.545 lars_epsilon: {"value": 0, "metadata": {"file": "common/fit.py", "lineno": 1077}}
[1,0]<stdout>:WARNING: Log validation: Key "lars_opt_weight_decay" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571693184.545 lars_opt_weight_decay: {"value": 0.0002, "metadata": {"file": "common/fit.py", "lineno": 1079}}
[1,0]<stdout>:using wd on conv0_weight
[1,0]<stdout>:skipping wd on bn0_gamma
[1,0]<stdout>:skipping wd on bn0_beta
[1,0]<stdout>:using wd on stage1_unit1_conv1_weight
[1,0]<stdout>:skipping wd on stage1_unit1_bn1_gamma
[1,0]<stdout>:skipping wd on stage1_unit1_bn1_beta
[1,0]<stdout>:using wd on stage1_unit1_conv2_weight
[1,0]<stdout>:skipping wd on stage1_unit1_bn2_gamma
[1,0]<stdout>:skipping wd on stage1_unit1_bn2_beta
[1,0]<stdout>:using wd on stage1_unit1_conv3_weight
[1,0]<stdout>:skipping wd on stage1_unit1_bn3_gamma
[1,0]<stdout>:skipping wd on stage1_unit1_bn3_beta
[1,0]<stdout>:using wd on stage1_unit1_conv1sc_weight
[1,0]<stdout>:skipping wd on stage1_unit1_bn_sc_gamma
[1,0]<stdout>:skipping wd on stage1_unit1_bn_sc_beta
[1,0]<stdout>:using wd on stage1_unit2_conv1_weight
[1,0]<stdout>:skipping wd on stage1_unit2_bn1_gamma
[1,0]<stdout>:skipping wd on stage1_unit2_bn1_beta
[1,0]<stdout>:using wd on stage1_unit2_conv2_weight
[1,0]<stdout>:skipping wd on stage1_unit2_bn2_gamma
[1,0]<stdout>:skipping wd on stage1_unit2_bn2_beta
[1,0]<stdout>:using wd on stage1_unit2_conv3_weight
[1,0]<stdout>:skipping wd on stage1_unit2_bn3_gamma
[1,0]<stdout>:skipping wd on stage1_unit2_bn3_beta
[1,0]<stdout>:using wd on stage1_unit3_conv1_weight
[1,0]<stdout>:skipping wd on stage1_unit3_bn1_gamma
[1,0]<stdout>:skipping wd on stage1_unit3_bn1_beta
[1,0]<stdout>:using wd on stage1_unit3_conv2_weight
[1,0]<stdout>:skipping wd on stage1_unit3_bn2_gamma
[1,0]<stdout>:skipping wd on stage1_unit3_bn2_beta
[1,0]<stdout>:using wd on stage1_unit3_conv3_weight
[1,0]<stdout>:skipping wd on stage1_unit3_bn3_gamma
[1,0]<stdout>:skipping wd on stage1_unit3_bn3_beta
[1,0]<stdout>:using wd on stage2_unit1_conv1_weight
[1,0]<stdout>:skipping wd on stage2_unit1_bn1_gamma
[1,0]<stdout>:skipping wd on stage2_unit1_bn1_beta
[1,0]<stdout>:using wd on stage2_unit1_conv2_weight
[1,0]<stdout>:skipping wd on stage2_unit1_bn2_gamma
[1,0]<stdout>:skipping wd on stage2_unit1_bn2_beta
[1,0]<stdout>:using wd on stage2_unit1_conv3_weight
[1,0]<stdout>:skipping wd on stage2_unit1_bn3_gamma
[1,0]<stdout>:skipping wd on stage2_unit1_bn3_beta
[1,0]<stdout>:using wd on stage2_unit1_conv1sc_weight
[1,0]<stdout>:skipping wd on stage2_unit1_bn_sc_gamma
[1,0]<stdout>:skipping wd on stage2_unit1_bn_sc_beta
[1,0]<stdout>:using wd on stage2_unit2_conv1_weight
[1,0]<stdout>:skipping wd on stage2_unit2_bn1_gamma
[1,0]<stdout>:skipping wd on stage2_unit2_bn1_beta
[1,0]<stdout>:using wd on stage2_unit2_conv2_weight
[1,0]<stdout>:skipping wd on stage2_unit2_bn2_gamma
[1,0]<stdout>:skipping wd on stage2_unit2_bn2_beta
[1,0]<stdout>:using wd on stage2_unit2_conv3_weight
[1,0]<stdout>:skipping wd on stage2_unit2_bn3_gamma
[1,0]<stdout>:skipping wd on stage2_unit2_bn3_beta
[1,0]<stdout>:using wd on stage2_unit3_conv1_weight
[1,0]<stdout>:skipping wd on stage2_unit3_bn1_gamma
[1,0]<stdout>:skipping wd on stage2_unit3_bn1_beta
[1,0]<stdout>:using wd on stage2_unit3_conv2_weight
[1,0]<stdout>:skipping wd on stage2_unit3_bn2_gamma
[1,0]<stdout>:skipping wd on stage2_unit3_bn2_beta
[1,0]<stdout>:using wd on stage2_unit3_conv3_weight
[1,0]<stdout>:skipping wd on stage2_unit3_bn3_gamma
[1,0]<stdout>:skipping wd on stage2_unit3_bn3_beta
[1,0]<stdout>:using wd on stage2_unit4_conv1_weight
[1,0]<stdout>:skipping wd on stage2_unit4_bn1_gamma
[1,0]<stdout>:skipping wd on stage2_unit4_bn1_beta
[1,0]<stdout>:using wd on stage2_unit4_conv2_weight
[1,0]<stdout>:skipping wd on stage2_unit4_bn2_gamma
[1,0]<stdout>:skipping wd on stage2_unit4_bn2_beta
[1,0]<stdout>:using wd on stage2_unit4_conv3_weight
[1,0]<stdout>:skipping wd on stage2_unit4_bn3_gamma
[1,0]<stdout>:skipping wd on stage2_unit4_bn3_beta
[1,0]<stdout>:using wd on stage3_unit1_conv1_weight
[1,0]<stdout>:skipping wd on stage3_unit1_bn1_gamma
[1,0]<stdout>:skipping wd on stage3_unit1_bn1_beta
[1,0]<stdout>:using wd on stage3_unit1_conv2_weight
[1,0]<stdout>:skipping wd on stage3_unit1_bn2_gamma
[1,0]<stdout>:skipping wd on stage3_unit1_bn2_beta
[1,0]<stdout>:using wd on stage3_unit1_conv3_weight
[1,0]<stdout>:skipping wd on stage3_unit1_bn3_gamma
[1,0]<stdout>:skipping wd on stage3_unit1_bn3_beta
[1,0]<stdout>:using wd on stage3_unit1_conv1sc_weight
[1,0]<stdout>:skipping wd on stage3_unit1_bn_sc_gamma
[1,0]<stdout>:skipping wd on stage3_unit1_bn_sc_beta
[1,0]<stdout>:using wd on stage3_unit2_conv1_weight
[1,0]<stdout>:skipping wd on stage3_unit2_bn1_gamma
[1,0]<stdout>:skipping wd on stage3_unit2_bn1_beta
[1,0]<stdout>:using wd on stage3_unit2_conv2_weight
[1,0]<stdout>:skipping wd on stage3_unit2_bn2_gamma
[1,0]<stdout>:skipping wd on stage3_unit2_bn2_beta
[1,0]<stdout>:using wd on stage3_unit2_conv3_weight
[1,0]<stdout>:skipping wd on stage3_unit2_bn3_gamma
[1,0]<stdout>:skipping wd on stage3_unit2_bn3_beta
[1,0]<stdout>:using wd on stage3_unit3_conv1_weight
[1,0]<stdout>:skipping wd on stage3_unit3_bn1_gamma
[1,0]<stdout>:skipping wd on stage3_unit3_bn1_beta
[1,0]<stdout>:using wd on stage3_unit3_conv2_weight
[1,0]<stdout>:skipping wd on stage3_unit3_bn2_gamma
[1,0]<stdout>:skipping wd on [1,0]<stdout>:stage3_unit3_bn2_beta
[1,0]<stdout>:using wd on stage3_unit3_conv3_weight
[1,0]<stdout>:skipping wd on stage3_unit3_bn3_gamma
[1,0]<stdout>:skipping wd on stage3_unit3_bn3_beta
[1,0]<stdout>:using wd on stage3_unit4_conv1_weight
[1,0]<stdout>:skipping wd on stage3_unit4_bn1_gamma
[1,0]<stdout>:skipping wd on stage3_unit4_bn1_beta
[1,0]<stdout>:using wd on stage3_unit4_conv2_weight
[1,0]<stdout>:skipping wd on stage3_unit4_bn2_gamma
[1,0]<stdout>:skipping wd on stage3_unit4_bn2_beta
[1,0]<stdout>:using wd on stage3_unit4_conv3_weight
[1,0]<stdout>:skipping wd on stage3_unit4_bn3_gamma
[1,0]<stdout>:skipping wd on stage3_unit4_bn3_beta
[1,0]<stdout>:using wd on stage3_unit5_conv1_weight
[1,0]<stdout>:skipping wd on stage3_unit5_bn1_gamma
[1,0]<stdout>:skipping wd on stage3_unit5_bn1_beta
[1,0]<stdout>:using wd on stage3_unit5_conv2_weight
[1,0]<stdout>:skipping wd on stage3_unit5_bn2_gamma
[1,0]<stdout>:skipping wd on stage3_unit5_bn2_beta
[1,0]<stdout>:using wd on stage3_unit5_conv3_weight
[1,0]<stdout>:skipping wd on stage3_unit5_bn3_gamma
[1,0]<stdout>:skipping wd on stage3_unit5_bn3_beta
[1,0]<stdout>:using wd on stage3_unit6_conv1_weight
[1,0]<stdout>:skipping wd on stage3_unit6_bn1_gamma
[1,0]<stdout>:skipping wd on stage3_unit6_bn1_beta
[1,0]<stdout>:using wd on stage3_unit6_conv2_weight
[1,0]<stdout>:skipping wd on stage3_unit6_bn2_gamma
[1,0]<stdout>:skipping wd on stage3_unit6_bn2_beta
[1,0]<stdout>:using wd on stage3_unit6_conv3_weight
[1,0]<stdout>:skipping wd on stage3_unit6_bn3_gamma
[1,0]<stdout>:skipping wd on stage3_unit6_bn3_beta
[1,0]<stdout>:using wd on stage4_unit1_conv1_weight
[1,0]<stdout>:skipping wd on stage4_unit1_bn1_gamma
[1,0]<stdout>:skipping wd on stage4_unit1_bn1_beta
[1,0]<stdout>:using wd on stage4_unit1_conv2_weight
[1,0]<stdout>:skipping wd on stage4_unit1_bn2_gamma
[1,0]<stdout>:skipping wd on stage4_unit1_bn2_beta
[1,0]<stdout>:using wd on stage4_unit1_conv3_weight
[1,0]<stdout>:skipping wd on stage4_unit1_bn3_gamma
[1,0]<stdout>:skipping wd on stage4_unit1_bn3_beta
[1,0]<stdout>:using wd on stage4_unit1_conv1sc_weight
[1,0]<stdout>:skipping wd on stage4_unit1_bn_sc_gamma
[1,0]<stdout>:skipping wd on stage4_unit1_bn_sc_beta
[1,0]<stdout>:using wd on stage4_unit2_conv1_weight
[1,0]<stdout>:skipping wd on stage4_unit2_bn1_gamma
[1,0]<stdout>:skipping wd on stage4_unit2_bn1_beta
[1,0]<stdout>:using wd on stage4_unit2_conv2_weight
[1,0]<stdout>:skipping wd on stage4_unit2_bn2_gamma
[1,0]<stdout>:skipping wd on stage4_unit2_bn2_beta
[1,0]<stdout>:using wd on stage4_unit2_conv3_weight
[1,0]<stdout>:skipping wd on stage4_unit2_bn3_gamma
[1,0]<stdout>:skipping wd on stage4_unit2_bn3_beta
[1,0]<stdout>:using wd on stage4_unit3_conv1_weight
[1,0]<stdout>:skipping wd on stage4_unit3_bn1_gamma
[1,0]<stdout>:skipping wd on stage4_unit3_bn1_beta
[1,0]<stdout>:using wd on stage4_unit3_conv2_weight
[1,0]<stdout>:skipping wd on stage4_unit3_bn2_gamma
[1,0]<stdout>:skipping wd on stage4_unit3_bn2_beta
[1,0]<stdout>:using wd on stage4_unit3_conv3_weight
[1,0]<stdout>:skipping wd on stage4_unit3_bn3_gamma
[1,0]<stdout>:skipping wd on stage4_unit3_bn3_beta
[1,0]<stdout>:using wd on fc1_weight
[1,0]<stdout>:using wd on fc1_bias
[1,0]<stdout>::::MLL 1571693184.546 global_batch_size: {"value": 2176, "metadata": {"file": "common/fit.py", "lineno": 1112}}
[1,0]<stderr>:2019-10-21 21:26:24,546 Node[0] Already bound, ignoring bind()
[1,0]<stdout>::::MLL 1571693184.547 block_start: {"value": null, "metadata": {"file": "common/fit.py", "first_epoch_num": 1, "lineno": 888, "epoch_count": 4}}
[1,0]<stdout>::::MLL 1571693184.547 epoch_start: {"value": null, "metadata": {"file": "common/fit.py", "epoch_num": 1, "lineno": 893}}
[1,2]<stderr>:2019-10-21 21:26:24,582 Node[2] Already bound, ignoring bind()
[1,3]<stderr>:2019-10-21 21:26:24,671 Node[3] Already bound, ignoring bind()
[1,4]<stderr>:/workspace/image_classification/common/fit.py:858: UserWarning: Parameters already initialized and force_init=False. init_params call ignored.
[1,4]<stderr>:  allow_missing=allow_missing, force_init=force_init)
[1,4]<stderr>:/workspace/image_classification/common/fit.py:860: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.00045955882352941176 vs. 0.003676470588235294). Is this intended?
[1,4]<stderr>:  optimizer_params=optimizer_params)
[1,4]<stderr>:Traceback (most recent call last):
[1,4]<stderr>:  File "train_imagenet.py", line 191, in <module>
[1,4]<stderr>:    fit.fit(args, kv, model, initializer, lambda_fnc_dali_get_rec_iter, devs, arg_params, aux_params)
[1,4]<stderr>:  File "/workspace/image_classification/common/fit.py", line 1140, in fit
[1,4]<stderr>:    monitor=None)
[1,4]<stderr>:  File "/workspace/image_classification/common/fit.py", line 910, in mlperf_fit
[1,4]<stderr>:    pre_sliced=True)
[1,4]<stderr>:  File "/opt/mxnet/python/mxnet/module/module.py", line 777, in update_metric
[1,4]<stderr>:    self._exec_group.update_metric(eval_metric, labels, pre_sliced, label_pads)
[1,4]<stderr>:  File "/opt/mxnet/python/mxnet/module/executor_group.py", line 664, in update_metric
[1,4]<stderr>:    eval_metric.update_dict(labels_, preds)
[1,4]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 350, in update_dict
[1,4]<stderr>:    metric.update_dict(labels, preds)
[1,4]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 133, in update_dict
[1,4]<stderr>:    self.update(label, pred)
[1,4]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 496, in update
[1,4]<stderr>:    pred_label = pred_label.asnumpy().astype('int32')
[1,4]<stderr>:  File "/opt/mxnet/python/mxnet/ndarray/ndarray.py", line 1996, in asnumpy
[1,4]<stderr>:    ctypes.c_size_t(data.size)))
[1,4]<stderr>:  File "/opt/mxnet/python/mxnet/base.py", line 252, in check_call
[1,4]<stderr>:    raise MXNetError(py_str(_LIB.MXGetLastError()))
[1,4]<stderr>:mxnet.base.MXNetError: [21:26:24] src/operator/nn/./cudnn/cudnn_convolution-inl.h:1024: Failed to get backprop-to-data convolution algorithm 1 with workspace size of 536870912 bytes, please consider reducing batch/model size or increasing the workspace size
[1,4]<stderr>:Stack trace:
[1,4]<stderr>:  [bt] (0) /usr/local/lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x32) [0x7fffbdeb05d2]
[1,4]<stderr>:  [bt] (1) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::CuDNNAlgoSetter(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)+0x1a83) [0x7fffc103f013]
[1,4]<stderr>:  [bt] (2) /usr/local/lib/libmxnet.so(std::_Function_handler<void (mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*), mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::SelectAlgo(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t)::{lambda(mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)#1}>::_M_invoke(std::_Any_data const&, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*&&, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*&&, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*&&)+0x596) [0x7fffc104ba46]
[1,4]<stderr>:  [bt] (3) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNAlgoReg<mxnet::op::ConvolutionParam>::FindOrElseRegister(mxnet::op::ConvolutionParam const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t, cudnnDataType_t, int, bool, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*, std::function<void (mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)> const&)+0x3d3) [0x7fffc104e223]
[1,4]<stderr>:  [bt] (4) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::SelectAlgo(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t)+0x1f1) [0x7fffc104f501]
[1,4]<stderr>:  [bt] (5) /usr/local/lib/libmxnet.so(+0x4300c16) [0x7fffc1016c16]
[1,4]<stderr>:  [bt] (6) /usr/local/lib/libm[1,4]<stderr>:xnet.so(void mxnet::op::ConvolutionCompute<mshadow::gpu>(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)+0xc67) [0x7fffc10199a7]
[1,4]<stderr>:  [bt] (7) /usr/local/lib/libmxnet.so(mxnet::exec::FComputeExecutor::Run(mxnet::RunContext, bool)+0x76) [0x7fffc00841f6]
[1,4]<stderr>:  [bt] (8) /usr/local/lib/libmxnet.so(+0x332bb63) [0x7fffc0041b63]
[1,4]<stderr>:
[1,4]<stderr>:
[1,6]<stderr>:/workspace/image_classification/common/fit.py:858: UserWarning: Parameters already initialized and force_init=False. init_params call ignored.
[1,6]<stderr>:  allow_missing=allow_missing, force_init=force_init)
[1,6]<stderr>:/workspace/image_classification/common/fit.py:860: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.00045955882352941176 vs. 0.003676470588235294). Is this intended?
[1,6]<stderr>:  optimizer_params=optimizer_params)
[1,6]<stderr>:Traceback (most recent call last):
[1,6]<stderr>:  File "train_imagenet.py", line 191, in <module>
[1,6]<stderr>:    fit.fit(args, kv, model, initializer, lambda_fnc_dali_get_rec_iter, devs, arg_params, aux_params)
[1,6]<stderr>:  File "/workspace/image_classification/common/fit.py", line 1140, in fit
[1,6]<stderr>:    monitor=None)
[1,6]<stderr>:  File "/workspace/image_classification/common/fit.py", line 910, in mlperf_fit
[1,6]<stderr>:    pre_sliced=True)
[1,6]<stderr>:  File "/opt/mxnet/python/mxnet/module/module.py", line 777, in update_metric
[1,6]<stderr>:    self._exec_group.update_metric(eval_metric, labels, pre_sliced, label_pads)
[1,6]<stderr>:  File "/opt/mxnet/python/mxnet/module/executor_group.py", line 664, in update_metric
[1,6]<stderr>:    eval_metric.update_dict(labels_, preds)
[1,6]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 350, in update_dict
[1,6]<stderr>:    metric.update_dict(labels, preds)
[1,6]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 133, in update_dict
[1,6]<stderr>:    self.update(label, pred)
[1,6]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 496, in update
[1,6]<stderr>:    pred_label = pred_label.asnumpy().astype('int32')
[1,6]<stderr>:  File "/opt/mxnet/python/mxnet/ndarray/ndarray.py", line 1996, in asnumpy
[1,6]<stderr>:    ctypes.c_size_t(data.size)))
[1,6]<stderr>:  File "/opt/mxnet/python/mxnet/base.py", line 252, in check_call
[1,6]<stderr>:    raise MXNetError(py_str(_LIB.MXGetLastError()))
[1,6]<stderr>:mxnet.base.MXNetError: [21:26:24] src/operator/nn/./cudnn/cudnn_convolution-inl.h:1024: Failed to get backprop-to-data convolution algorithm 1 with workspace size of 536870912 bytes, please consider reducing batch/model size or increasing the workspace size
[1,6]<stderr>:Stack trace:
[1,6]<stderr>:  [bt] (0) /usr/local/lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x32) [0x7fffbdeb05d2]
[1,6]<stderr>:  [bt] (1) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::CuDNNAlgoSetter(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)+0x1a83) [0x7fffc103f013]
[1,6]<stderr>:  [bt] (2) /usr/local/lib/libmxnet.so(std::_Function_handler<void (mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*), mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::SelectAlgo(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t)::{lambda(mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)#1}>::_M_invoke(std::_Any_data const&, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*&&, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*&&, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*&&)+0x596) [0x7fffc104ba46]
[1,6]<stderr>:  [bt] (3) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNAlgoReg<mxnet::op::ConvolutionParam>::FindOrElseRegister(mxnet::op::ConvolutionParam const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t, cudnnDataType_t, int, bool, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*, std::function<void (mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)> const&)+0x3d3) [0x7fffc104e223]
[1,6]<stderr>:  [bt] (4) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::SelectAlgo(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t)+0x1f1) [0x7fffc104f501]
[1,6]<stderr>:  [bt] (5) /usr/local/lib/libmxnet.so(+0x4300c16) [0x7fffc1016c16]
[1,6]<stderr>:  [bt] (6) /usr/local/lib/libm[1,6]<stderr>:xnet.so(void mxnet::op::ConvolutionCompute<mshadow::gpu>(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)+0xc67) [0x7fffc10199a7]
[1,6]<stderr>:  [bt] (7) /usr/local/lib/libmxnet.so(mxnet::exec::FComputeExecutor::Run(mxnet::RunContext, bool)+0x76) [0x7fffc00841f6]
[1,6]<stderr>:  [bt] (8) /usr/local/lib/libmxnet.so(+0x332bb63) [0x7fffc0041b63]
[1,6]<stderr>:
[1,6]<stderr>:
[1,7]<stderr>:/workspace/image_classification/common/fit.py:858: UserWarning: Parameters already initialized and force_init=False. init_params call ignored.
[1,7]<stderr>:  allow_missing=allow_missing, force_init=force_init)
[1,7]<stderr>:/workspace/image_classification/common/fit.py:860: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.00045955882352941176 vs. 0.003676470588235294). Is this intended?
[1,7]<stderr>:  optimizer_params=optimizer_params)
[1,7]<stderr>:Traceback (most recent call last):
[1,7]<stderr>:  File "train_imagenet.py", line 191, in <module>
[1,7]<stderr>:    fit.fit(args, kv, model, initializer, lambda_fnc_dali_get_rec_iter, devs, arg_params, aux_params)
[1,7]<stderr>:  File "/workspace/image_classification/common/fit.py", line 1140, in fit
[1,7]<stderr>:    monitor=None)
[1,7]<stderr>:  File "/workspace/image_classification/common/fit.py", line 910, in mlperf_fit
[1,7]<stderr>:    pre_sliced=True)
[1,7]<stderr>:  File "/opt/mxnet/python/mxnet/module/module.py", line 777, in update_metric
[1,7]<stderr>:    self._exec_group.update_metric(eval_metric, labels, pre_sliced, label_pads)
[1,7]<stderr>:  File "/opt/mxnet/python/mxnet/module/executor_group.py", line 664, in update_metric
[1,7]<stderr>:    eval_metric.update_dict(labels_, preds)
[1,7]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 350, in update_dict
[1,7]<stderr>:    metric.update_dict(labels, preds)
[1,7]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 133, in update_dict
[1,7]<stderr>:    self.update(label, pred)
[1,7]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 496, in update
[1,7]<stderr>:    pred_label = pred_label.asnumpy().astype('int32')
[1,7]<stderr>:  File "/opt/mxnet/python/mxnet/ndarray/ndarray.py", line 1996, in asnumpy
[1,7]<stderr>:    ctypes.c_size_t(data.size)))
[1,7]<stderr>:  File "/opt/mxnet/python/mxnet/base.py", line 252, in check_call
[1,7]<stderr>:    raise MXNetError(py_str(_LIB.MXGetLastError()))
[1,7]<stderr>:mxnet.base.MXNetError: [21:26:24] src/operator/nn/./cudnn/cudnn_convolution-inl.h:1024: Failed to get backprop-to-data convolution algorithm 1 with workspace size of 536870912 bytes, please consider reducing batch/model size or increasing the workspace size
[1,7]<stderr>:Stack trace:
[1,7]<stderr>:  [bt] (0) /usr/local/lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x32) [0x7fffbdeb05d2]
[1,7]<stderr>:  [bt] (1) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::CuDNNAlgoSetter(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)+0x1a83) [0x7fffc103f013]
[1,7]<stderr>:  [bt] (2) /usr/local/lib/libmxnet.so(std::_Function_handler<void (mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*), mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::SelectAlgo(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t)::{lambda(mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)#1}>::_M_invoke(std::_Any_data const&, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*&&, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*&&, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*&&)+0x596) [0x7fffc104ba46]
[1,7]<stderr>:  [bt] (3) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNAlgoReg<mxnet::op::ConvolutionParam>::FindOrElseRegister(mxnet::op::ConvolutionParam const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t, cudnnDataType_t, int, bool, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*, std::function<void (mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)> const&)+0x3d3) [0x7fffc104e223]
[1,7]<stderr>:  [bt] (4) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::SelectAlgo(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t)+0x1f1) [0x7fffc104f501]
[1,7]<stderr>:  [bt] (5) /usr/local/lib/libmxnet.so(+0x4300c16) [0x7fffc1016c16]
[1,7]<stderr>:  [bt] (6) /usr/local/lib/libm[1,7]<stderr>:xnet.so(void mxnet::op::ConvolutionCompute<mshadow::gpu>(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)+0xc67) [0x7fffc10199a7]
[1,7]<stderr>:  [bt] (7) /usr/local/lib/libmxnet.so(mxnet::exec::FComputeExecutor::Run(mxnet::RunContext, bool)+0x76) [0x7fffc00841f6]
[1,7]<stderr>:  [bt] (8) /usr/local/lib/libmxnet.so(+0x332bb63) [0x7fffc0041b63]
[1,7]<stderr>:
[1,7]<stderr>:
[1,1]<stderr>:/workspace/image_classification/common/fit.py:858: UserWarning: Parameters already initialized and force_init=False. init_params call ignored.
[1,1]<stderr>:  allow_missing=allow_missing, force_init=force_init)
[1,1]<stderr>:/workspace/image_classification/common/fit.py:860: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.00045955882352941176 vs. 0.003676470588235294). Is this intended?
[1,1]<stderr>:  optimizer_params=optimizer_params)
[1,1]<stderr>:Traceback (most recent call last):
[1,1]<stderr>:  File "train_imagenet.py", line 191, in <module>
[1,1]<stderr>:    fit.fit(args, kv, model, initializer, lambda_fnc_dali_get_rec_iter, devs, arg_params, aux_params)
[1,1]<stderr>:  File "/workspace/image_classification/common/fit.py", line 1140, in fit
[1,1]<stderr>:    monitor=None)
[1,1]<stderr>:  File "/workspace/image_classification/common/fit.py", line 910, in mlperf_fit
[1,1]<stderr>:    pre_sliced=True)
[1,1]<stderr>:  File "/opt/mxnet/python/mxnet/module/module.py", line 777, in update_metric
[1,1]<stderr>:    self._exec_group.update_metric(eval_metric, labels, pre_sliced, label_pads)
[1,1]<stderr>:  File "/opt/mxnet/python/mxnet/module/executor_group.py", line 664, in update_metric
[1,1]<stderr>:    eval_metric.update_dict(labels_, preds)
[1,1]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 350, in update_dict
[1,1]<stderr>:    metric.update_dict(labels, preds)
[1,1]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 133, in update_dict
[1,1]<stderr>:    self.update(label, pred)
[1,1]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 496, in update
[1,1]<stderr>:    pred_label = pred_label.asnumpy().astype('int32')
[1,1]<stderr>:  File "/opt/mxnet/python/mxnet/ndarray/ndarray.py", line 1996, in asnumpy
[1,1]<stderr>:    ctypes.c_size_t(data.size)))
[1,1]<stderr>:  File "/opt/mxnet/python/mxnet/base.py", line 252, in check_call
[1,1]<stderr>:    raise MXNetError(py_str(_LIB.MXGetLastError()))
[1,1]<stderr>:mxnet.base.MXNetError: [21:26:24] src/operator/nn/./cudnn/cudnn_convolution-inl.h:1024: Failed to get backprop-to-data convolution algorithm 1 with workspace size of 536870912 bytes, please consider reducing batch/model size or increasing the workspace size
[1,1]<stderr>:Stack trace:
[1,1]<stderr>:  [bt] (0) /usr/local/lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x32) [0x7fffbdeb05d2]
[1,1]<stderr>:  [bt] (1) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::CuDNNAlgoSetter(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)+0x1a83) [0x7fffc103f013]
[1,1]<stderr>:  [bt] (2) /usr/local/lib/libmxnet.so(std::_Function_handler<void (mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*), mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::SelectAlgo(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t)::{lambda(mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)#1}>::_M_invoke(std::_Any_data const&, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*&&, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*&&, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*&&)+0x596) [0x7fffc104ba46]
[1,1]<stderr>:  [bt] (3) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNAlgoReg<mxnet::op::ConvolutionParam>::FindOrElseRegister(mxnet::op::ConvolutionParam const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t, cudnnDataType_t, int, bool, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*, std::function<void (mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)> const&)+0x3d3) [0x7fffc104e223]
[1,1]<stderr>:  [bt] (4) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::SelectAlgo(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t)+0x1f1) [0x7fffc104f501]
[1,1]<stderr>:  [bt] (5) /usr/local/lib/libmxnet.so(+0x4300c16) [0x7fffc1016c16]
[1,1]<stderr>:  [bt] (6) /usr/local/lib/libm[1,1]<stderr>:xnet.so(void mxnet::op::ConvolutionCompute<mshadow::gpu>(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)+0xc67) [0x7fffc10199a7]
[1,1]<stderr>:  [bt] (7) /usr/local/lib/libmxnet.so(mxnet::exec::FComputeExecutor::Run(mxnet::RunContext, bool)+0x76) [0x7fffc00841f6]
[1,1]<stderr>:  [bt] (8) /usr/local/lib/libmxnet.so(+0x332bb63) [0x7fffc0041b63]
[1,1]<stderr>:
[1,1]<stderr>:
[1,2]<stderr>:/workspace/image_classification/common/fit.py:858: UserWarning: Parameters already initialized and force_init=False. init_params call ignored.
[1,2]<stderr>:  allow_missing=allow_missing, force_init=force_init)
[1,2]<stderr>:/workspace/image_classification/common/fit.py:860: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.00045955882352941176 vs. 0.003676470588235294). Is this intended?
[1,2]<stderr>:  optimizer_params=optimizer_params)
[1,2]<stderr>:Traceback (most recent call last):
[1,2]<stderr>:  File "train_imagenet.py", line 191, in <module>
[1,2]<stderr>:    fit.fit(args, kv, model, initializer, lambda_fnc_dali_get_rec_iter, devs, arg_params, aux_params)
[1,2]<stderr>:  File "/workspace/image_classification/common/fit.py", line 1140, in fit
[1,2]<stderr>:    monitor=None)
[1,2]<stderr>:  File "/workspace/image_classification/common/fit.py", line 910, in mlperf_fit
[1,2]<stderr>:    pre_sliced=True)
[1,2]<stderr>:  File "/opt/mxnet/python/mxnet/module/module.py", line 777, in update_metric
[1,2]<stderr>:    self._exec_group.update_metric(eval_metric, labels, pre_sliced, label_pads)
[1,2]<stderr>:  File "/opt/mxnet/python/mxnet/module/executor_group.py", line 664, in update_metric
[1,2]<stderr>:    eval_metric.update_dict(labels_, preds)
[1,2]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 350, in update_dict
[1,2]<stderr>:    metric.update_dict(labels, preds)
[1,2]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 133, in update_dict
[1,2]<stderr>:    self.update(label, pred)
[1,2]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 496, in update
[1,2]<stderr>:    pred_label = pred_label.asnumpy().astype('int32')
[1,2]<stderr>:  File "/opt/mxnet/python/mxnet/ndarray/ndarray.py", line 1996, in asnumpy
[1,2]<stderr>:    ctypes.c_size_t(data.size)))
[1,2]<stderr>:  File "/opt/mxnet/python/mxnet/base.py", line 252, in check_call
[1,2]<stderr>:    raise MXNetError(py_str(_LIB.MXGetLastError()))
[1,2]<stderr>:mxnet.base.MXNetError: [21:26:24] src/operator/nn/./cudnn/cudnn_convolution-inl.h:1024: Failed to get backprop-to-data convolution algorithm 1 with workspace size of 536870912 bytes, please consider reducing batch/model size or increasing the workspace size
[1,2]<stderr>:Stack trace:
[1,2]<stderr>:  [bt] (0) /usr/local/lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x32) [0x7fffbdeb05d2]
[1,2]<stderr>:  [bt] (1) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::CuDNNAlgoSetter(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)+0x1a83) [0x7fffc103f013]
[1,2]<stderr>:  [bt] (2) /usr/local/lib/libmxnet.so(std::_Function_handler<void (mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*), mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::SelectAlgo(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t)::{lambda(mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)#1}>::_M_invoke(std::_Any_data const&, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*&&, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*&&, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*&&)+0x596) [0x7fffc104ba46]
[1,2]<stderr>:  [bt] (3) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNAlgoReg<mxnet::op::ConvolutionParam>::FindOrElseRegister(mxnet::op::ConvolutionParam const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t, cudnnDataType_t, int, bool, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*, std::function<void (mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)> const&)+0x3d3) [0x7fffc104e223]
[1,2]<stderr>:  [bt] (4) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::SelectAlgo(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t)+0x1f1) [0x7fffc104f501]
[1,2]<stderr>:  [bt] (5) /usr/local/lib/libmxnet.so(+0x4300c16) [0x7fffc1016c16]
[1,2]<stderr>:  [bt] (6) /usr/local/lib/libm[1,2]<stderr>:xnet.so(void mxnet::op::ConvolutionCompute<mshadow::gpu>(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)+0xc67) [0x7fffc10199a7]
[1,2]<stderr>:  [bt] (7) /usr/local/lib/libmxnet.so(mxnet::exec::FComputeExecutor::Run(mxnet::RunContext, bool)+0x76) [0x7fffc00841f6]
[1,2]<stderr>:  [bt] (8) /usr/local/lib/libmxnet.so(+0x332bb63) [0x7fffc0041b63]
[1,2]<stderr>:
[1,2]<stderr>:
[1,5]<stderr>:/workspace/image_classification/common/fit.py:858: UserWarning: Parameters already initialized and force_init=False. init_params call ignored.
[1,5]<stderr>:  allow_missing=allow_missing, force_init=force_init)
[1,5]<stderr>:/workspace/image_classification/common/fit.py:860: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.00045955882352941176 vs. 0.003676470588235294). Is this intended?
[1,5]<stderr>:  optimizer_params=optimizer_params)
[1,5]<stderr>:Traceback (most recent call last):
[1,5]<stderr>:  File "train_imagenet.py", line 191, in <module>
[1,5]<stderr>:    fit.fit(args, kv, model, initializer, lambda_fnc_dali_get_rec_iter, devs, arg_params, aux_params)
[1,5]<stderr>:  File "/workspace/image_classification/common/fit.py", line 1140, in fit
[1,5]<stderr>:    monitor=None)
[1,5]<stderr>:  File "/workspace/image_classification/common/fit.py", line 910, in mlperf_fit
[1,5]<stderr>:    pre_sliced=True)
[1,5]<stderr>:  File "/opt/mxnet/python/mxnet/module/module.py", line 777, in update_metric
[1,5]<stderr>:    self._exec_group.update_metric(eval_metric, labels, pre_sliced, label_pads)
[1,5]<stderr>:  File "/opt/mxnet/python/mxnet/module/executor_group.py", line 664, in update_metric
[1,5]<stderr>:    eval_metric.update_dict(labels_, preds)
[1,5]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 350, in update_dict
[1,5]<stderr>:    metric.update_dict(labels, preds)
[1,5]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 133, in update_dict
[1,5]<stderr>:    self.update(label, pred)
[1,5]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 496, in update
[1,5]<stderr>:    pred_label = pred_label.asnumpy().astype('int32')
[1,5]<stderr>:  File "/opt/mxnet/python/mxnet/ndarray/ndarray.py", line 1996, in asnumpy
[1,5]<stderr>:    ctypes.c_size_t(data.size)))
[1,5]<stderr>:  File "/opt/mxnet/python/mxnet/base.py", line 252, in check_call
[1,5]<stderr>:    raise MXNetError(py_str(_LIB.MXGetLastError()))
[1,5]<stderr>:mxnet.base.MXNetError: [21:26:24] src/operator/nn/./cudnn/cudnn_convolution-inl.h:1024: Failed to get backprop-to-data convolution algorithm 1 with workspace size of 536870912 bytes, please consider reducing batch/model size or increasing the workspace size
[1,5]<stderr>:Stack trace:
[1,5]<stderr>:  [bt] (0) /usr/local/lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x32) [0x7fffbdeb05d2]
[1,5]<stderr>:  [bt] (1) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::CuDNNAlgoSetter(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)+0x1a83) [0x7fffc103f013]
[1,5]<stderr>:  [bt] (2) /usr/local/lib/libmxnet.so(std::_Function_handler<void (mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*), mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::SelectAlgo(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t)::{lambda(mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)#1}>::_M_invoke(std::_Any_data const&, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*&&, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*&&, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*&&)+0x596) [0x7fffc104ba46]
[1,5]<stderr>:  [bt] (3) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNAlgoReg<mxnet::op::ConvolutionParam>::FindOrElseRegister(mxnet::op::ConvolutionParam const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t, cudnnDataType_t, int, bool, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*, std::function<void (mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)> const&)+0x3d3) [0x7fffc104e223]
[1,5]<stderr>:  [bt] (4) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::SelectAlgo(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t)+0x1f1) [0x7fffc104f501]
[1,5]<stderr>:  [bt] (5) /usr/local/lib/libmxnet.so(+0x4300c16) [0x7fffc1016c16]
[1,5]<stderr>:  [bt] (6) /usr/local/lib/libm[1,5]<stderr>:xnet.so(void mxnet::op::ConvolutionCompute<mshadow::gpu>(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)+0xc67) [0x7fffc10199a7]
[1,5]<stderr>:  [bt] (7) /usr/local/lib/libmxnet.so(mxnet::exec::FComputeExecutor::Run(mxnet::RunContext, bool)+0x76) [0x7fffc00841f6]
[1,5]<stderr>:  [bt] (8) /usr/local/lib/libmxnet.so(+0x332bb63) [0x7fffc0041b63]
[1,5]<stderr>:
[1,5]<stderr>:
[1,0]<stderr>:/workspace/image_classification/common/fit.py:858: UserWarning: Parameters already initialized and force_init=False. init_params call ignored.
[1,0]<stderr>:  allow_missing=allow_missing, force_init=force_init)
[1,0]<stderr>:/workspace/image_classification/common/fit.py:860: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.00045955882352941176 vs. 0.003676470588235294). Is this intended?
[1,0]<stderr>:  optimizer_params=optimizer_params)
[1,0]<stderr>:Traceback (most recent call last):
[1,0]<stderr>:  File "train_imagenet.py", line 191, in <module>
[1,0]<stderr>:    fit.fit(args, kv, model, initializer, lambda_fnc_dali_get_rec_iter, devs, arg_params, aux_params)
[1,0]<stderr>:  File "/workspace/image_classification/common/fit.py", line 1140, in fit
[1,0]<stderr>:    monitor=None)
[1,0]<stderr>:  File "/workspace/image_classification/common/fit.py", line 910, in mlperf_fit
[1,0]<stderr>:    pre_sliced=True)
[1,0]<stderr>:  File "/opt/mxnet/python/mxnet/module/module.py", line 777, in update_metric
[1,0]<stderr>:    self._exec_group.update_metric(eval_metric, labels, pre_sliced, label_pads)
[1,0]<stderr>:  File "/opt/mxnet/python/mxnet/module/executor_group.py", line 664, in update_metric
[1,0]<stderr>:    eval_metric.update_dict(labels_, preds)
[1,0]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 350, in update_dict
[1,0]<stderr>:    metric.update_dict(labels, preds)
[1,0]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 133, in update_dict
[1,0]<stderr>:    self.update(label, pred)
[1,0]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 496, in update
[1,0]<stderr>:    pred_label = pred_label.asnumpy().astype('int32')
[1,0]<stderr>:  File "/opt/mxnet/python/mxnet/ndarray/ndarray.py", line 1996, in asnumpy
[1,0]<stderr>:    ctypes.c_size_t(data.size)))
[1,0]<stderr>:  File "/opt/mxnet/python/mxnet/base.py", line 252, in check_call
[1,0]<stderr>:    raise MXNetError(py_str(_LIB.MXGetLastError()))
[1,0]<stderr>:mxnet.base.MXNetError: [21:26:24] src/operator/nn/./cudnn/cudnn_convolution-inl.h:1024: Failed to get backprop-to-data convolution algorithm 1 with workspace size of 536870912 bytes, please consider reducing batch/model size or increasing the workspace size
[1,0]<stderr>:Stack trace:
[1,0]<stderr>:  [bt] (0) /usr/local/lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x32) [0x7fffbdeb05d2]
[1,0]<stderr>:  [bt] (1) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::CuDNNAlgoSetter(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)+0x1a83) [0x7fffc103f013]
[1,0]<stderr>:  [bt] (2) /usr/local/lib/libmxnet.so(std::_Function_handler<void (mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*), mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::SelectAlgo(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t)::{lambda(mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)#1}>::_M_invoke(std::_Any_data const&, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*&&, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*&&, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*&&)+0x596) [0x7fffc104ba46]
[1,0]<stderr>:  [bt] (3) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNAlgoReg<mxnet::op::ConvolutionParam>::FindOrElseRegister(mxnet::op::ConvolutionParam const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t, cudnnDataType_t, int, bool, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*, std::function<void (mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)> const&)+0x3d3) [0x7fffc104e223]
[1,0]<stderr>:  [bt] (4) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::SelectAlgo(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t)+0x1f1) [0x7fffc104f501]
[1,0]<stderr>:  [bt] (5) /usr/local/lib/libmxnet.so(+0x4300c16) [0x7fffc1016c16]
[1,0]<stderr>:  [bt] (6) /usr/local/lib/libm[1,0]<stderr>:xnet.so(void mxnet::op::ConvolutionCompute<mshadow::gpu>(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)+0xc67) [0x7fffc10199a7]
[1,0]<stderr>:  [bt] (7) /usr/local/lib/libmxnet.so(mxnet::exec::FComputeExecutor::Run(mxnet::RunContext, bool)+0x76) [0x7fffc00841f6]
[1,0]<stderr>:  [bt] (8) /usr/local/lib/libmxnet.so(+0x332bb63) [0x7fffc0041b63]
[1,0]<stderr>:
[1,0]<stderr>:
[1,3]<stderr>:/workspace/image_classification/common/fit.py:858: UserWarning: Parameters already initialized and force_init=False. init_params call ignored.
[1,3]<stderr>:  allow_missing=allow_missing, force_init=force_init)
[1,3]<stderr>:/workspace/image_classification/common/fit.py:860: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.00045955882352941176 vs. 0.003676470588235294). Is this intended?
[1,3]<stderr>:  optimizer_params=optimizer_params)
[1,3]<stderr>:Traceback (most recent call last):
[1,3]<stderr>:  File "train_imagenet.py", line 191, in <module>
[1,3]<stderr>:    fit.fit(args, kv, model, initializer, lambda_fnc_dali_get_rec_iter, devs, arg_params, aux_params)
[1,3]<stderr>:  File "/workspace/image_classification/common/fit.py", line 1140, in fit
[1,3]<stderr>:    monitor=None)
[1,3]<stderr>:  File "/workspace/image_classification/common/fit.py", line 910, in mlperf_fit
[1,3]<stderr>:    pre_sliced=True)
[1,3]<stderr>:  File "/opt/mxnet/python/mxnet/module/module.py", line 777, in update_metric
[1,3]<stderr>:    self._exec_group.update_metric(eval_metric, labels, pre_sliced, label_pads)
[1,3]<stderr>:  File "/opt/mxnet/python/mxnet/module/executor_group.py", line 664, in update_metric
[1,3]<stderr>:    eval_metric.update_dict(labels_, preds)
[1,3]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 350, in update_dict
[1,3]<stderr>:    metric.update_dict(labels, preds)
[1,3]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 133, in update_dict
[1,3]<stderr>:    self.update(label, pred)
[1,3]<stderr>:  File "/opt/mxnet/python/mxnet/metric.py", line 496, in update
[1,3]<stderr>:    pred_label = pred_label.asnumpy().astype('int32')
[1,3]<stderr>:  File "/opt/mxnet/python/mxnet/ndarray/ndarray.py", line 1996, in asnumpy
[1,3]<stderr>:    ctypes.c_size_t(data.size)))
[1,3]<stderr>:  File "/opt/mxnet/python/mxnet/base.py", line 252, in check_call
[1,3]<stderr>:    raise MXNetError(py_str(_LIB.MXGetLastError()))
[1,3]<stderr>:mxnet.base.MXNetError: [21:26:24] src/operator/nn/./cudnn/cudnn_convolution-inl.h:1024: Failed to get backprop-to-data convolution algorithm 1 with workspace size of 536870912 bytes, please consider reducing batch/model size or increasing the workspace size
[1,3]<stderr>:Stack trace:
[1,3]<stderr>:  [bt] (0) /usr/local/lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x32) [0x7fffbdeb05d2]
[1,3]<stderr>:  [bt] (1) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::CuDNNAlgoSetter(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)+0x1a83) [0x7fffc103f013]
[1,3]<stderr>:  [bt] (2) /usr/local/lib/libmxnet.so(std::_Function_handler<void (mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*), mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::SelectAlgo(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t)::{lambda(mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)#1}>::_M_invoke(std::_Any_data const&, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*&&, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*&&, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*&&)+0x596) [0x7fffc104ba46]
[1,3]<stderr>:  [bt] (3) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNAlgoReg<mxnet::op::ConvolutionParam>::FindOrElseRegister(mxnet::op::ConvolutionParam const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t, cudnnDataType_t, int, bool, mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*, std::function<void (mxnet::op::CuDNNAlgo<cudnnConvolutionFwdAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdDataAlgo_t>*, mxnet::op::CuDNNAlgo<cudnnConvolutionBwdFilterAlgo_t>*)> const&)+0x3d3) [0x7fffc104e223]
[1,3]<stderr>:  [bt] (4) /usr/local/lib/libmxnet.so(mxnet::op::CuDNNConvolutionOp<mshadow::half::half_t>::SelectAlgo(mxnet::RunContext const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, cudnnDataType_t, cudnnDataType_t)+0x1f1) [0x7fffc104f501]
[1,3]<stderr>:  [bt] (5) /usr/local/lib/libmxnet.so(+0x4300c16) [0x7fffc1016c16]
[1,3]<stderr>:  [bt] (6) /usr/local/lib/libm[1,3]<stderr>:xnet.so(void mxnet::op::ConvolutionCompute<mshadow::gpu>(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)+0xc67) [0x7fffc10199a7]
[1,3]<stderr>:  [bt] (7) /usr/local/lib/libmxnet.so(mxnet::exec::FComputeExecutor::Run(mxnet::RunContext, bool)+0x76) [0x7fffc00841f6]
[1,3]<stderr>:  [bt] (8) /usr/local/lib/libmxnet.so(+0x332bb63) [0x7fffc0041b63]
[1,3]<stderr>:
[1,3]<stderr>:
[1,0]<stderr>:[dss01:02289] *** Process received signal ***
[1,0]<stderr>:[dss01:02289] Signal: Segmentation fault (11)
[1,0]<stderr>:[dss01:02289] Signal code:  (128)
[1,0]<stderr>:[dss01:02289] Failing at address: (nil)
[1,0]<stderr>:[dss01:02289] [ 0] [1,0]<stderr>:/lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7ffff7bcb390]
[1,0]<stderr>:[dss01:02289] [ 1] [1,0]<stderr>:/usr/local/lib/libmxnet.so(_ZNSt16_Sp_counted_baseILN9__gnu_cxx12_Lock_policyE2EE10_M_releaseEv+0xe)[0x7fffbdeb467e]
[1,0]<stderr>:[dss01:02289] [ 2] [1,0]<stderr>:/usr/local/lib/libmxnet.so(_ZN5mxnet6engine14ThreadedEngineD1Ev+0x1f0)[0x7fffc09142f0]
[1,0]<stderr>:[dss01:02289] [ 3] [1,0]<stderr>:/usr/local/lib/libmxnet.so(_ZN5mxnet6engine23ThreadedEnginePerDeviceD0Ev+0x9)[0x7fffc092ad99]
[1,0]<stderr>:[dss01:02289] [ 4] [1,0]<stderr>:/usr/local/lib/libmxnet.so(_ZNSt10shared_ptrIN5mxnet6EngineEED1Ev+0x52)[0x7fffc091b5b2]
[1,0]<stderr>:[dss01:02289] [ 5] [1,0]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x39ff8)[0x7ffff7829ff8]
[1,0]<stderr>:[dss01:02289] [ 6] [1,0]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(+0x3a045)[0x7ffff782a045]
[1,0]<stderr>:[dss01:02289] [ 7] [1,0]<stderr>:/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf7)[0x7ffff7810837]
[1,0]<stderr>:[dss01:02289] [ 8] [1,0]<stderr>:python(_start+0x29)[0x5d6999]
[1,0]<stderr>:[dss01:02289] *** End of error message ***
[1,0]<stderr>:./run_and_time.sh: line 105:  2289 Segmentation fault      (core dumped) ${BIND} python train_imagenet.py "${PARAMS[@]}"
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[65363,1],7]
  Exit code:    1
--------------------------------------------------------------------------
