Beginning trial 2 of 3
Run vars: id 191021131937151309967
Gathering sys log on dss01
:::MLL 1571690597.598 submission_benchmark: {"metadata": {"lineno": 226, "file": "mlperf_log_utils.py"}, "value": "resnet"}
:::MLL 1571690597.598 submission_org: {"metadata": {"lineno": 231, "file": "mlperf_log_utils.py"}, "value": "NVIDIA"}
WARNING: Log validation: Key "submission_division" is not in known resnet keys.
:::MLL 1571690597.599 submission_division: {"metadata": {"lineno": 235, "file": "mlperf_log_utils.py"}, "value": "closed"}
:::MLL 1571690597.600 submission_status: {"metadata": {"lineno": 239, "file": "mlperf_log_utils.py"}, "value": "onprem"}
:::MLL 1571690597.600 submission_platform: {"metadata": {"lineno": 243, "file": "mlperf_log_utils.py"}, "value": "1xDSS8440"}
:::MLL 1571690597.601 submission_entry: {"metadata": {"lineno": 247, "file": "mlperf_log_utils.py"}, "value": "{'os': '\\\\S / ', 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'hardware': 'DSS8440', 'framework': 'MXNet NVIDIA Release 19.05', 'notes': 'N/A', 'nodes': \"{'num_cores': '40', 'num_vcpus': '40', 'num_network_cards': '1', 'sys_storage_type': 'SATA SSD', 'cpu_accel_interconnect': 'UPI', 'num_accelerators': '8', 'notes': '', 'cpu': '2x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz', 'num_nodes': '1', 'sys_storage_size': '1x 447.1G + 1x 931.5G', 'sys_mem_size': '754 GB', 'network_card': 'Mellanox Technologies MT27700 Family [ConnectX-4]', 'accelerator': 'Tesla V100-PCIE-32GB'}\", 'libraries': \"{'container_base': 'Ubuntu-16.04', 'trt_version': '5.1.5.0', 'cuda_driver_version': '418.67', 'mofed_version': '5.0-0', 'cublas_version': '10.2.0.163', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'dali_version': '0.9.1', 'openmpi_version': '3.1.3', 'cuda_version': '10.1.163'}\", 'power': 'N/A'}"}
:::MLL 1571690597.601 submission_poc_name: {"metadata": {"lineno": 251, "file": "mlperf_log_utils.py"}, "value": "Paulius Micikevicius"}
:::MLL 1571690597.602 submission_poc_email: {"metadata": {"lineno": 255, "file": "mlperf_log_utils.py"}, "value": "pauliusm@nvidia.com"}
Clearing cache on dss01
:::MLL 1571690617.969 cache_clear: {"metadata": {"lineno": 1, "file": "<string>"}, "value": true}

Launching user script on master node:
[1,0]<stdout>:STARTING TIMING RUN AT 2019-10-21 08:43:38 PM
[1,0]<stdout>:running benchmark
[1,2]<stdout>:STARTING TIMING RUN AT 2019-10-21 08:43:38 PM
[1,2]<stdout>:running benchmark
[1,3]<stdout>:STARTING TIMING RUN AT 2019-10-21 08:43:38 PM
[1,3]<stdout>:running benchmark
[1,1]<stdout>:STARTING TIMING RUN AT 2019-10-21 08:43:38 PM
[1,1]<stdout>:running benchmark
[1,4]<stdout>:STARTING TIMING RUN AT 2019-10-21 08:43:38 PM
[1,4]<stdout>:running benchmark
[1,5]<stdout>:STARTING TIMING RUN AT 2019-10-21 08:43:38 PM
[1,5]<stdout>:running benchmark
[1,6]<stdout>:STARTING TIMING RUN AT 2019-10-21 08:43:38 PM
[1,6]<stdout>:running benchmark
[1,7]<stdout>:STARTING TIMING RUN AT 2019-10-21 08:43:38 PM
[1,7]<stdout>:running benchmark
[1,7]<stderr>:[20:43:39] src/operator/nn/cudnn/cudnn_algoreg.cc:53: cuDNN library mismatch: linked-against version 7604 != compiled-against version 7600
[1,0]<stderr>:[20:43:39] src/operator/nn/cudnn/cudnn_algoreg.cc:53: cuDNN library mismatch: linked-against version 7604 != compiled-against version 7600
[1,1]<stderr>:[20:43:39] src/operator/nn/cudnn/cudnn_algoreg.cc:53: cuDNN library mismatch: linked-against version 7604 != compiled-against version 7600
[1,2]<stderr>:[20:43:39] src/operator/nn/cudnn/cudnn_algoreg.cc:53: cuDNN library mismatch: linked-against version 7604 != compiled-against version 7600
[1,3]<stderr>:[20:43:39] src/operator/nn/cudnn/cudnn_algoreg.cc:53: cuDNN library mismatch: linked-against version 7604 != compiled-against version 7600
[1,4]<stderr>:[20:43:39] src/operator/nn/cudnn/cudnn_algoreg.cc:53: cuDNN library mismatch: linked-against version 7604 != compiled-against version 7600
[1,5]<stderr>:[20:43:39] src/operator/nn/cudnn/cudnn_algoreg.cc:53: cuDNN library mismatch: linked-against version 7604 != compiled-against version 7600
[1,6]<stderr>:[20:43:39] src/operator/nn/cudnn/cudnn_algoreg.cc:53: cuDNN library mismatch: linked-against version 7604 != compiled-against version 7600
--------------------------------------------------------------------------
WARNING: One or more nonexistent OpenFabrics devices/ports were
specified:

  Host:                 dss01
  MCA parameter:        mca_btl_if_include
  Nonexistent entities: mlx5_3

These entities will be ignored.  You can disable this warning by
setting the btl_openib_warn_nonexistent_if MCA parameter to 0.
--------------------------------------------------------------------------
[1,7]<stdout>::::MLL 1571690626.930 init_start: {"metadata": {"file": "train_imagenet.py", "lineno": 83}, "value": null}
[1,3]<stdout>::::MLL 1571690626.930 init_start: {"value": null, "metadata": {"lineno": 83, "file": "train_imagenet.py"}}
[1,0]<stdout>::::MLL 1571690626.930 init_start: {"value": null, "metadata": {"file": "train_imagenet.py", "lineno": 83}}
[1,2]<stdout>::::MLL 1571690626.930 init_start: {"metadata": {"file": "train_imagenet.py", "lineno": 83}, "value": null}
[1,1]<stdout>::::MLL 1571690626.930 init_start: {"value": null, "metadata": {"file": "train_imagenet.py", "lineno": 83}}
[1,4]<stdout>::::MLL 1571690626.930 init_start: {"metadata": {"file": "train_imagenet.py", "lineno": 83}, "value": null}
[1,5]<stdout>::::MLL 1571690626.930 init_start: {"metadata": {"file": "train_imagenet.py", "lineno": 83}, "value": null}
[1,6]<stdout>::::MLL 1571690626.933 init_start: {"value": null, "metadata": {"lineno": 83, "file": "train_imagenet.py"}}
[dss01:00877] 7 more processes have sent help message help-mpi-btl-openib.txt / nonexistent port
[dss01:00877] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_initial_shape" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690639.587 model_hp_initial_shape: {"value": [4, 224, 224], "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 266}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690639.589 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690639.590 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690639.590 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690639.592 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690639.592 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690639.593 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690639.594 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690639.595 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690639.596 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690639.597 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690639.598 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690639.599 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690639.600 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690639.601 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690639.602 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_shorcut_add" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690639.603 model_hp_shorcut_add: {"value": null, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 192}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_final_shape" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690639.604 model_hp_final_shape: {"value": 1000, "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 309}}
[1,0]<stdout>:WARNING: Log validation: Key "model_hp_loss_fn" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690639.604 model_hp_loss_fn: {"value": "categorical_cross_entropy", "metadata": {"file": "symbols/resnet-v1b-normconv-fl.py", "lineno": 320}}
[1,0]<stdout>::::MLL 1571690639.618 model_bn_span: {"value": 256, "metadata": {"file": "common/dali.py", "lineno": 229}}
[1,2]<stdout>:WARNING: `nvJPEGDecoderRandomCrop` is now deprecated. Use `ImageDecoderRandomCrop` instead
[1,7]<stdout>:WARNING: `nvJPEGDecoderRandomCrop` is now deprecated. Use `ImageDecoderRandomCrop` instead
[1,1]<stdout>:WARNING: `nvJPEGDecoderRandomCrop` is now deprecated. Use `ImageDecoderRandomCrop` instead
[1,0]<stdout>:WARNING: `nvJPEGDecoderRandomCrop` is now deprecated. Use `ImageDecoderRandomCrop` instead
[1,3]<stdout>:WARNING: `nvJPEGDecoderRandomCrop` is now deprecated. Use `ImageDecoderRandomCrop` instead
[1,4]<stdout>:WARNING: `nvJPEGDecoderRandomCrop` is now deprecated. Use `ImageDecoderRandomCrop` instead
[1,6]<stdout>:WARNING: `nvJPEGDecoderRandomCrop` is now deprecated. Use `ImageDecoderRandomCrop` instead
[1,5]<stdout>:WARNING: `nvJPEGDecoderRandomCrop` is now deprecated. Use `ImageDecoderRandomCrop` instead
[1,2]<stdout>:WARNING: `nvJPEGDecoder` is now deprecated. Use `ImageDecoder` instead
[1,7]<stdout>:WARNING: `nvJPEGDecoder` is now deprecated. Use `ImageDecoder` instead
[1,1]<stdout>:WARNING: `nvJPEGDecoder` is now deprecated. Use `ImageDecoder` instead
[1,0]<stdout>:WARNING: `nvJPEGDecoder` is now deprecated. Use `ImageDecoder` instead
[1,3]<stdout>:WARNING: `nvJPEGDecoder` is now deprecated. Use `ImageDecoder` instead
[1,4]<stdout>:WARNING: `nvJPEGDecoder` is now deprecated. Use `ImageDecoder` instead
[1,6]<stdout>:WARNING: `nvJPEGDecoder` is now deprecated. Use `ImageDecoder` instead
[1,5]<stdout>:WARNING: `nvJPEGDecoder` is now deprecated. Use `ImageDecoder` instead
[1,0]<stdout>::::MLL 1571690648.747 init_stop: {"value": null, "metadata": {"file": "train_imagenet.py", "lineno": 187}}
[1,0]<stdout>::::MLL 1571690648.748 run_start: {"value": null, "metadata": {"file": "train_imagenet.py", "lineno": 190}}
[1,0]<stderr>:2019-10-21 20:44:08,748 Node[0] start with arguments Namespace(accuracy_threshold=0.759, batch_size=256, batchnorm_eps=1e-05, batchnorm_layout='NHWC', batchnorm_mom=0.9, bn_gamma_init0=False, conv_algo=1, conv_layout='NHWC', custom_bn_off=0, dali_cache_size=0, dali_nvjpeg_memory_padding=256, dali_prefetch_queue=8, dali_roi_decode=True, dali_threads=3, data_train='/data/train.rec', data_train_idx='/data/train.idx', data_val='/data/val.rec', data_val_idx='/data/val.idx', disp_batches=20, dtype='float16', epoch_size=0, eval_offset=3, eval_period=4, force_tensor_core=1, fuse_bn_add_relu=1, fuse_bn_relu=1, gpus='0', image_shape='4,224,224', initializer='default', input_layout='NHWC', kv_store='horovod', label_smoothing=0.1, lars_eps=0, lars_eta=0.001, lazy_init_sanity=False, log='', logging_dir='logs', lr=6.0, lr_factor=0.1, lr_step_epochs='pow2', max_random_area=1.0, max_random_aspect_ratio=1.3333333333333333, min_random_area=0.05, min_random_aspect_ratio=0.75, model_prefix=None, mom=0.9, network='resnet-v1b-normconv-fl', num_classes=1000, num_epochs=72, num_examples=1281167, num_layers=50, optimizer='sgdwfastlars', pooling_layout='NHWC', profile_server_suffix='', profile_worker_suffix='', resize=256, save_period=1, seed=23123, separ_val=False, top_k=0, use_dali=True, verbose=0, warmup_epochs=5, warmup_strategy='linear', wd=0.0002)
[1,7]<stderr>:2019-10-21 20:44:08,748 Node[7] start with arguments Namespace(accuracy_threshold=0.759, batch_size=256, batchnorm_eps=1e-05, batchnorm_layout='NHWC', batchnorm_mom=0.9, bn_gamma_init0=False, conv_algo=1, conv_layout='NHWC', custom_bn_off=0, dali_cache_size=0, dali_nvjpeg_memory_padding=256, dali_prefetch_queue=8, dali_roi_decode=True, dali_threads=3, data_train='/data/train.rec', data_train_idx='/data/train.idx', data_val='/data/val.rec', data_val_idx='/data/val.idx', disp_batches=20, dtype='float16', epoch_size=0, eval_offset=3, eval_period=4, force_tensor_core=1, fuse_bn_add_relu=1, fuse_bn_relu=1, gpus='7', image_shape='4,224,224', initializer='default', input_layout='NHWC', kv_store='horovod', label_smoothing=0.1, lars_eps=0, lars_eta=0.001, lazy_init_sanity=False, log='', logging_dir='logs', lr=6.0, lr_factor=0.1, lr_step_epochs='pow2', max_random_area=1.0, max_random_aspect_ratio=1.3333333333333333, min_random_area=0.05, min_random_aspect_ratio=0.75, model_prefix=None, mom=0.9, network='resnet-v1b-normconv-fl', num_classes=1000, num_epochs=72, num_examples=1281167, num_layers=50, optimizer='sgdwfastlars', pooling_layout='NHWC', profile_server_suffix='', profile_worker_suffix='', resize=256, save_period=1, seed=37146, separ_val=False, top_k=0, use_dali=True, verbose=0, warmup_epochs=5, warmup_strategy='linear', wd=0.0002)
[1,1]<stderr>:2019-10-21 20:44:08,748 Node[1] start with arguments Namespace(accuracy_threshold=0.759, batch_size=256, batchnorm_eps=1e-05, batchnorm_layout='NHWC', batchnorm_mom=0.9, bn_gamma_init0=False, conv_algo=1, conv_layout='NHWC', custom_bn_off=0, dali_cache_size=0, dali_nvjpeg_memory_padding=256, dali_prefetch_queue=8, dali_roi_decode=True, dali_threads=3, data_train='/data/train.rec', data_train_idx='/data/train.idx', data_val='/data/val.rec', data_val_idx='/data/val.idx', disp_batches=20, dtype='float16', epoch_size=0, eval_offset=3, eval_period=4, force_tensor_core=1, fuse_bn_add_relu=1, fuse_bn_relu=1, gpus='1', image_shape='4,224,224', initializer='default', input_layout='NHWC', kv_store='horovod', label_smoothing=0.1, lars_eps=0, lars_eta=0.001, lazy_init_sanity=False, log='', logging_dir='logs', lr=6.0, lr_factor=0.1, lr_step_epochs='pow2', max_random_area=1.0, max_random_aspect_ratio=1.3333333333333333, min_random_area=0.05, min_random_aspect_ratio=0.75, model_prefix=None, mom=0.9, network='resnet-v1b-normconv-fl', num_classes=1000, num_epochs=72, num_examples=1281167, num_layers=50, optimizer='sgdwfastlars', pooling_layout='NHWC', profile_server_suffix='', profile_worker_suffix='', resize=256, save_period=1, seed=34236, separ_val=False, top_k=0, use_dali=True, verbose=0, warmup_epochs=5, warmup_strategy='linear', wd=0.0002)
[1,2]<stderr>:2019-10-21 20:44:08,748 Node[2] start with arguments Namespace(accuracy_threshold=0.759, batch_size=256, batchnorm_eps=1e-05, batchnorm_layout='NHWC', batchnorm_mom=0.9, bn_gamma_init0=False, conv_algo=1, conv_layout='NHWC', custom_bn_off=0, dali_cache_size=0, dali_nvjpeg_memory_padding=256, dali_prefetch_queue=8, dali_roi_decode=True, dali_threads=3, data_train='/data/train.rec', data_train_idx='/data/train.idx', data_val='/data/val.rec', data_val_idx='/data/val.idx', disp_batches=20, dtype='float16', epoch_size=0, eval_offset=3, eval_period=4, force_tensor_core=1, fuse_bn_add_relu=1, fuse_bn_relu=1, gpus='2', image_shape='4,224,224', initializer='default', input_layout='NHWC', kv_store='horovod', label_smoothing=0.1, lars_eps=0, lars_eta=0.001, lazy_init_sanity=False, log='', logging_dir='logs', lr=6.0, lr_factor=0.1, lr_step_epochs='pow2', max_random_area=1.0, max_random_aspect_ratio=1.3333333333333333, min_random_area=0.05, min_random_aspect_ratio=0.75, model_prefix=None, mom=0.9, network='resnet-v1b-normconv-fl', num_classes=1000, num_epochs=72, num_examples=1281167, num_layers=50, optimizer='sgdwfastlars', pooling_layout='NHWC', profile_server_suffix='', profile_worker_suffix='', resize=256, save_period=1, seed=21236, separ_val=False, top_k=0, use_dali=True, verbose=0, warmup_epochs=5, warmup_strategy='linear', wd=0.0002)
[1,6]<stderr>:2019-10-21 20:44:08,748 Node[6] start with arguments Namespace(accuracy_threshold=0.759, batch_size=256, batchnorm_eps=1e-05, batchnorm_layout='NHWC', batchnorm_mom=0.9, bn_gamma_init0=False, conv_algo=1, conv_layout='NHWC', custom_bn_off=0, dali_cache_size=0, dali_nvjpeg_memory_padding=256, dali_prefetch_queue=8, dali_roi_decode=True, dali_threads=3, data_train='/data/train.rec', data_train_idx='/data/train.idx', data_val='/data/val.rec', data_val_idx='/data/val.idx', disp_batches=20, dtype='float16', epoch_size=0, eval_offset=3, eval_period=4, force_tensor_core=1, fuse_bn_add_relu=1, fuse_bn_relu=1, gpus='6', image_shape='4,224,224', initializer='default', input_layout='NHWC', kv_store='horovod', label_smoothing=0.1, lars_eps=0, lars_eta=0.001, lazy_init_sanity=False, log='', logging_dir='logs', lr=6.0, lr_factor=0.1, lr_step_epochs='pow2', max_random_area=1.0, max_random_aspect_ratio=1.3333333333333333, min_random_area=0.05, min_random_aspect_ratio=0.75, model_prefix=None, mom=0.9, network='resnet-v1b-normconv-fl', num_classes=1000, num_epochs=72, num_examples=1281167, num_layers=50, optimizer='sgdwfastlars', pooling_layout='NHWC', profile_server_suffix='', profile_worker_suffix='', resize=256, save_period=1, seed=24086, separ_val=False, top_k=0, use_dali=True, verbose=0, warmup_epochs=5, warmup_strategy='linear', wd=0.0002)
[1,5]<stderr>:2019-10-21 20:44:08,748 Node[5] start with arguments Namespace(accuracy_threshold=0.759, batch_size=256, batchnorm_eps=1e-05, batchnorm_layout='NHWC', batchnorm_mom=0.9, bn_gamma_init0=False, conv_algo=1, conv_layout='NHWC', custom_bn_off=0, dali_cache_size=0, dali_nvjpeg_memory_padding=256, dali_prefetch_queue=8, dali_roi_decode=True, dali_threads=3, data_train='/data/train.rec', data_train_idx='/data/train.idx', data_val='/data/val.rec', data_val_idx='/data/val.idx', disp_batches=20, dtype='float16', epoch_size=0, eval_offset=3, eval_period=4, force_tensor_core=1, fuse_bn_add_relu=1, fuse_bn_relu=1, gpus='5', image_shape='4,224,224', initializer='default', input_layout='NHWC', kv_store='horovod', label_smoothing=0.1, lars_eps=0, lars_eta=0.001, lazy_init_sanity=False, log='', logging_dir='logs', lr=6.0, lr_factor=0.1, lr_step_epochs='pow2', max_random_area=1.0, max_random_aspect_ratio=1.3333333333333333, min_random_area=0.05, min_random_aspect_ratio=0.75, model_prefix=None, mom=0.9, network='resnet-v1b-normconv-fl', num_classes=1000, num_epochs=72, num_examples=1281167, num_layers=50, optimizer='sgdwfastlars', pooling_layout='NHWC', profile_server_suffix='', profile_worker_suffix='', resize=256, save_period=1, seed=60252, separ_val=False, top_k=0, use_dali=True, verbose=0, warmup_epochs=5, warmup_strategy='linear', wd=0.0002)
[1,3]<stderr>:2019-10-21 20:44:08,748 Node[3] start with arguments Namespace(accuracy_threshold=0.759, batch_size=256, batchnorm_eps=1e-05, batchnorm_layout='NHWC', batchnorm_mom=0.9, bn_gamma_init0=False, conv_algo=1, conv_layout='NHWC', custom_bn_off=0, dali_cache_size=0, dali_nvjpeg_memory_padding=256, dali_prefetch_queue=8, dali_roi_decode=True, dali_threads=3, data_train='/data/train.rec', data_train_idx='/data/train.idx', data_val='/data/val.rec', data_val_idx='/data/val.idx', disp_batches=20, dtype='float16', epoch_size=0, eval_offset=3, eval_period=4, force_tensor_core=1, fuse_bn_add_relu=1, fuse_bn_relu=1, gpus='3', image_shape='4,224,224', initializer='default', input_layout='NHWC', kv_store='horovod', label_smoothing=0.1, lars_eps=0, lars_eta=0.001, lazy_init_sanity=False, log='', logging_dir='logs', lr=6.0, lr_factor=0.1, lr_step_epochs='pow2', max_random_area=1.0, max_random_aspect_ratio=1.3333333333333333, min_random_area=0.05, min_random_aspect_ratio=0.75, model_prefix=None, mom=0.9, network='resnet-v1b-normconv-fl', num_classes=1000, num_epochs=72, num_examples=1281167, num_layers=50, optimizer='sgdwfastlars', pooling_layout='NHWC', profile_server_suffix='', profile_worker_suffix='', resize=256, save_period=1, seed=33142, separ_val=False, top_k=0, use_dali=True, verbose=0, warmup_epochs=5, warmup_strategy='linear', wd=0.0002)
[1,4]<stderr>:2019-10-21 20:44:08,748 Node[4] start with arguments Namespace(accuracy_threshold=0.759, batch_size=256, batchnorm_eps=1e-05, batchnorm_layout='NHWC', batchnorm_mom=0.9, bn_gamma_init0=False, conv_algo=1, conv_layout='NHWC', custom_bn_off=0, dali_cache_size=0, dali_nvjpeg_memory_padding=256, dali_prefetch_queue=8, dali_roi_decode=True, dali_threads=3, data_train='/data/train.rec', data_train_idx='/data/train.idx', data_val='/data/val.rec', data_val_idx='/data/val.idx', disp_batches=20, dtype='float16', epoch_size=0, eval_offset=3, eval_period=4, force_tensor_core=1, fuse_bn_add_relu=1, fuse_bn_relu=1, gpus='4', image_shape='4,224,224', initializer='default', input_layout='NHWC', kv_store='horovod', label_smoothing=0.1, lars_eps=0, lars_eta=0.001, lazy_init_sanity=False, log='', logging_dir='logs', lr=6.0, lr_factor=0.1, lr_step_epochs='pow2', max_random_area=1.0, max_random_aspect_ratio=1.3333333333333333, min_random_area=0.05, min_random_aspect_ratio=0.75, model_prefix=None, mom=0.9, network='resnet-v1b-normconv-fl', num_classes=1000, num_epochs=72, num_examples=1281167, num_layers=50, optimizer='sgdwfastlars', pooling_layout='NHWC', profile_server_suffix='', profile_worker_suffix='', resize=256, save_period=1, seed=62968, separ_val=False, top_k=0, use_dali=True, verbose=0, warmup_epochs=5, warmup_strategy='linear', wd=0.0002)
[1,4]<stderr>:2019-10-21 20:44:11,472 Node[4] Already bound, ignoring bind()
[1,2]<stderr>:2019-10-21 20:44:11,482 Node[2] Already bound, ignoring bind()
[1,5]<stderr>:2019-10-21 20:44:11,551 Node[5] Already bound, ignoring bind()
[1,3]<stderr>:2019-10-21 20:44:11,621 Node[3] Already bound, ignoring bind()
[1,6]<stderr>:2019-10-21 20:44:11,621 Node[6] Already bound, ignoring bind()
[1,7]<stderr>:2019-10-21 20:44:11,685 Node[7] Already bound, ignoring bind()
[1,0]<stdout>::::MLL 1571690651.756 opt_base_learning_rate: {"value": 6.0, "metadata": {"file": "common/fit.py", "lineno": 651}}
[1,0]<stdout>::::MLL 1571690651.756 opt_learning_rate_warmup_epochs: {"value": 5, "metadata": {"file": "common/fit.py", "lineno": 652}}
[1,0]<stdout>::::MLL 1571690651.756 lars_opt_learning_rate_decay_steps: {"value": 41942, "metadata": {"file": "common/fit.py", "lineno": 662}}
[1,0]<stdout>::::MLL 1571690651.757 lars_opt_learning_rate_decay_poly_power: {"value": 2, "metadata": {"file": "common/fit.py", "lineno": 698}}
[1,0]<stdout>::::MLL 1571690651.757 lars_opt_end_learning_rate: {"value": 0.0001, "metadata": {"file": "common/fit.py", "lineno": 699}}
[1,0]<stderr>:2019-10-21 20:44:11,759 Node[0] Already bound, ignoring bind()
[1,0]<stdout>::::MLL 1571690651.757 opt_name: {"value": "lars", "metadata": {"file": "common/fit.py", "lineno": 1075}}
[1,0]<stdout>::::MLL 1571690651.758 lars_epsilon: {"value": 0, "metadata": {"file": "common/fit.py", "lineno": 1077}}
[1,0]<stdout>:WARNING: Log validation: Key "lars_opt_weight_decay" is not in known resnet keys.
[1,0]<stdout>::::MLL 1571690651.758 lars_opt_weight_decay: {"value": 0.0002, "metadata": {"file": "common/fit.py", "lineno": 1079}}
[1,0]<stdout>:using wd on conv0_weight
[1,0]<stdout>:skipping wd on bn0_gamma
[1,0]<stdout>:skipping wd on bn0_beta
[1,0]<stdout>:using wd on stage1_unit1_conv1_weight
[1,0]<stdout>:skipping wd on stage1_unit1_bn1_gamma
[1,0]<stdout>:skipping wd on stage1_unit1_bn1_beta
[1,0]<stdout>:using wd on stage1_unit1_conv2_weight
[1,0]<stdout>:skipping wd on stage1_unit1_bn2_gamma
[1,0]<stdout>:skipping wd on stage1_unit1_bn2_beta
[1,0]<stdout>:using wd on stage1_unit1_conv3_weight
[1,0]<stdout>:skipping wd on stage1_unit1_bn3_gamma
[1,0]<stdout>:skipping wd on stage1_unit1_bn3_beta
[1,0]<stdout>:using wd on stage1_unit1_conv1sc_weight
[1,0]<stdout>:skipping wd on stage1_unit1_bn_sc_gamma
[1,0]<stdout>:skipping wd on stage1_unit1_bn_sc_beta
[1,0]<stdout>:using wd on stage1_unit2_conv1_weight
[1,0]<stdout>:skipping wd on stage1_unit2_bn1_gamma
[1,0]<stdout>:skipping wd on stage1_unit2_bn1_beta
[1,0]<stdout>:using wd on stage1_unit2_conv2_weight
[1,0]<stdout>:skipping wd on stage1_unit2_bn2_gamma
[1,0]<stdout>:skipping wd on stage1_unit2_bn2_beta
[1,0]<stdout>:using wd on stage1_unit2_conv3_weight
[1,0]<stdout>:skipping wd on stage1_unit2_bn3_gamma
[1,0]<stdout>:skipping wd on stage1_unit2_bn3_beta
[1,0]<stdout>:using wd on stage1_unit3_conv1_weight
[1,0]<stdout>:skipping wd on stage1_unit3_bn1_gamma
[1,0]<stdout>:skipping wd on stage1_unit3_bn1_beta
[1,0]<stdout>:using wd on stage1_unit3_conv2_weight
[1,0]<stdout>:skipping wd on stage1_unit3_bn2_gamma
[1,0]<stdout>:skipping wd on stage1_unit3_bn2_beta
[1,0]<stdout>:using wd on stage1_unit3_conv3_weight
[1,0]<stdout>:skipping wd on stage1_unit3_bn3_gamma
[1,0]<stdout>:skipping wd on stage1_unit3_bn3_beta
[1,0]<stdout>:using wd on stage2_unit1_conv1_weight
[1,0]<stdout>:skipping wd on stage2_unit1_bn1_gamma
[1,0]<stdout>:skipping wd on stage2_unit1_bn1_beta
[1,0]<stdout>:using wd on stage2_unit1_conv2_weight
[1,0]<stdout>:skipping wd on stage2_unit1_bn2_gamma
[1,0]<stdout>:skipping wd on stage2_unit1_bn2_beta
[1,0]<stdout>:using wd on stage2_unit1_conv3_weight
[1,0]<stdout>:skipping wd on stage2_unit1_bn3_gamma
[1,0]<stdout>:skipping wd on stage2_unit1_bn3_beta
[1,0]<stdout>:using wd on stage2_unit1_conv1sc_weight
[1,0]<stdout>:skipping wd on stage2_unit1_bn_sc_gamma
[1,0]<stdout>:skipping wd on stage2_unit1_bn_sc_beta
[1,0]<stdout>:using wd on stage2_unit2_conv1_weight
[1,0]<stdout>:skipping wd on stage2_unit2_bn1_gamma
[1,0]<stdout>:skipping wd on stage2_unit2_bn1_beta
[1,0]<stdout>:using wd on stage2_unit2_conv2_weight
[1,0]<stdout>:skipping wd on stage2_unit2_bn2_gamma
[1,0]<stdout>:skipping wd on stage2_unit2_bn2_beta
[1,0]<stdout>:using wd on stage2_unit2_conv3_weight
[1,0]<stdout>:skipping wd on stage2_unit2_bn3_gamma
[1,0]<stdout>:skipping wd on stage2_unit2_bn3_beta
[1,0]<stdout>:using wd on stage2_unit3_conv1_weight
[1,0]<stdout>:skipping wd on stage2_unit3_bn1_gamma
[1,0]<stdout>:skipping wd on stage2_unit3_bn1_beta
[1,0]<stdout>:using wd on stage2_unit3_conv2_weight
[1,0]<stdout>:skipping wd on stage2_unit3_bn2_gamma
[1,0]<stdout>:skipping wd on stage2_unit3_bn2_beta
[1,0]<stdout>:using wd on stage2_unit3_conv3_weight
[1,0]<stdout>:skipping wd on stage2_unit3_bn3_gamma
[1,0]<stdout>:skipping wd on stage2_unit3_bn3_beta
[1,0]<stdout>:using wd on stage2_unit4_conv1_weight
[1,0]<stdout>:skipping wd on stage2_unit4_bn1_gamma
[1,0]<stdout>:skipping wd on stage2_unit4_bn1_beta
[1,0]<stdout>:using wd on stage2_unit4_conv2_weight
[1,0]<stdout>:skipping wd on stage2_unit4_bn2_gamma
[1,0]<stdout>:skipping wd on stage2_unit4_bn2_beta
[1,0]<stdout>:using wd on stage2_unit4_conv3_weight
[1,0]<stdout>:skipping wd on stage2_unit4_bn3_gamma
[1,0]<stdout>:skipping wd on stage2_unit4_bn3_beta
[1,0]<stdout>:using wd on stage3_unit1_conv1_weight
[1,0]<stdout>:skipping wd on stage3_unit1_bn1_gamma
[1,0]<stdout>:skipping wd on stage3_unit1_bn1_beta
[1,0]<stdout>:using wd on stage3_unit1_conv2_weight
[1,0]<stdout>:skipping wd on stage3_unit1_bn2_gamma
[1,0]<stdout>:skipping wd on stage3_unit1_bn2_beta
[1,0]<stdout>:using wd on stage3_unit1_conv3_weight
[1,0]<stdout>:skipping wd on stage3_unit1_bn3_gamma
[1,0]<stdout>:skipping wd on stage3_unit1_bn3_beta
[1,0]<stdout>:using wd on stage3_unit1_conv1sc_weight
[1,0]<stdout>:skipping wd on stage3_unit1_bn_sc_gamma
[1,0]<stdout>:skipping wd on stage3_unit1_bn_sc_beta
[1,0]<stdout>:using wd on stage3_unit2_conv1_weight
[1,0]<stdout>:skipping wd on stage3_unit2_bn1_gamma
[1,0]<stdout>:skipping wd on stage3_unit2_bn1_beta
[1,0]<stdout>:using wd on stage3_unit2_conv2_weight
[1,0]<stdout>:skipping wd on stage3_unit2_bn2_gamma
[1,0]<stdout>:skipping wd on stage3_unit2_bn2_beta
[1,0]<stdout>:using wd on stage3_unit2_conv3_weight
[1,0]<stdout>:skipping wd on stage3_unit2_bn3_gamma
[1,0]<stdout>:skipping wd on stage3_unit2_bn3_beta
[1,0]<stdout>:using wd on stage3_unit3_conv1_weight
[1,0]<stdout>:skipping wd on stage3_unit3_bn1_gamma
[1,0]<stdout>:skipping wd on stage3_unit3_bn1_beta
[1,0]<stdout>:using wd on stage3_unit3_conv2_weight
[1,0]<stdout>:skipping wd on stage3_unit3_bn2_gamma
[1,0]<stdout>:skipping wd on stage3_unit3_bn2_beta
[1,0]<stdout>:using wd on stage3_unit3_conv3_weight
[1,0]<stdout>:skipping wd on stage3_unit3_bn3_gamma
[1,0]<stdout>:skipping wd on stage3_unit3_bn3_beta
[1,0]<stdout>:using wd on stage3_unit4_conv1_weight
[1,0]<stdout>:skipping wd on stage3_unit4_bn1_gamma
[1,0]<stdout>:skipping wd on stage3_unit4_bn1_beta
[1,0]<stdout>:using wd on stage3_unit4_conv2_weight
[1,0]<stdout>:skipping wd on stage3_unit4_bn2_gamma
[1,0]<stdout>:skipping wd on stage3_unit4_bn2_beta
[1,0]<stdout>:using wd on stage3_unit4_conv3_weight
[1,0]<stdout>:skipping wd on stage3_unit4_bn3_gamma
[1,0]<stdout>:skipping wd on stage3_unit4_bn3_beta
[1,0]<stdout>:using wd on stage3_unit5_conv1_weight
[1,0]<stdout>:skipping wd on stage3_unit5_bn1_gamma
[1,0]<stdout>:skipping wd on stage3_unit5_bn1_beta
[1,0]<stdout>:using wd on stage3_unit5_conv2_weight
[1,0]<stdout>:skipping wd on stage3_unit5_bn2_gamma
[1,0]<stdout>:skipping wd on stage3_unit5_bn2_beta
[1,0]<stdout>:using wd on stage3_unit5_conv3_weight
[1,0]<stdout>:skipping wd on stage3_unit5_bn3_gamma
[1,0]<stdout>:skipping wd on stage3_unit5_bn3_beta
[1,0]<stdout>:using wd on stage3_unit6_conv1_weight
[1,0]<stdout>:skipping wd on stage3_unit6_bn1_gamma
[1,0]<stdout>:skipping wd on stage3_unit6_bn1_beta
[1,0]<stdout>:using wd on stage3_unit6_conv2_weight
[1,0]<stdout>:skipping wd on stage3_unit6_bn2_gamma
[1,0]<stdout>:skipping wd on stage3_unit6_bn2_beta
[1,0]<stdout>:using wd on stage3_unit6_conv3_weight
[1,0]<stdout>:skipping wd on stage3_unit6_bn3_gamma
[1,0]<stdout>:skipping wd on stage3_unit6_bn3_beta
[1,0]<stdout>:using wd on stage4_unit1_conv1_weight
[1,0]<stdout>:skipping wd on stage4_unit1_bn1_gamma
[1,0]<stdout>:skipping wd on stage4_unit1_bn1_beta
[1,0]<stdout>:using wd on stage4_unit1_conv2_weight
[1,0]<stdout>:skipping wd on stage4_unit1_bn2_gamma
[1,0]<stdout>:skipping wd on stage4_unit1_bn2_beta
[1,0]<stdout>:using wd on stage4_unit1_conv3_weight
[1,0]<stdout>:skipping wd on stage4_unit1_bn3_gamma
[1,0]<stdout>:skipping wd on stage4_unit1_bn3_beta
[1,0]<stdout>:using wd on stage4_unit1_conv1sc_weight
[1,0]<stdout>:skipping wd on stage4_unit1_bn_sc_gamma
[1,0]<stdout>:skipping wd on stage4_unit1_bn_sc_beta
[1,0]<stdout>:using wd on stage4_unit2_conv1_weight
[1,0]<stdout>:skipping wd on stage4_unit2_bn1_gamma
[1,0]<stdout>:skipping wd on stage4_unit2_bn1_beta
[1,0]<stdout>:using wd [1,0]<stdout>:on stage4_unit2_conv2_weight
[1,0]<stdout>:skipping wd on stage4_unit2_bn2_gamma
[1,0]<stdout>:skipping wd on stage4_unit2_bn2_beta
[1,0]<stdout>:using wd on stage4_unit2_conv3_weight
[1,0]<stdout>:skipping wd on stage4_unit2_bn3_gamma
[1,0]<stdout>:skipping wd on stage4_unit2_bn3_beta
[1,0]<stdout>:using wd on stage4_unit3_conv1_weight
[1,0]<stdout>:skipping wd on stage4_unit3_bn1_gamma
[1,0]<stdout>:skipping wd on stage4_unit3_bn1_beta
[1,0]<stdout>:using wd on stage4_unit3_conv2_weight
[1,0]<stdout>:skipping wd on stage4_unit3_bn2_gamma
[1,0]<stdout>:skipping wd on stage4_unit3_bn2_beta
[1,0]<stdout>:using wd on stage4_unit3_conv3_weight
[1,0]<stdout>:skipping wd on stage4_unit3_bn3_gamma
[1,0]<stdout>:skipping wd on stage4_unit3_bn3_beta
[1,0]<stdout>:using wd on fc1_weight
[1,0]<stdout>:using wd on fc1_bias
[1,0]<stdout>::::MLL 1571690651.759 global_batch_size: {"value": 2048, "metadata": {"file": "common/fit.py", "lineno": 1112}}
[1,0]<stdout>::::MLL 1571690651.760 block_start: {"value": null, "metadata": {"epoch_count": 4, "first_epoch_num": 1, "file": "common/fit.py", "lineno": 888}}
[1,0]<stdout>::::MLL 1571690651.760 epoch_start: {"value": null, "metadata": {"file": "common/fit.py", "lineno": 893, "epoch_num": 1}}
[1,1]<stderr>:2019-10-21 20:44:11,782 Node[1] Already bound, ignoring bind()
[1,0]<stderr>:/workspace/image_classification/common/fit.py:858: UserWarning: Parameters already initialized and force_init=False. init_params call ignored.
[1,0]<stderr>:  allow_missing=allow_missing, force_init=force_init)
[1,0]<stderr>:/workspace/image_classification/common/fit.py:860: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.00048828125 vs. 0.00390625). Is this intended?
[1,0]<stderr>:  optimizer_params=optimizer_params)
[1,0]<stderr>:2019-10-21 20:44:23,034 Node[0] Epoch[0] Batch [0-20]	Speed: 8552.24 samples/sec	accuracy=0.001116
[1,0]<stderr>:2019-10-21 20:44:27,668 Node[0] Epoch[0] Batch [20-40]	Speed: 8840.25 samples/sec	accuracy=0.000977
[1,0]<stderr>:2019-10-21 20:44:32,349 Node[0] Epoch[0] Batch [40-60]	Speed: 8751.28 samples/sec	accuracy=0.000977
[1,0]<stderr>:2019-10-21 20:44:37,006 Node[0] Epoch[0] Batch [60-80]	Speed: 8796.21 samples/sec	accuracy=0.001563
[1,0]<stderr>:2019-10-21 20:44:41,661 Node[0] Epoch[0] Batch [80-100]	Speed: 8800.92 samples/sec	accuracy=0.003125
[1,0]<stderr>:2019-10-21 20:44:46,232 Node[0] Epoch[0] Batch [100-120]	Speed: 8960.13 samples/sec	accuracy=0.003516
[1,0]<stderr>:2019-10-21 20:44:50,817 Node[0] Epoch[0] Batch [120-140]	Speed: 8934.59 samples/sec	accuracy=0.004102
[1,0]<stderr>:2019-10-21 20:44:55,432 Node[0] Epoch[0] Batch [140-160]	Speed: 8876.53 samples/sec	accuracy=0.006250
[1,0]<stderr>:2019-10-21 20:44:59,977 Node[0] Epoch[0] Batch [160-180]	Speed: 9014.18 samples/sec	accuracy=0.005469
[1,0]<stderr>:2019-10-21 20:45:04,628 Node[0] Epoch[0] Batch [180-200]	Speed: 8806.41 samples/sec	accuracy=0.008789
[1,0]<stderr>:2019-10-21 20:45:09,186 Node[0] Epoch[0] Batch [200-220]	Speed: 8988.52 samples/sec	accuracy=0.011719
[1,0]<stderr>:2019-10-21 20:45:13,855 Node[0] Epoch[0] Batch [220-240]	Speed: 8772.74 samples/sec	accuracy=0.013867
[1,0]<stderr>:2019-10-21 20:45:18,395 Node[0] Epoch[0] Batch [240-260]	Speed: 9022.49 samples/sec	accuracy=0.014844
[1,0]<stderr>:2019-10-21 20:45:22,969 Node[0] Epoch[0] Batch [260-280]	Speed: 8955.35 samples/sec	accuracy=0.021094
[1,0]<stderr>:2019-10-21 20:45:27,578 Node[0] Epoch[0] Batch [280-300]	Speed: 8888.44 samples/sec	accuracy=0.023242
[1,0]<stderr>:2019-10-21 20:45:32,215 Node[0] Epoch[0] Batch [300-320]	Speed: 8834.83 samples/sec	accuracy=0.022461
[1,0]<stderr>:2019-10-21 20:45:36,704 Node[0] Epoch[0] Batch [320-340]	Speed: 9124.19 samples/sec	accuracy=0.030859
[1,0]<stderr>:2019-10-21 20:45:41,373 Node[0] Epoch[0] Batch [340-360]	Speed: 8772.95 samples/sec	accuracy=0.039844
[1,0]<stderr>:2019-10-21 20:45:45,866 Node[0] Epoch[0] Batch [360-380]	Speed: 9118.16 samples/sec	accuracy=0.036133
[1,0]<stderr>:2019-10-21 20:45:50,450 Node[0] Epoch[0] Batch [380-400]	Speed: 8934.73 samples/sec	accuracy=0.042969
[1,0]<stderr>:2019-10-21 20:45:54,998 Node[0] Epoch[0] Batch [400-420]	Speed: 9008.63 samples/sec	accuracy=0.045508
[1,0]<stderr>:2019-10-21 20:45:59,553 Node[0] Epoch[0] Batch [420-440]	Speed: 8991.58 samples/sec	accuracy=0.051758
[1,0]<stderr>:2019-10-21 20:46:04,106 Node[0] Epoch[0] Batch [440-460]	Speed: 8997.23 samples/sec	accuracy=0.055273
[1,0]<stderr>:2019-10-21 20:46:08,610 Node[0] Epoch[0] Batch [460-480]	Speed: 9094.80 samples/sec	accuracy=0.058203
[1,0]<stderr>:2019-10-21 20:46:13,182 Node[0] Epoch[0] Batch [480-500]	Speed: 8959.93 samples/sec	accuracy=0.060156
[1,0]<stderr>:2019-10-21 20:46:17,738 Node[0] Epoch[0] Batch [500-520]	Speed: 8992.56 samples/sec	accuracy=0.064844
[1,0]<stderr>:2019-10-21 20:46:22,289 Node[0] Epoch[0] Batch [520-540]	Speed: 9002.71 samples/sec	accuracy=0.067773
[1,0]<stderr>:2019-10-21 20:46:26,852 Node[0] Epoch[0] Batch [540-560]	Speed: 8977.34 samples/sec	accuracy=0.073242
[1,0]<stderr>:2019-10-21 20:46:31,428 Node[0] Epoch[0] Batch [560-580]	Speed: 8952.93 samples/sec	accuracy=0.082617
[1,0]<stderr>:2019-10-21 20:46:35,962 Node[0] Epoch[0] Batch [580-600]	Speed: 9035.47 samples/sec	accuracy=0.085742
[1,0]<stderr>:2019-10-21 20:46:40,457 Node[0] Epoch[0] Batch [600-620]	Speed: 9113.26 samples/sec	accuracy=0.091406
[1,0]<stdout>::::MLL 1571690801.579 epoch_stop: {"value": null, "metadata": {"file": "common/fit.py", "lineno": 932, "epoch_num": 1}}
[1,0]<stderr>:2019-10-21 20:46:41,579 Node[0] Epoch[0] Time cost=149.819
[1,0]<stdout>::::MLL 1571690801.580 epoch_start: {"value": null, "metadata": {"file": "common/fit.py", "lineno": 893, "epoch_num": 2}}
[1,0]<stderr>:2019-10-21 20:46:46,422 Node[0] Epoch[1] Batch [0-20]	Speed: 8880.79 samples/sec	accuracy=0.091332
[1,0]<stderr>:2019-10-21 20:46:50,923 Node[0] Epoch[1] Batch [20-40]	Speed: 9100.89 samples/sec	accuracy=0.092383
[1,0]<stderr>:2019-10-21 20:46:55,442 Node[0] Epoch[1] Batch [40-60]	Speed: 9064.70 samples/sec	accuracy=0.101953
[1,0]<stderr>:2019-10-21 20:46:59,915 Node[0] Epoch[1] Batch [60-80]	Speed: 9159.64 samples/sec	accuracy=0.110937
[1,0]<stderr>:2019-10-21 20:47:04,407 Node[0] Epoch[1] Batch [80-100]	Speed: 9120.38 samples/sec	accuracy=0.105664
[1,0]<stderr>:2019-10-21 20:47:08,891 Node[0] Epoch[1] Batch [100-120]	Speed: 9137.18 samples/sec	accuracy=0.119531
[1,0]<stderr>:2019-10-21 20:47:13,384 Node[0] Epoch[1] Batch [120-140]	Speed: 9117.95 samples/sec	accuracy=0.124414
[1,0]<stderr>:2019-10-21 20:47:17,921 Node[0] Epoch[1] Batch [140-160]	Speed: 9028.32 samples/sec	accuracy=0.128320
[1,0]<stderr>:2019-10-21 20:47:22,397 Node[0] Epoch[1] Batch [160-180]	Speed: 9152.48 samples/sec	accuracy=0.135547
[1,0]<stderr>:2019-10-21 20:47:26,937 Node[0] Epoch[1] Batch [180-200]	Speed: 9023.55 samples/sec	accuracy=0.130664
[1,0]<stderr>:2019-10-21 20:47:31,432 Node[0] Epoch[1] Batch [200-220]	Speed: 9111.99 samples/sec	accuracy=0.148242
[1,0]<stderr>:2019-10-21 20:47:36,007 Node[0] Epoch[1] Batch [220-240]	Speed: 8954.35 samples/sec	accuracy=0.145313
[1,0]<stderr>:2019-10-21 20:47:40,507 Node[0] Epoch[1] Batch [240-260]	Speed: 9104.13 samples/sec	accuracy=0.152148
[1,0]<stderr>:2019-10-21 20:47:45,027 Node[0] Epoch[1] Batch [260-280]	Speed: 9061.80 samples/sec	accuracy=0.159766
[1,0]<stderr>:2019-10-21 20:47:49,553 Node[0] Epoch[1] Batch [280-300]	Speed: 9051.02 samples/sec	accuracy=0.152539
[1,0]<stderr>:2019-10-21 20:47:54,082 Node[0] Epoch[1] Batch [300-320]	Speed: 9044.69 samples/sec	accuracy=0.166602
[1,0]<stderr>:2019-10-21 20:47:58,581 Node[0] Epoch[1] Batch [320-340]	Speed: 9105.40 samples/sec	accuracy=0.164844
[1,0]<stderr>:2019-10-21 20:48:03,078 Node[0] Epoch[1] Batch [340-360]	Speed: 9110.20 samples/sec	accuracy=0.178125
[1,0]<stderr>:2019-10-21 20:48:07,660 Node[0] Epoch[1] Batch [360-380]	Speed: 8940.44 samples/sec	accuracy=0.174414
[1,0]<stderr>:2019-10-21 20:48:12,137 Node[0] Epoch[1] Batch [380-400]	Speed: 9150.85 samples/sec	accuracy=0.180078
[1,0]<stderr>:2019-10-21 20:48:16,639 Node[0] Epoch[1] Batch [400-420]	Speed: 9098.40 samples/sec	accuracy=0.185742
[1,0]<stderr>:2019-10-21 20:48:21,170 Node[0] Epoch[1] Batch [420-440]	Speed: 9041.82 samples/sec	accuracy=0.184570
[1,0]<stderr>:2019-10-21 20:48:25,660 Node[0] Epoch[1] Batch [440-460]	Speed: 9122.46 samples/sec	accuracy=0.198633
[1,0]<stderr>:2019-10-21 20:48:30,145 Node[0] Epoch[1] Batch [460-480]	Speed: 9133.62 samples/sec	accuracy=0.204102
[1,0]<stderr>:2019-10-21 20:48:34,659 Node[0] Epoch[1] Batch [480-500]	Speed: 9075.21 samples/sec	accuracy=0.200586
[1,0]<stderr>:2019-10-21 20:48:39,142 Node[0] Epoch[1] Batch [500-520]	Speed: 9137.06 samples/sec	accuracy=0.207227
[1,0]<stderr>:2019-10-21 20:48:43,645 Node[0] Epoch[1] Batch [520-540]	Speed: 9098.65 samples/sec	accuracy=0.211133
[1,0]<stderr>:2019-10-21 20:48:48,110 Node[0] Epoch[1] Batch [540-560]	Speed: 9173.45 samples/sec	accuracy=0.211719
[1,0]<stderr>:2019-10-21 20:48:52,591 Node[0] Epoch[1] Batch [560-580]	Speed: 9141.74 samples/sec	accuracy=0.212109
[1,0]<stderr>:2019-10-21 20:48:57,084 Node[0] Epoch[1] Batch [580-600]	Speed: 9116.78 samples/sec	accuracy=0.221680
[1,0]<stderr>:2019-10-21 20:49:01,613 Node[0] Epoch[1] Batch [600-620]	Speed: 9044.35 samples/sec	accuracy=0.235156
[1,0]<stdout>::::MLL 1571690942.736 epoch_stop: {"value": null, "metadata": {"file": "common/fit.py", "lineno": 932, "epoch_num": 2}}
[1,0]<stderr>:2019-10-21 20:49:02,736 Node[0] Epoch[1] Time cost=141.156
[1,0]<stdout>::::MLL 1571690942.737 epoch_start: {"value": null, "metadata": {"file": "common/fit.py", "lineno": 893, "epoch_num": 3}}
[1,0]<stderr>:2019-10-21 20:49:07,451 Node[0] Epoch[2] Batch [0-20]	Speed: 9134.67 samples/sec	accuracy=0.233073
[1,0]<stderr>:2019-10-21 20:49:11,914 Node[0] Epoch[2] Batch [20-40]	Speed: 9178.44 samples/sec	accuracy=0.231445
[1,0]<stderr>:2019-10-21 20:49:16,449 Node[0] Epoch[2] Batch [40-60]	Speed: 9031.91 samples/sec	accuracy=0.226758
[1,0]<stderr>:2019-10-21 20:49:20,941 Node[0] Epoch[2] Batch [60-80]	Speed: 9118.68 samples/sec	accuracy=0.251953
[1,0]<stderr>:2019-10-21 20:49:25,502 Node[0] Epoch[2] Batch [80-100]	Speed: 8982.70 samples/sec	accuracy=0.241992
[1,0]<stderr>:2019-10-21 20:49:30,036 Node[0] Epoch[2] Batch [100-120]	Speed: 9034.94 samples/sec	accuracy=0.257812
[1,0]<stderr>:2019-10-21 20:49:34,563 Node[0] Epoch[2] Batch [120-140]	Speed: 9048.06 samples/sec	accuracy=0.250000
[1,0]<stderr>:2019-10-21 20:49:39,036 Node[0] Epoch[2] Batch [140-160]	Speed: 9158.46 samples/sec	accuracy=0.254102
[1,0]<stderr>:2019-10-21 20:49:43,555 Node[0] Epoch[2] Batch [160-180]	Speed: 9063.27 samples/sec	accuracy=0.243945
[1,0]<stderr>:2019-10-21 20:49:48,053 Node[0] Epoch[2] Batch [180-200]	Speed: 9108.24 samples/sec	accuracy=0.261328
[1,0]<stderr>:2019-10-21 20:49:52,549 Node[0] Epoch[2] Batch [200-220]	Speed: 9110.39 samples/sec	accuracy=0.271094
[1,0]<stderr>:2019-10-21 20:49:57,026 Node[0] Epoch[2] Batch [220-240]	Speed: 9149.77 samples/sec	accuracy=0.266406
[1,0]<stderr>:2019-10-21 20:50:01,501 Node[0] Epoch[2] Batch [240-260]	Speed: 9154.69 samples/sec	accuracy=0.265039
[1,0]<stderr>:2019-10-21 20:50:06,004 Node[0] Epoch[2] Batch [260-280]	Speed: 9096.47 samples/sec	accuracy=0.276758
[1,0]<stderr>:2019-10-21 20:50:10,490 Node[0] Epoch[2] Batch [280-300]	Speed: 9130.92 samples/sec	accuracy=0.275586
[1,0]<stderr>:2019-10-21 20:50:14,991 Node[0] Epoch[2] Batch [300-320]	Speed: 9101.28 samples/sec	accuracy=0.272656
[1,0]<stderr>:2019-10-21 20:50:19,489 Node[0] Epoch[2] Batch [320-340]	Speed: 9106.88 samples/sec	accuracy=0.288477
[1,0]<stderr>:2019-10-21 20:50:23,972 Node[0] Epoch[2] Batch [340-360]	Speed: 9137.81 samples/sec	accuracy=0.275391
[1,0]<stderr>:2019-10-21 20:50:28,494 Node[0] Epoch[2] Batch [360-380]	Speed: 9058.81 samples/sec	accuracy=0.294141
[1,0]<stderr>:2019-10-21 20:50:32,971 Node[0] Epoch[2] Batch [380-400]	Speed: 9149.79 samples/sec	accuracy=0.286719
[1,0]<stderr>:2019-10-21 20:50:37,476 Node[0] Epoch[2] Batch [400-420]	Speed: 9094.05 samples/sec	accuracy=0.288281
[1,0]<stderr>:2019-10-21 20:50:41,971 Node[0] Epoch[2] Batch [420-440]	Speed: 9111.92 samples/sec	accuracy=0.302148
[1,0]<stderr>:2019-10-21 20:50:46,476 Node[0] Epoch[2] Batch [440-460]	Speed: 9093.30 samples/sec	accuracy=0.302734
[1,0]<stderr>:2019-10-21 20:50:50,973 Node[0] Epoch[2] Batch [460-480]	Speed: 9110.02 samples/sec	accuracy=0.308203
[1,0]<stderr>:2019-10-21 20:50:55,428 Node[0] Epoch[2] Batch [480-500]	Speed: 9195.38 samples/sec	accuracy=0.300781
[1,0]<stderr>:2019-10-21 20:50:59,923 Node[0] Epoch[2] Batch [500-520]	Speed: 9112.73 samples/sec	accuracy=0.298047
[1,0]<stderr>:2019-10-21 20:51:04,407 Node[0] Epoch[2] Batch [520-540]	Speed: 9134.21 samples/sec	accuracy=0.311133
[1,0]<stderr>:2019-10-21 20:51:08,893 Node[0] Epoch[2] Batch [540-560]	Speed: 9131.60 samples/sec	accuracy=0.306250
[1,0]<stderr>:2019-10-21 20:51:13,380 Node[0] Epoch[2] Batch [560-580]	Speed: 9130.64 samples/sec	accuracy=0.323633
[1,0]<stderr>:2019-10-21 20:51:17,882 Node[0] Epoch[2] Batch [580-600]	Speed: 9098.57 samples/sec	accuracy=0.311914
[1,0]<stderr>:2019-10-21 20:51:22,372 Node[0] Epoch[2] Batch [600-620]	Speed: 9123.07 samples/sec	accuracy=0.315625
[1,0]<stderr>:2019-10-21 20:51:23,252 Node[0] Epoch[2] Time cost=140.516
[1,0]<stdout>::::MLL 1571691083.253 epoch_stop: {"value": null, "metadata": {"file": "common/fit.py", "lineno": 932, "epoch_num": 3}}
[1,0]<stdout>::::MLL 1571691083.253 epoch_start: {"value": null, "metadata": {"file": "common/fit.py", "lineno": 893, "epoch_num": 4}}
[1,0]<stderr>:2019-10-21 20:51:27,996 Node[0] Epoch[3] Batch [0-20]	Speed: 9081.43 samples/sec	accuracy=0.312314
[1,0]<stderr>:2019-10-21 20:51:32,526 Node[0] Epoch[3] Batch [20-40]	Speed: 9041.41 samples/sec	accuracy=0.317773
[1,0]<stderr>:2019-10-21 20:51:37,029 Node[0] Epoch[3] Batch [40-60]	Speed: 9098.19 samples/sec	accuracy=0.327734
[1,0]<stderr>:2019-10-21 20:51:41,521 Node[0] Epoch[3] Batch [60-80]	Speed: 9118.42 samples/sec	accuracy=0.336914
[1,0]<stderr>:2019-10-21 20:51:46,003 Node[0] Epoch[3] Batch [80-100]	Speed: 9140.74 samples/sec	accuracy=0.325977
[1,0]<stderr>:2019-10-21 20:51:50,479 Node[0] Epoch[3] Batch [100-120]	Speed: 9152.02 samples/sec	accuracy=0.328906
[1,0]<stderr>:2019-10-21 20:51:54,962 Node[0] Epoch[3] Batch [120-140]	Speed: 9135.73 samples/sec	accuracy=0.329688
[1,0]<stderr>:2019-10-21 20:51:59,488 Node[0] Epoch[3] Batch [140-160]	Speed: 9050.87 samples/sec	accuracy=0.341602
[1,0]<stderr>:2019-10-21 20:52:04,048 Node[0] Epoch[3] Batch [160-180]	Speed: 8983.34 samples/sec	accuracy=0.327539
[1,0]<stderr>:2019-10-21 20:52:08,514 Node[0] Epoch[3] Batch [180-200]	Speed: 9173.23 samples/sec	accuracy=0.322656
[1,0]<stderr>:2019-10-21 20:52:13,036 Node[0] Epoch[3] Batch [200-220]	Speed: 9057.75 samples/sec	accuracy=0.328516
[1,0]<stderr>:2019-10-21 20:52:17,545 Node[0] Epoch[3] Batch [220-240]	Speed: 9085.15 samples/sec	accuracy=0.340234
[1,0]<stderr>:2019-10-21 20:52:21,999 Node[0] Epoch[3] Batch [240-260]	Speed: 9198.70 samples/sec	accuracy=0.337695
[1,0]<stderr>:2019-10-21 20:52:26,483 Node[0] Epoch[3] Batch [260-280]	Speed: 9135.48 samples/sec	accuracy=0.341602
[1,0]<stderr>:2019-10-21 20:52:31,012 Node[0] Epoch[3] Batch [280-300]	Speed: 9042.98 samples/sec	accuracy=0.351562
[1,0]<stderr>:2019-10-21 20:52:35,480 Node[0] Epoch[3] Batch [300-320]	Speed: 9168.78 samples/sec	accuracy=0.352148
[1,0]<stderr>:2019-10-21 20:52:40,015 Node[0] Epoch[3] Batch [320-340]	Speed: 9032.76 samples/sec	accuracy=0.337891
[1,0]<stderr>:2019-10-21 20:52:44,522 Node[0] Epoch[3] Batch [340-360]	Speed: 9088.49 samples/sec	accuracy=0.356250
[1,0]<stderr>:2019-10-21 20:52:49,019 Node[0] Epoch[3] Batch [360-380]	Speed: 9110.32 samples/sec	accuracy=0.346094
[1,0]<stderr>:2019-10-21 20:52:53,547 Node[0] Epoch[3] Batch [380-400]	Speed: 9045.37 samples/sec	accuracy=0.353125
[1,0]<stderr>:2019-10-21 20:52:58,032 Node[0] Epoch[3] Batch [400-420]	Speed: 9134.25 samples/sec	accuracy=0.349023
[1,0]<stderr>:2019-10-21 20:53:02,534 Node[0] Epoch[3] Batch [420-440]	Speed: 9099.24 samples/sec	accuracy=0.350000
[1,0]<stderr>:2019-10-21 20:53:07,080 Node[0] Epoch[3] Batch [440-460]	Speed: 9011.06 samples/sec	accuracy=0.338867
[1,0]<stderr>:2019-10-21 20:53:11,573 Node[0] Epoch[3] Batch [460-480]	Speed: 9118.87 samples/sec	accuracy=0.356445
[1,0]<stderr>:2019-10-21 20:53:16,087 Node[0] Epoch[3] Batch [480-500]	Speed: 9077.41 samples/sec	accuracy=0.356250
[1,0]<stderr>:2019-10-21 20:53:20,546 Node[0] Epoch[3] Batch [500-520]	Speed: 9186.41 samples/sec	accuracy=0.368555
[1,0]<stderr>:2019-10-21 20:53:25,031 Node[0] Epoch[3] Batch [520-540]	Speed: 9133.01 samples/sec	accuracy=0.351953
[1,0]<stderr>:2019-10-21 20:53:29,512 Node[0] Epoch[3] Batch [540-560]	Speed: 9142.00 samples/sec	accuracy=0.365234
[1,0]<stderr>:2019-10-21 20:53:34,036 Node[0] Epoch[3] Batch [560-580]	Speed: 9056.39 samples/sec	accuracy=0.348438
[1,0]<stderr>:2019-10-21 20:53:38,541 Node[0] Epoch[3] Batch [580-600]	Speed: 9094.48 samples/sec	accuracy=0.369727
[1,0]<stderr>:2019-10-21 20:53:43,009 Node[0] Epoch[3] Batch [600-620]	Speed: 9168.19 samples/sec	accuracy=0.370898
[1,3]<stderr>:/workspace/image_classification/common/fit.py:858: UserWarning: Parameters already initialized and force_init=False. init_params call ignored.
[1,3]<stderr>:  allow_missing=allow_missing, force_init=force_init)
[1,3]<stderr>:/workspace/image_classification/common/fit.py:860: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.00048828125 vs. 0.00390625). Is this intended?
[1,3]<stderr>:  optimizer_params=optimizer_params)
[1,3]<stderr>:2019-10-21 20:53:44,119 Node[3] DALI iterator does not support resetting while epoch is not finished. Ignoring...
[1,6]<stderr>:/workspace/image_classification/common/fit.py:858: UserWarning: Parameters already initialized and force_init=False. init_params call ignored.
[1,6]<stderr>:  allow_missing=allow_missing, force_init=force_init)
[1,6]<stderr>:/workspace/image_classification/common/fit.py:860: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.00048828125 vs. 0.00390625). Is this intended?
[1,6]<stderr>:  optimizer_params=optimizer_params)
[1,6]<stderr>:2019-10-21 20:53:44,120 Node[6] DALI iterator does not support resetting while epoch is not finished. Ignoring...
[1,5]<stderr>:/workspace/image_classification/common/fit.py:858: UserWarning: Parameters already initialized and force_init=False. init_params call ignored.
[1,5]<stderr>:  allow_missing=allow_missing, force_init=force_init)
[1,5]<stderr>:/workspace/image_classification/common/fit.py:860: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.00048828125 vs. 0.00390625). Is this intended?
[1,5]<stderr>:  optimizer_params=optimizer_params)
[1,5]<stderr>:2019-10-21 20:53:44,120 Node[5] DALI iterator does not support resetting while epoch is not finished. Ignoring...
[1,7]<stderr>:/workspace/image_classification/common/fit.py:858: UserWarning: Parameters already initialized and force_init=False. init_params call ignored.
[1,7]<stderr>:  allow_missing=allow_missing, force_init=force_init)
[1,7]<stderr>:/workspace/image_classification/common/fit.py:860: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.00048828125 vs. 0.00390625). Is this intended?
[1,7]<stderr>:  optimizer_params=optimizer_params)
[1,7]<stderr>:2019-10-21 20:53:44,120 Node[7] DALI iterator does not support resetting while epoch is not finished. Ignoring...
[1,4]<stderr>:/workspace/image_classification/common/fit.py:858: UserWarning: Parameters already initialized and force_init=False. init_params call ignored.
[1,4]<stderr>:  allow_missing=allow_missing, force_init=force_init)
[1,4]<stderr>:/workspace/image_classification/common/fit.py:860: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.00048828125 vs. 0.00390625). Is this intended?
[1,4]<stderr>:  optimizer_params=optimizer_params)
[1,4]<stderr>:2019-10-21 20:53:44,120 Node[4] DALI iterator does not support resetting while epoch is not finished. Ignoring...
[1,2]<stderr>:/workspace/image_classification/common/fit.py:858: UserWarning: Parameters already initialized and force_init=False. init_params call ignored.
[1,2]<stderr>:  allow_missing=allow_missing, force_init=force_init)
[1,2]<stderr>:/workspace/image_classification/common/fit.py:860: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.00048828125 vs. 0.00390625). Is this intended?
[1,2]<stderr>:  optimizer_params=optimizer_params)
[1,2]<stderr>:2019-10-21 20:53:44,121 Node[2] DALI iterator does not support resetting while epoch is not finished. Ignoring...
[1,1]<stderr>:/workspace/image_classification/common/fit.py:858: UserWarning: Parameters already initialized and force_init=False. init_params call ignored.
[1,1]<stderr>:  allow_missing=allow_missing, force_init=force_init)
[1,1]<stderr>:/workspace/image_classification/common/fit.py:860: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.00048828125 vs. 0.00390625). Is this intended?
[1,1]<stderr>:  optimizer_params=optimizer_params)
[1,1]<stderr>:2019-10-21 20:53:44,121 Node[1] DALI iterator does not support resetting while epoch is not finished. Ignoring...
[1,0]<stderr>:2019-10-21 20:53:44,122 Node[0] Epoch[3] Time cost=140.869
[1,0]<stderr>:2019-10-21 20:53:44,123 Node[0] DALI iterator does not support resetting while epoch is not finished. Ignoring...
[1,0]<stdout>::::MLL 1571691224.122 epoch_stop: {"value": null, "metadata": {"file": "common/fit.py", "lineno": 932, "epoch_num": 4}}
[1,0]<stdout>::::MLL 1571691224.123 eval_start: {"value": null, "metadata": {"file": "common/fit.py", "lineno": 956, "epoch_num": 4}}
[1,0]<stderr>:2019-10-21 20:53:48,148 Node[0] Epoch[3] Validation-accuracy=0.337120
[1,0]<stderr>:2019-10-21 20:53:48,148 Node[0] Epoch[3] Validation-correct-count=2107.000000
[1,0]<stderr>:2019-10-21 20:53:48,148 Node[0] Epoch[3] Validation-total-count=6250.000000
[1,0]<stdout>::::MLL 1571691228.212 eval_stop: {"value": null, "metadata": {"file": "common/fit.py", "lineno": 977, "epoch_num": 4}}
[1,0]<stdout>::::MLL 1571691228.212 eval_accuracy: {"value": 0.33294, "metadata": {"file": "common/fit.py", "lineno": 980, "epoch_num": 4}}
[1,0]<stdout>::::MLL 1571691228.212 block_stop: {"value": null, "metadata": {"file": "common/fit.py", "first_epoch_num": 1, "lineno": 984}}
[1,0]<stdout>::::MLL 1571691228.213 block_start: {"value": null, "metadata": {"epoch_count": 4, "first_epoch_num": 5, "file": "common/fit.py", "lineno": 997}}
[1,0]<stdout>::::MLL 1571691228.213 epoch_start: {"value": null, "metadata": {"file": "common/fit.py", "lineno": 893, "epoch_num": 5}}
[1,0]<stderr>:2019-10-21 20:53:53,570 Node[0] Epoch[4] Batch [0-20]	Speed: 7891.82 samples/sec	accuracy=0.364583
[1,0]<stderr>:2019-10-21 20:53:58,048 Node[0] Epoch[4] Batch [20-40]	Speed: 9148.02 samples/sec	accuracy=0.365820
[1,0]<stderr>:2019-10-21 20:54:02,526 Node[0] Epoch[4] Batch [40-60]	Speed: 9146.80 samples/sec	accuracy=0.360938
[1,0]<stderr>:2019-10-21 20:54:07,018 Node[0] Epoch[4] Batch [60-80]	Speed: 9118.86 samples/sec	accuracy=0.380273
[1,0]<stderr>:2019-10-21 20:54:11,522 Node[0] Epoch[4] Batch [80-100]	Speed: 9095.15 samples/sec	accuracy=0.378906
[1,0]<stderr>:2019-10-21 20:54:16,005 Node[0] Epoch[4] Batch [100-120]	Speed: 9137.38 samples/sec	accuracy=0.360547
[1,0]<stderr>:2019-10-21 20:54:20,491 Node[0] Epoch[4] Batch [120-140]	Speed: 9131.26 samples/sec	accuracy=0.366797
[1,0]<stderr>:2019-10-21 20:54:24,997 Node[0] Epoch[4] Batch [140-160]	Speed: 9092.48 samples/sec	accuracy=0.370508
[1,0]<stderr>:2019-10-21 20:54:29,474 Node[0] Epoch[4] Batch [160-180]	Speed: 9149.11 samples/sec	accuracy=0.373437
[1,0]<stderr>:2019-10-21 20:54:33,959 Node[0] Epoch[4] Batch [180-200]	Speed: 9133.25 samples/sec	accuracy=0.380859
[1,0]<stderr>:2019-10-21 20:54:38,453 Node[0] Epoch[4] Batch [200-220]	Speed: 9115.57 samples/sec	accuracy=0.367773
[1,0]<stderr>:2019-10-21 20:54:42,951 Node[0] Epoch[4] Batch [220-240]	Speed: 9107.45 samples/sec	accuracy=0.393164
[1,0]<stderr>:2019-10-21 20:54:47,425 Node[0] Epoch[4] Batch [240-260]	Speed: 9154.42 samples/sec	accuracy=0.380078
[1,0]<stderr>:2019-10-21 20:54:51,895 Node[0] Epoch[4] Batch [260-280]	Speed: 9164.74 samples/sec	accuracy=0.368164
[1,0]<stderr>:2019-10-21 20:54:56,431 Node[0] Epoch[4] Batch [280-300]	Speed: 9030.69 samples/sec	accuracy=0.379102
