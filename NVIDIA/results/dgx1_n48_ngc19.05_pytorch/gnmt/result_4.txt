Beginning trial 3 of 5
Gathering sys log on sc-sdgx-333
:::MLL 1560827982.535 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1560827982.535 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1560827982.536 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1560827982.537 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1560827982.538 submission_platform: {"value": "48xDGX-1 with V100", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1560827982.538 submission_entry: {"value": "{'hardware': 'DGX-1 with V100', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': 'Ubuntu 18.04.2 LTS / NVIDIA DGX Server 4.0.5', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.4-2.0.7', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '48', 'cpu': '2x Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz', 'num_cores': '40', 'num_vcpus': '80', 'accelerator': 'Tesla V100-SXM2-16GB', 'num_accelerators': '8', 'sys_mem_size': '503 GB', 'sys_storage_type': 'SATA SSD', 'sys_storage_size': '1x 7T + 1x 446.6G', 'cpu_accel_interconnect': 'QPI', 'network_card': 'Mellanox Technologies MT27700 Family [ConnectX-4]', 'num_network_cards': '4', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1560827982.539 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1560827982.540 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
:::MLL 1560828024.960 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.082 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.243 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.314 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.338 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.359 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.434 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.448 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.587 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.596 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.638 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.642 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.657 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.664 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.666 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.701 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.706 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.764 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.775 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.777 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.781 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.789 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.851 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.855 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.884 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.933 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.965 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.971 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828025.987 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828026.032 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828026.040 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828026.060 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828026.070 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828026.073 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828026.088 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828026.092 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828026.107 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828026.119 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828026.147 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828026.150 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828026.167 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828026.201 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828026.236 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828026.259 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828026.276 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828026.285 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828026.316 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1560828029.923 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node sc-sdgx-333
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-334
+ pids+=($!)
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ set +x
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-333
Launching on node sc-sdgx-335
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-334
+ pids+=($!)
+ set +x
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-333 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=0 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
Launching on node sc-sdgx-336
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-335
+ pids+=($!)
+ set +x
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-334 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=1 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
Launching on node sc-sdgx-342
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-336
+ pids+=($!)
+ set +x
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-335 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=2 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
Launching on node sc-sdgx-343
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-336 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=3 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-342
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-344
+ pids+=($!)
+ set +x
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
Launching on node sc-sdgx-345
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-343
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-342 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=4 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-344
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-346
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-343 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=5 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-344 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=6 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-345
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-352
+ pids+=($!)
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ set +x
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-346
Launching on node sc-sdgx-353
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-345 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=7 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-352
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-354
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-346 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=8 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-352 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=9 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-353
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-355
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-353 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=10 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ pids+=($!)
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-354
+ set +x
Launching on node sc-sdgx-360
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-354 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=11 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-355
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-361
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-355 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=12 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
+ pids+=($!)
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ set +x
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-360
Launching on node sc-sdgx-362
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ pids+=($!)
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-361
+ set +x
Launching on node sc-sdgx-363
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-360 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=13 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-361 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=14 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-362
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-364
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-362 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=15 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-369
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-363
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ pids+=($!)
+ set +x
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-364
Launching on node sc-sdgx-370
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-363 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=16 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ pids+=($!)
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-369
+ set +x
Launching on node sc-sdgx-371
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-364 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=17 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-370
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-369 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=18 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-372
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-370 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=19 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-371
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-377
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-371 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=20 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-372
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-372 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=21 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-378
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-377
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-379
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-377 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=22 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-378
+ pids+=($!)
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-378 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=23 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
+ set +x
Launching on node sc-sdgx-380
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-379
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-385
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-379 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=24 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-380
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-386
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-380 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=25 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-385
+ pids+=($!)
+ set +x
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-385 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=26 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
Launching on node sc-sdgx-387
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-386
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-388
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-386 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=27 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-387
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-389
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-387 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=28 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-388
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-394
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-388 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=29 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-389
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-395
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-389 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=30 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ pids+=($!)
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-394
+ set +x
Launching on node sc-sdgx-407
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-394 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=31 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-395
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-408
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-395 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=32 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
+ pids+=($!)
+ set +x
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
Launching on node sc-sdgx-414
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-407
+ pids+=($!)
+ set +x
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
Launching on node sc-sdgx-415
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-408
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-407 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=33 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-414
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-416
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-408 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=34 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-414 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=35 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ pids+=($!)
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-415
+ set +x
Launching on node sc-sdgx-417
+ pids+=($!)
+ set +x
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
Launching on node sc-sdgx-418
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-416
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-415 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=36 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-417
+ pids+=($!)
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-416 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=37 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
+ set +x
Launching on node sc-sdgx-423
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-417 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=38 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-418
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-424
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-418 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=39 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-423
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-425
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-423 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=40 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-424
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-426
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-424 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=41 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-425
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-427
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-425 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=42 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-426
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-433
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-426 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=43 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-427
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-434
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-427 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=44 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-433
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-435
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-433 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=45 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-434
+ pids+=($!)
+ set +x
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-434 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=46 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
+++ echo srun --mem=0 -N 1 -n 1 -w sc-sdgx-435
+ srun --mem=0 -N 1 -n 1 -w sc-sdgx-435 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=48 --node_rank=47 --master_addr=172.22.0.134 --master_port=4744' -e LR=4.5e-3 -e TRAIN_BATCH_SIZE=32 -e TEST_BATCH_SIZE=8 -e WARMUP_STEPS=200 -e REMAIN_STEPS=1924 -e DECAY_INTERVAL=241 -e TARGET=24.0 -e NUMEPOCHS=12 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    ' -e SLURM_JOB_ID=341759 -e SLURM_NTASKS_PER_NODE=8 -e SLURM_NNODES=48 cont_341759 ./run_and_time.sh
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=4 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=0 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=45 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=12 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=23 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=10 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=3 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=20 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=13 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=35 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=43 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=39 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=31 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=18 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=17 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=6 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=19 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=14 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=28 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=46 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=32 --master_addr=172.22.0.134 --master_port=4744
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.5e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=8
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1924
+ DECAY_INTERVAL=241
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=4 --master_addr=172.22.0.134 --master_port=4744'
+ echo 'running benchmark'
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=4 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=24 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=30 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=5 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=47 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=1 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=37 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=8 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=2 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=27 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=26 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=22 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=7 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=15 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=33 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=38 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=36 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=21 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=34 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=11 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=40 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=29 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=25 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=16 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=42 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=44 --master_addr=172.22.0.134 --master_port=4744
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=41 --master_addr=172.22.0.134 --master_port=4744
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.5e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=8
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1924
+ DECAY_INTERVAL=241
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=0 --master_addr=172.22.0.134 --master_port=4744'
running benchmark
+ echo 'running benchmark'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=0 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.5e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=8
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1924
+ DECAY_INTERVAL=241
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=45 --master_addr=172.22.0.134 --master_port=4744'
+ echo 'running benchmark'
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=45 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
Run vars: id 341759 gpus 8 mparams  --nnodes=48 --node_rank=9 --master_addr=172.22.0.134 --master_port=4744
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.5e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=8
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1924
+ DECAY_INTERVAL=241
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
running benchmark
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=13 --master_addr=172.22.0.134 --master_port=4744'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=13 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.5e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=8
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1924
+ DECAY_INTERVAL=241
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=20 --master_addr=172.22.0.134 --master_port=4744'
+ echo 'running benchmark'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=20 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
running benchmark
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.5e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=32
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=8
+ LR=4.5e-3
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=32
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=8
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ LR=4.5e-3
+ REMAIN_STEPS=1924
+ LR=4.5e-3
+ DECAY_INTERVAL=241
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=32
+ NUMEPOCHS=12
+ TRAIN_BATCH_SIZE=32
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ TEST_BATCH_SIZE=8
+ REMAIN_STEPS=1924
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=8
+ DECAY_INTERVAL=241
+ BIND_LAUNCH=1
+ REMAIN_STEPS=1924
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MATH=amp_fp16
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=241
+ REMAIN_STEPS=1924
+ [[ 1 -eq 1 ]]
+ NUMEPOCHS=12
+ TARGET=24.0
+ DECAY_INTERVAL=241
+ MAX_SEQ_LEN=75
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=23 --master_addr=172.22.0.134 --master_port=4744'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ TARGET=24.0
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ NUMEPOCHS=12
+ echo 'running benchmark'
+ [[ 1 -eq 1 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=12 --master_addr=172.22.0.134 --master_port=4744'
+ BIND_LAUNCH=1
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ MATH=amp_fp16
+ NUMEPOCHS=12
+ [[ 1 -eq 1 ]]
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=23 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=10 --master_addr=172.22.0.134 --master_port=4744'
+ echo 'running benchmark'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=10 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=12 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=3 --master_addr=172.22.0.134 --master_port=4744'
+ echo 'running benchmark'
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=3 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
running benchmark
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=4.5e-3
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=32
+ PREPROC_DATADIR=/preproc_data
+ LR=4.5e-3
+ TEST_BATCH_SIZE=8
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=32
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=8
+ LR=4.5e-3
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ REMAIN_STEPS=1924
+ TRAIN_BATCH_SIZE=32
+ DECAY_INTERVAL=241
+ REMAIN_STEPS=1924
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=8
+ LR=4.5e-3
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=32
+ TARGET=24.0
+ DECAY_INTERVAL=241
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=8
+ REMAIN_STEPS=1924
+ DATASET_DIR=/data
+ DECAY_INTERVAL=241
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ TARGET=24.0
+ NUMEPOCHS=12
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=12
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ BIND_LAUNCH=1
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ MATH=amp_fp16
+ REMAIN_STEPS=1924
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=241
+ BIND_LAUNCH=1
+ PREPROC_DATADIR=/preproc_data
+ MATH=amp_fp16
+ DATASET_DIR=/data
+ NUMEPOCHS=12
+ [[ 1 -eq 1 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ LR=4.5e-3
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=32
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=8
+ LR=4.5e-3
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1924
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=31 --master_addr=172.22.0.134 --master_port=4744'
+ BIND_LAUNCH=1
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ [[ 1 -eq 1 ]]
+ NUMEPOCHS=12
+ TRAIN_BATCH_SIZE=32
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=241
+ MATH=amp_fp16
+ TEST_BATCH_SIZE=8
+ [[ 1 -eq 1 ]]
+ BIND_LAUNCH=1
+ LR=4.5e-3
+ MATH=amp_fp16
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=31 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ TRAIN_BATCH_SIZE=32
+ [[ 1 -eq 1 ]]
+ WARMUP_STEPS=200
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=43 --master_addr=172.22.0.134 --master_port=4744'
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=39 --master_addr=172.22.0.134 --master_port=4744'
+ echo 'running benchmark'
+ TARGET=24.0
+ TEST_BATCH_SIZE=8
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1924
+ REMAIN_STEPS=1924
+ DECAY_INTERVAL=241
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=35 --master_addr=172.22.0.134 --master_port=4744'
+ NUMEPOCHS=12
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=43 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ DECAY_INTERVAL=241
+ echo 'running benchmark'
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ echo 'running benchmark'
+ BIND_LAUNCH=1
+ NUMEPOCHS=12
+ MATH=amp_fp16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ TARGET=24.0
+ BIND_LAUNCH=1
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=39 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ MAX_SEQ_LEN=75
+ MATH=amp_fp16
+ NUMEPOCHS=12
+ [[ 1 -eq 1 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ BIND_LAUNCH=1
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=35 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=18 --master_addr=172.22.0.134 --master_port=4744'
+ echo 'running benchmark'
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=6 --master_addr=172.22.0.134 --master_port=4744'
+ echo 'running benchmark'
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=17 --master_addr=172.22.0.134 --master_port=4744'
+ echo 'running benchmark'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=18 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
running benchmark
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=6 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=17 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.5e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=8
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1924
+ DECAY_INTERVAL=241
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=46 --master_addr=172.22.0.134 --master_port=4744'
+ echo 'running benchmark'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=46 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
running benchmark
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.5e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=8
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1924
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=241
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ TARGET=24.0
+ LR=4.5e-3
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=32
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=8
+ LR=4.5e-3
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=32
+ DATASET_DIR=/data
+ NUMEPOCHS=12
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ REMAIN_STEPS=1924
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ LR=4.5e-3
+ TEST_BATCH_SIZE=8
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ BIND_LAUNCH=1
+ DATASET_DIR=/data
+ MATH=amp_fp16
+ DECAY_INTERVAL=241
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=32
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.5e-3
+ LR=4.5e-3
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ [[ 1 -eq 1 ]]
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ DATASET_DIR=/data
+ LR=4.5e-3
+ TEST_BATCH_SIZE=8
+ TRAIN_BATCH_SIZE=32
+ REMAIN_STEPS=1924
+ TEST_BATCH_SIZE=8
+ TRAIN_BATCH_SIZE=32
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=8
+ REMAIN_STEPS=1924
+ DECAY_INTERVAL=241
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=32
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=24 --master_addr=172.22.0.134 --master_port=4744'
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=241
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1924
+ TEST_BATCH_SIZE=8
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ LR=4.5e-3
+ NUMEPOCHS=12
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=32
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ LR=4.5e-3
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=241
+ LR=4.5e-3
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ REMAIN_STEPS=1924
+ NUMEPOCHS=12
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=8
running benchmark
+ BIND_LAUNCH=1
+ TRAIN_BATCH_SIZE=32
+ NUMEPOCHS=12
+ TEST_BATCH_SIZE=8
+ REMAIN_STEPS=1924
+ TRAIN_BATCH_SIZE=32
+ DECAY_INTERVAL=241
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ TARGET=24.0
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=24 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MATH=amp_fp16
+ REMAIN_STEPS=1924
+ DATASET_DIR=/data
+ DECAY_INTERVAL=241
+ WARMUP_STEPS=200
+ TARGET=24.0
+ DECAY_INTERVAL=241
+ TEST_BATCH_SIZE=8
+ TARGET=24.0
+ WARMUP_STEPS=200
+ BIND_LAUNCH=1
+ MAX_SEQ_LEN=75
+ MATH=amp_fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ LR=4.5e-3
+ MAX_SEQ_LEN=75
+ [[ 1 -eq 1 ]]
+ NUMEPOCHS=12
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=32 --master_addr=172.22.0.134 --master_port=4744'
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ REMAIN_STEPS=1924
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=241
+ NUMEPOCHS=12
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1924
+ NUMEPOCHS=12
+ NUMEPOCHS=12
+ DECAY_INTERVAL=241
+ BIND_LAUNCH=1
+ TRAIN_BATCH_SIZE=32
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=32 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ MATH=amp_fp16
+ RESULTS_DIR=gnmt_wmt16
+ [[ 1 -eq 1 ]]
running benchmark
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=28 --master_addr=172.22.0.134 --master_port=4744'
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ TEST_BATCH_SIZE=8
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ BIND_LAUNCH=1
+ LR=4.5e-3
+ MATH=amp_fp16
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=32
+ TARGET=24.0
+ TEST_BATCH_SIZE=8
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ BIND_LAUNCH=1
+ [[ 1 -eq 1 ]]
+ [[ 1 -eq 1 ]]
+ WARMUP_STEPS=200
+ [[ 1 -eq 1 ]]
+ REMAIN_STEPS=1924
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=241
+ NUMEPOCHS=12
+ REMAIN_STEPS=1924
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
running benchmark
+ MATH=amp_fp16
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=2 --master_addr=172.22.0.134 --master_port=4744'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ BIND_LAUNCH=1
+ TARGET=24.0
+ MATH=amp_fp16
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=19 --master_addr=172.22.0.134 --master_port=4744'
+ DECAY_INTERVAL=241
+ BIND_LAUNCH=1
+ echo 'running benchmark'
running benchmark
+ TARGET=24.0
+ MATH=amp_fp16
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ MAX_SEQ_LEN=75
+ BIND_LAUNCH=1
running benchmark
+ MATH=amp_fp16
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=28 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ [[ 1 -eq 1 ]]
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=2 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ [[ 1 -eq 1 ]]
+ [[ 1 -eq 1 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ NUMEPOCHS=12
+ [[ 1 -eq 1 ]]
+ BIND_LAUNCH=1
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=37 --master_addr=172.22.0.134 --master_port=4744'
+ MATH=amp_fp16
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=14 --master_addr=172.22.0.134 --master_port=4744'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=19 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ echo 'running benchmark'
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=47 --master_addr=172.22.0.134 --master_port=4744'
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=8 --master_addr=172.22.0.134 --master_port=4744'
+ BIND_LAUNCH=1
+ echo 'running benchmark'
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=30 --master_addr=172.22.0.134 --master_port=4744'
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=5 --master_addr=172.22.0.134 --master_port=4744'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=37 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ echo 'running benchmark'
running benchmark
+ MATH=amp_fp16
running benchmark
running benchmark
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=5 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=14 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ [[ 1 -eq 1 ]]
+ echo 'running benchmark'
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=1 --master_addr=172.22.0.134 --master_port=4744'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=47 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ echo 'running benchmark'
+ echo 'running benchmark'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=8 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=30 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=1 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.5e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=8
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1924
+ DECAY_INTERVAL=241
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
running benchmark
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=7 --master_addr=172.22.0.134 --master_port=4744'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.5e-3
+ TRAIN_BATCH_SIZE=32
+ DATASET_DIR=/data
running benchmark
+ TEST_BATCH_SIZE=8
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1924
+ LR=4.5e-3
+ DECAY_INTERVAL=241
+ TRAIN_BATCH_SIZE=32
+ TARGET=24.0
+ TEST_BATCH_SIZE=8
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ NUMEPOCHS=12
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=7 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ REMAIN_STEPS=1924
+ BIND_LAUNCH=1
+ DECAY_INTERVAL=241
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=26 --master_addr=172.22.0.134 --master_port=4744'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=26 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ DATASET_DIR=/data
+ NUMEPOCHS=12
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ RESULTS_DIR=gnmt_wmt16
+ BIND_LAUNCH=1
+ LR=4.5e-3
+ MATH=amp_fp16
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=8
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1924
+ DECAY_INTERVAL=241
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ 1 -eq 1 ]]
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=27 --master_addr=172.22.0.134 --master_port=4744'
+ echo 'running benchmark'
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=22 --master_addr=172.22.0.134 --master_port=4744'
+ echo 'running benchmark'
running benchmark
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=27 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=22 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.5e-3
+ LR=4.5e-3
+ TRAIN_BATCH_SIZE=32
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ TEST_BATCH_SIZE=8
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=32
running benchmark
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1924
+ LR=4.5e-3
+ TEST_BATCH_SIZE=8
+ TRAIN_BATCH_SIZE=32
+ DECAY_INTERVAL=241
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=8
+ REMAIN_STEPS=1924
+ DATASET_DIR=/data
+ DECAY_INTERVAL=241
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=12
+ NUMEPOCHS=12
+ REMAIN_STEPS=1924
+ LR=4.5e-3
+ DECAY_INTERVAL=241
+ LR=4.5e-3
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ TRAIN_BATCH_SIZE=32
+ TRAIN_BATCH_SIZE=32
+ MAX_SEQ_LEN=75
+ BIND_LAUNCH=1
+ NUMEPOCHS=12
+ MATH=amp_fp16
+ BIND_LAUNCH=1
+ TEST_BATCH_SIZE=8
+ TEST_BATCH_SIZE=8
+ MATH=amp_fp16
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ [[ 1 -eq 1 ]]
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1924
+ DATASET_DIR=/data
+ [[ 1 -eq 1 ]]
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=33 --master_addr=172.22.0.134 --master_port=4744'
+ REMAIN_STEPS=1924
+ DECAY_INTERVAL=241
+ RESULTS_DIR=gnmt_wmt16
+ BIND_LAUNCH=1
+ LR=4.5e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=32
+ DECAY_INTERVAL=241
+ TARGET=24.0
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=40 --master_addr=172.22.0.134 --master_port=4744'
+ DATASET_DIR=/data
+ MATH=amp_fp16
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=8
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ LR=4.5e-3
running benchmark
+ TRAIN_BATCH_SIZE=32
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1924
+ TEST_BATCH_SIZE=8
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ [[ 1 -eq 1 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ echo 'running benchmark'
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ DECAY_INTERVAL=241
+ BIND_LAUNCH=1
+ WARMUP_STEPS=200
+ MATH=amp_fp16
+ LR=4.5e-3
+ TRAIN_BATCH_SIZE=32
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ TARGET=24.0
+ BIND_LAUNCH=1
+ MAX_SEQ_LEN=75
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=33 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
running benchmark
+ [[ 1 -eq 1 ]]
+ REMAIN_STEPS=1924
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=34 --master_addr=172.22.0.134 --master_port=4744'
+ DECAY_INTERVAL=241
+ TEST_BATCH_SIZE=8
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=40 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ MATH=amp_fp16
+ NUMEPOCHS=12
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ TARGET=24.0
+ BIND_LAUNCH=1
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ MATH=amp_fp16
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=36 --master_addr=172.22.0.134 --master_port=4744'
+ [[ 1 -eq 1 ]]
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1924
+ NUMEPOCHS=12
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=21 --master_addr=172.22.0.134 --master_port=4744'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=34 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ DECAY_INTERVAL=241
running benchmark
+ BIND_LAUNCH=1
+ LR=4.5e-3
+ MATH=amp_fp16
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=32
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=36 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ [[ 1 -eq 1 ]]
+ [[ 1 -eq 1 ]]
running benchmark
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=15 --master_addr=172.22.0.134 --master_port=4744'
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=8
+ NUMEPOCHS=12
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=38 --master_addr=172.22.0.134 --master_port=4744'
+ REMAIN_STEPS=1924
+ echo 'running benchmark'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=15 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ BIND_LAUNCH=1
+ DECAY_INTERVAL=241
+ MATH=amp_fp16
+ TARGET=24.0
+ [[ 1 -eq 1 ]]
running benchmark
+ MAX_SEQ_LEN=75
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=11 --master_addr=172.22.0.134 --master_port=4744'
+ NUMEPOCHS=12
+ echo 'running benchmark'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=21 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ BIND_LAUNCH=1
+ echo 'running benchmark'
+ MATH=amp_fp16
running benchmark
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=29 --master_addr=172.22.0.134 --master_port=4744'
+ echo 'running benchmark'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=38 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=11 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=29 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.5e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=32
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=8
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ LR=4.5e-3
+ REMAIN_STEPS=1924
+ TRAIN_BATCH_SIZE=32
+ DECAY_INTERVAL=241
+ TEST_BATCH_SIZE=8
+ TARGET=24.0
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1924
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ NUMEPOCHS=12
+ DECAY_INTERVAL=241
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ DATASET_DIR=/data
+ BIND_LAUNCH=1
+ PREPROC_DATADIR=/preproc_data
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.5e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=8
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1924
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=16 --master_addr=172.22.0.134 --master_port=4744'
+ DECAY_INTERVAL=241
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=42 --master_addr=172.22.0.134 --master_port=4744'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=16 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
running benchmark
+ echo 'running benchmark'
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=25 --master_addr=172.22.0.134 --master_port=4744'
+ echo 'running benchmark'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=42 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=25 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.5e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=8
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1924
+ DECAY_INTERVAL=241
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
running benchmark
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=41 --master_addr=172.22.0.134 --master_port=4744'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=41 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.5e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=8
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1924
+ DECAY_INTERVAL=241
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=44 --master_addr=172.22.0.134 --master_port=4744'
+ echo 'running benchmark'
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=44 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
STARTING TIMING RUN AT 2019-06-18 03:20:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.5e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=8
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1924
+ DECAY_INTERVAL=241
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 2    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --nnodes=48 --node_rank=9 --master_addr=172.22.0.134 --master_port=4744'
running benchmark
+ echo 'running benchmark'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=48 --node_rank=9 --master_addr=172.22.0.134 --master_port=4744 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 12 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 8 --optimizer FusedAdam --lr 4.5e-3 --warmup-steps 200 --remain-steps 1924 --decay-interval 241 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 2
:::MLL 1560828032.428 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.428 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.428 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.430 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.434 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.435 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.437 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.438 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.449 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.450 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.450 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.451 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.452 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.452 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.453 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.460 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.460 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.460 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.467 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.468 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.468 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.468 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.470 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.472 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.472 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.481 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.482 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.483 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.509 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.512 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.512 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.512 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.514 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.516 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.516 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.519 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.520 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.522 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.524 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.538 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.548 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.548 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.548 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.548 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.548 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.548 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.548 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.548 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.553 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.553 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.553 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.554 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.554 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.555 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.558 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.565 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.580 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.580 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.584 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.584 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.584 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.585 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.588 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.591 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.594 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.597 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.597 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.596 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.596 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.596 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.596 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.597 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.597 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.597 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.598 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.599 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.597 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.597 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.599 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.601 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.602 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.603 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.607 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.607 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.607 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.608 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.613 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.612 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.615 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.616 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.619 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.619 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.622 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.623 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.623 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.623 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.623 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.624 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.624 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.631 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.631 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.636 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.636 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.637 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.636 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.636 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.637 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.637 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.641 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.641 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.642 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.642 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.642 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.642 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.641 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.643 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.643 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.643 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.643 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.643 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.643 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.643 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.643 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.643 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.643 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.644 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.644 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.645 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.647 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.647 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.647 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.649 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.649 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.649 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.648 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.649 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.652 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.652 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.653 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.652 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.654 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.650 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.650 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.654 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.651 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.658 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.658 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.659 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.659 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.661 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.659 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.662 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.661 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.663 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.662 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.663 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.663 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.664 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.664 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.664 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.661 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.666 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.666 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.666 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.667 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.667 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.667 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.666 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.666 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.668 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.669 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.668 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.666 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.670 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.671 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.669 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.671 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.671 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.671 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.674 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.678 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.680 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.680 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.680 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.680 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.681 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.685 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.683 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.682 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.692 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.692 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.692 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.692 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.692 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.692 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.693 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.690 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.690 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.691 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.693 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.694 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.695 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.695 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.695 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.695 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.695 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.694 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.695 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.695 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.695 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.695 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.696 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.695 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.696 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.696 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.699 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.698 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.698 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.696 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.699 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.700 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.702 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.700 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.703 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.704 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.704 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.704 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.704 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.704 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.704 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.704 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.704 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.705 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.706 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.708 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.706 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.707 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.712 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.712 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.713 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.713 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.709 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.712 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.715 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.714 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.714 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.714 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.714 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.715 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.715 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.717 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.719 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.720 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.717 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.718 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.720 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.720 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.720 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.720 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.720 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.719 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.720 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.723 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.723 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.723 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.723 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.723 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.723 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.723 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.723 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.723 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.723 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.721 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.724 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.724 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.724 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.725 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.725 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.726 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.723 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.723 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.728 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.727 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.728 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.727 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.729 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.732 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.733 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.732 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.732 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.732 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.732 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.732 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.732 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.732 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.732 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.731 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.734 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.734 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.739 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.737 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.738 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.740 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.740 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.740 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.738 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.741 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.743 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.744 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.743 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.744 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.744 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.744 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.744 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.746 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.746 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.746 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.747 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.748 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.748 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.748 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.751 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.752 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.752 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.753 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.758 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.758 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.759 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.759 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.759 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.759 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.758 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.758 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.758 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.759 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.761 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.759 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.763 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.767 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.768 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.769 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.773 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.773 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.773 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.774 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.775 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.775 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.775 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.775 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.774 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.778 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.779 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.783 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.783 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.783 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.784 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.784 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.784 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.784 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.787 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.790 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.790 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.790 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.791 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.792 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.792 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.794 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.794 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.794 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.795 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.795 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.795 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.795 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.796 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.796 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.801 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.801 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.801 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.801 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.802 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.801 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.809 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.822 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560828032.823 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=32134259, apex_num_allreduce_streams=2, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=241, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=True, env=False, epochs=12, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.0045, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='once', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=1924, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=8, test_loader_workers=0, train_batch_size=32, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 2410388872
0: Worker 0 is using worker seed: 2004080225
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.0045}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.0045
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1560828056.015 opt_base_learning_rate: {"value": 0.0045, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1560828057.592 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1560828057.594 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1560828057.595 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1560828057.898 global_batch_size: {"value": 12288, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 1924, 'decay_interval': 241, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 1924
0: Scheduler decay interval: 241
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1560828057.901 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1560828057.901 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1560828057.901 opt_learning_rate_decay_interval: {"value": 241, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1560828057.902 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1560828057.902 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1560828057.902 opt_learning_rate_remain_steps: {"value": 1924, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1560828057.902 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1560828057.952 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1560828057.953 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Sampler for epoch 0 uses seed 2471006011
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/321]	Time 0.364 (0.364)	Data 2.66e-01 (2.66e-01)	Tok/s 3401 (3401)	Loss/tok 10.5709 (10.5709)	LR 4.500e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
0: TRAIN [0][10/321]	Time 0.052 (0.083)	Data 9.89e-05 (2.42e-02)	Tok/s 41517 (30110)	Loss/tok 9.7874 (10.2145)	LR 5.167e-05
0: TRAIN [0][20/321]	Time 0.077 (0.067)	Data 8.34e-05 (1.27e-02)	Tok/s 48630 (32469)	Loss/tok 9.2975 (9.7780)	LR 6.504e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
0: TRAIN [0][30/321]	Time 0.046 (0.061)	Data 6.56e-05 (8.64e-03)	Tok/s 47376 (33785)	Loss/tok 8.7899 (9.4800)	LR 8.002e-05
0: TRAIN [0][40/321]	Time 0.049 (0.058)	Data 6.48e-05 (6.55e-03)	Tok/s 44727 (34532)	Loss/tok 8.4251 (9.2615)	LR 1.007e-04
0: TRAIN [0][50/321]	Time 0.044 (0.056)	Data 8.58e-05 (5.28e-03)	Tok/s 29542 (35219)	Loss/tok 8.1155 (9.0692)	LR 1.268e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
0: TRAIN [0][60/321]	Time 0.049 (0.055)	Data 6.51e-05 (4.43e-03)	Tok/s 43811 (36333)	Loss/tok 8.0017 (8.9128)	LR 1.525e-04
0: TRAIN [0][70/321]	Time 0.044 (0.054)	Data 6.51e-05 (3.81e-03)	Tok/s 28718 (36487)	Loss/tok 8.0376 (8.7975)	LR 1.920e-04
0: TRAIN [0][80/321]	Time 0.049 (0.053)	Data 7.03e-05 (3.35e-03)	Tok/s 42009 (36609)	Loss/tok 7.9881 (8.7109)	LR 2.417e-04
0: TRAIN [0][90/321]	Time 0.055 (0.052)	Data 6.41e-05 (2.99e-03)	Tok/s 52837 (36514)	Loss/tok 8.1909 (8.6308)	LR 3.042e-04
0: TRAIN [0][100/321]	Time 0.056 (0.052)	Data 6.51e-05 (2.70e-03)	Tok/s 53583 (36821)	Loss/tok 8.0050 (8.5542)	LR 3.830e-04
0: TRAIN [0][110/321]	Time 0.050 (0.052)	Data 6.51e-05 (2.47e-03)	Tok/s 40995 (36565)	Loss/tok 7.8693 (8.4975)	LR 4.822e-04
0: TRAIN [0][120/321]	Time 0.055 (0.051)	Data 6.37e-05 (2.27e-03)	Tok/s 53946 (36383)	Loss/tok 7.8761 (8.4350)	LR 6.070e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
0: TRAIN [0][130/321]	Time 0.044 (0.051)	Data 8.42e-05 (2.10e-03)	Tok/s 27237 (36461)	Loss/tok 7.5312 (8.3761)	LR 7.468e-04
0: TRAIN [0][140/321]	Time 0.044 (0.051)	Data 6.60e-05 (1.96e-03)	Tok/s 29074 (36387)	Loss/tok 7.3520 (8.3196)	LR 9.402e-04
0: TRAIN [0][150/321]	Time 0.049 (0.050)	Data 8.46e-05 (1.83e-03)	Tok/s 43471 (36099)	Loss/tok 7.3014 (8.2595)	LR 1.184e-03
0: TRAIN [0][160/321]	Time 0.044 (0.050)	Data 9.87e-05 (1.73e-03)	Tok/s 28942 (35979)	Loss/tok 6.9254 (8.1961)	LR 1.490e-03
0: TRAIN [0][170/321]	Time 0.064 (0.050)	Data 8.34e-05 (1.63e-03)	Tok/s 57241 (36044)	Loss/tok 7.2443 (8.1225)	LR 1.876e-03
0: TRAIN [0][180/321]	Time 0.064 (0.050)	Data 8.39e-05 (1.54e-03)	Tok/s 59417 (36386)	Loss/tok 6.9522 (8.0433)	LR 2.362e-03
0: TRAIN [0][190/321]	Time 0.044 (0.050)	Data 7.39e-05 (1.47e-03)	Tok/s 28804 (36448)	Loss/tok 6.6023 (7.9763)	LR 2.973e-03
0: TRAIN [0][200/321]	Time 0.050 (0.050)	Data 6.72e-05 (1.40e-03)	Tok/s 43336 (36457)	Loss/tok 6.7225 (7.9081)	LR 3.743e-03
0: TRAIN [0][210/321]	Time 0.039 (0.050)	Data 8.32e-05 (1.34e-03)	Tok/s 17379 (36425)	Loss/tok 5.3712 (7.8389)	LR 4.500e-03
0: TRAIN [0][220/321]	Time 0.040 (0.050)	Data 6.58e-05 (1.28e-03)	Tok/s 17560 (36427)	Loss/tok 5.4533 (7.7642)	LR 4.500e-03
0: TRAIN [0][230/321]	Time 0.043 (0.050)	Data 6.51e-05 (1.23e-03)	Tok/s 29740 (36503)	Loss/tok 5.9557 (7.6857)	LR 4.500e-03
0: TRAIN [0][240/321]	Time 0.045 (0.049)	Data 6.63e-05 (1.18e-03)	Tok/s 29428 (36395)	Loss/tok 5.6030 (7.6173)	LR 4.500e-03
0: TRAIN [0][250/321]	Time 0.055 (0.049)	Data 8.46e-05 (1.14e-03)	Tok/s 53306 (36470)	Loss/tok 5.7664 (7.5349)	LR 4.500e-03
0: TRAIN [0][260/321]	Time 0.056 (0.049)	Data 8.44e-05 (1.10e-03)	Tok/s 51908 (36550)	Loss/tok 5.5770 (7.4561)	LR 4.500e-03
0: TRAIN [0][270/321]	Time 0.044 (0.049)	Data 8.99e-05 (1.06e-03)	Tok/s 31715 (36506)	Loss/tok 4.8646 (7.3822)	LR 4.500e-03
0: TRAIN [0][280/321]	Time 0.050 (0.049)	Data 8.58e-05 (1.02e-03)	Tok/s 42757 (36437)	Loss/tok 5.3847 (7.3110)	LR 4.500e-03
0: TRAIN [0][290/321]	Time 0.049 (0.049)	Data 8.32e-05 (9.91e-04)	Tok/s 40829 (36588)	Loss/tok 4.8560 (7.2234)	LR 4.500e-03
0: TRAIN [0][300/321]	Time 0.044 (0.049)	Data 6.79e-05 (9.61e-04)	Tok/s 29112 (36610)	Loss/tok 4.4173 (7.1450)	LR 4.500e-03
0: TRAIN [0][310/321]	Time 0.045 (0.049)	Data 9.18e-05 (9.33e-04)	Tok/s 29309 (36544)	Loss/tok 4.0100 (7.0700)	LR 4.500e-03
0: TRAIN [0][320/321]	Time 0.043 (0.049)	Data 3.05e-05 (9.06e-04)	Tok/s 30067 (36655)	Loss/tok 4.4402 (6.9909)	LR 4.500e-03
:::MLL 1560828073.729 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1560828073.730 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/1]	Time 0.467 (0.467)	Decoder iters 149.0 (149.0)	Tok/s 2632 (2632)
0: Running moses detokenizer
0: BLEU(score=5.913978044376471, counts=[19674, 5463, 1992, 738], totals=[56815, 53812, 50809, 47809], precisions=[34.62817917803397, 10.15201070393221, 3.92056525418725, 1.5436424104248154], bp=0.8707839786345266, sys_len=56815, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1560828074.538 eval_accuracy: {"value": 5.91, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1560828074.538 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 7.0027	Test BLEU: 5.91
0: Performance: Epoch: 0	Training: 14091897 Tok/s
0: Finished epoch 0
:::MLL 1560828074.539 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1560828074.539 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1560828074.540 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Sampler for epoch 1 uses seed 1100761589
0: TRAIN [1][0/321]	Time 0.325 (0.325)	Data 2.52e-01 (2.52e-01)	Tok/s 3763 (3763)	Loss/tok 4.2701 (4.2701)	LR 4.500e-03
0: TRAIN [1][10/321]	Time 0.039 (0.072)	Data 6.60e-05 (2.30e-02)	Tok/s 17882 (32580)	Loss/tok 3.5649 (4.4454)	LR 4.500e-03
0: TRAIN [1][20/321]	Time 0.039 (0.061)	Data 6.70e-05 (1.21e-02)	Tok/s 17764 (35722)	Loss/tok 3.7337 (4.5192)	LR 4.500e-03
0: TRAIN [1][30/321]	Time 0.049 (0.055)	Data 6.77e-05 (8.21e-03)	Tok/s 43536 (33756)	Loss/tok 4.2735 (4.4149)	LR 4.500e-03
0: TRAIN [1][40/321]	Time 0.044 (0.054)	Data 8.49e-05 (6.23e-03)	Tok/s 28979 (35073)	Loss/tok 3.9983 (4.3988)	LR 4.500e-03
0: TRAIN [1][50/321]	Time 0.042 (0.053)	Data 8.82e-05 (5.02e-03)	Tok/s 31030 (36100)	Loss/tok 4.1601 (4.3866)	LR 4.500e-03
0: TRAIN [1][60/321]	Time 0.049 (0.052)	Data 1.03e-04 (4.21e-03)	Tok/s 42312 (36578)	Loss/tok 4.3224 (4.3600)	LR 4.500e-03
0: TRAIN [1][70/321]	Time 0.039 (0.051)	Data 8.44e-05 (3.63e-03)	Tok/s 18495 (36050)	Loss/tok 3.6811 (4.3193)	LR 4.500e-03
0: TRAIN [1][80/321]	Time 0.043 (0.050)	Data 8.63e-05 (3.19e-03)	Tok/s 27939 (35886)	Loss/tok 3.6577 (4.2933)	LR 4.500e-03
0: TRAIN [1][90/321]	Time 0.043 (0.050)	Data 6.77e-05 (2.85e-03)	Tok/s 30409 (36568)	Loss/tok 4.0859 (4.2850)	LR 4.500e-03
0: TRAIN [1][100/321]	Time 0.043 (0.050)	Data 5.22e-05 (2.58e-03)	Tok/s 28864 (36708)	Loss/tok 3.3753 (4.2473)	LR 4.500e-03
0: TRAIN [1][110/321]	Time 0.042 (0.050)	Data 5.05e-05 (2.35e-03)	Tok/s 32728 (37619)	Loss/tok 3.5775 (4.2326)	LR 4.500e-03
0: TRAIN [1][120/321]	Time 0.043 (0.050)	Data 6.99e-05 (2.16e-03)	Tok/s 29790 (37613)	Loss/tok 3.6706 (4.2172)	LR 4.500e-03
0: TRAIN [1][130/321]	Time 0.049 (0.050)	Data 6.89e-05 (2.00e-03)	Tok/s 41441 (38240)	Loss/tok 3.9475 (4.1979)	LR 4.500e-03
0: TRAIN [1][140/321]	Time 0.043 (0.050)	Data 6.91e-05 (1.87e-03)	Tok/s 28383 (38135)	Loss/tok 3.9691 (4.1681)	LR 4.500e-03
0: TRAIN [1][150/321]	Time 0.063 (0.050)	Data 6.13e-05 (1.75e-03)	Tok/s 57525 (37828)	Loss/tok 4.5668 (4.1455)	LR 4.500e-03
0: TRAIN [1][160/321]	Time 0.043 (0.049)	Data 6.99e-05 (1.64e-03)	Tok/s 31262 (37265)	Loss/tok 3.8334 (4.1242)	LR 4.500e-03
0: TRAIN [1][170/321]	Time 0.055 (0.049)	Data 6.94e-05 (1.55e-03)	Tok/s 52730 (37744)	Loss/tok 3.9308 (4.1099)	LR 4.500e-03
0: TRAIN [1][180/321]	Time 0.049 (0.049)	Data 6.87e-05 (1.47e-03)	Tok/s 41266 (37461)	Loss/tok 3.6861 (4.0887)	LR 4.500e-03
0: TRAIN [1][190/321]	Time 0.049 (0.049)	Data 7.96e-05 (1.40e-03)	Tok/s 42766 (37647)	Loss/tok 3.8389 (4.0835)	LR 4.500e-03
0: TRAIN [1][200/321]	Time 0.048 (0.049)	Data 5.20e-05 (1.33e-03)	Tok/s 44800 (37617)	Loss/tok 3.5297 (4.0724)	LR 4.500e-03
0: TRAIN [1][210/321]	Time 0.044 (0.049)	Data 5.32e-05 (1.27e-03)	Tok/s 29042 (37673)	Loss/tok 3.3893 (4.0550)	LR 4.500e-03
0: TRAIN [1][220/321]	Time 0.056 (0.049)	Data 6.96e-05 (1.22e-03)	Tok/s 54510 (37788)	Loss/tok 3.5321 (4.0369)	LR 4.500e-03
0: TRAIN [1][230/321]	Time 0.044 (0.049)	Data 7.08e-05 (1.17e-03)	Tok/s 29913 (37618)	Loss/tok 3.2093 (4.0224)	LR 4.500e-03
0: TRAIN [1][240/321]	Time 0.049 (0.049)	Data 6.99e-05 (1.12e-03)	Tok/s 42699 (37508)	Loss/tok 3.6360 (4.0055)	LR 4.500e-03
0: TRAIN [1][250/321]	Time 0.049 (0.048)	Data 6.99e-05 (1.08e-03)	Tok/s 41873 (37333)	Loss/tok 3.5149 (3.9890)	LR 4.500e-03
0: TRAIN [1][260/321]	Time 0.039 (0.048)	Data 7.06e-05 (1.04e-03)	Tok/s 15903 (37132)	Loss/tok 2.7246 (3.9788)	LR 4.500e-03
0: TRAIN [1][270/321]	Time 0.055 (0.048)	Data 6.99e-05 (1.01e-03)	Tok/s 54665 (37094)	Loss/tok 3.7870 (3.9666)	LR 4.500e-03
0: TRAIN [1][280/321]	Time 0.038 (0.048)	Data 7.30e-05 (9.72e-04)	Tok/s 17765 (37090)	Loss/tok 2.6228 (3.9528)	LR 4.500e-03
0: TRAIN [1][290/321]	Time 0.049 (0.048)	Data 5.27e-05 (9.41e-04)	Tok/s 44497 (37264)	Loss/tok 3.1913 (3.9402)	LR 4.500e-03
0: TRAIN [1][300/321]	Time 0.043 (0.048)	Data 7.20e-05 (9.12e-04)	Tok/s 31111 (37064)	Loss/tok 3.5198 (3.9273)	LR 4.500e-03
0: TRAIN [1][310/321]	Time 0.049 (0.048)	Data 5.25e-05 (8.85e-04)	Tok/s 41201 (37140)	Loss/tok 3.5938 (3.9175)	LR 4.500e-03
0: TRAIN [1][320/321]	Time 0.055 (0.048)	Data 3.00e-05 (8.59e-04)	Tok/s 52156 (37197)	Loss/tok 4.4465 (3.9082)	LR 4.500e-03
:::MLL 1560828090.002 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1560828090.003 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/1]	Time 0.399 (0.399)	Decoder iters 127.0 (127.0)	Tok/s 3159 (3159)
0: Running moses detokenizer
0: BLEU(score=18.96532258199215, counts=[33677, 15107, 7934, 4283], totals=[65058, 62055, 59052, 56053], precisions=[51.764579298472135, 24.34453307549754, 13.435616067195015, 7.640982641428648], bp=1.0, sys_len=65058, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1560828090.594 eval_accuracy: {"value": 18.97, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1560828090.594 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.9060	Test BLEU: 18.97
0: Performance: Epoch: 1	Training: 14282841 Tok/s
0: Finished epoch 1
:::MLL 1560828090.595 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1560828090.595 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1560828090.596 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Sampler for epoch 2 uses seed 1258833962
0: TRAIN [2][0/321]	Time 0.314 (0.314)	Data 1.93e-01 (1.93e-01)	Tok/s 11953 (11953)	Loss/tok 3.8204 (3.8204)	LR 4.500e-03
0: TRAIN [2][10/321]	Time 0.043 (0.073)	Data 8.42e-05 (1.76e-02)	Tok/s 30364 (38743)	Loss/tok 3.4654 (3.7126)	LR 4.500e-03
0: TRAIN [2][20/321]	Time 0.056 (0.061)	Data 8.61e-05 (9.25e-03)	Tok/s 51966 (38334)	Loss/tok 3.7317 (3.6379)	LR 4.500e-03
0: TRAIN [2][30/321]	Time 0.049 (0.056)	Data 8.32e-05 (6.29e-03)	Tok/s 42968 (37150)	Loss/tok 3.4439 (3.5875)	LR 4.500e-03
0: TRAIN [2][40/321]	Time 0.049 (0.054)	Data 6.63e-05 (4.77e-03)	Tok/s 40960 (36559)	Loss/tok 3.7280 (3.5958)	LR 4.500e-03
0: TRAIN [2][50/321]	Time 0.043 (0.052)	Data 8.32e-05 (3.85e-03)	Tok/s 29947 (36578)	Loss/tok 3.3241 (3.5758)	LR 4.500e-03
0: TRAIN [2][60/321]	Time 0.056 (0.051)	Data 6.58e-05 (3.23e-03)	Tok/s 51451 (35675)	Loss/tok 3.5535 (3.5412)	LR 4.500e-03
0: TRAIN [2][70/321]	Time 0.056 (0.051)	Data 4.82e-05 (2.79e-03)	Tok/s 52284 (36436)	Loss/tok 3.5735 (3.5440)	LR 4.500e-03
0: TRAIN [2][80/321]	Time 0.043 (0.050)	Data 7.70e-05 (2.45e-03)	Tok/s 29106 (36708)	Loss/tok 3.2657 (3.5499)	LR 4.500e-03
0: TRAIN [2][90/321]	Time 0.043 (0.051)	Data 6.68e-05 (2.19e-03)	Tok/s 28737 (37493)	Loss/tok 3.0794 (3.5853)	LR 4.500e-03
0: TRAIN [2][100/321]	Time 0.049 (0.050)	Data 9.56e-05 (1.98e-03)	Tok/s 42439 (36847)	Loss/tok 3.2335 (3.5606)	LR 4.500e-03
0: TRAIN [2][110/321]	Time 0.043 (0.050)	Data 8.25e-05 (1.81e-03)	Tok/s 31133 (37026)	Loss/tok 3.3518 (3.5542)	LR 4.500e-03
0: TRAIN [2][120/321]	Time 0.043 (0.050)	Data 8.34e-05 (1.67e-03)	Tok/s 29577 (37208)	Loss/tok 3.2321 (3.5473)	LR 4.500e-03
0: TRAIN [2][130/321]	Time 0.042 (0.049)	Data 6.63e-05 (1.55e-03)	Tok/s 28722 (36987)	Loss/tok 2.9041 (3.5435)	LR 4.500e-03
0: TRAIN [2][140/321]	Time 0.055 (0.049)	Data 8.46e-05 (1.44e-03)	Tok/s 52669 (36904)	Loss/tok 3.6222 (3.5372)	LR 4.500e-03
0: TRAIN [2][150/321]	Time 0.049 (0.049)	Data 8.49e-05 (1.35e-03)	Tok/s 41969 (37194)	Loss/tok 3.3699 (3.5402)	LR 4.500e-03
0: TRAIN [2][160/321]	Time 0.050 (0.049)	Data 6.99e-05 (1.27e-03)	Tok/s 43679 (37034)	Loss/tok 3.4263 (3.5321)	LR 4.500e-03
0: TRAIN [2][170/321]	Time 0.055 (0.049)	Data 7.34e-05 (1.20e-03)	Tok/s 23165 (36981)	Loss/tok 3.3798 (3.5266)	LR 4.500e-03
0: TRAIN [2][180/321]	Time 0.055 (0.049)	Data 6.63e-05 (1.14e-03)	Tok/s 53510 (36738)	Loss/tok 3.5498 (3.5165)	LR 4.500e-03
0: TRAIN [2][190/321]	Time 0.049 (0.048)	Data 8.37e-05 (1.09e-03)	Tok/s 42410 (36863)	Loss/tok 3.2421 (3.5150)	LR 4.500e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
0: TRAIN [2][200/321]	Time 0.043 (0.048)	Data 8.63e-05 (1.04e-03)	Tok/s 30111 (36936)	Loss/tok 3.3852 (3.5190)	LR 4.500e-03
0: TRAIN [2][210/321]	Time 0.043 (0.048)	Data 8.70e-05 (9.91e-04)	Tok/s 30981 (36935)	Loss/tok 3.2757 (3.5151)	LR 4.500e-03
0: TRAIN [2][220/321]	Time 0.049 (0.049)	Data 7.20e-05 (9.49e-04)	Tok/s 40895 (37239)	Loss/tok 3.8765 (3.5191)	LR 4.500e-03
0: TRAIN [2][230/321]	Time 0.049 (0.049)	Data 9.75e-05 (9.12e-04)	Tok/s 42256 (37514)	Loss/tok 3.8660 (3.5201)	LR 4.500e-03
0: TRAIN [2][240/321]	Time 0.049 (0.049)	Data 8.46e-05 (8.78e-04)	Tok/s 42261 (37465)	Loss/tok 3.6957 (3.5125)	LR 4.500e-03
0: TRAIN [2][250/321]	Time 0.056 (0.048)	Data 8.54e-05 (8.46e-04)	Tok/s 52533 (37522)	Loss/tok 3.6082 (3.5103)	LR 4.500e-03
0: TRAIN [2][260/321]	Time 0.044 (0.048)	Data 6.63e-05 (8.16e-04)	Tok/s 31642 (37278)	Loss/tok 2.9554 (3.5042)	LR 4.500e-03
0: TRAIN [2][270/321]	Time 0.049 (0.048)	Data 8.37e-05 (7.90e-04)	Tok/s 42746 (37277)	Loss/tok 3.4726 (3.5046)	LR 4.500e-03
0: TRAIN [2][280/321]	Time 0.064 (0.048)	Data 9.61e-05 (7.65e-04)	Tok/s 59609 (37450)	Loss/tok 3.8435 (3.5060)	LR 4.500e-03
0: TRAIN [2][290/321]	Time 0.064 (0.048)	Data 6.89e-05 (7.41e-04)	Tok/s 57882 (37220)	Loss/tok 3.6359 (3.4996)	LR 4.500e-03
0: TRAIN [2][300/321]	Time 0.049 (0.048)	Data 8.37e-05 (7.19e-04)	Tok/s 40777 (37225)	Loss/tok 3.1632 (3.5003)	LR 4.500e-03
0: TRAIN [2][310/321]	Time 0.038 (0.048)	Data 1.44e-04 (6.99e-04)	Tok/s 18306 (37198)	Loss/tok 2.7128 (3.5001)	LR 4.500e-03
0: TRAIN [2][320/321]	Time 0.044 (0.048)	Data 3.19e-05 (6.80e-04)	Tok/s 29857 (37052)	Loss/tok 3.3287 (3.4931)	LR 4.500e-03
:::MLL 1560828106.047 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1560828106.047 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/1]	Time 0.462 (0.462)	Decoder iters 149.0 (149.0)	Tok/s 2899 (2899)
0: Running moses detokenizer
0: BLEU(score=20.857469392642194, counts=[34600, 16266, 8825, 4925], totals=[62683, 59680, 56677, 53680], precisions=[55.19837914586092, 27.255361930294907, 15.570690050637825, 9.174739195230998], bp=0.9687052387730506, sys_len=62683, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1560828106.634 eval_accuracy: {"value": 20.86, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1560828106.635 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.4778	Test BLEU: 20.86
0: Performance: Epoch: 2	Training: 14244461 Tok/s
0: Finished epoch 2
:::MLL 1560828106.636 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1560828106.636 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1560828106.636 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Sampler for epoch 3 uses seed 863836905
0: TRAIN [3][0/321]	Time 0.245 (0.245)	Data 1.95e-01 (1.95e-01)	Tok/s 5555 (5555)	Loss/tok 3.0715 (3.0715)	LR 4.500e-03
0: TRAIN [3][10/321]	Time 0.043 (0.067)	Data 6.60e-05 (1.77e-02)	Tok/s 29119 (36514)	Loss/tok 3.1702 (3.3236)	LR 4.500e-03
0: TRAIN [3][20/321]	Time 0.043 (0.057)	Data 6.77e-05 (9.33e-03)	Tok/s 28315 (35672)	Loss/tok 3.3297 (3.3071)	LR 4.500e-03
0: TRAIN [3][30/321]	Time 0.043 (0.054)	Data 8.37e-05 (6.34e-03)	Tok/s 29184 (36946)	Loss/tok 3.1321 (3.3464)	LR 4.500e-03
0: TRAIN [3][40/321]	Time 0.049 (0.052)	Data 6.56e-05 (4.81e-03)	Tok/s 41905 (35665)	Loss/tok 3.4845 (3.3336)	LR 4.500e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
0: TRAIN [3][50/321]	Time 0.055 (0.051)	Data 9.51e-05 (3.88e-03)	Tok/s 53145 (35440)	Loss/tok 3.5099 (3.3367)	LR 4.500e-03
0: TRAIN [3][60/321]	Time 0.056 (0.050)	Data 6.53e-05 (3.26e-03)	Tok/s 51153 (36163)	Loss/tok 3.4945 (3.3497)	LR 4.500e-03
0: TRAIN [3][70/321]	Time 0.043 (0.050)	Data 7.72e-05 (2.81e-03)	Tok/s 30073 (36353)	Loss/tok 3.0562 (3.3437)	LR 4.500e-03
0: TRAIN [3][80/321]	Time 0.039 (0.049)	Data 8.82e-05 (2.47e-03)	Tok/s 17255 (36154)	Loss/tok 2.8251 (3.3337)	LR 4.500e-03
0: TRAIN [3][90/321]	Time 0.064 (0.049)	Data 8.37e-05 (2.21e-03)	Tok/s 60491 (35781)	Loss/tok 3.5406 (3.3297)	LR 4.500e-03
0: TRAIN [3][100/321]	Time 0.043 (0.049)	Data 8.39e-05 (2.00e-03)	Tok/s 29463 (36368)	Loss/tok 3.3301 (3.3456)	LR 4.500e-03
0: TRAIN [3][110/321]	Time 0.049 (0.049)	Data 8.13e-05 (1.82e-03)	Tok/s 43756 (36416)	Loss/tok 3.3364 (3.3418)	LR 4.500e-03
0: TRAIN [3][120/321]	Time 0.049 (0.049)	Data 9.35e-05 (1.68e-03)	Tok/s 42621 (36641)	Loss/tok 3.6075 (3.3550)	LR 4.500e-03
0: TRAIN [3][130/321]	Time 0.049 (0.049)	Data 5.67e-05 (1.56e-03)	Tok/s 42247 (36871)	Loss/tok 3.8256 (3.3536)	LR 4.500e-03
0: TRAIN [3][140/321]	Time 0.049 (0.049)	Data 8.56e-05 (1.45e-03)	Tok/s 41208 (36916)	Loss/tok 3.5483 (3.3514)	LR 4.500e-03
0: TRAIN [3][150/321]	Time 0.042 (0.048)	Data 9.08e-05 (1.36e-03)	Tok/s 30143 (36803)	Loss/tok 3.0272 (3.3441)	LR 4.500e-03
0: TRAIN [3][160/321]	Time 0.048 (0.048)	Data 6.58e-05 (1.28e-03)	Tok/s 42844 (37067)	Loss/tok 3.1864 (3.3425)	LR 4.500e-03
0: TRAIN [3][170/321]	Time 0.039 (0.048)	Data 8.96e-05 (1.21e-03)	Tok/s 17001 (36790)	Loss/tok 2.8387 (3.3461)	LR 4.500e-03
0: TRAIN [3][180/321]	Time 0.056 (0.048)	Data 8.25e-05 (1.15e-03)	Tok/s 53231 (36931)	Loss/tok 3.1278 (3.3389)	LR 4.500e-03
0: TRAIN [3][190/321]	Time 0.043 (0.048)	Data 6.58e-05 (1.09e-03)	Tok/s 29074 (37069)	Loss/tok 3.3230 (3.3411)	LR 4.500e-03
0: TRAIN [3][200/321]	Time 0.043 (0.048)	Data 8.30e-05 (1.04e-03)	Tok/s 32228 (37101)	Loss/tok 3.1997 (3.3421)	LR 4.500e-03
0: TRAIN [3][210/321]	Time 0.049 (0.048)	Data 6.58e-05 (9.95e-04)	Tok/s 43446 (36818)	Loss/tok 3.0452 (3.3326)	LR 4.500e-03
0: TRAIN [3][220/321]	Time 0.043 (0.048)	Data 8.56e-05 (9.53e-04)	Tok/s 31189 (36704)	Loss/tok 3.0905 (3.3293)	LR 4.500e-03
0: TRAIN [3][230/321]	Time 0.039 (0.048)	Data 6.56e-05 (9.15e-04)	Tok/s 16060 (36906)	Loss/tok 2.4402 (3.3329)	LR 4.500e-03
0: TRAIN [3][240/321]	Time 0.050 (0.048)	Data 6.58e-05 (8.81e-04)	Tok/s 42965 (36921)	Loss/tok 3.4455 (3.3361)	LR 4.500e-03
0: TRAIN [3][250/321]	Time 0.043 (0.048)	Data 6.60e-05 (8.48e-04)	Tok/s 29924 (37006)	Loss/tok 2.9370 (3.3351)	LR 4.500e-03
0: TRAIN [3][260/321]	Time 0.043 (0.048)	Data 7.01e-05 (8.19e-04)	Tok/s 30381 (36789)	Loss/tok 3.1415 (3.3288)	LR 4.500e-03
0: TRAIN [3][270/321]	Time 0.044 (0.048)	Data 6.68e-05 (7.91e-04)	Tok/s 29740 (36793)	Loss/tok 3.0324 (3.3253)	LR 4.500e-03
0: TRAIN [3][280/321]	Time 0.044 (0.048)	Data 8.49e-05 (7.66e-04)	Tok/s 29089 (36895)	Loss/tok 3.0020 (3.3278)	LR 4.500e-03
0: TRAIN [3][290/321]	Time 0.055 (0.048)	Data 7.80e-05 (7.42e-04)	Tok/s 51524 (36986)	Loss/tok 3.8020 (3.3258)	LR 4.500e-03
0: TRAIN [3][300/321]	Time 0.043 (0.048)	Data 6.63e-05 (7.20e-04)	Tok/s 28706 (36828)	Loss/tok 3.2517 (3.3232)	LR 4.500e-03
0: TRAIN [3][310/321]	Time 0.043 (0.048)	Data 6.68e-05 (6.99e-04)	Tok/s 28889 (36922)	Loss/tok 3.0725 (3.3252)	LR 4.500e-03
0: TRAIN [3][320/321]	Time 0.055 (0.048)	Data 2.98e-05 (6.80e-04)	Tok/s 53165 (37041)	Loss/tok 3.5266 (3.3276)	LR 4.500e-03
:::MLL 1560828122.068 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 524}}
:::MLL 1560828122.068 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [3][0/1]	Time 0.425 (0.425)	Decoder iters 137.0 (137.0)	Tok/s 3246 (3246)
0: Running moses detokenizer
0: BLEU(score=22.141002301867097, counts=[35748, 17197, 9426, 5394], totals=[64318, 61315, 58312, 55313], precisions=[55.58008644547405, 28.046970561852728, 16.164768829743448, 9.751776255129897], bp=0.9944493685242773, sys_len=64318, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1560828122.611 eval_accuracy: {"value": 22.14, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 535}}
:::MLL 1560828122.611 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 3	Training Loss: 3.3390	Test BLEU: 22.14
0: Performance: Epoch: 3	Training: 14245286 Tok/s
0: Finished epoch 3
:::MLL 1560828122.612 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 557}}
:::MLL 1560828122.612 block_start: {"value": null, "metadata": {"first_epoch_num": 5, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1560828122.613 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 514}}
0: Starting epoch 4
0: Sampler for epoch 4 uses seed 1682458435
0: TRAIN [4][0/321]	Time 0.246 (0.246)	Data 1.93e-01 (1.93e-01)	Tok/s 5211 (5211)	Loss/tok 2.8689 (2.8689)	LR 4.500e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
0: TRAIN [4][10/321]	Time 0.049 (0.065)	Data 8.68e-05 (1.76e-02)	Tok/s 42266 (36105)	Loss/tok 3.7202 (3.1831)	LR 4.500e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
0: TRAIN [4][20/321]	Time 0.045 (0.057)	Data 9.42e-05 (9.26e-03)	Tok/s 46654 (37822)	Loss/tok 2.8858 (3.2588)	LR 4.500e-03
0: TRAIN [4][30/321]	Time 0.055 (0.055)	Data 6.60e-05 (6.30e-03)	Tok/s 52427 (38658)	Loss/tok 3.3393 (3.2528)	LR 4.500e-03
0: TRAIN [4][40/321]	Time 0.038 (0.052)	Data 8.46e-05 (4.78e-03)	Tok/s 17270 (37723)	Loss/tok 2.8840 (3.2450)	LR 4.500e-03
0: TRAIN [4][50/321]	Time 0.049 (0.051)	Data 8.94e-05 (3.86e-03)	Tok/s 45231 (37678)	Loss/tok 3.4876 (3.2619)	LR 4.500e-03
0: TRAIN [4][60/321]	Time 0.049 (0.051)	Data 1.25e-04 (3.24e-03)	Tok/s 43302 (38764)	Loss/tok 3.2111 (3.3089)	LR 4.500e-03
0: TRAIN [4][70/321]	Time 0.043 (0.050)	Data 8.44e-05 (2.80e-03)	Tok/s 30335 (37765)	Loss/tok 3.2025 (3.3032)	LR 4.500e-03
0: TRAIN [4][80/321]	Time 0.049 (0.050)	Data 1.08e-04 (2.46e-03)	Tok/s 43837 (37842)	Loss/tok 3.0247 (3.2851)	LR 4.500e-03
0: TRAIN [4][90/321]	Time 0.043 (0.049)	Data 6.63e-05 (2.20e-03)	Tok/s 31513 (37567)	Loss/tok 2.9196 (3.2759)	LR 4.500e-03
0: TRAIN [4][100/321]	Time 0.043 (0.049)	Data 6.51e-05 (1.99e-03)	Tok/s 31025 (37658)	Loss/tok 3.3523 (3.2745)	LR 4.500e-03
0: TRAIN [4][110/321]	Time 0.056 (0.050)	Data 1.06e-04 (1.82e-03)	Tok/s 52270 (38258)	Loss/tok 3.4725 (3.2871)	LR 4.500e-03
0: TRAIN [4][120/321]	Time 0.063 (0.050)	Data 8.44e-05 (1.68e-03)	Tok/s 57763 (38520)	Loss/tok 4.0096 (3.2993)	LR 4.500e-03
0: TRAIN [4][130/321]	Time 0.050 (0.049)	Data 8.30e-05 (1.56e-03)	Tok/s 42238 (38185)	Loss/tok 3.6334 (3.2981)	LR 4.500e-03
0: TRAIN [4][140/321]	Time 0.038 (0.049)	Data 6.58e-05 (1.45e-03)	Tok/s 17006 (37971)	Loss/tok 2.7407 (3.2951)	LR 4.500e-03
0: TRAIN [4][150/321]	Time 0.043 (0.049)	Data 8.34e-05 (1.36e-03)	Tok/s 30063 (37816)	Loss/tok 2.6298 (3.2891)	LR 4.500e-03
0: TRAIN [4][160/321]	Time 0.055 (0.049)	Data 6.56e-05 (1.28e-03)	Tok/s 51265 (37905)	Loss/tok 3.5585 (3.2905)	LR 4.500e-03
0: TRAIN [4][170/321]	Time 0.038 (0.049)	Data 8.37e-05 (1.21e-03)	Tok/s 17881 (37581)	Loss/tok 2.3423 (3.2833)	LR 4.500e-03
0: TRAIN [4][180/321]	Time 0.049 (0.048)	Data 8.44e-05 (1.15e-03)	Tok/s 42142 (37459)	Loss/tok 3.2549 (3.2802)	LR 4.500e-03
0: TRAIN [4][190/321]	Time 0.042 (0.048)	Data 5.72e-05 (1.09e-03)	Tok/s 30850 (37206)	Loss/tok 2.9725 (3.2778)	LR 4.500e-03
0: TRAIN [4][200/321]	Time 0.043 (0.048)	Data 6.48e-05 (1.04e-03)	Tok/s 30024 (36960)	Loss/tok 3.0805 (3.2712)	LR 4.500e-03
0: TRAIN [4][210/321]	Time 0.043 (0.048)	Data 1.02e-04 (9.97e-04)	Tok/s 29771 (37226)	Loss/tok 2.8064 (3.2702)	LR 4.500e-03
0: TRAIN [4][220/321]	Time 0.049 (0.048)	Data 8.32e-05 (9.55e-04)	Tok/s 42720 (37511)	Loss/tok 3.1674 (3.2706)	LR 4.500e-03
0: TRAIN [4][230/321]	Time 0.048 (0.048)	Data 8.37e-05 (9.17e-04)	Tok/s 44077 (37425)	Loss/tok 3.1271 (3.2697)	LR 4.500e-03
0: TRAIN [4][240/321]	Time 0.043 (0.048)	Data 8.54e-05 (8.83e-04)	Tok/s 29190 (37288)	Loss/tok 3.2926 (3.2656)	LR 4.500e-03
0: TRAIN [4][250/321]	Time 0.055 (0.048)	Data 8.27e-05 (8.51e-04)	Tok/s 52102 (37271)	Loss/tok 3.3829 (3.2658)	LR 4.500e-03
0: TRAIN [4][260/321]	Time 0.055 (0.048)	Data 6.75e-05 (8.21e-04)	Tok/s 50849 (37276)	Loss/tok 3.4090 (3.2642)	LR 4.500e-03
0: TRAIN [4][270/321]	Time 0.049 (0.048)	Data 8.34e-05 (7.94e-04)	Tok/s 43040 (37249)	Loss/tok 3.0042 (3.2580)	LR 4.500e-03
0: TRAIN [4][280/321]	Time 0.043 (0.048)	Data 8.30e-05 (7.69e-04)	Tok/s 32360 (37272)	Loss/tok 2.9860 (3.2543)	LR 4.500e-03
0: TRAIN [4][290/321]	Time 0.043 (0.048)	Data 8.44e-05 (7.45e-04)	Tok/s 29424 (37287)	Loss/tok 3.1794 (3.2511)	LR 4.500e-03
0: TRAIN [4][300/321]	Time 0.049 (0.048)	Data 6.48e-05 (7.23e-04)	Tok/s 41231 (37172)	Loss/tok 3.8201 (3.2500)	LR 4.500e-03
0: TRAIN [4][310/321]	Time 0.049 (0.048)	Data 8.37e-05 (7.03e-04)	Tok/s 42255 (37330)	Loss/tok 3.2529 (3.2499)	LR 4.500e-03
0: TRAIN [4][320/321]	Time 0.042 (0.048)	Data 3.12e-05 (6.83e-04)	Tok/s 29159 (37276)	Loss/tok 2.7185 (3.2468)	LR 4.500e-03
:::MLL 1560828137.981 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 524}}
:::MLL 1560828137.981 eval_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [4][0/1]	Time 0.381 (0.381)	Decoder iters 123.0 (123.0)	Tok/s 3562 (3562)
0: Running moses detokenizer
0: BLEU(score=22.521361988735464, counts=[35637, 17374, 9647, 5586], totals=[63646, 60643, 57640, 54642], precisions=[55.99252113251422, 28.6496380456112, 16.736641221374047, 10.222905457340508], bp=0.9839469817889961, sys_len=63646, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1560828138.553 eval_accuracy: {"value": 22.52, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 535}}
:::MLL 1560828138.553 eval_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 4	Training Loss: 3.2541	Test BLEU: 22.52
0: Performance: Epoch: 4	Training: 14302516 Tok/s
0: Finished epoch 4
:::MLL 1560828138.554 block_stop: {"value": null, "metadata": {"first_epoch_num": 5, "file": "train.py", "lineno": 557}}
:::MLL 1560828138.554 block_start: {"value": null, "metadata": {"first_epoch_num": 6, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1560828138.555 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 514}}
0: Starting epoch 5
0: Sampler for epoch 5 uses seed 277139707
0: TRAIN [5][0/321]	Time 0.244 (0.244)	Data 1.93e-01 (1.93e-01)	Tok/s 5190 (5190)	Loss/tok 2.9982 (2.9982)	LR 4.500e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
0: TRAIN [5][10/321]	Time 0.038 (0.065)	Data 8.01e-05 (1.76e-02)	Tok/s 17243 (32458)	Loss/tok 2.8569 (3.3778)	LR 4.500e-03
0: TRAIN [5][20/321]	Time 0.049 (0.056)	Data 6.89e-05 (9.25e-03)	Tok/s 42178 (34010)	Loss/tok 2.9226 (3.2765)	LR 4.500e-03
0: TRAIN [5][30/321]	Time 0.063 (0.054)	Data 5.13e-05 (6.29e-03)	Tok/s 57893 (36904)	Loss/tok 3.5030 (3.2534)	LR 4.500e-03
0: TRAIN [5][40/321]	Time 0.043 (0.052)	Data 5.17e-05 (4.77e-03)	Tok/s 30325 (36316)	Loss/tok 3.1809 (3.2279)	LR 4.500e-03
0: TRAIN [5][50/321]	Time 0.043 (0.051)	Data 6.94e-05 (3.85e-03)	Tok/s 30769 (35916)	Loss/tok 2.9773 (3.2345)	LR 4.500e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
0: TRAIN [5][60/321]	Time 0.042 (0.051)	Data 7.06e-05 (3.23e-03)	Tok/s 30594 (37437)	Loss/tok 2.9012 (3.2453)	LR 4.500e-03
0: TRAIN [5][70/321]	Time 0.049 (0.050)	Data 8.75e-05 (2.79e-03)	Tok/s 42954 (37652)	Loss/tok 3.0753 (3.2288)	LR 4.500e-03
0: TRAIN [5][80/321]	Time 0.055 (0.050)	Data 7.01e-05 (2.45e-03)	Tok/s 50489 (37529)	Loss/tok 3.7156 (3.2267)	LR 4.500e-03
0: TRAIN [5][90/321]	Time 0.049 (0.050)	Data 1.00e-04 (2.19e-03)	Tok/s 41808 (37430)	Loss/tok 3.3571 (3.2257)	LR 4.500e-03
0: TRAIN [5][100/321]	Time 0.043 (0.049)	Data 7.01e-05 (1.98e-03)	Tok/s 29384 (37679)	Loss/tok 2.9627 (3.2191)	LR 4.500e-03
0: TRAIN [5][110/321]	Time 0.049 (0.049)	Data 1.22e-04 (1.81e-03)	Tok/s 43001 (37310)	Loss/tok 3.2741 (3.2161)	LR 4.500e-03
0: TRAIN [5][120/321]	Time 0.049 (0.049)	Data 9.37e-05 (1.67e-03)	Tok/s 43095 (37007)	Loss/tok 3.3229 (3.2123)	LR 4.500e-03
0: TRAIN [5][130/321]	Time 0.048 (0.049)	Data 8.63e-05 (1.55e-03)	Tok/s 43188 (37172)	Loss/tok 2.9748 (3.2098)	LR 4.500e-03
0: TRAIN [5][140/321]	Time 0.049 (0.048)	Data 6.91e-05 (1.44e-03)	Tok/s 43105 (37290)	Loss/tok 3.7232 (3.2050)	LR 4.500e-03
0: TRAIN [5][150/321]	Time 0.043 (0.048)	Data 8.94e-05 (1.35e-03)	Tok/s 30403 (37004)	Loss/tok 3.3052 (3.1959)	LR 4.500e-03
0: TRAIN [5][160/321]	Time 0.043 (0.048)	Data 5.17e-05 (1.27e-03)	Tok/s 30475 (36885)	Loss/tok 3.0607 (3.1929)	LR 4.500e-03
0: TRAIN [5][170/321]	Time 0.043 (0.048)	Data 6.84e-05 (1.20e-03)	Tok/s 29258 (36791)	Loss/tok 3.1386 (3.1897)	LR 4.500e-03
0: TRAIN [5][180/321]	Time 0.042 (0.048)	Data 9.08e-05 (1.14e-03)	Tok/s 30291 (36766)	Loss/tok 3.0308 (3.1838)	LR 4.500e-03
0: TRAIN [5][190/321]	Time 0.038 (0.048)	Data 9.99e-05 (1.09e-03)	Tok/s 17085 (36746)	Loss/tok 2.4659 (3.1887)	LR 4.500e-03
0: TRAIN [5][200/321]	Time 0.044 (0.048)	Data 6.84e-05 (1.04e-03)	Tok/s 29144 (36624)	Loss/tok 2.8447 (3.1858)	LR 4.500e-03
0: TRAIN [5][210/321]	Time 0.055 (0.048)	Data 6.91e-05 (9.91e-04)	Tok/s 51446 (36672)	Loss/tok 3.4235 (3.1877)	LR 4.500e-03
0: TRAIN [5][220/321]	Time 0.042 (0.048)	Data 8.58e-05 (9.50e-04)	Tok/s 30522 (36653)	Loss/tok 2.7387 (3.1830)	LR 4.500e-03
0: TRAIN [5][230/321]	Time 0.049 (0.047)	Data 7.06e-05 (9.12e-04)	Tok/s 43384 (36599)	Loss/tok 3.3144 (3.1842)	LR 4.500e-03
0: TRAIN [5][240/321]	Time 0.043 (0.047)	Data 8.11e-05 (8.78e-04)	Tok/s 27928 (36563)	Loss/tok 3.3207 (3.1850)	LR 4.500e-03
0: TRAIN [5][250/321]	Time 0.038 (0.047)	Data 6.87e-05 (8.46e-04)	Tok/s 17806 (36619)	Loss/tok 2.6122 (3.1858)	LR 4.500e-03
0: TRAIN [5][260/321]	Time 0.042 (0.047)	Data 6.89e-05 (8.17e-04)	Tok/s 31453 (36724)	Loss/tok 2.9584 (3.1857)	LR 4.500e-03
0: TRAIN [5][270/321]	Time 0.055 (0.047)	Data 8.73e-05 (7.90e-04)	Tok/s 52368 (36792)	Loss/tok 3.5094 (3.1870)	LR 4.500e-03
0: TRAIN [5][280/321]	Time 0.049 (0.047)	Data 8.70e-05 (7.64e-04)	Tok/s 43099 (36868)	Loss/tok 3.3553 (3.1907)	LR 4.500e-03
0: TRAIN [5][290/321]	Time 0.049 (0.047)	Data 8.30e-05 (7.41e-04)	Tok/s 42500 (36899)	Loss/tok 3.1347 (3.1915)	LR 4.500e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
0: TRAIN [5][300/321]	Time 0.056 (0.048)	Data 8.37e-05 (7.19e-04)	Tok/s 54790 (37114)	Loss/tok 3.4441 (3.1994)	LR 4.500e-03
0: TRAIN [5][310/321]	Time 0.049 (0.048)	Data 8.42e-05 (6.98e-04)	Tok/s 42770 (37187)	Loss/tok 3.1975 (3.2010)	LR 4.500e-03
0: TRAIN [5][320/321]	Time 0.056 (0.048)	Data 3.00e-05 (6.79e-04)	Tok/s 52021 (37333)	Loss/tok 3.0578 (3.2069)	LR 4.500e-03
:::MLL 1560828153.905 epoch_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 524}}
:::MLL 1560828153.906 eval_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [5][0/1]	Time 0.461 (0.461)	Decoder iters 149.0 (149.0)	Tok/s 3025 (3025)
0: Running moses detokenizer
0: BLEU(score=22.42696877580909, counts=[36304, 17707, 9901, 5728], totals=[66208, 63205, 60202, 57204], precisions=[54.83325277912035, 28.015188671782294, 16.44629746520049, 10.013285784210895], bp=1.0, sys_len=66208, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1560828154.486 eval_accuracy: {"value": 22.43, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 535}}
:::MLL 1560828154.487 eval_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 5	Training Loss: 3.2082	Test BLEU: 22.43
0: Performance: Epoch: 5	Training: 14315109 Tok/s
0: Finished epoch 5
:::MLL 1560828154.487 block_stop: {"value": null, "metadata": {"first_epoch_num": 6, "file": "train.py", "lineno": 557}}
:::MLL 1560828154.488 block_start: {"value": null, "metadata": {"first_epoch_num": 7, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1560828154.488 epoch_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 514}}
0: Starting epoch 6
0: Sampler for epoch 6 uses seed 3430745148
0: TRAIN [6][0/321]	Time 0.262 (0.262)	Data 1.94e-01 (1.94e-01)	Tok/s 14301 (14301)	Loss/tok 3.4688 (3.4688)	LR 4.500e-03
0: TRAIN [6][10/321]	Time 0.049 (0.067)	Data 8.54e-05 (1.77e-02)	Tok/s 42783 (34368)	Loss/tok 3.1911 (3.2030)	LR 4.500e-03
0: TRAIN [6][20/321]	Time 0.063 (0.058)	Data 8.34e-05 (9.31e-03)	Tok/s 56758 (36324)	Loss/tok 3.4079 (3.2314)	LR 2.250e-03
0: TRAIN [6][30/321]	Time 0.038 (0.054)	Data 8.34e-05 (6.33e-03)	Tok/s 17960 (36327)	Loss/tok 2.3774 (3.1698)	LR 2.250e-03
0: TRAIN [6][40/321]	Time 0.039 (0.052)	Data 6.56e-05 (4.81e-03)	Tok/s 16143 (35824)	Loss/tok 2.5365 (3.1422)	LR 2.250e-03
0: TRAIN [6][50/321]	Time 0.043 (0.051)	Data 8.44e-05 (3.88e-03)	Tok/s 31266 (35078)	Loss/tok 3.0929 (3.1293)	LR 2.250e-03
0: TRAIN [6][60/321]	Time 0.049 (0.050)	Data 8.34e-05 (3.26e-03)	Tok/s 41212 (35435)	Loss/tok 3.0945 (3.1345)	LR 2.250e-03
0: TRAIN [6][70/321]	Time 0.043 (0.050)	Data 1.12e-04 (2.81e-03)	Tok/s 30701 (35807)	Loss/tok 3.1035 (3.1427)	LR 2.250e-03
0: TRAIN [6][80/321]	Time 0.043 (0.049)	Data 8.42e-05 (2.47e-03)	Tok/s 28612 (34752)	Loss/tok 2.9307 (3.1286)	LR 2.250e-03
0: TRAIN [6][90/321]	Time 0.043 (0.049)	Data 8.27e-05 (2.21e-03)	Tok/s 29474 (35044)	Loss/tok 3.0348 (3.1157)	LR 2.250e-03
0: TRAIN [6][100/321]	Time 0.049 (0.049)	Data 9.23e-05 (2.00e-03)	Tok/s 43174 (35524)	Loss/tok 2.9346 (3.1244)	LR 2.250e-03
0: TRAIN [6][110/321]	Time 0.049 (0.048)	Data 7.27e-05 (1.83e-03)	Tok/s 41818 (35468)	Loss/tok 3.1100 (3.1208)	LR 2.250e-03
0: TRAIN [6][120/321]	Time 0.055 (0.048)	Data 6.60e-05 (1.68e-03)	Tok/s 53802 (36015)	Loss/tok 3.0491 (3.1167)	LR 2.250e-03
0: TRAIN [6][130/321]	Time 0.043 (0.048)	Data 5.65e-05 (1.56e-03)	Tok/s 31344 (35969)	Loss/tok 2.9347 (3.1041)	LR 2.250e-03
0: TRAIN [6][140/321]	Time 0.044 (0.048)	Data 8.32e-05 (1.46e-03)	Tok/s 32051 (35930)	Loss/tok 3.0458 (3.0974)	LR 2.250e-03
0: TRAIN [6][150/321]	Time 0.043 (0.048)	Data 6.65e-05 (1.37e-03)	Tok/s 30861 (36054)	Loss/tok 2.8958 (3.0968)	LR 2.250e-03
0: TRAIN [6][160/321]	Time 0.043 (0.048)	Data 1.08e-04 (1.29e-03)	Tok/s 31374 (35697)	Loss/tok 3.1744 (3.0949)	LR 2.250e-03
0: TRAIN [6][170/321]	Time 0.043 (0.048)	Data 8.23e-05 (1.22e-03)	Tok/s 30208 (35879)	Loss/tok 2.6618 (3.0916)	LR 2.250e-03
0: TRAIN [6][180/321]	Time 0.049 (0.048)	Data 8.25e-05 (1.15e-03)	Tok/s 42852 (35993)	Loss/tok 3.0009 (3.0900)	LR 2.250e-03
0: TRAIN [6][190/321]	Time 0.042 (0.048)	Data 8.42e-05 (1.10e-03)	Tok/s 29743 (36261)	Loss/tok 2.8160 (3.0839)	LR 2.250e-03
0: TRAIN [6][200/321]	Time 0.043 (0.048)	Data 8.27e-05 (1.05e-03)	Tok/s 29420 (36456)	Loss/tok 2.9954 (3.0909)	LR 2.250e-03
0: TRAIN [6][210/321]	Time 0.038 (0.048)	Data 9.16e-05 (1.00e-03)	Tok/s 16922 (36397)	Loss/tok 2.6135 (3.0895)	LR 2.250e-03
0: TRAIN [6][220/321]	Time 0.064 (0.048)	Data 1.27e-04 (9.61e-04)	Tok/s 60708 (36594)	Loss/tok 3.1452 (3.0926)	LR 2.250e-03
0: TRAIN [6][230/321]	Time 0.043 (0.048)	Data 8.49e-05 (9.23e-04)	Tok/s 30031 (36816)	Loss/tok 3.0203 (3.0952)	LR 2.250e-03
0: TRAIN [6][240/321]	Time 0.055 (0.048)	Data 8.37e-05 (8.88e-04)	Tok/s 53337 (36732)	Loss/tok 3.3018 (3.0944)	LR 2.250e-03
0: TRAIN [6][250/321]	Time 0.055 (0.048)	Data 8.27e-05 (8.56e-04)	Tok/s 51466 (36652)	Loss/tok 3.5291 (3.0960)	LR 2.250e-03
0: TRAIN [6][260/321]	Time 0.043 (0.047)	Data 8.27e-05 (8.26e-04)	Tok/s 27736 (36512)	Loss/tok 3.1657 (3.0961)	LR 1.125e-03
0: TRAIN [6][270/321]	Time 0.043 (0.047)	Data 6.56e-05 (7.99e-04)	Tok/s 28697 (36469)	Loss/tok 3.3614 (3.0975)	LR 1.125e-03
0: TRAIN [6][280/321]	Time 0.043 (0.048)	Data 8.34e-05 (7.73e-04)	Tok/s 29389 (36562)	Loss/tok 3.0449 (3.1058)	LR 1.125e-03
0: TRAIN [6][290/321]	Time 0.049 (0.047)	Data 8.37e-05 (7.50e-04)	Tok/s 43414 (36576)	Loss/tok 2.9700 (3.1060)	LR 1.125e-03
0: TRAIN [6][300/321]	Time 0.064 (0.048)	Data 8.32e-05 (7.28e-04)	Tok/s 59516 (36696)	Loss/tok 3.6266 (3.1062)	LR 1.125e-03
0: TRAIN [6][310/321]	Time 0.056 (0.048)	Data 8.51e-05 (7.07e-04)	Tok/s 54181 (36906)	Loss/tok 3.2114 (3.1082)	LR 1.125e-03
0: TRAIN [6][320/321]	Time 0.049 (0.048)	Data 3.03e-05 (6.88e-04)	Tok/s 43272 (37147)	Loss/tok 3.1278 (3.1062)	LR 1.125e-03
:::MLL 1560828169.862 epoch_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 524}}
:::MLL 1560828169.862 eval_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [6][0/1]	Time 0.316 (0.316)	Decoder iters 101.0 (101.0)	Tok/s 4399 (4399)
0: Running moses detokenizer
0: BLEU(score=24.0085368169405, counts=[36860, 18402, 10475, 6206], totals=[64954, 61951, 58948, 55950], precisions=[56.747852326261665, 29.704120998853934, 17.76989889394042, 11.092046470062556], bp=1.0, sys_len=64954, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1560828170.295 eval_accuracy: {"value": 24.01, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 535}}
:::MLL 1560828170.296 eval_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 6	Training Loss: 3.1080	Test BLEU: 24.01
0: Performance: Epoch: 6	Training: 14264395 Tok/s
0: Finished epoch 6
:::MLL 1560828170.296 block_stop: {"value": null, "metadata": {"first_epoch_num": 7, "file": "train.py", "lineno": 557}}
0: Closing preprocessed data file
:::MLL 1560828170.297 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 568}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
ENDING TIMING RUN AT 2019-06-18 03:22:55 AM
RESULT,RNN_TRANSLATOR,,145,nvidia,2019-06-18 03:20:30 AM
