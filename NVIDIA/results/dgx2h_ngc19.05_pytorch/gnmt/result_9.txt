Beginning trial 1 of 1
Gathering sys log on circe-n079
:::MLL 1560820719.663 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1560820719.663 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1560820719.663 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1560820719.664 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1560820719.664 submission_platform: {"value": "1xNVIDIA DGX-2H", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1560820719.665 submission_entry: {"value": "{'hardware': 'NVIDIA DGX-2H', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': 'Ubuntu 18.04.1 LTS / NVIDIA DGX Server 4.0.4 4.0.5', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.4-2.0.7', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Platinum 8174 CPU @ 3.10GHz', 'num_cores': '48', 'num_vcpus': '96', 'accelerator': 'Tesla V100-SXM3-32GB-H', 'num_accelerators': '16', 'sys_mem_size': '1510 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '2x 894.3G + 8x 3.5T', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27800 Family [ConnectX-5]', 'num_network_cards': '10', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1560820719.665 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1560820719.665 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
vm.drop_caches = 3
:::MLL 1560820721.273 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node circe-n079
+ pids+=($!)
+ set +x
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n079
+ srun --mem=0 -N 1 -n 1 -w circe-n079 docker exec -e DGXSYSTEM=DGX2 -e 'MULTI_NODE= --master_port=4735' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=128 -e TEST_BATCH_SIZE=64 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=110792 -e SLURM_NTASKS_PER_NODE=16 -e SLURM_NNODES=1 cont_110792 ./run_and_time.sh
Run vars: id 110792 gpus 16 mparams  --master_port=4735
STARTING TIMING RUN AT 2019-06-18 01:18:41 AM
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=128
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 24 --nproc_per_node 16  --master_port=4735'
+ echo 'running benchmark'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --master_port=4735 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 128 --test-batch-size 64 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1560820723.027 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560820723.029 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560820723.030 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560820723.030 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560820723.030 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560820723.031 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560820723.032 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560820723.035 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560820723.035 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560820723.036 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560820723.036 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560820723.039 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560820723.042 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560820723.044 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560820723.045 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560820723.053 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=64, test_loader_workers=0, train_batch_size=128, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 2805746316
0: Worker 0 is using worker seed: 357588751
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1560820752.605 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1560820755.448 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1560820755.449 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1560820755.449 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1560820755.768 global_batch_size: {"value": 2048, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1560820755.770 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1560820755.770 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1560820755.770 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1560820755.771 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1560820755.771 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1560820755.771 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1560820755.771 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1560820755.772 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1560820755.772 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 683178604
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/1938]	Time 0.444 (0.444)	Data 3.33e-01 (3.33e-01)	Tok/s 26582 (26582)	Loss/tok 10.6806 (10.6806)	LR 2.000e-05
0: TRAIN [0][10/1938]	Time 0.059 (0.113)	Data 9.08e-05 (3.03e-02)	Tok/s 87403 (81504)	Loss/tok 9.7040 (10.2120)	LR 2.518e-05
0: TRAIN [0][20/1938]	Time 0.105 (0.099)	Data 8.99e-05 (1.59e-02)	Tok/s 112307 (87534)	Loss/tok 9.3912 (9.8806)	LR 3.170e-05
0: TRAIN [0][30/1938]	Time 0.105 (0.093)	Data 8.34e-05 (1.08e-02)	Tok/s 111220 (90811)	Loss/tok 9.2635 (9.6649)	LR 3.991e-05
0: TRAIN [0][40/1938]	Time 0.059 (0.087)	Data 9.04e-05 (8.20e-03)	Tok/s 87104 (90904)	Loss/tok 8.6765 (9.5103)	LR 5.024e-05
0: TRAIN [0][50/1938]	Time 0.082 (0.082)	Data 8.13e-05 (6.61e-03)	Tok/s 102344 (90419)	Loss/tok 8.7074 (9.3767)	LR 6.325e-05
0: TRAIN [0][60/1938]	Time 0.059 (0.082)	Data 9.82e-05 (5.54e-03)	Tok/s 88189 (91521)	Loss/tok 8.3333 (9.2294)	LR 7.962e-05
0: TRAIN [0][70/1938]	Time 0.039 (0.079)	Data 9.11e-05 (4.77e-03)	Tok/s 69251 (91410)	Loss/tok 7.7483 (9.1115)	LR 1.002e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][80/1938]	Time 0.082 (0.078)	Data 9.20e-05 (4.19e-03)	Tok/s 101449 (91882)	Loss/tok 8.0784 (9.0101)	LR 1.233e-04
0: TRAIN [0][90/1938]	Time 0.105 (0.078)	Data 8.34e-05 (3.74e-03)	Tok/s 112032 (92243)	Loss/tok 8.2283 (8.9171)	LR 1.552e-04
0: TRAIN [0][100/1938]	Time 0.060 (0.077)	Data 7.92e-05 (3.38e-03)	Tok/s 87756 (92096)	Loss/tok 7.8129 (8.8304)	LR 1.954e-04
0: TRAIN [0][110/1938]	Time 0.059 (0.077)	Data 8.23e-05 (3.08e-03)	Tok/s 88491 (92092)	Loss/tok 7.8381 (8.7509)	LR 2.461e-04
0: TRAIN [0][120/1938]	Time 0.060 (0.076)	Data 8.32e-05 (2.83e-03)	Tok/s 87602 (92104)	Loss/tok 7.7898 (8.6834)	LR 3.098e-04
0: TRAIN [0][130/1938]	Time 0.060 (0.077)	Data 8.85e-05 (2.62e-03)	Tok/s 88355 (92871)	Loss/tok 7.6684 (8.6091)	LR 3.900e-04
0: TRAIN [0][140/1938]	Time 0.059 (0.077)	Data 7.94e-05 (2.44e-03)	Tok/s 87766 (93000)	Loss/tok 7.6879 (8.5567)	LR 4.909e-04
0: TRAIN [0][150/1938]	Time 0.105 (0.077)	Data 8.18e-05 (2.29e-03)	Tok/s 112744 (93365)	Loss/tok 7.8869 (8.5018)	LR 6.181e-04
0: TRAIN [0][160/1938]	Time 0.039 (0.077)	Data 8.20e-05 (2.15e-03)	Tok/s 67819 (93453)	Loss/tok 6.9887 (8.4522)	LR 7.781e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
0: TRAIN [0][170/1938]	Time 0.082 (0.077)	Data 1.52e-04 (2.03e-03)	Tok/s 101587 (93800)	Loss/tok 7.6899 (8.3982)	LR 9.573e-04
0: TRAIN [0][180/1938]	Time 0.105 (0.077)	Data 9.20e-05 (1.92e-03)	Tok/s 111352 (93491)	Loss/tok 7.6380 (8.3564)	LR 1.205e-03
0: TRAIN [0][190/1938]	Time 0.082 (0.077)	Data 8.58e-05 (1.83e-03)	Tok/s 103096 (93644)	Loss/tok 7.3974 (8.3045)	LR 1.517e-03
0: TRAIN [0][200/1938]	Time 0.082 (0.076)	Data 9.78e-05 (1.74e-03)	Tok/s 101976 (93695)	Loss/tok 7.1325 (8.2496)	LR 1.910e-03
0: TRAIN [0][210/1938]	Time 0.059 (0.076)	Data 8.63e-05 (1.66e-03)	Tok/s 86982 (93897)	Loss/tok 6.6846 (8.1880)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.039 (0.076)	Data 8.61e-05 (1.59e-03)	Tok/s 65939 (93299)	Loss/tok 5.6542 (8.1433)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.059 (0.075)	Data 9.35e-05 (1.53e-03)	Tok/s 86775 (93311)	Loss/tok 6.4941 (8.0875)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.039 (0.075)	Data 8.23e-05 (1.47e-03)	Tok/s 65834 (93265)	Loss/tok 5.5641 (8.0298)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.059 (0.075)	Data 8.44e-05 (1.41e-03)	Tok/s 85456 (93209)	Loss/tok 6.0662 (7.9742)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.060 (0.075)	Data 8.08e-05 (1.36e-03)	Tok/s 83713 (93316)	Loss/tok 5.9670 (7.9087)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.060 (0.075)	Data 8.06e-05 (1.31e-03)	Tok/s 86412 (93220)	Loss/tok 6.0721 (7.8541)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.059 (0.075)	Data 8.34e-05 (1.27e-03)	Tok/s 88889 (93275)	Loss/tok 5.9824 (7.7915)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.105 (0.075)	Data 8.49e-05 (1.23e-03)	Tok/s 109116 (93174)	Loss/tok 6.2467 (7.7352)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.082 (0.075)	Data 8.06e-05 (1.19e-03)	Tok/s 100446 (93225)	Loss/tok 6.0104 (7.6768)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.083 (0.075)	Data 8.18e-05 (1.16e-03)	Tok/s 102868 (93283)	Loss/tok 5.7901 (7.6174)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.059 (0.075)	Data 8.32e-05 (1.12e-03)	Tok/s 86849 (93371)	Loss/tok 5.4599 (7.5570)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.060 (0.074)	Data 8.25e-05 (1.09e-03)	Tok/s 87073 (93308)	Loss/tok 5.3658 (7.5041)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.060 (0.074)	Data 8.42e-05 (1.06e-03)	Tok/s 85788 (93366)	Loss/tok 5.2158 (7.4454)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.060 (0.074)	Data 9.37e-05 (1.03e-03)	Tok/s 85239 (93347)	Loss/tok 5.3686 (7.3918)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.082 (0.074)	Data 8.70e-05 (1.01e-03)	Tok/s 100672 (93272)	Loss/tok 5.5338 (7.3421)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.059 (0.074)	Data 8.85e-05 (9.83e-04)	Tok/s 85216 (93218)	Loss/tok 5.1305 (7.2879)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.082 (0.074)	Data 8.73e-05 (9.60e-04)	Tok/s 101800 (93277)	Loss/tok 5.3263 (7.2325)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.105 (0.074)	Data 9.39e-05 (9.37e-04)	Tok/s 111019 (93212)	Loss/tok 5.5455 (7.1829)	LR 2.000e-03
0: TRAIN [0][400/1938]	Time 0.105 (0.074)	Data 8.65e-05 (9.16e-04)	Tok/s 109459 (93308)	Loss/tok 5.4513 (7.1267)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.083 (0.074)	Data 8.34e-05 (8.96e-04)	Tok/s 103378 (93191)	Loss/tok 4.8981 (7.0809)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.105 (0.074)	Data 8.75e-05 (8.77e-04)	Tok/s 108231 (93188)	Loss/tok 5.3330 (7.0295)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.060 (0.074)	Data 1.01e-04 (8.59e-04)	Tok/s 86537 (93256)	Loss/tok 4.7245 (6.9780)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.060 (0.074)	Data 8.77e-05 (8.41e-04)	Tok/s 87432 (93401)	Loss/tok 4.4351 (6.9205)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.060 (0.074)	Data 8.37e-05 (8.24e-04)	Tok/s 83109 (93368)	Loss/tok 4.5441 (6.8745)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.134 (0.074)	Data 8.65e-05 (8.09e-04)	Tok/s 112793 (93552)	Loss/tok 5.0070 (6.8146)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.134 (0.075)	Data 9.87e-05 (7.93e-04)	Tok/s 110202 (93746)	Loss/tok 5.0893 (6.7584)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.060 (0.075)	Data 8.68e-05 (7.79e-04)	Tok/s 84202 (93805)	Loss/tok 4.2930 (6.7094)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.039 (0.075)	Data 8.42e-05 (7.65e-04)	Tok/s 67397 (93741)	Loss/tok 3.4377 (6.6682)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.083 (0.075)	Data 8.56e-05 (7.51e-04)	Tok/s 100920 (93886)	Loss/tok 4.4851 (6.6173)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.039 (0.075)	Data 8.63e-05 (7.38e-04)	Tok/s 67493 (93971)	Loss/tok 3.4972 (6.5698)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.106 (0.075)	Data 8.68e-05 (7.26e-04)	Tok/s 111501 (94054)	Loss/tok 4.7067 (6.5248)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.105 (0.075)	Data 1.00e-04 (7.14e-04)	Tok/s 109766 (94002)	Loss/tok 4.8304 (6.4881)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.105 (0.075)	Data 9.47e-05 (7.02e-04)	Tok/s 108738 (94035)	Loss/tok 4.6302 (6.4466)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.134 (0.076)	Data 8.56e-05 (6.91e-04)	Tok/s 111068 (94150)	Loss/tok 4.8098 (6.4032)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.134 (0.076)	Data 8.51e-05 (6.80e-04)	Tok/s 110661 (94190)	Loss/tok 4.8900 (6.3642)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.060 (0.076)	Data 8.37e-05 (6.70e-04)	Tok/s 85520 (94140)	Loss/tok 4.0226 (6.3306)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.060 (0.075)	Data 9.01e-05 (6.60e-04)	Tok/s 85958 (94122)	Loss/tok 3.9990 (6.2972)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.039 (0.075)	Data 8.49e-05 (6.50e-04)	Tok/s 66057 (93977)	Loss/tok 3.3697 (6.2669)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.060 (0.075)	Data 8.34e-05 (6.41e-04)	Tok/s 87063 (93926)	Loss/tok 4.1545 (6.2358)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][610/1938]	Time 0.134 (0.075)	Data 8.39e-05 (6.32e-04)	Tok/s 112741 (93946)	Loss/tok 4.4923 (6.2010)	LR 2.000e-03
0: TRAIN [0][620/1938]	Time 0.083 (0.075)	Data 8.63e-05 (6.23e-04)	Tok/s 102115 (93980)	Loss/tok 4.2213 (6.1681)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.083 (0.075)	Data 8.08e-05 (6.14e-04)	Tok/s 98838 (94016)	Loss/tok 4.1494 (6.1347)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.083 (0.076)	Data 8.80e-05 (6.06e-04)	Tok/s 102566 (94059)	Loss/tok 4.0709 (6.0992)	LR 2.000e-03
0: TRAIN [0][650/1938]	Time 0.106 (0.076)	Data 9.89e-05 (5.98e-04)	Tok/s 108764 (94162)	Loss/tok 4.5364 (6.0636)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.061 (0.076)	Data 8.51e-05 (5.90e-04)	Tok/s 84639 (94170)	Loss/tok 4.1671 (6.0331)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.061 (0.076)	Data 9.56e-05 (5.83e-04)	Tok/s 85768 (94090)	Loss/tok 3.9306 (6.0086)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.083 (0.076)	Data 8.61e-05 (5.76e-04)	Tok/s 99136 (94135)	Loss/tok 3.9840 (5.9764)	LR 2.000e-03
0: TRAIN [0][690/1938]	Time 0.062 (0.076)	Data 8.54e-05 (5.68e-04)	Tok/s 82936 (94120)	Loss/tok 3.8520 (5.9489)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.083 (0.076)	Data 8.68e-05 (5.62e-04)	Tok/s 102050 (94071)	Loss/tok 4.0114 (5.9238)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.106 (0.076)	Data 8.25e-05 (5.55e-04)	Tok/s 108720 (94034)	Loss/tok 4.4854 (5.8989)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.061 (0.076)	Data 8.58e-05 (5.48e-04)	Tok/s 83756 (93958)	Loss/tok 3.8562 (5.8757)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.083 (0.076)	Data 8.92e-05 (5.42e-04)	Tok/s 101668 (93923)	Loss/tok 4.0342 (5.8528)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.060 (0.076)	Data 8.27e-05 (5.36e-04)	Tok/s 87068 (93943)	Loss/tok 3.9151 (5.8269)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.134 (0.076)	Data 8.27e-05 (5.30e-04)	Tok/s 109301 (94059)	Loss/tok 4.5305 (5.7965)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.060 (0.076)	Data 8.18e-05 (5.24e-04)	Tok/s 84918 (94055)	Loss/tok 3.7352 (5.7733)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.060 (0.076)	Data 7.96e-05 (5.18e-04)	Tok/s 85781 (94037)	Loss/tok 3.8766 (5.7521)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.060 (0.076)	Data 9.56e-05 (5.13e-04)	Tok/s 85038 (94041)	Loss/tok 3.6197 (5.7282)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.060 (0.076)	Data 8.80e-05 (5.07e-04)	Tok/s 84788 (94023)	Loss/tok 3.4817 (5.7072)	LR 2.000e-03
0: TRAIN [0][800/1938]	Time 0.060 (0.076)	Data 8.18e-05 (5.02e-04)	Tok/s 85349 (93982)	Loss/tok 3.6653 (5.6868)	LR 2.000e-03
0: TRAIN [0][810/1938]	Time 0.106 (0.076)	Data 8.15e-05 (4.97e-04)	Tok/s 111698 (93882)	Loss/tok 4.2247 (5.6692)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.060 (0.076)	Data 8.01e-05 (4.92e-04)	Tok/s 83427 (93810)	Loss/tok 3.6943 (5.6501)	LR 2.000e-03
0: TRAIN [0][830/1938]	Time 0.060 (0.076)	Data 8.30e-05 (4.87e-04)	Tok/s 85678 (93866)	Loss/tok 3.4646 (5.6274)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.039 (0.076)	Data 9.06e-05 (4.82e-04)	Tok/s 65919 (93852)	Loss/tok 3.0701 (5.6076)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.106 (0.076)	Data 8.15e-05 (4.77e-04)	Tok/s 109444 (93849)	Loss/tok 3.9886 (5.5881)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.083 (0.076)	Data 8.61e-05 (4.73e-04)	Tok/s 98852 (93848)	Loss/tok 4.0292 (5.5678)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.106 (0.076)	Data 8.49e-05 (4.68e-04)	Tok/s 110489 (93918)	Loss/tok 4.0482 (5.5456)	LR 2.000e-03
0: TRAIN [0][880/1938]	Time 0.106 (0.076)	Data 9.66e-05 (4.64e-04)	Tok/s 111350 (93951)	Loss/tok 4.0532 (5.5259)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.083 (0.076)	Data 8.32e-05 (4.60e-04)	Tok/s 101553 (93961)	Loss/tok 3.9695 (5.5073)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.083 (0.076)	Data 7.89e-05 (4.55e-04)	Tok/s 102905 (93882)	Loss/tok 3.9697 (5.4925)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.039 (0.076)	Data 8.15e-05 (4.51e-04)	Tok/s 68495 (93836)	Loss/tok 3.1707 (5.4764)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][920/1938]	Time 0.083 (0.076)	Data 8.54e-05 (4.47e-04)	Tok/s 102557 (93865)	Loss/tok 3.8206 (5.4581)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.060 (0.076)	Data 7.87e-05 (4.43e-04)	Tok/s 84611 (93823)	Loss/tok 3.7389 (5.4425)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.060 (0.076)	Data 7.77e-05 (4.39e-04)	Tok/s 90049 (93822)	Loss/tok 3.7279 (5.4266)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.083 (0.076)	Data 8.23e-05 (4.36e-04)	Tok/s 102014 (93813)	Loss/tok 3.8971 (5.4109)	LR 2.000e-03
0: TRAIN [0][960/1938]	Time 0.060 (0.076)	Data 8.20e-05 (4.32e-04)	Tok/s 85653 (93863)	Loss/tok 3.6749 (5.3930)	LR 2.000e-03
0: TRAIN [0][970/1938]	Time 0.083 (0.076)	Data 7.70e-05 (4.28e-04)	Tok/s 102000 (93856)	Loss/tok 3.9158 (5.3771)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.106 (0.076)	Data 8.30e-05 (4.25e-04)	Tok/s 109731 (93876)	Loss/tok 3.9069 (5.3606)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.061 (0.076)	Data 8.75e-05 (4.21e-04)	Tok/s 83986 (93900)	Loss/tok 3.5620 (5.3439)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.083 (0.076)	Data 7.99e-05 (4.18e-04)	Tok/s 100351 (93941)	Loss/tok 3.7336 (5.3275)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.060 (0.076)	Data 8.03e-05 (4.15e-04)	Tok/s 86013 (93901)	Loss/tok 3.6053 (5.3139)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.106 (0.076)	Data 8.11e-05 (4.11e-04)	Tok/s 110184 (93896)	Loss/tok 3.9661 (5.2990)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.039 (0.076)	Data 8.49e-05 (4.08e-04)	Tok/s 66749 (93809)	Loss/tok 2.9490 (5.2872)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.039 (0.076)	Data 8.06e-05 (4.05e-04)	Tok/s 66065 (93810)	Loss/tok 3.1925 (5.2729)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.083 (0.075)	Data 8.11e-05 (4.02e-04)	Tok/s 103021 (93796)	Loss/tok 3.8541 (5.2590)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.060 (0.076)	Data 7.89e-05 (3.99e-04)	Tok/s 86281 (93826)	Loss/tok 3.5476 (5.2440)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.106 (0.075)	Data 7.92e-05 (3.96e-04)	Tok/s 109594 (93806)	Loss/tok 3.8836 (5.2309)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.060 (0.075)	Data 8.01e-05 (3.93e-04)	Tok/s 87420 (93797)	Loss/tok 3.5293 (5.2179)	LR 2.000e-03
0: TRAIN [0][1090/1938]	Time 0.060 (0.075)	Data 7.70e-05 (3.90e-04)	Tok/s 86454 (93717)	Loss/tok 3.5530 (5.2070)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.060 (0.075)	Data 8.13e-05 (3.87e-04)	Tok/s 85460 (93699)	Loss/tok 3.4727 (5.1947)	LR 2.000e-03
0: TRAIN [0][1110/1938]	Time 0.106 (0.075)	Data 8.13e-05 (3.85e-04)	Tok/s 110312 (93698)	Loss/tok 4.0851 (5.1817)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.060 (0.075)	Data 8.11e-05 (3.82e-04)	Tok/s 88237 (93709)	Loss/tok 3.4860 (5.1689)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.083 (0.075)	Data 8.54e-05 (3.79e-04)	Tok/s 99770 (93775)	Loss/tok 3.8427 (5.1543)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.083 (0.076)	Data 8.34e-05 (3.77e-04)	Tok/s 100710 (93844)	Loss/tok 3.7294 (5.1396)	LR 2.000e-03
0: TRAIN [0][1150/1938]	Time 0.083 (0.076)	Data 8.13e-05 (3.74e-04)	Tok/s 100667 (93877)	Loss/tok 3.9389 (5.1266)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.083 (0.076)	Data 7.58e-05 (3.72e-04)	Tok/s 102426 (93869)	Loss/tok 3.7197 (5.1153)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.083 (0.075)	Data 7.84e-05 (3.69e-04)	Tok/s 102726 (93859)	Loss/tok 3.6015 (5.1038)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.083 (0.075)	Data 8.15e-05 (3.67e-04)	Tok/s 101822 (93894)	Loss/tok 3.7527 (5.0914)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.106 (0.075)	Data 8.25e-05 (3.64e-04)	Tok/s 110697 (93851)	Loss/tok 4.0699 (5.0811)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1200/1938]	Time 0.040 (0.075)	Data 8.94e-05 (3.62e-04)	Tok/s 65400 (93875)	Loss/tok 2.8467 (5.0686)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.106 (0.075)	Data 8.27e-05 (3.60e-04)	Tok/s 111183 (93866)	Loss/tok 3.9023 (5.0573)	LR 2.000e-03
0: TRAIN [0][1220/1938]	Time 0.083 (0.075)	Data 8.01e-05 (3.57e-04)	Tok/s 100865 (93795)	Loss/tok 3.6569 (5.0480)	LR 2.000e-03
0: TRAIN [0][1230/1938]	Time 0.039 (0.075)	Data 7.89e-05 (3.55e-04)	Tok/s 65862 (93754)	Loss/tok 3.0869 (5.0382)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.083 (0.075)	Data 7.77e-05 (3.53e-04)	Tok/s 102664 (93745)	Loss/tok 3.8207 (5.0281)	LR 2.000e-03
0: TRAIN [0][1250/1938]	Time 0.083 (0.075)	Data 8.32e-05 (3.51e-04)	Tok/s 101220 (93789)	Loss/tok 3.8909 (5.0169)	LR 2.000e-03
0: TRAIN [0][1260/1938]	Time 0.083 (0.075)	Data 7.87e-05 (3.48e-04)	Tok/s 100190 (93763)	Loss/tok 3.8215 (5.0076)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.134 (0.075)	Data 8.01e-05 (3.46e-04)	Tok/s 109031 (93782)	Loss/tok 4.1702 (4.9966)	LR 2.000e-03
0: TRAIN [0][1280/1938]	Time 0.083 (0.075)	Data 7.89e-05 (3.44e-04)	Tok/s 103189 (93796)	Loss/tok 3.7192 (4.9858)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.060 (0.075)	Data 8.13e-05 (3.42e-04)	Tok/s 84209 (93799)	Loss/tok 3.3637 (4.9754)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.083 (0.075)	Data 8.15e-05 (3.40e-04)	Tok/s 102024 (93779)	Loss/tok 3.6607 (4.9659)	LR 2.000e-03
0: TRAIN [0][1310/1938]	Time 0.061 (0.075)	Data 8.39e-05 (3.38e-04)	Tok/s 84796 (93799)	Loss/tok 3.6040 (4.9554)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.083 (0.076)	Data 8.30e-05 (3.36e-04)	Tok/s 101680 (93840)	Loss/tok 3.5500 (4.9442)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1330/1938]	Time 0.083 (0.076)	Data 8.13e-05 (3.35e-04)	Tok/s 101667 (93822)	Loss/tok 3.6576 (4.9349)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.040 (0.076)	Data 7.99e-05 (3.33e-04)	Tok/s 67842 (93811)	Loss/tok 2.9752 (4.9256)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.060 (0.075)	Data 8.01e-05 (3.31e-04)	Tok/s 89253 (93766)	Loss/tok 3.4202 (4.9174)	LR 2.000e-03
0: TRAIN [0][1360/1938]	Time 0.040 (0.075)	Data 8.34e-05 (3.29e-04)	Tok/s 67980 (93722)	Loss/tok 3.0910 (4.9090)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.083 (0.075)	Data 7.72e-05 (3.27e-04)	Tok/s 101609 (93684)	Loss/tok 3.6741 (4.9008)	LR 2.000e-03
0: TRAIN [0][1380/1938]	Time 0.060 (0.075)	Data 8.01e-05 (3.25e-04)	Tok/s 86052 (93664)	Loss/tok 3.2981 (4.8925)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.083 (0.075)	Data 8.37e-05 (3.24e-04)	Tok/s 99482 (93692)	Loss/tok 3.7913 (4.8828)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.083 (0.075)	Data 8.27e-05 (3.22e-04)	Tok/s 100326 (93693)	Loss/tok 3.6705 (4.8740)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.083 (0.075)	Data 8.03e-05 (3.20e-04)	Tok/s 102186 (93687)	Loss/tok 3.5643 (4.8655)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.083 (0.075)	Data 8.32e-05 (3.18e-04)	Tok/s 102128 (93677)	Loss/tok 3.6840 (4.8576)	LR 2.000e-03
0: TRAIN [0][1430/1938]	Time 0.060 (0.075)	Data 7.84e-05 (3.17e-04)	Tok/s 86821 (93673)	Loss/tok 3.5160 (4.8493)	LR 2.000e-03
0: TRAIN [0][1440/1938]	Time 0.039 (0.075)	Data 8.15e-05 (3.15e-04)	Tok/s 67043 (93635)	Loss/tok 2.8521 (4.8418)	LR 2.000e-03
0: TRAIN [0][1450/1938]	Time 0.060 (0.075)	Data 8.30e-05 (3.14e-04)	Tok/s 86290 (93622)	Loss/tok 3.2287 (4.8342)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.061 (0.075)	Data 7.75e-05 (3.12e-04)	Tok/s 85627 (93611)	Loss/tok 3.3699 (4.8264)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.060 (0.075)	Data 8.13e-05 (3.10e-04)	Tok/s 88168 (93603)	Loss/tok 3.3184 (4.8185)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1480/1938]	Time 0.081 (0.075)	Data 1.02e-04 (3.09e-04)	Tok/s 106487 (93622)	Loss/tok 3.5275 (4.8100)	LR 2.000e-03
0: TRAIN [0][1490/1938]	Time 0.039 (0.075)	Data 7.80e-05 (3.07e-04)	Tok/s 68741 (93604)	Loss/tok 2.8276 (4.8020)	LR 2.000e-03
0: TRAIN [0][1500/1938]	Time 0.060 (0.075)	Data 9.56e-05 (3.06e-04)	Tok/s 83191 (93556)	Loss/tok 3.3912 (4.7956)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.060 (0.075)	Data 8.15e-05 (3.04e-04)	Tok/s 85272 (93561)	Loss/tok 3.4611 (4.7882)	LR 2.000e-03
0: TRAIN [0][1520/1938]	Time 0.040 (0.075)	Data 8.37e-05 (3.03e-04)	Tok/s 65937 (93571)	Loss/tok 2.8368 (4.7803)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.083 (0.075)	Data 8.27e-05 (3.02e-04)	Tok/s 100600 (93560)	Loss/tok 3.6323 (4.7732)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.106 (0.075)	Data 8.20e-05 (3.00e-04)	Tok/s 110159 (93584)	Loss/tok 3.8712 (4.7650)	LR 2.000e-03
0: TRAIN [0][1550/1938]	Time 0.083 (0.075)	Data 9.32e-05 (2.99e-04)	Tok/s 101155 (93585)	Loss/tok 3.5113 (4.7575)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.060 (0.075)	Data 7.96e-05 (2.97e-04)	Tok/s 85720 (93579)	Loss/tok 3.4701 (4.7507)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.061 (0.075)	Data 7.75e-05 (2.96e-04)	Tok/s 86427 (93530)	Loss/tok 3.3268 (4.7445)	LR 2.000e-03
0: TRAIN [0][1580/1938]	Time 0.060 (0.075)	Data 8.51e-05 (2.95e-04)	Tok/s 88304 (93549)	Loss/tok 3.3905 (4.7368)	LR 2.000e-03
0: TRAIN [0][1590/1938]	Time 0.060 (0.075)	Data 8.32e-05 (2.93e-04)	Tok/s 85858 (93533)	Loss/tok 3.3830 (4.7298)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.060 (0.075)	Data 7.89e-05 (2.92e-04)	Tok/s 87916 (93578)	Loss/tok 3.3224 (4.7218)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.083 (0.075)	Data 9.39e-05 (2.91e-04)	Tok/s 101525 (93633)	Loss/tok 3.7425 (4.7141)	LR 2.000e-03
0: TRAIN [0][1620/1938]	Time 0.060 (0.075)	Data 8.39e-05 (2.89e-04)	Tok/s 87765 (93637)	Loss/tok 3.3589 (4.7071)	LR 2.000e-03
0: TRAIN [0][1630/1938]	Time 0.083 (0.075)	Data 8.13e-05 (2.88e-04)	Tok/s 101977 (93637)	Loss/tok 3.7488 (4.7003)	LR 2.000e-03
0: TRAIN [0][1640/1938]	Time 0.060 (0.075)	Data 8.11e-05 (2.87e-04)	Tok/s 84055 (93632)	Loss/tok 3.3429 (4.6934)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1650/1938]	Time 0.106 (0.075)	Data 7.75e-05 (2.86e-04)	Tok/s 107476 (93577)	Loss/tok 3.9630 (4.6878)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.083 (0.075)	Data 8.32e-05 (2.84e-04)	Tok/s 100130 (93606)	Loss/tok 3.5145 (4.6805)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.106 (0.075)	Data 9.66e-05 (2.83e-04)	Tok/s 113254 (93606)	Loss/tok 3.6694 (4.6741)	LR 2.000e-03
0: TRAIN [0][1680/1938]	Time 0.083 (0.075)	Data 8.11e-05 (2.82e-04)	Tok/s 100527 (93634)	Loss/tok 3.6893 (4.6671)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.039 (0.075)	Data 7.96e-05 (2.81e-04)	Tok/s 66776 (93603)	Loss/tok 2.7822 (4.6613)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.060 (0.075)	Data 8.18e-05 (2.80e-04)	Tok/s 89837 (93579)	Loss/tok 3.3497 (4.6555)	LR 2.000e-03
0: TRAIN [0][1710/1938]	Time 0.083 (0.075)	Data 8.18e-05 (2.78e-04)	Tok/s 102776 (93607)	Loss/tok 3.6898 (4.6489)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.083 (0.075)	Data 8.37e-05 (2.77e-04)	Tok/s 101451 (93618)	Loss/tok 3.4883 (4.6421)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.060 (0.075)	Data 8.03e-05 (2.76e-04)	Tok/s 83525 (93633)	Loss/tok 3.4327 (4.6359)	LR 2.000e-03
0: TRAIN [0][1740/1938]	Time 0.060 (0.075)	Data 8.01e-05 (2.75e-04)	Tok/s 86045 (93619)	Loss/tok 3.5036 (4.6300)	LR 2.000e-03
0: TRAIN [0][1750/1938]	Time 0.083 (0.075)	Data 8.06e-05 (2.74e-04)	Tok/s 102430 (93634)	Loss/tok 3.4480 (4.6237)	LR 2.000e-03
0: TRAIN [0][1760/1938]	Time 0.134 (0.075)	Data 8.11e-05 (2.73e-04)	Tok/s 111259 (93650)	Loss/tok 3.7914 (4.6170)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.060 (0.075)	Data 8.89e-05 (2.72e-04)	Tok/s 83419 (93667)	Loss/tok 3.2881 (4.6105)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.060 (0.075)	Data 8.51e-05 (2.71e-04)	Tok/s 84689 (93660)	Loss/tok 3.3560 (4.6045)	LR 2.000e-03
0: TRAIN [0][1790/1938]	Time 0.060 (0.075)	Data 7.89e-05 (2.70e-04)	Tok/s 82519 (93644)	Loss/tok 3.2038 (4.5987)	LR 2.000e-03
0: TRAIN [0][1800/1938]	Time 0.083 (0.075)	Data 8.70e-05 (2.69e-04)	Tok/s 100702 (93652)	Loss/tok 3.5676 (4.5928)	LR 2.000e-03
0: TRAIN [0][1810/1938]	Time 0.083 (0.075)	Data 7.92e-05 (2.68e-04)	Tok/s 102688 (93599)	Loss/tok 3.5379 (4.5880)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.083 (0.075)	Data 7.96e-05 (2.67e-04)	Tok/s 101002 (93590)	Loss/tok 3.5572 (4.5826)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.060 (0.075)	Data 8.03e-05 (2.66e-04)	Tok/s 90316 (93578)	Loss/tok 3.1918 (4.5772)	LR 2.000e-03
0: TRAIN [0][1840/1938]	Time 0.083 (0.075)	Data 8.18e-05 (2.65e-04)	Tok/s 99600 (93590)	Loss/tok 3.5740 (4.5717)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.060 (0.075)	Data 8.08e-05 (2.64e-04)	Tok/s 83877 (93582)	Loss/tok 3.1871 (4.5660)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.060 (0.075)	Data 8.42e-05 (2.63e-04)	Tok/s 87494 (93580)	Loss/tok 3.3374 (4.5608)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1870/1938]	Time 0.060 (0.075)	Data 8.58e-05 (2.62e-04)	Tok/s 86960 (93566)	Loss/tok 3.3150 (4.5556)	LR 2.000e-03
0: TRAIN [0][1880/1938]	Time 0.106 (0.075)	Data 8.75e-05 (2.61e-04)	Tok/s 110384 (93625)	Loss/tok 3.6778 (4.5492)	LR 2.000e-03
0: TRAIN [0][1890/1938]	Time 0.060 (0.075)	Data 8.87e-05 (2.60e-04)	Tok/s 85643 (93659)	Loss/tok 3.2803 (4.5431)	LR 2.000e-03
0: TRAIN [0][1900/1938]	Time 0.107 (0.075)	Data 8.49e-05 (2.59e-04)	Tok/s 108134 (93658)	Loss/tok 3.7263 (4.5376)	LR 2.000e-03
0: TRAIN [0][1910/1938]	Time 0.083 (0.075)	Data 8.73e-05 (2.58e-04)	Tok/s 100519 (93653)	Loss/tok 3.4728 (4.5321)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.083 (0.075)	Data 8.25e-05 (2.57e-04)	Tok/s 101802 (93655)	Loss/tok 3.7116 (4.5269)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.060 (0.075)	Data 8.27e-05 (2.56e-04)	Tok/s 88142 (93611)	Loss/tok 3.2514 (4.5225)	LR 2.000e-03
:::MLL 1560820902.028 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1560820902.028 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.487 (0.487)	Decoder iters 149.0 (149.0)	Tok/s 18534 (18534)
0: Running moses detokenizer
0: BLEU(score=19.74217884408276, counts=[34882, 15965, 8504, 4683], totals=[66408, 63405, 60402, 57404], precisions=[52.52680399951813, 25.179402255342637, 14.079004006489852, 8.157968085847676], bp=1.0, sys_len=66408, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1560820903.249 eval_accuracy: {"value": 19.74, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1560820903.249 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 4.5180	Test BLEU: 19.74
0: Performance: Epoch: 0	Training: 1497253 Tok/s
0: Finished epoch 0
:::MLL 1560820903.250 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1560820903.250 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1560820903.250 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 2613280408
0: TRAIN [1][0/1938]	Time 0.404 (0.404)	Data 2.66e-01 (2.66e-01)	Tok/s 28320 (28320)	Loss/tok 3.7966 (3.7966)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.083 (0.104)	Data 8.06e-05 (2.42e-02)	Tok/s 101540 (86509)	Loss/tok 3.3573 (3.5280)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.060 (0.088)	Data 8.46e-05 (1.27e-02)	Tok/s 84432 (87923)	Loss/tok 3.1609 (3.4852)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.060 (0.086)	Data 7.94e-05 (8.65e-03)	Tok/s 85607 (91393)	Loss/tok 3.3233 (3.5099)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.083 (0.081)	Data 8.08e-05 (6.56e-03)	Tok/s 102029 (90293)	Loss/tok 3.3089 (3.4717)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.083 (0.079)	Data 8.13e-05 (5.29e-03)	Tok/s 100219 (90891)	Loss/tok 3.4684 (3.4703)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.083 (0.077)	Data 8.32e-05 (4.44e-03)	Tok/s 99424 (90564)	Loss/tok 3.5766 (3.4515)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.134 (0.076)	Data 9.16e-05 (3.83e-03)	Tok/s 111552 (90379)	Loss/tok 3.8428 (3.4514)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.060 (0.076)	Data 8.34e-05 (3.36e-03)	Tok/s 83895 (90958)	Loss/tok 3.2940 (3.4605)	LR 2.000e-03
0: TRAIN [1][90/1938]	Time 0.083 (0.076)	Data 8.20e-05 (3.00e-03)	Tok/s 100388 (91400)	Loss/tok 3.4155 (3.4611)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.061 (0.076)	Data 8.20e-05 (2.71e-03)	Tok/s 85792 (91291)	Loss/tok 3.4115 (3.4608)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.060 (0.074)	Data 8.23e-05 (2.48e-03)	Tok/s 85980 (90721)	Loss/tok 3.2219 (3.4502)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.060 (0.075)	Data 8.11e-05 (2.28e-03)	Tok/s 85674 (91231)	Loss/tok 3.1384 (3.4496)	LR 2.000e-03
0: TRAIN [1][130/1938]	Time 0.106 (0.075)	Data 8.39e-05 (2.11e-03)	Tok/s 110829 (91516)	Loss/tok 3.7261 (3.4553)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][140/1938]	Time 0.060 (0.076)	Data 8.23e-05 (1.97e-03)	Tok/s 87091 (92063)	Loss/tok 3.0909 (3.4620)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.083 (0.075)	Data 8.56e-05 (1.84e-03)	Tok/s 102410 (91737)	Loss/tok 3.3878 (3.4581)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.060 (0.075)	Data 7.99e-05 (1.73e-03)	Tok/s 85473 (91733)	Loss/tok 3.1518 (3.4555)	LR 2.000e-03
0: TRAIN [1][170/1938]	Time 0.060 (0.075)	Data 8.46e-05 (1.64e-03)	Tok/s 86365 (91566)	Loss/tok 3.2201 (3.4474)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.060 (0.075)	Data 9.16e-05 (1.55e-03)	Tok/s 83128 (91531)	Loss/tok 3.2543 (3.4491)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.083 (0.074)	Data 8.44e-05 (1.48e-03)	Tok/s 101370 (90994)	Loss/tok 3.5030 (3.4400)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.105 (0.074)	Data 8.92e-05 (1.41e-03)	Tok/s 109534 (91444)	Loss/tok 3.7104 (3.4542)	LR 2.000e-03
0: TRAIN [1][210/1938]	Time 0.083 (0.074)	Data 8.75e-05 (1.34e-03)	Tok/s 100303 (91343)	Loss/tok 3.5153 (3.4494)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.060 (0.074)	Data 8.96e-05 (1.29e-03)	Tok/s 86974 (91324)	Loss/tok 3.4170 (3.4495)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.060 (0.074)	Data 8.58e-05 (1.23e-03)	Tok/s 85806 (91517)	Loss/tok 3.3205 (3.4466)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.060 (0.074)	Data 1.02e-04 (1.19e-03)	Tok/s 84772 (91649)	Loss/tok 3.2533 (3.4525)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.083 (0.074)	Data 9.04e-05 (1.14e-03)	Tok/s 100012 (91565)	Loss/tok 3.2706 (3.4484)	LR 2.000e-03
0: TRAIN [1][260/1938]	Time 0.083 (0.074)	Data 8.46e-05 (1.10e-03)	Tok/s 100695 (91349)	Loss/tok 3.5642 (3.4449)	LR 2.000e-03
0: TRAIN [1][270/1938]	Time 0.060 (0.074)	Data 8.77e-05 (1.07e-03)	Tok/s 85047 (91347)	Loss/tok 3.2965 (3.4451)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.060 (0.073)	Data 8.58e-05 (1.03e-03)	Tok/s 86940 (91260)	Loss/tok 3.2577 (3.4397)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.060 (0.073)	Data 9.30e-05 (9.98e-04)	Tok/s 85999 (91127)	Loss/tok 2.9914 (3.4386)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.083 (0.073)	Data 8.34e-05 (9.68e-04)	Tok/s 99106 (90887)	Loss/tok 3.5510 (3.4333)	LR 2.000e-03
0: TRAIN [1][310/1938]	Time 0.134 (0.073)	Data 8.46e-05 (9.40e-04)	Tok/s 112001 (91077)	Loss/tok 4.0504 (3.4357)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.083 (0.073)	Data 8.68e-05 (9.13e-04)	Tok/s 103255 (91219)	Loss/tok 3.5741 (3.4370)	LR 2.000e-03
0: TRAIN [1][330/1938]	Time 0.060 (0.073)	Data 8.46e-05 (8.88e-04)	Tok/s 84546 (91089)	Loss/tok 3.2537 (3.4337)	LR 2.000e-03
0: TRAIN [1][340/1938]	Time 0.060 (0.073)	Data 9.63e-05 (8.65e-04)	Tok/s 87242 (91158)	Loss/tok 3.2982 (3.4322)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.083 (0.072)	Data 1.05e-04 (8.43e-04)	Tok/s 102511 (91139)	Loss/tok 3.3917 (3.4297)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.083 (0.073)	Data 8.49e-05 (8.22e-04)	Tok/s 102772 (91434)	Loss/tok 3.2782 (3.4337)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.039 (0.073)	Data 8.65e-05 (8.02e-04)	Tok/s 64484 (91401)	Loss/tok 2.8057 (3.4317)	LR 2.000e-03
0: TRAIN [1][380/1938]	Time 0.060 (0.073)	Data 8.15e-05 (7.83e-04)	Tok/s 83803 (91445)	Loss/tok 3.2676 (3.4300)	LR 2.000e-03
0: TRAIN [1][390/1938]	Time 0.106 (0.073)	Data 8.85e-05 (7.65e-04)	Tok/s 110198 (91596)	Loss/tok 3.5696 (3.4322)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][400/1938]	Time 0.083 (0.073)	Data 8.39e-05 (7.48e-04)	Tok/s 101412 (91835)	Loss/tok 3.4036 (3.4330)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.060 (0.073)	Data 9.13e-05 (7.32e-04)	Tok/s 84620 (91791)	Loss/tok 3.3144 (3.4309)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.060 (0.073)	Data 7.94e-05 (7.16e-04)	Tok/s 84783 (91797)	Loss/tok 3.2181 (3.4281)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][430/1938]	Time 0.134 (0.073)	Data 8.37e-05 (7.02e-04)	Tok/s 112811 (91895)	Loss/tok 3.7384 (3.4318)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.060 (0.073)	Data 8.06e-05 (6.88e-04)	Tok/s 83867 (91866)	Loss/tok 3.2039 (3.4291)	LR 2.000e-03
0: TRAIN [1][450/1938]	Time 0.060 (0.073)	Data 8.85e-05 (6.74e-04)	Tok/s 85830 (91869)	Loss/tok 3.2022 (3.4282)	LR 2.000e-03
0: TRAIN [1][460/1938]	Time 0.060 (0.073)	Data 7.96e-05 (6.61e-04)	Tok/s 85728 (91868)	Loss/tok 3.2891 (3.4254)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.134 (0.073)	Data 8.30e-05 (6.49e-04)	Tok/s 112172 (91910)	Loss/tok 3.9226 (3.4270)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.060 (0.073)	Data 8.20e-05 (6.37e-04)	Tok/s 86731 (91822)	Loss/tok 3.4814 (3.4256)	LR 2.000e-03
0: TRAIN [1][490/1938]	Time 0.060 (0.073)	Data 8.20e-05 (6.26e-04)	Tok/s 87015 (91906)	Loss/tok 3.0500 (3.4243)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.060 (0.073)	Data 8.08e-05 (6.15e-04)	Tok/s 86879 (91938)	Loss/tok 3.1512 (3.4241)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.083 (0.073)	Data 8.18e-05 (6.05e-04)	Tok/s 100154 (92054)	Loss/tok 3.3907 (3.4234)	LR 2.000e-03
0: TRAIN [1][520/1938]	Time 0.105 (0.073)	Data 7.70e-05 (5.95e-04)	Tok/s 110537 (92188)	Loss/tok 3.6042 (3.4254)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.083 (0.073)	Data 8.11e-05 (5.85e-04)	Tok/s 101680 (92221)	Loss/tok 3.4923 (3.4250)	LR 2.000e-03
0: TRAIN [1][540/1938]	Time 0.134 (0.074)	Data 8.65e-05 (5.76e-04)	Tok/s 111088 (92416)	Loss/tok 3.6514 (3.4307)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.060 (0.074)	Data 8.30e-05 (5.67e-04)	Tok/s 85535 (92423)	Loss/tok 3.3759 (3.4320)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.060 (0.074)	Data 8.06e-05 (5.58e-04)	Tok/s 85992 (92402)	Loss/tok 3.4043 (3.4309)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.083 (0.074)	Data 8.20e-05 (5.50e-04)	Tok/s 101199 (92362)	Loss/tok 3.3119 (3.4285)	LR 2.000e-03
0: TRAIN [1][580/1938]	Time 0.060 (0.074)	Data 1.01e-04 (5.42e-04)	Tok/s 84035 (92400)	Loss/tok 3.0423 (3.4271)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.134 (0.074)	Data 9.35e-05 (5.34e-04)	Tok/s 112171 (92462)	Loss/tok 3.8893 (3.4277)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.106 (0.074)	Data 8.70e-05 (5.27e-04)	Tok/s 109368 (92467)	Loss/tok 3.6795 (3.4283)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.039 (0.073)	Data 7.89e-05 (5.19e-04)	Tok/s 67022 (92299)	Loss/tok 2.6735 (3.4256)	LR 2.000e-03
0: TRAIN [1][620/1938]	Time 0.083 (0.073)	Data 8.32e-05 (5.12e-04)	Tok/s 101515 (92402)	Loss/tok 3.4593 (3.4261)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.059 (0.074)	Data 8.54e-05 (5.06e-04)	Tok/s 86010 (92471)	Loss/tok 3.1552 (3.4269)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.060 (0.074)	Data 8.82e-05 (4.99e-04)	Tok/s 87328 (92444)	Loss/tok 3.2123 (3.4276)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.106 (0.074)	Data 8.65e-05 (4.93e-04)	Tok/s 109902 (92562)	Loss/tok 3.5760 (3.4287)	LR 2.000e-03
0: TRAIN [1][660/1938]	Time 0.060 (0.074)	Data 8.75e-05 (4.87e-04)	Tok/s 88477 (92579)	Loss/tok 3.3262 (3.4312)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.060 (0.074)	Data 8.82e-05 (4.81e-04)	Tok/s 86136 (92538)	Loss/tok 3.2975 (3.4297)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.039 (0.074)	Data 9.56e-05 (4.75e-04)	Tok/s 67704 (92509)	Loss/tok 2.8173 (3.4269)	LR 2.000e-03
0: TRAIN [1][690/1938]	Time 0.083 (0.074)	Data 9.61e-05 (4.69e-04)	Tok/s 102988 (92497)	Loss/tok 3.5051 (3.4262)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][700/1938]	Time 0.060 (0.074)	Data 9.23e-05 (4.64e-04)	Tok/s 85902 (92553)	Loss/tok 3.1881 (3.4247)	LR 2.000e-03
0: TRAIN [1][710/1938]	Time 0.106 (0.074)	Data 8.46e-05 (4.59e-04)	Tok/s 112311 (92636)	Loss/tok 3.6151 (3.4250)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.083 (0.074)	Data 8.51e-05 (4.53e-04)	Tok/s 101484 (92701)	Loss/tok 3.4600 (3.4249)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.060 (0.074)	Data 8.20e-05 (4.48e-04)	Tok/s 83126 (92704)	Loss/tok 3.2199 (3.4239)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][740/1938]	Time 0.083 (0.074)	Data 8.58e-05 (4.44e-04)	Tok/s 99312 (92685)	Loss/tok 3.4818 (3.4241)	LR 2.000e-03
0: TRAIN [1][750/1938]	Time 0.134 (0.074)	Data 9.20e-05 (4.39e-04)	Tok/s 113692 (92695)	Loss/tok 3.6976 (3.4264)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.060 (0.074)	Data 8.61e-05 (4.34e-04)	Tok/s 87023 (92716)	Loss/tok 3.1368 (3.4256)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.060 (0.074)	Data 8.70e-05 (4.30e-04)	Tok/s 83512 (92781)	Loss/tok 3.1488 (3.4263)	LR 2.000e-03
0: TRAIN [1][780/1938]	Time 0.106 (0.074)	Data 8.65e-05 (4.25e-04)	Tok/s 108786 (92743)	Loss/tok 3.6590 (3.4250)	LR 2.000e-03
0: TRAIN [1][790/1938]	Time 0.083 (0.074)	Data 8.42e-05 (4.21e-04)	Tok/s 101976 (92776)	Loss/tok 3.3687 (3.4255)	LR 2.000e-03
0: TRAIN [1][800/1938]	Time 0.106 (0.074)	Data 8.46e-05 (4.17e-04)	Tok/s 111172 (92727)	Loss/tok 3.6858 (3.4248)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.105 (0.074)	Data 8.70e-05 (4.13e-04)	Tok/s 110864 (92743)	Loss/tok 3.4278 (3.4242)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.083 (0.074)	Data 9.92e-05 (4.09e-04)	Tok/s 101719 (92781)	Loss/tok 3.3515 (3.4252)	LR 2.000e-03
0: TRAIN [1][830/1938]	Time 0.083 (0.074)	Data 8.30e-05 (4.05e-04)	Tok/s 99476 (92786)	Loss/tok 3.4794 (3.4243)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.060 (0.074)	Data 8.49e-05 (4.01e-04)	Tok/s 85641 (92830)	Loss/tok 3.1450 (3.4248)	LR 2.000e-03
0: TRAIN [1][850/1938]	Time 0.083 (0.074)	Data 8.42e-05 (3.98e-04)	Tok/s 102011 (92821)	Loss/tok 3.6203 (3.4242)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.060 (0.074)	Data 8.46e-05 (3.94e-04)	Tok/s 85920 (92841)	Loss/tok 3.0265 (3.4250)	LR 2.000e-03
0: TRAIN [1][870/1938]	Time 0.060 (0.074)	Data 9.11e-05 (3.91e-04)	Tok/s 87406 (92875)	Loss/tok 3.2206 (3.4251)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.083 (0.074)	Data 8.85e-05 (3.87e-04)	Tok/s 100585 (92889)	Loss/tok 3.5489 (3.4248)	LR 2.000e-03
0: TRAIN [1][890/1938]	Time 0.083 (0.074)	Data 9.87e-05 (3.84e-04)	Tok/s 101585 (92841)	Loss/tok 3.4228 (3.4236)	LR 2.000e-03
0: TRAIN [1][900/1938]	Time 0.134 (0.074)	Data 8.61e-05 (3.81e-04)	Tok/s 109766 (92822)	Loss/tok 3.9350 (3.4239)	LR 2.000e-03
0: TRAIN [1][910/1938]	Time 0.106 (0.074)	Data 8.39e-05 (3.77e-04)	Tok/s 111854 (92790)	Loss/tok 3.5372 (3.4231)	LR 2.000e-03
0: TRAIN [1][920/1938]	Time 0.106 (0.074)	Data 1.03e-04 (3.74e-04)	Tok/s 111572 (92773)	Loss/tok 3.7215 (3.4235)	LR 2.000e-03
0: TRAIN [1][930/1938]	Time 0.060 (0.074)	Data 8.49e-05 (3.71e-04)	Tok/s 87704 (92765)	Loss/tok 3.1324 (3.4230)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.061 (0.074)	Data 8.80e-05 (3.68e-04)	Tok/s 85911 (92773)	Loss/tok 3.1270 (3.4233)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.083 (0.074)	Data 8.77e-05 (3.65e-04)	Tok/s 101464 (92725)	Loss/tok 3.4558 (3.4221)	LR 2.000e-03
0: TRAIN [1][960/1938]	Time 0.039 (0.074)	Data 8.70e-05 (3.62e-04)	Tok/s 66961 (92748)	Loss/tok 2.7308 (3.4213)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.106 (0.074)	Data 8.13e-05 (3.59e-04)	Tok/s 110526 (92830)	Loss/tok 3.5988 (3.4219)	LR 2.000e-03
0: TRAIN [1][980/1938]	Time 0.106 (0.074)	Data 8.51e-05 (3.57e-04)	Tok/s 109197 (92886)	Loss/tok 3.5611 (3.4226)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.060 (0.074)	Data 8.32e-05 (3.54e-04)	Tok/s 85057 (92859)	Loss/tok 3.3170 (3.4225)	LR 2.000e-03
0: TRAIN [1][1000/1938]	Time 0.083 (0.074)	Data 8.85e-05 (3.51e-04)	Tok/s 103063 (92934)	Loss/tok 3.4270 (3.4228)	LR 2.000e-03
0: TRAIN [1][1010/1938]	Time 0.083 (0.074)	Data 8.51e-05 (3.49e-04)	Tok/s 101037 (92937)	Loss/tok 3.5796 (3.4238)	LR 2.000e-03
0: TRAIN [1][1020/1938]	Time 0.083 (0.074)	Data 8.32e-05 (3.46e-04)	Tok/s 100489 (92998)	Loss/tok 3.2979 (3.4240)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1030/1938]	Time 0.083 (0.074)	Data 8.27e-05 (3.43e-04)	Tok/s 103148 (93047)	Loss/tok 3.3193 (3.4248)	LR 2.000e-03
0: TRAIN [1][1040/1938]	Time 0.134 (0.075)	Data 8.44e-05 (3.41e-04)	Tok/s 110456 (93096)	Loss/tok 3.9051 (3.4258)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.105 (0.075)	Data 8.39e-05 (3.39e-04)	Tok/s 108225 (93142)	Loss/tok 3.5960 (3.4264)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.060 (0.075)	Data 8.25e-05 (3.36e-04)	Tok/s 86085 (93174)	Loss/tok 3.4030 (3.4266)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.106 (0.075)	Data 8.49e-05 (3.34e-04)	Tok/s 108960 (93191)	Loss/tok 3.6342 (3.4265)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][1080/1938]	Time 0.134 (0.075)	Data 8.27e-05 (3.32e-04)	Tok/s 112834 (93169)	Loss/tok 3.7712 (3.4265)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.083 (0.075)	Data 8.73e-05 (3.29e-04)	Tok/s 101218 (93208)	Loss/tok 3.4354 (3.4267)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.083 (0.075)	Data 8.70e-05 (3.27e-04)	Tok/s 99546 (93154)	Loss/tok 3.5177 (3.4260)	LR 2.000e-03
0: TRAIN [1][1110/1938]	Time 0.060 (0.075)	Data 8.68e-05 (3.25e-04)	Tok/s 87320 (93209)	Loss/tok 3.1905 (3.4257)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.060 (0.075)	Data 8.34e-05 (3.23e-04)	Tok/s 91307 (93202)	Loss/tok 3.2232 (3.4246)	LR 2.000e-03
0: TRAIN [1][1130/1938]	Time 0.106 (0.075)	Data 1.00e-04 (3.21e-04)	Tok/s 110108 (93242)	Loss/tok 3.6601 (3.4252)	LR 2.000e-03
0: TRAIN [1][1140/1938]	Time 0.083 (0.075)	Data 9.11e-05 (3.19e-04)	Tok/s 101439 (93272)	Loss/tok 3.4232 (3.4261)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.106 (0.075)	Data 8.70e-05 (3.17e-04)	Tok/s 108593 (93300)	Loss/tok 3.7614 (3.4263)	LR 2.000e-03
0: TRAIN [1][1160/1938]	Time 0.060 (0.075)	Data 8.51e-05 (3.15e-04)	Tok/s 82442 (93305)	Loss/tok 3.1578 (3.4267)	LR 2.000e-03
0: TRAIN [1][1170/1938]	Time 0.060 (0.075)	Data 9.32e-05 (3.13e-04)	Tok/s 86587 (93266)	Loss/tok 3.1858 (3.4255)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.039 (0.075)	Data 8.89e-05 (3.11e-04)	Tok/s 67246 (93246)	Loss/tok 2.7046 (3.4247)	LR 2.000e-03
0: TRAIN [1][1190/1938]	Time 0.083 (0.075)	Data 7.96e-05 (3.09e-04)	Tok/s 103411 (93232)	Loss/tok 3.3753 (3.4248)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.083 (0.075)	Data 8.23e-05 (3.07e-04)	Tok/s 100579 (93230)	Loss/tok 3.3274 (3.4237)	LR 2.000e-03
0: TRAIN [1][1210/1938]	Time 0.083 (0.075)	Data 8.11e-05 (3.05e-04)	Tok/s 102002 (93215)	Loss/tok 3.2804 (3.4232)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.083 (0.075)	Data 8.82e-05 (3.04e-04)	Tok/s 101288 (93252)	Loss/tok 3.4435 (3.4237)	LR 2.000e-03
0: TRAIN [1][1230/1938]	Time 0.083 (0.075)	Data 7.89e-05 (3.02e-04)	Tok/s 101065 (93304)	Loss/tok 3.4082 (3.4236)	LR 2.000e-03
0: TRAIN [1][1240/1938]	Time 0.060 (0.075)	Data 8.23e-05 (3.00e-04)	Tok/s 86957 (93288)	Loss/tok 3.2554 (3.4228)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][1250/1938]	Time 0.060 (0.075)	Data 8.51e-05 (2.98e-04)	Tok/s 84603 (93326)	Loss/tok 3.0463 (3.4233)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.083 (0.075)	Data 8.80e-05 (2.97e-04)	Tok/s 103336 (93367)	Loss/tok 3.3515 (3.4230)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.060 (0.075)	Data 8.08e-05 (2.95e-04)	Tok/s 85724 (93387)	Loss/tok 3.2756 (3.4235)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.060 (0.075)	Data 9.25e-05 (2.93e-04)	Tok/s 85209 (93362)	Loss/tok 3.1679 (3.4224)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.060 (0.075)	Data 8.63e-05 (2.92e-04)	Tok/s 86882 (93367)	Loss/tok 3.3508 (3.4225)	LR 2.000e-03
0: TRAIN [1][1300/1938]	Time 0.083 (0.075)	Data 8.42e-05 (2.90e-04)	Tok/s 102273 (93380)	Loss/tok 3.3434 (3.4218)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.083 (0.075)	Data 9.51e-05 (2.89e-04)	Tok/s 101459 (93393)	Loss/tok 3.2814 (3.4216)	LR 2.000e-03
0: TRAIN [1][1320/1938]	Time 0.060 (0.075)	Data 8.08e-05 (2.87e-04)	Tok/s 85400 (93377)	Loss/tok 3.1700 (3.4209)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.134 (0.075)	Data 1.03e-04 (2.85e-04)	Tok/s 110215 (93319)	Loss/tok 3.8010 (3.4205)	LR 2.000e-03
0: TRAIN [1][1340/1938]	Time 0.060 (0.075)	Data 8.56e-05 (2.84e-04)	Tok/s 86585 (93359)	Loss/tok 3.3378 (3.4220)	LR 2.000e-03
0: TRAIN [1][1350/1938]	Time 0.060 (0.075)	Data 9.66e-05 (2.83e-04)	Tok/s 85971 (93335)	Loss/tok 3.1134 (3.4211)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.060 (0.075)	Data 8.99e-05 (2.81e-04)	Tok/s 85913 (93322)	Loss/tok 3.0444 (3.4201)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.083 (0.075)	Data 8.75e-05 (2.80e-04)	Tok/s 102198 (93310)	Loss/tok 3.4338 (3.4191)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.083 (0.075)	Data 9.39e-05 (2.78e-04)	Tok/s 102645 (93329)	Loss/tok 3.3862 (3.4187)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.060 (0.075)	Data 8.58e-05 (2.77e-04)	Tok/s 87123 (93354)	Loss/tok 3.0490 (3.4180)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.083 (0.075)	Data 9.66e-05 (2.76e-04)	Tok/s 102027 (93402)	Loss/tok 3.4178 (3.4179)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.083 (0.075)	Data 8.34e-05 (2.74e-04)	Tok/s 101291 (93419)	Loss/tok 3.2024 (3.4173)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.083 (0.075)	Data 8.11e-05 (2.73e-04)	Tok/s 100713 (93419)	Loss/tok 3.3140 (3.4168)	LR 2.000e-03
0: TRAIN [1][1430/1938]	Time 0.060 (0.075)	Data 8.15e-05 (2.72e-04)	Tok/s 86852 (93411)	Loss/tok 3.0175 (3.4164)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.083 (0.075)	Data 8.42e-05 (2.70e-04)	Tok/s 102090 (93483)	Loss/tok 3.1801 (3.4166)	LR 2.000e-03
0: TRAIN [1][1450/1938]	Time 0.083 (0.075)	Data 8.03e-05 (2.69e-04)	Tok/s 100371 (93492)	Loss/tok 3.4117 (3.4166)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.083 (0.075)	Data 7.99e-05 (2.68e-04)	Tok/s 100364 (93509)	Loss/tok 3.2835 (3.4168)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.060 (0.075)	Data 1.03e-04 (2.67e-04)	Tok/s 88506 (93547)	Loss/tok 3.0636 (3.4168)	LR 2.000e-03
0: TRAIN [1][1480/1938]	Time 0.083 (0.075)	Data 7.84e-05 (2.65e-04)	Tok/s 102466 (93570)	Loss/tok 3.2786 (3.4165)	LR 2.000e-03
0: TRAIN [1][1490/1938]	Time 0.060 (0.075)	Data 1.71e-04 (2.64e-04)	Tok/s 87612 (93562)	Loss/tok 3.2934 (3.4162)	LR 2.000e-03
0: TRAIN [1][1500/1938]	Time 0.083 (0.075)	Data 8.77e-05 (2.63e-04)	Tok/s 100606 (93585)	Loss/tok 3.4116 (3.4159)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1510/1938]	Time 0.057 (0.075)	Data 8.82e-05 (2.62e-04)	Tok/s 90998 (93582)	Loss/tok 3.1974 (3.4162)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.083 (0.075)	Data 1.01e-04 (2.61e-04)	Tok/s 102112 (93588)	Loss/tok 3.3154 (3.4153)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.060 (0.075)	Data 1.57e-04 (2.60e-04)	Tok/s 85887 (93577)	Loss/tok 3.0902 (3.4141)	LR 2.000e-03
0: TRAIN [1][1540/1938]	Time 0.039 (0.075)	Data 8.06e-05 (2.59e-04)	Tok/s 68875 (93540)	Loss/tok 2.7301 (3.4133)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][1550/1938]	Time 0.039 (0.075)	Data 8.39e-05 (2.57e-04)	Tok/s 66036 (93536)	Loss/tok 2.5994 (3.4135)	LR 2.000e-03
0: TRAIN [1][1560/1938]	Time 0.039 (0.075)	Data 8.34e-05 (2.56e-04)	Tok/s 68826 (93552)	Loss/tok 2.5935 (3.4138)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.083 (0.075)	Data 9.08e-05 (2.55e-04)	Tok/s 101794 (93588)	Loss/tok 3.3524 (3.4141)	LR 2.000e-03
0: TRAIN [1][1580/1938]	Time 0.083 (0.075)	Data 8.01e-05 (2.54e-04)	Tok/s 101538 (93566)	Loss/tok 3.3233 (3.4132)	LR 2.000e-03
0: TRAIN [1][1590/1938]	Time 0.060 (0.075)	Data 8.11e-05 (2.53e-04)	Tok/s 85365 (93549)	Loss/tok 2.9798 (3.4127)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.083 (0.075)	Data 8.56e-05 (2.52e-04)	Tok/s 101008 (93582)	Loss/tok 3.3483 (3.4128)	LR 2.000e-03
0: TRAIN [1][1610/1938]	Time 0.083 (0.075)	Data 7.94e-05 (2.51e-04)	Tok/s 101715 (93612)	Loss/tok 3.4523 (3.4133)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.105 (0.075)	Data 8.39e-05 (2.50e-04)	Tok/s 109345 (93618)	Loss/tok 3.4092 (3.4125)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.083 (0.075)	Data 8.34e-05 (2.49e-04)	Tok/s 103873 (93666)	Loss/tok 3.5184 (3.4123)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.060 (0.075)	Data 8.39e-05 (2.48e-04)	Tok/s 86019 (93649)	Loss/tok 3.1509 (3.4122)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.060 (0.075)	Data 8.25e-05 (2.47e-04)	Tok/s 84644 (93669)	Loss/tok 3.2552 (3.4117)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.134 (0.076)	Data 8.39e-05 (2.46e-04)	Tok/s 110045 (93701)	Loss/tok 3.6829 (3.4118)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.060 (0.076)	Data 7.94e-05 (2.45e-04)	Tok/s 84824 (93703)	Loss/tok 3.1338 (3.4114)	LR 2.000e-03
0: TRAIN [1][1680/1938]	Time 0.060 (0.076)	Data 7.99e-05 (2.44e-04)	Tok/s 86831 (93705)	Loss/tok 3.1569 (3.4112)	LR 2.000e-03
0: TRAIN [1][1690/1938]	Time 0.134 (0.076)	Data 1.04e-04 (2.43e-04)	Tok/s 109280 (93720)	Loss/tok 3.8050 (3.4113)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.061 (0.076)	Data 8.06e-05 (2.42e-04)	Tok/s 86843 (93713)	Loss/tok 3.1043 (3.4107)	LR 2.000e-03
0: TRAIN [1][1710/1938]	Time 0.040 (0.076)	Data 9.27e-05 (2.41e-04)	Tok/s 65409 (93708)	Loss/tok 2.6240 (3.4101)	LR 2.000e-03
0: TRAIN [1][1720/1938]	Time 0.060 (0.075)	Data 8.32e-05 (2.40e-04)	Tok/s 85679 (93673)	Loss/tok 3.0615 (3.4092)	LR 2.000e-03
0: TRAIN [1][1730/1938]	Time 0.060 (0.075)	Data 8.87e-05 (2.40e-04)	Tok/s 83260 (93674)	Loss/tok 3.0643 (3.4088)	LR 2.000e-03
0: TRAIN [1][1740/1938]	Time 0.039 (0.075)	Data 8.54e-05 (2.39e-04)	Tok/s 65123 (93660)	Loss/tok 2.7608 (3.4079)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.106 (0.075)	Data 9.70e-05 (2.38e-04)	Tok/s 111052 (93673)	Loss/tok 3.5408 (3.4075)	LR 2.000e-03
0: TRAIN [1][1760/1938]	Time 0.039 (0.075)	Data 1.03e-04 (2.37e-04)	Tok/s 67656 (93662)	Loss/tok 2.7726 (3.4066)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.060 (0.075)	Data 8.03e-05 (2.36e-04)	Tok/s 85981 (93659)	Loss/tok 3.1715 (3.4067)	LR 2.000e-03
0: TRAIN [1][1780/1938]	Time 0.134 (0.075)	Data 8.30e-05 (2.35e-04)	Tok/s 113556 (93637)	Loss/tok 3.6263 (3.4062)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.083 (0.076)	Data 8.23e-05 (2.35e-04)	Tok/s 101325 (93669)	Loss/tok 3.2437 (3.4064)	LR 2.000e-03
0: TRAIN [1][1800/1938]	Time 0.083 (0.075)	Data 8.99e-05 (2.34e-04)	Tok/s 100764 (93637)	Loss/tok 3.3820 (3.4057)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.060 (0.075)	Data 8.42e-05 (2.33e-04)	Tok/s 84335 (93568)	Loss/tok 3.2196 (3.4047)	LR 2.000e-03
0: TRAIN [1][1820/1938]	Time 0.060 (0.075)	Data 8.27e-05 (2.32e-04)	Tok/s 85584 (93590)	Loss/tok 3.1512 (3.4041)	LR 2.000e-03
0: TRAIN [1][1830/1938]	Time 0.060 (0.075)	Data 8.03e-05 (2.31e-04)	Tok/s 85431 (93590)	Loss/tok 3.0754 (3.4037)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.060 (0.075)	Data 7.92e-05 (2.30e-04)	Tok/s 86443 (93525)	Loss/tok 3.1305 (3.4026)	LR 2.000e-03
0: TRAIN [1][1850/1938]	Time 0.060 (0.075)	Data 8.68e-05 (2.30e-04)	Tok/s 85754 (93531)	Loss/tok 3.4275 (3.4024)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.040 (0.075)	Data 8.49e-05 (2.29e-04)	Tok/s 64066 (93495)	Loss/tok 2.5650 (3.4018)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.040 (0.075)	Data 8.20e-05 (2.28e-04)	Tok/s 67377 (93481)	Loss/tok 2.7107 (3.4013)	LR 2.000e-03
0: TRAIN [1][1880/1938]	Time 0.083 (0.075)	Data 8.25e-05 (2.27e-04)	Tok/s 100703 (93498)	Loss/tok 3.3471 (3.4011)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1890/1938]	Time 0.134 (0.075)	Data 8.96e-05 (2.27e-04)	Tok/s 111689 (93492)	Loss/tok 3.6232 (3.4008)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.134 (0.075)	Data 1.12e-04 (2.26e-04)	Tok/s 111184 (93503)	Loss/tok 3.6254 (3.4009)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.106 (0.075)	Data 8.37e-05 (2.25e-04)	Tok/s 110450 (93488)	Loss/tok 3.5494 (3.4002)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.134 (0.075)	Data 9.11e-05 (2.24e-04)	Tok/s 112340 (93513)	Loss/tok 3.4912 (3.3999)	LR 2.000e-03
0: TRAIN [1][1930/1938]	Time 0.060 (0.075)	Data 8.70e-05 (2.24e-04)	Tok/s 86860 (93513)	Loss/tok 2.9579 (3.3995)	LR 2.000e-03
:::MLL 1560821049.632 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1560821049.633 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.488 (0.488)	Decoder iters 149.0 (149.0)	Tok/s 18509 (18509)
0: Running moses detokenizer
0: BLEU(score=22.200833988717292, counts=[36003, 17354, 9632, 5555], totals=[65503, 62500, 59497, 56497], precisions=[54.963894783445035, 27.7664, 16.189051548817588, 9.83238048037949], bp=1.0, sys_len=65503, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1560821050.868 eval_accuracy: {"value": 22.2, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1560821050.868 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.3974	Test BLEU: 22.20
0: Performance: Epoch: 1	Training: 1496640 Tok/s
0: Finished epoch 1
:::MLL 1560821050.869 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1560821050.869 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1560821050.869 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 3504622814
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [2][0/1938]	Time 0.432 (0.432)	Data 2.63e-01 (2.63e-01)	Tok/s 33962 (33962)	Loss/tok 3.7645 (3.7645)	LR 2.000e-03
0: TRAIN [2][10/1938]	Time 0.083 (0.101)	Data 9.68e-05 (2.40e-02)	Tok/s 102226 (83702)	Loss/tok 3.2121 (3.2823)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.060 (0.093)	Data 8.20e-05 (1.26e-02)	Tok/s 85365 (89002)	Loss/tok 3.0342 (3.3318)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.060 (0.086)	Data 8.23e-05 (8.58e-03)	Tok/s 85317 (89804)	Loss/tok 2.9977 (3.2987)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.060 (0.081)	Data 8.18e-05 (6.51e-03)	Tok/s 87846 (89522)	Loss/tok 3.0702 (3.2518)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.134 (0.082)	Data 8.15e-05 (5.25e-03)	Tok/s 108443 (91027)	Loss/tok 3.6742 (3.2829)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.060 (0.082)	Data 8.34e-05 (4.40e-03)	Tok/s 84687 (91639)	Loss/tok 3.1875 (3.2825)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.083 (0.080)	Data 8.73e-05 (3.79e-03)	Tok/s 101484 (91685)	Loss/tok 3.1897 (3.2760)	LR 2.000e-03
0: TRAIN [2][80/1938]	Time 0.083 (0.080)	Data 8.32e-05 (3.33e-03)	Tok/s 102122 (92387)	Loss/tok 3.0900 (3.2779)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.083 (0.080)	Data 8.20e-05 (2.98e-03)	Tok/s 103723 (92868)	Loss/tok 3.1399 (3.2749)	LR 2.000e-03
0: TRAIN [2][100/1938]	Time 0.083 (0.080)	Data 8.58e-05 (2.69e-03)	Tok/s 101365 (92977)	Loss/tok 3.3073 (3.2703)	LR 2.000e-03
0: TRAIN [2][110/1938]	Time 0.060 (0.078)	Data 9.11e-05 (2.46e-03)	Tok/s 84701 (92628)	Loss/tok 2.9919 (3.2664)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.083 (0.077)	Data 8.56e-05 (2.26e-03)	Tok/s 100866 (92347)	Loss/tok 3.1951 (3.2557)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.106 (0.078)	Data 8.30e-05 (2.09e-03)	Tok/s 111311 (92644)	Loss/tok 3.4284 (3.2603)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.060 (0.077)	Data 7.82e-05 (1.95e-03)	Tok/s 84005 (92441)	Loss/tok 3.1759 (3.2526)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.083 (0.077)	Data 8.99e-05 (1.83e-03)	Tok/s 101907 (92462)	Loss/tok 3.0960 (3.2517)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.083 (0.077)	Data 8.46e-05 (1.72e-03)	Tok/s 102700 (92786)	Loss/tok 3.1558 (3.2594)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.106 (0.077)	Data 8.37e-05 (1.62e-03)	Tok/s 111991 (92736)	Loss/tok 3.3357 (3.2569)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.083 (0.077)	Data 8.37e-05 (1.54e-03)	Tok/s 100434 (93020)	Loss/tok 3.2655 (3.2615)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.083 (0.077)	Data 1.02e-04 (1.46e-03)	Tok/s 99903 (93261)	Loss/tok 3.2287 (3.2631)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.083 (0.078)	Data 8.49e-05 (1.39e-03)	Tok/s 102105 (93567)	Loss/tok 3.3689 (3.2652)	LR 2.000e-03
0: TRAIN [2][210/1938]	Time 0.083 (0.078)	Data 8.15e-05 (1.33e-03)	Tok/s 103212 (93914)	Loss/tok 3.2526 (3.2634)	LR 2.000e-03
0: TRAIN [2][220/1938]	Time 0.083 (0.077)	Data 8.03e-05 (1.28e-03)	Tok/s 99972 (93773)	Loss/tok 3.2878 (3.2609)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.060 (0.078)	Data 8.06e-05 (1.22e-03)	Tok/s 86969 (93922)	Loss/tok 3.1877 (3.2654)	LR 2.000e-03
0: TRAIN [2][240/1938]	Time 0.083 (0.078)	Data 8.37e-05 (1.18e-03)	Tok/s 100559 (94105)	Loss/tok 3.3431 (3.2664)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.060 (0.078)	Data 7.84e-05 (1.13e-03)	Tok/s 86839 (93940)	Loss/tok 3.0299 (3.2626)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.040 (0.077)	Data 8.34e-05 (1.09e-03)	Tok/s 64998 (93812)	Loss/tok 2.6420 (3.2610)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.134 (0.077)	Data 8.13e-05 (1.06e-03)	Tok/s 108199 (93582)	Loss/tok 3.8378 (3.2603)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.083 (0.077)	Data 8.30e-05 (1.02e-03)	Tok/s 100934 (93516)	Loss/tok 3.5231 (3.2585)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.083 (0.077)	Data 8.11e-05 (9.89e-04)	Tok/s 100472 (93742)	Loss/tok 3.3844 (3.2620)	LR 2.000e-03
0: TRAIN [2][300/1938]	Time 0.106 (0.077)	Data 8.03e-05 (9.59e-04)	Tok/s 108134 (93691)	Loss/tok 3.5123 (3.2606)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.106 (0.076)	Data 8.03e-05 (9.31e-04)	Tok/s 112493 (93545)	Loss/tok 3.3448 (3.2574)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.106 (0.076)	Data 8.30e-05 (9.04e-04)	Tok/s 110919 (93674)	Loss/tok 3.5702 (3.2568)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.083 (0.076)	Data 8.34e-05 (8.80e-04)	Tok/s 98071 (93543)	Loss/tok 3.2719 (3.2543)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][340/1938]	Time 0.082 (0.076)	Data 8.25e-05 (8.56e-04)	Tok/s 102021 (93425)	Loss/tok 3.3133 (3.2533)	LR 2.000e-03
0: TRAIN [2][350/1938]	Time 0.083 (0.076)	Data 8.65e-05 (8.34e-04)	Tok/s 99655 (93522)	Loss/tok 3.2193 (3.2553)	LR 2.000e-03
0: TRAIN [2][360/1938]	Time 0.060 (0.076)	Data 8.15e-05 (8.14e-04)	Tok/s 85638 (93447)	Loss/tok 3.0149 (3.2548)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.083 (0.076)	Data 7.99e-05 (7.94e-04)	Tok/s 101078 (93345)	Loss/tok 3.1429 (3.2523)	LR 2.000e-03
0: TRAIN [2][380/1938]	Time 0.083 (0.076)	Data 8.54e-05 (7.75e-04)	Tok/s 100116 (93426)	Loss/tok 3.4280 (3.2521)	LR 2.000e-03
0: TRAIN [2][390/1938]	Time 0.083 (0.076)	Data 8.44e-05 (7.58e-04)	Tok/s 99613 (93529)	Loss/tok 3.2393 (3.2516)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.106 (0.076)	Data 8.70e-05 (7.41e-04)	Tok/s 109407 (93526)	Loss/tok 3.3206 (3.2512)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.083 (0.076)	Data 8.32e-05 (7.25e-04)	Tok/s 101030 (93645)	Loss/tok 3.3799 (3.2517)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.039 (0.076)	Data 8.25e-05 (7.10e-04)	Tok/s 65903 (93569)	Loss/tok 2.6049 (3.2504)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.060 (0.076)	Data 9.32e-05 (6.95e-04)	Tok/s 87416 (93563)	Loss/tok 3.0288 (3.2502)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.083 (0.076)	Data 8.37e-05 (6.81e-04)	Tok/s 101727 (93449)	Loss/tok 3.2244 (3.2482)	LR 2.000e-03
0: TRAIN [2][450/1938]	Time 0.060 (0.075)	Data 8.11e-05 (6.68e-04)	Tok/s 85067 (93298)	Loss/tok 3.0352 (3.2465)	LR 2.000e-03
0: TRAIN [2][460/1938]	Time 0.061 (0.075)	Data 8.15e-05 (6.56e-04)	Tok/s 84887 (93407)	Loss/tok 3.0427 (3.2468)	LR 2.000e-03
0: TRAIN [2][470/1938]	Time 0.106 (0.075)	Data 8.46e-05 (6.43e-04)	Tok/s 112053 (93365)	Loss/tok 3.3728 (3.2462)	LR 2.000e-03
0: TRAIN [2][480/1938]	Time 0.061 (0.076)	Data 8.54e-05 (6.32e-04)	Tok/s 84883 (93463)	Loss/tok 3.1034 (3.2491)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][490/1938]	Time 0.060 (0.076)	Data 8.06e-05 (6.21e-04)	Tok/s 87223 (93532)	Loss/tok 3.0594 (3.2531)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.040 (0.076)	Data 8.01e-05 (6.10e-04)	Tok/s 68298 (93458)	Loss/tok 2.7089 (3.2518)	LR 2.000e-03
0: TRAIN [2][510/1938]	Time 0.060 (0.075)	Data 8.42e-05 (6.00e-04)	Tok/s 88168 (93308)	Loss/tok 2.9786 (3.2514)	LR 2.000e-03
0: TRAIN [2][520/1938]	Time 0.083 (0.075)	Data 8.44e-05 (5.90e-04)	Tok/s 102554 (93330)	Loss/tok 3.2958 (3.2510)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.134 (0.076)	Data 8.20e-05 (5.80e-04)	Tok/s 112743 (93278)	Loss/tok 3.5359 (3.2526)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.083 (0.075)	Data 8.70e-05 (5.71e-04)	Tok/s 100237 (93240)	Loss/tok 3.2820 (3.2522)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.060 (0.075)	Data 8.44e-05 (5.62e-04)	Tok/s 85454 (93290)	Loss/tok 2.9929 (3.2517)	LR 2.000e-03
0: TRAIN [2][560/1938]	Time 0.061 (0.075)	Data 8.37e-05 (5.54e-04)	Tok/s 86001 (93259)	Loss/tok 2.9808 (3.2509)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.060 (0.075)	Data 8.58e-05 (5.46e-04)	Tok/s 85530 (93113)	Loss/tok 3.2851 (3.2493)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.083 (0.075)	Data 8.13e-05 (5.38e-04)	Tok/s 104109 (93083)	Loss/tok 3.1321 (3.2484)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.083 (0.075)	Data 8.37e-05 (5.30e-04)	Tok/s 97579 (93074)	Loss/tok 3.2742 (3.2472)	LR 2.000e-03
0: TRAIN [2][600/1938]	Time 0.083 (0.075)	Data 8.11e-05 (5.23e-04)	Tok/s 99894 (93079)	Loss/tok 3.1739 (3.2456)	LR 2.000e-03
0: TRAIN [2][610/1938]	Time 0.083 (0.075)	Data 8.32e-05 (5.16e-04)	Tok/s 98858 (93034)	Loss/tok 3.3848 (3.2459)	LR 2.000e-03
0: TRAIN [2][620/1938]	Time 0.039 (0.075)	Data 8.25e-05 (5.09e-04)	Tok/s 69364 (93093)	Loss/tok 2.5890 (3.2475)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.106 (0.075)	Data 8.63e-05 (5.02e-04)	Tok/s 111137 (92973)	Loss/tok 3.4354 (3.2463)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.039 (0.075)	Data 8.34e-05 (4.96e-04)	Tok/s 66221 (93046)	Loss/tok 2.5285 (3.2466)	LR 2.000e-03
0: TRAIN [2][650/1938]	Time 0.039 (0.075)	Data 8.27e-05 (4.89e-04)	Tok/s 65661 (92933)	Loss/tok 2.7896 (3.2449)	LR 2.000e-03
0: TRAIN [2][660/1938]	Time 0.083 (0.075)	Data 8.54e-05 (4.83e-04)	Tok/s 100147 (92897)	Loss/tok 3.2990 (3.2460)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.060 (0.075)	Data 8.23e-05 (4.77e-04)	Tok/s 82778 (92894)	Loss/tok 2.8872 (3.2458)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.060 (0.075)	Data 7.94e-05 (4.71e-04)	Tok/s 84937 (92873)	Loss/tok 2.9682 (3.2463)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.060 (0.075)	Data 8.30e-05 (4.66e-04)	Tok/s 87630 (92835)	Loss/tok 3.3776 (3.2463)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.083 (0.075)	Data 8.58e-05 (4.60e-04)	Tok/s 101260 (92844)	Loss/tok 3.2592 (3.2465)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.060 (0.075)	Data 9.75e-05 (4.55e-04)	Tok/s 86156 (92929)	Loss/tok 3.1486 (3.2471)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.060 (0.075)	Data 9.66e-05 (4.50e-04)	Tok/s 83471 (92918)	Loss/tok 3.0602 (3.2468)	LR 2.000e-03
0: TRAIN [2][730/1938]	Time 0.083 (0.075)	Data 8.49e-05 (4.45e-04)	Tok/s 100902 (92916)	Loss/tok 3.2674 (3.2463)	LR 2.000e-03
0: TRAIN [2][740/1938]	Time 0.106 (0.075)	Data 8.18e-05 (4.40e-04)	Tok/s 110309 (92976)	Loss/tok 3.5123 (3.2466)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][750/1938]	Time 0.083 (0.075)	Data 9.44e-05 (4.35e-04)	Tok/s 101144 (92921)	Loss/tok 3.2208 (3.2460)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.060 (0.075)	Data 8.77e-05 (4.31e-04)	Tok/s 86992 (92902)	Loss/tok 3.1325 (3.2461)	LR 2.000e-03
0: TRAIN [2][770/1938]	Time 0.083 (0.075)	Data 8.65e-05 (4.26e-04)	Tok/s 101477 (92929)	Loss/tok 3.1443 (3.2454)	LR 2.000e-03
0: TRAIN [2][780/1938]	Time 0.083 (0.074)	Data 8.46e-05 (4.22e-04)	Tok/s 102486 (92939)	Loss/tok 3.2185 (3.2443)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.134 (0.075)	Data 8.44e-05 (4.18e-04)	Tok/s 111885 (93019)	Loss/tok 3.4762 (3.2459)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][800/1938]	Time 0.083 (0.075)	Data 8.39e-05 (4.14e-04)	Tok/s 99191 (93009)	Loss/tok 3.4128 (3.2461)	LR 2.000e-03
0: TRAIN [2][810/1938]	Time 0.060 (0.075)	Data 8.39e-05 (4.10e-04)	Tok/s 87220 (93058)	Loss/tok 3.0309 (3.2482)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.060 (0.075)	Data 8.30e-05 (4.06e-04)	Tok/s 83934 (93125)	Loss/tok 3.0548 (3.2494)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.083 (0.075)	Data 8.42e-05 (4.02e-04)	Tok/s 101353 (93117)	Loss/tok 3.2539 (3.2488)	LR 2.000e-03
0: TRAIN [2][840/1938]	Time 0.060 (0.075)	Data 8.39e-05 (3.98e-04)	Tok/s 85966 (93149)	Loss/tok 2.9363 (3.2485)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.083 (0.075)	Data 8.25e-05 (3.94e-04)	Tok/s 101910 (93134)	Loss/tok 3.3534 (3.2482)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.083 (0.075)	Data 8.51e-05 (3.91e-04)	Tok/s 100484 (93076)	Loss/tok 3.3470 (3.2490)	LR 2.000e-03
0: TRAIN [2][870/1938]	Time 0.106 (0.075)	Data 8.20e-05 (3.87e-04)	Tok/s 111822 (93099)	Loss/tok 3.2445 (3.2489)	LR 2.000e-03
0: TRAIN [2][880/1938]	Time 0.060 (0.075)	Data 8.49e-05 (3.84e-04)	Tok/s 85936 (93075)	Loss/tok 2.8681 (3.2481)	LR 2.000e-03
0: TRAIN [2][890/1938]	Time 0.083 (0.075)	Data 8.18e-05 (3.80e-04)	Tok/s 100491 (93035)	Loss/tok 3.1353 (3.2474)	LR 2.000e-03
0: TRAIN [2][900/1938]	Time 0.060 (0.075)	Data 8.30e-05 (3.77e-04)	Tok/s 86948 (93044)	Loss/tok 3.0748 (3.2481)	LR 2.000e-03
0: TRAIN [2][910/1938]	Time 0.106 (0.075)	Data 8.20e-05 (3.74e-04)	Tok/s 110485 (93050)	Loss/tok 3.3627 (3.2477)	LR 2.000e-03
0: TRAIN [2][920/1938]	Time 0.105 (0.075)	Data 9.11e-05 (3.71e-04)	Tok/s 110357 (93106)	Loss/tok 3.4866 (3.2487)	LR 2.000e-03
0: TRAIN [2][930/1938]	Time 0.060 (0.075)	Data 8.94e-05 (3.68e-04)	Tok/s 86817 (93102)	Loss/tok 3.2285 (3.2491)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.083 (0.075)	Data 8.25e-05 (3.65e-04)	Tok/s 102716 (93109)	Loss/tok 3.1292 (3.2495)	LR 2.000e-03
0: TRAIN [2][950/1938]	Time 0.083 (0.075)	Data 8.46e-05 (3.62e-04)	Tok/s 99243 (93075)	Loss/tok 3.3356 (3.2489)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][960/1938]	Time 0.083 (0.075)	Data 9.49e-05 (3.59e-04)	Tok/s 98640 (93120)	Loss/tok 3.2315 (3.2510)	LR 2.000e-03
0: TRAIN [2][970/1938]	Time 0.106 (0.075)	Data 8.92e-05 (3.56e-04)	Tok/s 110967 (93155)	Loss/tok 3.4834 (3.2519)	LR 2.000e-03
0: TRAIN [2][980/1938]	Time 0.060 (0.075)	Data 9.01e-05 (3.54e-04)	Tok/s 83078 (93150)	Loss/tok 3.1587 (3.2525)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.060 (0.075)	Data 8.85e-05 (3.51e-04)	Tok/s 85406 (93102)	Loss/tok 3.2230 (3.2518)	LR 2.000e-03
0: TRAIN [2][1000/1938]	Time 0.039 (0.075)	Data 8.18e-05 (3.48e-04)	Tok/s 68118 (93059)	Loss/tok 2.6974 (3.2511)	LR 2.000e-03
0: TRAIN [2][1010/1938]	Time 0.106 (0.075)	Data 8.08e-05 (3.46e-04)	Tok/s 109814 (93115)	Loss/tok 3.4952 (3.2525)	LR 2.000e-03
0: TRAIN [2][1020/1938]	Time 0.060 (0.075)	Data 1.04e-04 (3.43e-04)	Tok/s 88141 (93085)	Loss/tok 3.0770 (3.2525)	LR 2.000e-03
0: TRAIN [2][1030/1938]	Time 0.060 (0.075)	Data 8.37e-05 (3.41e-04)	Tok/s 89842 (93128)	Loss/tok 3.0880 (3.2525)	LR 2.000e-03
0: TRAIN [2][1040/1938]	Time 0.060 (0.075)	Data 8.44e-05 (3.38e-04)	Tok/s 85156 (93093)	Loss/tok 3.0147 (3.2521)	LR 2.000e-03
0: TRAIN [2][1050/1938]	Time 0.060 (0.075)	Data 8.18e-05 (3.36e-04)	Tok/s 87576 (93096)	Loss/tok 3.0616 (3.2526)	LR 2.000e-03
0: TRAIN [2][1060/1938]	Time 0.039 (0.075)	Data 9.04e-05 (3.33e-04)	Tok/s 67173 (93138)	Loss/tok 2.7317 (3.2537)	LR 2.000e-03
0: TRAIN [2][1070/1938]	Time 0.106 (0.075)	Data 7.99e-05 (3.31e-04)	Tok/s 111498 (93155)	Loss/tok 3.3377 (3.2536)	LR 2.000e-03
0: TRAIN [2][1080/1938]	Time 0.040 (0.075)	Data 8.68e-05 (3.29e-04)	Tok/s 67210 (93177)	Loss/tok 2.5444 (3.2537)	LR 2.000e-03
0: TRAIN [2][1090/1938]	Time 0.039 (0.075)	Data 8.51e-05 (3.27e-04)	Tok/s 68712 (93107)	Loss/tok 2.6474 (3.2531)	LR 2.000e-03
0: TRAIN [2][1100/1938]	Time 0.060 (0.075)	Data 8.61e-05 (3.24e-04)	Tok/s 86937 (93087)	Loss/tok 3.1583 (3.2531)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.060 (0.075)	Data 8.23e-05 (3.22e-04)	Tok/s 86461 (93139)	Loss/tok 3.0178 (3.2541)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.106 (0.075)	Data 8.44e-05 (3.20e-04)	Tok/s 108973 (93112)	Loss/tok 3.5722 (3.2538)	LR 2.000e-03
0: TRAIN [2][1130/1938]	Time 0.083 (0.075)	Data 8.34e-05 (3.18e-04)	Tok/s 102144 (93088)	Loss/tok 3.1321 (3.2534)	LR 2.000e-03
0: TRAIN [2][1140/1938]	Time 0.106 (0.075)	Data 8.25e-05 (3.16e-04)	Tok/s 109124 (93081)	Loss/tok 3.5725 (3.2536)	LR 2.000e-03
0: TRAIN [2][1150/1938]	Time 0.039 (0.075)	Data 8.34e-05 (3.14e-04)	Tok/s 66020 (93029)	Loss/tok 2.5583 (3.2532)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][1160/1938]	Time 0.083 (0.075)	Data 9.08e-05 (3.12e-04)	Tok/s 100359 (93074)	Loss/tok 3.1459 (3.2536)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.039 (0.075)	Data 8.34e-05 (3.10e-04)	Tok/s 67566 (93039)	Loss/tok 2.7136 (3.2530)	LR 2.000e-03
0: TRAIN [2][1180/1938]	Time 0.060 (0.075)	Data 8.30e-05 (3.08e-04)	Tok/s 84420 (92997)	Loss/tok 3.1043 (3.2523)	LR 2.000e-03
0: TRAIN [2][1190/1938]	Time 0.083 (0.075)	Data 8.46e-05 (3.06e-04)	Tok/s 99464 (92989)	Loss/tok 3.3410 (3.2522)	LR 2.000e-03
0: TRAIN [2][1200/1938]	Time 0.106 (0.075)	Data 8.42e-05 (3.05e-04)	Tok/s 108550 (92980)	Loss/tok 3.4147 (3.2522)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.106 (0.075)	Data 8.65e-05 (3.03e-04)	Tok/s 110065 (92945)	Loss/tok 3.5223 (3.2517)	LR 2.000e-03
0: TRAIN [2][1220/1938]	Time 0.060 (0.075)	Data 9.06e-05 (3.01e-04)	Tok/s 85332 (92954)	Loss/tok 3.1095 (3.2531)	LR 2.000e-03
0: TRAIN [2][1230/1938]	Time 0.106 (0.075)	Data 8.49e-05 (2.99e-04)	Tok/s 109849 (93005)	Loss/tok 3.4006 (3.2536)	LR 2.000e-03
0: TRAIN [2][1240/1938]	Time 0.060 (0.075)	Data 8.54e-05 (2.97e-04)	Tok/s 83653 (92996)	Loss/tok 3.0690 (3.2533)	LR 2.000e-03
0: TRAIN [2][1250/1938]	Time 0.060 (0.075)	Data 8.56e-05 (2.96e-04)	Tok/s 86263 (92990)	Loss/tok 3.1016 (3.2524)	LR 2.000e-03
0: TRAIN [2][1260/1938]	Time 0.083 (0.075)	Data 8.46e-05 (2.94e-04)	Tok/s 101784 (93019)	Loss/tok 3.1529 (3.2527)	LR 2.000e-03
0: TRAIN [2][1270/1938]	Time 0.106 (0.075)	Data 8.75e-05 (2.92e-04)	Tok/s 108478 (93026)	Loss/tok 3.4010 (3.2536)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.039 (0.075)	Data 8.15e-05 (2.91e-04)	Tok/s 66930 (93042)	Loss/tok 2.6295 (3.2542)	LR 2.000e-03
0: TRAIN [2][1290/1938]	Time 0.060 (0.075)	Data 8.25e-05 (2.89e-04)	Tok/s 85299 (93057)	Loss/tok 3.0104 (3.2538)	LR 2.000e-03
0: TRAIN [2][1300/1938]	Time 0.060 (0.075)	Data 8.32e-05 (2.88e-04)	Tok/s 85705 (92996)	Loss/tok 3.1758 (3.2530)	LR 2.000e-03
0: TRAIN [2][1310/1938]	Time 0.060 (0.075)	Data 8.82e-05 (2.86e-04)	Tok/s 84740 (92986)	Loss/tok 3.1677 (3.2533)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][1320/1938]	Time 0.103 (0.075)	Data 8.42e-05 (2.85e-04)	Tok/s 112959 (93065)	Loss/tok 3.2911 (3.2555)	LR 2.000e-03
0: TRAIN [2][1330/1938]	Time 0.106 (0.075)	Data 8.58e-05 (2.83e-04)	Tok/s 108088 (93069)	Loss/tok 3.6202 (3.2563)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.083 (0.075)	Data 8.46e-05 (2.82e-04)	Tok/s 103927 (93078)	Loss/tok 3.1574 (3.2558)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.083 (0.075)	Data 8.65e-05 (2.80e-04)	Tok/s 105014 (93114)	Loss/tok 3.2020 (3.2554)	LR 2.000e-03
0: TRAIN [2][1360/1938]	Time 0.060 (0.075)	Data 1.06e-04 (2.79e-04)	Tok/s 85227 (93110)	Loss/tok 2.9850 (3.2553)	LR 2.000e-03
0: TRAIN [2][1370/1938]	Time 0.060 (0.075)	Data 8.32e-05 (2.77e-04)	Tok/s 84484 (93111)	Loss/tok 3.0698 (3.2559)	LR 2.000e-03
0: TRAIN [2][1380/1938]	Time 0.039 (0.075)	Data 8.06e-05 (2.76e-04)	Tok/s 66942 (93106)	Loss/tok 2.6206 (3.2560)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.135 (0.075)	Data 7.89e-05 (2.75e-04)	Tok/s 112745 (93144)	Loss/tok 3.5516 (3.2567)	LR 2.000e-03
0: TRAIN [2][1400/1938]	Time 0.060 (0.075)	Data 7.87e-05 (2.73e-04)	Tok/s 85959 (93157)	Loss/tok 3.0674 (3.2565)	LR 2.000e-03
0: TRAIN [2][1410/1938]	Time 0.060 (0.075)	Data 8.15e-05 (2.72e-04)	Tok/s 86131 (93148)	Loss/tok 3.0904 (3.2557)	LR 2.000e-03
0: TRAIN [2][1420/1938]	Time 0.134 (0.075)	Data 8.34e-05 (2.71e-04)	Tok/s 107579 (93192)	Loss/tok 3.6156 (3.2561)	LR 2.000e-03
0: TRAIN [2][1430/1938]	Time 0.083 (0.075)	Data 8.51e-05 (2.69e-04)	Tok/s 103212 (93174)	Loss/tok 3.1923 (3.2556)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.039 (0.075)	Data 8.01e-05 (2.68e-04)	Tok/s 64615 (93154)	Loss/tok 2.7156 (3.2556)	LR 2.000e-03
0: TRAIN [2][1450/1938]	Time 0.060 (0.075)	Data 7.87e-05 (2.67e-04)	Tok/s 83004 (93170)	Loss/tok 3.2263 (3.2557)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][1460/1938]	Time 0.081 (0.075)	Data 9.80e-05 (2.65e-04)	Tok/s 102271 (93195)	Loss/tok 3.3903 (3.2559)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.083 (0.075)	Data 8.96e-05 (2.64e-04)	Tok/s 102129 (93200)	Loss/tok 3.2197 (3.2558)	LR 2.000e-03
0: TRAIN [2][1480/1938]	Time 0.106 (0.075)	Data 8.63e-05 (2.63e-04)	Tok/s 110169 (93214)	Loss/tok 3.4466 (3.2562)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.040 (0.075)	Data 8.49e-05 (2.62e-04)	Tok/s 66301 (93195)	Loss/tok 2.5932 (3.2559)	LR 2.000e-03
0: TRAIN [2][1500/1938]	Time 0.060 (0.075)	Data 8.37e-05 (2.61e-04)	Tok/s 85959 (93210)	Loss/tok 3.0227 (3.2565)	LR 2.000e-03
0: TRAIN [2][1510/1938]	Time 0.060 (0.075)	Data 8.80e-05 (2.60e-04)	Tok/s 88678 (93184)	Loss/tok 2.9191 (3.2562)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.060 (0.075)	Data 8.61e-05 (2.58e-04)	Tok/s 86930 (93198)	Loss/tok 3.1536 (3.2558)	LR 2.000e-03
0: TRAIN [2][1530/1938]	Time 0.039 (0.075)	Data 8.89e-05 (2.57e-04)	Tok/s 64819 (93197)	Loss/tok 2.7303 (3.2559)	LR 2.000e-03
0: TRAIN [2][1540/1938]	Time 0.039 (0.075)	Data 8.94e-05 (2.56e-04)	Tok/s 66693 (93221)	Loss/tok 2.6990 (3.2563)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.083 (0.075)	Data 8.56e-05 (2.55e-04)	Tok/s 101833 (93221)	Loss/tok 3.2564 (3.2561)	LR 2.000e-03
0: TRAIN [2][1560/1938]	Time 0.060 (0.075)	Data 8.54e-05 (2.54e-04)	Tok/s 86085 (93214)	Loss/tok 2.9869 (3.2557)	LR 2.000e-03
0: TRAIN [2][1570/1938]	Time 0.060 (0.075)	Data 8.77e-05 (2.53e-04)	Tok/s 85188 (93214)	Loss/tok 3.0574 (3.2553)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.060 (0.075)	Data 8.77e-05 (2.52e-04)	Tok/s 84515 (93253)	Loss/tok 3.0895 (3.2562)	LR 2.000e-03
0: TRAIN [2][1590/1938]	Time 0.040 (0.075)	Data 9.85e-05 (2.51e-04)	Tok/s 66887 (93210)	Loss/tok 2.7446 (3.2557)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.083 (0.075)	Data 8.94e-05 (2.50e-04)	Tok/s 97745 (93264)	Loss/tok 3.4436 (3.2569)	LR 2.000e-03
0: TRAIN [2][1610/1938]	Time 0.060 (0.075)	Data 8.80e-05 (2.49e-04)	Tok/s 85099 (93278)	Loss/tok 3.1387 (3.2571)	LR 2.000e-03
0: TRAIN [2][1620/1938]	Time 0.061 (0.075)	Data 8.85e-05 (2.48e-04)	Tok/s 83280 (93253)	Loss/tok 3.1048 (3.2569)	LR 2.000e-03
0: TRAIN [2][1630/1938]	Time 0.083 (0.075)	Data 8.54e-05 (2.47e-04)	Tok/s 102141 (93217)	Loss/tok 3.2277 (3.2561)	LR 2.000e-03
0: TRAIN [2][1640/1938]	Time 0.060 (0.075)	Data 9.04e-05 (2.46e-04)	Tok/s 86078 (93196)	Loss/tok 2.9458 (3.2563)	LR 2.000e-03
0: TRAIN [2][1650/1938]	Time 0.083 (0.075)	Data 8.30e-05 (2.45e-04)	Tok/s 100545 (93237)	Loss/tok 3.1970 (3.2565)	LR 2.000e-03
0: TRAIN [2][1660/1938]	Time 0.106 (0.075)	Data 8.42e-05 (2.44e-04)	Tok/s 111124 (93271)	Loss/tok 3.2963 (3.2565)	LR 2.000e-03
0: TRAIN [2][1670/1938]	Time 0.083 (0.075)	Data 8.06e-05 (2.43e-04)	Tok/s 102461 (93246)	Loss/tok 3.1011 (3.2558)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.083 (0.075)	Data 8.75e-05 (2.42e-04)	Tok/s 101298 (93240)	Loss/tok 3.2320 (3.2551)	LR 2.000e-03
0: TRAIN [2][1690/1938]	Time 0.060 (0.075)	Data 8.46e-05 (2.41e-04)	Tok/s 82366 (93240)	Loss/tok 3.0939 (3.2554)	LR 2.000e-03
0: TRAIN [2][1700/1938]	Time 0.083 (0.075)	Data 8.46e-05 (2.40e-04)	Tok/s 100889 (93256)	Loss/tok 3.2978 (3.2552)	LR 2.000e-03
0: TRAIN [2][1710/1938]	Time 0.134 (0.075)	Data 8.32e-05 (2.39e-04)	Tok/s 110766 (93277)	Loss/tok 3.5736 (3.2550)	LR 2.000e-03
0: TRAIN [2][1720/1938]	Time 0.060 (0.075)	Data 7.96e-05 (2.39e-04)	Tok/s 86170 (93260)	Loss/tok 3.0591 (3.2546)	LR 2.000e-03
0: TRAIN [2][1730/1938]	Time 0.040 (0.075)	Data 8.51e-05 (2.38e-04)	Tok/s 66860 (93265)	Loss/tok 2.5539 (3.2547)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1740/1938]	Time 0.106 (0.075)	Data 8.13e-05 (2.37e-04)	Tok/s 112411 (93318)	Loss/tok 3.4956 (3.2558)	LR 2.000e-03
0: TRAIN [2][1750/1938]	Time 0.060 (0.075)	Data 8.75e-05 (2.36e-04)	Tok/s 84810 (93283)	Loss/tok 3.0630 (3.2553)	LR 2.000e-03
0: TRAIN [2][1760/1938]	Time 0.083 (0.075)	Data 8.15e-05 (2.35e-04)	Tok/s 100399 (93266)	Loss/tok 3.1543 (3.2547)	LR 2.000e-03
0: TRAIN [2][1770/1938]	Time 0.106 (0.075)	Data 8.06e-05 (2.34e-04)	Tok/s 110458 (93259)	Loss/tok 3.4070 (3.2545)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.060 (0.075)	Data 8.20e-05 (2.33e-04)	Tok/s 89298 (93231)	Loss/tok 2.9814 (3.2542)	LR 2.000e-03
0: TRAIN [2][1790/1938]	Time 0.060 (0.075)	Data 8.06e-05 (2.32e-04)	Tok/s 85061 (93197)	Loss/tok 3.0365 (3.2541)	LR 2.000e-03
0: TRAIN [2][1800/1938]	Time 0.060 (0.075)	Data 8.70e-05 (2.32e-04)	Tok/s 84374 (93218)	Loss/tok 3.1592 (3.2541)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][1810/1938]	Time 0.083 (0.075)	Data 8.15e-05 (2.31e-04)	Tok/s 100614 (93241)	Loss/tok 3.1670 (3.2546)	LR 2.000e-03
0: TRAIN [2][1820/1938]	Time 0.060 (0.075)	Data 8.68e-05 (2.30e-04)	Tok/s 84713 (93248)	Loss/tok 2.9576 (3.2547)	LR 2.000e-03
0: TRAIN [2][1830/1938]	Time 0.083 (0.075)	Data 8.37e-05 (2.29e-04)	Tok/s 102384 (93285)	Loss/tok 3.2166 (3.2552)	LR 2.000e-03
0: TRAIN [2][1840/1938]	Time 0.083 (0.075)	Data 8.23e-05 (2.28e-04)	Tok/s 100818 (93259)	Loss/tok 3.3071 (3.2546)	LR 2.000e-03
0: TRAIN [2][1850/1938]	Time 0.039 (0.075)	Data 8.25e-05 (2.28e-04)	Tok/s 66684 (93269)	Loss/tok 2.5782 (3.2551)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.083 (0.075)	Data 8.34e-05 (2.27e-04)	Tok/s 102362 (93307)	Loss/tok 3.3680 (3.2552)	LR 2.000e-03
0: TRAIN [2][1870/1938]	Time 0.106 (0.075)	Data 8.08e-05 (2.26e-04)	Tok/s 110877 (93354)	Loss/tok 3.2597 (3.2550)	LR 2.000e-03
0: TRAIN [2][1880/1938]	Time 0.083 (0.075)	Data 9.66e-05 (2.25e-04)	Tok/s 102977 (93358)	Loss/tok 3.0006 (3.2546)	LR 2.000e-03
0: TRAIN [2][1890/1938]	Time 0.060 (0.075)	Data 8.03e-05 (2.25e-04)	Tok/s 86220 (93349)	Loss/tok 3.0546 (3.2544)	LR 2.000e-03
0: TRAIN [2][1900/1938]	Time 0.135 (0.075)	Data 8.11e-05 (2.24e-04)	Tok/s 110513 (93350)	Loss/tok 3.5318 (3.2543)	LR 2.000e-03
0: TRAIN [2][1910/1938]	Time 0.083 (0.075)	Data 8.77e-05 (2.23e-04)	Tok/s 101552 (93378)	Loss/tok 3.1122 (3.2546)	LR 2.000e-03
0: TRAIN [2][1920/1938]	Time 0.083 (0.075)	Data 8.30e-05 (2.22e-04)	Tok/s 99535 (93369)	Loss/tok 3.3757 (3.2543)	LR 2.000e-03
0: TRAIN [2][1930/1938]	Time 0.083 (0.075)	Data 8.44e-05 (2.22e-04)	Tok/s 99208 (93363)	Loss/tok 3.3067 (3.2539)	LR 2.000e-03
:::MLL 1560821197.471 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1560821197.471 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.379 (0.379)	Decoder iters 105.0 (105.0)	Tok/s 23662 (23662)
0: Running moses detokenizer
0: BLEU(score=22.663578162616897, counts=[36712, 17913, 10022, 5822], totals=[66350, 63347, 60344, 57344], precisions=[55.33082140165787, 28.277582205945034, 16.608113482699192, 10.152762276785714], bp=1.0, sys_len=66350, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1560821198.708 eval_accuracy: {"value": 22.66, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1560821198.709 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.2570	Test BLEU: 22.66
0: Performance: Epoch: 2	Training: 1494551 Tok/s
0: Finished epoch 2
:::MLL 1560821198.709 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1560821198.710 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1560821198.710 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 429183914
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][0/1938]	Time 0.381 (0.381)	Data 2.82e-01 (2.82e-01)	Tok/s 22126 (22126)	Loss/tok 3.1893 (3.1893)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [3][10/1938]	Time 0.083 (0.112)	Data 1.21e-04 (2.58e-02)	Tok/s 102314 (92116)	Loss/tok 3.0894 (3.3027)	LR 2.000e-03
0: TRAIN [3][20/1938]	Time 0.060 (0.091)	Data 8.42e-05 (1.35e-02)	Tok/s 86765 (91343)	Loss/tok 2.9620 (3.2362)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.040 (0.081)	Data 7.94e-05 (9.19e-03)	Tok/s 66021 (89334)	Loss/tok 2.4669 (3.1847)	LR 2.000e-03
0: TRAIN [3][40/1938]	Time 0.083 (0.078)	Data 8.44e-05 (6.97e-03)	Tok/s 100315 (89810)	Loss/tok 3.1044 (3.1606)	LR 2.000e-03
0: TRAIN [3][50/1938]	Time 0.039 (0.077)	Data 8.03e-05 (5.62e-03)	Tok/s 67562 (89588)	Loss/tok 2.5471 (3.1616)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.083 (0.078)	Data 9.80e-05 (4.71e-03)	Tok/s 100783 (91175)	Loss/tok 3.1909 (3.1722)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.083 (0.078)	Data 8.34e-05 (4.06e-03)	Tok/s 103211 (92019)	Loss/tok 3.0553 (3.1697)	LR 2.000e-03
0: TRAIN [3][80/1938]	Time 0.083 (0.078)	Data 7.89e-05 (3.57e-03)	Tok/s 98334 (92341)	Loss/tok 3.2491 (3.1693)	LR 2.000e-03
0: TRAIN [3][90/1938]	Time 0.083 (0.078)	Data 8.61e-05 (3.19e-03)	Tok/s 102226 (92901)	Loss/tok 3.3206 (3.1839)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.083 (0.078)	Data 8.01e-05 (2.88e-03)	Tok/s 102078 (93222)	Loss/tok 3.1649 (3.1842)	LR 2.000e-03
0: TRAIN [3][110/1938]	Time 0.083 (0.078)	Data 8.23e-05 (2.63e-03)	Tok/s 101744 (93241)	Loss/tok 3.1742 (3.1833)	LR 2.000e-03
0: TRAIN [3][120/1938]	Time 0.060 (0.077)	Data 8.58e-05 (2.42e-03)	Tok/s 87401 (92861)	Loss/tok 2.9346 (3.1731)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.059 (0.076)	Data 8.61e-05 (2.24e-03)	Tok/s 85177 (92554)	Loss/tok 3.0996 (3.1648)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.060 (0.075)	Data 8.27e-05 (2.09e-03)	Tok/s 86080 (91959)	Loss/tok 3.0327 (3.1568)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.060 (0.074)	Data 7.92e-05 (1.95e-03)	Tok/s 87207 (91821)	Loss/tok 3.0149 (3.1496)	LR 2.000e-03
0: TRAIN [3][160/1938]	Time 0.059 (0.073)	Data 7.99e-05 (1.84e-03)	Tok/s 87462 (91365)	Loss/tok 3.0004 (3.1410)	LR 2.000e-03
0: TRAIN [3][170/1938]	Time 0.083 (0.073)	Data 8.37e-05 (1.74e-03)	Tok/s 101200 (91582)	Loss/tok 3.2767 (3.1432)	LR 2.000e-03
0: TRAIN [3][180/1938]	Time 0.083 (0.074)	Data 8.39e-05 (1.64e-03)	Tok/s 101811 (92161)	Loss/tok 3.0382 (3.1541)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.135 (0.075)	Data 8.15e-05 (1.56e-03)	Tok/s 111783 (92492)	Loss/tok 3.3664 (3.1581)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.135 (0.076)	Data 8.18e-05 (1.49e-03)	Tok/s 110568 (92932)	Loss/tok 3.6077 (3.1709)	LR 2.000e-03
0: TRAIN [3][210/1938]	Time 0.106 (0.076)	Data 8.30e-05 (1.42e-03)	Tok/s 109673 (92891)	Loss/tok 3.3545 (3.1676)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.135 (0.076)	Data 8.32e-05 (1.36e-03)	Tok/s 112040 (92819)	Loss/tok 3.4356 (3.1726)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.060 (0.076)	Data 7.87e-05 (1.31e-03)	Tok/s 86638 (92864)	Loss/tok 3.1181 (3.1716)	LR 2.000e-03
0: TRAIN [3][240/1938]	Time 0.083 (0.075)	Data 8.44e-05 (1.26e-03)	Tok/s 102372 (92860)	Loss/tok 3.2524 (3.1689)	LR 2.000e-03
0: TRAIN [3][250/1938]	Time 0.083 (0.075)	Data 1.01e-04 (1.21e-03)	Tok/s 100334 (92778)	Loss/tok 3.3779 (3.1681)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.106 (0.075)	Data 8.27e-05 (1.17e-03)	Tok/s 111139 (92730)	Loss/tok 3.2894 (3.1705)	LR 2.000e-03
0: TRAIN [3][270/1938]	Time 0.083 (0.075)	Data 8.15e-05 (1.13e-03)	Tok/s 101127 (92729)	Loss/tok 3.2177 (3.1692)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.083 (0.075)	Data 8.27e-05 (1.09e-03)	Tok/s 99330 (92894)	Loss/tok 3.2880 (3.1704)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][290/1938]	Time 0.040 (0.075)	Data 8.27e-05 (1.05e-03)	Tok/s 65643 (92925)	Loss/tok 2.6457 (3.1742)	LR 2.000e-03
0: TRAIN [3][300/1938]	Time 0.060 (0.075)	Data 8.15e-05 (1.02e-03)	Tok/s 81299 (92760)	Loss/tok 3.0549 (3.1731)	LR 2.000e-03
0: TRAIN [3][310/1938]	Time 0.083 (0.075)	Data 9.78e-05 (9.92e-04)	Tok/s 99988 (92754)	Loss/tok 3.2338 (3.1715)	LR 2.000e-03
0: TRAIN [3][320/1938]	Time 0.134 (0.075)	Data 8.13e-05 (9.64e-04)	Tok/s 110019 (92882)	Loss/tok 3.4927 (3.1752)	LR 2.000e-03
0: TRAIN [3][330/1938]	Time 0.060 (0.075)	Data 8.15e-05 (9.37e-04)	Tok/s 87044 (92983)	Loss/tok 2.8171 (3.1736)	LR 2.000e-03
0: TRAIN [3][340/1938]	Time 0.060 (0.075)	Data 8.61e-05 (9.12e-04)	Tok/s 85657 (93100)	Loss/tok 2.9332 (3.1737)	LR 2.000e-03
0: TRAIN [3][350/1938]	Time 0.083 (0.075)	Data 1.11e-04 (8.89e-04)	Tok/s 98740 (93059)	Loss/tok 3.0692 (3.1721)	LR 2.000e-03
0: TRAIN [3][360/1938]	Time 0.106 (0.075)	Data 9.37e-05 (8.67e-04)	Tok/s 109917 (93133)	Loss/tok 3.3465 (3.1749)	LR 2.000e-03
0: TRAIN [3][370/1938]	Time 0.060 (0.076)	Data 8.39e-05 (8.46e-04)	Tok/s 88203 (93292)	Loss/tok 2.9655 (3.1808)	LR 2.000e-03
0: TRAIN [3][380/1938]	Time 0.060 (0.076)	Data 8.32e-05 (8.26e-04)	Tok/s 88110 (93196)	Loss/tok 2.8860 (3.1785)	LR 2.000e-03
0: TRAIN [3][390/1938]	Time 0.083 (0.075)	Data 8.42e-05 (8.07e-04)	Tok/s 101916 (93099)	Loss/tok 3.2090 (3.1761)	LR 2.000e-03
0: TRAIN [3][400/1938]	Time 0.106 (0.075)	Data 8.11e-05 (7.89e-04)	Tok/s 108910 (93158)	Loss/tok 3.3588 (3.1757)	LR 2.000e-03
0: TRAIN [3][410/1938]	Time 0.083 (0.075)	Data 8.54e-05 (7.71e-04)	Tok/s 100118 (93262)	Loss/tok 3.1323 (3.1748)	LR 2.000e-03
0: TRAIN [3][420/1938]	Time 0.060 (0.075)	Data 8.92e-05 (7.55e-04)	Tok/s 87202 (93218)	Loss/tok 3.0222 (3.1739)	LR 2.000e-03
0: TRAIN [3][430/1938]	Time 0.060 (0.075)	Data 8.20e-05 (7.39e-04)	Tok/s 86015 (93153)	Loss/tok 3.0155 (3.1728)	LR 2.000e-03
0: TRAIN [3][440/1938]	Time 0.060 (0.075)	Data 7.84e-05 (7.25e-04)	Tok/s 88983 (93156)	Loss/tok 2.8990 (3.1710)	LR 2.000e-03
0: TRAIN [3][450/1938]	Time 0.060 (0.075)	Data 7.99e-05 (7.10e-04)	Tok/s 85697 (93221)	Loss/tok 2.9124 (3.1698)	LR 2.000e-03
0: TRAIN [3][460/1938]	Time 0.106 (0.075)	Data 9.42e-05 (6.97e-04)	Tok/s 112418 (93230)	Loss/tok 3.2694 (3.1693)	LR 2.000e-03
0: TRAIN [3][470/1938]	Time 0.106 (0.075)	Data 8.11e-05 (6.84e-04)	Tok/s 111766 (93147)	Loss/tok 3.3227 (3.1685)	LR 2.000e-03
0: TRAIN [3][480/1938]	Time 0.060 (0.075)	Data 8.51e-05 (6.71e-04)	Tok/s 86055 (93117)	Loss/tok 3.0644 (3.1677)	LR 2.000e-03
0: TRAIN [3][490/1938]	Time 0.060 (0.075)	Data 8.34e-05 (6.59e-04)	Tok/s 86458 (93086)	Loss/tok 2.7780 (3.1659)	LR 2.000e-03
0: TRAIN [3][500/1938]	Time 0.060 (0.075)	Data 8.23e-05 (6.48e-04)	Tok/s 86201 (93239)	Loss/tok 2.8641 (3.1717)	LR 2.000e-03
0: TRAIN [3][510/1938]	Time 0.083 (0.075)	Data 8.18e-05 (6.36e-04)	Tok/s 100648 (93180)	Loss/tok 3.1506 (3.1711)	LR 2.000e-03
0: TRAIN [3][520/1938]	Time 0.135 (0.075)	Data 8.51e-05 (6.26e-04)	Tok/s 110015 (93183)	Loss/tok 3.5289 (3.1731)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][530/1938]	Time 0.060 (0.075)	Data 8.13e-05 (6.16e-04)	Tok/s 85685 (93189)	Loss/tok 2.9860 (3.1741)	LR 2.000e-03
0: TRAIN [3][540/1938]	Time 0.059 (0.075)	Data 7.96e-05 (6.06e-04)	Tok/s 88275 (93160)	Loss/tok 2.9150 (3.1726)	LR 2.000e-03
0: TRAIN [3][550/1938]	Time 0.083 (0.075)	Data 7.80e-05 (5.96e-04)	Tok/s 102063 (93085)	Loss/tok 3.0615 (3.1710)	LR 2.000e-03
0: TRAIN [3][560/1938]	Time 0.106 (0.075)	Data 8.46e-05 (5.87e-04)	Tok/s 110958 (93053)	Loss/tok 3.5015 (3.1717)	LR 2.000e-03
0: TRAIN [3][570/1938]	Time 0.083 (0.075)	Data 8.49e-05 (5.78e-04)	Tok/s 99979 (93093)	Loss/tok 3.2004 (3.1720)	LR 2.000e-03
0: TRAIN [3][580/1938]	Time 0.060 (0.075)	Data 8.51e-05 (5.70e-04)	Tok/s 85383 (92986)	Loss/tok 3.0969 (3.1705)	LR 2.000e-03
0: TRAIN [3][590/1938]	Time 0.083 (0.075)	Data 8.27e-05 (5.61e-04)	Tok/s 101873 (92987)	Loss/tok 3.1758 (3.1692)	LR 2.000e-03
0: TRAIN [3][600/1938]	Time 0.083 (0.075)	Data 8.56e-05 (5.54e-04)	Tok/s 102037 (92915)	Loss/tok 3.1045 (3.1682)	LR 2.000e-03
0: TRAIN [3][610/1938]	Time 0.060 (0.075)	Data 8.73e-05 (5.46e-04)	Tok/s 86361 (92967)	Loss/tok 2.9749 (3.1686)	LR 2.000e-03
0: TRAIN [3][620/1938]	Time 0.060 (0.075)	Data 8.08e-05 (5.38e-04)	Tok/s 86984 (93002)	Loss/tok 3.1528 (3.1694)	LR 2.000e-03
0: TRAIN [3][630/1938]	Time 0.106 (0.075)	Data 8.23e-05 (5.31e-04)	Tok/s 109677 (93091)	Loss/tok 3.5235 (3.1710)	LR 2.000e-03
0: TRAIN [3][640/1938]	Time 0.060 (0.074)	Data 8.46e-05 (5.24e-04)	Tok/s 86866 (92972)	Loss/tok 2.9778 (3.1695)	LR 2.000e-03
0: TRAIN [3][650/1938]	Time 0.083 (0.074)	Data 8.15e-05 (5.17e-04)	Tok/s 100865 (92847)	Loss/tok 3.2405 (3.1679)	LR 2.000e-03
0: TRAIN [3][660/1938]	Time 0.060 (0.074)	Data 8.03e-05 (5.11e-04)	Tok/s 87574 (92862)	Loss/tok 2.9491 (3.1664)	LR 2.000e-03
0: TRAIN [3][670/1938]	Time 0.060 (0.074)	Data 8.44e-05 (5.05e-04)	Tok/s 82216 (92750)	Loss/tok 3.1803 (3.1649)	LR 2.000e-03
0: TRAIN [3][680/1938]	Time 0.083 (0.074)	Data 8.46e-05 (4.98e-04)	Tok/s 101393 (92766)	Loss/tok 3.1842 (3.1656)	LR 1.000e-03
0: TRAIN [3][690/1938]	Time 0.083 (0.074)	Data 8.03e-05 (4.92e-04)	Tok/s 102882 (92765)	Loss/tok 3.0704 (3.1644)	LR 1.000e-03
0: TRAIN [3][700/1938]	Time 0.040 (0.074)	Data 9.42e-05 (4.87e-04)	Tok/s 65825 (92670)	Loss/tok 2.5706 (3.1632)	LR 1.000e-03
0: TRAIN [3][710/1938]	Time 0.083 (0.074)	Data 8.27e-05 (4.81e-04)	Tok/s 100705 (92599)	Loss/tok 3.0512 (3.1618)	LR 1.000e-03
0: TRAIN [3][720/1938]	Time 0.134 (0.074)	Data 9.01e-05 (4.75e-04)	Tok/s 111880 (92622)	Loss/tok 3.4218 (3.1624)	LR 1.000e-03
0: TRAIN [3][730/1938]	Time 0.060 (0.074)	Data 8.37e-05 (4.70e-04)	Tok/s 84280 (92618)	Loss/tok 3.0041 (3.1631)	LR 1.000e-03
0: TRAIN [3][740/1938]	Time 0.060 (0.074)	Data 8.01e-05 (4.65e-04)	Tok/s 85099 (92562)	Loss/tok 2.9987 (3.1614)	LR 1.000e-03
0: TRAIN [3][750/1938]	Time 0.040 (0.074)	Data 8.92e-05 (4.60e-04)	Tok/s 68061 (92570)	Loss/tok 2.6265 (3.1619)	LR 1.000e-03
0: TRAIN [3][760/1938]	Time 0.060 (0.074)	Data 9.49e-05 (4.55e-04)	Tok/s 85167 (92601)	Loss/tok 2.8868 (3.1627)	LR 1.000e-03
0: TRAIN [3][770/1938]	Time 0.083 (0.074)	Data 8.25e-05 (4.50e-04)	Tok/s 101158 (92629)	Loss/tok 3.1225 (3.1618)	LR 1.000e-03
0: TRAIN [3][780/1938]	Time 0.083 (0.074)	Data 8.23e-05 (4.45e-04)	Tok/s 101709 (92643)	Loss/tok 3.1338 (3.1606)	LR 1.000e-03
0: TRAIN [3][790/1938]	Time 0.061 (0.074)	Data 8.65e-05 (4.41e-04)	Tok/s 84997 (92738)	Loss/tok 3.0020 (3.1622)	LR 1.000e-03
0: TRAIN [3][800/1938]	Time 0.106 (0.074)	Data 9.70e-05 (4.36e-04)	Tok/s 108399 (92771)	Loss/tok 3.3519 (3.1621)	LR 1.000e-03
0: TRAIN [3][810/1938]	Time 0.106 (0.074)	Data 8.34e-05 (4.32e-04)	Tok/s 111302 (92825)	Loss/tok 3.2293 (3.1634)	LR 1.000e-03
0: TRAIN [3][820/1938]	Time 0.060 (0.074)	Data 8.37e-05 (4.28e-04)	Tok/s 88031 (92853)	Loss/tok 3.0884 (3.1626)	LR 1.000e-03
0: TRAIN [3][830/1938]	Time 0.060 (0.074)	Data 8.99e-05 (4.24e-04)	Tok/s 84230 (92825)	Loss/tok 2.8369 (3.1614)	LR 1.000e-03
0: TRAIN [3][840/1938]	Time 0.040 (0.074)	Data 8.25e-05 (4.19e-04)	Tok/s 62829 (92765)	Loss/tok 2.5621 (3.1601)	LR 1.000e-03
0: TRAIN [3][850/1938]	Time 0.061 (0.074)	Data 8.23e-05 (4.16e-04)	Tok/s 86959 (92728)	Loss/tok 2.7760 (3.1589)	LR 1.000e-03
0: TRAIN [3][860/1938]	Time 0.060 (0.074)	Data 8.23e-05 (4.12e-04)	Tok/s 84448 (92750)	Loss/tok 3.0302 (3.1605)	LR 1.000e-03
0: TRAIN [3][870/1938]	Time 0.083 (0.074)	Data 8.49e-05 (4.08e-04)	Tok/s 99894 (92689)	Loss/tok 3.2607 (3.1595)	LR 1.000e-03
0: TRAIN [3][880/1938]	Time 0.083 (0.074)	Data 8.11e-05 (4.04e-04)	Tok/s 102090 (92694)	Loss/tok 3.1699 (3.1594)	LR 1.000e-03
0: TRAIN [3][890/1938]	Time 0.060 (0.074)	Data 8.06e-05 (4.01e-04)	Tok/s 86896 (92718)	Loss/tok 3.1790 (3.1595)	LR 1.000e-03
0: TRAIN [3][900/1938]	Time 0.106 (0.074)	Data 9.11e-05 (3.97e-04)	Tok/s 109900 (92708)	Loss/tok 3.2706 (3.1591)	LR 1.000e-03
0: TRAIN [3][910/1938]	Time 0.083 (0.074)	Data 8.06e-05 (3.94e-04)	Tok/s 100369 (92785)	Loss/tok 3.1206 (3.1599)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][920/1938]	Time 0.060 (0.074)	Data 8.32e-05 (3.90e-04)	Tok/s 86527 (92811)	Loss/tok 3.0366 (3.1612)	LR 1.000e-03
0: TRAIN [3][930/1938]	Time 0.134 (0.074)	Data 8.75e-05 (3.87e-04)	Tok/s 111413 (92872)	Loss/tok 3.4124 (3.1623)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][940/1938]	Time 0.083 (0.074)	Data 9.49e-05 (3.84e-04)	Tok/s 100811 (92950)	Loss/tok 3.2456 (3.1634)	LR 1.000e-03
0: TRAIN [3][950/1938]	Time 0.060 (0.074)	Data 8.23e-05 (3.81e-04)	Tok/s 84515 (92903)	Loss/tok 2.8901 (3.1624)	LR 1.000e-03
0: TRAIN [3][960/1938]	Time 0.060 (0.074)	Data 8.49e-05 (3.78e-04)	Tok/s 85397 (92946)	Loss/tok 2.9554 (3.1625)	LR 1.000e-03
0: TRAIN [3][970/1938]	Time 0.060 (0.075)	Data 8.56e-05 (3.75e-04)	Tok/s 86069 (92977)	Loss/tok 3.1070 (3.1630)	LR 1.000e-03
0: TRAIN [3][980/1938]	Time 0.060 (0.074)	Data 8.37e-05 (3.72e-04)	Tok/s 86305 (92954)	Loss/tok 3.1181 (3.1623)	LR 1.000e-03
0: TRAIN [3][990/1938]	Time 0.061 (0.075)	Data 8.39e-05 (3.69e-04)	Tok/s 83278 (93005)	Loss/tok 2.8317 (3.1620)	LR 1.000e-03
0: TRAIN [3][1000/1938]	Time 0.039 (0.075)	Data 8.54e-05 (3.66e-04)	Tok/s 65124 (93062)	Loss/tok 2.6614 (3.1629)	LR 1.000e-03
0: TRAIN [3][1010/1938]	Time 0.083 (0.075)	Data 9.18e-05 (3.63e-04)	Tok/s 102244 (93127)	Loss/tok 3.0917 (3.1631)	LR 1.000e-03
0: TRAIN [3][1020/1938]	Time 0.134 (0.075)	Data 8.13e-05 (3.60e-04)	Tok/s 107593 (93037)	Loss/tok 3.7148 (3.1628)	LR 1.000e-03
0: TRAIN [3][1030/1938]	Time 0.060 (0.075)	Data 7.94e-05 (3.58e-04)	Tok/s 85379 (92975)	Loss/tok 2.9855 (3.1615)	LR 1.000e-03
0: TRAIN [3][1040/1938]	Time 0.060 (0.075)	Data 8.65e-05 (3.55e-04)	Tok/s 87733 (92954)	Loss/tok 2.8292 (3.1618)	LR 1.000e-03
0: TRAIN [3][1050/1938]	Time 0.106 (0.075)	Data 9.73e-05 (3.53e-04)	Tok/s 108661 (93033)	Loss/tok 3.3531 (3.1628)	LR 1.000e-03
0: TRAIN [3][1060/1938]	Time 0.106 (0.075)	Data 8.44e-05 (3.50e-04)	Tok/s 109198 (93038)	Loss/tok 3.3295 (3.1632)	LR 1.000e-03
0: TRAIN [3][1070/1938]	Time 0.060 (0.075)	Data 8.44e-05 (3.48e-04)	Tok/s 86890 (93020)	Loss/tok 3.0705 (3.1630)	LR 1.000e-03
0: TRAIN [3][1080/1938]	Time 0.060 (0.075)	Data 7.99e-05 (3.45e-04)	Tok/s 84753 (93012)	Loss/tok 2.9349 (3.1628)	LR 1.000e-03
0: TRAIN [3][1090/1938]	Time 0.083 (0.075)	Data 9.42e-05 (3.43e-04)	Tok/s 101638 (93019)	Loss/tok 3.1662 (3.1622)	LR 1.000e-03
0: TRAIN [3][1100/1938]	Time 0.106 (0.075)	Data 8.32e-05 (3.40e-04)	Tok/s 110171 (93018)	Loss/tok 3.4220 (3.1617)	LR 1.000e-03
0: TRAIN [3][1110/1938]	Time 0.106 (0.075)	Data 8.11e-05 (3.38e-04)	Tok/s 109760 (93012)	Loss/tok 3.2012 (3.1610)	LR 1.000e-03
0: TRAIN [3][1120/1938]	Time 0.083 (0.075)	Data 8.34e-05 (3.36e-04)	Tok/s 100410 (93025)	Loss/tok 3.0950 (3.1599)	LR 1.000e-03
0: TRAIN [3][1130/1938]	Time 0.083 (0.075)	Data 8.25e-05 (3.34e-04)	Tok/s 100009 (92987)	Loss/tok 3.1890 (3.1592)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][1140/1938]	Time 0.083 (0.075)	Data 8.54e-05 (3.31e-04)	Tok/s 99187 (93020)	Loss/tok 3.2156 (3.1600)	LR 1.000e-03
0: TRAIN [3][1150/1938]	Time 0.134 (0.075)	Data 8.68e-05 (3.29e-04)	Tok/s 112391 (93036)	Loss/tok 3.4027 (3.1596)	LR 1.000e-03
0: TRAIN [3][1160/1938]	Time 0.083 (0.075)	Data 8.37e-05 (3.27e-04)	Tok/s 100984 (93093)	Loss/tok 3.2479 (3.1589)	LR 1.000e-03
0: TRAIN [3][1170/1938]	Time 0.106 (0.075)	Data 8.34e-05 (3.25e-04)	Tok/s 112933 (93157)	Loss/tok 3.0175 (3.1595)	LR 1.000e-03
0: TRAIN [3][1180/1938]	Time 0.083 (0.075)	Data 9.37e-05 (3.23e-04)	Tok/s 98637 (93148)	Loss/tok 3.2079 (3.1594)	LR 1.000e-03
0: TRAIN [3][1190/1938]	Time 0.106 (0.075)	Data 9.01e-05 (3.21e-04)	Tok/s 111383 (93215)	Loss/tok 3.2783 (3.1603)	LR 1.000e-03
0: TRAIN [3][1200/1938]	Time 0.060 (0.075)	Data 8.80e-05 (3.19e-04)	Tok/s 82566 (93184)	Loss/tok 3.1759 (3.1599)	LR 1.000e-03
0: TRAIN [3][1210/1938]	Time 0.083 (0.075)	Data 8.92e-05 (3.17e-04)	Tok/s 102281 (93136)	Loss/tok 3.0937 (3.1584)	LR 1.000e-03
0: TRAIN [3][1220/1938]	Time 0.083 (0.075)	Data 8.30e-05 (3.15e-04)	Tok/s 100462 (93080)	Loss/tok 3.0142 (3.1576)	LR 1.000e-03
0: TRAIN [3][1230/1938]	Time 0.060 (0.075)	Data 8.87e-05 (3.14e-04)	Tok/s 85192 (93044)	Loss/tok 2.9260 (3.1564)	LR 1.000e-03
0: TRAIN [3][1240/1938]	Time 0.106 (0.075)	Data 8.63e-05 (3.12e-04)	Tok/s 108541 (93066)	Loss/tok 3.2885 (3.1561)	LR 1.000e-03
0: TRAIN [3][1250/1938]	Time 0.039 (0.075)	Data 8.92e-05 (3.10e-04)	Tok/s 67251 (93060)	Loss/tok 2.4403 (3.1554)	LR 1.000e-03
0: TRAIN [3][1260/1938]	Time 0.039 (0.075)	Data 8.46e-05 (3.08e-04)	Tok/s 68456 (93056)	Loss/tok 2.4997 (3.1550)	LR 1.000e-03
0: TRAIN [3][1270/1938]	Time 0.060 (0.075)	Data 8.37e-05 (3.06e-04)	Tok/s 86939 (93093)	Loss/tok 2.9181 (3.1551)	LR 1.000e-03
0: TRAIN [3][1280/1938]	Time 0.083 (0.075)	Data 8.15e-05 (3.05e-04)	Tok/s 99243 (93132)	Loss/tok 3.1903 (3.1558)	LR 1.000e-03
0: TRAIN [3][1290/1938]	Time 0.060 (0.075)	Data 8.20e-05 (3.03e-04)	Tok/s 86027 (93161)	Loss/tok 2.8677 (3.1558)	LR 1.000e-03
0: TRAIN [3][1300/1938]	Time 0.060 (0.075)	Data 9.75e-05 (3.01e-04)	Tok/s 83229 (93140)	Loss/tok 2.8984 (3.1552)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][1310/1938]	Time 0.106 (0.075)	Data 8.49e-05 (3.00e-04)	Tok/s 111635 (93165)	Loss/tok 3.1423 (3.1555)	LR 1.000e-03
0: TRAIN [3][1320/1938]	Time 0.060 (0.075)	Data 9.68e-05 (2.98e-04)	Tok/s 88318 (93199)	Loss/tok 2.9570 (3.1555)	LR 1.000e-03
0: TRAIN [3][1330/1938]	Time 0.060 (0.075)	Data 8.46e-05 (2.97e-04)	Tok/s 84421 (93221)	Loss/tok 2.9323 (3.1552)	LR 1.000e-03
0: TRAIN [3][1340/1938]	Time 0.083 (0.075)	Data 8.61e-05 (2.95e-04)	Tok/s 100681 (93157)	Loss/tok 3.1624 (3.1542)	LR 1.000e-03
0: TRAIN [3][1350/1938]	Time 0.039 (0.075)	Data 8.11e-05 (2.93e-04)	Tok/s 68829 (93166)	Loss/tok 2.5122 (3.1548)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][1360/1938]	Time 0.083 (0.075)	Data 8.13e-05 (2.92e-04)	Tok/s 99515 (93162)	Loss/tok 3.1888 (3.1546)	LR 1.000e-03
0: TRAIN [3][1370/1938]	Time 0.061 (0.075)	Data 8.13e-05 (2.90e-04)	Tok/s 85864 (93106)	Loss/tok 2.9983 (3.1534)	LR 1.000e-03
0: TRAIN [3][1380/1938]	Time 0.061 (0.075)	Data 8.73e-05 (2.89e-04)	Tok/s 84348 (93131)	Loss/tok 2.9830 (3.1533)	LR 1.000e-03
0: TRAIN [3][1390/1938]	Time 0.083 (0.075)	Data 8.25e-05 (2.87e-04)	Tok/s 103923 (93092)	Loss/tok 2.9609 (3.1524)	LR 1.000e-03
0: TRAIN [3][1400/1938]	Time 0.106 (0.075)	Data 1.03e-04 (2.86e-04)	Tok/s 109851 (93095)	Loss/tok 3.1795 (3.1518)	LR 1.000e-03
0: TRAIN [3][1410/1938]	Time 0.106 (0.075)	Data 8.30e-05 (2.85e-04)	Tok/s 107829 (93086)	Loss/tok 3.4079 (3.1514)	LR 1.000e-03
0: TRAIN [3][1420/1938]	Time 0.106 (0.075)	Data 8.32e-05 (2.83e-04)	Tok/s 108649 (93119)	Loss/tok 3.5050 (3.1521)	LR 1.000e-03
0: TRAIN [3][1430/1938]	Time 0.106 (0.075)	Data 8.01e-05 (2.82e-04)	Tok/s 109765 (93123)	Loss/tok 3.2218 (3.1517)	LR 1.000e-03
0: TRAIN [3][1440/1938]	Time 0.106 (0.075)	Data 8.18e-05 (2.80e-04)	Tok/s 110157 (93143)	Loss/tok 3.3080 (3.1513)	LR 1.000e-03
0: TRAIN [3][1450/1938]	Time 0.083 (0.075)	Data 8.15e-05 (2.79e-04)	Tok/s 102398 (93140)	Loss/tok 3.1065 (3.1512)	LR 1.000e-03
0: TRAIN [3][1460/1938]	Time 0.060 (0.075)	Data 8.15e-05 (2.78e-04)	Tok/s 86993 (93197)	Loss/tok 2.9607 (3.1516)	LR 1.000e-03
0: TRAIN [3][1470/1938]	Time 0.106 (0.075)	Data 8.49e-05 (2.76e-04)	Tok/s 110786 (93227)	Loss/tok 3.3031 (3.1518)	LR 1.000e-03
0: TRAIN [3][1480/1938]	Time 0.106 (0.075)	Data 8.27e-05 (2.75e-04)	Tok/s 111434 (93222)	Loss/tok 3.0989 (3.1511)	LR 1.000e-03
0: TRAIN [3][1490/1938]	Time 0.106 (0.075)	Data 8.37e-05 (2.74e-04)	Tok/s 109707 (93215)	Loss/tok 3.2370 (3.1520)	LR 5.000e-04
0: TRAIN [3][1500/1938]	Time 0.060 (0.075)	Data 8.56e-05 (2.72e-04)	Tok/s 88680 (93229)	Loss/tok 2.8630 (3.1517)	LR 5.000e-04
0: TRAIN [3][1510/1938]	Time 0.060 (0.075)	Data 7.92e-05 (2.71e-04)	Tok/s 83582 (93240)	Loss/tok 3.1358 (3.1516)	LR 5.000e-04
0: TRAIN [3][1520/1938]	Time 0.060 (0.075)	Data 8.20e-05 (2.70e-04)	Tok/s 84531 (93224)	Loss/tok 2.9764 (3.1512)	LR 5.000e-04
0: TRAIN [3][1530/1938]	Time 0.083 (0.075)	Data 7.96e-05 (2.69e-04)	Tok/s 102932 (93186)	Loss/tok 2.9974 (3.1504)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][1540/1938]	Time 0.083 (0.075)	Data 8.13e-05 (2.67e-04)	Tok/s 103151 (93202)	Loss/tok 3.1472 (3.1497)	LR 5.000e-04
0: TRAIN [3][1550/1938]	Time 0.106 (0.075)	Data 7.92e-05 (2.66e-04)	Tok/s 110014 (93221)	Loss/tok 3.3153 (3.1498)	LR 5.000e-04
0: TRAIN [3][1560/1938]	Time 0.106 (0.075)	Data 8.46e-05 (2.65e-04)	Tok/s 108002 (93230)	Loss/tok 3.5139 (3.1502)	LR 5.000e-04
0: TRAIN [3][1570/1938]	Time 0.060 (0.075)	Data 8.87e-05 (2.64e-04)	Tok/s 86850 (93234)	Loss/tok 2.8103 (3.1505)	LR 5.000e-04
0: TRAIN [3][1580/1938]	Time 0.083 (0.075)	Data 7.92e-05 (2.63e-04)	Tok/s 99803 (93257)	Loss/tok 3.1745 (3.1510)	LR 5.000e-04
0: TRAIN [3][1590/1938]	Time 0.083 (0.075)	Data 8.15e-05 (2.62e-04)	Tok/s 103381 (93275)	Loss/tok 3.1493 (3.1508)	LR 5.000e-04
0: TRAIN [3][1600/1938]	Time 0.060 (0.075)	Data 8.11e-05 (2.61e-04)	Tok/s 85923 (93230)	Loss/tok 2.9829 (3.1500)	LR 5.000e-04
0: TRAIN [3][1610/1938]	Time 0.060 (0.075)	Data 8.34e-05 (2.59e-04)	Tok/s 83739 (93243)	Loss/tok 2.9032 (3.1498)	LR 5.000e-04
0: TRAIN [3][1620/1938]	Time 0.060 (0.075)	Data 8.20e-05 (2.58e-04)	Tok/s 86139 (93277)	Loss/tok 3.0214 (3.1504)	LR 5.000e-04
0: TRAIN [3][1630/1938]	Time 0.060 (0.075)	Data 7.80e-05 (2.57e-04)	Tok/s 87602 (93261)	Loss/tok 3.0413 (3.1497)	LR 5.000e-04
0: TRAIN [3][1640/1938]	Time 0.060 (0.075)	Data 8.42e-05 (2.56e-04)	Tok/s 87295 (93256)	Loss/tok 2.9829 (3.1493)	LR 5.000e-04
0: TRAIN [3][1650/1938]	Time 0.083 (0.075)	Data 8.20e-05 (2.55e-04)	Tok/s 98573 (93244)	Loss/tok 3.1316 (3.1487)	LR 5.000e-04
0: TRAIN [3][1660/1938]	Time 0.083 (0.075)	Data 8.06e-05 (2.54e-04)	Tok/s 100105 (93276)	Loss/tok 3.2565 (3.1487)	LR 5.000e-04
0: TRAIN [3][1670/1938]	Time 0.060 (0.075)	Data 8.56e-05 (2.53e-04)	Tok/s 84354 (93217)	Loss/tok 2.8955 (3.1479)	LR 5.000e-04
0: TRAIN [3][1680/1938]	Time 0.083 (0.075)	Data 8.15e-05 (2.52e-04)	Tok/s 100208 (93236)	Loss/tok 3.2111 (3.1483)	LR 5.000e-04
0: TRAIN [3][1690/1938]	Time 0.083 (0.075)	Data 1.03e-04 (2.51e-04)	Tok/s 101912 (93227)	Loss/tok 3.0746 (3.1476)	LR 5.000e-04
0: TRAIN [3][1700/1938]	Time 0.135 (0.075)	Data 8.27e-05 (2.50e-04)	Tok/s 111282 (93253)	Loss/tok 3.3518 (3.1473)	LR 5.000e-04
0: TRAIN [3][1710/1938]	Time 0.039 (0.075)	Data 8.06e-05 (2.49e-04)	Tok/s 66769 (93252)	Loss/tok 2.5003 (3.1477)	LR 5.000e-04
0: TRAIN [3][1720/1938]	Time 0.083 (0.075)	Data 8.27e-05 (2.48e-04)	Tok/s 100624 (93256)	Loss/tok 3.1964 (3.1471)	LR 5.000e-04
0: TRAIN [3][1730/1938]	Time 0.106 (0.075)	Data 8.11e-05 (2.47e-04)	Tok/s 109191 (93305)	Loss/tok 3.2576 (3.1477)	LR 5.000e-04
0: TRAIN [3][1740/1938]	Time 0.106 (0.075)	Data 8.46e-05 (2.46e-04)	Tok/s 110423 (93333)	Loss/tok 3.1623 (3.1474)	LR 5.000e-04
0: TRAIN [3][1750/1938]	Time 0.060 (0.075)	Data 8.25e-05 (2.45e-04)	Tok/s 84413 (93334)	Loss/tok 2.9044 (3.1471)	LR 5.000e-04
0: TRAIN [3][1760/1938]	Time 0.134 (0.075)	Data 8.23e-05 (2.44e-04)	Tok/s 110693 (93322)	Loss/tok 3.4457 (3.1473)	LR 5.000e-04
0: TRAIN [3][1770/1938]	Time 0.106 (0.075)	Data 8.58e-05 (2.44e-04)	Tok/s 108859 (93318)	Loss/tok 3.3567 (3.1469)	LR 5.000e-04
0: TRAIN [3][1780/1938]	Time 0.060 (0.075)	Data 8.42e-05 (2.43e-04)	Tok/s 86725 (93291)	Loss/tok 2.9869 (3.1464)	LR 5.000e-04
0: TRAIN [3][1790/1938]	Time 0.106 (0.075)	Data 9.23e-05 (2.42e-04)	Tok/s 111127 (93316)	Loss/tok 3.3823 (3.1470)	LR 5.000e-04
0: TRAIN [3][1800/1938]	Time 0.060 (0.075)	Data 7.96e-05 (2.41e-04)	Tok/s 88477 (93312)	Loss/tok 3.0745 (3.1470)	LR 5.000e-04
0: TRAIN [3][1810/1938]	Time 0.060 (0.075)	Data 8.25e-05 (2.40e-04)	Tok/s 86760 (93289)	Loss/tok 2.9604 (3.1467)	LR 5.000e-04
0: TRAIN [3][1820/1938]	Time 0.060 (0.075)	Data 8.25e-05 (2.39e-04)	Tok/s 86808 (93321)	Loss/tok 2.8523 (3.1470)	LR 5.000e-04
0: TRAIN [3][1830/1938]	Time 0.060 (0.075)	Data 8.54e-05 (2.38e-04)	Tok/s 89079 (93313)	Loss/tok 2.9971 (3.1467)	LR 5.000e-04
0: TRAIN [3][1840/1938]	Time 0.083 (0.075)	Data 8.51e-05 (2.37e-04)	Tok/s 101105 (93311)	Loss/tok 3.1472 (3.1466)	LR 5.000e-04
0: TRAIN [3][1850/1938]	Time 0.083 (0.075)	Data 8.25e-05 (2.37e-04)	Tok/s 103170 (93318)	Loss/tok 3.1765 (3.1467)	LR 5.000e-04
0: TRAIN [3][1860/1938]	Time 0.040 (0.075)	Data 8.42e-05 (2.36e-04)	Tok/s 66145 (93281)	Loss/tok 2.6127 (3.1466)	LR 5.000e-04
0: TRAIN [3][1870/1938]	Time 0.083 (0.075)	Data 7.70e-05 (2.35e-04)	Tok/s 100735 (93309)	Loss/tok 3.1076 (3.1468)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][1880/1938]	Time 0.083 (0.075)	Data 8.11e-05 (2.34e-04)	Tok/s 102085 (93355)	Loss/tok 2.9789 (3.1476)	LR 5.000e-04
0: TRAIN [3][1890/1938]	Time 0.060 (0.075)	Data 8.01e-05 (2.33e-04)	Tok/s 85528 (93334)	Loss/tok 3.0078 (3.1474)	LR 5.000e-04
0: TRAIN [3][1900/1938]	Time 0.083 (0.075)	Data 8.01e-05 (2.33e-04)	Tok/s 101916 (93352)	Loss/tok 3.0526 (3.1472)	LR 5.000e-04
0: TRAIN [3][1910/1938]	Time 0.134 (0.075)	Data 8.20e-05 (2.32e-04)	Tok/s 110306 (93387)	Loss/tok 3.4873 (3.1477)	LR 5.000e-04
0: TRAIN [3][1920/1938]	Time 0.083 (0.075)	Data 8.11e-05 (2.31e-04)	Tok/s 100238 (93399)	Loss/tok 2.9650 (3.1480)	LR 5.000e-04
0: TRAIN [3][1930/1938]	Time 0.083 (0.075)	Data 8.80e-05 (2.30e-04)	Tok/s 101041 (93371)	Loss/tok 2.9911 (3.1475)	LR 5.000e-04
:::MLL 1560821345.296 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 524}}
:::MLL 1560821345.297 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.381 (0.381)	Decoder iters 105.0 (105.0)	Tok/s 23566 (23566)
0: Running moses detokenizer
0: BLEU(score=24.328707691232317, counts=[37182, 18730, 10713, 6368], totals=[65281, 62278, 59275, 56275], precisions=[56.956848087498656, 30.074825781174734, 18.073386756642766, 11.315859617947579], bp=1.0, sys_len=65281, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1560821346.379 eval_accuracy: {"value": 24.33, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 535}}
:::MLL 1560821346.380 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 3	Training Loss: 3.1471	Test BLEU: 24.33
0: Performance: Epoch: 3	Training: 1494348 Tok/s
0: Finished epoch 3
:::MLL 1560821346.380 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 557}}
0: Closing preprocessed data file
:::MLL 1560821346.381 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 568}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-06-18 01:29:14 AM
RESULT,RNN_TRANSLATOR,,633,nvidia,2019-06-18 01:18:41 AM
