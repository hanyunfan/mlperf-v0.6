Beginning trial 1 of 5
Gathering sys log on circe-n051
:::MLL 1558578505.358 submission_benchmark: {"value": "ssd", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1558578505.359 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known ssd keys.
:::MLL 1558578505.359 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1558578505.360 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1558578505.360 submission_platform: {"value": "4xNVIDIA DGX-2H", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1558578505.360 submission_entry: {"value": "{'hardware': 'NVIDIA DGX-2H', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': 'Ubuntu 18.04.1 LTS / NVIDIA DGX Server 4.0.4 4.0.5', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.4-2.0.7', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '4', 'cpu': '2x Intel(R) Xeon(R) Platinum 8174 CPU @ 3.10GHz', 'num_cores': '48', 'num_vcpus': '96', 'accelerator': 'Tesla V100-SXM3-32GB-H', 'num_accelerators': '16', 'sys_mem_size': '1510 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '2x 894.3G + 8x 3.5T', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27800 Family [ConnectX-5]', 'num_network_cards': '10', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1558578505.361 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1558578505.361 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
:::MLL 1558578506.723 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558578506.762 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558578506.773 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558578506.766 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node circe-n051
+ pids+=($!)
+ set +x
Launching on node circe-n052
+ pids+=($!)
+ set +x
Launching on node circe-n053
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ pids+=($!)
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n051
+ set +x
Launching on node circe-n054
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n052
+ pids+=($!)
+ set +x
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n053
+ srun --mem=0 -N 1 -n 1 -w circe-n052 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=1 --master_addr=10.0.1.51 --master_port=4728' -e SLURM_JOB_ID=89451 -e SLURM_NTASKS_PER_NODE=16 cont_89451 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w circe-n051 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=0 --master_addr=10.0.1.51 --master_port=4728' -e SLURM_JOB_ID=89451 -e SLURM_NTASKS_PER_NODE=16 cont_89451 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n054
+ srun --mem=0 -N 1 -n 1 -w circe-n053 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=2 --master_addr=10.0.1.51 --master_port=4728' -e SLURM_JOB_ID=89451 -e SLURM_NTASKS_PER_NODE=16 cont_89451 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w circe-n054 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=3 --master_addr=10.0.1.51 --master_port=4728' -e SLURM_JOB_ID=89451 -e SLURM_NTASKS_PER_NODE=16 cont_89451 ./run_and_time.sh
Run vars: id 89451 gpus 16 mparams  --nnodes=4 --node_rank=0 --master_addr=10.0.1.51 --master_port=4728
Run vars: id 89451 gpus 16 mparams  --nnodes=4 --node_rank=2 --master_addr=10.0.1.51 --master_port=4728
Run vars: id 89451 gpus 16 mparams  --nnodes=4 --node_rank=1 --master_addr=10.0.1.51 --master_port=4728
Run vars: id 89451 gpus 16 mparams  --nnodes=4 --node_rank=3 --master_addr=10.0.1.51 --master_port=4728
STARTING TIMING RUN AT 2019-05-23 02:28:26 AM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=0 --master_addr=10.0.1.51 --master_port=4728 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 02:28:26 AM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=2 --master_addr=10.0.1.51 --master_port=4728 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 02:28:26 AM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=3 --master_addr=10.0.1.51 --master_port=4728 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 02:28:26 AM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=1 --master_addr=10.0.1.51 --master_port=4728 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558578517.277 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.277 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.277 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.277 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.278 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.278 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558578517.280 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578517.281 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.281 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.281 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.281 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.281 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578517.282 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.282 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.282 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.282 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558578517.311 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.311 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.311 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.311 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.312 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.312 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.312 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.312 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558578517.313 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.313 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.313 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.313 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558578517.314 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578517.314 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578517.314 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558578517.315 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558578517.348 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.348 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.349 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558578517.350 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.350 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.351 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.351 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558578517.351 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.352 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.352 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.352 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.352 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.352 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.352 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558578517.353 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558578517.353 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558578517.363 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578517.366 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.366 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.367 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.367 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.367 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.367 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558578517.368 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.368 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.368 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.368 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.368 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578517.368 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558578517.369 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578517.369 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558578517.370 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
0 Using seed = 1784372399
1 Using seed = 1784372400
3 Using seed = 1784372402
4 Using seed = 1784372403
5 Using seed = 1784372404
2 Using seed = 1784372401
:::MLL 1558578548.679 max_samples: {"value": 1, "metadata": {"file": "utils.py", "lineno": 465}}
11 Using seed = 1784372410
9 Using seed = 1784372408
7 Using seed = 1784372406
8 Using seed = 1784372407
6 Using seed = 1784372405
23 Using seed = 1784372422
31 Using seed = 1784372430
17 Using seed = 1784372416
19 Using seed = 1784372418
22 Using seed = 1784372421
26 Using seed = 1784372425
28 Using seed = 1784372427
24 Using seed = 1784372423
30 Using seed = 1784372429
25 Using seed = 1784372424
29 Using seed = 1784372428
20 Using seed = 1784372419
21 Using seed = 1784372420
27 Using seed = 1784372426
18 Using seed = 1784372417
16 Using seed = 1784372415
32 Using seed = 1784372431
35 Using seed = 1784372434
41 Using seed = 1784372440
36 Using seed = 1784372435
47 Using seed = 1784372446
46 Using seed = 1784372445
33 Using seed = 1784372432
38 Using seed = 1784372437
42 Using seed = 1784372441
37 Using seed = 1784372436
44 Using seed = 1784372443
45 Using seed = 1784372444
34 Using seed = 1784372433
43 Using seed = 1784372442
39 Using seed = 1784372438
40 Using seed = 1784372439
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
58 Using seed = 1784372457
Delaying allreduces to the end of backward()
:::MLL 1558578549.435 model_bn_span: {"value": 24, "metadata": {"file": "train.py", "lineno": 480}}
:::MLL 1558578549.435 global_batch_size: {"value": 1536, "metadata": {"file": "train.py", "lineno": 481}}
:::MLL 1558578549.448 opt_base_learning_rate: {"value": 0.14, "metadata": {"file": "train.py", "lineno": 511}}
:::MLL 1558578549.448 opt_weight_decay: {"value": 0.00017, "metadata": {"file": "train.py", "lineno": 513}}
:::MLL 1558578549.448 opt_learning_rate_warmup_steps: {"value": 850, "metadata": {"file": "train.py", "lineno": 516}}
:::MLL 1558578549.449 opt_learning_rate_warmup_factor: {"value": 0, "metadata": {"file": "train.py", "lineno": 518}}
61 Using seed = 1784372460
63 Using seed = 1784372462
59 Using seed = 1784372458
51 Using seed = 1784372450
52 Using seed = 1784372451
50 Using seed = 1784372449
62 Using seed = 1784372461
54 Using seed = 1784372453
49 Using seed = 1784372448
57 Using seed = 1784372456
48 Using seed = 1784372447
56 Using seed = 1784372455
53 Using seed = 1784372452
55 Using seed = 1784372454
60 Using seed = 1784372459
10 Using seed = 1784372409
14 Using seed = 1784372413
15 Using seed = 1784372414
12 Using seed = 1784372411
13 Using seed = 1784372412
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
epoch nbatch loss
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
:::MLL 1558578560.380 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 604}}
:::MLL 1558578560.380 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 610}}
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
time_check a: 1558578562.089500189
time_check a: 1558578562.090569258
time_check a: 1558578562.089883804
time_check a: 1558578562.107694864
time_check b: 1558578568.818010569
time_check b: 1558578568.929917336
time_check b: 1558578568.960079670
time_check b: 1558578568.976058006
:::MLL 1558578570.037 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 32.74606450292497, "file": "train.py", "lineno": 669}}
:::MLL 1558578570.037 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 673}}
Iteration:      0, Loss function: 23.140, Average Loss: 0.023, avg. samples / sec: 103.67
Iteration:      0, Loss function: 23.022, Average Loss: 0.023, avg. samples / sec: 83.33
Iteration:      0, Loss function: 23.044, Average Loss: 0.023, avg. samples / sec: 101.34
Iteration:      0, Loss function: 22.948, Average Loss: 0.023, avg. samples / sec: 102.54
Iteration:     20, Loss function: 20.213, Average Loss: 0.446, avg. samples / sec: 20963.55
Iteration:     20, Loss function: 20.584, Average Loss: 0.447, avg. samples / sec: 20843.50
Iteration:     20, Loss function: 20.402, Average Loss: 0.445, avg. samples / sec: 20859.80
Iteration:     20, Loss function: 20.490, Average Loss: 0.443, avg. samples / sec: 20911.64
Iteration:     40, Loss function: 16.775, Average Loss: 0.824, avg. samples / sec: 28166.26
Iteration:     40, Loss function: 16.652, Average Loss: 0.825, avg. samples / sec: 28153.91
Iteration:     40, Loss function: 16.385, Average Loss: 0.828, avg. samples / sec: 28151.08
Iteration:     40, Loss function: 16.196, Average Loss: 0.825, avg. samples / sec: 28101.37
Iteration:     60, Loss function: 11.878, Average Loss: 1.068, avg. samples / sec: 28442.36
Iteration:     60, Loss function: 11.469, Average Loss: 1.062, avg. samples / sec: 28499.53
Iteration:     60, Loss function: 10.956, Average Loss: 1.060, avg. samples / sec: 28408.15
Iteration:     60, Loss function: 11.766, Average Loss: 1.060, avg. samples / sec: 28404.15
:::MLL 1558578575.812 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 819}}
:::MLL 1558578575.812 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 673}}
Iteration:     80, Loss function: 9.920, Average Loss: 1.245, avg. samples / sec: 28409.90
Iteration:     80, Loss function: 10.580, Average Loss: 1.256, avg. samples / sec: 28362.26
Iteration:     80, Loss function: 10.323, Average Loss: 1.249, avg. samples / sec: 28376.13
Iteration:     80, Loss function: 10.383, Average Loss: 1.249, avg. samples / sec: 28311.71
Iteration:    100, Loss function: 9.171, Average Loss: 1.418, avg. samples / sec: 28426.30
Iteration:    100, Loss function: 9.608, Average Loss: 1.415, avg. samples / sec: 28371.49
Iteration:    100, Loss function: 9.316, Average Loss: 1.414, avg. samples / sec: 28442.04
Iteration:    100, Loss function: 9.175, Average Loss: 1.425, avg. samples / sec: 28344.44
Iteration:    120, Loss function: 8.583, Average Loss: 1.563, avg. samples / sec: 28424.78
Iteration:    120, Loss function: 9.088, Average Loss: 1.566, avg. samples / sec: 28430.81
Iteration:    120, Loss function: 9.243, Average Loss: 1.572, avg. samples / sec: 28469.12
Iteration:    120, Loss function: 9.040, Average Loss: 1.565, avg. samples / sec: 28407.38
Iteration:    140, Loss function: 8.795, Average Loss: 1.707, avg. samples / sec: 28494.31
Iteration:    140, Loss function: 9.206, Average Loss: 1.710, avg. samples / sec: 28484.89
Iteration:    140, Loss function: 8.885, Average Loss: 1.717, avg. samples / sec: 28488.85
Iteration:    140, Loss function: 8.255, Average Loss: 1.708, avg. samples / sec: 28476.17
:::MLL 1558578579.922 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 819}}
:::MLL 1558578579.923 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 673}}
Iteration:    160, Loss function: 9.020, Average Loss: 1.844, avg. samples / sec: 28302.98
Iteration:    160, Loss function: 7.705, Average Loss: 1.842, avg. samples / sec: 28317.52
Iteration:    160, Loss function: 8.420, Average Loss: 1.853, avg. samples / sec: 28301.81
Iteration:    160, Loss function: 8.592, Average Loss: 1.845, avg. samples / sec: 28295.36
Iteration:    180, Loss function: 8.179, Average Loss: 1.974, avg. samples / sec: 28385.31
Iteration:    180, Loss function: 8.326, Average Loss: 1.985, avg. samples / sec: 28396.33
Iteration:    180, Loss function: 8.620, Average Loss: 1.977, avg. samples / sec: 28398.04
Iteration:    180, Loss function: 8.240, Average Loss: 1.974, avg. samples / sec: 28386.37
Iteration:    200, Loss function: 8.046, Average Loss: 2.096, avg. samples / sec: 28457.86
Iteration:    200, Loss function: 8.261, Average Loss: 2.098, avg. samples / sec: 28451.05
Iteration:    200, Loss function: 7.863, Average Loss: 2.095, avg. samples / sec: 28438.48
Iteration:    200, Loss function: 7.485, Average Loss: 2.106, avg. samples / sec: 28421.53
Iteration:    220, Loss function: 8.007, Average Loss: 2.214, avg. samples / sec: 28429.02
Iteration:    220, Loss function: 7.546, Average Loss: 2.211, avg. samples / sec: 28449.17
Iteration:    220, Loss function: 8.028, Average Loss: 2.215, avg. samples / sec: 28427.95
Iteration:    220, Loss function: 8.393, Average Loss: 2.222, avg. samples / sec: 28410.78
:::MLL 1558578584.094 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 819}}
:::MLL 1558578584.095 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 673}}
Iteration:    240, Loss function: 7.884, Average Loss: 2.330, avg. samples / sec: 28100.08
Iteration:    240, Loss function: 7.994, Average Loss: 2.337, avg. samples / sec: 28155.74
Iteration:    240, Loss function: 7.751, Average Loss: 2.328, avg. samples / sec: 28107.78
Iteration:    240, Loss function: 8.039, Average Loss: 2.327, avg. samples / sec: 28094.67
Iteration:    260, Loss function: 7.710, Average Loss: 2.427, avg. samples / sec: 28344.19
Iteration:    260, Loss function: 7.119, Average Loss: 2.432, avg. samples / sec: 28332.18
Iteration:    260, Loss function: 7.708, Average Loss: 2.440, avg. samples / sec: 28328.64
Iteration:    260, Loss function: 7.405, Average Loss: 2.432, avg. samples / sec: 28295.33
Iteration:    280, Loss function: 7.998, Average Loss: 2.533, avg. samples / sec: 28537.12
Iteration:    280, Loss function: 7.559, Average Loss: 2.542, avg. samples / sec: 28492.68
Iteration:    280, Loss function: 7.037, Average Loss: 2.527, avg. samples / sec: 28469.38
Iteration:    280, Loss function: 7.763, Average Loss: 2.534, avg. samples / sec: 28463.99
Iteration:    300, Loss function: 7.666, Average Loss: 2.635, avg. samples / sec: 28376.02
Iteration:    300, Loss function: 7.361, Average Loss: 2.621, avg. samples / sec: 28394.68
Iteration:    300, Loss function: 6.912, Average Loss: 2.628, avg. samples / sec: 28398.59
Iteration:    300, Loss function: 6.667, Average Loss: 2.625, avg. samples / sec: 28336.28
:::MLL 1558578588.209 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 819}}
:::MLL 1558578588.210 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 673}}
Iteration:    320, Loss function: 7.021, Average Loss: 2.720, avg. samples / sec: 28362.07
Iteration:    320, Loss function: 7.627, Average Loss: 2.726, avg. samples / sec: 28349.54
Iteration:    320, Loss function: 6.773, Average Loss: 2.712, avg. samples / sec: 28353.24
Iteration:    320, Loss function: 7.261, Average Loss: 2.716, avg. samples / sec: 28334.62
Iteration:    340, Loss function: 7.166, Average Loss: 2.798, avg. samples / sec: 28409.59
Iteration:    340, Loss function: 7.476, Average Loss: 2.802, avg. samples / sec: 28455.89
Iteration:    340, Loss function: 6.617, Average Loss: 2.804, avg. samples / sec: 28398.32
Iteration:    340, Loss function: 7.756, Average Loss: 2.811, avg. samples / sec: 28371.30
Iteration:    360, Loss function: 6.676, Average Loss: 2.887, avg. samples / sec: 28368.99
Iteration:    360, Loss function: 7.042, Average Loss: 2.878, avg. samples / sec: 28359.89
Iteration:    360, Loss function: 6.598, Average Loss: 2.886, avg. samples / sec: 28365.22
Iteration:    360, Loss function: 7.335, Average Loss: 2.892, avg. samples / sec: 28367.38
Iteration:    380, Loss function: 6.855, Average Loss: 2.957, avg. samples / sec: 28246.73
Iteration:    380, Loss function: 6.941, Average Loss: 2.972, avg. samples / sec: 28270.54
Iteration:    380, Loss function: 6.486, Average Loss: 2.966, avg. samples / sec: 28227.55
Iteration:    380, Loss function: 6.723, Average Loss: 2.966, avg. samples / sec: 28223.60
:::MLL 1558578592.329 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 819}}
:::MLL 1558578592.330 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 673}}
Iteration:    400, Loss function: 7.142, Average Loss: 3.044, avg. samples / sec: 28300.56
Iteration:    400, Loss function: 6.504, Average Loss: 3.033, avg. samples / sec: 28286.96
Iteration:    400, Loss function: 6.679, Average Loss: 3.045, avg. samples / sec: 28280.93
Iteration:    400, Loss function: 7.106, Average Loss: 3.040, avg. samples / sec: 28258.53
Iteration:    420, Loss function: 6.398, Average Loss: 3.104, avg. samples / sec: 28465.73
Iteration:    420, Loss function: 6.536, Average Loss: 3.112, avg. samples / sec: 28462.08
Iteration:    420, Loss function: 6.828, Average Loss: 3.109, avg. samples / sec: 28501.96
Iteration:    420, Loss function: 6.128, Average Loss: 3.113, avg. samples / sec: 28431.06
Iteration:    440, Loss function: 6.097, Average Loss: 3.172, avg. samples / sec: 28448.53
Iteration:    440, Loss function: 6.540, Average Loss: 3.182, avg. samples / sec: 28433.89
Iteration:    440, Loss function: 6.297, Average Loss: 3.180, avg. samples / sec: 28419.57
Iteration:    440, Loss function: 6.328, Average Loss: 3.184, avg. samples / sec: 28445.17
:::MLL 1558578596.495 epoch_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 819}}
:::MLL 1558578596.496 epoch_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 673}}
Iteration:    460, Loss function: 6.038, Average Loss: 3.249, avg. samples / sec: 28290.72
Iteration:    460, Loss function: 6.229, Average Loss: 3.245, avg. samples / sec: 28317.37
Iteration:    460, Loss function: 6.325, Average Loss: 3.246, avg. samples / sec: 28320.07
Iteration:    460, Loss function: 6.536, Average Loss: 3.235, avg. samples / sec: 28240.74
Iteration:    480, Loss function: 5.645, Average Loss: 3.312, avg. samples / sec: 28247.09
Iteration:    480, Loss function: 6.106, Average Loss: 3.307, avg. samples / sec: 28248.70
Iteration:    480, Loss function: 5.896, Average Loss: 3.308, avg. samples / sec: 28222.57
Iteration:    480, Loss function: 6.018, Average Loss: 3.295, avg. samples / sec: 28232.49
Iteration:    500, Loss function: 6.167, Average Loss: 3.353, avg. samples / sec: 28393.21
Iteration:    500, Loss function: 7.271, Average Loss: 3.366, avg. samples / sec: 28340.88
Iteration:    500, Loss function: 6.781, Average Loss: 3.370, avg. samples / sec: 28337.01
Iteration:    500, Loss function: 6.252, Average Loss: 3.364, avg. samples / sec: 28374.79
Iteration:    520, Loss function: 6.446, Average Loss: 3.412, avg. samples / sec: 28410.63
Iteration:    520, Loss function: 6.261, Average Loss: 3.422, avg. samples / sec: 28412.51
Iteration:    520, Loss function: 6.078, Average Loss: 3.422, avg. samples / sec: 28397.90
Iteration:    520, Loss function: 5.992, Average Loss: 3.428, avg. samples / sec: 28396.41
:::MLL 1558578600.616 epoch_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 819}}
:::MLL 1558578600.616 epoch_start: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 673}}
Iteration:    540, Loss function: 5.573, Average Loss: 3.464, avg. samples / sec: 28349.70
Iteration:    540, Loss function: 6.122, Average Loss: 3.477, avg. samples / sec: 28334.30
Iteration:    540, Loss function: 6.543, Average Loss: 3.482, avg. samples / sec: 28316.64
Iteration:    540, Loss function: 5.694, Average Loss: 3.475, avg. samples / sec: 28283.04
Iteration:    560, Loss function: 6.083, Average Loss: 3.513, avg. samples / sec: 28361.28
Iteration:    560, Loss function: 6.133, Average Loss: 3.526, avg. samples / sec: 28382.49
Iteration:    560, Loss function: 6.075, Average Loss: 3.524, avg. samples / sec: 28421.85
Iteration:    560, Loss function: 6.174, Average Loss: 3.530, avg. samples / sec: 28400.08
Iteration:    580, Loss function: 5.903, Average Loss: 3.570, avg. samples / sec: 28386.29
Iteration:    580, Loss function: 5.292, Average Loss: 3.576, avg. samples / sec: 28372.47
Iteration:    580, Loss function: 5.628, Average Loss: 3.579, avg. samples / sec: 28380.80
Iteration:    580, Loss function: 5.988, Average Loss: 3.563, avg. samples / sec: 28363.50
Iteration:    600, Loss function: 5.358, Average Loss: 3.625, avg. samples / sec: 28361.36
Iteration:    600, Loss function: 6.093, Average Loss: 3.616, avg. samples / sec: 28343.69
Iteration:    600, Loss function: 6.705, Average Loss: 3.609, avg. samples / sec: 28356.88
Iteration:    600, Loss function: 6.256, Average Loss: 3.624, avg. samples / sec: 28349.79
:::MLL 1558578604.732 epoch_stop: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 819}}
:::MLL 1558578604.733 epoch_start: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 673}}
Iteration:    620, Loss function: 5.956, Average Loss: 3.662, avg. samples / sec: 28375.75
Iteration:    620, Loss function: 5.536, Average Loss: 3.651, avg. samples / sec: 28370.40
Iteration:    620, Loss function: 5.673, Average Loss: 3.668, avg. samples / sec: 28370.58
Iteration:    620, Loss function: 5.393, Average Loss: 3.668, avg. samples / sec: 28336.64
Iteration:    640, Loss function: 5.381, Average Loss: 3.688, avg. samples / sec: 28441.58
Iteration:    640, Loss function: 5.420, Average Loss: 3.706, avg. samples / sec: 28458.93
Iteration:    640, Loss function: 6.390, Average Loss: 3.704, avg. samples / sec: 28425.82
Iteration:    640, Loss function: 5.998, Average Loss: 3.707, avg. samples / sec: 28402.67
Iteration:    660, Loss function: 4.991, Average Loss: 3.741, avg. samples / sec: 28430.70
Iteration:    660, Loss function: 5.493, Average Loss: 3.739, avg. samples / sec: 28430.50
Iteration:    660, Loss function: 5.957, Average Loss: 3.723, avg. samples / sec: 28411.82
Iteration:    660, Loss function: 5.345, Average Loss: 3.744, avg. samples / sec: 28410.36
Iteration:    680, Loss function: 6.500, Average Loss: 3.778, avg. samples / sec: 28406.22
Iteration:    680, Loss function: 5.615, Average Loss: 3.779, avg. samples / sec: 28397.78
Iteration:    680, Loss function: 5.275, Average Loss: 3.763, avg. samples / sec: 28406.24
Iteration:    680, Loss function: 5.677, Average Loss: 3.780, avg. samples / sec: 28422.77
:::MLL 1558578608.899 epoch_stop: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 819}}
:::MLL 1558578608.900 epoch_start: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 673}}
Iteration:    700, Loss function: 5.918, Average Loss: 3.817, avg. samples / sec: 28168.52
Iteration:    700, Loss function: 5.096, Average Loss: 3.811, avg. samples / sec: 28200.69
Iteration:    700, Loss function: 6.255, Average Loss: 3.796, avg. samples / sec: 28168.50
Iteration:    700, Loss function: 5.427, Average Loss: 3.814, avg. samples / sec: 28118.23
Iteration:    720, Loss function: 6.690, Average Loss: 3.857, avg. samples / sec: 28483.00
Iteration:    720, Loss function: 5.388, Average Loss: 3.852, avg. samples / sec: 28534.82
Iteration:    720, Loss function: 6.263, Average Loss: 3.853, avg. samples / sec: 28481.73
Iteration:    720, Loss function: 5.603, Average Loss: 3.838, avg. samples / sec: 28429.58
Iteration:    740, Loss function: 6.082, Average Loss: 3.886, avg. samples / sec: 28462.00
Iteration:    740, Loss function: 5.298, Average Loss: 3.872, avg. samples / sec: 28515.81
Iteration:    740, Loss function: 5.112, Average Loss: 3.885, avg. samples / sec: 28438.14
Iteration:    740, Loss function: 4.808, Average Loss: 3.889, avg. samples / sec: 28404.57
Iteration:    760, Loss function: 5.671, Average Loss: 3.921, avg. samples / sec: 28435.22
Iteration:    760, Loss function: 5.928, Average Loss: 3.903, avg. samples / sec: 28388.69
Iteration:    760, Loss function: 6.745, Average Loss: 3.916, avg. samples / sec: 28404.35
Iteration:    760, Loss function: 5.105, Average Loss: 3.916, avg. samples / sec: 28371.25
:::MLL 1558578613.013 epoch_stop: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 819}}
:::MLL 1558578613.014 epoch_start: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 673}}
Iteration:    780, Loss function: 5.234, Average Loss: 3.945, avg. samples / sec: 28356.03
Iteration:    780, Loss function: 4.965, Average Loss: 3.947, avg. samples / sec: 28350.81
Iteration:    780, Loss function: 5.086, Average Loss: 3.931, avg. samples / sec: 28319.66
Iteration:    780, Loss function: 5.001, Average Loss: 3.945, avg. samples / sec: 28324.15
Iteration:    800, Loss function: 5.334, Average Loss: 3.974, avg. samples / sec: 28483.00
Iteration:    800, Loss function: 5.759, Average Loss: 3.978, avg. samples / sec: 28439.24
Iteration:    800, Loss function: 4.860, Average Loss: 3.959, avg. samples / sec: 28426.49
Iteration:    800, Loss function: 5.902, Average Loss: 3.972, avg. samples / sec: 28389.66
Iteration:    820, Loss function: 5.251, Average Loss: 4.002, avg. samples / sec: 28385.19
Iteration:    820, Loss function: 5.375, Average Loss: 3.988, avg. samples / sec: 28434.03
Iteration:    820, Loss function: 4.848, Average Loss: 4.004, avg. samples / sec: 28394.20
Iteration:    820, Loss function: 5.653, Average Loss: 4.006, avg. samples / sec: 28338.84
:::MLL 1558578617.124 epoch_stop: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 819}}
:::MLL 1558578617.125 epoch_start: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 673}}
Iteration:    840, Loss function: 5.535, Average Loss: 4.028, avg. samples / sec: 28299.09
Iteration:    840, Loss function: 5.787, Average Loss: 4.013, avg. samples / sec: 28286.34
Iteration:    840, Loss function: 5.395, Average Loss: 4.030, avg. samples / sec: 28329.18
Iteration:    840, Loss function: 5.591, Average Loss: 4.032, avg. samples / sec: 28293.55
Iteration:    860, Loss function: 5.543, Average Loss: 4.053, avg. samples / sec: 28387.15
Iteration:    860, Loss function: 5.645, Average Loss: 4.058, avg. samples / sec: 28375.03
Iteration:    860, Loss function: 5.398, Average Loss: 4.058, avg. samples / sec: 28396.49
Iteration:    860, Loss function: 5.502, Average Loss: 4.040, avg. samples / sec: 28353.74
Iteration:    880, Loss function: 4.540, Average Loss: 4.081, avg. samples / sec: 28452.45
Iteration:    880, Loss function: 5.432, Average Loss: 4.074, avg. samples / sec: 28399.31
Iteration:    880, Loss function: 4.316, Average Loss: 4.065, avg. samples / sec: 28408.53
Iteration:    880, Loss function: 5.121, Average Loss: 4.079, avg. samples / sec: 28357.37
Iteration:    900, Loss function: 5.556, Average Loss: 4.099, avg. samples / sec: 28373.00
Iteration:    900, Loss function: 5.284, Average Loss: 4.101, avg. samples / sec: 28290.36
Iteration:    900, Loss function: 5.769, Average Loss: 4.084, avg. samples / sec: 28318.93
Iteration:    900, Loss function: 5.217, Average Loss: 4.094, avg. samples / sec: 28278.65
:::MLL 1558578621.299 epoch_stop: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 819}}
:::MLL 1558578621.299 epoch_start: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 673}}
Iteration:    920, Loss function: 4.471, Average Loss: 4.111, avg. samples / sec: 28306.40
Iteration:    920, Loss function: 5.190, Average Loss: 4.122, avg. samples / sec: 28283.20
Iteration:    920, Loss function: 4.745, Average Loss: 4.103, avg. samples / sec: 28278.11
Iteration:    920, Loss function: 4.811, Average Loss: 4.116, avg. samples / sec: 28229.85
Iteration:    940, Loss function: 5.237, Average Loss: 4.140, avg. samples / sec: 28424.73
Iteration:    940, Loss function: 5.710, Average Loss: 4.124, avg. samples / sec: 28442.47
Iteration:    940, Loss function: 5.022, Average Loss: 4.132, avg. samples / sec: 28466.78
Iteration:    940, Loss function: 5.087, Average Loss: 4.132, avg. samples / sec: 28387.92
Iteration:    960, Loss function: 4.864, Average Loss: 4.141, avg. samples / sec: 28273.54
Iteration:    960, Loss function: 5.188, Average Loss: 4.148, avg. samples / sec: 28264.32
Iteration:    960, Loss function: 5.409, Average Loss: 4.156, avg. samples / sec: 28261.84
Iteration:    960, Loss function: 4.604, Average Loss: 4.151, avg. samples / sec: 28284.40
Iteration:    980, Loss function: 4.500, Average Loss: 4.174, avg. samples / sec: 28426.05
Iteration:    980, Loss function: 5.674, Average Loss: 4.157, avg. samples / sec: 28412.41
Iteration:    980, Loss function: 4.036, Average Loss: 4.164, avg. samples / sec: 28370.45
Iteration:    980, Loss function: 4.408, Average Loss: 4.166, avg. samples / sec: 28377.61
:::MLL 1558578625.416 epoch_stop: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 819}}
:::MLL 1558578625.416 epoch_start: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 673}}
Iteration:   1000, Loss function: 5.018, Average Loss: 4.190, avg. samples / sec: 28298.07
Iteration:   1000, Loss function: 5.350, Average Loss: 4.172, avg. samples / sec: 28304.15
Iteration:   1000, Loss function: 4.937, Average Loss: 4.178, avg. samples / sec: 28328.78
Iteration:   1000, Loss function: 4.949, Average Loss: 4.181, avg. samples / sec: 28320.67
Iteration:   1020, Loss function: 4.970, Average Loss: 4.195, avg. samples / sec: 28403.53
Iteration:   1020, Loss function: 5.320, Average Loss: 4.193, avg. samples / sec: 28378.33
Iteration:   1020, Loss function: 4.493, Average Loss: 4.185, avg. samples / sec: 28339.00
Iteration:   1020, Loss function: 4.886, Average Loss: 4.202, avg. samples / sec: 28320.71
Iteration:   1040, Loss function: 5.575, Average Loss: 4.209, avg. samples / sec: 28447.18
Iteration:   1040, Loss function: 4.717, Average Loss: 4.215, avg. samples / sec: 28476.51
Iteration:   1040, Loss function: 4.649, Average Loss: 4.211, avg. samples / sec: 28419.58
Iteration:   1040, Loss function: 4.774, Average Loss: 4.198, avg. samples / sec: 28415.12
Iteration:   1060, Loss function: 4.828, Average Loss: 4.228, avg. samples / sec: 28429.06
Iteration:   1060, Loss function: 4.947, Average Loss: 4.212, avg. samples / sec: 28478.78
Iteration:   1060, Loss function: 4.679, Average Loss: 4.222, avg. samples / sec: 28420.40
Iteration:   1060, Loss function: 5.132, Average Loss: 4.221, avg. samples / sec: 28422.17
:::MLL 1558578629.528 epoch_stop: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 819}}
:::MLL 1558578629.529 epoch_start: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 673}}
Iteration:   1080, Loss function: 4.658, Average Loss: 4.241, avg. samples / sec: 28315.91
Iteration:   1080, Loss function: 4.350, Average Loss: 4.234, avg. samples / sec: 28316.94
Iteration:   1080, Loss function: 4.678, Average Loss: 4.225, avg. samples / sec: 28304.02
Iteration:   1080, Loss function: 5.051, Average Loss: 4.232, avg. samples / sec: 28276.91
Iteration:   1100, Loss function: 4.761, Average Loss: 4.251, avg. samples / sec: 28392.47
Iteration:   1100, Loss function: 5.242, Average Loss: 4.244, avg. samples / sec: 28444.38
Iteration:   1100, Loss function: 5.016, Average Loss: 4.242, avg. samples / sec: 28383.30
Iteration:   1100, Loss function: 4.334, Average Loss: 4.234, avg. samples / sec: 28378.70
Iteration:   1120, Loss function: 4.477, Average Loss: 4.243, avg. samples / sec: 28455.51
Iteration:   1120, Loss function: 4.730, Average Loss: 4.250, avg. samples / sec: 28441.43
Iteration:   1120, Loss function: 5.029, Average Loss: 4.262, avg. samples / sec: 28384.69
Iteration:   1120, Loss function: 4.849, Average Loss: 4.254, avg. samples / sec: 28371.47
Iteration:   1140, Loss function: 6.069, Average Loss: 4.275, avg. samples / sec: 28340.84
Iteration:   1140, Loss function: 5.553, Average Loss: 4.258, avg. samples / sec: 28291.00
Iteration:   1140, Loss function: 5.142, Average Loss: 4.267, avg. samples / sec: 28348.17
Iteration:   1140, Loss function: 4.594, Average Loss: 4.260, avg. samples / sec: 28277.39
:::MLL 1558578633.699 epoch_stop: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 819}}
:::MLL 1558578633.700 epoch_start: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 673}}
Iteration:   1160, Loss function: 4.315, Average Loss: 4.269, avg. samples / sec: 28328.08
Iteration:   1160, Loss function: 5.035, Average Loss: 4.272, avg. samples / sec: 28333.42
Iteration:   1160, Loss function: 4.889, Average Loss: 4.289, avg. samples / sec: 28293.04
Iteration:   1160, Loss function: 4.773, Average Loss: 4.277, avg. samples / sec: 28273.51
Iteration:   1180, Loss function: 4.006, Average Loss: 4.297, avg. samples / sec: 28415.74
Iteration:   1180, Loss function: 4.993, Average Loss: 4.284, avg. samples / sec: 28448.97
Iteration:   1180, Loss function: 4.751, Average Loss: 4.275, avg. samples / sec: 28392.73
Iteration:   1180, Loss function: 4.186, Average Loss: 4.274, avg. samples / sec: 28341.89
Iteration:   1200, Loss function: 5.087, Average Loss: 4.286, avg. samples / sec: 28382.74
Iteration:   1200, Loss function: 4.145, Average Loss: 4.307, avg. samples / sec: 28358.82
Iteration:   1200, Loss function: 4.685, Average Loss: 4.281, avg. samples / sec: 28402.58
Iteration:   1200, Loss function: 4.492, Average Loss: 4.294, avg. samples / sec: 28350.26
Iteration:   1220, Loss function: 5.350, Average Loss: 4.292, avg. samples / sec: 28358.72
Iteration:   1220, Loss function: 4.433, Average Loss: 4.301, avg. samples / sec: 28379.33
Iteration:   1220, Loss function: 4.517, Average Loss: 4.315, avg. samples / sec: 28348.45
Iteration:   1220, Loss function: 4.271, Average Loss: 4.288, avg. samples / sec: 28324.35
:::MLL 1558578637.815 epoch_stop: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 819}}
:::MLL 1558578637.816 epoch_start: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 673}}
Iteration:   1240, Loss function: 4.685, Average Loss: 4.294, avg. samples / sec: 28381.55
Iteration:   1240, Loss function: 4.620, Average Loss: 4.308, avg. samples / sec: 28331.65
Iteration:   1240, Loss function: 4.912, Average Loss: 4.320, avg. samples / sec: 28338.75
Iteration:   1240, Loss function: 5.104, Average Loss: 4.299, avg. samples / sec: 28262.39
Iteration:   1260, Loss function: 4.585, Average Loss: 4.313, avg. samples / sec: 28395.92
Iteration:   1260, Loss function: 4.374, Average Loss: 4.327, avg. samples / sec: 28400.50
Iteration:   1260, Loss function: 5.615, Average Loss: 4.300, avg. samples / sec: 28377.27
Iteration:   1260, Loss function: 4.511, Average Loss: 4.303, avg. samples / sec: 28440.71
Iteration:   1280, Loss function: 4.262, Average Loss: 4.332, avg. samples / sec: 28435.06
Iteration:   1280, Loss function: 4.670, Average Loss: 4.309, avg. samples / sec: 28436.67
Iteration:   1280, Loss function: 4.788, Average Loss: 4.321, avg. samples / sec: 28410.04
Iteration:   1280, Loss function: 4.624, Average Loss: 4.311, avg. samples / sec: 28392.24
:::MLL 1558578641.925 epoch_stop: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 819}}
:::MLL 1558578641.926 epoch_start: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 673}}
Iteration:   1300, Loss function: 4.598, Average Loss: 4.327, avg. samples / sec: 28431.35
Iteration:   1300, Loss function: 4.758, Average Loss: 4.317, avg. samples / sec: 28420.90
Iteration:   1300, Loss function: 4.533, Average Loss: 4.342, avg. samples / sec: 28410.54
Iteration:   1300, Loss function: 4.463, Average Loss: 4.317, avg. samples / sec: 28416.12
Iteration:   1320, Loss function: 4.578, Average Loss: 4.344, avg. samples / sec: 28256.24
Iteration:   1320, Loss function: 3.711, Average Loss: 4.332, avg. samples / sec: 28249.73
Iteration:   1320, Loss function: 5.049, Average Loss: 4.321, avg. samples / sec: 28218.93
Iteration:   1320, Loss function: 4.598, Average Loss: 4.321, avg. samples / sec: 28210.65
Iteration:   1340, Loss function: 4.682, Average Loss: 4.335, avg. samples / sec: 28345.23
Iteration:   1340, Loss function: 4.615, Average Loss: 4.326, avg. samples / sec: 28433.05
Iteration:   1340, Loss function: 4.336, Average Loss: 4.326, avg. samples / sec: 28375.38
Iteration:   1340, Loss function: 4.672, Average Loss: 4.351, avg. samples / sec: 28335.02
Iteration:   1360, Loss function: 4.925, Average Loss: 4.356, avg. samples / sec: 28322.26
Iteration:   1360, Loss function: 4.657, Average Loss: 4.332, avg. samples / sec: 28321.05
Iteration:   1360, Loss function: 4.252, Average Loss: 4.327, avg. samples / sec: 28314.69
Iteration:   1360, Loss function: 4.390, Average Loss: 4.339, avg. samples / sec: 28268.73
:::MLL 1558578646.103 epoch_stop: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 819}}
:::MLL 1558578646.104 epoch_start: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 673}}
Iteration:   1380, Loss function: 4.872, Average Loss: 4.326, avg. samples / sec: 28321.74
Iteration:   1380, Loss function: 5.172, Average Loss: 4.336, avg. samples / sec: 28294.75
Iteration:   1380, Loss function: 4.554, Average Loss: 4.359, avg. samples / sec: 28278.09
Iteration:   1380, Loss function: 4.396, Average Loss: 4.340, avg. samples / sec: 28323.76
Iteration:   1400, Loss function: 4.392, Average Loss: 4.337, avg. samples / sec: 28330.73
Iteration:   1400, Loss function: 4.391, Average Loss: 4.330, avg. samples / sec: 28294.60
Iteration:   1400, Loss function: 3.944, Average Loss: 4.342, avg. samples / sec: 28317.49
Iteration:   1400, Loss function: 4.746, Average Loss: 4.361, avg. samples / sec: 28289.52
Iteration:   1420, Loss function: 4.568, Average Loss: 4.339, avg. samples / sec: 28340.48
Iteration:   1420, Loss function: 4.222, Average Loss: 4.335, avg. samples / sec: 28349.77
Iteration:   1420, Loss function: 3.889, Average Loss: 4.364, avg. samples / sec: 28374.88
Iteration:   1420, Loss function: 4.091, Average Loss: 4.347, avg. samples / sec: 28334.28
Iteration:   1440, Loss function: 4.271, Average Loss: 4.366, avg. samples / sec: 28411.82
Iteration:   1440, Loss function: 4.556, Average Loss: 4.343, avg. samples / sec: 28383.04
Iteration:   1440, Loss function: 4.534, Average Loss: 4.336, avg. samples / sec: 28377.72
Iteration:   1440, Loss function: 3.946, Average Loss: 4.350, avg. samples / sec: 28406.43
:::MLL 1558578650.220 epoch_stop: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 819}}
:::MLL 1558578650.221 epoch_start: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 673}}
Iteration:   1460, Loss function: 4.641, Average Loss: 4.370, avg. samples / sec: 28453.18
Iteration:   1460, Loss function: 4.607, Average Loss: 4.351, avg. samples / sec: 28464.51
Iteration:   1460, Loss function: 4.534, Average Loss: 4.348, avg. samples / sec: 28431.44
Iteration:   1460, Loss function: 3.624, Average Loss: 4.336, avg. samples / sec: 28421.75
Iteration:   1480, Loss function: 4.272, Average Loss: 4.350, avg. samples / sec: 28447.98
Iteration:   1480, Loss function: 4.850, Average Loss: 4.336, avg. samples / sec: 28479.55
Iteration:   1480, Loss function: 4.687, Average Loss: 4.351, avg. samples / sec: 28454.14
Iteration:   1480, Loss function: 4.238, Average Loss: 4.369, avg. samples / sec: 28374.82
Iteration:   1500, Loss function: 3.834, Average Loss: 4.371, avg. samples / sec: 28439.77
Iteration:   1500, Loss function: 4.798, Average Loss: 4.338, avg. samples / sec: 28376.67
Iteration:   1500, Loss function: 4.336, Average Loss: 4.351, avg. samples / sec: 28365.15
Iteration:   1500, Loss function: 4.719, Average Loss: 4.354, avg. samples / sec: 28372.57
Iteration:   1520, Loss function: 4.722, Average Loss: 4.342, avg. samples / sec: 28388.19
Iteration:   1520, Loss function: 5.186, Average Loss: 4.371, avg. samples / sec: 28379.77
Iteration:   1520, Loss function: 4.736, Average Loss: 4.352, avg. samples / sec: 28388.60
Iteration:   1520, Loss function: 4.907, Average Loss: 4.355, avg. samples / sec: 28366.83
:::MLL 1558578654.334 epoch_stop: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 819}}
:::MLL 1558578654.335 epoch_start: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 673}}
Iteration:   1540, Loss function: 4.477, Average Loss: 4.373, avg. samples / sec: 28199.37
Iteration:   1540, Loss function: 4.091, Average Loss: 4.344, avg. samples / sec: 28182.98
Iteration:   1540, Loss function: 5.268, Average Loss: 4.357, avg. samples / sec: 28208.06
Iteration:   1540, Loss function: 4.944, Average Loss: 4.356, avg. samples / sec: 28169.60
Iteration:   1560, Loss function: 4.293, Average Loss: 4.358, avg. samples / sec: 28224.90
Iteration:   1560, Loss function: 5.074, Average Loss: 4.360, avg. samples / sec: 28239.65
Iteration:   1560, Loss function: 4.843, Average Loss: 4.340, avg. samples / sec: 28220.01
Iteration:   1560, Loss function: 4.459, Average Loss: 4.372, avg. samples / sec: 28206.83
Iteration:   1580, Loss function: 4.156, Average Loss: 4.363, avg. samples / sec: 28397.68
Iteration:   1580, Loss function: 4.070, Average Loss: 4.339, avg. samples / sec: 28392.16
Iteration:   1580, Loss function: 4.419, Average Loss: 4.373, avg. samples / sec: 28388.96
Iteration:   1580, Loss function: 4.443, Average Loss: 4.356, avg. samples / sec: 28382.82
Iteration:   1600, Loss function: 4.364, Average Loss: 4.370, avg. samples / sec: 28361.06
Iteration:   1600, Loss function: 4.283, Average Loss: 4.365, avg. samples / sec: 28347.62
Iteration:   1600, Loss function: 4.801, Average Loss: 4.358, avg. samples / sec: 28341.23
Iteration:   1600, Loss function: 4.346, Average Loss: 4.339, avg. samples / sec: 28320.87
:::MLL 1558578658.514 epoch_stop: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 819}}
:::MLL 1558578658.515 epoch_start: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 673}}
Iteration:   1620, Loss function: 4.806, Average Loss: 4.364, avg. samples / sec: 28162.94
Iteration:   1620, Loss function: 3.995, Average Loss: 4.370, avg. samples / sec: 28140.91
Iteration:   1620, Loss function: 4.101, Average Loss: 4.343, avg. samples / sec: 28167.70
Iteration:   1620, Loss function: 4.026, Average Loss: 4.355, avg. samples / sec: 28145.65
Iteration:   1640, Loss function: 4.308, Average Loss: 4.372, avg. samples / sec: 28400.21
Iteration:   1640, Loss function: 4.181, Average Loss: 4.343, avg. samples / sec: 28412.56
Iteration:   1640, Loss function: 4.731, Average Loss: 4.365, avg. samples / sec: 28375.73
Iteration:   1640, Loss function: 3.998, Average Loss: 4.355, avg. samples / sec: 28415.55
Iteration:   1660, Loss function: 4.561, Average Loss: 4.372, avg. samples / sec: 28254.49
Iteration:   1660, Loss function: 4.021, Average Loss: 4.355, avg. samples / sec: 28257.96
Iteration:   1660, Loss function: 4.611, Average Loss: 4.365, avg. samples / sec: 28256.30
Iteration:   1660, Loss function: 4.485, Average Loss: 4.344, avg. samples / sec: 28238.67
:::MLL 1558578662.638 epoch_stop: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 819}}
:::MLL 1558578662.639 epoch_start: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 673}}
Iteration:   1680, Loss function: 4.191, Average Loss: 4.345, avg. samples / sec: 28399.16
Iteration:   1680, Loss function: 4.165, Average Loss: 4.351, avg. samples / sec: 28383.02
Iteration:   1680, Loss function: 4.580, Average Loss: 4.373, avg. samples / sec: 28365.37
Iteration:   1680, Loss function: 4.274, Average Loss: 4.362, avg. samples / sec: 28359.75
Iteration:   1700, Loss function: 3.833, Average Loss: 4.350, avg. samples / sec: 28468.27
Iteration:   1700, Loss function: 4.133, Average Loss: 4.372, avg. samples / sec: 28478.01
Iteration:   1700, Loss function: 3.840, Average Loss: 4.342, avg. samples / sec: 28449.05
Iteration:   1700, Loss function: 4.129, Average Loss: 4.363, avg. samples / sec: 28476.65
Iteration:   1720, Loss function: 4.043, Average Loss: 4.341, avg. samples / sec: 28394.06
Iteration:   1720, Loss function: 4.747, Average Loss: 4.349, avg. samples / sec: 28372.57
Iteration:   1720, Loss function: 3.915, Average Loss: 4.362, avg. samples / sec: 28388.01
Iteration:   1720, Loss function: 4.332, Average Loss: 4.372, avg. samples / sec: 28331.80
Iteration:   1740, Loss function: 3.926, Average Loss: 4.342, avg. samples / sec: 28334.82
Iteration:   1740, Loss function: 3.721, Average Loss: 4.359, avg. samples / sec: 28344.24
Iteration:   1740, Loss function: 4.008, Average Loss: 4.350, avg. samples / sec: 28335.15
Iteration:   1740, Loss function: 3.408, Average Loss: 4.369, avg. samples / sec: 28373.24
:::MLL 1558578666.752 epoch_stop: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 819}}
:::MLL 1558578666.753 epoch_start: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 673}}
Iteration:   1760, Loss function: 4.111, Average Loss: 4.356, avg. samples / sec: 28328.58
Iteration:   1760, Loss function: 3.815, Average Loss: 4.368, avg. samples / sec: 28339.74
Iteration:   1760, Loss function: 4.563, Average Loss: 4.350, avg. samples / sec: 28326.23
Iteration:   1760, Loss function: 4.534, Average Loss: 4.341, avg. samples / sec: 28257.31
Iteration:   1780, Loss function: 4.596, Average Loss: 4.367, avg. samples / sec: 28327.92
Iteration:   1780, Loss function: 4.324, Average Loss: 4.350, avg. samples / sec: 28323.32
Iteration:   1780, Loss function: 3.813, Average Loss: 4.339, avg. samples / sec: 28342.20
Iteration:   1780, Loss function: 3.837, Average Loss: 4.357, avg. samples / sec: 28270.80
Iteration:   1800, Loss function: 4.038, Average Loss: 4.367, avg. samples / sec: 28396.13
Iteration:   1800, Loss function: 3.596, Average Loss: 4.356, avg. samples / sec: 28443.41
Iteration:   1800, Loss function: 4.061, Average Loss: 4.348, avg. samples / sec: 28363.28
Iteration:   1800, Loss function: 4.995, Average Loss: 4.337, avg. samples / sec: 28387.33
Iteration:   1820, Loss function: 3.551, Average Loss: 4.363, avg. samples / sec: 28329.48
Iteration:   1820, Loss function: 4.238, Average Loss: 4.350, avg. samples / sec: 28335.77
Iteration:   1820, Loss function: 4.726, Average Loss: 4.334, avg. samples / sec: 28344.69
Iteration:   1820, Loss function: 5.291, Average Loss: 4.348, avg. samples / sec: 28295.88
:::MLL 1558578670.933 epoch_stop: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 819}}
:::MLL 1558578670.933 epoch_start: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 673}}
Iteration:   1840, Loss function: 4.128, Average Loss: 4.331, avg. samples / sec: 28185.63
Iteration:   1840, Loss function: 4.794, Average Loss: 4.345, avg. samples / sec: 28203.55
Iteration:   1840, Loss function: 5.003, Average Loss: 4.349, avg. samples / sec: 28113.18
Iteration:   1840, Loss function: 4.175, Average Loss: 4.359, avg. samples / sec: 28054.13
Iteration:   1860, Loss function: 4.616, Average Loss: 4.333, avg. samples / sec: 28428.57
Iteration:   1860, Loss function: 4.836, Average Loss: 4.349, avg. samples / sec: 28444.78
Iteration:   1860, Loss function: 3.561, Average Loss: 4.357, avg. samples / sec: 28487.78
Iteration:   1860, Loss function: 4.139, Average Loss: 4.345, avg. samples / sec: 28390.70
Iteration:   1880, Loss function: 4.046, Average Loss: 4.347, avg. samples / sec: 28384.78
Iteration:   1880, Loss function: 4.748, Average Loss: 4.342, avg. samples / sec: 28414.52
Iteration:   1880, Loss function: 4.349, Average Loss: 4.353, avg. samples / sec: 28383.39
Iteration:   1880, Loss function: 4.123, Average Loss: 4.328, avg. samples / sec: 28324.23
Iteration:   1900, Loss function: 4.497, Average Loss: 4.342, avg. samples / sec: 28356.53
Iteration:   1900, Loss function: 4.334, Average Loss: 4.352, avg. samples / sec: 28358.27
Iteration:   1900, Loss function: 4.381, Average Loss: 4.329, avg. samples / sec: 28387.16
Iteration:   1900, Loss function: 4.313, Average Loss: 4.344, avg. samples / sec: 28338.89
:::MLL 1558578675.049 epoch_stop: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 819}}
:::MLL 1558578675.049 epoch_start: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 673}}
Iteration:   1920, Loss function: 4.050, Average Loss: 4.350, avg. samples / sec: 28276.44
Iteration:   1920, Loss function: 4.185, Average Loss: 4.346, avg. samples / sec: 28276.04
Iteration:   1920, Loss function: 3.889, Average Loss: 4.340, avg. samples / sec: 28220.35
Iteration:   1920, Loss function: 4.648, Average Loss: 4.326, avg. samples / sec: 28223.72
Iteration:   1940, Loss function: 4.378, Average Loss: 4.350, avg. samples / sec: 28315.25
Iteration:   1940, Loss function: 4.704, Average Loss: 4.345, avg. samples / sec: 28313.59
Iteration:   1940, Loss function: 5.259, Average Loss: 4.321, avg. samples / sec: 28341.44
Iteration:   1940, Loss function: 4.236, Average Loss: 4.336, avg. samples / sec: 28318.22
Iteration:   1960, Loss function: 4.243, Average Loss: 4.349, avg. samples / sec: 28346.32
Iteration:   1960, Loss function: 4.113, Average Loss: 4.320, avg. samples / sec: 28378.65
Iteration:   1960, Loss function: 3.564, Average Loss: 4.341, avg. samples / sec: 28353.14
Iteration:   1960, Loss function: 5.097, Average Loss: 4.339, avg. samples / sec: 28392.10
Iteration:   1980, Loss function: 4.474, Average Loss: 4.338, avg. samples / sec: 28440.47
Iteration:   1980, Loss function: 3.794, Average Loss: 4.334, avg. samples / sec: 28435.82
Iteration:   1980, Loss function: 4.233, Average Loss: 4.319, avg. samples / sec: 28436.48
Iteration:   1980, Loss function: 4.084, Average Loss: 4.346, avg. samples / sec: 28426.31
:::MLL 1558578679.167 epoch_stop: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 819}}
:::MLL 1558578679.167 epoch_start: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 673}}
Iteration:   2000, Loss function: 4.499, Average Loss: 4.334, avg. samples / sec: 28307.39
Iteration:   2000, Loss function: 4.016, Average Loss: 4.344, avg. samples / sec: 28311.98
Iteration:   2000, Loss function: 4.163, Average Loss: 4.318, avg. samples / sec: 28260.31
Iteration:   2000, Loss function: 4.151, Average Loss: 4.332, avg. samples / sec: 28254.86
Iteration:   2020, Loss function: 4.120, Average Loss: 4.339, avg. samples / sec: 28361.48
Iteration:   2020, Loss function: 4.135, Average Loss: 4.315, avg. samples / sec: 28404.47
Iteration:   2020, Loss function: 4.178, Average Loss: 4.329, avg. samples / sec: 28399.39
Iteration:   2020, Loss function: 4.185, Average Loss: 4.332, avg. samples / sec: 28323.55
Iteration:   2040, Loss function: 3.808, Average Loss: 4.325, avg. samples / sec: 28430.20
Iteration:   2040, Loss function: 4.367, Average Loss: 4.335, avg. samples / sec: 28417.55
Iteration:   2040, Loss function: 4.357, Average Loss: 4.312, avg. samples / sec: 28414.61
Iteration:   2040, Loss function: 4.533, Average Loss: 4.329, avg. samples / sec: 28448.57
Iteration:   2060, Loss function: 4.060, Average Loss: 4.309, avg. samples / sec: 28270.64
Iteration:   2060, Loss function: 4.273, Average Loss: 4.331, avg. samples / sec: 28256.06
Iteration:   2060, Loss function: 4.459, Average Loss: 4.328, avg. samples / sec: 28244.79
Iteration:   2060, Loss function: 4.030, Average Loss: 4.324, avg. samples / sec: 28224.83
:::MLL 1558578683.341 epoch_stop: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 819}}
:::MLL 1558578683.342 epoch_start: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 673}}
Iteration:   2080, Loss function: 4.146, Average Loss: 4.331, avg. samples / sec: 28323.60
Iteration:   2080, Loss function: 3.981, Average Loss: 4.326, avg. samples / sec: 28339.16
Iteration:   2080, Loss function: 3.946, Average Loss: 4.323, avg. samples / sec: 28352.42
Iteration:   2080, Loss function: 3.525, Average Loss: 4.307, avg. samples / sec: 28315.46
Iteration:   2100, Loss function: 4.564, Average Loss: 4.303, avg. samples / sec: 28383.14
Iteration:   2100, Loss function: 4.004, Average Loss: 4.327, avg. samples / sec: 28381.87
Iteration:   2100, Loss function: 4.063, Average Loss: 4.326, avg. samples / sec: 28375.93
Iteration:   2100, Loss function: 4.933, Average Loss: 4.321, avg. samples / sec: 28373.39
Iteration:   2120, Loss function: 4.512, Average Loss: 4.317, avg. samples / sec: 28445.94
Iteration:   2120, Loss function: 4.104, Average Loss: 4.322, avg. samples / sec: 28432.88
Iteration:   2120, Loss function: 4.582, Average Loss: 4.320, avg. samples / sec: 28438.21
Iteration:   2120, Loss function: 3.561, Average Loss: 4.301, avg. samples / sec: 28404.48
:::MLL 1558578687.460 epoch_stop: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 819}}
:::MLL 1558578687.461 epoch_start: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 673}}
Iteration:   2140, Loss function: 3.770, Average Loss: 4.317, avg. samples / sec: 28215.79
Iteration:   2140, Loss function: 3.537, Average Loss: 4.315, avg. samples / sec: 28217.24
Iteration:   2140, Loss function: 3.693, Average Loss: 4.312, avg. samples / sec: 28198.26
Iteration:   2140, Loss function: 3.792, Average Loss: 4.296, avg. samples / sec: 28204.61
Iteration:   2160, Loss function: 4.005, Average Loss: 4.290, avg. samples / sec: 28330.10
Iteration:   2160, Loss function: 4.570, Average Loss: 4.308, avg. samples / sec: 28303.85
Iteration:   2160, Loss function: 4.206, Average Loss: 4.310, avg. samples / sec: 28289.55
Iteration:   2160, Loss function: 4.254, Average Loss: 4.310, avg. samples / sec: 28260.43
Iteration:   2180, Loss function: 4.648, Average Loss: 4.285, avg. samples / sec: 28330.17
Iteration:   2180, Loss function: 4.273, Average Loss: 4.306, avg. samples / sec: 28323.30
Iteration:   2180, Loss function: 3.996, Average Loss: 4.308, avg. samples / sec: 28345.18
Iteration:   2180, Loss function: 4.288, Average Loss: 4.301, avg. samples / sec: 28290.60
Iteration:   2200, Loss function: 4.362, Average Loss: 4.300, avg. samples / sec: 28394.78
Iteration:   2200, Loss function: 4.917, Average Loss: 4.305, avg. samples / sec: 28359.86
Iteration:   2200, Loss function: 4.027, Average Loss: 4.305, avg. samples / sec: 28363.69
Iteration:   2200, Loss function: 4.426, Average Loss: 4.285, avg. samples / sec: 28343.76
:::MLL 1558578691.635 epoch_stop: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 819}}
:::MLL 1558578691.635 epoch_start: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 673}}
Iteration:   2220, Loss function: 4.183, Average Loss: 4.299, avg. samples / sec: 28373.88
Iteration:   2220, Loss function: 3.902, Average Loss: 4.299, avg. samples / sec: 28373.30
Iteration:   2220, Loss function: 4.168, Average Loss: 4.282, avg. samples / sec: 28343.67
Iteration:   2220, Loss function: 4.142, Average Loss: 4.297, avg. samples / sec: 28299.74
Iteration:   2240, Loss function: 3.251, Average Loss: 4.296, avg. samples / sec: 28312.95
Iteration:   2240, Loss function: 4.164, Average Loss: 4.292, avg. samples / sec: 28376.13
Iteration:   2240, Loss function: 4.677, Average Loss: 4.295, avg. samples / sec: 28298.21
Iteration:   2240, Loss function: 4.165, Average Loss: 4.275, avg. samples / sec: 28320.37
Iteration:   2260, Loss function: 4.285, Average Loss: 4.294, avg. samples / sec: 28332.96
Iteration:   2260, Loss function: 4.765, Average Loss: 4.272, avg. samples / sec: 28351.52
Iteration:   2260, Loss function: 5.081, Average Loss: 4.293, avg. samples / sec: 28328.98
Iteration:   2260, Loss function: 3.948, Average Loss: 4.290, avg. samples / sec: 28276.72
Iteration:   2280, Loss function: 4.130, Average Loss: 4.267, avg. samples / sec: 28436.64
Iteration:   2280, Loss function: 4.119, Average Loss: 4.284, avg. samples / sec: 28486.73
Iteration:   2280, Loss function: 4.566, Average Loss: 4.290, avg. samples / sec: 28421.95
Iteration:   2280, Loss function: 4.052, Average Loss: 4.290, avg. samples / sec: 28396.52
:::MLL 1558578695.755 epoch_stop: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 819}}
:::MLL 1558578695.756 epoch_start: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 673}}
Iteration:   2300, Loss function: 4.037, Average Loss: 4.262, avg. samples / sec: 28184.74
Iteration:   2300, Loss function: 4.582, Average Loss: 4.286, avg. samples / sec: 28210.36
Iteration:   2300, Loss function: 4.276, Average Loss: 4.287, avg. samples / sec: 28203.82
Iteration:   2300, Loss function: 3.372, Average Loss: 4.277, avg. samples / sec: 28147.32
Iteration:   2320, Loss function: 4.058, Average Loss: 4.259, avg. samples / sec: 28361.91
Iteration:   2320, Loss function: 4.347, Average Loss: 4.271, avg. samples / sec: 28404.00
Iteration:   2320, Loss function: 4.097, Average Loss: 4.281, avg. samples / sec: 28361.67
Iteration:   2320, Loss function: 4.109, Average Loss: 4.283, avg. samples / sec: 28364.42
Iteration:   2340, Loss function: 3.717, Average Loss: 4.270, avg. samples / sec: 28437.69
Iteration:   2340, Loss function: 4.288, Average Loss: 4.279, avg. samples / sec: 28442.52
Iteration:   2340, Loss function: 3.874, Average Loss: 4.256, avg. samples / sec: 28430.12
Iteration:   2340, Loss function: 4.331, Average Loss: 4.279, avg. samples / sec: 28379.44
Iteration:   2360, Loss function: 3.673, Average Loss: 4.274, avg. samples / sec: 28402.07
Iteration:   2360, Loss function: 3.545, Average Loss: 4.269, avg. samples / sec: 28339.75
Iteration:   2360, Loss function: 4.220, Average Loss: 4.277, avg. samples / sec: 28340.90
Iteration:   2360, Loss function: 3.867, Average Loss: 4.252, avg. samples / sec: 28341.18
:::MLL 1558578699.875 epoch_stop: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 819}}
:::MLL 1558578699.876 epoch_start: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 673}}
Iteration:   2380, Loss function: 3.972, Average Loss: 4.250, avg. samples / sec: 28223.19
Iteration:   2380, Loss function: 4.516, Average Loss: 4.266, avg. samples / sec: 28214.67
Iteration:   2380, Loss function: 4.673, Average Loss: 4.270, avg. samples / sec: 28208.86
Iteration:   2380, Loss function: 4.516, Average Loss: 4.272, avg. samples / sec: 28177.94
Iteration:   2400, Loss function: 4.155, Average Loss: 4.268, avg. samples / sec: 28480.06
Iteration:   2400, Loss function: 4.030, Average Loss: 4.246, avg. samples / sec: 28436.26
Iteration:   2400, Loss function: 4.258, Average Loss: 4.264, avg. samples / sec: 28426.19
Iteration:   2400, Loss function: 4.420, Average Loss: 4.262, avg. samples / sec: 28410.53
Iteration:   2420, Loss function: 4.127, Average Loss: 4.260, avg. samples / sec: 28335.47
Iteration:   2420, Loss function: 4.074, Average Loss: 4.243, avg. samples / sec: 28287.09
Iteration:   2420, Loss function: 4.589, Average Loss: 4.266, avg. samples / sec: 28242.75
Iteration:   2420, Loss function: 3.539, Average Loss: 4.260, avg. samples / sec: 28245.34
Iteration:   2440, Loss function: 4.145, Average Loss: 4.263, avg. samples / sec: 28476.97
Iteration:   2440, Loss function: 3.908, Average Loss: 4.255, avg. samples / sec: 28416.75
Iteration:   2440, Loss function: 5.007, Average Loss: 4.259, avg. samples / sec: 28494.85
Iteration:   2440, Loss function: 4.279, Average Loss: 4.241, avg. samples / sec: 28301.15
:::MLL 1558578704.050 epoch_stop: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 819}}
:::MLL 1558578704.050 epoch_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 673}}
Iteration:   2460, Loss function: 3.667, Average Loss: 4.250, avg. samples / sec: 28224.40
Iteration:   2460, Loss function: 3.957, Average Loss: 4.235, avg. samples / sec: 28351.33
Iteration:   2460, Loss function: 4.527, Average Loss: 4.260, avg. samples / sec: 28203.01
Iteration:   2460, Loss function: 4.504, Average Loss: 4.257, avg. samples / sec: 28183.90
Iteration:   2480, Loss function: 3.830, Average Loss: 4.256, avg. samples / sec: 28368.92
Iteration:   2480, Loss function: 4.855, Average Loss: 4.249, avg. samples / sec: 28345.72
Iteration:   2480, Loss function: 4.687, Average Loss: 4.254, avg. samples / sec: 28391.12
Iteration:   2480, Loss function: 4.379, Average Loss: 4.232, avg. samples / sec: 28322.74
Iteration:   2500, Loss function: 3.666, Average Loss: 4.242, avg. samples / sec: 28436.38
Iteration:   2500, Loss function: 4.161, Average Loss: 4.254, avg. samples / sec: 28426.73
Iteration:   2500, Loss function: 4.477, Average Loss: 4.253, avg. samples / sec: 28433.10
Iteration:   2500, Loss function: 4.018, Average Loss: 4.228, avg. samples / sec: 28433.68
:::MLL 1558578707.137 eval_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.19 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.19 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.19 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.19 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.41s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=2.85s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.15891
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.30139
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.15251
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.04322
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.17872
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.24587
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.17574
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.26023
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.27468
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.07060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.30580
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.42094
Current AP: 0.15891 AP goal: 0.23000
:::MLL 1558578712.642 eval_accuracy: {"value": 0.15891167129935446, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 389}}
:::MLL 1558578712.715 eval_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 392}}
:::MLL 1558578712.730 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 804}}
:::MLL 1558578712.731 block_start: {"value": null, "metadata": {"first_epoch_num": 33, "epoch_count": 10.915354834308324, "file": "train.py", "lineno": 813}}
:::MLL 1558578714.367 epoch_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 819}}
:::MLL 1558578714.368 epoch_start: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 673}}
Iteration:   2520, Loss function: 4.105, Average Loss: 4.252, avg. samples / sec: 4216.03
Iteration:   2520, Loss function: 3.424, Average Loss: 4.251, avg. samples / sec: 4215.85
Iteration:   2520, Loss function: 4.813, Average Loss: 4.225, avg. samples / sec: 4216.15
Iteration:   2520, Loss function: 3.395, Average Loss: 4.239, avg. samples / sec: 4214.99
Iteration:   2540, Loss function: 4.171, Average Loss: 4.247, avg. samples / sec: 27946.64
Iteration:   2540, Loss function: 3.850, Average Loss: 4.234, avg. samples / sec: 27985.33
Iteration:   2540, Loss function: 3.991, Average Loss: 4.244, avg. samples / sec: 27944.69
Iteration:   2540, Loss function: 3.638, Average Loss: 4.220, avg. samples / sec: 27947.91
Iteration:   2560, Loss function: 4.037, Average Loss: 4.232, avg. samples / sec: 27994.20
Iteration:   2560, Loss function: 3.898, Average Loss: 4.242, avg. samples / sec: 27992.52
Iteration:   2560, Loss function: 4.124, Average Loss: 4.218, avg. samples / sec: 27992.97
Iteration:   2560, Loss function: 3.637, Average Loss: 4.247, avg. samples / sec: 27963.84
Iteration:   2580, Loss function: 4.294, Average Loss: 4.229, avg. samples / sec: 28135.28
Iteration:   2580, Loss function: 3.942, Average Loss: 4.245, avg. samples / sec: 28163.42
Iteration:   2580, Loss function: 3.811, Average Loss: 4.240, avg. samples / sec: 28076.61
Iteration:   2580, Loss function: 4.185, Average Loss: 4.216, avg. samples / sec: 28070.23
:::MLL 1558578718.538 epoch_stop: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 819}}
:::MLL 1558578718.538 epoch_start: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 673}}
Iteration:   2600, Loss function: 4.160, Average Loss: 4.225, avg. samples / sec: 27942.33
Iteration:   2600, Loss function: 3.522, Average Loss: 4.208, avg. samples / sec: 28017.84
Iteration:   2600, Loss function: 3.834, Average Loss: 4.236, avg. samples / sec: 27980.64
Iteration:   2600, Loss function: 4.308, Average Loss: 4.239, avg. samples / sec: 27874.18
Iteration:   2620, Loss function: 4.077, Average Loss: 4.231, avg. samples / sec: 28197.35
Iteration:   2620, Loss function: 3.519, Average Loss: 4.221, avg. samples / sec: 28163.15
Iteration:   2620, Loss function: 4.534, Average Loss: 4.203, avg. samples / sec: 28167.43
Iteration:   2620, Loss function: 3.602, Average Loss: 4.235, avg. samples / sec: 28205.72
Iteration:   2640, Loss function: 3.310, Average Loss: 4.226, avg. samples / sec: 28043.60
Iteration:   2640, Loss function: 4.526, Average Loss: 4.216, avg. samples / sec: 28040.48
Iteration:   2640, Loss function: 3.777, Average Loss: 4.229, avg. samples / sec: 28062.16
Iteration:   2640, Loss function: 3.737, Average Loss: 4.202, avg. samples / sec: 28031.81
Iteration:   2660, Loss function: 3.804, Average Loss: 4.199, avg. samples / sec: 28118.13
Iteration:   2660, Loss function: 3.423, Average Loss: 4.223, avg. samples / sec: 28095.08
Iteration:   2660, Loss function: 4.250, Average Loss: 4.227, avg. samples / sec: 28101.31
Iteration:   2660, Loss function: 4.097, Average Loss: 4.211, avg. samples / sec: 28087.20
:::MLL 1558578722.750 epoch_stop: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 819}}
:::MLL 1558578722.750 epoch_start: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 673}}
Iteration:   2680, Loss function: 4.240, Average Loss: 4.224, avg. samples / sec: 27971.98
Iteration:   2680, Loss function: 3.865, Average Loss: 4.219, avg. samples / sec: 27968.42
Iteration:   2680, Loss function: 4.486, Average Loss: 4.206, avg. samples / sec: 27978.31
Iteration:   2680, Loss function: 4.153, Average Loss: 4.193, avg. samples / sec: 27952.88
Iteration:   2700, Loss function: 3.973, Average Loss: 4.190, avg. samples / sec: 28240.06
Iteration:   2700, Loss function: 4.024, Average Loss: 4.215, avg. samples / sec: 28216.65
Iteration:   2700, Loss function: 3.932, Average Loss: 4.219, avg. samples / sec: 28172.87
Iteration:   2700, Loss function: 3.605, Average Loss: 4.201, avg. samples / sec: 28142.04
Iteration:   2720, Loss function: 3.791, Average Loss: 4.185, avg. samples / sec: 28145.95
Iteration:   2720, Loss function: 3.710, Average Loss: 4.197, avg. samples / sec: 28227.59
Iteration:   2720, Loss function: 3.815, Average Loss: 4.209, avg. samples / sec: 28146.81
Iteration:   2720, Loss function: 3.712, Average Loss: 4.221, avg. samples / sec: 28179.62
Iteration:   2740, Loss function: 3.928, Average Loss: 4.183, avg. samples / sec: 27961.42
Iteration:   2740, Loss function: 3.799, Average Loss: 4.204, avg. samples / sec: 27973.52
Iteration:   2740, Loss function: 3.846, Average Loss: 4.216, avg. samples / sec: 27982.79
Iteration:   2740, Loss function: 3.508, Average Loss: 4.191, avg. samples / sec: 27965.43
:::MLL 1558578726.907 epoch_stop: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 819}}
:::MLL 1558578726.907 epoch_start: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 673}}
Iteration:   2760, Loss function: 4.234, Average Loss: 4.200, avg. samples / sec: 28081.11
Iteration:   2760, Loss function: 3.756, Average Loss: 4.179, avg. samples / sec: 28074.77
Iteration:   2760, Loss function: 3.948, Average Loss: 4.187, avg. samples / sec: 28068.22
Iteration:   2760, Loss function: 3.230, Average Loss: 4.211, avg. samples / sec: 28053.55
Iteration:   2780, Loss function: 3.507, Average Loss: 4.173, avg. samples / sec: 28137.36
Iteration:   2780, Loss function: 3.969, Average Loss: 4.209, avg. samples / sec: 28158.93
Iteration:   2780, Loss function: 4.413, Average Loss: 4.194, avg. samples / sec: 28116.36
Iteration:   2780, Loss function: 4.310, Average Loss: 4.182, avg. samples / sec: 28127.38
Iteration:   2800, Loss function: 4.194, Average Loss: 4.206, avg. samples / sec: 27991.32
Iteration:   2800, Loss function: 4.282, Average Loss: 4.191, avg. samples / sec: 28005.20
Iteration:   2800, Loss function: 4.210, Average Loss: 4.181, avg. samples / sec: 27998.03
Iteration:   2800, Loss function: 4.789, Average Loss: 4.169, avg. samples / sec: 27961.74
Iteration:   2820, Loss function: 3.868, Average Loss: 4.202, avg. samples / sec: 28132.30
Iteration:   2820, Loss function: 3.519, Average Loss: 4.167, avg. samples / sec: 28152.05
Iteration:   2820, Loss function: 3.760, Average Loss: 4.187, avg. samples / sec: 28070.77
Iteration:   2820, Loss function: 3.959, Average Loss: 4.176, avg. samples / sec: 28050.15
:::MLL 1558578731.062 epoch_stop: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 819}}
:::MLL 1558578731.062 epoch_start: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 673}}
Iteration:   2840, Loss function: 4.451, Average Loss: 4.199, avg. samples / sec: 28044.19
Iteration:   2840, Loss function: 4.074, Average Loss: 4.182, avg. samples / sec: 28100.16
Iteration:   2840, Loss function: 4.343, Average Loss: 4.160, avg. samples / sec: 28051.81
Iteration:   2840, Loss function: 4.603, Average Loss: 4.170, avg. samples / sec: 28126.96
Iteration:   2860, Loss function: 4.108, Average Loss: 4.158, avg. samples / sec: 28024.19
Iteration:   2860, Loss function: 3.775, Average Loss: 4.181, avg. samples / sec: 28023.53
Iteration:   2860, Loss function: 4.486, Average Loss: 4.167, avg. samples / sec: 28032.18
Iteration:   2860, Loss function: 4.641, Average Loss: 4.194, avg. samples / sec: 27979.68
Iteration:   2880, Loss function: 4.086, Average Loss: 4.152, avg. samples / sec: 27912.66
Iteration:   2880, Loss function: 4.103, Average Loss: 4.162, avg. samples / sec: 27898.34
Iteration:   2880, Loss function: 4.041, Average Loss: 4.191, avg. samples / sec: 27926.80
Iteration:   2880, Loss function: 3.920, Average Loss: 4.175, avg. samples / sec: 27889.21
Iteration:   2900, Loss function: 4.496, Average Loss: 4.158, avg. samples / sec: 28117.19
Iteration:   2900, Loss function: 4.063, Average Loss: 4.150, avg. samples / sec: 28041.64
Iteration:   2900, Loss function: 3.596, Average Loss: 4.171, avg. samples / sec: 28061.86
Iteration:   2900, Loss function: 3.815, Average Loss: 4.185, avg. samples / sec: 28014.72
:::MLL 1558578735.287 epoch_stop: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 819}}
:::MLL 1558578735.288 epoch_start: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 673}}
Iteration:   2920, Loss function: 3.724, Average Loss: 4.153, avg. samples / sec: 27923.47
Iteration:   2920, Loss function: 3.234, Average Loss: 4.177, avg. samples / sec: 28032.76
Iteration:   2920, Loss function: 3.776, Average Loss: 4.169, avg. samples / sec: 27983.54
Iteration:   2920, Loss function: 3.484, Average Loss: 4.146, avg. samples / sec: 27924.13
Iteration:   2940, Loss function: 3.822, Average Loss: 4.166, avg. samples / sec: 28061.49
Iteration:   2940, Loss function: 4.514, Average Loss: 4.149, avg. samples / sec: 28048.45
Iteration:   2940, Loss function: 4.408, Average Loss: 4.173, avg. samples / sec: 28049.93
Iteration:   2940, Loss function: 4.226, Average Loss: 4.142, avg. samples / sec: 27988.09
Iteration:   2960, Loss function: 4.243, Average Loss: 4.146, avg. samples / sec: 27957.80
Iteration:   2960, Loss function: 4.282, Average Loss: 4.169, avg. samples / sec: 27956.17
Iteration:   2960, Loss function: 4.842, Average Loss: 4.166, avg. samples / sec: 27942.72
Iteration:   2960, Loss function: 4.460, Average Loss: 4.141, avg. samples / sec: 28066.56
:::MLL 1558578739.453 epoch_stop: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 819}}
:::MLL 1558578739.454 epoch_start: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 673}}
Iteration:   2980, Loss function: 3.256, Average Loss: 4.140, avg. samples / sec: 28110.68
Iteration:   2980, Loss function: 3.608, Average Loss: 4.160, avg. samples / sec: 28084.43
Iteration:   2980, Loss function: 4.005, Average Loss: 4.143, avg. samples / sec: 28071.93
Iteration:   2980, Loss function: 4.381, Average Loss: 4.163, avg. samples / sec: 28055.98
Iteration:   3000, Loss function: 4.083, Average Loss: 4.159, avg. samples / sec: 28033.45
Iteration:   3000, Loss function: 4.433, Average Loss: 4.133, avg. samples / sec: 27971.22
Iteration:   3000, Loss function: 3.704, Average Loss: 4.155, avg. samples / sec: 27996.24
Iteration:   3000, Loss function: 4.136, Average Loss: 4.137, avg. samples / sec: 27970.42
Iteration:   3020, Loss function: 4.117, Average Loss: 4.153, avg. samples / sec: 27948.03
Iteration:   3020, Loss function: 3.737, Average Loss: 4.146, avg. samples / sec: 27955.66
Iteration:   3020, Loss function: 3.200, Average Loss: 4.129, avg. samples / sec: 27952.34
Iteration:   3020, Loss function: 3.668, Average Loss: 4.134, avg. samples / sec: 27980.47
Iteration:   3040, Loss function: 3.991, Average Loss: 4.148, avg. samples / sec: 27927.41
Iteration:   3040, Loss function: 4.291, Average Loss: 4.124, avg. samples / sec: 27922.22
Iteration:   3040, Loss function: 3.756, Average Loss: 4.139, avg. samples / sec: 27918.68
Iteration:   3040, Loss function: 3.114, Average Loss: 4.130, avg. samples / sec: 27919.49
:::MLL 1558578743.627 epoch_stop: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 819}}
:::MLL 1558578743.628 epoch_start: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 673}}
Iteration:   3060, Loss function: 3.746, Average Loss: 4.128, avg. samples / sec: 27922.37
Iteration:   3060, Loss function: 3.514, Average Loss: 4.131, avg. samples / sec: 27903.77
Iteration:   3060, Loss function: 3.515, Average Loss: 4.118, avg. samples / sec: 27893.35
Iteration:   3060, Loss function: 4.254, Average Loss: 4.143, avg. samples / sec: 27868.18
Iteration:   3080, Loss function: 3.943, Average Loss: 4.128, avg. samples / sec: 28075.48
Iteration:   3080, Loss function: 3.824, Average Loss: 4.138, avg. samples / sec: 28059.01
Iteration:   3080, Loss function: 3.548, Average Loss: 4.114, avg. samples / sec: 28045.32
Iteration:   3080, Loss function: 3.540, Average Loss: 4.123, avg. samples / sec: 28012.83
Iteration:   3100, Loss function: 3.748, Average Loss: 4.110, avg. samples / sec: 27944.52
Iteration:   3100, Loss function: 4.025, Average Loss: 4.124, avg. samples / sec: 27900.88
Iteration:   3100, Loss function: 4.082, Average Loss: 4.134, avg. samples / sec: 27918.72
Iteration:   3100, Loss function: 3.837, Average Loss: 4.118, avg. samples / sec: 27876.72
Iteration:   3120, Loss function: 3.742, Average Loss: 4.109, avg. samples / sec: 28056.55
Iteration:   3120, Loss function: 3.585, Average Loss: 4.115, avg. samples / sec: 28126.00
Iteration:   3120, Loss function: 3.911, Average Loss: 4.121, avg. samples / sec: 28055.91
Iteration:   3120, Loss function: 3.744, Average Loss: 4.129, avg. samples / sec: 28048.99
:::MLL 1558578747.852 epoch_stop: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 819}}
:::MLL 1558578747.852 epoch_start: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 673}}
Iteration:   3140, Loss function: 3.890, Average Loss: 4.104, avg. samples / sec: 28085.63
Iteration:   3140, Loss function: 4.149, Average Loss: 4.116, avg. samples / sec: 28087.62
Iteration:   3140, Loss function: 4.027, Average Loss: 4.109, avg. samples / sec: 28080.87
Iteration:   3140, Loss function: 4.334, Average Loss: 4.126, avg. samples / sec: 28093.84
Iteration:   3160, Loss function: 3.470, Average Loss: 4.113, avg. samples / sec: 28068.56
Iteration:   3160, Loss function: 3.785, Average Loss: 4.121, avg. samples / sec: 28087.13
Iteration:   3160, Loss function: 3.867, Average Loss: 4.101, avg. samples / sec: 28058.81
Iteration:   3160, Loss function: 3.379, Average Loss: 4.103, avg. samples / sec: 28052.91
Iteration:   3180, Loss function: 3.924, Average Loss: 4.110, avg. samples / sec: 28072.81
Iteration:   3180, Loss function: 4.479, Average Loss: 4.119, avg. samples / sec: 28070.17
Iteration:   3180, Loss function: 3.239, Average Loss: 4.094, avg. samples / sec: 28069.28
Iteration:   3180, Loss function: 4.047, Average Loss: 4.099, avg. samples / sec: 28087.10
Iteration:   3200, Loss function: 3.847, Average Loss: 4.109, avg. samples / sec: 27997.94
Iteration:   3200, Loss function: 3.702, Average Loss: 4.089, avg. samples / sec: 27997.17
Iteration:   3200, Loss function: 3.625, Average Loss: 4.099, avg. samples / sec: 27995.94
Iteration:   3200, Loss function: 3.276, Average Loss: 4.113, avg. samples / sec: 27966.18
:::MLL 1558578752.015 epoch_stop: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 819}}
:::MLL 1558578752.015 epoch_start: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 673}}
Iteration:   3220, Loss function: 4.030, Average Loss: 4.107, avg. samples / sec: 28088.51
Iteration:   3220, Loss function: 4.192, Average Loss: 4.095, avg. samples / sec: 28073.25
Iteration:   3220, Loss function: 3.653, Average Loss: 4.103, avg. samples / sec: 28047.92
Iteration:   3220, Loss function: 3.414, Average Loss: 4.088, avg. samples / sec: 28034.94
Iteration:   3240, Loss function: 3.943, Average Loss: 4.099, avg. samples / sec: 28080.06
Iteration:   3240, Loss function: 4.476, Average Loss: 4.104, avg. samples / sec: 28048.79
Iteration:   3240, Loss function: 4.427, Average Loss: 4.089, avg. samples / sec: 28031.84
Iteration:   3240, Loss function: 4.352, Average Loss: 4.083, avg. samples / sec: 28051.81
Iteration:   3260, Loss function: 3.587, Average Loss: 4.095, avg. samples / sec: 28045.17
Iteration:   3260, Loss function: 3.863, Average Loss: 4.081, avg. samples / sec: 28087.02
Iteration:   3260, Loss function: 3.827, Average Loss: 4.101, avg. samples / sec: 28019.92
Iteration:   3260, Loss function: 3.709, Average Loss: 4.087, avg. samples / sec: 27995.44
Iteration:   3280, Loss function: 3.897, Average Loss: 4.080, avg. samples / sec: 28031.80
Iteration:   3280, Loss function: 4.026, Average Loss: 4.095, avg. samples / sec: 28068.24
Iteration:   3280, Loss function: 4.739, Average Loss: 4.083, avg. samples / sec: 28100.89
Iteration:   3280, Loss function: 3.532, Average Loss: 4.093, avg. samples / sec: 27972.01
:::MLL 1558578756.177 epoch_stop: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 819}}
:::MLL 1558578756.177 epoch_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 673}}
Iteration:   3300, Loss function: 3.851, Average Loss: 4.091, avg. samples / sec: 28128.47
Iteration:   3300, Loss function: 3.690, Average Loss: 4.077, avg. samples / sec: 28120.25
Iteration:   3300, Loss function: 3.767, Average Loss: 4.080, avg. samples / sec: 28125.59
Iteration:   3300, Loss function: 3.499, Average Loss: 4.089, avg. samples / sec: 28136.38
Iteration:   3320, Loss function: 4.067, Average Loss: 4.074, avg. samples / sec: 28024.96
Iteration:   3320, Loss function: 4.175, Average Loss: 4.087, avg. samples / sec: 28008.91
Iteration:   3320, Loss function: 4.001, Average Loss: 4.086, avg. samples / sec: 28044.31
Iteration:   3320, Loss function: 4.106, Average Loss: 4.073, avg. samples / sec: 28001.94
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
:::MLL 1558578758.973 eval_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.10 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.10 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.10 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.10 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.55s)
DONE (t=0.55s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=3.21s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.18289
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.33429
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.18489
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.04461
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.19248
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.29660
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.19184
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.28217
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.29767
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.08105
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.31843
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.46887
Current AP: 0.18289 AP goal: 0.23000
:::MLL 1558578763.889 eval_accuracy: {"value": 0.182891538674639, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 389}}
:::MLL 1558578763.894 eval_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 392}}
:::MLL 1558578763.909 block_stop: {"value": null, "metadata": {"first_epoch_num": 33, "file": "train.py", "lineno": 804}}
:::MLL 1558578763.909 block_start: {"value": null, "metadata": {"first_epoch_num": 44, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3340, Loss function: 3.260, Average Loss: 4.070, avg. samples / sec: 5089.51
Iteration:   3340, Loss function: 2.884, Average Loss: 4.080, avg. samples / sec: 5088.71
Iteration:   3340, Loss function: 3.536, Average Loss: 4.069, avg. samples / sec: 5088.15
Iteration:   3340, Loss function: 4.350, Average Loss: 4.083, avg. samples / sec: 5086.90
:::MLL 1558578765.348 epoch_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 819}}
:::MLL 1558578765.348 epoch_start: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 673}}
Iteration:   3360, Loss function: 3.531, Average Loss: 4.060, avg. samples / sec: 27677.40
Iteration:   3360, Loss function: 3.861, Average Loss: 4.064, avg. samples / sec: 27650.47
Iteration:   3360, Loss function: 3.817, Average Loss: 4.071, avg. samples / sec: 27660.39
Iteration:   3360, Loss function: 3.663, Average Loss: 4.074, avg. samples / sec: 27695.05
Iteration:   3380, Loss function: 3.693, Average Loss: 4.067, avg. samples / sec: 28031.18
Iteration:   3380, Loss function: 3.681, Average Loss: 4.055, avg. samples / sec: 28003.12
Iteration:   3380, Loss function: 3.210, Average Loss: 4.048, avg. samples / sec: 27991.12
Iteration:   3380, Loss function: 3.555, Average Loss: 4.062, avg. samples / sec: 27983.25
Iteration:   3400, Loss function: 3.443, Average Loss: 4.037, avg. samples / sec: 28149.36
Iteration:   3400, Loss function: 3.372, Average Loss: 4.054, avg. samples / sec: 28132.22
Iteration:   3400, Loss function: 3.023, Average Loss: 4.047, avg. samples / sec: 28138.56
Iteration:   3400, Loss function: 3.936, Average Loss: 4.051, avg. samples / sec: 28125.24
Iteration:   3420, Loss function: 3.523, Average Loss: 4.044, avg. samples / sec: 28044.06
Iteration:   3420, Loss function: 3.926, Average Loss: 4.028, avg. samples / sec: 28032.47
Iteration:   3420, Loss function: 3.531, Average Loss: 4.044, avg. samples / sec: 28073.12
Iteration:   3420, Loss function: 3.654, Average Loss: 4.039, avg. samples / sec: 27994.54
:::MLL 1558578769.509 epoch_stop: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 819}}
:::MLL 1558578769.509 epoch_start: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 673}}
Iteration:   3440, Loss function: 3.582, Average Loss: 4.017, avg. samples / sec: 28068.06
Iteration:   3440, Loss function: 3.820, Average Loss: 4.036, avg. samples / sec: 28060.00
Iteration:   3440, Loss function: 3.428, Average Loss: 4.034, avg. samples / sec: 28067.13
Iteration:   3440, Loss function: 3.748, Average Loss: 4.030, avg. samples / sec: 28080.84
Iteration:   3460, Loss function: 3.728, Average Loss: 4.018, avg. samples / sec: 28019.05
Iteration:   3460, Loss function: 3.889, Average Loss: 4.026, avg. samples / sec: 27993.23
Iteration:   3460, Loss function: 3.380, Average Loss: 4.025, avg. samples / sec: 27938.79
Iteration:   3460, Loss function: 4.051, Average Loss: 4.005, avg. samples / sec: 27911.08
Iteration:   3480, Loss function: 3.524, Average Loss: 4.015, avg. samples / sec: 28074.21
Iteration:   3480, Loss function: 3.364, Average Loss: 4.015, avg. samples / sec: 28029.34
Iteration:   3480, Loss function: 3.324, Average Loss: 3.993, avg. samples / sec: 28092.41
Iteration:   3480, Loss function: 3.165, Average Loss: 4.008, avg. samples / sec: 27995.81
Iteration:   3500, Loss function: 2.730, Average Loss: 4.005, avg. samples / sec: 28116.77
Iteration:   3500, Loss function: 3.411, Average Loss: 4.002, avg. samples / sec: 28109.44
Iteration:   3500, Loss function: 2.967, Average Loss: 3.999, avg. samples / sec: 28095.22
Iteration:   3500, Loss function: 3.234, Average Loss: 3.981, avg. samples / sec: 28068.32
:::MLL 1558578773.674 epoch_stop: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 819}}
:::MLL 1558578773.674 epoch_start: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 673}}
Iteration:   3520, Loss function: 3.216, Average Loss: 3.994, avg. samples / sec: 28052.18
Iteration:   3520, Loss function: 3.745, Average Loss: 3.990, avg. samples / sec: 28046.70
Iteration:   3520, Loss function: 4.074, Average Loss: 3.989, avg. samples / sec: 28082.02
Iteration:   3520, Loss function: 3.623, Average Loss: 3.972, avg. samples / sec: 28059.23
Iteration:   3540, Loss function: 3.025, Average Loss: 3.976, avg. samples / sec: 27990.56
Iteration:   3540, Loss function: 3.095, Average Loss: 3.961, avg. samples / sec: 28018.31
Iteration:   3540, Loss function: 3.407, Average Loss: 3.982, avg. samples / sec: 27963.10
Iteration:   3540, Loss function: 4.361, Average Loss: 3.979, avg. samples / sec: 27977.70
Iteration:   3560, Loss function: 3.650, Average Loss: 3.964, avg. samples / sec: 27973.52
Iteration:   3560, Loss function: 3.575, Average Loss: 3.950, avg. samples / sec: 27982.02
Iteration:   3560, Loss function: 3.528, Average Loss: 3.970, avg. samples / sec: 27948.97
Iteration:   3560, Loss function: 3.594, Average Loss: 3.973, avg. samples / sec: 27911.26
Iteration:   3580, Loss function: 3.266, Average Loss: 3.957, avg. samples / sec: 28069.87
Iteration:   3580, Loss function: 3.214, Average Loss: 3.960, avg. samples / sec: 28138.79
Iteration:   3580, Loss function: 3.805, Average Loss: 3.961, avg. samples / sec: 28101.23
Iteration:   3580, Loss function: 3.579, Average Loss: 3.939, avg. samples / sec: 28055.09
:::MLL 1558578777.898 epoch_stop: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 819}}
:::MLL 1558578777.898 epoch_start: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 673}}
Iteration:   3600, Loss function: 3.578, Average Loss: 3.951, avg. samples / sec: 27873.03
Iteration:   3600, Loss function: 3.726, Average Loss: 3.951, avg. samples / sec: 27852.91
Iteration:   3600, Loss function: 3.412, Average Loss: 3.950, avg. samples / sec: 27858.10
Iteration:   3600, Loss function: 3.654, Average Loss: 3.930, avg. samples / sec: 27860.16
Iteration:   3620, Loss function: 3.315, Average Loss: 3.940, avg. samples / sec: 28074.34
Iteration:   3620, Loss function: 3.474, Average Loss: 3.940, avg. samples / sec: 28076.73
Iteration:   3620, Loss function: 2.894, Average Loss: 3.920, avg. samples / sec: 28073.48
Iteration:   3620, Loss function: 3.298, Average Loss: 3.942, avg. samples / sec: 28028.84
Iteration:   3640, Loss function: 3.690, Average Loss: 3.929, avg. samples / sec: 28045.78
Iteration:   3640, Loss function: 3.824, Average Loss: 3.909, avg. samples / sec: 28056.25
Iteration:   3640, Loss function: 3.034, Average Loss: 3.932, avg. samples / sec: 28049.21
Iteration:   3640, Loss function: 2.504, Average Loss: 3.932, avg. samples / sec: 28005.90
Iteration:   3660, Loss function: 3.093, Average Loss: 3.922, avg. samples / sec: 28056.01
Iteration:   3660, Loss function: 3.602, Average Loss: 3.919, avg. samples / sec: 28015.72
Iteration:   3660, Loss function: 3.418, Average Loss: 3.899, avg. samples / sec: 28003.53
Iteration:   3660, Loss function: 4.218, Average Loss: 3.925, avg. samples / sec: 28023.64
:::MLL 1558578782.065 epoch_stop: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 819}}
:::MLL 1558578782.066 epoch_start: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 673}}
Iteration:   3680, Loss function: 2.838, Average Loss: 3.890, avg. samples / sec: 27906.27
Iteration:   3680, Loss function: 3.393, Average Loss: 3.919, avg. samples / sec: 27922.54
Iteration:   3680, Loss function: 3.685, Average Loss: 3.907, avg. samples / sec: 27870.24
Iteration:   3680, Loss function: 3.759, Average Loss: 3.912, avg. samples / sec: 27852.64
Iteration:   3700, Loss function: 3.476, Average Loss: 3.902, avg. samples / sec: 28111.66
Iteration:   3700, Loss function: 2.607, Average Loss: 3.882, avg. samples / sec: 28068.85
Iteration:   3700, Loss function: 3.813, Average Loss: 3.909, avg. samples / sec: 28074.82
Iteration:   3700, Loss function: 3.700, Average Loss: 3.895, avg. samples / sec: 28078.29
Iteration:   3720, Loss function: 3.283, Average Loss: 3.871, avg. samples / sec: 28111.00
Iteration:   3720, Loss function: 3.932, Average Loss: 3.902, avg. samples / sec: 28106.55
Iteration:   3720, Loss function: 3.334, Average Loss: 3.882, avg. samples / sec: 28115.51
Iteration:   3720, Loss function: 2.974, Average Loss: 3.894, avg. samples / sec: 28077.65
Iteration:   3740, Loss function: 3.369, Average Loss: 3.873, avg. samples / sec: 28149.62
Iteration:   3740, Loss function: 3.592, Average Loss: 3.885, avg. samples / sec: 28162.58
Iteration:   3740, Loss function: 3.660, Average Loss: 3.893, avg. samples / sec: 28135.68
Iteration:   3740, Loss function: 3.693, Average Loss: 3.863, avg. samples / sec: 28099.73
:::MLL 1558578786.227 epoch_stop: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 819}}
:::MLL 1558578786.227 epoch_start: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 673}}
:::MLL 1558578786.780 eval_start: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.08 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.08 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.08 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.08 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.87s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22532
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.38507
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23154
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05713
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.23311
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.37296
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.21907
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.31905
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33553
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09553
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36207
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.53341
Current AP: 0.22532 AP goal: 0.23000
:::MLL 1558578791.314 eval_accuracy: {"value": 0.22531776456181662, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 389}}
:::MLL 1558578791.328 eval_stop: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 392}}
:::MLL 1558578791.342 block_stop: {"value": null, "metadata": {"first_epoch_num": 44, "file": "train.py", "lineno": 804}}
:::MLL 1558578791.343 block_start: {"value": null, "metadata": {"first_epoch_num": 50, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3760, Loss function: 3.583, Average Loss: 3.875, avg. samples / sec: 5420.84
Iteration:   3760, Loss function: 3.901, Average Loss: 3.856, avg. samples / sec: 5421.84
Iteration:   3760, Loss function: 3.662, Average Loss: 3.884, avg. samples / sec: 5419.27
Iteration:   3760, Loss function: 3.234, Average Loss: 3.865, avg. samples / sec: 5418.70
Iteration:   3780, Loss function: 3.987, Average Loss: 3.870, avg. samples / sec: 28087.35
Iteration:   3780, Loss function: 3.566, Average Loss: 3.850, avg. samples / sec: 28091.75
Iteration:   3780, Loss function: 4.053, Average Loss: 3.857, avg. samples / sec: 28133.08
Iteration:   3780, Loss function: 3.112, Average Loss: 3.870, avg. samples / sec: 28085.19
Iteration:   3800, Loss function: 3.602, Average Loss: 3.861, avg. samples / sec: 27837.25
Iteration:   3800, Loss function: 3.807, Average Loss: 3.862, avg. samples / sec: 27888.69
Iteration:   3800, Loss function: 3.239, Average Loss: 3.842, avg. samples / sec: 27830.77
Iteration:   3800, Loss function: 3.084, Average Loss: 3.849, avg. samples / sec: 27800.68
:::MLL 1558578795.025 epoch_stop: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 819}}
:::MLL 1558578795.025 epoch_start: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 673}}
Iteration:   3820, Loss function: 2.867, Average Loss: 3.834, avg. samples / sec: 27950.08
Iteration:   3820, Loss function: 3.399, Average Loss: 3.841, avg. samples / sec: 27953.62
Iteration:   3820, Loss function: 3.887, Average Loss: 3.852, avg. samples / sec: 27897.14
Iteration:   3820, Loss function: 3.464, Average Loss: 3.852, avg. samples / sec: 27890.47
Iteration:   3840, Loss function: 3.298, Average Loss: 3.843, avg. samples / sec: 28027.57
Iteration:   3840, Loss function: 3.160, Average Loss: 3.831, avg. samples / sec: 28002.77
Iteration:   3840, Loss function: 3.809, Average Loss: 3.827, avg. samples / sec: 27961.30
Iteration:   3840, Loss function: 3.073, Average Loss: 3.844, avg. samples / sec: 27961.18
Iteration:   3860, Loss function: 3.783, Average Loss: 3.824, avg. samples / sec: 28031.59
Iteration:   3860, Loss function: 3.422, Average Loss: 3.820, avg. samples / sec: 28043.98
Iteration:   3860, Loss function: 3.834, Average Loss: 3.835, avg. samples / sec: 27983.69
Iteration:   3860, Loss function: 3.355, Average Loss: 3.836, avg. samples / sec: 28035.94
Iteration:   3880, Loss function: 3.483, Average Loss: 3.817, avg. samples / sec: 28001.59
Iteration:   3880, Loss function: 3.352, Average Loss: 3.828, avg. samples / sec: 28042.22
Iteration:   3880, Loss function: 3.119, Average Loss: 3.814, avg. samples / sec: 27988.98
Iteration:   3880, Loss function: 2.890, Average Loss: 3.825, avg. samples / sec: 27996.83
:::MLL 1558578799.195 epoch_stop: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 819}}
:::MLL 1558578799.195 epoch_start: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 673}}
Iteration:   3900, Loss function: 3.728, Average Loss: 3.816, avg. samples / sec: 28107.27
Iteration:   3900, Loss function: 3.817, Average Loss: 3.804, avg. samples / sec: 28054.27
Iteration:   3900, Loss function: 3.100, Average Loss: 3.811, avg. samples / sec: 28031.16
Iteration:   3900, Loss function: 3.751, Average Loss: 3.819, avg. samples / sec: 28031.75
Iteration:   3920, Loss function: 3.987, Average Loss: 3.806, avg. samples / sec: 27959.39
Iteration:   3920, Loss function: 3.275, Average Loss: 3.811, avg. samples / sec: 27982.61
Iteration:   3920, Loss function: 2.811, Average Loss: 3.799, avg. samples / sec: 27974.50
Iteration:   3920, Loss function: 2.891, Average Loss: 3.801, avg. samples / sec: 27962.58
Iteration:   3940, Loss function: 3.571, Average Loss: 3.801, avg. samples / sec: 28156.21
Iteration:   3940, Loss function: 3.314, Average Loss: 3.793, avg. samples / sec: 28147.62
Iteration:   3940, Loss function: 4.198, Average Loss: 3.796, avg. samples / sec: 28162.40
Iteration:   3940, Loss function: 3.525, Average Loss: 3.802, avg. samples / sec: 28138.08
Iteration:   3960, Loss function: 2.973, Average Loss: 3.790, avg. samples / sec: 28258.69
Iteration:   3960, Loss function: 3.484, Average Loss: 3.788, avg. samples / sec: 28245.31
Iteration:   3960, Loss function: 3.990, Average Loss: 3.793, avg. samples / sec: 28228.35
Iteration:   3960, Loss function: 4.219, Average Loss: 3.795, avg. samples / sec: 28209.35
:::MLL 1558578803.346 epoch_stop: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 819}}
:::MLL 1558578803.346 epoch_start: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 673}}
Iteration:   3980, Loss function: 3.371, Average Loss: 3.782, avg. samples / sec: 28071.98
Iteration:   3980, Loss function: 4.369, Average Loss: 3.787, avg. samples / sec: 28095.55
Iteration:   3980, Loss function: 3.893, Average Loss: 3.784, avg. samples / sec: 28086.86
Iteration:   3980, Loss function: 2.898, Average Loss: 3.781, avg. samples / sec: 28058.65
Iteration:   4000, Loss function: 3.325, Average Loss: 3.772, avg. samples / sec: 28007.36
Iteration:   4000, Loss function: 3.336, Average Loss: 3.776, avg. samples / sec: 28026.04
Iteration:   4000, Loss function: 3.388, Average Loss: 3.773, avg. samples / sec: 28025.33
Iteration:   4000, Loss function: 3.547, Average Loss: 3.779, avg. samples / sec: 27957.28
Iteration:   4020, Loss function: 3.454, Average Loss: 3.765, avg. samples / sec: 28047.37
Iteration:   4020, Loss function: 3.249, Average Loss: 3.767, avg. samples / sec: 28035.56
Iteration:   4020, Loss function: 2.859, Average Loss: 3.772, avg. samples / sec: 28095.21
Iteration:   4020, Loss function: 3.515, Average Loss: 3.769, avg. samples / sec: 27990.28
Iteration:   4040, Loss function: 3.490, Average Loss: 3.762, avg. samples / sec: 28071.20
Iteration:   4040, Loss function: 3.836, Average Loss: 3.756, avg. samples / sec: 28010.33
Iteration:   4040, Loss function: 3.486, Average Loss: 3.765, avg. samples / sec: 28018.36
Iteration:   4040, Loss function: 3.272, Average Loss: 3.758, avg. samples / sec: 28002.93
:::MLL 1558578807.566 epoch_stop: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 819}}
:::MLL 1558578807.566 epoch_start: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 673}}
Iteration:   4060, Loss function: 3.138, Average Loss: 3.755, avg. samples / sec: 28028.02
Iteration:   4060, Loss function: 3.333, Average Loss: 3.754, avg. samples / sec: 28010.39
Iteration:   4060, Loss function: 3.213, Average Loss: 3.749, avg. samples / sec: 28002.63
Iteration:   4060, Loss function: 3.294, Average Loss: 3.751, avg. samples / sec: 27986.19
Iteration:   4080, Loss function: 3.564, Average Loss: 3.745, avg. samples / sec: 28170.25
Iteration:   4080, Loss function: 3.187, Average Loss: 3.740, avg. samples / sec: 28175.82
Iteration:   4080, Loss function: 3.532, Average Loss: 3.746, avg. samples / sec: 28149.38
Iteration:   4080, Loss function: 3.418, Average Loss: 3.742, avg. samples / sec: 28147.47
Iteration:   4100, Loss function: 3.906, Average Loss: 3.739, avg. samples / sec: 28001.38
Iteration:   4100, Loss function: 4.114, Average Loss: 3.736, avg. samples / sec: 28039.56
Iteration:   4100, Loss function: 3.624, Average Loss: 3.738, avg. samples / sec: 27955.86
Iteration:   4100, Loss function: 3.834, Average Loss: 3.736, avg. samples / sec: 27944.59
Iteration:   4120, Loss function: 3.348, Average Loss: 3.728, avg. samples / sec: 28064.08
Iteration:   4120, Loss function: 3.627, Average Loss: 3.731, avg. samples / sec: 28052.54
Iteration:   4120, Loss function: 3.804, Average Loss: 3.729, avg. samples / sec: 28093.84
Iteration:   4120, Loss function: 2.880, Average Loss: 3.732, avg. samples / sec: 28077.13
:::MLL 1558578811.728 epoch_stop: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 819}}
:::MLL 1558578811.728 epoch_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 673}}
Iteration:   4140, Loss function: 3.999, Average Loss: 3.720, avg. samples / sec: 27937.31
Iteration:   4140, Loss function: 3.458, Average Loss: 3.725, avg. samples / sec: 27942.39
Iteration:   4140, Loss function: 3.504, Average Loss: 3.725, avg. samples / sec: 27931.53
Iteration:   4140, Loss function: 3.492, Average Loss: 3.721, avg. samples / sec: 27928.50
Iteration:   4160, Loss function: 3.460, Average Loss: 3.712, avg. samples / sec: 27957.55
Iteration:   4160, Loss function: 3.909, Average Loss: 3.717, avg. samples / sec: 27955.44
Iteration:   4160, Loss function: 4.057, Average Loss: 3.716, avg. samples / sec: 27928.20
Iteration:   4160, Loss function: 3.645, Average Loss: 3.716, avg. samples / sec: 27910.34
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
:::MLL 1558578814.149 eval_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.13 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.13 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.13 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.13 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=2.79s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22780
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.39077
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23394
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05788
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.23719
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.37435
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32074
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33729
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09793
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36448
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.53239
Current AP: 0.22780 AP goal: 0.23000
:::MLL 1558578818.630 eval_accuracy: {"value": 0.22780344152275925, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 389}}
:::MLL 1558578818.718 eval_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 392}}
:::MLL 1558578818.732 block_stop: {"value": null, "metadata": {"first_epoch_num": 50, "file": "train.py", "lineno": 804}}
:::MLL 1558578818.732 block_start: {"value": null, "metadata": {"first_epoch_num": 55, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   4180, Loss function: 4.043, Average Loss: 3.709, avg. samples / sec: 5403.21
Iteration:   4180, Loss function: 3.212, Average Loss: 3.704, avg. samples / sec: 5402.58
Iteration:   4180, Loss function: 2.999, Average Loss: 3.710, avg. samples / sec: 5402.90
Iteration:   4180, Loss function: 3.268, Average Loss: 3.706, avg. samples / sec: 5403.91
:::MLL 1558578820.493 epoch_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 819}}
:::MLL 1558578820.493 epoch_start: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 673}}
Iteration:   4200, Loss function: 3.327, Average Loss: 3.701, avg. samples / sec: 27892.66
Iteration:   4200, Loss function: 3.362, Average Loss: 3.699, avg. samples / sec: 27921.13
Iteration:   4200, Loss function: 2.960, Average Loss: 3.703, avg. samples / sec: 27912.03
Iteration:   4200, Loss function: 3.201, Average Loss: 3.697, avg. samples / sec: 27801.50
Iteration:   4220, Loss function: 3.805, Average Loss: 3.691, avg. samples / sec: 27909.22
Iteration:   4220, Loss function: 3.205, Average Loss: 3.692, avg. samples / sec: 27921.40
Iteration:   4220, Loss function: 3.021, Average Loss: 3.694, avg. samples / sec: 27894.35
Iteration:   4220, Loss function: 2.953, Average Loss: 3.690, avg. samples / sec: 27977.07
Iteration:   4240, Loss function: 4.207, Average Loss: 3.688, avg. samples / sec: 28129.72
Iteration:   4240, Loss function: 3.267, Average Loss: 3.683, avg. samples / sec: 28121.23
Iteration:   4240, Loss function: 2.779, Average Loss: 3.685, avg. samples / sec: 28112.71
Iteration:   4240, Loss function: 3.398, Average Loss: 3.683, avg. samples / sec: 28107.43
Iteration:   4260, Loss function: 3.588, Average Loss: 3.678, avg. samples / sec: 28036.86
Iteration:   4260, Loss function: 2.438, Average Loss: 3.676, avg. samples / sec: 28049.39
Iteration:   4260, Loss function: 3.231, Average Loss: 3.672, avg. samples / sec: 27988.25
Iteration:   4260, Loss function: 3.491, Average Loss: 3.680, avg. samples / sec: 27960.23
:::MLL 1558578824.723 epoch_stop: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 819}}
:::MLL 1558578824.724 epoch_start: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 673}}
Iteration:   4280, Loss function: 3.316, Average Loss: 3.671, avg. samples / sec: 27879.98
Iteration:   4280, Loss function: 3.882, Average Loss: 3.663, avg. samples / sec: 27830.50
Iteration:   4280, Loss function: 3.698, Average Loss: 3.670, avg. samples / sec: 27803.02
Iteration:   4280, Loss function: 3.332, Average Loss: 3.672, avg. samples / sec: 27795.70
Iteration:   4300, Loss function: 3.566, Average Loss: 3.663, avg. samples / sec: 28033.31
Iteration:   4300, Loss function: 3.526, Average Loss: 3.657, avg. samples / sec: 28012.88
Iteration:   4300, Loss function: 3.564, Average Loss: 3.668, avg. samples / sec: 28036.68
Iteration:   4300, Loss function: 3.356, Average Loss: 3.663, avg. samples / sec: 27973.66
Iteration:   4320, Loss function: 3.197, Average Loss: 3.654, avg. samples / sec: 27938.33
Iteration:   4320, Loss function: 3.411, Average Loss: 3.662, avg. samples / sec: 27937.33
Iteration:   4320, Loss function: 4.009, Average Loss: 3.655, avg. samples / sec: 27948.91
Iteration:   4320, Loss function: 3.990, Average Loss: 3.649, avg. samples / sec: 27929.97
Iteration:   4340, Loss function: 3.231, Average Loss: 3.643, avg. samples / sec: 28024.99
Iteration:   4340, Loss function: 4.089, Average Loss: 3.652, avg. samples / sec: 28019.36
Iteration:   4340, Loss function: 2.940, Average Loss: 3.657, avg. samples / sec: 28009.47
Iteration:   4340, Loss function: 3.247, Average Loss: 3.646, avg. samples / sec: 27966.51
:::MLL 1558578828.899 epoch_stop: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 819}}
:::MLL 1558578828.899 epoch_start: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 673}}
Iteration:   4360, Loss function: 3.812, Average Loss: 3.645, avg. samples / sec: 27937.95
Iteration:   4360, Loss function: 3.375, Average Loss: 3.649, avg. samples / sec: 27944.97
Iteration:   4360, Loss function: 4.126, Average Loss: 3.641, avg. samples / sec: 27930.97
Iteration:   4360, Loss function: 3.078, Average Loss: 3.638, avg. samples / sec: 27824.33
Iteration:   4380, Loss function: 3.469, Average Loss: 3.639, avg. samples / sec: 28077.14
Iteration:   4380, Loss function: 3.501, Average Loss: 3.637, avg. samples / sec: 28107.28
Iteration:   4380, Loss function: 2.916, Average Loss: 3.643, avg. samples / sec: 28035.21
Iteration:   4380, Loss function: 3.914, Average Loss: 3.634, avg. samples / sec: 28135.60
Iteration:   4400, Loss function: 3.082, Average Loss: 3.634, avg. samples / sec: 27967.43
Iteration:   4400, Loss function: 3.832, Average Loss: 3.633, avg. samples / sec: 27978.70
Iteration:   4400, Loss function: 3.601, Average Loss: 3.627, avg. samples / sec: 28011.73
Iteration:   4400, Loss function: 3.802, Average Loss: 3.638, avg. samples / sec: 27946.05
Iteration:   4420, Loss function: 3.091, Average Loss: 3.619, avg. samples / sec: 28029.20
Iteration:   4420, Loss function: 3.978, Average Loss: 3.632, avg. samples / sec: 28079.34
Iteration:   4420, Loss function: 2.903, Average Loss: 3.627, avg. samples / sec: 27981.91
Iteration:   4420, Loss function: 3.500, Average Loss: 3.627, avg. samples / sec: 27983.24
:::MLL 1558578833.118 epoch_stop: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 819}}
:::MLL 1558578833.118 epoch_start: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 673}}
Iteration:   4440, Loss function: 3.363, Average Loss: 3.613, avg. samples / sec: 28021.88
Iteration:   4440, Loss function: 2.957, Average Loss: 3.621, avg. samples / sec: 28058.93
Iteration:   4440, Loss function: 3.144, Average Loss: 3.623, avg. samples / sec: 28058.70
Iteration:   4440, Loss function: 2.695, Average Loss: 3.625, avg. samples / sec: 27961.97
Iteration:   4460, Loss function: 2.832, Average Loss: 3.616, avg. samples / sec: 28022.86
Iteration:   4460, Loss function: 3.106, Average Loss: 3.608, avg. samples / sec: 27981.53
Iteration:   4460, Loss function: 3.520, Average Loss: 3.617, avg. samples / sec: 27974.61
Iteration:   4460, Loss function: 3.524, Average Loss: 3.622, avg. samples / sec: 28025.25
Iteration:   4480, Loss function: 3.532, Average Loss: 3.601, avg. samples / sec: 28109.41
Iteration:   4480, Loss function: 3.465, Average Loss: 3.613, avg. samples / sec: 28108.59
Iteration:   4480, Loss function: 2.637, Average Loss: 3.615, avg. samples / sec: 28108.56
Iteration:   4480, Loss function: 2.887, Average Loss: 3.608, avg. samples / sec: 28020.22
Iteration:   4500, Loss function: 3.403, Average Loss: 3.608, avg. samples / sec: 27987.52
Iteration:   4500, Loss function: 3.288, Average Loss: 3.611, avg. samples / sec: 27982.90
Iteration:   4500, Loss function: 3.925, Average Loss: 3.596, avg. samples / sec: 27954.00
Iteration:   4500, Loss function: 3.037, Average Loss: 3.602, avg. samples / sec: 27991.30
:::MLL 1558578837.287 epoch_stop: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 819}}
:::MLL 1558578837.287 epoch_start: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 673}}
Iteration:   4520, Loss function: 3.540, Average Loss: 3.604, avg. samples / sec: 28016.78
Iteration:   4520, Loss function: 3.123, Average Loss: 3.604, avg. samples / sec: 27977.19
Iteration:   4520, Loss function: 2.924, Average Loss: 3.592, avg. samples / sec: 27987.70
Iteration:   4520, Loss function: 3.273, Average Loss: 3.599, avg. samples / sec: 27978.43
Iteration:   4540, Loss function: 3.313, Average Loss: 3.592, avg. samples / sec: 28074.65
Iteration:   4540, Loss function: 3.222, Average Loss: 3.588, avg. samples / sec: 28050.76
Iteration:   4540, Loss function: 3.377, Average Loss: 3.600, avg. samples / sec: 27970.23
Iteration:   4540, Loss function: 3.445, Average Loss: 3.597, avg. samples / sec: 27979.48
Iteration:   4560, Loss function: 3.051, Average Loss: 3.580, avg. samples / sec: 28103.81
Iteration:   4560, Loss function: 3.017, Average Loss: 3.589, avg. samples / sec: 28159.92
Iteration:   4560, Loss function: 2.640, Average Loss: 3.588, avg. samples / sec: 28057.88
Iteration:   4560, Loss function: 2.845, Average Loss: 3.593, avg. samples / sec: 28100.00
Iteration:   4580, Loss function: 3.360, Average Loss: 3.584, avg. samples / sec: 28024.50
Iteration:   4580, Loss function: 3.262, Average Loss: 3.587, avg. samples / sec: 28024.49
Iteration:   4580, Loss function: 3.697, Average Loss: 3.575, avg. samples / sec: 27910.77
Iteration:   4580, Loss function: 3.648, Average Loss: 3.583, avg. samples / sec: 27899.53
:::MLL 1558578841.452 epoch_stop: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 819}}
:::MLL 1558578841.453 epoch_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 673}}
:::MLL 1558578841.619 eval_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.14 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.14 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.14 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.13 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.53s)
DONE (t=2.83s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.23129
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.39454
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23837
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05882
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24053
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.38055
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22260
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32448
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.34100
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09908
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36828
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.54107
Current AP: 0.23129 AP goal: 0.23000
:::MLL 1558578846.163 eval_accuracy: {"value": 0.23128512534942414, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 389}}
:::MLL 1558578846.208 eval_stop: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 392}}
:::MLL 1558578846.221 block_stop: {"value": null, "metadata": {"first_epoch_num": 55, "file": "train.py", "lineno": 804}}
:::MLL 1558578847.130 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 849}}

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-05-23 02:34:16 AM
RESULT,SINGLE_STAGE_DETECTOR,,350,nvidia,2019-05-23 02:28:26 AM
ENDING TIMING RUN AT 2019-05-23 02:34:16 AM
RESULT,SINGLE_STAGE_DETECTOR,,350,nvidia,2019-05-23 02:28:26 AM
ENDING TIMING RUN AT 2019-05-23 02:34:17 AM
RESULT,SINGLE_STAGE_DETECTOR,,351,nvidia,2019-05-23 02:28:26 AM
ENDING TIMING RUN AT 2019-05-23 02:34:17 AM
RESULT,SINGLE_STAGE_DETECTOR,,351,nvidia,2019-05-23 02:28:26 AM
