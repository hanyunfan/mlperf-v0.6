Beginning trial 3 of 5
Gathering sys log on circe-n051
:::MLL 1558579213.670 submission_benchmark: {"value": "ssd", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1558579213.670 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known ssd keys.
:::MLL 1558579213.671 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1558579213.671 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1558579213.671 submission_platform: {"value": "4xNVIDIA DGX-2H", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1558579213.672 submission_entry: {"value": "{'hardware': 'NVIDIA DGX-2H', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': 'Ubuntu 18.04.1 LTS / NVIDIA DGX Server 4.0.4 4.0.5', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.4-2.0.7', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '4', 'cpu': '2x Intel(R) Xeon(R) Platinum 8174 CPU @ 3.10GHz', 'num_cores': '48', 'num_vcpus': '96', 'accelerator': 'Tesla V100-SXM3-32GB-H', 'num_accelerators': '16', 'sys_mem_size': '1510 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '2x 894.3G + 8x 3.5T', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27800 Family [ConnectX-5]', 'num_network_cards': '10', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1558579213.672 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1558579213.672 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
:::MLL 1558579216.475 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558579216.508 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558579216.512 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558579216.549 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node circe-n051
+ pids+=($!)
+ set +x
Launching on node circe-n052
+ pids+=($!)
+ set +x
Launching on node circe-n053
+ pids+=($!)
+ set +x
Launching on node circe-n054
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n051
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n052
+ pids+=($!)
+ set +x
+ srun --mem=0 -N 1 -n 1 -w circe-n051 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=0 --master_addr=10.0.1.51 --master_port=4728' -e SLURM_JOB_ID=89451 -e SLURM_NTASKS_PER_NODE=16 cont_89451 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n053
+ srun --mem=0 -N 1 -n 1 -w circe-n052 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=1 --master_addr=10.0.1.51 --master_port=4728' -e SLURM_JOB_ID=89451 -e SLURM_NTASKS_PER_NODE=16 cont_89451 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n054
+ srun --mem=0 -N 1 -n 1 -w circe-n053 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=2 --master_addr=10.0.1.51 --master_port=4728' -e SLURM_JOB_ID=89451 -e SLURM_NTASKS_PER_NODE=16 cont_89451 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w circe-n054 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=3 --master_addr=10.0.1.51 --master_port=4728' -e SLURM_JOB_ID=89451 -e SLURM_NTASKS_PER_NODE=16 cont_89451 ./run_and_time.sh
Run vars: id 89451 gpus 16 mparams  --nnodes=4 --node_rank=0 --master_addr=10.0.1.51 --master_port=4728
Run vars: id 89451 gpus 16 mparams  --nnodes=4 --node_rank=2 --master_addr=10.0.1.51 --master_port=4728
Run vars: id 89451 gpus 16 mparams  --nnodes=4 --node_rank=1 --master_addr=10.0.1.51 --master_port=4728
Run vars: id 89451 gpus 16 mparams  --nnodes=4 --node_rank=3 --master_addr=10.0.1.51 --master_port=4728
STARTING TIMING RUN AT 2019-05-23 02:40:16 AM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=0 --master_addr=10.0.1.51 --master_port=4728 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 02:40:16 AM
running benchmark
STARTING TIMING RUN AT 2019-05-23 02:40:16 AM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ NUMEPOCHS=80
+ TORCH_MODEL_ZOO=/data/torchvision
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=1 --master_addr=10.0.1.51 --master_port=4728 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=2 --master_addr=10.0.1.51 --master_port=4728 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 02:40:16 AM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=3 --master_addr=10.0.1.51 --master_port=4728 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558579227.054 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.054 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.054 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.054 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.054 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.054 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.055 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.055 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.056 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579227.056 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579227.056 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558579227.056 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558579227.056 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.056 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.057 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558579227.058 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558579227.101 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.101 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.101 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558579227.108 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579227.113 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.113 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.113 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.114 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.114 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.114 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.114 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.114 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579227.115 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.115 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558579227.115 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558579227.117 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558579227.172 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.174 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.174 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.174 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579227.174 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.174 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.174 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.175 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.175 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.175 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.175 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.175 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.175 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558579227.176 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579227.176 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579227.176 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558579227.314 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.314 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.315 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558579227.315 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.315 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.316 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.316 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.316 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558579227.317 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579227.318 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.318 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579227.318 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579227.318 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579227.318 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579227.318 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558579227.319 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
0 Using seed = 2758539061
1 Using seed = 2758539062
3 Using seed = 2758539064
4 Using seed = 2758539065
5 Using seed = 2758539066
2 Using seed = 2758539063
:::MLL 1558579259.289 max_samples: {"value": 1, "metadata": {"file": "utils.py", "lineno": 465}}
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
7 Using seed = 2758539068
8 Using seed = 2758539069
11 Using seed = 2758539072
9 Using seed = 2758539070
6 Using seed = 2758539067
22 Using seed = 2758539083
28 Using seed = 2758539089
31 Using seed = 2758539092
30 Using seed = 2758539091
26 Using seed = 2758539087
19 Using seed = 2758539080
27 Using seed = 2758539088
25 Using seed = 2758539086
17 Using seed = 2758539078
29 Using seed = 2758539090
24 Using seed = 2758539085
20 Using seed = 2758539081
23 Using seed = 2758539084
18 Using seed = 2758539079
16 Using seed = 2758539077
21 Using seed = 2758539082
38 Using seed = 2758539099
33 Using seed = 2758539094
42 Using seed = 2758539103
47 Using seed = 2758539108
41 Using seed = 2758539102
35 Using seed = 2758539096
32 Using seed = 2758539093
39 Using seed = 2758539100
45 Using seed = 2758539106
44 Using seed = 2758539105
34 Using seed = 2758539095
37 Using seed = 2758539098
43 Using seed = 2758539104
36 Using seed = 2758539097
46 Using seed = 2758539107
40 Using seed = 2758539101
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
:::MLL 1558579260.059 model_bn_span: {"value": 24, "metadata": {"file": "train.py", "lineno": 480}}
:::MLL 1558579260.060 global_batch_size: {"value": 1536, "metadata": {"file": "train.py", "lineno": 481}}
:::MLL 1558579260.069 opt_base_learning_rate: {"value": 0.14, "metadata": {"file": "train.py", "lineno": 511}}
:::MLL 1558579260.069 opt_weight_decay: {"value": 0.00017, "metadata": {"file": "train.py", "lineno": 513}}
:::MLL 1558579260.069 opt_learning_rate_warmup_steps: {"value": 850, "metadata": {"file": "train.py", "lineno": 516}}
:::MLL 1558579260.069 opt_learning_rate_warmup_factor: {"value": 0, "metadata": {"file": "train.py", "lineno": 518}}
58 Using seed = 2758539119
61 Using seed = 2758539122
62 Using seed = 2758539123
63 Using seed = 2758539124
59 Using seed = 2758539120
54 Using seed = 2758539115
49 Using seed = 2758539110
50 Using seed = 2758539111
48 Using seed = 2758539109
51 Using seed = 2758539112
52 Using seed = 2758539113
57 Using seed = 2758539118
56 Using seed = 2758539117
55 Using seed = 2758539116
53 Using seed = 2758539114
60 Using seed = 2758539121
15 Using seed = 2758539076
12 Using seed = 2758539073
13 Using seed = 2758539074
14 Using seed = 2758539075
10 Using seed = 2758539071
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
epoch nbatch loss
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
:::MLL 1558579271.299 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 604}}
:::MLL 1558579271.299 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 610}}
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.48s)
creating index...
time_check a: 1558579273.020586014
time_check a: 1558579273.019663572
time_check a: 1558579273.021150112
time_check a: 1558579273.028752327
time_check b: 1558579279.791619301
time_check b: 1558579279.807259560
time_check b: 1558579279.848049402
time_check b: 1558579279.910208225
:::MLL 1558579280.776 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 32.74606450292497, "file": "train.py", "lineno": 669}}
:::MLL 1558579280.776 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 673}}
Iteration:      0, Loss function: 23.192, Average Loss: 0.023, avg. samples / sec: 103.63
Iteration:      0, Loss function: 22.688, Average Loss: 0.023, avg. samples / sec: 82.97
Iteration:      0, Loss function: 23.034, Average Loss: 0.023, avg. samples / sec: 102.22
Iteration:      0, Loss function: 22.683, Average Loss: 0.023, avg. samples / sec: 103.62
Iteration:     20, Loss function: 20.919, Average Loss: 0.447, avg. samples / sec: 20999.96
Iteration:     20, Loss function: 20.393, Average Loss: 0.443, avg. samples / sec: 20544.43
Iteration:     20, Loss function: 20.549, Average Loss: 0.445, avg. samples / sec: 21214.70
Iteration:     20, Loss function: 20.469, Average Loss: 0.445, avg. samples / sec: 20779.01
Iteration:     40, Loss function: 16.915, Average Loss: 0.826, avg. samples / sec: 28139.25
Iteration:     40, Loss function: 17.272, Average Loss: 0.830, avg. samples / sec: 28123.27
Iteration:     40, Loss function: 16.806, Average Loss: 0.826, avg. samples / sec: 28166.45
Iteration:     40, Loss function: 16.656, Average Loss: 0.827, avg. samples / sec: 28191.56
Iteration:     60, Loss function: 12.370, Average Loss: 1.076, avg. samples / sec: 28255.87
Iteration:     60, Loss function: 12.814, Average Loss: 1.085, avg. samples / sec: 28259.10
Iteration:     60, Loss function: 11.493, Average Loss: 1.078, avg. samples / sec: 28255.68
Iteration:     60, Loss function: 11.492, Average Loss: 1.088, avg. samples / sec: 28193.53
:::MLL 1558579286.537 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 819}}
:::MLL 1558579286.537 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 673}}
Iteration:     80, Loss function: 10.503, Average Loss: 1.267, avg. samples / sec: 28271.24
Iteration:     80, Loss function: 11.076, Average Loss: 1.280, avg. samples / sec: 28347.53
Iteration:     80, Loss function: 10.612, Average Loss: 1.277, avg. samples / sec: 28267.34
Iteration:     80, Loss function: 10.071, Average Loss: 1.266, avg. samples / sec: 28230.68
Iteration:    100, Loss function: 9.029, Average Loss: 1.433, avg. samples / sec: 28560.21
Iteration:    100, Loss function: 9.206, Average Loss: 1.444, avg. samples / sec: 28548.09
Iteration:    100, Loss function: 9.015, Average Loss: 1.430, avg. samples / sec: 28589.89
Iteration:    100, Loss function: 9.320, Average Loss: 1.442, avg. samples / sec: 28502.46
Iteration:    120, Loss function: 9.096, Average Loss: 1.596, avg. samples / sec: 28418.21
Iteration:    120, Loss function: 9.135, Average Loss: 1.581, avg. samples / sec: 28415.21
Iteration:    120, Loss function: 9.192, Average Loss: 1.592, avg. samples / sec: 28464.09
Iteration:    120, Loss function: 9.208, Average Loss: 1.582, avg. samples / sec: 28395.15
Iteration:    140, Loss function: 8.882, Average Loss: 1.740, avg. samples / sec: 28481.43
Iteration:    140, Loss function: 8.847, Average Loss: 1.729, avg. samples / sec: 28440.99
Iteration:    140, Loss function: 8.392, Average Loss: 1.743, avg. samples / sec: 28411.63
Iteration:    140, Loss function: 8.337, Average Loss: 1.727, avg. samples / sec: 28413.22
:::MLL 1558579290.642 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 819}}
:::MLL 1558579290.642 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 673}}
Iteration:    160, Loss function: 8.239, Average Loss: 1.878, avg. samples / sec: 28367.27
Iteration:    160, Loss function: 8.088, Average Loss: 1.864, avg. samples / sec: 28444.45
Iteration:    160, Loss function: 8.571, Average Loss: 1.864, avg. samples / sec: 28395.22
Iteration:    160, Loss function: 9.149, Average Loss: 1.881, avg. samples / sec: 28396.31
Iteration:    180, Loss function: 8.315, Average Loss: 2.011, avg. samples / sec: 28495.97
Iteration:    180, Loss function: 8.278, Average Loss: 1.994, avg. samples / sec: 28455.74
Iteration:    180, Loss function: 8.430, Average Loss: 2.008, avg. samples / sec: 28455.19
Iteration:    180, Loss function: 8.539, Average Loss: 1.996, avg. samples / sec: 28459.41
Iteration:    200, Loss function: 7.719, Average Loss: 2.132, avg. samples / sec: 28399.68
Iteration:    200, Loss function: 8.627, Average Loss: 2.118, avg. samples / sec: 28393.21
Iteration:    200, Loss function: 8.444, Average Loss: 2.120, avg. samples / sec: 28406.90
Iteration:    200, Loss function: 8.341, Average Loss: 2.136, avg. samples / sec: 28348.15
Iteration:    220, Loss function: 8.219, Average Loss: 2.253, avg. samples / sec: 28466.78
Iteration:    220, Loss function: 7.610, Average Loss: 2.235, avg. samples / sec: 28417.55
Iteration:    220, Loss function: 8.713, Average Loss: 2.249, avg. samples / sec: 28406.63
Iteration:    220, Loss function: 8.176, Average Loss: 2.236, avg. samples / sec: 28387.13
:::MLL 1558579294.807 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 819}}
:::MLL 1558579294.807 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 673}}
Iteration:    240, Loss function: 7.936, Average Loss: 2.361, avg. samples / sec: 28370.97
Iteration:    240, Loss function: 7.769, Average Loss: 2.365, avg. samples / sec: 28356.86
Iteration:    240, Loss function: 7.879, Average Loss: 2.349, avg. samples / sec: 28382.09
Iteration:    240, Loss function: 7.612, Average Loss: 2.347, avg. samples / sec: 28352.21
Iteration:    260, Loss function: 7.437, Average Loss: 2.470, avg. samples / sec: 28270.50
Iteration:    260, Loss function: 7.825, Average Loss: 2.454, avg. samples / sec: 28276.27
Iteration:    260, Loss function: 7.890, Average Loss: 2.457, avg. samples / sec: 28271.78
Iteration:    260, Loss function: 7.820, Average Loss: 2.469, avg. samples / sec: 28249.89
Iteration:    280, Loss function: 7.822, Average Loss: 2.556, avg. samples / sec: 28318.25
Iteration:    280, Loss function: 8.619, Average Loss: 2.561, avg. samples / sec: 28319.83
Iteration:    280, Loss function: 7.938, Average Loss: 2.575, avg. samples / sec: 28311.17
Iteration:    280, Loss function: 8.061, Average Loss: 2.570, avg. samples / sec: 28320.45
Iteration:    300, Loss function: 7.305, Average Loss: 2.670, avg. samples / sec: 28403.36
Iteration:    300, Loss function: 7.614, Average Loss: 2.657, avg. samples / sec: 28388.19
Iteration:    300, Loss function: 6.905, Average Loss: 2.675, avg. samples / sec: 28389.26
Iteration:    300, Loss function: 6.531, Average Loss: 2.661, avg. samples / sec: 28352.61
:::MLL 1558579298.927 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 819}}
:::MLL 1558579298.927 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 673}}
Iteration:    320, Loss function: 6.794, Average Loss: 2.765, avg. samples / sec: 28335.78
Iteration:    320, Loss function: 6.766, Average Loss: 2.748, avg. samples / sec: 28330.92
Iteration:    320, Loss function: 7.412, Average Loss: 2.760, avg. samples / sec: 28323.91
Iteration:    320, Loss function: 7.217, Average Loss: 2.751, avg. samples / sec: 28363.40
Iteration:    340, Loss function: 6.532, Average Loss: 2.848, avg. samples / sec: 28325.31
Iteration:    340, Loss function: 7.270, Average Loss: 2.837, avg. samples / sec: 28328.40
Iteration:    340, Loss function: 6.841, Average Loss: 2.833, avg. samples / sec: 28322.59
Iteration:    340, Loss function: 7.289, Average Loss: 2.842, avg. samples / sec: 28315.45
Iteration:    360, Loss function: 6.645, Average Loss: 2.931, avg. samples / sec: 28499.77
Iteration:    360, Loss function: 6.865, Average Loss: 2.920, avg. samples / sec: 28480.15
Iteration:    360, Loss function: 6.830, Average Loss: 2.914, avg. samples / sec: 28453.39
Iteration:    360, Loss function: 7.572, Average Loss: 2.922, avg. samples / sec: 28462.32
Iteration:    380, Loss function: 7.131, Average Loss: 2.999, avg. samples / sec: 28548.03
Iteration:    380, Loss function: 7.091, Average Loss: 2.997, avg. samples / sec: 28513.60
Iteration:    380, Loss function: 6.642, Average Loss: 3.005, avg. samples / sec: 28481.78
Iteration:    380, Loss function: 7.082, Average Loss: 2.990, avg. samples / sec: 28524.70
:::MLL 1558579303.037 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 819}}
:::MLL 1558579303.037 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 673}}
Iteration:    400, Loss function: 6.549, Average Loss: 3.070, avg. samples / sec: 28300.15
Iteration:    400, Loss function: 7.295, Average Loss: 3.079, avg. samples / sec: 28280.45
Iteration:    400, Loss function: 7.435, Average Loss: 3.077, avg. samples / sec: 28271.03
Iteration:    400, Loss function: 6.842, Average Loss: 3.085, avg. samples / sec: 28220.21
Iteration:    420, Loss function: 6.359, Average Loss: 3.143, avg. samples / sec: 28399.31
Iteration:    420, Loss function: 5.934, Average Loss: 3.148, avg. samples / sec: 28399.59
Iteration:    420, Loss function: 6.703, Average Loss: 3.155, avg. samples / sec: 28468.35
Iteration:    420, Loss function: 7.130, Average Loss: 3.148, avg. samples / sec: 28395.99
Iteration:    440, Loss function: 6.397, Average Loss: 3.217, avg. samples / sec: 28300.92
Iteration:    440, Loss function: 6.490, Average Loss: 3.223, avg. samples / sec: 28284.56
Iteration:    440, Loss function: 6.658, Average Loss: 3.212, avg. samples / sec: 28275.37
Iteration:    440, Loss function: 6.553, Average Loss: 3.216, avg. samples / sec: 28273.39
:::MLL 1558579307.205 epoch_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 819}}
:::MLL 1558579307.205 epoch_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 673}}
Iteration:    460, Loss function: 6.148, Average Loss: 3.291, avg. samples / sec: 28456.47
Iteration:    460, Loss function: 6.596, Average Loss: 3.280, avg. samples / sec: 28453.14
Iteration:    460, Loss function: 6.663, Average Loss: 3.276, avg. samples / sec: 28431.88
Iteration:    460, Loss function: 6.239, Average Loss: 3.283, avg. samples / sec: 28412.95
Iteration:    480, Loss function: 6.090, Average Loss: 3.351, avg. samples / sec: 28373.77
Iteration:    480, Loss function: 5.756, Average Loss: 3.336, avg. samples / sec: 28402.88
Iteration:    480, Loss function: 6.254, Average Loss: 3.343, avg. samples / sec: 28411.05
Iteration:    480, Loss function: 6.048, Average Loss: 3.342, avg. samples / sec: 28378.50
Iteration:    500, Loss function: 6.475, Average Loss: 3.403, avg. samples / sec: 28346.34
Iteration:    500, Loss function: 7.099, Average Loss: 3.420, avg. samples / sec: 28337.61
Iteration:    500, Loss function: 7.262, Average Loss: 3.409, avg. samples / sec: 28341.82
Iteration:    500, Loss function: 6.497, Average Loss: 3.410, avg. samples / sec: 28346.47
Iteration:    520, Loss function: 5.735, Average Loss: 3.477, avg. samples / sec: 28329.93
Iteration:    520, Loss function: 6.821, Average Loss: 3.462, avg. samples / sec: 28320.33
Iteration:    520, Loss function: 5.847, Average Loss: 3.466, avg. samples / sec: 28323.10
Iteration:    520, Loss function: 6.564, Average Loss: 3.466, avg. samples / sec: 28292.16
:::MLL 1558579311.327 epoch_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 819}}
:::MLL 1558579311.327 epoch_start: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 673}}
Iteration:    540, Loss function: 6.163, Average Loss: 3.530, avg. samples / sec: 28225.15
Iteration:    540, Loss function: 5.612, Average Loss: 3.518, avg. samples / sec: 28257.97
Iteration:    540, Loss function: 5.438, Average Loss: 3.513, avg. samples / sec: 28186.13
Iteration:    540, Loss function: 6.082, Average Loss: 3.519, avg. samples / sec: 28171.64
Iteration:    560, Loss function: 5.486, Average Loss: 3.568, avg. samples / sec: 28336.41
Iteration:    560, Loss function: 5.844, Average Loss: 3.568, avg. samples / sec: 28392.91
Iteration:    560, Loss function: 5.925, Average Loss: 3.582, avg. samples / sec: 28326.86
Iteration:    560, Loss function: 5.481, Average Loss: 3.563, avg. samples / sec: 28341.79
Iteration:    580, Loss function: 5.813, Average Loss: 3.629, avg. samples / sec: 28406.04
Iteration:    580, Loss function: 5.742, Average Loss: 3.615, avg. samples / sec: 28401.30
Iteration:    580, Loss function: 6.068, Average Loss: 3.609, avg. samples / sec: 28429.73
Iteration:    580, Loss function: 5.952, Average Loss: 3.612, avg. samples / sec: 28375.94
Iteration:    600, Loss function: 6.033, Average Loss: 3.652, avg. samples / sec: 28433.05
Iteration:    600, Loss function: 5.681, Average Loss: 3.655, avg. samples / sec: 28419.60
Iteration:    600, Loss function: 5.758, Average Loss: 3.661, avg. samples / sec: 28384.06
Iteration:    600, Loss function: 5.164, Average Loss: 3.673, avg. samples / sec: 28359.47
:::MLL 1558579315.441 epoch_stop: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 819}}
:::MLL 1558579315.441 epoch_start: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 673}}
Iteration:    620, Loss function: 5.765, Average Loss: 3.699, avg. samples / sec: 28333.35
Iteration:    620, Loss function: 6.055, Average Loss: 3.700, avg. samples / sec: 28360.98
Iteration:    620, Loss function: 5.478, Average Loss: 3.708, avg. samples / sec: 28373.70
Iteration:    620, Loss function: 6.077, Average Loss: 3.720, avg. samples / sec: 28378.17
Iteration:    640, Loss function: 5.632, Average Loss: 3.738, avg. samples / sec: 28540.99
Iteration:    640, Loss function: 5.486, Average Loss: 3.759, avg. samples / sec: 28548.93
Iteration:    640, Loss function: 6.137, Average Loss: 3.749, avg. samples / sec: 28529.15
Iteration:    640, Loss function: 6.206, Average Loss: 3.743, avg. samples / sec: 28515.21
Iteration:    660, Loss function: 5.954, Average Loss: 3.779, avg. samples / sec: 28435.85
Iteration:    660, Loss function: 5.477, Average Loss: 3.796, avg. samples / sec: 28412.66
Iteration:    660, Loss function: 5.654, Average Loss: 3.775, avg. samples / sec: 28387.73
Iteration:    660, Loss function: 5.430, Average Loss: 3.785, avg. samples / sec: 28397.85
Iteration:    680, Loss function: 7.310, Average Loss: 3.838, avg. samples / sec: 28325.10
Iteration:    680, Loss function: 7.467, Average Loss: 3.852, avg. samples / sec: 28318.94
Iteration:    680, Loss function: 6.839, Average Loss: 3.836, avg. samples / sec: 28290.95
Iteration:    680, Loss function: 7.298, Average Loss: 3.846, avg. samples / sec: 28296.12
:::MLL 1558579319.607 epoch_stop: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 819}}
:::MLL 1558579319.607 epoch_start: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 673}}
Iteration:    700, Loss function: 6.242, Average Loss: 3.898, avg. samples / sec: 28439.22
Iteration:    700, Loss function: 6.847, Average Loss: 3.889, avg. samples / sec: 28437.72
Iteration:    700, Loss function: 6.011, Average Loss: 3.891, avg. samples / sec: 28366.51
Iteration:    700, Loss function: 6.034, Average Loss: 3.909, avg. samples / sec: 28372.81
Iteration:    720, Loss function: 5.708, Average Loss: 3.946, avg. samples / sec: 28459.29
Iteration:    720, Loss function: 5.453, Average Loss: 3.927, avg. samples / sec: 28442.09
Iteration:    720, Loss function: 5.969, Average Loss: 3.934, avg. samples / sec: 28420.08
Iteration:    720, Loss function: 5.397, Average Loss: 3.928, avg. samples / sec: 28385.49
Iteration:    740, Loss function: 5.828, Average Loss: 3.980, avg. samples / sec: 28413.97
Iteration:    740, Loss function: 5.482, Average Loss: 3.964, avg. samples / sec: 28463.69
Iteration:    740, Loss function: 6.285, Average Loss: 3.961, avg. samples / sec: 28415.31
Iteration:    740, Loss function: 5.563, Average Loss: 3.967, avg. samples / sec: 28371.19
Iteration:    760, Loss function: 5.593, Average Loss: 4.012, avg. samples / sec: 28336.17
Iteration:    760, Loss function: 5.791, Average Loss: 3.996, avg. samples / sec: 28333.87
Iteration:    760, Loss function: 6.446, Average Loss: 4.002, avg. samples / sec: 28375.31
Iteration:    760, Loss function: 5.313, Average Loss: 3.993, avg. samples / sec: 28280.95
:::MLL 1558579323.721 epoch_stop: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 819}}
:::MLL 1558579323.722 epoch_start: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 673}}
Iteration:    780, Loss function: 5.377, Average Loss: 4.040, avg. samples / sec: 28297.61
Iteration:    780, Loss function: 5.637, Average Loss: 4.022, avg. samples / sec: 28343.02
Iteration:    780, Loss function: 5.153, Average Loss: 4.023, avg. samples / sec: 28287.13
Iteration:    780, Loss function: 5.292, Average Loss: 4.030, avg. samples / sec: 28296.57
Iteration:    800, Loss function: 5.558, Average Loss: 4.057, avg. samples / sec: 28456.94
Iteration:    800, Loss function: 5.647, Average Loss: 4.069, avg. samples / sec: 28435.29
Iteration:    800, Loss function: 5.266, Average Loss: 4.051, avg. samples / sec: 28445.63
Iteration:    800, Loss function: 4.846, Average Loss: 4.048, avg. samples / sec: 28441.11
Iteration:    820, Loss function: 5.886, Average Loss: 4.079, avg. samples / sec: 28416.17
Iteration:    820, Loss function: 5.306, Average Loss: 4.086, avg. samples / sec: 28399.23
Iteration:    820, Loss function: 5.557, Average Loss: 4.076, avg. samples / sec: 28409.48
Iteration:    820, Loss function: 5.627, Average Loss: 4.097, avg. samples / sec: 28400.30
:::MLL 1558579327.835 epoch_stop: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 819}}
:::MLL 1558579327.836 epoch_start: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 673}}
Iteration:    840, Loss function: 5.688, Average Loss: 4.107, avg. samples / sec: 28235.23
Iteration:    840, Loss function: 5.884, Average Loss: 4.104, avg. samples / sec: 28235.46
Iteration:    840, Loss function: 5.072, Average Loss: 4.112, avg. samples / sec: 28222.78
Iteration:    840, Loss function: 4.956, Average Loss: 4.122, avg. samples / sec: 28210.31
Iteration:    860, Loss function: 5.709, Average Loss: 4.130, avg. samples / sec: 28393.15
Iteration:    860, Loss function: 5.546, Average Loss: 4.128, avg. samples / sec: 28395.47
Iteration:    860, Loss function: 5.589, Average Loss: 4.137, avg. samples / sec: 28393.32
Iteration:    860, Loss function: 5.485, Average Loss: 4.143, avg. samples / sec: 28384.24
Iteration:    880, Loss function: 5.153, Average Loss: 4.168, avg. samples / sec: 28447.59
Iteration:    880, Loss function: 4.886, Average Loss: 4.154, avg. samples / sec: 28406.76
Iteration:    880, Loss function: 5.569, Average Loss: 4.153, avg. samples / sec: 28358.50
Iteration:    880, Loss function: 5.130, Average Loss: 4.160, avg. samples / sec: 28375.13
Iteration:    900, Loss function: 4.946, Average Loss: 4.175, avg. samples / sec: 28389.46
Iteration:    900, Loss function: 5.227, Average Loss: 4.175, avg. samples / sec: 28342.30
Iteration:    900, Loss function: 5.463, Average Loss: 4.179, avg. samples / sec: 28386.34
Iteration:    900, Loss function: 5.130, Average Loss: 4.189, avg. samples / sec: 28336.09
:::MLL 1558579332.005 epoch_stop: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 819}}
:::MLL 1558579332.006 epoch_start: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 673}}
Iteration:    920, Loss function: 4.963, Average Loss: 4.208, avg. samples / sec: 28371.57
Iteration:    920, Loss function: 5.182, Average Loss: 4.192, avg. samples / sec: 28361.78
Iteration:    920, Loss function: 4.997, Average Loss: 4.192, avg. samples / sec: 28328.55
Iteration:    920, Loss function: 4.656, Average Loss: 4.195, avg. samples / sec: 28322.42
Iteration:    940, Loss function: 5.028, Average Loss: 4.213, avg. samples / sec: 28363.94
Iteration:    940, Loss function: 5.352, Average Loss: 4.225, avg. samples / sec: 28299.94
Iteration:    940, Loss function: 4.977, Average Loss: 4.208, avg. samples / sec: 28334.22
Iteration:    940, Loss function: 5.282, Average Loss: 4.210, avg. samples / sec: 28294.25
Iteration:    960, Loss function: 5.062, Average Loss: 4.225, avg. samples / sec: 28379.32
Iteration:    960, Loss function: 5.394, Average Loss: 4.228, avg. samples / sec: 28377.73
Iteration:    960, Loss function: 4.454, Average Loss: 4.232, avg. samples / sec: 28319.39
Iteration:    960, Loss function: 5.393, Average Loss: 4.243, avg. samples / sec: 28325.13
Iteration:    980, Loss function: 4.812, Average Loss: 4.261, avg. samples / sec: 28453.66
Iteration:    980, Loss function: 4.651, Average Loss: 4.250, avg. samples / sec: 28426.93
Iteration:    980, Loss function: 5.786, Average Loss: 4.244, avg. samples / sec: 28406.16
Iteration:    980, Loss function: 4.442, Average Loss: 4.240, avg. samples / sec: 28390.30
:::MLL 1558579336.122 epoch_stop: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 819}}
:::MLL 1558579336.123 epoch_start: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 673}}
Iteration:   1000, Loss function: 4.917, Average Loss: 4.253, avg. samples / sec: 28327.75
Iteration:   1000, Loss function: 5.715, Average Loss: 4.278, avg. samples / sec: 28295.34
Iteration:   1000, Loss function: 4.901, Average Loss: 4.262, avg. samples / sec: 28287.18
Iteration:   1000, Loss function: 4.941, Average Loss: 4.259, avg. samples / sec: 28249.38
Iteration:   1020, Loss function: 4.881, Average Loss: 4.276, avg. samples / sec: 28412.62
Iteration:   1020, Loss function: 4.533, Average Loss: 4.274, avg. samples / sec: 28455.13
Iteration:   1020, Loss function: 4.604, Average Loss: 4.291, avg. samples / sec: 28398.28
Iteration:   1020, Loss function: 5.293, Average Loss: 4.267, avg. samples / sec: 28378.02
Iteration:   1040, Loss function: 4.385, Average Loss: 4.301, avg. samples / sec: 28453.21
Iteration:   1040, Loss function: 5.043, Average Loss: 4.286, avg. samples / sec: 28434.06
Iteration:   1040, Loss function: 4.401, Average Loss: 4.288, avg. samples / sec: 28412.61
Iteration:   1040, Loss function: 5.170, Average Loss: 4.280, avg. samples / sec: 28410.94
Iteration:   1060, Loss function: 4.839, Average Loss: 4.312, avg. samples / sec: 28285.05
Iteration:   1060, Loss function: 4.791, Average Loss: 4.299, avg. samples / sec: 28291.86
Iteration:   1060, Loss function: 5.155, Average Loss: 4.298, avg. samples / sec: 28301.39
Iteration:   1060, Loss function: 4.269, Average Loss: 4.291, avg. samples / sec: 28277.26
:::MLL 1558579340.240 epoch_stop: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 819}}
:::MLL 1558579340.241 epoch_start: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 673}}
Iteration:   1080, Loss function: 4.676, Average Loss: 4.323, avg. samples / sec: 28346.47
Iteration:   1080, Loss function: 4.438, Average Loss: 4.298, avg. samples / sec: 28395.28
Iteration:   1080, Loss function: 5.549, Average Loss: 4.309, avg. samples / sec: 28354.23
Iteration:   1080, Loss function: 4.485, Average Loss: 4.311, avg. samples / sec: 28345.01
Iteration:   1100, Loss function: 5.406, Average Loss: 4.335, avg. samples / sec: 28370.86
Iteration:   1100, Loss function: 4.728, Average Loss: 4.305, avg. samples / sec: 28379.27
Iteration:   1100, Loss function: 4.257, Average Loss: 4.319, avg. samples / sec: 28375.84
Iteration:   1100, Loss function: 5.528, Average Loss: 4.320, avg. samples / sec: 28372.37
Iteration:   1120, Loss function: 4.707, Average Loss: 4.316, avg. samples / sec: 28315.50
Iteration:   1120, Loss function: 5.066, Average Loss: 4.331, avg. samples / sec: 28321.42
Iteration:   1120, Loss function: 4.723, Average Loss: 4.345, avg. samples / sec: 28302.52
Iteration:   1120, Loss function: 4.364, Average Loss: 4.329, avg. samples / sec: 28291.70
Iteration:   1140, Loss function: 4.765, Average Loss: 4.343, avg. samples / sec: 28329.88
Iteration:   1140, Loss function: 5.239, Average Loss: 4.354, avg. samples / sec: 28339.26
Iteration:   1140, Loss function: 4.738, Average Loss: 4.326, avg. samples / sec: 28321.45
Iteration:   1140, Loss function: 5.018, Average Loss: 4.342, avg. samples / sec: 28352.06
:::MLL 1558579344.411 epoch_stop: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 819}}
:::MLL 1558579344.411 epoch_start: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 673}}
Iteration:   1160, Loss function: 4.354, Average Loss: 4.349, avg. samples / sec: 28404.70
Iteration:   1160, Loss function: 4.893, Average Loss: 4.336, avg. samples / sec: 28399.88
Iteration:   1160, Loss function: 5.177, Average Loss: 4.365, avg. samples / sec: 28388.37
Iteration:   1160, Loss function: 4.880, Average Loss: 4.353, avg. samples / sec: 28384.51
Iteration:   1180, Loss function: 4.325, Average Loss: 4.372, avg. samples / sec: 28425.74
Iteration:   1180, Loss function: 4.519, Average Loss: 4.341, avg. samples / sec: 28424.59
Iteration:   1180, Loss function: 5.200, Average Loss: 4.361, avg. samples / sec: 28427.28
Iteration:   1180, Loss function: 4.116, Average Loss: 4.354, avg. samples / sec: 28364.80
Iteration:   1200, Loss function: 5.328, Average Loss: 4.350, avg. samples / sec: 28345.67
Iteration:   1200, Loss function: 4.303, Average Loss: 4.370, avg. samples / sec: 28322.44
Iteration:   1200, Loss function: 4.639, Average Loss: 4.358, avg. samples / sec: 28358.45
Iteration:   1200, Loss function: 4.481, Average Loss: 4.380, avg. samples / sec: 28302.96
Iteration:   1220, Loss function: 4.414, Average Loss: 4.378, avg. samples / sec: 28414.46
Iteration:   1220, Loss function: 5.130, Average Loss: 4.355, avg. samples / sec: 28356.12
Iteration:   1220, Loss function: 4.986, Average Loss: 4.392, avg. samples / sec: 28398.82
Iteration:   1220, Loss function: 4.407, Average Loss: 4.366, avg. samples / sec: 28391.38
:::MLL 1558579348.525 epoch_stop: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 819}}
:::MLL 1558579348.525 epoch_start: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 673}}
Iteration:   1240, Loss function: 4.751, Average Loss: 4.395, avg. samples / sec: 28349.59
Iteration:   1240, Loss function: 4.768, Average Loss: 4.371, avg. samples / sec: 28347.43
Iteration:   1240, Loss function: 4.512, Average Loss: 4.385, avg. samples / sec: 28307.21
Iteration:   1240, Loss function: 4.966, Average Loss: 4.362, avg. samples / sec: 28283.63
Iteration:   1260, Loss function: 4.655, Average Loss: 4.365, avg. samples / sec: 28360.08
Iteration:   1260, Loss function: 4.335, Average Loss: 4.400, avg. samples / sec: 28293.67
Iteration:   1260, Loss function: 5.157, Average Loss: 4.375, avg. samples / sec: 28290.43
Iteration:   1260, Loss function: 5.051, Average Loss: 4.390, avg. samples / sec: 28268.14
Iteration:   1280, Loss function: 4.100, Average Loss: 4.404, avg. samples / sec: 28445.58
Iteration:   1280, Loss function: 4.783, Average Loss: 4.394, avg. samples / sec: 28468.68
Iteration:   1280, Loss function: 4.876, Average Loss: 4.382, avg. samples / sec: 28417.70
Iteration:   1280, Loss function: 4.759, Average Loss: 4.369, avg. samples / sec: 28389.13
:::MLL 1558579352.639 epoch_stop: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 819}}
:::MLL 1558579352.640 epoch_start: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 673}}
Iteration:   1300, Loss function: 4.374, Average Loss: 4.400, avg. samples / sec: 28398.33
Iteration:   1300, Loss function: 5.257, Average Loss: 4.374, avg. samples / sec: 28431.91
Iteration:   1300, Loss function: 4.675, Average Loss: 4.413, avg. samples / sec: 28372.41
Iteration:   1300, Loss function: 4.135, Average Loss: 4.388, avg. samples / sec: 28371.87
Iteration:   1320, Loss function: 4.535, Average Loss: 4.391, avg. samples / sec: 28425.09
Iteration:   1320, Loss function: 5.011, Average Loss: 4.417, avg. samples / sec: 28381.63
Iteration:   1320, Loss function: 4.962, Average Loss: 4.380, avg. samples / sec: 28371.85
Iteration:   1320, Loss function: 4.076, Average Loss: 4.405, avg. samples / sec: 28328.16
Iteration:   1340, Loss function: 4.923, Average Loss: 4.382, avg. samples / sec: 28427.48
Iteration:   1340, Loss function: 4.739, Average Loss: 4.395, avg. samples / sec: 28422.06
Iteration:   1340, Loss function: 4.608, Average Loss: 4.423, avg. samples / sec: 28422.92
Iteration:   1340, Loss function: 4.447, Average Loss: 4.408, avg. samples / sec: 28459.29
Iteration:   1360, Loss function: 4.380, Average Loss: 4.411, avg. samples / sec: 28375.93
Iteration:   1360, Loss function: 4.419, Average Loss: 4.401, avg. samples / sec: 28369.84
Iteration:   1360, Loss function: 4.666, Average Loss: 4.428, avg. samples / sec: 28367.11
Iteration:   1360, Loss function: 4.564, Average Loss: 4.384, avg. samples / sec: 28367.81
:::MLL 1558579356.807 epoch_stop: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 819}}
:::MLL 1558579356.807 epoch_start: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 673}}
Iteration:   1380, Loss function: 5.205, Average Loss: 4.386, avg. samples / sec: 28419.90
Iteration:   1380, Loss function: 4.763, Average Loss: 4.406, avg. samples / sec: 28420.76
Iteration:   1380, Loss function: 4.539, Average Loss: 4.415, avg. samples / sec: 28388.95
Iteration:   1380, Loss function: 4.353, Average Loss: 4.431, avg. samples / sec: 28384.06
Iteration:   1400, Loss function: 4.225, Average Loss: 4.417, avg. samples / sec: 28448.85
Iteration:   1400, Loss function: 4.624, Average Loss: 4.406, avg. samples / sec: 28410.97
Iteration:   1400, Loss function: 4.445, Average Loss: 4.433, avg. samples / sec: 28453.42
Iteration:   1400, Loss function: 4.903, Average Loss: 4.389, avg. samples / sec: 28384.00
Iteration:   1420, Loss function: 3.461, Average Loss: 4.432, avg. samples / sec: 28364.80
Iteration:   1420, Loss function: 4.507, Average Loss: 4.418, avg. samples / sec: 28359.27
Iteration:   1420, Loss function: 3.994, Average Loss: 4.390, avg. samples / sec: 28384.39
Iteration:   1420, Loss function: 4.431, Average Loss: 4.406, avg. samples / sec: 28308.76
Iteration:   1440, Loss function: 4.601, Average Loss: 4.432, avg. samples / sec: 28321.11
Iteration:   1440, Loss function: 4.649, Average Loss: 4.394, avg. samples / sec: 28331.34
Iteration:   1440, Loss function: 4.519, Average Loss: 4.410, avg. samples / sec: 28372.31
Iteration:   1440, Loss function: 4.166, Average Loss: 4.420, avg. samples / sec: 28269.82
:::MLL 1558579360.921 epoch_stop: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 819}}
:::MLL 1558579360.922 epoch_start: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 673}}
Iteration:   1460, Loss function: 4.707, Average Loss: 4.433, avg. samples / sec: 28331.98
Iteration:   1460, Loss function: 4.258, Average Loss: 4.393, avg. samples / sec: 28322.15
Iteration:   1460, Loss function: 4.456, Average Loss: 4.414, avg. samples / sec: 28322.89
Iteration:   1460, Loss function: 4.544, Average Loss: 4.420, avg. samples / sec: 28352.21
Iteration:   1480, Loss function: 4.309, Average Loss: 4.418, avg. samples / sec: 28487.62
Iteration:   1480, Loss function: 4.556, Average Loss: 4.432, avg. samples / sec: 28442.92
Iteration:   1480, Loss function: 4.972, Average Loss: 4.394, avg. samples / sec: 28457.43
Iteration:   1480, Loss function: 4.706, Average Loss: 4.416, avg. samples / sec: 28400.76
Iteration:   1500, Loss function: 4.489, Average Loss: 4.433, avg. samples / sec: 28422.26
Iteration:   1500, Loss function: 4.666, Average Loss: 4.419, avg. samples / sec: 28405.25
Iteration:   1500, Loss function: 4.692, Average Loss: 4.419, avg. samples / sec: 28464.11
Iteration:   1500, Loss function: 4.835, Average Loss: 4.394, avg. samples / sec: 28374.77
Iteration:   1520, Loss function: 4.482, Average Loss: 4.420, avg. samples / sec: 28282.29
Iteration:   1520, Loss function: 4.534, Average Loss: 4.434, avg. samples / sec: 28280.79
Iteration:   1520, Loss function: 5.205, Average Loss: 4.419, avg. samples / sec: 28296.89
Iteration:   1520, Loss function: 4.830, Average Loss: 4.396, avg. samples / sec: 28315.51
:::MLL 1558579365.039 epoch_stop: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 819}}
:::MLL 1558579365.040 epoch_start: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 673}}
Iteration:   1540, Loss function: 4.201, Average Loss: 4.396, avg. samples / sec: 28276.90
Iteration:   1540, Loss function: 4.186, Average Loss: 4.435, avg. samples / sec: 28263.80
Iteration:   1540, Loss function: 5.269, Average Loss: 4.423, avg. samples / sec: 28256.86
Iteration:   1540, Loss function: 4.597, Average Loss: 4.419, avg. samples / sec: 28211.55
Iteration:   1560, Loss function: 5.449, Average Loss: 4.429, avg. samples / sec: 28334.74
Iteration:   1560, Loss function: 4.814, Average Loss: 4.435, avg. samples / sec: 28325.48
Iteration:   1560, Loss function: 4.604, Average Loss: 4.422, avg. samples / sec: 28372.63
Iteration:   1560, Loss function: 4.804, Average Loss: 4.394, avg. samples / sec: 28311.81
Iteration:   1580, Loss function: 4.710, Average Loss: 4.393, avg. samples / sec: 28317.03
Iteration:   1580, Loss function: 4.188, Average Loss: 4.432, avg. samples / sec: 28308.18
Iteration:   1580, Loss function: 4.642, Average Loss: 4.436, avg. samples / sec: 28299.32
Iteration:   1580, Loss function: 4.696, Average Loss: 4.421, avg. samples / sec: 28300.11
Iteration:   1600, Loss function: 4.424, Average Loss: 4.434, avg. samples / sec: 28402.51
Iteration:   1600, Loss function: 4.917, Average Loss: 4.424, avg. samples / sec: 28402.01
Iteration:   1600, Loss function: 4.619, Average Loss: 4.432, avg. samples / sec: 28388.68
Iteration:   1600, Loss function: 4.678, Average Loss: 4.391, avg. samples / sec: 28362.95
:::MLL 1558579369.213 epoch_stop: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 819}}
:::MLL 1558579369.214 epoch_start: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 673}}
Iteration:   1620, Loss function: 4.217, Average Loss: 4.429, avg. samples / sec: 28277.00
Iteration:   1620, Loss function: 4.186, Average Loss: 4.393, avg. samples / sec: 28282.38
Iteration:   1620, Loss function: 4.144, Average Loss: 4.417, avg. samples / sec: 28254.20
Iteration:   1620, Loss function: 3.726, Average Loss: 4.430, avg. samples / sec: 28225.22
Iteration:   1640, Loss function: 4.300, Average Loss: 4.392, avg. samples / sec: 28226.29
Iteration:   1640, Loss function: 4.175, Average Loss: 4.428, avg. samples / sec: 28242.72
Iteration:   1640, Loss function: 4.224, Average Loss: 4.416, avg. samples / sec: 28221.62
Iteration:   1640, Loss function: 4.581, Average Loss: 4.428, avg. samples / sec: 28193.89
Iteration:   1660, Loss function: 4.434, Average Loss: 4.425, avg. samples / sec: 28429.38
Iteration:   1660, Loss function: 4.702, Average Loss: 4.392, avg. samples / sec: 28413.95
Iteration:   1660, Loss function: 4.408, Average Loss: 4.416, avg. samples / sec: 28414.97
Iteration:   1660, Loss function: 4.782, Average Loss: 4.428, avg. samples / sec: 28376.02
:::MLL 1558579373.341 epoch_stop: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 819}}
:::MLL 1558579373.341 epoch_start: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 673}}
Iteration:   1680, Loss function: 4.091, Average Loss: 4.413, avg. samples / sec: 28226.81
Iteration:   1680, Loss function: 4.295, Average Loss: 4.424, avg. samples / sec: 28216.51
Iteration:   1680, Loss function: 4.535, Average Loss: 4.394, avg. samples / sec: 28221.81
Iteration:   1680, Loss function: 5.024, Average Loss: 4.430, avg. samples / sec: 28256.13
Iteration:   1700, Loss function: 4.388, Average Loss: 4.428, avg. samples / sec: 28435.68
Iteration:   1700, Loss function: 3.708, Average Loss: 4.392, avg. samples / sec: 28422.17
Iteration:   1700, Loss function: 4.155, Average Loss: 4.412, avg. samples / sec: 28401.82
Iteration:   1700, Loss function: 4.330, Average Loss: 4.425, avg. samples / sec: 28363.26
Iteration:   1720, Loss function: 3.934, Average Loss: 4.424, avg. samples / sec: 28493.42
Iteration:   1720, Loss function: 4.647, Average Loss: 4.410, avg. samples / sec: 28405.41
Iteration:   1720, Loss function: 4.201, Average Loss: 4.428, avg. samples / sec: 28373.67
Iteration:   1720, Loss function: 4.507, Average Loss: 4.390, avg. samples / sec: 28371.36
Iteration:   1740, Loss function: 3.859, Average Loss: 4.408, avg. samples / sec: 28267.39
Iteration:   1740, Loss function: 3.971, Average Loss: 4.419, avg. samples / sec: 28218.16
Iteration:   1740, Loss function: 3.389, Average Loss: 4.423, avg. samples / sec: 28256.75
Iteration:   1740, Loss function: 4.486, Average Loss: 4.391, avg. samples / sec: 28211.70
:::MLL 1558579377.462 epoch_stop: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 819}}
:::MLL 1558579377.463 epoch_start: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 673}}
Iteration:   1760, Loss function: 3.598, Average Loss: 4.421, avg. samples / sec: 28262.12
Iteration:   1760, Loss function: 4.362, Average Loss: 4.415, avg. samples / sec: 28246.92
Iteration:   1760, Loss function: 4.580, Average Loss: 4.406, avg. samples / sec: 28229.30
Iteration:   1760, Loss function: 4.495, Average Loss: 4.391, avg. samples / sec: 28236.72
Iteration:   1780, Loss function: 3.999, Average Loss: 4.402, avg. samples / sec: 28275.96
Iteration:   1780, Loss function: 4.550, Average Loss: 4.418, avg. samples / sec: 28266.09
Iteration:   1780, Loss function: 4.369, Average Loss: 4.388, avg. samples / sec: 28330.21
Iteration:   1780, Loss function: 3.591, Average Loss: 4.414, avg. samples / sec: 28242.92
Iteration:   1800, Loss function: 4.135, Average Loss: 4.416, avg. samples / sec: 28381.37
Iteration:   1800, Loss function: 4.206, Average Loss: 4.410, avg. samples / sec: 28394.84
Iteration:   1800, Loss function: 3.649, Average Loss: 4.400, avg. samples / sec: 28374.46
Iteration:   1800, Loss function: 4.976, Average Loss: 4.386, avg. samples / sec: 28377.27
Iteration:   1820, Loss function: 3.513, Average Loss: 4.413, avg. samples / sec: 28311.69
Iteration:   1820, Loss function: 4.351, Average Loss: 4.406, avg. samples / sec: 28316.70
Iteration:   1820, Loss function: 5.179, Average Loss: 4.400, avg. samples / sec: 28297.90
Iteration:   1820, Loss function: 4.763, Average Loss: 4.386, avg. samples / sec: 28281.44
:::MLL 1558579381.638 epoch_stop: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 819}}
:::MLL 1558579381.639 epoch_start: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 673}}
Iteration:   1840, Loss function: 4.370, Average Loss: 4.382, avg. samples / sec: 28395.08
Iteration:   1840, Loss function: 5.223, Average Loss: 4.406, avg. samples / sec: 28333.26
Iteration:   1840, Loss function: 3.630, Average Loss: 4.408, avg. samples / sec: 28324.16
Iteration:   1840, Loss function: 4.731, Average Loss: 4.396, avg. samples / sec: 28334.25
Iteration:   1860, Loss function: 3.663, Average Loss: 4.404, avg. samples / sec: 28529.34
Iteration:   1860, Loss function: 4.546, Average Loss: 4.397, avg. samples / sec: 28544.70
Iteration:   1860, Loss function: 3.854, Average Loss: 4.383, avg. samples / sec: 28487.83
Iteration:   1860, Loss function: 4.614, Average Loss: 4.403, avg. samples / sec: 28442.69
Iteration:   1880, Loss function: 4.072, Average Loss: 4.379, avg. samples / sec: 28380.35
Iteration:   1880, Loss function: 4.072, Average Loss: 4.402, avg. samples / sec: 28442.50
Iteration:   1880, Loss function: 4.306, Average Loss: 4.401, avg. samples / sec: 28357.73
Iteration:   1880, Loss function: 4.982, Average Loss: 4.396, avg. samples / sec: 28291.93
Iteration:   1900, Loss function: 4.144, Average Loss: 4.394, avg. samples / sec: 28472.91
Iteration:   1900, Loss function: 4.819, Average Loss: 4.398, avg. samples / sec: 28395.68
Iteration:   1900, Loss function: 4.449, Average Loss: 4.399, avg. samples / sec: 28400.42
Iteration:   1900, Loss function: 4.585, Average Loss: 4.378, avg. samples / sec: 28388.81
:::MLL 1558579385.750 epoch_stop: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 819}}
:::MLL 1558579385.751 epoch_start: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 673}}
Iteration:   1920, Loss function: 3.949, Average Loss: 4.396, avg. samples / sec: 28249.52
Iteration:   1920, Loss function: 4.070, Average Loss: 4.396, avg. samples / sec: 28248.34
Iteration:   1920, Loss function: 4.202, Average Loss: 4.392, avg. samples / sec: 28227.16
Iteration:   1920, Loss function: 4.528, Average Loss: 4.374, avg. samples / sec: 28177.13
Iteration:   1940, Loss function: 5.341, Average Loss: 4.371, avg. samples / sec: 28423.73
Iteration:   1940, Loss function: 4.211, Average Loss: 4.393, avg. samples / sec: 28339.01
Iteration:   1940, Loss function: 4.528, Average Loss: 4.394, avg. samples / sec: 28343.90
Iteration:   1940, Loss function: 4.168, Average Loss: 4.387, avg. samples / sec: 28339.95
Iteration:   1960, Loss function: 4.532, Average Loss: 4.392, avg. samples / sec: 28379.07
Iteration:   1960, Loss function: 3.517, Average Loss: 4.389, avg. samples / sec: 28365.52
Iteration:   1960, Loss function: 4.174, Average Loss: 4.369, avg. samples / sec: 28356.45
Iteration:   1960, Loss function: 4.697, Average Loss: 4.387, avg. samples / sec: 28376.62
Iteration:   1980, Loss function: 4.495, Average Loss: 4.385, avg. samples / sec: 28358.60
Iteration:   1980, Loss function: 4.465, Average Loss: 4.391, avg. samples / sec: 28329.26
Iteration:   1980, Loss function: 4.472, Average Loss: 4.368, avg. samples / sec: 28348.95
Iteration:   1980, Loss function: 3.744, Average Loss: 4.385, avg. samples / sec: 28306.78
:::MLL 1558579389.869 epoch_stop: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 819}}
:::MLL 1558579389.869 epoch_start: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 673}}
Iteration:   2000, Loss function: 4.281, Average Loss: 4.382, avg. samples / sec: 28340.58
Iteration:   2000, Loss function: 4.073, Average Loss: 4.367, avg. samples / sec: 28348.40
Iteration:   2000, Loss function: 4.228, Average Loss: 4.388, avg. samples / sec: 28344.71
Iteration:   2000, Loss function: 3.956, Average Loss: 4.381, avg. samples / sec: 28360.89
Iteration:   2020, Loss function: 4.207, Average Loss: 4.363, avg. samples / sec: 28335.21
Iteration:   2020, Loss function: 4.035, Average Loss: 4.384, avg. samples / sec: 28333.94
Iteration:   2020, Loss function: 4.085, Average Loss: 4.379, avg. samples / sec: 28323.47
Iteration:   2020, Loss function: 4.310, Average Loss: 4.375, avg. samples / sec: 28293.67
Iteration:   2040, Loss function: 4.261, Average Loss: 4.380, avg. samples / sec: 28419.69
Iteration:   2040, Loss function: 4.011, Average Loss: 4.370, avg. samples / sec: 28470.19
Iteration:   2040, Loss function: 4.375, Average Loss: 4.375, avg. samples / sec: 28417.36
Iteration:   2040, Loss function: 4.387, Average Loss: 4.362, avg. samples / sec: 28379.98
Iteration:   2060, Loss function: 4.579, Average Loss: 4.373, avg. samples / sec: 28342.48
Iteration:   2060, Loss function: 4.953, Average Loss: 4.376, avg. samples / sec: 28347.20
Iteration:   2060, Loss function: 3.876, Average Loss: 4.357, avg. samples / sec: 28346.68
Iteration:   2060, Loss function: 4.307, Average Loss: 4.369, avg. samples / sec: 28299.39
:::MLL 1558579394.042 epoch_stop: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 819}}
:::MLL 1558579394.042 epoch_start: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 673}}
Iteration:   2080, Loss function: 4.278, Average Loss: 4.372, avg. samples / sec: 28237.43
Iteration:   2080, Loss function: 4.178, Average Loss: 4.355, avg. samples / sec: 28259.07
Iteration:   2080, Loss function: 4.360, Average Loss: 4.367, avg. samples / sec: 28249.88
Iteration:   2080, Loss function: 3.835, Average Loss: 4.373, avg. samples / sec: 28197.29
Iteration:   2100, Loss function: 3.567, Average Loss: 4.367, avg. samples / sec: 28333.35
Iteration:   2100, Loss function: 3.627, Average Loss: 4.372, avg. samples / sec: 28367.53
Iteration:   2100, Loss function: 4.562, Average Loss: 4.350, avg. samples / sec: 28318.48
Iteration:   2100, Loss function: 4.465, Average Loss: 4.366, avg. samples / sec: 28348.53
Iteration:   2120, Loss function: 4.132, Average Loss: 4.363, avg. samples / sec: 28450.49
Iteration:   2120, Loss function: 3.814, Average Loss: 4.362, avg. samples / sec: 28430.71
Iteration:   2120, Loss function: 4.200, Average Loss: 4.367, avg. samples / sec: 28405.57
Iteration:   2120, Loss function: 3.589, Average Loss: 4.345, avg. samples / sec: 28408.08
:::MLL 1558579398.162 epoch_stop: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 819}}
:::MLL 1558579398.162 epoch_start: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 673}}
Iteration:   2140, Loss function: 4.022, Average Loss: 4.358, avg. samples / sec: 28305.18
Iteration:   2140, Loss function: 3.979, Average Loss: 4.340, avg. samples / sec: 28347.34
Iteration:   2140, Loss function: 3.866, Average Loss: 4.357, avg. samples / sec: 28294.33
Iteration:   2140, Loss function: 3.802, Average Loss: 4.364, avg. samples / sec: 28287.72
Iteration:   2160, Loss function: 4.131, Average Loss: 4.338, avg. samples / sec: 28293.67
Iteration:   2160, Loss function: 4.290, Average Loss: 4.356, avg. samples / sec: 28293.33
Iteration:   2160, Loss function: 4.173, Average Loss: 4.353, avg. samples / sec: 28273.98
Iteration:   2160, Loss function: 3.793, Average Loss: 4.360, avg. samples / sec: 28304.64
Iteration:   2180, Loss function: 4.084, Average Loss: 4.354, avg. samples / sec: 28365.78
Iteration:   2180, Loss function: 4.640, Average Loss: 4.332, avg. samples / sec: 28319.39
Iteration:   2180, Loss function: 4.273, Average Loss: 4.351, avg. samples / sec: 28322.19
Iteration:   2180, Loss function: 3.831, Average Loss: 4.348, avg. samples / sec: 28315.62
Iteration:   2200, Loss function: 4.457, Average Loss: 4.345, avg. samples / sec: 28400.07
Iteration:   2200, Loss function: 4.021, Average Loss: 4.347, avg. samples / sec: 28383.04
Iteration:   2200, Loss function: 4.472, Average Loss: 4.351, avg. samples / sec: 28367.32
Iteration:   2200, Loss function: 3.971, Average Loss: 4.328, avg. samples / sec: 28330.81
:::MLL 1558579402.340 epoch_stop: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 819}}
:::MLL 1558579402.340 epoch_start: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 673}}
Iteration:   2220, Loss function: 3.761, Average Loss: 4.346, avg. samples / sec: 28256.95
Iteration:   2220, Loss function: 4.410, Average Loss: 4.323, avg. samples / sec: 28300.50
Iteration:   2220, Loss function: 4.102, Average Loss: 4.341, avg. samples / sec: 28253.82
Iteration:   2220, Loss function: 4.444, Average Loss: 4.343, avg. samples / sec: 28224.53
Iteration:   2240, Loss function: 3.455, Average Loss: 4.335, avg. samples / sec: 28342.63
Iteration:   2240, Loss function: 4.368, Average Loss: 4.317, avg. samples / sec: 28323.77
Iteration:   2240, Loss function: 4.482, Average Loss: 4.340, avg. samples / sec: 28311.19
Iteration:   2240, Loss function: 3.493, Average Loss: 4.336, avg. samples / sec: 28257.92
Iteration:   2260, Loss function: 4.276, Average Loss: 4.331, avg. samples / sec: 28340.60
Iteration:   2260, Loss function: 4.693, Average Loss: 4.334, avg. samples / sec: 28435.83
Iteration:   2260, Loss function: 4.823, Average Loss: 4.336, avg. samples / sec: 28359.68
Iteration:   2260, Loss function: 4.234, Average Loss: 4.314, avg. samples / sec: 28313.44
Iteration:   2280, Loss function: 4.013, Average Loss: 4.309, avg. samples / sec: 28242.63
Iteration:   2280, Loss function: 3.941, Average Loss: 4.327, avg. samples / sec: 28199.99
Iteration:   2280, Loss function: 4.321, Average Loss: 4.329, avg. samples / sec: 28195.44
Iteration:   2280, Loss function: 5.041, Average Loss: 4.332, avg. samples / sec: 28186.21
:::MLL 1558579406.464 epoch_stop: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 819}}
:::MLL 1558579406.464 epoch_start: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 673}}
Iteration:   2300, Loss function: 4.154, Average Loss: 4.304, avg. samples / sec: 28372.73
Iteration:   2300, Loss function: 4.565, Average Loss: 4.322, avg. samples / sec: 28367.60
Iteration:   2300, Loss function: 3.562, Average Loss: 4.322, avg. samples / sec: 28371.38
Iteration:   2300, Loss function: 4.737, Average Loss: 4.330, avg. samples / sec: 28381.63
Iteration:   2320, Loss function: 4.188, Average Loss: 4.327, avg. samples / sec: 28458.15
Iteration:   2320, Loss function: 3.679, Average Loss: 4.302, avg. samples / sec: 28441.91
Iteration:   2320, Loss function: 3.873, Average Loss: 4.317, avg. samples / sec: 28395.45
Iteration:   2320, Loss function: 4.510, Average Loss: 4.318, avg. samples / sec: 28373.64
Iteration:   2340, Loss function: 4.159, Average Loss: 4.311, avg. samples / sec: 28507.73
Iteration:   2340, Loss function: 3.909, Average Loss: 4.315, avg. samples / sec: 28530.29
Iteration:   2340, Loss function: 3.659, Average Loss: 4.323, avg. samples / sec: 28443.74
Iteration:   2340, Loss function: 3.693, Average Loss: 4.298, avg. samples / sec: 28401.32
Iteration:   2360, Loss function: 3.537, Average Loss: 4.306, avg. samples / sec: 28282.52
Iteration:   2360, Loss function: 4.737, Average Loss: 4.319, avg. samples / sec: 28284.79
Iteration:   2360, Loss function: 3.803, Average Loss: 4.293, avg. samples / sec: 28281.26
Iteration:   2360, Loss function: 3.837, Average Loss: 4.312, avg. samples / sec: 28226.03
:::MLL 1558579410.578 epoch_stop: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 819}}
:::MLL 1558579410.578 epoch_start: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 673}}
Iteration:   2380, Loss function: 4.479, Average Loss: 4.312, avg. samples / sec: 28175.40
Iteration:   2380, Loss function: 4.421, Average Loss: 4.290, avg. samples / sec: 28214.09
Iteration:   2380, Loss function: 4.601, Average Loss: 4.308, avg. samples / sec: 28207.95
Iteration:   2380, Loss function: 4.739, Average Loss: 4.301, avg. samples / sec: 28131.71
Iteration:   2400, Loss function: 3.759, Average Loss: 4.297, avg. samples / sec: 28427.94
Iteration:   2400, Loss function: 4.134, Average Loss: 4.285, avg. samples / sec: 28387.87
Iteration:   2400, Loss function: 3.973, Average Loss: 4.306, avg. samples / sec: 28374.30
Iteration:   2400, Loss function: 4.654, Average Loss: 4.304, avg. samples / sec: 28313.81
Iteration:   2420, Loss function: 4.542, Average Loss: 4.301, avg. samples / sec: 28426.55
Iteration:   2420, Loss function: 4.026, Average Loss: 4.281, avg. samples / sec: 28335.45
Iteration:   2420, Loss function: 3.464, Average Loss: 4.301, avg. samples / sec: 28337.61
Iteration:   2420, Loss function: 5.044, Average Loss: 4.296, avg. samples / sec: 28299.73
Iteration:   2440, Loss function: 4.310, Average Loss: 4.296, avg. samples / sec: 28296.69
Iteration:   2440, Loss function: 4.444, Average Loss: 4.298, avg. samples / sec: 28302.10
Iteration:   2440, Loss function: 3.793, Average Loss: 4.277, avg. samples / sec: 28249.75
Iteration:   2440, Loss function: 3.791, Average Loss: 4.293, avg. samples / sec: 28266.21
:::MLL 1558579414.763 epoch_stop: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 819}}
:::MLL 1558579414.764 epoch_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 673}}
Iteration:   2460, Loss function: 4.122, Average Loss: 4.291, avg. samples / sec: 28183.46
Iteration:   2460, Loss function: 3.831, Average Loss: 4.290, avg. samples / sec: 28103.83
Iteration:   2460, Loss function: 3.784, Average Loss: 4.271, avg. samples / sec: 28151.22
Iteration:   2460, Loss function: 4.634, Average Loss: 4.294, avg. samples / sec: 28057.79
Iteration:   2480, Loss function: 4.426, Average Loss: 4.268, avg. samples / sec: 28462.61
Iteration:   2480, Loss function: 4.151, Average Loss: 4.292, avg. samples / sec: 28504.89
Iteration:   2480, Loss function: 4.025, Average Loss: 4.285, avg. samples / sec: 28432.36
Iteration:   2480, Loss function: 4.152, Average Loss: 4.289, avg. samples / sec: 28416.84
Iteration:   2500, Loss function: 4.412, Average Loss: 4.266, avg. samples / sec: 28206.74
Iteration:   2500, Loss function: 3.725, Average Loss: 4.284, avg. samples / sec: 28249.80
Iteration:   2500, Loss function: 4.743, Average Loss: 4.290, avg. samples / sec: 28204.40
Iteration:   2500, Loss function: 4.844, Average Loss: 4.284, avg. samples / sec: 28200.87
:::MLL 1558579417.857 eval_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.26 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.26 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.26 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.26 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.42s)
DONE (t=0.42s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.42s)
DONE (t=0.42s)
DONE (t=0.42s)
DONE (t=0.42s)
DONE (t=2.99s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.14389
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.28118
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.13692
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.03312
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.14956
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.23208
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.16458
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.23560
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.24842
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.06062
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.26189
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.38585
Current AP: 0.14389 AP goal: 0.23000
:::MLL 1558579423.579 eval_accuracy: {"value": 0.14389373501748143, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 389}}
:::MLL 1558579423.597 eval_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 392}}
:::MLL 1558579423.612 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 804}}
:::MLL 1558579423.612 block_start: {"value": null, "metadata": {"first_epoch_num": 33, "epoch_count": 10.915354834308324, "file": "train.py", "lineno": 813}}
:::MLL 1558579425.245 epoch_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 819}}
:::MLL 1558579425.246 epoch_start: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 673}}
Iteration:   2520, Loss function: 3.876, Average Loss: 4.288, avg. samples / sec: 4126.79
Iteration:   2520, Loss function: 3.767, Average Loss: 4.279, avg. samples / sec: 4126.58
Iteration:   2520, Loss function: 4.320, Average Loss: 4.282, avg. samples / sec: 4126.76
Iteration:   2520, Loss function: 4.363, Average Loss: 4.262, avg. samples / sec: 4125.92
Iteration:   2540, Loss function: 4.126, Average Loss: 4.276, avg. samples / sec: 27937.26
Iteration:   2540, Loss function: 3.741, Average Loss: 4.258, avg. samples / sec: 27950.57
Iteration:   2540, Loss function: 3.513, Average Loss: 4.273, avg. samples / sec: 27891.47
Iteration:   2540, Loss function: 4.150, Average Loss: 4.282, avg. samples / sec: 27867.10
Iteration:   2560, Loss function: 4.255, Average Loss: 4.271, avg. samples / sec: 28076.18
Iteration:   2560, Loss function: 3.587, Average Loss: 4.275, avg. samples / sec: 28030.23
Iteration:   2560, Loss function: 4.221, Average Loss: 4.278, avg. samples / sec: 28078.69
Iteration:   2560, Loss function: 4.265, Average Loss: 4.251, avg. samples / sec: 27979.19
Iteration:   2580, Loss function: 4.367, Average Loss: 4.249, avg. samples / sec: 28214.57
Iteration:   2580, Loss function: 4.231, Average Loss: 4.267, avg. samples / sec: 28143.23
Iteration:   2580, Loss function: 4.253, Average Loss: 4.272, avg. samples / sec: 28140.47
Iteration:   2580, Loss function: 3.541, Average Loss: 4.273, avg. samples / sec: 28146.86
:::MLL 1558579429.409 epoch_stop: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 819}}
:::MLL 1558579429.409 epoch_start: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 673}}
Iteration:   2600, Loss function: 4.039, Average Loss: 4.263, avg. samples / sec: 28064.79
Iteration:   2600, Loss function: 3.887, Average Loss: 4.243, avg. samples / sec: 28059.82
Iteration:   2600, Loss function: 3.938, Average Loss: 4.269, avg. samples / sec: 28077.36
Iteration:   2600, Loss function: 4.295, Average Loss: 4.266, avg. samples / sec: 28065.34
Iteration:   2620, Loss function: 4.777, Average Loss: 4.239, avg. samples / sec: 28065.04
Iteration:   2620, Loss function: 3.575, Average Loss: 4.264, avg. samples / sec: 28067.46
Iteration:   2620, Loss function: 3.828, Average Loss: 4.258, avg. samples / sec: 28057.23
Iteration:   2620, Loss function: 3.910, Average Loss: 4.265, avg. samples / sec: 28029.16
Iteration:   2640, Loss function: 3.930, Average Loss: 4.259, avg. samples / sec: 28083.66
Iteration:   2640, Loss function: 3.822, Average Loss: 4.261, avg. samples / sec: 28113.62
Iteration:   2640, Loss function: 4.674, Average Loss: 4.253, avg. samples / sec: 28077.77
Iteration:   2640, Loss function: 3.745, Average Loss: 4.235, avg. samples / sec: 28062.23
Iteration:   2660, Loss function: 3.957, Average Loss: 4.231, avg. samples / sec: 28076.24
Iteration:   2660, Loss function: 4.605, Average Loss: 4.260, avg. samples / sec: 28059.13
Iteration:   2660, Loss function: 3.861, Average Loss: 4.249, avg. samples / sec: 28058.24
Iteration:   2660, Loss function: 3.838, Average Loss: 4.256, avg. samples / sec: 28016.05
:::MLL 1558579433.627 epoch_stop: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 819}}
:::MLL 1558579433.628 epoch_start: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 673}}
Iteration:   2680, Loss function: 4.044, Average Loss: 4.259, avg. samples / sec: 27955.97
Iteration:   2680, Loss function: 4.081, Average Loss: 4.252, avg. samples / sec: 27991.23
Iteration:   2680, Loss function: 3.982, Average Loss: 4.224, avg. samples / sec: 27940.86
Iteration:   2680, Loss function: 4.516, Average Loss: 4.244, avg. samples / sec: 27919.00
Iteration:   2700, Loss function: 4.031, Average Loss: 4.254, avg. samples / sec: 28080.95
Iteration:   2700, Loss function: 3.699, Average Loss: 4.238, avg. samples / sec: 28110.12
Iteration:   2700, Loss function: 3.812, Average Loss: 4.220, avg. samples / sec: 28074.52
Iteration:   2700, Loss function: 3.925, Average Loss: 4.247, avg. samples / sec: 28027.93
Iteration:   2720, Loss function: 3.708, Average Loss: 4.236, avg. samples / sec: 27979.07
Iteration:   2720, Loss function: 4.100, Average Loss: 4.254, avg. samples / sec: 27965.68
Iteration:   2720, Loss function: 3.881, Average Loss: 4.240, avg. samples / sec: 28013.29
Iteration:   2720, Loss function: 3.834, Average Loss: 4.216, avg. samples / sec: 27926.69
Iteration:   2740, Loss function: 3.951, Average Loss: 4.236, avg. samples / sec: 27965.39
Iteration:   2740, Loss function: 4.105, Average Loss: 4.250, avg. samples / sec: 27957.76
Iteration:   2740, Loss function: 3.818, Average Loss: 4.230, avg. samples / sec: 27951.17
Iteration:   2740, Loss function: 4.078, Average Loss: 4.215, avg. samples / sec: 27983.16
:::MLL 1558579437.796 epoch_stop: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 819}}
:::MLL 1558579437.797 epoch_start: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 673}}
Iteration:   2760, Loss function: 4.436, Average Loss: 4.232, avg. samples / sec: 27964.53
Iteration:   2760, Loss function: 4.086, Average Loss: 4.224, avg. samples / sec: 27958.46
Iteration:   2760, Loss function: 3.929, Average Loss: 4.246, avg. samples / sec: 27937.37
Iteration:   2760, Loss function: 4.365, Average Loss: 4.212, avg. samples / sec: 27911.64
Iteration:   2780, Loss function: 4.258, Average Loss: 4.243, avg. samples / sec: 28006.24
Iteration:   2780, Loss function: 3.611, Average Loss: 4.208, avg. samples / sec: 28059.49
Iteration:   2780, Loss function: 4.374, Average Loss: 4.218, avg. samples / sec: 27982.53
Iteration:   2780, Loss function: 4.224, Average Loss: 4.227, avg. samples / sec: 27963.66
Iteration:   2800, Loss function: 3.962, Average Loss: 4.216, avg. samples / sec: 28074.75
Iteration:   2800, Loss function: 4.110, Average Loss: 4.238, avg. samples / sec: 28062.03
Iteration:   2800, Loss function: 4.429, Average Loss: 4.202, avg. samples / sec: 28066.57
Iteration:   2800, Loss function: 4.052, Average Loss: 4.222, avg. samples / sec: 28061.83
Iteration:   2820, Loss function: 3.679, Average Loss: 4.218, avg. samples / sec: 27954.06
Iteration:   2820, Loss function: 3.801, Average Loss: 4.199, avg. samples / sec: 27935.34
Iteration:   2820, Loss function: 4.303, Average Loss: 4.234, avg. samples / sec: 27916.20
Iteration:   2820, Loss function: 3.430, Average Loss: 4.210, avg. samples / sec: 27892.35
:::MLL 1558579441.968 epoch_stop: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 819}}
:::MLL 1558579441.968 epoch_start: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 673}}
Iteration:   2840, Loss function: 4.264, Average Loss: 4.216, avg. samples / sec: 28000.49
Iteration:   2840, Loss function: 4.283, Average Loss: 4.193, avg. samples / sec: 27998.42
Iteration:   2840, Loss function: 3.919, Average Loss: 4.231, avg. samples / sec: 27987.11
Iteration:   2840, Loss function: 4.263, Average Loss: 4.205, avg. samples / sec: 27986.24
Iteration:   2860, Loss function: 3.931, Average Loss: 4.212, avg. samples / sec: 27918.64
Iteration:   2860, Loss function: 4.070, Average Loss: 4.190, avg. samples / sec: 27922.96
Iteration:   2860, Loss function: 3.945, Average Loss: 4.202, avg. samples / sec: 27970.62
Iteration:   2860, Loss function: 4.617, Average Loss: 4.226, avg. samples / sec: 27951.56
Iteration:   2880, Loss function: 4.389, Average Loss: 4.197, avg. samples / sec: 28058.56
Iteration:   2880, Loss function: 4.183, Average Loss: 4.222, avg. samples / sec: 28063.18
Iteration:   2880, Loss function: 4.111, Average Loss: 4.185, avg. samples / sec: 28055.64
Iteration:   2880, Loss function: 4.107, Average Loss: 4.205, avg. samples / sec: 28052.94
Iteration:   2900, Loss function: 3.944, Average Loss: 4.184, avg. samples / sec: 27991.29
Iteration:   2900, Loss function: 4.368, Average Loss: 4.218, avg. samples / sec: 27947.90
Iteration:   2900, Loss function: 3.688, Average Loss: 4.198, avg. samples / sec: 27941.34
Iteration:   2900, Loss function: 4.343, Average Loss: 4.191, avg. samples / sec: 27898.04
:::MLL 1558579446.196 epoch_stop: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 819}}
:::MLL 1558579446.196 epoch_start: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 673}}
Iteration:   2920, Loss function: 4.030, Average Loss: 4.180, avg. samples / sec: 27942.68
Iteration:   2920, Loss function: 3.039, Average Loss: 4.194, avg. samples / sec: 27978.99
Iteration:   2920, Loss function: 3.169, Average Loss: 4.209, avg. samples / sec: 27957.56
Iteration:   2920, Loss function: 4.066, Average Loss: 4.188, avg. samples / sec: 27983.69
Iteration:   2940, Loss function: 4.244, Average Loss: 4.204, avg. samples / sec: 28005.40
Iteration:   2940, Loss function: 3.730, Average Loss: 4.182, avg. samples / sec: 28030.51
Iteration:   2940, Loss function: 4.761, Average Loss: 4.176, avg. samples / sec: 27977.75
Iteration:   2940, Loss function: 3.798, Average Loss: 4.189, avg. samples / sec: 27986.28
Iteration:   2960, Loss function: 4.296, Average Loss: 4.201, avg. samples / sec: 28126.05
Iteration:   2960, Loss function: 4.181, Average Loss: 4.173, avg. samples / sec: 28127.99
Iteration:   2960, Loss function: 4.758, Average Loss: 4.188, avg. samples / sec: 28093.02
Iteration:   2960, Loss function: 3.996, Average Loss: 4.176, avg. samples / sec: 28068.21
:::MLL 1558579450.363 epoch_stop: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 819}}
:::MLL 1558579450.363 epoch_start: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 673}}
Iteration:   2980, Loss function: 3.018, Average Loss: 4.168, avg. samples / sec: 27965.50
Iteration:   2980, Loss function: 3.910, Average Loss: 4.181, avg. samples / sec: 27986.20
Iteration:   2980, Loss function: 4.053, Average Loss: 4.169, avg. samples / sec: 27981.00
Iteration:   2980, Loss function: 3.966, Average Loss: 4.193, avg. samples / sec: 27909.69
Iteration:   3000, Loss function: 4.133, Average Loss: 4.164, avg. samples / sec: 28129.44
Iteration:   3000, Loss function: 3.730, Average Loss: 4.190, avg. samples / sec: 28139.51
Iteration:   3000, Loss function: 3.888, Average Loss: 4.176, avg. samples / sec: 28107.63
Iteration:   3000, Loss function: 3.948, Average Loss: 4.161, avg. samples / sec: 28077.24
Iteration:   3020, Loss function: 3.861, Average Loss: 4.170, avg. samples / sec: 28008.13
Iteration:   3020, Loss function: 4.166, Average Loss: 4.184, avg. samples / sec: 28000.32
Iteration:   3020, Loss function: 3.684, Average Loss: 4.159, avg. samples / sec: 27990.69
Iteration:   3020, Loss function: 3.036, Average Loss: 4.157, avg. samples / sec: 27993.32
Iteration:   3040, Loss function: 3.488, Average Loss: 4.157, avg. samples / sec: 28060.47
Iteration:   3040, Loss function: 4.459, Average Loss: 4.153, avg. samples / sec: 28065.28
Iteration:   3040, Loss function: 4.237, Average Loss: 4.165, avg. samples / sec: 28035.49
Iteration:   3040, Loss function: 3.653, Average Loss: 4.179, avg. samples / sec: 28011.33
:::MLL 1558579454.520 epoch_stop: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 819}}
:::MLL 1558579454.520 epoch_start: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 673}}
Iteration:   3060, Loss function: 4.273, Average Loss: 4.161, avg. samples / sec: 28169.22
Iteration:   3060, Loss function: 4.707, Average Loss: 4.157, avg. samples / sec: 28159.72
Iteration:   3060, Loss function: 4.528, Average Loss: 4.176, avg. samples / sec: 28200.10
Iteration:   3060, Loss function: 3.721, Average Loss: 4.148, avg. samples / sec: 28092.36
Iteration:   3080, Loss function: 4.173, Average Loss: 4.173, avg. samples / sec: 28092.33
Iteration:   3080, Loss function: 3.841, Average Loss: 4.146, avg. samples / sec: 28159.25
Iteration:   3080, Loss function: 3.902, Average Loss: 4.156, avg. samples / sec: 28057.78
Iteration:   3080, Loss function: 3.994, Average Loss: 4.161, avg. samples / sec: 28005.15
Iteration:   3100, Loss function: 3.847, Average Loss: 4.171, avg. samples / sec: 27898.92
Iteration:   3100, Loss function: 4.910, Average Loss: 4.152, avg. samples / sec: 27919.84
Iteration:   3100, Loss function: 3.823, Average Loss: 4.157, avg. samples / sec: 27972.90
Iteration:   3100, Loss function: 3.948, Average Loss: 4.141, avg. samples / sec: 27879.56
Iteration:   3120, Loss function: 4.145, Average Loss: 4.166, avg. samples / sec: 28026.85
Iteration:   3120, Loss function: 3.738, Average Loss: 4.154, avg. samples / sec: 28042.36
Iteration:   3120, Loss function: 4.336, Average Loss: 4.138, avg. samples / sec: 28050.21
Iteration:   3120, Loss function: 3.541, Average Loss: 4.148, avg. samples / sec: 28008.31
:::MLL 1558579458.741 epoch_stop: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 819}}
:::MLL 1558579458.741 epoch_start: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 673}}
Iteration:   3140, Loss function: 4.060, Average Loss: 4.133, avg. samples / sec: 28119.59
Iteration:   3140, Loss function: 4.119, Average Loss: 4.143, avg. samples / sec: 28136.38
Iteration:   3140, Loss function: 4.272, Average Loss: 4.150, avg. samples / sec: 28087.93
Iteration:   3140, Loss function: 4.337, Average Loss: 4.162, avg. samples / sec: 28068.18
Iteration:   3160, Loss function: 3.683, Average Loss: 4.157, avg. samples / sec: 28090.10
Iteration:   3160, Loss function: 3.545, Average Loss: 4.146, avg. samples / sec: 28064.99
Iteration:   3160, Loss function: 4.407, Average Loss: 4.131, avg. samples / sec: 28030.11
Iteration:   3160, Loss function: 3.479, Average Loss: 4.139, avg. samples / sec: 28003.99
Iteration:   3180, Loss function: 4.275, Average Loss: 4.154, avg. samples / sec: 28026.34
Iteration:   3180, Loss function: 4.254, Average Loss: 4.143, avg. samples / sec: 28027.82
Iteration:   3180, Loss function: 4.190, Average Loss: 4.126, avg. samples / sec: 28031.42
Iteration:   3180, Loss function: 3.907, Average Loss: 4.136, avg. samples / sec: 28062.87
Iteration:   3200, Loss function: 3.515, Average Loss: 4.141, avg. samples / sec: 28054.46
Iteration:   3200, Loss function: 3.808, Average Loss: 4.135, avg. samples / sec: 28048.10
Iteration:   3200, Loss function: 3.381, Average Loss: 4.148, avg. samples / sec: 28025.53
Iteration:   3200, Loss function: 3.642, Average Loss: 4.122, avg. samples / sec: 28024.30
:::MLL 1558579462.905 epoch_stop: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 819}}
:::MLL 1558579462.906 epoch_start: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 673}}
Iteration:   3220, Loss function: 3.679, Average Loss: 4.137, avg. samples / sec: 27905.34
Iteration:   3220, Loss function: 3.860, Average Loss: 4.144, avg. samples / sec: 27930.25
Iteration:   3220, Loss function: 3.887, Average Loss: 4.130, avg. samples / sec: 27909.48
Iteration:   3220, Loss function: 3.516, Average Loss: 4.121, avg. samples / sec: 27859.22
Iteration:   3240, Loss function: 4.137, Average Loss: 4.130, avg. samples / sec: 28073.84
Iteration:   3240, Loss function: 4.552, Average Loss: 4.141, avg. samples / sec: 28073.49
Iteration:   3240, Loss function: 4.203, Average Loss: 4.118, avg. samples / sec: 28114.58
Iteration:   3240, Loss function: 4.578, Average Loss: 4.123, avg. samples / sec: 28024.33
Iteration:   3260, Loss function: 4.001, Average Loss: 4.127, avg. samples / sec: 28059.36
Iteration:   3260, Loss function: 3.523, Average Loss: 4.115, avg. samples / sec: 28097.75
Iteration:   3260, Loss function: 3.351, Average Loss: 4.135, avg. samples / sec: 28006.56
Iteration:   3260, Loss function: 3.860, Average Loss: 4.118, avg. samples / sec: 28038.59
Iteration:   3280, Loss function: 4.397, Average Loss: 4.114, avg. samples / sec: 28107.59
Iteration:   3280, Loss function: 3.731, Average Loss: 4.123, avg. samples / sec: 28012.12
Iteration:   3280, Loss function: 3.672, Average Loss: 4.115, avg. samples / sec: 28013.93
Iteration:   3280, Loss function: 4.616, Average Loss: 4.131, avg. samples / sec: 28037.49
:::MLL 1558579467.072 epoch_stop: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 819}}
:::MLL 1558579467.073 epoch_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 673}}
Iteration:   3300, Loss function: 3.970, Average Loss: 4.111, avg. samples / sec: 27981.08
Iteration:   3300, Loss function: 4.052, Average Loss: 4.127, avg. samples / sec: 28023.23
Iteration:   3300, Loss function: 3.731, Average Loss: 4.120, avg. samples / sec: 27986.91
Iteration:   3300, Loss function: 3.815, Average Loss: 4.112, avg. samples / sec: 27969.87
Iteration:   3320, Loss function: 3.715, Average Loss: 4.104, avg. samples / sec: 27999.35
Iteration:   3320, Loss function: 4.015, Average Loss: 4.108, avg. samples / sec: 28017.22
Iteration:   3320, Loss function: 3.978, Average Loss: 4.115, avg. samples / sec: 27955.10
Iteration:   3320, Loss function: 3.780, Average Loss: 4.124, avg. samples / sec: 27941.29
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
:::MLL 1558579469.871 eval_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.04 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.04 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.04 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.04 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.54s)
DONE (t=3.05s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.17539
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.32582
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.17394
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.04451
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.18066
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.28217
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.18537
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.27129
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.28578
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.07861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.30155
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.44323
Current AP: 0.17539 AP goal: 0.23000
:::MLL 1558579474.544 eval_accuracy: {"value": 0.1753883527671619, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 389}}
:::MLL 1558579474.617 eval_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 392}}
:::MLL 1558579474.632 block_stop: {"value": null, "metadata": {"first_epoch_num": 33, "file": "train.py", "lineno": 804}}
:::MLL 1558579474.632 block_start: {"value": null, "metadata": {"first_epoch_num": 44, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3340, Loss function: 3.498, Average Loss: 4.103, avg. samples / sec: 5240.09
Iteration:   3340, Loss function: 4.301, Average Loss: 4.119, avg. samples / sec: 5241.66
Iteration:   3340, Loss function: 2.755, Average Loss: 4.109, avg. samples / sec: 5241.46
Iteration:   3340, Loss function: 3.579, Average Loss: 4.100, avg. samples / sec: 5239.63
:::MLL 1558579476.062 epoch_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 819}}
:::MLL 1558579476.062 epoch_start: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 673}}
Iteration:   3360, Loss function: 3.846, Average Loss: 4.108, avg. samples / sec: 27970.09
Iteration:   3360, Loss function: 3.526, Average Loss: 4.099, avg. samples / sec: 27955.63
Iteration:   3360, Loss function: 3.428, Average Loss: 4.091, avg. samples / sec: 27951.47
Iteration:   3360, Loss function: 3.455, Average Loss: 4.098, avg. samples / sec: 27942.49
Iteration:   3380, Loss function: 3.691, Average Loss: 4.091, avg. samples / sec: 28126.67
Iteration:   3380, Loss function: 3.669, Average Loss: 4.079, avg. samples / sec: 28128.07
Iteration:   3380, Loss function: 3.863, Average Loss: 4.088, avg. samples / sec: 28117.88
Iteration:   3380, Loss function: 3.718, Average Loss: 4.101, avg. samples / sec: 28058.66
Iteration:   3400, Loss function: 3.179, Average Loss: 4.070, avg. samples / sec: 27992.69
Iteration:   3400, Loss function: 3.683, Average Loss: 4.078, avg. samples / sec: 27982.22
Iteration:   3400, Loss function: 3.252, Average Loss: 4.086, avg. samples / sec: 27960.78
Iteration:   3400, Loss function: 3.206, Average Loss: 4.080, avg. samples / sec: 27922.48
Iteration:   3420, Loss function: 3.515, Average Loss: 4.069, avg. samples / sec: 28124.71
Iteration:   3420, Loss function: 3.474, Average Loss: 4.070, avg. samples / sec: 28149.19
Iteration:   3420, Loss function: 3.986, Average Loss: 4.058, avg. samples / sec: 28040.86
Iteration:   3420, Loss function: 3.880, Average Loss: 4.078, avg. samples / sec: 28065.90
:::MLL 1558579480.226 epoch_stop: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 819}}
:::MLL 1558579480.227 epoch_start: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 673}}
Iteration:   3440, Loss function: 3.590, Average Loss: 4.070, avg. samples / sec: 28047.42
Iteration:   3440, Loss function: 3.356, Average Loss: 4.048, avg. samples / sec: 27989.64
Iteration:   3440, Loss function: 3.599, Average Loss: 4.059, avg. samples / sec: 27937.47
Iteration:   3440, Loss function: 4.122, Average Loss: 4.060, avg. samples / sec: 27884.08
Iteration:   3460, Loss function: 4.287, Average Loss: 4.051, avg. samples / sec: 28036.31
Iteration:   3460, Loss function: 3.678, Average Loss: 4.058, avg. samples / sec: 27999.49
Iteration:   3460, Loss function: 3.791, Average Loss: 4.048, avg. samples / sec: 28024.58
Iteration:   3460, Loss function: 4.351, Average Loss: 4.038, avg. samples / sec: 27958.24
Iteration:   3480, Loss function: 3.114, Average Loss: 4.045, avg. samples / sec: 28018.02
Iteration:   3480, Loss function: 3.133, Average Loss: 4.037, avg. samples / sec: 28015.88
Iteration:   3480, Loss function: 3.271, Average Loss: 4.040, avg. samples / sec: 27999.87
Iteration:   3480, Loss function: 3.412, Average Loss: 4.026, avg. samples / sec: 28037.24
Iteration:   3500, Loss function: 2.912, Average Loss: 4.026, avg. samples / sec: 28115.71
Iteration:   3500, Loss function: 3.170, Average Loss: 4.031, avg. samples / sec: 28100.94
Iteration:   3500, Loss function: 3.004, Average Loss: 4.033, avg. samples / sec: 28065.74
Iteration:   3500, Loss function: 3.113, Average Loss: 4.013, avg. samples / sec: 28072.09
:::MLL 1558579484.393 epoch_stop: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 819}}
:::MLL 1558579484.394 epoch_start: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 673}}
Iteration:   3520, Loss function: 3.436, Average Loss: 4.022, avg. samples / sec: 28003.64
Iteration:   3520, Loss function: 3.509, Average Loss: 4.002, avg. samples / sec: 28036.33
Iteration:   3520, Loss function: 3.688, Average Loss: 4.023, avg. samples / sec: 28014.86
Iteration:   3520, Loss function: 4.040, Average Loss: 4.018, avg. samples / sec: 27969.57
Iteration:   3540, Loss function: 3.697, Average Loss: 4.010, avg. samples / sec: 28023.89
Iteration:   3540, Loss function: 3.843, Average Loss: 4.005, avg. samples / sec: 28014.12
Iteration:   3540, Loss function: 3.509, Average Loss: 4.012, avg. samples / sec: 27989.23
Iteration:   3540, Loss function: 3.146, Average Loss: 3.991, avg. samples / sec: 27986.16
Iteration:   3560, Loss function: 3.041, Average Loss: 3.981, avg. samples / sec: 28090.30
Iteration:   3560, Loss function: 3.946, Average Loss: 3.998, avg. samples / sec: 28059.91
Iteration:   3560, Loss function: 3.671, Average Loss: 4.002, avg. samples / sec: 28054.79
Iteration:   3560, Loss function: 3.269, Average Loss: 4.000, avg. samples / sec: 28018.14
Iteration:   3580, Loss function: 3.220, Average Loss: 3.971, avg. samples / sec: 27969.77
Iteration:   3580, Loss function: 3.341, Average Loss: 3.992, avg. samples / sec: 28005.10
Iteration:   3580, Loss function: 3.727, Average Loss: 3.987, avg. samples / sec: 27935.55
Iteration:   3580, Loss function: 3.321, Average Loss: 3.989, avg. samples / sec: 27913.75
:::MLL 1558579488.614 epoch_stop: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 819}}
:::MLL 1558579488.615 epoch_start: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 673}}
Iteration:   3600, Loss function: 3.739, Average Loss: 3.963, avg. samples / sec: 28043.29
Iteration:   3600, Loss function: 3.685, Average Loss: 3.978, avg. samples / sec: 28069.46
Iteration:   3600, Loss function: 3.809, Average Loss: 3.986, avg. samples / sec: 27995.87
Iteration:   3600, Loss function: 3.515, Average Loss: 3.979, avg. samples / sec: 28073.49
Iteration:   3620, Loss function: 3.648, Average Loss: 3.969, avg. samples / sec: 28163.98
Iteration:   3620, Loss function: 3.669, Average Loss: 3.968, avg. samples / sec: 28119.50
Iteration:   3620, Loss function: 3.508, Average Loss: 3.952, avg. samples / sec: 28069.42
Iteration:   3620, Loss function: 3.856, Average Loss: 3.975, avg. samples / sec: 28106.95
Iteration:   3640, Loss function: 3.791, Average Loss: 3.965, avg. samples / sec: 28115.60
Iteration:   3640, Loss function: 4.554, Average Loss: 3.942, avg. samples / sec: 28093.43
Iteration:   3640, Loss function: 3.164, Average Loss: 3.959, avg. samples / sec: 28041.26
Iteration:   3640, Loss function: 2.939, Average Loss: 3.958, avg. samples / sec: 28028.23
Iteration:   3660, Loss function: 3.584, Average Loss: 3.955, avg. samples / sec: 27972.63
Iteration:   3660, Loss function: 4.433, Average Loss: 3.952, avg. samples / sec: 27962.62
Iteration:   3660, Loss function: 3.422, Average Loss: 3.931, avg. samples / sec: 27931.67
Iteration:   3660, Loss function: 3.187, Average Loss: 3.947, avg. samples / sec: 27941.88
:::MLL 1558579492.780 epoch_stop: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 819}}
:::MLL 1558579492.780 epoch_start: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 673}}
Iteration:   3680, Loss function: 3.753, Average Loss: 3.936, avg. samples / sec: 28036.43
Iteration:   3680, Loss function: 2.740, Average Loss: 3.922, avg. samples / sec: 28031.25
Iteration:   3680, Loss function: 3.510, Average Loss: 3.942, avg. samples / sec: 28023.85
Iteration:   3680, Loss function: 3.234, Average Loss: 3.944, avg. samples / sec: 27935.48
Iteration:   3700, Loss function: 2.986, Average Loss: 3.927, avg. samples / sec: 28031.05
Iteration:   3700, Loss function: 3.456, Average Loss: 3.933, avg. samples / sec: 28072.76
Iteration:   3700, Loss function: 2.876, Average Loss: 3.913, avg. samples / sec: 28019.81
Iteration:   3700, Loss function: 3.598, Average Loss: 3.932, avg. samples / sec: 28020.67
Iteration:   3720, Loss function: 3.141, Average Loss: 3.919, avg. samples / sec: 28147.83
Iteration:   3720, Loss function: 3.228, Average Loss: 3.922, avg. samples / sec: 28141.70
Iteration:   3720, Loss function: 3.778, Average Loss: 3.904, avg. samples / sec: 28125.62
Iteration:   3720, Loss function: 2.893, Average Loss: 3.918, avg. samples / sec: 28102.45
Iteration:   3740, Loss function: 3.874, Average Loss: 3.897, avg. samples / sec: 28095.08
Iteration:   3740, Loss function: 3.791, Average Loss: 3.912, avg. samples / sec: 28069.09
Iteration:   3740, Loss function: 3.060, Average Loss: 3.908, avg. samples / sec: 28099.37
Iteration:   3740, Loss function: 3.844, Average Loss: 3.909, avg. samples / sec: 27977.78
:::MLL 1558579496.939 epoch_stop: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 819}}
:::MLL 1558579496.939 epoch_start: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 673}}
:::MLL 1558579497.487 eval_start: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.10 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.11 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.11 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.11 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.50s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=2.89s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22560
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.38965
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23065
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.06133
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.23872
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.36158
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.21921
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32201
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33840
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.10174
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36926
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.52281
Current AP: 0.22560 AP goal: 0.23000
:::MLL 1558579502.055 eval_accuracy: {"value": 0.2255960876983254, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 389}}
:::MLL 1558579502.063 eval_stop: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 392}}
:::MLL 1558579502.078 block_stop: {"value": null, "metadata": {"first_epoch_num": 44, "file": "train.py", "lineno": 804}}
:::MLL 1558579502.079 block_start: {"value": null, "metadata": {"first_epoch_num": 50, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3760, Loss function: 3.888, Average Loss: 3.903, avg. samples / sec: 5399.11
Iteration:   3760, Loss function: 3.888, Average Loss: 3.890, avg. samples / sec: 5398.51
Iteration:   3760, Loss function: 3.453, Average Loss: 3.900, avg. samples / sec: 5398.68
Iteration:   3760, Loss function: 3.326, Average Loss: 3.898, avg. samples / sec: 5401.45
Iteration:   3780, Loss function: 4.028, Average Loss: 3.893, avg. samples / sec: 27937.03
Iteration:   3780, Loss function: 2.661, Average Loss: 3.890, avg. samples / sec: 27907.21
Iteration:   3780, Loss function: 3.785, Average Loss: 3.892, avg. samples / sec: 27896.99
Iteration:   3780, Loss function: 3.778, Average Loss: 3.886, avg. samples / sec: 27878.09
Iteration:   3800, Loss function: 3.305, Average Loss: 3.883, avg. samples / sec: 27728.01
Iteration:   3800, Loss function: 3.553, Average Loss: 3.881, avg. samples / sec: 27705.69
Iteration:   3800, Loss function: 3.163, Average Loss: 3.884, avg. samples / sec: 27670.54
Iteration:   3800, Loss function: 2.957, Average Loss: 3.876, avg. samples / sec: 27642.44
:::MLL 1558579505.773 epoch_stop: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 819}}
:::MLL 1558579505.773 epoch_start: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 673}}
Iteration:   3820, Loss function: 2.904, Average Loss: 3.877, avg. samples / sec: 27992.44
Iteration:   3820, Loss function: 3.439, Average Loss: 3.872, avg. samples / sec: 27978.47
Iteration:   3820, Loss function: 3.274, Average Loss: 3.867, avg. samples / sec: 28061.55
Iteration:   3820, Loss function: 3.618, Average Loss: 3.873, avg. samples / sec: 27917.64
Iteration:   3840, Loss function: 3.327, Average Loss: 3.868, avg. samples / sec: 28113.58
Iteration:   3840, Loss function: 4.041, Average Loss: 3.859, avg. samples / sec: 28082.81
Iteration:   3840, Loss function: 3.011, Average Loss: 3.861, avg. samples / sec: 28071.55
Iteration:   3840, Loss function: 3.052, Average Loss: 3.865, avg. samples / sec: 28116.20
Iteration:   3860, Loss function: 3.200, Average Loss: 3.856, avg. samples / sec: 28059.06
Iteration:   3860, Loss function: 3.534, Average Loss: 3.861, avg. samples / sec: 28003.64
Iteration:   3860, Loss function: 3.650, Average Loss: 3.853, avg. samples / sec: 28039.59
Iteration:   3860, Loss function: 3.915, Average Loss: 3.852, avg. samples / sec: 28002.32
Iteration:   3880, Loss function: 3.629, Average Loss: 3.847, avg. samples / sec: 28188.25
Iteration:   3880, Loss function: 3.971, Average Loss: 3.846, avg. samples / sec: 28198.56
Iteration:   3880, Loss function: 3.314, Average Loss: 3.844, avg. samples / sec: 28224.38
Iteration:   3880, Loss function: 3.617, Average Loss: 3.854, avg. samples / sec: 28154.74
:::MLL 1558579509.927 epoch_stop: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 819}}
:::MLL 1558579509.928 epoch_start: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 673}}
Iteration:   3900, Loss function: 3.330, Average Loss: 3.836, avg. samples / sec: 28082.38
Iteration:   3900, Loss function: 3.691, Average Loss: 3.840, avg. samples / sec: 28066.07
Iteration:   3900, Loss function: 3.396, Average Loss: 3.834, avg. samples / sec: 28075.90
Iteration:   3900, Loss function: 3.336, Average Loss: 3.845, avg. samples / sec: 28056.64
Iteration:   3920, Loss function: 4.061, Average Loss: 3.827, avg. samples / sec: 28142.81
Iteration:   3920, Loss function: 2.887, Average Loss: 3.829, avg. samples / sec: 28154.86
Iteration:   3920, Loss function: 3.491, Average Loss: 3.831, avg. samples / sec: 28154.16
Iteration:   3920, Loss function: 2.943, Average Loss: 3.834, avg. samples / sec: 28178.92
Iteration:   3940, Loss function: 3.839, Average Loss: 3.823, avg. samples / sec: 28160.88
Iteration:   3940, Loss function: 3.625, Average Loss: 3.820, avg. samples / sec: 28147.55
Iteration:   3940, Loss function: 3.789, Average Loss: 3.827, avg. samples / sec: 28168.08
Iteration:   3940, Loss function: 3.687, Average Loss: 3.822, avg. samples / sec: 28145.18
Iteration:   3960, Loss function: 3.399, Average Loss: 3.821, avg. samples / sec: 27953.27
Iteration:   3960, Loss function: 3.450, Average Loss: 3.815, avg. samples / sec: 27948.30
Iteration:   3960, Loss function: 3.644, Average Loss: 3.815, avg. samples / sec: 27927.20
Iteration:   3960, Loss function: 3.988, Average Loss: 3.812, avg. samples / sec: 27932.58
:::MLL 1558579514.086 epoch_stop: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 819}}
:::MLL 1558579514.086 epoch_start: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 673}}
Iteration:   3980, Loss function: 3.253, Average Loss: 3.808, avg. samples / sec: 27943.77
Iteration:   3980, Loss function: 3.687, Average Loss: 3.812, avg. samples / sec: 27935.59
Iteration:   3980, Loss function: 3.498, Average Loss: 3.807, avg. samples / sec: 27934.10
Iteration:   3980, Loss function: 3.580, Average Loss: 3.802, avg. samples / sec: 27895.14
Iteration:   4000, Loss function: 3.567, Average Loss: 3.800, avg. samples / sec: 27962.76
Iteration:   4000, Loss function: 3.195, Average Loss: 3.803, avg. samples / sec: 27947.53
Iteration:   4000, Loss function: 3.716, Average Loss: 3.794, avg. samples / sec: 28001.04
Iteration:   4000, Loss function: 3.408, Average Loss: 3.799, avg. samples / sec: 27920.44
Iteration:   4020, Loss function: 3.349, Average Loss: 3.796, avg. samples / sec: 28024.73
Iteration:   4020, Loss function: 3.220, Average Loss: 3.792, avg. samples / sec: 28037.30
Iteration:   4020, Loss function: 2.821, Average Loss: 3.791, avg. samples / sec: 28004.37
Iteration:   4020, Loss function: 3.103, Average Loss: 3.786, avg. samples / sec: 27962.73
Iteration:   4040, Loss function: 3.158, Average Loss: 3.782, avg. samples / sec: 28011.61
Iteration:   4040, Loss function: 3.779, Average Loss: 3.778, avg. samples / sec: 28049.48
Iteration:   4040, Loss function: 3.292, Average Loss: 3.787, avg. samples / sec: 27978.92
Iteration:   4040, Loss function: 3.799, Average Loss: 3.785, avg. samples / sec: 27965.83
:::MLL 1558579518.315 epoch_stop: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 819}}
:::MLL 1558579518.315 epoch_start: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 673}}
Iteration:   4060, Loss function: 3.438, Average Loss: 3.776, avg. samples / sec: 28119.71
Iteration:   4060, Loss function: 3.277, Average Loss: 3.775, avg. samples / sec: 28059.26
Iteration:   4060, Loss function: 3.358, Average Loss: 3.768, avg. samples / sec: 28057.99
Iteration:   4060, Loss function: 3.119, Average Loss: 3.779, avg. samples / sec: 28045.67
Iteration:   4080, Loss function: 3.368, Average Loss: 3.759, avg. samples / sec: 28007.72
Iteration:   4080, Loss function: 3.770, Average Loss: 3.768, avg. samples / sec: 27993.96
Iteration:   4080, Loss function: 3.139, Average Loss: 3.768, avg. samples / sec: 27963.56
Iteration:   4080, Loss function: 3.707, Average Loss: 3.770, avg. samples / sec: 27989.37
Iteration:   4100, Loss function: 3.964, Average Loss: 3.765, avg. samples / sec: 28066.28
Iteration:   4100, Loss function: 4.280, Average Loss: 3.752, avg. samples / sec: 28030.21
Iteration:   4100, Loss function: 3.248, Average Loss: 3.761, avg. samples / sec: 28038.65
Iteration:   4100, Loss function: 3.637, Average Loss: 3.760, avg. samples / sec: 27968.58
Iteration:   4120, Loss function: 3.074, Average Loss: 3.757, avg. samples / sec: 28040.50
Iteration:   4120, Loss function: 2.899, Average Loss: 3.755, avg. samples / sec: 28109.84
Iteration:   4120, Loss function: 3.670, Average Loss: 3.743, avg. samples / sec: 28035.81
Iteration:   4120, Loss function: 3.773, Average Loss: 3.756, avg. samples / sec: 28029.85
:::MLL 1558579522.479 epoch_stop: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 819}}
:::MLL 1558579522.480 epoch_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 673}}
Iteration:   4140, Loss function: 3.617, Average Loss: 3.748, avg. samples / sec: 27954.21
Iteration:   4140, Loss function: 3.327, Average Loss: 3.737, avg. samples / sec: 27898.02
Iteration:   4140, Loss function: 3.701, Average Loss: 3.750, avg. samples / sec: 27889.36
Iteration:   4140, Loss function: 3.374, Average Loss: 3.750, avg. samples / sec: 27851.36
Iteration:   4160, Loss function: 3.563, Average Loss: 3.741, avg. samples / sec: 28032.71
Iteration:   4160, Loss function: 3.676, Average Loss: 3.741, avg. samples / sec: 28049.49
Iteration:   4160, Loss function: 3.848, Average Loss: 3.731, avg. samples / sec: 27995.54
Iteration:   4160, Loss function: 3.272, Average Loss: 3.741, avg. samples / sec: 27932.69
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
:::MLL 1558579524.894 eval_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.16 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.16 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.16 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.16 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.50s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=2.82s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22795
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.39192
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23310
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.06074
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24114
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.37007
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22041
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32327
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33971
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.10170
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.37100
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.52785
Current AP: 0.22795 AP goal: 0.23000
:::MLL 1558579529.439 eval_accuracy: {"value": 0.2279545481124623, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 389}}
:::MLL 1558579529.483 eval_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 392}}
:::MLL 1558579529.496 block_stop: {"value": null, "metadata": {"first_epoch_num": 50, "file": "train.py", "lineno": 804}}
:::MLL 1558579529.497 block_start: {"value": null, "metadata": {"first_epoch_num": 55, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   4180, Loss function: 3.516, Average Loss: 3.736, avg. samples / sec: 5390.28
Iteration:   4180, Loss function: 3.983, Average Loss: 3.726, avg. samples / sec: 5389.98
Iteration:   4180, Loss function: 3.658, Average Loss: 3.731, avg. samples / sec: 5390.46
Iteration:   4180, Loss function: 3.318, Average Loss: 3.730, avg. samples / sec: 5386.70
:::MLL 1558579531.255 epoch_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 819}}
:::MLL 1558579531.256 epoch_start: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 673}}
Iteration:   4200, Loss function: 3.609, Average Loss: 3.718, avg. samples / sec: 27958.10
Iteration:   4200, Loss function: 3.451, Average Loss: 3.724, avg. samples / sec: 27950.73
Iteration:   4200, Loss function: 3.305, Average Loss: 3.724, avg. samples / sec: 27987.96
Iteration:   4200, Loss function: 3.540, Average Loss: 3.730, avg. samples / sec: 27861.51
Iteration:   4220, Loss function: 3.186, Average Loss: 3.718, avg. samples / sec: 27906.61
Iteration:   4220, Loss function: 3.169, Average Loss: 3.715, avg. samples / sec: 27887.38
Iteration:   4220, Loss function: 3.905, Average Loss: 3.709, avg. samples / sec: 27874.04
Iteration:   4220, Loss function: 3.007, Average Loss: 3.720, avg. samples / sec: 27909.07
Iteration:   4240, Loss function: 2.945, Average Loss: 3.704, avg. samples / sec: 27970.99
Iteration:   4240, Loss function: 2.768, Average Loss: 3.710, avg. samples / sec: 28006.89
Iteration:   4240, Loss function: 3.500, Average Loss: 3.704, avg. samples / sec: 27939.03
Iteration:   4240, Loss function: 3.113, Average Loss: 3.711, avg. samples / sec: 27911.01
Iteration:   4260, Loss function: 2.763, Average Loss: 3.704, avg. samples / sec: 28049.98
Iteration:   4260, Loss function: 4.504, Average Loss: 3.704, avg. samples / sec: 27991.59
Iteration:   4260, Loss function: 3.461, Average Loss: 3.694, avg. samples / sec: 27964.08
Iteration:   4260, Loss function: 3.078, Average Loss: 3.697, avg. samples / sec: 27993.45
:::MLL 1558579535.488 epoch_stop: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 819}}
:::MLL 1558579535.489 epoch_start: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 673}}
Iteration:   4280, Loss function: 3.098, Average Loss: 3.689, avg. samples / sec: 27992.93
Iteration:   4280, Loss function: 3.358, Average Loss: 3.696, avg. samples / sec: 27958.42
Iteration:   4280, Loss function: 3.930, Average Loss: 3.684, avg. samples / sec: 27977.08
Iteration:   4280, Loss function: 3.735, Average Loss: 3.697, avg. samples / sec: 27889.01
Iteration:   4300, Loss function: 4.025, Average Loss: 3.678, avg. samples / sec: 28059.62
Iteration:   4300, Loss function: 3.169, Average Loss: 3.689, avg. samples / sec: 28110.10
Iteration:   4300, Loss function: 3.675, Average Loss: 3.692, avg. samples / sec: 28043.78
Iteration:   4300, Loss function: 3.036, Average Loss: 3.682, avg. samples / sec: 27987.32
Iteration:   4320, Loss function: 3.244, Average Loss: 3.678, avg. samples / sec: 28017.97
Iteration:   4320, Loss function: 3.845, Average Loss: 3.676, avg. samples / sec: 28073.40
Iteration:   4320, Loss function: 3.039, Average Loss: 3.671, avg. samples / sec: 27988.31
Iteration:   4320, Loss function: 3.473, Average Loss: 3.684, avg. samples / sec: 27971.85
Iteration:   4340, Loss function: 3.264, Average Loss: 3.666, avg. samples / sec: 27989.14
Iteration:   4340, Loss function: 3.190, Average Loss: 3.679, avg. samples / sec: 28012.14
Iteration:   4340, Loss function: 4.086, Average Loss: 3.671, avg. samples / sec: 27954.92
Iteration:   4340, Loss function: 3.542, Average Loss: 3.670, avg. samples / sec: 27941.50
:::MLL 1558579539.657 epoch_stop: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 819}}
:::MLL 1558579539.657 epoch_start: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 673}}
Iteration:   4360, Loss function: 2.964, Average Loss: 3.662, avg. samples / sec: 28011.16
Iteration:   4360, Loss function: 3.685, Average Loss: 3.662, avg. samples / sec: 28008.25
Iteration:   4360, Loss function: 3.718, Average Loss: 3.666, avg. samples / sec: 27993.12
Iteration:   4360, Loss function: 3.642, Average Loss: 3.670, avg. samples / sec: 27937.23
Iteration:   4380, Loss function: 3.237, Average Loss: 3.656, avg. samples / sec: 27977.54
Iteration:   4380, Loss function: 3.685, Average Loss: 3.657, avg. samples / sec: 27955.47
Iteration:   4380, Loss function: 2.858, Average Loss: 3.666, avg. samples / sec: 27974.05
Iteration:   4380, Loss function: 3.739, Average Loss: 3.662, avg. samples / sec: 27917.12
Iteration:   4400, Loss function: 3.384, Average Loss: 3.657, avg. samples / sec: 28137.18
Iteration:   4400, Loss function: 3.201, Average Loss: 3.652, avg. samples / sec: 28047.82
Iteration:   4400, Loss function: 4.097, Average Loss: 3.659, avg. samples / sec: 28099.70
Iteration:   4400, Loss function: 3.749, Average Loss: 3.653, avg. samples / sec: 28039.07
Iteration:   4420, Loss function: 3.050, Average Loss: 3.646, avg. samples / sec: 27905.89
Iteration:   4420, Loss function: 4.064, Average Loss: 3.653, avg. samples / sec: 27905.41
Iteration:   4420, Loss function: 3.220, Average Loss: 3.652, avg. samples / sec: 27854.96
Iteration:   4420, Loss function: 2.955, Average Loss: 3.645, avg. samples / sec: 27842.98
:::MLL 1558579543.888 epoch_stop: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 819}}
:::MLL 1558579543.888 epoch_start: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 673}}
Iteration:   4440, Loss function: 3.042, Average Loss: 3.639, avg. samples / sec: 27933.84
Iteration:   4440, Loss function: 3.074, Average Loss: 3.648, avg. samples / sec: 27932.31
Iteration:   4440, Loss function: 3.238, Average Loss: 3.649, avg. samples / sec: 27956.19
Iteration:   4440, Loss function: 2.859, Average Loss: 3.640, avg. samples / sec: 27952.52
Iteration:   4460, Loss function: 3.135, Average Loss: 3.634, avg. samples / sec: 28054.62
Iteration:   4460, Loss function: 3.181, Average Loss: 3.634, avg. samples / sec: 28083.05
Iteration:   4460, Loss function: 3.248, Average Loss: 3.644, avg. samples / sec: 28048.98
Iteration:   4460, Loss function: 3.565, Average Loss: 3.644, avg. samples / sec: 28045.32
Iteration:   4480, Loss function: 2.824, Average Loss: 3.638, avg. samples / sec: 28084.35
Iteration:   4480, Loss function: 3.378, Average Loss: 3.626, avg. samples / sec: 28071.01
Iteration:   4480, Loss function: 3.178, Average Loss: 3.626, avg. samples / sec: 28064.93
Iteration:   4480, Loss function: 3.113, Average Loss: 3.637, avg. samples / sec: 28082.23
Iteration:   4500, Loss function: 3.430, Average Loss: 3.630, avg. samples / sec: 28033.20
Iteration:   4500, Loss function: 3.069, Average Loss: 3.632, avg. samples / sec: 28004.74
Iteration:   4500, Loss function: 3.600, Average Loss: 3.622, avg. samples / sec: 27994.69
Iteration:   4500, Loss function: 3.197, Average Loss: 3.620, avg. samples / sec: 27968.92
:::MLL 1558579548.055 epoch_stop: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 819}}
:::MLL 1558579548.055 epoch_start: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 673}}
Iteration:   4520, Loss function: 3.580, Average Loss: 3.618, avg. samples / sec: 27967.41
Iteration:   4520, Loss function: 3.474, Average Loss: 3.624, avg. samples / sec: 27929.26
Iteration:   4520, Loss function: 3.148, Average Loss: 3.624, avg. samples / sec: 27940.68
Iteration:   4520, Loss function: 3.816, Average Loss: 3.616, avg. samples / sec: 27927.62
Iteration:   4540, Loss function: 3.884, Average Loss: 3.618, avg. samples / sec: 27991.76
Iteration:   4540, Loss function: 2.818, Average Loss: 3.611, avg. samples / sec: 27980.69
Iteration:   4540, Loss function: 3.248, Average Loss: 3.610, avg. samples / sec: 28034.98
Iteration:   4540, Loss function: 3.402, Average Loss: 3.619, avg. samples / sec: 27955.81
Iteration:   4560, Loss function: 3.157, Average Loss: 3.603, avg. samples / sec: 28011.06
Iteration:   4560, Loss function: 3.026, Average Loss: 3.607, avg. samples / sec: 28021.93
Iteration:   4560, Loss function: 3.084, Average Loss: 3.612, avg. samples / sec: 28026.94
Iteration:   4560, Loss function: 3.048, Average Loss: 3.611, avg. samples / sec: 27965.15
Iteration:   4580, Loss function: 3.832, Average Loss: 3.597, avg. samples / sec: 28103.56
Iteration:   4580, Loss function: 3.250, Average Loss: 3.602, avg. samples / sec: 28095.90
Iteration:   4580, Loss function: 3.965, Average Loss: 3.605, avg. samples / sec: 28105.22
Iteration:   4580, Loss function: 3.104, Average Loss: 3.605, avg. samples / sec: 28044.58
:::MLL 1558579552.220 epoch_stop: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 819}}
:::MLL 1558579552.220 epoch_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 673}}
:::MLL 1558579552.387 eval_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.15 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.15 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.15 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.15 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.53s)
DONE (t=2.85s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.23169
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.39683
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23589
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.06155
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24429
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.37409
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22333
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32730
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.34357
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.10408
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.37517
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.53031
Current AP: 0.23169 AP goal: 0.23000
:::MLL 1558579556.970 eval_accuracy: {"value": 0.23169059778833898, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 389}}
:::MLL 1558579556.970 eval_stop: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 392}}
:::MLL 1558579556.984 block_stop: {"value": null, "metadata": {"first_epoch_num": 55, "file": "train.py", "lineno": 804}}
:::MLL 1558579557.900 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 849}}

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-05-23 02:46:07 AM
RESULT,SINGLE_STAGE_DETECTOR,,351,nvidia,2019-05-23 02:40:16 AM
ENDING TIMING RUN AT 2019-05-23 02:46:07 AM
RESULT,SINGLE_STAGE_DETECTOR,,351,nvidia,2019-05-23 02:40:16 AM
ENDING TIMING RUN AT 2019-05-23 02:46:07 AM
RESULT,SINGLE_STAGE_DETECTOR,,351,nvidia,2019-05-23 02:40:16 AM
ENDING TIMING RUN AT 2019-05-23 02:46:07 AM
RESULT,SINGLE_STAGE_DETECTOR,,351,nvidia,2019-05-23 02:40:16 AM
