Beginning trial 4 of 5
Gathering sys log on circe-n051
:::MLL 1558579568.843 submission_benchmark: {"value": "ssd", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1558579568.843 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known ssd keys.
:::MLL 1558579568.843 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1558579568.844 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1558579568.844 submission_platform: {"value": "4xNVIDIA DGX-2H", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1558579568.845 submission_entry: {"value": "{'hardware': 'NVIDIA DGX-2H', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': 'Ubuntu 18.04.1 LTS / NVIDIA DGX Server 4.0.4 4.0.5', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.4-2.0.7', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '4', 'cpu': '2x Intel(R) Xeon(R) Platinum 8174 CPU @ 3.10GHz', 'num_cores': '48', 'num_vcpus': '96', 'accelerator': 'Tesla V100-SXM3-32GB-H', 'num_accelerators': '16', 'sys_mem_size': '1510 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '2x 894.3G + 8x 3.5T', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27800 Family [ConnectX-5]', 'num_network_cards': '10', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1558579568.845 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1558579568.845 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
:::MLL 1558579571.674 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558579571.682 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558579571.721 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558579571.737 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node circe-n051
+ pids+=($!)
+ set +x
Launching on node circe-n052
+ pids+=($!)
+ set +x
Launching on node circe-n053
+ pids+=($!)
+ set +x
Launching on node circe-n054
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n051
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n052
+ pids+=($!)
+ set +x
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n053
+ srun --mem=0 -N 1 -n 1 -w circe-n051 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=0 --master_addr=10.0.1.51 --master_port=4728' -e SLURM_JOB_ID=89451 -e SLURM_NTASKS_PER_NODE=16 cont_89451 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w circe-n052 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=1 --master_addr=10.0.1.51 --master_port=4728' -e SLURM_JOB_ID=89451 -e SLURM_NTASKS_PER_NODE=16 cont_89451 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ srun --mem=0 -N 1 -n 1 -w circe-n053 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=2 --master_addr=10.0.1.51 --master_port=4728' -e SLURM_JOB_ID=89451 -e SLURM_NTASKS_PER_NODE=16 cont_89451 ./run_and_time.sh
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n054
+ srun --mem=0 -N 1 -n 1 -w circe-n054 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=3 --master_addr=10.0.1.51 --master_port=4728' -e SLURM_JOB_ID=89451 -e SLURM_NTASKS_PER_NODE=16 cont_89451 ./run_and_time.sh
Run vars: id 89451 gpus 16 mparams  --nnodes=4 --node_rank=0 --master_addr=10.0.1.51 --master_port=4728
Run vars: id 89451 gpus 16 mparams  --nnodes=4 --node_rank=2 --master_addr=10.0.1.51 --master_port=4728
Run vars: id 89451 gpus 16 mparams  --nnodes=4 --node_rank=3 --master_addr=10.0.1.51 --master_port=4728
Run vars: id 89451 gpus 16 mparams  --nnodes=4 --node_rank=1 --master_addr=10.0.1.51 --master_port=4728
STARTING TIMING RUN AT 2019-05-23 02:46:11 AM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=0 --master_addr=10.0.1.51 --master_port=4728 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 02:46:11 AM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=2 --master_addr=10.0.1.51 --master_port=4728 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 02:46:11 AM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=3 --master_addr=10.0.1.51 --master_port=4728 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 02:46:11 AM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=1 --master_addr=10.0.1.51 --master_port=4728 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558579582.141 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.141 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.141 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.141 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.141 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.141 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.142 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.142 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.142 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.142 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.142 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.142 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.143 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.143 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558579582.143 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579582.144 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558579582.294 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.295 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558579582.297 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.297 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.298 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.298 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.298 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.298 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.298 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.298 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.298 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.298 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558579582.299 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.299 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.300 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558579582.300 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558579582.373 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.374 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558579582.379 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.379 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.379 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.379 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.379 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.380 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.380 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.380 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579582.380 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579582.381 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579582.381 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.381 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.381 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558579582.382 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558579582.416 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.416 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.417 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.417 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.417 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.417 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558579582.418 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579582.418 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579582.419 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558579582.419 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.419 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.419 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.419 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579582.420 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.420 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579582.420 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
0 Using seed = 2636207995
1 Using seed = 2636207996
3 Using seed = 2636207998
4 Using seed = 2636207999
5 Using seed = 2636208000
2 Using seed = 2636207997
:::MLL 1558579614.352 max_samples: {"value": 1, "metadata": {"file": "utils.py", "lineno": 465}}
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
:::MLL 1558579615.044 model_bn_span: {"value": 24, "metadata": {"file": "train.py", "lineno": 480}}
:::MLL 1558579615.044 global_batch_size: {"value": 1536, "metadata": {"file": "train.py", "lineno": 481}}
:::MLL 1558579615.100 opt_base_learning_rate: {"value": 0.14, "metadata": {"file": "train.py", "lineno": 511}}
:::MLL 1558579615.100 opt_weight_decay: {"value": 0.00017, "metadata": {"file": "train.py", "lineno": 513}}
:::MLL 1558579615.100 opt_learning_rate_warmup_steps: {"value": 850, "metadata": {"file": "train.py", "lineno": 516}}
:::MLL 1558579615.100 opt_learning_rate_warmup_factor: {"value": 0, "metadata": {"file": "train.py", "lineno": 518}}
9 Using seed = 2636208004
11 Using seed = 2636208006
8 Using seed = 2636208003
7 Using seed = 2636208002
6 Using seed = 2636208001
22 Using seed = 2636208017
26 Using seed = 2636208021
31 Using seed = 2636208026
30 Using seed = 2636208025
28 Using seed = 2636208023
27 Using seed = 2636208022
17 Using seed = 2636208012
20 Using seed = 2636208015
19 Using seed = 2636208014
29 Using seed = 2636208024
25 Using seed = 2636208020
16 Using seed = 2636208011
18 Using seed = 2636208013
24 Using seed = 2636208019
23 Using seed = 2636208018
21 Using seed = 2636208016
38 Using seed = 2636208033
39 Using seed = 2636208034
47 Using seed = 2636208042
32 Using seed = 2636208027
41 Using seed = 2636208036
37 Using seed = 2636208032
44 Using seed = 2636208039
46 Using seed = 2636208041
42 Using seed = 2636208037
43 Using seed = 2636208038
40 Using seed = 2636208035
36 Using seed = 2636208031
34 Using seed = 2636208029
35 Using seed = 2636208030
45 Using seed = 2636208040
33 Using seed = 2636208028
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
58 Using seed = 2636208053
62 Using seed = 2636208057
63 Using seed = 2636208058
61 Using seed = 2636208056
52 Using seed = 2636208047
48 Using seed = 2636208043
51 Using seed = 2636208046
53 Using seed = 2636208048
50 Using seed = 2636208045
49 Using seed = 2636208044
60 Using seed = 2636208055
59 Using seed = 2636208054
57 Using seed = 2636208052
55 Using seed = 2636208050
56 Using seed = 2636208051
54 Using seed = 2636208049
12 Using seed = 2636208007
15 Using seed = 2636208010
14 Using seed = 2636208009
13 Using seed = 2636208008
10 Using seed = 2636208005
epoch nbatch loss
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
:::MLL 1558579626.374 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 604}}
:::MLL 1558579626.375 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 610}}
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
time_check a: 1558579628.089945555
time_check a: 1558579628.091577291
time_check a: 1558579628.090866089
time_check a: 1558579628.093186140
time_check b: 1558579634.868403196
time_check b: 1558579634.960251808
time_check b: 1558579635.013458014
time_check b: 1558579635.033850908
:::MLL 1558579636.400 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 32.74606450292497, "file": "train.py", "lineno": 669}}
:::MLL 1558579636.401 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 673}}
Iteration:      0, Loss function: 22.497, Average Loss: 0.022, avg. samples / sec: 97.06
Iteration:      0, Loss function: 22.476, Average Loss: 0.022, avg. samples / sec: 77.66
Iteration:      0, Loss function: 22.728, Average Loss: 0.023, avg. samples / sec: 99.16
Iteration:      0, Loss function: 22.946, Average Loss: 0.023, avg. samples / sec: 100.53
Iteration:     20, Loss function: 20.592, Average Loss: 0.445, avg. samples / sec: 21564.82
Iteration:     20, Loss function: 20.200, Average Loss: 0.442, avg. samples / sec: 21085.13
Iteration:     20, Loss function: 20.443, Average Loss: 0.441, avg. samples / sec: 21469.89
Iteration:     20, Loss function: 20.375, Average Loss: 0.441, avg. samples / sec: 21287.40
Iteration:     40, Loss function: 16.708, Average Loss: 0.818, avg. samples / sec: 27471.99
Iteration:     40, Loss function: 16.366, Average Loss: 0.817, avg. samples / sec: 27419.03
Iteration:     40, Loss function: 16.410, Average Loss: 0.817, avg. samples / sec: 27476.18
Iteration:     40, Loss function: 16.551, Average Loss: 0.820, avg. samples / sec: 27355.86
Iteration:     60, Loss function: 11.587, Average Loss: 1.060, avg. samples / sec: 28255.73
Iteration:     60, Loss function: 10.874, Average Loss: 1.056, avg. samples / sec: 28250.81
Iteration:     60, Loss function: 12.583, Average Loss: 1.062, avg. samples / sec: 28307.05
Iteration:     60, Loss function: 12.240, Average Loss: 1.055, avg. samples / sec: 28238.75
:::MLL 1558579642.169 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 819}}
:::MLL 1558579642.169 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 673}}
Iteration:     80, Loss function: 10.083, Average Loss: 1.261, avg. samples / sec: 28252.62
Iteration:     80, Loss function: 10.297, Average Loss: 1.258, avg. samples / sec: 28247.31
Iteration:     80, Loss function: 10.835, Average Loss: 1.264, avg. samples / sec: 28242.91
Iteration:     80, Loss function: 10.554, Average Loss: 1.263, avg. samples / sec: 28212.50
Iteration:    100, Loss function: 9.196, Average Loss: 1.430, avg. samples / sec: 28437.37
Iteration:    100, Loss function: 9.826, Average Loss: 1.431, avg. samples / sec: 28458.28
Iteration:    100, Loss function: 9.533, Average Loss: 1.432, avg. samples / sec: 28435.98
Iteration:    100, Loss function: 9.569, Average Loss: 1.428, avg. samples / sec: 28417.03
Iteration:    120, Loss function: 8.768, Average Loss: 1.582, avg. samples / sec: 28354.75
Iteration:    120, Loss function: 9.006, Average Loss: 1.581, avg. samples / sec: 28339.18
Iteration:    120, Loss function: 9.086, Average Loss: 1.582, avg. samples / sec: 28336.28
Iteration:    120, Loss function: 8.692, Average Loss: 1.580, avg. samples / sec: 28345.40
Iteration:    140, Loss function: 8.997, Average Loss: 1.732, avg. samples / sec: 28491.16
Iteration:    140, Loss function: 8.788, Average Loss: 1.729, avg. samples / sec: 28497.18
Iteration:    140, Loss function: 8.570, Average Loss: 1.726, avg. samples / sec: 28471.07
Iteration:    140, Loss function: 8.515, Average Loss: 1.729, avg. samples / sec: 28415.61
:::MLL 1558579646.284 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 819}}
:::MLL 1558579646.285 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 673}}
Iteration:    160, Loss function: 8.329, Average Loss: 1.870, avg. samples / sec: 28289.83
Iteration:    160, Loss function: 8.652, Average Loss: 1.868, avg. samples / sec: 28348.32
Iteration:    160, Loss function: 7.427, Average Loss: 1.861, avg. samples / sec: 28296.64
Iteration:    160, Loss function: 8.707, Average Loss: 1.864, avg. samples / sec: 28274.75
Iteration:    180, Loss function: 8.294, Average Loss: 1.992, avg. samples / sec: 28423.50
Iteration:    180, Loss function: 8.249, Average Loss: 1.999, avg. samples / sec: 28415.49
Iteration:    180, Loss function: 8.226, Average Loss: 1.994, avg. samples / sec: 28414.28
Iteration:    180, Loss function: 8.260, Average Loss: 1.989, avg. samples / sec: 28390.24
Iteration:    200, Loss function: 8.469, Average Loss: 2.118, avg. samples / sec: 28380.64
Iteration:    200, Loss function: 8.467, Average Loss: 2.122, avg. samples / sec: 28324.88
Iteration:    200, Loss function: 8.058, Average Loss: 2.126, avg. samples / sec: 28317.96
Iteration:    200, Loss function: 8.510, Average Loss: 2.124, avg. samples / sec: 28313.12
Iteration:    220, Loss function: 7.631, Average Loss: 2.238, avg. samples / sec: 28281.25
Iteration:    220, Loss function: 7.972, Average Loss: 2.241, avg. samples / sec: 28291.37
Iteration:    220, Loss function: 7.411, Average Loss: 2.234, avg. samples / sec: 28253.87
Iteration:    220, Loss function: 8.292, Average Loss: 2.244, avg. samples / sec: 28272.43
:::MLL 1558579650.460 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 819}}
:::MLL 1558579650.461 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 673}}
Iteration:    240, Loss function: 7.622, Average Loss: 2.349, avg. samples / sec: 28338.58
Iteration:    240, Loss function: 8.175, Average Loss: 2.355, avg. samples / sec: 28344.87
Iteration:    240, Loss function: 7.569, Average Loss: 2.352, avg. samples / sec: 28329.32
Iteration:    240, Loss function: 7.918, Average Loss: 2.345, avg. samples / sec: 28323.95
Iteration:    260, Loss function: 7.391, Average Loss: 2.457, avg. samples / sec: 28445.77
Iteration:    260, Loss function: 7.606, Average Loss: 2.459, avg. samples / sec: 28458.77
Iteration:    260, Loss function: 7.676, Average Loss: 2.451, avg. samples / sec: 28460.73
Iteration:    260, Loss function: 7.713, Average Loss: 2.463, avg. samples / sec: 28393.28
Iteration:    280, Loss function: 7.683, Average Loss: 2.551, avg. samples / sec: 28452.83
Iteration:    280, Loss function: 8.391, Average Loss: 2.558, avg. samples / sec: 28450.75
Iteration:    280, Loss function: 8.263, Average Loss: 2.560, avg. samples / sec: 28431.13
Iteration:    280, Loss function: 7.849, Average Loss: 2.563, avg. samples / sec: 28493.16
Iteration:    300, Loss function: 7.369, Average Loss: 2.648, avg. samples / sec: 28454.87
Iteration:    300, Loss function: 7.151, Average Loss: 2.659, avg. samples / sec: 28469.23
Iteration:    300, Loss function: 6.663, Average Loss: 2.653, avg. samples / sec: 28450.59
Iteration:    300, Loss function: 7.030, Average Loss: 2.656, avg. samples / sec: 28421.14
:::MLL 1558579654.566 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 819}}
:::MLL 1558579654.566 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 673}}
Iteration:    320, Loss function: 7.405, Average Loss: 2.751, avg. samples / sec: 28325.46
Iteration:    320, Loss function: 6.954, Average Loss: 2.749, avg. samples / sec: 28360.75
Iteration:    320, Loss function: 7.255, Average Loss: 2.744, avg. samples / sec: 28315.96
Iteration:    320, Loss function: 7.080, Average Loss: 2.738, avg. samples / sec: 28311.65
Iteration:    340, Loss function: 7.341, Average Loss: 2.826, avg. samples / sec: 28399.72
Iteration:    340, Loss function: 7.509, Average Loss: 2.835, avg. samples / sec: 28381.65
Iteration:    340, Loss function: 7.425, Average Loss: 2.832, avg. samples / sec: 28383.72
Iteration:    340, Loss function: 7.401, Average Loss: 2.836, avg. samples / sec: 28345.48
Iteration:    360, Loss function: 6.654, Average Loss: 2.919, avg. samples / sec: 28473.88
Iteration:    360, Loss function: 7.279, Average Loss: 2.917, avg. samples / sec: 28415.99
Iteration:    360, Loss function: 6.851, Average Loss: 2.907, avg. samples / sec: 28365.15
Iteration:    360, Loss function: 6.780, Average Loss: 2.914, avg. samples / sec: 28378.43
Iteration:    380, Loss function: 7.137, Average Loss: 3.000, avg. samples / sec: 28374.48
Iteration:    380, Loss function: 7.255, Average Loss: 2.992, avg. samples / sec: 28355.69
Iteration:    380, Loss function: 6.885, Average Loss: 3.004, avg. samples / sec: 28301.92
Iteration:    380, Loss function: 7.026, Average Loss: 2.999, avg. samples / sec: 28277.55
:::MLL 1558579658.682 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 819}}
:::MLL 1558579658.683 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 673}}
Iteration:    400, Loss function: 7.001, Average Loss: 3.076, avg. samples / sec: 28409.97
Iteration:    400, Loss function: 7.060, Average Loss: 3.084, avg. samples / sec: 28367.67
Iteration:    400, Loss function: 6.330, Average Loss: 3.070, avg. samples / sec: 28361.54
Iteration:    400, Loss function: 7.383, Average Loss: 3.079, avg. samples / sec: 28331.24
Iteration:    420, Loss function: 5.845, Average Loss: 3.143, avg. samples / sec: 28513.42
Iteration:    420, Loss function: 6.919, Average Loss: 3.152, avg. samples / sec: 28512.48
Iteration:    420, Loss function: 6.371, Average Loss: 3.142, avg. samples / sec: 28513.38
Iteration:    420, Loss function: 6.931, Average Loss: 3.149, avg. samples / sec: 28520.04
Iteration:    440, Loss function: 6.443, Average Loss: 3.216, avg. samples / sec: 28330.58
Iteration:    440, Loss function: 6.537, Average Loss: 3.222, avg. samples / sec: 28337.06
Iteration:    440, Loss function: 6.569, Average Loss: 3.215, avg. samples / sec: 28313.12
Iteration:    440, Loss function: 6.641, Average Loss: 3.225, avg. samples / sec: 28295.26
:::MLL 1558579662.848 epoch_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 819}}
:::MLL 1558579662.848 epoch_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 673}}
Iteration:    460, Loss function: 6.472, Average Loss: 3.287, avg. samples / sec: 28369.39
Iteration:    460, Loss function: 6.518, Average Loss: 3.278, avg. samples / sec: 28360.65
Iteration:    460, Loss function: 6.642, Average Loss: 3.278, avg. samples / sec: 28358.07
Iteration:    460, Loss function: 6.148, Average Loss: 3.293, avg. samples / sec: 28350.31
Iteration:    480, Loss function: 5.805, Average Loss: 3.352, avg. samples / sec: 28413.24
Iteration:    480, Loss function: 6.154, Average Loss: 3.338, avg. samples / sec: 28372.42
Iteration:    480, Loss function: 6.311, Average Loss: 3.347, avg. samples / sec: 28343.38
Iteration:    480, Loss function: 5.898, Average Loss: 3.337, avg. samples / sec: 28341.55
Iteration:    500, Loss function: 6.156, Average Loss: 3.396, avg. samples / sec: 28325.06
Iteration:    500, Loss function: 6.685, Average Loss: 3.412, avg. samples / sec: 28273.92
Iteration:    500, Loss function: 7.071, Average Loss: 3.409, avg. samples / sec: 28290.88
Iteration:    500, Loss function: 5.677, Average Loss: 3.398, avg. samples / sec: 28237.68
Iteration:    520, Loss function: 6.129, Average Loss: 3.451, avg. samples / sec: 28368.62
Iteration:    520, Loss function: 6.334, Average Loss: 3.467, avg. samples / sec: 28372.26
Iteration:    520, Loss function: 6.816, Average Loss: 3.464, avg. samples / sec: 28379.58
Iteration:    520, Loss function: 6.687, Average Loss: 3.454, avg. samples / sec: 28411.66
:::MLL 1558579666.968 epoch_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 819}}
:::MLL 1558579666.969 epoch_start: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 673}}
Iteration:    540, Loss function: 6.108, Average Loss: 3.521, avg. samples / sec: 28308.89
Iteration:    540, Loss function: 5.811, Average Loss: 3.508, avg. samples / sec: 28311.72
Iteration:    540, Loss function: 5.618, Average Loss: 3.516, avg. samples / sec: 28308.44
Iteration:    540, Loss function: 6.113, Average Loss: 3.506, avg. samples / sec: 28294.65
Iteration:    560, Loss function: 6.044, Average Loss: 3.569, avg. samples / sec: 28413.86
Iteration:    560, Loss function: 5.557, Average Loss: 3.557, avg. samples / sec: 28410.60
Iteration:    560, Loss function: 5.393, Average Loss: 3.565, avg. samples / sec: 28401.92
Iteration:    560, Loss function: 5.762, Average Loss: 3.555, avg. samples / sec: 28393.35
Iteration:    580, Loss function: 6.060, Average Loss: 3.619, avg. samples / sec: 28505.01
Iteration:    580, Loss function: 5.963, Average Loss: 3.607, avg. samples / sec: 28524.92
Iteration:    580, Loss function: 5.959, Average Loss: 3.613, avg. samples / sec: 28507.45
Iteration:    580, Loss function: 5.949, Average Loss: 3.608, avg. samples / sec: 28493.07
Iteration:    600, Loss function: 5.645, Average Loss: 3.658, avg. samples / sec: 28480.59
Iteration:    600, Loss function: 5.409, Average Loss: 3.655, avg. samples / sec: 28466.26
Iteration:    600, Loss function: 5.621, Average Loss: 3.654, avg. samples / sec: 28431.00
Iteration:    600, Loss function: 4.869, Average Loss: 3.664, avg. samples / sec: 28402.21
:::MLL 1558579671.074 epoch_stop: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 819}}
:::MLL 1558579671.075 epoch_start: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 673}}
Iteration:    620, Loss function: 5.695, Average Loss: 3.706, avg. samples / sec: 28354.11
Iteration:    620, Loss function: 5.901, Average Loss: 3.702, avg. samples / sec: 28281.33
Iteration:    620, Loss function: 5.746, Average Loss: 3.695, avg. samples / sec: 28329.62
Iteration:    620, Loss function: 5.550, Average Loss: 3.699, avg. samples / sec: 28284.41
Iteration:    640, Loss function: 6.274, Average Loss: 3.744, avg. samples / sec: 28427.34
Iteration:    640, Loss function: 6.151, Average Loss: 3.737, avg. samples / sec: 28419.06
Iteration:    640, Loss function: 5.968, Average Loss: 3.749, avg. samples / sec: 28409.46
Iteration:    640, Loss function: 6.801, Average Loss: 3.749, avg. samples / sec: 28376.63
Iteration:    660, Loss function: 5.151, Average Loss: 3.785, avg. samples / sec: 28405.12
Iteration:    660, Loss function: 5.404, Average Loss: 3.785, avg. samples / sec: 28440.96
Iteration:    660, Loss function: 5.490, Average Loss: 3.776, avg. samples / sec: 28393.17
Iteration:    660, Loss function: 5.412, Average Loss: 3.783, avg. samples / sec: 28371.19
Iteration:    680, Loss function: 5.991, Average Loss: 3.826, avg. samples / sec: 28388.24
Iteration:    680, Loss function: 6.274, Average Loss: 3.823, avg. samples / sec: 28374.75
Iteration:    680, Loss function: 5.977, Average Loss: 3.822, avg. samples / sec: 28387.62
Iteration:    680, Loss function: 5.404, Average Loss: 3.819, avg. samples / sec: 28359.88
:::MLL 1558579675.243 epoch_stop: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 819}}
:::MLL 1558579675.244 epoch_start: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 673}}
Iteration:    700, Loss function: 5.584, Average Loss: 3.857, avg. samples / sec: 28337.16
Iteration:    700, Loss function: 5.623, Average Loss: 3.864, avg. samples / sec: 28304.56
Iteration:    700, Loss function: 6.012, Average Loss: 3.857, avg. samples / sec: 28334.20
Iteration:    700, Loss function: 5.540, Average Loss: 3.865, avg. samples / sec: 28289.03
Iteration:    720, Loss function: 5.559, Average Loss: 3.896, avg. samples / sec: 28477.21
Iteration:    720, Loss function: 5.032, Average Loss: 3.894, avg. samples / sec: 28469.67
Iteration:    720, Loss function: 5.602, Average Loss: 3.890, avg. samples / sec: 28449.06
Iteration:    720, Loss function: 5.611, Average Loss: 3.888, avg. samples / sec: 28386.97
Iteration:    740, Loss function: 5.239, Average Loss: 3.918, avg. samples / sec: 28417.23
Iteration:    740, Loss function: 5.062, Average Loss: 3.921, avg. samples / sec: 28358.19
Iteration:    740, Loss function: 5.418, Average Loss: 3.923, avg. samples / sec: 28341.62
Iteration:    740, Loss function: 5.387, Average Loss: 3.926, avg. samples / sec: 28293.69
Iteration:    760, Loss function: 5.342, Average Loss: 3.955, avg. samples / sec: 28516.86
Iteration:    760, Loss function: 6.719, Average Loss: 3.951, avg. samples / sec: 28494.10
Iteration:    760, Loss function: 5.291, Average Loss: 3.956, avg. samples / sec: 28535.41
Iteration:    760, Loss function: 5.479, Average Loss: 3.952, avg. samples / sec: 28428.01
:::MLL 1558579679.353 epoch_stop: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 819}}
:::MLL 1558579679.353 epoch_start: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 673}}
Iteration:    780, Loss function: 5.408, Average Loss: 3.984, avg. samples / sec: 28325.23
Iteration:    780, Loss function: 5.579, Average Loss: 3.989, avg. samples / sec: 28309.95
Iteration:    780, Loss function: 5.581, Average Loss: 3.985, avg. samples / sec: 28387.26
Iteration:    780, Loss function: 5.144, Average Loss: 3.987, avg. samples / sec: 28306.58
Iteration:    800, Loss function: 5.401, Average Loss: 4.017, avg. samples / sec: 28375.45
Iteration:    800, Loss function: 5.897, Average Loss: 4.013, avg. samples / sec: 28346.57
Iteration:    800, Loss function: 4.901, Average Loss: 4.016, avg. samples / sec: 28341.36
Iteration:    800, Loss function: 4.940, Average Loss: 4.013, avg. samples / sec: 28309.72
Iteration:    820, Loss function: 5.374, Average Loss: 4.040, avg. samples / sec: 28358.19
Iteration:    820, Loss function: 4.882, Average Loss: 4.036, avg. samples / sec: 28391.98
Iteration:    820, Loss function: 5.576, Average Loss: 4.040, avg. samples / sec: 28334.66
Iteration:    820, Loss function: 5.020, Average Loss: 4.039, avg. samples / sec: 28330.83
:::MLL 1558579683.476 epoch_stop: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 819}}
:::MLL 1558579683.476 epoch_start: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 673}}
Iteration:    840, Loss function: 5.205, Average Loss: 4.068, avg. samples / sec: 28219.64
Iteration:    840, Loss function: 5.783, Average Loss: 4.067, avg. samples / sec: 28227.13
Iteration:    840, Loss function: 6.108, Average Loss: 4.063, avg. samples / sec: 28210.70
Iteration:    840, Loss function: 5.586, Average Loss: 4.068, avg. samples / sec: 28198.12
Iteration:    860, Loss function: 5.178, Average Loss: 4.089, avg. samples / sec: 28414.66
Iteration:    860, Loss function: 4.723, Average Loss: 4.090, avg. samples / sec: 28390.54
Iteration:    860, Loss function: 5.516, Average Loss: 4.092, avg. samples / sec: 28384.62
Iteration:    860, Loss function: 5.590, Average Loss: 4.091, avg. samples / sec: 28394.34
Iteration:    880, Loss function: 4.982, Average Loss: 4.115, avg. samples / sec: 28492.69
Iteration:    880, Loss function: 4.783, Average Loss: 4.115, avg. samples / sec: 28453.37
Iteration:    880, Loss function: 6.277, Average Loss: 4.113, avg. samples / sec: 28457.87
Iteration:    880, Loss function: 5.053, Average Loss: 4.114, avg. samples / sec: 28469.47
Iteration:    900, Loss function: 6.201, Average Loss: 4.134, avg. samples / sec: 28429.68
Iteration:    900, Loss function: 5.226, Average Loss: 4.133, avg. samples / sec: 28418.48
Iteration:    900, Loss function: 5.091, Average Loss: 4.136, avg. samples / sec: 28414.71
Iteration:    900, Loss function: 4.868, Average Loss: 4.136, avg. samples / sec: 28360.53
:::MLL 1558579687.641 epoch_stop: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 819}}
:::MLL 1558579687.642 epoch_start: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 673}}
Iteration:    920, Loss function: 4.773, Average Loss: 4.151, avg. samples / sec: 28337.26
Iteration:    920, Loss function: 5.456, Average Loss: 4.153, avg. samples / sec: 28347.31
Iteration:    920, Loss function: 4.773, Average Loss: 4.150, avg. samples / sec: 28336.93
Iteration:    920, Loss function: 5.193, Average Loss: 4.155, avg. samples / sec: 28382.49
Iteration:    940, Loss function: 5.357, Average Loss: 4.168, avg. samples / sec: 28397.86
Iteration:    940, Loss function: 5.049, Average Loss: 4.166, avg. samples / sec: 28392.69
Iteration:    940, Loss function: 5.008, Average Loss: 4.171, avg. samples / sec: 28402.71
Iteration:    940, Loss function: 5.018, Average Loss: 4.174, avg. samples / sec: 28381.45
Iteration:    960, Loss function: 5.027, Average Loss: 4.182, avg. samples / sec: 28316.85
Iteration:    960, Loss function: 5.075, Average Loss: 4.186, avg. samples / sec: 28308.75
Iteration:    960, Loss function: 5.371, Average Loss: 4.189, avg. samples / sec: 28311.78
Iteration:    960, Loss function: 4.968, Average Loss: 4.193, avg. samples / sec: 28298.02
Iteration:    980, Loss function: 5.306, Average Loss: 4.212, avg. samples / sec: 28454.04
Iteration:    980, Loss function: 5.950, Average Loss: 4.204, avg. samples / sec: 28434.17
Iteration:    980, Loss function: 4.560, Average Loss: 4.212, avg. samples / sec: 28453.87
Iteration:    980, Loss function: 4.306, Average Loss: 4.204, avg. samples / sec: 28408.55
:::MLL 1558579691.755 epoch_stop: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 819}}
:::MLL 1558579691.755 epoch_start: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 673}}
Iteration:   1000, Loss function: 4.883, Average Loss: 4.221, avg. samples / sec: 28422.69
Iteration:   1000, Loss function: 5.581, Average Loss: 4.229, avg. samples / sec: 28378.80
Iteration:   1000, Loss function: 5.636, Average Loss: 4.219, avg. samples / sec: 28392.80
Iteration:   1000, Loss function: 4.705, Average Loss: 4.227, avg. samples / sec: 28386.37
Iteration:   1020, Loss function: 5.367, Average Loss: 4.239, avg. samples / sec: 28443.64
Iteration:   1020, Loss function: 4.919, Average Loss: 4.241, avg. samples / sec: 28429.01
Iteration:   1020, Loss function: 5.500, Average Loss: 4.236, avg. samples / sec: 28411.54
Iteration:   1020, Loss function: 4.718, Average Loss: 4.233, avg. samples / sec: 28406.93
Iteration:   1040, Loss function: 5.104, Average Loss: 4.248, avg. samples / sec: 28514.36
Iteration:   1040, Loss function: 4.260, Average Loss: 4.253, avg. samples / sec: 28495.46
Iteration:   1040, Loss function: 4.143, Average Loss: 4.253, avg. samples / sec: 28496.00
Iteration:   1040, Loss function: 5.306, Average Loss: 4.246, avg. samples / sec: 28491.15
Iteration:   1060, Loss function: 4.700, Average Loss: 4.258, avg. samples / sec: 28465.71
Iteration:   1060, Loss function: 4.735, Average Loss: 4.266, avg. samples / sec: 28458.51
Iteration:   1060, Loss function: 4.600, Average Loss: 4.258, avg. samples / sec: 28494.74
Iteration:   1060, Loss function: 5.102, Average Loss: 4.261, avg. samples / sec: 28403.80
:::MLL 1558579695.861 epoch_stop: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 819}}
:::MLL 1558579695.861 epoch_start: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 673}}
Iteration:   1080, Loss function: 4.605, Average Loss: 4.277, avg. samples / sec: 28305.94
Iteration:   1080, Loss function: 4.748, Average Loss: 4.267, avg. samples / sec: 28286.68
Iteration:   1080, Loss function: 4.485, Average Loss: 4.269, avg. samples / sec: 28288.55
Iteration:   1080, Loss function: 5.293, Average Loss: 4.271, avg. samples / sec: 28343.55
Iteration:   1100, Loss function: 4.753, Average Loss: 4.278, avg. samples / sec: 28304.56
Iteration:   1100, Loss function: 5.232, Average Loss: 4.284, avg. samples / sec: 28307.91
Iteration:   1100, Loss function: 4.348, Average Loss: 4.282, avg. samples / sec: 28300.65
Iteration:   1100, Loss function: 5.142, Average Loss: 4.291, avg. samples / sec: 28251.59
Iteration:   1120, Loss function: 4.296, Average Loss: 4.289, avg. samples / sec: 28475.74
Iteration:   1120, Loss function: 4.449, Average Loss: 4.300, avg. samples / sec: 28514.39
Iteration:   1120, Loss function: 4.993, Average Loss: 4.297, avg. samples / sec: 28469.50
Iteration:   1120, Loss function: 5.149, Average Loss: 4.292, avg. samples / sec: 28456.09
Iteration:   1140, Loss function: 5.490, Average Loss: 4.308, avg. samples / sec: 28451.09
Iteration:   1140, Loss function: 4.991, Average Loss: 4.310, avg. samples / sec: 28453.49
Iteration:   1140, Loss function: 4.631, Average Loss: 4.299, avg. samples / sec: 28423.80
Iteration:   1140, Loss function: 5.387, Average Loss: 4.301, avg. samples / sec: 28413.04
:::MLL 1558579700.026 epoch_stop: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 819}}
:::MLL 1558579700.026 epoch_start: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 673}}
Iteration:   1160, Loss function: 4.550, Average Loss: 4.310, avg. samples / sec: 28403.62
Iteration:   1160, Loss function: 4.880, Average Loss: 4.307, avg. samples / sec: 28357.38
Iteration:   1160, Loss function: 5.043, Average Loss: 4.320, avg. samples / sec: 28326.25
Iteration:   1160, Loss function: 4.771, Average Loss: 4.319, avg. samples / sec: 28327.59
Iteration:   1180, Loss function: 3.855, Average Loss: 4.315, avg. samples / sec: 28500.04
Iteration:   1180, Loss function: 4.437, Average Loss: 4.328, avg. samples / sec: 28505.95
Iteration:   1180, Loss function: 4.738, Average Loss: 4.313, avg. samples / sec: 28497.77
Iteration:   1180, Loss function: 4.678, Average Loss: 4.328, avg. samples / sec: 28485.48
Iteration:   1200, Loss function: 4.885, Average Loss: 4.338, avg. samples / sec: 28391.60
Iteration:   1200, Loss function: 4.707, Average Loss: 4.323, avg. samples / sec: 28389.66
Iteration:   1200, Loss function: 4.551, Average Loss: 4.340, avg. samples / sec: 28378.88
Iteration:   1200, Loss function: 4.161, Average Loss: 4.322, avg. samples / sec: 28316.97
Iteration:   1220, Loss function: 4.911, Average Loss: 4.346, avg. samples / sec: 28423.44
Iteration:   1220, Loss function: 4.862, Average Loss: 4.347, avg. samples / sec: 28380.92
Iteration:   1220, Loss function: 4.317, Average Loss: 4.329, avg. samples / sec: 28408.37
Iteration:   1220, Loss function: 5.484, Average Loss: 4.331, avg. samples / sec: 28315.63
:::MLL 1558579704.137 epoch_stop: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 819}}
:::MLL 1558579704.138 epoch_start: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 673}}
Iteration:   1240, Loss function: 5.105, Average Loss: 4.334, avg. samples / sec: 28268.85
Iteration:   1240, Loss function: 4.672, Average Loss: 4.336, avg. samples / sec: 28293.47
Iteration:   1240, Loss function: 4.980, Average Loss: 4.353, avg. samples / sec: 28210.23
Iteration:   1240, Loss function: 4.395, Average Loss: 4.353, avg. samples / sec: 28198.50
Iteration:   1260, Loss function: 4.552, Average Loss: 4.359, avg. samples / sec: 28290.94
Iteration:   1260, Loss function: 4.587, Average Loss: 4.358, avg. samples / sec: 28284.86
Iteration:   1260, Loss function: 5.480, Average Loss: 4.338, avg. samples / sec: 28262.12
Iteration:   1260, Loss function: 4.492, Average Loss: 4.340, avg. samples / sec: 28234.09
Iteration:   1280, Loss function: 4.785, Average Loss: 4.361, avg. samples / sec: 28377.71
Iteration:   1280, Loss function: 4.502, Average Loss: 4.363, avg. samples / sec: 28372.65
Iteration:   1280, Loss function: 4.925, Average Loss: 4.347, avg. samples / sec: 28402.70
Iteration:   1280, Loss function: 4.417, Average Loss: 4.346, avg. samples / sec: 28325.11
:::MLL 1558579708.265 epoch_stop: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 819}}
:::MLL 1558579708.266 epoch_start: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 673}}
Iteration:   1300, Loss function: 4.352, Average Loss: 4.370, avg. samples / sec: 28225.64
Iteration:   1300, Loss function: 4.369, Average Loss: 4.354, avg. samples / sec: 28277.45
Iteration:   1300, Loss function: 4.674, Average Loss: 4.353, avg. samples / sec: 28225.35
Iteration:   1300, Loss function: 4.185, Average Loss: 4.367, avg. samples / sec: 28216.50
Iteration:   1320, Loss function: 4.771, Average Loss: 4.373, avg. samples / sec: 28398.72
Iteration:   1320, Loss function: 3.723, Average Loss: 4.372, avg. samples / sec: 28397.14
Iteration:   1320, Loss function: 4.683, Average Loss: 4.359, avg. samples / sec: 28392.72
Iteration:   1320, Loss function: 4.910, Average Loss: 4.356, avg. samples / sec: 28382.85
Iteration:   1340, Loss function: 4.479, Average Loss: 4.379, avg. samples / sec: 28313.94
Iteration:   1340, Loss function: 4.277, Average Loss: 4.359, avg. samples / sec: 28335.72
Iteration:   1340, Loss function: 4.644, Average Loss: 4.376, avg. samples / sec: 28305.99
Iteration:   1340, Loss function: 4.465, Average Loss: 4.364, avg. samples / sec: 28264.25
Iteration:   1360, Loss function: 5.015, Average Loss: 4.384, avg. samples / sec: 28322.20
Iteration:   1360, Loss function: 4.290, Average Loss: 4.358, avg. samples / sec: 28321.55
Iteration:   1360, Loss function: 4.806, Average Loss: 4.368, avg. samples / sec: 28381.07
Iteration:   1360, Loss function: 4.493, Average Loss: 4.380, avg. samples / sec: 28329.45
:::MLL 1558579712.438 epoch_stop: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 819}}
:::MLL 1558579712.439 epoch_start: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 673}}
Iteration:   1380, Loss function: 5.238, Average Loss: 4.391, avg. samples / sec: 28377.34
Iteration:   1380, Loss function: 4.759, Average Loss: 4.373, avg. samples / sec: 28369.24
Iteration:   1380, Loss function: 4.394, Average Loss: 4.384, avg. samples / sec: 28379.81
Iteration:   1380, Loss function: 4.590, Average Loss: 4.362, avg. samples / sec: 28368.71
Iteration:   1400, Loss function: 4.126, Average Loss: 4.387, avg. samples / sec: 28428.25
Iteration:   1400, Loss function: 4.423, Average Loss: 4.375, avg. samples / sec: 28432.33
Iteration:   1400, Loss function: 4.927, Average Loss: 4.366, avg. samples / sec: 28422.89
Iteration:   1400, Loss function: 4.164, Average Loss: 4.394, avg. samples / sec: 28397.82
Iteration:   1420, Loss function: 4.306, Average Loss: 4.396, avg. samples / sec: 28449.79
Iteration:   1420, Loss function: 3.846, Average Loss: 4.388, avg. samples / sec: 28427.21
Iteration:   1420, Loss function: 4.567, Average Loss: 4.376, avg. samples / sec: 28404.43
Iteration:   1420, Loss function: 4.313, Average Loss: 4.367, avg. samples / sec: 28403.24
Iteration:   1440, Loss function: 4.334, Average Loss: 4.398, avg. samples / sec: 28352.71
Iteration:   1440, Loss function: 4.293, Average Loss: 4.379, avg. samples / sec: 28372.08
Iteration:   1440, Loss function: 4.219, Average Loss: 4.390, avg. samples / sec: 28343.71
Iteration:   1440, Loss function: 4.368, Average Loss: 4.369, avg. samples / sec: 28373.04
:::MLL 1558579716.551 epoch_stop: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 819}}
:::MLL 1558579716.551 epoch_start: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 673}}
Iteration:   1460, Loss function: 5.141, Average Loss: 4.401, avg. samples / sec: 28286.65
Iteration:   1460, Loss function: 4.611, Average Loss: 4.390, avg. samples / sec: 28245.55
Iteration:   1460, Loss function: 4.367, Average Loss: 4.381, avg. samples / sec: 28237.87
Iteration:   1460, Loss function: 4.006, Average Loss: 4.370, avg. samples / sec: 28227.07
Iteration:   1480, Loss function: 4.242, Average Loss: 4.400, avg. samples / sec: 28380.05
Iteration:   1480, Loss function: 4.989, Average Loss: 4.370, avg. samples / sec: 28452.86
Iteration:   1480, Loss function: 4.724, Average Loss: 4.389, avg. samples / sec: 28424.86
Iteration:   1480, Loss function: 4.868, Average Loss: 4.383, avg. samples / sec: 28383.93
Iteration:   1500, Loss function: 4.011, Average Loss: 4.400, avg. samples / sec: 28411.64
Iteration:   1500, Loss function: 4.754, Average Loss: 4.391, avg. samples / sec: 28403.80
Iteration:   1500, Loss function: 4.950, Average Loss: 4.373, avg. samples / sec: 28376.95
Iteration:   1500, Loss function: 4.898, Average Loss: 4.385, avg. samples / sec: 28373.74
Iteration:   1520, Loss function: 4.808, Average Loss: 4.403, avg. samples / sec: 28401.28
Iteration:   1520, Loss function: 5.481, Average Loss: 4.378, avg. samples / sec: 28434.33
Iteration:   1520, Loss function: 4.953, Average Loss: 4.386, avg. samples / sec: 28479.52
Iteration:   1520, Loss function: 4.601, Average Loss: 4.392, avg. samples / sec: 28397.84
:::MLL 1558579720.665 epoch_stop: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 819}}
:::MLL 1558579720.665 epoch_start: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 673}}
Iteration:   1540, Loss function: 4.274, Average Loss: 4.404, avg. samples / sec: 28374.25
Iteration:   1540, Loss function: 4.485, Average Loss: 4.387, avg. samples / sec: 28368.24
Iteration:   1540, Loss function: 4.883, Average Loss: 4.379, avg. samples / sec: 28364.45
Iteration:   1540, Loss function: 4.581, Average Loss: 4.392, avg. samples / sec: 28358.72
Iteration:   1560, Loss function: 4.630, Average Loss: 4.377, avg. samples / sec: 28479.19
Iteration:   1560, Loss function: 4.317, Average Loss: 4.405, avg. samples / sec: 28469.21
Iteration:   1560, Loss function: 5.002, Average Loss: 4.396, avg. samples / sec: 28491.34
Iteration:   1560, Loss function: 4.317, Average Loss: 4.389, avg. samples / sec: 28474.22
Iteration:   1580, Loss function: 4.922, Average Loss: 4.406, avg. samples / sec: 28425.53
Iteration:   1580, Loss function: 3.950, Average Loss: 4.378, avg. samples / sec: 28425.70
Iteration:   1580, Loss function: 4.266, Average Loss: 4.401, avg. samples / sec: 28429.35
Iteration:   1580, Loss function: 4.447, Average Loss: 4.389, avg. samples / sec: 28423.05
Iteration:   1600, Loss function: 4.534, Average Loss: 4.377, avg. samples / sec: 28345.40
Iteration:   1600, Loss function: 4.338, Average Loss: 4.401, avg. samples / sec: 28339.02
Iteration:   1600, Loss function: 3.826, Average Loss: 4.403, avg. samples / sec: 28339.87
Iteration:   1600, Loss function: 5.230, Average Loss: 4.390, avg. samples / sec: 28336.59
:::MLL 1558579724.830 epoch_stop: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 819}}
:::MLL 1558579724.830 epoch_start: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 673}}
Iteration:   1620, Loss function: 4.171, Average Loss: 4.401, avg. samples / sec: 28189.88
Iteration:   1620, Loss function: 3.911, Average Loss: 4.400, avg. samples / sec: 28147.53
Iteration:   1620, Loss function: 4.224, Average Loss: 4.387, avg. samples / sec: 28156.54
Iteration:   1620, Loss function: 3.984, Average Loss: 4.382, avg. samples / sec: 28128.77
Iteration:   1640, Loss function: 4.606, Average Loss: 4.401, avg. samples / sec: 28383.87
Iteration:   1640, Loss function: 4.300, Average Loss: 4.388, avg. samples / sec: 28388.38
Iteration:   1640, Loss function: 4.908, Average Loss: 4.403, avg. samples / sec: 28335.54
Iteration:   1640, Loss function: 4.723, Average Loss: 4.382, avg. samples / sec: 28353.87
Iteration:   1660, Loss function: 4.546, Average Loss: 4.400, avg. samples / sec: 28253.80
Iteration:   1660, Loss function: 4.673, Average Loss: 4.383, avg. samples / sec: 28291.00
Iteration:   1660, Loss function: 4.714, Average Loss: 4.404, avg. samples / sec: 28249.81
Iteration:   1660, Loss function: 4.337, Average Loss: 4.387, avg. samples / sec: 28230.58
:::MLL 1558579728.958 epoch_stop: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 819}}
:::MLL 1558579728.958 epoch_start: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 673}}
Iteration:   1680, Loss function: 4.025, Average Loss: 4.384, avg. samples / sec: 28330.63
Iteration:   1680, Loss function: 4.158, Average Loss: 4.401, avg. samples / sec: 28313.99
Iteration:   1680, Loss function: 4.168, Average Loss: 4.383, avg. samples / sec: 28295.26
Iteration:   1680, Loss function: 4.527, Average Loss: 4.401, avg. samples / sec: 28263.28
Iteration:   1700, Loss function: 3.959, Average Loss: 4.398, avg. samples / sec: 28556.65
Iteration:   1700, Loss function: 3.938, Average Loss: 4.381, avg. samples / sec: 28519.71
Iteration:   1700, Loss function: 4.216, Average Loss: 4.403, avg. samples / sec: 28498.09
Iteration:   1700, Loss function: 4.169, Average Loss: 4.384, avg. samples / sec: 28472.39
Iteration:   1720, Loss function: 4.196, Average Loss: 4.401, avg. samples / sec: 28475.59
Iteration:   1720, Loss function: 4.020, Average Loss: 4.379, avg. samples / sec: 28470.89
Iteration:   1720, Loss function: 4.578, Average Loss: 4.385, avg. samples / sec: 28445.22
Iteration:   1720, Loss function: 4.790, Average Loss: 4.401, avg. samples / sec: 28393.92
Iteration:   1740, Loss function: 4.523, Average Loss: 4.385, avg. samples / sec: 28304.57
Iteration:   1740, Loss function: 4.582, Average Loss: 4.381, avg. samples / sec: 28250.13
Iteration:   1740, Loss function: 3.447, Average Loss: 4.396, avg. samples / sec: 28307.82
Iteration:   1740, Loss function: 4.147, Average Loss: 4.399, avg. samples / sec: 28211.55
:::MLL 1558579733.073 epoch_stop: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 819}}
:::MLL 1558579733.074 epoch_start: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 673}}
Iteration:   1760, Loss function: 3.889, Average Loss: 4.394, avg. samples / sec: 28292.28
Iteration:   1760, Loss function: 3.887, Average Loss: 4.394, avg. samples / sec: 28248.55
Iteration:   1760, Loss function: 4.644, Average Loss: 4.381, avg. samples / sec: 28236.63
Iteration:   1760, Loss function: 4.720, Average Loss: 4.384, avg. samples / sec: 28189.73
Iteration:   1780, Loss function: 5.062, Average Loss: 4.392, avg. samples / sec: 28297.91
Iteration:   1780, Loss function: 3.873, Average Loss: 4.380, avg. samples / sec: 28294.39
Iteration:   1780, Loss function: 3.895, Average Loss: 4.381, avg. samples / sec: 28309.26
Iteration:   1780, Loss function: 4.160, Average Loss: 4.395, avg. samples / sec: 28238.25
Iteration:   1800, Loss function: 4.214, Average Loss: 4.392, avg. samples / sec: 28309.05
Iteration:   1800, Loss function: 4.227, Average Loss: 4.378, avg. samples / sec: 28342.10
Iteration:   1800, Loss function: 5.204, Average Loss: 4.378, avg. samples / sec: 28301.56
Iteration:   1800, Loss function: 4.023, Average Loss: 4.391, avg. samples / sec: 28295.29
Iteration:   1820, Loss function: 5.283, Average Loss: 4.379, avg. samples / sec: 28476.87
Iteration:   1820, Loss function: 4.918, Average Loss: 4.378, avg. samples / sec: 28478.63
Iteration:   1820, Loss function: 3.669, Average Loss: 4.390, avg. samples / sec: 28457.24
Iteration:   1820, Loss function: 4.720, Average Loss: 4.390, avg. samples / sec: 28443.75
:::MLL 1558579737.251 epoch_stop: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 819}}
:::MLL 1558579737.251 epoch_start: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 673}}
Iteration:   1840, Loss function: 4.893, Average Loss: 4.390, avg. samples / sec: 28323.69
Iteration:   1840, Loss function: 3.907, Average Loss: 4.384, avg. samples / sec: 28230.19
Iteration:   1840, Loss function: 4.963, Average Loss: 4.378, avg. samples / sec: 28186.90
Iteration:   1840, Loss function: 3.994, Average Loss: 4.375, avg. samples / sec: 28164.06
Iteration:   1860, Loss function: 4.195, Average Loss: 4.382, avg. samples / sec: 28309.50
Iteration:   1860, Loss function: 3.884, Average Loss: 4.378, avg. samples / sec: 28335.42
Iteration:   1860, Loss function: 3.876, Average Loss: 4.371, avg. samples / sec: 28365.87
Iteration:   1860, Loss function: 4.392, Average Loss: 4.388, avg. samples / sec: 28286.93
Iteration:   1880, Loss function: 4.359, Average Loss: 4.367, avg. samples / sec: 28311.64
Iteration:   1880, Loss function: 4.123, Average Loss: 4.380, avg. samples / sec: 28290.57
Iteration:   1880, Loss function: 3.957, Average Loss: 4.388, avg. samples / sec: 28298.72
Iteration:   1880, Loss function: 4.204, Average Loss: 4.377, avg. samples / sec: 28280.63
Iteration:   1900, Loss function: 4.570, Average Loss: 4.384, avg. samples / sec: 28369.91
Iteration:   1900, Loss function: 4.595, Average Loss: 4.378, avg. samples / sec: 28366.63
Iteration:   1900, Loss function: 4.799, Average Loss: 4.376, avg. samples / sec: 28362.82
Iteration:   1900, Loss function: 4.071, Average Loss: 4.365, avg. samples / sec: 28307.63
:::MLL 1558579741.372 epoch_stop: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 819}}
:::MLL 1558579741.373 epoch_start: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 673}}
Iteration:   1920, Loss function: 4.043, Average Loss: 4.376, avg. samples / sec: 28331.06
Iteration:   1920, Loss function: 4.024, Average Loss: 4.382, avg. samples / sec: 28331.12
Iteration:   1920, Loss function: 3.925, Average Loss: 4.374, avg. samples / sec: 28349.81
Iteration:   1920, Loss function: 4.378, Average Loss: 4.361, avg. samples / sec: 28375.36
Iteration:   1940, Loss function: 4.439, Average Loss: 4.376, avg. samples / sec: 28373.82
Iteration:   1940, Loss function: 4.765, Average Loss: 4.382, avg. samples / sec: 28369.85
Iteration:   1940, Loss function: 4.193, Average Loss: 4.370, avg. samples / sec: 28363.66
Iteration:   1940, Loss function: 5.501, Average Loss: 4.357, avg. samples / sec: 28323.02
Iteration:   1960, Loss function: 4.032, Average Loss: 4.353, avg. samples / sec: 28451.04
Iteration:   1960, Loss function: 4.329, Average Loss: 4.375, avg. samples / sec: 28395.78
Iteration:   1960, Loss function: 3.509, Average Loss: 4.376, avg. samples / sec: 28392.99
Iteration:   1960, Loss function: 4.988, Average Loss: 4.370, avg. samples / sec: 28403.90
Iteration:   1980, Loss function: 4.462, Average Loss: 4.350, avg. samples / sec: 28355.37
Iteration:   1980, Loss function: 4.044, Average Loss: 4.367, avg. samples / sec: 28358.17
Iteration:   1980, Loss function: 3.702, Average Loss: 4.369, avg. samples / sec: 28349.81
Iteration:   1980, Loss function: 4.141, Average Loss: 4.370, avg. samples / sec: 28344.80
:::MLL 1558579745.488 epoch_stop: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 819}}
:::MLL 1558579745.488 epoch_start: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 673}}
Iteration:   2000, Loss function: 4.140, Average Loss: 4.349, avg. samples / sec: 28298.43
Iteration:   2000, Loss function: 4.181, Average Loss: 4.366, avg. samples / sec: 28293.69
Iteration:   2000, Loss function: 4.034, Average Loss: 4.368, avg. samples / sec: 28252.89
Iteration:   2000, Loss function: 3.880, Average Loss: 4.369, avg. samples / sec: 28225.54
Iteration:   2020, Loss function: 4.103, Average Loss: 4.365, avg. samples / sec: 28368.67
Iteration:   2020, Loss function: 4.544, Average Loss: 4.365, avg. samples / sec: 28328.15
Iteration:   2020, Loss function: 4.176, Average Loss: 4.363, avg. samples / sec: 28279.64
Iteration:   2020, Loss function: 4.142, Average Loss: 4.344, avg. samples / sec: 28231.55
Iteration:   2040, Loss function: 4.355, Average Loss: 4.361, avg. samples / sec: 28094.48
Iteration:   2040, Loss function: 4.652, Average Loss: 4.344, avg. samples / sec: 28150.69
Iteration:   2040, Loss function: 3.933, Average Loss: 4.360, avg. samples / sec: 28094.65
Iteration:   2040, Loss function: 4.189, Average Loss: 4.359, avg. samples / sec: 28067.14
Iteration:   2060, Loss function: 4.262, Average Loss: 4.357, avg. samples / sec: 28442.24
Iteration:   2060, Loss function: 3.927, Average Loss: 4.341, avg. samples / sec: 28441.49
Iteration:   2060, Loss function: 4.525, Average Loss: 4.360, avg. samples / sec: 28433.04
Iteration:   2060, Loss function: 4.843, Average Loss: 4.359, avg. samples / sec: 28456.72
:::MLL 1558579749.670 epoch_stop: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 819}}
:::MLL 1558579749.670 epoch_start: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 673}}
Iteration:   2080, Loss function: 4.581, Average Loss: 4.360, avg. samples / sec: 28300.76
Iteration:   2080, Loss function: 4.196, Average Loss: 4.357, avg. samples / sec: 28281.23
Iteration:   2080, Loss function: 4.155, Average Loss: 4.337, avg. samples / sec: 28271.88
Iteration:   2080, Loss function: 3.800, Average Loss: 4.359, avg. samples / sec: 28300.43
Iteration:   2100, Loss function: 3.970, Average Loss: 4.351, avg. samples / sec: 28436.56
Iteration:   2100, Loss function: 3.938, Average Loss: 4.355, avg. samples / sec: 28437.53
Iteration:   2100, Loss function: 4.594, Average Loss: 4.358, avg. samples / sec: 28429.36
Iteration:   2100, Loss function: 4.306, Average Loss: 4.333, avg. samples / sec: 28429.60
Iteration:   2120, Loss function: 4.314, Average Loss: 4.347, avg. samples / sec: 28336.44
Iteration:   2120, Loss function: 4.300, Average Loss: 4.350, avg. samples / sec: 28327.68
Iteration:   2120, Loss function: 3.265, Average Loss: 4.329, avg. samples / sec: 28326.73
Iteration:   2120, Loss function: 4.506, Average Loss: 4.353, avg. samples / sec: 28283.57
:::MLL 1558579753.789 epoch_stop: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 819}}
:::MLL 1558579753.789 epoch_start: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 673}}
Iteration:   2140, Loss function: 3.763, Average Loss: 4.340, avg. samples / sec: 28288.27
Iteration:   2140, Loss function: 3.951, Average Loss: 4.345, avg. samples / sec: 28285.58
Iteration:   2140, Loss function: 3.818, Average Loss: 4.324, avg. samples / sec: 28289.65
Iteration:   2140, Loss function: 3.629, Average Loss: 4.347, avg. samples / sec: 28283.37
Iteration:   2160, Loss function: 3.554, Average Loss: 4.319, avg. samples / sec: 28252.82
Iteration:   2160, Loss function: 4.008, Average Loss: 4.334, avg. samples / sec: 28221.90
Iteration:   2160, Loss function: 3.860, Average Loss: 4.340, avg. samples / sec: 28235.16
Iteration:   2160, Loss function: 4.583, Average Loss: 4.345, avg. samples / sec: 28251.49
Iteration:   2180, Loss function: 4.378, Average Loss: 4.312, avg. samples / sec: 28370.34
Iteration:   2180, Loss function: 4.427, Average Loss: 4.333, avg. samples / sec: 28366.06
Iteration:   2180, Loss function: 4.224, Average Loss: 4.338, avg. samples / sec: 28399.58
Iteration:   2180, Loss function: 4.407, Average Loss: 4.335, avg. samples / sec: 28333.47
Iteration:   2200, Loss function: 4.207, Average Loss: 4.334, avg. samples / sec: 28292.23
Iteration:   2200, Loss function: 4.589, Average Loss: 4.333, avg. samples / sec: 28314.96
Iteration:   2200, Loss function: 4.065, Average Loss: 4.329, avg. samples / sec: 28275.76
Iteration:   2200, Loss function: 4.518, Average Loss: 4.307, avg. samples / sec: 28237.65
:::MLL 1558579757.972 epoch_stop: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 819}}
:::MLL 1558579757.972 epoch_start: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 673}}
Iteration:   2220, Loss function: 3.972, Average Loss: 4.323, avg. samples / sec: 28279.30
Iteration:   2220, Loss function: 4.701, Average Loss: 4.333, avg. samples / sec: 28239.66
Iteration:   2220, Loss function: 4.316, Average Loss: 4.303, avg. samples / sec: 28261.42
Iteration:   2220, Loss function: 4.076, Average Loss: 4.329, avg. samples / sec: 28218.97
Iteration:   2240, Loss function: 3.138, Average Loss: 4.318, avg. samples / sec: 28300.75
Iteration:   2240, Loss function: 4.324, Average Loss: 4.297, avg. samples / sec: 28336.28
Iteration:   2240, Loss function: 3.617, Average Loss: 4.326, avg. samples / sec: 28316.76
Iteration:   2240, Loss function: 4.290, Average Loss: 4.322, avg. samples / sec: 28310.35
Iteration:   2260, Loss function: 4.140, Average Loss: 4.323, avg. samples / sec: 28456.33
Iteration:   2260, Loss function: 4.276, Average Loss: 4.295, avg. samples / sec: 28452.15
Iteration:   2260, Loss function: 4.943, Average Loss: 4.318, avg. samples / sec: 28478.38
Iteration:   2260, Loss function: 4.258, Average Loss: 4.315, avg. samples / sec: 28385.38
Iteration:   2280, Loss function: 3.574, Average Loss: 4.310, avg. samples / sec: 28445.13
Iteration:   2280, Loss function: 4.095, Average Loss: 4.318, avg. samples / sec: 28375.23
Iteration:   2280, Loss function: 3.972, Average Loss: 4.290, avg. samples / sec: 28372.07
Iteration:   2280, Loss function: 4.985, Average Loss: 4.315, avg. samples / sec: 28321.52
:::MLL 1558579762.090 epoch_stop: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 819}}
:::MLL 1558579762.091 epoch_start: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 673}}
Iteration:   2300, Loss function: 4.415, Average Loss: 4.312, avg. samples / sec: 28330.03
Iteration:   2300, Loss function: 4.503, Average Loss: 4.304, avg. samples / sec: 28248.01
Iteration:   2300, Loss function: 3.838, Average Loss: 4.284, avg. samples / sec: 28265.35
Iteration:   2300, Loss function: 3.519, Average Loss: 4.313, avg. samples / sec: 28196.87
Iteration:   2320, Loss function: 4.198, Average Loss: 4.308, avg. samples / sec: 28415.64
Iteration:   2320, Loss function: 3.621, Average Loss: 4.280, avg. samples / sec: 28408.65
Iteration:   2320, Loss function: 3.627, Average Loss: 4.297, avg. samples / sec: 28398.14
Iteration:   2320, Loss function: 4.177, Average Loss: 4.307, avg. samples / sec: 28439.57
Iteration:   2340, Loss function: 4.136, Average Loss: 4.292, avg. samples / sec: 28263.64
Iteration:   2340, Loss function: 3.525, Average Loss: 4.303, avg. samples / sec: 28243.08
Iteration:   2340, Loss function: 3.999, Average Loss: 4.304, avg. samples / sec: 28276.80
Iteration:   2340, Loss function: 3.940, Average Loss: 4.277, avg. samples / sec: 28243.82
Iteration:   2360, Loss function: 3.782, Average Loss: 4.273, avg. samples / sec: 28307.00
Iteration:   2360, Loss function: 4.630, Average Loss: 4.300, avg. samples / sec: 28296.06
Iteration:   2360, Loss function: 3.466, Average Loss: 4.289, avg. samples / sec: 28272.21
Iteration:   2360, Loss function: 3.504, Average Loss: 4.299, avg. samples / sec: 28274.28
:::MLL 1558579766.213 epoch_stop: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 819}}
:::MLL 1558579766.214 epoch_start: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 673}}
Iteration:   2380, Loss function: 4.624, Average Loss: 4.294, avg. samples / sec: 28305.05
Iteration:   2380, Loss function: 4.123, Average Loss: 4.297, avg. samples / sec: 28318.48
Iteration:   2380, Loss function: 3.903, Average Loss: 4.269, avg. samples / sec: 28251.98
Iteration:   2380, Loss function: 4.619, Average Loss: 4.284, avg. samples / sec: 28266.62
Iteration:   2400, Loss function: 3.765, Average Loss: 4.279, avg. samples / sec: 28347.17
Iteration:   2400, Loss function: 4.463, Average Loss: 4.266, avg. samples / sec: 28337.19
Iteration:   2400, Loss function: 4.502, Average Loss: 4.293, avg. samples / sec: 28244.11
Iteration:   2400, Loss function: 3.690, Average Loss: 4.288, avg. samples / sec: 28227.93
Iteration:   2420, Loss function: 4.179, Average Loss: 4.288, avg. samples / sec: 28447.10
Iteration:   2420, Loss function: 3.439, Average Loss: 4.281, avg. samples / sec: 28446.70
Iteration:   2420, Loss function: 4.698, Average Loss: 4.263, avg. samples / sec: 28381.22
Iteration:   2420, Loss function: 4.650, Average Loss: 4.276, avg. samples / sec: 28368.92
Iteration:   2440, Loss function: 4.561, Average Loss: 4.281, avg. samples / sec: 28313.31
Iteration:   2440, Loss function: 3.923, Average Loss: 4.272, avg. samples / sec: 28334.59
Iteration:   2440, Loss function: 4.089, Average Loss: 4.257, avg. samples / sec: 28312.19
Iteration:   2440, Loss function: 4.803, Average Loss: 4.278, avg. samples / sec: 28258.74
:::MLL 1558579770.391 epoch_stop: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 819}}
:::MLL 1558579770.392 epoch_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 673}}
Iteration:   2460, Loss function: 4.585, Average Loss: 4.271, avg. samples / sec: 28242.82
Iteration:   2460, Loss function: 4.351, Average Loss: 4.274, avg. samples / sec: 28300.29
Iteration:   2460, Loss function: 3.646, Average Loss: 4.276, avg. samples / sec: 28232.26
Iteration:   2460, Loss function: 3.705, Average Loss: 4.253, avg. samples / sec: 28234.95
Iteration:   2480, Loss function: 4.309, Average Loss: 4.249, avg. samples / sec: 28447.01
Iteration:   2480, Loss function: 4.278, Average Loss: 4.276, avg. samples / sec: 28434.00
Iteration:   2480, Loss function: 4.178, Average Loss: 4.272, avg. samples / sec: 28424.42
Iteration:   2480, Loss function: 4.095, Average Loss: 4.268, avg. samples / sec: 28415.72
Iteration:   2500, Loss function: 4.519, Average Loss: 4.266, avg. samples / sec: 28342.82
Iteration:   2500, Loss function: 4.341, Average Loss: 4.247, avg. samples / sec: 28320.66
Iteration:   2500, Loss function: 4.667, Average Loss: 4.270, avg. samples / sec: 28327.81
Iteration:   2500, Loss function: 4.044, Average Loss: 4.271, avg. samples / sec: 28315.56
:::MLL 1558579773.480 eval_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.24 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.24 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.24 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.24 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.34s)
DONE (t=0.35s)
DONE (t=0.35s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=2.57s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.15521
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.29400
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.15016
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.03823
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.17330
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.24664
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.17114
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.25173
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.26502
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.06979
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.28647
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.41494
Current AP: 0.15521 AP goal: 0.23000
:::MLL 1558579778.694 eval_accuracy: {"value": 0.1552096293359188, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 389}}
:::MLL 1558579778.735 eval_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 392}}
:::MLL 1558579778.749 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 804}}
:::MLL 1558579778.750 block_start: {"value": null, "metadata": {"first_epoch_num": 33, "epoch_count": 10.915354834308324, "file": "train.py", "lineno": 813}}
:::MLL 1558579780.384 epoch_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 819}}
:::MLL 1558579780.385 epoch_start: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 673}}
Iteration:   2520, Loss function: 3.730, Average Loss: 4.267, avg. samples / sec: 4413.24
Iteration:   2520, Loss function: 3.285, Average Loss: 4.265, avg. samples / sec: 4413.23
Iteration:   2520, Loss function: 4.506, Average Loss: 4.266, avg. samples / sec: 4412.48
Iteration:   2520, Loss function: 4.753, Average Loss: 4.243, avg. samples / sec: 4412.11
Iteration:   2540, Loss function: 3.424, Average Loss: 4.238, avg. samples / sec: 28002.44
Iteration:   2540, Loss function: 4.118, Average Loss: 4.261, avg. samples / sec: 27975.18
Iteration:   2540, Loss function: 4.336, Average Loss: 4.264, avg. samples / sec: 27952.59
Iteration:   2540, Loss function: 3.606, Average Loss: 4.261, avg. samples / sec: 27960.64
Iteration:   2560, Loss function: 4.163, Average Loss: 4.259, avg. samples / sec: 28087.14
Iteration:   2560, Loss function: 4.596, Average Loss: 4.262, avg. samples / sec: 28080.41
Iteration:   2560, Loss function: 3.495, Average Loss: 4.261, avg. samples / sec: 28070.45
Iteration:   2560, Loss function: 4.428, Average Loss: 4.235, avg. samples / sec: 28055.32
Iteration:   2580, Loss function: 4.151, Average Loss: 4.259, avg. samples / sec: 28212.65
Iteration:   2580, Loss function: 4.577, Average Loss: 4.256, avg. samples / sec: 28203.62
Iteration:   2580, Loss function: 4.123, Average Loss: 4.259, avg. samples / sec: 28165.19
Iteration:   2580, Loss function: 4.369, Average Loss: 4.234, avg. samples / sec: 28150.63
:::MLL 1558579784.541 epoch_stop: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 819}}
:::MLL 1558579784.541 epoch_start: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 673}}
Iteration:   2600, Loss function: 4.093, Average Loss: 4.252, avg. samples / sec: 28099.25
Iteration:   2600, Loss function: 3.886, Average Loss: 4.228, avg. samples / sec: 28174.10
Iteration:   2600, Loss function: 4.110, Average Loss: 4.254, avg. samples / sec: 28142.95
Iteration:   2600, Loss function: 3.887, Average Loss: 4.256, avg. samples / sec: 28047.27
Iteration:   2620, Loss function: 3.595, Average Loss: 4.249, avg. samples / sec: 28230.05
Iteration:   2620, Loss function: 4.016, Average Loss: 4.249, avg. samples / sec: 28168.37
Iteration:   2620, Loss function: 3.960, Average Loss: 4.249, avg. samples / sec: 28119.54
Iteration:   2620, Loss function: 4.599, Average Loss: 4.223, avg. samples / sec: 28119.23
Iteration:   2640, Loss function: 3.424, Average Loss: 4.218, avg. samples / sec: 28099.71
Iteration:   2640, Loss function: 3.624, Average Loss: 4.244, avg. samples / sec: 28037.99
Iteration:   2640, Loss function: 4.516, Average Loss: 4.244, avg. samples / sec: 28094.99
Iteration:   2640, Loss function: 3.421, Average Loss: 4.245, avg. samples / sec: 28006.07
Iteration:   2660, Loss function: 3.818, Average Loss: 4.241, avg. samples / sec: 28009.48
Iteration:   2660, Loss function: 4.338, Average Loss: 4.243, avg. samples / sec: 28048.94
Iteration:   2660, Loss function: 4.048, Average Loss: 4.215, avg. samples / sec: 27966.40
Iteration:   2660, Loss function: 3.757, Average Loss: 4.241, avg. samples / sec: 27936.84
:::MLL 1558579788.756 epoch_stop: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 819}}
:::MLL 1558579788.756 epoch_start: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 673}}
Iteration:   2680, Loss function: 3.913, Average Loss: 4.237, avg. samples / sec: 28099.09
Iteration:   2680, Loss function: 4.208, Average Loss: 4.239, avg. samples / sec: 28107.30
Iteration:   2680, Loss function: 4.338, Average Loss: 4.236, avg. samples / sec: 28171.95
Iteration:   2680, Loss function: 4.153, Average Loss: 4.208, avg. samples / sec: 28134.34
Iteration:   2700, Loss function: 3.644, Average Loss: 4.204, avg. samples / sec: 28092.21
Iteration:   2700, Loss function: 3.728, Average Loss: 4.232, avg. samples / sec: 28074.02
Iteration:   2700, Loss function: 3.817, Average Loss: 4.231, avg. samples / sec: 28038.07
Iteration:   2700, Loss function: 3.873, Average Loss: 4.233, avg. samples / sec: 27954.33
Iteration:   2720, Loss function: 3.836, Average Loss: 4.231, avg. samples / sec: 28232.81
Iteration:   2720, Loss function: 4.051, Average Loss: 4.228, avg. samples / sec: 28137.23
Iteration:   2720, Loss function: 3.666, Average Loss: 4.201, avg. samples / sec: 28084.34
Iteration:   2720, Loss function: 3.938, Average Loss: 4.228, avg. samples / sec: 28075.59
Iteration:   2740, Loss function: 3.911, Average Loss: 4.223, avg. samples / sec: 27920.39
Iteration:   2740, Loss function: 4.032, Average Loss: 4.200, avg. samples / sec: 27888.11
Iteration:   2740, Loss function: 3.658, Average Loss: 4.223, avg. samples / sec: 27882.07
Iteration:   2740, Loss function: 4.123, Average Loss: 4.226, avg. samples / sec: 27837.66
:::MLL 1558579792.921 epoch_stop: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 819}}
:::MLL 1558579792.922 epoch_start: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 673}}
Iteration:   2760, Loss function: 3.951, Average Loss: 4.222, avg. samples / sec: 27984.48
Iteration:   2760, Loss function: 4.323, Average Loss: 4.218, avg. samples / sec: 27937.75
Iteration:   2760, Loss function: 4.347, Average Loss: 4.196, avg. samples / sec: 27948.62
Iteration:   2760, Loss function: 3.828, Average Loss: 4.219, avg. samples / sec: 27948.55
Iteration:   2780, Loss function: 4.480, Average Loss: 4.220, avg. samples / sec: 27918.99
Iteration:   2780, Loss function: 3.874, Average Loss: 4.213, avg. samples / sec: 27912.43
Iteration:   2780, Loss function: 4.440, Average Loss: 4.214, avg. samples / sec: 27905.82
Iteration:   2780, Loss function: 3.676, Average Loss: 4.194, avg. samples / sec: 27872.36
Iteration:   2800, Loss function: 4.415, Average Loss: 4.216, avg. samples / sec: 27937.55
Iteration:   2800, Loss function: 4.272, Average Loss: 4.212, avg. samples / sec: 27946.85
Iteration:   2800, Loss function: 4.606, Average Loss: 4.189, avg. samples / sec: 27926.94
Iteration:   2800, Loss function: 3.898, Average Loss: 4.211, avg. samples / sec: 27887.14
Iteration:   2820, Loss function: 3.690, Average Loss: 4.204, avg. samples / sec: 28040.95
Iteration:   2820, Loss function: 3.752, Average Loss: 4.187, avg. samples / sec: 28036.74
Iteration:   2820, Loss function: 4.107, Average Loss: 4.211, avg. samples / sec: 27970.94
Iteration:   2820, Loss function: 3.704, Average Loss: 4.206, avg. samples / sec: 27924.94
:::MLL 1558579797.100 epoch_stop: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 819}}
:::MLL 1558579797.101 epoch_start: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 673}}
Iteration:   2840, Loss function: 4.161, Average Loss: 4.201, avg. samples / sec: 28012.92
Iteration:   2840, Loss function: 4.494, Average Loss: 4.203, avg. samples / sec: 27953.70
Iteration:   2840, Loss function: 4.207, Average Loss: 4.209, avg. samples / sec: 27954.50
Iteration:   2840, Loss function: 4.323, Average Loss: 4.181, avg. samples / sec: 27938.23
Iteration:   2860, Loss function: 4.008, Average Loss: 4.177, avg. samples / sec: 28016.44
Iteration:   2860, Loss function: 3.463, Average Loss: 4.200, avg. samples / sec: 28001.47
Iteration:   2860, Loss function: 4.597, Average Loss: 4.199, avg. samples / sec: 27989.84
Iteration:   2860, Loss function: 4.040, Average Loss: 4.205, avg. samples / sec: 27990.14
Iteration:   2880, Loss function: 3.857, Average Loss: 4.194, avg. samples / sec: 27862.80
Iteration:   2880, Loss function: 4.125, Average Loss: 4.201, avg. samples / sec: 27869.38
Iteration:   2880, Loss function: 4.163, Average Loss: 4.173, avg. samples / sec: 27851.32
Iteration:   2880, Loss function: 4.337, Average Loss: 4.194, avg. samples / sec: 27853.41
Iteration:   2900, Loss function: 3.707, Average Loss: 4.172, avg. samples / sec: 28125.59
Iteration:   2900, Loss function: 3.643, Average Loss: 4.190, avg. samples / sec: 28110.05
Iteration:   2900, Loss function: 5.005, Average Loss: 4.191, avg. samples / sec: 28118.83
Iteration:   2900, Loss function: 4.066, Average Loss: 4.197, avg. samples / sec: 28109.66
:::MLL 1558579801.325 epoch_stop: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 819}}
:::MLL 1558579801.326 epoch_start: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 673}}
Iteration:   2920, Loss function: 3.623, Average Loss: 4.190, avg. samples / sec: 28074.29
Iteration:   2920, Loss function: 3.479, Average Loss: 4.188, avg. samples / sec: 28064.36
Iteration:   2920, Loss function: 3.655, Average Loss: 4.187, avg. samples / sec: 28070.18
Iteration:   2920, Loss function: 3.307, Average Loss: 4.169, avg. samples / sec: 28050.33
Iteration:   2940, Loss function: 4.198, Average Loss: 4.187, avg. samples / sec: 27981.26
Iteration:   2940, Loss function: 3.535, Average Loss: 4.184, avg. samples / sec: 27981.83
Iteration:   2940, Loss function: 4.274, Average Loss: 4.183, avg. samples / sec: 27972.75
Iteration:   2940, Loss function: 4.349, Average Loss: 4.166, avg. samples / sec: 27975.16
Iteration:   2960, Loss function: 4.307, Average Loss: 4.183, avg. samples / sec: 28094.87
Iteration:   2960, Loss function: 4.323, Average Loss: 4.164, avg. samples / sec: 28101.27
Iteration:   2960, Loss function: 4.098, Average Loss: 4.180, avg. samples / sec: 28066.37
Iteration:   2960, Loss function: 4.630, Average Loss: 4.183, avg. samples / sec: 28045.93
:::MLL 1558579805.489 epoch_stop: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 819}}
:::MLL 1558579805.489 epoch_start: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 673}}
Iteration:   2980, Loss function: 3.147, Average Loss: 4.160, avg. samples / sec: 28013.93
Iteration:   2980, Loss function: 3.735, Average Loss: 4.175, avg. samples / sec: 28030.25
Iteration:   2980, Loss function: 4.514, Average Loss: 4.179, avg. samples / sec: 28017.25
Iteration:   2980, Loss function: 3.611, Average Loss: 4.177, avg. samples / sec: 27955.67
Iteration:   3000, Loss function: 3.990, Average Loss: 4.175, avg. samples / sec: 28084.59
Iteration:   3000, Loss function: 4.057, Average Loss: 4.168, avg. samples / sec: 28044.83
Iteration:   3000, Loss function: 4.260, Average Loss: 4.172, avg. samples / sec: 28045.10
Iteration:   3000, Loss function: 4.033, Average Loss: 4.151, avg. samples / sec: 27983.64
Iteration:   3020, Loss function: 3.545, Average Loss: 4.150, avg. samples / sec: 28018.46
Iteration:   3020, Loss function: 4.462, Average Loss: 4.165, avg. samples / sec: 27994.71
Iteration:   3020, Loss function: 4.314, Average Loss: 4.167, avg. samples / sec: 27929.25
Iteration:   3020, Loss function: 4.192, Average Loss: 4.172, avg. samples / sec: 27898.06
Iteration:   3040, Loss function: 3.369, Average Loss: 4.165, avg. samples / sec: 28107.89
Iteration:   3040, Loss function: 3.816, Average Loss: 4.169, avg. samples / sec: 28113.63
Iteration:   3040, Loss function: 4.448, Average Loss: 4.149, avg. samples / sec: 28049.68
Iteration:   3040, Loss function: 3.924, Average Loss: 4.161, avg. samples / sec: 28048.34
:::MLL 1558579809.654 epoch_stop: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 819}}
:::MLL 1558579809.655 epoch_start: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 673}}
Iteration:   3060, Loss function: 4.677, Average Loss: 4.165, avg. samples / sec: 28079.34
Iteration:   3060, Loss function: 3.791, Average Loss: 4.144, avg. samples / sec: 28091.71
Iteration:   3060, Loss function: 4.583, Average Loss: 4.163, avg. samples / sec: 28059.08
Iteration:   3060, Loss function: 4.199, Average Loss: 4.156, avg. samples / sec: 28063.63
Iteration:   3080, Loss function: 3.733, Average Loss: 4.140, avg. samples / sec: 28060.05
Iteration:   3080, Loss function: 4.099, Average Loss: 4.160, avg. samples / sec: 28057.98
Iteration:   3080, Loss function: 3.629, Average Loss: 4.156, avg. samples / sec: 28057.08
Iteration:   3080, Loss function: 4.353, Average Loss: 4.153, avg. samples / sec: 28055.37
Iteration:   3100, Loss function: 4.218, Average Loss: 4.151, avg. samples / sec: 28016.02
Iteration:   3100, Loss function: 3.424, Average Loss: 4.158, avg. samples / sec: 27998.59
Iteration:   3100, Loss function: 3.637, Average Loss: 4.136, avg. samples / sec: 27984.80
Iteration:   3100, Loss function: 3.754, Average Loss: 4.149, avg. samples / sec: 28022.43
Iteration:   3120, Loss function: 3.552, Average Loss: 4.144, avg. samples / sec: 28020.29
Iteration:   3120, Loss function: 3.568, Average Loss: 4.150, avg. samples / sec: 27997.59
Iteration:   3120, Loss function: 4.009, Average Loss: 4.152, avg. samples / sec: 27992.17
Iteration:   3120, Loss function: 4.746, Average Loss: 4.134, avg. samples / sec: 27996.99
:::MLL 1558579813.877 epoch_stop: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 819}}
:::MLL 1558579813.878 epoch_start: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 673}}
Iteration:   3140, Loss function: 4.441, Average Loss: 4.140, avg. samples / sec: 27942.39
Iteration:   3140, Loss function: 4.226, Average Loss: 4.146, avg. samples / sec: 27945.71
Iteration:   3140, Loss function: 4.027, Average Loss: 4.130, avg. samples / sec: 27955.23
Iteration:   3140, Loss function: 3.894, Average Loss: 4.149, avg. samples / sec: 27906.00
Iteration:   3160, Loss function: 3.949, Average Loss: 4.145, avg. samples / sec: 28108.75
Iteration:   3160, Loss function: 3.724, Average Loss: 4.142, avg. samples / sec: 28071.30
Iteration:   3160, Loss function: 3.588, Average Loss: 4.137, avg. samples / sec: 28070.73
Iteration:   3160, Loss function: 3.933, Average Loss: 4.127, avg. samples / sec: 28058.19
Iteration:   3180, Loss function: 3.653, Average Loss: 4.138, avg. samples / sec: 28095.28
Iteration:   3180, Loss function: 4.256, Average Loss: 4.142, avg. samples / sec: 28080.15
Iteration:   3180, Loss function: 3.615, Average Loss: 4.135, avg. samples / sec: 28090.80
Iteration:   3180, Loss function: 3.532, Average Loss: 4.123, avg. samples / sec: 28100.39
Iteration:   3200, Loss function: 3.571, Average Loss: 4.133, avg. samples / sec: 28050.37
Iteration:   3200, Loss function: 4.072, Average Loss: 4.132, avg. samples / sec: 28036.07
Iteration:   3200, Loss function: 3.872, Average Loss: 4.119, avg. samples / sec: 28029.61
Iteration:   3200, Loss function: 4.057, Average Loss: 4.140, avg. samples / sec: 28028.14
:::MLL 1558579818.037 epoch_stop: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 819}}
:::MLL 1558579818.038 epoch_start: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 673}}
Iteration:   3220, Loss function: 4.094, Average Loss: 4.128, avg. samples / sec: 28051.25
Iteration:   3220, Loss function: 3.796, Average Loss: 4.118, avg. samples / sec: 28057.37
Iteration:   3220, Loss function: 3.756, Average Loss: 4.133, avg. samples / sec: 28055.33
Iteration:   3220, Loss function: 3.903, Average Loss: 4.128, avg. samples / sec: 27988.20
Iteration:   3240, Loss function: 4.209, Average Loss: 4.112, avg. samples / sec: 28106.85
Iteration:   3240, Loss function: 4.260, Average Loss: 4.123, avg. samples / sec: 28095.90
Iteration:   3240, Loss function: 4.369, Average Loss: 4.125, avg. samples / sec: 28138.40
Iteration:   3240, Loss function: 4.989, Average Loss: 4.129, avg. samples / sec: 28091.85
Iteration:   3260, Loss function: 3.516, Average Loss: 4.119, avg. samples / sec: 27980.80
Iteration:   3260, Loss function: 3.559, Average Loss: 4.119, avg. samples / sec: 27973.55
Iteration:   3260, Loss function: 4.125, Average Loss: 4.127, avg. samples / sec: 27987.63
Iteration:   3260, Loss function: 4.226, Average Loss: 4.110, avg. samples / sec: 27959.73
Iteration:   3280, Loss function: 4.316, Average Loss: 4.122, avg. samples / sec: 28082.27
Iteration:   3280, Loss function: 4.417, Average Loss: 4.115, avg. samples / sec: 28074.18
Iteration:   3280, Loss function: 3.510, Average Loss: 4.117, avg. samples / sec: 28053.90
Iteration:   3280, Loss function: 3.671, Average Loss: 4.109, avg. samples / sec: 28047.77
:::MLL 1558579822.202 epoch_stop: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 819}}
:::MLL 1558579822.203 epoch_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 673}}
Iteration:   3300, Loss function: 3.625, Average Loss: 4.112, avg. samples / sec: 28003.66
Iteration:   3300, Loss function: 3.417, Average Loss: 4.117, avg. samples / sec: 27971.77
Iteration:   3300, Loss function: 3.983, Average Loss: 4.110, avg. samples / sec: 27974.06
Iteration:   3300, Loss function: 3.863, Average Loss: 4.104, avg. samples / sec: 27962.45
Iteration:   3320, Loss function: 3.916, Average Loss: 4.109, avg. samples / sec: 28018.57
Iteration:   3320, Loss function: 4.080, Average Loss: 4.108, avg. samples / sec: 28005.23
Iteration:   3320, Loss function: 4.197, Average Loss: 4.100, avg. samples / sec: 28058.29
Iteration:   3320, Loss function: 3.818, Average Loss: 4.106, avg. samples / sec: 27993.80
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
:::MLL 1558579824.997 eval_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.02 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.02 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.02 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.02 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.43s)
DONE (t=0.43s)
DONE (t=0.43s)
DONE (t=0.43s)
DONE (t=0.43s)
DONE (t=0.43s)
DONE (t=0.43s)
DONE (t=0.43s)
DONE (t=0.43s)
DONE (t=0.43s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
DONE (t=0.44s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=2.79s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.17569
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.31998
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.17661
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.04262
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.19095
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.28859
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.18558
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.27276
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.28669
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.06991
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.30814
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.46267
Current AP: 0.17569 AP goal: 0.23000
:::MLL 1558579829.309 eval_accuracy: {"value": 0.17568913699327895, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 389}}
:::MLL 1558579829.331 eval_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 392}}
:::MLL 1558579829.346 block_stop: {"value": null, "metadata": {"first_epoch_num": 33, "file": "train.py", "lineno": 804}}
:::MLL 1558579829.346 block_start: {"value": null, "metadata": {"first_epoch_num": 44, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3340, Loss function: 3.375, Average Loss: 4.096, avg. samples / sec: 5643.07
Iteration:   3340, Loss function: 4.613, Average Loss: 4.098, avg. samples / sec: 5643.32
Iteration:   3340, Loss function: 3.822, Average Loss: 4.107, avg. samples / sec: 5640.42
Iteration:   3340, Loss function: 3.349, Average Loss: 4.103, avg. samples / sec: 5640.15
:::MLL 1558579830.777 epoch_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 819}}
:::MLL 1558579830.777 epoch_start: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 673}}
Iteration:   3360, Loss function: 3.630, Average Loss: 4.094, avg. samples / sec: 27920.81
Iteration:   3360, Loss function: 3.399, Average Loss: 4.096, avg. samples / sec: 27891.02
Iteration:   3360, Loss function: 3.895, Average Loss: 4.091, avg. samples / sec: 27835.11
Iteration:   3360, Loss function: 3.847, Average Loss: 4.092, avg. samples / sec: 27824.26
Iteration:   3380, Loss function: 3.611, Average Loss: 4.085, avg. samples / sec: 27923.38
Iteration:   3380, Loss function: 3.500, Average Loss: 4.083, avg. samples / sec: 27911.07
Iteration:   3380, Loss function: 3.464, Average Loss: 4.082, avg. samples / sec: 27856.63
Iteration:   3380, Loss function: 3.474, Average Loss: 4.085, avg. samples / sec: 27830.80
Iteration:   3400, Loss function: 3.094, Average Loss: 4.072, avg. samples / sec: 27992.98
Iteration:   3400, Loss function: 3.451, Average Loss: 4.072, avg. samples / sec: 27930.14
Iteration:   3400, Loss function: 3.225, Average Loss: 4.075, avg. samples / sec: 27942.42
Iteration:   3400, Loss function: 3.704, Average Loss: 4.073, avg. samples / sec: 27995.21
Iteration:   3420, Loss function: 3.617, Average Loss: 4.064, avg. samples / sec: 27958.79
Iteration:   3420, Loss function: 4.005, Average Loss: 4.060, avg. samples / sec: 27943.49
Iteration:   3420, Loss function: 3.840, Average Loss: 4.065, avg. samples / sec: 27957.45
Iteration:   3420, Loss function: 3.311, Average Loss: 4.065, avg. samples / sec: 27932.59
:::MLL 1558579834.959 epoch_stop: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 819}}
:::MLL 1558579834.959 epoch_start: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 673}}
Iteration:   3440, Loss function: 3.420, Average Loss: 4.055, avg. samples / sec: 27944.19
Iteration:   3440, Loss function: 3.585, Average Loss: 4.056, avg. samples / sec: 27942.74
Iteration:   3440, Loss function: 3.321, Average Loss: 4.051, avg. samples / sec: 27908.82
Iteration:   3440, Loss function: 3.352, Average Loss: 4.054, avg. samples / sec: 27889.89
Iteration:   3460, Loss function: 3.619, Average Loss: 4.045, avg. samples / sec: 28051.15
Iteration:   3460, Loss function: 3.684, Average Loss: 4.038, avg. samples / sec: 28067.68
Iteration:   3460, Loss function: 3.654, Average Loss: 4.042, avg. samples / sec: 28108.57
Iteration:   3460, Loss function: 3.709, Average Loss: 4.047, avg. samples / sec: 28021.18
Iteration:   3480, Loss function: 2.931, Average Loss: 4.032, avg. samples / sec: 28052.63
Iteration:   3480, Loss function: 3.090, Average Loss: 4.037, avg. samples / sec: 28090.51
Iteration:   3480, Loss function: 3.359, Average Loss: 4.032, avg. samples / sec: 28078.48
Iteration:   3480, Loss function: 3.131, Average Loss: 4.023, avg. samples / sec: 28069.82
Iteration:   3500, Loss function: 2.969, Average Loss: 4.027, avg. samples / sec: 28034.94
Iteration:   3500, Loss function: 3.137, Average Loss: 4.022, avg. samples / sec: 28024.11
Iteration:   3500, Loss function: 3.503, Average Loss: 4.010, avg. samples / sec: 28026.61
Iteration:   3500, Loss function: 2.958, Average Loss: 4.020, avg. samples / sec: 27983.36
:::MLL 1558579839.122 epoch_stop: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 819}}
:::MLL 1558579839.122 epoch_start: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 673}}
Iteration:   3520, Loss function: 2.886, Average Loss: 4.018, avg. samples / sec: 28057.98
Iteration:   3520, Loss function: 3.416, Average Loss: 4.000, avg. samples / sec: 28068.78
Iteration:   3520, Loss function: 3.925, Average Loss: 4.013, avg. samples / sec: 28050.54
Iteration:   3520, Loss function: 3.974, Average Loss: 4.010, avg. samples / sec: 28088.02
Iteration:   3540, Loss function: 3.058, Average Loss: 3.999, avg. samples / sec: 28035.45
Iteration:   3540, Loss function: 3.637, Average Loss: 4.008, avg. samples / sec: 28011.46
Iteration:   3540, Loss function: 3.169, Average Loss: 3.988, avg. samples / sec: 28009.19
Iteration:   3540, Loss function: 4.159, Average Loss: 3.998, avg. samples / sec: 27942.13
Iteration:   3560, Loss function: 3.338, Average Loss: 3.988, avg. samples / sec: 28020.07
Iteration:   3560, Loss function: 3.553, Average Loss: 3.998, avg. samples / sec: 27918.58
Iteration:   3560, Loss function: 3.365, Average Loss: 3.987, avg. samples / sec: 27909.22
Iteration:   3560, Loss function: 3.388, Average Loss: 3.978, avg. samples / sec: 27916.04
Iteration:   3580, Loss function: 3.814, Average Loss: 3.979, avg. samples / sec: 27995.26
Iteration:   3580, Loss function: 3.222, Average Loss: 3.983, avg. samples / sec: 27983.58
Iteration:   3580, Loss function: 3.020, Average Loss: 3.967, avg. samples / sec: 27983.41
Iteration:   3580, Loss function: 3.606, Average Loss: 3.977, avg. samples / sec: 27923.09
:::MLL 1558579843.349 epoch_stop: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 819}}
:::MLL 1558579843.350 epoch_start: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 673}}
Iteration:   3600, Loss function: 3.404, Average Loss: 3.972, avg. samples / sec: 28000.75
Iteration:   3600, Loss function: 4.317, Average Loss: 3.972, avg. samples / sec: 27981.01
Iteration:   3600, Loss function: 3.655, Average Loss: 3.959, avg. samples / sec: 27972.29
Iteration:   3600, Loss function: 3.316, Average Loss: 3.965, avg. samples / sec: 28003.83
Iteration:   3620, Loss function: 2.985, Average Loss: 3.947, avg. samples / sec: 28173.93
Iteration:   3620, Loss function: 3.092, Average Loss: 3.960, avg. samples / sec: 28130.35
Iteration:   3620, Loss function: 3.434, Average Loss: 3.954, avg. samples / sec: 28170.43
Iteration:   3620, Loss function: 3.679, Average Loss: 3.961, avg. samples / sec: 28094.95
Iteration:   3640, Loss function: 3.422, Average Loss: 3.951, avg. samples / sec: 28192.02
Iteration:   3640, Loss function: 2.889, Average Loss: 3.943, avg. samples / sec: 28119.19
Iteration:   3640, Loss function: 3.049, Average Loss: 3.949, avg. samples / sec: 28104.67
Iteration:   3640, Loss function: 4.003, Average Loss: 3.934, avg. samples / sec: 28077.80
Iteration:   3660, Loss function: 3.019, Average Loss: 3.938, avg. samples / sec: 28175.40
Iteration:   3660, Loss function: 3.490, Average Loss: 3.940, avg. samples / sec: 28137.29
Iteration:   3660, Loss function: 4.008, Average Loss: 3.934, avg. samples / sec: 28166.63
Iteration:   3660, Loss function: 3.181, Average Loss: 3.925, avg. samples / sec: 28123.82
:::MLL 1558579847.501 epoch_stop: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 819}}
:::MLL 1558579847.501 epoch_start: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 673}}
Iteration:   3680, Loss function: 2.994, Average Loss: 3.916, avg. samples / sec: 27949.96
Iteration:   3680, Loss function: 3.525, Average Loss: 3.928, avg. samples / sec: 27871.08
Iteration:   3680, Loss function: 3.424, Average Loss: 3.926, avg. samples / sec: 27869.33
Iteration:   3680, Loss function: 3.525, Average Loss: 3.926, avg. samples / sec: 27876.67
Iteration:   3700, Loss function: 2.910, Average Loss: 3.909, avg. samples / sec: 28030.11
Iteration:   3700, Loss function: 3.556, Average Loss: 3.917, avg. samples / sec: 28032.07
Iteration:   3700, Loss function: 3.620, Average Loss: 3.917, avg. samples / sec: 28025.49
Iteration:   3700, Loss function: 3.507, Average Loss: 3.918, avg. samples / sec: 27984.45
Iteration:   3720, Loss function: 3.578, Average Loss: 3.900, avg. samples / sec: 28107.70
Iteration:   3720, Loss function: 3.291, Average Loss: 3.904, avg. samples / sec: 28149.22
Iteration:   3720, Loss function: 2.994, Average Loss: 3.907, avg. samples / sec: 28095.08
Iteration:   3720, Loss function: 3.224, Average Loss: 3.911, avg. samples / sec: 28100.05
Iteration:   3740, Loss function: 3.701, Average Loss: 3.899, avg. samples / sec: 28023.47
Iteration:   3740, Loss function: 3.050, Average Loss: 3.894, avg. samples / sec: 28005.39
Iteration:   3740, Loss function: 3.951, Average Loss: 3.893, avg. samples / sec: 27991.73
Iteration:   3740, Loss function: 3.456, Average Loss: 3.900, avg. samples / sec: 27905.52
:::MLL 1558579851.672 epoch_stop: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 819}}
:::MLL 1558579851.672 epoch_start: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 673}}
:::MLL 1558579852.219 eval_start: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.10 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.10 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.10 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.10 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.49s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=2.80s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22499
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.38412
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.22924
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05563
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.23459
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.36775
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.21879
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.31759
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33365
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09404
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.35958
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.52758
Current AP: 0.22499 AP goal: 0.23000
:::MLL 1558579856.679 eval_accuracy: {"value": 0.22498611374924365, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 389}}
:::MLL 1558579856.702 eval_stop: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 392}}
:::MLL 1558579856.717 block_stop: {"value": null, "metadata": {"first_epoch_num": 44, "file": "train.py", "lineno": 804}}
:::MLL 1558579856.717 block_start: {"value": null, "metadata": {"first_epoch_num": 50, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3760, Loss function: 3.382, Average Loss: 3.889, avg. samples / sec: 5486.29
Iteration:   3760, Loss function: 3.369, Average Loss: 3.886, avg. samples / sec: 5486.43
Iteration:   3760, Loss function: 3.689, Average Loss: 3.886, avg. samples / sec: 5486.61
Iteration:   3760, Loss function: 3.790, Average Loss: 3.889, avg. samples / sec: 5489.93
Iteration:   3780, Loss function: 2.961, Average Loss: 3.878, avg. samples / sec: 27855.13
Iteration:   3780, Loss function: 3.849, Average Loss: 3.881, avg. samples / sec: 27827.20
Iteration:   3780, Loss function: 3.886, Average Loss: 3.883, avg. samples / sec: 27822.33
Iteration:   3780, Loss function: 4.024, Average Loss: 3.877, avg. samples / sec: 27726.32
Iteration:   3800, Loss function: 3.568, Average Loss: 3.870, avg. samples / sec: 28003.03
Iteration:   3800, Loss function: 3.594, Average Loss: 3.875, avg. samples / sec: 28009.96
Iteration:   3800, Loss function: 3.382, Average Loss: 3.872, avg. samples / sec: 27997.38
Iteration:   3800, Loss function: 3.098, Average Loss: 3.868, avg. samples / sec: 28042.01
:::MLL 1558579860.406 epoch_stop: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 819}}
:::MLL 1558579860.406 epoch_start: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 673}}
Iteration:   3820, Loss function: 3.495, Average Loss: 3.864, avg. samples / sec: 27916.02
Iteration:   3820, Loss function: 3.543, Average Loss: 3.860, avg. samples / sec: 27968.23
Iteration:   3820, Loss function: 3.454, Average Loss: 3.861, avg. samples / sec: 27877.35
Iteration:   3820, Loss function: 3.785, Average Loss: 3.866, avg. samples / sec: 27876.09
Iteration:   3840, Loss function: 3.088, Average Loss: 3.850, avg. samples / sec: 27936.74
Iteration:   3840, Loss function: 4.078, Average Loss: 3.857, avg. samples / sec: 27932.11
Iteration:   3840, Loss function: 3.247, Average Loss: 3.858, avg. samples / sec: 27956.14
Iteration:   3840, Loss function: 3.294, Average Loss: 3.850, avg. samples / sec: 27910.82
Iteration:   3860, Loss function: 3.610, Average Loss: 3.849, avg. samples / sec: 27856.12
Iteration:   3860, Loss function: 3.741, Average Loss: 3.843, avg. samples / sec: 27846.45
Iteration:   3860, Loss function: 3.128, Average Loss: 3.850, avg. samples / sec: 27849.57
Iteration:   3860, Loss function: 3.607, Average Loss: 3.842, avg. samples / sec: 27886.13
Iteration:   3880, Loss function: 3.632, Average Loss: 3.834, avg. samples / sec: 27979.57
Iteration:   3880, Loss function: 3.764, Average Loss: 3.842, avg. samples / sec: 27959.69
Iteration:   3880, Loss function: 3.446, Average Loss: 3.835, avg. samples / sec: 27927.81
Iteration:   3880, Loss function: 3.153, Average Loss: 3.840, avg. samples / sec: 27923.51
:::MLL 1558579864.585 epoch_stop: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 819}}
:::MLL 1558579864.585 epoch_start: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 673}}
Iteration:   3900, Loss function: 3.278, Average Loss: 3.831, avg. samples / sec: 28089.95
Iteration:   3900, Loss function: 2.969, Average Loss: 3.827, avg. samples / sec: 28071.85
Iteration:   3900, Loss function: 3.793, Average Loss: 3.826, avg. samples / sec: 28018.92
Iteration:   3900, Loss function: 3.897, Average Loss: 3.834, avg. samples / sec: 28015.70
Iteration:   3920, Loss function: 3.202, Average Loss: 3.819, avg. samples / sec: 28157.31
Iteration:   3920, Loss function: 4.015, Average Loss: 3.822, avg. samples / sec: 28140.36
Iteration:   3920, Loss function: 3.263, Average Loss: 3.818, avg. samples / sec: 28131.22
Iteration:   3920, Loss function: 2.858, Average Loss: 3.828, avg. samples / sec: 28115.91
Iteration:   3940, Loss function: 3.094, Average Loss: 3.810, avg. samples / sec: 28176.10
Iteration:   3940, Loss function: 3.349, Average Loss: 3.817, avg. samples / sec: 28150.13
Iteration:   3940, Loss function: 3.897, Average Loss: 3.820, avg. samples / sec: 28193.54
Iteration:   3940, Loss function: 3.725, Average Loss: 3.812, avg. samples / sec: 28088.96
Iteration:   3960, Loss function: 3.611, Average Loss: 3.803, avg. samples / sec: 28051.87
Iteration:   3960, Loss function: 3.150, Average Loss: 3.814, avg. samples / sec: 28053.33
Iteration:   3960, Loss function: 3.735, Average Loss: 3.811, avg. samples / sec: 28038.16
Iteration:   3960, Loss function: 3.007, Average Loss: 3.802, avg. samples / sec: 28056.25
:::MLL 1558579868.739 epoch_stop: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 819}}
:::MLL 1558579868.739 epoch_start: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 673}}
Iteration:   3980, Loss function: 3.488, Average Loss: 3.793, avg. samples / sec: 28007.44
Iteration:   3980, Loss function: 3.647, Average Loss: 3.800, avg. samples / sec: 27974.87
Iteration:   3980, Loss function: 3.365, Average Loss: 3.795, avg. samples / sec: 27926.60
Iteration:   3980, Loss function: 2.960, Average Loss: 3.806, avg. samples / sec: 27903.28
Iteration:   4000, Loss function: 3.420, Average Loss: 3.788, avg. samples / sec: 28075.28
Iteration:   4000, Loss function: 3.688, Average Loss: 3.792, avg. samples / sec: 28039.16
Iteration:   4000, Loss function: 2.903, Average Loss: 3.784, avg. samples / sec: 27994.36
Iteration:   4000, Loss function: 3.410, Average Loss: 3.798, avg. samples / sec: 28028.91
Iteration:   4020, Loss function: 3.221, Average Loss: 3.790, avg. samples / sec: 28163.03
Iteration:   4020, Loss function: 2.998, Average Loss: 3.782, avg. samples / sec: 28082.64
Iteration:   4020, Loss function: 3.139, Average Loss: 3.778, avg. samples / sec: 28114.37
Iteration:   4020, Loss function: 2.734, Average Loss: 3.785, avg. samples / sec: 28070.13
Iteration:   4040, Loss function: 3.616, Average Loss: 3.779, avg. samples / sec: 28018.26
Iteration:   4040, Loss function: 3.421, Average Loss: 3.772, avg. samples / sec: 28003.76
Iteration:   4040, Loss function: 3.993, Average Loss: 3.781, avg. samples / sec: 27991.91
Iteration:   4040, Loss function: 3.290, Average Loss: 3.770, avg. samples / sec: 28003.03
:::MLL 1558579872.960 epoch_stop: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 819}}
:::MLL 1558579872.961 epoch_start: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 673}}
Iteration:   4060, Loss function: 3.090, Average Loss: 3.764, avg. samples / sec: 27854.42
Iteration:   4060, Loss function: 3.514, Average Loss: 3.769, avg. samples / sec: 27823.32
Iteration:   4060, Loss function: 3.345, Average Loss: 3.762, avg. samples / sec: 27832.28
Iteration:   4060, Loss function: 3.189, Average Loss: 3.772, avg. samples / sec: 27802.48
Iteration:   4080, Loss function: 3.337, Average Loss: 3.760, avg. samples / sec: 28082.73
Iteration:   4080, Loss function: 3.229, Average Loss: 3.763, avg. samples / sec: 28097.89
Iteration:   4080, Loss function: 3.295, Average Loss: 3.753, avg. samples / sec: 28071.35
Iteration:   4080, Loss function: 3.792, Average Loss: 3.757, avg. samples / sec: 28009.45
Iteration:   4100, Loss function: 3.686, Average Loss: 3.747, avg. samples / sec: 28158.37
Iteration:   4100, Loss function: 3.484, Average Loss: 3.751, avg. samples / sec: 28162.38
Iteration:   4100, Loss function: 3.446, Average Loss: 3.756, avg. samples / sec: 28109.13
Iteration:   4100, Loss function: 3.834, Average Loss: 3.753, avg. samples / sec: 28085.94
Iteration:   4120, Loss function: 3.347, Average Loss: 3.745, avg. samples / sec: 28141.26
Iteration:   4120, Loss function: 3.649, Average Loss: 3.738, avg. samples / sec: 28080.71
Iteration:   4120, Loss function: 3.891, Average Loss: 3.750, avg. samples / sec: 28119.52
Iteration:   4120, Loss function: 3.049, Average Loss: 3.745, avg. samples / sec: 28091.52
:::MLL 1558579877.125 epoch_stop: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 819}}
:::MLL 1558579877.125 epoch_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 673}}
Iteration:   4140, Loss function: 3.409, Average Loss: 3.740, avg. samples / sec: 28047.57
Iteration:   4140, Loss function: 3.293, Average Loss: 3.741, avg. samples / sec: 28037.34
Iteration:   4140, Loss function: 3.954, Average Loss: 3.731, avg. samples / sec: 28030.53
Iteration:   4140, Loss function: 3.624, Average Loss: 3.739, avg. samples / sec: 28009.69
Iteration:   4160, Loss function: 3.348, Average Loss: 3.723, avg. samples / sec: 27983.53
Iteration:   4160, Loss function: 3.666, Average Loss: 3.737, avg. samples / sec: 27971.75
Iteration:   4160, Loss function: 3.513, Average Loss: 3.731, avg. samples / sec: 27957.15
Iteration:   4160, Loss function: 3.882, Average Loss: 3.731, avg. samples / sec: 27970.54
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
:::MLL 1558579879.537 eval_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.16 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.16 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.16 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.16 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.50s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.52s)
DONE (t=2.82s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22763
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.38940
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23308
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05618
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.23595
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.37219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22026
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32017
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33606
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09806
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36146
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.53020
Current AP: 0.22763 AP goal: 0.23000
:::MLL 1558579884.090 eval_accuracy: {"value": 0.22762506797793164, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 389}}
:::MLL 1558579884.117 eval_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 392}}
:::MLL 1558579884.131 block_stop: {"value": null, "metadata": {"first_epoch_num": 50, "file": "train.py", "lineno": 804}}
:::MLL 1558579884.131 block_start: {"value": null, "metadata": {"first_epoch_num": 55, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   4180, Loss function: 4.056, Average Loss: 3.725, avg. samples / sec: 5398.21
Iteration:   4180, Loss function: 3.006, Average Loss: 3.713, avg. samples / sec: 5396.90
Iteration:   4180, Loss function: 3.447, Average Loss: 3.725, avg. samples / sec: 5397.67
Iteration:   4180, Loss function: 3.340, Average Loss: 3.727, avg. samples / sec: 5396.75
:::MLL 1558579885.897 epoch_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 819}}
:::MLL 1558579885.897 epoch_start: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 673}}
Iteration:   4200, Loss function: 3.058, Average Loss: 3.708, avg. samples / sec: 27769.95
Iteration:   4200, Loss function: 3.429, Average Loss: 3.721, avg. samples / sec: 27784.28
Iteration:   4200, Loss function: 3.171, Average Loss: 3.718, avg. samples / sec: 27769.01
Iteration:   4200, Loss function: 3.605, Average Loss: 3.717, avg. samples / sec: 27767.01
Iteration:   4220, Loss function: 3.087, Average Loss: 3.701, avg. samples / sec: 27989.43
Iteration:   4220, Loss function: 2.973, Average Loss: 3.708, avg. samples / sec: 27990.16
Iteration:   4220, Loss function: 4.138, Average Loss: 3.708, avg. samples / sec: 27979.79
Iteration:   4220, Loss function: 3.544, Average Loss: 3.714, avg. samples / sec: 27939.04
Iteration:   4240, Loss function: 3.281, Average Loss: 3.692, avg. samples / sec: 27938.09
Iteration:   4240, Loss function: 2.611, Average Loss: 3.703, avg. samples / sec: 27991.24
Iteration:   4240, Loss function: 3.857, Average Loss: 3.702, avg. samples / sec: 27912.93
Iteration:   4240, Loss function: 3.234, Average Loss: 3.699, avg. samples / sec: 27898.82
Iteration:   4260, Loss function: 2.956, Average Loss: 3.693, avg. samples / sec: 28050.68
Iteration:   4260, Loss function: 2.595, Average Loss: 3.687, avg. samples / sec: 28031.95
Iteration:   4260, Loss function: 3.613, Average Loss: 3.692, avg. samples / sec: 28054.92
Iteration:   4260, Loss function: 3.229, Average Loss: 3.695, avg. samples / sec: 28049.16
:::MLL 1558579890.124 epoch_stop: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 819}}
:::MLL 1558579890.125 epoch_start: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 673}}
Iteration:   4280, Loss function: 3.555, Average Loss: 3.680, avg. samples / sec: 27987.01
Iteration:   4280, Loss function: 3.373, Average Loss: 3.685, avg. samples / sec: 28002.80
Iteration:   4280, Loss function: 3.166, Average Loss: 3.685, avg. samples / sec: 27999.16
Iteration:   4280, Loss function: 3.907, Average Loss: 3.684, avg. samples / sec: 27952.74
Iteration:   4300, Loss function: 3.314, Average Loss: 3.678, avg. samples / sec: 28042.46
Iteration:   4300, Loss function: 3.418, Average Loss: 3.674, avg. samples / sec: 28031.28
Iteration:   4300, Loss function: 3.253, Average Loss: 3.679, avg. samples / sec: 28019.34
Iteration:   4300, Loss function: 3.690, Average Loss: 3.680, avg. samples / sec: 27997.92
Iteration:   4320, Loss function: 3.785, Average Loss: 3.672, avg. samples / sec: 28112.72
Iteration:   4320, Loss function: 3.627, Average Loss: 3.670, avg. samples / sec: 28055.15
Iteration:   4320, Loss function: 3.280, Average Loss: 3.665, avg. samples / sec: 28055.78
Iteration:   4320, Loss function: 3.523, Average Loss: 3.673, avg. samples / sec: 28040.67
Iteration:   4340, Loss function: 3.048, Average Loss: 3.666, avg. samples / sec: 28246.42
Iteration:   4340, Loss function: 3.128, Average Loss: 3.666, avg. samples / sec: 28159.31
Iteration:   4340, Loss function: 3.904, Average Loss: 3.665, avg. samples / sec: 28182.00
Iteration:   4340, Loss function: 3.189, Average Loss: 3.657, avg. samples / sec: 28179.37
:::MLL 1558579894.281 epoch_stop: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 819}}
:::MLL 1558579894.281 epoch_start: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 673}}
Iteration:   4360, Loss function: 3.890, Average Loss: 3.657, avg. samples / sec: 28065.71
Iteration:   4360, Loss function: 3.804, Average Loss: 3.653, avg. samples / sec: 28063.79
Iteration:   4360, Loss function: 3.813, Average Loss: 3.659, avg. samples / sec: 28043.56
Iteration:   4360, Loss function: 3.223, Average Loss: 3.658, avg. samples / sec: 28007.89
Iteration:   4380, Loss function: 2.989, Average Loss: 3.652, avg. samples / sec: 28074.59
Iteration:   4380, Loss function: 3.605, Average Loss: 3.651, avg. samples / sec: 28051.52
Iteration:   4380, Loss function: 3.780, Average Loss: 3.652, avg. samples / sec: 28069.20
Iteration:   4380, Loss function: 3.288, Average Loss: 3.651, avg. samples / sec: 28000.70
Iteration:   4400, Loss function: 3.323, Average Loss: 3.645, avg. samples / sec: 28153.56
Iteration:   4400, Loss function: 4.534, Average Loss: 3.649, avg. samples / sec: 28171.34
Iteration:   4400, Loss function: 3.041, Average Loss: 3.645, avg. samples / sec: 28166.59
Iteration:   4400, Loss function: 4.309, Average Loss: 3.647, avg. samples / sec: 28072.31
Iteration:   4420, Loss function: 2.847, Average Loss: 3.640, avg. samples / sec: 28088.16
Iteration:   4420, Loss function: 3.737, Average Loss: 3.640, avg. samples / sec: 28117.36
Iteration:   4420, Loss function: 3.163, Average Loss: 3.640, avg. samples / sec: 28042.27
Iteration:   4420, Loss function: 3.086, Average Loss: 3.642, avg. samples / sec: 28038.62
:::MLL 1558579898.495 epoch_stop: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 819}}
:::MLL 1558579898.496 epoch_start: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 673}}
Iteration:   4440, Loss function: 3.690, Average Loss: 3.635, avg. samples / sec: 27946.28
Iteration:   4440, Loss function: 3.302, Average Loss: 3.635, avg. samples / sec: 27955.47
Iteration:   4440, Loss function: 2.680, Average Loss: 3.635, avg. samples / sec: 27944.69
Iteration:   4440, Loss function: 2.922, Average Loss: 3.638, avg. samples / sec: 27951.24
Iteration:   4460, Loss function: 3.341, Average Loss: 3.633, avg. samples / sec: 28000.53
Iteration:   4460, Loss function: 3.156, Average Loss: 3.630, avg. samples / sec: 27990.33
Iteration:   4460, Loss function: 3.646, Average Loss: 3.629, avg. samples / sec: 27957.46
Iteration:   4460, Loss function: 3.477, Average Loss: 3.632, avg. samples / sec: 27942.51
Iteration:   4480, Loss function: 3.229, Average Loss: 3.623, avg. samples / sec: 28048.67
Iteration:   4480, Loss function: 2.789, Average Loss: 3.626, avg. samples / sec: 28035.97
Iteration:   4480, Loss function: 3.810, Average Loss: 3.625, avg. samples / sec: 28074.01
Iteration:   4480, Loss function: 3.456, Average Loss: 3.622, avg. samples / sec: 28015.86
Iteration:   4500, Loss function: 3.905, Average Loss: 3.617, avg. samples / sec: 28021.18
Iteration:   4500, Loss function: 2.935, Average Loss: 3.616, avg. samples / sec: 27949.80
Iteration:   4500, Loss function: 3.380, Average Loss: 3.621, avg. samples / sec: 27931.57
Iteration:   4500, Loss function: 3.699, Average Loss: 3.620, avg. samples / sec: 27949.04
:::MLL 1558579902.667 epoch_stop: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 819}}
:::MLL 1558579902.668 epoch_start: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 673}}
Iteration:   4520, Loss function: 3.167, Average Loss: 3.614, avg. samples / sec: 27993.34
Iteration:   4520, Loss function: 3.342, Average Loss: 3.612, avg. samples / sec: 27945.48
Iteration:   4520, Loss function: 2.736, Average Loss: 3.613, avg. samples / sec: 27977.16
Iteration:   4520, Loss function: 3.309, Average Loss: 3.613, avg. samples / sec: 27848.97
Iteration:   4540, Loss function: 3.593, Average Loss: 3.608, avg. samples / sec: 27997.05
Iteration:   4540, Loss function: 2.860, Average Loss: 3.606, avg. samples / sec: 28003.70
Iteration:   4540, Loss function: 4.109, Average Loss: 3.607, avg. samples / sec: 28083.74
Iteration:   4540, Loss function: 3.051, Average Loss: 3.608, avg. samples / sec: 27970.60
Iteration:   4560, Loss function: 2.878, Average Loss: 3.599, avg. samples / sec: 28002.51
Iteration:   4560, Loss function: 3.117, Average Loss: 3.600, avg. samples / sec: 27998.51
Iteration:   4560, Loss function: 3.172, Average Loss: 3.601, avg. samples / sec: 27995.02
Iteration:   4560, Loss function: 2.734, Average Loss: 3.603, avg. samples / sec: 27983.10
Iteration:   4580, Loss function: 3.730, Average Loss: 3.599, avg. samples / sec: 27992.76
Iteration:   4580, Loss function: 3.444, Average Loss: 3.595, avg. samples / sec: 27981.32
Iteration:   4580, Loss function: 3.573, Average Loss: 3.592, avg. samples / sec: 27949.07
Iteration:   4580, Loss function: 3.249, Average Loss: 3.596, avg. samples / sec: 27891.68
:::MLL 1558579906.841 epoch_stop: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 819}}
:::MLL 1558579906.841 epoch_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 673}}
:::MLL 1558579907.007 eval_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.17 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.17 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.17 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.17 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=2.81s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.23172
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.39413
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23851
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05784
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24178
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.37799
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22349
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32464
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.34135
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09992
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36854
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.53351
Current AP: 0.23172 AP goal: 0.23000
:::MLL 1558579911.567 eval_accuracy: {"value": 0.23172320605622834, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 389}}
:::MLL 1558579911.598 eval_stop: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 392}}
:::MLL 1558579911.611 block_stop: {"value": null, "metadata": {"first_epoch_num": 55, "file": "train.py", "lineno": 804}}
:::MLL 1558579912.526 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 849}}

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-05-23 02:52:02 AM
RESULT,SINGLE_STAGE_DETECTOR,,351,nvidia,2019-05-23 02:46:11 AM
ENDING TIMING RUN AT 2019-05-23 02:52:02 AM
RESULT,SINGLE_STAGE_DETECTOR,,351,nvidia,2019-05-23 02:46:11 AM
ENDING TIMING RUN AT 2019-05-23 02:52:02 AM
RESULT,SINGLE_STAGE_DETECTOR,,351,nvidia,2019-05-23 02:46:11 AM
ENDING TIMING RUN AT 2019-05-23 02:52:02 AM
RESULT,SINGLE_STAGE_DETECTOR,,351,nvidia,2019-05-23 02:46:11 AM
