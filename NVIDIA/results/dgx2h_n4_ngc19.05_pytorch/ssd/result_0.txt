Beginning trial 5 of 5
Gathering sys log on circe-n001
:::MLL 1558579923.475 submission_benchmark: {"value": "ssd", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1558579923.476 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known ssd keys.
:::MLL 1558579923.476 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1558579923.477 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1558579923.477 submission_platform: {"value": "4xNVIDIA DGX-2H", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1558579923.477 submission_entry: {"value": "{'hardware': 'NVIDIA DGX-2H', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': 'Ubuntu 18.04.1 LTS / NVIDIA DGX Server 4.0.4 4.0.5', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.4-2.0.7', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '4', 'cpu': '2x Intel(R) Xeon(R) Platinum 8174 CPU @ 3.10GHz', 'num_cores': '48', 'num_vcpus': '96', 'accelerator': 'Tesla V100-SXM3-32GB-H', 'num_accelerators': '16', 'sys_mem_size': '1510 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '2x 894.3G + 8x 3.5T', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27800 Family [ConnectX-5]', 'num_network_cards': '10', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1558579923.478 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1558579923.478 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
:::MLL 1558579926.307 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558579926.323 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558579926.316 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558579926.329 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node circe-n001
+ pids+=($!)
+ set +x
Launching on node circe-n002
+ pids+=($!)
+ set +x
Launching on node circe-n003
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ pids+=($!)
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n001
+ set +x
Launching on node circe-n004
+ pids+=($!)
+ set +x
+ srun --mem=0 -N 1 -n 1 -w circe-n001 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=0 --master_addr=10.0.1.1 --master_port=5066' -e SLURM_JOB_ID=89450 -e SLURM_NTASKS_PER_NODE=16 cont_89450 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n002
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n003
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n004
+ srun --mem=0 -N 1 -n 1 -w circe-n002 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=1 --master_addr=10.0.1.1 --master_port=5066' -e SLURM_JOB_ID=89450 -e SLURM_NTASKS_PER_NODE=16 cont_89450 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w circe-n003 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=2 --master_addr=10.0.1.1 --master_port=5066' -e SLURM_JOB_ID=89450 -e SLURM_NTASKS_PER_NODE=16 cont_89450 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w circe-n004 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=3 --master_addr=10.0.1.1 --master_port=5066' -e SLURM_JOB_ID=89450 -e SLURM_NTASKS_PER_NODE=16 cont_89450 ./run_and_time.sh
Run vars: id 89450 gpus 16 mparams  --nnodes=4 --node_rank=0 --master_addr=10.0.1.1 --master_port=5066
Run vars: id 89450 gpus 16 mparams  --nnodes=4 --node_rank=3 --master_addr=10.0.1.1 --master_port=5066
Run vars: id 89450 gpus 16 mparams  --nnodes=4 --node_rank=2 --master_addr=10.0.1.1 --master_port=5066
Run vars: id 89450 gpus 16 mparams  --nnodes=4 --node_rank=1 --master_addr=10.0.1.1 --master_port=5066
STARTING TIMING RUN AT 2019-05-23 02:52:06 AM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=0 --master_addr=10.0.1.1 --master_port=5066 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 02:52:06 AM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=3 --master_addr=10.0.1.1 --master_port=5066 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 02:52:06 AM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=2 --master_addr=10.0.1.1 --master_port=5066 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 02:52:06 AM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=1 --master_addr=10.0.1.1 --master_port=5066 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558579936.888 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.888 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.888 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.888 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.888 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.889 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.889 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.890 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558579936.890 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558579936.890 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579936.890 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.890 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.890 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.890 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.891 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.891 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558579936.912 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.913 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579936.913 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558579936.916 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.917 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.917 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.917 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579936.918 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558579936.919 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.920 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.920 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.920 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579936.920 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.920 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558579936.922 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.922 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558579936.981 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.983 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558579936.986 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.986 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.987 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.987 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.987 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.988 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579936.988 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.988 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.988 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.988 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558579936.988 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.988 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.989 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579936.989 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558579937.066 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579937.067 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579937.067 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579937.068 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579937.068 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579937.068 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579937.068 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579937.069 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579937.069 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579937.069 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579937.070 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558579937.070 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579937.070 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579937.070 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558579937.070 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558579937.071 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
0 Using seed = 3929068151
1 Using seed = 3929068152
4 Using seed = 3929068155
3 Using seed = 3929068154
5 Using seed = 3929068156
2 Using seed = 3929068153
:::MLL 1558579969.521 max_samples: {"value": 1, "metadata": {"file": "utils.py", "lineno": 465}}
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
9 Using seed = 3929068160
11 Using seed = 3929068162
8 Using seed = 3929068159
7 Using seed = 3929068158
6 Using seed = 3929068157
22 Using seed = 3929068173
17 Using seed = 3929068168
19 Using seed = 3929068170
31 Using seed = 3929068182
18 Using seed = 3929068169
16 Using seed = 3929068167
20 Using seed = 3929068171
23 Using seed = 3929068174
21 Using seed = 3929068172
29 Using seed = 3929068180
30 Using seed = 3929068181
27 Using seed = 3929068178
24 Using seed = 3929068175
28 Using seed = 3929068179
25 Using seed = 3929068176
26 Using seed = 3929068177
33 Using seed = 3929068184
34 Using seed = 3929068185
42 Using seed = 3929068193
36 Using seed = 3929068187
45 Using seed = 3929068196
35 Using seed = 3929068186
41 Using seed = 3929068192
44 Using seed = 3929068195
37 Using seed = 3929068188
38 Using seed = 3929068189
46 Using seed = 3929068197
43 Using seed = 3929068194
47 Using seed = 3929068198
39 Using seed = 3929068190
32 Using seed = 3929068183
40 Using seed = 3929068191
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
58 Using seed = 3929068209
Delaying allreduces to the end of backward()
:::MLL 1558579970.278 model_bn_span: {"value": 24, "metadata": {"file": "train.py", "lineno": 480}}
:::MLL 1558579970.278 global_batch_size: {"value": 1536, "metadata": {"file": "train.py", "lineno": 481}}
62 Using seed = 3929068213
61 Using seed = 3929068212
63 Using seed = 3929068214
48 Using seed = 3929068199
60 Using seed = 3929068211
:::MLL 1558579970.289 opt_base_learning_rate: {"value": 0.14, "metadata": {"file": "train.py", "lineno": 511}}
:::MLL 1558579970.289 opt_weight_decay: {"value": 0.00017, "metadata": {"file": "train.py", "lineno": 513}}
:::MLL 1558579970.289 opt_learning_rate_warmup_steps: {"value": 850, "metadata": {"file": "train.py", "lineno": 516}}
:::MLL 1558579970.289 opt_learning_rate_warmup_factor: {"value": 0, "metadata": {"file": "train.py", "lineno": 518}}
49 Using seed = 3929068200
51 Using seed = 3929068202
52 Using seed = 3929068203
53 Using seed = 3929068204
50 Using seed = 3929068201
55 Using seed = 3929068206
57 Using seed = 3929068208
59 Using seed = 3929068210
56 Using seed = 3929068207
54 Using seed = 3929068205
10 Using seed = 3929068161
15 Using seed = 3929068166
14 Using seed = 3929068165
13 Using seed = 3929068164
12 Using seed = 3929068163
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
epoch nbatch loss
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
:::MLL 1558579980.938 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 604}}
:::MLL 1558579980.938 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 610}}
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
time_check a: 1558579982.648403645
time_check a: 1558579982.650462866
time_check a: 1558579982.650318861
time_check a: 1558579982.669745207
time_check b: 1558579989.346438646
time_check b: 1558579989.429739952
time_check b: 1558579989.605721235
time_check b: 1558579989.658742666
:::MLL 1558579990.622 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 32.74606450292497, "file": "train.py", "lineno": 669}}
:::MLL 1558579990.623 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 673}}
Iteration:      0, Loss function: 22.567, Average Loss: 0.023, avg. samples / sec: 105.40
Iteration:      0, Loss function: 22.661, Average Loss: 0.023, avg. samples / sec: 104.17
Iteration:      0, Loss function: 22.965, Average Loss: 0.023, avg. samples / sec: 102.93
Iteration:      0, Loss function: 22.730, Average Loss: 0.023, avg. samples / sec: 84.43
Iteration:     20, Loss function: 20.260, Average Loss: 0.442, avg. samples / sec: 21096.37
Iteration:     20, Loss function: 20.460, Average Loss: 0.443, avg. samples / sec: 21123.43
Iteration:     20, Loss function: 20.331, Average Loss: 0.443, avg. samples / sec: 21121.82
Iteration:     20, Loss function: 20.427, Average Loss: 0.442, avg. samples / sec: 20925.80
Iteration:     40, Loss function: 17.392, Average Loss: 0.825, avg. samples / sec: 27868.60
Iteration:     40, Loss function: 17.457, Average Loss: 0.828, avg. samples / sec: 27806.00
Iteration:     40, Loss function: 17.394, Average Loss: 0.827, avg. samples / sec: 27785.99
Iteration:     40, Loss function: 16.908, Average Loss: 0.827, avg. samples / sec: 27782.05
Iteration:     60, Loss function: 10.466, Average Loss: 1.069, avg. samples / sec: 28308.34
Iteration:     60, Loss function: 11.922, Average Loss: 1.076, avg. samples / sec: 28280.77
Iteration:     60, Loss function: 11.551, Average Loss: 1.069, avg. samples / sec: 28258.19
Iteration:     60, Loss function: 11.334, Average Loss: 1.074, avg. samples / sec: 28279.28
:::MLL 1558579996.403 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 819}}
:::MLL 1558579996.404 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 673}}
Iteration:     80, Loss function: 10.658, Average Loss: 1.265, avg. samples / sec: 28245.76
Iteration:     80, Loss function: 10.002, Average Loss: 1.255, avg. samples / sec: 28231.89
Iteration:     80, Loss function: 10.097, Average Loss: 1.259, avg. samples / sec: 28248.47
Iteration:     80, Loss function: 10.517, Average Loss: 1.262, avg. samples / sec: 28242.91
Iteration:    100, Loss function: 9.338, Average Loss: 1.427, avg. samples / sec: 28419.08
Iteration:    100, Loss function: 9.426, Average Loss: 1.424, avg. samples / sec: 28400.71
Iteration:    100, Loss function: 9.083, Average Loss: 1.433, avg. samples / sec: 28385.50
Iteration:    100, Loss function: 9.609, Average Loss: 1.429, avg. samples / sec: 28405.85
Iteration:    120, Loss function: 8.968, Average Loss: 1.573, avg. samples / sec: 28305.59
Iteration:    120, Loss function: 9.008, Average Loss: 1.582, avg. samples / sec: 28309.24
Iteration:    120, Loss function: 8.970, Average Loss: 1.575, avg. samples / sec: 28281.34
Iteration:    120, Loss function: 8.803, Average Loss: 1.579, avg. samples / sec: 28293.79
Iteration:    140, Loss function: 8.509, Average Loss: 1.722, avg. samples / sec: 28498.36
Iteration:    140, Loss function: 8.726, Average Loss: 1.725, avg. samples / sec: 28460.30
Iteration:    140, Loss function: 8.969, Average Loss: 1.718, avg. samples / sec: 28447.67
Iteration:    140, Loss function: 8.360, Average Loss: 1.717, avg. samples / sec: 28449.13
:::MLL 1558580000.519 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 819}}
:::MLL 1558580000.520 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 673}}
Iteration:    160, Loss function: 8.511, Average Loss: 1.851, avg. samples / sec: 28312.45
Iteration:    160, Loss function: 7.711, Average Loss: 1.852, avg. samples / sec: 28322.65
Iteration:    160, Loss function: 7.937, Average Loss: 1.859, avg. samples / sec: 28304.36
Iteration:    160, Loss function: 8.089, Average Loss: 1.858, avg. samples / sec: 28230.50
Iteration:    180, Loss function: 8.062, Average Loss: 1.984, avg. samples / sec: 28308.46
Iteration:    180, Loss function: 8.407, Average Loss: 1.977, avg. samples / sec: 28297.69
Iteration:    180, Loss function: 8.206, Average Loss: 1.976, avg. samples / sec: 28297.02
Iteration:    180, Loss function: 8.398, Average Loss: 1.982, avg. samples / sec: 28336.23
Iteration:    200, Loss function: 7.441, Average Loss: 2.105, avg. samples / sec: 28466.24
Iteration:    200, Loss function: 8.250, Average Loss: 2.107, avg. samples / sec: 28505.35
Iteration:    200, Loss function: 8.159, Average Loss: 2.100, avg. samples / sec: 28454.53
Iteration:    200, Loss function: 8.151, Average Loss: 2.098, avg. samples / sec: 28407.69
Iteration:    220, Loss function: 7.394, Average Loss: 2.210, avg. samples / sec: 28495.87
Iteration:    220, Loss function: 7.365, Average Loss: 2.219, avg. samples / sec: 28425.90
Iteration:    220, Loss function: 8.346, Average Loss: 2.217, avg. samples / sec: 28410.35
Iteration:    220, Loss function: 7.890, Average Loss: 2.212, avg. samples / sec: 28398.27
:::MLL 1558580004.685 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 819}}
:::MLL 1558580004.686 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 673}}
Iteration:    240, Loss function: 7.863, Average Loss: 2.330, avg. samples / sec: 28418.02
Iteration:    240, Loss function: 7.696, Average Loss: 2.324, avg. samples / sec: 28442.09
Iteration:    240, Loss function: 7.817, Average Loss: 2.324, avg. samples / sec: 28372.24
Iteration:    240, Loss function: 7.408, Average Loss: 2.332, avg. samples / sec: 28356.78
Iteration:    260, Loss function: 7.304, Average Loss: 2.427, avg. samples / sec: 28376.59
Iteration:    260, Loss function: 7.252, Average Loss: 2.435, avg. samples / sec: 28365.42
Iteration:    260, Loss function: 7.493, Average Loss: 2.425, avg. samples / sec: 28387.44
Iteration:    260, Loss function: 7.383, Average Loss: 2.435, avg. samples / sec: 28382.12
Iteration:    280, Loss function: 7.802, Average Loss: 2.527, avg. samples / sec: 28438.87
Iteration:    280, Loss function: 8.780, Average Loss: 2.530, avg. samples / sec: 28429.38
Iteration:    280, Loss function: 8.185, Average Loss: 2.536, avg. samples / sec: 28465.57
Iteration:    280, Loss function: 7.743, Average Loss: 2.535, avg. samples / sec: 28381.59
Iteration:    300, Loss function: 6.499, Average Loss: 2.628, avg. samples / sec: 28312.37
Iteration:    300, Loss function: 7.264, Average Loss: 2.625, avg. samples / sec: 28305.26
Iteration:    300, Loss function: 7.150, Average Loss: 2.631, avg. samples / sec: 28357.18
Iteration:    300, Loss function: 6.501, Average Loss: 2.632, avg. samples / sec: 28278.16
:::MLL 1558580008.798 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 819}}
:::MLL 1558580008.798 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 673}}
Iteration:    320, Loss function: 7.210, Average Loss: 2.718, avg. samples / sec: 28474.26
Iteration:    320, Loss function: 6.940, Average Loss: 2.721, avg. samples / sec: 28508.36
Iteration:    320, Loss function: 6.549, Average Loss: 2.712, avg. samples / sec: 28463.64
Iteration:    320, Loss function: 7.212, Average Loss: 2.718, avg. samples / sec: 28392.94
Iteration:    340, Loss function: 6.862, Average Loss: 2.794, avg. samples / sec: 28384.76
Iteration:    340, Loss function: 7.122, Average Loss: 2.799, avg. samples / sec: 28372.14
Iteration:    340, Loss function: 7.562, Average Loss: 2.802, avg. samples / sec: 28405.07
Iteration:    340, Loss function: 6.898, Average Loss: 2.802, avg. samples / sec: 28333.55
Iteration:    360, Loss function: 7.211, Average Loss: 2.879, avg. samples / sec: 28337.27
Iteration:    360, Loss function: 6.964, Average Loss: 2.884, avg. samples / sec: 28373.50
Iteration:    360, Loss function: 7.038, Average Loss: 2.877, avg. samples / sec: 28320.30
Iteration:    360, Loss function: 6.547, Average Loss: 2.885, avg. samples / sec: 28331.51
Iteration:    380, Loss function: 6.891, Average Loss: 2.954, avg. samples / sec: 28478.78
Iteration:    380, Loss function: 6.944, Average Loss: 2.958, avg. samples / sec: 28475.50
Iteration:    380, Loss function: 6.941, Average Loss: 2.951, avg. samples / sec: 28479.57
Iteration:    380, Loss function: 6.596, Average Loss: 2.961, avg. samples / sec: 28509.88
:::MLL 1558580012.909 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 819}}
:::MLL 1558580012.909 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 673}}
Iteration:    400, Loss function: 6.491, Average Loss: 3.030, avg. samples / sec: 28450.61
Iteration:    400, Loss function: 7.295, Average Loss: 3.038, avg. samples / sec: 28447.49
Iteration:    400, Loss function: 7.267, Average Loss: 3.032, avg. samples / sec: 28433.25
Iteration:    400, Loss function: 6.947, Average Loss: 3.037, avg. samples / sec: 28417.93
Iteration:    420, Loss function: 5.737, Average Loss: 3.102, avg. samples / sec: 28279.30
Iteration:    420, Loss function: 6.385, Average Loss: 3.107, avg. samples / sec: 28276.70
Iteration:    420, Loss function: 6.475, Average Loss: 3.106, avg. samples / sec: 28295.20
Iteration:    420, Loss function: 6.163, Average Loss: 3.103, avg. samples / sec: 28246.17
Iteration:    440, Loss function: 6.834, Average Loss: 3.171, avg. samples / sec: 28237.40
Iteration:    440, Loss function: 6.777, Average Loss: 3.174, avg. samples / sec: 28202.33
Iteration:    440, Loss function: 7.170, Average Loss: 3.175, avg. samples / sec: 28188.83
Iteration:    440, Loss function: 7.191, Average Loss: 3.170, avg. samples / sec: 28145.83
:::MLL 1558580017.086 epoch_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 819}}
:::MLL 1558580017.086 epoch_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 673}}
Iteration:    460, Loss function: 7.464, Average Loss: 3.255, avg. samples / sec: 28374.02
Iteration:    460, Loss function: 6.942, Average Loss: 3.263, avg. samples / sec: 28328.15
Iteration:    460, Loss function: 6.979, Average Loss: 3.262, avg. samples / sec: 28335.29
Iteration:    460, Loss function: 7.258, Average Loss: 3.256, avg. samples / sec: 28309.59
Iteration:    480, Loss function: 6.361, Average Loss: 3.324, avg. samples / sec: 28344.99
Iteration:    480, Loss function: 5.806, Average Loss: 3.321, avg. samples / sec: 28344.65
Iteration:    480, Loss function: 6.891, Average Loss: 3.327, avg. samples / sec: 28343.88
Iteration:    480, Loss function: 6.263, Average Loss: 3.329, avg. samples / sec: 28321.24
Iteration:    500, Loss function: 6.809, Average Loss: 3.385, avg. samples / sec: 28407.42
Iteration:    500, Loss function: 6.481, Average Loss: 3.379, avg. samples / sec: 28395.63
Iteration:    500, Loss function: 6.339, Average Loss: 3.389, avg. samples / sec: 28408.30
Iteration:    500, Loss function: 6.075, Average Loss: 3.380, avg. samples / sec: 28374.92
Iteration:    520, Loss function: 6.737, Average Loss: 3.436, avg. samples / sec: 28338.73
Iteration:    520, Loss function: 5.847, Average Loss: 3.435, avg. samples / sec: 28356.16
Iteration:    520, Loss function: 5.788, Average Loss: 3.442, avg. samples / sec: 28339.54
Iteration:    520, Loss function: 6.684, Average Loss: 3.439, avg. samples / sec: 28324.80
:::MLL 1558580021.203 epoch_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 819}}
:::MLL 1558580021.203 epoch_start: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 673}}
Iteration:    540, Loss function: 5.597, Average Loss: 3.486, avg. samples / sec: 28343.46
Iteration:    540, Loss function: 5.750, Average Loss: 3.489, avg. samples / sec: 28342.20
Iteration:    540, Loss function: 6.214, Average Loss: 3.488, avg. samples / sec: 28333.37
Iteration:    540, Loss function: 6.233, Average Loss: 3.495, avg. samples / sec: 28340.24
Iteration:    560, Loss function: 5.806, Average Loss: 3.535, avg. samples / sec: 28306.94
Iteration:    560, Loss function: 5.972, Average Loss: 3.540, avg. samples / sec: 28307.75
Iteration:    560, Loss function: 5.694, Average Loss: 3.543, avg. samples / sec: 28305.00
Iteration:    560, Loss function: 5.946, Average Loss: 3.539, avg. samples / sec: 28274.63
Iteration:    580, Loss function: 5.834, Average Loss: 3.590, avg. samples / sec: 28466.91
Iteration:    580, Loss function: 5.442, Average Loss: 3.588, avg. samples / sec: 28498.45
Iteration:    580, Loss function: 5.908, Average Loss: 3.586, avg. samples / sec: 28434.32
Iteration:    580, Loss function: 5.836, Average Loss: 3.582, avg. samples / sec: 28411.08
Iteration:    600, Loss function: 5.948, Average Loss: 3.623, avg. samples / sec: 28371.26
Iteration:    600, Loss function: 4.949, Average Loss: 3.633, avg. samples / sec: 28323.97
Iteration:    600, Loss function: 5.760, Average Loss: 3.635, avg. samples / sec: 28324.67
Iteration:    600, Loss function: 5.596, Average Loss: 3.632, avg. samples / sec: 28318.31
:::MLL 1558580025.319 epoch_stop: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 819}}
:::MLL 1558580025.320 epoch_start: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 673}}
Iteration:    620, Loss function: 6.053, Average Loss: 3.678, avg. samples / sec: 28291.65
Iteration:    620, Loss function: 5.783, Average Loss: 3.677, avg. samples / sec: 28237.14
Iteration:    620, Loss function: 5.777, Average Loss: 3.668, avg. samples / sec: 28208.09
Iteration:    620, Loss function: 5.373, Average Loss: 3.680, avg. samples / sec: 28206.58
Iteration:    640, Loss function: 6.035, Average Loss: 3.706, avg. samples / sec: 28440.48
Iteration:    640, Loss function: 5.963, Average Loss: 3.723, avg. samples / sec: 28441.00
Iteration:    640, Loss function: 6.375, Average Loss: 3.721, avg. samples / sec: 28370.96
Iteration:    640, Loss function: 5.374, Average Loss: 3.717, avg. samples / sec: 28370.94
Iteration:    660, Loss function: 5.885, Average Loss: 3.748, avg. samples / sec: 28377.79
Iteration:    660, Loss function: 5.420, Average Loss: 3.756, avg. samples / sec: 28414.35
Iteration:    660, Loss function: 5.762, Average Loss: 3.760, avg. samples / sec: 28393.76
Iteration:    660, Loss function: 5.391, Average Loss: 3.763, avg. samples / sec: 28363.68
Iteration:    680, Loss function: 5.001, Average Loss: 3.784, avg. samples / sec: 28332.38
Iteration:    680, Loss function: 5.861, Average Loss: 3.799, avg. samples / sec: 28347.44
Iteration:    680, Loss function: 5.894, Average Loss: 3.796, avg. samples / sec: 28316.87
Iteration:    680, Loss function: 5.960, Average Loss: 3.792, avg. samples / sec: 28318.80
:::MLL 1558580029.494 epoch_stop: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 819}}
:::MLL 1558580029.494 epoch_start: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 673}}
Iteration:    700, Loss function: 6.054, Average Loss: 3.830, avg. samples / sec: 28316.66
Iteration:    700, Loss function: 5.840, Average Loss: 3.831, avg. samples / sec: 28312.38
Iteration:    700, Loss function: 5.773, Average Loss: 3.832, avg. samples / sec: 28296.34
Iteration:    700, Loss function: 6.185, Average Loss: 3.819, avg. samples / sec: 28292.25
Iteration:    720, Loss function: 5.217, Average Loss: 3.856, avg. samples / sec: 28317.69
Iteration:    720, Loss function: 5.677, Average Loss: 3.868, avg. samples / sec: 28310.69
Iteration:    720, Loss function: 5.095, Average Loss: 3.863, avg. samples / sec: 28309.34
Iteration:    720, Loss function: 5.398, Average Loss: 3.863, avg. samples / sec: 28279.73
Iteration:    740, Loss function: 5.590, Average Loss: 3.897, avg. samples / sec: 28399.86
Iteration:    740, Loss function: 6.096, Average Loss: 3.893, avg. samples / sec: 28396.40
Iteration:    740, Loss function: 5.371, Average Loss: 3.892, avg. samples / sec: 28420.66
Iteration:    740, Loss function: 5.548, Average Loss: 3.886, avg. samples / sec: 28383.95
Iteration:    760, Loss function: 5.617, Average Loss: 3.925, avg. samples / sec: 28427.85
Iteration:    760, Loss function: 6.695, Average Loss: 3.929, avg. samples / sec: 28420.05
Iteration:    760, Loss function: 5.352, Average Loss: 3.924, avg. samples / sec: 28405.97
Iteration:    760, Loss function: 5.666, Average Loss: 3.916, avg. samples / sec: 28390.86
:::MLL 1558580033.610 epoch_stop: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 819}}
:::MLL 1558580033.610 epoch_start: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 673}}
Iteration:    780, Loss function: 5.637, Average Loss: 3.961, avg. samples / sec: 28382.55
Iteration:    780, Loss function: 5.474, Average Loss: 3.954, avg. samples / sec: 28375.30
Iteration:    780, Loss function: 5.768, Average Loss: 3.958, avg. samples / sec: 28359.67
Iteration:    780, Loss function: 5.375, Average Loss: 3.947, avg. samples / sec: 28380.27
Iteration:    800, Loss function: 5.454, Average Loss: 3.983, avg. samples / sec: 28385.53
Iteration:    800, Loss function: 5.057, Average Loss: 3.986, avg. samples / sec: 28404.12
Iteration:    800, Loss function: 5.916, Average Loss: 3.991, avg. samples / sec: 28325.96
Iteration:    800, Loss function: 4.687, Average Loss: 3.972, avg. samples / sec: 28339.66
Iteration:    820, Loss function: 6.855, Average Loss: 4.003, avg. samples / sec: 28444.52
Iteration:    820, Loss function: 6.263, Average Loss: 4.012, avg. samples / sec: 28353.94
Iteration:    820, Loss function: 6.035, Average Loss: 4.025, avg. samples / sec: 28407.50
Iteration:    820, Loss function: 6.566, Average Loss: 4.016, avg. samples / sec: 28336.68
:::MLL 1558580037.723 epoch_stop: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 819}}
:::MLL 1558580037.723 epoch_start: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 673}}
Iteration:    840, Loss function: 5.875, Average Loss: 4.039, avg. samples / sec: 28395.04
Iteration:    840, Loss function: 5.154, Average Loss: 4.060, avg. samples / sec: 28401.95
Iteration:    840, Loss function: 5.118, Average Loss: 4.052, avg. samples / sec: 28440.02
Iteration:    840, Loss function: 5.412, Average Loss: 4.048, avg. samples / sec: 28362.08
Iteration:    860, Loss function: 5.925, Average Loss: 4.084, avg. samples / sec: 28455.04
Iteration:    860, Loss function: 5.321, Average Loss: 4.070, avg. samples / sec: 28486.71
Iteration:    860, Loss function: 5.462, Average Loss: 4.075, avg. samples / sec: 28430.70
Iteration:    860, Loss function: 5.212, Average Loss: 4.062, avg. samples / sec: 28409.95
Iteration:    880, Loss function: 4.712, Average Loss: 4.090, avg. samples / sec: 28386.62
Iteration:    880, Loss function: 4.664, Average Loss: 4.095, avg. samples / sec: 28350.28
Iteration:    880, Loss function: 5.045, Average Loss: 4.106, avg. samples / sec: 28331.52
Iteration:    880, Loss function: 5.991, Average Loss: 4.097, avg. samples / sec: 28319.04
Iteration:    900, Loss function: 5.169, Average Loss: 4.108, avg. samples / sec: 28353.39
Iteration:    900, Loss function: 4.952, Average Loss: 4.114, avg. samples / sec: 28347.76
Iteration:    900, Loss function: 4.913, Average Loss: 4.118, avg. samples / sec: 28349.33
Iteration:    900, Loss function: 5.588, Average Loss: 4.124, avg. samples / sec: 28314.51
:::MLL 1558580041.892 epoch_stop: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 819}}
:::MLL 1558580041.892 epoch_start: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 673}}
Iteration:    920, Loss function: 5.055, Average Loss: 4.126, avg. samples / sec: 28369.06
Iteration:    920, Loss function: 5.634, Average Loss: 4.133, avg. samples / sec: 28375.08
Iteration:    920, Loss function: 4.710, Average Loss: 4.137, avg. samples / sec: 28408.97
Iteration:    920, Loss function: 4.797, Average Loss: 4.141, avg. samples / sec: 28383.96
Iteration:    940, Loss function: 5.016, Average Loss: 4.158, avg. samples / sec: 28472.36
Iteration:    940, Loss function: 5.325, Average Loss: 4.152, avg. samples / sec: 28440.41
Iteration:    940, Loss function: 5.376, Average Loss: 4.146, avg. samples / sec: 28434.69
Iteration:    940, Loss function: 5.323, Average Loss: 4.157, avg. samples / sec: 28435.40
Iteration:    960, Loss function: 4.884, Average Loss: 4.165, avg. samples / sec: 28389.71
Iteration:    960, Loss function: 5.215, Average Loss: 4.169, avg. samples / sec: 28377.93
Iteration:    960, Loss function: 4.433, Average Loss: 4.175, avg. samples / sec: 28353.47
Iteration:    960, Loss function: 5.029, Average Loss: 4.173, avg. samples / sec: 28321.97
Iteration:    980, Loss function: 4.667, Average Loss: 4.186, avg. samples / sec: 28485.88
Iteration:    980, Loss function: 4.460, Average Loss: 4.190, avg. samples / sec: 28509.06
Iteration:    980, Loss function: 5.537, Average Loss: 4.180, avg. samples / sec: 28463.27
Iteration:    980, Loss function: 4.340, Average Loss: 4.188, avg. samples / sec: 28485.91
:::MLL 1558580046.000 epoch_stop: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 819}}
:::MLL 1558580046.001 epoch_start: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 673}}
Iteration:   1000, Loss function: 4.656, Average Loss: 4.203, avg. samples / sec: 28366.95
Iteration:   1000, Loss function: 5.007, Average Loss: 4.201, avg. samples / sec: 28344.49
Iteration:   1000, Loss function: 4.753, Average Loss: 4.202, avg. samples / sec: 28380.53
Iteration:   1000, Loss function: 4.977, Average Loss: 4.192, avg. samples / sec: 28339.34
Iteration:   1020, Loss function: 5.372, Average Loss: 4.217, avg. samples / sec: 28337.70
Iteration:   1020, Loss function: 5.044, Average Loss: 4.213, avg. samples / sec: 28344.08
Iteration:   1020, Loss function: 5.011, Average Loss: 4.217, avg. samples / sec: 28349.22
Iteration:   1020, Loss function: 4.756, Average Loss: 4.208, avg. samples / sec: 28350.44
Iteration:   1040, Loss function: 5.146, Average Loss: 4.231, avg. samples / sec: 28353.35
Iteration:   1040, Loss function: 4.483, Average Loss: 4.232, avg. samples / sec: 28326.90
Iteration:   1040, Loss function: 4.978, Average Loss: 4.221, avg. samples / sec: 28339.03
Iteration:   1040, Loss function: 4.485, Average Loss: 4.224, avg. samples / sec: 28312.94
Iteration:   1060, Loss function: 4.758, Average Loss: 4.244, avg. samples / sec: 28359.00
Iteration:   1060, Loss function: 5.033, Average Loss: 4.241, avg. samples / sec: 28352.23
Iteration:   1060, Loss function: 4.790, Average Loss: 4.236, avg. samples / sec: 28351.28
Iteration:   1060, Loss function: 4.616, Average Loss: 4.234, avg. samples / sec: 28322.19
:::MLL 1558580050.119 epoch_stop: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 819}}
:::MLL 1558580050.119 epoch_start: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 673}}
Iteration:   1080, Loss function: 4.692, Average Loss: 4.253, avg. samples / sec: 28269.77
Iteration:   1080, Loss function: 5.315, Average Loss: 4.252, avg. samples / sec: 28281.06
Iteration:   1080, Loss function: 5.277, Average Loss: 4.250, avg. samples / sec: 28283.15
Iteration:   1080, Loss function: 4.481, Average Loss: 4.247, avg. samples / sec: 28250.90
Iteration:   1100, Loss function: 5.260, Average Loss: 4.267, avg. samples / sec: 28358.99
Iteration:   1100, Loss function: 4.558, Average Loss: 4.263, avg. samples / sec: 28352.14
Iteration:   1100, Loss function: 5.229, Average Loss: 4.263, avg. samples / sec: 28332.21
Iteration:   1100, Loss function: 4.501, Average Loss: 4.257, avg. samples / sec: 28370.12
Iteration:   1120, Loss function: 4.122, Average Loss: 4.271, avg. samples / sec: 28461.80
Iteration:   1120, Loss function: 5.180, Average Loss: 4.277, avg. samples / sec: 28446.27
Iteration:   1120, Loss function: 4.152, Average Loss: 4.267, avg. samples / sec: 28477.70
Iteration:   1120, Loss function: 4.342, Average Loss: 4.272, avg. samples / sec: 28454.87
Iteration:   1140, Loss function: 5.382, Average Loss: 4.281, avg. samples / sec: 28521.06
Iteration:   1140, Loss function: 4.623, Average Loss: 4.288, avg. samples / sec: 28473.57
Iteration:   1140, Loss function: 5.849, Average Loss: 4.278, avg. samples / sec: 28502.80
Iteration:   1140, Loss function: 5.167, Average Loss: 4.284, avg. samples / sec: 28443.03
:::MLL 1558580054.287 epoch_stop: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 819}}
:::MLL 1558580054.287 epoch_start: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 673}}
Iteration:   1160, Loss function: 4.800, Average Loss: 4.300, avg. samples / sec: 28226.73
Iteration:   1160, Loss function: 4.409, Average Loss: 4.289, avg. samples / sec: 28220.29
Iteration:   1160, Loss function: 4.823, Average Loss: 4.295, avg. samples / sec: 28239.72
Iteration:   1160, Loss function: 5.122, Average Loss: 4.295, avg. samples / sec: 28191.56
Iteration:   1180, Loss function: 4.030, Average Loss: 4.294, avg. samples / sec: 28338.31
Iteration:   1180, Loss function: 5.280, Average Loss: 4.299, avg. samples / sec: 28334.96
Iteration:   1180, Loss function: 5.165, Average Loss: 4.307, avg. samples / sec: 28330.00
Iteration:   1180, Loss function: 4.432, Average Loss: 4.304, avg. samples / sec: 28349.10
Iteration:   1200, Loss function: 5.136, Average Loss: 4.308, avg. samples / sec: 28366.90
Iteration:   1200, Loss function: 4.484, Average Loss: 4.301, avg. samples / sec: 28346.33
Iteration:   1200, Loss function: 4.717, Average Loss: 4.315, avg. samples / sec: 28330.05
Iteration:   1200, Loss function: 4.435, Average Loss: 4.315, avg. samples / sec: 28323.23
Iteration:   1220, Loss function: 4.701, Average Loss: 4.322, avg. samples / sec: 28410.79
Iteration:   1220, Loss function: 4.431, Average Loss: 4.322, avg. samples / sec: 28407.51
Iteration:   1220, Loss function: 5.221, Average Loss: 4.313, avg. samples / sec: 28311.09
Iteration:   1220, Loss function: 4.282, Average Loss: 4.309, avg. samples / sec: 28315.00
:::MLL 1558580058.407 epoch_stop: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 819}}
:::MLL 1558580058.407 epoch_start: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 673}}
Iteration:   1240, Loss function: 4.808, Average Loss: 4.318, avg. samples / sec: 28217.03
Iteration:   1240, Loss function: 4.921, Average Loss: 4.315, avg. samples / sec: 28228.86
Iteration:   1240, Loss function: 4.305, Average Loss: 4.328, avg. samples / sec: 28149.35
Iteration:   1240, Loss function: 5.009, Average Loss: 4.327, avg. samples / sec: 28109.59
Iteration:   1260, Loss function: 4.351, Average Loss: 4.332, avg. samples / sec: 28318.50
Iteration:   1260, Loss function: 4.739, Average Loss: 4.323, avg. samples / sec: 28295.21
Iteration:   1260, Loss function: 4.837, Average Loss: 4.333, avg. samples / sec: 28353.69
Iteration:   1260, Loss function: 5.339, Average Loss: 4.320, avg. samples / sec: 28162.59
Iteration:   1280, Loss function: 4.903, Average Loss: 4.337, avg. samples / sec: 28357.14
Iteration:   1280, Loss function: 4.711, Average Loss: 4.329, avg. samples / sec: 28492.58
Iteration:   1280, Loss function: 4.331, Average Loss: 4.341, avg. samples / sec: 28333.54
Iteration:   1280, Loss function: 5.003, Average Loss: 4.329, avg. samples / sec: 28326.47
:::MLL 1558580062.534 epoch_stop: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 819}}
:::MLL 1558580062.534 epoch_start: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 673}}
Iteration:   1300, Loss function: 4.727, Average Loss: 4.336, avg. samples / sec: 28347.75
Iteration:   1300, Loss function: 4.753, Average Loss: 4.351, avg. samples / sec: 28337.88
Iteration:   1300, Loss function: 4.308, Average Loss: 4.346, avg. samples / sec: 28275.05
Iteration:   1300, Loss function: 4.155, Average Loss: 4.334, avg. samples / sec: 28274.71
Iteration:   1320, Loss function: 4.409, Average Loss: 4.354, avg. samples / sec: 28341.83
Iteration:   1320, Loss function: 3.941, Average Loss: 4.351, avg. samples / sec: 28366.17
Iteration:   1320, Loss function: 4.483, Average Loss: 4.340, avg. samples / sec: 28327.38
Iteration:   1320, Loss function: 4.379, Average Loss: 4.336, avg. samples / sec: 28320.71
Iteration:   1340, Loss function: 4.502, Average Loss: 4.354, avg. samples / sec: 28335.47
Iteration:   1340, Loss function: 4.417, Average Loss: 4.344, avg. samples / sec: 28340.24
Iteration:   1340, Loss function: 4.779, Average Loss: 4.341, avg. samples / sec: 28382.52
Iteration:   1340, Loss function: 4.655, Average Loss: 4.360, avg. samples / sec: 28320.89
Iteration:   1360, Loss function: 4.702, Average Loss: 4.350, avg. samples / sec: 28410.21
Iteration:   1360, Loss function: 4.621, Average Loss: 4.360, avg. samples / sec: 28402.95
Iteration:   1360, Loss function: 4.863, Average Loss: 4.367, avg. samples / sec: 28410.38
Iteration:   1360, Loss function: 4.752, Average Loss: 4.349, avg. samples / sec: 28396.83
:::MLL 1558580066.707 epoch_stop: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 819}}
:::MLL 1558580066.708 epoch_start: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 673}}
Iteration:   1380, Loss function: 5.019, Average Loss: 4.350, avg. samples / sec: 28323.70
Iteration:   1380, Loss function: 4.789, Average Loss: 4.354, avg. samples / sec: 28310.77
Iteration:   1380, Loss function: 3.964, Average Loss: 4.363, avg. samples / sec: 28299.61
Iteration:   1380, Loss function: 4.291, Average Loss: 4.369, avg. samples / sec: 28272.81
Iteration:   1400, Loss function: 4.335, Average Loss: 4.365, avg. samples / sec: 28260.88
Iteration:   1400, Loss function: 4.222, Average Loss: 4.354, avg. samples / sec: 28238.46
Iteration:   1400, Loss function: 4.309, Average Loss: 4.370, avg. samples / sec: 28277.23
Iteration:   1400, Loss function: 4.494, Average Loss: 4.355, avg. samples / sec: 28231.90
Iteration:   1420, Loss function: 3.581, Average Loss: 4.371, avg. samples / sec: 28258.77
Iteration:   1420, Loss function: 4.583, Average Loss: 4.356, avg. samples / sec: 28263.44
Iteration:   1420, Loss function: 4.353, Average Loss: 4.357, avg. samples / sec: 28245.81
Iteration:   1420, Loss function: 4.124, Average Loss: 4.367, avg. samples / sec: 28204.82
Iteration:   1440, Loss function: 4.346, Average Loss: 4.371, avg. samples / sec: 28357.27
Iteration:   1440, Loss function: 4.334, Average Loss: 4.359, avg. samples / sec: 28354.06
Iteration:   1440, Loss function: 3.954, Average Loss: 4.368, avg. samples / sec: 28384.92
Iteration:   1440, Loss function: 5.167, Average Loss: 4.358, avg. samples / sec: 28348.52
:::MLL 1558580070.840 epoch_stop: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 819}}
:::MLL 1558580070.840 epoch_start: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 673}}
Iteration:   1460, Loss function: 3.788, Average Loss: 4.357, avg. samples / sec: 28208.93
Iteration:   1460, Loss function: 4.786, Average Loss: 4.366, avg. samples / sec: 28207.11
Iteration:   1460, Loss function: 4.689, Average Loss: 4.374, avg. samples / sec: 28182.18
Iteration:   1460, Loss function: 4.191, Average Loss: 4.360, avg. samples / sec: 28161.66
Iteration:   1480, Loss function: 3.920, Average Loss: 4.368, avg. samples / sec: 28405.69
Iteration:   1480, Loss function: 4.618, Average Loss: 4.358, avg. samples / sec: 28400.76
Iteration:   1480, Loss function: 4.196, Average Loss: 4.373, avg. samples / sec: 28412.38
Iteration:   1480, Loss function: 4.504, Average Loss: 4.363, avg. samples / sec: 28433.66
Iteration:   1500, Loss function: 4.611, Average Loss: 4.360, avg. samples / sec: 28434.59
Iteration:   1500, Loss function: 4.450, Average Loss: 4.368, avg. samples / sec: 28430.27
Iteration:   1500, Loss function: 4.589, Average Loss: 4.372, avg. samples / sec: 28388.52
Iteration:   1500, Loss function: 4.546, Average Loss: 4.376, avg. samples / sec: 28393.20
Iteration:   1520, Loss function: 4.598, Average Loss: 4.376, avg. samples / sec: 28485.11
Iteration:   1520, Loss function: 4.625, Average Loss: 4.365, avg. samples / sec: 28436.83
Iteration:   1520, Loss function: 4.445, Average Loss: 4.374, avg. samples / sec: 28474.22
Iteration:   1520, Loss function: 4.863, Average Loss: 4.366, avg. samples / sec: 28440.42
:::MLL 1558580074.950 epoch_stop: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 819}}
:::MLL 1558580074.950 epoch_start: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 673}}
Iteration:   1540, Loss function: 4.049, Average Loss: 4.377, avg. samples / sec: 28383.95
Iteration:   1540, Loss function: 4.592, Average Loss: 4.365, avg. samples / sec: 28379.65
Iteration:   1540, Loss function: 4.810, Average Loss: 4.367, avg. samples / sec: 28387.43
Iteration:   1540, Loss function: 5.341, Average Loss: 4.375, avg. samples / sec: 28374.90
Iteration:   1560, Loss function: 4.575, Average Loss: 4.363, avg. samples / sec: 28385.94
Iteration:   1560, Loss function: 4.775, Average Loss: 4.378, avg. samples / sec: 28376.87
Iteration:   1560, Loss function: 4.893, Average Loss: 4.377, avg. samples / sec: 28394.46
Iteration:   1560, Loss function: 4.788, Average Loss: 4.367, avg. samples / sec: 28327.63
Iteration:   1580, Loss function: 4.330, Average Loss: 4.384, avg. samples / sec: 28335.73
Iteration:   1580, Loss function: 4.355, Average Loss: 4.361, avg. samples / sec: 28328.83
Iteration:   1580, Loss function: 4.842, Average Loss: 4.380, avg. samples / sec: 28324.18
Iteration:   1580, Loss function: 4.315, Average Loss: 4.367, avg. samples / sec: 28315.98
Iteration:   1600, Loss function: 4.705, Average Loss: 4.385, avg. samples / sec: 28406.74
Iteration:   1600, Loss function: 4.640, Average Loss: 4.360, avg. samples / sec: 28401.40
Iteration:   1600, Loss function: 4.086, Average Loss: 4.379, avg. samples / sec: 28397.80
Iteration:   1600, Loss function: 4.468, Average Loss: 4.367, avg. samples / sec: 28462.79
:::MLL 1558580079.117 epoch_stop: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 819}}
:::MLL 1558580079.117 epoch_start: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 673}}
Iteration:   1620, Loss function: 4.000, Average Loss: 4.363, avg. samples / sec: 28358.39
Iteration:   1620, Loss function: 4.030, Average Loss: 4.384, avg. samples / sec: 28350.79
Iteration:   1620, Loss function: 4.052, Average Loss: 4.362, avg. samples / sec: 28360.22
Iteration:   1620, Loss function: 3.926, Average Loss: 4.375, avg. samples / sec: 28333.56
Iteration:   1640, Loss function: 4.482, Average Loss: 4.363, avg. samples / sec: 28335.65
Iteration:   1640, Loss function: 3.953, Average Loss: 4.363, avg. samples / sec: 28326.95
Iteration:   1640, Loss function: 4.598, Average Loss: 4.386, avg. samples / sec: 28278.88
Iteration:   1640, Loss function: 4.542, Average Loss: 4.375, avg. samples / sec: 28310.69
Iteration:   1660, Loss function: 4.754, Average Loss: 4.374, avg. samples / sec: 28357.90
Iteration:   1660, Loss function: 4.068, Average Loss: 4.386, avg. samples / sec: 28349.13
Iteration:   1660, Loss function: 4.391, Average Loss: 4.362, avg. samples / sec: 28306.76
Iteration:   1660, Loss function: 4.725, Average Loss: 4.365, avg. samples / sec: 28282.36
:::MLL 1558580083.243 epoch_stop: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 819}}
:::MLL 1558580083.243 epoch_start: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 673}}
Iteration:   1680, Loss function: 4.433, Average Loss: 4.361, avg. samples / sec: 28187.72
Iteration:   1680, Loss function: 4.193, Average Loss: 4.368, avg. samples / sec: 28176.10
Iteration:   1680, Loss function: 3.910, Average Loss: 4.383, avg. samples / sec: 28140.58
Iteration:   1680, Loss function: 4.560, Average Loss: 4.378, avg. samples / sec: 28120.29
Iteration:   1700, Loss function: 4.625, Average Loss: 4.376, avg. samples / sec: 28444.96
Iteration:   1700, Loss function: 4.062, Average Loss: 4.359, avg. samples / sec: 28380.13
Iteration:   1700, Loss function: 4.224, Average Loss: 4.385, avg. samples / sec: 28418.99
Iteration:   1700, Loss function: 3.803, Average Loss: 4.365, avg. samples / sec: 28389.63
Iteration:   1720, Loss function: 3.974, Average Loss: 4.384, avg. samples / sec: 28395.10
Iteration:   1720, Loss function: 4.046, Average Loss: 4.364, avg. samples / sec: 28393.66
Iteration:   1720, Loss function: 4.579, Average Loss: 4.376, avg. samples / sec: 28378.05
Iteration:   1720, Loss function: 5.217, Average Loss: 4.359, avg. samples / sec: 28378.61
Iteration:   1740, Loss function: 3.870, Average Loss: 4.381, avg. samples / sec: 28190.40
Iteration:   1740, Loss function: 4.293, Average Loss: 4.367, avg. samples / sec: 28192.44
Iteration:   1740, Loss function: 4.229, Average Loss: 4.359, avg. samples / sec: 28194.89
Iteration:   1740, Loss function: 3.480, Average Loss: 4.373, avg. samples / sec: 28185.76
:::MLL 1558580087.365 epoch_stop: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 819}}
:::MLL 1558580087.366 epoch_start: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 673}}
Iteration:   1760, Loss function: 4.117, Average Loss: 4.376, avg. samples / sec: 28304.68
Iteration:   1760, Loss function: 3.772, Average Loss: 4.372, avg. samples / sec: 28307.88
Iteration:   1760, Loss function: 4.815, Average Loss: 4.358, avg. samples / sec: 28290.73
Iteration:   1760, Loss function: 4.716, Average Loss: 4.366, avg. samples / sec: 28258.25
Iteration:   1780, Loss function: 4.226, Average Loss: 4.371, avg. samples / sec: 28369.15
Iteration:   1780, Loss function: 3.764, Average Loss: 4.364, avg. samples / sec: 28397.75
Iteration:   1780, Loss function: 3.871, Average Loss: 4.356, avg. samples / sec: 28358.67
Iteration:   1780, Loss function: 3.661, Average Loss: 4.376, avg. samples / sec: 28327.08
Iteration:   1800, Loss function: 4.210, Average Loss: 4.375, avg. samples / sec: 28396.10
Iteration:   1800, Loss function: 4.893, Average Loss: 4.363, avg. samples / sec: 28371.59
Iteration:   1800, Loss function: 4.079, Average Loss: 4.354, avg. samples / sec: 28382.79
Iteration:   1800, Loss function: 4.164, Average Loss: 4.371, avg. samples / sec: 28282.74
Iteration:   1820, Loss function: 4.008, Average Loss: 4.370, avg. samples / sec: 28271.15
Iteration:   1820, Loss function: 3.502, Average Loss: 4.367, avg. samples / sec: 28356.36
Iteration:   1820, Loss function: 4.624, Average Loss: 4.359, avg. samples / sec: 28260.51
Iteration:   1820, Loss function: 5.067, Average Loss: 4.355, avg. samples / sec: 28227.48
:::MLL 1558580091.541 epoch_stop: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 819}}
:::MLL 1558580091.542 epoch_start: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 673}}
Iteration:   1840, Loss function: 4.447, Average Loss: 4.356, avg. samples / sec: 28381.43
Iteration:   1840, Loss function: 4.719, Average Loss: 4.369, avg. samples / sec: 28362.33
Iteration:   1840, Loss function: 4.500, Average Loss: 4.351, avg. samples / sec: 28347.60
Iteration:   1840, Loss function: 4.002, Average Loss: 4.361, avg. samples / sec: 28285.54
Iteration:   1860, Loss function: 3.830, Average Loss: 4.357, avg. samples / sec: 28478.90
Iteration:   1860, Loss function: 4.180, Average Loss: 4.355, avg. samples / sec: 28388.92
Iteration:   1860, Loss function: 4.369, Average Loss: 4.351, avg. samples / sec: 28438.16
Iteration:   1860, Loss function: 4.630, Average Loss: 4.368, avg. samples / sec: 28331.36
Iteration:   1880, Loss function: 4.253, Average Loss: 4.350, avg. samples / sec: 28387.59
Iteration:   1880, Loss function: 4.180, Average Loss: 4.367, avg. samples / sec: 28451.03
Iteration:   1880, Loss function: 4.298, Average Loss: 4.355, avg. samples / sec: 28375.43
Iteration:   1880, Loss function: 4.582, Average Loss: 4.349, avg. samples / sec: 28359.09
Iteration:   1900, Loss function: 4.553, Average Loss: 4.349, avg. samples / sec: 28280.53
Iteration:   1900, Loss function: 5.085, Average Loss: 4.364, avg. samples / sec: 28278.09
Iteration:   1900, Loss function: 4.200, Average Loss: 4.354, avg. samples / sec: 28272.90
Iteration:   1900, Loss function: 3.988, Average Loss: 4.350, avg. samples / sec: 28267.78
:::MLL 1558580095.658 epoch_stop: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 819}}
:::MLL 1558580095.658 epoch_start: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 673}}
Iteration:   1920, Loss function: 4.450, Average Loss: 4.351, avg. samples / sec: 28330.74
Iteration:   1920, Loss function: 4.150, Average Loss: 4.365, avg. samples / sec: 28317.55
Iteration:   1920, Loss function: 4.700, Average Loss: 4.345, avg. samples / sec: 28290.39
Iteration:   1920, Loss function: 3.786, Average Loss: 4.349, avg. samples / sec: 28352.41
Iteration:   1940, Loss function: 5.240, Average Loss: 4.342, avg. samples / sec: 28260.65
Iteration:   1940, Loss function: 3.983, Average Loss: 4.345, avg. samples / sec: 28250.34
Iteration:   1940, Loss function: 4.230, Average Loss: 4.350, avg. samples / sec: 28223.39
Iteration:   1940, Loss function: 4.474, Average Loss: 4.365, avg. samples / sec: 28217.89
Iteration:   1960, Loss function: 3.458, Average Loss: 4.358, avg. samples / sec: 28386.76
Iteration:   1960, Loss function: 4.113, Average Loss: 4.341, avg. samples / sec: 28363.06
Iteration:   1960, Loss function: 4.239, Average Loss: 4.347, avg. samples / sec: 28379.05
Iteration:   1960, Loss function: 4.906, Average Loss: 4.344, avg. samples / sec: 28370.37
Iteration:   1980, Loss function: 4.522, Average Loss: 4.340, avg. samples / sec: 28331.93
Iteration:   1980, Loss function: 4.508, Average Loss: 4.342, avg. samples / sec: 28334.63
Iteration:   1980, Loss function: 4.217, Average Loss: 4.343, avg. samples / sec: 28325.21
Iteration:   1980, Loss function: 4.101, Average Loss: 4.351, avg. samples / sec: 28279.94
:::MLL 1558580099.784 epoch_stop: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 819}}
:::MLL 1558580099.785 epoch_start: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 673}}
Iteration:   2000, Loss function: 4.900, Average Loss: 4.351, avg. samples / sec: 28299.05
Iteration:   2000, Loss function: 4.058, Average Loss: 4.339, avg. samples / sec: 28247.50
Iteration:   2000, Loss function: 3.809, Average Loss: 4.338, avg. samples / sec: 28245.77
Iteration:   2000, Loss function: 3.966, Average Loss: 4.340, avg. samples / sec: 28213.88
Iteration:   2020, Loss function: 4.265, Average Loss: 4.334, avg. samples / sec: 28321.63
Iteration:   2020, Loss function: 3.997, Average Loss: 4.335, avg. samples / sec: 28319.67
Iteration:   2020, Loss function: 4.550, Average Loss: 4.345, avg. samples / sec: 28289.34
Iteration:   2020, Loss function: 4.317, Average Loss: 4.337, avg. samples / sec: 28318.70
Iteration:   2040, Loss function: 4.001, Average Loss: 4.341, avg. samples / sec: 28237.75
Iteration:   2040, Loss function: 4.320, Average Loss: 4.333, avg. samples / sec: 28208.79
Iteration:   2040, Loss function: 4.393, Average Loss: 4.332, avg. samples / sec: 28205.06
Iteration:   2040, Loss function: 4.146, Average Loss: 4.334, avg. samples / sec: 28235.03
Iteration:   2060, Loss function: 4.091, Average Loss: 4.328, avg. samples / sec: 28377.73
Iteration:   2060, Loss function: 4.206, Average Loss: 4.342, avg. samples / sec: 28329.08
Iteration:   2060, Loss function: 4.683, Average Loss: 4.333, avg. samples / sec: 28319.08
Iteration:   2060, Loss function: 4.207, Average Loss: 4.330, avg. samples / sec: 28333.69
:::MLL 1558580103.964 epoch_stop: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 819}}
:::MLL 1558580103.965 epoch_start: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 673}}
Iteration:   2080, Loss function: 3.778, Average Loss: 4.324, avg. samples / sec: 28315.04
Iteration:   2080, Loss function: 4.229, Average Loss: 4.340, avg. samples / sec: 28357.50
Iteration:   2080, Loss function: 4.412, Average Loss: 4.331, avg. samples / sec: 28361.87
Iteration:   2080, Loss function: 4.059, Average Loss: 4.333, avg. samples / sec: 28362.08
Iteration:   2100, Loss function: 4.317, Average Loss: 4.327, avg. samples / sec: 28264.92
Iteration:   2100, Loss function: 4.452, Average Loss: 4.319, avg. samples / sec: 28254.78
Iteration:   2100, Loss function: 4.605, Average Loss: 4.339, avg. samples / sec: 28258.38
Iteration:   2100, Loss function: 4.037, Average Loss: 4.332, avg. samples / sec: 28229.08
Iteration:   2120, Loss function: 3.988, Average Loss: 4.326, avg. samples / sec: 28298.47
Iteration:   2120, Loss function: 4.190, Average Loss: 4.328, avg. samples / sec: 28334.43
Iteration:   2120, Loss function: 3.758, Average Loss: 4.336, avg. samples / sec: 28292.11
Iteration:   2120, Loss function: 3.342, Average Loss: 4.315, avg. samples / sec: 28280.80
:::MLL 1558580108.087 epoch_stop: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 819}}
:::MLL 1558580108.087 epoch_start: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 673}}
Iteration:   2140, Loss function: 3.456, Average Loss: 4.309, avg. samples / sec: 28317.07
Iteration:   2140, Loss function: 3.945, Average Loss: 4.328, avg. samples / sec: 28300.40
Iteration:   2140, Loss function: 4.168, Average Loss: 4.323, avg. samples / sec: 28289.69
Iteration:   2140, Loss function: 3.876, Average Loss: 4.320, avg. samples / sec: 28259.29
Iteration:   2160, Loss function: 4.840, Average Loss: 4.306, avg. samples / sec: 28314.79
Iteration:   2160, Loss function: 4.241, Average Loss: 4.313, avg. samples / sec: 28352.35
Iteration:   2160, Loss function: 4.461, Average Loss: 4.317, avg. samples / sec: 28268.91
Iteration:   2160, Loss function: 4.245, Average Loss: 4.326, avg. samples / sec: 28253.84
Iteration:   2180, Loss function: 4.240, Average Loss: 4.312, avg. samples / sec: 28292.80
Iteration:   2180, Loss function: 4.059, Average Loss: 4.320, avg. samples / sec: 28302.40
Iteration:   2180, Loss function: 3.920, Average Loss: 4.299, avg. samples / sec: 28215.21
Iteration:   2180, Loss function: 4.252, Average Loss: 4.312, avg. samples / sec: 28197.62
Iteration:   2200, Loss function: 4.347, Average Loss: 4.310, avg. samples / sec: 28424.54
Iteration:   2200, Loss function: 4.469, Average Loss: 4.316, avg. samples / sec: 28422.25
Iteration:   2200, Loss function: 4.123, Average Loss: 4.306, avg. samples / sec: 28457.56
Iteration:   2200, Loss function: 4.579, Average Loss: 4.297, avg. samples / sec: 28422.94
:::MLL 1558580112.266 epoch_stop: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 819}}
:::MLL 1558580112.266 epoch_start: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 673}}
Iteration:   2220, Loss function: 4.456, Average Loss: 4.302, avg. samples / sec: 28317.77
Iteration:   2220, Loss function: 4.031, Average Loss: 4.305, avg. samples / sec: 28304.19
Iteration:   2220, Loss function: 4.075, Average Loss: 4.314, avg. samples / sec: 28283.23
Iteration:   2220, Loss function: 4.136, Average Loss: 4.293, avg. samples / sec: 28287.93
Iteration:   2240, Loss function: 4.376, Average Loss: 4.288, avg. samples / sec: 28397.09
Iteration:   2240, Loss function: 3.520, Average Loss: 4.309, avg. samples / sec: 28380.16
Iteration:   2240, Loss function: 4.402, Average Loss: 4.301, avg. samples / sec: 28340.33
Iteration:   2240, Loss function: 3.257, Average Loss: 4.296, avg. samples / sec: 28305.57
Iteration:   2260, Loss function: 4.740, Average Loss: 4.293, avg. samples / sec: 28393.45
Iteration:   2260, Loss function: 4.129, Average Loss: 4.307, avg. samples / sec: 28331.21
Iteration:   2260, Loss function: 5.125, Average Loss: 4.299, avg. samples / sec: 28314.06
Iteration:   2260, Loss function: 4.460, Average Loss: 4.286, avg. samples / sec: 28285.65
Iteration:   2280, Loss function: 4.527, Average Loss: 4.296, avg. samples / sec: 28481.79
Iteration:   2280, Loss function: 3.521, Average Loss: 4.290, avg. samples / sec: 28434.19
Iteration:   2280, Loss function: 4.067, Average Loss: 4.303, avg. samples / sec: 28439.11
Iteration:   2280, Loss function: 3.791, Average Loss: 4.282, avg. samples / sec: 28456.29
:::MLL 1558580116.381 epoch_stop: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 819}}
:::MLL 1558580116.381 epoch_start: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 673}}
Iteration:   2300, Loss function: 4.203, Average Loss: 4.286, avg. samples / sec: 28264.86
Iteration:   2300, Loss function: 4.117, Average Loss: 4.292, avg. samples / sec: 28253.70
Iteration:   2300, Loss function: 3.474, Average Loss: 4.296, avg. samples / sec: 28227.07
Iteration:   2300, Loss function: 3.908, Average Loss: 4.276, avg. samples / sec: 28203.01
Iteration:   2320, Loss function: 3.838, Average Loss: 4.290, avg. samples / sec: 28385.24
Iteration:   2320, Loss function: 3.889, Average Loss: 4.273, avg. samples / sec: 28471.73
Iteration:   2320, Loss function: 4.341, Average Loss: 4.291, avg. samples / sec: 28403.14
Iteration:   2320, Loss function: 3.983, Average Loss: 4.282, avg. samples / sec: 28284.74
Iteration:   2340, Loss function: 4.257, Average Loss: 4.288, avg. samples / sec: 28402.59
Iteration:   2340, Loss function: 4.203, Average Loss: 4.272, avg. samples / sec: 28386.73
Iteration:   2340, Loss function: 3.985, Average Loss: 4.278, avg. samples / sec: 28465.39
Iteration:   2340, Loss function: 3.810, Average Loss: 4.285, avg. samples / sec: 28318.08
Iteration:   2360, Loss function: 3.915, Average Loss: 4.282, avg. samples / sec: 28407.67
Iteration:   2360, Loss function: 3.336, Average Loss: 4.286, avg. samples / sec: 28330.03
Iteration:   2360, Loss function: 3.754, Average Loss: 4.277, avg. samples / sec: 28336.59
Iteration:   2360, Loss function: 3.684, Average Loss: 4.270, avg. samples / sec: 28321.80
:::MLL 1558580120.503 epoch_stop: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 819}}
:::MLL 1558580120.504 epoch_start: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 673}}
Iteration:   2380, Loss function: 4.671, Average Loss: 4.275, avg. samples / sec: 28282.90
Iteration:   2380, Loss function: 3.974, Average Loss: 4.282, avg. samples / sec: 28283.52
Iteration:   2380, Loss function: 4.662, Average Loss: 4.273, avg. samples / sec: 28286.87
Iteration:   2380, Loss function: 4.458, Average Loss: 4.266, avg. samples / sec: 28265.88
Iteration:   2400, Loss function: 4.317, Average Loss: 4.261, avg. samples / sec: 28420.98
Iteration:   2400, Loss function: 3.889, Average Loss: 4.268, avg. samples / sec: 28393.46
Iteration:   2400, Loss function: 3.958, Average Loss: 4.269, avg. samples / sec: 28368.89
Iteration:   2400, Loss function: 4.682, Average Loss: 4.277, avg. samples / sec: 28332.61
Iteration:   2420, Loss function: 4.691, Average Loss: 4.275, avg. samples / sec: 28368.46
Iteration:   2420, Loss function: 4.320, Average Loss: 4.258, avg. samples / sec: 28305.89
Iteration:   2420, Loss function: 3.816, Average Loss: 4.265, avg. samples / sec: 28311.40
Iteration:   2420, Loss function: 4.923, Average Loss: 4.268, avg. samples / sec: 28283.51
Iteration:   2440, Loss function: 4.077, Average Loss: 4.271, avg. samples / sec: 28267.65
Iteration:   2440, Loss function: 4.290, Average Loss: 4.267, avg. samples / sec: 28289.26
Iteration:   2440, Loss function: 4.160, Average Loss: 4.254, avg. samples / sec: 28232.72
Iteration:   2440, Loss function: 4.705, Average Loss: 4.264, avg. samples / sec: 28234.33
:::MLL 1558580124.679 epoch_stop: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 819}}
:::MLL 1558580124.679 epoch_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 673}}
Iteration:   2460, Loss function: 3.999, Average Loss: 4.250, avg. samples / sec: 28006.81
Iteration:   2460, Loss function: 4.034, Average Loss: 4.266, avg. samples / sec: 27968.82
Iteration:   2460, Loss function: 3.930, Average Loss: 4.262, avg. samples / sec: 27970.10
Iteration:   2460, Loss function: 4.459, Average Loss: 4.259, avg. samples / sec: 27966.97
Iteration:   2480, Loss function: 4.266, Average Loss: 4.247, avg. samples / sec: 28296.65
Iteration:   2480, Loss function: 3.882, Average Loss: 4.257, avg. samples / sec: 28282.18
Iteration:   2480, Loss function: 4.533, Average Loss: 4.259, avg. samples / sec: 28310.15
Iteration:   2480, Loss function: 4.031, Average Loss: 4.264, avg. samples / sec: 28251.38
Iteration:   2500, Loss function: 4.191, Average Loss: 4.244, avg. samples / sec: 28240.32
Iteration:   2500, Loss function: 4.383, Average Loss: 4.257, avg. samples / sec: 28269.93
Iteration:   2500, Loss function: 4.164, Average Loss: 4.255, avg. samples / sec: 28251.31
Iteration:   2500, Loss function: 3.799, Average Loss: 4.259, avg. samples / sec: 28237.47
:::MLL 1558580127.789 eval_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.24 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.25 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.25 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.25 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.39s)
DONE (t=0.40s)
DONE (t=0.41s)
DONE (t=2.89s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.15394
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.29319
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.14890
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.03851
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.16358
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.24362
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.17087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.25144
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.26549
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.06478
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.28689
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.40522
Current AP: 0.15394 AP goal: 0.23000
:::MLL 1558580133.375 eval_accuracy: {"value": 0.15393832463997786, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 389}}
:::MLL 1558580133.420 eval_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 392}}
:::MLL 1558580133.434 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 804}}
:::MLL 1558580133.435 block_start: {"value": null, "metadata": {"first_epoch_num": 33, "epoch_count": 10.915354834308324, "file": "train.py", "lineno": 813}}
:::MLL 1558580135.066 epoch_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 819}}
:::MLL 1558580135.066 epoch_start: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 673}}
Iteration:   2520, Loss function: 3.849, Average Loss: 4.254, avg. samples / sec: 4189.28
Iteration:   2520, Loss function: 4.688, Average Loss: 4.240, avg. samples / sec: 4188.68
Iteration:   2520, Loss function: 3.375, Average Loss: 4.254, avg. samples / sec: 4189.57
Iteration:   2520, Loss function: 4.432, Average Loss: 4.254, avg. samples / sec: 4188.52
Iteration:   2540, Loss function: 3.791, Average Loss: 4.250, avg. samples / sec: 27952.08
Iteration:   2540, Loss function: 3.665, Average Loss: 4.237, avg. samples / sec: 27933.26
Iteration:   2540, Loss function: 4.454, Average Loss: 4.249, avg. samples / sec: 27943.87
Iteration:   2540, Loss function: 4.243, Average Loss: 4.253, avg. samples / sec: 27842.96
Iteration:   2560, Loss function: 3.905, Average Loss: 4.248, avg. samples / sec: 28045.60
Iteration:   2560, Loss function: 3.901, Average Loss: 4.247, avg. samples / sec: 28031.31
Iteration:   2560, Loss function: 4.039, Average Loss: 4.233, avg. samples / sec: 27989.86
Iteration:   2560, Loss function: 4.967, Average Loss: 4.252, avg. samples / sec: 28048.19
Iteration:   2580, Loss function: 4.615, Average Loss: 4.245, avg. samples / sec: 28105.43
Iteration:   2580, Loss function: 4.538, Average Loss: 4.230, avg. samples / sec: 28145.52
Iteration:   2580, Loss function: 3.959, Average Loss: 4.246, avg. samples / sec: 28057.56
Iteration:   2580, Loss function: 4.079, Average Loss: 4.248, avg. samples / sec: 28099.88
:::MLL 1558580139.229 epoch_stop: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 819}}
:::MLL 1558580139.230 epoch_start: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 673}}
Iteration:   2600, Loss function: 3.788, Average Loss: 4.240, avg. samples / sec: 28129.15
Iteration:   2600, Loss function: 3.523, Average Loss: 4.224, avg. samples / sec: 28133.35
Iteration:   2600, Loss function: 3.913, Average Loss: 4.245, avg. samples / sec: 28181.16
Iteration:   2600, Loss function: 4.133, Average Loss: 4.242, avg. samples / sec: 28161.81
Iteration:   2620, Loss function: 3.833, Average Loss: 4.240, avg. samples / sec: 28155.35
Iteration:   2620, Loss function: 4.514, Average Loss: 4.218, avg. samples / sec: 28140.84
Iteration:   2620, Loss function: 4.038, Average Loss: 4.236, avg. samples / sec: 28137.97
Iteration:   2620, Loss function: 3.710, Average Loss: 4.238, avg. samples / sec: 28136.89
Iteration:   2640, Loss function: 4.950, Average Loss: 4.230, avg. samples / sec: 28052.44
Iteration:   2640, Loss function: 3.898, Average Loss: 4.237, avg. samples / sec: 28045.74
Iteration:   2640, Loss function: 3.335, Average Loss: 4.231, avg. samples / sec: 28050.79
Iteration:   2640, Loss function: 3.941, Average Loss: 4.214, avg. samples / sec: 28012.80
Iteration:   2660, Loss function: 3.731, Average Loss: 4.235, avg. samples / sec: 27786.26
Iteration:   2660, Loss function: 3.853, Average Loss: 4.227, avg. samples / sec: 27720.79
Iteration:   2660, Loss function: 3.918, Average Loss: 4.210, avg. samples / sec: 27760.21
Iteration:   2660, Loss function: 4.431, Average Loss: 4.230, avg. samples / sec: 27710.63
:::MLL 1558580143.454 epoch_stop: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 819}}
:::MLL 1558580143.454 epoch_start: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 673}}
Iteration:   2680, Loss function: 4.123, Average Loss: 4.226, avg. samples / sec: 28059.83
Iteration:   2680, Loss function: 3.859, Average Loss: 4.231, avg. samples / sec: 27968.21
Iteration:   2680, Loss function: 3.925, Average Loss: 4.221, avg. samples / sec: 28023.62
Iteration:   2680, Loss function: 4.097, Average Loss: 4.202, avg. samples / sec: 28017.05
Iteration:   2700, Loss function: 3.715, Average Loss: 4.221, avg. samples / sec: 28146.97
Iteration:   2700, Loss function: 3.741, Average Loss: 4.225, avg. samples / sec: 28148.76
Iteration:   2700, Loss function: 3.676, Average Loss: 4.198, avg. samples / sec: 28134.39
Iteration:   2700, Loss function: 3.794, Average Loss: 4.215, avg. samples / sec: 28117.59
Iteration:   2720, Loss function: 3.337, Average Loss: 4.218, avg. samples / sec: 28071.17
Iteration:   2720, Loss function: 3.608, Average Loss: 4.210, avg. samples / sec: 28099.66
Iteration:   2720, Loss function: 4.607, Average Loss: 4.195, avg. samples / sec: 28049.52
Iteration:   2720, Loss function: 3.606, Average Loss: 4.219, avg. samples / sec: 28019.26
Iteration:   2740, Loss function: 3.934, Average Loss: 4.194, avg. samples / sec: 27972.53
Iteration:   2740, Loss function: 3.889, Average Loss: 4.213, avg. samples / sec: 27920.86
Iteration:   2740, Loss function: 4.002, Average Loss: 4.214, avg. samples / sec: 27974.17
Iteration:   2740, Loss function: 3.842, Average Loss: 4.205, avg. samples / sec: 27913.88
:::MLL 1558580147.617 epoch_stop: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 819}}
:::MLL 1558580147.617 epoch_start: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 673}}
Iteration:   2760, Loss function: 4.379, Average Loss: 4.209, avg. samples / sec: 27960.75
Iteration:   2760, Loss function: 4.045, Average Loss: 4.191, avg. samples / sec: 27945.35
Iteration:   2760, Loss function: 3.705, Average Loss: 4.202, avg. samples / sec: 27955.38
Iteration:   2760, Loss function: 3.720, Average Loss: 4.212, avg. samples / sec: 27944.71
Iteration:   2780, Loss function: 3.425, Average Loss: 4.188, avg. samples / sec: 28007.44
Iteration:   2780, Loss function: 4.497, Average Loss: 4.193, avg. samples / sec: 28005.40
Iteration:   2780, Loss function: 4.061, Average Loss: 4.207, avg. samples / sec: 27998.40
Iteration:   2780, Loss function: 4.262, Average Loss: 4.205, avg. samples / sec: 27978.31
Iteration:   2800, Loss function: 4.509, Average Loss: 4.185, avg. samples / sec: 28049.71
Iteration:   2800, Loss function: 4.584, Average Loss: 4.203, avg. samples / sec: 28063.81
Iteration:   2800, Loss function: 3.990, Average Loss: 4.202, avg. samples / sec: 28061.87
Iteration:   2800, Loss function: 4.511, Average Loss: 4.192, avg. samples / sec: 28043.35
Iteration:   2820, Loss function: 3.669, Average Loss: 4.188, avg. samples / sec: 28055.14
Iteration:   2820, Loss function: 3.415, Average Loss: 4.198, avg. samples / sec: 28052.99
Iteration:   2820, Loss function: 4.352, Average Loss: 4.200, avg. samples / sec: 28039.51
Iteration:   2820, Loss function: 3.525, Average Loss: 4.184, avg. samples / sec: 28033.50
:::MLL 1558580151.786 epoch_stop: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 819}}
:::MLL 1558580151.787 epoch_start: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 673}}
Iteration:   2840, Loss function: 4.024, Average Loss: 4.182, avg. samples / sec: 28008.32
Iteration:   2840, Loss function: 4.139, Average Loss: 4.196, avg. samples / sec: 27994.72
Iteration:   2840, Loss function: 4.321, Average Loss: 4.178, avg. samples / sec: 27977.00
Iteration:   2840, Loss function: 4.007, Average Loss: 4.194, avg. samples / sec: 27947.86
Iteration:   2860, Loss function: 4.209, Average Loss: 4.189, avg. samples / sec: 28011.24
Iteration:   2860, Loss function: 3.970, Average Loss: 4.173, avg. samples / sec: 28030.81
Iteration:   2860, Loss function: 3.886, Average Loss: 4.191, avg. samples / sec: 28045.82
Iteration:   2860, Loss function: 4.496, Average Loss: 4.178, avg. samples / sec: 27928.09
Iteration:   2880, Loss function: 4.095, Average Loss: 4.187, avg. samples / sec: 28170.76
Iteration:   2880, Loss function: 4.062, Average Loss: 4.174, avg. samples / sec: 28240.58
Iteration:   2880, Loss function: 3.767, Average Loss: 4.186, avg. samples / sec: 28175.71
Iteration:   2880, Loss function: 3.875, Average Loss: 4.167, avg. samples / sec: 28165.73
Iteration:   2900, Loss function: 3.205, Average Loss: 4.165, avg. samples / sec: 27972.86
Iteration:   2900, Loss function: 3.719, Average Loss: 4.183, avg. samples / sec: 27955.16
Iteration:   2900, Loss function: 3.695, Average Loss: 4.181, avg. samples / sec: 27955.09
Iteration:   2900, Loss function: 4.304, Average Loss: 4.170, avg. samples / sec: 27891.26
:::MLL 1558580156.007 epoch_stop: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 819}}
:::MLL 1558580156.007 epoch_start: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 673}}
Iteration:   2920, Loss function: 4.046, Average Loss: 4.167, avg. samples / sec: 28130.91
Iteration:   2920, Loss function: 3.268, Average Loss: 4.175, avg. samples / sec: 28062.79
Iteration:   2920, Loss function: 3.160, Average Loss: 4.161, avg. samples / sec: 28022.22
Iteration:   2920, Loss function: 3.412, Average Loss: 4.178, avg. samples / sec: 27995.10
Iteration:   2940, Loss function: 3.747, Average Loss: 4.174, avg. samples / sec: 28133.78
Iteration:   2940, Loss function: 4.008, Average Loss: 4.170, avg. samples / sec: 28062.90
Iteration:   2940, Loss function: 3.921, Average Loss: 4.162, avg. samples / sec: 28030.04
Iteration:   2940, Loss function: 4.296, Average Loss: 4.158, avg. samples / sec: 28031.89
Iteration:   2960, Loss function: 4.199, Average Loss: 4.161, avg. samples / sec: 28213.77
Iteration:   2960, Loss function: 4.067, Average Loss: 4.166, avg. samples / sec: 28174.84
Iteration:   2960, Loss function: 4.318, Average Loss: 4.171, avg. samples / sec: 28113.90
Iteration:   2960, Loss function: 4.148, Average Loss: 4.154, avg. samples / sec: 28171.78
:::MLL 1558580160.166 epoch_stop: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 819}}
:::MLL 1558580160.167 epoch_start: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 673}}
Iteration:   2980, Loss function: 2.995, Average Loss: 4.150, avg. samples / sec: 28000.20
Iteration:   2980, Loss function: 3.928, Average Loss: 4.155, avg. samples / sec: 27920.05
Iteration:   2980, Loss function: 3.692, Average Loss: 4.164, avg. samples / sec: 27970.76
Iteration:   2980, Loss function: 4.419, Average Loss: 4.160, avg. samples / sec: 27853.80
Iteration:   3000, Loss function: 4.278, Average Loss: 4.149, avg. samples / sec: 28075.00
Iteration:   3000, Loss function: 4.391, Average Loss: 4.160, avg. samples / sec: 28149.04
Iteration:   3000, Loss function: 4.393, Average Loss: 4.146, avg. samples / sec: 28046.60
Iteration:   3000, Loss function: 4.335, Average Loss: 4.160, avg. samples / sec: 28063.96
Iteration:   3020, Loss function: 4.124, Average Loss: 4.147, avg. samples / sec: 27950.73
Iteration:   3020, Loss function: 4.458, Average Loss: 4.156, avg. samples / sec: 27954.04
Iteration:   3020, Loss function: 4.201, Average Loss: 4.155, avg. samples / sec: 27969.67
Iteration:   3020, Loss function: 3.039, Average Loss: 4.140, avg. samples / sec: 27944.86
Iteration:   3040, Loss function: 3.719, Average Loss: 4.143, avg. samples / sec: 28008.71
Iteration:   3040, Loss function: 3.802, Average Loss: 4.150, avg. samples / sec: 28013.66
Iteration:   3040, Loss function: 3.513, Average Loss: 4.151, avg. samples / sec: 28009.10
Iteration:   3040, Loss function: 4.181, Average Loss: 4.136, avg. samples / sec: 28019.80
:::MLL 1558580164.340 epoch_stop: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 819}}
:::MLL 1558580164.340 epoch_start: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 673}}
Iteration:   3060, Loss function: 4.075, Average Loss: 4.147, avg. samples / sec: 27850.73
Iteration:   3060, Loss function: 4.034, Average Loss: 4.146, avg. samples / sec: 27846.39
Iteration:   3060, Loss function: 4.104, Average Loss: 4.143, avg. samples / sec: 27836.44
Iteration:   3060, Loss function: 4.031, Average Loss: 4.130, avg. samples / sec: 27819.44
Iteration:   3080, Loss function: 3.762, Average Loss: 4.138, avg. samples / sec: 27887.04
Iteration:   3080, Loss function: 3.819, Average Loss: 4.125, avg. samples / sec: 27913.91
Iteration:   3080, Loss function: 4.130, Average Loss: 4.142, avg. samples / sec: 27869.80
Iteration:   3080, Loss function: 3.639, Average Loss: 4.142, avg. samples / sec: 27862.10
Iteration:   3100, Loss function: 3.517, Average Loss: 4.137, avg. samples / sec: 28064.77
Iteration:   3100, Loss function: 3.850, Average Loss: 4.121, avg. samples / sec: 28033.00
Iteration:   3100, Loss function: 3.617, Average Loss: 4.139, avg. samples / sec: 28000.39
Iteration:   3100, Loss function: 3.863, Average Loss: 4.133, avg. samples / sec: 27966.42
Iteration:   3120, Loss function: 4.016, Average Loss: 4.120, avg. samples / sec: 27910.87
Iteration:   3120, Loss function: 4.036, Average Loss: 4.130, avg. samples / sec: 27969.06
Iteration:   3120, Loss function: 3.920, Average Loss: 4.135, avg. samples / sec: 27924.39
Iteration:   3120, Loss function: 4.029, Average Loss: 4.134, avg. samples / sec: 27855.91
:::MLL 1558580168.571 epoch_stop: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 819}}
:::MLL 1558580168.572 epoch_start: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 673}}
Iteration:   3140, Loss function: 3.743, Average Loss: 4.116, avg. samples / sec: 28011.19
Iteration:   3140, Loss function: 4.434, Average Loss: 4.129, avg. samples / sec: 28044.04
Iteration:   3140, Loss function: 4.048, Average Loss: 4.131, avg. samples / sec: 28021.18
Iteration:   3140, Loss function: 4.356, Average Loss: 4.127, avg. samples / sec: 27994.50
Iteration:   3160, Loss function: 3.898, Average Loss: 4.127, avg. samples / sec: 27990.08
Iteration:   3160, Loss function: 3.276, Average Loss: 4.123, avg. samples / sec: 28008.37
Iteration:   3160, Loss function: 4.208, Average Loss: 4.111, avg. samples / sec: 27971.65
Iteration:   3160, Loss function: 3.592, Average Loss: 4.125, avg. samples / sec: 27951.14
Iteration:   3180, Loss function: 4.091, Average Loss: 4.110, avg. samples / sec: 28008.30
Iteration:   3180, Loss function: 3.736, Average Loss: 4.124, avg. samples / sec: 27977.52
Iteration:   3180, Loss function: 4.256, Average Loss: 4.124, avg. samples / sec: 28001.69
Iteration:   3180, Loss function: 3.948, Average Loss: 4.120, avg. samples / sec: 27912.38
Iteration:   3200, Loss function: 3.194, Average Loss: 4.118, avg. samples / sec: 28095.55
Iteration:   3200, Loss function: 3.961, Average Loss: 4.106, avg. samples / sec: 28052.90
Iteration:   3200, Loss function: 3.853, Average Loss: 4.117, avg. samples / sec: 28118.55
Iteration:   3200, Loss function: 3.776, Average Loss: 4.124, avg. samples / sec: 28024.57
:::MLL 1558580172.738 epoch_stop: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 819}}
:::MLL 1558580172.738 epoch_start: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 673}}
Iteration:   3220, Loss function: 3.806, Average Loss: 4.119, avg. samples / sec: 27990.60
Iteration:   3220, Loss function: 4.152, Average Loss: 4.112, avg. samples / sec: 27960.53
Iteration:   3220, Loss function: 4.249, Average Loss: 4.115, avg. samples / sec: 27938.54
Iteration:   3220, Loss function: 3.679, Average Loss: 4.104, avg. samples / sec: 27937.84
Iteration:   3240, Loss function: 4.123, Average Loss: 4.099, avg. samples / sec: 28041.96
Iteration:   3240, Loss function: 4.349, Average Loss: 4.113, avg. samples / sec: 28029.30
Iteration:   3240, Loss function: 4.324, Average Loss: 4.106, avg. samples / sec: 28033.35
Iteration:   3240, Loss function: 4.266, Average Loss: 4.110, avg. samples / sec: 28004.81
Iteration:   3260, Loss function: 3.532, Average Loss: 4.109, avg. samples / sec: 28006.53
Iteration:   3260, Loss function: 3.968, Average Loss: 4.106, avg. samples / sec: 28008.92
Iteration:   3260, Loss function: 3.797, Average Loss: 4.103, avg. samples / sec: 27965.55
Iteration:   3260, Loss function: 3.965, Average Loss: 4.094, avg. samples / sec: 27930.00
Iteration:   3280, Loss function: 4.776, Average Loss: 4.103, avg. samples / sec: 28049.88
Iteration:   3280, Loss function: 3.709, Average Loss: 4.106, avg. samples / sec: 28004.92
Iteration:   3280, Loss function: 4.278, Average Loss: 4.101, avg. samples / sec: 28052.50
Iteration:   3280, Loss function: 3.973, Average Loss: 4.096, avg. samples / sec: 28030.31
:::MLL 1558580176.909 epoch_stop: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 819}}
:::MLL 1558580176.909 epoch_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 673}}
Iteration:   3300, Loss function: 3.944, Average Loss: 4.100, avg. samples / sec: 27988.36
Iteration:   3300, Loss function: 3.383, Average Loss: 4.105, avg. samples / sec: 27988.45
Iteration:   3300, Loss function: 3.541, Average Loss: 4.094, avg. samples / sec: 28003.68
Iteration:   3300, Loss function: 4.062, Average Loss: 4.099, avg. samples / sec: 27887.44
Iteration:   3320, Loss function: 4.121, Average Loss: 4.101, avg. samples / sec: 28004.12
Iteration:   3320, Loss function: 4.140, Average Loss: 4.090, avg. samples / sec: 28032.00
Iteration:   3320, Loss function: 3.822, Average Loss: 4.094, avg. samples / sec: 27984.86
Iteration:   3320, Loss function: 4.187, Average Loss: 4.095, avg. samples / sec: 28009.66
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
:::MLL 1558580179.707 eval_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.12 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.12 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.12 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.12 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.54s)
DONE (t=3.10s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.17703
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.32270
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.17662
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.04383
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.19000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.28976
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.18609
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.27228
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.28671
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.07681
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.30158
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.45966
Current AP: 0.17703 AP goal: 0.23000
:::MLL 1558580184.523 eval_accuracy: {"value": 0.1770255799151485, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 389}}
:::MLL 1558580184.563 eval_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 392}}
:::MLL 1558580184.577 block_stop: {"value": null, "metadata": {"first_epoch_num": 33, "file": "train.py", "lineno": 804}}
:::MLL 1558580184.577 block_start: {"value": null, "metadata": {"first_epoch_num": 44, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3340, Loss function: 3.253, Average Loss: 4.097, avg. samples / sec: 5142.69
Iteration:   3340, Loss function: 3.575, Average Loss: 4.090, avg. samples / sec: 5142.68
Iteration:   3340, Loss function: 4.277, Average Loss: 4.091, avg. samples / sec: 5144.66
Iteration:   3340, Loss function: 3.767, Average Loss: 4.085, avg. samples / sec: 5140.42
:::MLL 1558580186.013 epoch_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 819}}
:::MLL 1558580186.013 epoch_start: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 673}}
Iteration:   3360, Loss function: 3.803, Average Loss: 4.082, avg. samples / sec: 27947.17
Iteration:   3360, Loss function: 3.521, Average Loss: 4.080, avg. samples / sec: 27876.93
Iteration:   3360, Loss function: 3.501, Average Loss: 4.088, avg. samples / sec: 27843.04
Iteration:   3360, Loss function: 3.674, Average Loss: 4.083, avg. samples / sec: 27863.68
Iteration:   3380, Loss function: 3.679, Average Loss: 4.075, avg. samples / sec: 27868.56
Iteration:   3380, Loss function: 3.579, Average Loss: 4.077, avg. samples / sec: 27865.71
Iteration:   3380, Loss function: 3.620, Average Loss: 4.078, avg. samples / sec: 27854.87
Iteration:   3380, Loss function: 3.721, Average Loss: 4.069, avg. samples / sec: 27810.21
Iteration:   3400, Loss function: 3.270, Average Loss: 4.065, avg. samples / sec: 27993.38
Iteration:   3400, Loss function: 3.855, Average Loss: 4.066, avg. samples / sec: 28000.23
Iteration:   3400, Loss function: 3.575, Average Loss: 4.060, avg. samples / sec: 28016.93
Iteration:   3400, Loss function: 3.185, Average Loss: 4.067, avg. samples / sec: 27942.03
Iteration:   3420, Loss function: 3.791, Average Loss: 4.058, avg. samples / sec: 28021.37
Iteration:   3420, Loss function: 4.048, Average Loss: 4.050, avg. samples / sec: 27995.77
Iteration:   3420, Loss function: 4.111, Average Loss: 4.055, avg. samples / sec: 27988.38
Iteration:   3420, Loss function: 3.115, Average Loss: 4.057, avg. samples / sec: 27949.70
:::MLL 1558580190.195 epoch_stop: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 819}}
:::MLL 1558580190.195 epoch_start: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 673}}
Iteration:   3440, Loss function: 3.961, Average Loss: 4.040, avg. samples / sec: 27885.08
Iteration:   3440, Loss function: 3.725, Average Loss: 4.047, avg. samples / sec: 27916.85
Iteration:   3440, Loss function: 4.159, Average Loss: 4.048, avg. samples / sec: 27870.14
Iteration:   3440, Loss function: 3.634, Average Loss: 4.048, avg. samples / sec: 27793.58
Iteration:   3460, Loss function: 3.108, Average Loss: 4.036, avg. samples / sec: 28008.35
Iteration:   3460, Loss function: 3.403, Average Loss: 4.041, avg. samples / sec: 28048.77
Iteration:   3460, Loss function: 3.898, Average Loss: 4.028, avg. samples / sec: 27963.99
Iteration:   3460, Loss function: 4.035, Average Loss: 4.035, avg. samples / sec: 27970.60
Iteration:   3480, Loss function: 3.334, Average Loss: 4.028, avg. samples / sec: 28102.35
Iteration:   3480, Loss function: 3.286, Average Loss: 4.023, avg. samples / sec: 28096.36
Iteration:   3480, Loss function: 2.932, Average Loss: 4.015, avg. samples / sec: 28120.42
Iteration:   3480, Loss function: 3.069, Average Loss: 4.025, avg. samples / sec: 28132.68
Iteration:   3500, Loss function: 3.451, Average Loss: 4.012, avg. samples / sec: 28075.08
Iteration:   3500, Loss function: 3.121, Average Loss: 4.003, avg. samples / sec: 28075.21
Iteration:   3500, Loss function: 3.250, Average Loss: 4.015, avg. samples / sec: 28076.73
Iteration:   3500, Loss function: 3.163, Average Loss: 4.019, avg. samples / sec: 28034.17
:::MLL 1558580194.360 epoch_stop: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 819}}
:::MLL 1558580194.360 epoch_start: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 673}}
Iteration:   3520, Loss function: 3.543, Average Loss: 4.010, avg. samples / sec: 27912.36
Iteration:   3520, Loss function: 3.604, Average Loss: 4.003, avg. samples / sec: 27841.31
Iteration:   3520, Loss function: 3.461, Average Loss: 3.992, avg. samples / sec: 27842.97
Iteration:   3520, Loss function: 4.054, Average Loss: 4.005, avg. samples / sec: 27821.10
Iteration:   3540, Loss function: 3.303, Average Loss: 4.000, avg. samples / sec: 27994.81
Iteration:   3540, Loss function: 3.357, Average Loss: 3.990, avg. samples / sec: 28032.52
Iteration:   3540, Loss function: 3.327, Average Loss: 3.982, avg. samples / sec: 28020.94
Iteration:   3540, Loss function: 3.792, Average Loss: 3.995, avg. samples / sec: 28021.70
Iteration:   3560, Loss function: 3.884, Average Loss: 3.985, avg. samples / sec: 28046.77
Iteration:   3560, Loss function: 3.532, Average Loss: 3.979, avg. samples / sec: 28004.64
Iteration:   3560, Loss function: 2.935, Average Loss: 3.972, avg. samples / sec: 28009.91
Iteration:   3560, Loss function: 3.447, Average Loss: 3.991, avg. samples / sec: 27975.95
Iteration:   3580, Loss function: 3.510, Average Loss: 3.971, avg. samples / sec: 27983.68
Iteration:   3580, Loss function: 3.878, Average Loss: 3.976, avg. samples / sec: 27960.49
Iteration:   3580, Loss function: 3.517, Average Loss: 3.961, avg. samples / sec: 27967.56
Iteration:   3580, Loss function: 3.013, Average Loss: 3.978, avg. samples / sec: 27981.87
:::MLL 1558580198.589 epoch_stop: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 819}}
:::MLL 1558580198.590 epoch_start: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 673}}
Iteration:   3600, Loss function: 3.638, Average Loss: 3.963, avg. samples / sec: 27968.90
Iteration:   3600, Loss function: 3.885, Average Loss: 3.968, avg. samples / sec: 27989.72
Iteration:   3600, Loss function: 3.662, Average Loss: 3.952, avg. samples / sec: 27956.98
Iteration:   3600, Loss function: 3.815, Average Loss: 3.966, avg. samples / sec: 27912.81
Iteration:   3620, Loss function: 3.672, Average Loss: 3.958, avg. samples / sec: 27966.15
Iteration:   3620, Loss function: 3.767, Average Loss: 3.951, avg. samples / sec: 27952.07
Iteration:   3620, Loss function: 2.819, Average Loss: 3.941, avg. samples / sec: 27985.08
Iteration:   3620, Loss function: 3.148, Average Loss: 3.954, avg. samples / sec: 27991.07
Iteration:   3640, Loss function: 4.189, Average Loss: 3.930, avg. samples / sec: 28056.67
Iteration:   3640, Loss function: 2.657, Average Loss: 3.943, avg. samples / sec: 28087.52
Iteration:   3640, Loss function: 3.465, Average Loss: 3.940, avg. samples / sec: 28033.04
Iteration:   3640, Loss function: 3.081, Average Loss: 3.947, avg. samples / sec: 28011.30
Iteration:   3660, Loss function: 2.993, Average Loss: 3.935, avg. samples / sec: 28037.79
Iteration:   3660, Loss function: 2.925, Average Loss: 3.919, avg. samples / sec: 27997.75
Iteration:   3660, Loss function: 4.227, Average Loss: 3.935, avg. samples / sec: 27964.08
Iteration:   3660, Loss function: 3.649, Average Loss: 3.931, avg. samples / sec: 27964.51
:::MLL 1558580202.761 epoch_stop: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 819}}
:::MLL 1558580202.761 epoch_start: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 673}}
Iteration:   3680, Loss function: 2.703, Average Loss: 3.910, avg. samples / sec: 27864.16
Iteration:   3680, Loss function: 3.287, Average Loss: 3.927, avg. samples / sec: 27880.01
Iteration:   3680, Loss function: 3.831, Average Loss: 3.925, avg. samples / sec: 27842.05
Iteration:   3680, Loss function: 3.360, Average Loss: 3.919, avg. samples / sec: 27892.08
Iteration:   3700, Loss function: 3.425, Average Loss: 3.907, avg. samples / sec: 28018.62
Iteration:   3700, Loss function: 3.326, Average Loss: 3.915, avg. samples / sec: 28006.74
Iteration:   3700, Loss function: 3.249, Average Loss: 3.901, avg. samples / sec: 27983.99
Iteration:   3700, Loss function: 3.772, Average Loss: 3.915, avg. samples / sec: 27993.12
Iteration:   3720, Loss function: 3.443, Average Loss: 3.892, avg. samples / sec: 27988.50
Iteration:   3720, Loss function: 3.069, Average Loss: 3.895, avg. samples / sec: 27977.64
Iteration:   3720, Loss function: 2.958, Average Loss: 3.907, avg. samples / sec: 27975.13
Iteration:   3720, Loss function: 3.454, Average Loss: 3.907, avg. samples / sec: 27975.82
Iteration:   3740, Loss function: 3.020, Average Loss: 3.886, avg. samples / sec: 27999.33
Iteration:   3740, Loss function: 3.490, Average Loss: 3.897, avg. samples / sec: 28000.58
Iteration:   3740, Loss function: 3.795, Average Loss: 3.897, avg. samples / sec: 28002.01
Iteration:   3740, Loss function: 3.760, Average Loss: 3.883, avg. samples / sec: 27943.71
:::MLL 1558580206.935 epoch_stop: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 819}}
:::MLL 1558580206.935 epoch_start: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 673}}
:::MLL 1558580207.486 eval_start: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.14 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.14 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.14 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.14 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.55s)
DONE (t=2.84s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22510
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.38597
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23280
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05663
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24012
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.36419
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.21881
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.31955
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33546
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09530
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36641
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.52882
Current AP: 0.22510 AP goal: 0.23000
:::MLL 1558580212.046 eval_accuracy: {"value": 0.22509929057427927, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 389}}
:::MLL 1558580212.093 eval_stop: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 392}}
:::MLL 1558580212.108 block_stop: {"value": null, "metadata": {"first_epoch_num": 44, "file": "train.py", "lineno": 804}}
:::MLL 1558580212.108 block_start: {"value": null, "metadata": {"first_epoch_num": 50, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3760, Loss function: 3.223, Average Loss: 3.875, avg. samples / sec: 5369.97
Iteration:   3760, Loss function: 3.709, Average Loss: 3.875, avg. samples / sec: 5371.33
Iteration:   3760, Loss function: 3.249, Average Loss: 3.889, avg. samples / sec: 5369.43
Iteration:   3760, Loss function: 3.744, Average Loss: 3.890, avg. samples / sec: 5368.77
Iteration:   3780, Loss function: 3.089, Average Loss: 3.877, avg. samples / sec: 27912.96
Iteration:   3780, Loss function: 3.340, Average Loss: 3.882, avg. samples / sec: 27891.06
Iteration:   3780, Loss function: 4.630, Average Loss: 3.867, avg. samples / sec: 27831.72
Iteration:   3780, Loss function: 3.611, Average Loss: 3.871, avg. samples / sec: 27831.30
Iteration:   3800, Loss function: 3.519, Average Loss: 3.871, avg. samples / sec: 27970.27
Iteration:   3800, Loss function: 3.058, Average Loss: 3.857, avg. samples / sec: 27992.98
Iteration:   3800, Loss function: 3.370, Average Loss: 3.874, avg. samples / sec: 27901.36
Iteration:   3800, Loss function: 3.705, Average Loss: 3.862, avg. samples / sec: 27931.19
:::MLL 1558580215.789 epoch_stop: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 819}}
:::MLL 1558580215.790 epoch_start: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 673}}
Iteration:   3820, Loss function: 3.250, Average Loss: 3.853, avg. samples / sec: 28030.20
Iteration:   3820, Loss function: 3.117, Average Loss: 3.862, avg. samples / sec: 27929.27
Iteration:   3820, Loss function: 3.542, Average Loss: 3.866, avg. samples / sec: 27995.33
Iteration:   3820, Loss function: 2.974, Average Loss: 3.850, avg. samples / sec: 27944.89
Iteration:   3840, Loss function: 3.380, Average Loss: 3.839, avg. samples / sec: 28102.98
Iteration:   3840, Loss function: 3.851, Average Loss: 3.844, avg. samples / sec: 28035.06
Iteration:   3840, Loss function: 3.543, Average Loss: 3.857, avg. samples / sec: 28035.30
Iteration:   3840, Loss function: 3.170, Average Loss: 3.853, avg. samples / sec: 28027.00
Iteration:   3860, Loss function: 3.204, Average Loss: 3.850, avg. samples / sec: 28043.66
Iteration:   3860, Loss function: 3.981, Average Loss: 3.832, avg. samples / sec: 27976.38
Iteration:   3860, Loss function: 3.524, Average Loss: 3.845, avg. samples / sec: 28032.49
Iteration:   3860, Loss function: 3.783, Average Loss: 3.839, avg. samples / sec: 28003.20
Iteration:   3880, Loss function: 3.217, Average Loss: 3.826, avg. samples / sec: 28035.07
Iteration:   3880, Loss function: 3.036, Average Loss: 3.837, avg. samples / sec: 28046.40
Iteration:   3880, Loss function: 3.272, Average Loss: 3.832, avg. samples / sec: 28048.67
Iteration:   3880, Loss function: 3.283, Average Loss: 3.841, avg. samples / sec: 28019.10
:::MLL 1558580219.956 epoch_stop: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 819}}
:::MLL 1558580219.957 epoch_start: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 673}}
Iteration:   3900, Loss function: 3.481, Average Loss: 3.830, avg. samples / sec: 28023.44
Iteration:   3900, Loss function: 3.568, Average Loss: 3.824, avg. samples / sec: 28011.20
Iteration:   3900, Loss function: 3.302, Average Loss: 3.819, avg. samples / sec: 27996.99
Iteration:   3900, Loss function: 3.645, Average Loss: 3.828, avg. samples / sec: 27996.03
Iteration:   3920, Loss function: 3.615, Average Loss: 3.820, avg. samples / sec: 28135.53
Iteration:   3920, Loss function: 4.141, Average Loss: 3.823, avg. samples / sec: 28109.33
Iteration:   3920, Loss function: 2.859, Average Loss: 3.818, avg. samples / sec: 28106.21
Iteration:   3920, Loss function: 2.920, Average Loss: 3.809, avg. samples / sec: 28107.59
Iteration:   3940, Loss function: 3.338, Average Loss: 3.812, avg. samples / sec: 28066.51
Iteration:   3940, Loss function: 3.237, Average Loss: 3.817, avg. samples / sec: 28073.14
Iteration:   3940, Loss function: 3.873, Average Loss: 3.805, avg. samples / sec: 28057.89
Iteration:   3940, Loss function: 3.576, Average Loss: 3.810, avg. samples / sec: 28051.82
Iteration:   3960, Loss function: 3.013, Average Loss: 3.803, avg. samples / sec: 28364.25
Iteration:   3960, Loss function: 4.248, Average Loss: 3.812, avg. samples / sec: 28322.47
Iteration:   3960, Loss function: 3.792, Average Loss: 3.803, avg. samples / sec: 28320.68
Iteration:   3960, Loss function: 3.055, Average Loss: 3.797, avg. samples / sec: 28320.86
:::MLL 1558580224.106 epoch_stop: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 819}}
:::MLL 1558580224.107 epoch_start: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 673}}
Iteration:   3980, Loss function: 2.925, Average Loss: 3.788, avg. samples / sec: 28121.49
Iteration:   3980, Loss function: 3.588, Average Loss: 3.801, avg. samples / sec: 28086.39
Iteration:   3980, Loss function: 3.331, Average Loss: 3.796, avg. samples / sec: 28085.16
Iteration:   3980, Loss function: 3.010, Average Loss: 3.796, avg. samples / sec: 28031.64
Iteration:   4000, Loss function: 3.577, Average Loss: 3.789, avg. samples / sec: 28154.64
Iteration:   4000, Loss function: 3.376, Average Loss: 3.778, avg. samples / sec: 28096.62
Iteration:   4000, Loss function: 3.601, Average Loss: 3.793, avg. samples / sec: 28097.57
Iteration:   4000, Loss function: 3.866, Average Loss: 3.788, avg. samples / sec: 28070.48
Iteration:   4020, Loss function: 3.278, Average Loss: 3.771, avg. samples / sec: 28093.96
Iteration:   4020, Loss function: 3.871, Average Loss: 3.781, avg. samples / sec: 28049.26
Iteration:   4020, Loss function: 2.957, Average Loss: 3.779, avg. samples / sec: 28085.02
Iteration:   4020, Loss function: 3.165, Average Loss: 3.783, avg. samples / sec: 28041.59
Iteration:   4040, Loss function: 3.526, Average Loss: 3.773, avg. samples / sec: 28057.03
Iteration:   4040, Loss function: 3.540, Average Loss: 3.776, avg. samples / sec: 28068.05
Iteration:   4040, Loss function: 3.327, Average Loss: 3.765, avg. samples / sec: 28006.08
Iteration:   4040, Loss function: 3.572, Average Loss: 3.771, avg. samples / sec: 28044.61
:::MLL 1558580228.321 epoch_stop: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 819}}
:::MLL 1558580228.321 epoch_start: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 673}}
Iteration:   4060, Loss function: 4.021, Average Loss: 3.758, avg. samples / sec: 27951.38
Iteration:   4060, Loss function: 3.398, Average Loss: 3.764, avg. samples / sec: 27952.37
Iteration:   4060, Loss function: 3.218, Average Loss: 3.766, avg. samples / sec: 27934.35
Iteration:   4060, Loss function: 3.691, Average Loss: 3.765, avg. samples / sec: 27879.92
Iteration:   4080, Loss function: 3.331, Average Loss: 3.748, avg. samples / sec: 27943.63
Iteration:   4080, Loss function: 3.534, Average Loss: 3.756, avg. samples / sec: 27952.28
Iteration:   4080, Loss function: 3.616, Average Loss: 3.757, avg. samples / sec: 27939.61
Iteration:   4080, Loss function: 3.253, Average Loss: 3.756, avg. samples / sec: 27967.35
Iteration:   4100, Loss function: 3.690, Average Loss: 3.749, avg. samples / sec: 28031.90
Iteration:   4100, Loss function: 3.554, Average Loss: 3.743, avg. samples / sec: 27989.65
Iteration:   4100, Loss function: 3.616, Average Loss: 3.752, avg. samples / sec: 27936.40
Iteration:   4100, Loss function: 4.060, Average Loss: 3.750, avg. samples / sec: 27924.30
Iteration:   4120, Loss function: 3.055, Average Loss: 3.745, avg. samples / sec: 28038.01
Iteration:   4120, Loss function: 3.217, Average Loss: 3.741, avg. samples / sec: 28043.85
Iteration:   4120, Loss function: 3.467, Average Loss: 3.736, avg. samples / sec: 27978.71
Iteration:   4120, Loss function: 3.631, Average Loss: 3.742, avg. samples / sec: 27967.50
:::MLL 1558580232.493 epoch_stop: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 819}}
:::MLL 1558580232.493 epoch_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 673}}
Iteration:   4140, Loss function: 3.527, Average Loss: 3.735, avg. samples / sec: 28029.41
Iteration:   4140, Loss function: 3.189, Average Loss: 3.739, avg. samples / sec: 27996.28
Iteration:   4140, Loss function: 3.650, Average Loss: 3.730, avg. samples / sec: 27967.77
Iteration:   4140, Loss function: 3.034, Average Loss: 3.733, avg. samples / sec: 27939.34
Iteration:   4160, Loss function: 3.457, Average Loss: 3.723, avg. samples / sec: 27932.65
Iteration:   4160, Loss function: 4.119, Average Loss: 3.730, avg. samples / sec: 27865.42
Iteration:   4160, Loss function: 3.487, Average Loss: 3.729, avg. samples / sec: 27888.90
Iteration:   4160, Loss function: 3.793, Average Loss: 3.726, avg. samples / sec: 27917.10
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
:::MLL 1558580234.912 eval_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.17 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.17 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.17 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.17 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=2.82s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22828
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.39015
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23366
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05482
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24052
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.37123
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22065
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32214
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33760
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09584
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36842
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.52934
Current AP: 0.22828 AP goal: 0.23000
:::MLL 1558580239.464 eval_accuracy: {"value": 0.22827570019513713, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 389}}
:::MLL 1558580239.522 eval_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 392}}
:::MLL 1558580239.535 block_stop: {"value": null, "metadata": {"first_epoch_num": 50, "file": "train.py", "lineno": 804}}
:::MLL 1558580239.536 block_start: {"value": null, "metadata": {"first_epoch_num": 55, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   4180, Loss function: 3.567, Average Loss: 3.720, avg. samples / sec: 5367.97
Iteration:   4180, Loss function: 3.446, Average Loss: 3.713, avg. samples / sec: 5366.46
Iteration:   4180, Loss function: 3.977, Average Loss: 3.722, avg. samples / sec: 5366.80
Iteration:   4180, Loss function: 3.200, Average Loss: 3.722, avg. samples / sec: 5365.22
:::MLL 1558580241.305 epoch_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 819}}
:::MLL 1558580241.305 epoch_start: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 673}}
Iteration:   4200, Loss function: 3.592, Average Loss: 3.713, avg. samples / sec: 27669.82
Iteration:   4200, Loss function: 3.383, Average Loss: 3.715, avg. samples / sec: 27736.27
Iteration:   4200, Loss function: 3.587, Average Loss: 3.713, avg. samples / sec: 27722.65
Iteration:   4200, Loss function: 2.984, Average Loss: 3.707, avg. samples / sec: 27661.28
Iteration:   4220, Loss function: 2.830, Average Loss: 3.705, avg. samples / sec: 28046.25
Iteration:   4220, Loss function: 3.966, Average Loss: 3.705, avg. samples / sec: 28040.86
Iteration:   4220, Loss function: 3.144, Average Loss: 3.700, avg. samples / sec: 28052.70
Iteration:   4220, Loss function: 3.159, Average Loss: 3.704, avg. samples / sec: 28005.06
Iteration:   4240, Loss function: 3.009, Average Loss: 3.695, avg. samples / sec: 27960.36
Iteration:   4240, Loss function: 3.036, Average Loss: 3.694, avg. samples / sec: 27923.01
Iteration:   4240, Loss function: 3.491, Average Loss: 3.701, avg. samples / sec: 27922.42
Iteration:   4240, Loss function: 3.531, Average Loss: 3.691, avg. samples / sec: 27835.07
Iteration:   4260, Loss function: 3.930, Average Loss: 3.687, avg. samples / sec: 27939.99
Iteration:   4260, Loss function: 3.190, Average Loss: 3.683, avg. samples / sec: 27927.49
Iteration:   4260, Loss function: 3.091, Average Loss: 3.693, avg. samples / sec: 27938.33
Iteration:   4260, Loss function: 2.537, Average Loss: 3.684, avg. samples / sec: 28049.76
:::MLL 1558580245.539 epoch_stop: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 819}}
:::MLL 1558580245.540 epoch_start: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 673}}
Iteration:   4280, Loss function: 3.687, Average Loss: 3.681, avg. samples / sec: 27877.53
Iteration:   4280, Loss function: 3.408, Average Loss: 3.686, avg. samples / sec: 27876.67
Iteration:   4280, Loss function: 3.566, Average Loss: 3.678, avg. samples / sec: 27874.30
Iteration:   4280, Loss function: 3.502, Average Loss: 3.675, avg. samples / sec: 27868.35
Iteration:   4300, Loss function: 3.536, Average Loss: 3.674, avg. samples / sec: 27950.87
Iteration:   4300, Loss function: 3.415, Average Loss: 3.669, avg. samples / sec: 27968.38
Iteration:   4300, Loss function: 3.257, Average Loss: 3.678, avg. samples / sec: 27951.21
Iteration:   4300, Loss function: 3.325, Average Loss: 3.671, avg. samples / sec: 27944.94
Iteration:   4320, Loss function: 3.603, Average Loss: 3.663, avg. samples / sec: 27943.23
Iteration:   4320, Loss function: 3.650, Average Loss: 3.668, avg. samples / sec: 27939.70
Iteration:   4320, Loss function: 3.063, Average Loss: 3.662, avg. samples / sec: 27944.96
Iteration:   4320, Loss function: 3.389, Average Loss: 3.670, avg. samples / sec: 27916.07
Iteration:   4340, Loss function: 3.142, Average Loss: 3.656, avg. samples / sec: 27983.55
Iteration:   4340, Loss function: 3.190, Average Loss: 3.661, avg. samples / sec: 27944.48
Iteration:   4340, Loss function: 4.188, Average Loss: 3.666, avg. samples / sec: 27974.09
Iteration:   4340, Loss function: 3.417, Average Loss: 3.654, avg. samples / sec: 27929.04
:::MLL 1558580249.710 epoch_stop: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 819}}
:::MLL 1558580249.710 epoch_start: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 673}}
Iteration:   4360, Loss function: 3.609, Average Loss: 3.659, avg. samples / sec: 28084.75
Iteration:   4360, Loss function: 3.489, Average Loss: 3.649, avg. samples / sec: 28100.11
Iteration:   4360, Loss function: 3.212, Average Loss: 3.652, avg. samples / sec: 27996.18
Iteration:   4360, Loss function: 3.486, Average Loss: 3.651, avg. samples / sec: 28035.81
Iteration:   4380, Loss function: 3.920, Average Loss: 3.648, avg. samples / sec: 28012.17
Iteration:   4380, Loss function: 3.422, Average Loss: 3.646, avg. samples / sec: 27960.46
Iteration:   4380, Loss function: 3.577, Average Loss: 3.652, avg. samples / sec: 27937.39
Iteration:   4380, Loss function: 3.124, Average Loss: 3.647, avg. samples / sec: 27975.44
Iteration:   4400, Loss function: 3.594, Average Loss: 3.641, avg. samples / sec: 28027.25
Iteration:   4400, Loss function: 3.128, Average Loss: 3.648, avg. samples / sec: 28042.81
Iteration:   4400, Loss function: 4.059, Average Loss: 3.641, avg. samples / sec: 28004.04
Iteration:   4400, Loss function: 4.003, Average Loss: 3.640, avg. samples / sec: 27986.53
Iteration:   4420, Loss function: 3.253, Average Loss: 3.635, avg. samples / sec: 27990.10
Iteration:   4420, Loss function: 3.313, Average Loss: 3.634, avg. samples / sec: 27965.51
Iteration:   4420, Loss function: 4.208, Average Loss: 3.635, avg. samples / sec: 28028.57
Iteration:   4420, Loss function: 2.889, Average Loss: 3.641, avg. samples / sec: 27930.95
:::MLL 1558580253.937 epoch_stop: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 819}}
:::MLL 1558580253.938 epoch_start: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 673}}
Iteration:   4440, Loss function: 3.169, Average Loss: 3.629, avg. samples / sec: 27907.63
Iteration:   4440, Loss function: 3.244, Average Loss: 3.635, avg. samples / sec: 27938.90
Iteration:   4440, Loss function: 3.168, Average Loss: 3.629, avg. samples / sec: 27859.80
Iteration:   4440, Loss function: 3.113, Average Loss: 3.629, avg. samples / sec: 27861.37
Iteration:   4460, Loss function: 3.754, Average Loss: 3.623, avg. samples / sec: 28028.04
Iteration:   4460, Loss function: 2.957, Average Loss: 3.624, avg. samples / sec: 28003.31
Iteration:   4460, Loss function: 3.471, Average Loss: 3.631, avg. samples / sec: 27976.77
Iteration:   4460, Loss function: 3.147, Average Loss: 3.624, avg. samples / sec: 27964.36
Iteration:   4480, Loss function: 3.218, Average Loss: 3.618, avg. samples / sec: 27990.61
Iteration:   4480, Loss function: 2.655, Average Loss: 3.617, avg. samples / sec: 28001.40
Iteration:   4480, Loss function: 3.312, Average Loss: 3.622, avg. samples / sec: 27960.32
Iteration:   4480, Loss function: 3.171, Average Loss: 3.617, avg. samples / sec: 27955.12
Iteration:   4500, Loss function: 3.452, Average Loss: 3.610, avg. samples / sec: 28142.28
Iteration:   4500, Loss function: 3.493, Average Loss: 3.618, avg. samples / sec: 28180.30
Iteration:   4500, Loss function: 3.331, Average Loss: 3.612, avg. samples / sec: 28106.21
Iteration:   4500, Loss function: 4.081, Average Loss: 3.611, avg. samples / sec: 28140.32
:::MLL 1558580258.107 epoch_stop: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 819}}
:::MLL 1558580258.107 epoch_start: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 673}}
Iteration:   4520, Loss function: 3.369, Average Loss: 3.606, avg. samples / sec: 28037.50
Iteration:   4520, Loss function: 2.967, Average Loss: 3.604, avg. samples / sec: 28078.86
Iteration:   4520, Loss function: 3.612, Average Loss: 3.607, avg. samples / sec: 28070.39
Iteration:   4520, Loss function: 3.521, Average Loss: 3.613, avg. samples / sec: 27987.54
Iteration:   4540, Loss function: 3.438, Average Loss: 3.609, avg. samples / sec: 27925.77
Iteration:   4540, Loss function: 3.128, Average Loss: 3.602, avg. samples / sec: 27869.73
Iteration:   4540, Loss function: 3.143, Average Loss: 3.598, avg. samples / sec: 27851.13
Iteration:   4540, Loss function: 4.145, Average Loss: 3.600, avg. samples / sec: 27846.47
Iteration:   4560, Loss function: 3.073, Average Loss: 3.592, avg. samples / sec: 27945.92
Iteration:   4560, Loss function: 2.630, Average Loss: 3.603, avg. samples / sec: 27913.41
Iteration:   4560, Loss function: 3.108, Average Loss: 3.592, avg. samples / sec: 27946.26
Iteration:   4560, Loss function: 3.209, Average Loss: 3.596, avg. samples / sec: 27922.44
Iteration:   4580, Loss function: 3.305, Average Loss: 3.598, avg. samples / sec: 27938.15
Iteration:   4580, Loss function: 3.152, Average Loss: 3.590, avg. samples / sec: 27929.83
Iteration:   4580, Loss function: 3.408, Average Loss: 3.586, avg. samples / sec: 27881.55
Iteration:   4580, Loss function: 3.946, Average Loss: 3.586, avg. samples / sec: 27878.26
:::MLL 1558580262.285 epoch_stop: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 819}}
:::MLL 1558580262.286 epoch_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 673}}
:::MLL 1558580262.452 eval_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.20 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.20 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.20 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.20 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.54s)
DONE (t=2.77s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.23182
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.39396
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23704
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05651
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24530
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.37464
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32644
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.34208
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09926
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.37346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.53807
Current AP: 0.23182 AP goal: 0.23000
:::MLL 1558580267.000 eval_accuracy: {"value": 0.2318152002245126, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 389}}
:::MLL 1558580267.072 eval_stop: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 392}}
:::MLL 1558580267.086 block_stop: {"value": null, "metadata": {"first_epoch_num": 55, "file": "train.py", "lineno": 804}}
:::MLL 1558580268.007 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 849}}

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-05-23 02:57:57 AM
RESULT,SINGLE_STAGE_DETECTOR,,351,nvidia,2019-05-23 02:52:06 AM
ENDING TIMING RUN AT 2019-05-23 02:57:57 AM
RESULT,SINGLE_STAGE_DETECTOR,,351,nvidia,2019-05-23 02:52:06 AM
ENDING TIMING RUN AT 2019-05-23 02:57:57 AM
RESULT,SINGLE_STAGE_DETECTOR,,351,nvidia,2019-05-23 02:52:06 AM
ENDING TIMING RUN AT 2019-05-23 02:57:57 AM
RESULT,SINGLE_STAGE_DETECTOR,,351,nvidia,2019-05-23 02:52:06 AM
