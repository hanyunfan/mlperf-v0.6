Beginning trial 2 of 5
Gathering sys log on circe-n051
:::MLL 1558578858.198 submission_benchmark: {"value": "ssd", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1558578858.199 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known ssd keys.
:::MLL 1558578858.199 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1558578858.200 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1558578858.200 submission_platform: {"value": "4xNVIDIA DGX-2H", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1558578858.200 submission_entry: {"value": "{'hardware': 'NVIDIA DGX-2H', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': 'Ubuntu 18.04.1 LTS / NVIDIA DGX Server 4.0.4 4.0.5', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.4-2.0.7', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '4', 'cpu': '2x Intel(R) Xeon(R) Platinum 8174 CPU @ 3.10GHz', 'num_cores': '48', 'num_vcpus': '96', 'accelerator': 'Tesla V100-SXM3-32GB-H', 'num_accelerators': '16', 'sys_mem_size': '1510 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '2x 894.3G + 8x 3.5T', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27800 Family [ConnectX-5]', 'num_network_cards': '10', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1558578858.201 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1558578858.201 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
:::MLL 1558578861.055 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558578861.059 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558578861.069 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558578861.118 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node circe-n051
+ pids+=($!)
+ set +x
Launching on node circe-n052
+ pids+=($!)
+ set +x
Launching on node circe-n053
+ pids+=($!)
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ set +x
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n051
Launching on node circe-n054
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n052
+ srun --mem=0 -N 1 -n 1 -w circe-n051 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=0 --master_addr=10.0.1.51 --master_port=4728' -e SLURM_JOB_ID=89451 -e SLURM_NTASKS_PER_NODE=16 cont_89451 ./run_and_time.sh
+ pids+=($!)
+ set +x
+ srun --mem=0 -N 1 -n 1 -w circe-n052 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=1 --master_addr=10.0.1.51 --master_port=4728' -e SLURM_JOB_ID=89451 -e SLURM_NTASKS_PER_NODE=16 cont_89451 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n053
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n054
+ srun --mem=0 -N 1 -n 1 -w circe-n053 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=2 --master_addr=10.0.1.51 --master_port=4728' -e SLURM_JOB_ID=89451 -e SLURM_NTASKS_PER_NODE=16 cont_89451 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w circe-n054 docker exec -e DGXSYSTEM=DGX2_multi_4x16x24 -e 'MULTI_NODE= --nnodes=4 --node_rank=3 --master_addr=10.0.1.51 --master_port=4728' -e SLURM_JOB_ID=89451 -e SLURM_NTASKS_PER_NODE=16 cont_89451 ./run_and_time.sh
Run vars: id 89451 gpus 16 mparams  --nnodes=4 --node_rank=0 --master_addr=10.0.1.51 --master_port=4728
Run vars: id 89451 gpus 16 mparams  --nnodes=4 --node_rank=2 --master_addr=10.0.1.51 --master_port=4728
Run vars: id 89451 gpus 16 mparams  --nnodes=4 --node_rank=3 --master_addr=10.0.1.51 --master_port=4728
Run vars: id 89451 gpus 16 mparams  --nnodes=4 --node_rank=1 --master_addr=10.0.1.51 --master_port=4728
STARTING TIMING RUN AT 2019-05-23 02:34:21 AM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=0 --master_addr=10.0.1.51 --master_port=4728 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 02:34:21 AM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
STARTING TIMING RUN AT 2019-05-23 02:34:21 AM
+ export DATASET_DIR=/data/coco2017
running benchmark
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=2 --master_addr=10.0.1.51 --master_port=4728 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=3 --master_addr=10.0.1.51 --master_port=4728 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 02:34:21 AM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=4 --node_rank=1 --master_addr=10.0.1.51 --master_port=4728 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 24 --eval-batch-size 40 --warmup 850 --num-workers 3 --lr 2.9e-3 --wd 1.7e-4 --use-nvjpeg --use-roi-decode
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558578871.710 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.710 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.711 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.711 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578871.712 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.712 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.712 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.712 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578871.712 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.713 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.713 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578871.713 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.713 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.713 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.713 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558578871.714 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558578871.756 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.756 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.757 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578871.758 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.758 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.758 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.758 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578871.758 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558578871.757 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.757 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.758 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.759 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.759 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578871.758 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.759 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.758 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558578871.759 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558578871.760 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.759 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578871.759 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578871.759 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.760 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578871.760 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578871.760 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558578871.760 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578871.761 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558578871.760 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.760 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.760 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558578871.761 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.761 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558578871.761 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']:::MLL 1558578871.791 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.791 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.792 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
:::MLL 1558578871.796 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.796 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.796 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.796 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578871.798 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.798 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
:::MLL 1558578871.798 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578871.798 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.798 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.799 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.799 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558578871.799 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1558578871.799 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
BN group: 1
0 Using seed = 945905820
1 Using seed = 945905821
3 Using seed = 945905823
4 Using seed = 945905824
5 Using seed = 945905825
2 Using seed = 945905822
:::MLL 1558578903.831 max_samples: {"value": 1, "metadata": {"file": "utils.py", "lineno": 465}}
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
:::MLL 1558578904.533 model_bn_span: {"value": 24, "metadata": {"file": "train.py", "lineno": 480}}
:::MLL 1558578904.533 global_batch_size: {"value": 1536, "metadata": {"file": "train.py", "lineno": 481}}
:::MLL 1558578904.543 opt_base_learning_rate: {"value": 0.14, "metadata": {"file": "train.py", "lineno": 511}}
:::MLL 1558578904.543 opt_weight_decay: {"value": 0.00017, "metadata": {"file": "train.py", "lineno": 513}}
:::MLL 1558578904.544 opt_learning_rate_warmup_steps: {"value": 850, "metadata": {"file": "train.py", "lineno": 516}}
:::MLL 1558578904.544 opt_learning_rate_warmup_factor: {"value": 0, "metadata": {"file": "train.py", "lineno": 518}}
8 Using seed = 945905828
11 Using seed = 945905831
9 Using seed = 945905829
7 Using seed = 945905827
6 Using seed = 945905826
18 Using seed = 945905838
20 Using seed = 945905840
22 Using seed = 945905842
19 Using seed = 945905839
30 Using seed = 945905850
16 Using seed = 945905836
28 Using seed = 945905848
25 Using seed = 945905845
29 Using seed = 945905849
26 Using seed = 945905846
31 Using seed = 945905851
27 Using seed = 945905847
23 Using seed = 945905843
17 Using seed = 945905837
24 Using seed = 945905844
21 Using seed = 945905841
32 Using seed = 945905852
36 Using seed = 945905856
35 Using seed = 945905855
39 Using seed = 945905859
34 Using seed = 945905854
38 Using seed = 945905858
45 Using seed = 945905865
33 Using seed = 945905853
47 Using seed = 945905867
46 Using seed = 945905866
42 Using seed = 945905862
43 Using seed = 945905863
41 Using seed = 945905861
44 Using seed = 945905864
37 Using seed = 945905857
40 Using seed = 945905860
58 Using seed = 945905878
61 Using seed = 945905881
62 Using seed = 945905882
63 Using seed = 945905883
50 Using seed = 945905870
48 Using seed = 945905868
51 Using seed = 945905871
49 Using seed = 945905869
52 Using seed = 945905872
53 Using seed = 945905873
60 Using seed = 945905880
57 Using seed = 945905877
59 Using seed = 945905879
56 Using seed = 945905876
55 Using seed = 945905875
54 Using seed = 945905874
10 Using seed = 945905830
14 Using seed = 945905834
15 Using seed = 945905835
12 Using seed = 945905832
13 Using seed = 945905833
epoch nbatch loss
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
:::MLL 1558578915.689 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 604}}
:::MLL 1558578915.689 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 610}}
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.47s)
creating index...
time_check a: 1558578917.407392025
time_check a: 1558578917.420775414
time_check a: 1558578917.414642096
time_check a: 1558578917.419625044
time_check b: 1558578924.032830238
time_check b: 1558578924.291132927
time_check b: 1558578924.295931339
time_check b: 1558578924.336683512
:::MLL 1558578925.381 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 32.74606450292497, "file": "train.py", "lineno": 669}}
:::MLL 1558578925.382 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 673}}
Iteration:      0, Loss function: 22.330, Average Loss: 0.022, avg. samples / sec: 102.31
Iteration:      0, Loss function: 22.701, Average Loss: 0.023, avg. samples / sec: 102.20
Iteration:      0, Loss function: 22.306, Average Loss: 0.022, avg. samples / sec: 79.36
Iteration:      0, Loss function: 22.677, Average Loss: 0.023, avg. samples / sec: 103.85
Iteration:     20, Loss function: 20.333, Average Loss: 0.443, avg. samples / sec: 21150.43
Iteration:     20, Loss function: 20.375, Average Loss: 0.442, avg. samples / sec: 21187.84
Iteration:     20, Loss function: 20.556, Average Loss: 0.442, avg. samples / sec: 21189.43
Iteration:     20, Loss function: 20.571, Average Loss: 0.445, avg. samples / sec: 21415.20
Iteration:     40, Loss function: 17.230, Average Loss: 0.823, avg. samples / sec: 28159.13
Iteration:     40, Loss function: 16.854, Average Loss: 0.822, avg. samples / sec: 28198.51
Iteration:     40, Loss function: 17.063, Average Loss: 0.826, avg. samples / sec: 28169.80
Iteration:     40, Loss function: 16.727, Average Loss: 0.822, avg. samples / sec: 28077.16
Iteration:     60, Loss function: 13.315, Average Loss: 1.074, avg. samples / sec: 28089.51
Iteration:     60, Loss function: 13.708, Average Loss: 1.082, avg. samples / sec: 28128.72
Iteration:     60, Loss function: 11.906, Average Loss: 1.071, avg. samples / sec: 28135.81
Iteration:     60, Loss function: 13.580, Average Loss: 1.082, avg. samples / sec: 28036.46
:::MLL 1558578931.134 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 819}}
:::MLL 1558578931.134 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 673}}
Iteration:     80, Loss function: 9.866, Average Loss: 1.275, avg. samples / sec: 28364.55
Iteration:     80, Loss function: 10.239, Average Loss: 1.286, avg. samples / sec: 28360.86
Iteration:     80, Loss function: 9.926, Average Loss: 1.279, avg. samples / sec: 28297.16
Iteration:     80, Loss function: 10.351, Average Loss: 1.287, avg. samples / sec: 28293.82
Iteration:    100, Loss function: 9.494, Average Loss: 1.451, avg. samples / sec: 28344.04
Iteration:    100, Loss function: 8.923, Average Loss: 1.446, avg. samples / sec: 28347.85
Iteration:    100, Loss function: 9.276, Average Loss: 1.441, avg. samples / sec: 28306.68
Iteration:    100, Loss function: 9.321, Average Loss: 1.453, avg. samples / sec: 28272.60
Iteration:    120, Loss function: 8.984, Average Loss: 1.594, avg. samples / sec: 28468.25
Iteration:    120, Loss function: 9.209, Average Loss: 1.598, avg. samples / sec: 28453.14
Iteration:    120, Loss function: 8.638, Average Loss: 1.603, avg. samples / sec: 28446.98
Iteration:    120, Loss function: 9.336, Average Loss: 1.603, avg. samples / sec: 28536.24
Iteration:    140, Loss function: 8.539, Average Loss: 1.743, avg. samples / sec: 28396.05
Iteration:    140, Loss function: 8.896, Average Loss: 1.749, avg. samples / sec: 28400.39
Iteration:    140, Loss function: 8.694, Average Loss: 1.739, avg. samples / sec: 28353.13
Iteration:    140, Loss function: 8.893, Average Loss: 1.749, avg. samples / sec: 28345.86
:::MLL 1558578935.248 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 819}}
:::MLL 1558578935.248 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 673}}
Iteration:    160, Loss function: 8.366, Average Loss: 1.887, avg. samples / sec: 28322.44
Iteration:    160, Loss function: 7.886, Average Loss: 1.880, avg. samples / sec: 28325.87
Iteration:    160, Loss function: 8.637, Average Loss: 1.887, avg. samples / sec: 28367.56
Iteration:    160, Loss function: 8.774, Average Loss: 1.874, avg. samples / sec: 28318.05
Iteration:    180, Loss function: 8.070, Average Loss: 2.014, avg. samples / sec: 28380.83
Iteration:    180, Loss function: 8.123, Average Loss: 2.009, avg. samples / sec: 28368.43
Iteration:    180, Loss function: 8.496, Average Loss: 2.017, avg. samples / sec: 28370.56
Iteration:    180, Loss function: 8.699, Average Loss: 2.004, avg. samples / sec: 28369.12
Iteration:    200, Loss function: 8.260, Average Loss: 2.126, avg. samples / sec: 28499.13
Iteration:    200, Loss function: 8.431, Average Loss: 2.131, avg. samples / sec: 28455.79
Iteration:    200, Loss function: 7.785, Average Loss: 2.138, avg. samples / sec: 28447.63
Iteration:    200, Loss function: 8.344, Average Loss: 2.138, avg. samples / sec: 28436.30
Iteration:    220, Loss function: 8.273, Average Loss: 2.257, avg. samples / sec: 28432.63
Iteration:    220, Loss function: 8.123, Average Loss: 2.251, avg. samples / sec: 28409.00
Iteration:    220, Loss function: 8.623, Average Loss: 2.245, avg. samples / sec: 28401.56
Iteration:    220, Loss function: 9.050, Average Loss: 2.259, avg. samples / sec: 28359.70
:::MLL 1558578939.428 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 819}}
:::MLL 1558578939.428 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 673}}
Iteration:    240, Loss function: 7.494, Average Loss: 2.373, avg. samples / sec: 28004.43
Iteration:    240, Loss function: 7.626, Average Loss: 2.366, avg. samples / sec: 27997.30
Iteration:    240, Loss function: 7.923, Average Loss: 2.358, avg. samples / sec: 28004.36
Iteration:    240, Loss function: 7.884, Average Loss: 2.373, avg. samples / sec: 28031.70
Iteration:    260, Loss function: 7.641, Average Loss: 2.477, avg. samples / sec: 28286.14
Iteration:    260, Loss function: 7.747, Average Loss: 2.462, avg. samples / sec: 28290.41
Iteration:    260, Loss function: 7.923, Average Loss: 2.469, avg. samples / sec: 28289.34
Iteration:    260, Loss function: 7.650, Average Loss: 2.478, avg. samples / sec: 28280.98
Iteration:    280, Loss function: 7.417, Average Loss: 2.580, avg. samples / sec: 28381.34
Iteration:    280, Loss function: 7.867, Average Loss: 2.582, avg. samples / sec: 28313.64
Iteration:    280, Loss function: 8.117, Average Loss: 2.567, avg. samples / sec: 28312.39
Iteration:    280, Loss function: 7.311, Average Loss: 2.574, avg. samples / sec: 28311.42
Iteration:    300, Loss function: 7.348, Average Loss: 2.675, avg. samples / sec: 28367.01
Iteration:    300, Loss function: 7.613, Average Loss: 2.671, avg. samples / sec: 28400.13
Iteration:    300, Loss function: 6.850, Average Loss: 2.661, avg. samples / sec: 28393.24
Iteration:    300, Loss function: 7.130, Average Loss: 2.675, avg. samples / sec: 28331.80
:::MLL 1558578943.547 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 819}}
:::MLL 1558578943.548 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 673}}
Iteration:    320, Loss function: 7.454, Average Loss: 2.766, avg. samples / sec: 28329.60
Iteration:    320, Loss function: 7.236, Average Loss: 2.753, avg. samples / sec: 28328.66
Iteration:    320, Loss function: 6.919, Average Loss: 2.765, avg. samples / sec: 28356.44
Iteration:    320, Loss function: 6.753, Average Loss: 2.759, avg. samples / sec: 28285.04
Iteration:    340, Loss function: 6.868, Average Loss: 2.849, avg. samples / sec: 28421.52
Iteration:    340, Loss function: 7.123, Average Loss: 2.843, avg. samples / sec: 28427.55
Iteration:    340, Loss function: 7.456, Average Loss: 2.847, avg. samples / sec: 28377.98
Iteration:    340, Loss function: 7.337, Average Loss: 2.839, avg. samples / sec: 28370.59
Iteration:    360, Loss function: 6.935, Average Loss: 2.927, avg. samples / sec: 28382.44
Iteration:    360, Loss function: 7.018, Average Loss: 2.927, avg. samples / sec: 28343.48
Iteration:    360, Loss function: 7.690, Average Loss: 2.939, avg. samples / sec: 28323.86
Iteration:    360, Loss function: 7.824, Average Loss: 2.933, avg. samples / sec: 28291.55
Iteration:    380, Loss function: 6.411, Average Loss: 3.005, avg. samples / sec: 28339.67
Iteration:    380, Loss function: 6.668, Average Loss: 3.011, avg. samples / sec: 28378.32
Iteration:    380, Loss function: 7.126, Average Loss: 3.006, avg. samples / sec: 28325.78
Iteration:    380, Loss function: 6.549, Average Loss: 3.017, avg. samples / sec: 28311.16
:::MLL 1558578947.665 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 819}}
:::MLL 1558578947.666 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 673}}
Iteration:    400, Loss function: 6.438, Average Loss: 3.079, avg. samples / sec: 28405.37
Iteration:    400, Loss function: 7.345, Average Loss: 3.084, avg. samples / sec: 28408.39
Iteration:    400, Loss function: 7.571, Average Loss: 3.078, avg. samples / sec: 28358.19
Iteration:    400, Loss function: 6.801, Average Loss: 3.091, avg. samples / sec: 28395.93
Iteration:    420, Loss function: 6.563, Average Loss: 3.160, avg. samples / sec: 28408.34
Iteration:    420, Loss function: 6.983, Average Loss: 3.157, avg. samples / sec: 28402.53
Iteration:    420, Loss function: 7.065, Average Loss: 3.156, avg. samples / sec: 28406.18
Iteration:    420, Loss function: 6.803, Average Loss: 3.166, avg. samples / sec: 28409.06
Iteration:    440, Loss function: 6.159, Average Loss: 3.229, avg. samples / sec: 28317.46
Iteration:    440, Loss function: 6.669, Average Loss: 3.239, avg. samples / sec: 28336.55
Iteration:    440, Loss function: 6.193, Average Loss: 3.228, avg. samples / sec: 28308.35
Iteration:    440, Loss function: 6.561, Average Loss: 3.232, avg. samples / sec: 28259.56
:::MLL 1558578951.838 epoch_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 819}}
:::MLL 1558578951.838 epoch_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 673}}
Iteration:    460, Loss function: 6.937, Average Loss: 3.293, avg. samples / sec: 28255.74
Iteration:    460, Loss function: 6.638, Average Loss: 3.294, avg. samples / sec: 28292.53
Iteration:    460, Loss function: 6.496, Average Loss: 3.307, avg. samples / sec: 28237.93
Iteration:    460, Loss function: 6.588, Average Loss: 3.294, avg. samples / sec: 28220.01
Iteration:    480, Loss function: 6.009, Average Loss: 3.369, avg. samples / sec: 28269.15
Iteration:    480, Loss function: 6.143, Average Loss: 3.358, avg. samples / sec: 28235.84
Iteration:    480, Loss function: 6.295, Average Loss: 3.358, avg. samples / sec: 28245.09
Iteration:    480, Loss function: 6.391, Average Loss: 3.357, avg. samples / sec: 28226.40
Iteration:    500, Loss function: 6.644, Average Loss: 3.430, avg. samples / sec: 28422.98
Iteration:    500, Loss function: 6.004, Average Loss: 3.416, avg. samples / sec: 28421.26
Iteration:    500, Loss function: 5.982, Average Loss: 3.417, avg. samples / sec: 28408.00
Iteration:    500, Loss function: 6.803, Average Loss: 3.416, avg. samples / sec: 28458.93
Iteration:    520, Loss function: 6.300, Average Loss: 3.475, avg. samples / sec: 28396.00
Iteration:    520, Loss function: 6.460, Average Loss: 3.487, avg. samples / sec: 28385.67
Iteration:    520, Loss function: 6.353, Average Loss: 3.475, avg. samples / sec: 28412.61
Iteration:    520, Loss function: 6.396, Average Loss: 3.474, avg. samples / sec: 28386.63
:::MLL 1558578955.953 epoch_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 819}}
:::MLL 1558578955.953 epoch_start: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 673}}
Iteration:    540, Loss function: 5.992, Average Loss: 3.528, avg. samples / sec: 28429.62
Iteration:    540, Loss function: 5.450, Average Loss: 3.525, avg. samples / sec: 28412.47
Iteration:    540, Loss function: 6.528, Average Loss: 3.541, avg. samples / sec: 28376.36
Iteration:    540, Loss function: 6.253, Average Loss: 3.530, avg. samples / sec: 28369.71
Iteration:    560, Loss function: 5.586, Average Loss: 3.578, avg. samples / sec: 28372.98
Iteration:    560, Loss function: 5.850, Average Loss: 3.576, avg. samples / sec: 28414.36
Iteration:    560, Loss function: 5.810, Average Loss: 3.592, avg. samples / sec: 28418.07
Iteration:    560, Loss function: 6.204, Average Loss: 3.585, avg. samples / sec: 28386.34
Iteration:    580, Loss function: 6.194, Average Loss: 3.621, avg. samples / sec: 28374.07
Iteration:    580, Loss function: 5.528, Average Loss: 3.632, avg. samples / sec: 28410.07
Iteration:    580, Loss function: 6.000, Average Loss: 3.628, avg. samples / sec: 28355.71
Iteration:    580, Loss function: 5.636, Average Loss: 3.641, avg. samples / sec: 28338.77
Iteration:    600, Loss function: 4.603, Average Loss: 3.683, avg. samples / sec: 28469.28
Iteration:    600, Loss function: 5.965, Average Loss: 3.669, avg. samples / sec: 28438.21
Iteration:    600, Loss function: 5.847, Average Loss: 3.678, avg. samples / sec: 28422.50
Iteration:    600, Loss function: 5.906, Average Loss: 3.663, avg. samples / sec: 28422.49
:::MLL 1558578960.068 epoch_stop: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 819}}
:::MLL 1558578960.068 epoch_start: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 673}}
Iteration:    620, Loss function: 6.160, Average Loss: 3.734, avg. samples / sec: 28226.34
Iteration:    620, Loss function: 6.440, Average Loss: 3.714, avg. samples / sec: 28221.96
Iteration:    620, Loss function: 5.578, Average Loss: 3.729, avg. samples / sec: 28215.49
Iteration:    620, Loss function: 5.860, Average Loss: 3.718, avg. samples / sec: 28166.23
Iteration:    640, Loss function: 5.884, Average Loss: 3.757, avg. samples / sec: 28396.09
Iteration:    640, Loss function: 5.939, Average Loss: 3.774, avg. samples / sec: 28346.89
Iteration:    640, Loss function: 6.285, Average Loss: 3.758, avg. samples / sec: 28319.45
Iteration:    640, Loss function: 5.708, Average Loss: 3.776, avg. samples / sec: 28261.89
Iteration:    660, Loss function: 5.502, Average Loss: 3.812, avg. samples / sec: 28520.53
Iteration:    660, Loss function: 5.744, Average Loss: 3.795, avg. samples / sec: 28466.55
Iteration:    660, Loss function: 5.286, Average Loss: 3.793, avg. samples / sec: 28447.22
Iteration:    660, Loss function: 5.175, Average Loss: 3.810, avg. samples / sec: 28402.17
Iteration:    680, Loss function: 5.111, Average Loss: 3.836, avg. samples / sec: 28350.27
Iteration:    680, Loss function: 5.795, Average Loss: 3.836, avg. samples / sec: 28334.10
Iteration:    680, Loss function: 6.112, Average Loss: 3.851, avg. samples / sec: 28323.60
Iteration:    680, Loss function: 5.720, Average Loss: 3.850, avg. samples / sec: 28354.68
:::MLL 1558578964.240 epoch_stop: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 819}}
:::MLL 1558578964.240 epoch_start: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 673}}
Iteration:    700, Loss function: 5.727, Average Loss: 3.884, avg. samples / sec: 28327.76
Iteration:    700, Loss function: 5.823, Average Loss: 3.868, avg. samples / sec: 28275.29
Iteration:    700, Loss function: 6.361, Average Loss: 3.872, avg. samples / sec: 28276.51
Iteration:    700, Loss function: 5.848, Average Loss: 3.890, avg. samples / sec: 28269.00
Iteration:    720, Loss function: 5.954, Average Loss: 3.921, avg. samples / sec: 28342.37
Iteration:    720, Loss function: 5.216, Average Loss: 3.903, avg. samples / sec: 28329.62
Iteration:    720, Loss function: 5.106, Average Loss: 3.904, avg. samples / sec: 28309.00
Iteration:    720, Loss function: 5.633, Average Loss: 3.918, avg. samples / sec: 28301.75
Iteration:    740, Loss function: 5.280, Average Loss: 3.949, avg. samples / sec: 28336.45
Iteration:    740, Loss function: 6.044, Average Loss: 3.939, avg. samples / sec: 28332.78
Iteration:    740, Loss function: 4.868, Average Loss: 3.953, avg. samples / sec: 28324.95
Iteration:    740, Loss function: 5.546, Average Loss: 3.938, avg. samples / sec: 28326.42
Iteration:    760, Loss function: 5.484, Average Loss: 3.987, avg. samples / sec: 28353.61
Iteration:    760, Loss function: 6.878, Average Loss: 3.984, avg. samples / sec: 28348.62
Iteration:    760, Loss function: 5.232, Average Loss: 3.972, avg. samples / sec: 28323.80
Iteration:    760, Loss function: 5.248, Average Loss: 3.970, avg. samples / sec: 28314.85
:::MLL 1558578968.365 epoch_stop: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 819}}
:::MLL 1558578968.365 epoch_start: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 673}}
Iteration:    780, Loss function: 5.782, Average Loss: 4.014, avg. samples / sec: 28384.74
Iteration:    780, Loss function: 5.321, Average Loss: 4.000, avg. samples / sec: 28428.67
Iteration:    780, Loss function: 5.361, Average Loss: 4.016, avg. samples / sec: 28384.99
Iteration:    780, Loss function: 6.002, Average Loss: 4.002, avg. samples / sec: 28403.39
Iteration:    800, Loss function: 6.052, Average Loss: 4.047, avg. samples / sec: 28391.90
Iteration:    800, Loss function: 5.104, Average Loss: 4.031, avg. samples / sec: 28384.15
Iteration:    800, Loss function: 5.793, Average Loss: 4.035, avg. samples / sec: 28388.68
Iteration:    800, Loss function: 5.764, Average Loss: 4.048, avg. samples / sec: 28357.40
Iteration:    820, Loss function: 5.026, Average Loss: 4.060, avg. samples / sec: 28306.35
Iteration:    820, Loss function: 5.543, Average Loss: 4.057, avg. samples / sec: 28301.86
Iteration:    820, Loss function: 5.691, Average Loss: 4.074, avg. samples / sec: 28317.97
Iteration:    820, Loss function: 4.895, Average Loss: 4.076, avg. samples / sec: 28246.47
:::MLL 1558578972.479 epoch_stop: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 819}}
:::MLL 1558578972.479 epoch_start: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 673}}
Iteration:    840, Loss function: 5.975, Average Loss: 4.085, avg. samples / sec: 28348.07
Iteration:    840, Loss function: 5.620, Average Loss: 4.088, avg. samples / sec: 28322.77
Iteration:    840, Loss function: 5.503, Average Loss: 4.103, avg. samples / sec: 28368.16
Iteration:    840, Loss function: 5.339, Average Loss: 4.102, avg. samples / sec: 28305.82
Iteration:    860, Loss function: 5.093, Average Loss: 4.108, avg. samples / sec: 28381.17
Iteration:    860, Loss function: 5.503, Average Loss: 4.127, avg. samples / sec: 28345.85
Iteration:    860, Loss function: 5.112, Average Loss: 4.123, avg. samples / sec: 28361.76
Iteration:    860, Loss function: 5.803, Average Loss: 4.109, avg. samples / sec: 28296.09
Iteration:    880, Loss function: 4.778, Average Loss: 4.145, avg. samples / sec: 28346.21
Iteration:    880, Loss function: 5.594, Average Loss: 4.128, avg. samples / sec: 28290.96
Iteration:    880, Loss function: 4.722, Average Loss: 4.134, avg. samples / sec: 28335.06
Iteration:    880, Loss function: 4.971, Average Loss: 4.146, avg. samples / sec: 28292.93
Iteration:    900, Loss function: 5.292, Average Loss: 4.152, avg. samples / sec: 28371.59
Iteration:    900, Loss function: 4.924, Average Loss: 4.150, avg. samples / sec: 28347.97
Iteration:    900, Loss function: 5.098, Average Loss: 4.162, avg. samples / sec: 28387.89
Iteration:    900, Loss function: 5.446, Average Loss: 4.165, avg. samples / sec: 28336.10
:::MLL 1558578976.657 epoch_stop: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 819}}
:::MLL 1558578976.657 epoch_start: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 673}}
Iteration:    920, Loss function: 5.210, Average Loss: 4.181, avg. samples / sec: 28224.21
Iteration:    920, Loss function: 5.379, Average Loss: 4.185, avg. samples / sec: 28228.61
Iteration:    920, Loss function: 5.098, Average Loss: 4.168, avg. samples / sec: 28220.90
Iteration:    920, Loss function: 4.907, Average Loss: 4.172, avg. samples / sec: 28187.09
Iteration:    940, Loss function: 5.431, Average Loss: 4.204, avg. samples / sec: 28336.89
Iteration:    940, Loss function: 4.758, Average Loss: 4.197, avg. samples / sec: 28335.14
Iteration:    940, Loss function: 5.551, Average Loss: 4.192, avg. samples / sec: 28346.84
Iteration:    940, Loss function: 4.799, Average Loss: 4.191, avg. samples / sec: 28287.34
Iteration:    960, Loss function: 5.100, Average Loss: 4.222, avg. samples / sec: 28363.09
Iteration:    960, Loss function: 4.892, Average Loss: 4.210, avg. samples / sec: 28357.39
Iteration:    960, Loss function: 4.960, Average Loss: 4.209, avg. samples / sec: 28385.11
Iteration:    960, Loss function: 4.615, Average Loss: 4.208, avg. samples / sec: 28388.19
Iteration:    980, Loss function: 4.508, Average Loss: 4.241, avg. samples / sec: 28347.09
Iteration:    980, Loss function: 4.693, Average Loss: 4.225, avg. samples / sec: 28352.02
Iteration:    980, Loss function: 4.544, Average Loss: 4.225, avg. samples / sec: 28327.84
Iteration:    980, Loss function: 5.873, Average Loss: 4.224, avg. samples / sec: 28326.59
:::MLL 1558578980.780 epoch_stop: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 819}}
:::MLL 1558578980.781 epoch_start: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 673}}
Iteration:   1000, Loss function: 4.751, Average Loss: 4.241, avg. samples / sec: 28282.81
Iteration:   1000, Loss function: 5.192, Average Loss: 4.238, avg. samples / sec: 28266.87
Iteration:   1000, Loss function: 5.039, Average Loss: 4.241, avg. samples / sec: 28261.02
Iteration:   1000, Loss function: 5.229, Average Loss: 4.259, avg. samples / sec: 28191.93
Iteration:   1020, Loss function: 5.036, Average Loss: 4.254, avg. samples / sec: 28283.56
Iteration:   1020, Loss function: 4.554, Average Loss: 4.253, avg. samples / sec: 28279.95
Iteration:   1020, Loss function: 5.008, Average Loss: 4.255, avg. samples / sec: 28251.11
Iteration:   1020, Loss function: 5.019, Average Loss: 4.272, avg. samples / sec: 28280.71
Iteration:   1040, Loss function: 5.332, Average Loss: 4.270, avg. samples / sec: 28401.32
Iteration:   1040, Loss function: 5.521, Average Loss: 4.266, avg. samples / sec: 28369.07
Iteration:   1040, Loss function: 4.331, Average Loss: 4.269, avg. samples / sec: 28362.22
Iteration:   1040, Loss function: 4.658, Average Loss: 4.285, avg. samples / sec: 28406.80
Iteration:   1060, Loss function: 5.115, Average Loss: 4.281, avg. samples / sec: 28537.08
Iteration:   1060, Loss function: 4.610, Average Loss: 4.284, avg. samples / sec: 28482.50
Iteration:   1060, Loss function: 4.858, Average Loss: 4.281, avg. samples / sec: 28496.92
Iteration:   1060, Loss function: 5.058, Average Loss: 4.301, avg. samples / sec: 28507.28
:::MLL 1558578984.894 epoch_stop: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 819}}
:::MLL 1558578984.895 epoch_start: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 673}}
Iteration:   1080, Loss function: 4.647, Average Loss: 4.312, avg. samples / sec: 28458.73
Iteration:   1080, Loss function: 4.308, Average Loss: 4.293, avg. samples / sec: 28443.45
Iteration:   1080, Loss function: 5.368, Average Loss: 4.292, avg. samples / sec: 28412.08
Iteration:   1080, Loss function: 4.641, Average Loss: 4.293, avg. samples / sec: 28420.37
Iteration:   1100, Loss function: 5.237, Average Loss: 4.322, avg. samples / sec: 28317.95
Iteration:   1100, Loss function: 4.748, Average Loss: 4.301, avg. samples / sec: 28313.16
Iteration:   1100, Loss function: 5.208, Average Loss: 4.302, avg. samples / sec: 28308.49
Iteration:   1100, Loss function: 4.642, Average Loss: 4.303, avg. samples / sec: 28309.73
Iteration:   1120, Loss function: 5.127, Average Loss: 4.315, avg. samples / sec: 28326.66
Iteration:   1120, Loss function: 5.013, Average Loss: 4.333, avg. samples / sec: 28308.80
Iteration:   1120, Loss function: 4.368, Average Loss: 4.311, avg. samples / sec: 28287.69
Iteration:   1120, Loss function: 4.377, Average Loss: 4.314, avg. samples / sec: 28322.48
Iteration:   1140, Loss function: 4.926, Average Loss: 4.326, avg. samples / sec: 28356.80
Iteration:   1140, Loss function: 5.416, Average Loss: 4.344, avg. samples / sec: 28321.05
Iteration:   1140, Loss function: 4.933, Average Loss: 4.328, avg. samples / sec: 28304.15
Iteration:   1140, Loss function: 4.655, Average Loss: 4.321, avg. samples / sec: 28298.49
:::MLL 1558578989.066 epoch_stop: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 819}}
:::MLL 1558578989.067 epoch_start: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 673}}
Iteration:   1160, Loss function: 4.566, Average Loss: 4.334, avg. samples / sec: 28363.74
Iteration:   1160, Loss function: 4.584, Average Loss: 4.330, avg. samples / sec: 28409.76
Iteration:   1160, Loss function: 5.464, Average Loss: 4.354, avg. samples / sec: 28350.39
Iteration:   1160, Loss function: 5.230, Average Loss: 4.339, avg. samples / sec: 28335.92
Iteration:   1180, Loss function: 3.871, Average Loss: 4.338, avg. samples / sec: 28374.65
Iteration:   1180, Loss function: 4.879, Average Loss: 4.347, avg. samples / sec: 28419.35
Iteration:   1180, Loss function: 4.691, Average Loss: 4.335, avg. samples / sec: 28377.57
Iteration:   1180, Loss function: 4.468, Average Loss: 4.361, avg. samples / sec: 28379.39
Iteration:   1200, Loss function: 4.741, Average Loss: 4.347, avg. samples / sec: 28446.39
Iteration:   1200, Loss function: 4.582, Average Loss: 4.371, avg. samples / sec: 28456.49
Iteration:   1200, Loss function: 4.437, Average Loss: 4.358, avg. samples / sec: 28434.27
Iteration:   1200, Loss function: 5.255, Average Loss: 4.346, avg. samples / sec: 28434.00
Iteration:   1220, Loss function: 4.344, Average Loss: 4.356, avg. samples / sec: 28284.57
Iteration:   1220, Loss function: 4.370, Average Loss: 4.366, avg. samples / sec: 28291.78
Iteration:   1220, Loss function: 4.529, Average Loss: 4.380, avg. samples / sec: 28281.76
Iteration:   1220, Loss function: 4.966, Average Loss: 4.351, avg. samples / sec: 28237.87
:::MLL 1558578993.184 epoch_stop: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 819}}
:::MLL 1558578993.184 epoch_start: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 673}}
Iteration:   1240, Loss function: 4.823, Average Loss: 4.362, avg. samples / sec: 28264.07
Iteration:   1240, Loss function: 5.145, Average Loss: 4.357, avg. samples / sec: 28330.53
Iteration:   1240, Loss function: 4.479, Average Loss: 4.373, avg. samples / sec: 28260.89
Iteration:   1240, Loss function: 4.708, Average Loss: 4.386, avg. samples / sec: 28253.34
Iteration:   1260, Loss function: 4.666, Average Loss: 4.391, avg. samples / sec: 28371.09
Iteration:   1260, Loss function: 4.648, Average Loss: 4.363, avg. samples / sec: 28354.98
Iteration:   1260, Loss function: 5.671, Average Loss: 4.369, avg. samples / sec: 28343.02
Iteration:   1260, Loss function: 4.982, Average Loss: 4.379, avg. samples / sec: 28337.24
Iteration:   1280, Loss function: 4.408, Average Loss: 4.394, avg. samples / sec: 28378.83
Iteration:   1280, Loss function: 4.377, Average Loss: 4.376, avg. samples / sec: 28386.91
Iteration:   1280, Loss function: 4.921, Average Loss: 4.369, avg. samples / sec: 28375.43
Iteration:   1280, Loss function: 4.580, Average Loss: 4.384, avg. samples / sec: 28388.06
:::MLL 1558578997.301 epoch_stop: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 819}}
:::MLL 1558578997.301 epoch_start: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 673}}
Iteration:   1300, Loss function: 4.122, Average Loss: 4.389, avg. samples / sec: 28394.46
Iteration:   1300, Loss function: 4.371, Average Loss: 4.403, avg. samples / sec: 28365.18
Iteration:   1300, Loss function: 4.336, Average Loss: 4.374, avg. samples / sec: 28369.39
Iteration:   1300, Loss function: 4.582, Average Loss: 4.381, avg. samples / sec: 28314.73
Iteration:   1320, Loss function: 4.924, Average Loss: 4.405, avg. samples / sec: 28395.78
Iteration:   1320, Loss function: 4.835, Average Loss: 4.386, avg. samples / sec: 28450.31
Iteration:   1320, Loss function: 4.517, Average Loss: 4.377, avg. samples / sec: 28390.06
Iteration:   1320, Loss function: 4.386, Average Loss: 4.395, avg. samples / sec: 28373.24
Iteration:   1340, Loss function: 4.370, Average Loss: 4.383, avg. samples / sec: 28265.35
Iteration:   1340, Loss function: 4.584, Average Loss: 4.398, avg. samples / sec: 28266.03
Iteration:   1340, Loss function: 4.901, Average Loss: 4.411, avg. samples / sec: 28249.45
Iteration:   1340, Loss function: 4.533, Average Loss: 4.389, avg. samples / sec: 28247.15
Iteration:   1360, Loss function: 4.912, Average Loss: 4.415, avg. samples / sec: 28337.38
Iteration:   1360, Loss function: 4.574, Average Loss: 4.385, avg. samples / sec: 28325.28
Iteration:   1360, Loss function: 4.172, Average Loss: 4.401, avg. samples / sec: 28311.45
Iteration:   1360, Loss function: 4.687, Average Loss: 4.394, avg. samples / sec: 28297.35
:::MLL 1558579001.476 epoch_stop: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 819}}
:::MLL 1558579001.477 epoch_start: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 673}}
Iteration:   1380, Loss function: 4.591, Average Loss: 4.419, avg. samples / sec: 28349.43
Iteration:   1380, Loss function: 4.978, Average Loss: 4.387, avg. samples / sec: 28350.87
Iteration:   1380, Loss function: 5.141, Average Loss: 4.400, avg. samples / sec: 28383.68
Iteration:   1380, Loss function: 4.609, Average Loss: 4.405, avg. samples / sec: 28304.75
Iteration:   1400, Loss function: 4.710, Average Loss: 4.422, avg. samples / sec: 28318.95
Iteration:   1400, Loss function: 4.508, Average Loss: 4.402, avg. samples / sec: 28316.05
Iteration:   1400, Loss function: 4.059, Average Loss: 4.409, avg. samples / sec: 28371.38
Iteration:   1400, Loss function: 4.277, Average Loss: 4.391, avg. samples / sec: 28268.32
Iteration:   1420, Loss function: 4.537, Average Loss: 4.411, avg. samples / sec: 28456.21
Iteration:   1420, Loss function: 3.770, Average Loss: 4.422, avg. samples / sec: 28433.67
Iteration:   1420, Loss function: 4.439, Average Loss: 4.401, avg. samples / sec: 28435.43
Iteration:   1420, Loss function: 4.296, Average Loss: 4.393, avg. samples / sec: 28451.23
Iteration:   1440, Loss function: 4.393, Average Loss: 4.421, avg. samples / sec: 28298.49
Iteration:   1440, Loss function: 4.819, Average Loss: 4.395, avg. samples / sec: 28327.04
Iteration:   1440, Loss function: 4.453, Average Loss: 4.411, avg. samples / sec: 28285.43
Iteration:   1440, Loss function: 4.364, Average Loss: 4.404, avg. samples / sec: 28296.80
:::MLL 1558579005.598 epoch_stop: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 819}}
:::MLL 1558579005.598 epoch_start: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 673}}
Iteration:   1460, Loss function: 4.511, Average Loss: 4.424, avg. samples / sec: 28245.73
Iteration:   1460, Loss function: 4.713, Average Loss: 4.412, avg. samples / sec: 28249.67
Iteration:   1460, Loss function: 4.496, Average Loss: 4.408, avg. samples / sec: 28253.63
Iteration:   1460, Loss function: 3.749, Average Loss: 4.397, avg. samples / sec: 28236.96
Iteration:   1480, Loss function: 5.305, Average Loss: 4.413, avg. samples / sec: 28291.51
Iteration:   1480, Loss function: 4.577, Average Loss: 4.415, avg. samples / sec: 28289.59
Iteration:   1480, Loss function: 4.728, Average Loss: 4.400, avg. samples / sec: 28300.48
Iteration:   1480, Loss function: 4.361, Average Loss: 4.426, avg. samples / sec: 28279.17
Iteration:   1500, Loss function: 4.949, Average Loss: 4.402, avg. samples / sec: 28372.41
Iteration:   1500, Loss function: 3.831, Average Loss: 4.426, avg. samples / sec: 28374.93
Iteration:   1500, Loss function: 4.541, Average Loss: 4.415, avg. samples / sec: 28362.96
Iteration:   1500, Loss function: 4.745, Average Loss: 4.416, avg. samples / sec: 28366.80
Iteration:   1520, Loss function: 5.002, Average Loss: 4.417, avg. samples / sec: 28409.37
Iteration:   1520, Loss function: 4.777, Average Loss: 4.403, avg. samples / sec: 28402.06
Iteration:   1520, Loss function: 4.829, Average Loss: 4.412, avg. samples / sec: 28407.44
Iteration:   1520, Loss function: 4.717, Average Loss: 4.426, avg. samples / sec: 28393.54
:::MLL 1558579009.719 epoch_stop: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 819}}
:::MLL 1558579009.719 epoch_start: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 673}}
Iteration:   1540, Loss function: 4.750, Average Loss: 4.413, avg. samples / sec: 28247.73
Iteration:   1540, Loss function: 3.973, Average Loss: 4.402, avg. samples / sec: 28240.27
Iteration:   1540, Loss function: 4.927, Average Loss: 4.417, avg. samples / sec: 28224.29
Iteration:   1540, Loss function: 4.203, Average Loss: 4.426, avg. samples / sec: 28215.97
Iteration:   1560, Loss function: 4.727, Average Loss: 4.419, avg. samples / sec: 28506.34
Iteration:   1560, Loss function: 4.365, Average Loss: 4.412, avg. samples / sec: 28484.14
Iteration:   1560, Loss function: 4.923, Average Loss: 4.426, avg. samples / sec: 28523.95
Iteration:   1560, Loss function: 4.573, Average Loss: 4.401, avg. samples / sec: 28421.06
Iteration:   1580, Loss function: 4.256, Average Loss: 4.402, avg. samples / sec: 28425.63
Iteration:   1580, Loss function: 4.211, Average Loss: 4.413, avg. samples / sec: 28339.05
Iteration:   1580, Loss function: 4.786, Average Loss: 4.427, avg. samples / sec: 28342.01
Iteration:   1580, Loss function: 4.118, Average Loss: 4.424, avg. samples / sec: 28308.44
Iteration:   1600, Loss function: 4.129, Average Loss: 4.424, avg. samples / sec: 28341.51
Iteration:   1600, Loss function: 4.077, Average Loss: 4.423, avg. samples / sec: 28358.29
Iteration:   1600, Loss function: 4.643, Average Loss: 4.413, avg. samples / sec: 28318.88
Iteration:   1600, Loss function: 4.188, Average Loss: 4.401, avg. samples / sec: 28258.61
:::MLL 1558579013.890 epoch_stop: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 819}}
:::MLL 1558579013.891 epoch_start: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 673}}
Iteration:   1620, Loss function: 4.394, Average Loss: 4.408, avg. samples / sec: 28321.48
Iteration:   1620, Loss function: 3.787, Average Loss: 4.420, avg. samples / sec: 28299.76
Iteration:   1620, Loss function: 4.280, Average Loss: 4.420, avg. samples / sec: 28294.11
Iteration:   1620, Loss function: 3.902, Average Loss: 4.402, avg. samples / sec: 28332.40
Iteration:   1640, Loss function: 4.799, Average Loss: 4.420, avg. samples / sec: 28423.16
Iteration:   1640, Loss function: 4.339, Average Loss: 4.402, avg. samples / sec: 28428.56
Iteration:   1640, Loss function: 4.728, Average Loss: 4.421, avg. samples / sec: 28387.57
Iteration:   1640, Loss function: 3.888, Average Loss: 4.409, avg. samples / sec: 28346.54
Iteration:   1660, Loss function: 4.672, Average Loss: 4.420, avg. samples / sec: 28408.12
Iteration:   1660, Loss function: 4.368, Average Loss: 4.421, avg. samples / sec: 28448.18
Iteration:   1660, Loss function: 4.483, Average Loss: 4.410, avg. samples / sec: 28469.67
Iteration:   1660, Loss function: 4.820, Average Loss: 4.404, avg. samples / sec: 28408.18
:::MLL 1558579018.005 epoch_stop: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 819}}
:::MLL 1558579018.005 epoch_start: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 673}}
Iteration:   1680, Loss function: 4.294, Average Loss: 4.407, avg. samples / sec: 28276.13
Iteration:   1680, Loss function: 4.233, Average Loss: 4.419, avg. samples / sec: 28244.22
Iteration:   1680, Loss function: 4.301, Average Loss: 4.406, avg. samples / sec: 28273.10
Iteration:   1680, Loss function: 3.974, Average Loss: 4.419, avg. samples / sec: 28240.53
Iteration:   1700, Loss function: 3.836, Average Loss: 4.404, avg. samples / sec: 28386.54
Iteration:   1700, Loss function: 4.196, Average Loss: 4.420, avg. samples / sec: 28413.62
Iteration:   1700, Loss function: 3.864, Average Loss: 4.404, avg. samples / sec: 28393.31
Iteration:   1700, Loss function: 4.303, Average Loss: 4.418, avg. samples / sec: 28346.94
Iteration:   1720, Loss function: 5.152, Average Loss: 4.404, avg. samples / sec: 28347.02
Iteration:   1720, Loss function: 4.583, Average Loss: 4.403, avg. samples / sec: 28352.16
Iteration:   1720, Loss function: 4.061, Average Loss: 4.417, avg. samples / sec: 28310.45
Iteration:   1720, Loss function: 4.708, Average Loss: 4.417, avg. samples / sec: 28319.23
Iteration:   1740, Loss function: 4.120, Average Loss: 4.403, avg. samples / sec: 28268.14
Iteration:   1740, Loss function: 4.327, Average Loss: 4.403, avg. samples / sec: 28270.78
Iteration:   1740, Loss function: 3.376, Average Loss: 4.412, avg. samples / sec: 28344.17
Iteration:   1740, Loss function: 4.169, Average Loss: 4.414, avg. samples / sec: 28274.18
:::MLL 1558579022.126 epoch_stop: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 819}}
:::MLL 1558579022.127 epoch_start: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 673}}
Iteration:   1760, Loss function: 4.996, Average Loss: 4.403, avg. samples / sec: 28312.78
Iteration:   1760, Loss function: 4.260, Average Loss: 4.404, avg. samples / sec: 28288.54
Iteration:   1760, Loss function: 3.624, Average Loss: 4.409, avg. samples / sec: 28284.76
Iteration:   1760, Loss function: 4.522, Average Loss: 4.409, avg. samples / sec: 28297.10
Iteration:   1780, Loss function: 4.640, Average Loss: 4.407, avg. samples / sec: 28521.84
Iteration:   1780, Loss function: 3.943, Average Loss: 4.402, avg. samples / sec: 28486.45
Iteration:   1780, Loss function: 4.235, Average Loss: 4.402, avg. samples / sec: 28511.47
Iteration:   1780, Loss function: 3.879, Average Loss: 4.409, avg. samples / sec: 28516.54
Iteration:   1800, Loss function: 4.817, Average Loss: 4.406, avg. samples / sec: 28372.88
Iteration:   1800, Loss function: 4.229, Average Loss: 4.400, avg. samples / sec: 28378.23
Iteration:   1800, Loss function: 5.430, Average Loss: 4.400, avg. samples / sec: 28367.95
Iteration:   1800, Loss function: 4.507, Average Loss: 4.409, avg. samples / sec: 28387.37
Iteration:   1820, Loss function: 4.492, Average Loss: 4.398, avg. samples / sec: 28425.66
Iteration:   1820, Loss function: 4.092, Average Loss: 4.406, avg. samples / sec: 28390.14
Iteration:   1820, Loss function: 5.161, Average Loss: 4.402, avg. samples / sec: 28377.32
Iteration:   1820, Loss function: 3.697, Average Loss: 4.403, avg. samples / sec: 28364.63
:::MLL 1558579026.291 epoch_stop: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 819}}
:::MLL 1558579026.291 epoch_start: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 673}}
Iteration:   1840, Loss function: 3.994, Average Loss: 4.393, avg. samples / sec: 28312.32
Iteration:   1840, Loss function: 4.834, Average Loss: 4.403, avg. samples / sec: 28343.64
Iteration:   1840, Loss function: 4.006, Average Loss: 4.396, avg. samples / sec: 28356.63
Iteration:   1840, Loss function: 4.341, Average Loss: 4.398, avg. samples / sec: 28294.65
Iteration:   1860, Loss function: 3.696, Average Loss: 4.394, avg. samples / sec: 28366.86
Iteration:   1860, Loss function: 3.894, Average Loss: 4.397, avg. samples / sec: 28413.94
Iteration:   1860, Loss function: 4.239, Average Loss: 4.391, avg. samples / sec: 28351.52
Iteration:   1860, Loss function: 4.366, Average Loss: 4.400, avg. samples / sec: 28341.51
Iteration:   1880, Loss function: 3.864, Average Loss: 4.390, avg. samples / sec: 28367.96
Iteration:   1880, Loss function: 4.416, Average Loss: 4.395, avg. samples / sec: 28372.61
Iteration:   1880, Loss function: 3.790, Average Loss: 4.398, avg. samples / sec: 28391.19
Iteration:   1880, Loss function: 4.016, Average Loss: 4.384, avg. samples / sec: 28336.45
Iteration:   1900, Loss function: 4.339, Average Loss: 4.387, avg. samples / sec: 28318.81
Iteration:   1900, Loss function: 4.293, Average Loss: 4.394, avg. samples / sec: 28314.20
Iteration:   1900, Loss function: 4.381, Average Loss: 4.383, avg. samples / sec: 28338.25
Iteration:   1900, Loss function: 4.534, Average Loss: 4.393, avg. samples / sec: 28269.90
:::MLL 1558579030.415 epoch_stop: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 819}}
:::MLL 1558579030.415 epoch_start: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 673}}
Iteration:   1920, Loss function: 4.278, Average Loss: 4.384, avg. samples / sec: 28053.18
Iteration:   1920, Loss function: 4.442, Average Loss: 4.380, avg. samples / sec: 28069.14
Iteration:   1920, Loss function: 3.537, Average Loss: 4.390, avg. samples / sec: 28043.74
Iteration:   1920, Loss function: 4.126, Average Loss: 4.393, avg. samples / sec: 28053.16
Iteration:   1940, Loss function: 4.679, Average Loss: 4.384, avg. samples / sec: 28361.24
Iteration:   1940, Loss function: 5.232, Average Loss: 4.374, avg. samples / sec: 28362.41
Iteration:   1940, Loss function: 4.372, Average Loss: 4.391, avg. samples / sec: 28399.05
Iteration:   1940, Loss function: 3.939, Average Loss: 4.385, avg. samples / sec: 28336.65
Iteration:   1960, Loss function: 4.153, Average Loss: 4.382, avg. samples / sec: 28377.38
Iteration:   1960, Loss function: 4.081, Average Loss: 4.371, avg. samples / sec: 28371.28
Iteration:   1960, Loss function: 4.933, Average Loss: 4.384, avg. samples / sec: 28406.91
Iteration:   1960, Loss function: 4.077, Average Loss: 4.388, avg. samples / sec: 28350.77
Iteration:   1980, Loss function: 3.713, Average Loss: 4.382, avg. samples / sec: 28410.55
Iteration:   1980, Loss function: 4.280, Average Loss: 4.377, avg. samples / sec: 28368.61
Iteration:   1980, Loss function: 4.166, Average Loss: 4.368, avg. samples / sec: 28378.90
Iteration:   1980, Loss function: 4.519, Average Loss: 4.383, avg. samples / sec: 28333.82
:::MLL 1558579034.538 epoch_stop: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 819}}
:::MLL 1558579034.539 epoch_start: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 673}}
Iteration:   2000, Loss function: 3.977, Average Loss: 4.375, avg. samples / sec: 28314.07
Iteration:   2000, Loss function: 4.076, Average Loss: 4.367, avg. samples / sec: 28310.97
Iteration:   2000, Loss function: 4.401, Average Loss: 4.380, avg. samples / sec: 28281.87
Iteration:   2000, Loss function: 4.248, Average Loss: 4.380, avg. samples / sec: 28279.52
Iteration:   2020, Loss function: 4.127, Average Loss: 4.362, avg. samples / sec: 28280.58
Iteration:   2020, Loss function: 3.962, Average Loss: 4.374, avg. samples / sec: 28293.05
Iteration:   2020, Loss function: 4.261, Average Loss: 4.376, avg. samples / sec: 28314.76
Iteration:   2020, Loss function: 4.090, Average Loss: 4.370, avg. samples / sec: 28231.72
Iteration:   2040, Loss function: 4.074, Average Loss: 4.370, avg. samples / sec: 28367.64
Iteration:   2040, Loss function: 4.498, Average Loss: 4.359, avg. samples / sec: 28319.86
Iteration:   2040, Loss function: 3.988, Average Loss: 4.371, avg. samples / sec: 28345.42
Iteration:   2040, Loss function: 4.139, Average Loss: 4.366, avg. samples / sec: 28350.05
Iteration:   2060, Loss function: 4.374, Average Loss: 4.369, avg. samples / sec: 28378.60
Iteration:   2060, Loss function: 4.270, Average Loss: 4.360, avg. samples / sec: 28392.34
Iteration:   2060, Loss function: 4.103, Average Loss: 4.368, avg. samples / sec: 28386.16
Iteration:   2060, Loss function: 3.723, Average Loss: 4.354, avg. samples / sec: 28363.79
:::MLL 1558579038.712 epoch_stop: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 819}}
:::MLL 1558579038.712 epoch_start: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 673}}
Iteration:   2080, Loss function: 3.953, Average Loss: 4.360, avg. samples / sec: 28429.48
Iteration:   2080, Loss function: 4.173, Average Loss: 4.368, avg. samples / sec: 28429.53
Iteration:   2080, Loss function: 4.207, Average Loss: 4.351, avg. samples / sec: 28429.29
Iteration:   2080, Loss function: 4.531, Average Loss: 4.367, avg. samples / sec: 28333.18
Iteration:   2100, Loss function: 5.034, Average Loss: 4.365, avg. samples / sec: 28338.78
Iteration:   2100, Loss function: 3.665, Average Loss: 4.357, avg. samples / sec: 28278.32
Iteration:   2100, Loss function: 4.060, Average Loss: 4.348, avg. samples / sec: 28276.60
Iteration:   2100, Loss function: 3.963, Average Loss: 4.366, avg. samples / sec: 28245.88
Iteration:   2120, Loss function: 3.678, Average Loss: 4.354, avg. samples / sec: 28389.47
Iteration:   2120, Loss function: 4.422, Average Loss: 4.361, avg. samples / sec: 28384.63
Iteration:   2120, Loss function: 3.342, Average Loss: 4.345, avg. samples / sec: 28399.66
Iteration:   2120, Loss function: 4.447, Average Loss: 4.361, avg. samples / sec: 28422.26
:::MLL 1558579042.829 epoch_stop: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 819}}
:::MLL 1558579042.830 epoch_start: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 673}}
Iteration:   2140, Loss function: 3.792, Average Loss: 4.348, avg. samples / sec: 28321.64
Iteration:   2140, Loss function: 3.640, Average Loss: 4.338, avg. samples / sec: 28319.61
Iteration:   2140, Loss function: 3.758, Average Loss: 4.355, avg. samples / sec: 28314.89
Iteration:   2140, Loss function: 4.187, Average Loss: 4.357, avg. samples / sec: 28320.69
Iteration:   2160, Loss function: 3.937, Average Loss: 4.341, avg. samples / sec: 28332.82
Iteration:   2160, Loss function: 3.883, Average Loss: 4.351, avg. samples / sec: 28337.16
Iteration:   2160, Loss function: 3.844, Average Loss: 4.331, avg. samples / sec: 28336.33
Iteration:   2160, Loss function: 4.115, Average Loss: 4.352, avg. samples / sec: 28299.77
Iteration:   2180, Loss function: 4.436, Average Loss: 4.324, avg. samples / sec: 28461.63
Iteration:   2180, Loss function: 4.800, Average Loss: 4.348, avg. samples / sec: 28418.62
Iteration:   2180, Loss function: 3.955, Average Loss: 4.338, avg. samples / sec: 28410.13
Iteration:   2180, Loss function: 4.420, Average Loss: 4.344, avg. samples / sec: 28433.71
Iteration:   2200, Loss function: 3.932, Average Loss: 4.332, avg. samples / sec: 28398.41
Iteration:   2200, Loss function: 4.487, Average Loss: 4.321, avg. samples / sec: 28332.69
Iteration:   2200, Loss function: 4.552, Average Loss: 4.342, avg. samples / sec: 28397.62
Iteration:   2200, Loss function: 4.438, Average Loss: 4.345, avg. samples / sec: 28352.05
:::MLL 1558579047.004 epoch_stop: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 819}}
:::MLL 1558579047.005 epoch_start: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 673}}
Iteration:   2220, Loss function: 4.343, Average Loss: 4.328, avg. samples / sec: 28243.50
Iteration:   2220, Loss function: 4.324, Average Loss: 4.339, avg. samples / sec: 28273.01
Iteration:   2220, Loss function: 4.014, Average Loss: 4.338, avg. samples / sec: 28241.00
Iteration:   2220, Loss function: 3.960, Average Loss: 4.316, avg. samples / sec: 28212.25
Iteration:   2240, Loss function: 3.433, Average Loss: 4.334, avg. samples / sec: 28359.25
Iteration:   2240, Loss function: 3.592, Average Loss: 4.323, avg. samples / sec: 28335.18
Iteration:   2240, Loss function: 4.319, Average Loss: 4.311, avg. samples / sec: 28379.03
Iteration:   2240, Loss function: 4.965, Average Loss: 4.334, avg. samples / sec: 28329.46
Iteration:   2260, Loss function: 4.493, Average Loss: 4.308, avg. samples / sec: 28305.83
Iteration:   2260, Loss function: 4.089, Average Loss: 4.318, avg. samples / sec: 28303.34
Iteration:   2260, Loss function: 4.108, Average Loss: 4.329, avg. samples / sec: 28296.62
Iteration:   2260, Loss function: 5.043, Average Loss: 4.332, avg. samples / sec: 28315.33
Iteration:   2280, Loss function: 3.986, Average Loss: 4.313, avg. samples / sec: 28314.88
Iteration:   2280, Loss function: 3.909, Average Loss: 4.305, avg. samples / sec: 28312.03
Iteration:   2280, Loss function: 4.020, Average Loss: 4.327, avg. samples / sec: 28309.72
Iteration:   2280, Loss function: 4.794, Average Loss: 4.329, avg. samples / sec: 28282.64
:::MLL 1558579051.127 epoch_stop: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 819}}
:::MLL 1558579051.127 epoch_start: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 673}}
Iteration:   2300, Loss function: 3.678, Average Loss: 4.299, avg. samples / sec: 28251.57
Iteration:   2300, Loss function: 4.195, Average Loss: 4.309, avg. samples / sec: 28242.58
Iteration:   2300, Loss function: 4.481, Average Loss: 4.327, avg. samples / sec: 28278.35
Iteration:   2300, Loss function: 3.742, Average Loss: 4.323, avg. samples / sec: 28199.22
Iteration:   2320, Loss function: 4.305, Average Loss: 4.323, avg. samples / sec: 28303.41
Iteration:   2320, Loss function: 4.072, Average Loss: 4.297, avg. samples / sec: 28289.54
Iteration:   2320, Loss function: 4.101, Average Loss: 4.319, avg. samples / sec: 28342.99
Iteration:   2320, Loss function: 3.829, Average Loss: 4.305, avg. samples / sec: 28259.44
Iteration:   2340, Loss function: 4.267, Average Loss: 4.299, avg. samples / sec: 28394.32
Iteration:   2340, Loss function: 3.877, Average Loss: 4.293, avg. samples / sec: 28363.09
Iteration:   2340, Loss function: 3.525, Average Loss: 4.314, avg. samples / sec: 28350.26
Iteration:   2340, Loss function: 3.588, Average Loss: 4.319, avg. samples / sec: 28332.11
Iteration:   2360, Loss function: 3.514, Average Loss: 4.309, avg. samples / sec: 28380.46
Iteration:   2360, Loss function: 3.596, Average Loss: 4.295, avg. samples / sec: 28363.91
Iteration:   2360, Loss function: 4.039, Average Loss: 4.314, avg. samples / sec: 28378.49
Iteration:   2360, Loss function: 4.061, Average Loss: 4.290, avg. samples / sec: 28343.90
:::MLL 1558579055.249 epoch_stop: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 819}}
:::MLL 1558579055.250 epoch_start: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 673}}
Iteration:   2380, Loss function: 4.744, Average Loss: 4.307, avg. samples / sec: 28290.77
Iteration:   2380, Loss function: 4.510, Average Loss: 4.304, avg. samples / sec: 28274.97
Iteration:   2380, Loss function: 4.755, Average Loss: 4.290, avg. samples / sec: 28255.02
Iteration:   2380, Loss function: 4.118, Average Loss: 4.286, avg. samples / sec: 28251.09
Iteration:   2400, Loss function: 4.130, Average Loss: 4.286, avg. samples / sec: 28384.84
Iteration:   2400, Loss function: 4.596, Average Loss: 4.301, avg. samples / sec: 28352.16
Iteration:   2400, Loss function: 4.396, Average Loss: 4.283, avg. samples / sec: 28393.42
Iteration:   2400, Loss function: 4.149, Average Loss: 4.302, avg. samples / sec: 28272.90
Iteration:   2420, Loss function: 3.923, Average Loss: 4.296, avg. samples / sec: 28419.40
Iteration:   2420, Loss function: 4.213, Average Loss: 4.297, avg. samples / sec: 28338.91
Iteration:   2420, Loss function: 4.481, Average Loss: 4.281, avg. samples / sec: 28343.67
Iteration:   2420, Loss function: 4.916, Average Loss: 4.284, avg. samples / sec: 28301.70
Iteration:   2440, Loss function: 3.641, Average Loss: 4.292, avg. samples / sec: 28343.63
Iteration:   2440, Loss function: 4.537, Average Loss: 4.294, avg. samples / sec: 28329.91
Iteration:   2440, Loss function: 4.064, Average Loss: 4.282, avg. samples / sec: 28297.15
Iteration:   2440, Loss function: 3.874, Average Loss: 4.277, avg. samples / sec: 28254.33
:::MLL 1558579059.425 epoch_stop: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 819}}
:::MLL 1558579059.425 epoch_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 673}}
Iteration:   2460, Loss function: 4.431, Average Loss: 4.292, avg. samples / sec: 28286.03
Iteration:   2460, Loss function: 3.767, Average Loss: 4.279, avg. samples / sec: 28349.33
Iteration:   2460, Loss function: 4.200, Average Loss: 4.287, avg. samples / sec: 28269.04
Iteration:   2460, Loss function: 4.124, Average Loss: 4.270, avg. samples / sec: 28353.45
Iteration:   2480, Loss function: 4.254, Average Loss: 4.267, avg. samples / sec: 28448.10
Iteration:   2480, Loss function: 4.353, Average Loss: 4.288, avg. samples / sec: 28436.84
Iteration:   2480, Loss function: 3.929, Average Loss: 4.289, avg. samples / sec: 28424.98
Iteration:   2480, Loss function: 4.051, Average Loss: 4.273, avg. samples / sec: 28424.66
Iteration:   2500, Loss function: 4.394, Average Loss: 4.270, avg. samples / sec: 28184.15
Iteration:   2500, Loss function: 3.998, Average Loss: 4.263, avg. samples / sec: 28172.67
Iteration:   2500, Loss function: 4.357, Average Loss: 4.286, avg. samples / sec: 28180.25
Iteration:   2500, Loss function: 3.894, Average Loss: 4.280, avg. samples / sec: 28140.82
:::MLL 1558579062.520 eval_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.33 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.33 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.33 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 2.33 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.46s)
DONE (t=2.88s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.15676
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.29976
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.15032
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.04141
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.16977
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.24975
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.17233
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.25082
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.26468
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.07286
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.28376
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.40893
Current AP: 0.15676 AP goal: 0.23000
:::MLL 1558579068.196 eval_accuracy: {"value": 0.15675904866605156, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 389}}
:::MLL 1558579068.266 eval_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 392}}
:::MLL 1558579068.280 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 804}}
:::MLL 1558579068.281 block_start: {"value": null, "metadata": {"first_epoch_num": 33, "epoch_count": 10.915354834308324, "file": "train.py", "lineno": 813}}
:::MLL 1558579069.935 epoch_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 819}}
:::MLL 1558579069.936 epoch_start: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 673}}
Iteration:   2520, Loss function: 4.296, Average Loss: 4.268, avg. samples / sec: 4111.71
Iteration:   2520, Loss function: 3.821, Average Loss: 4.281, avg. samples / sec: 4111.58
Iteration:   2520, Loss function: 4.687, Average Loss: 4.257, avg. samples / sec: 4111.30
Iteration:   2520, Loss function: 3.439, Average Loss: 4.275, avg. samples / sec: 4111.70
Iteration:   2540, Loss function: 4.186, Average Loss: 4.277, avg. samples / sec: 27864.16
Iteration:   2540, Loss function: 3.688, Average Loss: 4.272, avg. samples / sec: 27889.66
Iteration:   2540, Loss function: 3.986, Average Loss: 4.252, avg. samples / sec: 27867.93
Iteration:   2540, Loss function: 4.293, Average Loss: 4.265, avg. samples / sec: 27843.26
Iteration:   2560, Loss function: 3.591, Average Loss: 4.260, avg. samples / sec: 27783.77
Iteration:   2560, Loss function: 4.673, Average Loss: 4.275, avg. samples / sec: 27766.05
Iteration:   2560, Loss function: 4.747, Average Loss: 4.248, avg. samples / sec: 27775.18
Iteration:   2560, Loss function: 4.125, Average Loss: 4.269, avg. samples / sec: 27759.22
Iteration:   2580, Loss function: 4.282, Average Loss: 4.270, avg. samples / sec: 28089.03
Iteration:   2580, Loss function: 4.213, Average Loss: 4.264, avg. samples / sec: 28067.25
Iteration:   2580, Loss function: 4.464, Average Loss: 4.245, avg. samples / sec: 28045.54
Iteration:   2580, Loss function: 4.237, Average Loss: 4.259, avg. samples / sec: 28031.89
:::MLL 1558579074.115 epoch_stop: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 819}}
:::MLL 1558579074.116 epoch_start: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 673}}
Iteration:   2600, Loss function: 4.079, Average Loss: 4.259, avg. samples / sec: 28089.30
Iteration:   2600, Loss function: 3.869, Average Loss: 4.237, avg. samples / sec: 28090.62
Iteration:   2600, Loss function: 4.206, Average Loss: 4.266, avg. samples / sec: 27974.78
Iteration:   2600, Loss function: 3.903, Average Loss: 4.254, avg. samples / sec: 28014.16
Iteration:   2620, Loss function: 3.824, Average Loss: 4.264, avg. samples / sec: 28168.02
Iteration:   2620, Loss function: 4.362, Average Loss: 4.233, avg. samples / sec: 28089.88
Iteration:   2620, Loss function: 3.901, Average Loss: 4.252, avg. samples / sec: 28162.85
Iteration:   2620, Loss function: 4.072, Average Loss: 4.257, avg. samples / sec: 27960.31
Iteration:   2640, Loss function: 3.742, Average Loss: 4.261, avg. samples / sec: 27982.84
Iteration:   2640, Loss function: 4.000, Average Loss: 4.230, avg. samples / sec: 27969.68
Iteration:   2640, Loss function: 3.443, Average Loss: 4.245, avg. samples / sec: 27967.28
Iteration:   2640, Loss function: 4.188, Average Loss: 4.253, avg. samples / sec: 28059.44
Iteration:   2660, Loss function: 4.500, Average Loss: 4.243, avg. samples / sec: 28039.13
Iteration:   2660, Loss function: 3.945, Average Loss: 4.248, avg. samples / sec: 28042.81
Iteration:   2660, Loss function: 3.782, Average Loss: 4.258, avg. samples / sec: 27961.35
Iteration:   2660, Loss function: 3.827, Average Loss: 4.225, avg. samples / sec: 27955.54
:::MLL 1558579078.335 epoch_stop: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 819}}
:::MLL 1558579078.336 epoch_start: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 673}}
Iteration:   2680, Loss function: 3.964, Average Loss: 4.255, avg. samples / sec: 28037.54
Iteration:   2680, Loss function: 3.998, Average Loss: 4.217, avg. samples / sec: 28046.01
Iteration:   2680, Loss function: 4.292, Average Loss: 4.239, avg. samples / sec: 27957.73
Iteration:   2680, Loss function: 4.357, Average Loss: 4.243, avg. samples / sec: 27956.82
Iteration:   2700, Loss function: 3.837, Average Loss: 4.233, avg. samples / sec: 28128.38
Iteration:   2700, Loss function: 4.395, Average Loss: 4.237, avg. samples / sec: 28135.73
Iteration:   2700, Loss function: 3.642, Average Loss: 4.213, avg. samples / sec: 28105.41
Iteration:   2700, Loss function: 3.778, Average Loss: 4.250, avg. samples / sec: 28079.55
Iteration:   2720, Loss function: 3.857, Average Loss: 4.229, avg. samples / sec: 27934.89
Iteration:   2720, Loss function: 3.918, Average Loss: 4.234, avg. samples / sec: 27935.86
Iteration:   2720, Loss function: 3.876, Average Loss: 4.245, avg. samples / sec: 27942.63
Iteration:   2720, Loss function: 3.808, Average Loss: 4.210, avg. samples / sec: 27933.89
Iteration:   2740, Loss function: 3.519, Average Loss: 4.240, avg. samples / sec: 28134.47
Iteration:   2740, Loss function: 4.273, Average Loss: 4.224, avg. samples / sec: 28117.24
Iteration:   2740, Loss function: 4.143, Average Loss: 4.208, avg. samples / sec: 28090.04
Iteration:   2740, Loss function: 3.647, Average Loss: 4.229, avg. samples / sec: 28078.59
:::MLL 1558579082.499 epoch_stop: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 819}}
:::MLL 1558579082.500 epoch_start: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 673}}
Iteration:   2760, Loss function: 3.780, Average Loss: 4.204, avg. samples / sec: 28119.72
Iteration:   2760, Loss function: 3.539, Average Loss: 4.219, avg. samples / sec: 28089.01
Iteration:   2760, Loss function: 3.813, Average Loss: 4.227, avg. samples / sec: 28118.60
Iteration:   2760, Loss function: 4.389, Average Loss: 4.236, avg. samples / sec: 28040.41
Iteration:   2780, Loss function: 4.084, Average Loss: 4.215, avg. samples / sec: 28081.21
Iteration:   2780, Loss function: 3.391, Average Loss: 4.200, avg. samples / sec: 28078.67
Iteration:   2780, Loss function: 4.282, Average Loss: 4.230, avg. samples / sec: 28111.10
Iteration:   2780, Loss function: 4.415, Average Loss: 4.222, avg. samples / sec: 28048.82
Iteration:   2800, Loss function: 4.555, Average Loss: 4.212, avg. samples / sec: 28021.39
Iteration:   2800, Loss function: 4.123, Average Loss: 4.229, avg. samples / sec: 28025.45
Iteration:   2800, Loss function: 4.568, Average Loss: 4.197, avg. samples / sec: 28018.43
Iteration:   2800, Loss function: 4.289, Average Loss: 4.220, avg. samples / sec: 28041.15
Iteration:   2820, Loss function: 4.221, Average Loss: 4.208, avg. samples / sec: 28063.62
Iteration:   2820, Loss function: 4.154, Average Loss: 4.195, avg. samples / sec: 28065.31
Iteration:   2820, Loss function: 3.755, Average Loss: 4.225, avg. samples / sec: 28057.70
Iteration:   2820, Loss function: 3.464, Average Loss: 4.214, avg. samples / sec: 28072.16
:::MLL 1558579086.661 epoch_stop: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 819}}
:::MLL 1558579086.661 epoch_start: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 673}}
Iteration:   2840, Loss function: 3.987, Average Loss: 4.220, avg. samples / sec: 28048.48
Iteration:   2840, Loss function: 4.353, Average Loss: 4.205, avg. samples / sec: 28032.58
Iteration:   2840, Loss function: 4.607, Average Loss: 4.188, avg. samples / sec: 28018.42
Iteration:   2840, Loss function: 4.504, Average Loss: 4.209, avg. samples / sec: 28002.07
Iteration:   2860, Loss function: 4.207, Average Loss: 4.183, avg. samples / sec: 28169.88
Iteration:   2860, Loss function: 4.884, Average Loss: 4.208, avg. samples / sec: 28189.13
Iteration:   2860, Loss function: 4.714, Average Loss: 4.203, avg. samples / sec: 28144.45
Iteration:   2860, Loss function: 3.544, Average Loss: 4.217, avg. samples / sec: 28101.72
Iteration:   2880, Loss function: 4.353, Average Loss: 4.204, avg. samples / sec: 28065.02
Iteration:   2880, Loss function: 3.942, Average Loss: 4.211, avg. samples / sec: 28101.62
Iteration:   2880, Loss function: 4.003, Average Loss: 4.178, avg. samples / sec: 28044.33
Iteration:   2880, Loss function: 4.115, Average Loss: 4.199, avg. samples / sec: 28050.37
Iteration:   2900, Loss function: 3.534, Average Loss: 4.206, avg. samples / sec: 28033.53
Iteration:   2900, Loss function: 3.624, Average Loss: 4.176, avg. samples / sec: 28041.20
Iteration:   2900, Loss function: 4.086, Average Loss: 4.195, avg. samples / sec: 28038.12
Iteration:   2900, Loss function: 4.165, Average Loss: 4.198, avg. samples / sec: 28025.97
:::MLL 1558579090.873 epoch_stop: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 819}}
:::MLL 1558579090.873 epoch_start: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 673}}
Iteration:   2920, Loss function: 4.397, Average Loss: 4.194, avg. samples / sec: 27986.19
Iteration:   2920, Loss function: 2.944, Average Loss: 4.187, avg. samples / sec: 27955.10
Iteration:   2920, Loss function: 3.449, Average Loss: 4.201, avg. samples / sec: 27933.54
Iteration:   2920, Loss function: 3.222, Average Loss: 4.172, avg. samples / sec: 27931.29
Iteration:   2940, Loss function: 3.214, Average Loss: 4.197, avg. samples / sec: 28087.71
Iteration:   2940, Loss function: 4.456, Average Loss: 4.168, avg. samples / sec: 28081.53
Iteration:   2940, Loss function: 4.406, Average Loss: 4.188, avg. samples / sec: 28025.31
Iteration:   2940, Loss function: 3.798, Average Loss: 4.183, avg. samples / sec: 28048.78
Iteration:   2960, Loss function: 4.102, Average Loss: 4.183, avg. samples / sec: 28143.49
Iteration:   2960, Loss function: 4.561, Average Loss: 4.193, avg. samples / sec: 28123.57
Iteration:   2960, Loss function: 4.476, Average Loss: 4.180, avg. samples / sec: 28136.50
Iteration:   2960, Loss function: 4.357, Average Loss: 4.165, avg. samples / sec: 28115.09
:::MLL 1558579095.037 epoch_stop: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 819}}
:::MLL 1558579095.037 epoch_start: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 673}}
Iteration:   2980, Loss function: 3.458, Average Loss: 4.161, avg. samples / sec: 28016.34
Iteration:   2980, Loss function: 3.400, Average Loss: 4.178, avg. samples / sec: 27975.84
Iteration:   2980, Loss function: 3.722, Average Loss: 4.188, avg. samples / sec: 27976.84
Iteration:   2980, Loss function: 3.887, Average Loss: 4.176, avg. samples / sec: 27938.07
Iteration:   3000, Loss function: 3.805, Average Loss: 4.170, avg. samples / sec: 28118.28
Iteration:   3000, Loss function: 4.267, Average Loss: 4.183, avg. samples / sec: 28117.21
Iteration:   3000, Loss function: 4.007, Average Loss: 4.155, avg. samples / sec: 28088.80
Iteration:   3000, Loss function: 3.904, Average Loss: 4.173, avg. samples / sec: 28128.93
Iteration:   3020, Loss function: 3.252, Average Loss: 4.152, avg. samples / sec: 28042.68
Iteration:   3020, Loss function: 4.031, Average Loss: 4.176, avg. samples / sec: 28026.86
Iteration:   3020, Loss function: 4.222, Average Loss: 4.168, avg. samples / sec: 28012.26
Iteration:   3020, Loss function: 4.391, Average Loss: 4.167, avg. samples / sec: 28048.16
Iteration:   3040, Loss function: 3.572, Average Loss: 4.166, avg. samples / sec: 28173.29
Iteration:   3040, Loss function: 3.804, Average Loss: 4.163, avg. samples / sec: 28176.36
Iteration:   3040, Loss function: 4.349, Average Loss: 4.150, avg. samples / sec: 28148.77
Iteration:   3040, Loss function: 3.848, Average Loss: 4.171, avg. samples / sec: 28150.91
:::MLL 1558579099.191 epoch_stop: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 819}}
:::MLL 1558579099.191 epoch_start: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 673}}
Iteration:   3060, Loss function: 4.376, Average Loss: 4.160, avg. samples / sec: 28122.38
Iteration:   3060, Loss function: 3.843, Average Loss: 4.166, avg. samples / sec: 28124.68
Iteration:   3060, Loss function: 3.707, Average Loss: 4.145, avg. samples / sec: 28114.57
Iteration:   3060, Loss function: 4.055, Average Loss: 4.162, avg. samples / sec: 28104.92
Iteration:   3080, Loss function: 4.308, Average Loss: 4.156, avg. samples / sec: 28163.04
Iteration:   3080, Loss function: 3.407, Average Loss: 4.159, avg. samples / sec: 28152.99
Iteration:   3080, Loss function: 3.860, Average Loss: 4.164, avg. samples / sec: 28130.18
Iteration:   3080, Loss function: 3.836, Average Loss: 4.140, avg. samples / sec: 28117.04
Iteration:   3100, Loss function: 3.635, Average Loss: 4.137, avg. samples / sec: 28088.92
Iteration:   3100, Loss function: 4.125, Average Loss: 4.153, avg. samples / sec: 28052.50
Iteration:   3100, Loss function: 3.767, Average Loss: 4.156, avg. samples / sec: 28067.41
Iteration:   3100, Loss function: 3.746, Average Loss: 4.152, avg. samples / sec: 28029.17
Iteration:   3120, Loss function: 3.971, Average Loss: 4.133, avg. samples / sec: 28052.53
Iteration:   3120, Loss function: 3.979, Average Loss: 4.147, avg. samples / sec: 28011.20
Iteration:   3120, Loss function: 4.340, Average Loss: 4.152, avg. samples / sec: 28003.55
Iteration:   3120, Loss function: 3.675, Average Loss: 4.151, avg. samples / sec: 27994.61
:::MLL 1558579103.405 epoch_stop: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 819}}
:::MLL 1558579103.405 epoch_start: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 673}}
Iteration:   3140, Loss function: 4.157, Average Loss: 4.142, avg. samples / sec: 28050.93
Iteration:   3140, Loss function: 4.273, Average Loss: 4.148, avg. samples / sec: 28064.84
Iteration:   3140, Loss function: 3.885, Average Loss: 4.128, avg. samples / sec: 28005.06
Iteration:   3140, Loss function: 4.006, Average Loss: 4.146, avg. samples / sec: 28057.10
Iteration:   3160, Loss function: 3.681, Average Loss: 4.141, avg. samples / sec: 28125.17
Iteration:   3160, Loss function: 3.480, Average Loss: 4.145, avg. samples / sec: 28100.91
Iteration:   3160, Loss function: 3.675, Average Loss: 4.135, avg. samples / sec: 28101.26
Iteration:   3160, Loss function: 4.027, Average Loss: 4.125, avg. samples / sec: 28055.57
Iteration:   3180, Loss function: 3.754, Average Loss: 4.141, avg. samples / sec: 28142.96
Iteration:   3180, Loss function: 4.128, Average Loss: 4.134, avg. samples / sec: 28142.32
Iteration:   3180, Loss function: 4.372, Average Loss: 4.138, avg. samples / sec: 28089.11
Iteration:   3180, Loss function: 3.700, Average Loss: 4.120, avg. samples / sec: 28109.44
Iteration:   3200, Loss function: 3.291, Average Loss: 4.129, avg. samples / sec: 28080.66
Iteration:   3200, Loss function: 3.533, Average Loss: 4.115, avg. samples / sec: 28157.59
Iteration:   3200, Loss function: 4.078, Average Loss: 4.140, avg. samples / sec: 28053.33
Iteration:   3200, Loss function: 3.902, Average Loss: 4.135, avg. samples / sec: 28035.78
:::MLL 1558579107.561 epoch_stop: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 819}}
:::MLL 1558579107.562 epoch_start: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 673}}
Iteration:   3220, Loss function: 3.473, Average Loss: 4.112, avg. samples / sec: 28008.07
Iteration:   3220, Loss function: 3.684, Average Loss: 4.134, avg. samples / sec: 28014.30
Iteration:   3220, Loss function: 3.790, Average Loss: 4.130, avg. samples / sec: 28068.87
Iteration:   3220, Loss function: 4.056, Average Loss: 4.123, avg. samples / sec: 27974.81
Iteration:   3240, Loss function: 3.980, Average Loss: 4.128, avg. samples / sec: 28161.17
Iteration:   3240, Loss function: 4.191, Average Loss: 4.119, avg. samples / sec: 28163.89
Iteration:   3240, Loss function: 4.468, Average Loss: 4.123, avg. samples / sec: 28135.81
Iteration:   3240, Loss function: 4.168, Average Loss: 4.106, avg. samples / sec: 28113.94
Iteration:   3260, Loss function: 3.560, Average Loss: 4.115, avg. samples / sec: 28054.90
Iteration:   3260, Loss function: 4.270, Average Loss: 4.121, avg. samples / sec: 28065.15
Iteration:   3260, Loss function: 3.592, Average Loss: 4.102, avg. samples / sec: 28024.10
Iteration:   3260, Loss function: 3.869, Average Loss: 4.122, avg. samples / sec: 27971.73
Iteration:   3280, Loss function: 3.622, Average Loss: 4.101, avg. samples / sec: 28090.51
Iteration:   3280, Loss function: 4.984, Average Loss: 4.111, avg. samples / sec: 28041.46
Iteration:   3280, Loss function: 3.476, Average Loss: 4.119, avg. samples / sec: 28103.00
Iteration:   3280, Loss function: 4.077, Average Loss: 4.118, avg. samples / sec: 28025.64
:::MLL 1558579111.722 epoch_stop: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 819}}
:::MLL 1558579111.722 epoch_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 673}}
Iteration:   3300, Loss function: 3.755, Average Loss: 4.118, avg. samples / sec: 27876.70
Iteration:   3300, Loss function: 3.856, Average Loss: 4.114, avg. samples / sec: 27887.38
Iteration:   3300, Loss function: 3.766, Average Loss: 4.098, avg. samples / sec: 27863.90
Iteration:   3300, Loss function: 3.867, Average Loss: 4.105, avg. samples / sec: 27796.14
Iteration:   3320, Loss function: 4.413, Average Loss: 4.094, avg. samples / sec: 28041.00
Iteration:   3320, Loss function: 3.605, Average Loss: 4.114, avg. samples / sec: 28033.74
Iteration:   3320, Loss function: 4.224, Average Loss: 4.103, avg. samples / sec: 28095.95
Iteration:   3320, Loss function: 3.453, Average Loss: 4.107, avg. samples / sec: 27993.19
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
:::MLL 1558579114.521 eval_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.07 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.07 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.07 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.07 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.48s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.50s)
DONE (t=0.50s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.54s)
DONE (t=2.98s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.18487
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.33634
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.18604
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.04504
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.19590
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.29405
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.19030
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.28027
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.29537
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.07882
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.31720
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.46297
Current AP: 0.18487 AP goal: 0.23000
:::MLL 1558579119.123 eval_accuracy: {"value": 0.1848698456097888, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 389}}
:::MLL 1558579119.265 eval_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 392}}
:::MLL 1558579119.279 block_stop: {"value": null, "metadata": {"first_epoch_num": 33, "file": "train.py", "lineno": 804}}
:::MLL 1558579119.280 block_start: {"value": null, "metadata": {"first_epoch_num": 44, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3340, Loss function: 4.311, Average Loss: 4.097, avg. samples / sec: 5244.14
Iteration:   3340, Loss function: 3.777, Average Loss: 4.091, avg. samples / sec: 5243.24
Iteration:   3340, Loss function: 3.815, Average Loss: 4.102, avg. samples / sec: 5243.37
Iteration:   3340, Loss function: 3.492, Average Loss: 4.111, avg. samples / sec: 5241.70
:::MLL 1558579120.717 epoch_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 819}}
:::MLL 1558579120.717 epoch_start: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 673}}
Iteration:   3360, Loss function: 3.415, Average Loss: 4.092, avg. samples / sec: 27873.07
Iteration:   3360, Loss function: 3.994, Average Loss: 4.102, avg. samples / sec: 27876.72
Iteration:   3360, Loss function: 3.759, Average Loss: 4.088, avg. samples / sec: 27818.24
Iteration:   3360, Loss function: 3.310, Average Loss: 4.087, avg. samples / sec: 27791.57
Iteration:   3380, Loss function: 3.715, Average Loss: 4.092, avg. samples / sec: 28064.14
Iteration:   3380, Loss function: 3.877, Average Loss: 4.079, avg. samples / sec: 28082.66
Iteration:   3380, Loss function: 4.106, Average Loss: 4.078, avg. samples / sec: 28048.02
Iteration:   3380, Loss function: 3.763, Average Loss: 4.081, avg. samples / sec: 27973.63
Iteration:   3400, Loss function: 3.539, Average Loss: 4.066, avg. samples / sec: 28016.87
Iteration:   3400, Loss function: 3.640, Average Loss: 4.080, avg. samples / sec: 28014.92
Iteration:   3400, Loss function: 3.232, Average Loss: 4.071, avg. samples / sec: 28093.53
Iteration:   3400, Loss function: 3.366, Average Loss: 4.072, avg. samples / sec: 28030.75
Iteration:   3420, Loss function: 3.527, Average Loss: 4.072, avg. samples / sec: 28061.15
Iteration:   3420, Loss function: 4.005, Average Loss: 4.061, avg. samples / sec: 28068.86
Iteration:   3420, Loss function: 4.140, Average Loss: 4.058, avg. samples / sec: 28055.66
Iteration:   3420, Loss function: 3.623, Average Loss: 4.062, avg. samples / sec: 28063.21
:::MLL 1558579124.878 epoch_stop: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 819}}
:::MLL 1558579124.879 epoch_start: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 673}}
Iteration:   3440, Loss function: 3.611, Average Loss: 4.064, avg. samples / sec: 28038.99
Iteration:   3440, Loss function: 3.755, Average Loss: 4.052, avg. samples / sec: 28019.76
Iteration:   3440, Loss function: 3.724, Average Loss: 4.050, avg. samples / sec: 28019.14
Iteration:   3440, Loss function: 3.741, Average Loss: 4.051, avg. samples / sec: 27991.69
Iteration:   3460, Loss function: 3.789, Average Loss: 4.055, avg. samples / sec: 27993.72
Iteration:   3460, Loss function: 3.801, Average Loss: 4.041, avg. samples / sec: 27996.85
Iteration:   3460, Loss function: 3.754, Average Loss: 4.040, avg. samples / sec: 28018.52
Iteration:   3460, Loss function: 3.401, Average Loss: 4.039, avg. samples / sec: 27977.59
Iteration:   3480, Loss function: 3.066, Average Loss: 4.025, avg. samples / sec: 28190.80
Iteration:   3480, Loss function: 3.200, Average Loss: 4.043, avg. samples / sec: 28135.62
Iteration:   3480, Loss function: 2.954, Average Loss: 4.026, avg. samples / sec: 28141.61
Iteration:   3480, Loss function: 3.295, Average Loss: 4.028, avg. samples / sec: 28137.37
Iteration:   3500, Loss function: 3.095, Average Loss: 4.035, avg. samples / sec: 28028.97
Iteration:   3500, Loss function: 3.399, Average Loss: 4.018, avg. samples / sec: 28047.87
Iteration:   3500, Loss function: 3.200, Average Loss: 4.015, avg. samples / sec: 27983.58
Iteration:   3500, Loss function: 3.368, Average Loss: 4.014, avg. samples / sec: 27995.99
:::MLL 1558579129.045 epoch_stop: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 819}}
:::MLL 1558579129.045 epoch_start: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 673}}
Iteration:   3520, Loss function: 3.144, Average Loss: 4.025, avg. samples / sec: 27972.75
Iteration:   3520, Loss function: 3.648, Average Loss: 4.006, avg. samples / sec: 28001.05
Iteration:   3520, Loss function: 3.725, Average Loss: 4.003, avg. samples / sec: 28008.68
Iteration:   3520, Loss function: 3.934, Average Loss: 4.007, avg. samples / sec: 27970.48
Iteration:   3540, Loss function: 3.562, Average Loss: 3.993, avg. samples / sec: 28041.64
Iteration:   3540, Loss function: 3.997, Average Loss: 3.995, avg. samples / sec: 28040.40
Iteration:   3540, Loss function: 4.005, Average Loss: 3.993, avg. samples / sec: 28030.20
Iteration:   3540, Loss function: 3.654, Average Loss: 4.014, avg. samples / sec: 28003.48
Iteration:   3560, Loss function: 3.703, Average Loss: 3.986, avg. samples / sec: 27911.27
Iteration:   3560, Loss function: 3.799, Average Loss: 3.983, avg. samples / sec: 27897.00
Iteration:   3560, Loss function: 3.668, Average Loss: 4.004, avg. samples / sec: 27925.45
Iteration:   3560, Loss function: 3.603, Average Loss: 3.984, avg. samples / sec: 27898.45
Iteration:   3580, Loss function: 3.258, Average Loss: 3.974, avg. samples / sec: 28003.34
Iteration:   3580, Loss function: 3.696, Average Loss: 3.974, avg. samples / sec: 28013.65
Iteration:   3580, Loss function: 3.258, Average Loss: 3.993, avg. samples / sec: 28004.78
Iteration:   3580, Loss function: 3.511, Average Loss: 3.976, avg. samples / sec: 27953.38
:::MLL 1558579133.270 epoch_stop: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 819}}
:::MLL 1558579133.270 epoch_start: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 673}}
Iteration:   3600, Loss function: 3.617, Average Loss: 3.981, avg. samples / sec: 28010.80
Iteration:   3600, Loss function: 3.649, Average Loss: 3.966, avg. samples / sec: 28045.13
Iteration:   3600, Loss function: 3.883, Average Loss: 3.966, avg. samples / sec: 27981.13
Iteration:   3600, Loss function: 3.900, Average Loss: 3.967, avg. samples / sec: 27953.05
Iteration:   3620, Loss function: 3.617, Average Loss: 3.971, avg. samples / sec: 28063.90
Iteration:   3620, Loss function: 3.010, Average Loss: 3.955, avg. samples / sec: 28087.25
Iteration:   3620, Loss function: 3.374, Average Loss: 3.955, avg. samples / sec: 28044.85
Iteration:   3620, Loss function: 3.480, Average Loss: 3.956, avg. samples / sec: 28055.81
Iteration:   3640, Loss function: 3.262, Average Loss: 3.945, avg. samples / sec: 28064.79
Iteration:   3640, Loss function: 3.173, Average Loss: 3.961, avg. samples / sec: 27999.79
Iteration:   3640, Loss function: 2.576, Average Loss: 3.945, avg. samples / sec: 28022.59
Iteration:   3640, Loss function: 3.745, Average Loss: 3.943, avg. samples / sec: 27985.40
Iteration:   3660, Loss function: 3.081, Average Loss: 3.950, avg. samples / sec: 28011.40
Iteration:   3660, Loss function: 3.660, Average Loss: 3.937, avg. samples / sec: 28012.65
Iteration:   3660, Loss function: 3.172, Average Loss: 3.935, avg. samples / sec: 28006.56
Iteration:   3660, Loss function: 3.798, Average Loss: 3.934, avg. samples / sec: 28016.25
:::MLL 1558579137.438 epoch_stop: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 819}}
:::MLL 1558579137.439 epoch_start: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 673}}
Iteration:   3680, Loss function: 3.827, Average Loss: 3.938, avg. samples / sec: 28080.50
Iteration:   3680, Loss function: 3.042, Average Loss: 3.926, avg. samples / sec: 28089.80
Iteration:   3680, Loss function: 3.228, Average Loss: 3.928, avg. samples / sec: 28078.37
Iteration:   3680, Loss function: 3.510, Average Loss: 3.923, avg. samples / sec: 28068.84
Iteration:   3700, Loss function: 3.323, Average Loss: 3.912, avg. samples / sec: 27972.12
Iteration:   3700, Loss function: 3.145, Average Loss: 3.928, avg. samples / sec: 27950.53
Iteration:   3700, Loss function: 2.782, Average Loss: 3.917, avg. samples / sec: 27946.36
Iteration:   3700, Loss function: 3.684, Average Loss: 3.917, avg. samples / sec: 27913.15
Iteration:   3720, Loss function: 3.485, Average Loss: 3.906, avg. samples / sec: 28061.51
Iteration:   3720, Loss function: 2.784, Average Loss: 3.919, avg. samples / sec: 28046.96
Iteration:   3720, Loss function: 3.159, Average Loss: 3.899, avg. samples / sec: 28033.72
Iteration:   3720, Loss function: 3.216, Average Loss: 3.909, avg. samples / sec: 28034.20
Iteration:   3740, Loss function: 3.984, Average Loss: 3.898, avg. samples / sec: 28045.81
Iteration:   3740, Loss function: 3.668, Average Loss: 3.899, avg. samples / sec: 28104.05
Iteration:   3740, Loss function: 3.874, Average Loss: 3.910, avg. samples / sec: 28047.45
Iteration:   3740, Loss function: 3.232, Average Loss: 3.889, avg. samples / sec: 28026.56
:::MLL 1558579141.602 epoch_stop: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 819}}
:::MLL 1558579141.602 epoch_start: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 673}}
:::MLL 1558579142.150 eval_start: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.08 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.08 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.08 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.08 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.56s)
DONE (t=2.89s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22413
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.38530
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.22700
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.06078
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.23832
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.36261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.21796
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.31930
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33521
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09909
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36507
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.51940
Current AP: 0.22413 AP goal: 0.23000
:::MLL 1558579146.701 eval_accuracy: {"value": 0.22413428701936586, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 389}}
:::MLL 1558579146.702 eval_stop: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 392}}
:::MLL 1558579146.717 block_stop: {"value": null, "metadata": {"first_epoch_num": 44, "file": "train.py", "lineno": 804}}
:::MLL 1558579146.717 block_start: {"value": null, "metadata": {"first_epoch_num": 50, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3760, Loss function: 3.256, Average Loss: 3.900, avg. samples / sec: 5421.34
Iteration:   3760, Loss function: 3.439, Average Loss: 3.890, avg. samples / sec: 5420.94
Iteration:   3760, Loss function: 3.592, Average Loss: 3.889, avg. samples / sec: 5419.86
Iteration:   3760, Loss function: 3.297, Average Loss: 3.880, avg. samples / sec: 5420.14
Iteration:   3780, Loss function: 3.201, Average Loss: 3.878, avg. samples / sec: 27869.45
Iteration:   3780, Loss function: 3.482, Average Loss: 3.893, avg. samples / sec: 27827.35
Iteration:   3780, Loss function: 4.167, Average Loss: 3.872, avg. samples / sec: 27882.38
Iteration:   3780, Loss function: 3.414, Average Loss: 3.885, avg. samples / sec: 27781.02
Iteration:   3800, Loss function: 3.505, Average Loss: 3.884, avg. samples / sec: 28081.78
Iteration:   3800, Loss function: 2.990, Average Loss: 3.876, avg. samples / sec: 28121.64
Iteration:   3800, Loss function: 3.427, Average Loss: 3.870, avg. samples / sec: 28051.09
Iteration:   3800, Loss function: 3.165, Average Loss: 3.864, avg. samples / sec: 28034.22
:::MLL 1558579150.399 epoch_stop: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 819}}
:::MLL 1558579150.400 epoch_start: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 673}}
Iteration:   3820, Loss function: 2.872, Average Loss: 3.866, avg. samples / sec: 27979.19
Iteration:   3820, Loss function: 3.233, Average Loss: 3.856, avg. samples / sec: 28012.40
Iteration:   3820, Loss function: 3.616, Average Loss: 3.875, avg. samples / sec: 27955.81
Iteration:   3820, Loss function: 3.388, Average Loss: 3.859, avg. samples / sec: 27946.46
Iteration:   3840, Loss function: 3.180, Average Loss: 3.867, avg. samples / sec: 27922.65
Iteration:   3840, Loss function: 3.469, Average Loss: 3.845, avg. samples / sec: 27914.41
Iteration:   3840, Loss function: 3.387, Average Loss: 3.851, avg. samples / sec: 27950.92
Iteration:   3840, Loss function: 3.524, Average Loss: 3.856, avg. samples / sec: 27855.61
Iteration:   3860, Loss function: 3.711, Average Loss: 3.850, avg. samples / sec: 28048.43
Iteration:   3860, Loss function: 3.567, Average Loss: 3.839, avg. samples / sec: 27992.56
Iteration:   3860, Loss function: 3.310, Average Loss: 3.860, avg. samples / sec: 27981.06
Iteration:   3860, Loss function: 3.671, Average Loss: 3.841, avg. samples / sec: 27971.28
Iteration:   3880, Loss function: 3.763, Average Loss: 3.833, avg. samples / sec: 28167.85
Iteration:   3880, Loss function: 4.007, Average Loss: 3.843, avg. samples / sec: 28165.95
Iteration:   3880, Loss function: 3.322, Average Loss: 3.850, avg. samples / sec: 28175.68
Iteration:   3880, Loss function: 3.304, Average Loss: 3.833, avg. samples / sec: 28140.45
:::MLL 1558579154.565 epoch_stop: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 819}}
:::MLL 1558579154.565 epoch_start: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 673}}
Iteration:   3900, Loss function: 3.300, Average Loss: 3.840, avg. samples / sec: 28121.45
Iteration:   3900, Loss function: 3.007, Average Loss: 3.825, avg. samples / sec: 28106.33
Iteration:   3900, Loss function: 3.783, Average Loss: 3.826, avg. samples / sec: 28156.97
Iteration:   3900, Loss function: 3.843, Average Loss: 3.836, avg. samples / sec: 28100.04
Iteration:   3920, Loss function: 4.081, Average Loss: 3.830, avg. samples / sec: 28250.68
Iteration:   3920, Loss function: 3.336, Average Loss: 3.816, avg. samples / sec: 28256.47
Iteration:   3920, Loss function: 3.018, Average Loss: 3.815, avg. samples / sec: 28239.22
Iteration:   3920, Loss function: 2.381, Average Loss: 3.829, avg. samples / sec: 28210.74
Iteration:   3940, Loss function: 3.524, Average Loss: 3.823, avg. samples / sec: 28096.87
Iteration:   3940, Loss function: 3.013, Average Loss: 3.807, avg. samples / sec: 28105.13
Iteration:   3940, Loss function: 3.470, Average Loss: 3.819, avg. samples / sec: 28139.47
Iteration:   3940, Loss function: 3.925, Average Loss: 3.809, avg. samples / sec: 28081.37
Iteration:   3960, Loss function: 3.239, Average Loss: 3.800, avg. samples / sec: 28076.15
Iteration:   3960, Loss function: 3.460, Average Loss: 3.813, avg. samples / sec: 28052.76
Iteration:   3960, Loss function: 4.537, Average Loss: 3.818, avg. samples / sec: 28027.15
Iteration:   3960, Loss function: 3.564, Average Loss: 3.801, avg. samples / sec: 28030.54
:::MLL 1558579158.716 epoch_stop: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 819}}
:::MLL 1558579158.717 epoch_start: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 673}}
Iteration:   3980, Loss function: 3.683, Average Loss: 3.807, avg. samples / sec: 27898.93
Iteration:   3980, Loss function: 3.252, Average Loss: 3.791, avg. samples / sec: 27874.27
Iteration:   3980, Loss function: 3.222, Average Loss: 3.805, avg. samples / sec: 27878.56
Iteration:   3980, Loss function: 3.740, Average Loss: 3.792, avg. samples / sec: 27876.93
Iteration:   4000, Loss function: 3.108, Average Loss: 3.782, avg. samples / sec: 28042.28
Iteration:   4000, Loss function: 3.765, Average Loss: 3.785, avg. samples / sec: 28050.74
Iteration:   4000, Loss function: 3.212, Average Loss: 3.797, avg. samples / sec: 28036.79
Iteration:   4000, Loss function: 3.451, Average Loss: 3.798, avg. samples / sec: 28027.22
Iteration:   4020, Loss function: 3.258, Average Loss: 3.776, avg. samples / sec: 28033.27
Iteration:   4020, Loss function: 3.078, Average Loss: 3.777, avg. samples / sec: 28028.04
Iteration:   4020, Loss function: 3.030, Average Loss: 3.789, avg. samples / sec: 27997.70
Iteration:   4020, Loss function: 3.465, Average Loss: 3.789, avg. samples / sec: 27991.66
Iteration:   4040, Loss function: 3.652, Average Loss: 3.781, avg. samples / sec: 28058.14
Iteration:   4040, Loss function: 3.730, Average Loss: 3.781, avg. samples / sec: 28059.99
Iteration:   4040, Loss function: 3.074, Average Loss: 3.767, avg. samples / sec: 28007.60
Iteration:   4040, Loss function: 3.342, Average Loss: 3.770, avg. samples / sec: 27987.97
:::MLL 1558579162.944 epoch_stop: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 819}}
:::MLL 1558579162.944 epoch_start: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 673}}
Iteration:   4060, Loss function: 3.390, Average Loss: 3.762, avg. samples / sec: 28021.94
Iteration:   4060, Loss function: 3.404, Average Loss: 3.772, avg. samples / sec: 27971.72
Iteration:   4060, Loss function: 3.458, Average Loss: 3.760, avg. samples / sec: 27974.19
Iteration:   4060, Loss function: 3.593, Average Loss: 3.772, avg. samples / sec: 27962.95
Iteration:   4080, Loss function: 3.519, Average Loss: 3.752, avg. samples / sec: 27923.00
Iteration:   4080, Loss function: 3.358, Average Loss: 3.763, avg. samples / sec: 27920.78
Iteration:   4080, Loss function: 3.796, Average Loss: 3.755, avg. samples / sec: 27891.31
Iteration:   4080, Loss function: 3.427, Average Loss: 3.763, avg. samples / sec: 27901.52
Iteration:   4100, Loss function: 4.250, Average Loss: 3.755, avg. samples / sec: 27897.15
Iteration:   4100, Loss function: 3.816, Average Loss: 3.756, avg. samples / sec: 27887.38
Iteration:   4100, Loss function: 3.579, Average Loss: 3.747, avg. samples / sec: 27885.46
Iteration:   4100, Loss function: 3.808, Average Loss: 3.745, avg. samples / sec: 27874.46
Iteration:   4120, Loss function: 3.697, Average Loss: 3.750, avg. samples / sec: 27978.32
Iteration:   4120, Loss function: 3.298, Average Loss: 3.746, avg. samples / sec: 27972.48
Iteration:   4120, Loss function: 3.173, Average Loss: 3.739, avg. samples / sec: 27972.46
Iteration:   4120, Loss function: 2.863, Average Loss: 3.743, avg. samples / sec: 27865.56
:::MLL 1558579167.124 epoch_stop: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 819}}
:::MLL 1558579167.125 epoch_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 673}}
Iteration:   4140, Loss function: 3.464, Average Loss: 3.741, avg. samples / sec: 27969.53
Iteration:   4140, Loss function: 3.699, Average Loss: 3.730, avg. samples / sec: 27976.65
Iteration:   4140, Loss function: 3.309, Average Loss: 3.740, avg. samples / sec: 27970.88
Iteration:   4140, Loss function: 3.515, Average Loss: 3.738, avg. samples / sec: 28039.23
Iteration:   4160, Loss function: 3.561, Average Loss: 3.722, avg. samples / sec: 27969.56
Iteration:   4160, Loss function: 3.924, Average Loss: 3.734, avg. samples / sec: 27943.77
Iteration:   4160, Loss function: 3.682, Average Loss: 3.730, avg. samples / sec: 27989.88
Iteration:   4160, Loss function: 3.234, Average Loss: 3.734, avg. samples / sec: 27919.66
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
:::MLL 1558579169.538 eval_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.14 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.14 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.14 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.14 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.54s)
DONE (t=0.58s)
DONE (t=2.83s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22766
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.38981
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23350
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.06035
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24198
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.36576
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22156
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32273
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33949
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.10123
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.37183
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.52307
Current AP: 0.22766 AP goal: 0.23000
:::MLL 1558579174.089 eval_accuracy: {"value": 0.22766025131921988, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 389}}
:::MLL 1558579174.183 eval_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 392}}
:::MLL 1558579174.197 block_stop: {"value": null, "metadata": {"first_epoch_num": 50, "file": "train.py", "lineno": 804}}
:::MLL 1558579174.197 block_start: {"value": null, "metadata": {"first_epoch_num": 55, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   4180, Loss function: 3.391, Average Loss: 3.713, avg. samples / sec: 5337.53
Iteration:   4180, Loss function: 3.501, Average Loss: 3.725, avg. samples / sec: 5338.41
Iteration:   4180, Loss function: 4.588, Average Loss: 3.728, avg. samples / sec: 5338.35
Iteration:   4180, Loss function: 3.059, Average Loss: 3.726, avg. samples / sec: 5338.36
:::MLL 1558579175.958 epoch_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 819}}
:::MLL 1558579175.959 epoch_start: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 673}}
Iteration:   4200, Loss function: 2.866, Average Loss: 3.719, avg. samples / sec: 27825.17
Iteration:   4200, Loss function: 3.504, Average Loss: 3.720, avg. samples / sec: 27818.20
Iteration:   4200, Loss function: 3.257, Average Loss: 3.720, avg. samples / sec: 27822.21
Iteration:   4200, Loss function: 3.399, Average Loss: 3.707, avg. samples / sec: 27785.08
Iteration:   4220, Loss function: 3.144, Average Loss: 3.701, avg. samples / sec: 27984.79
Iteration:   4220, Loss function: 2.747, Average Loss: 3.709, avg. samples / sec: 27937.16
Iteration:   4220, Loss function: 2.935, Average Loss: 3.709, avg. samples / sec: 27949.42
Iteration:   4220, Loss function: 3.609, Average Loss: 3.710, avg. samples / sec: 27936.22
Iteration:   4240, Loss function: 3.447, Average Loss: 3.701, avg. samples / sec: 27912.93
Iteration:   4240, Loss function: 2.948, Average Loss: 3.692, avg. samples / sec: 27894.19
Iteration:   4240, Loss function: 2.975, Average Loss: 3.700, avg. samples / sec: 27898.53
Iteration:   4240, Loss function: 3.688, Average Loss: 3.704, avg. samples / sec: 27871.82
Iteration:   4260, Loss function: 3.017, Average Loss: 3.689, avg. samples / sec: 28055.32
Iteration:   4260, Loss function: 2.567, Average Loss: 3.685, avg. samples / sec: 28052.36
Iteration:   4260, Loss function: 2.993, Average Loss: 3.698, avg. samples / sec: 28085.05
Iteration:   4260, Loss function: 3.642, Average Loss: 3.695, avg. samples / sec: 27963.59
:::MLL 1558579180.191 epoch_stop: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 819}}
:::MLL 1558579180.191 epoch_start: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 673}}
Iteration:   4280, Loss function: 3.540, Average Loss: 3.689, avg. samples / sec: 27976.95
Iteration:   4280, Loss function: 4.036, Average Loss: 3.681, avg. samples / sec: 27876.65
Iteration:   4280, Loss function: 3.575, Average Loss: 3.689, avg. samples / sec: 27891.93
Iteration:   4280, Loss function: 3.629, Average Loss: 3.678, avg. samples / sec: 27875.29
Iteration:   4300, Loss function: 3.460, Average Loss: 3.671, avg. samples / sec: 28031.51
Iteration:   4300, Loss function: 3.604, Average Loss: 3.674, avg. samples / sec: 28018.08
Iteration:   4300, Loss function: 3.588, Average Loss: 3.683, avg. samples / sec: 28012.65
Iteration:   4300, Loss function: 3.391, Average Loss: 3.681, avg. samples / sec: 28013.21
Iteration:   4320, Loss function: 3.108, Average Loss: 3.673, avg. samples / sec: 27989.04
Iteration:   4320, Loss function: 3.134, Average Loss: 3.663, avg. samples / sec: 27976.17
Iteration:   4320, Loss function: 4.041, Average Loss: 3.667, avg. samples / sec: 27974.14
Iteration:   4320, Loss function: 3.981, Average Loss: 3.678, avg. samples / sec: 27977.23
Iteration:   4340, Loss function: 4.508, Average Loss: 3.670, avg. samples / sec: 28078.55
Iteration:   4340, Loss function: 3.512, Average Loss: 3.660, avg. samples / sec: 28089.78
Iteration:   4340, Loss function: 3.238, Average Loss: 3.673, avg. samples / sec: 28075.54
Iteration:   4340, Loss function: 3.552, Average Loss: 3.655, avg. samples / sec: 28062.60
:::MLL 1558579184.358 epoch_stop: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 819}}
:::MLL 1558579184.359 epoch_start: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 673}}
Iteration:   4360, Loss function: 3.906, Average Loss: 3.651, avg. samples / sec: 28009.11
Iteration:   4360, Loss function: 3.627, Average Loss: 3.665, avg. samples / sec: 27952.16
Iteration:   4360, Loss function: 3.918, Average Loss: 3.663, avg. samples / sec: 27936.92
Iteration:   4360, Loss function: 2.712, Average Loss: 3.655, avg. samples / sec: 27923.28
Iteration:   4380, Loss function: 4.179, Average Loss: 3.649, avg. samples / sec: 28025.16
Iteration:   4380, Loss function: 3.358, Average Loss: 3.659, avg. samples / sec: 28000.47
Iteration:   4380, Loss function: 3.323, Average Loss: 3.647, avg. samples / sec: 27916.39
Iteration:   4380, Loss function: 3.558, Average Loss: 3.657, avg. samples / sec: 27964.79
Iteration:   4400, Loss function: 3.539, Average Loss: 3.640, avg. samples / sec: 28179.40
Iteration:   4400, Loss function: 3.538, Average Loss: 3.653, avg. samples / sec: 28148.51
Iteration:   4400, Loss function: 3.988, Average Loss: 3.645, avg. samples / sec: 28103.27
Iteration:   4400, Loss function: 3.952, Average Loss: 3.653, avg. samples / sec: 28106.00
Iteration:   4420, Loss function: 2.763, Average Loss: 3.648, avg. samples / sec: 28052.91
Iteration:   4420, Loss function: 3.218, Average Loss: 3.638, avg. samples / sec: 28001.84
Iteration:   4420, Loss function: 4.063, Average Loss: 3.648, avg. samples / sec: 27953.55
Iteration:   4420, Loss function: 3.164, Average Loss: 3.635, avg. samples / sec: 27936.73
:::MLL 1558579188.584 epoch_stop: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 819}}
:::MLL 1558579188.584 epoch_start: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 673}}
Iteration:   4440, Loss function: 3.172, Average Loss: 3.643, avg. samples / sec: 27896.06
Iteration:   4440, Loss function: 2.807, Average Loss: 3.631, avg. samples / sec: 27860.88
Iteration:   4440, Loss function: 2.876, Average Loss: 3.643, avg. samples / sec: 27866.03
Iteration:   4440, Loss function: 3.523, Average Loss: 3.630, avg. samples / sec: 27882.66
Iteration:   4460, Loss function: 3.109, Average Loss: 3.626, avg. samples / sec: 27994.89
Iteration:   4460, Loss function: 3.590, Average Loss: 3.624, avg. samples / sec: 28006.50
Iteration:   4460, Loss function: 3.162, Average Loss: 3.638, avg. samples / sec: 27952.50
Iteration:   4460, Loss function: 3.226, Average Loss: 3.638, avg. samples / sec: 27880.57
Iteration:   4480, Loss function: 3.165, Average Loss: 3.630, avg. samples / sec: 27965.89
Iteration:   4480, Loss function: 3.308, Average Loss: 3.619, avg. samples / sec: 27886.75
Iteration:   4480, Loss function: 3.495, Average Loss: 3.618, avg. samples / sec: 27864.12
Iteration:   4480, Loss function: 2.622, Average Loss: 3.631, avg. samples / sec: 27919.17
Iteration:   4500, Loss function: 3.182, Average Loss: 3.609, avg. samples / sec: 27955.01
Iteration:   4500, Loss function: 3.199, Average Loss: 3.625, avg. samples / sec: 27930.11
Iteration:   4500, Loss function: 3.112, Average Loss: 3.625, avg. samples / sec: 27904.70
Iteration:   4500, Loss function: 3.687, Average Loss: 3.614, avg. samples / sec: 27873.61
:::MLL 1558579192.764 epoch_stop: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 819}}
:::MLL 1558579192.764 epoch_start: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 673}}
Iteration:   4520, Loss function: 3.834, Average Loss: 3.619, avg. samples / sec: 27998.99
Iteration:   4520, Loss function: 3.193, Average Loss: 3.606, avg. samples / sec: 27975.91
Iteration:   4520, Loss function: 3.154, Average Loss: 3.608, avg. samples / sec: 28024.33
Iteration:   4520, Loss function: 3.620, Average Loss: 3.622, avg. samples / sec: 27983.70
Iteration:   4540, Loss function: 3.655, Average Loss: 3.617, avg. samples / sec: 28063.26
Iteration:   4540, Loss function: 3.272, Average Loss: 3.602, avg. samples / sec: 28055.81
Iteration:   4540, Loss function: 3.337, Average Loss: 3.613, avg. samples / sec: 28034.81
Iteration:   4540, Loss function: 3.954, Average Loss: 3.600, avg. samples / sec: 28013.61
Iteration:   4560, Loss function: 3.103, Average Loss: 3.593, avg. samples / sec: 28084.89
Iteration:   4560, Loss function: 2.682, Average Loss: 3.611, avg. samples / sec: 28043.28
Iteration:   4560, Loss function: 3.059, Average Loss: 3.606, avg. samples / sec: 28049.09
Iteration:   4560, Loss function: 3.108, Average Loss: 3.595, avg. samples / sec: 28020.81
Iteration:   4580, Loss function: 3.843, Average Loss: 3.585, avg. samples / sec: 27979.96
Iteration:   4580, Loss function: 2.916, Average Loss: 3.600, avg. samples / sec: 27975.03
Iteration:   4580, Loss function: 3.295, Average Loss: 3.607, avg. samples / sec: 27949.11
Iteration:   4580, Loss function: 3.658, Average Loss: 3.589, avg. samples / sec: 27961.72
:::MLL 1558579196.930 epoch_stop: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 819}}
:::MLL 1558579196.931 epoch_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 673}}
:::MLL 1558579197.097 eval_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.16 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.16 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.16 s
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 1.16 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.56s)
DONE (t=2.77s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.23073
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.39372
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23408
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.06132
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24365
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.37341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22266
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32471
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.34118
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.10170
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.37145
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.53099
Current AP: 0.23073 AP goal: 0.23000
:::MLL 1558579201.594 eval_accuracy: {"value": 0.23072648014655223, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 389}}
:::MLL 1558579201.675 eval_stop: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 392}}
:::MLL 1558579201.688 block_stop: {"value": null, "metadata": {"first_epoch_num": 55, "file": "train.py", "lineno": 804}}
:::MLL 1558579202.604 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 849}}

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x

Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '24', '--eval-batch-size', '40', '--warmup', '850', '--num-workers', '3', '--lr', '2.9e-3', '--wd', '1.7e-4', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-05-23 02:40:12 AM
RESULT,SINGLE_STAGE_DETECTOR,,351,nvidia,2019-05-23 02:34:21 AM
ENDING TIMING RUN AT 2019-05-23 02:40:12 AM
RESULT,SINGLE_STAGE_DETECTOR,,351,nvidia,2019-05-23 02:34:21 AM
ENDING TIMING RUN AT 2019-05-23 02:40:12 AM
RESULT,SINGLE_STAGE_DETECTOR,,351,nvidia,2019-05-23 02:34:21 AM
ENDING TIMING RUN AT 2019-05-23 02:40:12 AM
RESULT,SINGLE_STAGE_DETECTOR,,351,nvidia,2019-05-23 02:34:21 AM
