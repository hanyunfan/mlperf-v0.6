Beginning trial 5 of 5
Gathering sys log on circe-n001
:::MLL 1558639565.391 submission_benchmark: {"value": "ssd", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1558639565.392 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known ssd keys.
:::MLL 1558639565.392 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1558639565.393 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1558639565.393 submission_platform: {"value": "15xNVIDIA DGX-2H", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1558639565.393 submission_entry: {"value": "{'hardware': 'NVIDIA DGX-2H', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': 'Ubuntu 18.04.1 LTS / NVIDIA DGX Server 4.0.4 4.0.5', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.4-2.0.7', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '15', 'cpu': '2x Intel(R) Xeon(R) Platinum 8174 CPU @ 3.10GHz', 'num_cores': '48', 'num_vcpus': '96', 'accelerator': 'Tesla V100-SXM3-32GB-H', 'num_accelerators': '16', 'sys_mem_size': '1510 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '2x 894.3G + 8x 3.5T', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27800 Family [ConnectX-5]', 'num_network_cards': '10', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1558639565.394 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1558639565.394 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
:::MLL 1558639568.278 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639568.256 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639568.260 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639568.283 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639568.289 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639568.273 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639568.287 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639568.301 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639568.297 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639568.296 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639568.308 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639568.298 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639568.310 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639568.322 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639568.324 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node circe-n001
+ pids+=($!)
+ set +x
Launching on node circe-n002
+ pids+=($!)
+ set +x
Launching on node circe-n003
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ pids+=($!)
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n001
+ set +x
Launching on node circe-n004
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n002
+ pids+=($!)
+ set +x
Launching on node circe-n005
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n003
+ srun --mem=0 -N 1 -n 1 -w circe-n001 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=0 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w circe-n002 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=1 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
+ pids+=($!)
+ srun --mem=0 -N 1 -n 1 -w circe-n003 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=2 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
+ set +x
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
Launching on node circe-n006
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n004
+ pids+=($!)
+ set +x
Launching on node circe-n007
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n005
+ srun --mem=0 -N 1 -n 1 -w circe-n004 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=3 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n008
+ srun --mem=0 -N 1 -n 1 -w circe-n005 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=4 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n006
+ pids+=($!)
+ set +x
Launching on node circe-n009
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n007
+ srun --mem=0 -N 1 -n 1 -w circe-n006 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=5 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n010
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n008
+ srun --mem=0 -N 1 -n 1 -w circe-n007 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=6 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n011
+ srun --mem=0 -N 1 -n 1 -w circe-n008 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=7 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n009
+ pids+=($!)
+ set +x
Launching on node circe-n012
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n010
+ pids+=($!)
+ set +x
Launching on node circe-n013
+ srun --mem=0 -N 1 -n 1 -w circe-n009 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=8 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n011
+ srun --mem=0 -N 1 -n 1 -w circe-n010 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=9 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n014
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n012
+ srun --mem=0 -N 1 -n 1 -w circe-n011 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=10 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n015
+ srun --mem=0 -N 1 -n 1 -w circe-n012 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=11 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n013
+ pids+=($!)
+ set +x
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n014
+ srun --mem=0 -N 1 -n 1 -w circe-n013 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=12 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w circe-n014 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=13 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n015
+ srun --mem=0 -N 1 -n 1 -w circe-n015 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=14 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=0 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=3 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=2 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=10 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=13 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=5 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=8 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=4 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=14 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=12 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=6 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=9 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=7 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=1 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=11 --master_addr=10.0.1.1 --master_port=4715
STARTING TIMING RUN AT 2019-05-23 07:26:08 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=0 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:26:08 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=3 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
STARTING TIMING RUN AT 2019-05-23 07:26:08 PM
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=2 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:26:08 PM
running benchmark
STARTING TIMING RUN AT 2019-05-23 07:26:08 PM
STARTING TIMING RUN AT 2019-05-23 07:26:08 PM
running benchmark
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ NUMEPOCHS=80
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=8 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ export DATASET_DIR=/data/coco2017
+ echo 'running benchmark'
+ DATASET_DIR=/data/coco2017
+ export DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ DATASET_DIR=/data/coco2017
+ TORCH_MODEL_ZOO=/data/torchvision
+ export TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=5 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=4 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:26:08 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
STARTING TIMING RUN AT 2019-05-23 07:26:08 PM
STARTING TIMING RUN AT 2019-05-23 07:26:08 PM
running benchmark
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=14 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ NUMEPOCHS=80
+ export TORCH_MODEL_ZOO=/data/torchvision
+ echo 'running benchmark'
+ TORCH_MODEL_ZOO=/data/torchvision
+ export DATASET_DIR=/data/coco2017
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=12 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=10 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:26:08 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=13 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
STARTING TIMING RUN AT 2019-05-23 07:26:08 PM
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=6 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:26:08 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=7 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:26:08 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ NUMEPOCHS=80
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=9 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:26:08 PM
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=1 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:26:08 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=11 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding::::MLL 1558639578.797 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.797 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.797 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.797 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.798 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.798 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.798 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.799 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.799 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.799 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639578.799 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.800 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.800 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.800 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.800 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639578.801 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639578.792 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.793 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.794 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.795 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.795 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.795 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.795 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.796 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.796 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.796 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.796 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.796 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.797 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.797 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639578.797 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.797 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639578.809 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.809 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.810 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.811 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.812 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.812 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.812 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.812 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.812 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.812 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.813 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.813 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.813 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.813 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.813 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.814 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639578.812 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.813 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.815 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.816 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.816 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.817 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.817 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.817 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.817 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.817 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.818 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.818 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.818 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.818 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.818 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.818 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639578.862 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.863 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.863 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.863 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.863 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.863 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.864 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639578.865 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.865 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.865 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.866 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.866 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.866 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.866 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.866 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.866 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
Binding::::MLL 1558639578.862 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
Binding::::MLL 1558639578.865 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.865 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.865 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.865 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.865 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.866 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639578.863 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639578.864 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.867 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.867 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639578.867 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.867 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.867 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.867 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.865 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.868 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.868 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
Binding::::MLL 1558639578.868 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.869 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.869 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.866 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639578.868 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.871 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.869 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.869 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.869 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.869 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.870 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.870 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.870 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.870 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.871 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639578.871 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.875 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.875 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639578.877 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.877 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.878 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.878 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.878 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.879 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.879 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.879 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.880 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.880 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.880 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.880 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639578.909 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.910 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.910 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.911 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.911 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.911 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.912 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.912 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.912 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.912 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.913 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.913 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.913 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.913 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.913 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.913 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639578.932 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.933 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.935 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.936 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.936 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.936 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.936 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.937 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.937 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.937 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.937 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.937 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.937 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.938 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.938 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.938 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639578.924 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.925 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.925 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.926 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.926 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.926 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.926 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.926 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.927 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.927 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.927 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.927 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.927 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.927 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.928 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639578.928 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639578.942 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.944 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
Binding::::MLL 1558639578.965 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.965 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.965 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.966 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.966 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.966 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.967 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.946 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.946 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.967 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.967 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.968 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.968 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.968 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.968 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.948 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.969 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.969 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.949 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.949 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.969 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.949 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.950 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.950 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.950 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.950 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.950 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639578.951 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.951 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.951 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639578.968 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.969 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.969 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.970 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.971 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.972 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.973 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.974 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.974 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.974 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.975 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.975 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.976 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639578.976 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.976 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.976 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639578.976 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.976 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.977 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.978 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639578.979 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.980 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.980 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.981 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.981 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.981 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.981 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.982 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639578.982 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.983 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639578.983 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639578.983 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
0 Using seed = 3160669894
1 Using seed = 3160669895
3 Using seed = 3160669897
4 Using seed = 3160669898
5 Using seed = 3160669899
2 Using seed = 3160669896
:::MLL 1558639612.257 max_samples: {"value": 1, "metadata": {"file": "utils.py", "lineno": 465}}
9 Using seed = 3160669903
11 Using seed = 3160669905
8 Using seed = 3160669902
7 Using seed = 3160669901
6 Using seed = 3160669900
23 Using seed = 3160669917
22 Using seed = 3160669916
27 Using seed = 3160669921
16 Using seed = 3160669910
21 Using seed = 3160669915
28 Using seed = 3160669922
29 Using seed = 3160669923
26 Using seed = 3160669920
30 Using seed = 3160669924
19 Using seed = 3160669913
31 Using seed = 3160669925
17 Using seed = 3160669911
25 Using seed = 3160669919
20 Using seed = 3160669914
18 Using seed = 3160669912
24 Using seed = 3160669918
42 Using seed = 3160669936
45 Using seed = 3160669939
46 Using seed = 3160669940
47 Using seed = 3160669941
34 Using seed = 3160669928
35 Using seed = 3160669929
32 Using seed = 3160669926
37 Using seed = 3160669931
36 Using seed = 3160669930
33 Using seed = 3160669927
44 Using seed = 3160669938
43 Using seed = 3160669937
41 Using seed = 3160669935
40 Using seed = 3160669934
39 Using seed = 3160669933
38 Using seed = 3160669932
58 Using seed = 3160669952
59 Using seed = 3160669953
54 Using seed = 3160669948
62 Using seed = 3160669956
60 Using seed = 3160669954
48 Using seed = 3160669942
63 Using seed = 3160669957
56 Using seed = 3160669950
57 Using seed = 3160669951
49 Using seed = 3160669943
61 Using seed = 3160669955
52 Using seed = 3160669946
51 Using seed = 3160669945
50 Using seed = 3160669944
53 Using seed = 3160669947
55 Using seed = 3160669949
68 Using seed = 3160669962
64 Using seed = 3160669958
79 Using seed = 3160669973
77 Using seed = 3160669971
70 Using seed = 3160669964
73 Using seed = 3160669967
67 Using seed = 3160669961
75 Using seed = 3160669969
65 Using seed = 3160669959
78 Using seed = 3160669972
76 Using seed = 3160669970
66 Using seed = 3160669960
74 Using seed = 3160669968
69 Using seed = 3160669963
72 Using seed = 3160669966
71 Using seed = 3160669965
86 Using seed = 3160669980
85 Using seed = 3160669979
83 Using seed = 3160669977
87 Using seed = 3160669981
84 Using seed = 3160669978
81 Using seed = 3160669975
80 Using seed = 3160669974
82 Using seed = 3160669976
95 Using seed = 3160669989
94 Using seed = 3160669988
93 Using seed = 3160669987
92 Using seed = 3160669986
89 Using seed = 3160669983
90 Using seed = 3160669984
91 Using seed = 3160669985
88 Using seed = 3160669982
109 Using seed = 3160670003
102 Using seed = 3160669996
105 Using seed = 3160669999
108 Using seed = 3160670002
104 Using seed = 3160669998
101 Using seed = 3160669995
110 Using seed = 3160670004
99 Using seed = 3160669993
106 Using seed = 3160670000
97 Using seed = 3160669991
111 Using seed = 3160670005
98 Using seed = 3160669992
100 Using seed = 3160669994
107 Using seed = 3160670001
96 Using seed = 3160669990
103 Using seed = 3160669997
118 Using seed = 3160670012
115 Using seed = 3160670009
116 Using seed = 3160670010
112 Using seed = 3160670006
114 Using seed = 3160670008
121 Using seed = 3160670015
126 Using seed = 3160670020
113 Using seed = 3160670007
122 Using seed = 3160670016
127 Using seed = 3160670021
123 Using seed = 3160670017
125 Using seed = 3160670019
120 Using seed = 3160670014
124 Using seed = 3160670018
117 Using seed = 3160670011
119 Using seed = 3160670013
140 Using seed = 3160670034
138 Using seed = 3160670032
128 Using seed = 3160670022
132 Using seed = 3160670026
137 Using seed = 3160670031
141 Using seed = 3160670035
143 Using seed = 3160670037
130 Using seed = 3160670024
142 Using seed = 3160670036
139 Using seed = 3160670033
131 Using seed = 3160670025
134 Using seed = 3160670028
129 Using seed = 3160670023
133 Using seed = 3160670027
136 Using seed = 3160670030
135 Using seed = 3160670029
150 Using seed = 3160670044
158 Using seed = 3160670052
148 Using seed = 3160670042
159 Using seed = 3160670053
144 Using seed = 3160670038
156 Using seed = 3160670050
153 Using seed = 3160670047
155 Using seed = 3160670049
154 Using seed = 3160670048
152 Using seed = 3160670046
147 Using seed = 3160670041
145 Using seed = 3160670039
157 Using seed = 3160670051
146 Using seed = 3160670040
149 Using seed = 3160670043
151 Using seed = 3160670045
162 Using seed = 3160670056
163 Using seed = 3160670057
166 Using seed = 3160670060
165 Using seed = 3160670059
164 Using seed = 3160670058
170 Using seed = 3160670064
175 Using seed = 3160670069
161 Using seed = 3160670055
172 Using seed = 3160670066
171 Using seed = 3160670065
169 Using seed = 3160670063
174 Using seed = 3160670068
173 Using seed = 3160670067
160 Using seed = 3160670054
168 Using seed = 3160670062
167 Using seed = 3160670061
191 Using seed = 3160670085
190 Using seed = 3160670084
187 Using seed = 3160670081
184 Using seed = 3160670078
189 Using seed = 3160670083
186 Using seed = 3160670080
177 Using seed = 3160670071
182 Using seed = 3160670076
179 Using seed = 3160670073
188 Using seed = 3160670082
180 Using seed = 3160670074
176 Using seed = 3160670070
178 Using seed = 3160670072
181 Using seed = 3160670075
185 Using seed = 3160670079
183 Using seed = 3160670077
203 Using seed = 3160670097
204 Using seed = 3160670098
207 Using seed = 3160670101
198 Using seed = 3160670092
202 Using seed = 3160670096
206 Using seed = 3160670100
205 Using seed = 3160670099
192 Using seed = 3160670086
195 Using seed = 3160670089
193 Using seed = 3160670087
194 Using seed = 3160670088
197 Using seed = 3160670091
199 Using seed = 3160670093
200 Using seed = 3160670094
201 Using seed = 3160670095
196 Using seed = 3160670090
220 Using seed = 3160670114
221 Using seed = 3160670115
218 Using seed = 3160670112
217 Using seed = 3160670111
219 Using seed = 3160670113
214 Using seed = 3160670108
209 Using seed = 3160670103
208 Using seed = 3160670102
211 Using seed = 3160670105
212 Using seed = 3160670106
216 Using seed = 3160670110
223 Using seed = 3160670117
222 Using seed = 3160670116
210 Using seed = 3160670104
213 Using seed = 3160670107
215 Using seed = 3160670109
237 Using seed = 3160670131
234 Using seed = 3160670128
233 Using seed = 3160670127
236 Using seed = 3160670130
238 Using seed = 3160670132
239 Using seed = 3160670133
235 Using seed = 3160670129
227 Using seed = 3160670121
230 Using seed = 3160670124
225 Using seed = 3160670119
224 Using seed = 3160670118
232 Using seed = 3160670126
228 Using seed = 3160670122
226 Using seed = 3160670120
231 Using seed = 3160670125
229 Using seed = 3160670123
12 Using seed = 3160669906
13 Using seed = 3160669907
14 Using seed = 3160669908
15 Using seed = 3160669909
10 Using seed = 3160669904
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
:::MLL 1558639617.572 model_bn_span: {"value": 28, "metadata": {"file": "train.py", "lineno": 480}}
:::MLL 1558639617.573 global_batch_size: {"value": 1680, "metadata": {"file": "train.py", "lineno": 481}}
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
:::MLL 1558639617.591 opt_base_learning_rate: {"value": 0.1625, "metadata": {"file": "train.py", "lineno": 511}}
:::MLL 1558639617.591 opt_weight_decay: {"value": 0.0002, "metadata": {"file": "train.py", "lineno": 513}}
:::MLL 1558639617.591 opt_learning_rate_warmup_steps: {"value": 1250, "metadata": {"file": "train.py", "lineno": 516}}
:::MLL 1558639617.591 opt_learning_rate_warmup_factor: {"value": 0, "metadata": {"file": "train.py", "lineno": 518}}
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
:::MLL 1558639624.621 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 604}}
:::MLL 1558639624.621 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 610}}
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
time_check a: 1558639626.323275566
time_check a: 1558639626.327993155
time_check a: 1558639626.335101366
time_check a: 1558639626.328719854
time_check a: 1558639626.361018419
time_check a: 1558639626.335711002
time_check a: 1558639626.327271461
time_check a: 1558639626.347023964
time_check a: 1558639626.339300394
time_check a: 1558639626.352220774
time_check a: 1558639626.348036289
time_check a: 1558639626.329852819
time_check a: 1558639626.348701000
time_check a: 1558639626.347391605
time_check a: 1558639626.365847349
time_check b: 1558639632.754067421
time_check b: 1558639633.068294764
time_check b: 1558639633.146918774
time_check b: 1558639633.145463705
time_check b: 1558639633.187940836
time_check b: 1558639633.187854290
time_check b: 1558639633.204433680
time_check b: 1558639633.235531569
time_check b: 1558639633.233086824
time_check b: 1558639633.255573988
time_check b: 1558639633.254292011
time_check b: 1558639633.283141136
time_check b: 1558639633.294287682
time_check b: 1558639633.305251837
time_check b: 1558639633.349795341
:::MLL 1558639634.448 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 32.74606450292497, "file": "train.py", "lineno": 669}}
:::MLL 1558639634.449 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 673}}
Iteration:      0, Loss function: 23.008, Average Loss: 0.023, avg. samples / sec: 117.49
Iteration:      0, Loss function: 22.322, Average Loss: 0.022, avg. samples / sec: 108.58
Iteration:      0, Loss function: 21.979, Average Loss: 0.022, avg. samples / sec: 120.00
Iteration:      0, Loss function: 22.401, Average Loss: 0.022, avg. samples / sec: 116.87
Iteration:      0, Loss function: 22.528, Average Loss: 0.023, avg. samples / sec: 114.92
Iteration:      0, Loss function: 22.983, Average Loss: 0.023, avg. samples / sec: 116.58
Iteration:      0, Loss function: 22.838, Average Loss: 0.023, avg. samples / sec: 98.97
Iteration:      0, Loss function: 22.632, Average Loss: 0.023, avg. samples / sec: 118.92
Iteration:      0, Loss function: 22.177, Average Loss: 0.022, avg. samples / sec: 118.12
Iteration:      0, Loss function: 22.490, Average Loss: 0.022, avg. samples / sec: 117.49
Iteration:      0, Loss function: 23.130, Average Loss: 0.023, avg. samples / sec: 120.30
Iteration:      0, Loss function: 22.656, Average Loss: 0.023, avg. samples / sec: 108.32
Iteration:      0, Loss function: 22.131, Average Loss: 0.022, avg. samples / sec: 119.77
Iteration:      0, Loss function: 22.676, Average Loss: 0.023, avg. samples / sec: 117.60
Iteration:      0, Loss function: 22.505, Average Loss: 0.023, avg. samples / sec: 118.11
Iteration:     20, Loss function: 21.120, Average Loss: 0.441, avg. samples / sec: 38777.46
Iteration:     20, Loss function: 20.346, Average Loss: 0.441, avg. samples / sec: 39186.90
Iteration:     20, Loss function: 22.497, Average Loss: 0.444, avg. samples / sec: 38608.69
Iteration:     20, Loss function: 21.070, Average Loss: 0.442, avg. samples / sec: 38916.15
Iteration:     20, Loss function: 21.042, Average Loss: 0.444, avg. samples / sec: 37989.30
Iteration:     20, Loss function: 20.215, Average Loss: 0.443, avg. samples / sec: 38323.21
Iteration:     20, Loss function: 21.910, Average Loss: 0.445, avg. samples / sec: 38357.21
Iteration:     20, Loss function: 20.537, Average Loss: 0.446, avg. samples / sec: 38364.84
Iteration:     20, Loss function: 20.515, Average Loss: 0.444, avg. samples / sec: 38487.70
Iteration:     20, Loss function: 19.958, Average Loss: 0.442, avg. samples / sec: 37909.15
Iteration:     20, Loss function: 20.864, Average Loss: 0.446, avg. samples / sec: 37929.98
Iteration:     20, Loss function: 20.811, Average Loss: 0.443, avg. samples / sec: 38989.53
Iteration:     20, Loss function: 20.313, Average Loss: 0.442, avg. samples / sec: 38082.26
Iteration:     20, Loss function: 20.723, Average Loss: 0.444, avg. samples / sec: 38422.60
Iteration:     20, Loss function: 20.279, Average Loss: 0.444, avg. samples / sec: 38095.47
Iteration:     40, Loss function: 19.021, Average Loss: 0.836, avg. samples / sec: 55706.82
Iteration:     40, Loss function: 19.152, Average Loss: 0.834, avg. samples / sec: 54769.67
Iteration:     40, Loss function: 18.540, Average Loss: 0.833, avg. samples / sec: 55278.33
Iteration:     40, Loss function: 18.858, Average Loss: 0.831, avg. samples / sec: 54758.27
Iteration:     40, Loss function: 19.374, Average Loss: 0.833, avg. samples / sec: 55272.37
Iteration:     40, Loss function: 20.048, Average Loss: 0.838, avg. samples / sec: 55809.42
Iteration:     40, Loss function: 19.199, Average Loss: 0.837, avg. samples / sec: 55672.38
Iteration:     40, Loss function: 18.867, Average Loss: 0.835, avg. samples / sec: 55148.45
Iteration:     40, Loss function: 18.965, Average Loss: 0.836, avg. samples / sec: 54975.55
Iteration:     40, Loss function: 19.258, Average Loss: 0.830, avg. samples / sec: 55473.31
Iteration:     40, Loss function: 18.725, Average Loss: 0.835, avg. samples / sec: 54879.13
Iteration:     40, Loss function: 18.339, Average Loss: 0.834, avg. samples / sec: 54847.16
Iteration:     40, Loss function: 20.849, Average Loss: 0.838, avg. samples / sec: 54718.79
Iteration:     40, Loss function: 18.751, Average Loss: 0.831, avg. samples / sec: 55059.14
Iteration:     40, Loss function: 18.564, Average Loss: 0.837, avg. samples / sec: 54455.44
Iteration:     60, Loss function: 12.117, Average Loss: 1.110, avg. samples / sec: 58316.29
Iteration:     60, Loss function: 13.227, Average Loss: 1.102, avg. samples / sec: 58178.76
Iteration:     60, Loss function: 12.833, Average Loss: 1.109, avg. samples / sec: 57752.21
Iteration:     60, Loss function: 12.932, Average Loss: 1.104, avg. samples / sec: 57753.04
Iteration:     60, Loss function: 14.065, Average Loss: 1.102, avg. samples / sec: 57290.92
Iteration:     60, Loss function: 12.748, Average Loss: 1.111, avg. samples / sec: 57750.48
Iteration:     60, Loss function: 12.948, Average Loss: 1.102, avg. samples / sec: 57382.11
Iteration:     60, Loss function: 12.728, Average Loss: 1.106, avg. samples / sec: 57688.14
Iteration:     60, Loss function: 11.431, Average Loss: 1.102, avg. samples / sec: 57696.91
Iteration:     60, Loss function: 12.938, Average Loss: 1.099, avg. samples / sec: 57680.16
Iteration:     60, Loss function: 14.256, Average Loss: 1.114, avg. samples / sec: 57384.89
Iteration:     60, Loss function: 13.653, Average Loss: 1.104, avg. samples / sec: 57438.02
Iteration:     60, Loss function: 14.240, Average Loss: 1.106, avg. samples / sec: 57351.91
Iteration:     60, Loss function: 13.574, Average Loss: 1.109, avg. samples / sec: 56899.61
Iteration:     60, Loss function: 12.691, Average Loss: 1.117, avg. samples / sec: 56042.19
:::MLL 1558639637.561 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 819}}
:::MLL 1558639637.561 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 673}}
Iteration:     80, Loss function: 11.444, Average Loss: 1.302, avg. samples / sec: 58046.60
Iteration:     80, Loss function: 10.956, Average Loss: 1.308, avg. samples / sec: 58423.13
Iteration:     80, Loss function: 11.466, Average Loss: 1.303, avg. samples / sec: 57763.88
Iteration:     80, Loss function: 11.151, Average Loss: 1.312, avg. samples / sec: 58934.68
Iteration:     80, Loss function: 10.641, Average Loss: 1.312, avg. samples / sec: 57876.42
Iteration:     80, Loss function: 10.991, Average Loss: 1.319, avg. samples / sec: 59392.28
Iteration:     80, Loss function: 10.617, Average Loss: 1.302, avg. samples / sec: 57775.60
Iteration:     80, Loss function: 11.014, Average Loss: 1.300, avg. samples / sec: 57825.60
Iteration:     80, Loss function: 11.157, Average Loss: 1.307, avg. samples / sec: 57710.68
Iteration:     80, Loss function: 10.900, Average Loss: 1.313, avg. samples / sec: 58086.99
Iteration:     80, Loss function: 10.902, Average Loss: 1.314, avg. samples / sec: 57477.22
Iteration:     80, Loss function: 10.644, Average Loss: 1.307, avg. samples / sec: 58162.65
Iteration:     80, Loss function: 9.983, Average Loss: 1.312, avg. samples / sec: 57533.53
Iteration:     80, Loss function: 9.916, Average Loss: 1.306, avg. samples / sec: 57424.78
Iteration:     80, Loss function: 10.794, Average Loss: 1.314, avg. samples / sec: 57377.57
Iteration:    100, Loss function: 8.205, Average Loss: 1.476, avg. samples / sec: 58626.92
Iteration:    100, Loss function: 8.784, Average Loss: 1.475, avg. samples / sec: 58307.70
Iteration:    100, Loss function: 8.955, Average Loss: 1.471, avg. samples / sec: 58381.57
Iteration:    100, Loss function: 9.475, Average Loss: 1.472, avg. samples / sec: 58343.41
Iteration:    100, Loss function: 9.296, Average Loss: 1.484, avg. samples / sec: 58177.63
Iteration:    100, Loss function: 9.247, Average Loss: 1.484, avg. samples / sec: 58223.99
Iteration:    100, Loss function: 9.810, Average Loss: 1.485, avg. samples / sec: 58398.55
Iteration:    100, Loss function: 9.204, Average Loss: 1.482, avg. samples / sec: 58232.44
Iteration:    100, Loss function: 10.168, Average Loss: 1.483, avg. samples / sec: 58098.84
Iteration:    100, Loss function: 9.395, Average Loss: 1.489, avg. samples / sec: 58191.06
Iteration:    100, Loss function: 9.729, Average Loss: 1.489, avg. samples / sec: 58184.74
Iteration:    100, Loss function: 9.109, Average Loss: 1.485, avg. samples / sec: 58325.18
Iteration:    100, Loss function: 9.202, Average Loss: 1.495, avg. samples / sec: 57974.89
Iteration:    100, Loss function: 9.699, Average Loss: 1.472, avg. samples / sec: 57735.84
Iteration:    100, Loss function: 9.871, Average Loss: 1.491, avg. samples / sec: 58208.82
Iteration:    120, Loss function: 8.390, Average Loss: 1.644, avg. samples / sec: 59376.29
Iteration:    120, Loss function: 9.265, Average Loss: 1.628, avg. samples / sec: 58791.66
Iteration:    120, Loss function: 9.187, Average Loss: 1.635, avg. samples / sec: 58898.89
Iteration:    120, Loss function: 10.465, Average Loss: 1.623, avg. samples / sec: 59180.66
Iteration:    120, Loss function: 7.821, Average Loss: 1.622, avg. samples / sec: 58594.16
Iteration:    120, Loss function: 8.491, Average Loss: 1.626, avg. samples / sec: 58501.10
Iteration:    120, Loss function: 9.147, Average Loss: 1.633, avg. samples / sec: 58953.61
Iteration:    120, Loss function: 8.845, Average Loss: 1.639, avg. samples / sec: 58751.42
Iteration:    120, Loss function: 8.848, Average Loss: 1.648, avg. samples / sec: 58974.73
Iteration:    120, Loss function: 8.599, Average Loss: 1.621, avg. samples / sec: 58542.07
Iteration:    120, Loss function: 8.086, Average Loss: 1.635, avg. samples / sec: 58637.67
Iteration:    120, Loss function: 9.636, Average Loss: 1.638, avg. samples / sec: 58595.93
Iteration:    120, Loss function: 9.099, Average Loss: 1.638, avg. samples / sec: 58436.18
Iteration:    120, Loss function: 9.739, Average Loss: 1.635, avg. samples / sec: 58500.22
Iteration:    120, Loss function: 9.824, Average Loss: 1.637, avg. samples / sec: 58612.90
:::MLL 1558639639.579 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 819}}
:::MLL 1558639639.580 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 673}}
Iteration:    140, Loss function: 7.681, Average Loss: 1.773, avg. samples / sec: 58040.94
Iteration:    140, Loss function: 10.379, Average Loss: 1.767, avg. samples / sec: 58030.37
Iteration:    140, Loss function: 9.491, Average Loss: 1.769, avg. samples / sec: 57952.10
Iteration:    140, Loss function: 9.071, Average Loss: 1.781, avg. samples / sec: 58298.03
Iteration:    140, Loss function: 8.850, Average Loss: 1.791, avg. samples / sec: 58007.20
Iteration:    140, Loss function: 8.729, Average Loss: 1.776, avg. samples / sec: 57876.23
Iteration:    140, Loss function: 8.154, Average Loss: 1.781, avg. samples / sec: 57922.64
Iteration:    140, Loss function: 9.359, Average Loss: 1.778, avg. samples / sec: 58165.38
Iteration:    140, Loss function: 8.308, Average Loss: 1.777, avg. samples / sec: 57863.47
Iteration:    140, Loss function: 8.594, Average Loss: 1.789, avg. samples / sec: 57528.65
Iteration:    140, Loss function: 9.112, Average Loss: 1.786, avg. samples / sec: 57898.94
Iteration:    140, Loss function: 8.663, Average Loss: 1.773, avg. samples / sec: 57679.08
Iteration:    140, Loss function: 8.912, Average Loss: 1.771, avg. samples / sec: 57461.87
Iteration:    140, Loss function: 8.859, Average Loss: 1.765, avg. samples / sec: 57626.53
Iteration:    140, Loss function: 8.757, Average Loss: 1.779, avg. samples / sec: 57753.32
Iteration:    160, Loss function: 8.615, Average Loss: 1.931, avg. samples / sec: 58622.89
Iteration:    160, Loss function: 7.675, Average Loss: 1.916, avg. samples / sec: 58670.14
Iteration:    160, Loss function: 9.086, Average Loss: 1.909, avg. samples / sec: 58473.28
Iteration:    160, Loss function: 9.036, Average Loss: 1.907, avg. samples / sec: 58442.10
Iteration:    160, Loss function: 9.151, Average Loss: 1.917, avg. samples / sec: 58516.96
Iteration:    160, Loss function: 8.959, Average Loss: 1.912, avg. samples / sec: 58342.68
Iteration:    160, Loss function: 9.177, Average Loss: 1.920, avg. samples / sec: 58380.84
Iteration:    160, Loss function: 8.621, Average Loss: 1.904, avg. samples / sec: 58822.48
Iteration:    160, Loss function: 8.206, Average Loss: 1.904, avg. samples / sec: 58739.17
Iteration:    160, Loss function: 9.482, Average Loss: 1.916, avg. samples / sec: 58386.99
Iteration:    160, Loss function: 9.418, Average Loss: 1.935, avg. samples / sec: 58580.35
Iteration:    160, Loss function: 8.119, Average Loss: 1.917, avg. samples / sec: 58361.48
Iteration:    160, Loss function: 9.116, Average Loss: 1.928, avg. samples / sec: 58629.01
Iteration:    160, Loss function: 9.236, Average Loss: 1.917, avg. samples / sec: 58733.72
Iteration:    160, Loss function: 9.139, Average Loss: 1.908, avg. samples / sec: 58524.66
Iteration:    180, Loss function: 8.335, Average Loss: 2.050, avg. samples / sec: 56225.08
Iteration:    180, Loss function: 9.291, Average Loss: 2.040, avg. samples / sec: 56281.55
Iteration:    180, Loss function: 9.222, Average Loss: 2.052, avg. samples / sec: 56260.84
Iteration:    180, Loss function: 8.707, Average Loss: 2.058, avg. samples / sec: 56311.24
Iteration:    180, Loss function: 9.375, Average Loss: 2.041, avg. samples / sec: 56072.16
Iteration:    180, Loss function: 8.023, Average Loss: 2.043, avg. samples / sec: 56110.78
Iteration:    180, Loss function: 8.429, Average Loss: 2.051, avg. samples / sec: 56250.60
Iteration:    180, Loss function: 8.844, Average Loss: 2.053, avg. samples / sec: 56100.26
Iteration:    180, Loss function: 9.528, Average Loss: 2.038, avg. samples / sec: 55965.89
Iteration:    180, Loss function: 9.135, Average Loss: 2.045, avg. samples / sec: 55903.53
Iteration:    180, Loss function: 9.555, Average Loss: 2.064, avg. samples / sec: 55846.93
Iteration:    180, Loss function: 7.741, Average Loss: 2.042, avg. samples / sec: 56255.04
Iteration:    180, Loss function: 8.316, Average Loss: 2.052, avg. samples / sec: 56027.35
Iteration:    180, Loss function: 8.784, Average Loss: 2.038, avg. samples / sec: 55957.92
Iteration:    180, Loss function: 10.183, Average Loss: 2.069, avg. samples / sec: 56002.79
Iteration:    200, Loss function: 7.930, Average Loss: 2.176, avg. samples / sec: 59208.08
Iteration:    200, Loss function: 7.800, Average Loss: 2.182, avg. samples / sec: 59222.69
Iteration:    200, Loss function: 8.725, Average Loss: 2.195, avg. samples / sec: 59540.50
Iteration:    200, Loss function: 7.337, Average Loss: 2.180, avg. samples / sec: 59203.43
Iteration:    200, Loss function: 8.016, Average Loss: 2.167, avg. samples / sec: 59339.51
Iteration:    200, Loss function: 8.237, Average Loss: 2.179, avg. samples / sec: 59236.05
Iteration:    200, Loss function: 8.734, Average Loss: 2.164, avg. samples / sec: 59405.42
Iteration:    200, Loss function: 8.366, Average Loss: 2.172, avg. samples / sec: 59263.58
Iteration:    200, Loss function: 8.134, Average Loss: 2.166, avg. samples / sec: 59297.29
Iteration:    200, Loss function: 8.659, Average Loss: 2.166, avg. samples / sec: 59153.53
Iteration:    200, Loss function: 8.703, Average Loss: 2.170, avg. samples / sec: 59121.64
Iteration:    200, Loss function: 7.425, Average Loss: 2.173, avg. samples / sec: 59099.11
Iteration:    200, Loss function: 7.885, Average Loss: 2.193, avg. samples / sec: 59146.23
Iteration:    200, Loss function: 7.527, Average Loss: 2.163, avg. samples / sec: 58924.53
Iteration:    200, Loss function: 8.961, Average Loss: 2.183, avg. samples / sec: 59113.29
:::MLL 1558639641.604 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 819}}
:::MLL 1558639641.605 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 673}}
Iteration:    220, Loss function: 8.896, Average Loss: 2.319, avg. samples / sec: 58542.87
Iteration:    220, Loss function: 7.527, Average Loss: 2.299, avg. samples / sec: 58484.34
Iteration:    220, Loss function: 8.367, Average Loss: 2.283, avg. samples / sec: 58615.80
Iteration:    220, Loss function: 7.838, Average Loss: 2.313, avg. samples / sec: 58723.85
Iteration:    220, Loss function: 8.604, Average Loss: 2.290, avg. samples / sec: 58599.03
Iteration:    220, Loss function: 9.122, Average Loss: 2.299, avg. samples / sec: 58743.83
Iteration:    220, Loss function: 8.550, Average Loss: 2.285, avg. samples / sec: 58337.61
Iteration:    220, Loss function: 8.834, Average Loss: 2.284, avg. samples / sec: 58472.60
Iteration:    220, Loss function: 8.047, Average Loss: 2.300, avg. samples / sec: 58310.67
Iteration:    220, Loss function: 7.583, Average Loss: 2.295, avg. samples / sec: 58412.04
Iteration:    220, Loss function: 8.554, Average Loss: 2.293, avg. samples / sec: 58263.52
Iteration:    220, Loss function: 8.380, Average Loss: 2.280, avg. samples / sec: 58522.94
Iteration:    220, Loss function: 8.484, Average Loss: 2.290, avg. samples / sec: 58390.42
Iteration:    220, Loss function: 8.584, Average Loss: 2.285, avg. samples / sec: 58364.91
Iteration:    220, Loss function: 9.459, Average Loss: 2.292, avg. samples / sec: 58485.49
Iteration:    240, Loss function: 7.963, Average Loss: 2.402, avg. samples / sec: 59920.62
Iteration:    240, Loss function: 7.673, Average Loss: 2.406, avg. samples / sec: 60070.64
Iteration:    240, Loss function: 6.779, Average Loss: 2.427, avg. samples / sec: 59726.55
Iteration:    240, Loss function: 8.060, Average Loss: 2.395, avg. samples / sec: 59871.43
Iteration:    240, Loss function: 8.600, Average Loss: 2.401, avg. samples / sec: 59698.54
Iteration:    240, Loss function: 8.302, Average Loss: 2.400, avg. samples / sec: 59894.87
Iteration:    240, Loss function: 6.781, Average Loss: 2.411, avg. samples / sec: 59628.25
Iteration:    240, Loss function: 7.853, Average Loss: 2.407, avg. samples / sec: 59810.48
Iteration:    240, Loss function: 8.118, Average Loss: 2.397, avg. samples / sec: 59759.85
Iteration:    240, Loss function: 7.398, Average Loss: 2.423, avg. samples / sec: 59603.58
Iteration:    240, Loss function: 8.093, Average Loss: 2.395, avg. samples / sec: 59772.35
Iteration:    240, Loss function: 8.478, Average Loss: 2.415, avg. samples / sec: 59729.74
Iteration:    240, Loss function: 6.509, Average Loss: 2.410, avg. samples / sec: 59738.27
Iteration:    240, Loss function: 7.707, Average Loss: 2.398, avg. samples / sec: 59776.53
Iteration:    240, Loss function: 7.687, Average Loss: 2.410, avg. samples / sec: 59403.65
Iteration:    260, Loss function: 7.436, Average Loss: 2.506, avg. samples / sec: 57299.12
Iteration:    260, Loss function: 7.345, Average Loss: 2.511, avg. samples / sec: 57561.05
Iteration:    260, Loss function: 7.315, Average Loss: 2.516, avg. samples / sec: 57221.71
Iteration:    260, Loss function: 7.518, Average Loss: 2.530, avg. samples / sec: 57015.33
Iteration:    260, Loss function: 8.162, Average Loss: 2.512, avg. samples / sec: 57090.47
Iteration:    260, Loss function: 7.643, Average Loss: 2.513, avg. samples / sec: 57079.76
Iteration:    260, Loss function: 7.864, Average Loss: 2.508, avg. samples / sec: 56852.69
Iteration:    260, Loss function: 7.462, Average Loss: 2.511, avg. samples / sec: 56843.50
Iteration:    260, Loss function: 7.510, Average Loss: 2.497, avg. samples / sec: 57023.01
Iteration:    260, Loss function: 7.114, Average Loss: 2.507, avg. samples / sec: 56905.58
Iteration:    260, Loss function: 8.736, Average Loss: 2.519, avg. samples / sec: 57021.19
Iteration:    260, Loss function: 7.754, Average Loss: 2.498, avg. samples / sec: 56863.82
Iteration:    260, Loss function: 8.349, Average Loss: 2.525, avg. samples / sec: 56975.68
Iteration:    260, Loss function: 7.157, Average Loss: 2.502, avg. samples / sec: 56859.57
Iteration:    260, Loss function: 8.146, Average Loss: 2.499, avg. samples / sec: 56955.40
:::MLL 1558639643.610 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 819}}
:::MLL 1558639643.611 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 673}}
Iteration:    280, Loss function: 8.255, Average Loss: 2.620, avg. samples / sec: 59702.82
Iteration:    280, Loss function: 8.613, Average Loss: 2.643, avg. samples / sec: 59627.03
Iteration:    280, Loss function: 8.278, Average Loss: 2.625, avg. samples / sec: 59483.53
Iteration:    280, Loss function: 7.686, Average Loss: 2.627, avg. samples / sec: 59779.73
Iteration:    280, Loss function: 7.517, Average Loss: 2.608, avg. samples / sec: 59834.22
Iteration:    280, Loss function: 8.855, Average Loss: 2.635, avg. samples / sec: 59768.42
Iteration:    280, Loss function: 9.300, Average Loss: 2.608, avg. samples / sec: 59766.59
Iteration:    280, Loss function: 7.906, Average Loss: 2.620, avg. samples / sec: 59565.74
Iteration:    280, Loss function: 7.770, Average Loss: 2.623, avg. samples / sec: 59536.30
Iteration:    280, Loss function: 7.770, Average Loss: 2.601, avg. samples / sec: 59616.06
Iteration:    280, Loss function: 7.894, Average Loss: 2.624, avg. samples / sec: 59478.76
Iteration:    280, Loss function: 7.494, Average Loss: 2.609, avg. samples / sec: 59678.24
Iteration:    280, Loss function: 8.741, Average Loss: 2.613, avg. samples / sec: 59275.74
Iteration:    280, Loss function: 8.120, Average Loss: 2.620, avg. samples / sec: 59367.44
Iteration:    280, Loss function: 8.086, Average Loss: 2.616, avg. samples / sec: 59441.45
Iteration:    300, Loss function: 7.160, Average Loss: 2.699, avg. samples / sec: 57668.22
Iteration:    300, Loss function: 8.660, Average Loss: 2.705, avg. samples / sec: 57533.16
Iteration:    300, Loss function: 6.551, Average Loss: 2.706, avg. samples / sec: 57623.42
Iteration:    300, Loss function: 7.035, Average Loss: 2.716, avg. samples / sec: 57441.30
Iteration:    300, Loss function: 7.301, Average Loss: 2.714, avg. samples / sec: 57294.37
Iteration:    300, Loss function: 7.360, Average Loss: 2.719, avg. samples / sec: 57557.76
Iteration:    300, Loss function: 7.250, Average Loss: 2.713, avg. samples / sec: 57495.20
Iteration:    300, Loss function: 7.872, Average Loss: 2.714, avg. samples / sec: 57583.77
Iteration:    300, Loss function: 8.219, Average Loss: 2.710, avg. samples / sec: 57642.89
Iteration:    300, Loss function: 8.330, Average Loss: 2.737, avg. samples / sec: 57341.08
Iteration:    300, Loss function: 7.027, Average Loss: 2.716, avg. samples / sec: 57616.99
Iteration:    300, Loss function: 6.342, Average Loss: 2.717, avg. samples / sec: 57426.11
Iteration:    300, Loss function: 6.946, Average Loss: 2.702, avg. samples / sec: 57191.36
Iteration:    300, Loss function: 7.080, Average Loss: 2.721, avg. samples / sec: 57169.67
Iteration:    300, Loss function: 7.487, Average Loss: 2.741, avg. samples / sec: 56988.63
Iteration:    320, Loss function: 8.447, Average Loss: 2.802, avg. samples / sec: 58349.78
Iteration:    320, Loss function: 7.813, Average Loss: 2.827, avg. samples / sec: 58351.72
Iteration:    320, Loss function: 6.561, Average Loss: 2.794, avg. samples / sec: 58446.51
Iteration:    320, Loss function: 7.431, Average Loss: 2.813, avg. samples / sec: 58458.56
Iteration:    320, Loss function: 7.759, Average Loss: 2.803, avg. samples / sec: 58145.39
Iteration:    320, Loss function: 7.027, Average Loss: 2.805, avg. samples / sec: 58123.67
Iteration:    320, Loss function: 7.314, Average Loss: 2.829, avg. samples / sec: 58517.95
Iteration:    320, Loss function: 6.915, Average Loss: 2.807, avg. samples / sec: 58254.10
Iteration:    320, Loss function: 8.002, Average Loss: 2.789, avg. samples / sec: 57991.78
Iteration:    320, Loss function: 7.245, Average Loss: 2.805, avg. samples / sec: 58231.72
Iteration:    320, Loss function: 6.467, Average Loss: 2.796, avg. samples / sec: 58068.29
Iteration:    320, Loss function: 7.779, Average Loss: 2.795, avg. samples / sec: 58023.37
Iteration:    320, Loss function: 6.851, Average Loss: 2.804, avg. samples / sec: 58125.18
Iteration:    320, Loss function: 7.970, Average Loss: 2.809, avg. samples / sec: 57999.71
Iteration:    320, Loss function: 7.500, Average Loss: 2.798, avg. samples / sec: 57909.38
Iteration:    340, Loss function: 6.750, Average Loss: 2.886, avg. samples / sec: 56564.15
Iteration:    340, Loss function: 7.406, Average Loss: 2.897, avg. samples / sec: 56535.60
Iteration:    340, Loss function: 7.641, Average Loss: 2.911, avg. samples / sec: 56430.31
Iteration:    340, Loss function: 8.271, Average Loss: 2.888, avg. samples / sec: 56529.39
Iteration:    340, Loss function: 6.600, Average Loss: 2.892, avg. samples / sec: 56495.24
Iteration:    340, Loss function: 7.468, Average Loss: 2.902, avg. samples / sec: 56621.33
Iteration:    340, Loss function: 7.687, Average Loss: 2.888, avg. samples / sec: 56684.78
Iteration:    340, Loss function: 7.740, Average Loss: 2.878, avg. samples / sec: 56413.44
Iteration:    340, Loss function: 6.882, Average Loss: 2.880, avg. samples / sec: 56343.79
Iteration:    340, Loss function: 6.934, Average Loss: 2.897, avg. samples / sec: 56375.01
Iteration:    340, Loss function: 6.905, Average Loss: 2.887, avg. samples / sec: 56194.14
Iteration:    340, Loss function: 7.322, Average Loss: 2.902, avg. samples / sec: 56279.15
Iteration:    340, Loss function: 7.698, Average Loss: 2.887, avg. samples / sec: 56346.11
Iteration:    340, Loss function: 7.347, Average Loss: 2.923, avg. samples / sec: 56266.25
Iteration:    340, Loss function: 6.736, Average Loss: 2.891, avg. samples / sec: 56371.38
:::MLL 1558639645.653 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 819}}
:::MLL 1558639645.653 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 673}}
Iteration:    360, Loss function: 7.284, Average Loss: 3.012, avg. samples / sec: 58892.30
Iteration:    360, Loss function: 7.216, Average Loss: 2.985, avg. samples / sec: 58575.70
Iteration:    360, Loss function: 7.200, Average Loss: 2.968, avg. samples / sec: 58468.79
Iteration:    360, Loss function: 5.427, Average Loss: 2.977, avg. samples / sec: 58467.02
Iteration:    360, Loss function: 8.132, Average Loss: 2.969, avg. samples / sec: 58702.30
Iteration:    360, Loss function: 6.552, Average Loss: 2.970, avg. samples / sec: 58538.06
Iteration:    360, Loss function: 7.137, Average Loss: 2.956, avg. samples / sec: 58568.34
Iteration:    360, Loss function: 7.816, Average Loss: 2.975, avg. samples / sec: 58408.14
Iteration:    360, Loss function: 6.845, Average Loss: 2.980, avg. samples / sec: 58595.59
Iteration:    360, Loss function: 7.154, Average Loss: 2.992, avg. samples / sec: 58355.10
Iteration:    360, Loss function: 8.185, Average Loss: 2.973, avg. samples / sec: 58530.57
Iteration:    360, Loss function: 6.465, Average Loss: 2.956, avg. samples / sec: 58442.58
Iteration:    360, Loss function: 6.767, Average Loss: 2.968, avg. samples / sec: 58320.86
Iteration:    360, Loss function: 7.851, Average Loss: 2.976, avg. samples / sec: 58536.50
Iteration:    360, Loss function: 8.111, Average Loss: 2.980, avg. samples / sec: 58380.07
Iteration:    380, Loss function: 7.170, Average Loss: 3.057, avg. samples / sec: 57710.63
Iteration:    380, Loss function: 7.153, Average Loss: 3.054, avg. samples / sec: 57764.35
Iteration:    380, Loss function: 7.024, Average Loss: 3.064, avg. samples / sec: 57673.93
Iteration:    380, Loss function: 6.462, Average Loss: 3.045, avg. samples / sec: 57543.66
Iteration:    380, Loss function: 5.623, Average Loss: 3.066, avg. samples / sec: 57492.67
Iteration:    380, Loss function: 7.114, Average Loss: 3.050, avg. samples / sec: 57546.90
Iteration:    380, Loss function: 6.558, Average Loss: 3.090, avg. samples / sec: 57402.88
Iteration:    380, Loss function: 7.873, Average Loss: 3.059, avg. samples / sec: 57776.74
Iteration:    380, Loss function: 6.147, Average Loss: 3.071, avg. samples / sec: 57639.44
Iteration:    380, Loss function: 8.210, Average Loss: 3.035, avg. samples / sec: 57563.35
Iteration:    380, Loss function: 6.173, Average Loss: 3.053, avg. samples / sec: 57593.13
Iteration:    380, Loss function: 7.656, Average Loss: 3.030, avg. samples / sec: 57614.25
Iteration:    380, Loss function: 6.581, Average Loss: 3.059, avg. samples / sec: 57452.17
Iteration:    380, Loss function: 7.459, Average Loss: 3.053, avg. samples / sec: 57473.02
Iteration:    380, Loss function: 6.690, Average Loss: 3.056, avg. samples / sec: 57592.57
Iteration:    400, Loss function: 7.160, Average Loss: 3.124, avg. samples / sec: 58376.18
Iteration:    400, Loss function: 6.878, Average Loss: 3.119, avg. samples / sec: 58340.34
Iteration:    400, Loss function: 5.938, Average Loss: 3.126, avg. samples / sec: 58391.17
Iteration:    400, Loss function: 6.969, Average Loss: 3.142, avg. samples / sec: 58326.09
Iteration:    400, Loss function: 6.476, Average Loss: 3.167, avg. samples / sec: 58292.66
Iteration:    400, Loss function: 7.618, Average Loss: 3.145, avg. samples / sec: 58281.30
Iteration:    400, Loss function: 6.358, Average Loss: 3.130, avg. samples / sec: 58194.81
Iteration:    400, Loss function: 8.013, Average Loss: 3.133, avg. samples / sec: 58346.23
Iteration:    400, Loss function: 7.222, Average Loss: 3.132, avg. samples / sec: 58246.37
Iteration:    400, Loss function: 5.737, Average Loss: 3.109, avg. samples / sec: 58258.20
Iteration:    400, Loss function: 6.230, Average Loss: 3.127, avg. samples / sec: 58142.39
Iteration:    400, Loss function: 7.098, Average Loss: 3.138, avg. samples / sec: 58346.62
Iteration:    400, Loss function: 7.516, Average Loss: 3.142, avg. samples / sec: 58091.22
Iteration:    400, Loss function: 6.424, Average Loss: 3.102, avg. samples / sec: 58184.26
Iteration:    400, Loss function: 7.499, Average Loss: 3.135, avg. samples / sec: 58153.77
:::MLL 1558639647.678 epoch_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 819}}
:::MLL 1558639647.679 epoch_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 673}}
Iteration:    420, Loss function: 6.669, Average Loss: 3.216, avg. samples / sec: 58472.99
Iteration:    420, Loss function: 6.962, Average Loss: 3.200, avg. samples / sec: 58447.40
Iteration:    420, Loss function: 7.089, Average Loss: 3.211, avg. samples / sec: 58607.44
Iteration:    420, Loss function: 6.972, Average Loss: 3.196, avg. samples / sec: 58379.73
Iteration:    420, Loss function: 7.347, Average Loss: 3.216, avg. samples / sec: 58620.26
Iteration:    420, Loss function: 6.352, Average Loss: 3.205, avg. samples / sec: 58495.49
Iteration:    420, Loss function: 5.912, Average Loss: 3.214, avg. samples / sec: 58482.77
Iteration:    420, Loss function: 6.581, Average Loss: 3.174, avg. samples / sec: 58641.63
Iteration:    420, Loss function: 7.213, Average Loss: 3.204, avg. samples / sec: 58410.10
Iteration:    420, Loss function: 6.397, Average Loss: 3.197, avg. samples / sec: 58388.10
Iteration:    420, Loss function: 8.039, Average Loss: 3.204, avg. samples / sec: 58320.32
Iteration:    420, Loss function: 7.397, Average Loss: 3.193, avg. samples / sec: 58102.07
Iteration:    420, Loss function: 7.273, Average Loss: 3.181, avg. samples / sec: 58199.64
Iteration:    420, Loss function: 6.007, Average Loss: 3.245, avg. samples / sec: 58096.35
Iteration:    420, Loss function: 6.737, Average Loss: 3.206, avg. samples / sec: 58223.73
Iteration:    440, Loss function: 6.181, Average Loss: 3.261, avg. samples / sec: 57284.07
Iteration:    440, Loss function: 6.276, Average Loss: 3.280, avg. samples / sec: 57234.66
Iteration:    440, Loss function: 6.021, Average Loss: 3.267, avg. samples / sec: 57396.29
Iteration:    440, Loss function: 5.575, Average Loss: 3.279, avg. samples / sec: 57225.85
Iteration:    440, Loss function: 6.695, Average Loss: 3.246, avg. samples / sec: 57520.64
Iteration:    440, Loss function: 6.962, Average Loss: 3.279, avg. samples / sec: 57653.07
Iteration:    440, Loss function: 6.750, Average Loss: 3.314, avg. samples / sec: 57551.06
Iteration:    440, Loss function: 6.880, Average Loss: 3.243, avg. samples / sec: 57202.97
Iteration:    440, Loss function: 5.311, Average Loss: 3.263, avg. samples / sec: 57418.34
Iteration:    440, Loss function: 7.327, Average Loss: 3.277, avg. samples / sec: 57132.33
Iteration:    440, Loss function: 6.874, Average Loss: 3.280, avg. samples / sec: 57238.05
Iteration:    440, Loss function: 7.936, Average Loss: 3.285, avg. samples / sec: 57115.78
Iteration:    440, Loss function: 8.798, Average Loss: 3.277, avg. samples / sec: 57234.52
Iteration:    440, Loss function: 7.266, Average Loss: 3.274, avg. samples / sec: 57002.07
Iteration:    440, Loss function: 6.853, Average Loss: 3.288, avg. samples / sec: 56804.34
Iteration:    460, Loss function: 5.237, Average Loss: 3.332, avg. samples / sec: 58038.33
Iteration:    460, Loss function: 6.681, Average Loss: 3.330, avg. samples / sec: 57740.59
Iteration:    460, Loss function: 6.443, Average Loss: 3.350, avg. samples / sec: 57962.95
Iteration:    460, Loss function: 6.987, Average Loss: 3.312, avg. samples / sec: 57804.75
Iteration:    460, Loss function: 6.668, Average Loss: 3.351, avg. samples / sec: 57865.37
Iteration:    460, Loss function: 6.401, Average Loss: 3.345, avg. samples / sec: 57857.81
Iteration:    460, Loss function: 6.222, Average Loss: 3.345, avg. samples / sec: 57929.78
Iteration:    460, Loss function: 6.777, Average Loss: 3.351, avg. samples / sec: 57695.44
Iteration:    460, Loss function: 6.750, Average Loss: 3.309, avg. samples / sec: 57758.50
Iteration:    460, Loss function: 6.545, Average Loss: 3.351, avg. samples / sec: 57819.88
Iteration:    460, Loss function: 6.565, Average Loss: 3.383, avg. samples / sec: 57716.80
Iteration:    460, Loss function: 6.452, Average Loss: 3.357, avg. samples / sec: 58040.74
Iteration:    460, Loss function: 7.193, Average Loss: 3.349, avg. samples / sec: 57651.26
Iteration:    460, Loss function: 5.779, Average Loss: 3.347, avg. samples / sec: 57646.61
Iteration:    460, Loss function: 6.632, Average Loss: 3.339, avg. samples / sec: 57464.35
Iteration:    480, Loss function: 6.755, Average Loss: 3.412, avg. samples / sec: 58717.03
Iteration:    480, Loss function: 7.419, Average Loss: 3.412, avg. samples / sec: 58672.46
Iteration:    480, Loss function: 6.047, Average Loss: 3.414, avg. samples / sec: 58716.02
Iteration:    480, Loss function: 5.952, Average Loss: 3.368, avg. samples / sec: 58694.43
Iteration:    480, Loss function: 5.941, Average Loss: 3.407, avg. samples / sec: 58781.07
Iteration:    480, Loss function: 6.796, Average Loss: 3.405, avg. samples / sec: 58647.14
Iteration:    480, Loss function: 6.638, Average Loss: 3.402, avg. samples / sec: 58904.23
Iteration:    480, Loss function: 7.455, Average Loss: 3.375, avg. samples / sec: 58565.91
Iteration:    480, Loss function: 6.096, Average Loss: 3.419, avg. samples / sec: 58672.58
Iteration:    480, Loss function: 6.709, Average Loss: 3.413, avg. samples / sec: 58628.14
Iteration:    480, Loss function: 5.431, Average Loss: 3.389, avg. samples / sec: 58479.56
Iteration:    480, Loss function: 6.569, Average Loss: 3.446, avg. samples / sec: 58604.05
Iteration:    480, Loss function: 7.222, Average Loss: 3.402, avg. samples / sec: 58519.58
Iteration:    480, Loss function: 5.825, Average Loss: 3.395, avg. samples / sec: 58302.93
Iteration:    480, Loss function: 5.980, Average Loss: 3.415, avg. samples / sec: 58410.82
:::MLL 1558639649.702 epoch_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 819}}
:::MLL 1558639649.703 epoch_start: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 673}}
Iteration:    500, Loss function: 6.507, Average Loss: 3.463, avg. samples / sec: 59260.86
Iteration:    500, Loss function: 5.891, Average Loss: 3.462, avg. samples / sec: 59291.23
Iteration:    500, Loss function: 6.359, Average Loss: 3.477, avg. samples / sec: 59199.08
Iteration:    500, Loss function: 5.761, Average Loss: 3.470, avg. samples / sec: 59188.36
Iteration:    500, Loss function: 6.413, Average Loss: 3.461, avg. samples / sec: 59145.91
Iteration:    500, Loss function: 6.429, Average Loss: 3.428, avg. samples / sec: 59100.82
Iteration:    500, Loss function: 4.917, Average Loss: 3.428, avg. samples / sec: 59158.30
Iteration:    500, Loss function: 6.756, Average Loss: 3.444, avg. samples / sec: 59171.07
Iteration:    500, Loss function: 5.270, Average Loss: 3.472, avg. samples / sec: 58971.57
Iteration:    500, Loss function: 5.581, Average Loss: 3.472, avg. samples / sec: 58957.61
Iteration:    500, Loss function: 6.610, Average Loss: 3.460, avg. samples / sec: 59142.41
Iteration:    500, Loss function: 7.325, Average Loss: 3.470, avg. samples / sec: 58898.84
Iteration:    500, Loss function: 5.399, Average Loss: 3.504, avg. samples / sec: 59084.54
Iteration:    500, Loss function: 6.549, Average Loss: 3.463, avg. samples / sec: 58945.18
Iteration:    500, Loss function: 6.613, Average Loss: 3.475, avg. samples / sec: 59110.43
Iteration:    520, Loss function: 6.453, Average Loss: 3.522, avg. samples / sec: 57918.11
Iteration:    520, Loss function: 5.815, Average Loss: 3.535, avg. samples / sec: 57391.83
Iteration:    520, Loss function: 7.388, Average Loss: 3.487, avg. samples / sec: 57368.44
Iteration:    520, Loss function: 6.285, Average Loss: 3.523, avg. samples / sec: 57493.89
Iteration:    520, Loss function: 7.055, Average Loss: 3.535, avg. samples / sec: 57422.83
Iteration:    520, Loss function: 5.941, Average Loss: 3.521, avg. samples / sec: 57311.38
Iteration:    520, Loss function: 6.801, Average Loss: 3.521, avg. samples / sec: 57404.26
Iteration:    520, Loss function: 6.484, Average Loss: 3.534, avg. samples / sec: 57623.18
Iteration:    520, Loss function: 4.946, Average Loss: 3.560, avg. samples / sec: 57400.05
Iteration:    520, Loss function: 7.187, Average Loss: 3.490, avg. samples / sec: 57238.10
Iteration:    520, Loss function: 5.724, Average Loss: 3.531, avg. samples / sec: 57283.89
Iteration:    520, Loss function: 7.078, Average Loss: 3.505, avg. samples / sec: 57210.22
Iteration:    520, Loss function: 7.114, Average Loss: 3.523, avg. samples / sec: 57029.54
Iteration:    520, Loss function: 6.790, Average Loss: 3.530, avg. samples / sec: 57290.48
Iteration:    520, Loss function: 5.847, Average Loss: 3.535, avg. samples / sec: 56823.28
Iteration:    540, Loss function: 6.556, Average Loss: 3.588, avg. samples / sec: 57348.29
Iteration:    540, Loss function: 6.803, Average Loss: 3.547, avg. samples / sec: 57332.92
Iteration:    540, Loss function: 6.555, Average Loss: 3.589, avg. samples / sec: 57728.48
Iteration:    540, Loss function: 6.416, Average Loss: 3.577, avg. samples / sec: 57242.21
Iteration:    540, Loss function: 6.545, Average Loss: 3.586, avg. samples / sec: 57279.79
Iteration:    540, Loss function: 6.390, Average Loss: 3.556, avg. samples / sec: 57322.33
Iteration:    540, Loss function: 6.003, Average Loss: 3.611, avg. samples / sec: 57204.92
Iteration:    540, Loss function: 5.540, Average Loss: 3.569, avg. samples / sec: 56501.94
Iteration:    540, Loss function: 6.339, Average Loss: 3.538, avg. samples / sec: 57081.12
Iteration:    540, Loss function: 6.355, Average Loss: 3.572, avg. samples / sec: 57120.32
Iteration:    540, Loss function: 5.193, Average Loss: 3.574, avg. samples / sec: 57279.25
Iteration:    540, Loss function: 6.296, Average Loss: 3.587, avg. samples / sec: 57022.09
Iteration:    540, Loss function: 6.554, Average Loss: 3.577, avg. samples / sec: 57044.04
Iteration:    540, Loss function: 6.470, Average Loss: 3.584, avg. samples / sec: 57210.15
Iteration:    540, Loss function: 7.081, Average Loss: 3.590, avg. samples / sec: 57018.65
:::MLL 1558639651.727 epoch_stop: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 819}}
:::MLL 1558639651.727 epoch_start: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 673}}
Iteration:    560, Loss function: 5.008, Average Loss: 3.617, avg. samples / sec: 59656.09
Iteration:    560, Loss function: 6.207, Average Loss: 3.630, avg. samples / sec: 59676.98
Iteration:    560, Loss function: 7.121, Average Loss: 3.641, avg. samples / sec: 59408.15
Iteration:    560, Loss function: 6.168, Average Loss: 3.619, avg. samples / sec: 59593.73
Iteration:    560, Loss function: 5.873, Average Loss: 3.639, avg. samples / sec: 59482.45
Iteration:    560, Loss function: 6.583, Average Loss: 3.636, avg. samples / sec: 59589.64
Iteration:    560, Loss function: 7.534, Average Loss: 3.601, avg. samples / sec: 59417.04
Iteration:    560, Loss function: 6.739, Average Loss: 3.589, avg. samples / sec: 59521.14
Iteration:    560, Loss function: 6.495, Average Loss: 3.656, avg. samples / sec: 59503.87
Iteration:    560, Loss function: 6.044, Average Loss: 3.639, avg. samples / sec: 59632.16
Iteration:    560, Loss function: 6.749, Average Loss: 3.625, avg. samples / sec: 59421.10
Iteration:    560, Loss function: 7.813, Average Loss: 3.627, avg. samples / sec: 59514.90
Iteration:    560, Loss function: 5.932, Average Loss: 3.637, avg. samples / sec: 59326.05
Iteration:    560, Loss function: 7.634, Average Loss: 3.604, avg. samples / sec: 59265.45
Iteration:    560, Loss function: 6.290, Average Loss: 3.631, avg. samples / sec: 59388.15
Iteration:    580, Loss function: 5.008, Average Loss: 3.670, avg. samples / sec: 57071.35
Iteration:    580, Loss function: 5.666, Average Loss: 3.691, avg. samples / sec: 57087.16
Iteration:    580, Loss function: 6.582, Average Loss: 3.681, avg. samples / sec: 57153.65
Iteration:    580, Loss function: 6.685, Average Loss: 3.688, avg. samples / sec: 57352.89
Iteration:    580, Loss function: 6.932, Average Loss: 3.674, avg. samples / sec: 57079.94
Iteration:    580, Loss function: 6.957, Average Loss: 3.690, avg. samples / sec: 57034.46
Iteration:    580, Loss function: 4.717, Average Loss: 3.697, avg. samples / sec: 57224.85
Iteration:    580, Loss function: 6.233, Average Loss: 3.695, avg. samples / sec: 57013.16
Iteration:    580, Loss function: 5.911, Average Loss: 3.698, avg. samples / sec: 56956.92
Iteration:    580, Loss function: 6.495, Average Loss: 3.712, avg. samples / sec: 56999.47
Iteration:    580, Loss function: 6.338, Average Loss: 3.663, avg. samples / sec: 57170.04
Iteration:    580, Loss function: 6.197, Average Loss: 3.642, avg. samples / sec: 56958.48
Iteration:    580, Loss function: 5.151, Average Loss: 3.676, avg. samples / sec: 56945.20
Iteration:    580, Loss function: 5.808, Average Loss: 3.684, avg. samples / sec: 56853.52
Iteration:    580, Loss function: 6.149, Average Loss: 3.660, avg. samples / sec: 56917.44
Iteration:    600, Loss function: 6.059, Average Loss: 3.736, avg. samples / sec: 60000.75
Iteration:    600, Loss function: 5.350, Average Loss: 3.736, avg. samples / sec: 59739.99
Iteration:    600, Loss function: 6.353, Average Loss: 3.708, avg. samples / sec: 59959.42
Iteration:    600, Loss function: 6.086, Average Loss: 3.757, avg. samples / sec: 59866.83
Iteration:    600, Loss function: 5.706, Average Loss: 3.716, avg. samples / sec: 59703.62
Iteration:    600, Loss function: 4.855, Average Loss: 3.715, avg. samples / sec: 59590.02
Iteration:    600, Loss function: 7.339, Average Loss: 3.742, avg. samples / sec: 59808.14
Iteration:    600, Loss function: 6.176, Average Loss: 3.726, avg. samples / sec: 59632.64
Iteration:    600, Loss function: 5.800, Average Loss: 3.744, avg. samples / sec: 59701.27
Iteration:    600, Loss function: 5.949, Average Loss: 3.736, avg. samples / sec: 59609.43
Iteration:    600, Loss function: 6.020, Average Loss: 3.738, avg. samples / sec: 59667.55
Iteration:    600, Loss function: 6.222, Average Loss: 3.691, avg. samples / sec: 59798.27
Iteration:    600, Loss function: 7.484, Average Loss: 3.726, avg. samples / sec: 59766.87
Iteration:    600, Loss function: 5.939, Average Loss: 3.728, avg. samples / sec: 59709.01
Iteration:    600, Loss function: 4.623, Average Loss: 3.706, avg. samples / sec: 59610.89
Iteration:    620, Loss function: 5.406, Average Loss: 3.779, avg. samples / sec: 59845.12
Iteration:    620, Loss function: 6.591, Average Loss: 3.781, avg. samples / sec: 59791.90
Iteration:    620, Loss function: 5.451, Average Loss: 3.756, avg. samples / sec: 59703.22
Iteration:    620, Loss function: 5.689, Average Loss: 3.786, avg. samples / sec: 59648.26
Iteration:    620, Loss function: 6.638, Average Loss: 3.734, avg. samples / sec: 59700.74
Iteration:    620, Loss function: 6.984, Average Loss: 3.780, avg. samples / sec: 59515.08
Iteration:    620, Loss function: 5.550, Average Loss: 3.762, avg. samples / sec: 59735.56
Iteration:    620, Loss function: 8.091, Average Loss: 3.801, avg. samples / sec: 59552.05
Iteration:    620, Loss function: 4.887, Average Loss: 3.751, avg. samples / sec: 59512.79
Iteration:    620, Loss function: 6.503, Average Loss: 3.770, avg. samples / sec: 59741.94
Iteration:    620, Loss function: 5.547, Average Loss: 3.780, avg. samples / sec: 59272.73
Iteration:    620, Loss function: 6.038, Average Loss: 3.756, avg. samples / sec: 59442.16
Iteration:    620, Loss function: 5.454, Average Loss: 3.748, avg. samples / sec: 59603.68
Iteration:    620, Loss function: 6.815, Average Loss: 3.768, avg. samples / sec: 59361.28
Iteration:    620, Loss function: 6.538, Average Loss: 3.789, avg. samples / sec: 58615.31
:::MLL 1558639653.731 epoch_stop: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 819}}
:::MLL 1558639653.732 epoch_start: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 673}}
Iteration:    640, Loss function: 5.522, Average Loss: 3.792, avg. samples / sec: 59015.16
Iteration:    640, Loss function: 6.701, Average Loss: 3.808, avg. samples / sec: 59038.35
Iteration:    640, Loss function: 6.186, Average Loss: 3.801, avg. samples / sec: 58978.88
Iteration:    640, Loss function: 4.975, Average Loss: 3.825, avg. samples / sec: 58765.90
Iteration:    640, Loss function: 5.693, Average Loss: 3.837, avg. samples / sec: 58934.68
Iteration:    640, Loss function: 5.589, Average Loss: 3.820, avg. samples / sec: 58796.67
Iteration:    640, Loss function: 5.423, Average Loss: 3.808, avg. samples / sec: 59169.87
Iteration:    640, Loss function: 5.999, Average Loss: 3.780, avg. samples / sec: 58803.27
Iteration:    640, Loss function: 6.408, Average Loss: 3.822, avg. samples / sec: 58798.24
Iteration:    640, Loss function: 5.332, Average Loss: 3.797, avg. samples / sec: 58924.36
Iteration:    640, Loss function: 6.438, Average Loss: 3.824, avg. samples / sec: 58730.39
Iteration:    640, Loss function: 6.014, Average Loss: 3.820, avg. samples / sec: 58887.18
Iteration:    640, Loss function: 4.939, Average Loss: 3.831, avg. samples / sec: 59720.73
Iteration:    640, Loss function: 5.677, Average Loss: 3.789, avg. samples / sec: 58948.19
Iteration:    640, Loss function: 6.372, Average Loss: 3.805, avg. samples / sec: 58604.71
Iteration:    660, Loss function: 6.055, Average Loss: 3.855, avg. samples / sec: 59717.67
Iteration:    660, Loss function: 5.157, Average Loss: 3.847, avg. samples / sec: 59513.62
Iteration:    660, Loss function: 5.746, Average Loss: 3.839, avg. samples / sec: 59665.41
Iteration:    660, Loss function: 5.896, Average Loss: 3.832, avg. samples / sec: 59452.79
Iteration:    660, Loss function: 5.449, Average Loss: 3.856, avg. samples / sec: 59515.16
Iteration:    660, Loss function: 6.060, Average Loss: 3.863, avg. samples / sec: 59682.79
Iteration:    660, Loss function: 6.622, Average Loss: 3.843, avg. samples / sec: 59758.26
Iteration:    660, Loss function: 7.162, Average Loss: 3.832, avg. samples / sec: 59686.73
Iteration:    660, Loss function: 6.969, Average Loss: 3.871, avg. samples / sec: 59667.00
Iteration:    660, Loss function: 7.258, Average Loss: 3.838, avg. samples / sec: 59379.09
Iteration:    660, Loss function: 5.876, Average Loss: 3.847, avg. samples / sec: 59411.43
Iteration:    660, Loss function: 5.772, Average Loss: 3.864, avg. samples / sec: 59330.15
Iteration:    660, Loss function: 5.733, Average Loss: 3.817, avg. samples / sec: 59450.53
Iteration:    660, Loss function: 5.346, Average Loss: 3.871, avg. samples / sec: 59317.61
Iteration:    660, Loss function: 7.632, Average Loss: 3.859, avg. samples / sec: 59505.91
Iteration:    680, Loss function: 6.932, Average Loss: 3.886, avg. samples / sec: 59789.92
Iteration:    680, Loss function: 4.423, Average Loss: 3.882, avg. samples / sec: 59823.07
Iteration:    680, Loss function: 5.816, Average Loss: 3.893, avg. samples / sec: 59693.56
Iteration:    680, Loss function: 5.560, Average Loss: 3.874, avg. samples / sec: 59669.07
Iteration:    680, Loss function: 5.342, Average Loss: 3.875, avg. samples / sec: 59703.57
Iteration:    680, Loss function: 5.989, Average Loss: 3.914, avg. samples / sec: 59704.66
Iteration:    680, Loss function: 6.503, Average Loss: 3.892, avg. samples / sec: 59578.08
Iteration:    680, Loss function: 5.110, Average Loss: 3.874, avg. samples / sec: 59610.87
Iteration:    680, Loss function: 4.636, Average Loss: 3.860, avg. samples / sec: 59752.38
Iteration:    680, Loss function: 4.283, Average Loss: 3.910, avg. samples / sec: 59736.02
Iteration:    680, Loss function: 4.970, Average Loss: 3.897, avg. samples / sec: 59696.80
Iteration:    680, Loss function: 6.157, Average Loss: 3.889, avg. samples / sec: 59510.20
Iteration:    680, Loss function: 5.013, Average Loss: 3.895, avg. samples / sec: 59431.03
Iteration:    680, Loss function: 4.742, Average Loss: 3.902, avg. samples / sec: 59472.56
Iteration:    680, Loss function: 5.175, Average Loss: 3.903, avg. samples / sec: 59500.08
:::MLL 1558639655.711 epoch_stop: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 819}}
:::MLL 1558639655.711 epoch_start: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 673}}
Iteration:    700, Loss function: 5.604, Average Loss: 3.892, avg. samples / sec: 59247.48
Iteration:    700, Loss function: 5.749, Average Loss: 3.945, avg. samples / sec: 59063.14
Iteration:    700, Loss function: 6.438, Average Loss: 3.935, avg. samples / sec: 59202.36
Iteration:    700, Loss function: 6.954, Average Loss: 3.924, avg. samples / sec: 59143.92
Iteration:    700, Loss function: 4.360, Average Loss: 3.946, avg. samples / sec: 59105.25
Iteration:    700, Loss function: 5.978, Average Loss: 3.930, avg. samples / sec: 59099.75
Iteration:    700, Loss function: 5.626, Average Loss: 3.931, avg. samples / sec: 58993.57
Iteration:    700, Loss function: 6.554, Average Loss: 3.920, avg. samples / sec: 58909.87
Iteration:    700, Loss function: 5.934, Average Loss: 3.925, avg. samples / sec: 59083.50
Iteration:    700, Loss function: 5.544, Average Loss: 3.923, avg. samples / sec: 58878.81
Iteration:    700, Loss function: 6.210, Average Loss: 3.923, avg. samples / sec: 58871.21
Iteration:    700, Loss function: 5.757, Average Loss: 3.904, avg. samples / sec: 58831.50
Iteration:    700, Loss function: 5.753, Average Loss: 3.909, avg. samples / sec: 58891.36
Iteration:    700, Loss function: 5.782, Average Loss: 3.918, avg. samples / sec: 58768.96
Iteration:    700, Loss function: 5.964, Average Loss: 3.938, avg. samples / sec: 58945.43
Iteration:    720, Loss function: 5.871, Average Loss: 3.956, avg. samples / sec: 56086.24
Iteration:    720, Loss function: 5.007, Average Loss: 3.973, avg. samples / sec: 56315.58
Iteration:    720, Loss function: 6.175, Average Loss: 3.973, avg. samples / sec: 55902.56
Iteration:    720, Loss function: 6.490, Average Loss: 3.968, avg. samples / sec: 55910.21
Iteration:    720, Loss function: 5.692, Average Loss: 3.939, avg. samples / sec: 56053.91
Iteration:    720, Loss function: 5.607, Average Loss: 3.963, avg. samples / sec: 55915.13
Iteration:    720, Loss function: 6.806, Average Loss: 3.959, avg. samples / sec: 55887.81
Iteration:    720, Loss function: 5.532, Average Loss: 3.956, avg. samples / sec: 55905.82
Iteration:    720, Loss function: 5.721, Average Loss: 3.952, avg. samples / sec: 56066.94
Iteration:    720, Loss function: 6.201, Average Loss: 3.954, avg. samples / sec: 55903.31
Iteration:    720, Loss function: 5.032, Average Loss: 3.925, avg. samples / sec: 55640.40
Iteration:    720, Loss function: 6.166, Average Loss: 3.974, avg. samples / sec: 55811.81
Iteration:    720, Loss function: 6.529, Average Loss: 3.982, avg. samples / sec: 55791.73
Iteration:    720, Loss function: 5.845, Average Loss: 3.943, avg. samples / sec: 55928.23
Iteration:    720, Loss function: 5.460, Average Loss: 3.957, avg. samples / sec: 55809.69
Iteration:    740, Loss function: 4.953, Average Loss: 4.003, avg. samples / sec: 59992.34
Iteration:    740, Loss function: 5.264, Average Loss: 3.990, avg. samples / sec: 59984.35
Iteration:    740, Loss function: 6.328, Average Loss: 4.012, avg. samples / sec: 59761.91
Iteration:    740, Loss function: 5.535, Average Loss: 3.994, avg. samples / sec: 59679.25
Iteration:    740, Loss function: 5.665, Average Loss: 3.985, avg. samples / sec: 59856.89
Iteration:    740, Loss function: 5.352, Average Loss: 3.991, avg. samples / sec: 59934.73
Iteration:    740, Loss function: 4.942, Average Loss: 3.973, avg. samples / sec: 59723.33
Iteration:    740, Loss function: 6.535, Average Loss: 3.998, avg. samples / sec: 59761.25
Iteration:    740, Loss function: 6.057, Average Loss: 3.959, avg. samples / sec: 59810.50
Iteration:    740, Loss function: 6.185, Average Loss: 4.008, avg. samples / sec: 59629.78
Iteration:    740, Loss function: 5.048, Average Loss: 3.974, avg. samples / sec: 59799.69
Iteration:    740, Loss function: 6.597, Average Loss: 3.985, avg. samples / sec: 59697.07
Iteration:    740, Loss function: 5.538, Average Loss: 3.989, avg. samples / sec: 59676.12
Iteration:    740, Loss function: 7.241, Average Loss: 4.011, avg. samples / sec: 59718.37
Iteration:    740, Loss function: 5.533, Average Loss: 4.015, avg. samples / sec: 59039.31
Iteration:    760, Loss function: 6.448, Average Loss: 3.994, avg. samples / sec: 58591.50
Iteration:    760, Loss function: 5.251, Average Loss: 4.032, avg. samples / sec: 58227.39
Iteration:    760, Loss function: 6.606, Average Loss: 4.019, avg. samples / sec: 58281.98
Iteration:    760, Loss function: 5.513, Average Loss: 4.001, avg. samples / sec: 58555.28
Iteration:    760, Loss function: 5.094, Average Loss: 4.034, avg. samples / sec: 58512.88
Iteration:    760, Loss function: 4.378, Average Loss: 4.046, avg. samples / sec: 59232.89
Iteration:    760, Loss function: 5.635, Average Loss: 4.004, avg. samples / sec: 58410.32
Iteration:    760, Loss function: 5.272, Average Loss: 4.041, avg. samples / sec: 58556.71
Iteration:    760, Loss function: 5.772, Average Loss: 4.015, avg. samples / sec: 58485.75
Iteration:    760, Loss function: 5.257, Average Loss: 4.016, avg. samples / sec: 58362.18
Iteration:    760, Loss function: 5.649, Average Loss: 4.026, avg. samples / sec: 58347.15
Iteration:    760, Loss function: 5.608, Average Loss: 4.022, avg. samples / sec: 58289.06
Iteration:    760, Loss function: 4.359, Average Loss: 4.021, avg. samples / sec: 58402.55
Iteration:    760, Loss function: 5.585, Average Loss: 4.044, avg. samples / sec: 58225.99
Iteration:    760, Loss function: 5.233, Average Loss: 4.027, avg. samples / sec: 58254.01
:::MLL 1558639657.734 epoch_stop: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 819}}
:::MLL 1558639657.734 epoch_start: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 673}}
Iteration:    780, Loss function: 4.733, Average Loss: 4.044, avg. samples / sec: 59169.45
Iteration:    780, Loss function: 4.973, Average Loss: 4.034, avg. samples / sec: 59245.67
Iteration:    780, Loss function: 4.574, Average Loss: 4.025, avg. samples / sec: 59138.14
Iteration:    780, Loss function: 5.993, Average Loss: 4.060, avg. samples / sec: 59146.83
Iteration:    780, Loss function: 5.141, Average Loss: 4.048, avg. samples / sec: 59303.88
Iteration:    780, Loss function: 5.260, Average Loss: 4.049, avg. samples / sec: 59176.38
Iteration:    780, Loss function: 6.119, Average Loss: 4.047, avg. samples / sec: 59184.86
Iteration:    780, Loss function: 5.436, Average Loss: 4.078, avg. samples / sec: 59078.96
Iteration:    780, Loss function: 5.305, Average Loss: 4.048, avg. samples / sec: 59106.32
Iteration:    780, Loss function: 4.783, Average Loss: 4.055, avg. samples / sec: 58923.91
Iteration:    780, Loss function: 5.765, Average Loss: 4.070, avg. samples / sec: 59004.49
Iteration:    780, Loss function: 5.547, Average Loss: 4.023, avg. samples / sec: 58852.48
Iteration:    780, Loss function: 4.941, Average Loss: 4.069, avg. samples / sec: 59106.81
Iteration:    780, Loss function: 4.994, Average Loss: 4.058, avg. samples / sec: 59085.45
Iteration:    780, Loss function: 4.791, Average Loss: 4.045, avg. samples / sec: 58873.35
Iteration:    800, Loss function: 5.679, Average Loss: 4.095, avg. samples / sec: 58951.30
Iteration:    800, Loss function: 5.636, Average Loss: 4.084, avg. samples / sec: 58919.82
Iteration:    800, Loss function: 5.318, Average Loss: 4.049, avg. samples / sec: 58924.50
Iteration:    800, Loss function: 4.146, Average Loss: 4.066, avg. samples / sec: 58715.97
Iteration:    800, Loss function: 6.481, Average Loss: 4.105, avg. samples / sec: 58766.90
Iteration:    800, Loss function: 5.490, Average Loss: 4.095, avg. samples / sec: 58858.89
Iteration:    800, Loss function: 5.674, Average Loss: 4.063, avg. samples / sec: 58596.13
Iteration:    800, Loss function: 5.236, Average Loss: 4.081, avg. samples / sec: 58680.70
Iteration:    800, Loss function: 5.157, Average Loss: 4.088, avg. samples / sec: 58898.82
Iteration:    800, Loss function: 5.245, Average Loss: 4.073, avg. samples / sec: 59002.11
Iteration:    800, Loss function: 4.865, Average Loss: 4.074, avg. samples / sec: 58543.60
Iteration:    800, Loss function: 4.195, Average Loss: 4.081, avg. samples / sec: 58731.61
Iteration:    800, Loss function: 5.255, Average Loss: 4.071, avg. samples / sec: 58614.63
Iteration:    800, Loss function: 4.876, Average Loss: 4.051, avg. samples / sec: 58559.32
Iteration:    800, Loss function: 4.855, Average Loss: 4.090, avg. samples / sec: 58556.01
Iteration:    820, Loss function: 6.856, Average Loss: 4.120, avg. samples / sec: 57821.02
Iteration:    820, Loss function: 5.318, Average Loss: 4.101, avg. samples / sec: 57944.31
Iteration:    820, Loss function: 5.666, Average Loss: 4.074, avg. samples / sec: 57823.39
Iteration:    820, Loss function: 5.093, Average Loss: 4.113, avg. samples / sec: 57925.73
Iteration:    820, Loss function: 5.454, Average Loss: 4.121, avg. samples / sec: 57830.89
Iteration:    820, Loss function: 4.813, Average Loss: 4.085, avg. samples / sec: 57816.11
Iteration:    820, Loss function: 5.229, Average Loss: 4.098, avg. samples / sec: 57863.04
Iteration:    820, Loss function: 4.802, Average Loss: 4.127, avg. samples / sec: 57796.64
Iteration:    820, Loss function: 4.707, Average Loss: 4.108, avg. samples / sec: 57854.61
Iteration:    820, Loss function: 4.810, Average Loss: 4.103, avg. samples / sec: 57812.38
Iteration:    820, Loss function: 5.465, Average Loss: 4.115, avg. samples / sec: 57701.58
Iteration:    820, Loss function: 5.019, Average Loss: 4.095, avg. samples / sec: 57822.04
Iteration:    820, Loss function: 5.268, Average Loss: 4.075, avg. samples / sec: 57829.51
Iteration:    820, Loss function: 5.215, Average Loss: 4.092, avg. samples / sec: 57700.68
Iteration:    820, Loss function: 5.384, Average Loss: 4.115, avg. samples / sec: 57538.30
:::MLL 1558639659.743 epoch_stop: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 819}}
:::MLL 1558639659.744 epoch_start: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 673}}
Iteration:    840, Loss function: 6.219, Average Loss: 4.114, avg. samples / sec: 58899.16
Iteration:    840, Loss function: 6.428, Average Loss: 4.129, avg. samples / sec: 58915.46
Iteration:    840, Loss function: 5.869, Average Loss: 4.125, avg. samples / sec: 58799.32
Iteration:    840, Loss function: 4.330, Average Loss: 4.145, avg. samples / sec: 58855.67
Iteration:    840, Loss function: 6.409, Average Loss: 4.152, avg. samples / sec: 58802.26
Iteration:    840, Loss function: 5.190, Average Loss: 4.099, avg. samples / sec: 58844.86
Iteration:    840, Loss function: 5.492, Average Loss: 4.132, avg. samples / sec: 58775.63
Iteration:    840, Loss function: 5.726, Average Loss: 4.114, avg. samples / sec: 58830.37
Iteration:    840, Loss function: 5.281, Average Loss: 4.138, avg. samples / sec: 58657.79
Iteration:    840, Loss function: 4.664, Average Loss: 4.144, avg. samples / sec: 58561.68
Iteration:    840, Loss function: 5.652, Average Loss: 4.140, avg. samples / sec: 58963.48
Iteration:    840, Loss function: 4.750, Average Loss: 4.122, avg. samples / sec: 58652.90
Iteration:    840, Loss function: 5.456, Average Loss: 4.119, avg. samples / sec: 58637.82
Iteration:    840, Loss function: 4.934, Average Loss: 4.098, avg. samples / sec: 58531.44
Iteration:    840, Loss function: 4.646, Average Loss: 4.128, avg. samples / sec: 58564.62
Iteration:    860, Loss function: 5.069, Average Loss: 4.162, avg. samples / sec: 59755.82
Iteration:    860, Loss function: 6.567, Average Loss: 4.157, avg. samples / sec: 59462.92
Iteration:    860, Loss function: 5.129, Average Loss: 4.183, avg. samples / sec: 59551.47
Iteration:    860, Loss function: 4.760, Average Loss: 4.122, avg. samples / sec: 59518.47
Iteration:    860, Loss function: 4.997, Average Loss: 4.147, avg. samples / sec: 59682.64
Iteration:    860, Loss function: 6.543, Average Loss: 4.172, avg. samples / sec: 59382.19
Iteration:    860, Loss function: 5.887, Average Loss: 4.150, avg. samples / sec: 59343.99
Iteration:    860, Loss function: 5.069, Average Loss: 4.136, avg. samples / sec: 59428.52
Iteration:    860, Loss function: 6.010, Average Loss: 4.170, avg. samples / sec: 59492.82
Iteration:    860, Loss function: 3.897, Average Loss: 4.158, avg. samples / sec: 59384.47
Iteration:    860, Loss function: 4.784, Average Loss: 4.155, avg. samples / sec: 59538.92
Iteration:    860, Loss function: 5.645, Average Loss: 4.153, avg. samples / sec: 59170.05
Iteration:    860, Loss function: 6.234, Average Loss: 4.177, avg. samples / sec: 59396.86
Iteration:    860, Loss function: 5.629, Average Loss: 4.125, avg. samples / sec: 59415.89
Iteration:    860, Loss function: 5.411, Average Loss: 4.144, avg. samples / sec: 59335.32
Iteration:    880, Loss function: 5.991, Average Loss: 4.182, avg. samples / sec: 56588.31
Iteration:    880, Loss function: 4.335, Average Loss: 4.181, avg. samples / sec: 56800.17
Iteration:    880, Loss function: 4.337, Average Loss: 4.162, avg. samples / sec: 56744.94
Iteration:    880, Loss function: 5.341, Average Loss: 4.171, avg. samples / sec: 56898.12
Iteration:    880, Loss function: 5.088, Average Loss: 4.193, avg. samples / sec: 56649.14
Iteration:    880, Loss function: 4.259, Average Loss: 4.185, avg. samples / sec: 56705.24
Iteration:    880, Loss function: 5.435, Average Loss: 4.175, avg. samples / sec: 56766.40
Iteration:    880, Loss function: 4.403, Average Loss: 4.140, avg. samples / sec: 56554.30
Iteration:    880, Loss function: 5.472, Average Loss: 4.202, avg. samples / sec: 56506.61
Iteration:    880, Loss function: 5.460, Average Loss: 4.164, avg. samples / sec: 56541.82
Iteration:    880, Loss function: 6.695, Average Loss: 4.175, avg. samples / sec: 56698.41
Iteration:    880, Loss function: 5.916, Average Loss: 4.201, avg. samples / sec: 56680.95
Iteration:    880, Loss function: 5.414, Average Loss: 4.167, avg. samples / sec: 56526.92
Iteration:    880, Loss function: 6.834, Average Loss: 4.181, avg. samples / sec: 56403.26
Iteration:    880, Loss function: 5.344, Average Loss: 4.144, avg. samples / sec: 56702.86
Iteration:    900, Loss function: 4.210, Average Loss: 4.207, avg. samples / sec: 58212.98
Iteration:    900, Loss function: 6.802, Average Loss: 4.222, avg. samples / sec: 58279.44
Iteration:    900, Loss function: 5.696, Average Loss: 4.206, avg. samples / sec: 58361.60
Iteration:    900, Loss function: 5.820, Average Loss: 4.188, avg. samples / sec: 58119.33
Iteration:    900, Loss function: 4.573, Average Loss: 4.207, avg. samples / sec: 58187.24
Iteration:    900, Loss function: 5.854, Average Loss: 4.205, avg. samples / sec: 58042.15
Iteration:    900, Loss function: 5.064, Average Loss: 4.219, avg. samples / sec: 58134.79
Iteration:    900, Loss function: 4.304, Average Loss: 4.185, avg. samples / sec: 58238.69
Iteration:    900, Loss function: 6.176, Average Loss: 4.160, avg. samples / sec: 58139.11
Iteration:    900, Loss function: 4.665, Average Loss: 4.189, avg. samples / sec: 58075.66
Iteration:    900, Loss function: 6.398, Average Loss: 4.226, avg. samples / sec: 58198.60
Iteration:    900, Loss function: 5.545, Average Loss: 4.165, avg. samples / sec: 58198.51
Iteration:    900, Loss function: 4.877, Average Loss: 4.184, avg. samples / sec: 58100.45
Iteration:    900, Loss function: 5.436, Average Loss: 4.197, avg. samples / sec: 58155.49
Iteration:    900, Loss function: 4.797, Average Loss: 4.191, avg. samples / sec: 58046.53
:::MLL 1558639661.757 epoch_stop: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 819}}
:::MLL 1558639661.757 epoch_start: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 673}}
Iteration:    920, Loss function: 4.856, Average Loss: 4.210, avg. samples / sec: 58144.41
Iteration:    920, Loss function: 4.573, Average Loss: 4.244, avg. samples / sec: 58036.37
Iteration:    920, Loss function: 4.803, Average Loss: 4.218, avg. samples / sec: 58211.05
Iteration:    920, Loss function: 4.368, Average Loss: 4.223, avg. samples / sec: 58219.66
Iteration:    920, Loss function: 5.464, Average Loss: 4.252, avg. samples / sec: 58157.68
Iteration:    920, Loss function: 5.738, Average Loss: 4.229, avg. samples / sec: 58062.96
Iteration:    920, Loss function: 5.225, Average Loss: 4.217, avg. samples / sec: 58124.00
Iteration:    920, Loss function: 4.955, Average Loss: 4.227, avg. samples / sec: 57960.92
Iteration:    920, Loss function: 4.698, Average Loss: 4.234, avg. samples / sec: 57898.15
Iteration:    920, Loss function: 3.435, Average Loss: 4.234, avg. samples / sec: 58015.87
Iteration:    920, Loss function: 4.814, Average Loss: 4.207, avg. samples / sec: 58096.28
Iteration:    920, Loss function: 5.864, Average Loss: 4.196, avg. samples / sec: 58069.59
Iteration:    920, Loss function: 4.813, Average Loss: 4.207, avg. samples / sec: 58003.81
Iteration:    920, Loss function: 4.648, Average Loss: 4.187, avg. samples / sec: 57995.84
Iteration:    920, Loss function: 5.037, Average Loss: 4.234, avg. samples / sec: 57793.42
Iteration:    940, Loss function: 5.149, Average Loss: 4.232, avg. samples / sec: 56262.12
Iteration:    940, Loss function: 4.701, Average Loss: 4.267, avg. samples / sec: 56277.48
Iteration:    940, Loss function: 5.083, Average Loss: 4.215, avg. samples / sec: 56366.06
Iteration:    940, Loss function: 4.119, Average Loss: 4.237, avg. samples / sec: 56267.04
Iteration:    940, Loss function: 4.625, Average Loss: 4.246, avg. samples / sec: 56243.48
Iteration:    940, Loss function: 5.499, Average Loss: 4.229, avg. samples / sec: 56160.10
Iteration:    940, Loss function: 5.880, Average Loss: 4.261, avg. samples / sec: 56192.19
Iteration:    940, Loss function: 4.298, Average Loss: 4.252, avg. samples / sec: 56437.32
Iteration:    940, Loss function: 5.252, Average Loss: 4.241, avg. samples / sec: 56228.45
Iteration:    940, Loss function: 5.587, Average Loss: 4.225, avg. samples / sec: 56251.85
Iteration:    940, Loss function: 4.416, Average Loss: 4.246, avg. samples / sec: 56210.44
Iteration:    940, Loss function: 5.031, Average Loss: 4.221, avg. samples / sec: 56247.36
Iteration:    940, Loss function: 5.475, Average Loss: 4.248, avg. samples / sec: 56165.12
Iteration:    940, Loss function: 6.175, Average Loss: 4.206, avg. samples / sec: 56185.70
Iteration:    940, Loss function: 4.543, Average Loss: 4.239, avg. samples / sec: 55859.22
Iteration:    960, Loss function: 4.779, Average Loss: 4.275, avg. samples / sec: 60199.74
Iteration:    960, Loss function: 6.963, Average Loss: 4.260, avg. samples / sec: 60163.89
Iteration:    960, Loss function: 4.727, Average Loss: 4.259, avg. samples / sec: 60324.72
Iteration:    960, Loss function: 5.529, Average Loss: 4.285, avg. samples / sec: 60123.36
Iteration:    960, Loss function: 5.545, Average Loss: 4.251, avg. samples / sec: 60083.27
Iteration:    960, Loss function: 4.948, Average Loss: 4.253, avg. samples / sec: 60109.15
Iteration:    960, Loss function: 4.971, Average Loss: 4.270, avg. samples / sec: 60182.67
Iteration:    960, Loss function: 4.705, Average Loss: 4.242, avg. samples / sec: 60216.46
Iteration:    960, Loss function: 5.684, Average Loss: 4.223, avg. samples / sec: 60272.50
Iteration:    960, Loss function: 6.399, Average Loss: 4.241, avg. samples / sec: 60112.02
Iteration:    960, Loss function: 5.615, Average Loss: 4.260, avg. samples / sec: 60098.67
Iteration:    960, Loss function: 5.889, Average Loss: 4.251, avg. samples / sec: 60415.20
Iteration:    960, Loss function: 4.485, Average Loss: 4.251, avg. samples / sec: 59950.95
Iteration:    960, Loss function: 5.306, Average Loss: 4.261, avg. samples / sec: 59956.97
Iteration:    960, Loss function: 4.947, Average Loss: 4.230, avg. samples / sec: 59825.20
:::MLL 1558639663.776 epoch_stop: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 819}}
:::MLL 1558639663.776 epoch_start: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 673}}
Iteration:    980, Loss function: 4.818, Average Loss: 4.276, avg. samples / sec: 59920.16
Iteration:    980, Loss function: 5.303, Average Loss: 4.303, avg. samples / sec: 59834.70
Iteration:    980, Loss function: 6.683, Average Loss: 4.270, avg. samples / sec: 60049.32
Iteration:    980, Loss function: 5.251, Average Loss: 4.263, avg. samples / sec: 59918.99
Iteration:    980, Loss function: 5.803, Average Loss: 4.292, avg. samples / sec: 59801.62
Iteration:    980, Loss function: 5.025, Average Loss: 4.272, avg. samples / sec: 59833.36
Iteration:    980, Loss function: 5.206, Average Loss: 4.278, avg. samples / sec: 59786.65
Iteration:    980, Loss function: 6.077, Average Loss: 4.281, avg. samples / sec: 59926.71
Iteration:    980, Loss function: 5.175, Average Loss: 4.279, avg. samples / sec: 60095.21
Iteration:    980, Loss function: 5.182, Average Loss: 4.243, avg. samples / sec: 59879.22
Iteration:    980, Loss function: 5.633, Average Loss: 4.254, avg. samples / sec: 59897.59
Iteration:    980, Loss function: 4.917, Average Loss: 4.273, avg. samples / sec: 59928.59
Iteration:    980, Loss function: 5.162, Average Loss: 4.287, avg. samples / sec: 59747.21
Iteration:    980, Loss function: 4.538, Average Loss: 4.251, avg. samples / sec: 59982.00
Iteration:    980, Loss function: 4.808, Average Loss: 4.282, avg. samples / sec: 59556.25
Iteration:   1000, Loss function: 5.672, Average Loss: 4.282, avg. samples / sec: 57033.86
Iteration:   1000, Loss function: 4.519, Average Loss: 4.273, avg. samples / sec: 56949.39
Iteration:   1000, Loss function: 4.689, Average Loss: 4.302, avg. samples / sec: 57013.53
Iteration:   1000, Loss function: 6.580, Average Loss: 4.307, avg. samples / sec: 56830.20
Iteration:   1000, Loss function: 4.743, Average Loss: 4.284, avg. samples / sec: 56805.12
Iteration:   1000, Loss function: 5.255, Average Loss: 4.275, avg. samples / sec: 56792.62
Iteration:   1000, Loss function: 5.295, Average Loss: 4.296, avg. samples / sec: 56858.56
Iteration:   1000, Loss function: 6.987, Average Loss: 4.297, avg. samples / sec: 56830.59
Iteration:   1000, Loss function: 4.201, Average Loss: 4.283, avg. samples / sec: 56817.94
Iteration:   1000, Loss function: 5.070, Average Loss: 4.294, avg. samples / sec: 56827.29
Iteration:   1000, Loss function: 5.255, Average Loss: 4.299, avg. samples / sec: 57030.58
Iteration:   1000, Loss function: 3.940, Average Loss: 4.286, avg. samples / sec: 56692.28
Iteration:   1000, Loss function: 5.104, Average Loss: 4.321, avg. samples / sec: 56713.68
Iteration:   1000, Loss function: 6.487, Average Loss: 4.258, avg. samples / sec: 56758.38
Iteration:   1000, Loss function: 6.140, Average Loss: 4.267, avg. samples / sec: 56823.88
Iteration:   1020, Loss function: 4.332, Average Loss: 4.328, avg. samples / sec: 60445.00
Iteration:   1020, Loss function: 4.642, Average Loss: 4.318, avg. samples / sec: 60289.00
Iteration:   1020, Loss function: 5.478, Average Loss: 4.319, avg. samples / sec: 60162.60
Iteration:   1020, Loss function: 5.518, Average Loss: 4.303, avg. samples / sec: 60117.95
Iteration:   1020, Loss function: 4.519, Average Loss: 4.274, avg. samples / sec: 60351.02
Iteration:   1020, Loss function: 4.433, Average Loss: 4.296, avg. samples / sec: 60212.70
Iteration:   1020, Loss function: 5.997, Average Loss: 4.316, avg. samples / sec: 60197.07
Iteration:   1020, Loss function: 4.413, Average Loss: 4.319, avg. samples / sec: 60179.41
Iteration:   1020, Loss function: 4.647, Average Loss: 4.295, avg. samples / sec: 60080.20
Iteration:   1020, Loss function: 6.344, Average Loss: 4.307, avg. samples / sec: 60222.02
Iteration:   1020, Loss function: 5.506, Average Loss: 4.306, avg. samples / sec: 60115.64
Iteration:   1020, Loss function: 5.900, Average Loss: 4.344, avg. samples / sec: 60222.58
Iteration:   1020, Loss function: 5.012, Average Loss: 4.289, avg. samples / sec: 60251.60
Iteration:   1020, Loss function: 4.366, Average Loss: 4.313, avg. samples / sec: 60108.95
Iteration:   1020, Loss function: 5.382, Average Loss: 4.305, avg. samples / sec: 60017.64
Iteration:   1040, Loss function: 5.684, Average Loss: 4.349, avg. samples / sec: 57840.98
Iteration:   1040, Loss function: 4.597, Average Loss: 4.337, avg. samples / sec: 58062.07
Iteration:   1040, Loss function: 4.805, Average Loss: 4.331, avg. samples / sec: 58052.27
Iteration:   1040, Loss function: 4.991, Average Loss: 4.330, avg. samples / sec: 57954.06
Iteration:   1040, Loss function: 4.948, Average Loss: 4.352, avg. samples / sec: 58076.50
Iteration:   1040, Loss function: 4.751, Average Loss: 4.319, avg. samples / sec: 58018.86
Iteration:   1040, Loss function: 5.810, Average Loss: 4.324, avg. samples / sec: 58116.19
Iteration:   1040, Loss function: 5.139, Average Loss: 4.289, avg. samples / sec: 57876.66
Iteration:   1040, Loss function: 3.709, Average Loss: 4.308, avg. samples / sec: 57887.14
Iteration:   1040, Loss function: 5.396, Average Loss: 4.319, avg. samples / sec: 57840.50
Iteration:   1040, Loss function: 4.961, Average Loss: 4.308, avg. samples / sec: 57908.83
Iteration:   1040, Loss function: 5.020, Average Loss: 4.332, avg. samples / sec: 57781.50
Iteration:   1040, Loss function: 6.462, Average Loss: 4.333, avg. samples / sec: 57894.11
Iteration:   1040, Loss function: 3.795, Average Loss: 4.327, avg. samples / sec: 57830.65
Iteration:   1040, Loss function: 4.085, Average Loss: 4.310, avg. samples / sec: 57786.14
:::MLL 1558639665.785 epoch_stop: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 819}}
:::MLL 1558639665.786 epoch_start: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 673}}
Iteration:   1060, Loss function: 4.706, Average Loss: 4.334, avg. samples / sec: 57407.82
Iteration:   1060, Loss function: 4.243, Average Loss: 4.365, avg. samples / sec: 57185.84
Iteration:   1060, Loss function: 6.301, Average Loss: 4.369, avg. samples / sec: 57256.87
Iteration:   1060, Loss function: 4.493, Average Loss: 4.302, avg. samples / sec: 57327.69
Iteration:   1060, Loss function: 5.178, Average Loss: 4.349, avg. samples / sec: 57193.69
Iteration:   1060, Loss function: 5.744, Average Loss: 4.323, avg. samples / sec: 57308.21
Iteration:   1060, Loss function: 5.837, Average Loss: 4.347, avg. samples / sec: 57162.78
Iteration:   1060, Loss function: 5.562, Average Loss: 4.325, avg. samples / sec: 57303.57
Iteration:   1060, Loss function: 5.602, Average Loss: 4.347, avg. samples / sec: 57160.37
Iteration:   1060, Loss function: 4.718, Average Loss: 4.325, avg. samples / sec: 57219.58
Iteration:   1060, Loss function: 4.552, Average Loss: 4.342, avg. samples / sec: 57358.66
Iteration:   1060, Loss function: 4.447, Average Loss: 4.337, avg. samples / sec: 57349.13
Iteration:   1060, Loss function: 5.410, Average Loss: 4.325, avg. samples / sec: 57430.86
Iteration:   1060, Loss function: 5.468, Average Loss: 4.342, avg. samples / sec: 57270.18
Iteration:   1060, Loss function: 5.934, Average Loss: 4.335, avg. samples / sec: 56938.85
Iteration:   1080, Loss function: 5.857, Average Loss: 4.360, avg. samples / sec: 59900.83
Iteration:   1080, Loss function: 5.474, Average Loss: 4.348, avg. samples / sec: 59624.11
Iteration:   1080, Loss function: 5.925, Average Loss: 4.342, avg. samples / sec: 59724.57
Iteration:   1080, Loss function: 5.968, Average Loss: 4.359, avg. samples / sec: 59671.90
Iteration:   1080, Loss function: 5.305, Average Loss: 4.346, avg. samples / sec: 60007.54
Iteration:   1080, Loss function: 4.331, Average Loss: 4.346, avg. samples / sec: 59697.71
Iteration:   1080, Loss function: 4.396, Average Loss: 4.310, avg. samples / sec: 59563.55
Iteration:   1080, Loss function: 4.863, Average Loss: 4.348, avg. samples / sec: 59705.29
Iteration:   1080, Loss function: 5.115, Average Loss: 4.380, avg. samples / sec: 59530.94
Iteration:   1080, Loss function: 4.687, Average Loss: 4.380, avg. samples / sec: 59497.34
Iteration:   1080, Loss function: 5.177, Average Loss: 4.342, avg. samples / sec: 59584.48
Iteration:   1080, Loss function: 4.892, Average Loss: 4.362, avg. samples / sec: 59506.23
Iteration:   1080, Loss function: 3.610, Average Loss: 4.348, avg. samples / sec: 59609.13
Iteration:   1080, Loss function: 4.851, Average Loss: 4.363, avg. samples / sec: 59542.71
Iteration:   1080, Loss function: 4.875, Average Loss: 4.333, avg. samples / sec: 59489.91
Iteration:   1100, Loss function: 4.436, Average Loss: 4.368, avg. samples / sec: 59799.66
Iteration:   1100, Loss function: 3.550, Average Loss: 4.375, avg. samples / sec: 59434.18
Iteration:   1100, Loss function: 4.483, Average Loss: 4.362, avg. samples / sec: 59556.88
Iteration:   1100, Loss function: 5.421, Average Loss: 4.362, avg. samples / sec: 59701.65
Iteration:   1100, Loss function: 4.963, Average Loss: 4.351, avg. samples / sec: 59684.00
Iteration:   1100, Loss function: 5.281, Average Loss: 4.357, avg. samples / sec: 59552.96
Iteration:   1100, Loss function: 4.889, Average Loss: 4.364, avg. samples / sec: 59566.77
Iteration:   1100, Loss function: 5.060, Average Loss: 4.391, avg. samples / sec: 59563.22
Iteration:   1100, Loss function: 5.316, Average Loss: 4.376, avg. samples / sec: 59588.69
Iteration:   1100, Loss function: 4.471, Average Loss: 4.376, avg. samples / sec: 59591.43
Iteration:   1100, Loss function: 4.872, Average Loss: 4.349, avg. samples / sec: 59606.53
Iteration:   1100, Loss function: 4.411, Average Loss: 4.361, avg. samples / sec: 59434.08
Iteration:   1100, Loss function: 6.218, Average Loss: 4.393, avg. samples / sec: 59451.26
Iteration:   1100, Loss function: 5.508, Average Loss: 4.364, avg. samples / sec: 59246.59
Iteration:   1100, Loss function: 5.098, Average Loss: 4.321, avg. samples / sec: 59285.29
:::MLL 1558639667.782 epoch_stop: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 819}}
:::MLL 1558639667.782 epoch_start: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 673}}
Iteration:   1120, Loss function: 4.244, Average Loss: 4.403, avg. samples / sec: 60192.41
Iteration:   1120, Loss function: 5.966, Average Loss: 4.369, avg. samples / sec: 59928.13
Iteration:   1120, Loss function: 5.969, Average Loss: 4.362, avg. samples / sec: 59823.45
Iteration:   1120, Loss function: 4.452, Average Loss: 4.381, avg. samples / sec: 59927.27
Iteration:   1120, Loss function: 5.496, Average Loss: 4.374, avg. samples / sec: 59578.76
Iteration:   1120, Loss function: 3.746, Average Loss: 4.373, avg. samples / sec: 59763.96
Iteration:   1120, Loss function: 3.400, Average Loss: 4.380, avg. samples / sec: 59885.58
Iteration:   1120, Loss function: 4.838, Average Loss: 4.332, avg. samples / sec: 60105.00
Iteration:   1120, Loss function: 4.938, Average Loss: 4.402, avg. samples / sec: 59827.01
Iteration:   1120, Loss function: 6.496, Average Loss: 4.360, avg. samples / sec: 59857.17
Iteration:   1120, Loss function: 4.020, Average Loss: 4.371, avg. samples / sec: 59706.38
Iteration:   1120, Loss function: 7.172, Average Loss: 4.376, avg. samples / sec: 59737.59
Iteration:   1120, Loss function: 4.275, Average Loss: 4.368, avg. samples / sec: 59774.23
Iteration:   1120, Loss function: 4.052, Average Loss: 4.378, avg. samples / sec: 59828.66
Iteration:   1120, Loss function: 5.592, Average Loss: 4.389, avg. samples / sec: 59445.32
Iteration:   1140, Loss function: 4.937, Average Loss: 4.385, avg. samples / sec: 57076.99
Iteration:   1140, Loss function: 5.356, Average Loss: 4.404, avg. samples / sec: 57348.43
Iteration:   1140, Loss function: 4.518, Average Loss: 4.394, avg. samples / sec: 57257.68
Iteration:   1140, Loss function: 5.414, Average Loss: 4.385, avg. samples / sec: 57178.90
Iteration:   1140, Loss function: 6.512, Average Loss: 4.342, avg. samples / sec: 57088.11
Iteration:   1140, Loss function: 4.014, Average Loss: 4.388, avg. samples / sec: 57066.98
Iteration:   1140, Loss function: 5.785, Average Loss: 4.377, avg. samples / sec: 57186.79
Iteration:   1140, Loss function: 4.849, Average Loss: 4.389, avg. samples / sec: 57092.22
Iteration:   1140, Loss function: 4.854, Average Loss: 4.391, avg. samples / sec: 56986.33
Iteration:   1140, Loss function: 4.672, Average Loss: 4.391, avg. samples / sec: 56973.75
Iteration:   1140, Loss function: 4.860, Average Loss: 4.371, avg. samples / sec: 56972.69
Iteration:   1140, Loss function: 5.426, Average Loss: 4.382, avg. samples / sec: 56907.58
Iteration:   1140, Loss function: 4.649, Average Loss: 4.409, avg. samples / sec: 56788.87
Iteration:   1140, Loss function: 6.284, Average Loss: 4.414, avg. samples / sec: 56975.36
Iteration:   1140, Loss function: 7.208, Average Loss: 4.376, avg. samples / sec: 56930.09
Iteration:   1160, Loss function: 4.527, Average Loss: 4.393, avg. samples / sec: 58896.85
Iteration:   1160, Loss function: 3.988, Average Loss: 4.380, avg. samples / sec: 59118.96
Iteration:   1160, Loss function: 4.911, Average Loss: 4.397, avg. samples / sec: 58860.91
Iteration:   1160, Loss function: 4.011, Average Loss: 4.354, avg. samples / sec: 58870.28
Iteration:   1160, Loss function: 4.901, Average Loss: 4.398, avg. samples / sec: 58817.53
Iteration:   1160, Loss function: 5.485, Average Loss: 4.414, avg. samples / sec: 58800.94
Iteration:   1160, Loss function: 4.864, Average Loss: 4.390, avg. samples / sec: 58901.82
Iteration:   1160, Loss function: 4.512, Average Loss: 4.399, avg. samples / sec: 58834.62
Iteration:   1160, Loss function: 4.667, Average Loss: 4.399, avg. samples / sec: 58843.19
Iteration:   1160, Loss function: 4.527, Average Loss: 4.385, avg. samples / sec: 58810.41
Iteration:   1160, Loss function: 5.188, Average Loss: 4.415, avg. samples / sec: 58835.18
Iteration:   1160, Loss function: 4.823, Average Loss: 4.422, avg. samples / sec: 58819.27
Iteration:   1160, Loss function: 4.188, Average Loss: 4.382, avg. samples / sec: 58753.28
Iteration:   1160, Loss function: 4.812, Average Loss: 4.403, avg. samples / sec: 58711.52
Iteration:   1160, Loss function: 5.377, Average Loss: 4.399, avg. samples / sec: 58640.58
Iteration:   1180, Loss function: 6.354, Average Loss: 4.408, avg. samples / sec: 58051.55
Iteration:   1180, Loss function: 5.183, Average Loss: 4.409, avg. samples / sec: 58083.78
Iteration:   1180, Loss function: 6.007, Average Loss: 4.404, avg. samples / sec: 58133.50
Iteration:   1180, Loss function: 5.769, Average Loss: 4.392, avg. samples / sec: 58032.95
Iteration:   1180, Loss function: 5.083, Average Loss: 4.411, avg. samples / sec: 58130.98
Iteration:   1180, Loss function: 4.844, Average Loss: 4.405, avg. samples / sec: 58099.30
Iteration:   1180, Loss function: 5.499, Average Loss: 4.413, avg. samples / sec: 58128.51
Iteration:   1180, Loss function: 5.960, Average Loss: 4.390, avg. samples / sec: 58216.54
Iteration:   1180, Loss function: 5.794, Average Loss: 4.418, avg. samples / sec: 58253.57
Iteration:   1180, Loss function: 5.949, Average Loss: 4.431, avg. samples / sec: 58092.45
Iteration:   1180, Loss function: 7.749, Average Loss: 4.397, avg. samples / sec: 58106.27
Iteration:   1180, Loss function: 4.722, Average Loss: 4.372, avg. samples / sec: 57952.17
Iteration:   1180, Loss function: 5.816, Average Loss: 4.437, avg. samples / sec: 58107.75
Iteration:   1180, Loss function: 5.006, Average Loss: 4.413, avg. samples / sec: 58130.04
Iteration:   1180, Loss function: 5.743, Average Loss: 4.424, avg. samples / sec: 58004.41
:::MLL 1558639669.799 epoch_stop: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 819}}
:::MLL 1558639669.799 epoch_start: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 673}}
Iteration:   1200, Loss function: 5.684, Average Loss: 4.405, avg. samples / sec: 59558.16
Iteration:   1200, Loss function: 4.350, Average Loss: 4.412, avg. samples / sec: 59403.42
Iteration:   1200, Loss function: 4.620, Average Loss: 4.430, avg. samples / sec: 59566.52
Iteration:   1200, Loss function: 4.514, Average Loss: 4.387, avg. samples / sec: 59455.02
Iteration:   1200, Loss function: 5.829, Average Loss: 4.416, avg. samples / sec: 59363.73
Iteration:   1200, Loss function: 5.131, Average Loss: 4.439, avg. samples / sec: 59510.91
Iteration:   1200, Loss function: 4.079, Average Loss: 4.418, avg. samples / sec: 59294.17
Iteration:   1200, Loss function: 5.333, Average Loss: 4.427, avg. samples / sec: 59284.97
Iteration:   1200, Loss function: 6.345, Average Loss: 4.428, avg. samples / sec: 59246.64
Iteration:   1200, Loss function: 4.079, Average Loss: 4.437, avg. samples / sec: 59266.30
Iteration:   1200, Loss function: 4.768, Average Loss: 4.421, avg. samples / sec: 59209.55
Iteration:   1200, Loss function: 5.252, Average Loss: 4.427, avg. samples / sec: 59185.78
Iteration:   1200, Loss function: 4.636, Average Loss: 4.425, avg. samples / sec: 59137.02
Iteration:   1200, Loss function: 4.801, Average Loss: 4.451, avg. samples / sec: 59318.83
Iteration:   1200, Loss function: 5.332, Average Loss: 4.434, avg. samples / sec: 59122.91
Iteration:   1220, Loss function: 4.462, Average Loss: 4.434, avg. samples / sec: 58573.16
Iteration:   1220, Loss function: 6.126, Average Loss: 4.414, avg. samples / sec: 58223.70
Iteration:   1220, Loss function: 3.781, Average Loss: 4.461, avg. samples / sec: 58541.02
Iteration:   1220, Loss function: 4.327, Average Loss: 4.417, avg. samples / sec: 58306.43
Iteration:   1220, Loss function: 3.295, Average Loss: 4.391, avg. samples / sec: 58370.04
Iteration:   1220, Loss function: 4.737, Average Loss: 4.434, avg. samples / sec: 58299.96
Iteration:   1220, Loss function: 4.755, Average Loss: 4.427, avg. samples / sec: 58414.41
Iteration:   1220, Loss function: 5.254, Average Loss: 4.434, avg. samples / sec: 58452.71
Iteration:   1220, Loss function: 3.377, Average Loss: 4.443, avg. samples / sec: 58364.14
Iteration:   1220, Loss function: 4.800, Average Loss: 4.423, avg. samples / sec: 58326.00
Iteration:   1220, Loss function: 5.492, Average Loss: 4.445, avg. samples / sec: 58392.17
Iteration:   1220, Loss function: 3.776, Average Loss: 4.425, avg. samples / sec: 58361.16
Iteration:   1220, Loss function: 6.113, Average Loss: 4.429, avg. samples / sec: 58282.24
Iteration:   1220, Loss function: 3.772, Average Loss: 4.442, avg. samples / sec: 58407.87
Iteration:   1220, Loss function: 4.991, Average Loss: 4.428, avg. samples / sec: 58218.65
Iteration:   1240, Loss function: 3.601, Average Loss: 4.432, avg. samples / sec: 60592.75
Iteration:   1240, Loss function: 3.978, Average Loss: 4.420, avg. samples / sec: 60420.77
Iteration:   1240, Loss function: 5.744, Average Loss: 4.452, avg. samples / sec: 60502.90
Iteration:   1240, Loss function: 4.478, Average Loss: 4.432, avg. samples / sec: 60577.72
Iteration:   1240, Loss function: 4.677, Average Loss: 4.427, avg. samples / sec: 60640.07
Iteration:   1240, Loss function: 4.708, Average Loss: 4.389, avg. samples / sec: 60325.70
Iteration:   1240, Loss function: 5.323, Average Loss: 4.448, avg. samples / sec: 60400.32
Iteration:   1240, Loss function: 4.544, Average Loss: 4.467, avg. samples / sec: 60256.52
Iteration:   1240, Loss function: 4.062, Average Loss: 4.426, avg. samples / sec: 60240.60
Iteration:   1240, Loss function: 4.664, Average Loss: 4.435, avg. samples / sec: 60241.45
Iteration:   1240, Loss function: 5.068, Average Loss: 4.451, avg. samples / sec: 60417.15
Iteration:   1240, Loss function: 5.283, Average Loss: 4.442, avg. samples / sec: 60133.67
Iteration:   1240, Loss function: 4.950, Average Loss: 4.440, avg. samples / sec: 60391.59
Iteration:   1240, Loss function: 4.443, Average Loss: 4.441, avg. samples / sec: 60074.13
Iteration:   1240, Loss function: 4.135, Average Loss: 4.441, avg. samples / sec: 60072.82
:::MLL 1558639671.781 epoch_stop: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 819}}
:::MLL 1558639671.781 epoch_start: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 673}}
Iteration:   1260, Loss function: 5.027, Average Loss: 4.443, avg. samples / sec: 60121.10
Iteration:   1260, Loss function: 5.261, Average Loss: 4.457, avg. samples / sec: 59920.36
Iteration:   1260, Loss function: 3.706, Average Loss: 4.469, avg. samples / sec: 59962.81
Iteration:   1260, Loss function: 4.041, Average Loss: 4.434, avg. samples / sec: 59984.71
Iteration:   1260, Loss function: 3.799, Average Loss: 4.436, avg. samples / sec: 59786.88
Iteration:   1260, Loss function: 4.584, Average Loss: 4.452, avg. samples / sec: 60152.38
Iteration:   1260, Loss function: 4.073, Average Loss: 4.456, avg. samples / sec: 59956.28
Iteration:   1260, Loss function: 5.213, Average Loss: 4.464, avg. samples / sec: 59945.31
Iteration:   1260, Loss function: 5.662, Average Loss: 4.392, avg. samples / sec: 59809.33
Iteration:   1260, Loss function: 4.357, Average Loss: 4.458, avg. samples / sec: 59665.05
Iteration:   1260, Loss function: 4.262, Average Loss: 4.439, avg. samples / sec: 59582.16
Iteration:   1260, Loss function: 6.068, Average Loss: 4.430, avg. samples / sec: 59689.67
Iteration:   1260, Loss function: 4.507, Average Loss: 4.445, avg. samples / sec: 60011.27
Iteration:   1260, Loss function: 5.450, Average Loss: 4.429, avg. samples / sec: 59574.86
Iteration:   1260, Loss function: 4.224, Average Loss: 4.450, avg. samples / sec: 59798.60
Iteration:   1280, Loss function: 5.361, Average Loss: 4.436, avg. samples / sec: 57474.26
Iteration:   1280, Loss function: 4.142, Average Loss: 4.450, avg. samples / sec: 57421.88
Iteration:   1280, Loss function: 4.920, Average Loss: 4.465, avg. samples / sec: 57241.42
Iteration:   1280, Loss function: 3.659, Average Loss: 4.394, avg. samples / sec: 57325.43
Iteration:   1280, Loss function: 4.312, Average Loss: 4.461, avg. samples / sec: 57314.71
Iteration:   1280, Loss function: 3.971, Average Loss: 4.479, avg. samples / sec: 57217.77
Iteration:   1280, Loss function: 4.900, Average Loss: 4.441, avg. samples / sec: 57352.63
Iteration:   1280, Loss function: 3.916, Average Loss: 4.453, avg. samples / sec: 57410.13
Iteration:   1280, Loss function: 5.747, Average Loss: 4.440, avg. samples / sec: 57183.87
Iteration:   1280, Loss function: 5.969, Average Loss: 4.448, avg. samples / sec: 57295.86
Iteration:   1280, Loss function: 4.848, Average Loss: 4.446, avg. samples / sec: 57057.64
Iteration:   1280, Loss function: 5.182, Average Loss: 4.467, avg. samples / sec: 57198.68
Iteration:   1280, Loss function: 5.594, Average Loss: 4.447, avg. samples / sec: 57049.81
Iteration:   1280, Loss function: 4.669, Average Loss: 4.468, avg. samples / sec: 57098.61
Iteration:   1280, Loss function: 4.880, Average Loss: 4.458, avg. samples / sec: 57030.86
Iteration:   1300, Loss function: 4.186, Average Loss: 4.442, avg. samples / sec: 57275.90
Iteration:   1300, Loss function: 5.134, Average Loss: 4.468, avg. samples / sec: 57400.08
Iteration:   1300, Loss function: 5.926, Average Loss: 4.445, avg. samples / sec: 57199.37
Iteration:   1300, Loss function: 4.517, Average Loss: 4.484, avg. samples / sec: 57196.82
Iteration:   1300, Loss function: 4.637, Average Loss: 4.397, avg. samples / sec: 57165.13
Iteration:   1300, Loss function: 4.944, Average Loss: 4.453, avg. samples / sec: 57070.38
Iteration:   1300, Loss function: 4.769, Average Loss: 4.458, avg. samples / sec: 57132.40
Iteration:   1300, Loss function: 4.007, Average Loss: 4.451, avg. samples / sec: 57137.71
Iteration:   1300, Loss function: 4.183, Average Loss: 4.470, avg. samples / sec: 57043.63
Iteration:   1300, Loss function: 2.844, Average Loss: 4.454, avg. samples / sec: 57236.43
Iteration:   1300, Loss function: 5.038, Average Loss: 4.457, avg. samples / sec: 57128.54
Iteration:   1300, Loss function: 4.477, Average Loss: 4.464, avg. samples / sec: 57045.20
Iteration:   1300, Loss function: 4.099, Average Loss: 4.464, avg. samples / sec: 57269.73
Iteration:   1300, Loss function: 4.454, Average Loss: 4.457, avg. samples / sec: 57100.71
Iteration:   1300, Loss function: 4.920, Average Loss: 4.472, avg. samples / sec: 57206.15
Iteration:   1320, Loss function: 4.850, Average Loss: 4.445, avg. samples / sec: 57630.70
Iteration:   1320, Loss function: 5.547, Average Loss: 4.487, avg. samples / sec: 57599.77
Iteration:   1320, Loss function: 4.347, Average Loss: 4.466, avg. samples / sec: 57722.10
Iteration:   1320, Loss function: 4.652, Average Loss: 4.406, avg. samples / sec: 57567.59
Iteration:   1320, Loss function: 5.318, Average Loss: 4.465, avg. samples / sec: 57675.56
Iteration:   1320, Loss function: 4.102, Average Loss: 4.477, avg. samples / sec: 57709.24
Iteration:   1320, Loss function: 3.633, Average Loss: 4.465, avg. samples / sec: 57613.24
Iteration:   1320, Loss function: 4.625, Average Loss: 4.474, avg. samples / sec: 57624.08
Iteration:   1320, Loss function: 5.473, Average Loss: 4.457, avg. samples / sec: 57546.95
Iteration:   1320, Loss function: 3.671, Average Loss: 4.475, avg. samples / sec: 57403.12
Iteration:   1320, Loss function: 4.022, Average Loss: 4.475, avg. samples / sec: 57594.10
Iteration:   1320, Loss function: 6.258, Average Loss: 4.462, avg. samples / sec: 57591.98
Iteration:   1320, Loss function: 4.477, Average Loss: 4.462, avg. samples / sec: 57523.15
Iteration:   1320, Loss function: 5.312, Average Loss: 4.449, avg. samples / sec: 57274.50
Iteration:   1320, Loss function: 3.753, Average Loss: 4.455, avg. samples / sec: 57526.81
:::MLL 1558639673.818 epoch_stop: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 819}}
:::MLL 1558639673.818 epoch_start: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 673}}
Iteration:   1340, Loss function: 4.825, Average Loss: 4.464, avg. samples / sec: 59629.51
Iteration:   1340, Loss function: 4.517, Average Loss: 4.470, avg. samples / sec: 59553.53
Iteration:   1340, Loss function: 3.636, Average Loss: 4.464, avg. samples / sec: 59461.77
Iteration:   1340, Loss function: 4.362, Average Loss: 4.408, avg. samples / sec: 59320.93
Iteration:   1340, Loss function: 4.041, Average Loss: 4.490, avg. samples / sec: 59261.54
Iteration:   1340, Loss function: 4.166, Average Loss: 4.466, avg. samples / sec: 59443.03
Iteration:   1340, Loss function: 6.144, Average Loss: 4.478, avg. samples / sec: 59410.61
Iteration:   1340, Loss function: 4.240, Average Loss: 4.473, avg. samples / sec: 59323.15
Iteration:   1340, Loss function: 3.885, Average Loss: 4.457, avg. samples / sec: 59349.51
Iteration:   1340, Loss function: 4.464, Average Loss: 4.450, avg. samples / sec: 59332.22
Iteration:   1340, Loss function: 5.352, Average Loss: 4.477, avg. samples / sec: 59291.68
Iteration:   1340, Loss function: 5.195, Average Loss: 4.470, avg. samples / sec: 59212.06
Iteration:   1340, Loss function: 5.175, Average Loss: 4.454, avg. samples / sec: 58998.83
Iteration:   1340, Loss function: 4.350, Average Loss: 4.486, avg. samples / sec: 59081.27
Iteration:   1340, Loss function: 6.031, Average Loss: 4.470, avg. samples / sec: 58970.34
Iteration:   1360, Loss function: 5.018, Average Loss: 4.479, avg. samples / sec: 60712.88
Iteration:   1360, Loss function: 6.078, Average Loss: 4.462, avg. samples / sec: 60599.91
Iteration:   1360, Loss function: 3.805, Average Loss: 4.468, avg. samples / sec: 60028.71
Iteration:   1360, Loss function: 4.034, Average Loss: 4.492, avg. samples / sec: 60535.98
Iteration:   1360, Loss function: 4.332, Average Loss: 4.483, avg. samples / sec: 60328.95
Iteration:   1360, Loss function: 4.555, Average Loss: 4.477, avg. samples / sec: 60381.50
Iteration:   1360, Loss function: 4.816, Average Loss: 4.415, avg. samples / sec: 60226.29
Iteration:   1360, Loss function: 4.605, Average Loss: 4.496, avg. samples / sec: 60221.32
Iteration:   1360, Loss function: 4.467, Average Loss: 4.483, avg. samples / sec: 60181.64
Iteration:   1360, Loss function: 3.748, Average Loss: 4.452, avg. samples / sec: 60298.73
Iteration:   1360, Loss function: 4.572, Average Loss: 4.462, avg. samples / sec: 60280.96
Iteration:   1360, Loss function: 5.307, Average Loss: 4.473, avg. samples / sec: 60145.84
Iteration:   1360, Loss function: 4.316, Average Loss: 4.474, avg. samples / sec: 59922.86
Iteration:   1360, Loss function: 5.180, Average Loss: 4.473, avg. samples / sec: 59929.20
Iteration:   1360, Loss function: 3.811, Average Loss: 4.481, avg. samples / sec: 60042.82
Iteration:   1380, Loss function: 5.673, Average Loss: 4.481, avg. samples / sec: 58426.52
Iteration:   1380, Loss function: 4.519, Average Loss: 4.467, avg. samples / sec: 58190.07
Iteration:   1380, Loss function: 4.157, Average Loss: 4.421, avg. samples / sec: 58251.00
Iteration:   1380, Loss function: 4.539, Average Loss: 4.496, avg. samples / sec: 58251.77
Iteration:   1380, Loss function: 3.820, Average Loss: 4.464, avg. samples / sec: 58302.25
Iteration:   1380, Loss function: 5.286, Average Loss: 4.484, avg. samples / sec: 58206.22
Iteration:   1380, Loss function: 4.234, Average Loss: 4.475, avg. samples / sec: 58445.22
Iteration:   1380, Loss function: 4.390, Average Loss: 4.457, avg. samples / sec: 58205.67
Iteration:   1380, Loss function: 4.764, Average Loss: 4.486, avg. samples / sec: 58010.31
Iteration:   1380, Loss function: 4.251, Average Loss: 4.477, avg. samples / sec: 58263.64
Iteration:   1380, Loss function: 5.189, Average Loss: 4.484, avg. samples / sec: 58152.73
Iteration:   1380, Loss function: 4.797, Average Loss: 4.473, avg. samples / sec: 58024.49
Iteration:   1380, Loss function: 6.069, Average Loss: 4.485, avg. samples / sec: 58337.63
Iteration:   1380, Loss function: 5.784, Average Loss: 4.479, avg. samples / sec: 58010.00
Iteration:   1380, Loss function: 4.783, Average Loss: 4.496, avg. samples / sec: 57779.46
:::MLL 1558639675.812 epoch_stop: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 819}}
:::MLL 1558639675.812 epoch_start: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 673}}
Iteration:   1400, Loss function: 4.378, Average Loss: 4.467, avg. samples / sec: 58988.78
Iteration:   1400, Loss function: 3.760, Average Loss: 4.494, avg. samples / sec: 58964.39
Iteration:   1400, Loss function: 4.828, Average Loss: 4.480, avg. samples / sec: 59178.05
Iteration:   1400, Loss function: 5.122, Average Loss: 4.499, avg. samples / sec: 59355.31
Iteration:   1400, Loss function: 4.180, Average Loss: 4.470, avg. samples / sec: 58812.79
Iteration:   1400, Loss function: 3.867, Average Loss: 4.487, avg. samples / sec: 58934.16
Iteration:   1400, Loss function: 3.918, Average Loss: 4.483, avg. samples / sec: 58852.50
Iteration:   1400, Loss function: 4.982, Average Loss: 4.486, avg. samples / sec: 59028.98
Iteration:   1400, Loss function: 4.738, Average Loss: 4.483, avg. samples / sec: 58723.80
Iteration:   1400, Loss function: 4.885, Average Loss: 4.477, avg. samples / sec: 58825.65
Iteration:   1400, Loss function: 5.156, Average Loss: 4.485, avg. samples / sec: 58918.15
Iteration:   1400, Loss function: 3.707, Average Loss: 4.477, avg. samples / sec: 58936.60
Iteration:   1400, Loss function: 4.682, Average Loss: 4.480, avg. samples / sec: 58810.65
Iteration:   1400, Loss function: 2.789, Average Loss: 4.424, avg. samples / sec: 58615.31
Iteration:   1400, Loss function: 4.714, Average Loss: 4.463, avg. samples / sec: 58710.59
Iteration:   1420, Loss function: 4.263, Average Loss: 4.486, avg. samples / sec: 58776.39
Iteration:   1420, Loss function: 3.940, Average Loss: 4.485, avg. samples / sec: 58754.65
Iteration:   1420, Loss function: 4.570, Average Loss: 4.469, avg. samples / sec: 58694.26
Iteration:   1420, Loss function: 3.543, Average Loss: 4.458, avg. samples / sec: 58904.26
Iteration:   1420, Loss function: 4.708, Average Loss: 4.504, avg. samples / sec: 58625.09
Iteration:   1420, Loss function: 3.259, Average Loss: 4.481, avg. samples / sec: 58548.39
Iteration:   1420, Loss function: 4.773, Average Loss: 4.478, avg. samples / sec: 58716.24
Iteration:   1420, Loss function: 4.863, Average Loss: 4.486, avg. samples / sec: 58645.29
Iteration:   1420, Loss function: 4.731, Average Loss: 4.496, avg. samples / sec: 58497.14
Iteration:   1420, Loss function: 4.751, Average Loss: 4.472, avg. samples / sec: 58446.99
Iteration:   1420, Loss function: 5.881, Average Loss: 4.487, avg. samples / sec: 58611.43
Iteration:   1420, Loss function: 5.162, Average Loss: 4.482, avg. samples / sec: 58524.03
Iteration:   1420, Loss function: 4.757, Average Loss: 4.428, avg. samples / sec: 58693.89
Iteration:   1420, Loss function: 6.272, Average Loss: 4.477, avg. samples / sec: 58562.26
Iteration:   1420, Loss function: 4.689, Average Loss: 4.486, avg. samples / sec: 58572.95
Iteration:   1440, Loss function: 3.669, Average Loss: 4.488, avg. samples / sec: 57970.00
Iteration:   1440, Loss function: 4.387, Average Loss: 4.490, avg. samples / sec: 57757.61
Iteration:   1440, Loss function: 4.657, Average Loss: 4.482, avg. samples / sec: 57644.49
Iteration:   1440, Loss function: 5.741, Average Loss: 4.478, avg. samples / sec: 57718.17
Iteration:   1440, Loss function: 4.767, Average Loss: 4.485, avg. samples / sec: 57897.70
Iteration:   1440, Loss function: 4.203, Average Loss: 4.492, avg. samples / sec: 57596.17
Iteration:   1440, Loss function: 4.145, Average Loss: 4.468, avg. samples / sec: 57575.58
Iteration:   1440, Loss function: 6.276, Average Loss: 4.490, avg. samples / sec: 57765.30
Iteration:   1440, Loss function: 4.365, Average Loss: 4.488, avg. samples / sec: 57634.49
Iteration:   1440, Loss function: 3.915, Average Loss: 4.457, avg. samples / sec: 57588.12
Iteration:   1440, Loss function: 3.723, Average Loss: 4.502, avg. samples / sec: 57634.99
Iteration:   1440, Loss function: 4.698, Average Loss: 4.512, avg. samples / sec: 57565.37
Iteration:   1440, Loss function: 5.581, Average Loss: 4.490, avg. samples / sec: 57642.16
Iteration:   1440, Loss function: 3.887, Average Loss: 4.435, avg. samples / sec: 57623.39
Iteration:   1440, Loss function: 4.896, Average Loss: 4.474, avg. samples / sec: 57486.03
Iteration:   1460, Loss function: 4.841, Average Loss: 4.493, avg. samples / sec: 60309.18
Iteration:   1460, Loss function: 4.200, Average Loss: 4.482, avg. samples / sec: 60257.19
Iteration:   1460, Loss function: 4.337, Average Loss: 4.484, avg. samples / sec: 60194.78
Iteration:   1460, Loss function: 4.786, Average Loss: 4.479, avg. samples / sec: 60541.63
Iteration:   1460, Loss function: 4.150, Average Loss: 4.486, avg. samples / sec: 60211.62
Iteration:   1460, Loss function: 3.927, Average Loss: 4.458, avg. samples / sec: 60299.94
Iteration:   1460, Loss function: 4.163, Average Loss: 4.466, avg. samples / sec: 60256.50
Iteration:   1460, Loss function: 5.127, Average Loss: 4.485, avg. samples / sec: 60127.18
Iteration:   1460, Loss function: 4.058, Average Loss: 4.490, avg. samples / sec: 60145.56
Iteration:   1460, Loss function: 4.693, Average Loss: 4.494, avg. samples / sec: 60328.82
Iteration:   1460, Loss function: 5.473, Average Loss: 4.517, avg. samples / sec: 60267.34
Iteration:   1460, Loss function: 3.700, Average Loss: 4.502, avg. samples / sec: 60261.08
Iteration:   1460, Loss function: 4.450, Average Loss: 4.437, avg. samples / sec: 60398.81
Iteration:   1460, Loss function: 4.790, Average Loss: 4.486, avg. samples / sec: 60191.97
Iteration:   1460, Loss function: 4.177, Average Loss: 4.493, avg. samples / sec: 60120.82
:::MLL 1558639677.807 epoch_stop: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 819}}
:::MLL 1558639677.808 epoch_start: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 673}}
Iteration:   1480, Loss function: 4.608, Average Loss: 4.501, avg. samples / sec: 59544.85
Iteration:   1480, Loss function: 4.436, Average Loss: 4.488, avg. samples / sec: 59435.26
Iteration:   1480, Loss function: 3.353, Average Loss: 4.492, avg. samples / sec: 59352.81
Iteration:   1480, Loss function: 5.054, Average Loss: 4.483, avg. samples / sec: 59351.01
Iteration:   1480, Loss function: 3.471, Average Loss: 4.513, avg. samples / sec: 59455.95
Iteration:   1480, Loss function: 3.995, Average Loss: 4.462, avg. samples / sec: 59377.34
Iteration:   1480, Loss function: 5.248, Average Loss: 4.462, avg. samples / sec: 59346.71
Iteration:   1480, Loss function: 4.630, Average Loss: 4.488, avg. samples / sec: 59417.35
Iteration:   1480, Loss function: 4.777, Average Loss: 4.492, avg. samples / sec: 59373.49
Iteration:   1480, Loss function: 4.352, Average Loss: 4.479, avg. samples / sec: 59171.19
Iteration:   1480, Loss function: 4.255, Average Loss: 4.508, avg. samples / sec: 59243.85
Iteration:   1480, Loss function: 3.693, Average Loss: 4.433, avg. samples / sec: 59228.68
Iteration:   1480, Loss function: 4.534, Average Loss: 4.491, avg. samples / sec: 59138.24
Iteration:   1480, Loss function: 4.567, Average Loss: 4.481, avg. samples / sec: 59071.83
Iteration:   1480, Loss function: 4.920, Average Loss: 4.492, avg. samples / sec: 59133.92
Iteration:   1500, Loss function: 4.514, Average Loss: 4.493, avg. samples / sec: 59812.74
Iteration:   1500, Loss function: 4.571, Average Loss: 4.487, avg. samples / sec: 59606.51
Iteration:   1500, Loss function: 4.777, Average Loss: 4.482, avg. samples / sec: 59901.18
Iteration:   1500, Loss function: 3.581, Average Loss: 4.512, avg. samples / sec: 59816.49
Iteration:   1500, Loss function: 4.923, Average Loss: 4.495, avg. samples / sec: 59543.27
Iteration:   1500, Loss function: 4.803, Average Loss: 4.483, avg. samples / sec: 59772.45
Iteration:   1500, Loss function: 5.965, Average Loss: 4.501, avg. samples / sec: 59519.58
Iteration:   1500, Loss function: 4.042, Average Loss: 4.494, avg. samples / sec: 59864.13
Iteration:   1500, Loss function: 3.992, Average Loss: 4.464, avg. samples / sec: 59579.49
Iteration:   1500, Loss function: 3.694, Average Loss: 4.496, avg. samples / sec: 59709.87
Iteration:   1500, Loss function: 4.167, Average Loss: 4.438, avg. samples / sec: 59746.10
Iteration:   1500, Loss function: 4.981, Average Loss: 4.491, avg. samples / sec: 59742.93
Iteration:   1500, Loss function: 5.572, Average Loss: 4.501, avg. samples / sec: 59355.71
Iteration:   1500, Loss function: 5.627, Average Loss: 4.519, avg. samples / sec: 59438.70
Iteration:   1500, Loss function: 5.455, Average Loss: 4.461, avg. samples / sec: 59436.69
Iteration:   1520, Loss function: 3.772, Average Loss: 4.495, avg. samples / sec: 59380.29
Iteration:   1520, Loss function: 3.700, Average Loss: 4.502, avg. samples / sec: 59445.14
Iteration:   1520, Loss function: 4.636, Average Loss: 4.500, avg. samples / sec: 59308.87
Iteration:   1520, Loss function: 4.016, Average Loss: 4.489, avg. samples / sec: 59223.91
Iteration:   1520, Loss function: 3.940, Average Loss: 4.512, avg. samples / sec: 59387.57
Iteration:   1520, Loss function: 4.152, Average Loss: 4.466, avg. samples / sec: 59274.75
Iteration:   1520, Loss function: 5.062, Average Loss: 4.464, avg. samples / sec: 59441.38
Iteration:   1520, Loss function: 3.443, Average Loss: 4.493, avg. samples / sec: 59211.99
Iteration:   1520, Loss function: 5.268, Average Loss: 4.494, avg. samples / sec: 59037.56
Iteration:   1520, Loss function: 3.583, Average Loss: 4.437, avg. samples / sec: 59274.72
Iteration:   1520, Loss function: 4.029, Average Loss: 4.491, avg. samples / sec: 59289.36
Iteration:   1520, Loss function: 4.429, Average Loss: 4.509, avg. samples / sec: 59149.91
Iteration:   1520, Loss function: 4.635, Average Loss: 4.492, avg. samples / sec: 59169.92
Iteration:   1520, Loss function: 4.779, Average Loss: 4.485, avg. samples / sec: 59098.61
Iteration:   1520, Loss function: 3.501, Average Loss: 4.481, avg. samples / sec: 59118.37
:::MLL 1558639679.781 epoch_stop: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 819}}
:::MLL 1558639679.781 epoch_start: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 673}}
Iteration:   1540, Loss function: 4.486, Average Loss: 4.491, avg. samples / sec: 60319.76
Iteration:   1540, Loss function: 4.433, Average Loss: 4.476, avg. samples / sec: 60375.16
Iteration:   1540, Loss function: 4.224, Average Loss: 4.506, avg. samples / sec: 60262.27
Iteration:   1540, Loss function: 4.715, Average Loss: 4.441, avg. samples / sec: 60233.68
Iteration:   1540, Loss function: 4.332, Average Loss: 4.491, avg. samples / sec: 60216.15
Iteration:   1540, Loss function: 3.631, Average Loss: 4.493, avg. samples / sec: 60035.64
Iteration:   1540, Loss function: 4.259, Average Loss: 4.493, avg. samples / sec: 60225.88
Iteration:   1540, Loss function: 4.612, Average Loss: 4.491, avg. samples / sec: 60112.79
Iteration:   1540, Loss function: 5.116, Average Loss: 4.488, avg. samples / sec: 60224.85
Iteration:   1540, Loss function: 4.715, Average Loss: 4.466, avg. samples / sec: 60102.13
Iteration:   1540, Loss function: 3.440, Average Loss: 4.486, avg. samples / sec: 60013.52
Iteration:   1540, Loss function: 4.213, Average Loss: 4.502, avg. samples / sec: 59966.00
Iteration:   1540, Loss function: 4.719, Average Loss: 4.460, avg. samples / sec: 60009.66
Iteration:   1540, Loss function: 5.327, Average Loss: 4.500, avg. samples / sec: 59954.65
Iteration:   1540, Loss function: 4.260, Average Loss: 4.511, avg. samples / sec: 59984.10
Iteration:   1560, Loss function: 4.453, Average Loss: 4.511, avg. samples / sec: 55645.85
Iteration:   1560, Loss function: 4.974, Average Loss: 4.505, avg. samples / sec: 55620.52
Iteration:   1560, Loss function: 4.382, Average Loss: 4.488, avg. samples / sec: 55501.78
Iteration:   1560, Loss function: 2.873, Average Loss: 4.501, avg. samples / sec: 55530.89
Iteration:   1560, Loss function: 3.475, Average Loss: 4.465, avg. samples / sec: 55428.28
Iteration:   1560, Loss function: 5.172, Average Loss: 4.458, avg. samples / sec: 55476.28
Iteration:   1560, Loss function: 4.491, Average Loss: 4.490, avg. samples / sec: 55388.13
Iteration:   1560, Loss function: 4.300, Average Loss: 4.488, avg. samples / sec: 55260.77
Iteration:   1560, Loss function: 3.823, Average Loss: 4.492, avg. samples / sec: 55353.73
Iteration:   1560, Loss function: 4.148, Average Loss: 4.476, avg. samples / sec: 55249.85
Iteration:   1560, Loss function: 4.699, Average Loss: 4.487, avg. samples / sec: 55413.39
Iteration:   1560, Loss function: 4.065, Average Loss: 4.502, avg. samples / sec: 55265.65
Iteration:   1560, Loss function: 5.692, Average Loss: 4.490, avg. samples / sec: 55280.06
Iteration:   1560, Loss function: 4.281, Average Loss: 4.445, avg. samples / sec: 55258.45
Iteration:   1560, Loss function: 4.130, Average Loss: 4.492, avg. samples / sec: 55267.62
Iteration:   1580, Loss function: 4.342, Average Loss: 4.490, avg. samples / sec: 57257.77
Iteration:   1580, Loss function: 5.142, Average Loss: 4.495, avg. samples / sec: 57317.13
Iteration:   1580, Loss function: 4.819, Average Loss: 4.491, avg. samples / sec: 57103.10
Iteration:   1580, Loss function: 5.044, Average Loss: 4.444, avg. samples / sec: 57274.55
Iteration:   1580, Loss function: 4.320, Average Loss: 4.466, avg. samples / sec: 57164.80
Iteration:   1580, Loss function: 4.335, Average Loss: 4.492, avg. samples / sec: 57174.94
Iteration:   1580, Loss function: 4.458, Average Loss: 4.462, avg. samples / sec: 57155.00
Iteration:   1580, Loss function: 4.085, Average Loss: 4.490, avg. samples / sec: 57220.39
Iteration:   1580, Loss function: 5.107, Average Loss: 4.489, avg. samples / sec: 57159.91
Iteration:   1580, Loss function: 5.126, Average Loss: 4.505, avg. samples / sec: 56971.91
Iteration:   1580, Loss function: 4.266, Average Loss: 4.507, avg. samples / sec: 56937.79
Iteration:   1580, Loss function: 4.058, Average Loss: 4.493, avg. samples / sec: 57112.17
Iteration:   1580, Loss function: 4.457, Average Loss: 4.480, avg. samples / sec: 57123.77
Iteration:   1580, Loss function: 4.137, Average Loss: 4.499, avg. samples / sec: 56976.35
Iteration:   1580, Loss function: 4.003, Average Loss: 4.495, avg. samples / sec: 57036.26
Iteration:   1600, Loss function: 4.242, Average Loss: 4.500, avg. samples / sec: 56801.87
Iteration:   1600, Loss function: 4.896, Average Loss: 4.501, avg. samples / sec: 56879.59
Iteration:   1600, Loss function: 4.962, Average Loss: 4.487, avg. samples / sec: 56774.73
Iteration:   1600, Loss function: 4.152, Average Loss: 4.466, avg. samples / sec: 56796.05
Iteration:   1600, Loss function: 4.537, Average Loss: 4.489, avg. samples / sec: 56781.64
Iteration:   1600, Loss function: 4.719, Average Loss: 4.480, avg. samples / sec: 56885.49
Iteration:   1600, Loss function: 4.478, Average Loss: 4.499, avg. samples / sec: 57005.35
Iteration:   1600, Loss function: 4.507, Average Loss: 4.493, avg. samples / sec: 56811.55
Iteration:   1600, Loss function: 4.969, Average Loss: 4.507, avg. samples / sec: 56849.89
Iteration:   1600, Loss function: 4.929, Average Loss: 4.492, avg. samples / sec: 56854.92
Iteration:   1600, Loss function: 4.523, Average Loss: 4.504, avg. samples / sec: 56852.23
Iteration:   1600, Loss function: 3.800, Average Loss: 4.461, avg. samples / sec: 56755.96
Iteration:   1600, Loss function: 3.900, Average Loss: 4.487, avg. samples / sec: 56756.64
Iteration:   1600, Loss function: 5.101, Average Loss: 4.483, avg. samples / sec: 56736.10
Iteration:   1600, Loss function: 4.763, Average Loss: 4.446, avg. samples / sec: 56653.31
:::MLL 1558639681.845 epoch_stop: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 819}}
:::MLL 1558639681.845 epoch_start: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 673}}
Iteration:   1620, Loss function: 4.658, Average Loss: 4.486, avg. samples / sec: 58846.90
Iteration:   1620, Loss function: 4.831, Average Loss: 4.463, avg. samples / sec: 58753.50
Iteration:   1620, Loss function: 3.995, Average Loss: 4.485, avg. samples / sec: 58764.57
Iteration:   1620, Loss function: 4.389, Average Loss: 4.488, avg. samples / sec: 58594.64
Iteration:   1620, Loss function: 3.630, Average Loss: 4.466, avg. samples / sec: 58573.77
Iteration:   1620, Loss function: 4.294, Average Loss: 4.489, avg. samples / sec: 58552.67
Iteration:   1620, Loss function: 5.031, Average Loss: 4.493, avg. samples / sec: 58583.07
Iteration:   1620, Loss function: 5.158, Average Loss: 4.500, avg. samples / sec: 58557.83
Iteration:   1620, Loss function: 3.616, Average Loss: 4.478, avg. samples / sec: 58548.08
Iteration:   1620, Loss function: 4.361, Average Loss: 4.492, avg. samples / sec: 58542.85
Iteration:   1620, Loss function: 4.660, Average Loss: 4.447, avg. samples / sec: 58610.75
Iteration:   1620, Loss function: 3.658, Average Loss: 4.506, avg. samples / sec: 58459.43
Iteration:   1620, Loss function: 3.792, Average Loss: 4.497, avg. samples / sec: 58379.93
Iteration:   1620, Loss function: 3.311, Average Loss: 4.494, avg. samples / sec: 58370.01
Iteration:   1620, Loss function: 5.080, Average Loss: 4.501, avg. samples / sec: 58461.95
Iteration:   1640, Loss function: 3.417, Average Loss: 4.496, avg. samples / sec: 59990.17
Iteration:   1640, Loss function: 4.005, Average Loss: 4.491, avg. samples / sec: 60109.90
Iteration:   1640, Loss function: 3.377, Average Loss: 4.474, avg. samples / sec: 59940.34
Iteration:   1640, Loss function: 4.450, Average Loss: 4.500, avg. samples / sec: 60109.13
Iteration:   1640, Loss function: 5.976, Average Loss: 4.497, avg. samples / sec: 60060.86
Iteration:   1640, Loss function: 4.178, Average Loss: 4.504, avg. samples / sec: 60026.25
Iteration:   1640, Loss function: 4.990, Average Loss: 4.490, avg. samples / sec: 59913.23
Iteration:   1640, Loss function: 4.852, Average Loss: 4.488, avg. samples / sec: 59842.55
Iteration:   1640, Loss function: 4.404, Average Loss: 4.484, avg. samples / sec: 59644.37
Iteration:   1640, Loss function: 4.080, Average Loss: 4.495, avg. samples / sec: 59804.97
Iteration:   1640, Loss function: 3.754, Average Loss: 4.487, avg. samples / sec: 59746.55
Iteration:   1640, Loss function: 4.088, Average Loss: 4.466, avg. samples / sec: 59667.63
Iteration:   1640, Loss function: 3.497, Average Loss: 4.485, avg. samples / sec: 59649.19
Iteration:   1640, Loss function: 6.122, Average Loss: 4.468, avg. samples / sec: 59713.67
Iteration:   1640, Loss function: 5.685, Average Loss: 4.453, avg. samples / sec: 59822.26
Iteration:   1660, Loss function: 5.026, Average Loss: 4.504, avg. samples / sec: 59854.65
Iteration:   1660, Loss function: 3.326, Average Loss: 4.493, avg. samples / sec: 59694.04
Iteration:   1660, Loss function: 3.074, Average Loss: 4.499, avg. samples / sec: 59736.73
Iteration:   1660, Loss function: 5.390, Average Loss: 4.451, avg. samples / sec: 59917.35
Iteration:   1660, Loss function: 4.687, Average Loss: 4.466, avg. samples / sec: 59885.53
Iteration:   1660, Loss function: 3.839, Average Loss: 4.496, avg. samples / sec: 59818.68
Iteration:   1660, Loss function: 5.100, Average Loss: 4.496, avg. samples / sec: 59639.58
Iteration:   1660, Loss function: 5.396, Average Loss: 4.489, avg. samples / sec: 59600.28
Iteration:   1660, Loss function: 4.333, Average Loss: 4.473, avg. samples / sec: 59738.50
Iteration:   1660, Loss function: 3.995, Average Loss: 4.489, avg. samples / sec: 59680.04
Iteration:   1660, Loss function: 3.877, Average Loss: 4.486, avg. samples / sec: 59696.72
Iteration:   1660, Loss function: 5.178, Average Loss: 4.490, avg. samples / sec: 59622.19
Iteration:   1660, Loss function: 4.119, Average Loss: 4.488, avg. samples / sec: 59665.89
Iteration:   1660, Loss function: 3.478, Average Loss: 4.496, avg. samples / sec: 59580.40
Iteration:   1660, Loss function: 4.713, Average Loss: 4.478, avg. samples / sec: 59500.38
:::MLL 1558639683.828 epoch_stop: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 819}}
:::MLL 1558639683.828 epoch_start: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 673}}
Iteration:   1680, Loss function: 3.400, Average Loss: 4.484, avg. samples / sec: 59994.67
Iteration:   1680, Loss function: 4.341, Average Loss: 4.493, avg. samples / sec: 59794.34
Iteration:   1680, Loss function: 3.870, Average Loss: 4.487, avg. samples / sec: 59834.68
Iteration:   1680, Loss function: 3.825, Average Loss: 4.485, avg. samples / sec: 59707.72
Iteration:   1680, Loss function: 4.660, Average Loss: 4.486, avg. samples / sec: 59708.13
Iteration:   1680, Loss function: 4.969, Average Loss: 4.496, avg. samples / sec: 59499.22
Iteration:   1680, Loss function: 3.577, Average Loss: 4.469, avg. samples / sec: 59543.47
Iteration:   1680, Loss function: 4.317, Average Loss: 4.498, avg. samples / sec: 59693.53
Iteration:   1680, Loss function: 4.344, Average Loss: 4.468, avg. samples / sec: 59612.28
Iteration:   1680, Loss function: 4.574, Average Loss: 4.501, avg. samples / sec: 59397.41
Iteration:   1680, Loss function: 5.659, Average Loss: 4.443, avg. samples / sec: 59490.79
Iteration:   1680, Loss function: 5.229, Average Loss: 4.477, avg. samples / sec: 59697.71
Iteration:   1680, Loss function: 4.627, Average Loss: 4.490, avg. samples / sec: 59592.39
Iteration:   1680, Loss function: 4.209, Average Loss: 4.495, avg. samples / sec: 59428.29
Iteration:   1680, Loss function: 5.539, Average Loss: 4.496, avg. samples / sec: 59478.08
Iteration:   1700, Loss function: 3.553, Average Loss: 4.462, avg. samples / sec: 56535.65
Iteration:   1700, Loss function: 3.489, Average Loss: 4.480, avg. samples / sec: 56372.55
Iteration:   1700, Loss function: 4.143, Average Loss: 4.497, avg. samples / sec: 56541.11
Iteration:   1700, Loss function: 4.220, Average Loss: 4.496, avg. samples / sec: 56587.11
Iteration:   1700, Loss function: 4.211, Average Loss: 4.493, avg. samples / sec: 56449.66
Iteration:   1700, Loss function: 4.434, Average Loss: 4.485, avg. samples / sec: 56495.56
Iteration:   1700, Loss function: 3.244, Average Loss: 4.440, avg. samples / sec: 56485.71
Iteration:   1700, Loss function: 5.190, Average Loss: 4.488, avg. samples / sec: 56350.87
Iteration:   1700, Loss function: 4.027, Average Loss: 4.488, avg. samples / sec: 56139.01
Iteration:   1700, Loss function: 4.346, Average Loss: 4.493, avg. samples / sec: 56417.51
Iteration:   1700, Loss function: 4.290, Average Loss: 4.470, avg. samples / sec: 56436.14
Iteration:   1700, Loss function: 4.690, Average Loss: 4.467, avg. samples / sec: 56356.77
Iteration:   1700, Loss function: 3.397, Average Loss: 4.500, avg. samples / sec: 56386.99
Iteration:   1700, Loss function: 3.811, Average Loss: 4.492, avg. samples / sec: 56133.26
Iteration:   1700, Loss function: 4.406, Average Loss: 4.486, avg. samples / sec: 56330.46
Iteration:   1720, Loss function: 4.501, Average Loss: 4.465, avg. samples / sec: 58042.35
Iteration:   1720, Loss function: 4.441, Average Loss: 4.484, avg. samples / sec: 58047.58
Iteration:   1720, Loss function: 3.822, Average Loss: 4.491, avg. samples / sec: 57984.17
Iteration:   1720, Loss function: 4.657, Average Loss: 4.444, avg. samples / sec: 57874.16
Iteration:   1720, Loss function: 3.817, Average Loss: 4.478, avg. samples / sec: 57885.40
Iteration:   1720, Loss function: 3.841, Average Loss: 4.494, avg. samples / sec: 57893.25
Iteration:   1720, Loss function: 5.111, Average Loss: 4.498, avg. samples / sec: 57902.77
Iteration:   1720, Loss function: 4.219, Average Loss: 4.497, avg. samples / sec: 57777.19
Iteration:   1720, Loss function: 3.314, Average Loss: 4.482, avg. samples / sec: 57803.11
Iteration:   1720, Loss function: 4.809, Average Loss: 4.483, avg. samples / sec: 57769.25
Iteration:   1720, Loss function: 3.294, Average Loss: 4.482, avg. samples / sec: 57699.34
Iteration:   1720, Loss function: 3.382, Average Loss: 4.459, avg. samples / sec: 57692.23
Iteration:   1720, Loss function: 3.999, Average Loss: 4.500, avg. samples / sec: 57755.52
Iteration:   1720, Loss function: 4.166, Average Loss: 4.476, avg. samples / sec: 57787.71
Iteration:   1720, Loss function: 4.088, Average Loss: 4.494, avg. samples / sec: 57636.10
Iteration:   1740, Loss function: 4.555, Average Loss: 4.476, avg. samples / sec: 56898.00
Iteration:   1740, Loss function: 3.930, Average Loss: 4.480, avg. samples / sec: 56873.87
Iteration:   1740, Loss function: 4.819, Average Loss: 4.494, avg. samples / sec: 56978.03
Iteration:   1740, Loss function: 3.471, Average Loss: 4.495, avg. samples / sec: 56772.60
Iteration:   1740, Loss function: 3.852, Average Loss: 4.489, avg. samples / sec: 56669.73
Iteration:   1740, Loss function: 4.890, Average Loss: 4.457, avg. samples / sec: 56784.75
Iteration:   1740, Loss function: 6.075, Average Loss: 4.484, avg. samples / sec: 56762.27
Iteration:   1740, Loss function: 5.891, Average Loss: 4.479, avg. samples / sec: 56666.84
Iteration:   1740, Loss function: 4.022, Average Loss: 4.463, avg. samples / sec: 56561.13
Iteration:   1740, Loss function: 6.278, Average Loss: 4.481, avg. samples / sec: 56762.56
Iteration:   1740, Loss function: 4.853, Average Loss: 4.490, avg. samples / sec: 56653.58
Iteration:   1740, Loss function: 3.016, Average Loss: 4.500, avg. samples / sec: 56726.26
Iteration:   1740, Loss function: 4.118, Average Loss: 4.495, avg. samples / sec: 56670.60
Iteration:   1740, Loss function: 5.462, Average Loss: 4.488, avg. samples / sec: 56546.99
Iteration:   1740, Loss function: 4.372, Average Loss: 4.441, avg. samples / sec: 56602.77
:::MLL 1558639685.876 epoch_stop: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 819}}
:::MLL 1558639685.876 epoch_start: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 673}}
Iteration:   1760, Loss function: 3.806, Average Loss: 4.476, avg. samples / sec: 60343.73
Iteration:   1760, Loss function: 5.089, Average Loss: 4.456, avg. samples / sec: 60252.89
Iteration:   1760, Loss function: 4.269, Average Loss: 4.489, avg. samples / sec: 60307.27
Iteration:   1760, Loss function: 5.176, Average Loss: 4.497, avg. samples / sec: 60264.12
Iteration:   1760, Loss function: 4.518, Average Loss: 4.489, avg. samples / sec: 60261.65
Iteration:   1760, Loss function: 4.331, Average Loss: 4.491, avg. samples / sec: 60157.60
Iteration:   1760, Loss function: 4.266, Average Loss: 4.485, avg. samples / sec: 60196.50
Iteration:   1760, Loss function: 4.715, Average Loss: 4.471, avg. samples / sec: 59967.99
Iteration:   1760, Loss function: 4.770, Average Loss: 4.475, avg. samples / sec: 59954.90
Iteration:   1760, Loss function: 4.840, Average Loss: 4.478, avg. samples / sec: 60087.55
Iteration:   1760, Loss function: 3.587, Average Loss: 4.476, avg. samples / sec: 60101.05
Iteration:   1760, Loss function: 4.599, Average Loss: 4.492, avg. samples / sec: 59905.59
Iteration:   1760, Loss function: 5.063, Average Loss: 4.468, avg. samples / sec: 60028.63
Iteration:   1760, Loss function: 3.513, Average Loss: 4.495, avg. samples / sec: 59928.46
Iteration:   1760, Loss function: 3.806, Average Loss: 4.444, avg. samples / sec: 60032.24
Iteration:   1780, Loss function: 4.474, Average Loss: 4.498, avg. samples / sec: 55702.06
Iteration:   1780, Loss function: 4.604, Average Loss: 4.480, avg. samples / sec: 55762.43
Iteration:   1780, Loss function: 4.315, Average Loss: 4.467, avg. samples / sec: 55705.96
Iteration:   1780, Loss function: 4.568, Average Loss: 4.444, avg. samples / sec: 55834.65
Iteration:   1780, Loss function: 4.066, Average Loss: 4.471, avg. samples / sec: 55723.49
Iteration:   1780, Loss function: 4.472, Average Loss: 4.487, avg. samples / sec: 55607.01
Iteration:   1780, Loss function: 3.841, Average Loss: 4.470, avg. samples / sec: 55540.38
Iteration:   1780, Loss function: 5.183, Average Loss: 4.492, avg. samples / sec: 55811.41
Iteration:   1780, Loss function: 4.634, Average Loss: 4.455, avg. samples / sec: 55555.89
Iteration:   1780, Loss function: 3.529, Average Loss: 4.467, avg. samples / sec: 55782.87
Iteration:   1780, Loss function: 4.471, Average Loss: 4.489, avg. samples / sec: 55752.79
Iteration:   1780, Loss function: 4.966, Average Loss: 4.487, avg. samples / sec: 55580.53
Iteration:   1780, Loss function: 2.819, Average Loss: 4.486, avg. samples / sec: 55621.07
Iteration:   1780, Loss function: 3.432, Average Loss: 4.491, avg. samples / sec: 55541.13
Iteration:   1780, Loss function: 3.720, Average Loss: 4.468, avg. samples / sec: 55529.31
Iteration:   1800, Loss function: 3.881, Average Loss: 4.466, avg. samples / sec: 59845.83
Iteration:   1800, Loss function: 4.537, Average Loss: 4.450, avg. samples / sec: 59808.14
Iteration:   1800, Loss function: 4.847, Average Loss: 4.501, avg. samples / sec: 59682.16
Iteration:   1800, Loss function: 4.579, Average Loss: 4.487, avg. samples / sec: 59755.65
Iteration:   1800, Loss function: 5.350, Average Loss: 4.481, avg. samples / sec: 59800.38
Iteration:   1800, Loss function: 3.535, Average Loss: 4.470, avg. samples / sec: 59936.85
Iteration:   1800, Loss function: 4.778, Average Loss: 4.438, avg. samples / sec: 59684.23
Iteration:   1800, Loss function: 4.393, Average Loss: 4.464, avg. samples / sec: 59651.31
Iteration:   1800, Loss function: 4.508, Average Loss: 4.462, avg. samples / sec: 59643.94
Iteration:   1800, Loss function: 4.324, Average Loss: 4.478, avg. samples / sec: 59601.54
Iteration:   1800, Loss function: 3.625, Average Loss: 4.489, avg. samples / sec: 59698.90
Iteration:   1800, Loss function: 4.276, Average Loss: 4.485, avg. samples / sec: 59549.96
Iteration:   1800, Loss function: 3.823, Average Loss: 4.481, avg. samples / sec: 59632.99
Iteration:   1800, Loss function: 4.001, Average Loss: 4.490, avg. samples / sec: 59500.18
Iteration:   1800, Loss function: 5.001, Average Loss: 4.470, avg. samples / sec: 59427.82
:::MLL 1558639687.885 epoch_stop: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 819}}
:::MLL 1558639687.886 epoch_start: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 673}}
Iteration:   1820, Loss function: 4.290, Average Loss: 4.488, avg. samples / sec: 60187.22
Iteration:   1820, Loss function: 4.993, Average Loss: 4.480, avg. samples / sec: 59829.62
Iteration:   1820, Loss function: 3.977, Average Loss: 4.463, avg. samples / sec: 59729.69
Iteration:   1820, Loss function: 4.253, Average Loss: 4.467, avg. samples / sec: 59868.48
Iteration:   1820, Loss function: 5.114, Average Loss: 4.480, avg. samples / sec: 59904.19
Iteration:   1820, Loss function: 4.549, Average Loss: 4.468, avg. samples / sec: 60134.16
Iteration:   1820, Loss function: 4.362, Average Loss: 4.458, avg. samples / sec: 59871.82
Iteration:   1820, Loss function: 5.567, Average Loss: 4.464, avg. samples / sec: 59860.45
Iteration:   1820, Loss function: 4.909, Average Loss: 4.480, avg. samples / sec: 59743.36
Iteration:   1820, Loss function: 3.815, Average Loss: 4.480, avg. samples / sec: 59836.96
Iteration:   1820, Loss function: 3.642, Average Loss: 4.476, avg. samples / sec: 59927.50
Iteration:   1820, Loss function: 4.659, Average Loss: 4.486, avg. samples / sec: 59874.59
Iteration:   1820, Loss function: 4.792, Average Loss: 4.443, avg. samples / sec: 59734.47
Iteration:   1820, Loss function: 5.159, Average Loss: 4.501, avg. samples / sec: 59631.68
Iteration:   1820, Loss function: 2.360, Average Loss: 4.444, avg. samples / sec: 59599.70
Iteration:   1840, Loss function: 4.180, Average Loss: 4.457, avg. samples / sec: 59506.84
Iteration:   1840, Loss function: 3.761, Average Loss: 4.438, avg. samples / sec: 59638.29
Iteration:   1840, Loss function: 5.465, Average Loss: 4.482, avg. samples / sec: 59619.11
Iteration:   1840, Loss function: 4.101, Average Loss: 4.445, avg. samples / sec: 59642.07
Iteration:   1840, Loss function: 2.568, Average Loss: 4.479, avg. samples / sec: 59518.10
Iteration:   1840, Loss function: 4.311, Average Loss: 4.500, avg. samples / sec: 59584.23
Iteration:   1840, Loss function: 4.421, Average Loss: 4.480, avg. samples / sec: 59503.22
Iteration:   1840, Loss function: 3.868, Average Loss: 4.464, avg. samples / sec: 59406.20
Iteration:   1840, Loss function: 3.772, Average Loss: 4.453, avg. samples / sec: 59361.41
Iteration:   1840, Loss function: 4.207, Average Loss: 4.478, avg. samples / sec: 59337.81
Iteration:   1840, Loss function: 4.799, Average Loss: 4.478, avg. samples / sec: 59258.02
Iteration:   1840, Loss function: 5.520, Average Loss: 4.465, avg. samples / sec: 59275.89
Iteration:   1840, Loss function: 4.140, Average Loss: 4.482, avg. samples / sec: 59154.67
Iteration:   1840, Loss function: 4.365, Average Loss: 4.458, avg. samples / sec: 59194.25
Iteration:   1840, Loss function: 4.637, Average Loss: 4.472, avg. samples / sec: 59321.40
Iteration:   1860, Loss function: 5.972, Average Loss: 4.479, avg. samples / sec: 59764.97
Iteration:   1860, Loss function: 3.903, Average Loss: 4.463, avg. samples / sec: 59712.12
Iteration:   1860, Loss function: 4.246, Average Loss: 4.478, avg. samples / sec: 59716.65
Iteration:   1860, Loss function: 6.168, Average Loss: 4.453, avg. samples / sec: 59463.32
Iteration:   1860, Loss function: 3.856, Average Loss: 4.478, avg. samples / sec: 59838.41
Iteration:   1860, Loss function: 3.255, Average Loss: 4.446, avg. samples / sec: 59505.98
Iteration:   1860, Loss function: 5.199, Average Loss: 4.477, avg. samples / sec: 59543.65
Iteration:   1860, Loss function: 5.159, Average Loss: 4.438, avg. samples / sec: 59440.55
Iteration:   1860, Loss function: 2.839, Average Loss: 4.464, avg. samples / sec: 59691.18
Iteration:   1860, Loss function: 4.973, Average Loss: 4.450, avg. samples / sec: 59577.90
Iteration:   1860, Loss function: 3.563, Average Loss: 4.468, avg. samples / sec: 59703.90
Iteration:   1860, Loss function: 3.894, Average Loss: 4.475, avg. samples / sec: 59432.81
Iteration:   1860, Loss function: 3.075, Average Loss: 4.455, avg. samples / sec: 59530.59
Iteration:   1860, Loss function: 4.561, Average Loss: 4.497, avg. samples / sec: 59316.04
Iteration:   1860, Loss function: 5.322, Average Loss: 4.476, avg. samples / sec: 59203.08
Iteration:   1880, Loss function: 4.882, Average Loss: 4.458, avg. samples / sec: 60511.73
Iteration:   1880, Loss function: 4.485, Average Loss: 4.456, avg. samples / sec: 60554.01
Iteration:   1880, Loss function: 5.009, Average Loss: 4.477, avg. samples / sec: 60530.60
Iteration:   1880, Loss function: 5.323, Average Loss: 4.472, avg. samples / sec: 60112.33
Iteration:   1880, Loss function: 4.761, Average Loss: 4.473, avg. samples / sec: 60215.25
Iteration:   1880, Loss function: 3.352, Average Loss: 4.431, avg. samples / sec: 60202.44
Iteration:   1880, Loss function: 4.020, Average Loss: 4.442, avg. samples / sec: 60146.17
Iteration:   1880, Loss function: 5.226, Average Loss: 4.494, avg. samples / sec: 60372.37
Iteration:   1880, Loss function: 4.323, Average Loss: 4.451, avg. samples / sec: 60132.29
Iteration:   1880, Loss function: 4.437, Average Loss: 4.479, avg. samples / sec: 60029.50
Iteration:   1880, Loss function: 4.793, Average Loss: 4.476, avg. samples / sec: 60181.41
Iteration:   1880, Loss function: 3.853, Average Loss: 4.458, avg. samples / sec: 59925.43
Iteration:   1880, Loss function: 4.167, Average Loss: 4.459, avg. samples / sec: 60104.28
Iteration:   1880, Loss function: 5.085, Average Loss: 4.477, avg. samples / sec: 60006.27
Iteration:   1880, Loss function: 4.952, Average Loss: 4.450, avg. samples / sec: 59975.72
:::MLL 1558639689.859 epoch_stop: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 819}}
:::MLL 1558639689.860 epoch_start: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 673}}
Iteration:   1900, Loss function: 4.949, Average Loss: 4.455, avg. samples / sec: 59335.52
Iteration:   1900, Loss function: 3.483, Average Loss: 4.451, avg. samples / sec: 59052.13
Iteration:   1900, Loss function: 3.180, Average Loss: 4.470, avg. samples / sec: 59150.82
Iteration:   1900, Loss function: 3.830, Average Loss: 4.429, avg. samples / sec: 59172.18
Iteration:   1900, Loss function: 5.097, Average Loss: 4.470, avg. samples / sec: 59170.77
Iteration:   1900, Loss function: 4.688, Average Loss: 4.470, avg. samples / sec: 59249.88
Iteration:   1900, Loss function: 3.607, Average Loss: 4.456, avg. samples / sec: 59223.38
Iteration:   1900, Loss function: 5.019, Average Loss: 4.444, avg. samples / sec: 59117.40
Iteration:   1900, Loss function: 3.837, Average Loss: 4.439, avg. samples / sec: 59089.12
Iteration:   1900, Loss function: 4.705, Average Loss: 4.460, avg. samples / sec: 58787.05
Iteration:   1900, Loss function: 3.988, Average Loss: 4.489, avg. samples / sec: 59085.28
Iteration:   1900, Loss function: 5.436, Average Loss: 4.445, avg. samples / sec: 59283.07
Iteration:   1900, Loss function: 3.695, Average Loss: 4.478, avg. samples / sec: 58916.25
Iteration:   1900, Loss function: 3.267, Average Loss: 4.469, avg. samples / sec: 58911.08
Iteration:   1900, Loss function: 3.973, Average Loss: 4.471, avg. samples / sec: 59001.45
Iteration:   1920, Loss function: 5.880, Average Loss: 4.460, avg. samples / sec: 60055.80
Iteration:   1920, Loss function: 4.039, Average Loss: 4.476, avg. samples / sec: 60124.16
Iteration:   1920, Loss function: 3.929, Average Loss: 4.441, avg. samples / sec: 60031.47
Iteration:   1920, Loss function: 3.353, Average Loss: 4.469, avg. samples / sec: 59918.25
Iteration:   1920, Loss function: 5.091, Average Loss: 4.452, avg. samples / sec: 59905.51
Iteration:   1920, Loss function: 6.247, Average Loss: 4.441, avg. samples / sec: 60002.46
Iteration:   1920, Loss function: 3.876, Average Loss: 4.462, avg. samples / sec: 60090.80
Iteration:   1920, Loss function: 5.360, Average Loss: 4.428, avg. samples / sec: 59872.65
Iteration:   1920, Loss function: 4.477, Average Loss: 4.467, avg. samples / sec: 60112.15
Iteration:   1920, Loss function: 3.869, Average Loss: 4.467, avg. samples / sec: 59907.32
Iteration:   1920, Loss function: 3.844, Average Loss: 4.467, avg. samples / sec: 59882.65
Iteration:   1920, Loss function: 3.285, Average Loss: 4.450, avg. samples / sec: 59740.58
Iteration:   1920, Loss function: 4.793, Average Loss: 4.487, avg. samples / sec: 59872.76
Iteration:   1920, Loss function: 4.423, Average Loss: 4.443, avg. samples / sec: 59871.56
Iteration:   1920, Loss function: 4.662, Average Loss: 4.455, avg. samples / sec: 59053.34
Iteration:   1940, Loss function: 4.602, Average Loss: 4.459, avg. samples / sec: 58352.78
Iteration:   1940, Loss function: 4.411, Average Loss: 4.450, avg. samples / sec: 59184.14
Iteration:   1940, Loss function: 5.465, Average Loss: 4.470, avg. samples / sec: 58233.40
Iteration:   1940, Loss function: 2.673, Average Loss: 4.468, avg. samples / sec: 58200.96
Iteration:   1940, Loss function: 5.209, Average Loss: 4.458, avg. samples / sec: 58237.13
Iteration:   1940, Loss function: 3.541, Average Loss: 4.479, avg. samples / sec: 58324.57
Iteration:   1940, Loss function: 4.424, Average Loss: 4.459, avg. samples / sec: 58223.27
Iteration:   1940, Loss function: 4.467, Average Loss: 4.465, avg. samples / sec: 58253.93
Iteration:   1940, Loss function: 3.740, Average Loss: 4.441, avg. samples / sec: 58134.72
Iteration:   1940, Loss function: 4.238, Average Loss: 4.435, avg. samples / sec: 58082.68
Iteration:   1940, Loss function: 4.357, Average Loss: 4.457, avg. samples / sec: 58054.64
Iteration:   1940, Loss function: 3.289, Average Loss: 4.448, avg. samples / sec: 58057.27
Iteration:   1940, Loss function: 3.867, Average Loss: 4.442, avg. samples / sec: 58219.98
Iteration:   1940, Loss function: 4.958, Average Loss: 4.425, avg. samples / sec: 58034.46
Iteration:   1940, Loss function: 4.888, Average Loss: 4.446, avg. samples / sec: 58105.57
:::MLL 1558639691.850 epoch_stop: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 819}}
:::MLL 1558639691.850 epoch_start: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 673}}
Iteration:   1960, Loss function: 3.883, Average Loss: 4.456, avg. samples / sec: 58945.40
Iteration:   1960, Loss function: 4.140, Average Loss: 4.437, avg. samples / sec: 59088.77
Iteration:   1960, Loss function: 3.999, Average Loss: 4.452, avg. samples / sec: 58860.79
Iteration:   1960, Loss function: 3.459, Average Loss: 4.442, avg. samples / sec: 59121.10
Iteration:   1960, Loss function: 4.153, Average Loss: 4.449, avg. samples / sec: 59030.76
Iteration:   1960, Loss function: 3.646, Average Loss: 4.420, avg. samples / sec: 59081.49
Iteration:   1960, Loss function: 4.489, Average Loss: 4.433, avg. samples / sec: 58965.58
Iteration:   1960, Loss function: 4.097, Average Loss: 4.478, avg. samples / sec: 58888.75
Iteration:   1960, Loss function: 4.991, Average Loss: 4.453, avg. samples / sec: 58863.66
Iteration:   1960, Loss function: 5.270, Average Loss: 4.457, avg. samples / sec: 58834.52
Iteration:   1960, Loss function: 4.069, Average Loss: 4.460, avg. samples / sec: 58842.40
Iteration:   1960, Loss function: 4.141, Average Loss: 4.450, avg. samples / sec: 58917.80
Iteration:   1960, Loss function: 4.472, Average Loss: 4.464, avg. samples / sec: 58776.66
Iteration:   1960, Loss function: 4.592, Average Loss: 4.437, avg. samples / sec: 58914.72
Iteration:   1960, Loss function: 3.626, Average Loss: 4.465, avg. samples / sec: 58629.60
Iteration:   1980, Loss function: 3.739, Average Loss: 4.447, avg. samples / sec: 59068.02
Iteration:   1980, Loss function: 3.995, Average Loss: 4.464, avg. samples / sec: 59068.07
Iteration:   1980, Loss function: 3.933, Average Loss: 4.465, avg. samples / sec: 59138.14
Iteration:   1980, Loss function: 4.161, Average Loss: 4.480, avg. samples / sec: 58891.29
Iteration:   1980, Loss function: 4.351, Average Loss: 4.447, avg. samples / sec: 58864.28
Iteration:   1980, Loss function: 3.636, Average Loss: 4.455, avg. samples / sec: 58887.64
Iteration:   1980, Loss function: 3.830, Average Loss: 4.438, avg. samples / sec: 58690.47
Iteration:   1980, Loss function: 3.030, Average Loss: 4.456, avg. samples / sec: 58862.78
Iteration:   1980, Loss function: 5.070, Average Loss: 4.449, avg. samples / sec: 58768.13
Iteration:   1980, Loss function: 4.741, Average Loss: 4.433, avg. samples / sec: 58714.16
Iteration:   1980, Loss function: 4.845, Average Loss: 4.453, avg. samples / sec: 58537.47
Iteration:   1980, Loss function: 4.481, Average Loss: 4.435, avg. samples / sec: 58794.75
Iteration:   1980, Loss function: 4.414, Average Loss: 4.440, avg. samples / sec: 58632.70
Iteration:   1980, Loss function: 4.979, Average Loss: 4.450, avg. samples / sec: 58584.73
Iteration:   1980, Loss function: 4.015, Average Loss: 4.415, avg. samples / sec: 58622.38
Iteration:   2000, Loss function: 3.385, Average Loss: 4.459, avg. samples / sec: 58892.07
Iteration:   2000, Loss function: 4.479, Average Loss: 4.452, avg. samples / sec: 59017.21
Iteration:   2000, Loss function: 3.114, Average Loss: 4.445, avg. samples / sec: 59146.16
Iteration:   2000, Loss function: 3.147, Average Loss: 4.445, avg. samples / sec: 59042.06
Iteration:   2000, Loss function: 4.260, Average Loss: 4.437, avg. samples / sec: 58912.11
Iteration:   2000, Loss function: 4.396, Average Loss: 4.472, avg. samples / sec: 58836.09
Iteration:   2000, Loss function: 2.413, Average Loss: 4.459, avg. samples / sec: 58764.28
Iteration:   2000, Loss function: 4.254, Average Loss: 4.430, avg. samples / sec: 59029.45
Iteration:   2000, Loss function: 4.336, Average Loss: 4.450, avg. samples / sec: 58838.20
Iteration:   2000, Loss function: 4.952, Average Loss: 4.444, avg. samples / sec: 58641.19
Iteration:   2000, Loss function: 4.874, Average Loss: 4.434, avg. samples / sec: 58989.37
Iteration:   2000, Loss function: 3.794, Average Loss: 4.433, avg. samples / sec: 58923.74
Iteration:   2000, Loss function: 4.142, Average Loss: 4.448, avg. samples / sec: 58909.03
Iteration:   2000, Loss function: 4.245, Average Loss: 4.409, avg. samples / sec: 59003.99
Iteration:   2000, Loss function: 4.980, Average Loss: 4.451, avg. samples / sec: 58567.03
Iteration:   2020, Loss function: 4.311, Average Loss: 4.427, avg. samples / sec: 60487.01
Iteration:   2020, Loss function: 5.174, Average Loss: 4.427, avg. samples / sec: 60404.33
Iteration:   2020, Loss function: 4.769, Average Loss: 4.472, avg. samples / sec: 60349.34
Iteration:   2020, Loss function: 4.125, Average Loss: 4.445, avg. samples / sec: 60430.88
Iteration:   2020, Loss function: 4.259, Average Loss: 4.439, avg. samples / sec: 60233.39
Iteration:   2020, Loss function: 3.451, Average Loss: 4.431, avg. samples / sec: 60259.33
Iteration:   2020, Loss function: 2.939, Average Loss: 4.457, avg. samples / sec: 60090.52
Iteration:   2020, Loss function: 4.335, Average Loss: 4.460, avg. samples / sec: 60256.08
Iteration:   2020, Loss function: 4.531, Average Loss: 4.446, avg. samples / sec: 60488.90
Iteration:   2020, Loss function: 6.027, Average Loss: 4.448, avg. samples / sec: 60225.08
Iteration:   2020, Loss function: 5.396, Average Loss: 4.432, avg. samples / sec: 60221.50
Iteration:   2020, Loss function: 4.807, Average Loss: 4.440, avg. samples / sec: 60221.25
Iteration:   2020, Loss function: 5.162, Average Loss: 4.409, avg. samples / sec: 60240.35
Iteration:   2020, Loss function: 3.338, Average Loss: 4.439, avg. samples / sec: 60048.17
Iteration:   2020, Loss function: 5.137, Average Loss: 4.453, avg. samples / sec: 59957.17
:::MLL 1558639693.832 epoch_stop: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 819}}
:::MLL 1558639693.832 epoch_start: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 673}}
Iteration:   2040, Loss function: 4.134, Average Loss: 4.408, avg. samples / sec: 58189.76
Iteration:   2040, Loss function: 4.710, Average Loss: 4.470, avg. samples / sec: 57951.77
Iteration:   2040, Loss function: 3.334, Average Loss: 4.437, avg. samples / sec: 58121.10
Iteration:   2040, Loss function: 4.800, Average Loss: 4.418, avg. samples / sec: 57844.49
Iteration:   2040, Loss function: 4.190, Average Loss: 4.438, avg. samples / sec: 58023.49
Iteration:   2040, Loss function: 3.931, Average Loss: 4.446, avg. samples / sec: 58038.26
Iteration:   2040, Loss function: 4.835, Average Loss: 4.425, avg. samples / sec: 57908.33
Iteration:   2040, Loss function: 5.389, Average Loss: 4.449, avg. samples / sec: 58108.37
Iteration:   2040, Loss function: 4.197, Average Loss: 4.424, avg. samples / sec: 57802.64
Iteration:   2040, Loss function: 3.784, Average Loss: 4.424, avg. samples / sec: 58011.67
Iteration:   2040, Loss function: 3.512, Average Loss: 4.440, avg. samples / sec: 57825.24
Iteration:   2040, Loss function: 3.473, Average Loss: 4.437, avg. samples / sec: 57989.66
Iteration:   2040, Loss function: 4.097, Average Loss: 4.452, avg. samples / sec: 57837.20
Iteration:   2040, Loss function: 4.696, Average Loss: 4.455, avg. samples / sec: 57856.77
Iteration:   2040, Loss function: 5.299, Average Loss: 4.432, avg. samples / sec: 57781.26
Iteration:   2060, Loss function: 4.720, Average Loss: 4.439, avg. samples / sec: 58275.28
Iteration:   2060, Loss function: 5.770, Average Loss: 4.456, avg. samples / sec: 58296.78
Iteration:   2060, Loss function: 4.376, Average Loss: 4.431, avg. samples / sec: 58100.37
Iteration:   2060, Loss function: 5.645, Average Loss: 4.426, avg. samples / sec: 58280.12
Iteration:   2060, Loss function: 4.649, Average Loss: 4.429, avg. samples / sec: 58258.51
Iteration:   2060, Loss function: 4.244, Average Loss: 4.420, avg. samples / sec: 58149.16
Iteration:   2060, Loss function: 4.757, Average Loss: 4.431, avg. samples / sec: 58157.87
Iteration:   2060, Loss function: 4.280, Average Loss: 4.433, avg. samples / sec: 58054.95
Iteration:   2060, Loss function: 5.071, Average Loss: 4.420, avg. samples / sec: 58134.55
Iteration:   2060, Loss function: 3.917, Average Loss: 4.415, avg. samples / sec: 58057.74
Iteration:   2060, Loss function: 3.553, Average Loss: 4.465, avg. samples / sec: 57987.89
Iteration:   2060, Loss function: 4.449, Average Loss: 4.445, avg. samples / sec: 58094.12
Iteration:   2060, Loss function: 4.198, Average Loss: 4.406, avg. samples / sec: 57924.35
Iteration:   2060, Loss function: 4.313, Average Loss: 4.448, avg. samples / sec: 58133.02
Iteration:   2060, Loss function: 3.315, Average Loss: 4.415, avg. samples / sec: 57960.82
Iteration:   2080, Loss function: 4.484, Average Loss: 4.445, avg. samples / sec: 58701.99
Iteration:   2080, Loss function: 4.412, Average Loss: 4.457, avg. samples / sec: 58685.90
Iteration:   2080, Loss function: 4.779, Average Loss: 4.434, avg. samples / sec: 58446.73
Iteration:   2080, Loss function: 3.685, Average Loss: 4.403, avg. samples / sec: 58684.92
Iteration:   2080, Loss function: 6.081, Average Loss: 4.419, avg. samples / sec: 58598.83
Iteration:   2080, Loss function: 4.376, Average Loss: 4.427, avg. samples / sec: 58585.29
Iteration:   2080, Loss function: 3.870, Average Loss: 4.418, avg. samples / sec: 58497.04
Iteration:   2080, Loss function: 4.038, Average Loss: 4.410, avg. samples / sec: 58522.13
Iteration:   2080, Loss function: 4.640, Average Loss: 4.406, avg. samples / sec: 58681.48
Iteration:   2080, Loss function: 3.850, Average Loss: 4.428, avg. samples / sec: 58464.26
Iteration:   2080, Loss function: 3.619, Average Loss: 4.454, avg. samples / sec: 58356.81
Iteration:   2080, Loss function: 6.026, Average Loss: 4.431, avg. samples / sec: 58373.54
Iteration:   2080, Loss function: 5.536, Average Loss: 4.449, avg. samples / sec: 58489.90
Iteration:   2080, Loss function: 3.665, Average Loss: 4.421, avg. samples / sec: 58315.35
Iteration:   2080, Loss function: 3.373, Average Loss: 4.424, avg. samples / sec: 58300.54
:::MLL 1558639695.842 epoch_stop: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 819}}
:::MLL 1558639695.843 epoch_start: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 673}}
Iteration:   2100, Loss function: 3.558, Average Loss: 4.428, avg. samples / sec: 59988.54
Iteration:   2100, Loss function: 4.376, Average Loss: 4.418, avg. samples / sec: 60101.87
Iteration:   2100, Loss function: 4.614, Average Loss: 4.420, avg. samples / sec: 60056.64
Iteration:   2100, Loss function: 3.731, Average Loss: 4.420, avg. samples / sec: 60120.95
Iteration:   2100, Loss function: 3.529, Average Loss: 4.453, avg. samples / sec: 59924.39
Iteration:   2100, Loss function: 3.914, Average Loss: 4.446, avg. samples / sec: 60147.28
Iteration:   2100, Loss function: 4.327, Average Loss: 4.418, avg. samples / sec: 60188.84
Iteration:   2100, Loss function: 4.303, Average Loss: 4.437, avg. samples / sec: 59852.72
Iteration:   2100, Loss function: 3.552, Average Loss: 4.415, avg. samples / sec: 59998.98
Iteration:   2100, Loss function: 3.728, Average Loss: 4.420, avg. samples / sec: 60141.22
Iteration:   2100, Loss function: 5.122, Average Loss: 4.396, avg. samples / sec: 59855.97
Iteration:   2100, Loss function: 4.134, Average Loss: 4.427, avg. samples / sec: 60042.52
Iteration:   2100, Loss function: 2.850, Average Loss: 4.413, avg. samples / sec: 59856.15
Iteration:   2100, Loss function: 4.387, Average Loss: 4.442, avg. samples / sec: 59959.11
Iteration:   2100, Loss function: 3.795, Average Loss: 4.406, avg. samples / sec: 59795.07
Iteration:   2120, Loss function: 4.960, Average Loss: 4.417, avg. samples / sec: 59247.83
Iteration:   2120, Loss function: 5.916, Average Loss: 4.411, avg. samples / sec: 59276.94
Iteration:   2120, Loss function: 3.953, Average Loss: 4.404, avg. samples / sec: 59403.14
Iteration:   2120, Loss function: 3.757, Average Loss: 4.443, avg. samples / sec: 59105.38
Iteration:   2120, Loss function: 3.855, Average Loss: 4.416, avg. samples / sec: 59060.47
Iteration:   2120, Loss function: 6.119, Average Loss: 4.418, avg. samples / sec: 59176.81
Iteration:   2120, Loss function: 3.821, Average Loss: 4.414, avg. samples / sec: 59075.79
Iteration:   2120, Loss function: 4.022, Average Loss: 4.387, avg. samples / sec: 59167.12
Iteration:   2120, Loss function: 4.311, Average Loss: 4.433, avg. samples / sec: 59251.77
Iteration:   2120, Loss function: 4.181, Average Loss: 4.417, avg. samples / sec: 58983.27
Iteration:   2120, Loss function: 4.646, Average Loss: 4.416, avg. samples / sec: 59023.91
Iteration:   2120, Loss function: 5.226, Average Loss: 4.420, avg. samples / sec: 59083.79
Iteration:   2120, Loss function: 5.222, Average Loss: 4.452, avg. samples / sec: 58943.38
Iteration:   2120, Loss function: 4.953, Average Loss: 4.427, avg. samples / sec: 58860.20
Iteration:   2120, Loss function: 5.283, Average Loss: 4.434, avg. samples / sec: 58278.84
Iteration:   2140, Loss function: 4.009, Average Loss: 4.403, avg. samples / sec: 59417.72
Iteration:   2140, Loss function: 5.496, Average Loss: 4.404, avg. samples / sec: 59333.42
Iteration:   2140, Loss function: 4.464, Average Loss: 4.435, avg. samples / sec: 59342.91
Iteration:   2140, Loss function: 3.871, Average Loss: 4.415, avg. samples / sec: 59342.84
Iteration:   2140, Loss function: 4.689, Average Loss: 4.411, avg. samples / sec: 59246.29
Iteration:   2140, Loss function: 4.507, Average Loss: 4.423, avg. samples / sec: 59405.22
Iteration:   2140, Loss function: 3.328, Average Loss: 4.449, avg. samples / sec: 59353.41
Iteration:   2140, Loss function: 3.992, Average Loss: 4.413, avg. samples / sec: 59129.06
Iteration:   2140, Loss function: 5.389, Average Loss: 4.420, avg. samples / sec: 59316.96
Iteration:   2140, Loss function: 2.782, Average Loss: 4.425, avg. samples / sec: 60069.90
Iteration:   2140, Loss function: 4.066, Average Loss: 4.417, avg. samples / sec: 59234.21
Iteration:   2140, Loss function: 3.387, Average Loss: 4.424, avg. samples / sec: 59175.26
Iteration:   2140, Loss function: 4.320, Average Loss: 4.402, avg. samples / sec: 59071.81
Iteration:   2140, Loss function: 3.765, Average Loss: 4.386, avg. samples / sec: 59037.14
Iteration:   2140, Loss function: 3.645, Average Loss: 4.411, avg. samples / sec: 58953.37
Iteration:   2160, Loss function: 4.046, Average Loss: 4.434, avg. samples / sec: 57247.68
Iteration:   2160, Loss function: 4.166, Average Loss: 4.384, avg. samples / sec: 57530.52
Iteration:   2160, Loss function: 4.414, Average Loss: 4.410, avg. samples / sec: 57339.78
Iteration:   2160, Loss function: 3.842, Average Loss: 4.396, avg. samples / sec: 57066.89
Iteration:   2160, Loss function: 2.910, Average Loss: 4.416, avg. samples / sec: 57235.08
Iteration:   2160, Loss function: 4.448, Average Loss: 4.420, avg. samples / sec: 57270.76
Iteration:   2160, Loss function: 4.457, Average Loss: 4.411, avg. samples / sec: 57112.63
Iteration:   2160, Loss function: 3.730, Average Loss: 4.420, avg. samples / sec: 57191.29
Iteration:   2160, Loss function: 4.640, Average Loss: 4.407, avg. samples / sec: 57154.51
Iteration:   2160, Loss function: 4.168, Average Loss: 4.405, avg. samples / sec: 56987.00
Iteration:   2160, Loss function: 4.532, Average Loss: 4.444, avg. samples / sec: 57119.81
Iteration:   2160, Loss function: 4.793, Average Loss: 4.423, avg. samples / sec: 57085.29
Iteration:   2160, Loss function: 3.456, Average Loss: 4.403, avg. samples / sec: 57320.51
Iteration:   2160, Loss function: 3.954, Average Loss: 4.398, avg. samples / sec: 57135.48
Iteration:   2160, Loss function: 3.909, Average Loss: 4.417, avg. samples / sec: 56979.53
:::MLL 1558639697.845 epoch_stop: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 819}}
:::MLL 1558639697.845 epoch_start: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 673}}
Iteration:   2180, Loss function: 4.429, Average Loss: 4.413, avg. samples / sec: 59616.94
Iteration:   2180, Loss function: 4.278, Average Loss: 4.420, avg. samples / sec: 59621.28
Iteration:   2180, Loss function: 4.121, Average Loss: 4.430, avg. samples / sec: 59414.04
Iteration:   2180, Loss function: 4.723, Average Loss: 4.438, avg. samples / sec: 59635.59
Iteration:   2180, Loss function: 4.544, Average Loss: 4.396, avg. samples / sec: 59722.30
Iteration:   2180, Loss function: 4.343, Average Loss: 4.399, avg. samples / sec: 59660.91
Iteration:   2180, Loss function: 3.013, Average Loss: 4.416, avg. samples / sec: 59711.52
Iteration:   2180, Loss function: 4.193, Average Loss: 4.404, avg. samples / sec: 59415.64
Iteration:   2180, Loss function: 4.162, Average Loss: 4.414, avg. samples / sec: 59597.83
Iteration:   2180, Loss function: 3.093, Average Loss: 4.400, avg. samples / sec: 59539.32
Iteration:   2180, Loss function: 4.696, Average Loss: 4.395, avg. samples / sec: 59409.73
Iteration:   2180, Loss function: 3.336, Average Loss: 4.379, avg. samples / sec: 59287.16
Iteration:   2180, Loss function: 4.834, Average Loss: 4.408, avg. samples / sec: 59423.91
Iteration:   2180, Loss function: 5.492, Average Loss: 4.413, avg. samples / sec: 59296.84
Iteration:   2180, Loss function: 3.812, Average Loss: 4.405, avg. samples / sec: 59258.32
Iteration:   2200, Loss function: 3.771, Average Loss: 4.394, avg. samples / sec: 58616.41
Iteration:   2200, Loss function: 4.208, Average Loss: 4.401, avg. samples / sec: 58730.93
Iteration:   2200, Loss function: 4.460, Average Loss: 4.393, avg. samples / sec: 58612.36
Iteration:   2200, Loss function: 4.608, Average Loss: 4.402, avg. samples / sec: 58873.94
Iteration:   2200, Loss function: 4.368, Average Loss: 4.424, avg. samples / sec: 58471.85
Iteration:   2200, Loss function: 3.219, Average Loss: 4.390, avg. samples / sec: 58526.12
Iteration:   2200, Loss function: 3.748, Average Loss: 4.433, avg. samples / sec: 58509.96
Iteration:   2200, Loss function: 5.222, Average Loss: 4.409, avg. samples / sec: 58781.12
Iteration:   2200, Loss function: 3.652, Average Loss: 4.410, avg. samples / sec: 58380.38
Iteration:   2200, Loss function: 3.894, Average Loss: 4.410, avg. samples / sec: 58519.97
Iteration:   2200, Loss function: 3.308, Average Loss: 4.395, avg. samples / sec: 58558.76
Iteration:   2200, Loss function: 4.009, Average Loss: 4.421, avg. samples / sec: 58373.90
Iteration:   2200, Loss function: 4.263, Average Loss: 4.375, avg. samples / sec: 58503.60
Iteration:   2200, Loss function: 4.010, Average Loss: 4.412, avg. samples / sec: 58400.88
Iteration:   2200, Loss function: 5.114, Average Loss: 4.397, avg. samples / sec: 58424.60
Iteration:   2220, Loss function: 4.870, Average Loss: 4.430, avg. samples / sec: 60630.42
Iteration:   2220, Loss function: 4.148, Average Loss: 4.394, avg. samples / sec: 60661.60
Iteration:   2220, Loss function: 3.546, Average Loss: 4.385, avg. samples / sec: 60454.49
Iteration:   2220, Loss function: 3.270, Average Loss: 4.403, avg. samples / sec: 60496.46
Iteration:   2220, Loss function: 4.679, Average Loss: 4.407, avg. samples / sec: 60557.94
Iteration:   2220, Loss function: 4.313, Average Loss: 4.401, avg. samples / sec: 60329.78
Iteration:   2220, Loss function: 5.552, Average Loss: 4.404, avg. samples / sec: 60456.62
Iteration:   2220, Loss function: 3.957, Average Loss: 4.394, avg. samples / sec: 60326.34
Iteration:   2220, Loss function: 4.245, Average Loss: 4.396, avg. samples / sec: 60298.08
Iteration:   2220, Loss function: 2.191, Average Loss: 4.370, avg. samples / sec: 60408.50
Iteration:   2220, Loss function: 5.726, Average Loss: 4.390, avg. samples / sec: 60334.04
Iteration:   2220, Loss function: 4.897, Average Loss: 4.415, avg. samples / sec: 60238.31
Iteration:   2220, Loss function: 3.755, Average Loss: 4.411, avg. samples / sec: 60341.97
Iteration:   2220, Loss function: 2.635, Average Loss: 4.393, avg. samples / sec: 60138.81
Iteration:   2220, Loss function: 4.207, Average Loss: 4.407, avg. samples / sec: 59994.69
:::MLL 1558639699.823 epoch_stop: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 819}}
:::MLL 1558639699.823 epoch_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 673}}
Iteration:   2240, Loss function: 3.875, Average Loss: 4.390, avg. samples / sec: 60032.75
Iteration:   2240, Loss function: 3.257, Average Loss: 4.399, avg. samples / sec: 59848.45
Iteration:   2240, Loss function: 4.296, Average Loss: 4.399, avg. samples / sec: 59784.55
Iteration:   2240, Loss function: 4.435, Average Loss: 4.381, avg. samples / sec: 60038.04
Iteration:   2240, Loss function: 2.536, Average Loss: 4.408, avg. samples / sec: 59928.54
Iteration:   2240, Loss function: 3.880, Average Loss: 4.421, avg. samples / sec: 59518.77
Iteration:   2240, Loss function: 4.146, Average Loss: 4.384, avg. samples / sec: 59619.19
Iteration:   2240, Loss function: 4.945, Average Loss: 4.401, avg. samples / sec: 59694.55
Iteration:   2240, Loss function: 4.933, Average Loss: 4.409, avg. samples / sec: 59852.39
Iteration:   2240, Loss function: 3.272, Average Loss: 4.374, avg. samples / sec: 59628.09
Iteration:   2240, Loss function: 4.428, Average Loss: 4.387, avg. samples / sec: 59703.80
Iteration:   2240, Loss function: 4.964, Average Loss: 4.394, avg. samples / sec: 59739.76
Iteration:   2240, Loss function: 5.071, Average Loss: 4.397, avg. samples / sec: 60084.04
Iteration:   2240, Loss function: 3.450, Average Loss: 4.398, avg. samples / sec: 59622.72
Iteration:   2240, Loss function: 4.280, Average Loss: 4.364, avg. samples / sec: 59693.46
Iteration:   2260, Loss function: 5.209, Average Loss: 4.394, avg. samples / sec: 59687.52
Iteration:   2260, Loss function: 4.498, Average Loss: 4.391, avg. samples / sec: 59509.50
Iteration:   2260, Loss function: 4.876, Average Loss: 4.402, avg. samples / sec: 59635.28
Iteration:   2260, Loss function: 4.822, Average Loss: 4.412, avg. samples / sec: 59545.88
Iteration:   2260, Loss function: 4.915, Average Loss: 4.374, avg. samples / sec: 59530.24
Iteration:   2260, Loss function: 4.380, Average Loss: 4.396, avg. samples / sec: 59389.38
Iteration:   2260, Loss function: 4.221, Average Loss: 4.396, avg. samples / sec: 59521.26
Iteration:   2260, Loss function: 5.893, Average Loss: 4.390, avg. samples / sec: 59572.97
Iteration:   2260, Loss function: 5.224, Average Loss: 4.387, avg. samples / sec: 59556.28
Iteration:   2260, Loss function: 3.779, Average Loss: 4.410, avg. samples / sec: 59420.45
Iteration:   2260, Loss function: 2.773, Average Loss: 4.359, avg. samples / sec: 59599.40
Iteration:   2260, Loss function: 4.339, Average Loss: 4.385, avg. samples / sec: 59256.63
Iteration:   2260, Loss function: 3.418, Average Loss: 4.376, avg. samples / sec: 59328.27
Iteration:   2260, Loss function: 4.111, Average Loss: 4.395, avg. samples / sec: 59427.39
Iteration:   2260, Loss function: 4.562, Average Loss: 4.369, avg. samples / sec: 59278.56
Iteration:   2280, Loss function: 4.006, Average Loss: 4.379, avg. samples / sec: 59806.77
Iteration:   2280, Loss function: 4.776, Average Loss: 4.386, avg. samples / sec: 59549.00
Iteration:   2280, Loss function: 3.867, Average Loss: 4.408, avg. samples / sec: 59589.42
Iteration:   2280, Loss function: 4.430, Average Loss: 4.391, avg. samples / sec: 59472.61
Iteration:   2280, Loss function: 4.394, Average Loss: 4.395, avg. samples / sec: 59509.25
Iteration:   2280, Loss function: 3.395, Average Loss: 4.401, avg. samples / sec: 59405.80
Iteration:   2280, Loss function: 3.400, Average Loss: 4.383, avg. samples / sec: 59497.52
Iteration:   2280, Loss function: 4.828, Average Loss: 4.362, avg. samples / sec: 59742.93
Iteration:   2280, Loss function: 4.209, Average Loss: 4.412, avg. samples / sec: 59423.13
Iteration:   2280, Loss function: 3.867, Average Loss: 4.355, avg. samples / sec: 59517.32
Iteration:   2280, Loss function: 4.006, Average Loss: 4.370, avg. samples / sec: 59413.69
Iteration:   2280, Loss function: 4.116, Average Loss: 4.394, avg. samples / sec: 59355.61
Iteration:   2280, Loss function: 4.641, Average Loss: 4.375, avg. samples / sec: 59418.25
Iteration:   2280, Loss function: 4.054, Average Loss: 4.386, avg. samples / sec: 59452.24
Iteration:   2280, Loss function: 3.951, Average Loss: 4.387, avg. samples / sec: 59256.53
:::MLL 1558639701.143 eval_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.03 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.03 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.03 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.03 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.03 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.03 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.03 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.03 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.03 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.03 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.03 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.03 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.03 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.03 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.03 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.46s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.46s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.46s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.47s)
DONE (t=0.48s)
DONE (t=0.48s)
DONE (t=0.48s)
DONE (t=0.48s)
DONE (t=0.48s)
DONE (t=0.48s)
DONE (t=0.48s)
DONE (t=0.48s)
DONE (t=0.48s)
DONE (t=0.48s)
DONE (t=0.49s)
DONE (t=0.53s)
DONE (t=3.18s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.15156
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.28543
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.14841
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.03651
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.15498
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.24641
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.16734
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.24531
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.25905
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.06450
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.27476
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.39468
Current AP: 0.15156 AP goal: 0.23000
:::MLL 1558639705.886 eval_accuracy: {"value": 0.15155641629990457, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 389}}
:::MLL 1558639705.960 eval_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 392}}
:::MLL 1558639705.970 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 804}}
:::MLL 1558639705.970 block_start: {"value": null, "metadata": {"first_epoch_num": 33, "epoch_count": 10.915354834308324, "file": "train.py", "lineno": 813}}
Iteration:   2300, Loss function: 4.599, Average Loss: 4.357, avg. samples / sec: 6074.13
Iteration:   2300, Loss function: 3.801, Average Loss: 4.404, avg. samples / sec: 6073.79
Iteration:   2300, Loss function: 3.405, Average Loss: 4.376, avg. samples / sec: 6070.97
Iteration:   2300, Loss function: 4.062, Average Loss: 4.372, avg. samples / sec: 6074.02
Iteration:   2300, Loss function: 5.302, Average Loss: 4.399, avg. samples / sec: 6072.07
Iteration:   2300, Loss function: 4.539, Average Loss: 4.379, avg. samples / sec: 6071.98
Iteration:   2300, Loss function: 5.855, Average Loss: 4.397, avg. samples / sec: 6071.53
Iteration:   2300, Loss function: 4.326, Average Loss: 4.375, avg. samples / sec: 6069.06
Iteration:   2300, Loss function: 3.981, Average Loss: 4.357, avg. samples / sec: 6071.29
Iteration:   2300, Loss function: 3.630, Average Loss: 4.362, avg. samples / sec: 6071.66
Iteration:   2300, Loss function: 4.916, Average Loss: 4.380, avg. samples / sec: 6072.71
Iteration:   2300, Loss function: 3.638, Average Loss: 4.383, avg. samples / sec: 6072.92
Iteration:   2300, Loss function: 4.181, Average Loss: 4.385, avg. samples / sec: 6069.46
Iteration:   2300, Loss function: 4.419, Average Loss: 4.409, avg. samples / sec: 6069.22
Iteration:   2300, Loss function: 3.884, Average Loss: 4.396, avg. samples / sec: 6071.18
:::MLL 1558639706.772 epoch_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 819}}
:::MLL 1558639706.773 epoch_start: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 673}}
Iteration:   2320, Loss function: 4.276, Average Loss: 4.353, avg. samples / sec: 57112.28
Iteration:   2320, Loss function: 4.541, Average Loss: 4.366, avg. samples / sec: 57298.72
Iteration:   2320, Loss function: 4.358, Average Loss: 4.376, avg. samples / sec: 57248.28
Iteration:   2320, Loss function: 5.087, Average Loss: 4.373, avg. samples / sec: 57170.85
Iteration:   2320, Loss function: 3.918, Average Loss: 4.377, avg. samples / sec: 57331.64
Iteration:   2320, Loss function: 3.536, Average Loss: 4.406, avg. samples / sec: 57052.70
Iteration:   2320, Loss function: 4.028, Average Loss: 4.383, avg. samples / sec: 57323.05
Iteration:   2320, Loss function: 4.658, Average Loss: 4.395, avg. samples / sec: 57187.72
Iteration:   2320, Loss function: 4.707, Average Loss: 4.392, avg. samples / sec: 57364.63
Iteration:   2320, Loss function: 4.243, Average Loss: 4.377, avg. samples / sec: 57262.40
Iteration:   2320, Loss function: 4.432, Average Loss: 4.357, avg. samples / sec: 57230.22
Iteration:   2320, Loss function: 3.685, Average Loss: 4.394, avg. samples / sec: 57129.00
Iteration:   2320, Loss function: 4.877, Average Loss: 4.369, avg. samples / sec: 57067.05
Iteration:   2320, Loss function: 5.458, Average Loss: 4.360, avg. samples / sec: 57178.53
Iteration:   2320, Loss function: 4.533, Average Loss: 4.402, avg. samples / sec: 57102.17
Iteration:   2340, Loss function: 5.104, Average Loss: 4.356, avg. samples / sec: 59723.01
Iteration:   2340, Loss function: 4.149, Average Loss: 4.369, avg. samples / sec: 59689.89
Iteration:   2340, Loss function: 5.329, Average Loss: 4.376, avg. samples / sec: 59597.88
Iteration:   2340, Loss function: 4.153, Average Loss: 4.387, avg. samples / sec: 59620.63
Iteration:   2340, Loss function: 4.050, Average Loss: 4.350, avg. samples / sec: 59447.32
Iteration:   2340, Loss function: 4.172, Average Loss: 4.372, avg. samples / sec: 59490.86
Iteration:   2340, Loss function: 4.827, Average Loss: 4.371, avg. samples / sec: 59613.34
Iteration:   2340, Loss function: 3.330, Average Loss: 4.371, avg. samples / sec: 59504.10
Iteration:   2340, Loss function: 4.036, Average Loss: 4.361, avg. samples / sec: 59443.21
Iteration:   2340, Loss function: 4.244, Average Loss: 4.361, avg. samples / sec: 59603.86
Iteration:   2340, Loss function: 4.054, Average Loss: 4.390, avg. samples / sec: 59527.12
Iteration:   2340, Loss function: 4.013, Average Loss: 4.396, avg. samples / sec: 59747.26
Iteration:   2340, Loss function: 4.982, Average Loss: 4.364, avg. samples / sec: 59446.22
Iteration:   2340, Loss function: 4.253, Average Loss: 4.384, avg. samples / sec: 59411.53
Iteration:   2340, Loss function: 3.297, Average Loss: 4.400, avg. samples / sec: 59331.74
Iteration:   2360, Loss function: 3.582, Average Loss: 4.373, avg. samples / sec: 56871.90
Iteration:   2360, Loss function: 4.010, Average Loss: 4.351, avg. samples / sec: 56735.83
Iteration:   2360, Loss function: 4.439, Average Loss: 4.366, avg. samples / sec: 56847.12
Iteration:   2360, Loss function: 3.746, Average Loss: 4.357, avg. samples / sec: 56856.91
Iteration:   2360, Loss function: 3.113, Average Loss: 4.394, avg. samples / sec: 56878.39
Iteration:   2360, Loss function: 4.117, Average Loss: 4.355, avg. samples / sec: 56835.04
Iteration:   2360, Loss function: 3.400, Average Loss: 4.366, avg. samples / sec: 56815.79
Iteration:   2360, Loss function: 4.312, Average Loss: 4.381, avg. samples / sec: 56932.41
Iteration:   2360, Loss function: 4.991, Average Loss: 4.365, avg. samples / sec: 56771.25
Iteration:   2360, Loss function: 3.888, Average Loss: 4.381, avg. samples / sec: 56730.44
Iteration:   2360, Loss function: 4.262, Average Loss: 4.350, avg. samples / sec: 56740.05
Iteration:   2360, Loss function: 2.858, Average Loss: 4.361, avg. samples / sec: 56597.04
Iteration:   2360, Loss function: 3.846, Average Loss: 4.392, avg. samples / sec: 56859.18
Iteration:   2360, Loss function: 4.588, Average Loss: 4.383, avg. samples / sec: 56664.04
Iteration:   2360, Loss function: 4.286, Average Loss: 4.357, avg. samples / sec: 56703.52
:::MLL 1558639708.805 epoch_stop: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 819}}
:::MLL 1558639708.806 epoch_start: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 673}}
Iteration:   2380, Loss function: 4.247, Average Loss: 4.357, avg. samples / sec: 58589.14
Iteration:   2380, Loss function: 5.309, Average Loss: 4.356, avg. samples / sec: 58394.25
Iteration:   2380, Loss function: 3.659, Average Loss: 4.344, avg. samples / sec: 58342.58
Iteration:   2380, Loss function: 4.348, Average Loss: 4.352, avg. samples / sec: 58588.12
Iteration:   2380, Loss function: 4.786, Average Loss: 4.346, avg. samples / sec: 58396.62
Iteration:   2380, Loss function: 3.819, Average Loss: 4.351, avg. samples / sec: 58430.68
Iteration:   2380, Loss function: 3.847, Average Loss: 4.355, avg. samples / sec: 58432.55
Iteration:   2380, Loss function: 4.301, Average Loss: 4.384, avg. samples / sec: 58481.85
Iteration:   2380, Loss function: 4.382, Average Loss: 4.376, avg. samples / sec: 58307.13
Iteration:   2380, Loss function: 3.837, Average Loss: 4.375, avg. samples / sec: 58312.12
Iteration:   2380, Loss function: 4.400, Average Loss: 4.359, avg. samples / sec: 58189.98
Iteration:   2380, Loss function: 3.546, Average Loss: 4.386, avg. samples / sec: 58377.97
Iteration:   2380, Loss function: 4.036, Average Loss: 4.391, avg. samples / sec: 58213.24
Iteration:   2380, Loss function: 3.991, Average Loss: 4.362, avg. samples / sec: 58218.34
Iteration:   2380, Loss function: 5.079, Average Loss: 4.371, avg. samples / sec: 57978.06
Iteration:   2400, Loss function: 3.435, Average Loss: 4.342, avg. samples / sec: 57535.18
Iteration:   2400, Loss function: 4.937, Average Loss: 4.356, avg. samples / sec: 57570.69
Iteration:   2400, Loss function: 4.593, Average Loss: 4.360, avg. samples / sec: 57608.01
Iteration:   2400, Loss function: 3.344, Average Loss: 4.348, avg. samples / sec: 57435.50
Iteration:   2400, Loss function: 3.951, Average Loss: 4.385, avg. samples / sec: 57585.04
Iteration:   2400, Loss function: 5.400, Average Loss: 4.372, avg. samples / sec: 57517.59
Iteration:   2400, Loss function: 2.691, Average Loss: 4.342, avg. samples / sec: 57375.28
Iteration:   2400, Loss function: 4.927, Average Loss: 4.350, avg. samples / sec: 57358.42
Iteration:   2400, Loss function: 3.339, Average Loss: 4.369, avg. samples / sec: 57420.33
Iteration:   2400, Loss function: 4.321, Average Loss: 4.347, avg. samples / sec: 57329.19
Iteration:   2400, Loss function: 4.229, Average Loss: 4.377, avg. samples / sec: 57360.80
Iteration:   2400, Loss function: 3.267, Average Loss: 4.354, avg. samples / sec: 57439.50
Iteration:   2400, Loss function: 3.752, Average Loss: 4.379, avg. samples / sec: 57406.18
Iteration:   2400, Loss function: 2.700, Average Loss: 4.354, avg. samples / sec: 57072.36
Iteration:   2400, Loss function: 6.040, Average Loss: 4.372, avg. samples / sec: 57497.36
Iteration:   2420, Loss function: 4.804, Average Loss: 4.345, avg. samples / sec: 57127.45
Iteration:   2420, Loss function: 6.221, Average Loss: 4.356, avg. samples / sec: 57033.93
Iteration:   2420, Loss function: 3.552, Average Loss: 4.348, avg. samples / sec: 57275.85
Iteration:   2420, Loss function: 5.051, Average Loss: 4.342, avg. samples / sec: 57127.98
Iteration:   2420, Loss function: 4.603, Average Loss: 4.344, avg. samples / sec: 57170.48
Iteration:   2420, Loss function: 3.729, Average Loss: 4.332, avg. samples / sec: 56964.08
Iteration:   2420, Loss function: 4.304, Average Loss: 4.339, avg. samples / sec: 57090.17
Iteration:   2420, Loss function: 3.828, Average Loss: 4.370, avg. samples / sec: 57202.55
Iteration:   2420, Loss function: 4.016, Average Loss: 4.368, avg. samples / sec: 57166.59
Iteration:   2420, Loss function: 4.858, Average Loss: 4.378, avg. samples / sec: 57023.71
Iteration:   2420, Loss function: 5.728, Average Loss: 4.370, avg. samples / sec: 57184.80
Iteration:   2420, Loss function: 3.243, Average Loss: 4.346, avg. samples / sec: 57111.47
Iteration:   2420, Loss function: 3.693, Average Loss: 4.362, avg. samples / sec: 56965.71
Iteration:   2420, Loss function: 3.432, Average Loss: 4.359, avg. samples / sec: 56880.57
Iteration:   2420, Loss function: 3.263, Average Loss: 4.362, avg. samples / sec: 57003.18
Iteration:   2440, Loss function: 4.439, Average Loss: 4.329, avg. samples / sec: 57984.24
Iteration:   2440, Loss function: 3.965, Average Loss: 4.374, avg. samples / sec: 57999.49
Iteration:   2440, Loss function: 3.843, Average Loss: 4.360, avg. samples / sec: 57977.66
Iteration:   2440, Loss function: 5.250, Average Loss: 4.361, avg. samples / sec: 58060.16
Iteration:   2440, Loss function: 3.384, Average Loss: 4.338, avg. samples / sec: 57922.57
Iteration:   2440, Loss function: 4.562, Average Loss: 4.336, avg. samples / sec: 58012.81
Iteration:   2440, Loss function: 4.580, Average Loss: 4.348, avg. samples / sec: 57828.85
Iteration:   2440, Loss function: 4.916, Average Loss: 4.335, avg. samples / sec: 57866.01
Iteration:   2440, Loss function: 4.438, Average Loss: 4.353, avg. samples / sec: 57982.64
Iteration:   2440, Loss function: 2.456, Average Loss: 4.341, avg. samples / sec: 57815.56
Iteration:   2440, Loss function: 4.126, Average Loss: 4.365, avg. samples / sec: 57839.98
Iteration:   2440, Loss function: 3.616, Average Loss: 4.365, avg. samples / sec: 57867.53
Iteration:   2440, Loss function: 4.417, Average Loss: 4.336, avg. samples / sec: 57770.91
Iteration:   2440, Loss function: 4.421, Average Loss: 4.338, avg. samples / sec: 57646.54
Iteration:   2440, Loss function: 4.756, Average Loss: 4.355, avg. samples / sec: 57832.72
:::MLL 1558639710.841 epoch_stop: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 819}}
:::MLL 1558639710.841 epoch_start: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 673}}
Iteration:   2460, Loss function: 6.143, Average Loss: 4.356, avg. samples / sec: 57793.94
Iteration:   2460, Loss function: 5.133, Average Loss: 4.335, avg. samples / sec: 57778.51
Iteration:   2460, Loss function: 3.325, Average Loss: 4.324, avg. samples / sec: 57634.92
Iteration:   2460, Loss function: 4.988, Average Loss: 4.333, avg. samples / sec: 57845.01
Iteration:   2460, Loss function: 4.241, Average Loss: 4.342, avg. samples / sec: 57713.61
Iteration:   2460, Loss function: 4.415, Average Loss: 4.346, avg. samples / sec: 57695.46
Iteration:   2460, Loss function: 5.703, Average Loss: 4.342, avg. samples / sec: 57667.11
Iteration:   2460, Loss function: 3.324, Average Loss: 4.328, avg. samples / sec: 57761.44
Iteration:   2460, Loss function: 3.499, Average Loss: 4.333, avg. samples / sec: 57645.50
Iteration:   2460, Loss function: 3.619, Average Loss: 4.331, avg. samples / sec: 57621.60
Iteration:   2460, Loss function: 3.465, Average Loss: 4.357, avg. samples / sec: 57587.93
Iteration:   2460, Loss function: 4.010, Average Loss: 4.367, avg. samples / sec: 57569.33
Iteration:   2460, Loss function: 3.622, Average Loss: 4.350, avg. samples / sec: 57804.20
Iteration:   2460, Loss function: 3.561, Average Loss: 4.366, avg. samples / sec: 57699.76
Iteration:   2460, Loss function: 3.984, Average Loss: 4.363, avg. samples / sec: 57691.24
Iteration:   2480, Loss function: 4.463, Average Loss: 4.332, avg. samples / sec: 58687.81
Iteration:   2480, Loss function: 4.752, Average Loss: 4.350, avg. samples / sec: 58750.29
Iteration:   2480, Loss function: 4.834, Average Loss: 4.333, avg. samples / sec: 58703.21
Iteration:   2480, Loss function: 4.249, Average Loss: 4.334, avg. samples / sec: 58627.01
Iteration:   2480, Loss function: 4.597, Average Loss: 4.357, avg. samples / sec: 58746.91
Iteration:   2480, Loss function: 3.969, Average Loss: 4.328, avg. samples / sec: 58665.33
Iteration:   2480, Loss function: 4.605, Average Loss: 4.362, avg. samples / sec: 58706.31
Iteration:   2480, Loss function: 4.009, Average Loss: 4.349, avg. samples / sec: 58417.82
Iteration:   2480, Loss function: 3.251, Average Loss: 4.356, avg. samples / sec: 58627.53
Iteration:   2480, Loss function: 5.380, Average Loss: 4.346, avg. samples / sec: 58650.66
Iteration:   2480, Loss function: 4.392, Average Loss: 4.320, avg. samples / sec: 58598.91
Iteration:   2480, Loss function: 3.683, Average Loss: 4.339, avg. samples / sec: 58586.82
Iteration:   2480, Loss function: 3.117, Average Loss: 4.325, avg. samples / sec: 58597.59
Iteration:   2480, Loss function: 4.766, Average Loss: 4.342, avg. samples / sec: 58528.02
Iteration:   2480, Loss function: 5.110, Average Loss: 4.322, avg. samples / sec: 58433.69
Iteration:   2500, Loss function: 3.972, Average Loss: 4.336, avg. samples / sec: 59520.84
Iteration:   2500, Loss function: 4.523, Average Loss: 4.336, avg. samples / sec: 59655.96
Iteration:   2500, Loss function: 4.005, Average Loss: 4.345, avg. samples / sec: 59510.30
Iteration:   2500, Loss function: 4.521, Average Loss: 4.340, avg. samples / sec: 59704.86
Iteration:   2500, Loss function: 4.487, Average Loss: 4.364, avg. samples / sec: 59572.04
Iteration:   2500, Loss function: 2.640, Average Loss: 4.338, avg. samples / sec: 59563.15
Iteration:   2500, Loss function: 5.019, Average Loss: 4.326, avg. samples / sec: 59612.03
Iteration:   2500, Loss function: 4.277, Average Loss: 4.342, avg. samples / sec: 59582.24
Iteration:   2500, Loss function: 4.902, Average Loss: 4.311, avg. samples / sec: 59572.94
Iteration:   2500, Loss function: 3.537, Average Loss: 4.316, avg. samples / sec: 59625.90
Iteration:   2500, Loss function: 3.887, Average Loss: 4.332, avg. samples / sec: 59307.32
Iteration:   2500, Loss function: 3.760, Average Loss: 4.321, avg. samples / sec: 59398.49
Iteration:   2500, Loss function: 3.718, Average Loss: 4.354, avg. samples / sec: 59379.94
Iteration:   2500, Loss function: 3.399, Average Loss: 4.348, avg. samples / sec: 59403.24
Iteration:   2500, Loss function: 3.688, Average Loss: 4.331, avg. samples / sec: 59164.36
:::MLL 1558639712.848 epoch_stop: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 819}}
:::MLL 1558639712.849 epoch_start: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 673}}
Iteration:   2520, Loss function: 3.828, Average Loss: 4.333, avg. samples / sec: 59277.56
Iteration:   2520, Loss function: 4.282, Average Loss: 4.327, avg. samples / sec: 59220.32
Iteration:   2520, Loss function: 5.641, Average Loss: 4.350, avg. samples / sec: 59308.12
Iteration:   2520, Loss function: 4.111, Average Loss: 4.317, avg. samples / sec: 59156.93
Iteration:   2520, Loss function: 3.352, Average Loss: 4.343, avg. samples / sec: 59312.59
Iteration:   2520, Loss function: 4.517, Average Loss: 4.341, avg. samples / sec: 59061.83
Iteration:   2520, Loss function: 3.512, Average Loss: 4.324, avg. samples / sec: 59446.27
Iteration:   2520, Loss function: 4.249, Average Loss: 4.338, avg. samples / sec: 59119.36
Iteration:   2520, Loss function: 5.268, Average Loss: 4.316, avg. samples / sec: 59140.60
Iteration:   2520, Loss function: 4.388, Average Loss: 4.300, avg. samples / sec: 59107.61
Iteration:   2520, Loss function: 3.636, Average Loss: 4.336, avg. samples / sec: 59025.39
Iteration:   2520, Loss function: 3.917, Average Loss: 4.326, avg. samples / sec: 59006.54
Iteration:   2520, Loss function: 3.911, Average Loss: 4.357, avg. samples / sec: 58987.59
Iteration:   2520, Loss function: 3.869, Average Loss: 4.328, avg. samples / sec: 59078.67
Iteration:   2520, Loss function: 3.467, Average Loss: 4.313, avg. samples / sec: 59123.03
Iteration:   2540, Loss function: 4.286, Average Loss: 4.315, avg. samples / sec: 58079.11
Iteration:   2540, Loss function: 3.915, Average Loss: 4.333, avg. samples / sec: 58117.50
Iteration:   2540, Loss function: 4.352, Average Loss: 4.321, avg. samples / sec: 57912.64
Iteration:   2540, Loss function: 3.616, Average Loss: 4.336, avg. samples / sec: 58067.03
Iteration:   2540, Loss function: 3.298, Average Loss: 4.319, avg. samples / sec: 58115.88
Iteration:   2540, Loss function: 3.177, Average Loss: 4.313, avg. samples / sec: 58062.07
Iteration:   2540, Loss function: 4.976, Average Loss: 4.355, avg. samples / sec: 58118.32
Iteration:   2540, Loss function: 3.967, Average Loss: 4.338, avg. samples / sec: 58029.39
Iteration:   2540, Loss function: 5.258, Average Loss: 4.301, avg. samples / sec: 58000.61
Iteration:   2540, Loss function: 3.676, Average Loss: 4.321, avg. samples / sec: 58080.52
Iteration:   2540, Loss function: 3.286, Average Loss: 4.311, avg. samples / sec: 58048.80
Iteration:   2540, Loss function: 3.502, Average Loss: 4.329, avg. samples / sec: 57928.80
Iteration:   2540, Loss function: 4.498, Average Loss: 4.320, avg. samples / sec: 57899.86
Iteration:   2540, Loss function: 3.414, Average Loss: 4.326, avg. samples / sec: 57730.49
Iteration:   2540, Loss function: 5.128, Average Loss: 4.342, avg. samples / sec: 57795.27
Iteration:   2560, Loss function: 4.930, Average Loss: 4.326, avg. samples / sec: 58019.07
Iteration:   2560, Loss function: 3.652, Average Loss: 4.305, avg. samples / sec: 57849.19
Iteration:   2560, Loss function: 3.922, Average Loss: 4.320, avg. samples / sec: 57822.28
Iteration:   2560, Loss function: 3.465, Average Loss: 4.321, avg. samples / sec: 57991.07
Iteration:   2560, Loss function: 6.127, Average Loss: 4.347, avg. samples / sec: 57834.73
Iteration:   2560, Loss function: 4.279, Average Loss: 4.330, avg. samples / sec: 57827.50
Iteration:   2560, Loss function: 4.094, Average Loss: 4.316, avg. samples / sec: 57769.92
Iteration:   2560, Loss function: 4.179, Average Loss: 4.319, avg. samples / sec: 57920.30
Iteration:   2560, Loss function: 4.151, Average Loss: 4.333, avg. samples / sec: 57722.52
Iteration:   2560, Loss function: 4.243, Average Loss: 4.307, avg. samples / sec: 57861.97
Iteration:   2560, Loss function: 4.474, Average Loss: 4.310, avg. samples / sec: 57667.02
Iteration:   2560, Loss function: 2.806, Average Loss: 4.330, avg. samples / sec: 57658.29
Iteration:   2560, Loss function: 3.355, Average Loss: 4.336, avg. samples / sec: 57888.14
Iteration:   2560, Loss function: 3.624, Average Loss: 4.314, avg. samples / sec: 57777.99
Iteration:   2560, Loss function: 4.855, Average Loss: 4.293, avg. samples / sec: 57768.05
Iteration:   2580, Loss function: 3.459, Average Loss: 4.323, avg. samples / sec: 58243.92
Iteration:   2580, Loss function: 3.167, Average Loss: 4.282, avg. samples / sec: 58354.13
Iteration:   2580, Loss function: 3.816, Average Loss: 4.326, avg. samples / sec: 58272.17
Iteration:   2580, Loss function: 4.385, Average Loss: 4.310, avg. samples / sec: 58199.20
Iteration:   2580, Loss function: 3.755, Average Loss: 4.316, avg. samples / sec: 58197.45
Iteration:   2580, Loss function: 5.038, Average Loss: 4.309, avg. samples / sec: 58255.43
Iteration:   2580, Loss function: 4.418, Average Loss: 4.328, avg. samples / sec: 58258.99
Iteration:   2580, Loss function: 4.302, Average Loss: 4.324, avg. samples / sec: 58192.16
Iteration:   2580, Loss function: 4.065, Average Loss: 4.299, avg. samples / sec: 58168.14
Iteration:   2580, Loss function: 3.431, Average Loss: 4.323, avg. samples / sec: 58057.72
Iteration:   2580, Loss function: 4.345, Average Loss: 4.294, avg. samples / sec: 57948.60
Iteration:   2580, Loss function: 4.650, Average Loss: 4.308, avg. samples / sec: 58039.50
Iteration:   2580, Loss function: 3.411, Average Loss: 4.319, avg. samples / sec: 57880.55
Iteration:   2580, Loss function: 3.811, Average Loss: 4.339, avg. samples / sec: 57881.17
Iteration:   2580, Loss function: 4.339, Average Loss: 4.319, avg. samples / sec: 57811.53
:::MLL 1558639714.869 epoch_stop: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 819}}
:::MLL 1558639714.869 epoch_start: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 673}}
Iteration:   2600, Loss function: 4.688, Average Loss: 4.304, avg. samples / sec: 57581.72
Iteration:   2600, Loss function: 4.655, Average Loss: 4.308, avg. samples / sec: 57617.76
Iteration:   2600, Loss function: 3.309, Average Loss: 4.286, avg. samples / sec: 57524.16
Iteration:   2600, Loss function: 4.470, Average Loss: 4.320, avg. samples / sec: 57452.42
Iteration:   2600, Loss function: 4.715, Average Loss: 4.298, avg. samples / sec: 57589.79
Iteration:   2600, Loss function: 4.498, Average Loss: 4.319, avg. samples / sec: 57655.50
Iteration:   2600, Loss function: 4.334, Average Loss: 4.319, avg. samples / sec: 57516.86
Iteration:   2600, Loss function: 5.114, Average Loss: 4.302, avg. samples / sec: 57714.11
Iteration:   2600, Loss function: 4.659, Average Loss: 4.324, avg. samples / sec: 57539.76
Iteration:   2600, Loss function: 2.768, Average Loss: 4.314, avg. samples / sec: 57829.54
Iteration:   2600, Loss function: 3.738, Average Loss: 4.318, avg. samples / sec: 57524.72
Iteration:   2600, Loss function: 4.967, Average Loss: 4.316, avg. samples / sec: 57729.17
Iteration:   2600, Loss function: 4.555, Average Loss: 4.333, avg. samples / sec: 57685.31
Iteration:   2600, Loss function: 2.968, Average Loss: 4.284, avg. samples / sec: 57563.35
Iteration:   2600, Loss function: 4.213, Average Loss: 4.318, avg. samples / sec: 57378.11
Iteration:   2620, Loss function: 3.831, Average Loss: 4.313, avg. samples / sec: 58482.82
Iteration:   2620, Loss function: 4.069, Average Loss: 4.310, avg. samples / sec: 58533.95
Iteration:   2620, Loss function: 4.025, Average Loss: 4.295, avg. samples / sec: 58405.31
Iteration:   2620, Loss function: 4.427, Average Loss: 4.318, avg. samples / sec: 58410.32
Iteration:   2620, Loss function: 3.415, Average Loss: 4.308, avg. samples / sec: 58365.23
Iteration:   2620, Loss function: 3.123, Average Loss: 4.299, avg. samples / sec: 58241.27
Iteration:   2620, Loss function: 3.588, Average Loss: 4.303, avg. samples / sec: 58230.37
Iteration:   2620, Loss function: 4.160, Average Loss: 4.317, avg. samples / sec: 58233.88
Iteration:   2620, Loss function: 4.942, Average Loss: 4.333, avg. samples / sec: 58399.47
Iteration:   2620, Loss function: 3.143, Average Loss: 4.306, avg. samples / sec: 58270.72
Iteration:   2620, Loss function: 3.794, Average Loss: 4.310, avg. samples / sec: 58390.96
Iteration:   2620, Loss function: 5.185, Average Loss: 4.303, avg. samples / sec: 58200.33
Iteration:   2620, Loss function: 2.991, Average Loss: 4.312, avg. samples / sec: 58194.04
Iteration:   2620, Loss function: 3.862, Average Loss: 4.285, avg. samples / sec: 58054.30
Iteration:   2620, Loss function: 4.349, Average Loss: 4.276, avg. samples / sec: 58270.77
Iteration:   2640, Loss function: 4.274, Average Loss: 4.292, avg. samples / sec: 55993.93
Iteration:   2640, Loss function: 3.546, Average Loss: 4.310, avg. samples / sec: 55945.17
Iteration:   2640, Loss function: 4.887, Average Loss: 4.313, avg. samples / sec: 55852.84
Iteration:   2640, Loss function: 3.910, Average Loss: 4.300, avg. samples / sec: 56004.75
Iteration:   2640, Loss function: 4.662, Average Loss: 4.302, avg. samples / sec: 56101.78
Iteration:   2640, Loss function: 3.467, Average Loss: 4.293, avg. samples / sec: 55992.44
Iteration:   2640, Loss function: 2.960, Average Loss: 4.296, avg. samples / sec: 56098.94
Iteration:   2640, Loss function: 3.129, Average Loss: 4.329, avg. samples / sec: 56011.25
Iteration:   2640, Loss function: 3.681, Average Loss: 4.306, avg. samples / sec: 55937.06
Iteration:   2640, Loss function: 3.112, Average Loss: 4.282, avg. samples / sec: 56141.85
Iteration:   2640, Loss function: 4.345, Average Loss: 4.308, avg. samples / sec: 55791.44
Iteration:   2640, Loss function: 3.823, Average Loss: 4.301, avg. samples / sec: 56012.74
Iteration:   2640, Loss function: 4.594, Average Loss: 4.311, avg. samples / sec: 55929.22
Iteration:   2640, Loss function: 4.596, Average Loss: 4.307, avg. samples / sec: 55951.76
Iteration:   2640, Loss function: 3.675, Average Loss: 4.268, avg. samples / sec: 55917.37
:::MLL 1558639716.920 epoch_stop: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 819}}
:::MLL 1558639716.921 epoch_start: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 673}}
Iteration:   2660, Loss function: 4.269, Average Loss: 4.322, avg. samples / sec: 58844.69
Iteration:   2660, Loss function: 4.402, Average Loss: 4.308, avg. samples / sec: 58799.61
Iteration:   2660, Loss function: 4.115, Average Loss: 4.291, avg. samples / sec: 58565.47
Iteration:   2660, Loss function: 2.829, Average Loss: 4.305, avg. samples / sec: 58659.69
Iteration:   2660, Loss function: 3.838, Average Loss: 4.301, avg. samples / sec: 58797.40
Iteration:   2660, Loss function: 4.772, Average Loss: 4.276, avg. samples / sec: 58665.45
Iteration:   2660, Loss function: 3.442, Average Loss: 4.293, avg. samples / sec: 58630.97
Iteration:   2660, Loss function: 4.403, Average Loss: 4.298, avg. samples / sec: 58637.85
Iteration:   2660, Loss function: 3.983, Average Loss: 4.307, avg. samples / sec: 58648.44
Iteration:   2660, Loss function: 4.590, Average Loss: 4.313, avg. samples / sec: 58513.61
Iteration:   2660, Loss function: 3.777, Average Loss: 4.286, avg. samples / sec: 58511.49
Iteration:   2660, Loss function: 4.299, Average Loss: 4.270, avg. samples / sec: 58763.91
Iteration:   2660, Loss function: 3.801, Average Loss: 4.286, avg. samples / sec: 58469.45
Iteration:   2660, Loss function: 4.027, Average Loss: 4.291, avg. samples / sec: 58445.30
Iteration:   2660, Loss function: 4.758, Average Loss: 4.295, avg. samples / sec: 58467.51
Iteration:   2680, Loss function: 4.155, Average Loss: 4.292, avg. samples / sec: 55724.68
Iteration:   2680, Loss function: 3.380, Average Loss: 4.294, avg. samples / sec: 55492.14
Iteration:   2680, Loss function: 4.590, Average Loss: 4.293, avg. samples / sec: 55457.62
Iteration:   2680, Loss function: 4.639, Average Loss: 4.288, avg. samples / sec: 55473.22
Iteration:   2680, Loss function: 3.712, Average Loss: 4.305, avg. samples / sec: 55388.61
Iteration:   2680, Loss function: 4.104, Average Loss: 4.286, avg. samples / sec: 55476.76
Iteration:   2680, Loss function: 4.747, Average Loss: 4.286, avg. samples / sec: 55625.86
Iteration:   2680, Loss function: 3.160, Average Loss: 4.266, avg. samples / sec: 55449.28
Iteration:   2680, Loss function: 5.064, Average Loss: 4.309, avg. samples / sec: 55485.63
Iteration:   2680, Loss function: 4.388, Average Loss: 4.302, avg. samples / sec: 55386.10
Iteration:   2680, Loss function: 3.625, Average Loss: 4.280, avg. samples / sec: 55509.21
Iteration:   2680, Loss function: 5.077, Average Loss: 4.270, avg. samples / sec: 55479.93
Iteration:   2680, Loss function: 3.515, Average Loss: 4.283, avg. samples / sec: 55466.74
Iteration:   2680, Loss function: 4.053, Average Loss: 4.286, avg. samples / sec: 55236.27
Iteration:   2680, Loss function: 4.800, Average Loss: 4.313, avg. samples / sec: 55042.67
Iteration:   2700, Loss function: 3.365, Average Loss: 4.300, avg. samples / sec: 59236.75
Iteration:   2700, Loss function: 3.167, Average Loss: 4.272, avg. samples / sec: 59329.27
Iteration:   2700, Loss function: 3.080, Average Loss: 4.286, avg. samples / sec: 59128.24
Iteration:   2700, Loss function: 4.894, Average Loss: 4.285, avg. samples / sec: 59055.42
Iteration:   2700, Loss function: 3.978, Average Loss: 4.287, avg. samples / sec: 59095.41
Iteration:   2700, Loss function: 2.966, Average Loss: 4.307, avg. samples / sec: 59162.42
Iteration:   2700, Loss function: 3.639, Average Loss: 4.276, avg. samples / sec: 59193.36
Iteration:   2700, Loss function: 4.626, Average Loss: 4.282, avg. samples / sec: 59076.69
Iteration:   2700, Loss function: 5.242, Average Loss: 4.268, avg. samples / sec: 59147.15
Iteration:   2700, Loss function: 3.166, Average Loss: 4.304, avg. samples / sec: 59290.06
Iteration:   2700, Loss function: 5.748, Average Loss: 4.282, avg. samples / sec: 58982.21
Iteration:   2700, Loss function: 3.466, Average Loss: 4.265, avg. samples / sec: 58963.92
Iteration:   2700, Loss function: 4.334, Average Loss: 4.284, avg. samples / sec: 58878.07
Iteration:   2700, Loss function: 5.455, Average Loss: 4.282, avg. samples / sec: 59060.84
Iteration:   2700, Loss function: 4.361, Average Loss: 4.303, avg. samples / sec: 58931.43
Iteration:   2720, Loss function: 3.768, Average Loss: 4.286, avg. samples / sec: 56521.41
Iteration:   2720, Loss function: 3.198, Average Loss: 4.279, avg. samples / sec: 56579.36
Iteration:   2720, Loss function: 5.489, Average Loss: 4.288, avg. samples / sec: 56595.59
Iteration:   2720, Loss function: 3.911, Average Loss: 4.299, avg. samples / sec: 56678.78
Iteration:   2720, Loss function: 3.355, Average Loss: 4.276, avg. samples / sec: 56609.73
Iteration:   2720, Loss function: 3.558, Average Loss: 4.299, avg. samples / sec: 56548.40
Iteration:   2720, Loss function: 4.247, Average Loss: 4.277, avg. samples / sec: 56788.34
Iteration:   2720, Loss function: 4.062, Average Loss: 4.280, avg. samples / sec: 56747.96
Iteration:   2720, Loss function: 3.239, Average Loss: 4.266, avg. samples / sec: 56657.93
Iteration:   2720, Loss function: 4.137, Average Loss: 4.268, avg. samples / sec: 56400.21
Iteration:   2720, Loss function: 3.702, Average Loss: 4.275, avg. samples / sec: 56512.98
Iteration:   2720, Loss function: 4.109, Average Loss: 4.303, avg. samples / sec: 56752.94
Iteration:   2720, Loss function: 4.205, Average Loss: 4.264, avg. samples / sec: 56523.68
Iteration:   2720, Loss function: 3.896, Average Loss: 4.275, avg. samples / sec: 56543.90
Iteration:   2720, Loss function: 4.651, Average Loss: 4.301, avg. samples / sec: 56312.95
:::MLL 1558639718.968 epoch_stop: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 819}}
:::MLL 1558639718.969 epoch_start: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 673}}
Iteration:   2740, Loss function: 4.595, Average Loss: 4.297, avg. samples / sec: 59905.74
Iteration:   2740, Loss function: 4.534, Average Loss: 4.295, avg. samples / sec: 59638.59
Iteration:   2740, Loss function: 3.616, Average Loss: 4.273, avg. samples / sec: 59625.75
Iteration:   2740, Loss function: 3.803, Average Loss: 4.284, avg. samples / sec: 59608.09
Iteration:   2740, Loss function: 3.213, Average Loss: 4.277, avg. samples / sec: 59606.91
Iteration:   2740, Loss function: 3.910, Average Loss: 4.292, avg. samples / sec: 59590.88
Iteration:   2740, Loss function: 4.204, Average Loss: 4.282, avg. samples / sec: 59520.84
Iteration:   2740, Loss function: 4.310, Average Loss: 4.265, avg. samples / sec: 59635.16
Iteration:   2740, Loss function: 3.665, Average Loss: 4.296, avg. samples / sec: 59627.11
Iteration:   2740, Loss function: 4.018, Average Loss: 4.263, avg. samples / sec: 59604.84
Iteration:   2740, Loss function: 3.435, Average Loss: 4.272, avg. samples / sec: 59501.18
Iteration:   2740, Loss function: 3.816, Average Loss: 4.259, avg. samples / sec: 59554.36
Iteration:   2740, Loss function: 3.789, Average Loss: 4.270, avg. samples / sec: 59637.33
Iteration:   2740, Loss function: 4.929, Average Loss: 4.255, avg. samples / sec: 59540.85
Iteration:   2740, Loss function: 4.688, Average Loss: 4.271, avg. samples / sec: 59430.73
Iteration:   2760, Loss function: 3.564, Average Loss: 4.293, avg. samples / sec: 57883.93
Iteration:   2760, Loss function: 2.116, Average Loss: 4.286, avg. samples / sec: 58025.83
Iteration:   2760, Loss function: 3.937, Average Loss: 4.276, avg. samples / sec: 57864.77
Iteration:   2760, Loss function: 5.165, Average Loss: 4.252, avg. samples / sec: 58083.90
Iteration:   2760, Loss function: 3.490, Average Loss: 4.276, avg. samples / sec: 57926.85
Iteration:   2760, Loss function: 4.021, Average Loss: 4.294, avg. samples / sec: 57792.64
Iteration:   2760, Loss function: 3.030, Average Loss: 4.292, avg. samples / sec: 57919.57
Iteration:   2760, Loss function: 3.661, Average Loss: 4.273, avg. samples / sec: 57824.36
Iteration:   2760, Loss function: 3.906, Average Loss: 4.257, avg. samples / sec: 57857.81
Iteration:   2760, Loss function: 3.293, Average Loss: 4.266, avg. samples / sec: 57846.96
Iteration:   2760, Loss function: 3.506, Average Loss: 4.270, avg. samples / sec: 57694.24
Iteration:   2760, Loss function: 4.907, Average Loss: 4.263, avg. samples / sec: 57878.75
Iteration:   2760, Loss function: 3.343, Average Loss: 4.262, avg. samples / sec: 57687.53
Iteration:   2760, Loss function: 4.074, Average Loss: 4.266, avg. samples / sec: 57707.30
Iteration:   2760, Loss function: 4.160, Average Loss: 4.253, avg. samples / sec: 57671.17
Iteration:   2780, Loss function: 3.347, Average Loss: 4.273, avg. samples / sec: 60034.87
Iteration:   2780, Loss function: 4.945, Average Loss: 4.259, avg. samples / sec: 60094.34
Iteration:   2780, Loss function: 3.666, Average Loss: 4.252, avg. samples / sec: 60287.35
Iteration:   2780, Loss function: 3.880, Average Loss: 4.267, avg. samples / sec: 60063.09
Iteration:   2780, Loss function: 3.846, Average Loss: 4.262, avg. samples / sec: 60197.61
Iteration:   2780, Loss function: 4.460, Average Loss: 4.282, avg. samples / sec: 59724.09
Iteration:   2780, Loss function: 4.014, Average Loss: 4.259, avg. samples / sec: 60099.08
Iteration:   2780, Loss function: 3.529, Average Loss: 4.261, avg. samples / sec: 60017.48
Iteration:   2780, Loss function: 3.412, Average Loss: 4.290, avg. samples / sec: 59640.03
Iteration:   2780, Loss function: 3.909, Average Loss: 4.251, avg. samples / sec: 59882.27
Iteration:   2780, Loss function: 3.800, Average Loss: 4.290, avg. samples / sec: 59798.62
Iteration:   2780, Loss function: 4.116, Average Loss: 4.248, avg. samples / sec: 59747.59
Iteration:   2780, Loss function: 4.028, Average Loss: 4.276, avg. samples / sec: 59726.19
Iteration:   2780, Loss function: 4.121, Average Loss: 4.266, avg. samples / sec: 59811.21
Iteration:   2780, Loss function: 3.447, Average Loss: 4.284, avg. samples / sec: 59696.59
:::MLL 1558639720.963 epoch_stop: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 819}}
:::MLL 1558639720.964 epoch_start: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 673}}
Iteration:   2800, Loss function: 2.999, Average Loss: 4.262, avg. samples / sec: 59102.43
Iteration:   2800, Loss function: 2.502, Average Loss: 4.272, avg. samples / sec: 59238.67
Iteration:   2800, Loss function: 3.687, Average Loss: 4.283, avg. samples / sec: 59175.89
Iteration:   2800, Loss function: 4.205, Average Loss: 4.256, avg. samples / sec: 59119.96
Iteration:   2800, Loss function: 5.109, Average Loss: 4.241, avg. samples / sec: 59163.17
Iteration:   2800, Loss function: 2.660, Average Loss: 4.250, avg. samples / sec: 58979.45
Iteration:   2800, Loss function: 3.413, Average Loss: 4.283, avg. samples / sec: 59024.38
Iteration:   2800, Loss function: 4.231, Average Loss: 4.261, avg. samples / sec: 59046.49
Iteration:   2800, Loss function: 3.673, Average Loss: 4.247, avg. samples / sec: 58909.11
Iteration:   2800, Loss function: 5.055, Average Loss: 4.268, avg. samples / sec: 58758.72
Iteration:   2800, Loss function: 4.281, Average Loss: 4.258, avg. samples / sec: 58806.26
Iteration:   2800, Loss function: 3.513, Average Loss: 4.247, avg. samples / sec: 58969.48
Iteration:   2800, Loss function: 4.658, Average Loss: 4.280, avg. samples / sec: 59012.69
Iteration:   2800, Loss function: 4.578, Average Loss: 4.284, avg. samples / sec: 58820.03
Iteration:   2800, Loss function: 3.883, Average Loss: 4.258, avg. samples / sec: 58802.73
Iteration:   2820, Loss function: 3.702, Average Loss: 4.243, avg. samples / sec: 58729.43
Iteration:   2820, Loss function: 2.736, Average Loss: 4.243, avg. samples / sec: 58688.05
Iteration:   2820, Loss function: 4.430, Average Loss: 4.236, avg. samples / sec: 58644.21
Iteration:   2820, Loss function: 3.999, Average Loss: 4.254, avg. samples / sec: 59007.47
Iteration:   2820, Loss function: 4.775, Average Loss: 4.280, avg. samples / sec: 58683.73
Iteration:   2820, Loss function: 4.039, Average Loss: 4.253, avg. samples / sec: 58685.66
Iteration:   2820, Loss function: 5.025, Average Loss: 4.252, avg. samples / sec: 58554.42
Iteration:   2820, Loss function: 3.175, Average Loss: 4.262, avg. samples / sec: 58664.84
Iteration:   2820, Loss function: 5.054, Average Loss: 4.248, avg. samples / sec: 58683.33
Iteration:   2820, Loss function: 3.840, Average Loss: 4.278, avg. samples / sec: 58482.79
Iteration:   2820, Loss function: 4.748, Average Loss: 4.250, avg. samples / sec: 58636.24
Iteration:   2820, Loss function: 3.724, Average Loss: 4.270, avg. samples / sec: 58401.26
Iteration:   2820, Loss function: 4.541, Average Loss: 4.257, avg. samples / sec: 58350.85
Iteration:   2820, Loss function: 3.594, Average Loss: 4.282, avg. samples / sec: 58718.35
Iteration:   2820, Loss function: 2.140, Average Loss: 4.272, avg. samples / sec: 58575.28
Iteration:   2840, Loss function: 3.727, Average Loss: 4.252, avg. samples / sec: 57544.71
Iteration:   2840, Loss function: 4.694, Average Loss: 4.243, avg. samples / sec: 57276.39
Iteration:   2840, Loss function: 3.397, Average Loss: 4.273, avg. samples / sec: 57378.58
Iteration:   2840, Loss function: 3.577, Average Loss: 4.248, avg. samples / sec: 57285.98
Iteration:   2840, Loss function: 4.255, Average Loss: 4.258, avg. samples / sec: 57290.62
Iteration:   2840, Loss function: 4.084, Average Loss: 4.242, avg. samples / sec: 57314.80
Iteration:   2840, Loss function: 3.174, Average Loss: 4.243, avg. samples / sec: 57336.16
Iteration:   2840, Loss function: 3.811, Average Loss: 4.245, avg. samples / sec: 57175.21
Iteration:   2840, Loss function: 3.892, Average Loss: 4.252, avg. samples / sec: 57203.76
Iteration:   2840, Loss function: 3.988, Average Loss: 4.229, avg. samples / sec: 57135.18
Iteration:   2840, Loss function: 4.087, Average Loss: 4.272, avg. samples / sec: 57328.67
Iteration:   2840, Loss function: 4.573, Average Loss: 4.279, avg. samples / sec: 57097.59
Iteration:   2840, Loss function: 3.783, Average Loss: 4.247, avg. samples / sec: 57120.36
Iteration:   2840, Loss function: 4.019, Average Loss: 4.263, avg. samples / sec: 57195.82
Iteration:   2840, Loss function: 5.762, Average Loss: 4.273, avg. samples / sec: 57213.28
Iteration:   2860, Loss function: 3.700, Average Loss: 4.241, avg. samples / sec: 59517.27
Iteration:   2860, Loss function: 4.612, Average Loss: 4.242, avg. samples / sec: 59477.33
Iteration:   2860, Loss function: 3.957, Average Loss: 4.275, avg. samples / sec: 59807.96
Iteration:   2860, Loss function: 4.083, Average Loss: 4.248, avg. samples / sec: 59480.92
Iteration:   2860, Loss function: 4.413, Average Loss: 4.238, avg. samples / sec: 59479.29
Iteration:   2860, Loss function: 4.177, Average Loss: 4.244, avg. samples / sec: 59472.31
Iteration:   2860, Loss function: 4.617, Average Loss: 4.267, avg. samples / sec: 59388.60
Iteration:   2860, Loss function: 3.899, Average Loss: 4.271, avg. samples / sec: 59534.87
Iteration:   2860, Loss function: 5.423, Average Loss: 4.225, avg. samples / sec: 59467.11
Iteration:   2860, Loss function: 3.077, Average Loss: 4.247, avg. samples / sec: 59188.73
Iteration:   2860, Loss function: 3.609, Average Loss: 4.243, avg. samples / sec: 59507.79
Iteration:   2860, Loss function: 2.676, Average Loss: 4.261, avg. samples / sec: 59516.61
Iteration:   2860, Loss function: 3.439, Average Loss: 4.274, avg. samples / sec: 59424.24
Iteration:   2860, Loss function: 3.900, Average Loss: 4.242, avg. samples / sec: 59204.00
Iteration:   2860, Loss function: 3.878, Average Loss: 4.248, avg. samples / sec: 59202.31
:::MLL 1558639722.974 epoch_stop: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 819}}
:::MLL 1558639722.975 epoch_start: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 673}}
Iteration:   2880, Loss function: 4.070, Average Loss: 4.271, avg. samples / sec: 56971.81
Iteration:   2880, Loss function: 4.405, Average Loss: 4.214, avg. samples / sec: 57059.26
Iteration:   2880, Loss function: 5.378, Average Loss: 4.239, avg. samples / sec: 56928.46
Iteration:   2880, Loss function: 3.675, Average Loss: 4.264, avg. samples / sec: 57013.81
Iteration:   2880, Loss function: 4.258, Average Loss: 4.257, avg. samples / sec: 57105.02
Iteration:   2880, Loss function: 3.236, Average Loss: 4.239, avg. samples / sec: 57031.14
Iteration:   2880, Loss function: 2.954, Average Loss: 4.259, avg. samples / sec: 56952.57
Iteration:   2880, Loss function: 4.695, Average Loss: 4.241, avg. samples / sec: 56940.56
Iteration:   2880, Loss function: 4.293, Average Loss: 4.241, avg. samples / sec: 56934.07
Iteration:   2880, Loss function: 4.871, Average Loss: 4.269, avg. samples / sec: 57085.38
Iteration:   2880, Loss function: 3.499, Average Loss: 4.241, avg. samples / sec: 57173.10
Iteration:   2880, Loss function: 4.050, Average Loss: 4.244, avg. samples / sec: 57176.21
Iteration:   2880, Loss function: 4.422, Average Loss: 4.236, avg. samples / sec: 56777.54
Iteration:   2880, Loss function: 3.304, Average Loss: 4.240, avg. samples / sec: 56820.78
Iteration:   2880, Loss function: 4.731, Average Loss: 4.232, avg. samples / sec: 56950.45
Iteration:   2900, Loss function: 3.144, Average Loss: 4.234, avg. samples / sec: 59751.85
Iteration:   2900, Loss function: 5.113, Average Loss: 4.236, avg. samples / sec: 59778.89
Iteration:   2900, Loss function: 5.315, Average Loss: 4.214, avg. samples / sec: 59625.52
Iteration:   2900, Loss function: 4.944, Average Loss: 4.259, avg. samples / sec: 59585.54
Iteration:   2900, Loss function: 4.213, Average Loss: 4.229, avg. samples / sec: 59593.30
Iteration:   2900, Loss function: 3.914, Average Loss: 4.260, avg. samples / sec: 59515.86
Iteration:   2900, Loss function: 3.408, Average Loss: 4.267, avg. samples / sec: 59477.96
Iteration:   2900, Loss function: 3.546, Average Loss: 4.235, avg. samples / sec: 59506.86
Iteration:   2900, Loss function: 3.973, Average Loss: 4.250, avg. samples / sec: 59520.91
Iteration:   2900, Loss function: 3.663, Average Loss: 4.236, avg. samples / sec: 59657.37
Iteration:   2900, Loss function: 4.897, Average Loss: 4.262, avg. samples / sec: 59522.39
Iteration:   2900, Loss function: 3.037, Average Loss: 4.239, avg. samples / sec: 59509.22
Iteration:   2900, Loss function: 5.319, Average Loss: 4.226, avg. samples / sec: 59494.38
Iteration:   2900, Loss function: 1.998, Average Loss: 4.227, avg. samples / sec: 59359.16
Iteration:   2900, Loss function: 5.338, Average Loss: 4.232, avg. samples / sec: 59372.36
Iteration:   2920, Loss function: 3.781, Average Loss: 4.263, avg. samples / sec: 59848.32
Iteration:   2920, Loss function: 2.997, Average Loss: 4.230, avg. samples / sec: 59598.34
Iteration:   2920, Loss function: 5.014, Average Loss: 4.215, avg. samples / sec: 59570.27
Iteration:   2920, Loss function: 3.567, Average Loss: 4.226, avg. samples / sec: 59504.65
Iteration:   2920, Loss function: 4.162, Average Loss: 4.232, avg. samples / sec: 59714.10
Iteration:   2920, Loss function: 3.751, Average Loss: 4.232, avg. samples / sec: 59643.79
Iteration:   2920, Loss function: 3.945, Average Loss: 4.255, avg. samples / sec: 59605.65
Iteration:   2920, Loss function: 5.829, Average Loss: 4.222, avg. samples / sec: 59758.87
Iteration:   2920, Loss function: 5.021, Average Loss: 4.259, avg. samples / sec: 59622.17
Iteration:   2920, Loss function: 5.062, Average Loss: 4.224, avg. samples / sec: 59750.88
Iteration:   2920, Loss function: 4.833, Average Loss: 4.224, avg. samples / sec: 59527.17
Iteration:   2920, Loss function: 3.777, Average Loss: 4.232, avg. samples / sec: 59507.34
Iteration:   2920, Loss function: 4.214, Average Loss: 4.240, avg. samples / sec: 59497.11
Iteration:   2920, Loss function: 4.510, Average Loss: 4.258, avg. samples / sec: 59466.61
Iteration:   2920, Loss function: 4.099, Average Loss: 4.226, avg. samples / sec: 59483.80
:::MLL 1558639724.981 epoch_stop: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 819}}
:::MLL 1558639724.982 epoch_start: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 673}}
Iteration:   2940, Loss function: 3.942, Average Loss: 4.223, avg. samples / sec: 58465.88
Iteration:   2940, Loss function: 3.289, Average Loss: 4.212, avg. samples / sec: 58623.72
Iteration:   2940, Loss function: 3.672, Average Loss: 4.228, avg. samples / sec: 58550.07
Iteration:   2940, Loss function: 3.713, Average Loss: 4.229, avg. samples / sec: 58704.28
Iteration:   2940, Loss function: 4.460, Average Loss: 4.227, avg. samples / sec: 58511.13
Iteration:   2940, Loss function: 3.536, Average Loss: 4.223, avg. samples / sec: 58916.13
Iteration:   2940, Loss function: 3.087, Average Loss: 4.221, avg. samples / sec: 58626.62
Iteration:   2940, Loss function: 4.029, Average Loss: 4.256, avg. samples / sec: 58685.63
Iteration:   2940, Loss function: 4.654, Average Loss: 4.212, avg. samples / sec: 58447.65
Iteration:   2940, Loss function: 4.505, Average Loss: 4.229, avg. samples / sec: 58500.20
Iteration:   2940, Loss function: 4.081, Average Loss: 4.254, avg. samples / sec: 58548.83
Iteration:   2940, Loss function: 4.658, Average Loss: 4.215, avg. samples / sec: 58564.26
Iteration:   2940, Loss function: 5.378, Average Loss: 4.256, avg. samples / sec: 58236.70
Iteration:   2940, Loss function: 3.595, Average Loss: 4.251, avg. samples / sec: 58400.51
Iteration:   2940, Loss function: 3.652, Average Loss: 4.222, avg. samples / sec: 58508.24
Iteration:   2960, Loss function: 3.913, Average Loss: 4.218, avg. samples / sec: 58926.87
Iteration:   2960, Loss function: 3.733, Average Loss: 4.246, avg. samples / sec: 59008.73
Iteration:   2960, Loss function: 3.233, Average Loss: 4.213, avg. samples / sec: 59023.88
Iteration:   2960, Loss function: 3.829, Average Loss: 4.215, avg. samples / sec: 58821.70
Iteration:   2960, Loss function: 4.932, Average Loss: 4.207, avg. samples / sec: 58760.29
Iteration:   2960, Loss function: 3.732, Average Loss: 4.221, avg. samples / sec: 58712.50
Iteration:   2960, Loss function: 4.091, Average Loss: 4.225, avg. samples / sec: 58749.85
Iteration:   2960, Loss function: 4.475, Average Loss: 4.209, avg. samples / sec: 58702.35
Iteration:   2960, Loss function: 3.553, Average Loss: 4.209, avg. samples / sec: 58727.33
Iteration:   2960, Loss function: 4.177, Average Loss: 4.248, avg. samples / sec: 58667.46
Iteration:   2960, Loss function: 4.053, Average Loss: 4.224, avg. samples / sec: 58659.35
Iteration:   2960, Loss function: 3.200, Average Loss: 4.221, avg. samples / sec: 58584.29
Iteration:   2960, Loss function: 3.436, Average Loss: 4.222, avg. samples / sec: 58565.76
Iteration:   2960, Loss function: 4.188, Average Loss: 4.249, avg. samples / sec: 58638.70
Iteration:   2960, Loss function: 3.802, Average Loss: 4.253, avg. samples / sec: 58657.32
Iteration:   2980, Loss function: 3.646, Average Loss: 4.204, avg. samples / sec: 57317.39
Iteration:   2980, Loss function: 4.483, Average Loss: 4.208, avg. samples / sec: 57120.06
Iteration:   2980, Loss function: 3.735, Average Loss: 4.249, avg. samples / sec: 57393.23
Iteration:   2980, Loss function: 4.731, Average Loss: 4.244, avg. samples / sec: 57359.40
Iteration:   2980, Loss function: 4.721, Average Loss: 4.207, avg. samples / sec: 57128.21
Iteration:   2980, Loss function: 4.600, Average Loss: 4.203, avg. samples / sec: 57252.89
Iteration:   2980, Loss function: 4.147, Average Loss: 4.212, avg. samples / sec: 57006.13
Iteration:   2980, Loss function: 3.140, Average Loss: 4.214, avg. samples / sec: 57096.29
Iteration:   2980, Loss function: 4.250, Average Loss: 4.218, avg. samples / sec: 57127.03
Iteration:   2980, Loss function: 3.431, Average Loss: 4.244, avg. samples / sec: 57219.95
Iteration:   2980, Loss function: 4.562, Average Loss: 4.215, avg. samples / sec: 57263.03
Iteration:   2980, Loss function: 3.434, Average Loss: 4.243, avg. samples / sec: 56989.00
Iteration:   2980, Loss function: 4.835, Average Loss: 4.218, avg. samples / sec: 57121.66
Iteration:   2980, Loss function: 4.889, Average Loss: 4.222, avg. samples / sec: 57112.77
Iteration:   2980, Loss function: 3.262, Average Loss: 4.222, avg. samples / sec: 57164.29
Iteration:   3000, Loss function: 3.546, Average Loss: 4.248, avg. samples / sec: 58246.28
Iteration:   3000, Loss function: 4.646, Average Loss: 4.200, avg. samples / sec: 58178.44
Iteration:   3000, Loss function: 3.316, Average Loss: 4.236, avg. samples / sec: 58224.81
Iteration:   3000, Loss function: 4.003, Average Loss: 4.196, avg. samples / sec: 58148.77
Iteration:   3000, Loss function: 3.445, Average Loss: 4.217, avg. samples / sec: 58272.60
Iteration:   3000, Loss function: 3.899, Average Loss: 4.242, avg. samples / sec: 58067.53
Iteration:   3000, Loss function: 3.823, Average Loss: 4.203, avg. samples / sec: 58049.40
Iteration:   3000, Loss function: 3.587, Average Loss: 4.205, avg. samples / sec: 58131.53
Iteration:   3000, Loss function: 3.740, Average Loss: 4.240, avg. samples / sec: 58119.04
Iteration:   3000, Loss function: 3.112, Average Loss: 4.198, avg. samples / sec: 58002.19
Iteration:   3000, Loss function: 3.443, Average Loss: 4.207, avg. samples / sec: 58076.00
Iteration:   3000, Loss function: 4.244, Average Loss: 4.199, avg. samples / sec: 58056.45
Iteration:   3000, Loss function: 4.575, Average Loss: 4.217, avg. samples / sec: 58196.97
Iteration:   3000, Loss function: 3.979, Average Loss: 4.214, avg. samples / sec: 58020.50
Iteration:   3000, Loss function: 4.059, Average Loss: 4.213, avg. samples / sec: 57998.68
:::MLL 1558639727.005 epoch_stop: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 819}}
:::MLL 1558639727.005 epoch_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 673}}
Iteration:   3020, Loss function: 4.883, Average Loss: 4.209, avg. samples / sec: 59224.28
Iteration:   3020, Loss function: 3.818, Average Loss: 4.232, avg. samples / sec: 58963.65
Iteration:   3020, Loss function: 5.758, Average Loss: 4.200, avg. samples / sec: 58911.28
Iteration:   3020, Loss function: 3.353, Average Loss: 4.201, avg. samples / sec: 58968.02
Iteration:   3020, Loss function: 2.997, Average Loss: 4.231, avg. samples / sec: 58952.21
Iteration:   3020, Loss function: 3.540, Average Loss: 4.211, avg. samples / sec: 58969.33
Iteration:   3020, Loss function: 3.782, Average Loss: 4.192, avg. samples / sec: 58958.30
Iteration:   3020, Loss function: 3.621, Average Loss: 4.207, avg. samples / sec: 58882.35
Iteration:   3020, Loss function: 4.017, Average Loss: 4.242, avg. samples / sec: 58695.41
Iteration:   3020, Loss function: 3.426, Average Loss: 4.200, avg. samples / sec: 58872.39
Iteration:   3020, Loss function: 3.624, Average Loss: 4.190, avg. samples / sec: 58789.43
Iteration:   3020, Loss function: 3.641, Average Loss: 4.195, avg. samples / sec: 58829.97
Iteration:   3020, Loss function: 4.059, Average Loss: 4.207, avg. samples / sec: 58749.56
Iteration:   3020, Loss function: 4.128, Average Loss: 4.235, avg. samples / sec: 58696.22
Iteration:   3020, Loss function: 3.296, Average Loss: 4.215, avg. samples / sec: 58664.45
Iteration:   3040, Loss function: 3.339, Average Loss: 4.236, avg. samples / sec: 59598.44
Iteration:   3040, Loss function: 4.428, Average Loss: 4.203, avg. samples / sec: 59685.98
Iteration:   3040, Loss function: 3.443, Average Loss: 4.230, avg. samples / sec: 59718.15
Iteration:   3040, Loss function: 4.644, Average Loss: 4.198, avg. samples / sec: 59359.36
Iteration:   3040, Loss function: 3.653, Average Loss: 4.180, avg. samples / sec: 59526.52
Iteration:   3040, Loss function: 3.618, Average Loss: 4.226, avg. samples / sec: 59406.90
Iteration:   3040, Loss function: 4.484, Average Loss: 4.210, avg. samples / sec: 59408.15
Iteration:   3040, Loss function: 2.977, Average Loss: 4.192, avg. samples / sec: 59346.96
Iteration:   3040, Loss function: 4.794, Average Loss: 4.194, avg. samples / sec: 59514.38
Iteration:   3040, Loss function: 4.044, Average Loss: 4.204, avg. samples / sec: 59399.59
Iteration:   3040, Loss function: 3.460, Average Loss: 4.223, avg. samples / sec: 59272.73
Iteration:   3040, Loss function: 3.188, Average Loss: 4.196, avg. samples / sec: 59338.91
Iteration:   3040, Loss function: 3.697, Average Loss: 4.197, avg. samples / sec: 59385.77
Iteration:   3040, Loss function: 3.533, Average Loss: 4.204, avg. samples / sec: 59147.60
Iteration:   3040, Loss function: 3.951, Average Loss: 4.208, avg. samples / sec: 59594.18
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
:::MLL 1558639728.097 eval_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.60 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.60 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.60 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.60 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.60 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.60 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.60 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.60 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.60 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.60 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.60 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.60 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.60 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.60 s
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.60 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.56s)
DONE (t=0.57s)
DONE (t=0.57s)
DONE (t=0.57s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.61s)
DONE (t=3.21s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.17111
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.31512
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.16907
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.03894
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.18081
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.27788
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.18163
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.26608
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.28102
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.07586
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.30134
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.43331
Current AP: 0.17111 AP goal: 0.23000
:::MLL 1558639732.541 eval_accuracy: {"value": 0.17111060884733253, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 389}}
:::MLL 1558639732.541 eval_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 392}}
:::MLL 1558639732.551 block_stop: {"value": null, "metadata": {"first_epoch_num": 33, "file": "train.py", "lineno": 804}}
:::MLL 1558639732.552 block_start: {"value": null, "metadata": {"first_epoch_num": 44, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3060, Loss function: 4.581, Average Loss: 4.195, avg. samples / sec: 6677.44
Iteration:   3060, Loss function: 2.938, Average Loss: 4.201, avg. samples / sec: 6680.21
Iteration:   3060, Loss function: 4.117, Average Loss: 4.184, avg. samples / sec: 6677.48
Iteration:   3060, Loss function: 3.102, Average Loss: 4.226, avg. samples / sec: 6675.32
Iteration:   3060, Loss function: 2.511, Average Loss: 4.199, avg. samples / sec: 6677.38
Iteration:   3060, Loss function: 4.115, Average Loss: 4.190, avg. samples / sec: 6678.08
Iteration:   3060, Loss function: 3.439, Average Loss: 4.223, avg. samples / sec: 6676.65
Iteration:   3060, Loss function: 3.405, Average Loss: 4.206, avg. samples / sec: 6676.69
Iteration:   3060, Loss function: 3.485, Average Loss: 4.192, avg. samples / sec: 6675.88
Iteration:   3060, Loss function: 4.049, Average Loss: 4.171, avg. samples / sec: 6675.64
Iteration:   3060, Loss function: 5.891, Average Loss: 4.228, avg. samples / sec: 6674.80
Iteration:   3060, Loss function: 4.019, Average Loss: 4.218, avg. samples / sec: 6676.19
Iteration:   3060, Loss function: 4.208, Average Loss: 4.190, avg. samples / sec: 6675.22
Iteration:   3060, Loss function: 4.563, Average Loss: 4.194, avg. samples / sec: 6674.99
Iteration:   3060, Loss function: 3.873, Average Loss: 4.201, avg. samples / sec: 6674.96
:::MLL 1558639733.460 epoch_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 819}}
:::MLL 1558639733.461 epoch_start: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 673}}
Iteration:   3080, Loss function: 3.413, Average Loss: 4.173, avg. samples / sec: 58872.69
Iteration:   3080, Loss function: 2.675, Average Loss: 4.217, avg. samples / sec: 58844.57
Iteration:   3080, Loss function: 2.444, Average Loss: 4.218, avg. samples / sec: 58941.75
Iteration:   3080, Loss function: 4.396, Average Loss: 4.184, avg. samples / sec: 58881.96
Iteration:   3080, Loss function: 3.432, Average Loss: 4.189, avg. samples / sec: 58729.09
Iteration:   3080, Loss function: 3.879, Average Loss: 4.187, avg. samples / sec: 58809.16
Iteration:   3080, Loss function: 2.952, Average Loss: 4.179, avg. samples / sec: 58774.21
Iteration:   3080, Loss function: 4.967, Average Loss: 4.162, avg. samples / sec: 58813.99
Iteration:   3080, Loss function: 3.260, Average Loss: 4.210, avg. samples / sec: 58738.56
Iteration:   3080, Loss function: 3.283, Average Loss: 4.182, avg. samples / sec: 58868.33
Iteration:   3080, Loss function: 3.835, Average Loss: 4.180, avg. samples / sec: 58939.56
Iteration:   3080, Loss function: 3.253, Average Loss: 4.193, avg. samples / sec: 58976.90
Iteration:   3080, Loss function: 3.881, Average Loss: 4.193, avg. samples / sec: 58694.85
Iteration:   3080, Loss function: 2.712, Average Loss: 4.205, avg. samples / sec: 58747.06
Iteration:   3080, Loss function: 2.473, Average Loss: 4.183, avg. samples / sec: 58444.42
Iteration:   3100, Loss function: 4.309, Average Loss: 4.183, avg. samples / sec: 58874.63
Iteration:   3100, Loss function: 2.902, Average Loss: 4.180, avg. samples / sec: 58873.92
Iteration:   3100, Loss function: 3.544, Average Loss: 4.169, avg. samples / sec: 58843.04
Iteration:   3100, Loss function: 4.003, Average Loss: 4.183, avg. samples / sec: 58705.92
Iteration:   3100, Loss function: 3.855, Average Loss: 4.175, avg. samples / sec: 58665.65
Iteration:   3100, Loss function: 3.629, Average Loss: 4.147, avg. samples / sec: 58751.25
Iteration:   3100, Loss function: 4.000, Average Loss: 4.175, avg. samples / sec: 58645.14
Iteration:   3100, Loss function: 3.784, Average Loss: 4.201, avg. samples / sec: 58582.32
Iteration:   3100, Loss function: 2.645, Average Loss: 4.166, avg. samples / sec: 58500.49
Iteration:   3100, Loss function: 2.720, Average Loss: 4.167, avg. samples / sec: 58550.39
Iteration:   3100, Loss function: 3.450, Average Loss: 4.196, avg. samples / sec: 58541.48
Iteration:   3100, Loss function: 3.620, Average Loss: 4.206, avg. samples / sec: 58320.52
Iteration:   3100, Loss function: 3.191, Average Loss: 4.175, avg. samples / sec: 58419.59
Iteration:   3100, Loss function: 4.496, Average Loss: 4.172, avg. samples / sec: 58535.38
Iteration:   3100, Loss function: 5.318, Average Loss: 4.201, avg. samples / sec: 57337.77
Iteration:   3120, Loss function: 3.087, Average Loss: 4.195, avg. samples / sec: 55934.82
Iteration:   3120, Loss function: 3.057, Average Loss: 4.132, avg. samples / sec: 55856.45
Iteration:   3120, Loss function: 5.213, Average Loss: 4.171, avg. samples / sec: 55794.31
Iteration:   3120, Loss function: 3.374, Average Loss: 4.174, avg. samples / sec: 55809.58
Iteration:   3120, Loss function: 3.774, Average Loss: 4.153, avg. samples / sec: 55915.82
Iteration:   3120, Loss function: 4.396, Average Loss: 4.153, avg. samples / sec: 55769.38
Iteration:   3120, Loss function: 3.135, Average Loss: 4.160, avg. samples / sec: 55794.24
Iteration:   3120, Loss function: 4.077, Average Loss: 4.193, avg. samples / sec: 56083.80
Iteration:   3120, Loss function: 3.028, Average Loss: 4.161, avg. samples / sec: 56077.67
Iteration:   3120, Loss function: 2.817, Average Loss: 4.186, avg. samples / sec: 55965.43
Iteration:   3120, Loss function: 3.928, Average Loss: 4.196, avg. samples / sec: 57163.34
Iteration:   3120, Loss function: 3.195, Average Loss: 4.158, avg. samples / sec: 55761.64
Iteration:   3120, Loss function: 3.802, Average Loss: 4.153, avg. samples / sec: 55859.39
Iteration:   3120, Loss function: 2.867, Average Loss: 4.168, avg. samples / sec: 55626.67
Iteration:   3120, Loss function: 4.667, Average Loss: 4.163, avg. samples / sec: 55892.47
Iteration:   3140, Loss function: 5.105, Average Loss: 4.139, avg. samples / sec: 59653.03
Iteration:   3140, Loss function: 2.405, Average Loss: 4.150, avg. samples / sec: 59805.32
Iteration:   3140, Loss function: 3.006, Average Loss: 4.185, avg. samples / sec: 59721.36
Iteration:   3140, Loss function: 1.890, Average Loss: 4.160, avg. samples / sec: 59605.72
Iteration:   3140, Loss function: 3.926, Average Loss: 4.179, avg. samples / sec: 59520.63
Iteration:   3140, Loss function: 3.629, Average Loss: 4.137, avg. samples / sec: 59704.69
Iteration:   3140, Loss function: 3.717, Average Loss: 4.181, avg. samples / sec: 59589.67
Iteration:   3140, Loss function: 3.714, Average Loss: 4.144, avg. samples / sec: 59615.53
Iteration:   3140, Loss function: 4.411, Average Loss: 4.156, avg. samples / sec: 59513.52
Iteration:   3140, Loss function: 1.890, Average Loss: 4.150, avg. samples / sec: 59538.41
Iteration:   3140, Loss function: 4.338, Average Loss: 4.174, avg. samples / sec: 59562.54
Iteration:   3140, Loss function: 3.630, Average Loss: 4.122, avg. samples / sec: 59403.22
Iteration:   3140, Loss function: 3.205, Average Loss: 4.156, avg. samples / sec: 59756.64
Iteration:   3140, Loss function: 2.438, Average Loss: 4.141, avg. samples / sec: 59524.56
Iteration:   3140, Loss function: 3.954, Average Loss: 4.142, avg. samples / sec: 59289.31
:::MLL 1558639735.486 epoch_stop: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 819}}
:::MLL 1558639735.487 epoch_start: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 673}}
Iteration:   3160, Loss function: 3.157, Average Loss: 4.136, avg. samples / sec: 57980.16
Iteration:   3160, Loss function: 3.223, Average Loss: 4.161, avg. samples / sec: 58091.32
Iteration:   3160, Loss function: 4.963, Average Loss: 4.150, avg. samples / sec: 58027.17
Iteration:   3160, Loss function: 2.946, Average Loss: 4.132, avg. samples / sec: 57994.38
Iteration:   3160, Loss function: 2.567, Average Loss: 4.120, avg. samples / sec: 57888.04
Iteration:   3160, Loss function: 3.614, Average Loss: 4.163, avg. samples / sec: 57919.64
Iteration:   3160, Loss function: 3.678, Average Loss: 4.145, avg. samples / sec: 58034.53
Iteration:   3160, Loss function: 3.055, Average Loss: 4.166, avg. samples / sec: 57905.33
Iteration:   3160, Loss function: 5.108, Average Loss: 4.149, avg. samples / sec: 57834.78
Iteration:   3160, Loss function: 3.746, Average Loss: 4.110, avg. samples / sec: 57970.46
Iteration:   3160, Loss function: 3.483, Average Loss: 4.175, avg. samples / sec: 57767.41
Iteration:   3160, Loss function: 4.556, Average Loss: 4.137, avg. samples / sec: 57879.25
Iteration:   3160, Loss function: 3.864, Average Loss: 4.132, avg. samples / sec: 57959.39
Iteration:   3160, Loss function: 3.347, Average Loss: 4.129, avg. samples / sec: 57966.33
Iteration:   3160, Loss function: 3.461, Average Loss: 4.124, avg. samples / sec: 57666.43
Iteration:   3180, Loss function: 4.571, Average Loss: 4.134, avg. samples / sec: 58205.00
Iteration:   3180, Loss function: 3.803, Average Loss: 4.133, avg. samples / sec: 58229.57
Iteration:   3180, Loss function: 3.368, Average Loss: 4.123, avg. samples / sec: 58245.82
Iteration:   3180, Loss function: 5.031, Average Loss: 4.155, avg. samples / sec: 58168.53
Iteration:   3180, Loss function: 4.350, Average Loss: 4.111, avg. samples / sec: 58385.44
Iteration:   3180, Loss function: 3.877, Average Loss: 4.126, avg. samples / sec: 57941.81
Iteration:   3180, Loss function: 1.821, Average Loss: 4.097, avg. samples / sec: 58141.36
Iteration:   3180, Loss function: 3.589, Average Loss: 4.119, avg. samples / sec: 58164.06
Iteration:   3180, Loss function: 3.660, Average Loss: 4.136, avg. samples / sec: 57964.33
Iteration:   3180, Loss function: 3.560, Average Loss: 4.162, avg. samples / sec: 58109.93
Iteration:   3180, Loss function: 3.625, Average Loss: 4.114, avg. samples / sec: 58223.87
Iteration:   3180, Loss function: 1.977, Average Loss: 4.122, avg. samples / sec: 57932.09
Iteration:   3180, Loss function: 3.471, Average Loss: 4.149, avg. samples / sec: 57881.36
Iteration:   3180, Loss function: 3.302, Average Loss: 4.102, avg. samples / sec: 57920.57
Iteration:   3180, Loss function: 3.476, Average Loss: 4.159, avg. samples / sec: 57891.56
Iteration:   3200, Loss function: 3.025, Average Loss: 4.090, avg. samples / sec: 59252.94
Iteration:   3200, Loss function: 3.147, Average Loss: 4.104, avg. samples / sec: 59098.09
Iteration:   3200, Loss function: 3.610, Average Loss: 4.147, avg. samples / sec: 59247.76
Iteration:   3200, Loss function: 3.255, Average Loss: 4.111, avg. samples / sec: 58993.32
Iteration:   3200, Loss function: 3.725, Average Loss: 4.148, avg. samples / sec: 59063.96
Iteration:   3200, Loss function: 3.085, Average Loss: 4.120, avg. samples / sec: 58862.68
Iteration:   3200, Loss function: 3.979, Average Loss: 4.082, avg. samples / sec: 58961.85
Iteration:   3200, Loss function: 4.431, Average Loss: 4.095, avg. samples / sec: 58843.12
Iteration:   3200, Loss function: 3.498, Average Loss: 4.109, avg. samples / sec: 58997.52
Iteration:   3200, Loss function: 4.299, Average Loss: 4.103, avg. samples / sec: 58942.07
Iteration:   3200, Loss function: 2.896, Average Loss: 4.125, avg. samples / sec: 58896.33
Iteration:   3200, Loss function: 4.268, Average Loss: 4.117, avg. samples / sec: 58809.40
Iteration:   3200, Loss function: 2.811, Average Loss: 4.118, avg. samples / sec: 58687.17
Iteration:   3200, Loss function: 4.232, Average Loss: 4.140, avg. samples / sec: 58913.96
Iteration:   3200, Loss function: 3.973, Average Loss: 4.144, avg. samples / sec: 58674.95
:::MLL 1558639737.515 epoch_stop: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 819}}
:::MLL 1558639737.515 epoch_start: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 673}}
Iteration:   3220, Loss function: 3.017, Average Loss: 4.107, avg. samples / sec: 57295.25
Iteration:   3220, Loss function: 3.129, Average Loss: 4.136, avg. samples / sec: 57501.35
Iteration:   3220, Loss function: 2.760, Average Loss: 4.116, avg. samples / sec: 57289.82
Iteration:   3220, Loss function: 3.373, Average Loss: 4.094, avg. samples / sec: 57260.50
Iteration:   3220, Loss function: 3.565, Average Loss: 4.077, avg. samples / sec: 57000.34
Iteration:   3220, Loss function: 3.867, Average Loss: 4.133, avg. samples / sec: 57109.71
Iteration:   3220, Loss function: 2.594, Average Loss: 4.137, avg. samples / sec: 57046.07
Iteration:   3220, Loss function: 2.548, Average Loss: 4.068, avg. samples / sec: 57135.37
Iteration:   3220, Loss function: 4.270, Average Loss: 4.101, avg. samples / sec: 57026.59
Iteration:   3220, Loss function: 4.345, Average Loss: 4.088, avg. samples / sec: 56985.54
Iteration:   3220, Loss function: 4.280, Average Loss: 4.111, avg. samples / sec: 57229.41
Iteration:   3220, Loss function: 4.093, Average Loss: 4.105, avg. samples / sec: 57176.33
Iteration:   3220, Loss function: 3.262, Average Loss: 4.093, avg. samples / sec: 57090.19
Iteration:   3220, Loss function: 3.502, Average Loss: 4.086, avg. samples / sec: 57010.60
Iteration:   3220, Loss function: 2.831, Average Loss: 4.130, avg. samples / sec: 57093.31
Iteration:   3240, Loss function: 3.595, Average Loss: 4.088, avg. samples / sec: 58035.37
Iteration:   3240, Loss function: 3.721, Average Loss: 4.099, avg. samples / sec: 58100.13
Iteration:   3240, Loss function: 2.319, Average Loss: 4.074, avg. samples / sec: 58061.00
Iteration:   3240, Loss function: 3.488, Average Loss: 4.068, avg. samples / sec: 57984.12
Iteration:   3240, Loss function: 3.406, Average Loss: 4.107, avg. samples / sec: 57912.21
Iteration:   3240, Loss function: 3.695, Average Loss: 4.095, avg. samples / sec: 57676.24
Iteration:   3240, Loss function: 4.231, Average Loss: 4.097, avg. samples / sec: 57960.85
Iteration:   3240, Loss function: 3.308, Average Loss: 4.118, avg. samples / sec: 57814.61
Iteration:   3240, Loss function: 2.937, Average Loss: 4.111, avg. samples / sec: 58003.36
Iteration:   3240, Loss function: 3.462, Average Loss: 4.127, avg. samples / sec: 57635.79
Iteration:   3240, Loss function: 2.967, Average Loss: 4.082, avg. samples / sec: 57893.06
Iteration:   3240, Loss function: 4.048, Average Loss: 4.055, avg. samples / sec: 57778.06
Iteration:   3240, Loss function: 2.940, Average Loss: 4.075, avg. samples / sec: 57961.06
Iteration:   3240, Loss function: 3.371, Average Loss: 4.091, avg. samples / sec: 57756.94
Iteration:   3240, Loss function: 3.382, Average Loss: 4.125, avg. samples / sec: 57551.74
Iteration:   3260, Loss function: 3.514, Average Loss: 4.068, avg. samples / sec: 58602.56
Iteration:   3260, Loss function: 3.903, Average Loss: 4.115, avg. samples / sec: 58563.48
Iteration:   3260, Loss function: 4.376, Average Loss: 4.086, avg. samples / sec: 58456.91
Iteration:   3260, Loss function: 3.086, Average Loss: 4.063, avg. samples / sec: 58269.69
Iteration:   3260, Loss function: 3.913, Average Loss: 4.059, avg. samples / sec: 58265.52
Iteration:   3260, Loss function: 4.744, Average Loss: 4.076, avg. samples / sec: 58203.43
Iteration:   3260, Loss function: 2.978, Average Loss: 4.105, avg. samples / sec: 58389.24
Iteration:   3260, Loss function: 2.882, Average Loss: 4.079, avg. samples / sec: 58308.69
Iteration:   3260, Loss function: 3.917, Average Loss: 4.065, avg. samples / sec: 58403.44
Iteration:   3260, Loss function: 3.270, Average Loss: 4.118, avg. samples / sec: 58590.79
Iteration:   3260, Loss function: 3.238, Average Loss: 4.041, avg. samples / sec: 58363.22
Iteration:   3260, Loss function: 3.373, Average Loss: 4.103, avg. samples / sec: 58334.47
Iteration:   3260, Loss function: 3.199, Average Loss: 4.082, avg. samples / sec: 58375.14
Iteration:   3260, Loss function: 2.866, Average Loss: 4.099, avg. samples / sec: 58152.25
Iteration:   3260, Loss function: 3.055, Average Loss: 4.090, avg. samples / sec: 57863.85
Iteration:   3280, Loss function: 4.304, Average Loss: 4.058, avg. samples / sec: 57414.93
Iteration:   3280, Loss function: 4.869, Average Loss: 4.091, avg. samples / sec: 57462.90
Iteration:   3280, Loss function: 3.102, Average Loss: 4.097, avg. samples / sec: 57394.19
Iteration:   3280, Loss function: 5.615, Average Loss: 4.075, avg. samples / sec: 57202.79
Iteration:   3280, Loss function: 3.650, Average Loss: 4.104, avg. samples / sec: 57346.31
Iteration:   3280, Loss function: 4.299, Average Loss: 4.071, avg. samples / sec: 57383.95
Iteration:   3280, Loss function: 2.560, Average Loss: 4.095, avg. samples / sec: 57262.87
Iteration:   3280, Loss function: 4.107, Average Loss: 4.064, avg. samples / sec: 57172.34
Iteration:   3280, Loss function: 3.361, Average Loss: 4.051, avg. samples / sec: 57125.57
Iteration:   3280, Loss function: 4.268, Average Loss: 4.032, avg. samples / sec: 57257.38
Iteration:   3280, Loss function: 2.934, Average Loss: 4.103, avg. samples / sec: 57037.09
Iteration:   3280, Loss function: 3.378, Average Loss: 4.055, avg. samples / sec: 56955.63
Iteration:   3280, Loss function: 3.058, Average Loss: 4.040, avg. samples / sec: 57063.93
Iteration:   3280, Loss function: 2.585, Average Loss: 4.077, avg. samples / sec: 57411.58
Iteration:   3280, Loss function: 3.048, Average Loss: 4.062, avg. samples / sec: 57088.55
:::MLL 1558639739.541 epoch_stop: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 819}}
:::MLL 1558639739.542 epoch_start: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 673}}
Iteration:   3300, Loss function: 4.116, Average Loss: 4.062, avg. samples / sec: 57716.28
Iteration:   3300, Loss function: 3.615, Average Loss: 4.063, avg. samples / sec: 57615.06
Iteration:   3300, Loss function: 2.478, Average Loss: 4.042, avg. samples / sec: 57519.35
Iteration:   3300, Loss function: 2.784, Average Loss: 4.045, avg. samples / sec: 57763.83
Iteration:   3300, Loss function: 4.672, Average Loss: 4.087, avg. samples / sec: 57618.49
Iteration:   3300, Loss function: 4.947, Average Loss: 4.102, avg. samples / sec: 57691.07
Iteration:   3300, Loss function: 3.615, Average Loss: 4.038, avg. samples / sec: 57658.40
Iteration:   3300, Loss function: 2.461, Average Loss: 4.093, avg. samples / sec: 57542.08
Iteration:   3300, Loss function: 2.858, Average Loss: 4.060, avg. samples / sec: 57712.50
Iteration:   3300, Loss function: 2.543, Average Loss: 4.020, avg. samples / sec: 57635.72
Iteration:   3300, Loss function: 3.009, Average Loss: 4.063, avg. samples / sec: 57519.02
Iteration:   3300, Loss function: 3.370, Average Loss: 4.056, avg. samples / sec: 57761.94
Iteration:   3300, Loss function: 4.146, Average Loss: 4.026, avg. samples / sec: 57683.00
Iteration:   3300, Loss function: 2.579, Average Loss: 4.081, avg. samples / sec: 57405.41
Iteration:   3300, Loss function: 4.022, Average Loss: 4.086, avg. samples / sec: 57401.04
Iteration:   3320, Loss function: 3.008, Average Loss: 4.048, avg. samples / sec: 57618.40
Iteration:   3320, Loss function: 2.434, Average Loss: 4.028, avg. samples / sec: 57643.95
Iteration:   3320, Loss function: 3.394, Average Loss: 4.053, avg. samples / sec: 57591.68
Iteration:   3320, Loss function: 3.719, Average Loss: 4.032, avg. samples / sec: 57605.75
Iteration:   3320, Loss function: 3.256, Average Loss: 4.073, avg. samples / sec: 57756.02
Iteration:   3320, Loss function: 3.786, Average Loss: 4.076, avg. samples / sec: 57582.14
Iteration:   3320, Loss function: 3.031, Average Loss: 4.046, avg. samples / sec: 57649.09
Iteration:   3320, Loss function: 3.859, Average Loss: 4.071, avg. samples / sec: 57675.13
Iteration:   3320, Loss function: 2.361, Average Loss: 4.084, avg. samples / sec: 57598.95
Iteration:   3320, Loss function: 3.458, Average Loss: 4.042, avg. samples / sec: 57601.07
Iteration:   3320, Loss function: 4.002, Average Loss: 4.013, avg. samples / sec: 57564.25
Iteration:   3320, Loss function: 3.548, Average Loss: 4.090, avg. samples / sec: 57477.50
Iteration:   3320, Loss function: 2.805, Average Loss: 4.047, avg. samples / sec: 57505.52
Iteration:   3320, Loss function: 3.369, Average Loss: 4.026, avg. samples / sec: 57357.28
Iteration:   3320, Loss function: 3.689, Average Loss: 4.013, avg. samples / sec: 57434.30
Iteration:   3340, Loss function: 3.499, Average Loss: 4.038, avg. samples / sec: 59955.26
Iteration:   3340, Loss function: 4.268, Average Loss: 4.064, avg. samples / sec: 60068.24
Iteration:   3340, Loss function: 3.398, Average Loss: 4.076, avg. samples / sec: 60142.89
Iteration:   3340, Loss function: 3.694, Average Loss: 4.035, avg. samples / sec: 59993.57
Iteration:   3340, Loss function: 3.579, Average Loss: 4.059, avg. samples / sec: 59943.71
Iteration:   3340, Loss function: 2.344, Average Loss: 4.032, avg. samples / sec: 59994.67
Iteration:   3340, Loss function: 4.229, Average Loss: 4.014, avg. samples / sec: 60153.51
Iteration:   3340, Loss function: 3.729, Average Loss: 4.018, avg. samples / sec: 59878.18
Iteration:   3340, Loss function: 3.782, Average Loss: 4.041, avg. samples / sec: 59854.63
Iteration:   3340, Loss function: 3.780, Average Loss: 4.002, avg. samples / sec: 59967.22
Iteration:   3340, Loss function: 3.357, Average Loss: 4.001, avg. samples / sec: 60030.68
Iteration:   3340, Loss function: 3.442, Average Loss: 4.029, avg. samples / sec: 59913.84
Iteration:   3340, Loss function: 3.433, Average Loss: 4.020, avg. samples / sec: 59740.22
Iteration:   3340, Loss function: 4.591, Average Loss: 4.066, avg. samples / sec: 59715.89
Iteration:   3340, Loss function: 1.649, Average Loss: 4.072, avg. samples / sec: 59763.20
:::MLL 1558639741.555 epoch_stop: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 819}}
:::MLL 1558639741.556 epoch_start: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 673}}
Iteration:   3360, Loss function: 3.188, Average Loss: 4.020, avg. samples / sec: 59096.78
Iteration:   3360, Loss function: 3.542, Average Loss: 4.004, avg. samples / sec: 59249.75
Iteration:   3360, Loss function: 2.783, Average Loss: 4.049, avg. samples / sec: 59271.93
Iteration:   3360, Loss function: 3.472, Average Loss: 4.055, avg. samples / sec: 58925.88
Iteration:   3360, Loss function: 2.816, Average Loss: 4.026, avg. samples / sec: 58898.33
Iteration:   3360, Loss function: 4.083, Average Loss: 4.004, avg. samples / sec: 59032.79
Iteration:   3360, Loss function: 3.787, Average Loss: 4.005, avg. samples / sec: 59021.14
Iteration:   3360, Loss function: 2.657, Average Loss: 4.020, avg. samples / sec: 58951.32
Iteration:   3360, Loss function: 2.255, Average Loss: 4.034, avg. samples / sec: 59002.68
Iteration:   3360, Loss function: 4.161, Average Loss: 4.063, avg. samples / sec: 58891.85
Iteration:   3360, Loss function: 2.719, Average Loss: 3.992, avg. samples / sec: 58986.41
Iteration:   3360, Loss function: 3.988, Average Loss: 4.055, avg. samples / sec: 58893.97
Iteration:   3360, Loss function: 2.992, Average Loss: 3.993, avg. samples / sec: 59039.19
Iteration:   3360, Loss function: 4.141, Average Loss: 4.062, avg. samples / sec: 59085.28
Iteration:   3360, Loss function: 3.310, Average Loss: 4.016, avg. samples / sec: 59003.87
Iteration:   3380, Loss function: 4.160, Average Loss: 4.016, avg. samples / sec: 57490.04
Iteration:   3380, Loss function: 3.446, Average Loss: 4.038, avg. samples / sec: 57373.58
Iteration:   3380, Loss function: 3.638, Average Loss: 4.009, avg. samples / sec: 57421.31
Iteration:   3380, Loss function: 4.400, Average Loss: 3.994, avg. samples / sec: 57370.47
Iteration:   3380, Loss function: 3.220, Average Loss: 4.004, avg. samples / sec: 57525.38
Iteration:   3380, Loss function: 3.117, Average Loss: 4.018, avg. samples / sec: 57382.95
Iteration:   3380, Loss function: 3.555, Average Loss: 3.985, avg. samples / sec: 57410.48
Iteration:   3380, Loss function: 4.177, Average Loss: 3.995, avg. samples / sec: 57284.03
Iteration:   3380, Loss function: 3.367, Average Loss: 3.983, avg. samples / sec: 57396.08
Iteration:   3380, Loss function: 3.496, Average Loss: 4.048, avg. samples / sec: 57320.65
Iteration:   3380, Loss function: 2.693, Average Loss: 3.991, avg. samples / sec: 57190.02
Iteration:   3380, Loss function: 3.533, Average Loss: 4.044, avg. samples / sec: 57358.07
Iteration:   3380, Loss function: 3.160, Average Loss: 4.041, avg. samples / sec: 57191.46
Iteration:   3380, Loss function: 3.114, Average Loss: 4.013, avg. samples / sec: 57116.73
Iteration:   3380, Loss function: 4.416, Average Loss: 4.048, avg. samples / sec: 57211.26
Iteration:   3400, Loss function: 3.647, Average Loss: 3.980, avg. samples / sec: 56970.99
Iteration:   3400, Loss function: 3.107, Average Loss: 3.988, avg. samples / sec: 56920.94
Iteration:   3400, Loss function: 3.022, Average Loss: 4.007, avg. samples / sec: 56905.40
Iteration:   3400, Loss function: 3.982, Average Loss: 3.983, avg. samples / sec: 56862.85
Iteration:   3400, Loss function: 3.336, Average Loss: 4.028, avg. samples / sec: 56995.48
Iteration:   3400, Loss function: 4.021, Average Loss: 4.010, avg. samples / sec: 56710.96
Iteration:   3400, Loss function: 4.121, Average Loss: 4.000, avg. samples / sec: 56995.04
Iteration:   3400, Loss function: 3.130, Average Loss: 3.988, avg. samples / sec: 56896.72
Iteration:   3400, Loss function: 4.461, Average Loss: 3.972, avg. samples / sec: 56907.79
Iteration:   3400, Loss function: 3.962, Average Loss: 4.036, avg. samples / sec: 57101.71
Iteration:   3400, Loss function: 4.468, Average Loss: 4.030, avg. samples / sec: 56748.89
Iteration:   3400, Loss function: 2.116, Average Loss: 3.983, avg. samples / sec: 56897.57
Iteration:   3400, Loss function: 2.639, Average Loss: 3.997, avg. samples / sec: 56732.86
Iteration:   3400, Loss function: 4.452, Average Loss: 4.035, avg. samples / sec: 56850.42
Iteration:   3400, Loss function: 3.997, Average Loss: 4.037, avg. samples / sec: 56774.25
Iteration:   3420, Loss function: 2.625, Average Loss: 3.966, avg. samples / sec: 59314.51
Iteration:   3420, Loss function: 3.578, Average Loss: 4.002, avg. samples / sec: 59350.23
Iteration:   3420, Loss function: 3.867, Average Loss: 3.968, avg. samples / sec: 59376.89
Iteration:   3420, Loss function: 3.059, Average Loss: 3.974, avg. samples / sec: 59345.86
Iteration:   3420, Loss function: 3.381, Average Loss: 4.019, avg. samples / sec: 59321.40
Iteration:   3420, Loss function: 3.908, Average Loss: 4.017, avg. samples / sec: 59247.14
Iteration:   3420, Loss function: 2.518, Average Loss: 3.984, avg. samples / sec: 59279.41
Iteration:   3420, Loss function: 2.771, Average Loss: 3.990, avg. samples / sec: 59222.16
Iteration:   3420, Loss function: 3.499, Average Loss: 4.018, avg. samples / sec: 59244.32
Iteration:   3420, Loss function: 3.549, Average Loss: 3.976, avg. samples / sec: 59119.96
Iteration:   3420, Loss function: 2.693, Average Loss: 3.997, avg. samples / sec: 59090.01
Iteration:   3420, Loss function: 3.635, Average Loss: 4.022, avg. samples / sec: 59227.09
Iteration:   3420, Loss function: 2.686, Average Loss: 3.974, avg. samples / sec: 59102.20
Iteration:   3420, Loss function: 4.220, Average Loss: 3.973, avg. samples / sec: 58993.79
Iteration:   3420, Loss function: 3.056, Average Loss: 4.021, avg. samples / sec: 59134.29
:::MLL 1558639743.563 eval_start: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.56s)
DONE (t=2.80s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22365
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.38326
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.22902
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05498
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.23562
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.36155
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.21861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.31830
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33404
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09601
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36075
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.52188
Current AP: 0.22365 AP goal: 0.23000
:::MLL 1558639747.523 eval_accuracy: {"value": 0.22364876784661417, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 389}}
:::MLL 1558639747.564 eval_stop: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 392}}
:::MLL 1558639747.574 block_stop: {"value": null, "metadata": {"first_epoch_num": 44, "file": "train.py", "lineno": 804}}
:::MLL 1558639747.575 block_start: {"value": null, "metadata": {"first_epoch_num": 49, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
:::MLL 1558639747.601 epoch_stop: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 819}}
:::MLL 1558639747.602 epoch_start: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 673}}
Iteration:   3440, Loss function: 4.034, Average Loss: 3.966, avg. samples / sec: 7321.51
Iteration:   3440, Loss function: 3.616, Average Loss: 3.957, avg. samples / sec: 7318.92
Iteration:   3440, Loss function: 3.652, Average Loss: 4.018, avg. samples / sec: 7325.38
Iteration:   3440, Loss function: 3.554, Average Loss: 4.006, avg. samples / sec: 7321.16
Iteration:   3440, Loss function: 2.719, Average Loss: 3.953, avg. samples / sec: 7317.69
Iteration:   3440, Loss function: 4.446, Average Loss: 3.960, avg. samples / sec: 7318.60
Iteration:   3440, Loss function: 2.308, Average Loss: 4.005, avg. samples / sec: 7321.23
Iteration:   3440, Loss function: 4.267, Average Loss: 3.991, avg. samples / sec: 7317.45
Iteration:   3440, Loss function: 2.477, Average Loss: 3.960, avg. samples / sec: 7322.00
Iteration:   3440, Loss function: 3.241, Average Loss: 4.005, avg. samples / sec: 7318.07
Iteration:   3440, Loss function: 3.219, Average Loss: 4.010, avg. samples / sec: 7317.20
Iteration:   3440, Loss function: 2.373, Average Loss: 3.979, avg. samples / sec: 7319.55
Iteration:   3440, Loss function: 3.637, Average Loss: 3.967, avg. samples / sec: 7321.25
Iteration:   3440, Loss function: 4.802, Average Loss: 3.977, avg. samples / sec: 7317.92
Iteration:   3440, Loss function: 4.016, Average Loss: 3.970, avg. samples / sec: 7316.90
Iteration:   3460, Loss function: 2.654, Average Loss: 3.993, avg. samples / sec: 59632.08
Iteration:   3460, Loss function: 3.811, Average Loss: 3.968, avg. samples / sec: 59636.40
Iteration:   3460, Loss function: 4.428, Average Loss: 3.946, avg. samples / sec: 59470.58
Iteration:   3460, Loss function: 3.257, Average Loss: 3.998, avg. samples / sec: 59524.88
Iteration:   3460, Loss function: 2.453, Average Loss: 4.008, avg. samples / sec: 59218.98
Iteration:   3460, Loss function: 3.777, Average Loss: 3.971, avg. samples / sec: 59389.98
Iteration:   3460, Loss function: 3.752, Average Loss: 3.940, avg. samples / sec: 59216.76
Iteration:   3460, Loss function: 3.332, Average Loss: 3.956, avg. samples / sec: 59187.67
Iteration:   3460, Loss function: 3.296, Average Loss: 3.957, avg. samples / sec: 59126.43
Iteration:   3460, Loss function: 2.681, Average Loss: 3.999, avg. samples / sec: 59141.09
Iteration:   3460, Loss function: 4.692, Average Loss: 3.957, avg. samples / sec: 59314.89
Iteration:   3460, Loss function: 4.424, Average Loss: 3.980, avg. samples / sec: 59214.45
Iteration:   3460, Loss function: 4.327, Average Loss: 3.961, avg. samples / sec: 59322.00
Iteration:   3460, Loss function: 3.832, Average Loss: 3.994, avg. samples / sec: 59116.93
Iteration:   3460, Loss function: 4.316, Average Loss: 3.947, avg. samples / sec: 59041.12
Iteration:   3480, Loss function: 3.705, Average Loss: 3.943, avg. samples / sec: 55498.13
Iteration:   3480, Loss function: 3.305, Average Loss: 3.984, avg. samples / sec: 55369.89
Iteration:   3480, Loss function: 2.722, Average Loss: 3.946, avg. samples / sec: 55658.81
Iteration:   3480, Loss function: 3.477, Average Loss: 3.929, avg. samples / sec: 55596.48
Iteration:   3480, Loss function: 3.102, Average Loss: 3.954, avg. samples / sec: 55688.09
Iteration:   3480, Loss function: 3.548, Average Loss: 3.986, avg. samples / sec: 55388.80
Iteration:   3480, Loss function: 4.162, Average Loss: 3.963, avg. samples / sec: 55496.18
Iteration:   3480, Loss function: 2.873, Average Loss: 3.973, avg. samples / sec: 55570.10
Iteration:   3480, Loss function: 3.492, Average Loss: 3.943, avg. samples / sec: 55544.37
Iteration:   3480, Loss function: 3.365, Average Loss: 3.986, avg. samples / sec: 55632.07
Iteration:   3480, Loss function: 2.906, Average Loss: 3.950, avg. samples / sec: 55521.50
Iteration:   3480, Loss function: 2.506, Average Loss: 3.949, avg. samples / sec: 55264.97
Iteration:   3480, Loss function: 3.944, Average Loss: 3.997, avg. samples / sec: 55436.08
Iteration:   3480, Loss function: 2.965, Average Loss: 3.991, avg. samples / sec: 55479.16
Iteration:   3480, Loss function: 2.786, Average Loss: 3.936, avg. samples / sec: 55440.93
:::MLL 1558639749.640 epoch_stop: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 819}}
:::MLL 1558639749.641 epoch_start: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 673}}
Iteration:   3500, Loss function: 3.151, Average Loss: 3.990, avg. samples / sec: 58448.42
Iteration:   3500, Loss function: 2.768, Average Loss: 3.965, avg. samples / sec: 58358.19
Iteration:   3500, Loss function: 4.259, Average Loss: 3.933, avg. samples / sec: 58346.21
Iteration:   3500, Loss function: 3.038, Average Loss: 3.945, avg. samples / sec: 58271.69
Iteration:   3500, Loss function: 2.863, Average Loss: 3.974, avg. samples / sec: 58138.72
Iteration:   3500, Loss function: 3.635, Average Loss: 3.934, avg. samples / sec: 58175.49
Iteration:   3500, Loss function: 2.734, Average Loss: 3.973, avg. samples / sec: 58242.23
Iteration:   3500, Loss function: 3.582, Average Loss: 3.938, avg. samples / sec: 58235.33
Iteration:   3500, Loss function: 3.090, Average Loss: 3.980, avg. samples / sec: 58274.24
Iteration:   3500, Loss function: 3.267, Average Loss: 3.920, avg. samples / sec: 58092.90
Iteration:   3500, Loss function: 3.737, Average Loss: 3.940, avg. samples / sec: 58147.12
Iteration:   3500, Loss function: 4.389, Average Loss: 3.979, avg. samples / sec: 58091.75
Iteration:   3500, Loss function: 2.728, Average Loss: 3.927, avg. samples / sec: 58253.43
Iteration:   3500, Loss function: 3.160, Average Loss: 3.950, avg. samples / sec: 57946.05
Iteration:   3500, Loss function: 3.786, Average Loss: 3.933, avg. samples / sec: 57783.75
Iteration:   3520, Loss function: 2.863, Average Loss: 3.931, avg. samples / sec: 59591.69
Iteration:   3520, Loss function: 2.296, Average Loss: 3.941, avg. samples / sec: 59806.64
Iteration:   3520, Loss function: 3.567, Average Loss: 3.930, avg. samples / sec: 59639.42
Iteration:   3520, Loss function: 2.673, Average Loss: 3.923, avg. samples / sec: 59805.63
Iteration:   3520, Loss function: 3.914, Average Loss: 3.976, avg. samples / sec: 59315.79
Iteration:   3520, Loss function: 2.783, Average Loss: 3.926, avg. samples / sec: 59427.47
Iteration:   3520, Loss function: 3.586, Average Loss: 3.942, avg. samples / sec: 59332.32
Iteration:   3520, Loss function: 3.426, Average Loss: 3.917, avg. samples / sec: 59636.75
Iteration:   3520, Loss function: 2.707, Average Loss: 3.951, avg. samples / sec: 59292.33
Iteration:   3520, Loss function: 3.502, Average Loss: 3.963, avg. samples / sec: 59382.14
Iteration:   3520, Loss function: 3.738, Average Loss: 3.913, avg. samples / sec: 59442.98
Iteration:   3520, Loss function: 2.658, Average Loss: 3.968, avg. samples / sec: 59447.80
Iteration:   3520, Loss function: 2.880, Average Loss: 3.967, avg. samples / sec: 59253.51
Iteration:   3520, Loss function: 3.726, Average Loss: 3.929, avg. samples / sec: 59082.68
Iteration:   3520, Loss function: 2.510, Average Loss: 3.960, avg. samples / sec: 58289.76
Iteration:   3540, Loss function: 2.835, Average Loss: 3.919, avg. samples / sec: 57567.59
Iteration:   3540, Loss function: 3.558, Average Loss: 3.912, avg. samples / sec: 57539.12
Iteration:   3540, Loss function: 3.281, Average Loss: 3.962, avg. samples / sec: 57788.39
Iteration:   3540, Loss function: 2.304, Average Loss: 3.953, avg. samples / sec: 58635.58
Iteration:   3540, Loss function: 3.220, Average Loss: 3.905, avg. samples / sec: 57566.20
Iteration:   3540, Loss function: 3.336, Average Loss: 3.939, avg. samples / sec: 57534.99
Iteration:   3540, Loss function: 3.382, Average Loss: 3.956, avg. samples / sec: 57652.04
Iteration:   3540, Loss function: 3.272, Average Loss: 3.950, avg. samples / sec: 57489.24
Iteration:   3540, Loss function: 3.202, Average Loss: 3.932, avg. samples / sec: 57453.99
Iteration:   3540, Loss function: 3.182, Average Loss: 3.934, avg. samples / sec: 57374.51
Iteration:   3540, Loss function: 2.202, Average Loss: 3.964, avg. samples / sec: 57402.30
Iteration:   3540, Loss function: 3.330, Average Loss: 3.916, avg. samples / sec: 57320.14
Iteration:   3540, Loss function: 3.349, Average Loss: 3.914, avg. samples / sec: 57371.69
Iteration:   3540, Loss function: 2.998, Average Loss: 3.904, avg. samples / sec: 57327.55
Iteration:   3540, Loss function: 2.128, Average Loss: 3.919, avg. samples / sec: 57486.76
:::MLL 1558639751.361 epoch_stop: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 819}}
:::MLL 1558639751.361 epoch_start: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 673}}
Iteration:   3560, Loss function: 3.168, Average Loss: 3.930, avg. samples / sec: 59010.42
Iteration:   3560, Loss function: 2.748, Average Loss: 3.911, avg. samples / sec: 58799.39
Iteration:   3560, Loss function: 2.827, Average Loss: 3.923, avg. samples / sec: 58989.10
Iteration:   3560, Loss function: 3.345, Average Loss: 3.942, avg. samples / sec: 58947.08
Iteration:   3560, Loss function: 3.043, Average Loss: 3.947, avg. samples / sec: 58944.86
Iteration:   3560, Loss function: 3.108, Average Loss: 3.895, avg. samples / sec: 59093.43
Iteration:   3560, Loss function: 3.259, Average Loss: 3.936, avg. samples / sec: 58757.86
Iteration:   3560, Loss function: 3.803, Average Loss: 3.909, avg. samples / sec: 58911.28
Iteration:   3560, Loss function: 2.229, Average Loss: 3.941, avg. samples / sec: 58815.10
Iteration:   3560, Loss function: 5.735, Average Loss: 3.948, avg. samples / sec: 58704.11
Iteration:   3560, Loss function: 2.789, Average Loss: 3.899, avg. samples / sec: 58628.40
Iteration:   3560, Loss function: 3.380, Average Loss: 3.894, avg. samples / sec: 58657.10
Iteration:   3560, Loss function: 1.993, Average Loss: 3.910, avg. samples / sec: 58936.90
Iteration:   3560, Loss function: 2.562, Average Loss: 3.917, avg. samples / sec: 58728.58
Iteration:   3560, Loss function: 3.635, Average Loss: 3.898, avg. samples / sec: 58682.80
Iteration:   3580, Loss function: 2.885, Average Loss: 3.898, avg. samples / sec: 59085.43
Iteration:   3580, Loss function: 4.191, Average Loss: 3.930, avg. samples / sec: 59062.42
Iteration:   3580, Loss function: 3.239, Average Loss: 3.888, avg. samples / sec: 59122.06
Iteration:   3580, Loss function: 4.387, Average Loss: 3.892, avg. samples / sec: 59109.34
Iteration:   3580, Loss function: 3.440, Average Loss: 3.913, avg. samples / sec: 58853.02
Iteration:   3580, Loss function: 3.314, Average Loss: 3.905, avg. samples / sec: 59065.62
Iteration:   3580, Loss function: 5.303, Average Loss: 3.903, avg. samples / sec: 59050.15
Iteration:   3580, Loss function: 3.370, Average Loss: 3.928, avg. samples / sec: 58890.33
Iteration:   3580, Loss function: 3.688, Average Loss: 3.890, avg. samples / sec: 59128.22
Iteration:   3580, Loss function: 4.170, Average Loss: 3.942, avg. samples / sec: 58809.33
Iteration:   3580, Loss function: 4.132, Average Loss: 3.938, avg. samples / sec: 58830.88
Iteration:   3580, Loss function: 3.758, Average Loss: 3.882, avg. samples / sec: 58667.16
Iteration:   3580, Loss function: 3.107, Average Loss: 3.923, avg. samples / sec: 58614.21
Iteration:   3580, Loss function: 4.695, Average Loss: 3.920, avg. samples / sec: 58489.59
Iteration:   3580, Loss function: 4.027, Average Loss: 3.902, avg. samples / sec: 58636.21
Iteration:   3600, Loss function: 3.020, Average Loss: 3.904, avg. samples / sec: 59815.10
Iteration:   3600, Loss function: 2.955, Average Loss: 3.928, avg. samples / sec: 59790.33
Iteration:   3600, Loss function: 3.464, Average Loss: 3.896, avg. samples / sec: 60014.52
Iteration:   3600, Loss function: 2.796, Average Loss: 3.879, avg. samples / sec: 59577.05
Iteration:   3600, Loss function: 2.662, Average Loss: 3.930, avg. samples / sec: 59784.06
Iteration:   3600, Loss function: 2.993, Average Loss: 3.921, avg. samples / sec: 59505.81
Iteration:   3600, Loss function: 3.226, Average Loss: 3.919, avg. samples / sec: 59888.48
Iteration:   3600, Loss function: 3.318, Average Loss: 3.892, avg. samples / sec: 59649.77
Iteration:   3600, Loss function: 3.130, Average Loss: 3.899, avg. samples / sec: 59633.34
Iteration:   3600, Loss function: 2.283, Average Loss: 3.886, avg. samples / sec: 59356.66
Iteration:   3600, Loss function: 3.347, Average Loss: 3.866, avg. samples / sec: 59827.64
Iteration:   3600, Loss function: 3.768, Average Loss: 3.909, avg. samples / sec: 59851.93
Iteration:   3600, Loss function: 3.264, Average Loss: 3.912, avg. samples / sec: 59626.30
Iteration:   3600, Loss function: 4.100, Average Loss: 3.875, avg. samples / sec: 59570.88
Iteration:   3600, Loss function: 3.063, Average Loss: 3.879, avg. samples / sec: 59420.48
Iteration:   3620, Loss function: 3.065, Average Loss: 3.907, avg. samples / sec: 58089.29
Iteration:   3620, Loss function: 2.759, Average Loss: 3.900, avg. samples / sec: 58091.39
Iteration:   3620, Loss function: 2.346, Average Loss: 3.874, avg. samples / sec: 58083.30
Iteration:   3620, Loss function: 3.239, Average Loss: 3.913, avg. samples / sec: 57910.74
Iteration:   3620, Loss function: 4.416, Average Loss: 3.893, avg. samples / sec: 58009.59
Iteration:   3620, Loss function: 4.109, Average Loss: 3.885, avg. samples / sec: 57883.41
Iteration:   3620, Loss function: 2.544, Average Loss: 3.906, avg. samples / sec: 57880.29
Iteration:   3620, Loss function: 2.710, Average Loss: 3.916, avg. samples / sec: 57871.47
Iteration:   3620, Loss function: 3.522, Average Loss: 3.894, avg. samples / sec: 57686.66
Iteration:   3620, Loss function: 3.020, Average Loss: 3.906, avg. samples / sec: 57894.70
Iteration:   3620, Loss function: 3.178, Average Loss: 3.857, avg. samples / sec: 57870.12
Iteration:   3620, Loss function: 2.469, Average Loss: 3.868, avg. samples / sec: 57823.25
Iteration:   3620, Loss function: 3.977, Average Loss: 3.881, avg. samples / sec: 57832.19
Iteration:   3620, Loss function: 2.722, Average Loss: 3.872, avg. samples / sec: 57956.18
Iteration:   3620, Loss function: 3.170, Average Loss: 3.864, avg. samples / sec: 57590.69
:::MLL 1558639753.355 epoch_stop: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 819}}
:::MLL 1558639753.355 epoch_start: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 673}}
Iteration:   3640, Loss function: 3.651, Average Loss: 3.902, avg. samples / sec: 59184.86
Iteration:   3640, Loss function: 3.240, Average Loss: 3.863, avg. samples / sec: 59188.24
Iteration:   3640, Loss function: 2.986, Average Loss: 3.850, avg. samples / sec: 59149.96
Iteration:   3640, Loss function: 2.307, Average Loss: 3.895, avg. samples / sec: 58894.21
Iteration:   3640, Loss function: 2.598, Average Loss: 3.860, avg. samples / sec: 59525.08
Iteration:   3640, Loss function: 3.780, Average Loss: 3.866, avg. samples / sec: 59158.03
Iteration:   3640, Loss function: 2.907, Average Loss: 3.904, avg. samples / sec: 58952.97
Iteration:   3640, Loss function: 2.612, Average Loss: 3.893, avg. samples / sec: 58990.56
Iteration:   3640, Loss function: 3.103, Average Loss: 3.875, avg. samples / sec: 59030.11
Iteration:   3640, Loss function: 3.572, Average Loss: 3.883, avg. samples / sec: 58967.01
Iteration:   3640, Loss function: 3.156, Average Loss: 3.883, avg. samples / sec: 58802.95
Iteration:   3640, Loss function: 3.606, Average Loss: 3.865, avg. samples / sec: 58686.29
Iteration:   3640, Loss function: 3.276, Average Loss: 3.891, avg. samples / sec: 58891.83
Iteration:   3640, Loss function: 4.795, Average Loss: 3.876, avg. samples / sec: 58765.21
Iteration:   3640, Loss function: 4.244, Average Loss: 3.894, avg. samples / sec: 58642.41
Iteration:   3660, Loss function: 2.512, Average Loss: 3.851, avg. samples / sec: 56254.57
Iteration:   3660, Loss function: 2.492, Average Loss: 3.854, avg. samples / sec: 56428.89
Iteration:   3660, Loss function: 2.773, Average Loss: 3.890, avg. samples / sec: 56239.15
Iteration:   3660, Loss function: 4.370, Average Loss: 3.887, avg. samples / sec: 56306.13
Iteration:   3660, Loss function: 3.756, Average Loss: 3.870, avg. samples / sec: 56318.19
Iteration:   3660, Loss function: 2.210, Average Loss: 3.892, avg. samples / sec: 56119.07
Iteration:   3660, Loss function: 3.542, Average Loss: 3.884, avg. samples / sec: 56394.45
Iteration:   3660, Loss function: 2.671, Average Loss: 3.879, avg. samples / sec: 56297.34
Iteration:   3660, Loss function: 4.061, Average Loss: 3.862, avg. samples / sec: 56154.33
Iteration:   3660, Loss function: 3.389, Average Loss: 3.877, avg. samples / sec: 56248.15
Iteration:   3660, Loss function: 2.039, Average Loss: 3.866, avg. samples / sec: 56331.00
Iteration:   3660, Loss function: 3.179, Average Loss: 3.886, avg. samples / sec: 56073.27
Iteration:   3660, Loss function: 2.146, Average Loss: 3.836, avg. samples / sec: 56030.29
Iteration:   3660, Loss function: 4.020, Average Loss: 3.853, avg. samples / sec: 56046.38
Iteration:   3660, Loss function: 2.855, Average Loss: 3.879, avg. samples / sec: 56259.87
Iteration:   3680, Loss function: 4.298, Average Loss: 3.873, avg. samples / sec: 60201.62
Iteration:   3680, Loss function: 5.550, Average Loss: 3.863, avg. samples / sec: 60029.09
Iteration:   3680, Loss function: 3.474, Average Loss: 3.884, avg. samples / sec: 60006.67
Iteration:   3680, Loss function: 2.819, Average Loss: 3.840, avg. samples / sec: 59886.65
Iteration:   3680, Loss function: 3.861, Average Loss: 3.859, avg. samples / sec: 60096.18
Iteration:   3680, Loss function: 2.896, Average Loss: 3.852, avg. samples / sec: 60026.69
Iteration:   3680, Loss function: 3.225, Average Loss: 3.876, avg. samples / sec: 59912.41
Iteration:   3680, Loss function: 3.029, Average Loss: 3.825, avg. samples / sec: 60084.94
Iteration:   3680, Loss function: 2.660, Average Loss: 3.881, avg. samples / sec: 59861.01
Iteration:   3680, Loss function: 3.154, Average Loss: 3.852, avg. samples / sec: 59830.31
Iteration:   3680, Loss function: 3.813, Average Loss: 3.875, avg. samples / sec: 59866.50
Iteration:   3680, Loss function: 3.348, Average Loss: 3.848, avg. samples / sec: 60029.88
Iteration:   3680, Loss function: 4.136, Average Loss: 3.873, avg. samples / sec: 60054.29
Iteration:   3680, Loss function: 2.684, Average Loss: 3.865, avg. samples / sec: 59884.49
Iteration:   3680, Loss function: 3.169, Average Loss: 3.867, avg. samples / sec: 59871.99
:::MLL 1558639755.377 epoch_stop: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 819}}
:::MLL 1558639755.377 epoch_start: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 673}}
Iteration:   3700, Loss function: 3.438, Average Loss: 3.875, avg. samples / sec: 58603.12
Iteration:   3700, Loss function: 4.418, Average Loss: 3.843, avg. samples / sec: 58619.65
Iteration:   3700, Loss function: 4.119, Average Loss: 3.845, avg. samples / sec: 58531.18
Iteration:   3700, Loss function: 2.988, Average Loss: 3.835, avg. samples / sec: 58458.82
Iteration:   3700, Loss function: 4.164, Average Loss: 3.850, avg. samples / sec: 58447.69
Iteration:   3700, Loss function: 3.325, Average Loss: 3.866, avg. samples / sec: 58484.54
Iteration:   3700, Loss function: 3.600, Average Loss: 3.820, avg. samples / sec: 58459.53
Iteration:   3700, Loss function: 3.118, Average Loss: 3.858, avg. samples / sec: 58555.01
Iteration:   3700, Loss function: 3.975, Average Loss: 3.861, avg. samples / sec: 58505.95
Iteration:   3700, Loss function: 4.219, Average Loss: 3.839, avg. samples / sec: 58471.22
Iteration:   3700, Loss function: 3.496, Average Loss: 3.855, avg. samples / sec: 58501.02
Iteration:   3700, Loss function: 3.900, Average Loss: 3.855, avg. samples / sec: 58290.73
Iteration:   3700, Loss function: 2.655, Average Loss: 3.864, avg. samples / sec: 58226.21
Iteration:   3700, Loss function: 2.617, Average Loss: 3.863, avg. samples / sec: 58416.88
Iteration:   3700, Loss function: 3.341, Average Loss: 3.872, avg. samples / sec: 58253.69
Iteration:   3720, Loss function: 2.657, Average Loss: 3.844, avg. samples / sec: 57757.82
Iteration:   3720, Loss function: 4.023, Average Loss: 3.848, avg. samples / sec: 57724.08
Iteration:   3720, Loss function: 3.614, Average Loss: 3.867, avg. samples / sec: 57529.33
Iteration:   3720, Loss function: 2.164, Average Loss: 3.858, avg. samples / sec: 57756.47
Iteration:   3720, Loss function: 3.520, Average Loss: 3.842, avg. samples / sec: 57580.64
Iteration:   3720, Loss function: 3.409, Average Loss: 3.859, avg. samples / sec: 57665.44
Iteration:   3720, Loss function: 3.934, Average Loss: 3.857, avg. samples / sec: 57694.19
Iteration:   3720, Loss function: 2.706, Average Loss: 3.834, avg. samples / sec: 57514.72
Iteration:   3720, Loss function: 3.612, Average Loss: 3.835, avg. samples / sec: 57640.84
Iteration:   3720, Loss function: 4.111, Average Loss: 3.846, avg. samples / sec: 57619.95
Iteration:   3720, Loss function: 4.073, Average Loss: 3.824, avg. samples / sec: 57480.38
Iteration:   3720, Loss function: 4.140, Average Loss: 3.854, avg. samples / sec: 57644.04
Iteration:   3720, Loss function: 3.820, Average Loss: 3.814, avg. samples / sec: 57513.01
Iteration:   3720, Loss function: 2.668, Average Loss: 3.835, avg. samples / sec: 57340.57
Iteration:   3720, Loss function: 4.558, Average Loss: 3.868, avg. samples / sec: 57393.56
Iteration:   3740, Loss function: 3.742, Average Loss: 3.840, avg. samples / sec: 59641.80
Iteration:   3740, Loss function: 3.935, Average Loss: 3.817, avg. samples / sec: 59692.19
Iteration:   3740, Loss function: 3.901, Average Loss: 3.850, avg. samples / sec: 59619.37
Iteration:   3740, Loss function: 4.154, Average Loss: 3.861, avg. samples / sec: 59812.68
Iteration:   3740, Loss function: 3.044, Average Loss: 3.806, avg. samples / sec: 59707.75
Iteration:   3740, Loss function: 3.798, Average Loss: 3.837, avg. samples / sec: 59580.35
Iteration:   3740, Loss function: 4.111, Average Loss: 3.849, avg. samples / sec: 59509.22
Iteration:   3740, Loss function: 2.793, Average Loss: 3.865, avg. samples / sec: 59489.40
Iteration:   3740, Loss function: 4.270, Average Loss: 3.829, avg. samples / sec: 59662.55
Iteration:   3740, Loss function: 2.580, Average Loss: 3.830, avg. samples / sec: 59371.26
Iteration:   3740, Loss function: 3.065, Average Loss: 3.826, avg. samples / sec: 59443.54
Iteration:   3740, Loss function: 3.376, Average Loss: 3.830, avg. samples / sec: 59432.68
Iteration:   3740, Loss function: 3.774, Average Loss: 3.844, avg. samples / sec: 59477.53
Iteration:   3740, Loss function: 4.537, Average Loss: 3.839, avg. samples / sec: 59361.56
Iteration:   3740, Loss function: 3.100, Average Loss: 3.850, avg. samples / sec: 59268.84
Iteration:   3760, Loss function: 3.065, Average Loss: 3.832, avg. samples / sec: 58891.16
Iteration:   3760, Loss function: 2.459, Average Loss: 3.823, avg. samples / sec: 58918.37
Iteration:   3760, Loss function: 2.153, Average Loss: 3.857, avg. samples / sec: 58882.38
Iteration:   3760, Loss function: 4.452, Average Loss: 3.792, avg. samples / sec: 58787.05
Iteration:   3760, Loss function: 3.555, Average Loss: 3.832, avg. samples / sec: 58900.25
Iteration:   3760, Loss function: 3.841, Average Loss: 3.836, avg. samples / sec: 58927.88
Iteration:   3760, Loss function: 3.203, Average Loss: 3.816, avg. samples / sec: 58858.18
Iteration:   3760, Loss function: 3.629, Average Loss: 3.832, avg. samples / sec: 58577.67
Iteration:   3760, Loss function: 3.721, Average Loss: 3.824, avg. samples / sec: 58797.21
Iteration:   3760, Loss function: 4.123, Average Loss: 3.841, avg. samples / sec: 58916.99
Iteration:   3760, Loss function: 3.436, Average Loss: 3.806, avg. samples / sec: 58574.62
Iteration:   3760, Loss function: 2.673, Average Loss: 3.836, avg. samples / sec: 58634.92
Iteration:   3760, Loss function: 3.665, Average Loss: 3.855, avg. samples / sec: 58534.04
Iteration:   3760, Loss function: 3.490, Average Loss: 3.836, avg. samples / sec: 58499.76
Iteration:   3760, Loss function: 3.854, Average Loss: 3.815, avg. samples / sec: 58470.95
:::MLL 1558639757.386 epoch_stop: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 819}}
:::MLL 1558639757.386 epoch_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 673}}
Iteration:   3780, Loss function: 3.745, Average Loss: 3.832, avg. samples / sec: 58546.64
Iteration:   3780, Loss function: 3.827, Average Loss: 3.828, avg. samples / sec: 58640.02
Iteration:   3780, Loss function: 4.010, Average Loss: 3.824, avg. samples / sec: 58401.02
Iteration:   3780, Loss function: 3.886, Average Loss: 3.826, avg. samples / sec: 58372.74
Iteration:   3780, Loss function: 4.293, Average Loss: 3.795, avg. samples / sec: 58353.91
Iteration:   3780, Loss function: 3.013, Average Loss: 3.820, avg. samples / sec: 58226.83
Iteration:   3780, Loss function: 4.223, Average Loss: 3.782, avg. samples / sec: 58129.49
Iteration:   3780, Loss function: 3.183, Average Loss: 3.851, avg. samples / sec: 58109.91
Iteration:   3780, Loss function: 3.059, Average Loss: 3.849, avg. samples / sec: 58333.92
Iteration:   3780, Loss function: 3.463, Average Loss: 3.806, avg. samples / sec: 58168.26
Iteration:   3780, Loss function: 2.440, Average Loss: 3.826, avg. samples / sec: 58123.78
Iteration:   3780, Loss function: 2.384, Average Loss: 3.802, avg. samples / sec: 58535.28
Iteration:   3780, Loss function: 3.650, Average Loss: 3.825, avg. samples / sec: 57906.45
Iteration:   3780, Loss function: 2.771, Average Loss: 3.812, avg. samples / sec: 58044.74
Iteration:   3780, Loss function: 2.162, Average Loss: 3.814, avg. samples / sec: 57766.67
Iteration:   3800, Loss function: 2.462, Average Loss: 3.819, avg. samples / sec: 59431.78
Iteration:   3800, Loss function: 2.859, Average Loss: 3.818, avg. samples / sec: 59462.80
Iteration:   3800, Loss function: 3.218, Average Loss: 3.825, avg. samples / sec: 59033.18
Iteration:   3800, Loss function: 2.466, Average Loss: 3.791, avg. samples / sec: 59356.73
Iteration:   3800, Loss function: 3.095, Average Loss: 3.809, avg. samples / sec: 59452.59
Iteration:   3800, Loss function: 3.625, Average Loss: 3.802, avg. samples / sec: 59225.30
Iteration:   3800, Loss function: 3.393, Average Loss: 3.774, avg. samples / sec: 59157.33
Iteration:   3800, Loss function: 3.752, Average Loss: 3.817, avg. samples / sec: 58970.61
Iteration:   3800, Loss function: 2.937, Average Loss: 3.816, avg. samples / sec: 59025.42
Iteration:   3800, Loss function: 3.801, Average Loss: 3.844, avg. samples / sec: 59074.98
Iteration:   3800, Loss function: 3.944, Average Loss: 3.785, avg. samples / sec: 59028.83
Iteration:   3800, Loss function: 2.343, Average Loss: 3.840, avg. samples / sec: 59066.88
Iteration:   3800, Loss function: 2.361, Average Loss: 3.804, avg. samples / sec: 59402.44
Iteration:   3800, Loss function: 2.177, Average Loss: 3.811, avg. samples / sec: 58992.61
Iteration:   3800, Loss function: 2.311, Average Loss: 3.817, avg. samples / sec: 58806.73
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
:::MLL 1558639758.534 eval_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.54s)
DONE (t=0.56s)
DONE (t=2.81s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22555
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.38622
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23060
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05499
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.23948
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.36377
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.31973
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33601
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09723
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36394
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.52797
Current AP: 0.22555 AP goal: 0.23000
:::MLL 1558639762.563 eval_accuracy: {"value": 0.2255527788785899, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 389}}
:::MLL 1558639762.611 eval_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 392}}
:::MLL 1558639762.620 block_stop: {"value": null, "metadata": {"first_epoch_num": 49, "file": "train.py", "lineno": 804}}
:::MLL 1558639762.621 block_start: {"value": null, "metadata": {"first_epoch_num": 55, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3820, Loss function: 4.878, Average Loss: 3.834, avg. samples / sec: 7187.70
Iteration:   3820, Loss function: 2.559, Average Loss: 3.801, avg. samples / sec: 7184.42
Iteration:   3820, Loss function: 3.002, Average Loss: 3.810, avg. samples / sec: 7185.65
Iteration:   3820, Loss function: 2.370, Average Loss: 3.809, avg. samples / sec: 7182.07
Iteration:   3820, Loss function: 3.529, Average Loss: 3.816, avg. samples / sec: 7183.00
Iteration:   3820, Loss function: 2.809, Average Loss: 3.810, avg. samples / sec: 7185.77
Iteration:   3820, Loss function: 4.239, Average Loss: 3.806, avg. samples / sec: 7181.59
Iteration:   3820, Loss function: 4.045, Average Loss: 3.768, avg. samples / sec: 7184.25
Iteration:   3820, Loss function: 2.623, Average Loss: 3.809, avg. samples / sec: 7186.74
Iteration:   3820, Loss function: 4.286, Average Loss: 3.784, avg. samples / sec: 7181.92
Iteration:   3820, Loss function: 2.686, Average Loss: 3.829, avg. samples / sec: 7185.43
Iteration:   3820, Loss function: 4.120, Average Loss: 3.804, avg. samples / sec: 7185.24
Iteration:   3820, Loss function: 3.274, Average Loss: 3.795, avg. samples / sec: 7184.64
Iteration:   3820, Loss function: 3.120, Average Loss: 3.776, avg. samples / sec: 7183.73
Iteration:   3820, Loss function: 2.806, Average Loss: 3.796, avg. samples / sec: 7181.68
:::MLL 1558639763.486 epoch_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 819}}
:::MLL 1558639763.487 epoch_start: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 673}}
Iteration:   3840, Loss function: 4.020, Average Loss: 3.789, avg. samples / sec: 59346.04
Iteration:   3840, Loss function: 3.005, Average Loss: 3.799, avg. samples / sec: 59395.71
Iteration:   3840, Loss function: 3.278, Average Loss: 3.759, avg. samples / sec: 59325.72
Iteration:   3840, Loss function: 4.156, Average Loss: 3.770, avg. samples / sec: 59410.28
Iteration:   3840, Loss function: 3.653, Average Loss: 3.804, avg. samples / sec: 59294.75
Iteration:   3840, Loss function: 3.030, Average Loss: 3.802, avg. samples / sec: 59196.91
Iteration:   3840, Loss function: 3.228, Average Loss: 3.793, avg. samples / sec: 59285.39
Iteration:   3840, Loss function: 2.954, Average Loss: 3.821, avg. samples / sec: 59259.44
Iteration:   3840, Loss function: 4.544, Average Loss: 3.780, avg. samples / sec: 59247.43
Iteration:   3840, Loss function: 3.829, Average Loss: 3.797, avg. samples / sec: 59143.97
Iteration:   3840, Loss function: 3.929, Average Loss: 3.804, avg. samples / sec: 59171.46
Iteration:   3840, Loss function: 3.196, Average Loss: 3.805, avg. samples / sec: 59119.41
Iteration:   3840, Loss function: 3.051, Average Loss: 3.825, avg. samples / sec: 58996.75
Iteration:   3840, Loss function: 3.120, Average Loss: 3.785, avg. samples / sec: 59235.16
Iteration:   3840, Loss function: 4.631, Average Loss: 3.787, avg. samples / sec: 59299.26
Iteration:   3860, Loss function: 3.891, Average Loss: 3.794, avg. samples / sec: 58008.59
Iteration:   3860, Loss function: 2.903, Average Loss: 3.773, avg. samples / sec: 58236.17
Iteration:   3860, Loss function: 3.337, Average Loss: 3.801, avg. samples / sec: 58151.58
Iteration:   3860, Loss function: 1.930, Average Loss: 3.793, avg. samples / sec: 58090.43
Iteration:   3860, Loss function: 3.914, Average Loss: 3.789, avg. samples / sec: 58020.17
Iteration:   3860, Loss function: 3.038, Average Loss: 3.767, avg. samples / sec: 57953.27
Iteration:   3860, Loss function: 3.963, Average Loss: 3.771, avg. samples / sec: 58019.93
Iteration:   3860, Loss function: 3.058, Average Loss: 3.796, avg. samples / sec: 58011.40
Iteration:   3860, Loss function: 3.308, Average Loss: 3.782, avg. samples / sec: 57782.40
Iteration:   3860, Loss function: 2.905, Average Loss: 3.792, avg. samples / sec: 57934.14
Iteration:   3860, Loss function: 3.813, Average Loss: 3.787, avg. samples / sec: 57971.43
Iteration:   3860, Loss function: 2.361, Average Loss: 3.784, avg. samples / sec: 58041.60
Iteration:   3860, Loss function: 4.603, Average Loss: 3.816, avg. samples / sec: 57961.30
Iteration:   3860, Loss function: 2.667, Average Loss: 3.811, avg. samples / sec: 58002.52
Iteration:   3860, Loss function: 3.426, Average Loss: 3.749, avg. samples / sec: 57793.06
Iteration:   3880, Loss function: 4.126, Average Loss: 3.780, avg. samples / sec: 59140.18
Iteration:   3880, Loss function: 3.355, Average Loss: 3.739, avg. samples / sec: 59194.18
Iteration:   3880, Loss function: 3.848, Average Loss: 3.757, avg. samples / sec: 59048.62
Iteration:   3880, Loss function: 3.308, Average Loss: 3.770, avg. samples / sec: 59031.03
Iteration:   3880, Loss function: 3.827, Average Loss: 3.761, avg. samples / sec: 58870.87
Iteration:   3880, Loss function: 3.531, Average Loss: 3.786, avg. samples / sec: 59013.70
Iteration:   3880, Loss function: 3.383, Average Loss: 3.785, avg. samples / sec: 59028.63
Iteration:   3880, Loss function: 3.374, Average Loss: 3.805, avg. samples / sec: 59050.52
Iteration:   3880, Loss function: 3.144, Average Loss: 3.774, avg. samples / sec: 59046.17
Iteration:   3880, Loss function: 2.810, Average Loss: 3.795, avg. samples / sec: 58910.27
Iteration:   3880, Loss function: 3.748, Average Loss: 3.807, avg. samples / sec: 59071.78
Iteration:   3880, Loss function: 3.249, Average Loss: 3.764, avg. samples / sec: 58964.25
Iteration:   3880, Loss function: 3.805, Average Loss: 3.781, avg. samples / sec: 58843.88
Iteration:   3880, Loss function: 3.465, Average Loss: 3.787, avg. samples / sec: 58717.22
Iteration:   3880, Loss function: 3.328, Average Loss: 3.781, avg. samples / sec: 58843.78
Iteration:   3900, Loss function: 3.241, Average Loss: 3.772, avg. samples / sec: 59652.07
Iteration:   3900, Loss function: 3.111, Average Loss: 3.765, avg. samples / sec: 59672.28
Iteration:   3900, Loss function: 2.494, Average Loss: 3.776, avg. samples / sec: 59667.17
Iteration:   3900, Loss function: 3.624, Average Loss: 3.800, avg. samples / sec: 59674.30
Iteration:   3900, Loss function: 2.549, Average Loss: 3.777, avg. samples / sec: 59735.28
Iteration:   3900, Loss function: 3.865, Average Loss: 3.792, avg. samples / sec: 59593.37
Iteration:   3900, Loss function: 2.970, Average Loss: 3.752, avg. samples / sec: 59549.83
Iteration:   3900, Loss function: 2.996, Average Loss: 3.768, avg. samples / sec: 59540.50
Iteration:   3900, Loss function: 2.531, Average Loss: 3.749, avg. samples / sec: 59506.71
Iteration:   3900, Loss function: 2.753, Average Loss: 3.776, avg. samples / sec: 59636.02
Iteration:   3900, Loss function: 3.465, Average Loss: 3.736, avg. samples / sec: 59449.55
Iteration:   3900, Loss function: 3.749, Average Loss: 3.792, avg. samples / sec: 59508.47
Iteration:   3900, Loss function: 3.266, Average Loss: 3.760, avg. samples / sec: 59517.54
Iteration:   3900, Loss function: 4.341, Average Loss: 3.780, avg. samples / sec: 59414.24
Iteration:   3900, Loss function: 2.018, Average Loss: 3.771, avg. samples / sec: 59422.38
:::MLL 1558639765.484 epoch_stop: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 819}}
:::MLL 1558639765.485 epoch_start: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 673}}
Iteration:   3920, Loss function: 1.890, Average Loss: 3.763, avg. samples / sec: 59153.66
Iteration:   3920, Loss function: 4.285, Average Loss: 3.765, avg. samples / sec: 58956.03
Iteration:   3920, Loss function: 2.995, Average Loss: 3.752, avg. samples / sec: 59225.65
Iteration:   3920, Loss function: 2.751, Average Loss: 3.755, avg. samples / sec: 58942.57
Iteration:   3920, Loss function: 3.669, Average Loss: 3.748, avg. samples / sec: 59002.51
Iteration:   3920, Loss function: 4.756, Average Loss: 3.783, avg. samples / sec: 58937.14
Iteration:   3920, Loss function: 4.203, Average Loss: 3.768, avg. samples / sec: 58976.43
Iteration:   3920, Loss function: 4.871, Average Loss: 3.786, avg. samples / sec: 59007.18
Iteration:   3920, Loss function: 2.355, Average Loss: 3.762, avg. samples / sec: 58959.76
Iteration:   3920, Loss function: 2.431, Average Loss: 3.729, avg. samples / sec: 58972.66
Iteration:   3920, Loss function: 3.737, Average Loss: 3.770, avg. samples / sec: 59024.63
Iteration:   3920, Loss function: 4.144, Average Loss: 3.742, avg. samples / sec: 58854.35
Iteration:   3920, Loss function: 2.803, Average Loss: 3.767, avg. samples / sec: 58711.25
Iteration:   3920, Loss function: 3.474, Average Loss: 3.784, avg. samples / sec: 58652.07
Iteration:   3920, Loss function: 3.678, Average Loss: 3.766, avg. samples / sec: 58779.75
Iteration:   3940, Loss function: 2.789, Average Loss: 3.744, avg. samples / sec: 58360.66
Iteration:   3940, Loss function: 4.199, Average Loss: 3.756, avg. samples / sec: 58814.04
Iteration:   3940, Loss function: 2.251, Average Loss: 3.724, avg. samples / sec: 58391.80
Iteration:   3940, Loss function: 2.816, Average Loss: 3.746, avg. samples / sec: 58277.03
Iteration:   3940, Loss function: 2.585, Average Loss: 3.756, avg. samples / sec: 58372.38
Iteration:   3940, Loss function: 3.535, Average Loss: 3.762, avg. samples / sec: 58418.38
Iteration:   3940, Loss function: 2.582, Average Loss: 3.762, avg. samples / sec: 58316.29
Iteration:   3940, Loss function: 2.686, Average Loss: 3.778, avg. samples / sec: 58328.15
Iteration:   3940, Loss function: 3.630, Average Loss: 3.774, avg. samples / sec: 58517.47
Iteration:   3940, Loss function: 4.361, Average Loss: 3.761, avg. samples / sec: 58408.04
Iteration:   3940, Loss function: 3.110, Average Loss: 3.753, avg. samples / sec: 57982.81
Iteration:   3940, Loss function: 3.573, Average Loss: 3.775, avg. samples / sec: 58198.63
Iteration:   3940, Loss function: 3.034, Average Loss: 3.737, avg. samples / sec: 58167.09
Iteration:   3940, Loss function: 3.044, Average Loss: 3.756, avg. samples / sec: 57960.06
Iteration:   3940, Loss function: 3.820, Average Loss: 3.734, avg. samples / sec: 57971.03
Iteration:   3960, Loss function: 3.059, Average Loss: 3.751, avg. samples / sec: 58401.89
Iteration:   3960, Loss function: 2.987, Average Loss: 3.757, avg. samples / sec: 58344.76
Iteration:   3960, Loss function: 2.705, Average Loss: 3.719, avg. samples / sec: 58804.49
Iteration:   3960, Loss function: 2.718, Average Loss: 3.744, avg. samples / sec: 58419.15
Iteration:   3960, Loss function: 3.418, Average Loss: 3.731, avg. samples / sec: 58121.41
Iteration:   3960, Loss function: 3.166, Average Loss: 3.766, avg. samples / sec: 58277.69
Iteration:   3960, Loss function: 3.134, Average Loss: 3.759, avg. samples / sec: 58257.28
Iteration:   3960, Loss function: 3.715, Average Loss: 3.756, avg. samples / sec: 58255.40
Iteration:   3960, Loss function: 3.113, Average Loss: 3.738, avg. samples / sec: 58150.89
Iteration:   3960, Loss function: 2.929, Average Loss: 3.748, avg. samples / sec: 58363.56
Iteration:   3960, Loss function: 3.545, Average Loss: 3.728, avg. samples / sec: 58306.60
Iteration:   3960, Loss function: 4.386, Average Loss: 3.766, avg. samples / sec: 58295.07
Iteration:   3960, Loss function: 3.859, Average Loss: 3.765, avg. samples / sec: 58140.83
Iteration:   3960, Loss function: 3.256, Average Loss: 3.717, avg. samples / sec: 57996.65
Iteration:   3960, Loss function: 3.245, Average Loss: 3.748, avg. samples / sec: 57882.57
:::MLL 1558639767.495 epoch_stop: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 819}}
:::MLL 1558639767.495 epoch_start: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 673}}
Iteration:   3980, Loss function: 2.786, Average Loss: 3.758, avg. samples / sec: 58936.36
Iteration:   3980, Loss function: 3.607, Average Loss: 3.736, avg. samples / sec: 58841.76
Iteration:   3980, Loss function: 3.495, Average Loss: 3.704, avg. samples / sec: 59093.55
Iteration:   3980, Loss function: 3.402, Average Loss: 3.754, avg. samples / sec: 58975.52
Iteration:   3980, Loss function: 3.169, Average Loss: 3.721, avg. samples / sec: 58885.72
Iteration:   3980, Loss function: 3.467, Average Loss: 3.741, avg. samples / sec: 58873.38
Iteration:   3980, Loss function: 2.680, Average Loss: 3.713, avg. samples / sec: 58696.92
Iteration:   3980, Loss function: 2.881, Average Loss: 3.736, avg. samples / sec: 59081.71
Iteration:   3980, Loss function: 3.409, Average Loss: 3.732, avg. samples / sec: 58798.92
Iteration:   3980, Loss function: 2.673, Average Loss: 3.752, avg. samples / sec: 58726.67
Iteration:   3980, Loss function: 3.206, Average Loss: 3.746, avg. samples / sec: 58519.83
Iteration:   3980, Loss function: 4.200, Average Loss: 3.763, avg. samples / sec: 58790.12
Iteration:   3980, Loss function: 3.626, Average Loss: 3.749, avg. samples / sec: 58758.79
Iteration:   3980, Loss function: 3.639, Average Loss: 3.753, avg. samples / sec: 58521.84
Iteration:   3980, Loss function: 2.816, Average Loss: 3.724, avg. samples / sec: 58526.53
Iteration:   4000, Loss function: 4.899, Average Loss: 3.759, avg. samples / sec: 57795.03
Iteration:   4000, Loss function: 2.556, Average Loss: 3.742, avg. samples / sec: 57737.61
Iteration:   4000, Loss function: 3.714, Average Loss: 3.736, avg. samples / sec: 57631.62
Iteration:   4000, Loss function: 2.927, Average Loss: 3.746, avg. samples / sec: 57548.78
Iteration:   4000, Loss function: 2.483, Average Loss: 3.725, avg. samples / sec: 57666.95
Iteration:   4000, Loss function: 2.351, Average Loss: 3.752, avg. samples / sec: 57466.46
Iteration:   4000, Loss function: 3.378, Average Loss: 3.729, avg. samples / sec: 57622.45
Iteration:   4000, Loss function: 3.323, Average Loss: 3.697, avg. samples / sec: 57497.69
Iteration:   4000, Loss function: 2.287, Average Loss: 3.714, avg. samples / sec: 57541.61
Iteration:   4000, Loss function: 3.789, Average Loss: 3.729, avg. samples / sec: 57444.65
Iteration:   4000, Loss function: 2.851, Average Loss: 3.716, avg. samples / sec: 57732.95
Iteration:   4000, Loss function: 3.748, Average Loss: 3.739, avg. samples / sec: 57600.53
Iteration:   4000, Loss function: 2.825, Average Loss: 3.711, avg. samples / sec: 57493.77
Iteration:   4000, Loss function: 2.088, Average Loss: 3.744, avg. samples / sec: 57450.36
Iteration:   4000, Loss function: 3.428, Average Loss: 3.742, avg. samples / sec: 57499.94
Iteration:   4020, Loss function: 2.743, Average Loss: 3.723, avg. samples / sec: 58397.13
Iteration:   4020, Loss function: 3.978, Average Loss: 3.740, avg. samples / sec: 58521.89
Iteration:   4020, Loss function: 2.526, Average Loss: 3.749, avg. samples / sec: 58138.48
Iteration:   4020, Loss function: 3.561, Average Loss: 3.736, avg. samples / sec: 58481.85
Iteration:   4020, Loss function: 3.189, Average Loss: 3.733, avg. samples / sec: 58139.90
Iteration:   4020, Loss function: 2.499, Average Loss: 3.709, avg. samples / sec: 58161.06
Iteration:   4020, Loss function: 3.839, Average Loss: 3.720, avg. samples / sec: 58136.66
Iteration:   4020, Loss function: 3.025, Average Loss: 3.745, avg. samples / sec: 58088.26
Iteration:   4020, Loss function: 2.482, Average Loss: 3.688, avg. samples / sec: 58106.00
Iteration:   4020, Loss function: 4.217, Average Loss: 3.730, avg. samples / sec: 58036.35
Iteration:   4020, Loss function: 3.081, Average Loss: 3.708, avg. samples / sec: 58125.25
Iteration:   4020, Loss function: 3.703, Average Loss: 3.700, avg. samples / sec: 58156.17
Iteration:   4020, Loss function: 3.796, Average Loss: 3.733, avg. samples / sec: 58092.61
Iteration:   4020, Loss function: 4.334, Average Loss: 3.723, avg. samples / sec: 57990.78
Iteration:   4020, Loss function: 2.845, Average Loss: 3.739, avg. samples / sec: 57803.11
Iteration:   4040, Loss function: 3.346, Average Loss: 3.726, avg. samples / sec: 57469.01
Iteration:   4040, Loss function: 2.316, Average Loss: 3.702, avg. samples / sec: 57402.53
Iteration:   4040, Loss function: 3.016, Average Loss: 3.706, avg. samples / sec: 57354.18
Iteration:   4040, Loss function: 3.155, Average Loss: 3.724, avg. samples / sec: 57283.72
Iteration:   4040, Loss function: 4.031, Average Loss: 3.730, avg. samples / sec: 57650.55
Iteration:   4040, Loss function: 2.914, Average Loss: 3.727, avg. samples / sec: 57248.86
Iteration:   4040, Loss function: 4.254, Average Loss: 3.696, avg. samples / sec: 57416.80
Iteration:   4040, Loss function: 2.243, Average Loss: 3.728, avg. samples / sec: 57159.75
Iteration:   4040, Loss function: 2.813, Average Loss: 3.685, avg. samples / sec: 57354.43
Iteration:   4040, Loss function: 2.691, Average Loss: 3.710, avg. samples / sec: 57328.23
Iteration:   4040, Loss function: 3.203, Average Loss: 3.742, avg. samples / sec: 57309.09
Iteration:   4040, Loss function: 2.174, Average Loss: 3.725, avg. samples / sec: 57318.81
Iteration:   4040, Loss function: 3.629, Average Loss: 3.715, avg. samples / sec: 57388.27
Iteration:   4040, Loss function: 3.263, Average Loss: 3.739, avg. samples / sec: 57136.41
Iteration:   4040, Loss function: 3.230, Average Loss: 3.714, avg. samples / sec: 56992.27
:::MLL 1558639769.522 epoch_stop: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 819}}
:::MLL 1558639769.523 epoch_start: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 673}}
Iteration:   4060, Loss function: 3.590, Average Loss: 3.710, avg. samples / sec: 59264.60
Iteration:   4060, Loss function: 2.707, Average Loss: 3.688, avg. samples / sec: 59138.12
Iteration:   4060, Loss function: 2.791, Average Loss: 3.718, avg. samples / sec: 59119.06
Iteration:   4060, Loss function: 3.969, Average Loss: 3.681, avg. samples / sec: 59129.36
Iteration:   4060, Loss function: 3.216, Average Loss: 3.734, avg. samples / sec: 59135.78
Iteration:   4060, Loss function: 3.034, Average Loss: 3.718, avg. samples / sec: 59059.60
Iteration:   4060, Loss function: 2.587, Average Loss: 3.699, avg. samples / sec: 58991.79
Iteration:   4060, Loss function: 2.229, Average Loss: 3.724, avg. samples / sec: 58950.09
Iteration:   4060, Loss function: 2.643, Average Loss: 3.713, avg. samples / sec: 58917.01
Iteration:   4060, Loss function: 3.327, Average Loss: 3.701, avg. samples / sec: 58967.53
Iteration:   4060, Loss function: 3.482, Average Loss: 3.717, avg. samples / sec: 58982.93
Iteration:   4060, Loss function: 3.118, Average Loss: 3.730, avg. samples / sec: 58965.08
Iteration:   4060, Loss function: 3.891, Average Loss: 3.707, avg. samples / sec: 58946.34
Iteration:   4060, Loss function: 4.095, Average Loss: 3.718, avg. samples / sec: 58792.40
Iteration:   4060, Loss function: 3.931, Average Loss: 3.696, avg. samples / sec: 58733.69
Iteration:   4080, Loss function: 3.056, Average Loss: 3.718, avg. samples / sec: 59421.40
Iteration:   4080, Loss function: 2.405, Average Loss: 3.699, avg. samples / sec: 59508.29
Iteration:   4080, Loss function: 2.996, Average Loss: 3.700, avg. samples / sec: 59137.97
Iteration:   4080, Loss function: 3.702, Average Loss: 3.689, avg. samples / sec: 59488.95
Iteration:   4080, Loss function: 4.471, Average Loss: 3.713, avg. samples / sec: 59434.16
Iteration:   4080, Loss function: 2.883, Average Loss: 3.708, avg. samples / sec: 59164.90
Iteration:   4080, Loss function: 3.077, Average Loss: 3.722, avg. samples / sec: 59335.82
Iteration:   4080, Loss function: 3.569, Average Loss: 3.704, avg. samples / sec: 59273.55
Iteration:   4080, Loss function: 2.337, Average Loss: 3.694, avg. samples / sec: 59251.72
Iteration:   4080, Loss function: 3.895, Average Loss: 3.686, avg. samples / sec: 59024.58
Iteration:   4080, Loss function: 2.685, Average Loss: 3.708, avg. samples / sec: 59022.40
Iteration:   4080, Loss function: 3.416, Average Loss: 3.675, avg. samples / sec: 58957.41
Iteration:   4080, Loss function: 3.213, Average Loss: 3.714, avg. samples / sec: 59103.86
Iteration:   4080, Loss function: 3.775, Average Loss: 3.689, avg. samples / sec: 58977.52
Iteration:   4080, Loss function: 3.708, Average Loss: 3.727, avg. samples / sec: 58905.71
Iteration:   4100, Loss function: 3.830, Average Loss: 3.666, avg. samples / sec: 57462.17
Iteration:   4100, Loss function: 2.387, Average Loss: 3.682, avg. samples / sec: 57265.45
Iteration:   4100, Loss function: 4.131, Average Loss: 3.708, avg. samples / sec: 57244.19
Iteration:   4100, Loss function: 3.813, Average Loss: 3.709, avg. samples / sec: 57435.31
Iteration:   4100, Loss function: 3.262, Average Loss: 3.714, avg. samples / sec: 57460.69
Iteration:   4100, Loss function: 4.736, Average Loss: 3.684, avg. samples / sec: 57265.73
Iteration:   4100, Loss function: 3.957, Average Loss: 3.720, avg. samples / sec: 57196.08
Iteration:   4100, Loss function: 4.031, Average Loss: 3.700, avg. samples / sec: 57210.75
Iteration:   4100, Loss function: 3.633, Average Loss: 3.705, avg. samples / sec: 57174.50
Iteration:   4100, Loss function: 2.817, Average Loss: 3.716, avg. samples / sec: 57030.77
Iteration:   4100, Loss function: 3.398, Average Loss: 3.691, avg. samples / sec: 56984.65
Iteration:   4100, Loss function: 2.632, Average Loss: 3.689, avg. samples / sec: 57038.80
Iteration:   4100, Loss function: 2.255, Average Loss: 3.703, avg. samples / sec: 57138.43
Iteration:   4100, Loss function: 3.204, Average Loss: 3.686, avg. samples / sec: 57187.88
Iteration:   4100, Loss function: 3.532, Average Loss: 3.685, avg. samples / sec: 56002.50
:::MLL 1558639771.548 epoch_stop: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 819}}
:::MLL 1558639771.549 epoch_start: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 673}}
Iteration:   4120, Loss function: 3.598, Average Loss: 3.680, avg. samples / sec: 58161.95
Iteration:   4120, Loss function: 3.947, Average Loss: 3.688, avg. samples / sec: 57914.92
Iteration:   4120, Loss function: 3.370, Average Loss: 3.665, avg. samples / sec: 57788.47
Iteration:   4120, Loss function: 3.067, Average Loss: 3.713, avg. samples / sec: 57932.73
Iteration:   4120, Loss function: 2.719, Average Loss: 3.695, avg. samples / sec: 58025.21
Iteration:   4120, Loss function: 3.540, Average Loss: 3.713, avg. samples / sec: 57907.24
Iteration:   4120, Loss function: 2.975, Average Loss: 3.688, avg. samples / sec: 57953.32
Iteration:   4120, Loss function: 2.710, Average Loss: 3.680, avg. samples / sec: 57963.97
Iteration:   4120, Loss function: 3.501, Average Loss: 3.677, avg. samples / sec: 57786.50
Iteration:   4120, Loss function: 3.473, Average Loss: 3.709, avg. samples / sec: 57824.34
Iteration:   4120, Loss function: 4.457, Average Loss: 3.704, avg. samples / sec: 57697.50
Iteration:   4120, Loss function: 2.828, Average Loss: 3.678, avg. samples / sec: 57652.70
Iteration:   4120, Loss function: 3.405, Average Loss: 3.703, avg. samples / sec: 57618.89
Iteration:   4120, Loss function: 3.166, Average Loss: 3.701, avg. samples / sec: 57609.69
Iteration:   4120, Loss function: 2.942, Average Loss: 3.675, avg. samples / sec: 58952.65
Iteration:   4140, Loss function: 3.327, Average Loss: 3.666, avg. samples / sec: 59676.04
Iteration:   4140, Loss function: 3.069, Average Loss: 3.683, avg. samples / sec: 59376.64
Iteration:   4140, Loss function: 4.601, Average Loss: 3.676, avg. samples / sec: 59406.45
Iteration:   4140, Loss function: 3.555, Average Loss: 3.705, avg. samples / sec: 59331.67
Iteration:   4140, Loss function: 3.088, Average Loss: 3.670, avg. samples / sec: 59203.78
Iteration:   4140, Loss function: 3.120, Average Loss: 3.659, avg. samples / sec: 59208.31
Iteration:   4140, Loss function: 4.989, Average Loss: 3.692, avg. samples / sec: 59176.26
Iteration:   4140, Loss function: 2.685, Average Loss: 3.698, avg. samples / sec: 59365.83
Iteration:   4140, Loss function: 3.180, Average Loss: 3.700, avg. samples / sec: 59171.94
Iteration:   4140, Loss function: 2.732, Average Loss: 3.669, avg. samples / sec: 59151.50
Iteration:   4140, Loss function: 4.262, Average Loss: 3.671, avg. samples / sec: 59164.36
Iteration:   4140, Loss function: 3.061, Average Loss: 3.703, avg. samples / sec: 59099.60
Iteration:   4140, Loss function: 3.055, Average Loss: 3.693, avg. samples / sec: 59311.24
Iteration:   4140, Loss function: 2.919, Average Loss: 3.666, avg. samples / sec: 59192.22
Iteration:   4140, Loss function: 3.298, Average Loss: 3.697, avg. samples / sec: 58436.01
Iteration:   4160, Loss function: 3.098, Average Loss: 3.650, avg. samples / sec: 57236.70
Iteration:   4160, Loss function: 2.840, Average Loss: 3.698, avg. samples / sec: 57112.14
Iteration:   4160, Loss function: 3.509, Average Loss: 3.671, avg. samples / sec: 57162.41
Iteration:   4160, Loss function: 3.141, Average Loss: 3.660, avg. samples / sec: 57445.73
Iteration:   4160, Loss function: 3.091, Average Loss: 3.660, avg. samples / sec: 57223.25
Iteration:   4160, Loss function: 3.109, Average Loss: 3.694, avg. samples / sec: 57214.51
Iteration:   4160, Loss function: 4.043, Average Loss: 3.677, avg. samples / sec: 56948.19
Iteration:   4160, Loss function: 3.287, Average Loss: 3.692, avg. samples / sec: 57168.60
Iteration:   4160, Loss function: 4.433, Average Loss: 3.688, avg. samples / sec: 58046.51
Iteration:   4160, Loss function: 3.288, Average Loss: 3.658, avg. samples / sec: 56851.48
Iteration:   4160, Loss function: 3.243, Average Loss: 3.690, avg. samples / sec: 57085.45
Iteration:   4160, Loss function: 3.766, Average Loss: 3.675, avg. samples / sec: 56838.64
Iteration:   4160, Loss function: 3.997, Average Loss: 3.688, avg. samples / sec: 57109.88
Iteration:   4160, Loss function: 3.292, Average Loss: 3.664, avg. samples / sec: 57015.26
Iteration:   4160, Loss function: 2.857, Average Loss: 3.691, avg. samples / sec: 56874.33
Iteration:   4180, Loss function: 3.867, Average Loss: 3.690, avg. samples / sec: 59712.86
Iteration:   4180, Loss function: 2.597, Average Loss: 3.668, avg. samples / sec: 59800.98
Iteration:   4180, Loss function: 3.848, Average Loss: 3.672, avg. samples / sec: 59656.74
Iteration:   4180, Loss function: 3.849, Average Loss: 3.660, avg. samples / sec: 59846.19
Iteration:   4180, Loss function: 3.360, Average Loss: 3.665, avg. samples / sec: 59577.85
Iteration:   4180, Loss function: 3.904, Average Loss: 3.680, avg. samples / sec: 59671.47
Iteration:   4180, Loss function: 3.657, Average Loss: 3.643, avg. samples / sec: 59436.34
Iteration:   4180, Loss function: 3.039, Average Loss: 3.693, avg. samples / sec: 59447.67
Iteration:   4180, Loss function: 3.935, Average Loss: 3.688, avg. samples / sec: 59565.72
Iteration:   4180, Loss function: 3.579, Average Loss: 3.654, avg. samples / sec: 59484.03
Iteration:   4180, Loss function: 3.712, Average Loss: 3.654, avg. samples / sec: 59500.66
Iteration:   4180, Loss function: 3.910, Average Loss: 3.654, avg. samples / sec: 59568.81
Iteration:   4180, Loss function: 3.428, Average Loss: 3.685, avg. samples / sec: 59784.82
Iteration:   4180, Loss function: 3.779, Average Loss: 3.679, avg. samples / sec: 59476.80
Iteration:   4180, Loss function: 3.852, Average Loss: 3.678, avg. samples / sec: 59342.01
:::MLL 1558639773.550 epoch_stop: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 819}}
:::MLL 1558639773.550 epoch_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 673}}
:::MLL 1558639773.613 eval_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.57s)
DONE (t=2.85s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.23080
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.39320
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23694
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05734
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24388
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.37227
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22443
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32485
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.34162
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09896
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.37053
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.53352
Current AP: 0.23080 AP goal: 0.23000
:::MLL 1558639777.719 eval_accuracy: {"value": 0.23079615521855182, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 389}}
:::MLL 1558639777.726 eval_stop: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 392}}
:::MLL 1558639777.737 block_stop: {"value": null, "metadata": {"first_epoch_num": 55, "file": "train.py", "lineno": 804}}
:::MLL 1558639778.808 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 849}}
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-05-23 07:29:48 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:26:08 PM
ENDING TIMING RUN AT 2019-05-23 07:29:48 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:26:08 PM
ENDING TIMING RUN AT 2019-05-23 07:29:48 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:26:08 PM
ENDING TIMING RUN AT 2019-05-23 07:29:48 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:26:08 PM
ENDING TIMING RUN AT 2019-05-23 07:29:48 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:26:08 PM
ENDING TIMING RUN AT 2019-05-23 07:29:48 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:26:08 PM
ENDING TIMING RUN AT 2019-05-23 07:29:48 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:26:08 PM
ENDING TIMING RUN AT 2019-05-23 07:29:48 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:26:08 PM
ENDING TIMING RUN AT 2019-05-23 07:29:48 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:26:08 PM
ENDING TIMING RUN AT 2019-05-23 07:29:48 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:26:08 PM
ENDING TIMING RUN AT 2019-05-23 07:29:48 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:26:08 PM
ENDING TIMING RUN AT 2019-05-23 07:29:48 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:26:08 PM
ENDING TIMING RUN AT 2019-05-23 07:29:48 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:26:08 PM
ENDING TIMING RUN AT 2019-05-23 07:29:48 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:26:08 PM
ENDING TIMING RUN AT 2019-05-23 07:29:48 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:26:08 PM
