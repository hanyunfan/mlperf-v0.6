Beginning trial 1 of 5
Gathering sys log on circe-n016
:::MLL 1558638651.295 submission_benchmark: {"value": "ssd", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1558638651.295 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known ssd keys.
:::MLL 1558638651.296 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1558638651.296 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1558638651.296 submission_platform: {"value": "15xNVIDIA DGX-2H", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1558638651.297 submission_entry: {"value": "{'hardware': 'NVIDIA DGX-2H', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': 'Ubuntu 18.04.2 LTS / NVIDIA DGX Server 4.0.5', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.4-2.0.7', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '15', 'cpu': '2x Intel(R) Xeon(R) Platinum 8174 CPU @ 3.10GHz', 'num_cores': '48', 'num_vcpus': '96', 'accelerator': 'Tesla V100-SXM3-32GB-H', 'num_accelerators': '16', 'sys_mem_size': '1510 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '2x 894.3G + 8x 3.5T', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27800 Family [ConnectX-5]', 'num_network_cards': '10', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1558638651.297 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1558638651.298 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
:::MLL 1558638652.644 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638652.619 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638652.653 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638652.664 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638652.672 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638652.652 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638652.643 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638652.659 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638652.666 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638652.680 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638652.679 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638652.699 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638652.682 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638652.701 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638652.701 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node circe-n016
+ pids+=($!)
+ set +x
Launching on node circe-n017
+ pids+=($!)
+ set +x
Launching on node circe-n018
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n016
+ pids+=($!)
+ set +x
Launching on node circe-n019
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n017
+ srun --mem=0 -N 1 -n 1 -w circe-n016 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=0 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n020
+ srun --mem=0 -N 1 -n 1 -w circe-n017 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=1 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n018
+ pids+=($!)
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ set +x
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n019
Launching on node circe-n021
+ srun --mem=0 -N 1 -n 1 -w circe-n018 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=2 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n022
+ srun --mem=0 -N 1 -n 1 -w circe-n019 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=3 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n020
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n021
+ pids+=($!)
+ set +x
Launching on node circe-n023
+ srun --mem=0 -N 1 -n 1 -w circe-n020 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=4 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n024
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n022
+ srun --mem=0 -N 1 -n 1 -w circe-n021 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=5 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n025
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n023
+ srun --mem=0 -N 1 -n 1 -w circe-n022 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=6 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n026
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n024
+ srun --mem=0 -N 1 -n 1 -w circe-n023 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=7 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n027
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n025
+ srun --mem=0 -N 1 -n 1 -w circe-n024 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=8 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n028
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n026
+ srun --mem=0 -N 1 -n 1 -w circe-n025 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=9 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n029
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n027
+ srun --mem=0 -N 1 -n 1 -w circe-n026 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=10 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n030
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n028
+ srun --mem=0 -N 1 -n 1 -w circe-n027 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=11 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n029
+ srun --mem=0 -N 1 -n 1 -w circe-n028 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=12 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w circe-n029 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=13 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n030
+ srun --mem=0 -N 1 -n 1 -w circe-n030 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=14 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=0 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=7 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=9 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=13 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=6 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=5 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=10 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=1 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=12 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=4 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=2 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=8 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=3 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=14 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=11 --master_addr=10.0.1.16 --master_port=5204
STARTING TIMING RUN AT 2019-05-23 07:10:52 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=0 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:10:52 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
STARTING TIMING RUN AT 2019-05-23 07:10:52 PM
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=6 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=13 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:10:52 PM
running benchmark
STARTING TIMING RUN AT 2019-05-23 07:10:52 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ NUMEPOCHS=80
+ export DATASET_DIR=/data/coco2017
+ echo 'running benchmark'
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=7 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=9 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:10:52 PM
running benchmark
STARTING TIMING RUN AT 2019-05-23 07:10:52 PM
running benchmark
STARTING TIMING RUN AT 2019-05-23 07:10:52 PM
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ NUMEPOCHS=80
+ DATASET_DIR=/data/coco2017
+ echo 'running benchmark'
+ export TORCH_MODEL_ZOO=/data/torchvision
+ export DATASET_DIR=/data/coco2017
+ TORCH_MODEL_ZOO=/data/torchvision
running benchmark
STARTING TIMING RUN AT 2019-05-23 07:10:52 PM
+ DATASET_DIR=/data/coco2017
running benchmark
+ NUMEPOCHS=80
+ export TORCH_MODEL_ZOO=/data/torchvision
+ echo 'running benchmark'
+ TORCH_MODEL_ZOO=/data/torchvision
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=8 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=10 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=1 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=5 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:10:52 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=2 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:10:52 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=12 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:10:52 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=3 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:10:52 PM
STARTING TIMING RUN AT 2019-05-23 07:10:52 PM
running benchmark
running benchmark
+ NUMEPOCHS=80
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2019-05-23 07:10:52 PM
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
running benchmark
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ export DATASET_DIR=/data/coco2017
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=11 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ DATASET_DIR=/data/coco2017
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ DATASET_DIR=/data/coco2017
+ TORCH_MODEL_ZOO=/data/torchvision
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=4 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=14 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding::::MLL 1558638662.943 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638662.946 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638662.949 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638662.950 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638662.950 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638662.951 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638662.951 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638662.951 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638662.951 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638662.951 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638662.951 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638662.952 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638662.952 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638662.952 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638662.952 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638662.953 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638662.986 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638662.986 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638662.987 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638662.987 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638662.987 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638662.988 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638662.988 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638662.988 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638662.988 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638662.988 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638662.989 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638662.989 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638662.989 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638662.990 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638662.990 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638662.990 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638663.039 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.039 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.039 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.039 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.039 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.040 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638663.041 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.041 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.041 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.041 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.041 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.041 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.041 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.041 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.041 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.042 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638663.168 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.168 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.168 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.169 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.170 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638663.170 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638663.172 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.172 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.172 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.172 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.172 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.172 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.173 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
Binding::::MLL 1558638663.163 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.163 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.163 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.163 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.173 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.164 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.173 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.173 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.164 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638663.165 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638663.166 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.166 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.168 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.168 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.168 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.168 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.168 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.168 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.168 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
Binding::::MLL 1558638663.167 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.167 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638663.168 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638663.168 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638663.169 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.169 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.170 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.170 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.170 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.171 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638663.171 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.171 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.172 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.172 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.172 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.172 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638663.204 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.204 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.204 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638663.206 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.206 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.207 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.207 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.207 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.207 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.208 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638663.208 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.209 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.209 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.209 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.209 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.210 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638663.232 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.232 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.233 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.234 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.234 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.234 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.234 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.234 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.234 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.235 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.235 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.235 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.235 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638663.236 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.236 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.236 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638663.268 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.268 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.268 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.268 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.268 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.269 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638663.270 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638663.270 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.270 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.271 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.271 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.271 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.271 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.272 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.272 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638663.272 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638663.279 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638663.281 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.282 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.283 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.283 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.284 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.284 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.284 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638663.285 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.285 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.285 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.285 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638663.286 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.286 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.286 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.286 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638663.283 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.283 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.283 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.284 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638663.286 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.286 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.286 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.286 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.286 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.287 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.287 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.287 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.287 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.287 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638663.288 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.288 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638663.274 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.275 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.275 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.276 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.276 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.276 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.276 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.277 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.277 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638663.277 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.278 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638663.278 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.278 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.278 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638663.278 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638663.280 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638663.314 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.314 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.314 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.315 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.315 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.315 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.316 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638663.317 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.317 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.317 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.317 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.318 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.318 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638663.318 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.318 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.319 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
Binding::::MLL 1558638663.289 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.290 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.291 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638663.291 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.291 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638663.292 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.292 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638663.293 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.293 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.293 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.294 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.294 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.294 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.294 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.294 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.294 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638663.316 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.319 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.319 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.321 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.321 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.321 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.321 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.321 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.322 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.322 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.323 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638663.323 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.324 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638663.324 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638663.324 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638663.325 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
0 Using seed = 908653398
1 Using seed = 908653399
3 Using seed = 908653401
4 Using seed = 908653402
5 Using seed = 908653403
2 Using seed = 908653400
:::MLL 1558638695.602 max_samples: {"value": 1, "metadata": {"file": "utils.py", "lineno": 465}}
11 Using seed = 908653409
9 Using seed = 908653407
8 Using seed = 908653406
7 Using seed = 908653405
6 Using seed = 908653404
31 Using seed = 908653429
30 Using seed = 908653428
20 Using seed = 908653418
26 Using seed = 908653424
27 Using seed = 908653425
19 Using seed = 908653417
23 Using seed = 908653421
17 Using seed = 908653415
22 Using seed = 908653420
16 Using seed = 908653414
21 Using seed = 908653419
18 Using seed = 908653416
29 Using seed = 908653427
25 Using seed = 908653423
28 Using seed = 908653426
24 Using seed = 908653422
42 Using seed = 908653440
45 Using seed = 908653443
46 Using seed = 908653444
47 Using seed = 908653445
33 Using seed = 908653431
32 Using seed = 908653430
35 Using seed = 908653433
36 Using seed = 908653434
34 Using seed = 908653432
37 Using seed = 908653435
44 Using seed = 908653442
41 Using seed = 908653439
43 Using seed = 908653441
39 Using seed = 908653437
40 Using seed = 908653438
38 Using seed = 908653436
51 Using seed = 908653449
54 Using seed = 908653452
48 Using seed = 908653446
50 Using seed = 908653448
49 Using seed = 908653447
53 Using seed = 908653451
59 Using seed = 908653457
56 Using seed = 908653454
63 Using seed = 908653461
52 Using seed = 908653450
60 Using seed = 908653458
58 Using seed = 908653456
62 Using seed = 908653460
57 Using seed = 908653455
61 Using seed = 908653459
55 Using seed = 908653453
73 Using seed = 908653471
70 Using seed = 908653468
78 Using seed = 908653476
76 Using seed = 908653474
65 Using seed = 908653463
67 Using seed = 908653465
77 Using seed = 908653475
68 Using seed = 908653466
75 Using seed = 908653473
74 Using seed = 908653472
64 Using seed = 908653462
71 Using seed = 908653469
66 Using seed = 908653464
79 Using seed = 908653477
72 Using seed = 908653470
69 Using seed = 908653467
92 Using seed = 908653490
82 Using seed = 908653480
89 Using seed = 908653487
83 Using seed = 908653481
94 Using seed = 908653492
95 Using seed = 908653493
93 Using seed = 908653491
86 Using seed = 908653484
90 Using seed = 908653488
87 Using seed = 908653485
81 Using seed = 908653479
84 Using seed = 908653482
80 Using seed = 908653478
85 Using seed = 908653483
91 Using seed = 908653489
88 Using seed = 908653486
106 Using seed = 908653504
108 Using seed = 908653506
105 Using seed = 908653503
107 Using seed = 908653505
110 Using seed = 908653508
104 Using seed = 908653502
102 Using seed = 908653500
109 Using seed = 908653507
99 Using seed = 908653497
96 Using seed = 908653494
98 Using seed = 908653496
103 Using seed = 908653501
100 Using seed = 908653498
111 Using seed = 908653509
97 Using seed = 908653495
101 Using seed = 908653499
116 Using seed = 908653514
115 Using seed = 908653513
125 Using seed = 908653523
126 Using seed = 908653524
113 Using seed = 908653511
114 Using seed = 908653512
122 Using seed = 908653520
124 Using seed = 908653522
127 Using seed = 908653525
117 Using seed = 908653515
112 Using seed = 908653510
123 Using seed = 908653521
121 Using seed = 908653519
119 Using seed = 908653517
120 Using seed = 908653518
118 Using seed = 908653516
138 Using seed = 908653536
141 Using seed = 908653539
142 Using seed = 908653540
143 Using seed = 908653541
133 Using seed = 908653531
131 Using seed = 908653529
130 Using seed = 908653528
132 Using seed = 908653530
128 Using seed = 908653526
129 Using seed = 908653527
140 Using seed = 908653538
139 Using seed = 908653537
137 Using seed = 908653535
135 Using seed = 908653533
136 Using seed = 908653534
134 Using seed = 908653532
150 Using seed = 908653548
145 Using seed = 908653543
146 Using seed = 908653544
151 Using seed = 908653549
159 Using seed = 908653557
153 Using seed = 908653551
158 Using seed = 908653556
156 Using seed = 908653554
155 Using seed = 908653553
157 Using seed = 908653555
152 Using seed = 908653550
154 Using seed = 908653552
148 Using seed = 908653546
147 Using seed = 908653545
149 Using seed = 908653547
144 Using seed = 908653542
164 Using seed = 908653562
165 Using seed = 908653563
175 Using seed = 908653573
160 Using seed = 908653558
167 Using seed = 908653565
161 Using seed = 908653559
168 Using seed = 908653566
162 Using seed = 908653560
171 Using seed = 908653569
163 Using seed = 908653561
166 Using seed = 908653564
173 Using seed = 908653571
170 Using seed = 908653568
172 Using seed = 908653570
169 Using seed = 908653567
174 Using seed = 908653572
177 Using seed = 908653575
184 Using seed = 908653582
180 Using seed = 908653578
178 Using seed = 908653576
183 Using seed = 908653581
179 Using seed = 908653577
187 Using seed = 908653585
188 Using seed = 908653586
182 Using seed = 908653580
186 Using seed = 908653584
191 Using seed = 908653589
181 Using seed = 908653579
185 Using seed = 908653583
176 Using seed = 908653574
189 Using seed = 908653587
190 Using seed = 908653588
198 Using seed = 908653596
195 Using seed = 908653593
197 Using seed = 908653595
199 Using seed = 908653597
203 Using seed = 908653601
200 Using seed = 908653598
206 Using seed = 908653604
196 Using seed = 908653594
205 Using seed = 908653603
207 Using seed = 908653605
204 Using seed = 908653602
202 Using seed = 908653600
193 Using seed = 908653591
194 Using seed = 908653592
201 Using seed = 908653599
192 Using seed = 908653590
214 Using seed = 908653612
209 Using seed = 908653607
211 Using seed = 908653609
217 Using seed = 908653615
222 Using seed = 908653620
218 Using seed = 908653616
220 Using seed = 908653618
223 Using seed = 908653621
212 Using seed = 908653610
219 Using seed = 908653617
221 Using seed = 908653619
216 Using seed = 908653614
208 Using seed = 908653606
215 Using seed = 908653613
210 Using seed = 908653608
213 Using seed = 908653611
234 Using seed = 908653632
227 Using seed = 908653625
230 Using seed = 908653628
224 Using seed = 908653622
237 Using seed = 908653635
233 Using seed = 908653631
239 Using seed = 908653637
235 Using seed = 908653633
231 Using seed = 908653629
225 Using seed = 908653623
236 Using seed = 908653634
238 Using seed = 908653636
226 Using seed = 908653624
228 Using seed = 908653626
229 Using seed = 908653627
232 Using seed = 908653630
14 Using seed = 908653412
15 Using seed = 908653413
10 Using seed = 908653408
12 Using seed = 908653410
13 Using seed = 908653411
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
:::MLL 1558638700.594 model_bn_span: {"value": 28, "metadata": {"file": "train.py", "lineno": 480}}
:::MLL 1558638700.594 global_batch_size: {"value": 1680, "metadata": {"file": "train.py", "lineno": 481}}
:::MLL 1558638700.635 opt_base_learning_rate: {"value": 0.1625, "metadata": {"file": "train.py", "lineno": 511}}
:::MLL 1558638700.635 opt_weight_decay: {"value": 0.0002, "metadata": {"file": "train.py", "lineno": 513}}
:::MLL 1558638700.636 opt_learning_rate_warmup_steps: {"value": 1250, "metadata": {"file": "train.py", "lineno": 516}}
:::MLL 1558638700.636 opt_learning_rate_warmup_factor: {"value": 0, "metadata": {"file": "train.py", "lineno": 518}}
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
:::MLL 1558638708.265 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 604}}
:::MLL 1558638708.265 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 610}}
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.49s)
creating index...
time_check a: 1558638709.963714838
time_check a: 1558638709.957545042
time_check a: 1558638709.940861702
time_check a: 1558638709.970584631
time_check a: 1558638709.959885359
time_check a: 1558638709.958578110
time_check a: 1558638709.967839956
time_check a: 1558638709.958835125
time_check a: 1558638709.982846022
time_check a: 1558638709.969287157
time_check a: 1558638709.972127199
time_check a: 1558638709.974673271
time_check a: 1558638709.995744705
time_check a: 1558638709.961665392
time_check a: 1558638709.997828245
time_check b: 1558638716.749339104
time_check b: 1558638716.789793253
time_check b: 1558638716.777542353
time_check b: 1558638716.797885418
time_check b: 1558638716.831282377
time_check b: 1558638716.838794470
time_check b: 1558638716.822854042
time_check b: 1558638716.856327057
time_check b: 1558638716.875933886
time_check b: 1558638716.883429527
time_check b: 1558638716.904041767
time_check b: 1558638716.937317133
time_check b: 1558638716.917091131
time_check b: 1558638717.007388115
time_check b: 1558638717.036878347
:::MLL 1558638718.100 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 32.74606450292497, "file": "train.py", "lineno": 669}}
:::MLL 1558638718.101 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 673}}
Iteration:      0, Loss function: 22.279, Average Loss: 0.022, avg. samples / sec: 113.35
Iteration:      0, Loss function: 22.521, Average Loss: 0.023, avg. samples / sec: 115.49
Iteration:      0, Loss function: 22.251, Average Loss: 0.022, avg. samples / sec: 119.41
Iteration:      0, Loss function: 22.462, Average Loss: 0.022, avg. samples / sec: 119.76
Iteration:      0, Loss function: 22.569, Average Loss: 0.023, avg. samples / sec: 110.09
Iteration:      0, Loss function: 22.245, Average Loss: 0.022, avg. samples / sec: 117.39
Iteration:      0, Loss function: 22.592, Average Loss: 0.023, avg. samples / sec: 117.41
Iteration:      0, Loss function: 22.527, Average Loss: 0.023, avg. samples / sec: 117.96
Iteration:      0, Loss function: 22.516, Average Loss: 0.023, avg. samples / sec: 118.36
Iteration:      0, Loss function: 22.556, Average Loss: 0.023, avg. samples / sec: 114.55
Iteration:      0, Loss function: 22.073, Average Loss: 0.022, avg. samples / sec: 116.35
Iteration:      0, Loss function: 22.396, Average Loss: 0.022, avg. samples / sec: 116.15
Iteration:      0, Loss function: 22.765, Average Loss: 0.023, avg. samples / sec: 117.85
Iteration:      0, Loss function: 22.225, Average Loss: 0.022, avg. samples / sec: 113.16
Iteration:      0, Loss function: 22.833, Average Loss: 0.023, avg. samples / sec: 110.76
Iteration:     20, Loss function: 20.443, Average Loss: 0.443, avg. samples / sec: 37926.51
Iteration:     20, Loss function: 20.575, Average Loss: 0.443, avg. samples / sec: 37553.59
Iteration:     20, Loss function: 20.883, Average Loss: 0.444, avg. samples / sec: 37020.56
Iteration:     20, Loss function: 20.802, Average Loss: 0.442, avg. samples / sec: 37341.78
Iteration:     20, Loss function: 20.475, Average Loss: 0.443, avg. samples / sec: 38094.63
Iteration:     20, Loss function: 22.450, Average Loss: 0.448, avg. samples / sec: 37804.53
Iteration:     20, Loss function: 20.286, Average Loss: 0.442, avg. samples / sec: 37352.36
Iteration:     20, Loss function: 21.850, Average Loss: 0.444, avg. samples / sec: 37358.10
Iteration:     20, Loss function: 20.678, Average Loss: 0.443, avg. samples / sec: 37999.25
Iteration:     20, Loss function: 20.369, Average Loss: 0.441, avg. samples / sec: 37605.24
Iteration:     20, Loss function: 20.580, Average Loss: 0.441, avg. samples / sec: 37658.80
Iteration:     20, Loss function: 20.617, Average Loss: 0.444, avg. samples / sec: 37043.58
Iteration:     20, Loss function: 20.349, Average Loss: 0.443, avg. samples / sec: 36337.68
Iteration:     20, Loss function: 20.961, Average Loss: 0.442, avg. samples / sec: 36901.13
Iteration:     20, Loss function: 20.578, Average Loss: 0.444, avg. samples / sec: 37106.39
Iteration:     40, Loss function: 16.876, Average Loss: 0.827, avg. samples / sec: 54368.85
Iteration:     40, Loss function: 17.254, Average Loss: 0.823, avg. samples / sec: 55066.74
Iteration:     40, Loss function: 16.921, Average Loss: 0.826, avg. samples / sec: 55183.95
Iteration:     40, Loss function: 17.064, Average Loss: 0.826, avg. samples / sec: 54631.99
Iteration:     40, Loss function: 17.184, Average Loss: 0.828, avg. samples / sec: 56499.47
Iteration:     40, Loss function: 18.058, Average Loss: 0.831, avg. samples / sec: 53993.78
Iteration:     40, Loss function: 17.555, Average Loss: 0.827, avg. samples / sec: 54272.80
Iteration:     40, Loss function: 17.171, Average Loss: 0.828, avg. samples / sec: 54390.14
Iteration:     40, Loss function: 17.732, Average Loss: 0.830, avg. samples / sec: 55366.37
Iteration:     40, Loss function: 18.751, Average Loss: 0.833, avg. samples / sec: 53867.10
Iteration:     40, Loss function: 17.099, Average Loss: 0.834, avg. samples / sec: 53899.09
Iteration:     40, Loss function: 17.336, Average Loss: 0.827, avg. samples / sec: 53612.01
Iteration:     40, Loss function: 17.326, Average Loss: 0.831, avg. samples / sec: 53546.91
Iteration:     40, Loss function: 17.553, Average Loss: 0.824, avg. samples / sec: 54073.62
Iteration:     40, Loss function: 17.063, Average Loss: 0.824, avg. samples / sec: 54836.31
Iteration:     60, Loss function: 11.321, Average Loss: 1.079, avg. samples / sec: 57042.82
Iteration:     60, Loss function: 12.406, Average Loss: 1.076, avg. samples / sec: 57269.22
Iteration:     60, Loss function: 11.980, Average Loss: 1.085, avg. samples / sec: 57280.72
Iteration:     60, Loss function: 11.639, Average Loss: 1.074, avg. samples / sec: 57562.46
Iteration:     60, Loss function: 11.581, Average Loss: 1.076, avg. samples / sec: 56532.88
Iteration:     60, Loss function: 13.409, Average Loss: 1.076, avg. samples / sec: 56598.61
Iteration:     60, Loss function: 11.964, Average Loss: 1.076, avg. samples / sec: 56508.85
Iteration:     60, Loss function: 12.906, Average Loss: 1.083, avg. samples / sec: 56438.79
Iteration:     60, Loss function: 11.286, Average Loss: 1.088, avg. samples / sec: 56913.58
Iteration:     60, Loss function: 12.295, Average Loss: 1.078, avg. samples / sec: 56645.56
Iteration:     60, Loss function: 12.911, Average Loss: 1.081, avg. samples / sec: 56015.03
Iteration:     60, Loss function: 12.009, Average Loss: 1.084, avg. samples / sec: 56340.26
Iteration:     60, Loss function: 12.182, Average Loss: 1.086, avg. samples / sec: 56128.14
Iteration:     60, Loss function: 13.879, Average Loss: 1.087, avg. samples / sec: 56009.76
Iteration:     60, Loss function: 11.697, Average Loss: 1.077, avg. samples / sec: 55762.12
:::MLL 1558638721.252 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 819}}
:::MLL 1558638721.252 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 673}}
Iteration:     80, Loss function: 10.120, Average Loss: 1.268, avg. samples / sec: 56981.21
Iteration:     80, Loss function: 9.599, Average Loss: 1.263, avg. samples / sec: 56881.40
Iteration:     80, Loss function: 9.482, Average Loss: 1.274, avg. samples / sec: 57505.48
Iteration:     80, Loss function: 10.110, Average Loss: 1.278, avg. samples / sec: 56840.72
Iteration:     80, Loss function: 9.695, Average Loss: 1.271, avg. samples / sec: 57101.82
Iteration:     80, Loss function: 9.423, Average Loss: 1.268, avg. samples / sec: 56681.31
Iteration:     80, Loss function: 10.463, Average Loss: 1.268, avg. samples / sec: 56842.47
Iteration:     80, Loss function: 9.806, Average Loss: 1.266, avg. samples / sec: 58783.87
Iteration:     80, Loss function: 9.989, Average Loss: 1.269, avg. samples / sec: 57127.10
Iteration:     80, Loss function: 9.499, Average Loss: 1.278, avg. samples / sec: 57344.02
Iteration:     80, Loss function: 9.975, Average Loss: 1.278, avg. samples / sec: 57053.74
Iteration:     80, Loss function: 9.313, Average Loss: 1.261, avg. samples / sec: 56354.00
Iteration:     80, Loss function: 10.090, Average Loss: 1.259, avg. samples / sec: 56531.04
Iteration:     80, Loss function: 10.251, Average Loss: 1.278, avg. samples / sec: 56571.76
Iteration:     80, Loss function: 10.104, Average Loss: 1.273, avg. samples / sec: 56463.28
Iteration:    100, Loss function: 9.191, Average Loss: 1.441, avg. samples / sec: 59043.87
Iteration:    100, Loss function: 10.638, Average Loss: 1.438, avg. samples / sec: 58733.20
Iteration:    100, Loss function: 9.412, Average Loss: 1.436, avg. samples / sec: 58610.09
Iteration:    100, Loss function: 9.644, Average Loss: 1.434, avg. samples / sec: 58620.45
Iteration:    100, Loss function: 8.779, Average Loss: 1.426, avg. samples / sec: 59149.06
Iteration:    100, Loss function: 9.230, Average Loss: 1.440, avg. samples / sec: 58805.28
Iteration:    100, Loss function: 9.503, Average Loss: 1.433, avg. samples / sec: 58661.84
Iteration:    100, Loss function: 10.010, Average Loss: 1.448, avg. samples / sec: 58811.68
Iteration:    100, Loss function: 9.508, Average Loss: 1.432, avg. samples / sec: 58677.76
Iteration:    100, Loss function: 9.457, Average Loss: 1.443, avg. samples / sec: 58539.13
Iteration:    100, Loss function: 9.645, Average Loss: 1.447, avg. samples / sec: 58915.34
Iteration:    100, Loss function: 9.119, Average Loss: 1.443, avg. samples / sec: 58841.17
Iteration:    100, Loss function: 9.436, Average Loss: 1.434, avg. samples / sec: 58267.64
Iteration:    100, Loss function: 10.358, Average Loss: 1.447, avg. samples / sec: 58191.39
Iteration:    100, Loss function: 9.427, Average Loss: 1.428, avg. samples / sec: 58397.39
Iteration:    120, Loss function: 9.239, Average Loss: 1.594, avg. samples / sec: 58379.05
Iteration:    120, Loss function: 8.898, Average Loss: 1.586, avg. samples / sec: 58723.39
Iteration:    120, Loss function: 9.086, Average Loss: 1.595, avg. samples / sec: 58176.26
Iteration:    120, Loss function: 9.557, Average Loss: 1.597, avg. samples / sec: 58334.52
Iteration:    120, Loss function: 10.108, Average Loss: 1.587, avg. samples / sec: 58269.37
Iteration:    120, Loss function: 9.086, Average Loss: 1.599, avg. samples / sec: 58655.42
Iteration:    120, Loss function: 7.231, Average Loss: 1.580, avg. samples / sec: 58268.63
Iteration:    120, Loss function: 9.974, Average Loss: 1.590, avg. samples / sec: 58169.90
Iteration:    120, Loss function: 9.182, Average Loss: 1.597, avg. samples / sec: 58433.35
Iteration:    120, Loss function: 9.417, Average Loss: 1.583, avg. samples / sec: 58156.02
Iteration:    120, Loss function: 8.831, Average Loss: 1.606, avg. samples / sec: 58165.50
Iteration:    120, Loss function: 9.405, Average Loss: 1.583, avg. samples / sec: 58025.33
Iteration:    120, Loss function: 9.350, Average Loss: 1.588, avg. samples / sec: 57964.90
Iteration:    120, Loss function: 9.800, Average Loss: 1.578, avg. samples / sec: 58700.44
Iteration:    120, Loss function: 8.486, Average Loss: 1.598, avg. samples / sec: 58260.63
:::MLL 1558638723.272 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 819}}
:::MLL 1558638723.272 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 673}}
Iteration:    140, Loss function: 9.000, Average Loss: 1.743, avg. samples / sec: 58422.64
Iteration:    140, Loss function: 9.071, Average Loss: 1.740, avg. samples / sec: 58523.67
Iteration:    140, Loss function: 9.127, Average Loss: 1.725, avg. samples / sec: 58807.78
Iteration:    140, Loss function: 8.872, Average Loss: 1.745, avg. samples / sec: 58910.91
Iteration:    140, Loss function: 9.767, Average Loss: 1.737, avg. samples / sec: 58506.88
Iteration:    140, Loss function: 8.854, Average Loss: 1.735, avg. samples / sec: 58421.99
Iteration:    140, Loss function: 9.408, Average Loss: 1.733, avg. samples / sec: 58568.27
Iteration:    140, Loss function: 8.091, Average Loss: 1.731, avg. samples / sec: 58545.21
Iteration:    140, Loss function: 9.315, Average Loss: 1.742, avg. samples / sec: 58367.84
Iteration:    140, Loss function: 7.555, Average Loss: 1.741, avg. samples / sec: 58265.54
Iteration:    140, Loss function: 9.612, Average Loss: 1.740, avg. samples / sec: 58312.43
Iteration:    140, Loss function: 9.960, Average Loss: 1.729, avg. samples / sec: 58210.72
Iteration:    140, Loss function: 9.106, Average Loss: 1.754, avg. samples / sec: 58273.88
Iteration:    140, Loss function: 9.439, Average Loss: 1.736, avg. samples / sec: 58074.56
Iteration:    140, Loss function: 8.802, Average Loss: 1.733, avg. samples / sec: 57921.02
Iteration:    160, Loss function: 8.510, Average Loss: 1.899, avg. samples / sec: 55671.52
Iteration:    160, Loss function: 8.526, Average Loss: 1.877, avg. samples / sec: 55515.79
Iteration:    160, Loss function: 8.777, Average Loss: 1.877, avg. samples / sec: 55731.96
Iteration:    160, Loss function: 8.343, Average Loss: 1.885, avg. samples / sec: 55294.25
Iteration:    160, Loss function: 8.423, Average Loss: 1.880, avg. samples / sec: 55449.11
Iteration:    160, Loss function: 9.204, Average Loss: 1.873, avg. samples / sec: 55552.58
Iteration:    160, Loss function: 8.812, Average Loss: 1.864, avg. samples / sec: 55212.02
Iteration:    160, Loss function: 7.953, Average Loss: 1.880, avg. samples / sec: 55200.92
Iteration:    160, Loss function: 7.452, Average Loss: 1.871, avg. samples / sec: 55316.47
Iteration:    160, Loss function: 8.532, Average Loss: 1.886, avg. samples / sec: 55338.41
Iteration:    160, Loss function: 8.180, Average Loss: 1.873, avg. samples / sec: 55184.64
Iteration:    160, Loss function: 8.449, Average Loss: 1.874, avg. samples / sec: 55298.59
Iteration:    160, Loss function: 8.581, Average Loss: 1.884, avg. samples / sec: 55091.34
Iteration:    160, Loss function: 7.635, Average Loss: 1.871, avg. samples / sec: 55583.67
Iteration:    160, Loss function: 8.576, Average Loss: 1.877, avg. samples / sec: 54846.32
Iteration:    180, Loss function: 8.128, Average Loss: 2.006, avg. samples / sec: 58363.68
Iteration:    180, Loss function: 8.913, Average Loss: 2.009, avg. samples / sec: 58710.42
Iteration:    180, Loss function: 8.180, Average Loss: 2.007, avg. samples / sec: 58207.66
Iteration:    180, Loss function: 9.580, Average Loss: 2.014, avg. samples / sec: 58148.97
Iteration:    180, Loss function: 8.927, Average Loss: 2.016, avg. samples / sec: 58244.78
Iteration:    180, Loss function: 8.327, Average Loss: 2.010, avg. samples / sec: 58236.19
Iteration:    180, Loss function: 9.262, Average Loss: 2.002, avg. samples / sec: 58389.62
Iteration:    180, Loss function: 8.240, Average Loss: 2.009, avg. samples / sec: 58046.67
Iteration:    180, Loss function: 8.760, Average Loss: 1.996, avg. samples / sec: 58082.99
Iteration:    180, Loss function: 8.315, Average Loss: 1.996, avg. samples / sec: 57960.32
Iteration:    180, Loss function: 9.031, Average Loss: 2.003, avg. samples / sec: 58025.95
Iteration:    180, Loss function: 9.405, Average Loss: 2.030, avg. samples / sec: 57799.91
Iteration:    180, Loss function: 8.869, Average Loss: 2.008, avg. samples / sec: 57824.84
Iteration:    180, Loss function: 9.252, Average Loss: 2.004, avg. samples / sec: 57924.26
Iteration:    180, Loss function: 9.240, Average Loss: 2.000, avg. samples / sec: 57811.27
Iteration:    200, Loss function: 7.876, Average Loss: 2.142, avg. samples / sec: 59381.69
Iteration:    200, Loss function: 8.114, Average Loss: 2.135, avg. samples / sec: 59465.86
Iteration:    200, Loss function: 7.739, Average Loss: 2.130, avg. samples / sec: 59046.74
Iteration:    200, Loss function: 7.739, Average Loss: 2.130, avg. samples / sec: 59641.04
Iteration:    200, Loss function: 8.578, Average Loss: 2.134, avg. samples / sec: 59556.23
Iteration:    200, Loss function: 8.900, Average Loss: 2.135, avg. samples / sec: 59185.60
Iteration:    200, Loss function: 8.324, Average Loss: 2.127, avg. samples / sec: 59496.44
Iteration:    200, Loss function: 8.440, Average Loss: 2.156, avg. samples / sec: 59495.98
Iteration:    200, Loss function: 8.022, Average Loss: 2.132, avg. samples / sec: 59604.39
Iteration:    200, Loss function: 7.623, Average Loss: 2.120, avg. samples / sec: 59404.80
Iteration:    200, Loss function: 8.219, Average Loss: 2.127, avg. samples / sec: 59320.95
Iteration:    200, Loss function: 8.032, Average Loss: 2.140, avg. samples / sec: 59152.12
Iteration:    200, Loss function: 8.754, Average Loss: 2.136, avg. samples / sec: 59053.56
Iteration:    200, Loss function: 8.471, Average Loss: 2.138, avg. samples / sec: 59259.69
Iteration:    200, Loss function: 7.768, Average Loss: 2.129, avg. samples / sec: 59010.07
:::MLL 1558638725.307 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 819}}
:::MLL 1558638725.308 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 673}}
Iteration:    220, Loss function: 8.389, Average Loss: 2.251, avg. samples / sec: 58983.32
Iteration:    220, Loss function: 8.666, Average Loss: 2.238, avg. samples / sec: 58924.45
Iteration:    220, Loss function: 7.767, Average Loss: 2.244, avg. samples / sec: 58862.80
Iteration:    220, Loss function: 8.111, Average Loss: 2.248, avg. samples / sec: 58864.23
Iteration:    220, Loss function: 7.703, Average Loss: 2.260, avg. samples / sec: 58625.67
Iteration:    220, Loss function: 7.935, Average Loss: 2.250, avg. samples / sec: 58793.65
Iteration:    220, Loss function: 7.113, Average Loss: 2.247, avg. samples / sec: 58692.21
Iteration:    220, Loss function: 7.780, Average Loss: 2.250, avg. samples / sec: 58544.48
Iteration:    220, Loss function: 7.240, Average Loss: 2.255, avg. samples / sec: 58800.35
Iteration:    220, Loss function: 7.738, Average Loss: 2.244, avg. samples / sec: 58767.96
Iteration:    220, Loss function: 7.508, Average Loss: 2.245, avg. samples / sec: 58634.77
Iteration:    220, Loss function: 8.289, Average Loss: 2.248, avg. samples / sec: 58537.40
Iteration:    220, Loss function: 8.715, Average Loss: 2.253, avg. samples / sec: 58741.50
Iteration:    220, Loss function: 8.734, Average Loss: 2.278, avg. samples / sec: 58625.11
Iteration:    220, Loss function: 7.741, Average Loss: 2.242, avg. samples / sec: 58654.49
Iteration:    240, Loss function: 8.109, Average Loss: 2.363, avg. samples / sec: 58797.77
Iteration:    240, Loss function: 8.828, Average Loss: 2.376, avg. samples / sec: 58547.05
Iteration:    240, Loss function: 8.269, Average Loss: 2.358, avg. samples / sec: 58753.04
Iteration:    240, Loss function: 7.004, Average Loss: 2.392, avg. samples / sec: 58781.61
Iteration:    240, Loss function: 8.744, Average Loss: 2.370, avg. samples / sec: 58656.17
Iteration:    240, Loss function: 8.278, Average Loss: 2.369, avg. samples / sec: 58636.55
Iteration:    240, Loss function: 8.787, Average Loss: 2.362, avg. samples / sec: 58974.24
Iteration:    240, Loss function: 8.938, Average Loss: 2.366, avg. samples / sec: 58485.95
Iteration:    240, Loss function: 8.584, Average Loss: 2.357, avg. samples / sec: 58447.72
Iteration:    240, Loss function: 7.995, Average Loss: 2.375, avg. samples / sec: 58504.35
Iteration:    240, Loss function: 8.279, Average Loss: 2.364, avg. samples / sec: 58646.19
Iteration:    240, Loss function: 8.002, Average Loss: 2.374, avg. samples / sec: 58537.74
Iteration:    240, Loss function: 8.307, Average Loss: 2.364, avg. samples / sec: 58357.37
Iteration:    240, Loss function: 8.323, Average Loss: 2.368, avg. samples / sec: 58587.68
Iteration:    240, Loss function: 8.643, Average Loss: 2.368, avg. samples / sec: 58372.65
Iteration:    260, Loss function: 7.682, Average Loss: 2.476, avg. samples / sec: 55774.52
Iteration:    260, Loss function: 7.192, Average Loss: 2.485, avg. samples / sec: 55768.41
Iteration:    260, Loss function: 7.020, Average Loss: 2.470, avg. samples / sec: 55574.31
Iteration:    260, Loss function: 7.361, Average Loss: 2.476, avg. samples / sec: 55766.18
Iteration:    260, Loss function: 6.918, Average Loss: 2.481, avg. samples / sec: 55643.45
Iteration:    260, Loss function: 7.563, Average Loss: 2.477, avg. samples / sec: 55749.02
Iteration:    260, Loss function: 8.311, Average Loss: 2.472, avg. samples / sec: 55511.64
Iteration:    260, Loss function: 7.107, Average Loss: 2.469, avg. samples / sec: 55643.14
Iteration:    260, Loss function: 8.609, Average Loss: 2.481, avg. samples / sec: 55457.14
Iteration:    260, Loss function: 6.702, Average Loss: 2.502, avg. samples / sec: 55564.17
Iteration:    260, Loss function: 6.910, Average Loss: 2.486, avg. samples / sec: 55367.91
Iteration:    260, Loss function: 8.096, Average Loss: 2.481, avg. samples / sec: 55577.49
Iteration:    260, Loss function: 8.553, Average Loss: 2.474, avg. samples / sec: 55408.43
Iteration:    260, Loss function: 7.534, Average Loss: 2.470, avg. samples / sec: 55405.85
Iteration:    260, Loss function: 8.127, Average Loss: 2.488, avg. samples / sec: 55395.55
:::MLL 1558638727.339 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 819}}
:::MLL 1558638727.340 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 673}}
Iteration:    280, Loss function: 7.818, Average Loss: 2.580, avg. samples / sec: 59604.69
Iteration:    280, Loss function: 7.552, Average Loss: 2.605, avg. samples / sec: 59578.31
Iteration:    280, Loss function: 8.948, Average Loss: 2.576, avg. samples / sec: 59446.37
Iteration:    280, Loss function: 6.850, Average Loss: 2.568, avg. samples / sec: 59501.71
Iteration:    280, Loss function: 7.240, Average Loss: 2.568, avg. samples / sec: 59766.92
Iteration:    280, Loss function: 7.249, Average Loss: 2.573, avg. samples / sec: 59474.37
Iteration:    280, Loss function: 7.316, Average Loss: 2.587, avg. samples / sec: 59691.56
Iteration:    280, Loss function: 7.780, Average Loss: 2.592, avg. samples / sec: 59781.22
Iteration:    280, Loss function: 7.174, Average Loss: 2.576, avg. samples / sec: 59601.74
Iteration:    280, Loss function: 8.804, Average Loss: 2.578, avg. samples / sec: 59389.20
Iteration:    280, Loss function: 7.590, Average Loss: 2.571, avg. samples / sec: 59422.16
Iteration:    280, Loss function: 7.589, Average Loss: 2.583, avg. samples / sec: 59634.28
Iteration:    280, Loss function: 6.987, Average Loss: 2.575, avg. samples / sec: 59435.56
Iteration:    280, Loss function: 7.894, Average Loss: 2.582, avg. samples / sec: 59343.94
Iteration:    280, Loss function: 7.462, Average Loss: 2.581, avg. samples / sec: 59099.78
Iteration:    300, Loss function: 6.220, Average Loss: 2.674, avg. samples / sec: 60140.47
Iteration:    300, Loss function: 8.215, Average Loss: 2.669, avg. samples / sec: 59641.11
Iteration:    300, Loss function: 7.348, Average Loss: 2.681, avg. samples / sec: 59681.17
Iteration:    300, Loss function: 6.987, Average Loss: 2.664, avg. samples / sec: 59645.20
Iteration:    300, Loss function: 7.674, Average Loss: 2.703, avg. samples / sec: 59550.72
Iteration:    300, Loss function: 6.522, Average Loss: 2.674, avg. samples / sec: 59936.11
Iteration:    300, Loss function: 7.839, Average Loss: 2.669, avg. samples / sec: 59620.48
Iteration:    300, Loss function: 7.005, Average Loss: 2.666, avg. samples / sec: 59664.98
Iteration:    300, Loss function: 8.223, Average Loss: 2.674, avg. samples / sec: 59672.25
Iteration:    300, Loss function: 7.196, Average Loss: 2.672, avg. samples / sec: 59409.13
Iteration:    300, Loss function: 7.820, Average Loss: 2.671, avg. samples / sec: 59870.54
Iteration:    300, Loss function: 7.073, Average Loss: 2.675, avg. samples / sec: 59637.76
Iteration:    300, Loss function: 8.520, Average Loss: 2.692, avg. samples / sec: 59530.19
Iteration:    300, Loss function: 7.398, Average Loss: 2.669, avg. samples / sec: 59451.61
Iteration:    300, Loss function: 6.988, Average Loss: 2.663, avg. samples / sec: 59370.76
Iteration:    320, Loss function: 7.836, Average Loss: 2.758, avg. samples / sec: 57572.97
Iteration:    320, Loss function: 6.767, Average Loss: 2.764, avg. samples / sec: 57624.13
Iteration:    320, Loss function: 7.349, Average Loss: 2.759, avg. samples / sec: 57438.54
Iteration:    320, Loss function: 7.259, Average Loss: 2.781, avg. samples / sec: 57665.74
Iteration:    320, Loss function: 7.140, Average Loss: 2.774, avg. samples / sec: 57491.31
Iteration:    320, Loss function: 7.100, Average Loss: 2.752, avg. samples / sec: 57719.83
Iteration:    320, Loss function: 8.347, Average Loss: 2.757, avg. samples / sec: 57435.10
Iteration:    320, Loss function: 6.451, Average Loss: 2.761, avg. samples / sec: 57478.20
Iteration:    320, Loss function: 7.936, Average Loss: 2.760, avg. samples / sec: 57497.71
Iteration:    320, Loss function: 6.501, Average Loss: 2.762, avg. samples / sec: 57470.44
Iteration:    320, Loss function: 7.205, Average Loss: 2.762, avg. samples / sec: 57418.65
Iteration:    320, Loss function: 7.259, Average Loss: 2.793, avg. samples / sec: 57286.17
Iteration:    320, Loss function: 7.659, Average Loss: 2.757, avg. samples / sec: 57453.78
Iteration:    320, Loss function: 7.249, Average Loss: 2.758, avg. samples / sec: 57310.58
Iteration:    320, Loss function: 7.826, Average Loss: 2.762, avg. samples / sec: 57159.61
Iteration:    340, Loss function: 8.797, Average Loss: 2.857, avg. samples / sec: 57813.76
Iteration:    340, Loss function: 7.177, Average Loss: 2.857, avg. samples / sec: 58108.35
Iteration:    340, Loss function: 7.865, Average Loss: 2.854, avg. samples / sec: 57910.43
Iteration:    340, Loss function: 9.158, Average Loss: 2.856, avg. samples / sec: 57584.80
Iteration:    340, Loss function: 7.596, Average Loss: 2.853, avg. samples / sec: 57598.15
Iteration:    340, Loss function: 8.233, Average Loss: 2.861, avg. samples / sec: 57706.99
Iteration:    340, Loss function: 9.139, Average Loss: 2.858, avg. samples / sec: 57626.48
Iteration:    340, Loss function: 8.000, Average Loss: 2.861, avg. samples / sec: 57651.63
Iteration:    340, Loss function: 7.572, Average Loss: 2.858, avg. samples / sec: 57714.48
Iteration:    340, Loss function: 7.955, Average Loss: 2.870, avg. samples / sec: 57572.76
Iteration:    340, Loss function: 8.146, Average Loss: 2.876, avg. samples / sec: 57527.38
Iteration:    340, Loss function: 7.595, Average Loss: 2.855, avg. samples / sec: 57820.66
Iteration:    340, Loss function: 7.810, Average Loss: 2.859, avg. samples / sec: 57493.58
Iteration:    340, Loss function: 8.228, Average Loss: 2.891, avg. samples / sec: 57785.74
Iteration:    340, Loss function: 8.339, Average Loss: 2.854, avg. samples / sec: 57438.05
:::MLL 1558638729.352 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 819}}
:::MLL 1558638729.353 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 673}}
Iteration:    360, Loss function: 6.569, Average Loss: 2.941, avg. samples / sec: 58947.38
Iteration:    360, Loss function: 6.560, Average Loss: 2.942, avg. samples / sec: 58706.90
Iteration:    360, Loss function: 7.480, Average Loss: 2.944, avg. samples / sec: 58673.10
Iteration:    360, Loss function: 7.841, Average Loss: 2.947, avg. samples / sec: 58675.61
Iteration:    360, Loss function: 8.130, Average Loss: 2.983, avg. samples / sec: 58729.92
Iteration:    360, Loss function: 7.689, Average Loss: 2.943, avg. samples / sec: 58706.17
Iteration:    360, Loss function: 6.704, Average Loss: 2.943, avg. samples / sec: 58537.11
Iteration:    360, Loss function: 8.113, Average Loss: 2.951, avg. samples / sec: 58588.19
Iteration:    360, Loss function: 6.895, Average Loss: 2.964, avg. samples / sec: 58533.39
Iteration:    360, Loss function: 7.370, Average Loss: 2.944, avg. samples / sec: 58388.56
Iteration:    360, Loss function: 6.207, Average Loss: 2.945, avg. samples / sec: 58342.42
Iteration:    360, Loss function: 8.065, Average Loss: 2.961, avg. samples / sec: 58480.02
Iteration:    360, Loss function: 7.644, Average Loss: 2.945, avg. samples / sec: 58358.77
Iteration:    360, Loss function: 6.681, Average Loss: 2.939, avg. samples / sec: 58380.94
Iteration:    360, Loss function: 4.832, Average Loss: 2.945, avg. samples / sec: 58396.08
Iteration:    380, Loss function: 6.424, Average Loss: 3.043, avg. samples / sec: 58040.82
Iteration:    380, Loss function: 7.354, Average Loss: 3.025, avg. samples / sec: 57874.04
Iteration:    380, Loss function: 6.651, Average Loss: 3.018, avg. samples / sec: 57952.17
Iteration:    380, Loss function: 7.323, Average Loss: 3.014, avg. samples / sec: 58078.78
Iteration:    380, Loss function: 7.655, Average Loss: 3.025, avg. samples / sec: 57858.57
Iteration:    380, Loss function: 7.265, Average Loss: 3.024, avg. samples / sec: 58044.19
Iteration:    380, Loss function: 7.421, Average Loss: 3.064, avg. samples / sec: 57826.88
Iteration:    380, Loss function: 6.945, Average Loss: 3.028, avg. samples / sec: 57948.93
Iteration:    380, Loss function: 7.705, Average Loss: 3.021, avg. samples / sec: 57751.45
Iteration:    380, Loss function: 7.193, Average Loss: 3.028, avg. samples / sec: 57742.86
Iteration:    380, Loss function: 6.922, Average Loss: 3.025, avg. samples / sec: 57955.79
Iteration:    380, Loss function: 5.951, Average Loss: 3.021, avg. samples / sec: 57930.83
Iteration:    380, Loss function: 6.695, Average Loss: 3.025, avg. samples / sec: 57572.34
Iteration:    380, Loss function: 6.977, Average Loss: 3.038, avg. samples / sec: 57847.13
Iteration:    380, Loss function: 6.538, Average Loss: 3.021, avg. samples / sec: 57858.98
Iteration:    400, Loss function: 7.051, Average Loss: 3.095, avg. samples / sec: 56627.38
Iteration:    400, Loss function: 6.969, Average Loss: 3.105, avg. samples / sec: 56619.46
Iteration:    400, Loss function: 5.887, Average Loss: 3.105, avg. samples / sec: 56597.31
Iteration:    400, Loss function: 6.857, Average Loss: 3.144, avg. samples / sec: 56575.50
Iteration:    400, Loss function: 7.298, Average Loss: 3.098, avg. samples / sec: 56795.25
Iteration:    400, Loss function: 7.905, Average Loss: 3.103, avg. samples / sec: 56627.04
Iteration:    400, Loss function: 7.090, Average Loss: 3.116, avg. samples / sec: 56509.26
Iteration:    400, Loss function: 7.599, Average Loss: 3.122, avg. samples / sec: 56727.10
Iteration:    400, Loss function: 7.192, Average Loss: 3.103, avg. samples / sec: 56591.66
Iteration:    400, Loss function: 6.697, Average Loss: 3.099, avg. samples / sec: 56610.00
Iteration:    400, Loss function: 6.232, Average Loss: 3.101, avg. samples / sec: 56461.11
Iteration:    400, Loss function: 6.372, Average Loss: 3.099, avg. samples / sec: 56653.10
Iteration:    400, Loss function: 6.531, Average Loss: 3.102, avg. samples / sec: 56570.28
Iteration:    400, Loss function: 8.085, Average Loss: 3.102, avg. samples / sec: 56466.92
Iteration:    400, Loss function: 6.322, Average Loss: 3.088, avg. samples / sec: 56220.39
:::MLL 1558638731.386 epoch_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 819}}
:::MLL 1558638731.386 epoch_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 673}}
Iteration:    420, Loss function: 7.375, Average Loss: 3.168, avg. samples / sec: 59301.51
Iteration:    420, Loss function: 7.027, Average Loss: 3.179, avg. samples / sec: 59287.44
Iteration:    420, Loss function: 7.763, Average Loss: 3.175, avg. samples / sec: 59379.69
Iteration:    420, Loss function: 5.627, Average Loss: 3.171, avg. samples / sec: 59342.49
Iteration:    420, Loss function: 7.899, Average Loss: 3.173, avg. samples / sec: 59300.46
Iteration:    420, Loss function: 6.104, Average Loss: 3.174, avg. samples / sec: 59297.37
Iteration:    420, Loss function: 6.569, Average Loss: 3.176, avg. samples / sec: 59126.83
Iteration:    420, Loss function: 5.731, Average Loss: 3.184, avg. samples / sec: 59216.04
Iteration:    420, Loss function: 5.764, Average Loss: 3.222, avg. samples / sec: 59163.79
Iteration:    420, Loss function: 6.591, Average Loss: 3.172, avg. samples / sec: 59222.46
Iteration:    420, Loss function: 7.407, Average Loss: 3.169, avg. samples / sec: 59058.64
Iteration:    420, Loss function: 6.549, Average Loss: 3.160, avg. samples / sec: 59467.54
Iteration:    420, Loss function: 5.784, Average Loss: 3.177, avg. samples / sec: 59204.65
Iteration:    420, Loss function: 7.694, Average Loss: 3.178, avg. samples / sec: 59062.28
Iteration:    420, Loss function: 6.304, Average Loss: 3.193, avg. samples / sec: 59023.54
Iteration:    440, Loss function: 7.770, Average Loss: 3.253, avg. samples / sec: 57119.55
Iteration:    440, Loss function: 6.340, Average Loss: 3.240, avg. samples / sec: 57012.13
Iteration:    440, Loss function: 7.362, Average Loss: 3.244, avg. samples / sec: 56990.36
Iteration:    440, Loss function: 7.912, Average Loss: 3.249, avg. samples / sec: 57097.61
Iteration:    440, Loss function: 6.846, Average Loss: 3.240, avg. samples / sec: 57007.86
Iteration:    440, Loss function: 8.870, Average Loss: 3.246, avg. samples / sec: 56881.24
Iteration:    440, Loss function: 7.345, Average Loss: 3.252, avg. samples / sec: 56924.64
Iteration:    440, Loss function: 6.180, Average Loss: 3.243, avg. samples / sec: 56828.78
Iteration:    440, Loss function: 7.125, Average Loss: 3.226, avg. samples / sec: 56978.01
Iteration:    440, Loss function: 6.554, Average Loss: 3.262, avg. samples / sec: 57032.06
Iteration:    440, Loss function: 6.690, Average Loss: 3.291, avg. samples / sec: 56865.26
Iteration:    440, Loss function: 5.718, Average Loss: 3.238, avg. samples / sec: 56797.95
Iteration:    440, Loss function: 8.018, Average Loss: 3.241, avg. samples / sec: 56648.07
Iteration:    440, Loss function: 6.064, Average Loss: 3.239, avg. samples / sec: 56659.48
Iteration:    440, Loss function: 7.590, Average Loss: 3.249, avg. samples / sec: 56743.36
Iteration:    460, Loss function: 7.402, Average Loss: 3.324, avg. samples / sec: 57168.56
Iteration:    460, Loss function: 7.469, Average Loss: 3.377, avg. samples / sec: 57067.37
Iteration:    460, Loss function: 7.226, Average Loss: 3.348, avg. samples / sec: 57050.00
Iteration:    460, Loss function: 6.954, Average Loss: 3.332, avg. samples / sec: 56928.32
Iteration:    460, Loss function: 6.878, Average Loss: 3.317, avg. samples / sec: 57190.90
Iteration:    460, Loss function: 7.466, Average Loss: 3.336, avg. samples / sec: 56737.47
Iteration:    460, Loss function: 7.580, Average Loss: 3.337, avg. samples / sec: 56916.52
Iteration:    460, Loss function: 6.177, Average Loss: 3.324, avg. samples / sec: 57120.55
Iteration:    460, Loss function: 6.760, Average Loss: 3.304, avg. samples / sec: 56952.41
Iteration:    460, Loss function: 6.800, Average Loss: 3.335, avg. samples / sec: 56831.42
Iteration:    460, Loss function: 5.469, Average Loss: 3.320, avg. samples / sec: 56861.85
Iteration:    460, Loss function: 6.661, Average Loss: 3.327, avg. samples / sec: 56818.36
Iteration:    460, Loss function: 7.487, Average Loss: 3.325, avg. samples / sec: 56918.27
Iteration:    460, Loss function: 6.513, Average Loss: 3.330, avg. samples / sec: 57183.29
Iteration:    460, Loss function: 6.629, Average Loss: 3.325, avg. samples / sec: 56705.83
Iteration:    480, Loss function: 5.950, Average Loss: 3.405, avg. samples / sec: 57645.50
Iteration:    480, Loss function: 6.736, Average Loss: 3.418, avg. samples / sec: 57578.36
Iteration:    480, Loss function: 5.551, Average Loss: 3.389, avg. samples / sec: 57586.33
Iteration:    480, Loss function: 6.824, Average Loss: 3.383, avg. samples / sec: 57533.74
Iteration:    480, Loss function: 6.857, Average Loss: 3.397, avg. samples / sec: 57540.93
Iteration:    480, Loss function: 6.069, Average Loss: 3.442, avg. samples / sec: 57448.65
Iteration:    480, Loss function: 6.472, Average Loss: 3.373, avg. samples / sec: 57491.14
Iteration:    480, Loss function: 6.634, Average Loss: 3.389, avg. samples / sec: 57507.54
Iteration:    480, Loss function: 7.055, Average Loss: 3.396, avg. samples / sec: 57480.47
Iteration:    480, Loss function: 7.120, Average Loss: 3.391, avg. samples / sec: 57352.35
Iteration:    480, Loss function: 5.936, Average Loss: 3.393, avg. samples / sec: 57428.54
Iteration:    480, Loss function: 7.118, Average Loss: 3.402, avg. samples / sec: 57380.35
Iteration:    480, Loss function: 7.836, Average Loss: 3.406, avg. samples / sec: 57371.20
Iteration:    480, Loss function: 5.912, Average Loss: 3.396, avg. samples / sec: 57289.64
Iteration:    480, Loss function: 6.899, Average Loss: 3.395, avg. samples / sec: 57347.41
:::MLL 1558638733.431 epoch_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 819}}
:::MLL 1558638733.431 epoch_start: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 673}}
Iteration:    500, Loss function: 6.410, Average Loss: 3.452, avg. samples / sec: 59620.15
Iteration:    500, Loss function: 7.278, Average Loss: 3.446, avg. samples / sec: 59611.02
Iteration:    500, Loss function: 6.796, Average Loss: 3.432, avg. samples / sec: 59518.50
Iteration:    500, Loss function: 6.972, Average Loss: 3.461, avg. samples / sec: 59323.58
Iteration:    500, Loss function: 6.200, Average Loss: 3.456, avg. samples / sec: 59724.09
Iteration:    500, Loss function: 5.848, Average Loss: 3.436, avg. samples / sec: 59365.63
Iteration:    500, Loss function: 5.885, Average Loss: 3.455, avg. samples / sec: 59363.01
Iteration:    500, Loss function: 6.605, Average Loss: 3.454, avg. samples / sec: 59449.68
Iteration:    500, Loss function: 6.551, Average Loss: 3.444, avg. samples / sec: 59271.26
Iteration:    500, Loss function: 5.781, Average Loss: 3.463, avg. samples / sec: 59487.55
Iteration:    500, Loss function: 6.400, Average Loss: 3.457, avg. samples / sec: 59610.04
Iteration:    500, Loss function: 5.916, Average Loss: 3.502, avg. samples / sec: 59337.81
Iteration:    500, Loss function: 5.927, Average Loss: 3.448, avg. samples / sec: 59405.50
Iteration:    500, Loss function: 6.255, Average Loss: 3.476, avg. samples / sec: 59155.77
Iteration:    500, Loss function: 5.246, Average Loss: 3.460, avg. samples / sec: 59175.34
Iteration:    520, Loss function: 7.627, Average Loss: 3.524, avg. samples / sec: 59039.34
Iteration:    520, Loss function: 6.966, Average Loss: 3.495, avg. samples / sec: 59048.71
Iteration:    520, Loss function: 5.697, Average Loss: 3.505, avg. samples / sec: 59143.87
Iteration:    520, Loss function: 6.507, Average Loss: 3.535, avg. samples / sec: 59179.34
Iteration:    520, Loss function: 6.249, Average Loss: 3.511, avg. samples / sec: 59067.90
Iteration:    520, Loss function: 6.088, Average Loss: 3.511, avg. samples / sec: 58842.80
Iteration:    520, Loss function: 6.422, Average Loss: 3.518, avg. samples / sec: 59351.56
Iteration:    520, Loss function: 5.784, Average Loss: 3.500, avg. samples / sec: 59059.33
Iteration:    520, Loss function: 6.359, Average Loss: 3.515, avg. samples / sec: 58962.62
Iteration:    520, Loss function: 5.504, Average Loss: 3.524, avg. samples / sec: 59048.15
Iteration:    520, Loss function: 5.398, Average Loss: 3.553, avg. samples / sec: 59046.41
Iteration:    520, Loss function: 6.383, Average Loss: 3.512, avg. samples / sec: 59041.10
Iteration:    520, Loss function: 6.761, Average Loss: 3.500, avg. samples / sec: 58845.40
Iteration:    520, Loss function: 5.801, Average Loss: 3.485, avg. samples / sec: 58828.38
Iteration:    520, Loss function: 6.142, Average Loss: 3.515, avg. samples / sec: 58928.91
Iteration:    540, Loss function: 6.218, Average Loss: 3.577, avg. samples / sec: 58133.04
Iteration:    540, Loss function: 6.150, Average Loss: 3.554, avg. samples / sec: 58177.63
Iteration:    540, Loss function: 6.875, Average Loss: 3.552, avg. samples / sec: 58115.52
Iteration:    540, Loss function: 7.221, Average Loss: 3.573, avg. samples / sec: 58128.94
Iteration:    540, Loss function: 6.387, Average Loss: 3.553, avg. samples / sec: 58105.67
Iteration:    540, Loss function: 6.610, Average Loss: 3.582, avg. samples / sec: 58085.43
Iteration:    540, Loss function: 5.636, Average Loss: 3.567, avg. samples / sec: 58083.64
Iteration:    540, Loss function: 6.084, Average Loss: 3.602, avg. samples / sec: 58084.66
Iteration:    540, Loss function: 7.008, Average Loss: 3.589, avg. samples / sec: 58014.13
Iteration:    540, Loss function: 5.959, Average Loss: 3.567, avg. samples / sec: 58034.51
Iteration:    540, Loss function: 6.685, Average Loss: 3.536, avg. samples / sec: 58086.29
Iteration:    540, Loss function: 6.822, Average Loss: 3.555, avg. samples / sec: 57961.32
Iteration:    540, Loss function: 5.684, Average Loss: 3.563, avg. samples / sec: 58073.44
Iteration:    540, Loss function: 7.391, Average Loss: 3.566, avg. samples / sec: 57939.47
Iteration:    540, Loss function: 5.692, Average Loss: 3.563, avg. samples / sec: 57896.63
:::MLL 1558638735.427 epoch_stop: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 819}}
:::MLL 1558638735.428 epoch_start: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 673}}
Iteration:    560, Loss function: 7.091, Average Loss: 3.609, avg. samples / sec: 59663.56
Iteration:    560, Loss function: 5.832, Average Loss: 3.613, avg. samples / sec: 59861.31
Iteration:    560, Loss function: 5.976, Average Loss: 3.623, avg. samples / sec: 59608.04
Iteration:    560, Loss function: 5.437, Average Loss: 3.607, avg. samples / sec: 59891.05
Iteration:    560, Loss function: 6.741, Average Loss: 3.601, avg. samples / sec: 59725.23
Iteration:    560, Loss function: 5.702, Average Loss: 3.622, avg. samples / sec: 59744.17
Iteration:    560, Loss function: 5.210, Average Loss: 3.633, avg. samples / sec: 59727.79
Iteration:    560, Loss function: 5.740, Average Loss: 3.652, avg. samples / sec: 59719.69
Iteration:    560, Loss function: 5.523, Average Loss: 3.610, avg. samples / sec: 59555.57
Iteration:    560, Loss function: 5.598, Average Loss: 3.627, avg. samples / sec: 59599.67
Iteration:    560, Loss function: 5.708, Average Loss: 3.641, avg. samples / sec: 59664.62
Iteration:    560, Loss function: 6.136, Average Loss: 3.619, avg. samples / sec: 59699.27
Iteration:    560, Loss function: 5.461, Average Loss: 3.608, avg. samples / sec: 59667.00
Iteration:    560, Loss function: 5.054, Average Loss: 3.614, avg. samples / sec: 59597.91
Iteration:    560, Loss function: 6.858, Average Loss: 3.587, avg. samples / sec: 59484.08
Iteration:    580, Loss function: 6.850, Average Loss: 3.653, avg. samples / sec: 58639.99
Iteration:    580, Loss function: 5.873, Average Loss: 3.657, avg. samples / sec: 58740.47
Iteration:    580, Loss function: 6.181, Average Loss: 3.672, avg. samples / sec: 58553.79
Iteration:    580, Loss function: 6.582, Average Loss: 3.681, avg. samples / sec: 58579.06
Iteration:    580, Loss function: 7.423, Average Loss: 3.662, avg. samples / sec: 58552.21
Iteration:    580, Loss function: 5.575, Average Loss: 3.660, avg. samples / sec: 58518.32
Iteration:    580, Loss function: 5.960, Average Loss: 3.652, avg. samples / sec: 58534.21
Iteration:    580, Loss function: 5.325, Average Loss: 3.687, avg. samples / sec: 58630.70
Iteration:    580, Loss function: 5.675, Average Loss: 3.668, avg. samples / sec: 58521.36
Iteration:    580, Loss function: 4.691, Average Loss: 3.678, avg. samples / sec: 58592.31
Iteration:    580, Loss function: 6.300, Average Loss: 3.662, avg. samples / sec: 58542.55
Iteration:    580, Loss function: 5.690, Average Loss: 3.631, avg. samples / sec: 58753.97
Iteration:    580, Loss function: 5.574, Average Loss: 3.658, avg. samples / sec: 58489.03
Iteration:    580, Loss function: 5.896, Average Loss: 3.662, avg. samples / sec: 58598.86
Iteration:    580, Loss function: 7.272, Average Loss: 3.700, avg. samples / sec: 58340.15
Iteration:    600, Loss function: 6.440, Average Loss: 3.702, avg. samples / sec: 57363.47
Iteration:    600, Loss function: 5.144, Average Loss: 3.728, avg. samples / sec: 57424.52
Iteration:    600, Loss function: 6.872, Average Loss: 3.732, avg. samples / sec: 57395.99
Iteration:    600, Loss function: 6.017, Average Loss: 3.681, avg. samples / sec: 57444.37
Iteration:    600, Loss function: 5.954, Average Loss: 3.701, avg. samples / sec: 57319.11
Iteration:    600, Loss function: 6.159, Average Loss: 3.705, avg. samples / sec: 57418.39
Iteration:    600, Loss function: 6.179, Average Loss: 3.723, avg. samples / sec: 57198.33
Iteration:    600, Loss function: 5.432, Average Loss: 3.712, avg. samples / sec: 57374.51
Iteration:    600, Loss function: 6.525, Average Loss: 3.750, avg. samples / sec: 57452.61
Iteration:    600, Loss function: 7.049, Average Loss: 3.706, avg. samples / sec: 57160.67
Iteration:    600, Loss function: 7.223, Average Loss: 3.713, avg. samples / sec: 57282.84
Iteration:    600, Loss function: 5.439, Average Loss: 3.717, avg. samples / sec: 57226.59
Iteration:    600, Loss function: 6.071, Average Loss: 3.733, avg. samples / sec: 57132.24
Iteration:    600, Loss function: 6.706, Average Loss: 3.713, avg. samples / sec: 57127.56
Iteration:    600, Loss function: 6.576, Average Loss: 3.710, avg. samples / sec: 57098.68
Iteration:    620, Loss function: 5.330, Average Loss: 3.757, avg. samples / sec: 59342.59
Iteration:    620, Loss function: 5.462, Average Loss: 3.771, avg. samples / sec: 59365.13
Iteration:    620, Loss function: 6.299, Average Loss: 3.743, avg. samples / sec: 59038.05
Iteration:    620, Loss function: 5.894, Average Loss: 3.745, avg. samples / sec: 59179.74
Iteration:    620, Loss function: 4.869, Average Loss: 3.753, avg. samples / sec: 59375.64
Iteration:    620, Loss function: 6.419, Average Loss: 3.725, avg. samples / sec: 59086.72
Iteration:    620, Loss function: 7.815, Average Loss: 3.795, avg. samples / sec: 59219.97
Iteration:    620, Loss function: 6.919, Average Loss: 3.775, avg. samples / sec: 59026.41
Iteration:    620, Loss function: 5.371, Average Loss: 3.774, avg. samples / sec: 58947.45
Iteration:    620, Loss function: 6.316, Average Loss: 3.763, avg. samples / sec: 59141.49
Iteration:    620, Loss function: 6.668, Average Loss: 3.761, avg. samples / sec: 59159.24
Iteration:    620, Loss function: 6.227, Average Loss: 3.770, avg. samples / sec: 59056.31
Iteration:    620, Loss function: 5.830, Average Loss: 3.752, avg. samples / sec: 59080.05
Iteration:    620, Loss function: 6.593, Average Loss: 3.750, avg. samples / sec: 58933.28
Iteration:    620, Loss function: 6.233, Average Loss: 3.752, avg. samples / sec: 58955.29
:::MLL 1558638737.436 epoch_stop: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 819}}
:::MLL 1558638737.436 epoch_start: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 673}}
Iteration:    640, Loss function: 6.345, Average Loss: 3.833, avg. samples / sec: 57894.58
Iteration:    640, Loss function: 5.547, Average Loss: 3.812, avg. samples / sec: 57986.80
Iteration:    640, Loss function: 6.714, Average Loss: 3.806, avg. samples / sec: 57934.45
Iteration:    640, Loss function: 5.803, Average Loss: 3.791, avg. samples / sec: 58015.66
Iteration:    640, Loss function: 5.040, Average Loss: 3.818, avg. samples / sec: 57809.97
Iteration:    640, Loss function: 6.429, Average Loss: 3.806, avg. samples / sec: 57697.85
Iteration:    640, Loss function: 5.656, Average Loss: 3.785, avg. samples / sec: 57740.69
Iteration:    640, Loss function: 4.596, Average Loss: 3.770, avg. samples / sec: 57733.07
Iteration:    640, Loss function: 5.063, Average Loss: 3.801, avg. samples / sec: 57861.35
Iteration:    640, Loss function: 6.142, Average Loss: 3.794, avg. samples / sec: 57694.14
Iteration:    640, Loss function: 5.342, Average Loss: 3.813, avg. samples / sec: 57814.16
Iteration:    640, Loss function: 5.984, Average Loss: 3.795, avg. samples / sec: 57861.28
Iteration:    640, Loss function: 6.196, Average Loss: 3.793, avg. samples / sec: 57975.15
Iteration:    640, Loss function: 4.685, Average Loss: 3.782, avg. samples / sec: 57650.17
Iteration:    640, Loss function: 5.041, Average Loss: 3.819, avg. samples / sec: 57567.00
Iteration:    660, Loss function: 5.437, Average Loss: 3.846, avg. samples / sec: 58054.44
Iteration:    660, Loss function: 3.979, Average Loss: 3.864, avg. samples / sec: 57966.78
Iteration:    660, Loss function: 7.002, Average Loss: 3.834, avg. samples / sec: 58110.20
Iteration:    660, Loss function: 5.787, Average Loss: 3.839, avg. samples / sec: 58052.65
Iteration:    660, Loss function: 7.313, Average Loss: 3.859, avg. samples / sec: 57926.28
Iteration:    660, Loss function: 6.232, Average Loss: 3.830, avg. samples / sec: 58021.22
Iteration:    660, Loss function: 5.004, Average Loss: 3.842, avg. samples / sec: 57883.05
Iteration:    660, Loss function: 5.139, Average Loss: 3.833, avg. samples / sec: 57891.97
Iteration:    660, Loss function: 6.176, Average Loss: 3.825, avg. samples / sec: 58001.69
Iteration:    660, Loss function: 6.000, Average Loss: 3.802, avg. samples / sec: 57967.95
Iteration:    660, Loss function: 4.900, Average Loss: 3.847, avg. samples / sec: 57845.49
Iteration:    660, Loss function: 6.479, Average Loss: 3.852, avg. samples / sec: 57959.16
Iteration:    660, Loss function: 5.162, Average Loss: 3.838, avg. samples / sec: 57914.31
Iteration:    660, Loss function: 6.988, Average Loss: 3.828, avg. samples / sec: 57739.34
Iteration:    660, Loss function: 5.099, Average Loss: 3.856, avg. samples / sec: 57809.09
Iteration:    680, Loss function: 6.058, Average Loss: 3.869, avg. samples / sec: 56679.08
Iteration:    680, Loss function: 5.302, Average Loss: 3.875, avg. samples / sec: 56541.70
Iteration:    680, Loss function: 6.122, Average Loss: 3.899, avg. samples / sec: 56427.24
Iteration:    680, Loss function: 4.822, Average Loss: 3.869, avg. samples / sec: 56372.69
Iteration:    680, Loss function: 5.156, Average Loss: 3.891, avg. samples / sec: 56420.33
Iteration:    680, Loss function: 7.615, Average Loss: 3.878, avg. samples / sec: 56301.00
Iteration:    680, Loss function: 5.028, Average Loss: 3.888, avg. samples / sec: 56650.21
Iteration:    680, Loss function: 4.355, Average Loss: 3.843, avg. samples / sec: 56397.75
Iteration:    680, Loss function: 6.769, Average Loss: 3.860, avg. samples / sec: 56392.58
Iteration:    680, Loss function: 6.199, Average Loss: 3.888, avg. samples / sec: 56228.71
Iteration:    680, Loss function: 4.901, Average Loss: 3.905, avg. samples / sec: 56218.55
Iteration:    680, Loss function: 6.134, Average Loss: 3.888, avg. samples / sec: 56345.30
Iteration:    680, Loss function: 4.674, Average Loss: 3.872, avg. samples / sec: 56255.49
Iteration:    680, Loss function: 5.788, Average Loss: 3.880, avg. samples / sec: 56295.69
Iteration:    680, Loss function: 6.745, Average Loss: 3.873, avg. samples / sec: 56276.68
:::MLL 1558638739.495 epoch_stop: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 819}}
:::MLL 1558638739.495 epoch_start: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 673}}
Iteration:    700, Loss function: 5.825, Average Loss: 3.874, avg. samples / sec: 57908.62
Iteration:    700, Loss function: 5.698, Average Loss: 3.895, avg. samples / sec: 57890.52
Iteration:    700, Loss function: 5.467, Average Loss: 3.904, avg. samples / sec: 57845.51
Iteration:    700, Loss function: 5.715, Average Loss: 3.921, avg. samples / sec: 57820.83
Iteration:    700, Loss function: 5.424, Average Loss: 3.909, avg. samples / sec: 57944.98
Iteration:    700, Loss function: 5.663, Average Loss: 3.912, avg. samples / sec: 57849.69
Iteration:    700, Loss function: 5.607, Average Loss: 3.929, avg. samples / sec: 57740.71
Iteration:    700, Loss function: 5.676, Average Loss: 3.907, avg. samples / sec: 57663.67
Iteration:    700, Loss function: 5.555, Average Loss: 3.912, avg. samples / sec: 57615.97
Iteration:    700, Loss function: 7.055, Average Loss: 3.912, avg. samples / sec: 57802.54
Iteration:    700, Loss function: 5.189, Average Loss: 3.943, avg. samples / sec: 57730.54
Iteration:    700, Loss function: 6.398, Average Loss: 3.924, avg. samples / sec: 57689.77
Iteration:    700, Loss function: 6.344, Average Loss: 3.912, avg. samples / sec: 57656.99
Iteration:    700, Loss function: 5.589, Average Loss: 3.933, avg. samples / sec: 57584.87
Iteration:    700, Loss function: 5.607, Average Loss: 3.921, avg. samples / sec: 57671.12
Iteration:    720, Loss function: 5.174, Average Loss: 3.929, avg. samples / sec: 56463.07
Iteration:    720, Loss function: 6.517, Average Loss: 3.957, avg. samples / sec: 56641.92
Iteration:    720, Loss function: 5.384, Average Loss: 3.943, avg. samples / sec: 56530.30
Iteration:    720, Loss function: 5.815, Average Loss: 3.951, avg. samples / sec: 56687.38
Iteration:    720, Loss function: 3.690, Average Loss: 3.963, avg. samples / sec: 56531.88
Iteration:    720, Loss function: 6.201, Average Loss: 3.964, avg. samples / sec: 56434.20
Iteration:    720, Loss function: 4.584, Average Loss: 3.937, avg. samples / sec: 56422.72
Iteration:    720, Loss function: 5.096, Average Loss: 3.962, avg. samples / sec: 56587.16
Iteration:    720, Loss function: 5.310, Average Loss: 3.949, avg. samples / sec: 56576.46
Iteration:    720, Loss function: 4.622, Average Loss: 3.905, avg. samples / sec: 56292.84
Iteration:    720, Loss function: 6.673, Average Loss: 3.943, avg. samples / sec: 56434.72
Iteration:    720, Loss function: 5.590, Average Loss: 3.945, avg. samples / sec: 56431.83
Iteration:    720, Loss function: 6.152, Average Loss: 3.946, avg. samples / sec: 56420.01
Iteration:    720, Loss function: 5.122, Average Loss: 3.935, avg. samples / sec: 56270.65
Iteration:    720, Loss function: 6.353, Average Loss: 3.978, avg. samples / sec: 55773.00
Iteration:    740, Loss function: 5.431, Average Loss: 3.993, avg. samples / sec: 58524.32
Iteration:    740, Loss function: 5.193, Average Loss: 3.978, avg. samples / sec: 58609.12
Iteration:    740, Loss function: 6.051, Average Loss: 4.014, avg. samples / sec: 59375.26
Iteration:    740, Loss function: 5.415, Average Loss: 3.988, avg. samples / sec: 58471.29
Iteration:    740, Loss function: 5.183, Average Loss: 3.968, avg. samples / sec: 58644.97
Iteration:    740, Loss function: 5.247, Average Loss: 3.978, avg. samples / sec: 58418.91
Iteration:    740, Loss function: 6.241, Average Loss: 3.981, avg. samples / sec: 58535.16
Iteration:    740, Loss function: 6.030, Average Loss: 3.998, avg. samples / sec: 58422.45
Iteration:    740, Loss function: 6.398, Average Loss: 4.000, avg. samples / sec: 58448.49
Iteration:    740, Loss function: 5.142, Average Loss: 3.964, avg. samples / sec: 58347.08
Iteration:    740, Loss function: 7.044, Average Loss: 3.999, avg. samples / sec: 58392.46
Iteration:    740, Loss function: 6.860, Average Loss: 3.975, avg. samples / sec: 58362.13
Iteration:    740, Loss function: 6.032, Average Loss: 3.938, avg. samples / sec: 58404.41
Iteration:    740, Loss function: 6.444, Average Loss: 3.983, avg. samples / sec: 58438.17
Iteration:    740, Loss function: 5.555, Average Loss: 3.985, avg. samples / sec: 58245.60
Iteration:    760, Loss function: 5.132, Average Loss: 4.006, avg. samples / sec: 58738.86
Iteration:    760, Loss function: 5.123, Average Loss: 3.974, avg. samples / sec: 58875.32
Iteration:    760, Loss function: 5.131, Average Loss: 4.001, avg. samples / sec: 58848.13
Iteration:    760, Loss function: 5.954, Average Loss: 3.999, avg. samples / sec: 58689.91
Iteration:    760, Loss function: 5.298, Average Loss: 4.035, avg. samples / sec: 58742.09
Iteration:    760, Loss function: 4.143, Average Loss: 4.016, avg. samples / sec: 58817.13
Iteration:    760, Loss function: 6.137, Average Loss: 4.012, avg. samples / sec: 58689.32
Iteration:    760, Loss function: 5.644, Average Loss: 4.028, avg. samples / sec: 58737.88
Iteration:    760, Loss function: 4.854, Average Loss: 4.025, avg. samples / sec: 58677.42
Iteration:    760, Loss function: 5.573, Average Loss: 4.013, avg. samples / sec: 58644.85
Iteration:    760, Loss function: 4.633, Average Loss: 4.018, avg. samples / sec: 58523.52
Iteration:    760, Loss function: 4.044, Average Loss: 4.012, avg. samples / sec: 58787.20
Iteration:    760, Loss function: 5.532, Average Loss: 3.997, avg. samples / sec: 58562.26
Iteration:    760, Loss function: 5.490, Average Loss: 4.024, avg. samples / sec: 58356.62
Iteration:    760, Loss function: 5.093, Average Loss: 4.043, avg. samples / sec: 58377.68
:::MLL 1558638741.519 epoch_stop: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 819}}
:::MLL 1558638741.519 epoch_start: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 673}}
Iteration:    780, Loss function: 4.952, Average Loss: 4.024, avg. samples / sec: 58682.41
Iteration:    780, Loss function: 4.451, Average Loss: 4.060, avg. samples / sec: 58662.69
Iteration:    780, Loss function: 5.799, Average Loss: 4.044, avg. samples / sec: 58749.68
Iteration:    780, Loss function: 5.967, Average Loss: 4.055, avg. samples / sec: 58640.63
Iteration:    780, Loss function: 5.266, Average Loss: 4.029, avg. samples / sec: 58553.70
Iteration:    780, Loss function: 4.635, Average Loss: 4.050, avg. samples / sec: 58803.27
Iteration:    780, Loss function: 4.809, Average Loss: 4.034, avg. samples / sec: 58605.71
Iteration:    780, Loss function: 5.415, Average Loss: 4.040, avg. samples / sec: 58684.87
Iteration:    780, Loss function: 5.639, Average Loss: 4.001, avg. samples / sec: 58447.91
Iteration:    780, Loss function: 5.680, Average Loss: 4.079, avg. samples / sec: 58726.79
Iteration:    780, Loss function: 5.810, Average Loss: 4.048, avg. samples / sec: 58435.99
Iteration:    780, Loss function: 4.155, Average Loss: 4.044, avg. samples / sec: 58417.53
Iteration:    780, Loss function: 4.930, Average Loss: 4.031, avg. samples / sec: 58251.89
Iteration:    780, Loss function: 5.210, Average Loss: 4.050, avg. samples / sec: 58407.51
Iteration:    780, Loss function: 4.839, Average Loss: 4.029, avg. samples / sec: 58319.75
Iteration:    800, Loss function: 5.182, Average Loss: 4.079, avg. samples / sec: 59450.16
Iteration:    800, Loss function: 5.535, Average Loss: 4.058, avg. samples / sec: 59204.85
Iteration:    800, Loss function: 5.445, Average Loss: 4.088, avg. samples / sec: 59103.57
Iteration:    800, Loss function: 6.272, Average Loss: 4.083, avg. samples / sec: 59168.63
Iteration:    800, Loss function: 5.070, Average Loss: 4.081, avg. samples / sec: 59175.09
Iteration:    800, Loss function: 7.017, Average Loss: 4.059, avg. samples / sec: 59556.65
Iteration:    800, Loss function: 5.303, Average Loss: 4.052, avg. samples / sec: 58986.90
Iteration:    800, Loss function: 5.538, Average Loss: 4.076, avg. samples / sec: 59299.61
Iteration:    800, Loss function: 4.907, Average Loss: 4.029, avg. samples / sec: 59192.24
Iteration:    800, Loss function: 6.971, Average Loss: 4.105, avg. samples / sec: 59213.23
Iteration:    800, Loss function: 5.293, Average Loss: 4.063, avg. samples / sec: 59108.35
Iteration:    800, Loss function: 4.588, Average Loss: 4.068, avg. samples / sec: 59044.88
Iteration:    800, Loss function: 6.469, Average Loss: 4.059, avg. samples / sec: 59226.87
Iteration:    800, Loss function: 6.469, Average Loss: 4.075, avg. samples / sec: 58931.40
Iteration:    800, Loss function: 4.415, Average Loss: 4.068, avg. samples / sec: 59129.03
Iteration:    820, Loss function: 5.449, Average Loss: 4.093, avg. samples / sec: 59902.97
Iteration:    820, Loss function: 5.208, Average Loss: 4.108, avg. samples / sec: 59654.09
Iteration:    820, Loss function: 5.817, Average Loss: 4.105, avg. samples / sec: 59888.02
Iteration:    820, Loss function: 5.830, Average Loss: 4.097, avg. samples / sec: 59843.62
Iteration:    820, Loss function: 5.198, Average Loss: 4.093, avg. samples / sec: 59574.96
Iteration:    820, Loss function: 5.449, Average Loss: 4.091, avg. samples / sec: 59779.80
Iteration:    820, Loss function: 3.883, Average Loss: 4.101, avg. samples / sec: 59673.26
Iteration:    820, Loss function: 5.032, Average Loss: 4.089, avg. samples / sec: 59610.69
Iteration:    820, Loss function: 5.453, Average Loss: 4.124, avg. samples / sec: 59493.82
Iteration:    820, Loss function: 5.386, Average Loss: 4.076, avg. samples / sec: 59557.51
Iteration:    820, Loss function: 5.689, Average Loss: 4.111, avg. samples / sec: 59478.03
Iteration:    820, Loss function: 5.748, Average Loss: 4.108, avg. samples / sec: 59494.48
Iteration:    820, Loss function: 4.658, Average Loss: 4.097, avg. samples / sec: 59652.30
Iteration:    820, Loss function: 6.633, Average Loss: 4.059, avg. samples / sec: 59555.12
Iteration:    820, Loss function: 6.662, Average Loss: 4.131, avg. samples / sec: 59480.06
:::MLL 1558638743.535 epoch_stop: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 819}}
:::MLL 1558638743.535 epoch_start: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 673}}
Iteration:    840, Loss function: 5.372, Average Loss: 4.124, avg. samples / sec: 56772.97
Iteration:    840, Loss function: 4.953, Average Loss: 4.082, avg. samples / sec: 56910.92
Iteration:    840, Loss function: 4.141, Average Loss: 4.146, avg. samples / sec: 56842.35
Iteration:    840, Loss function: 5.486, Average Loss: 4.120, avg. samples / sec: 56733.54
Iteration:    840, Loss function: 5.550, Average Loss: 4.108, avg. samples / sec: 56737.20
Iteration:    840, Loss function: 5.250, Average Loss: 4.117, avg. samples / sec: 56564.58
Iteration:    840, Loss function: 5.857, Average Loss: 4.115, avg. samples / sec: 56756.25
Iteration:    840, Loss function: 5.755, Average Loss: 4.124, avg. samples / sec: 56810.98
Iteration:    840, Loss function: 6.076, Average Loss: 4.157, avg. samples / sec: 56891.82
Iteration:    840, Loss function: 4.765, Average Loss: 4.120, avg. samples / sec: 56662.24
Iteration:    840, Loss function: 5.369, Average Loss: 4.131, avg. samples / sec: 56551.89
Iteration:    840, Loss function: 4.885, Average Loss: 4.093, avg. samples / sec: 56715.19
Iteration:    840, Loss function: 4.076, Average Loss: 4.124, avg. samples / sec: 56608.89
Iteration:    840, Loss function: 4.935, Average Loss: 4.128, avg. samples / sec: 56657.66
Iteration:    840, Loss function: 4.768, Average Loss: 4.134, avg. samples / sec: 56611.82
Iteration:    860, Loss function: 5.187, Average Loss: 4.152, avg. samples / sec: 58328.43
Iteration:    860, Loss function: 6.192, Average Loss: 4.146, avg. samples / sec: 58107.54
Iteration:    860, Loss function: 5.285, Average Loss: 4.141, avg. samples / sec: 58044.40
Iteration:    860, Loss function: 4.334, Average Loss: 4.149, avg. samples / sec: 58120.60
Iteration:    860, Loss function: 4.499, Average Loss: 4.145, avg. samples / sec: 58158.69
Iteration:    860, Loss function: 5.631, Average Loss: 4.128, avg. samples / sec: 58011.07
Iteration:    860, Loss function: 6.293, Average Loss: 4.146, avg. samples / sec: 58021.25
Iteration:    860, Loss function: 4.696, Average Loss: 4.132, avg. samples / sec: 57965.21
Iteration:    860, Loss function: 5.180, Average Loss: 4.144, avg. samples / sec: 57978.73
Iteration:    860, Loss function: 4.766, Average Loss: 4.178, avg. samples / sec: 57983.65
Iteration:    860, Loss function: 5.896, Average Loss: 4.165, avg. samples / sec: 57894.44
Iteration:    860, Loss function: 6.308, Average Loss: 4.157, avg. samples / sec: 58135.41
Iteration:    860, Loss function: 4.062, Average Loss: 4.109, avg. samples / sec: 58014.20
Iteration:    860, Loss function: 5.323, Average Loss: 4.104, avg. samples / sec: 57725.08
Iteration:    860, Loss function: 5.443, Average Loss: 4.144, avg. samples / sec: 57593.37
Iteration:    880, Loss function: 6.068, Average Loss: 4.169, avg. samples / sec: 60206.02
Iteration:    880, Loss function: 5.725, Average Loss: 4.126, avg. samples / sec: 60103.36
Iteration:    880, Loss function: 6.155, Average Loss: 4.189, avg. samples / sec: 59934.61
Iteration:    880, Loss function: 5.860, Average Loss: 4.202, avg. samples / sec: 59918.86
Iteration:    880, Loss function: 5.635, Average Loss: 4.172, avg. samples / sec: 59677.74
Iteration:    880, Loss function: 4.708, Average Loss: 4.170, avg. samples / sec: 59878.18
Iteration:    880, Loss function: 5.476, Average Loss: 4.161, avg. samples / sec: 59858.69
Iteration:    880, Loss function: 3.918, Average Loss: 4.130, avg. samples / sec: 59845.93
Iteration:    880, Loss function: 5.820, Average Loss: 4.184, avg. samples / sec: 59815.88
Iteration:    880, Loss function: 4.846, Average Loss: 4.172, avg. samples / sec: 59625.07
Iteration:    880, Loss function: 6.244, Average Loss: 4.154, avg. samples / sec: 59603.61
Iteration:    880, Loss function: 4.832, Average Loss: 4.166, avg. samples / sec: 59518.00
Iteration:    880, Loss function: 4.638, Average Loss: 4.165, avg. samples / sec: 59616.72
Iteration:    880, Loss function: 4.505, Average Loss: 4.170, avg. samples / sec: 59409.63
Iteration:    880, Loss function: 6.069, Average Loss: 4.175, avg. samples / sec: 59147.27
Iteration:    900, Loss function: 5.832, Average Loss: 4.155, avg. samples / sec: 59640.36
Iteration:    900, Loss function: 5.190, Average Loss: 4.200, avg. samples / sec: 60138.04
Iteration:    900, Loss function: 4.079, Average Loss: 4.189, avg. samples / sec: 59426.06
Iteration:    900, Loss function: 5.663, Average Loss: 4.181, avg. samples / sec: 59446.37
Iteration:    900, Loss function: 3.455, Average Loss: 4.197, avg. samples / sec: 59432.30
Iteration:    900, Loss function: 6.398, Average Loss: 4.222, avg. samples / sec: 59379.82
Iteration:    900, Loss function: 5.863, Average Loss: 4.211, avg. samples / sec: 59511.44
Iteration:    900, Loss function: 3.418, Average Loss: 4.183, avg. samples / sec: 59602.72
Iteration:    900, Loss function: 5.081, Average Loss: 4.194, avg. samples / sec: 59303.31
Iteration:    900, Loss function: 4.633, Average Loss: 4.215, avg. samples / sec: 59325.87
Iteration:    900, Loss function: 5.923, Average Loss: 4.193, avg. samples / sec: 59706.36
Iteration:    900, Loss function: 4.429, Average Loss: 4.191, avg. samples / sec: 59522.90
Iteration:    900, Loss function: 4.749, Average Loss: 4.149, avg. samples / sec: 59303.46
Iteration:    900, Loss function: 4.568, Average Loss: 4.179, avg. samples / sec: 59468.92
Iteration:    900, Loss function: 3.993, Average Loss: 4.186, avg. samples / sec: 59501.16
:::MLL 1558638745.521 epoch_stop: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 819}}
:::MLL 1558638745.524 epoch_start: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 673}}
Iteration:    920, Loss function: 4.536, Average Loss: 4.174, avg. samples / sec: 59171.41
Iteration:    920, Loss function: 4.878, Average Loss: 4.230, avg. samples / sec: 59245.17
Iteration:    920, Loss function: 6.331, Average Loss: 4.221, avg. samples / sec: 59149.88
Iteration:    920, Loss function: 4.230, Average Loss: 4.226, avg. samples / sec: 59286.37
Iteration:    920, Loss function: 4.754, Average Loss: 4.240, avg. samples / sec: 59206.34
Iteration:    920, Loss function: 5.493, Average Loss: 4.200, avg. samples / sec: 59228.59
Iteration:    920, Loss function: 5.008, Average Loss: 4.213, avg. samples / sec: 59236.00
Iteration:    920, Loss function: 5.390, Average Loss: 4.209, avg. samples / sec: 59137.67
Iteration:    920, Loss function: 5.234, Average Loss: 4.213, avg. samples / sec: 59197.29
Iteration:    920, Loss function: 4.889, Average Loss: 4.196, avg. samples / sec: 59279.21
Iteration:    920, Loss function: 5.041, Average Loss: 4.209, avg. samples / sec: 59170.02
Iteration:    920, Loss function: 5.612, Average Loss: 4.198, avg. samples / sec: 59102.15
Iteration:    920, Loss function: 5.813, Average Loss: 4.171, avg. samples / sec: 59143.68
Iteration:    920, Loss function: 5.373, Average Loss: 4.217, avg. samples / sec: 59053.14
Iteration:    920, Loss function: 4.530, Average Loss: 4.203, avg. samples / sec: 59173.65
Iteration:    940, Loss function: 6.301, Average Loss: 4.259, avg. samples / sec: 59268.51
Iteration:    940, Loss function: 5.568, Average Loss: 4.238, avg. samples / sec: 59271.75
Iteration:    940, Loss function: 5.171, Average Loss: 4.237, avg. samples / sec: 59223.03
Iteration:    940, Loss function: 5.639, Average Loss: 4.219, avg. samples / sec: 59235.71
Iteration:    940, Loss function: 4.749, Average Loss: 4.250, avg. samples / sec: 59187.62
Iteration:    940, Loss function: 5.207, Average Loss: 4.227, avg. samples / sec: 59285.64
Iteration:    940, Loss function: 5.744, Average Loss: 4.234, avg. samples / sec: 59240.26
Iteration:    940, Loss function: 6.326, Average Loss: 4.191, avg. samples / sec: 59051.86
Iteration:    940, Loss function: 5.569, Average Loss: 4.236, avg. samples / sec: 59282.10
Iteration:    940, Loss function: 5.317, Average Loss: 4.214, avg. samples / sec: 59200.50
Iteration:    940, Loss function: 5.607, Average Loss: 4.231, avg. samples / sec: 59170.39
Iteration:    940, Loss function: 5.495, Average Loss: 4.244, avg. samples / sec: 59070.69
Iteration:    940, Loss function: 5.253, Average Loss: 4.220, avg. samples / sec: 59238.59
Iteration:    940, Loss function: 5.264, Average Loss: 4.191, avg. samples / sec: 59143.35
Iteration:    940, Loss function: 4.613, Average Loss: 4.218, avg. samples / sec: 59083.84
Iteration:    960, Loss function: 6.954, Average Loss: 4.254, avg. samples / sec: 59985.60
Iteration:    960, Loss function: 5.155, Average Loss: 4.252, avg. samples / sec: 60073.87
Iteration:    960, Loss function: 5.197, Average Loss: 4.265, avg. samples / sec: 60086.83
Iteration:    960, Loss function: 5.713, Average Loss: 4.256, avg. samples / sec: 59905.16
Iteration:    960, Loss function: 6.146, Average Loss: 4.211, avg. samples / sec: 59971.23
Iteration:    960, Loss function: 6.181, Average Loss: 4.245, avg. samples / sec: 59855.11
Iteration:    960, Loss function: 5.230, Average Loss: 4.274, avg. samples / sec: 59827.72
Iteration:    960, Loss function: 4.796, Average Loss: 4.250, avg. samples / sec: 59906.25
Iteration:    960, Loss function: 5.539, Average Loss: 4.209, avg. samples / sec: 60011.40
Iteration:    960, Loss function: 4.553, Average Loss: 4.235, avg. samples / sec: 59858.08
Iteration:    960, Loss function: 3.921, Average Loss: 4.277, avg. samples / sec: 59702.71
Iteration:    960, Loss function: 5.374, Average Loss: 4.245, avg. samples / sec: 59974.78
Iteration:    960, Loss function: 4.860, Average Loss: 4.259, avg. samples / sec: 59714.96
Iteration:    960, Loss function: 5.107, Average Loss: 4.242, avg. samples / sec: 59637.00
Iteration:    960, Loss function: 4.473, Average Loss: 4.242, avg. samples / sec: 59800.58
:::MLL 1558638747.504 epoch_stop: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 819}}
:::MLL 1558638747.504 epoch_start: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 673}}
Iteration:    980, Loss function: 5.163, Average Loss: 4.281, avg. samples / sec: 59497.87
Iteration:    980, Loss function: 5.697, Average Loss: 4.255, avg. samples / sec: 59675.39
Iteration:    980, Loss function: 3.759, Average Loss: 4.263, avg. samples / sec: 59657.90
Iteration:    980, Loss function: 4.485, Average Loss: 4.256, avg. samples / sec: 59744.30
Iteration:    980, Loss function: 5.890, Average Loss: 4.270, avg. samples / sec: 59408.98
Iteration:    980, Loss function: 5.517, Average Loss: 4.223, avg. samples / sec: 59413.41
Iteration:    980, Loss function: 5.934, Average Loss: 4.259, avg. samples / sec: 59626.68
Iteration:    980, Loss function: 5.001, Average Loss: 4.269, avg. samples / sec: 59390.00
Iteration:    980, Loss function: 4.977, Average Loss: 4.227, avg. samples / sec: 59421.85
Iteration:    980, Loss function: 4.538, Average Loss: 4.287, avg. samples / sec: 59385.50
Iteration:    980, Loss function: 4.889, Average Loss: 4.289, avg. samples / sec: 59446.24
Iteration:    980, Loss function: 6.851, Average Loss: 4.265, avg. samples / sec: 59229.78
Iteration:    980, Loss function: 4.997, Average Loss: 4.278, avg. samples / sec: 59526.42
Iteration:    980, Loss function: 5.869, Average Loss: 4.268, avg. samples / sec: 59154.77
Iteration:    980, Loss function: 4.571, Average Loss: 4.265, avg. samples / sec: 59281.95
Iteration:   1000, Loss function: 5.045, Average Loss: 4.286, avg. samples / sec: 59554.26
Iteration:   1000, Loss function: 4.604, Average Loss: 4.246, avg. samples / sec: 59468.90
Iteration:   1000, Loss function: 5.189, Average Loss: 4.306, avg. samples / sec: 59392.20
Iteration:   1000, Loss function: 5.099, Average Loss: 4.306, avg. samples / sec: 59376.24
Iteration:   1000, Loss function: 3.906, Average Loss: 4.278, avg. samples / sec: 59461.39
Iteration:   1000, Loss function: 5.178, Average Loss: 4.274, avg. samples / sec: 59188.93
Iteration:   1000, Loss function: 4.706, Average Loss: 4.302, avg. samples / sec: 59088.40
Iteration:   1000, Loss function: 3.822, Average Loss: 4.276, avg. samples / sec: 59133.67
Iteration:   1000, Loss function: 4.803, Average Loss: 4.281, avg. samples / sec: 59322.30
Iteration:   1000, Loss function: 4.001, Average Loss: 4.284, avg. samples / sec: 59344.91
Iteration:   1000, Loss function: 5.916, Average Loss: 4.238, avg. samples / sec: 59174.39
Iteration:   1000, Loss function: 5.249, Average Loss: 4.290, avg. samples / sec: 59248.90
Iteration:   1000, Loss function: 4.360, Average Loss: 4.288, avg. samples / sec: 59132.78
Iteration:   1000, Loss function: 5.907, Average Loss: 4.274, avg. samples / sec: 58983.10
Iteration:   1000, Loss function: 4.482, Average Loss: 4.271, avg. samples / sec: 59044.53
Iteration:   1020, Loss function: 4.622, Average Loss: 4.249, avg. samples / sec: 59817.79
Iteration:   1020, Loss function: 4.696, Average Loss: 4.290, avg. samples / sec: 59628.50
Iteration:   1020, Loss function: 5.927, Average Loss: 4.308, avg. samples / sec: 59676.47
Iteration:   1020, Loss function: 5.564, Average Loss: 4.301, avg. samples / sec: 59386.82
Iteration:   1020, Loss function: 4.647, Average Loss: 4.323, avg. samples / sec: 59548.60
Iteration:   1020, Loss function: 4.595, Average Loss: 4.321, avg. samples / sec: 59581.00
Iteration:   1020, Loss function: 5.158, Average Loss: 4.296, avg. samples / sec: 59469.82
Iteration:   1020, Loss function: 6.157, Average Loss: 4.326, avg. samples / sec: 59441.28
Iteration:   1020, Loss function: 4.203, Average Loss: 4.304, avg. samples / sec: 59509.25
Iteration:   1020, Loss function: 5.647, Average Loss: 4.297, avg. samples / sec: 59470.38
Iteration:   1020, Loss function: 4.500, Average Loss: 4.287, avg. samples / sec: 59659.32
Iteration:   1020, Loss function: 4.254, Average Loss: 4.298, avg. samples / sec: 59395.16
Iteration:   1020, Loss function: 4.891, Average Loss: 4.263, avg. samples / sec: 59214.80
Iteration:   1020, Loss function: 4.542, Average Loss: 4.292, avg. samples / sec: 59242.15
Iteration:   1020, Loss function: 6.291, Average Loss: 4.298, avg. samples / sec: 59249.88
Iteration:   1040, Loss function: 4.034, Average Loss: 4.337, avg. samples / sec: 57452.52
Iteration:   1040, Loss function: 4.663, Average Loss: 4.309, avg. samples / sec: 57855.58
Iteration:   1040, Loss function: 4.394, Average Loss: 4.332, avg. samples / sec: 57513.06
Iteration:   1040, Loss function: 4.372, Average Loss: 4.339, avg. samples / sec: 57423.28
Iteration:   1040, Loss function: 3.555, Average Loss: 4.304, avg. samples / sec: 57378.30
Iteration:   1040, Loss function: 6.750, Average Loss: 4.320, avg. samples / sec: 57542.48
Iteration:   1040, Loss function: 4.823, Average Loss: 4.315, avg. samples / sec: 57377.25
Iteration:   1040, Loss function: 4.273, Average Loss: 4.281, avg. samples / sec: 57601.18
Iteration:   1040, Loss function: 4.910, Average Loss: 4.266, avg. samples / sec: 57204.18
Iteration:   1040, Loss function: 3.760, Average Loss: 4.297, avg. samples / sec: 57509.23
Iteration:   1040, Loss function: 5.955, Average Loss: 4.313, avg. samples / sec: 57426.88
Iteration:   1040, Loss function: 5.271, Average Loss: 4.302, avg. samples / sec: 57627.73
Iteration:   1040, Loss function: 4.876, Average Loss: 4.321, avg. samples / sec: 57262.26
Iteration:   1040, Loss function: 4.706, Average Loss: 4.308, avg. samples / sec: 57429.90
Iteration:   1040, Loss function: 5.430, Average Loss: 4.315, avg. samples / sec: 57359.26
:::MLL 1558638749.498 epoch_stop: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 819}}
:::MLL 1558638749.498 epoch_start: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 673}}
Iteration:   1060, Loss function: 4.487, Average Loss: 4.353, avg. samples / sec: 57731.84
Iteration:   1060, Loss function: 6.187, Average Loss: 4.298, avg. samples / sec: 57735.34
Iteration:   1060, Loss function: 5.299, Average Loss: 4.349, avg. samples / sec: 57692.30
Iteration:   1060, Loss function: 5.055, Average Loss: 4.280, avg. samples / sec: 57720.35
Iteration:   1060, Loss function: 5.051, Average Loss: 4.314, avg. samples / sec: 57686.84
Iteration:   1060, Loss function: 5.503, Average Loss: 4.350, avg. samples / sec: 57654.04
Iteration:   1060, Loss function: 6.723, Average Loss: 4.329, avg. samples / sec: 57642.32
Iteration:   1060, Loss function: 5.815, Average Loss: 4.330, avg. samples / sec: 57852.78
Iteration:   1060, Loss function: 5.407, Average Loss: 4.317, avg. samples / sec: 57733.66
Iteration:   1060, Loss function: 5.004, Average Loss: 4.313, avg. samples / sec: 57659.68
Iteration:   1060, Loss function: 4.497, Average Loss: 4.331, avg. samples / sec: 57615.93
Iteration:   1060, Loss function: 4.587, Average Loss: 4.329, avg. samples / sec: 57636.57
Iteration:   1060, Loss function: 5.504, Average Loss: 4.322, avg. samples / sec: 57760.97
Iteration:   1060, Loss function: 5.599, Average Loss: 4.325, avg. samples / sec: 57645.53
Iteration:   1060, Loss function: 4.686, Average Loss: 4.329, avg. samples / sec: 57584.83
Iteration:   1080, Loss function: 4.602, Average Loss: 4.370, avg. samples / sec: 55985.33
Iteration:   1080, Loss function: 4.998, Average Loss: 4.327, avg. samples / sec: 56060.27
Iteration:   1080, Loss function: 4.437, Average Loss: 4.293, avg. samples / sec: 56065.57
Iteration:   1080, Loss function: 4.472, Average Loss: 4.347, avg. samples / sec: 56266.41
Iteration:   1080, Loss function: 5.665, Average Loss: 4.347, avg. samples / sec: 56087.02
Iteration:   1080, Loss function: 4.582, Average Loss: 4.329, avg. samples / sec: 56054.74
Iteration:   1080, Loss function: 4.868, Average Loss: 4.345, avg. samples / sec: 56049.43
Iteration:   1080, Loss function: 5.226, Average Loss: 4.326, avg. samples / sec: 56068.67
Iteration:   1080, Loss function: 4.733, Average Loss: 4.345, avg. samples / sec: 56054.11
Iteration:   1080, Loss function: 5.117, Average Loss: 4.343, avg. samples / sec: 56000.43
Iteration:   1080, Loss function: 4.940, Average Loss: 4.338, avg. samples / sec: 56040.83
Iteration:   1080, Loss function: 4.409, Average Loss: 4.318, avg. samples / sec: 55921.32
Iteration:   1080, Loss function: 5.010, Average Loss: 4.337, avg. samples / sec: 56019.09
Iteration:   1080, Loss function: 5.244, Average Loss: 4.365, avg. samples / sec: 55852.29
Iteration:   1080, Loss function: 4.379, Average Loss: 4.362, avg. samples / sec: 55884.67
Iteration:   1100, Loss function: 4.546, Average Loss: 4.372, avg. samples / sec: 56469.73
Iteration:   1100, Loss function: 5.307, Average Loss: 4.359, avg. samples / sec: 56322.83
Iteration:   1100, Loss function: 4.727, Average Loss: 4.354, avg. samples / sec: 56364.52
Iteration:   1100, Loss function: 5.108, Average Loss: 4.340, avg. samples / sec: 56223.71
Iteration:   1100, Loss function: 5.052, Average Loss: 4.355, avg. samples / sec: 56223.33
Iteration:   1100, Loss function: 5.278, Average Loss: 4.378, avg. samples / sec: 56387.01
Iteration:   1100, Loss function: 4.286, Average Loss: 4.376, avg. samples / sec: 56167.04
Iteration:   1100, Loss function: 4.620, Average Loss: 4.347, avg. samples / sec: 56286.77
Iteration:   1100, Loss function: 5.182, Average Loss: 4.357, avg. samples / sec: 56196.65
Iteration:   1100, Loss function: 4.969, Average Loss: 4.301, avg. samples / sec: 56121.46
Iteration:   1100, Loss function: 5.500, Average Loss: 4.342, avg. samples / sec: 56159.32
Iteration:   1100, Loss function: 3.596, Average Loss: 4.348, avg. samples / sec: 56185.27
Iteration:   1100, Loss function: 4.371, Average Loss: 4.337, avg. samples / sec: 56138.78
Iteration:   1100, Loss function: 4.552, Average Loss: 4.357, avg. samples / sec: 56112.86
Iteration:   1100, Loss function: 4.995, Average Loss: 4.326, avg. samples / sec: 56027.08
:::MLL 1558638751.579 epoch_stop: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 819}}
:::MLL 1558638751.580 epoch_start: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 673}}
Iteration:   1120, Loss function: 4.724, Average Loss: 4.335, avg. samples / sec: 58478.06
Iteration:   1120, Loss function: 5.765, Average Loss: 4.356, avg. samples / sec: 58255.62
Iteration:   1120, Loss function: 5.602, Average Loss: 4.383, avg. samples / sec: 58037.80
Iteration:   1120, Loss function: 3.447, Average Loss: 4.389, avg. samples / sec: 58101.16
Iteration:   1120, Loss function: 3.437, Average Loss: 4.366, avg. samples / sec: 58008.06
Iteration:   1120, Loss function: 5.739, Average Loss: 4.366, avg. samples / sec: 58048.75
Iteration:   1120, Loss function: 3.639, Average Loss: 4.363, avg. samples / sec: 58159.93
Iteration:   1120, Loss function: 4.257, Average Loss: 4.353, avg. samples / sec: 57999.35
Iteration:   1120, Loss function: 4.804, Average Loss: 4.389, avg. samples / sec: 57979.78
Iteration:   1120, Loss function: 3.908, Average Loss: 4.373, avg. samples / sec: 57892.85
Iteration:   1120, Loss function: 4.822, Average Loss: 4.311, avg. samples / sec: 58023.61
Iteration:   1120, Loss function: 6.373, Average Loss: 4.346, avg. samples / sec: 58075.76
Iteration:   1120, Loss function: 4.130, Average Loss: 4.358, avg. samples / sec: 57992.24
Iteration:   1120, Loss function: 6.215, Average Loss: 4.352, avg. samples / sec: 58013.10
Iteration:   1120, Loss function: 5.486, Average Loss: 4.370, avg. samples / sec: 57951.77
Iteration:   1140, Loss function: 4.021, Average Loss: 4.391, avg. samples / sec: 59020.03
Iteration:   1140, Loss function: 3.551, Average Loss: 4.398, avg. samples / sec: 59022.67
Iteration:   1140, Loss function: 5.762, Average Loss: 4.403, avg. samples / sec: 59115.27
Iteration:   1140, Loss function: 4.916, Average Loss: 4.367, avg. samples / sec: 58912.98
Iteration:   1140, Loss function: 4.965, Average Loss: 4.377, avg. samples / sec: 58978.93
Iteration:   1140, Loss function: 4.355, Average Loss: 4.362, avg. samples / sec: 59107.73
Iteration:   1140, Loss function: 3.825, Average Loss: 4.387, avg. samples / sec: 59054.97
Iteration:   1140, Loss function: 4.194, Average Loss: 4.380, avg. samples / sec: 59100.99
Iteration:   1140, Loss function: 4.961, Average Loss: 4.349, avg. samples / sec: 58788.03
Iteration:   1140, Loss function: 4.435, Average Loss: 4.369, avg. samples / sec: 58917.97
Iteration:   1140, Loss function: 4.421, Average Loss: 4.367, avg. samples / sec: 58928.84
Iteration:   1140, Loss function: 5.679, Average Loss: 4.320, avg. samples / sec: 58969.03
Iteration:   1140, Loss function: 5.387, Average Loss: 4.372, avg. samples / sec: 58859.02
Iteration:   1140, Loss function: 4.717, Average Loss: 4.367, avg. samples / sec: 58960.91
Iteration:   1140, Loss function: 4.617, Average Loss: 4.357, avg. samples / sec: 58844.42
Iteration:   1160, Loss function: 5.221, Average Loss: 4.384, avg. samples / sec: 59376.59
Iteration:   1160, Loss function: 5.173, Average Loss: 4.405, avg. samples / sec: 59236.35
Iteration:   1160, Loss function: 4.986, Average Loss: 4.364, avg. samples / sec: 59560.66
Iteration:   1160, Loss function: 3.687, Average Loss: 4.379, avg. samples / sec: 59271.43
Iteration:   1160, Loss function: 4.161, Average Loss: 4.381, avg. samples / sec: 59424.56
Iteration:   1160, Loss function: 6.243, Average Loss: 4.392, avg. samples / sec: 59316.49
Iteration:   1160, Loss function: 5.510, Average Loss: 4.409, avg. samples / sec: 59226.15
Iteration:   1160, Loss function: 4.365, Average Loss: 4.375, avg. samples / sec: 59341.31
Iteration:   1160, Loss function: 4.540, Average Loss: 4.376, avg. samples / sec: 59399.61
Iteration:   1160, Loss function: 5.093, Average Loss: 4.358, avg. samples / sec: 59296.20
Iteration:   1160, Loss function: 5.527, Average Loss: 4.380, avg. samples / sec: 59298.47
Iteration:   1160, Loss function: 4.842, Average Loss: 4.371, avg. samples / sec: 59190.72
Iteration:   1160, Loss function: 4.618, Average Loss: 4.395, avg. samples / sec: 59166.84
Iteration:   1160, Loss function: 3.209, Average Loss: 4.336, avg. samples / sec: 59129.06
Iteration:   1160, Loss function: 5.153, Average Loss: 4.401, avg. samples / sec: 58892.86
Iteration:   1180, Loss function: 5.285, Average Loss: 4.414, avg. samples / sec: 59803.85
Iteration:   1180, Loss function: 6.119, Average Loss: 4.386, avg. samples / sec: 59747.21
Iteration:   1180, Loss function: 4.481, Average Loss: 4.382, avg. samples / sec: 59773.57
Iteration:   1180, Loss function: 5.540, Average Loss: 4.404, avg. samples / sec: 59667.98
Iteration:   1180, Loss function: 5.368, Average Loss: 4.390, avg. samples / sec: 59656.21
Iteration:   1180, Loss function: 4.686, Average Loss: 4.404, avg. samples / sec: 59835.90
Iteration:   1180, Loss function: 5.778, Average Loss: 4.368, avg. samples / sec: 59696.90
Iteration:   1180, Loss function: 5.138, Average Loss: 4.413, avg. samples / sec: 59890.34
Iteration:   1180, Loss function: 5.416, Average Loss: 4.390, avg. samples / sec: 59552.38
Iteration:   1180, Loss function: 5.163, Average Loss: 4.350, avg. samples / sec: 59818.96
Iteration:   1180, Loss function: 5.288, Average Loss: 4.422, avg. samples / sec: 59507.52
Iteration:   1180, Loss function: 6.318, Average Loss: 4.401, avg. samples / sec: 59389.90
Iteration:   1180, Loss function: 4.885, Average Loss: 4.385, avg. samples / sec: 59485.99
Iteration:   1180, Loss function: 4.957, Average Loss: 4.391, avg. samples / sec: 59539.52
Iteration:   1180, Loss function: 5.059, Average Loss: 4.375, avg. samples / sec: 59328.47
:::MLL 1558638753.560 epoch_stop: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 819}}
:::MLL 1558638753.561 epoch_start: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 673}}
Iteration:   1200, Loss function: 5.504, Average Loss: 4.420, avg. samples / sec: 57200.79
Iteration:   1200, Loss function: 4.278, Average Loss: 4.387, avg. samples / sec: 57432.45
Iteration:   1200, Loss function: 5.824, Average Loss: 4.394, avg. samples / sec: 57307.86
Iteration:   1200, Loss function: 4.922, Average Loss: 4.426, avg. samples / sec: 56957.65
Iteration:   1200, Loss function: 5.412, Average Loss: 4.396, avg. samples / sec: 57098.21
Iteration:   1200, Loss function: 5.588, Average Loss: 4.381, avg. samples / sec: 57131.48
Iteration:   1200, Loss function: 5.027, Average Loss: 4.427, avg. samples / sec: 57254.89
Iteration:   1200, Loss function: 4.696, Average Loss: 4.410, avg. samples / sec: 57241.42
Iteration:   1200, Loss function: 4.372, Average Loss: 4.409, avg. samples / sec: 57072.02
Iteration:   1200, Loss function: 4.765, Average Loss: 4.361, avg. samples / sec: 57190.34
Iteration:   1200, Loss function: 3.684, Average Loss: 4.407, avg. samples / sec: 57053.72
Iteration:   1200, Loss function: 4.520, Average Loss: 4.392, avg. samples / sec: 57002.19
Iteration:   1200, Loss function: 6.552, Average Loss: 4.404, avg. samples / sec: 57093.01
Iteration:   1200, Loss function: 5.673, Average Loss: 4.397, avg. samples / sec: 56935.36
Iteration:   1200, Loss function: 4.293, Average Loss: 4.398, avg. samples / sec: 57130.44
Iteration:   1220, Loss function: 4.163, Average Loss: 4.417, avg. samples / sec: 58607.02
Iteration:   1220, Loss function: 5.895, Average Loss: 4.401, avg. samples / sec: 58505.10
Iteration:   1220, Loss function: 3.505, Average Loss: 4.440, avg. samples / sec: 58507.27
Iteration:   1220, Loss function: 3.971, Average Loss: 4.395, avg. samples / sec: 58582.90
Iteration:   1220, Loss function: 4.993, Average Loss: 4.405, avg. samples / sec: 58640.19
Iteration:   1220, Loss function: 6.353, Average Loss: 4.404, avg. samples / sec: 58472.55
Iteration:   1220, Loss function: 3.623, Average Loss: 4.372, avg. samples / sec: 58479.90
Iteration:   1220, Loss function: 4.375, Average Loss: 4.430, avg. samples / sec: 58430.95
Iteration:   1220, Loss function: 4.320, Average Loss: 4.403, avg. samples / sec: 58563.72
Iteration:   1220, Loss function: 5.203, Average Loss: 4.424, avg. samples / sec: 58397.56
Iteration:   1220, Loss function: 4.701, Average Loss: 4.390, avg. samples / sec: 58435.70
Iteration:   1220, Loss function: 4.809, Average Loss: 4.419, avg. samples / sec: 58405.64
Iteration:   1220, Loss function: 5.016, Average Loss: 4.393, avg. samples / sec: 58273.95
Iteration:   1220, Loss function: 5.018, Average Loss: 4.416, avg. samples / sec: 58350.05
Iteration:   1220, Loss function: 4.563, Average Loss: 4.411, avg. samples / sec: 57871.07
Iteration:   1240, Loss function: 5.582, Average Loss: 4.428, avg. samples / sec: 59070.20
Iteration:   1240, Loss function: 6.334, Average Loss: 4.443, avg. samples / sec: 59042.36
Iteration:   1240, Loss function: 4.482, Average Loss: 4.446, avg. samples / sec: 59002.61
Iteration:   1240, Loss function: 4.675, Average Loss: 4.405, avg. samples / sec: 58984.31
Iteration:   1240, Loss function: 4.565, Average Loss: 4.371, avg. samples / sec: 59023.84
Iteration:   1240, Loss function: 3.607, Average Loss: 4.412, avg. samples / sec: 58983.45
Iteration:   1240, Loss function: 5.015, Average Loss: 4.427, avg. samples / sec: 58777.05
Iteration:   1240, Loss function: 4.255, Average Loss: 4.411, avg. samples / sec: 58861.18
Iteration:   1240, Loss function: 4.956, Average Loss: 4.421, avg. samples / sec: 59601.19
Iteration:   1240, Loss function: 4.878, Average Loss: 4.418, avg. samples / sec: 59057.10
Iteration:   1240, Loss function: 3.097, Average Loss: 4.400, avg. samples / sec: 59030.58
Iteration:   1240, Loss function: 3.949, Average Loss: 4.427, avg. samples / sec: 58900.32
Iteration:   1240, Loss function: 4.081, Average Loss: 4.403, avg. samples / sec: 58843.73
Iteration:   1240, Loss function: 4.359, Average Loss: 4.406, avg. samples / sec: 58733.50
Iteration:   1240, Loss function: 4.450, Average Loss: 4.414, avg. samples / sec: 58687.93
:::MLL 1558638755.572 epoch_stop: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 819}}
:::MLL 1558638755.572 epoch_start: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 673}}
Iteration:   1260, Loss function: 4.361, Average Loss: 4.407, avg. samples / sec: 60242.51
Iteration:   1260, Loss function: 3.745, Average Loss: 4.423, avg. samples / sec: 60149.46
Iteration:   1260, Loss function: 5.784, Average Loss: 4.376, avg. samples / sec: 60086.75
Iteration:   1260, Loss function: 4.710, Average Loss: 4.419, avg. samples / sec: 60201.31
Iteration:   1260, Loss function: 4.263, Average Loss: 4.432, avg. samples / sec: 60148.56
Iteration:   1260, Loss function: 3.503, Average Loss: 4.411, avg. samples / sec: 59998.91
Iteration:   1260, Loss function: 4.846, Average Loss: 4.421, avg. samples / sec: 60311.58
Iteration:   1260, Loss function: 4.451, Average Loss: 4.444, avg. samples / sec: 59953.75
Iteration:   1260, Loss function: 4.653, Average Loss: 4.450, avg. samples / sec: 59924.21
Iteration:   1260, Loss function: 4.120, Average Loss: 4.431, avg. samples / sec: 60137.96
Iteration:   1260, Loss function: 4.481, Average Loss: 4.413, avg. samples / sec: 60144.07
Iteration:   1260, Loss function: 4.652, Average Loss: 4.429, avg. samples / sec: 60053.95
Iteration:   1260, Loss function: 3.991, Average Loss: 4.429, avg. samples / sec: 60044.49
Iteration:   1260, Loss function: 4.086, Average Loss: 4.417, avg. samples / sec: 60067.06
Iteration:   1260, Loss function: 5.075, Average Loss: 4.430, avg. samples / sec: 59819.51
Iteration:   1280, Loss function: 4.875, Average Loss: 4.452, avg. samples / sec: 55335.89
Iteration:   1280, Loss function: 4.458, Average Loss: 4.418, avg. samples / sec: 55370.87
Iteration:   1280, Loss function: 4.762, Average Loss: 4.436, avg. samples / sec: 55241.82
Iteration:   1280, Loss function: 5.785, Average Loss: 4.418, avg. samples / sec: 55258.19
Iteration:   1280, Loss function: 4.909, Average Loss: 4.438, avg. samples / sec: 55293.12
Iteration:   1280, Loss function: 5.307, Average Loss: 4.430, avg. samples / sec: 55142.97
Iteration:   1280, Loss function: 5.505, Average Loss: 4.419, avg. samples / sec: 55125.71
Iteration:   1280, Loss function: 5.048, Average Loss: 4.439, avg. samples / sec: 55277.74
Iteration:   1280, Loss function: 4.359, Average Loss: 4.424, avg. samples / sec: 55330.35
Iteration:   1280, Loss function: 4.781, Average Loss: 4.426, avg. samples / sec: 55191.06
Iteration:   1280, Loss function: 4.880, Average Loss: 4.460, avg. samples / sec: 55220.82
Iteration:   1280, Loss function: 5.146, Average Loss: 4.428, avg. samples / sec: 55105.56
Iteration:   1280, Loss function: 3.972, Average Loss: 4.380, avg. samples / sec: 55045.10
Iteration:   1280, Loss function: 5.429, Average Loss: 4.441, avg. samples / sec: 55244.59
Iteration:   1280, Loss function: 4.440, Average Loss: 4.432, avg. samples / sec: 55123.64
Iteration:   1300, Loss function: 5.520, Average Loss: 4.438, avg. samples / sec: 57821.42
Iteration:   1300, Loss function: 6.036, Average Loss: 4.446, avg. samples / sec: 57739.53
Iteration:   1300, Loss function: 4.888, Average Loss: 4.454, avg. samples / sec: 57594.57
Iteration:   1300, Loss function: 5.105, Average Loss: 4.430, avg. samples / sec: 57621.67
Iteration:   1300, Loss function: 4.671, Average Loss: 4.437, avg. samples / sec: 57673.95
Iteration:   1300, Loss function: 4.034, Average Loss: 4.430, avg. samples / sec: 57614.02
Iteration:   1300, Loss function: 5.150, Average Loss: 4.469, avg. samples / sec: 57603.09
Iteration:   1300, Loss function: 3.973, Average Loss: 4.425, avg. samples / sec: 57560.11
Iteration:   1300, Loss function: 4.922, Average Loss: 4.388, avg. samples / sec: 57627.05
Iteration:   1300, Loss function: 4.662, Average Loss: 4.444, avg. samples / sec: 57456.69
Iteration:   1300, Loss function: 4.707, Average Loss: 4.439, avg. samples / sec: 57499.85
Iteration:   1300, Loss function: 5.512, Average Loss: 4.423, avg. samples / sec: 57411.54
Iteration:   1300, Loss function: 4.355, Average Loss: 4.449, avg. samples / sec: 57500.36
Iteration:   1300, Loss function: 5.350, Average Loss: 4.459, avg. samples / sec: 57382.76
Iteration:   1300, Loss function: 4.868, Average Loss: 4.428, avg. samples / sec: 57386.01
Iteration:   1320, Loss function: 4.610, Average Loss: 4.436, avg. samples / sec: 60321.31
Iteration:   1320, Loss function: 5.729, Average Loss: 4.429, avg. samples / sec: 60223.95
Iteration:   1320, Loss function: 5.109, Average Loss: 4.400, avg. samples / sec: 60276.88
Iteration:   1320, Loss function: 4.164, Average Loss: 4.438, avg. samples / sec: 60221.79
Iteration:   1320, Loss function: 4.346, Average Loss: 4.447, avg. samples / sec: 60044.77
Iteration:   1320, Loss function: 5.672, Average Loss: 4.446, avg. samples / sec: 60084.29
Iteration:   1320, Loss function: 4.926, Average Loss: 4.447, avg. samples / sec: 60155.52
Iteration:   1320, Loss function: 4.725, Average Loss: 4.453, avg. samples / sec: 60022.75
Iteration:   1320, Loss function: 5.334, Average Loss: 4.470, avg. samples / sec: 60067.73
Iteration:   1320, Loss function: 5.858, Average Loss: 4.459, avg. samples / sec: 60010.51
Iteration:   1320, Loss function: 4.209, Average Loss: 4.465, avg. samples / sec: 60136.24
Iteration:   1320, Loss function: 5.199, Average Loss: 4.429, avg. samples / sec: 60074.79
Iteration:   1320, Loss function: 4.451, Average Loss: 4.461, avg. samples / sec: 60071.54
Iteration:   1320, Loss function: 5.268, Average Loss: 4.449, avg. samples / sec: 59991.30
Iteration:   1320, Loss function: 3.537, Average Loss: 4.431, avg. samples / sec: 60056.05
:::MLL 1558638757.605 epoch_stop: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 819}}
:::MLL 1558638757.606 epoch_start: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 673}}
Iteration:   1340, Loss function: 4.959, Average Loss: 4.445, avg. samples / sec: 58595.45
Iteration:   1340, Loss function: 4.497, Average Loss: 4.451, avg. samples / sec: 58674.54
Iteration:   1340, Loss function: 3.368, Average Loss: 4.462, avg. samples / sec: 58697.24
Iteration:   1340, Loss function: 4.592, Average Loss: 4.456, avg. samples / sec: 58806.70
Iteration:   1340, Loss function: 4.965, Average Loss: 4.435, avg. samples / sec: 58726.72
Iteration:   1340, Loss function: 4.590, Average Loss: 4.459, avg. samples / sec: 58621.19
Iteration:   1340, Loss function: 3.858, Average Loss: 4.437, avg. samples / sec: 58354.64
Iteration:   1340, Loss function: 3.778, Average Loss: 4.432, avg. samples / sec: 58702.65
Iteration:   1340, Loss function: 6.405, Average Loss: 4.435, avg. samples / sec: 58337.05
Iteration:   1340, Loss function: 3.727, Average Loss: 4.475, avg. samples / sec: 58514.41
Iteration:   1340, Loss function: 5.554, Average Loss: 4.466, avg. samples / sec: 58511.64
Iteration:   1340, Loss function: 5.427, Average Loss: 4.403, avg. samples / sec: 58344.08
Iteration:   1340, Loss function: 5.765, Average Loss: 4.461, avg. samples / sec: 58549.15
Iteration:   1340, Loss function: 5.229, Average Loss: 4.455, avg. samples / sec: 58357.20
Iteration:   1340, Loss function: 4.461, Average Loss: 4.444, avg. samples / sec: 58120.79
Iteration:   1360, Loss function: 4.214, Average Loss: 4.446, avg. samples / sec: 60825.06
Iteration:   1360, Loss function: 5.040, Average Loss: 4.461, avg. samples / sec: 60169.05
Iteration:   1360, Loss function: 4.585, Average Loss: 4.459, avg. samples / sec: 60458.23
Iteration:   1360, Loss function: 4.564, Average Loss: 4.449, avg. samples / sec: 60091.03
Iteration:   1360, Loss function: 6.285, Average Loss: 4.466, avg. samples / sec: 60169.03
Iteration:   1360, Loss function: 5.663, Average Loss: 4.441, avg. samples / sec: 60242.41
Iteration:   1360, Loss function: 4.172, Average Loss: 4.409, avg. samples / sec: 60305.59
Iteration:   1360, Loss function: 3.850, Average Loss: 4.480, avg. samples / sec: 60257.96
Iteration:   1360, Loss function: 4.485, Average Loss: 4.447, avg. samples / sec: 60130.65
Iteration:   1360, Loss function: 5.131, Average Loss: 4.469, avg. samples / sec: 60019.02
Iteration:   1360, Loss function: 4.666, Average Loss: 4.434, avg. samples / sec: 60155.44
Iteration:   1360, Loss function: 3.451, Average Loss: 4.433, avg. samples / sec: 60034.92
Iteration:   1360, Loss function: 4.155, Average Loss: 4.465, avg. samples / sec: 60200.54
Iteration:   1360, Loss function: 4.710, Average Loss: 4.465, avg. samples / sec: 60174.11
Iteration:   1360, Loss function: 4.603, Average Loss: 4.457, avg. samples / sec: 59846.26
Iteration:   1380, Loss function: 3.890, Average Loss: 4.453, avg. samples / sec: 59105.80
Iteration:   1380, Loss function: 4.037, Average Loss: 4.443, avg. samples / sec: 59164.86
Iteration:   1380, Loss function: 4.947, Average Loss: 4.466, avg. samples / sec: 58985.99
Iteration:   1380, Loss function: 3.792, Average Loss: 4.410, avg. samples / sec: 59045.25
Iteration:   1380, Loss function: 5.299, Average Loss: 4.440, avg. samples / sec: 59093.33
Iteration:   1380, Loss function: 4.703, Average Loss: 4.449, avg. samples / sec: 59011.23
Iteration:   1380, Loss function: 4.219, Average Loss: 4.470, avg. samples / sec: 58977.40
Iteration:   1380, Loss function: 4.729, Average Loss: 4.469, avg. samples / sec: 59030.58
Iteration:   1380, Loss function: 4.456, Average Loss: 4.467, avg. samples / sec: 59059.40
Iteration:   1380, Loss function: 6.081, Average Loss: 4.471, avg. samples / sec: 59048.22
Iteration:   1380, Loss function: 4.904, Average Loss: 4.459, avg. samples / sec: 59095.74
Iteration:   1380, Loss function: 4.063, Average Loss: 4.452, avg. samples / sec: 58937.86
Iteration:   1380, Loss function: 4.439, Average Loss: 4.451, avg. samples / sec: 58715.34
Iteration:   1380, Loss function: 4.950, Average Loss: 4.483, avg. samples / sec: 58897.54
Iteration:   1380, Loss function: 5.190, Average Loss: 4.460, avg. samples / sec: 58758.01
:::MLL 1558638759.592 epoch_stop: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 819}}
:::MLL 1558638759.592 epoch_start: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 673}}
Iteration:   1400, Loss function: 3.842, Average Loss: 4.483, avg. samples / sec: 59651.29
Iteration:   1400, Loss function: 4.053, Average Loss: 4.447, avg. samples / sec: 59297.97
Iteration:   1400, Loss function: 4.115, Average Loss: 4.440, avg. samples / sec: 59319.16
Iteration:   1400, Loss function: 3.837, Average Loss: 4.451, avg. samples / sec: 59410.61
Iteration:   1400, Loss function: 4.815, Average Loss: 4.460, avg. samples / sec: 59497.79
Iteration:   1400, Loss function: 5.309, Average Loss: 4.465, avg. samples / sec: 59206.07
Iteration:   1400, Loss function: 4.979, Average Loss: 4.466, avg. samples / sec: 59347.06
Iteration:   1400, Loss function: 4.937, Average Loss: 4.470, avg. samples / sec: 59298.86
Iteration:   1400, Loss function: 4.222, Average Loss: 4.473, avg. samples / sec: 59187.89
Iteration:   1400, Loss function: 5.153, Average Loss: 4.472, avg. samples / sec: 59211.34
Iteration:   1400, Loss function: 4.269, Average Loss: 4.451, avg. samples / sec: 59176.23
Iteration:   1400, Loss function: 5.077, Average Loss: 4.454, avg. samples / sec: 59250.75
Iteration:   1400, Loss function: 4.608, Average Loss: 4.456, avg. samples / sec: 59001.30
Iteration:   1400, Loss function: 5.554, Average Loss: 4.471, avg. samples / sec: 59134.24
Iteration:   1400, Loss function: 3.897, Average Loss: 4.407, avg. samples / sec: 59008.83
Iteration:   1420, Loss function: 4.658, Average Loss: 4.463, avg. samples / sec: 59761.35
Iteration:   1420, Loss function: 4.792, Average Loss: 4.472, avg. samples / sec: 59724.27
Iteration:   1420, Loss function: 5.159, Average Loss: 4.449, avg. samples / sec: 59520.71
Iteration:   1420, Loss function: 4.496, Average Loss: 4.483, avg. samples / sec: 59282.13
Iteration:   1420, Loss function: 4.990, Average Loss: 4.471, avg. samples / sec: 59724.83
Iteration:   1420, Loss function: 3.377, Average Loss: 4.461, avg. samples / sec: 59478.13
Iteration:   1420, Loss function: 3.988, Average Loss: 4.472, avg. samples / sec: 59513.02
Iteration:   1420, Loss function: 4.059, Average Loss: 4.472, avg. samples / sec: 59515.43
Iteration:   1420, Loss function: 4.549, Average Loss: 4.446, avg. samples / sec: 59326.45
Iteration:   1420, Loss function: 4.710, Average Loss: 4.471, avg. samples / sec: 59437.67
Iteration:   1420, Loss function: 4.866, Average Loss: 4.414, avg. samples / sec: 59631.47
Iteration:   1420, Loss function: 4.489, Average Loss: 4.474, avg. samples / sec: 59468.90
Iteration:   1420, Loss function: 3.904, Average Loss: 4.452, avg. samples / sec: 59418.32
Iteration:   1420, Loss function: 4.152, Average Loss: 4.452, avg. samples / sec: 59350.83
Iteration:   1420, Loss function: 4.115, Average Loss: 4.451, avg. samples / sec: 59173.00
Iteration:   1440, Loss function: 5.322, Average Loss: 4.478, avg. samples / sec: 57344.40
Iteration:   1440, Loss function: 4.379, Average Loss: 4.469, avg. samples / sec: 57282.00
Iteration:   1440, Loss function: 3.919, Average Loss: 4.466, avg. samples / sec: 57105.04
Iteration:   1440, Loss function: 3.099, Average Loss: 4.451, avg. samples / sec: 57454.67
Iteration:   1440, Loss function: 6.162, Average Loss: 4.459, avg. samples / sec: 57455.80
Iteration:   1440, Loss function: 4.406, Average Loss: 4.470, avg. samples / sec: 57176.31
Iteration:   1440, Loss function: 4.097, Average Loss: 4.462, avg. samples / sec: 57306.85
Iteration:   1440, Loss function: 4.862, Average Loss: 4.452, avg. samples / sec: 57189.88
Iteration:   1440, Loss function: 5.229, Average Loss: 4.473, avg. samples / sec: 57139.56
Iteration:   1440, Loss function: 5.081, Average Loss: 4.472, avg. samples / sec: 56987.11
Iteration:   1440, Loss function: 3.894, Average Loss: 4.484, avg. samples / sec: 57061.09
Iteration:   1440, Loss function: 5.512, Average Loss: 4.476, avg. samples / sec: 57066.19
Iteration:   1440, Loss function: 4.045, Average Loss: 4.471, avg. samples / sec: 57176.98
Iteration:   1440, Loss function: 3.397, Average Loss: 4.420, avg. samples / sec: 57153.47
Iteration:   1440, Loss function: 3.892, Average Loss: 4.449, avg. samples / sec: 56975.85
Iteration:   1460, Loss function: 4.726, Average Loss: 4.456, avg. samples / sec: 60220.45
Iteration:   1460, Loss function: 4.427, Average Loss: 4.468, avg. samples / sec: 59971.77
Iteration:   1460, Loss function: 4.839, Average Loss: 4.478, avg. samples / sec: 60153.85
Iteration:   1460, Loss function: 4.289, Average Loss: 4.447, avg. samples / sec: 60075.10
Iteration:   1460, Loss function: 4.249, Average Loss: 4.465, avg. samples / sec: 59929.20
Iteration:   1460, Loss function: 4.594, Average Loss: 4.463, avg. samples / sec: 60014.88
Iteration:   1460, Loss function: 4.563, Average Loss: 4.471, avg. samples / sec: 60105.82
Iteration:   1460, Loss function: 4.621, Average Loss: 4.422, avg. samples / sec: 60084.91
Iteration:   1460, Loss function: 4.815, Average Loss: 4.477, avg. samples / sec: 59992.96
Iteration:   1460, Loss function: 4.707, Average Loss: 4.473, avg. samples / sec: 59916.87
Iteration:   1460, Loss function: 3.759, Average Loss: 4.463, avg. samples / sec: 59872.43
Iteration:   1460, Loss function: 3.341, Average Loss: 4.451, avg. samples / sec: 59850.10
Iteration:   1460, Loss function: 5.248, Average Loss: 4.482, avg. samples / sec: 59752.45
Iteration:   1460, Loss function: 4.654, Average Loss: 4.475, avg. samples / sec: 59844.59
Iteration:   1460, Loss function: 3.518, Average Loss: 4.486, avg. samples / sec: 59791.67
:::MLL 1558638761.590 epoch_stop: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 819}}
:::MLL 1558638761.591 epoch_start: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 673}}
Iteration:   1480, Loss function: 3.035, Average Loss: 4.465, avg. samples / sec: 58568.03
Iteration:   1480, Loss function: 4.794, Average Loss: 4.465, avg. samples / sec: 58511.18
Iteration:   1480, Loss function: 5.827, Average Loss: 4.484, avg. samples / sec: 58418.11
Iteration:   1480, Loss function: 4.593, Average Loss: 4.476, avg. samples / sec: 58642.31
Iteration:   1480, Loss function: 4.695, Average Loss: 4.471, avg. samples / sec: 58516.62
Iteration:   1480, Loss function: 5.551, Average Loss: 4.477, avg. samples / sec: 58507.90
Iteration:   1480, Loss function: 5.073, Average Loss: 4.473, avg. samples / sec: 58396.30
Iteration:   1480, Loss function: 4.004, Average Loss: 4.446, avg. samples / sec: 58338.21
Iteration:   1480, Loss function: 4.491, Average Loss: 4.466, avg. samples / sec: 58296.68
Iteration:   1480, Loss function: 3.615, Average Loss: 4.486, avg. samples / sec: 58615.36
Iteration:   1480, Loss function: 5.007, Average Loss: 4.453, avg. samples / sec: 58434.29
Iteration:   1480, Loss function: 3.706, Average Loss: 4.476, avg. samples / sec: 58364.72
Iteration:   1480, Loss function: 3.869, Average Loss: 4.416, avg. samples / sec: 58235.25
Iteration:   1480, Loss function: 4.392, Average Loss: 4.458, avg. samples / sec: 58102.84
Iteration:   1480, Loss function: 4.850, Average Loss: 4.462, avg. samples / sec: 58291.16
Iteration:   1500, Loss function: 5.262, Average Loss: 4.450, avg. samples / sec: 58358.87
Iteration:   1500, Loss function: 4.228, Average Loss: 4.464, avg. samples / sec: 58514.02
Iteration:   1500, Loss function: 4.504, Average Loss: 4.483, avg. samples / sec: 58190.12
Iteration:   1500, Loss function: 4.850, Average Loss: 4.470, avg. samples / sec: 58215.43
Iteration:   1500, Loss function: 2.729, Average Loss: 4.465, avg. samples / sec: 58079.30
Iteration:   1500, Loss function: 4.896, Average Loss: 4.478, avg. samples / sec: 58132.10
Iteration:   1500, Loss function: 3.827, Average Loss: 4.463, avg. samples / sec: 58385.10
Iteration:   1500, Loss function: 3.513, Average Loss: 4.489, avg. samples / sec: 58182.29
Iteration:   1500, Loss function: 4.255, Average Loss: 4.423, avg. samples / sec: 58312.29
Iteration:   1500, Loss function: 5.069, Average Loss: 4.456, avg. samples / sec: 58176.19
Iteration:   1500, Loss function: 5.104, Average Loss: 4.482, avg. samples / sec: 58246.88
Iteration:   1500, Loss function: 4.731, Average Loss: 4.476, avg. samples / sec: 58092.16
Iteration:   1500, Loss function: 5.924, Average Loss: 4.471, avg. samples / sec: 57887.57
Iteration:   1500, Loss function: 5.678, Average Loss: 4.484, avg. samples / sec: 57922.95
Iteration:   1500, Loss function: 5.096, Average Loss: 4.477, avg. samples / sec: 57951.03
Iteration:   1520, Loss function: 3.184, Average Loss: 4.452, avg. samples / sec: 60139.65
Iteration:   1520, Loss function: 3.830, Average Loss: 4.469, avg. samples / sec: 60292.10
Iteration:   1520, Loss function: 4.524, Average Loss: 4.490, avg. samples / sec: 60338.97
Iteration:   1520, Loss function: 4.893, Average Loss: 4.470, avg. samples / sec: 60386.19
Iteration:   1520, Loss function: 3.275, Average Loss: 4.464, avg. samples / sec: 60242.71
Iteration:   1520, Loss function: 4.874, Average Loss: 4.486, avg. samples / sec: 60148.17
Iteration:   1520, Loss function: 2.727, Average Loss: 4.476, avg. samples / sec: 60302.08
Iteration:   1520, Loss function: 3.775, Average Loss: 4.463, avg. samples / sec: 60069.59
Iteration:   1520, Loss function: 5.147, Average Loss: 4.478, avg. samples / sec: 60160.16
Iteration:   1520, Loss function: 3.699, Average Loss: 4.493, avg. samples / sec: 60358.07
Iteration:   1520, Loss function: 4.666, Average Loss: 4.460, avg. samples / sec: 60233.88
Iteration:   1520, Loss function: 5.360, Average Loss: 4.477, avg. samples / sec: 60241.71
Iteration:   1520, Loss function: 5.475, Average Loss: 4.479, avg. samples / sec: 60241.35
Iteration:   1520, Loss function: 4.091, Average Loss: 4.427, avg. samples / sec: 60082.94
Iteration:   1520, Loss function: 4.139, Average Loss: 4.479, avg. samples / sec: 59919.72
:::MLL 1558638763.578 epoch_stop: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 819}}
:::MLL 1558638763.579 epoch_start: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 673}}
Iteration:   1540, Loss function: 4.284, Average Loss: 4.434, avg. samples / sec: 59964.72
Iteration:   1540, Loss function: 4.262, Average Loss: 4.448, avg. samples / sec: 59668.82
Iteration:   1540, Loss function: 5.482, Average Loss: 4.481, avg. samples / sec: 59792.64
Iteration:   1540, Loss function: 4.459, Average Loss: 4.482, avg. samples / sec: 60016.23
Iteration:   1540, Loss function: 4.518, Average Loss: 4.478, avg. samples / sec: 59728.09
Iteration:   1540, Loss function: 4.089, Average Loss: 4.489, avg. samples / sec: 59698.79
Iteration:   1540, Loss function: 4.203, Average Loss: 4.478, avg. samples / sec: 59810.22
Iteration:   1540, Loss function: 4.371, Average Loss: 4.468, avg. samples / sec: 59515.96
Iteration:   1540, Loss function: 4.419, Average Loss: 4.494, avg. samples / sec: 59510.10
Iteration:   1540, Loss function: 4.787, Average Loss: 4.475, avg. samples / sec: 59589.11
Iteration:   1540, Loss function: 4.179, Average Loss: 4.463, avg. samples / sec: 59553.38
Iteration:   1540, Loss function: 3.526, Average Loss: 4.462, avg. samples / sec: 59581.51
Iteration:   1540, Loss function: 4.794, Average Loss: 4.454, avg. samples / sec: 59603.93
Iteration:   1540, Loss function: 3.560, Average Loss: 4.484, avg. samples / sec: 59488.75
Iteration:   1540, Loss function: 5.400, Average Loss: 4.471, avg. samples / sec: 59413.51
Iteration:   1560, Loss function: 3.533, Average Loss: 4.488, avg. samples / sec: 57588.59
Iteration:   1560, Loss function: 4.806, Average Loss: 4.478, avg. samples / sec: 57540.13
Iteration:   1560, Loss function: 3.930, Average Loss: 4.431, avg. samples / sec: 57437.72
Iteration:   1560, Loss function: 4.581, Average Loss: 4.462, avg. samples / sec: 57591.63
Iteration:   1560, Loss function: 5.596, Average Loss: 4.476, avg. samples / sec: 57538.56
Iteration:   1560, Loss function: 3.897, Average Loss: 4.484, avg. samples / sec: 57675.11
Iteration:   1560, Loss function: 4.405, Average Loss: 4.478, avg. samples / sec: 57429.13
Iteration:   1560, Loss function: 3.720, Average Loss: 4.467, avg. samples / sec: 57554.23
Iteration:   1560, Loss function: 4.005, Average Loss: 4.472, avg. samples / sec: 57696.36
Iteration:   1560, Loss function: 4.990, Average Loss: 4.456, avg. samples / sec: 57589.89
Iteration:   1560, Loss function: 3.744, Average Loss: 4.491, avg. samples / sec: 57519.09
Iteration:   1560, Loss function: 4.507, Average Loss: 4.476, avg. samples / sec: 57518.76
Iteration:   1560, Loss function: 4.812, Average Loss: 4.442, avg. samples / sec: 57324.06
Iteration:   1560, Loss function: 4.061, Average Loss: 4.479, avg. samples / sec: 57302.80
Iteration:   1560, Loss function: 4.294, Average Loss: 4.461, avg. samples / sec: 57452.85
Iteration:   1580, Loss function: 3.959, Average Loss: 4.464, avg. samples / sec: 58883.04
Iteration:   1580, Loss function: 4.009, Average Loss: 4.476, avg. samples / sec: 58787.25
Iteration:   1580, Loss function: 4.992, Average Loss: 4.430, avg. samples / sec: 58743.17
Iteration:   1580, Loss function: 3.905, Average Loss: 4.465, avg. samples / sec: 58739.52
Iteration:   1580, Loss function: 5.797, Average Loss: 4.484, avg. samples / sec: 58860.91
Iteration:   1580, Loss function: 5.330, Average Loss: 4.478, avg. samples / sec: 58632.55
Iteration:   1580, Loss function: 4.990, Average Loss: 4.472, avg. samples / sec: 58693.04
Iteration:   1580, Loss function: 4.269, Average Loss: 4.462, avg. samples / sec: 58807.63
Iteration:   1580, Loss function: 4.147, Average Loss: 4.488, avg. samples / sec: 58643.82
Iteration:   1580, Loss function: 4.413, Average Loss: 4.485, avg. samples / sec: 58498.21
Iteration:   1580, Loss function: 4.707, Average Loss: 4.461, avg. samples / sec: 58598.05
Iteration:   1580, Loss function: 5.484, Average Loss: 4.446, avg. samples / sec: 58605.37
Iteration:   1580, Loss function: 4.249, Average Loss: 4.485, avg. samples / sec: 58481.43
Iteration:   1580, Loss function: 4.892, Average Loss: 4.477, avg. samples / sec: 58542.73
Iteration:   1580, Loss function: 4.622, Average Loss: 4.479, avg. samples / sec: 58450.17
Iteration:   1600, Loss function: 4.956, Average Loss: 4.490, avg. samples / sec: 60043.03
Iteration:   1600, Loss function: 5.511, Average Loss: 4.492, avg. samples / sec: 60004.32
Iteration:   1600, Loss function: 5.245, Average Loss: 4.437, avg. samples / sec: 59749.24
Iteration:   1600, Loss function: 3.997, Average Loss: 4.463, avg. samples / sec: 59808.22
Iteration:   1600, Loss function: 3.887, Average Loss: 4.477, avg. samples / sec: 60031.80
Iteration:   1600, Loss function: 4.948, Average Loss: 4.480, avg. samples / sec: 59988.97
Iteration:   1600, Loss function: 4.228, Average Loss: 4.485, avg. samples / sec: 59737.82
Iteration:   1600, Loss function: 5.450, Average Loss: 4.463, avg. samples / sec: 59501.41
Iteration:   1600, Loss function: 4.514, Average Loss: 4.473, avg. samples / sec: 59722.52
Iteration:   1600, Loss function: 4.399, Average Loss: 4.446, avg. samples / sec: 59876.70
Iteration:   1600, Loss function: 5.686, Average Loss: 4.472, avg. samples / sec: 59571.00
Iteration:   1600, Loss function: 4.094, Average Loss: 4.467, avg. samples / sec: 59640.23
Iteration:   1600, Loss function: 3.862, Average Loss: 4.484, avg. samples / sec: 59693.41
Iteration:   1600, Loss function: 3.781, Average Loss: 4.461, avg. samples / sec: 59724.40
Iteration:   1600, Loss function: 4.269, Average Loss: 4.483, avg. samples / sec: 59781.63
:::MLL 1558638765.579 epoch_stop: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 819}}
:::MLL 1558638765.579 epoch_start: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 673}}
Iteration:   1620, Loss function: 4.223, Average Loss: 4.466, avg. samples / sec: 58704.55
Iteration:   1620, Loss function: 4.028, Average Loss: 4.479, avg. samples / sec: 58565.08
Iteration:   1620, Loss function: 4.134, Average Loss: 4.476, avg. samples / sec: 58560.07
Iteration:   1620, Loss function: 3.899, Average Loss: 4.462, avg. samples / sec: 58610.07
Iteration:   1620, Loss function: 3.497, Average Loss: 4.481, avg. samples / sec: 58576.30
Iteration:   1620, Loss function: 4.020, Average Loss: 4.494, avg. samples / sec: 58342.17
Iteration:   1620, Loss function: 4.199, Average Loss: 4.464, avg. samples / sec: 58604.24
Iteration:   1620, Loss function: 4.005, Average Loss: 4.460, avg. samples / sec: 58406.56
Iteration:   1620, Loss function: 3.267, Average Loss: 4.469, avg. samples / sec: 58468.35
Iteration:   1620, Loss function: 3.672, Average Loss: 4.484, avg. samples / sec: 58593.23
Iteration:   1620, Loss function: 3.792, Average Loss: 4.445, avg. samples / sec: 58311.59
Iteration:   1620, Loss function: 4.873, Average Loss: 4.490, avg. samples / sec: 58239.44
Iteration:   1620, Loss function: 4.893, Average Loss: 4.472, avg. samples / sec: 58461.61
Iteration:   1620, Loss function: 4.037, Average Loss: 4.447, avg. samples / sec: 58446.05
Iteration:   1620, Loss function: 4.399, Average Loss: 4.483, avg. samples / sec: 58274.67
Iteration:   1640, Loss function: 4.043, Average Loss: 4.468, avg. samples / sec: 58889.22
Iteration:   1640, Loss function: 5.637, Average Loss: 4.480, avg. samples / sec: 58720.84
Iteration:   1640, Loss function: 4.900, Average Loss: 4.449, avg. samples / sec: 58893.38
Iteration:   1640, Loss function: 4.805, Average Loss: 4.484, avg. samples / sec: 58990.31
Iteration:   1640, Loss function: 4.050, Average Loss: 4.459, avg. samples / sec: 58824.79
Iteration:   1640, Loss function: 4.031, Average Loss: 4.488, avg. samples / sec: 58815.88
Iteration:   1640, Loss function: 4.311, Average Loss: 4.476, avg. samples / sec: 58633.62
Iteration:   1640, Loss function: 4.436, Average Loss: 4.466, avg. samples / sec: 58766.19
Iteration:   1640, Loss function: 5.515, Average Loss: 4.477, avg. samples / sec: 58762.00
Iteration:   1640, Loss function: 4.328, Average Loss: 4.493, avg. samples / sec: 58700.62
Iteration:   1640, Loss function: 4.228, Average Loss: 4.470, avg. samples / sec: 58788.18
Iteration:   1640, Loss function: 5.584, Average Loss: 4.467, avg. samples / sec: 58539.27
Iteration:   1640, Loss function: 6.172, Average Loss: 4.479, avg. samples / sec: 58662.01
Iteration:   1640, Loss function: 3.819, Average Loss: 4.459, avg. samples / sec: 58554.42
Iteration:   1640, Loss function: 6.103, Average Loss: 4.448, avg. samples / sec: 58693.75
Iteration:   1660, Loss function: 3.966, Average Loss: 4.474, avg. samples / sec: 58236.99
Iteration:   1660, Loss function: 3.155, Average Loss: 4.486, avg. samples / sec: 58111.71
Iteration:   1660, Loss function: 5.112, Average Loss: 4.464, avg. samples / sec: 58184.09
Iteration:   1660, Loss function: 3.576, Average Loss: 4.483, avg. samples / sec: 57996.84
Iteration:   1660, Loss function: 3.987, Average Loss: 4.472, avg. samples / sec: 58084.57
Iteration:   1660, Loss function: 4.822, Average Loss: 4.474, avg. samples / sec: 58070.97
Iteration:   1660, Loss function: 3.311, Average Loss: 4.488, avg. samples / sec: 58066.48
Iteration:   1660, Loss function: 4.032, Average Loss: 4.466, avg. samples / sec: 58005.87
Iteration:   1660, Loss function: 3.216, Average Loss: 4.459, avg. samples / sec: 57970.50
Iteration:   1660, Loss function: 4.382, Average Loss: 4.450, avg. samples / sec: 57952.82
Iteration:   1660, Loss function: 2.971, Average Loss: 4.485, avg. samples / sec: 57999.85
Iteration:   1660, Loss function: 4.343, Average Loss: 4.442, avg. samples / sec: 58114.56
Iteration:   1660, Loss function: 4.912, Average Loss: 4.463, avg. samples / sec: 57841.93
Iteration:   1660, Loss function: 5.088, Average Loss: 4.478, avg. samples / sec: 57927.33
Iteration:   1660, Loss function: 4.033, Average Loss: 4.458, avg. samples / sec: 57856.58
:::MLL 1558638767.583 epoch_stop: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 819}}
:::MLL 1558638767.583 epoch_start: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 673}}
Iteration:   1680, Loss function: 4.461, Average Loss: 4.474, avg. samples / sec: 60199.97
Iteration:   1680, Loss function: 4.073, Average Loss: 4.472, avg. samples / sec: 59989.48
Iteration:   1680, Loss function: 4.316, Average Loss: 4.458, avg. samples / sec: 60225.16
Iteration:   1680, Loss function: 3.484, Average Loss: 4.459, avg. samples / sec: 60303.68
Iteration:   1680, Loss function: 3.616, Average Loss: 4.461, avg. samples / sec: 60018.43
Iteration:   1680, Loss function: 4.259, Average Loss: 4.484, avg. samples / sec: 59958.32
Iteration:   1680, Loss function: 3.803, Average Loss: 4.460, avg. samples / sec: 60151.59
Iteration:   1680, Loss function: 5.413, Average Loss: 4.475, avg. samples / sec: 60288.79
Iteration:   1680, Loss function: 5.351, Average Loss: 4.439, avg. samples / sec: 60155.41
Iteration:   1680, Loss function: 4.508, Average Loss: 4.471, avg. samples / sec: 60046.38
Iteration:   1680, Loss function: 3.820, Average Loss: 4.482, avg. samples / sec: 60016.36
Iteration:   1680, Loss function: 4.730, Average Loss: 4.452, avg. samples / sec: 60276.39
Iteration:   1680, Loss function: 4.895, Average Loss: 4.445, avg. samples / sec: 60001.26
Iteration:   1680, Loss function: 4.049, Average Loss: 4.483, avg. samples / sec: 59979.88
Iteration:   1680, Loss function: 4.028, Average Loss: 4.490, avg. samples / sec: 59717.34
Iteration:   1700, Loss function: 5.488, Average Loss: 4.492, avg. samples / sec: 59492.19
Iteration:   1700, Loss function: 3.102, Average Loss: 4.439, avg. samples / sec: 59035.11
Iteration:   1700, Loss function: 4.154, Average Loss: 4.479, avg. samples / sec: 59004.31
Iteration:   1700, Loss function: 4.097, Average Loss: 4.454, avg. samples / sec: 58897.32
Iteration:   1700, Loss function: 4.545, Average Loss: 4.476, avg. samples / sec: 58980.48
Iteration:   1700, Loss function: 4.013, Average Loss: 4.469, avg. samples / sec: 59008.22
Iteration:   1700, Loss function: 3.479, Average Loss: 4.476, avg. samples / sec: 59004.31
Iteration:   1700, Loss function: 4.420, Average Loss: 4.456, avg. samples / sec: 58895.42
Iteration:   1700, Loss function: 4.987, Average Loss: 4.449, avg. samples / sec: 59024.65
Iteration:   1700, Loss function: 4.843, Average Loss: 4.458, avg. samples / sec: 58868.56
Iteration:   1700, Loss function: 3.510, Average Loss: 4.471, avg. samples / sec: 58786.34
Iteration:   1700, Loss function: 6.185, Average Loss: 4.462, avg. samples / sec: 58799.27
Iteration:   1700, Loss function: 3.888, Average Loss: 4.441, avg. samples / sec: 58941.78
Iteration:   1700, Loss function: 4.600, Average Loss: 4.468, avg. samples / sec: 58680.94
Iteration:   1700, Loss function: 5.078, Average Loss: 4.479, avg. samples / sec: 58815.05
Iteration:   1720, Loss function: 4.269, Average Loss: 4.474, avg. samples / sec: 59243.30
Iteration:   1720, Loss function: 4.929, Average Loss: 4.459, avg. samples / sec: 59224.48
Iteration:   1720, Loss function: 3.439, Average Loss: 4.456, avg. samples / sec: 59149.36
Iteration:   1720, Loss function: 3.166, Average Loss: 4.464, avg. samples / sec: 59118.72
Iteration:   1720, Loss function: 5.020, Average Loss: 4.494, avg. samples / sec: 58934.51
Iteration:   1720, Loss function: 3.387, Average Loss: 4.476, avg. samples / sec: 59084.83
Iteration:   1720, Loss function: 4.397, Average Loss: 4.450, avg. samples / sec: 59181.78
Iteration:   1720, Loss function: 4.461, Average Loss: 4.481, avg. samples / sec: 58916.42
Iteration:   1720, Loss function: 5.618, Average Loss: 4.444, avg. samples / sec: 58893.82
Iteration:   1720, Loss function: 4.673, Average Loss: 4.470, avg. samples / sec: 59160.93
Iteration:   1720, Loss function: 4.502, Average Loss: 4.455, avg. samples / sec: 59022.67
Iteration:   1720, Loss function: 5.315, Average Loss: 4.483, avg. samples / sec: 59233.61
Iteration:   1720, Loss function: 4.614, Average Loss: 4.478, avg. samples / sec: 58825.26
Iteration:   1720, Loss function: 4.031, Average Loss: 4.459, avg. samples / sec: 58914.13
Iteration:   1720, Loss function: 4.298, Average Loss: 4.459, avg. samples / sec: 58786.68
Iteration:   1740, Loss function: 4.528, Average Loss: 4.470, avg. samples / sec: 59481.34
Iteration:   1740, Loss function: 4.035, Average Loss: 4.473, avg. samples / sec: 59369.31
Iteration:   1740, Loss function: 4.307, Average Loss: 4.477, avg. samples / sec: 59700.51
Iteration:   1740, Loss function: 5.343, Average Loss: 4.479, avg. samples / sec: 59509.25
Iteration:   1740, Loss function: 3.779, Average Loss: 4.456, avg. samples / sec: 59691.84
Iteration:   1740, Loss function: 4.253, Average Loss: 4.462, avg. samples / sec: 59310.97
Iteration:   1740, Loss function: 5.169, Average Loss: 4.453, avg. samples / sec: 59407.50
Iteration:   1740, Loss function: 4.730, Average Loss: 4.453, avg. samples / sec: 59293.05
Iteration:   1740, Loss function: 2.528, Average Loss: 4.484, avg. samples / sec: 59472.91
Iteration:   1740, Loss function: 4.857, Average Loss: 4.462, avg. samples / sec: 59286.57
Iteration:   1740, Loss function: 3.831, Average Loss: 4.444, avg. samples / sec: 59405.80
Iteration:   1740, Loss function: 3.897, Average Loss: 4.492, avg. samples / sec: 59251.10
Iteration:   1740, Loss function: 3.389, Average Loss: 4.458, avg. samples / sec: 59506.79
Iteration:   1740, Loss function: 5.653, Average Loss: 4.459, avg. samples / sec: 59343.34
Iteration:   1740, Loss function: 4.966, Average Loss: 4.473, avg. samples / sec: 59295.82
:::MLL 1558638769.567 epoch_stop: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 819}}
:::MLL 1558638769.568 epoch_start: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 673}}
Iteration:   1760, Loss function: 4.157, Average Loss: 4.457, avg. samples / sec: 59866.75
Iteration:   1760, Loss function: 4.771, Average Loss: 4.483, avg. samples / sec: 59831.88
Iteration:   1760, Loss function: 4.644, Average Loss: 4.456, avg. samples / sec: 59618.26
Iteration:   1760, Loss function: 4.115, Average Loss: 4.461, avg. samples / sec: 59728.80
Iteration:   1760, Loss function: 5.228, Average Loss: 4.468, avg. samples / sec: 59842.76
Iteration:   1760, Loss function: 3.929, Average Loss: 4.473, avg. samples / sec: 59557.26
Iteration:   1760, Loss function: 4.609, Average Loss: 4.469, avg. samples / sec: 59512.79
Iteration:   1760, Loss function: 5.306, Average Loss: 4.450, avg. samples / sec: 59638.04
Iteration:   1760, Loss function: 5.239, Average Loss: 4.455, avg. samples / sec: 59715.77
Iteration:   1760, Loss function: 4.976, Average Loss: 4.479, avg. samples / sec: 59513.75
Iteration:   1760, Loss function: 4.174, Average Loss: 4.469, avg. samples / sec: 59446.27
Iteration:   1760, Loss function: 4.605, Average Loss: 4.462, avg. samples / sec: 59497.62
Iteration:   1760, Loss function: 3.834, Average Loss: 4.442, avg. samples / sec: 59550.24
Iteration:   1760, Loss function: 4.505, Average Loss: 4.452, avg. samples / sec: 59422.91
Iteration:   1760, Loss function: 5.162, Average Loss: 4.484, avg. samples / sec: 58803.27
Iteration:   1780, Loss function: 3.480, Average Loss: 4.460, avg. samples / sec: 58425.45
Iteration:   1780, Loss function: 4.538, Average Loss: 4.464, avg. samples / sec: 58348.19
Iteration:   1780, Loss function: 3.427, Average Loss: 4.455, avg. samples / sec: 58370.95
Iteration:   1780, Loss function: 3.419, Average Loss: 4.463, avg. samples / sec: 58439.12
Iteration:   1780, Loss function: 5.090, Average Loss: 4.467, avg. samples / sec: 58418.50
Iteration:   1780, Loss function: 2.836, Average Loss: 4.477, avg. samples / sec: 58358.70
Iteration:   1780, Loss function: 4.150, Average Loss: 4.487, avg. samples / sec: 59131.76
Iteration:   1780, Loss function: 4.022, Average Loss: 4.456, avg. samples / sec: 58220.31
Iteration:   1780, Loss function: 4.797, Average Loss: 4.472, avg. samples / sec: 58197.55
Iteration:   1780, Loss function: 4.997, Average Loss: 4.452, avg. samples / sec: 58404.41
Iteration:   1780, Loss function: 3.645, Average Loss: 4.455, avg. samples / sec: 58092.02
Iteration:   1780, Loss function: 4.943, Average Loss: 4.440, avg. samples / sec: 58346.50
Iteration:   1780, Loss function: 4.519, Average Loss: 4.446, avg. samples / sec: 58186.32
Iteration:   1780, Loss function: 3.858, Average Loss: 4.482, avg. samples / sec: 58062.94
Iteration:   1780, Loss function: 4.603, Average Loss: 4.455, avg. samples / sec: 58065.88
Iteration:   1800, Loss function: 4.002, Average Loss: 4.452, avg. samples / sec: 60278.17
Iteration:   1800, Loss function: 4.857, Average Loss: 4.448, avg. samples / sec: 59965.18
Iteration:   1800, Loss function: 3.857, Average Loss: 4.486, avg. samples / sec: 60159.01
Iteration:   1800, Loss function: 3.542, Average Loss: 4.460, avg. samples / sec: 59892.91
Iteration:   1800, Loss function: 4.103, Average Loss: 4.462, avg. samples / sec: 59917.94
Iteration:   1800, Loss function: 4.083, Average Loss: 4.477, avg. samples / sec: 59952.22
Iteration:   1800, Loss function: 5.551, Average Loss: 4.465, avg. samples / sec: 59912.77
Iteration:   1800, Loss function: 4.070, Average Loss: 4.457, avg. samples / sec: 59978.28
Iteration:   1800, Loss function: 4.164, Average Loss: 4.432, avg. samples / sec: 60039.09
Iteration:   1800, Loss function: 3.331, Average Loss: 4.489, avg. samples / sec: 59938.56
Iteration:   1800, Loss function: 5.188, Average Loss: 4.457, avg. samples / sec: 59774.73
Iteration:   1800, Loss function: 4.967, Average Loss: 4.453, avg. samples / sec: 59942.48
Iteration:   1800, Loss function: 3.820, Average Loss: 4.442, avg. samples / sec: 59966.48
Iteration:   1800, Loss function: 4.824, Average Loss: 4.469, avg. samples / sec: 59878.81
Iteration:   1800, Loss function: 4.129, Average Loss: 4.452, avg. samples / sec: 59897.87
:::MLL 1558638771.553 epoch_stop: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 819}}
:::MLL 1558638771.553 epoch_start: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 673}}
Iteration:   1820, Loss function: 4.919, Average Loss: 4.455, avg. samples / sec: 59741.23
Iteration:   1820, Loss function: 4.787, Average Loss: 4.465, avg. samples / sec: 59794.16
Iteration:   1820, Loss function: 3.961, Average Loss: 4.450, avg. samples / sec: 59748.25
Iteration:   1820, Loss function: 3.263, Average Loss: 4.439, avg. samples / sec: 59520.16
Iteration:   1820, Loss function: 4.569, Average Loss: 4.454, avg. samples / sec: 59574.38
Iteration:   1820, Loss function: 4.649, Average Loss: 4.487, avg. samples / sec: 59594.43
Iteration:   1820, Loss function: 4.950, Average Loss: 4.455, avg. samples / sec: 59600.10
Iteration:   1820, Loss function: 5.003, Average Loss: 4.461, avg. samples / sec: 59529.16
Iteration:   1820, Loss function: 2.887, Average Loss: 4.435, avg. samples / sec: 59654.09
Iteration:   1820, Loss function: 4.729, Average Loss: 4.465, avg. samples / sec: 59506.21
Iteration:   1820, Loss function: 3.220, Average Loss: 4.470, avg. samples / sec: 59496.81
Iteration:   1820, Loss function: 4.975, Average Loss: 4.455, avg. samples / sec: 59457.28
Iteration:   1820, Loss function: 4.411, Average Loss: 4.483, avg. samples / sec: 59436.14
Iteration:   1820, Loss function: 3.830, Average Loss: 4.448, avg. samples / sec: 59350.36
Iteration:   1820, Loss function: 4.511, Average Loss: 4.436, avg. samples / sec: 59379.77
Iteration:   1840, Loss function: 4.711, Average Loss: 4.446, avg. samples / sec: 57724.70
Iteration:   1840, Loss function: 5.638, Average Loss: 4.454, avg. samples / sec: 57744.40
Iteration:   1840, Loss function: 3.802, Average Loss: 4.434, avg. samples / sec: 57688.02
Iteration:   1840, Loss function: 3.783, Average Loss: 4.459, avg. samples / sec: 57692.25
Iteration:   1840, Loss function: 3.885, Average Loss: 4.433, avg. samples / sec: 57828.14
Iteration:   1840, Loss function: 4.555, Average Loss: 4.487, avg. samples / sec: 57628.06
Iteration:   1840, Loss function: 5.167, Average Loss: 4.449, avg. samples / sec: 57548.00
Iteration:   1840, Loss function: 4.608, Average Loss: 4.457, avg. samples / sec: 57636.83
Iteration:   1840, Loss function: 4.072, Average Loss: 4.448, avg. samples / sec: 57590.71
Iteration:   1840, Loss function: 2.529, Average Loss: 4.464, avg. samples / sec: 57506.30
Iteration:   1840, Loss function: 4.755, Average Loss: 4.451, avg. samples / sec: 57607.05
Iteration:   1840, Loss function: 4.227, Average Loss: 4.439, avg. samples / sec: 57538.39
Iteration:   1840, Loss function: 4.165, Average Loss: 4.469, avg. samples / sec: 57494.10
Iteration:   1840, Loss function: 3.876, Average Loss: 4.445, avg. samples / sec: 57460.04
Iteration:   1840, Loss function: 3.676, Average Loss: 4.474, avg. samples / sec: 57216.74
Iteration:   1860, Loss function: 4.655, Average Loss: 4.469, avg. samples / sec: 58083.87
Iteration:   1860, Loss function: 5.827, Average Loss: 4.446, avg. samples / sec: 57924.61
Iteration:   1860, Loss function: 4.720, Average Loss: 4.432, avg. samples / sec: 57818.91
Iteration:   1860, Loss function: 4.841, Average Loss: 4.430, avg. samples / sec: 57740.47
Iteration:   1860, Loss function: 3.668, Average Loss: 4.456, avg. samples / sec: 57792.64
Iteration:   1860, Loss function: 5.799, Average Loss: 4.445, avg. samples / sec: 57828.54
Iteration:   1860, Loss function: 4.459, Average Loss: 4.440, avg. samples / sec: 57682.69
Iteration:   1860, Loss function: 3.607, Average Loss: 4.468, avg. samples / sec: 58301.05
Iteration:   1860, Loss function: 3.775, Average Loss: 4.440, avg. samples / sec: 57929.76
Iteration:   1860, Loss function: 4.349, Average Loss: 4.458, avg. samples / sec: 57804.51
Iteration:   1860, Loss function: 4.777, Average Loss: 4.442, avg. samples / sec: 57806.20
Iteration:   1860, Loss function: 2.910, Average Loss: 4.451, avg. samples / sec: 57689.89
Iteration:   1860, Loss function: 3.706, Average Loss: 4.458, avg. samples / sec: 57802.88
Iteration:   1860, Loss function: 4.207, Average Loss: 4.443, avg. samples / sec: 57910.88
Iteration:   1860, Loss function: 4.488, Average Loss: 4.487, avg. samples / sec: 57645.98
Iteration:   1880, Loss function: 3.933, Average Loss: 4.441, avg. samples / sec: 59313.02
Iteration:   1880, Loss function: 4.032, Average Loss: 4.455, avg. samples / sec: 59096.33
Iteration:   1880, Loss function: 4.775, Average Loss: 4.435, avg. samples / sec: 59057.03
Iteration:   1880, Loss function: 4.668, Average Loss: 4.452, avg. samples / sec: 59092.54
Iteration:   1880, Loss function: 3.923, Average Loss: 4.467, avg. samples / sec: 58962.91
Iteration:   1880, Loss function: 3.148, Average Loss: 4.433, avg. samples / sec: 59013.92
Iteration:   1880, Loss function: 4.014, Average Loss: 4.441, avg. samples / sec: 59193.26
Iteration:   1880, Loss function: 6.893, Average Loss: 4.463, avg. samples / sec: 59072.03
Iteration:   1880, Loss function: 5.076, Average Loss: 4.461, avg. samples / sec: 59012.79
Iteration:   1880, Loss function: 4.922, Average Loss: 4.444, avg. samples / sec: 58989.81
Iteration:   1880, Loss function: 5.451, Average Loss: 4.444, avg. samples / sec: 58950.31
Iteration:   1880, Loss function: 4.508, Average Loss: 4.447, avg. samples / sec: 58939.68
Iteration:   1880, Loss function: 4.181, Average Loss: 4.443, avg. samples / sec: 58901.60
Iteration:   1880, Loss function: 5.056, Average Loss: 4.472, avg. samples / sec: 58880.78
Iteration:   1880, Loss function: 5.161, Average Loss: 4.489, avg. samples / sec: 58915.22
:::MLL 1558638773.572 epoch_stop: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 819}}
:::MLL 1558638773.572 epoch_start: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 673}}
Iteration:   1900, Loss function: 3.612, Average Loss: 4.441, avg. samples / sec: 59454.04
Iteration:   1900, Loss function: 3.456, Average Loss: 4.459, avg. samples / sec: 59354.31
Iteration:   1900, Loss function: 3.961, Average Loss: 4.439, avg. samples / sec: 59244.42
Iteration:   1900, Loss function: 3.458, Average Loss: 4.450, avg. samples / sec: 59187.57
Iteration:   1900, Loss function: 3.126, Average Loss: 4.436, avg. samples / sec: 58987.54
Iteration:   1900, Loss function: 5.031, Average Loss: 4.463, avg. samples / sec: 59321.33
Iteration:   1900, Loss function: 3.584, Average Loss: 4.441, avg. samples / sec: 59284.45
Iteration:   1900, Loss function: 3.943, Average Loss: 4.426, avg. samples / sec: 59063.07
Iteration:   1900, Loss function: 3.491, Average Loss: 4.433, avg. samples / sec: 59088.75
Iteration:   1900, Loss function: 3.528, Average Loss: 4.439, avg. samples / sec: 59133.80
Iteration:   1900, Loss function: 3.834, Average Loss: 4.456, avg. samples / sec: 59104.88
Iteration:   1900, Loss function: 5.184, Average Loss: 4.440, avg. samples / sec: 59150.95
Iteration:   1900, Loss function: 2.899, Average Loss: 4.461, avg. samples / sec: 59012.86
Iteration:   1900, Loss function: 4.341, Average Loss: 4.452, avg. samples / sec: 58998.01
Iteration:   1900, Loss function: 4.305, Average Loss: 4.482, avg. samples / sec: 59314.94
Iteration:   1920, Loss function: 4.495, Average Loss: 4.439, avg. samples / sec: 60168.51
Iteration:   1920, Loss function: 4.372, Average Loss: 4.440, avg. samples / sec: 60011.81
Iteration:   1920, Loss function: 3.676, Average Loss: 4.424, avg. samples / sec: 59995.10
Iteration:   1920, Loss function: 5.924, Average Loss: 4.436, avg. samples / sec: 59841.41
Iteration:   1920, Loss function: 4.581, Average Loss: 4.481, avg. samples / sec: 60068.26
Iteration:   1920, Loss function: 3.801, Average Loss: 4.452, avg. samples / sec: 59994.34
Iteration:   1920, Loss function: 2.875, Average Loss: 4.453, avg. samples / sec: 60010.71
Iteration:   1920, Loss function: 4.534, Average Loss: 4.427, avg. samples / sec: 59955.41
Iteration:   1920, Loss function: 4.757, Average Loss: 4.438, avg. samples / sec: 59941.21
Iteration:   1920, Loss function: 4.266, Average Loss: 4.439, avg. samples / sec: 59763.65
Iteration:   1920, Loss function: 3.456, Average Loss: 4.434, avg. samples / sec: 59602.98
Iteration:   1920, Loss function: 3.099, Average Loss: 4.442, avg. samples / sec: 59735.89
Iteration:   1920, Loss function: 4.732, Average Loss: 4.451, avg. samples / sec: 59654.60
Iteration:   1920, Loss function: 3.674, Average Loss: 4.460, avg. samples / sec: 59813.09
Iteration:   1920, Loss function: 5.647, Average Loss: 4.450, avg. samples / sec: 59922.96
Iteration:   1940, Loss function: 3.402, Average Loss: 4.435, avg. samples / sec: 59704.64
Iteration:   1940, Loss function: 3.468, Average Loss: 4.433, avg. samples / sec: 59634.50
Iteration:   1940, Loss function: 3.455, Average Loss: 4.433, avg. samples / sec: 59673.34
Iteration:   1940, Loss function: 4.084, Average Loss: 4.472, avg. samples / sec: 59599.45
Iteration:   1940, Loss function: 5.229, Average Loss: 4.429, avg. samples / sec: 59651.77
Iteration:   1940, Loss function: 4.524, Average Loss: 4.433, avg. samples / sec: 59421.90
Iteration:   1940, Loss function: 3.228, Average Loss: 4.436, avg. samples / sec: 59465.43
Iteration:   1940, Loss function: 4.998, Average Loss: 4.428, avg. samples / sec: 59563.85
Iteration:   1940, Loss function: 4.775, Average Loss: 4.443, avg. samples / sec: 59551.52
Iteration:   1940, Loss function: 4.396, Average Loss: 4.447, avg. samples / sec: 59477.28
Iteration:   1940, Loss function: 4.284, Average Loss: 4.460, avg. samples / sec: 59541.36
Iteration:   1940, Loss function: 4.705, Average Loss: 4.450, avg. samples / sec: 59526.47
Iteration:   1940, Loss function: 4.199, Average Loss: 4.448, avg. samples / sec: 59366.79
Iteration:   1940, Loss function: 5.324, Average Loss: 4.440, avg. samples / sec: 59417.45
Iteration:   1940, Loss function: 4.839, Average Loss: 4.425, avg. samples / sec: 59228.71
:::MLL 1558638775.542 epoch_stop: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 819}}
:::MLL 1558638775.542 epoch_start: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 673}}
Iteration:   1960, Loss function: 3.485, Average Loss: 4.420, avg. samples / sec: 59963.68
Iteration:   1960, Loss function: 3.560, Average Loss: 4.426, avg. samples / sec: 59856.00
Iteration:   1960, Loss function: 3.932, Average Loss: 4.439, avg. samples / sec: 59937.03
Iteration:   1960, Loss function: 3.304, Average Loss: 4.428, avg. samples / sec: 59715.31
Iteration:   1960, Loss function: 3.685, Average Loss: 4.467, avg. samples / sec: 59724.65
Iteration:   1960, Loss function: 3.372, Average Loss: 4.432, avg. samples / sec: 59867.42
Iteration:   1960, Loss function: 4.734, Average Loss: 4.441, avg. samples / sec: 59969.73
Iteration:   1960, Loss function: 3.541, Average Loss: 4.442, avg. samples / sec: 59890.19
Iteration:   1960, Loss function: 5.054, Average Loss: 4.427, avg. samples / sec: 59703.98
Iteration:   1960, Loss function: 4.095, Average Loss: 4.419, avg. samples / sec: 60031.49
Iteration:   1960, Loss function: 4.335, Average Loss: 4.432, avg. samples / sec: 59596.55
Iteration:   1960, Loss function: 4.304, Average Loss: 4.457, avg. samples / sec: 59786.12
Iteration:   1960, Loss function: 4.370, Average Loss: 4.428, avg. samples / sec: 59684.99
Iteration:   1960, Loss function: 3.784, Average Loss: 4.428, avg. samples / sec: 59534.24
Iteration:   1960, Loss function: 3.919, Average Loss: 4.431, avg. samples / sec: 59843.19
Iteration:   1980, Loss function: 4.897, Average Loss: 4.425, avg. samples / sec: 59824.77
Iteration:   1980, Loss function: 4.343, Average Loss: 4.418, avg. samples / sec: 59556.38
Iteration:   1980, Loss function: 3.940, Average Loss: 4.424, avg. samples / sec: 59843.67
Iteration:   1980, Loss function: 4.976, Average Loss: 4.436, avg. samples / sec: 59709.59
Iteration:   1980, Loss function: 3.234, Average Loss: 4.434, avg. samples / sec: 59696.42
Iteration:   1980, Loss function: 4.345, Average Loss: 4.435, avg. samples / sec: 59604.06
Iteration:   1980, Loss function: 4.020, Average Loss: 4.416, avg. samples / sec: 59700.79
Iteration:   1980, Loss function: 3.924, Average Loss: 4.427, avg. samples / sec: 59800.02
Iteration:   1980, Loss function: 4.101, Average Loss: 4.430, avg. samples / sec: 59713.95
Iteration:   1980, Loss function: 3.626, Average Loss: 4.423, avg. samples / sec: 59640.69
Iteration:   1980, Loss function: 3.753, Average Loss: 4.431, avg. samples / sec: 59732.04
Iteration:   1980, Loss function: 3.451, Average Loss: 4.450, avg. samples / sec: 59667.93
Iteration:   1980, Loss function: 4.347, Average Loss: 4.426, avg. samples / sec: 59480.09
Iteration:   1980, Loss function: 3.846, Average Loss: 4.430, avg. samples / sec: 59525.24
Iteration:   1980, Loss function: 4.588, Average Loss: 4.467, avg. samples / sec: 59362.36
Iteration:   2000, Loss function: 4.451, Average Loss: 4.441, avg. samples / sec: 59274.57
Iteration:   2000, Loss function: 3.778, Average Loss: 4.430, avg. samples / sec: 59116.29
Iteration:   2000, Loss function: 2.999, Average Loss: 4.431, avg. samples / sec: 59149.26
Iteration:   2000, Loss function: 4.459, Average Loss: 4.410, avg. samples / sec: 59160.21
Iteration:   2000, Loss function: 4.566, Average Loss: 4.462, avg. samples / sec: 59429.65
Iteration:   2000, Loss function: 4.144, Average Loss: 4.431, avg. samples / sec: 59203.06
Iteration:   2000, Loss function: 2.752, Average Loss: 4.421, avg. samples / sec: 58916.89
Iteration:   2000, Loss function: 4.665, Average Loss: 4.427, avg. samples / sec: 59059.13
Iteration:   2000, Loss function: 4.914, Average Loss: 4.416, avg. samples / sec: 59117.25
Iteration:   2000, Loss function: 3.965, Average Loss: 4.431, avg. samples / sec: 59070.57
Iteration:   2000, Loss function: 3.896, Average Loss: 4.418, avg. samples / sec: 58985.86
Iteration:   2000, Loss function: 3.248, Average Loss: 4.409, avg. samples / sec: 58978.80
Iteration:   2000, Loss function: 5.009, Average Loss: 4.424, avg. samples / sec: 59058.22
Iteration:   2000, Loss function: 4.849, Average Loss: 4.423, avg. samples / sec: 59156.01
Iteration:   2000, Loss function: 3.376, Average Loss: 4.422, avg. samples / sec: 58986.83
Iteration:   2020, Loss function: 4.010, Average Loss: 4.420, avg. samples / sec: 59058.79
Iteration:   2020, Loss function: 4.906, Average Loss: 4.457, avg. samples / sec: 58950.41
Iteration:   2020, Loss function: 5.093, Average Loss: 4.430, avg. samples / sec: 58928.54
Iteration:   2020, Loss function: 3.986, Average Loss: 4.424, avg. samples / sec: 59012.42
Iteration:   2020, Loss function: 4.600, Average Loss: 4.419, avg. samples / sec: 59163.54
Iteration:   2020, Loss function: 3.919, Average Loss: 4.423, avg. samples / sec: 58862.46
Iteration:   2020, Loss function: 5.466, Average Loss: 4.412, avg. samples / sec: 58968.04
Iteration:   2020, Loss function: 3.958, Average Loss: 4.420, avg. samples / sec: 58929.28
Iteration:   2020, Loss function: 4.201, Average Loss: 4.414, avg. samples / sec: 58889.42
Iteration:   2020, Loss function: 5.066, Average Loss: 4.412, avg. samples / sec: 58916.69
Iteration:   2020, Loss function: 4.396, Average Loss: 4.441, avg. samples / sec: 58789.38
Iteration:   2020, Loss function: 4.211, Average Loss: 4.403, avg. samples / sec: 58754.16
Iteration:   2020, Loss function: 3.710, Average Loss: 4.421, avg. samples / sec: 58758.11
Iteration:   2020, Loss function: 4.897, Average Loss: 4.418, avg. samples / sec: 58803.46
Iteration:   2020, Loss function: 2.552, Average Loss: 4.411, avg. samples / sec: 58651.02
:::MLL 1558638777.522 epoch_stop: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 819}}
:::MLL 1558638777.522 epoch_start: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 673}}
Iteration:   2040, Loss function: 4.926, Average Loss: 4.437, avg. samples / sec: 59784.82
Iteration:   2040, Loss function: 4.207, Average Loss: 4.409, avg. samples / sec: 59684.61
Iteration:   2040, Loss function: 4.800, Average Loss: 4.402, avg. samples / sec: 59748.68
Iteration:   2040, Loss function: 3.999, Average Loss: 4.407, avg. samples / sec: 59640.51
Iteration:   2040, Loss function: 4.476, Average Loss: 4.452, avg. samples / sec: 59472.96
Iteration:   2040, Loss function: 4.354, Average Loss: 4.416, avg. samples / sec: 59478.78
Iteration:   2040, Loss function: 3.625, Average Loss: 4.418, avg. samples / sec: 59527.95
Iteration:   2040, Loss function: 4.122, Average Loss: 4.427, avg. samples / sec: 59439.77
Iteration:   2040, Loss function: 4.642, Average Loss: 4.407, avg. samples / sec: 59482.22
Iteration:   2040, Loss function: 3.943, Average Loss: 4.403, avg. samples / sec: 59790.91
Iteration:   2040, Loss function: 3.397, Average Loss: 4.414, avg. samples / sec: 59391.08
Iteration:   2040, Loss function: 5.281, Average Loss: 4.414, avg. samples / sec: 59392.30
Iteration:   2040, Loss function: 4.528, Average Loss: 4.414, avg. samples / sec: 59549.73
Iteration:   2040, Loss function: 4.353, Average Loss: 4.412, avg. samples / sec: 59601.19
Iteration:   2040, Loss function: 3.553, Average Loss: 4.417, avg. samples / sec: 59410.71
Iteration:   2060, Loss function: 4.019, Average Loss: 4.404, avg. samples / sec: 60391.02
Iteration:   2060, Loss function: 5.187, Average Loss: 4.411, avg. samples / sec: 60450.19
Iteration:   2060, Loss function: 4.986, Average Loss: 4.405, avg. samples / sec: 60324.77
Iteration:   2060, Loss function: 3.903, Average Loss: 4.396, avg. samples / sec: 60325.60
Iteration:   2060, Loss function: 3.723, Average Loss: 4.450, avg. samples / sec: 60408.26
Iteration:   2060, Loss function: 5.017, Average Loss: 4.406, avg. samples / sec: 60489.03
Iteration:   2060, Loss function: 5.500, Average Loss: 4.429, avg. samples / sec: 60430.36
Iteration:   2060, Loss function: 4.183, Average Loss: 4.406, avg. samples / sec: 60521.63
Iteration:   2060, Loss function: 5.051, Average Loss: 4.410, avg. samples / sec: 60497.42
Iteration:   2060, Loss function: 3.602, Average Loss: 4.398, avg. samples / sec: 60443.47
Iteration:   2060, Loss function: 6.119, Average Loss: 4.413, avg. samples / sec: 60480.33
Iteration:   2060, Loss function: 4.532, Average Loss: 4.413, avg. samples / sec: 60413.65
Iteration:   2060, Loss function: 3.580, Average Loss: 4.411, avg. samples / sec: 60298.39
Iteration:   2060, Loss function: 3.795, Average Loss: 4.433, avg. samples / sec: 60053.70
Iteration:   2060, Loss function: 3.395, Average Loss: 4.404, avg. samples / sec: 60312.74
Iteration:   2080, Loss function: 4.632, Average Loss: 4.445, avg. samples / sec: 59543.42
Iteration:   2080, Loss function: 4.201, Average Loss: 4.427, avg. samples / sec: 59652.55
Iteration:   2080, Loss function: 3.121, Average Loss: 4.403, avg. samples / sec: 59480.19
Iteration:   2080, Loss function: 5.443, Average Loss: 4.404, avg. samples / sec: 59431.35
Iteration:   2080, Loss function: 3.969, Average Loss: 4.404, avg. samples / sec: 59370.94
Iteration:   2080, Loss function: 3.645, Average Loss: 4.407, avg. samples / sec: 59451.76
Iteration:   2080, Loss function: 4.339, Average Loss: 4.402, avg. samples / sec: 59351.48
Iteration:   2080, Loss function: 3.764, Average Loss: 4.404, avg. samples / sec: 59303.71
Iteration:   2080, Loss function: 3.299, Average Loss: 4.398, avg. samples / sec: 59478.16
Iteration:   2080, Loss function: 5.265, Average Loss: 4.406, avg. samples / sec: 59347.31
Iteration:   2080, Loss function: 4.294, Average Loss: 4.397, avg. samples / sec: 59344.61
Iteration:   2080, Loss function: 4.127, Average Loss: 4.402, avg. samples / sec: 59396.71
Iteration:   2080, Loss function: 3.961, Average Loss: 4.425, avg. samples / sec: 59219.48
Iteration:   2080, Loss function: 3.977, Average Loss: 4.389, avg. samples / sec: 59142.24
Iteration:   2080, Loss function: 3.733, Average Loss: 4.405, avg. samples / sec: 58371.15
:::MLL 1558638779.499 epoch_stop: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 819}}
:::MLL 1558638779.500 epoch_start: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 673}}
Iteration:   2100, Loss function: 5.025, Average Loss: 4.398, avg. samples / sec: 59121.32
Iteration:   2100, Loss function: 4.358, Average Loss: 4.392, avg. samples / sec: 59167.41
Iteration:   2100, Loss function: 3.761, Average Loss: 4.417, avg. samples / sec: 59260.14
Iteration:   2100, Loss function: 3.620, Average Loss: 4.399, avg. samples / sec: 58987.96
Iteration:   2100, Loss function: 3.297, Average Loss: 4.438, avg. samples / sec: 58896.09
Iteration:   2100, Loss function: 3.803, Average Loss: 4.398, avg. samples / sec: 59062.62
Iteration:   2100, Loss function: 4.366, Average Loss: 4.393, avg. samples / sec: 59089.47
Iteration:   2100, Loss function: 2.313, Average Loss: 4.398, avg. samples / sec: 59113.98
Iteration:   2100, Loss function: 4.977, Average Loss: 4.421, avg. samples / sec: 58917.14
Iteration:   2100, Loss function: 4.083, Average Loss: 4.390, avg. samples / sec: 59056.83
Iteration:   2100, Loss function: 3.519, Average Loss: 4.389, avg. samples / sec: 59239.96
Iteration:   2100, Loss function: 4.371, Average Loss: 4.404, avg. samples / sec: 58881.44
Iteration:   2100, Loss function: 4.146, Average Loss: 4.408, avg. samples / sec: 60047.99
Iteration:   2100, Loss function: 4.174, Average Loss: 4.396, avg. samples / sec: 58969.60
Iteration:   2100, Loss function: 4.257, Average Loss: 4.396, avg. samples / sec: 59002.21
Iteration:   2120, Loss function: 4.700, Average Loss: 4.419, avg. samples / sec: 57638.43
Iteration:   2120, Loss function: 5.460, Average Loss: 4.407, avg. samples / sec: 57689.73
Iteration:   2120, Loss function: 4.718, Average Loss: 4.437, avg. samples / sec: 57548.03
Iteration:   2120, Loss function: 4.613, Average Loss: 4.391, avg. samples / sec: 57507.05
Iteration:   2120, Loss function: 4.427, Average Loss: 4.395, avg. samples / sec: 57549.53
Iteration:   2120, Loss function: 4.310, Average Loss: 4.392, avg. samples / sec: 57647.74
Iteration:   2120, Loss function: 3.911, Average Loss: 4.418, avg. samples / sec: 57498.70
Iteration:   2120, Loss function: 3.581, Average Loss: 4.396, avg. samples / sec: 57501.42
Iteration:   2120, Loss function: 4.159, Average Loss: 4.380, avg. samples / sec: 57550.47
Iteration:   2120, Loss function: 3.931, Average Loss: 4.394, avg. samples / sec: 57646.78
Iteration:   2120, Loss function: 5.306, Average Loss: 4.396, avg. samples / sec: 57470.65
Iteration:   2120, Loss function: 3.697, Average Loss: 4.386, avg. samples / sec: 57510.24
Iteration:   2120, Loss function: 4.696, Average Loss: 4.397, avg. samples / sec: 57380.42
Iteration:   2120, Loss function: 3.929, Average Loss: 4.389, avg. samples / sec: 57432.34
Iteration:   2120, Loss function: 4.134, Average Loss: 4.398, avg. samples / sec: 57464.86
Iteration:   2140, Loss function: 4.684, Average Loss: 4.380, avg. samples / sec: 57992.28
Iteration:   2140, Loss function: 3.754, Average Loss: 4.390, avg. samples / sec: 57937.47
Iteration:   2140, Loss function: 4.501, Average Loss: 4.389, avg. samples / sec: 57849.12
Iteration:   2140, Loss function: 2.987, Average Loss: 4.390, avg. samples / sec: 57904.84
Iteration:   2140, Loss function: 4.097, Average Loss: 4.406, avg. samples / sec: 57805.27
Iteration:   2140, Loss function: 3.378, Average Loss: 4.434, avg. samples / sec: 57813.17
Iteration:   2140, Loss function: 3.670, Average Loss: 4.379, avg. samples / sec: 57843.21
Iteration:   2140, Loss function: 4.543, Average Loss: 4.386, avg. samples / sec: 57849.00
Iteration:   2140, Loss function: 3.550, Average Loss: 4.387, avg. samples / sec: 57756.09
Iteration:   2140, Loss function: 3.774, Average Loss: 4.412, avg. samples / sec: 57767.31
Iteration:   2140, Loss function: 4.041, Average Loss: 4.394, avg. samples / sec: 57757.56
Iteration:   2140, Loss function: 3.994, Average Loss: 4.394, avg. samples / sec: 57736.21
Iteration:   2140, Loss function: 2.545, Average Loss: 4.409, avg. samples / sec: 57614.80
Iteration:   2140, Loss function: 5.552, Average Loss: 4.390, avg. samples / sec: 57721.62
Iteration:   2140, Loss function: 5.050, Average Loss: 4.397, avg. samples / sec: 57791.40
Iteration:   2160, Loss function: 3.125, Average Loss: 4.397, avg. samples / sec: 57759.83
Iteration:   2160, Loss function: 4.139, Average Loss: 4.386, avg. samples / sec: 57486.62
Iteration:   2160, Loss function: 4.369, Average Loss: 4.376, avg. samples / sec: 57511.67
Iteration:   2160, Loss function: 4.099, Average Loss: 4.410, avg. samples / sec: 57556.53
Iteration:   2160, Loss function: 3.771, Average Loss: 4.396, avg. samples / sec: 57595.68
Iteration:   2160, Loss function: 4.699, Average Loss: 4.373, avg. samples / sec: 57434.35
Iteration:   2160, Loss function: 3.836, Average Loss: 4.387, avg. samples / sec: 57451.25
Iteration:   2160, Loss function: 4.069, Average Loss: 4.396, avg. samples / sec: 57444.11
Iteration:   2160, Loss function: 4.474, Average Loss: 4.386, avg. samples / sec: 57550.80
Iteration:   2160, Loss function: 3.592, Average Loss: 4.382, avg. samples / sec: 57486.88
Iteration:   2160, Loss function: 4.727, Average Loss: 4.431, avg. samples / sec: 57436.71
Iteration:   2160, Loss function: 4.165, Average Loss: 4.398, avg. samples / sec: 57542.03
Iteration:   2160, Loss function: 3.369, Average Loss: 4.382, avg. samples / sec: 57407.35
Iteration:   2160, Loss function: 3.876, Average Loss: 4.383, avg. samples / sec: 57472.11
Iteration:   2160, Loss function: 3.885, Average Loss: 4.391, avg. samples / sec: 57533.60
:::MLL 1558638781.526 epoch_stop: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 819}}
:::MLL 1558638781.527 epoch_start: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 673}}
Iteration:   2180, Loss function: 4.310, Average Loss: 4.394, avg. samples / sec: 59650.63
Iteration:   2180, Loss function: 3.504, Average Loss: 4.369, avg. samples / sec: 59516.61
Iteration:   2180, Loss function: 6.387, Average Loss: 4.427, avg. samples / sec: 59575.26
Iteration:   2180, Loss function: 4.084, Average Loss: 4.398, avg. samples / sec: 59505.33
Iteration:   2180, Loss function: 3.842, Average Loss: 4.407, avg. samples / sec: 59487.37
Iteration:   2180, Loss function: 4.880, Average Loss: 4.378, avg. samples / sec: 59554.47
Iteration:   2180, Loss function: 3.040, Average Loss: 4.387, avg. samples / sec: 59457.81
Iteration:   2180, Loss function: 5.356, Average Loss: 4.369, avg. samples / sec: 59445.37
Iteration:   2180, Loss function: 4.637, Average Loss: 4.379, avg. samples / sec: 59489.63
Iteration:   2180, Loss function: 4.116, Average Loss: 4.393, avg. samples / sec: 59451.11
Iteration:   2180, Loss function: 3.960, Average Loss: 4.386, avg. samples / sec: 59416.29
Iteration:   2180, Loss function: 4.585, Average Loss: 4.385, avg. samples / sec: 59385.67
Iteration:   2180, Loss function: 4.657, Average Loss: 4.384, avg. samples / sec: 59350.18
Iteration:   2180, Loss function: 4.692, Average Loss: 4.385, avg. samples / sec: 59210.72
Iteration:   2180, Loss function: 4.960, Average Loss: 4.393, avg. samples / sec: 59265.30
Iteration:   2200, Loss function: 3.366, Average Loss: 4.400, avg. samples / sec: 60543.50
Iteration:   2200, Loss function: 4.102, Average Loss: 4.371, avg. samples / sec: 60598.64
Iteration:   2200, Loss function: 3.658, Average Loss: 4.392, avg. samples / sec: 60283.25
Iteration:   2200, Loss function: 4.540, Average Loss: 4.369, avg. samples / sec: 60474.29
Iteration:   2200, Loss function: 4.562, Average Loss: 4.379, avg. samples / sec: 60726.48
Iteration:   2200, Loss function: 4.137, Average Loss: 4.376, avg. samples / sec: 60483.29
Iteration:   2200, Loss function: 4.435, Average Loss: 4.378, avg. samples / sec: 60498.77
Iteration:   2200, Loss function: 4.276, Average Loss: 4.401, avg. samples / sec: 60426.16
Iteration:   2200, Loss function: 3.568, Average Loss: 4.388, avg. samples / sec: 60608.33
Iteration:   2200, Loss function: 3.995, Average Loss: 4.380, avg. samples / sec: 60564.55
Iteration:   2200, Loss function: 3.950, Average Loss: 4.382, avg. samples / sec: 60432.51
Iteration:   2200, Loss function: 5.645, Average Loss: 4.390, avg. samples / sec: 60698.42
Iteration:   2200, Loss function: 3.438, Average Loss: 4.421, avg. samples / sec: 60367.02
Iteration:   2200, Loss function: 3.964, Average Loss: 4.389, avg. samples / sec: 60458.12
Iteration:   2200, Loss function: 4.065, Average Loss: 4.380, avg. samples / sec: 60411.76
Iteration:   2220, Loss function: 4.756, Average Loss: 4.413, avg. samples / sec: 58336.26
Iteration:   2220, Loss function: 4.079, Average Loss: 4.385, avg. samples / sec: 58316.10
Iteration:   2220, Loss function: 5.673, Average Loss: 4.377, avg. samples / sec: 58450.34
Iteration:   2220, Loss function: 3.834, Average Loss: 4.383, avg. samples / sec: 58144.96
Iteration:   2220, Loss function: 5.364, Average Loss: 4.380, avg. samples / sec: 58213.63
Iteration:   2220, Loss function: 4.340, Average Loss: 4.397, avg. samples / sec: 58043.49
Iteration:   2220, Loss function: 5.530, Average Loss: 4.368, avg. samples / sec: 58044.74
Iteration:   2220, Loss function: 3.937, Average Loss: 4.392, avg. samples / sec: 58158.73
Iteration:   2220, Loss function: 3.743, Average Loss: 4.370, avg. samples / sec: 58087.66
Iteration:   2220, Loss function: 4.304, Average Loss: 4.383, avg. samples / sec: 58130.45
Iteration:   2220, Loss function: 4.374, Average Loss: 4.378, avg. samples / sec: 58089.55
Iteration:   2220, Loss function: 3.821, Average Loss: 4.373, avg. samples / sec: 57989.09
Iteration:   2220, Loss function: 6.569, Average Loss: 4.380, avg. samples / sec: 57975.61
Iteration:   2220, Loss function: 2.727, Average Loss: 4.362, avg. samples / sec: 57910.17
Iteration:   2220, Loss function: 2.205, Average Loss: 4.379, avg. samples / sec: 58000.42
:::MLL 1558638783.507 epoch_stop: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 819}}
:::MLL 1558638783.508 epoch_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 673}}
Iteration:   2240, Loss function: 4.491, Average Loss: 4.374, avg. samples / sec: 60131.59
Iteration:   2240, Loss function: 3.767, Average Loss: 4.370, avg. samples / sec: 60359.49
Iteration:   2240, Loss function: 3.585, Average Loss: 4.362, avg. samples / sec: 60252.01
Iteration:   2240, Loss function: 4.723, Average Loss: 4.358, avg. samples / sec: 60362.21
Iteration:   2240, Loss function: 4.167, Average Loss: 4.369, avg. samples / sec: 60162.73
Iteration:   2240, Loss function: 5.148, Average Loss: 4.383, avg. samples / sec: 60058.97
Iteration:   2240, Loss function: 3.145, Average Loss: 4.374, avg. samples / sec: 60066.34
Iteration:   2240, Loss function: 3.274, Average Loss: 4.376, avg. samples / sec: 60179.23
Iteration:   2240, Loss function: 4.266, Average Loss: 4.379, avg. samples / sec: 60120.82
Iteration:   2240, Loss function: 5.291, Average Loss: 4.394, avg. samples / sec: 60008.28
Iteration:   2240, Loss function: 4.134, Average Loss: 4.405, avg. samples / sec: 59779.52
Iteration:   2240, Loss function: 5.048, Average Loss: 4.390, avg. samples / sec: 59930.40
Iteration:   2240, Loss function: 4.296, Average Loss: 4.368, avg. samples / sec: 60100.90
Iteration:   2240, Loss function: 3.497, Average Loss: 4.367, avg. samples / sec: 59755.24
Iteration:   2240, Loss function: 4.769, Average Loss: 4.376, avg. samples / sec: 59918.25
Iteration:   2260, Loss function: 4.197, Average Loss: 4.374, avg. samples / sec: 59795.96
Iteration:   2260, Loss function: 4.694, Average Loss: 4.357, avg. samples / sec: 59604.51
Iteration:   2260, Loss function: 4.119, Average Loss: 4.391, avg. samples / sec: 59754.00
Iteration:   2260, Loss function: 4.386, Average Loss: 4.397, avg. samples / sec: 59849.41
Iteration:   2260, Loss function: 4.226, Average Loss: 4.360, avg. samples / sec: 59608.98
Iteration:   2260, Loss function: 4.126, Average Loss: 4.376, avg. samples / sec: 59682.34
Iteration:   2260, Loss function: 4.478, Average Loss: 4.380, avg. samples / sec: 59549.21
Iteration:   2260, Loss function: 5.865, Average Loss: 4.372, avg. samples / sec: 59834.88
Iteration:   2260, Loss function: 2.501, Average Loss: 4.352, avg. samples / sec: 59466.64
Iteration:   2260, Loss function: 4.650, Average Loss: 4.361, avg. samples / sec: 59753.65
Iteration:   2260, Loss function: 4.293, Average Loss: 4.365, avg. samples / sec: 59321.18
Iteration:   2260, Loss function: 4.309, Average Loss: 4.386, avg. samples / sec: 59600.93
Iteration:   2260, Loss function: 3.591, Average Loss: 4.375, avg. samples / sec: 59321.45
Iteration:   2260, Loss function: 4.074, Average Loss: 4.367, avg. samples / sec: 59528.13
Iteration:   2260, Loss function: 4.004, Average Loss: 4.370, avg. samples / sec: 59089.91
Iteration:   2280, Loss function: 4.307, Average Loss: 4.365, avg. samples / sec: 56880.30
Iteration:   2280, Loss function: 4.727, Average Loss: 4.363, avg. samples / sec: 56651.28
Iteration:   2280, Loss function: 4.649, Average Loss: 4.376, avg. samples / sec: 56721.23
Iteration:   2280, Loss function: 3.166, Average Loss: 4.353, avg. samples / sec: 56395.09
Iteration:   2280, Loss function: 5.982, Average Loss: 4.375, avg. samples / sec: 56450.54
Iteration:   2280, Loss function: 4.275, Average Loss: 4.350, avg. samples / sec: 56502.67
Iteration:   2280, Loss function: 4.260, Average Loss: 4.370, avg. samples / sec: 56653.83
Iteration:   2280, Loss function: 4.469, Average Loss: 4.355, avg. samples / sec: 56326.43
Iteration:   2280, Loss function: 2.831, Average Loss: 4.359, avg. samples / sec: 56464.14
Iteration:   2280, Loss function: 4.399, Average Loss: 4.387, avg. samples / sec: 56521.07
Iteration:   2280, Loss function: 3.496, Average Loss: 4.389, avg. samples / sec: 56276.95
Iteration:   2280, Loss function: 4.124, Average Loss: 4.371, avg. samples / sec: 56309.82
Iteration:   2280, Loss function: 4.427, Average Loss: 4.368, avg. samples / sec: 56354.00
Iteration:   2280, Loss function: 5.610, Average Loss: 4.397, avg. samples / sec: 56212.90
Iteration:   2280, Loss function: 4.934, Average Loss: 4.371, avg. samples / sec: 56089.94
:::MLL 1558638784.850 eval_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.98 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.98 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.98 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.98 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.98 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.98 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.98 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.98 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.98 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.98 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.98 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.98 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.98 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.98 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.98 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.31s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.32s)
DONE (t=0.33s)
DONE (t=0.35s)
DONE (t=2.59s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.14372
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.27662
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.13498
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.03129
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.14581
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.23956
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.16072
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.23216
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.24295
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05619
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24853
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.38689
Current AP: 0.14372 AP goal: 0.23000
:::MLL 1558638788.796 eval_accuracy: {"value": 0.14372476881200252, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 389}}
:::MLL 1558638788.816 eval_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 392}}
:::MLL 1558638788.826 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 804}}
:::MLL 1558638788.826 block_start: {"value": null, "metadata": {"first_epoch_num": 33, "epoch_count": 10.915354834308324, "file": "train.py", "lineno": 813}}
Iteration:   2300, Loss function: 3.810, Average Loss: 4.350, avg. samples / sec: 7183.30
Iteration:   2300, Loss function: 4.171, Average Loss: 4.364, avg. samples / sec: 7181.55
Iteration:   2300, Loss function: 5.273, Average Loss: 4.353, avg. samples / sec: 7181.95
Iteration:   2300, Loss function: 4.606, Average Loss: 4.367, avg. samples / sec: 7183.04
Iteration:   2300, Loss function: 4.628, Average Loss: 4.377, avg. samples / sec: 7180.07
Iteration:   2300, Loss function: 4.183, Average Loss: 4.363, avg. samples / sec: 7178.50
Iteration:   2300, Loss function: 4.212, Average Loss: 4.351, avg. samples / sec: 7180.42
Iteration:   2300, Loss function: 4.425, Average Loss: 4.390, avg. samples / sec: 7181.76
Iteration:   2300, Loss function: 3.507, Average Loss: 4.349, avg. samples / sec: 7179.92
Iteration:   2300, Loss function: 4.814, Average Loss: 4.359, avg. samples / sec: 7178.43
Iteration:   2300, Loss function: 5.582, Average Loss: 4.373, avg. samples / sec: 7182.54
Iteration:   2300, Loss function: 4.854, Average Loss: 4.385, avg. samples / sec: 7179.40
Iteration:   2300, Loss function: 3.326, Average Loss: 4.388, avg. samples / sec: 7180.00
Iteration:   2300, Loss function: 6.043, Average Loss: 4.371, avg. samples / sec: 7177.69
Iteration:   2300, Loss function: 3.593, Average Loss: 4.363, avg. samples / sec: 7178.45
:::MLL 1558638789.631 epoch_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 819}}
:::MLL 1558638789.631 epoch_start: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 673}}
Iteration:   2320, Loss function: 4.138, Average Loss: 4.339, avg. samples / sec: 58945.48
Iteration:   2320, Loss function: 3.493, Average Loss: 4.390, avg. samples / sec: 59124.02
Iteration:   2320, Loss function: 4.888, Average Loss: 4.367, avg. samples / sec: 59031.80
Iteration:   2320, Loss function: 4.237, Average Loss: 4.362, avg. samples / sec: 58869.88
Iteration:   2320, Loss function: 4.375, Average Loss: 4.344, avg. samples / sec: 58783.18
Iteration:   2320, Loss function: 4.313, Average Loss: 4.348, avg. samples / sec: 58920.56
Iteration:   2320, Loss function: 3.340, Average Loss: 4.352, avg. samples / sec: 59056.56
Iteration:   2320, Loss function: 4.483, Average Loss: 4.380, avg. samples / sec: 58920.78
Iteration:   2320, Loss function: 5.202, Average Loss: 4.386, avg. samples / sec: 58770.43
Iteration:   2320, Loss function: 4.474, Average Loss: 4.365, avg. samples / sec: 58941.98
Iteration:   2320, Loss function: 4.252, Average Loss: 4.356, avg. samples / sec: 58774.70
Iteration:   2320, Loss function: 3.702, Average Loss: 4.358, avg. samples / sec: 58627.21
Iteration:   2320, Loss function: 4.668, Average Loss: 4.352, avg. samples / sec: 58607.68
Iteration:   2320, Loss function: 3.740, Average Loss: 4.357, avg. samples / sec: 58527.63
Iteration:   2320, Loss function: 4.414, Average Loss: 4.373, avg. samples / sec: 58476.34
Iteration:   2340, Loss function: 4.906, Average Loss: 4.348, avg. samples / sec: 57222.37
Iteration:   2340, Loss function: 4.913, Average Loss: 4.346, avg. samples / sec: 57196.61
Iteration:   2340, Loss function: 3.113, Average Loss: 4.363, avg. samples / sec: 57080.94
Iteration:   2340, Loss function: 5.891, Average Loss: 4.360, avg. samples / sec: 57275.04
Iteration:   2340, Loss function: 3.608, Average Loss: 4.355, avg. samples / sec: 57069.59
Iteration:   2340, Loss function: 4.423, Average Loss: 4.373, avg. samples / sec: 57217.02
Iteration:   2340, Loss function: 5.289, Average Loss: 4.351, avg. samples / sec: 57270.06
Iteration:   2340, Loss function: 5.139, Average Loss: 4.360, avg. samples / sec: 57240.61
Iteration:   2340, Loss function: 3.136, Average Loss: 4.386, avg. samples / sec: 57041.69
Iteration:   2340, Loss function: 3.912, Average Loss: 4.338, avg. samples / sec: 57008.99
Iteration:   2340, Loss function: 3.975, Average Loss: 4.348, avg. samples / sec: 57385.24
Iteration:   2340, Loss function: 4.269, Average Loss: 4.367, avg. samples / sec: 57416.99
Iteration:   2340, Loss function: 3.862, Average Loss: 4.379, avg. samples / sec: 57160.21
Iteration:   2340, Loss function: 3.438, Average Loss: 4.347, avg. samples / sec: 57162.71
Iteration:   2340, Loss function: 3.492, Average Loss: 4.344, avg. samples / sec: 57019.21
Iteration:   2360, Loss function: 4.988, Average Loss: 4.355, avg. samples / sec: 57988.18
Iteration:   2360, Loss function: 4.090, Average Loss: 4.371, avg. samples / sec: 57966.52
Iteration:   2360, Loss function: 3.449, Average Loss: 4.337, avg. samples / sec: 57972.53
Iteration:   2360, Loss function: 3.232, Average Loss: 4.350, avg. samples / sec: 57806.34
Iteration:   2360, Loss function: 3.786, Average Loss: 4.347, avg. samples / sec: 57943.33
Iteration:   2360, Loss function: 3.371, Average Loss: 4.347, avg. samples / sec: 58076.07
Iteration:   2360, Loss function: 4.237, Average Loss: 4.384, avg. samples / sec: 57901.86
Iteration:   2360, Loss function: 3.954, Average Loss: 4.353, avg. samples / sec: 57864.82
Iteration:   2360, Loss function: 3.899, Average Loss: 4.355, avg. samples / sec: 57822.09
Iteration:   2360, Loss function: 4.021, Average Loss: 4.347, avg. samples / sec: 57794.01
Iteration:   2360, Loss function: 3.579, Average Loss: 4.366, avg. samples / sec: 57849.29
Iteration:   2360, Loss function: 4.875, Average Loss: 4.351, avg. samples / sec: 57931.95
Iteration:   2360, Loss function: 4.635, Average Loss: 4.362, avg. samples / sec: 57770.04
Iteration:   2360, Loss function: 3.272, Average Loss: 4.380, avg. samples / sec: 57838.96
Iteration:   2360, Loss function: 3.020, Average Loss: 4.343, avg. samples / sec: 57545.86
:::MLL 1558638791.652 epoch_stop: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 819}}
:::MLL 1558638791.652 epoch_start: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 673}}
Iteration:   2380, Loss function: 2.971, Average Loss: 4.350, avg. samples / sec: 59514.95
Iteration:   2380, Loss function: 3.292, Average Loss: 4.336, avg. samples / sec: 59386.80
Iteration:   2380, Loss function: 5.651, Average Loss: 4.344, avg. samples / sec: 59318.08
Iteration:   2380, Loss function: 4.533, Average Loss: 4.340, avg. samples / sec: 59293.00
Iteration:   2380, Loss function: 3.954, Average Loss: 4.337, avg. samples / sec: 59574.20
Iteration:   2380, Loss function: 4.120, Average Loss: 4.348, avg. samples / sec: 59394.86
Iteration:   2380, Loss function: 3.391, Average Loss: 4.375, avg. samples / sec: 59325.95
Iteration:   2380, Loss function: 5.219, Average Loss: 4.348, avg. samples / sec: 59330.54
Iteration:   2380, Loss function: 4.248, Average Loss: 4.372, avg. samples / sec: 59403.95
Iteration:   2380, Loss function: 3.992, Average Loss: 4.363, avg. samples / sec: 59346.39
Iteration:   2380, Loss function: 4.532, Average Loss: 4.365, avg. samples / sec: 59144.15
Iteration:   2380, Loss function: 4.822, Average Loss: 4.344, avg. samples / sec: 59308.32
Iteration:   2380, Loss function: 4.687, Average Loss: 4.343, avg. samples / sec: 59177.08
Iteration:   2380, Loss function: 4.308, Average Loss: 4.358, avg. samples / sec: 59279.41
Iteration:   2380, Loss function: 4.026, Average Loss: 4.346, avg. samples / sec: 59006.44
Iteration:   2400, Loss function: 3.813, Average Loss: 4.340, avg. samples / sec: 56146.86
Iteration:   2400, Loss function: 5.215, Average Loss: 4.337, avg. samples / sec: 56114.40
Iteration:   2400, Loss function: 3.217, Average Loss: 4.333, avg. samples / sec: 56147.78
Iteration:   2400, Loss function: 3.688, Average Loss: 4.350, avg. samples / sec: 56293.22
Iteration:   2400, Loss function: 4.186, Average Loss: 4.367, avg. samples / sec: 56157.78
Iteration:   2400, Loss function: 4.840, Average Loss: 4.342, avg. samples / sec: 56239.89
Iteration:   2400, Loss function: 5.129, Average Loss: 4.342, avg. samples / sec: 56171.32
Iteration:   2400, Loss function: 3.828, Average Loss: 4.363, avg. samples / sec: 56176.07
Iteration:   2400, Loss function: 4.892, Average Loss: 4.362, avg. samples / sec: 56211.29
Iteration:   2400, Loss function: 2.765, Average Loss: 4.341, avg. samples / sec: 56119.63
Iteration:   2400, Loss function: 5.264, Average Loss: 4.342, avg. samples / sec: 56319.05
Iteration:   2400, Loss function: 3.982, Average Loss: 4.344, avg. samples / sec: 56054.05
Iteration:   2400, Loss function: 3.289, Average Loss: 4.333, avg. samples / sec: 56214.79
Iteration:   2400, Loss function: 3.599, Average Loss: 4.332, avg. samples / sec: 56056.10
Iteration:   2400, Loss function: 3.169, Average Loss: 4.354, avg. samples / sec: 56066.07
Iteration:   2420, Loss function: 3.485, Average Loss: 4.339, avg. samples / sec: 58740.77
Iteration:   2420, Loss function: 4.591, Average Loss: 4.342, avg. samples / sec: 58658.69
Iteration:   2420, Loss function: 4.149, Average Loss: 4.347, avg. samples / sec: 58656.37
Iteration:   2420, Loss function: 3.533, Average Loss: 4.332, avg. samples / sec: 58488.37
Iteration:   2420, Loss function: 4.285, Average Loss: 4.339, avg. samples / sec: 58559.44
Iteration:   2420, Loss function: 5.934, Average Loss: 4.337, avg. samples / sec: 58474.57
Iteration:   2420, Loss function: 5.165, Average Loss: 4.342, avg. samples / sec: 58437.06
Iteration:   2420, Loss function: 4.139, Average Loss: 4.365, avg. samples / sec: 58440.01
Iteration:   2420, Loss function: 4.018, Average Loss: 4.334, avg. samples / sec: 58414.89
Iteration:   2420, Loss function: 5.038, Average Loss: 4.334, avg. samples / sec: 58386.04
Iteration:   2420, Loss function: 4.387, Average Loss: 4.330, avg. samples / sec: 58490.46
Iteration:   2420, Loss function: 3.667, Average Loss: 4.353, avg. samples / sec: 58380.97
Iteration:   2420, Loss function: 6.335, Average Loss: 4.346, avg. samples / sec: 58513.75
Iteration:   2420, Loss function: 4.069, Average Loss: 4.330, avg. samples / sec: 58410.95
Iteration:   2420, Loss function: 3.966, Average Loss: 4.357, avg. samples / sec: 58333.65
Iteration:   2440, Loss function: 3.233, Average Loss: 4.330, avg. samples / sec: 59683.37
Iteration:   2440, Loss function: 3.830, Average Loss: 4.337, avg. samples / sec: 59410.56
Iteration:   2440, Loss function: 4.108, Average Loss: 4.355, avg. samples / sec: 59538.79
Iteration:   2440, Loss function: 4.736, Average Loss: 4.343, avg. samples / sec: 59609.86
Iteration:   2440, Loss function: 2.600, Average Loss: 4.335, avg. samples / sec: 59211.29
Iteration:   2440, Loss function: 3.683, Average Loss: 4.341, avg. samples / sec: 59283.85
Iteration:   2440, Loss function: 4.795, Average Loss: 4.330, avg. samples / sec: 59297.59
Iteration:   2440, Loss function: 4.391, Average Loss: 4.348, avg. samples / sec: 59507.97
Iteration:   2440, Loss function: 4.211, Average Loss: 4.339, avg. samples / sec: 59166.52
Iteration:   2440, Loss function: 4.309, Average Loss: 4.354, avg. samples / sec: 59479.21
Iteration:   2440, Loss function: 3.829, Average Loss: 4.325, avg. samples / sec: 59361.48
Iteration:   2440, Loss function: 4.660, Average Loss: 4.330, avg. samples / sec: 59311.37
Iteration:   2440, Loss function: 5.526, Average Loss: 4.341, avg. samples / sec: 59282.58
Iteration:   2440, Loss function: 4.346, Average Loss: 4.328, avg. samples / sec: 59285.59
Iteration:   2440, Loss function: 3.299, Average Loss: 4.330, avg. samples / sec: 59192.86
:::MLL 1558638793.679 epoch_stop: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 819}}
:::MLL 1558638793.679 epoch_start: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 673}}
Iteration:   2460, Loss function: 4.665, Average Loss: 4.338, avg. samples / sec: 58919.55
Iteration:   2460, Loss function: 5.265, Average Loss: 4.327, avg. samples / sec: 59078.62
Iteration:   2460, Loss function: 3.901, Average Loss: 4.349, avg. samples / sec: 59010.93
Iteration:   2460, Loss function: 4.176, Average Loss: 4.328, avg. samples / sec: 58933.57
Iteration:   2460, Loss function: 3.838, Average Loss: 4.326, avg. samples / sec: 58982.06
Iteration:   2460, Loss function: 5.540, Average Loss: 4.323, avg. samples / sec: 59064.95
Iteration:   2460, Loss function: 4.077, Average Loss: 4.338, avg. samples / sec: 58805.13
Iteration:   2460, Loss function: 3.485, Average Loss: 4.320, avg. samples / sec: 58961.28
Iteration:   2460, Loss function: 4.250, Average Loss: 4.347, avg. samples / sec: 58884.22
Iteration:   2460, Loss function: 3.917, Average Loss: 4.335, avg. samples / sec: 58844.17
Iteration:   2460, Loss function: 3.840, Average Loss: 4.334, avg. samples / sec: 58769.43
Iteration:   2460, Loss function: 3.649, Average Loss: 4.322, avg. samples / sec: 59026.46
Iteration:   2460, Loss function: 3.756, Average Loss: 4.341, avg. samples / sec: 58797.18
Iteration:   2460, Loss function: 5.180, Average Loss: 4.351, avg. samples / sec: 58485.85
Iteration:   2460, Loss function: 5.486, Average Loss: 4.328, avg. samples / sec: 58362.32
Iteration:   2480, Loss function: 4.278, Average Loss: 4.340, avg. samples / sec: 56746.06
Iteration:   2480, Loss function: 3.529, Average Loss: 4.329, avg. samples / sec: 57063.79
Iteration:   2480, Loss function: 3.913, Average Loss: 4.344, avg. samples / sec: 56918.45
Iteration:   2480, Loss function: 4.928, Average Loss: 4.322, avg. samples / sec: 56682.98
Iteration:   2480, Loss function: 4.030, Average Loss: 4.340, avg. samples / sec: 56821.38
Iteration:   2480, Loss function: 2.984, Average Loss: 4.328, avg. samples / sec: 56610.64
Iteration:   2480, Loss function: 5.645, Average Loss: 4.333, avg. samples / sec: 56459.52
Iteration:   2480, Loss function: 4.949, Average Loss: 4.314, avg. samples / sec: 56663.17
Iteration:   2480, Loss function: 4.631, Average Loss: 4.327, avg. samples / sec: 56515.74
Iteration:   2480, Loss function: 3.887, Average Loss: 4.323, avg. samples / sec: 56516.54
Iteration:   2480, Loss function: 3.998, Average Loss: 4.324, avg. samples / sec: 56462.85
Iteration:   2480, Loss function: 3.408, Average Loss: 4.322, avg. samples / sec: 56522.88
Iteration:   2480, Loss function: 4.013, Average Loss: 4.332, avg. samples / sec: 56511.03
Iteration:   2480, Loss function: 5.080, Average Loss: 4.344, avg. samples / sec: 56448.62
Iteration:   2480, Loss function: 3.842, Average Loss: 4.316, avg. samples / sec: 56503.69
Iteration:   2500, Loss function: 4.199, Average Loss: 4.325, avg. samples / sec: 59542.71
Iteration:   2500, Loss function: 3.927, Average Loss: 4.312, avg. samples / sec: 59524.13
Iteration:   2500, Loss function: 2.748, Average Loss: 4.339, avg. samples / sec: 59388.70
Iteration:   2500, Loss function: 3.516, Average Loss: 4.324, avg. samples / sec: 59454.57
Iteration:   2500, Loss function: 2.626, Average Loss: 4.335, avg. samples / sec: 59314.49
Iteration:   2500, Loss function: 4.066, Average Loss: 4.334, avg. samples / sec: 59204.72
Iteration:   2500, Loss function: 5.411, Average Loss: 4.308, avg. samples / sec: 59340.16
Iteration:   2500, Loss function: 4.695, Average Loss: 4.320, avg. samples / sec: 59395.58
Iteration:   2500, Loss function: 3.273, Average Loss: 4.323, avg. samples / sec: 59218.85
Iteration:   2500, Loss function: 3.628, Average Loss: 4.325, avg. samples / sec: 59279.86
Iteration:   2500, Loss function: 3.413, Average Loss: 4.324, avg. samples / sec: 59390.70
Iteration:   2500, Loss function: 4.893, Average Loss: 4.339, avg. samples / sec: 59395.48
Iteration:   2500, Loss function: 3.967, Average Loss: 4.318, avg. samples / sec: 59342.61
Iteration:   2500, Loss function: 3.847, Average Loss: 4.324, avg. samples / sec: 59272.53
Iteration:   2500, Loss function: 4.225, Average Loss: 4.331, avg. samples / sec: 57971.74
:::MLL 1558638795.700 epoch_stop: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 819}}
:::MLL 1558638795.700 epoch_start: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 673}}
Iteration:   2520, Loss function: 5.100, Average Loss: 4.325, avg. samples / sec: 59548.30
Iteration:   2520, Loss function: 4.126, Average Loss: 4.299, avg. samples / sec: 58126.16
Iteration:   2520, Loss function: 3.506, Average Loss: 4.327, avg. samples / sec: 58081.55
Iteration:   2520, Loss function: 3.686, Average Loss: 4.338, avg. samples / sec: 58142.03
Iteration:   2520, Loss function: 3.199, Average Loss: 4.304, avg. samples / sec: 58018.50
Iteration:   2520, Loss function: 5.360, Average Loss: 4.318, avg. samples / sec: 58183.01
Iteration:   2520, Loss function: 5.428, Average Loss: 4.325, avg. samples / sec: 58051.24
Iteration:   2520, Loss function: 4.245, Average Loss: 4.314, avg. samples / sec: 58087.59
Iteration:   2520, Loss function: 3.457, Average Loss: 4.316, avg. samples / sec: 57840.76
Iteration:   2520, Loss function: 3.471, Average Loss: 4.316, avg. samples / sec: 58068.82
Iteration:   2520, Loss function: 5.647, Average Loss: 4.337, avg. samples / sec: 57974.01
Iteration:   2520, Loss function: 3.466, Average Loss: 4.307, avg. samples / sec: 58031.47
Iteration:   2520, Loss function: 5.101, Average Loss: 4.319, avg. samples / sec: 57994.81
Iteration:   2520, Loss function: 3.565, Average Loss: 4.318, avg. samples / sec: 57928.71
Iteration:   2520, Loss function: 3.226, Average Loss: 4.311, avg. samples / sec: 57753.46
Iteration:   2540, Loss function: 4.098, Average Loss: 4.334, avg. samples / sec: 56653.72
Iteration:   2540, Loss function: 4.296, Average Loss: 4.304, avg. samples / sec: 56622.28
Iteration:   2540, Loss function: 4.332, Average Loss: 4.298, avg. samples / sec: 56557.61
Iteration:   2540, Loss function: 4.266, Average Loss: 4.313, avg. samples / sec: 56611.21
Iteration:   2540, Loss function: 2.924, Average Loss: 4.312, avg. samples / sec: 56639.62
Iteration:   2540, Loss function: 4.418, Average Loss: 4.313, avg. samples / sec: 56681.61
Iteration:   2540, Loss function: 3.399, Average Loss: 4.315, avg. samples / sec: 56587.70
Iteration:   2540, Loss function: 3.970, Average Loss: 4.307, avg. samples / sec: 56588.16
Iteration:   2540, Loss function: 4.838, Average Loss: 4.317, avg. samples / sec: 56626.56
Iteration:   2540, Loss function: 3.970, Average Loss: 4.329, avg. samples / sec: 56564.15
Iteration:   2540, Loss function: 3.279, Average Loss: 4.320, avg. samples / sec: 56475.52
Iteration:   2540, Loss function: 5.146, Average Loss: 4.317, avg. samples / sec: 56402.38
Iteration:   2540, Loss function: 4.311, Average Loss: 4.312, avg. samples / sec: 56518.08
Iteration:   2540, Loss function: 3.429, Average Loss: 4.300, avg. samples / sec: 56499.02
Iteration:   2540, Loss function: 4.575, Average Loss: 4.307, avg. samples / sec: 56711.97
Iteration:   2560, Loss function: 3.975, Average Loss: 4.297, avg. samples / sec: 57593.02
Iteration:   2560, Loss function: 3.555, Average Loss: 4.300, avg. samples / sec: 57291.78
Iteration:   2560, Loss function: 3.893, Average Loss: 4.326, avg. samples / sec: 57251.40
Iteration:   2560, Loss function: 3.910, Average Loss: 4.307, avg. samples / sec: 57445.49
Iteration:   2560, Loss function: 3.681, Average Loss: 4.315, avg. samples / sec: 57326.69
Iteration:   2560, Loss function: 3.357, Average Loss: 4.318, avg. samples / sec: 57392.81
Iteration:   2560, Loss function: 3.295, Average Loss: 4.311, avg. samples / sec: 57361.58
Iteration:   2560, Loss function: 4.495, Average Loss: 4.308, avg. samples / sec: 57485.68
Iteration:   2560, Loss function: 4.430, Average Loss: 4.314, avg. samples / sec: 57246.52
Iteration:   2560, Loss function: 3.634, Average Loss: 4.295, avg. samples / sec: 57222.83
Iteration:   2560, Loss function: 4.659, Average Loss: 4.308, avg. samples / sec: 57277.55
Iteration:   2560, Loss function: 3.912, Average Loss: 4.308, avg. samples / sec: 57186.42
Iteration:   2560, Loss function: 3.510, Average Loss: 4.312, avg. samples / sec: 57272.20
Iteration:   2560, Loss function: 4.394, Average Loss: 4.318, avg. samples / sec: 57143.27
Iteration:   2560, Loss function: 5.450, Average Loss: 4.324, avg. samples / sec: 57195.26
Iteration:   2580, Loss function: 3.095, Average Loss: 4.302, avg. samples / sec: 57897.08
Iteration:   2580, Loss function: 3.354, Average Loss: 4.306, avg. samples / sec: 57859.64
Iteration:   2580, Loss function: 2.693, Average Loss: 4.319, avg. samples / sec: 57738.49
Iteration:   2580, Loss function: 4.260, Average Loss: 4.315, avg. samples / sec: 57752.71
Iteration:   2580, Loss function: 3.581, Average Loss: 4.313, avg. samples / sec: 57907.02
Iteration:   2580, Loss function: 3.867, Average Loss: 4.310, avg. samples / sec: 57872.78
Iteration:   2580, Loss function: 3.731, Average Loss: 4.299, avg. samples / sec: 57646.87
Iteration:   2580, Loss function: 3.780, Average Loss: 4.285, avg. samples / sec: 57742.01
Iteration:   2580, Loss function: 4.992, Average Loss: 4.308, avg. samples / sec: 57626.32
Iteration:   2580, Loss function: 3.833, Average Loss: 4.317, avg. samples / sec: 57826.43
Iteration:   2580, Loss function: 4.898, Average Loss: 4.308, avg. samples / sec: 57658.19
Iteration:   2580, Loss function: 3.032, Average Loss: 4.317, avg. samples / sec: 57616.21
Iteration:   2580, Loss function: 3.598, Average Loss: 4.307, avg. samples / sec: 57617.62
Iteration:   2580, Loss function: 4.299, Average Loss: 4.294, avg. samples / sec: 57458.54
Iteration:   2580, Loss function: 3.865, Average Loss: 4.301, avg. samples / sec: 57627.57
:::MLL 1558638797.742 epoch_stop: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 819}}
:::MLL 1558638797.743 epoch_start: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 673}}
Iteration:   2600, Loss function: 3.820, Average Loss: 4.314, avg. samples / sec: 59818.37
Iteration:   2600, Loss function: 5.088, Average Loss: 4.299, avg. samples / sec: 59876.11
Iteration:   2600, Loss function: 4.306, Average Loss: 4.295, avg. samples / sec: 59788.48
Iteration:   2600, Loss function: 3.144, Average Loss: 4.282, avg. samples / sec: 59748.93
Iteration:   2600, Loss function: 2.983, Average Loss: 4.305, avg. samples / sec: 59648.01
Iteration:   2600, Loss function: 4.249, Average Loss: 4.302, avg. samples / sec: 59754.38
Iteration:   2600, Loss function: 4.926, Average Loss: 4.302, avg. samples / sec: 59624.86
Iteration:   2600, Loss function: 4.084, Average Loss: 4.315, avg. samples / sec: 59737.76
Iteration:   2600, Loss function: 3.691, Average Loss: 4.302, avg. samples / sec: 59536.65
Iteration:   2600, Loss function: 2.954, Average Loss: 4.287, avg. samples / sec: 59767.46
Iteration:   2600, Loss function: 3.759, Average Loss: 4.294, avg. samples / sec: 59439.87
Iteration:   2600, Loss function: 4.005, Average Loss: 4.307, avg. samples / sec: 59637.23
Iteration:   2600, Loss function: 4.321, Average Loss: 4.308, avg. samples / sec: 59425.91
Iteration:   2600, Loss function: 4.396, Average Loss: 4.309, avg. samples / sec: 59475.14
Iteration:   2600, Loss function: 4.603, Average Loss: 4.285, avg. samples / sec: 59427.22
Iteration:   2620, Loss function: 4.496, Average Loss: 4.299, avg. samples / sec: 59991.63
Iteration:   2620, Loss function: 3.577, Average Loss: 4.307, avg. samples / sec: 59930.71
Iteration:   2620, Loss function: 3.685, Average Loss: 4.288, avg. samples / sec: 59772.07
Iteration:   2620, Loss function: 3.325, Average Loss: 4.299, avg. samples / sec: 59931.65
Iteration:   2620, Loss function: 4.840, Average Loss: 4.281, avg. samples / sec: 59866.22
Iteration:   2620, Loss function: 4.460, Average Loss: 4.308, avg. samples / sec: 60018.76
Iteration:   2620, Loss function: 4.043, Average Loss: 4.296, avg. samples / sec: 59755.75
Iteration:   2620, Loss function: 4.399, Average Loss: 4.286, avg. samples / sec: 60077.02
Iteration:   2620, Loss function: 3.169, Average Loss: 4.285, avg. samples / sec: 59767.10
Iteration:   2620, Loss function: 3.470, Average Loss: 4.300, avg. samples / sec: 59636.09
Iteration:   2620, Loss function: 2.807, Average Loss: 4.296, avg. samples / sec: 59679.93
Iteration:   2620, Loss function: 3.330, Average Loss: 4.306, avg. samples / sec: 59383.72
Iteration:   2620, Loss function: 5.595, Average Loss: 4.297, avg. samples / sec: 59295.77
Iteration:   2620, Loss function: 4.988, Average Loss: 4.281, avg. samples / sec: 59377.97
Iteration:   2620, Loss function: 3.199, Average Loss: 4.297, avg. samples / sec: 59532.73
Iteration:   2640, Loss function: 4.538, Average Loss: 4.298, avg. samples / sec: 57936.19
Iteration:   2640, Loss function: 3.437, Average Loss: 4.278, avg. samples / sec: 58010.86
Iteration:   2640, Loss function: 5.009, Average Loss: 4.288, avg. samples / sec: 58310.02
Iteration:   2640, Loss function: 3.709, Average Loss: 4.307, avg. samples / sec: 57756.49
Iteration:   2640, Loss function: 4.235, Average Loss: 4.286, avg. samples / sec: 57925.68
Iteration:   2640, Loss function: 4.392, Average Loss: 4.298, avg. samples / sec: 57978.97
Iteration:   2640, Loss function: 3.127, Average Loss: 4.281, avg. samples / sec: 58190.98
Iteration:   2640, Loss function: 3.239, Average Loss: 4.288, avg. samples / sec: 57929.07
Iteration:   2640, Loss function: 4.478, Average Loss: 4.286, avg. samples / sec: 57728.29
Iteration:   2640, Loss function: 4.932, Average Loss: 4.297, avg. samples / sec: 57551.13
Iteration:   2640, Loss function: 3.457, Average Loss: 4.289, avg. samples / sec: 57713.49
Iteration:   2640, Loss function: 2.762, Average Loss: 4.291, avg. samples / sec: 57990.45
Iteration:   2640, Loss function: 3.577, Average Loss: 4.305, avg. samples / sec: 57696.53
Iteration:   2640, Loss function: 3.148, Average Loss: 4.272, avg. samples / sec: 57605.68
Iteration:   2640, Loss function: 4.676, Average Loss: 4.298, avg. samples / sec: 57764.16
:::MLL 1558638799.733 epoch_stop: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 819}}
:::MLL 1558638799.733 epoch_start: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 673}}
Iteration:   2660, Loss function: 4.079, Average Loss: 4.302, avg. samples / sec: 59950.13
Iteration:   2660, Loss function: 4.186, Average Loss: 4.296, avg. samples / sec: 59646.26
Iteration:   2660, Loss function: 4.201, Average Loss: 4.291, avg. samples / sec: 59923.98
Iteration:   2660, Loss function: 4.128, Average Loss: 4.293, avg. samples / sec: 59776.30
Iteration:   2660, Loss function: 4.528, Average Loss: 4.283, avg. samples / sec: 59642.05
Iteration:   2660, Loss function: 4.052, Average Loss: 4.290, avg. samples / sec: 59465.01
Iteration:   2660, Loss function: 3.613, Average Loss: 4.283, avg. samples / sec: 59566.17
Iteration:   2660, Loss function: 4.315, Average Loss: 4.271, avg. samples / sec: 59459.51
Iteration:   2660, Loss function: 4.563, Average Loss: 4.282, avg. samples / sec: 59662.45
Iteration:   2660, Loss function: 2.510, Average Loss: 4.295, avg. samples / sec: 59352.83
Iteration:   2660, Loss function: 3.737, Average Loss: 4.272, avg. samples / sec: 59631.07
Iteration:   2660, Loss function: 4.192, Average Loss: 4.274, avg. samples / sec: 59340.14
Iteration:   2660, Loss function: 3.631, Average Loss: 4.279, avg. samples / sec: 59294.82
Iteration:   2660, Loss function: 4.403, Average Loss: 4.296, avg. samples / sec: 59470.35
Iteration:   2660, Loss function: 4.302, Average Loss: 4.281, avg. samples / sec: 59150.30
Iteration:   2680, Loss function: 3.612, Average Loss: 4.285, avg. samples / sec: 58546.54
Iteration:   2680, Loss function: 5.403, Average Loss: 4.278, avg. samples / sec: 58949.20
Iteration:   2680, Loss function: 4.563, Average Loss: 4.272, avg. samples / sec: 58754.46
Iteration:   2680, Loss function: 4.298, Average Loss: 4.267, avg. samples / sec: 58557.95
Iteration:   2680, Loss function: 4.068, Average Loss: 4.275, avg. samples / sec: 58593.30
Iteration:   2680, Loss function: 3.456, Average Loss: 4.287, avg. samples / sec: 58428.07
Iteration:   2680, Loss function: 5.447, Average Loss: 4.291, avg. samples / sec: 58289.62
Iteration:   2680, Loss function: 4.286, Average Loss: 4.282, avg. samples / sec: 58444.30
Iteration:   2680, Loss function: 4.253, Average Loss: 4.280, avg. samples / sec: 58483.62
Iteration:   2680, Loss function: 5.210, Average Loss: 4.294, avg. samples / sec: 58711.89
Iteration:   2680, Loss function: 2.764, Average Loss: 4.266, avg. samples / sec: 58614.73
Iteration:   2680, Loss function: 3.786, Average Loss: 4.283, avg. samples / sec: 58321.43
Iteration:   2680, Loss function: 4.205, Average Loss: 4.278, avg. samples / sec: 58336.55
Iteration:   2680, Loss function: 4.641, Average Loss: 4.272, avg. samples / sec: 58550.19
Iteration:   2680, Loss function: 3.286, Average Loss: 4.282, avg. samples / sec: 58506.20
Iteration:   2700, Loss function: 3.854, Average Loss: 4.269, avg. samples / sec: 57662.89
Iteration:   2700, Loss function: 2.835, Average Loss: 4.279, avg. samples / sec: 57554.75
Iteration:   2700, Loss function: 4.404, Average Loss: 4.264, avg. samples / sec: 57513.71
Iteration:   2700, Loss function: 3.819, Average Loss: 4.275, avg. samples / sec: 57465.59
Iteration:   2700, Loss function: 4.428, Average Loss: 4.274, avg. samples / sec: 57551.27
Iteration:   2700, Loss function: 4.210, Average Loss: 4.281, avg. samples / sec: 57386.38
Iteration:   2700, Loss function: 2.961, Average Loss: 4.290, avg. samples / sec: 57571.25
Iteration:   2700, Loss function: 3.680, Average Loss: 4.268, avg. samples / sec: 57647.53
Iteration:   2700, Loss function: 4.472, Average Loss: 4.288, avg. samples / sec: 57597.28
Iteration:   2700, Loss function: 3.400, Average Loss: 4.269, avg. samples / sec: 57501.75
Iteration:   2700, Loss function: 2.794, Average Loss: 4.271, avg. samples / sec: 57512.12
Iteration:   2700, Loss function: 4.881, Average Loss: 4.266, avg. samples / sec: 57420.87
Iteration:   2700, Loss function: 3.954, Average Loss: 4.276, avg. samples / sec: 57597.96
Iteration:   2700, Loss function: 4.898, Average Loss: 4.276, avg. samples / sec: 57538.06
Iteration:   2700, Loss function: 4.019, Average Loss: 4.287, avg. samples / sec: 57404.80
Iteration:   2720, Loss function: 3.361, Average Loss: 4.278, avg. samples / sec: 60392.40
Iteration:   2720, Loss function: 3.555, Average Loss: 4.274, avg. samples / sec: 60458.90
Iteration:   2720, Loss function: 5.739, Average Loss: 4.271, avg. samples / sec: 60349.70
Iteration:   2720, Loss function: 4.569, Average Loss: 4.265, avg. samples / sec: 60303.94
Iteration:   2720, Loss function: 4.823, Average Loss: 4.272, avg. samples / sec: 60228.40
Iteration:   2720, Loss function: 4.494, Average Loss: 4.266, avg. samples / sec: 60250.57
Iteration:   2720, Loss function: 4.664, Average Loss: 4.283, avg. samples / sec: 60362.31
Iteration:   2720, Loss function: 3.843, Average Loss: 4.281, avg. samples / sec: 60177.97
Iteration:   2720, Loss function: 4.366, Average Loss: 4.260, avg. samples / sec: 60225.52
Iteration:   2720, Loss function: 3.764, Average Loss: 4.265, avg. samples / sec: 60208.79
Iteration:   2720, Loss function: 3.021, Average Loss: 4.254, avg. samples / sec: 60112.26
Iteration:   2720, Loss function: 4.433, Average Loss: 4.287, avg. samples / sec: 60126.69
Iteration:   2720, Loss function: 4.534, Average Loss: 4.272, avg. samples / sec: 60199.28
Iteration:   2720, Loss function: 4.703, Average Loss: 4.281, avg. samples / sec: 60013.80
Iteration:   2720, Loss function: 2.974, Average Loss: 4.267, avg. samples / sec: 59953.24
:::MLL 1558638801.739 epoch_stop: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 819}}
:::MLL 1558638801.739 epoch_start: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 673}}
Iteration:   2740, Loss function: 3.594, Average Loss: 4.252, avg. samples / sec: 57572.03
Iteration:   2740, Loss function: 4.004, Average Loss: 4.275, avg. samples / sec: 57501.51
Iteration:   2740, Loss function: 3.672, Average Loss: 4.259, avg. samples / sec: 57649.75
Iteration:   2740, Loss function: 1.997, Average Loss: 4.270, avg. samples / sec: 57558.20
Iteration:   2740, Loss function: 4.243, Average Loss: 4.266, avg. samples / sec: 57422.55
Iteration:   2740, Loss function: 4.492, Average Loss: 4.282, avg. samples / sec: 57448.77
Iteration:   2740, Loss function: 4.906, Average Loss: 4.279, avg. samples / sec: 57595.44
Iteration:   2740, Loss function: 4.838, Average Loss: 4.284, avg. samples / sec: 57530.17
Iteration:   2740, Loss function: 5.208, Average Loss: 4.273, avg. samples / sec: 57259.15
Iteration:   2740, Loss function: 3.904, Average Loss: 4.258, avg. samples / sec: 57348.81
Iteration:   2740, Loss function: 3.605, Average Loss: 4.267, avg. samples / sec: 57143.71
Iteration:   2740, Loss function: 4.642, Average Loss: 4.260, avg. samples / sec: 57185.82
Iteration:   2740, Loss function: 3.840, Average Loss: 4.268, avg. samples / sec: 57125.64
Iteration:   2740, Loss function: 4.005, Average Loss: 4.260, avg. samples / sec: 57222.92
Iteration:   2740, Loss function: 4.004, Average Loss: 4.261, avg. samples / sec: 57225.15
Iteration:   2760, Loss function: 3.439, Average Loss: 4.253, avg. samples / sec: 57971.55
Iteration:   2760, Loss function: 3.565, Average Loss: 4.256, avg. samples / sec: 57669.73
Iteration:   2760, Loss function: 3.632, Average Loss: 4.253, avg. samples / sec: 57868.69
Iteration:   2760, Loss function: 3.576, Average Loss: 4.280, avg. samples / sec: 57656.33
Iteration:   2760, Loss function: 3.422, Average Loss: 4.257, avg. samples / sec: 57801.12
Iteration:   2760, Loss function: 3.961, Average Loss: 4.271, avg. samples / sec: 57639.87
Iteration:   2760, Loss function: 4.400, Average Loss: 4.265, avg. samples / sec: 57614.28
Iteration:   2760, Loss function: 2.867, Average Loss: 4.264, avg. samples / sec: 57802.05
Iteration:   2760, Loss function: 3.889, Average Loss: 4.248, avg. samples / sec: 57525.29
Iteration:   2760, Loss function: 4.464, Average Loss: 4.260, avg. samples / sec: 57814.47
Iteration:   2760, Loss function: 2.913, Average Loss: 4.251, avg. samples / sec: 57659.73
Iteration:   2760, Loss function: 2.867, Average Loss: 4.273, avg. samples / sec: 57496.94
Iteration:   2760, Loss function: 4.299, Average Loss: 4.257, avg. samples / sec: 57496.84
Iteration:   2760, Loss function: 4.132, Average Loss: 4.279, avg. samples / sec: 57406.72
Iteration:   2760, Loss function: 3.644, Average Loss: 4.277, avg. samples / sec: 57385.07
Iteration:   2780, Loss function: 3.969, Average Loss: 4.272, avg. samples / sec: 58592.26
Iteration:   2780, Loss function: 3.003, Average Loss: 4.255, avg. samples / sec: 58511.44
Iteration:   2780, Loss function: 4.378, Average Loss: 4.249, avg. samples / sec: 58368.68
Iteration:   2780, Loss function: 2.322, Average Loss: 4.251, avg. samples / sec: 58599.69
Iteration:   2780, Loss function: 4.442, Average Loss: 4.262, avg. samples / sec: 58463.58
Iteration:   2780, Loss function: 5.840, Average Loss: 4.245, avg. samples / sec: 58487.33
Iteration:   2780, Loss function: 4.448, Average Loss: 4.275, avg. samples / sec: 58641.94
Iteration:   2780, Loss function: 3.506, Average Loss: 4.267, avg. samples / sec: 58501.87
Iteration:   2780, Loss function: 4.140, Average Loss: 4.251, avg. samples / sec: 58334.57
Iteration:   2780, Loss function: 4.901, Average Loss: 4.245, avg. samples / sec: 58493.88
Iteration:   2780, Loss function: 3.512, Average Loss: 4.274, avg. samples / sec: 58613.36
Iteration:   2780, Loss function: 4.018, Average Loss: 4.267, avg. samples / sec: 58353.77
Iteration:   2780, Loss function: 4.166, Average Loss: 4.257, avg. samples / sec: 58368.90
Iteration:   2780, Loss function: 3.754, Average Loss: 4.260, avg. samples / sec: 58255.28
Iteration:   2780, Loss function: 3.898, Average Loss: 4.253, avg. samples / sec: 58185.29
:::MLL 1558638803.757 epoch_stop: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 819}}
:::MLL 1558638803.757 epoch_start: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 673}}
Iteration:   2800, Loss function: 4.582, Average Loss: 4.251, avg. samples / sec: 59853.58
Iteration:   2800, Loss function: 3.637, Average Loss: 4.241, avg. samples / sec: 59692.04
Iteration:   2800, Loss function: 5.949, Average Loss: 4.244, avg. samples / sec: 59668.21
Iteration:   2800, Loss function: 4.462, Average Loss: 4.267, avg. samples / sec: 59425.46
Iteration:   2800, Loss function: 4.695, Average Loss: 4.267, avg. samples / sec: 59692.17
Iteration:   2800, Loss function: 5.883, Average Loss: 4.272, avg. samples / sec: 59686.13
Iteration:   2800, Loss function: 3.065, Average Loss: 4.258, avg. samples / sec: 59533.63
Iteration:   2800, Loss function: 3.828, Average Loss: 4.254, avg. samples / sec: 59746.48
Iteration:   2800, Loss function: 3.647, Average Loss: 4.267, avg. samples / sec: 59562.97
Iteration:   2800, Loss function: 4.028, Average Loss: 4.247, avg. samples / sec: 59481.97
Iteration:   2800, Loss function: 3.195, Average Loss: 4.240, avg. samples / sec: 59507.97
Iteration:   2800, Loss function: 4.114, Average Loss: 4.247, avg. samples / sec: 59344.46
Iteration:   2800, Loss function: 3.840, Average Loss: 4.271, avg. samples / sec: 59382.12
Iteration:   2800, Loss function: 2.941, Average Loss: 4.247, avg. samples / sec: 59496.36
Iteration:   2800, Loss function: 3.761, Average Loss: 4.246, avg. samples / sec: 59250.77
Iteration:   2820, Loss function: 3.772, Average Loss: 4.265, avg. samples / sec: 55910.03
Iteration:   2820, Loss function: 4.061, Average Loss: 4.240, avg. samples / sec: 55871.22
Iteration:   2820, Loss function: 4.111, Average Loss: 4.248, avg. samples / sec: 55824.46
Iteration:   2820, Loss function: 3.026, Average Loss: 4.241, avg. samples / sec: 56086.66
Iteration:   2820, Loss function: 3.995, Average Loss: 4.264, avg. samples / sec: 56049.90
Iteration:   2820, Loss function: 4.226, Average Loss: 4.242, avg. samples / sec: 55893.95
Iteration:   2820, Loss function: 3.648, Average Loss: 4.272, avg. samples / sec: 55803.06
Iteration:   2820, Loss function: 2.585, Average Loss: 4.260, avg. samples / sec: 55787.55
Iteration:   2820, Loss function: 3.951, Average Loss: 4.261, avg. samples / sec: 55874.72
Iteration:   2820, Loss function: 3.209, Average Loss: 4.243, avg. samples / sec: 55952.65
Iteration:   2820, Loss function: 3.189, Average Loss: 4.254, avg. samples / sec: 55845.12
Iteration:   2820, Loss function: 5.239, Average Loss: 4.242, avg. samples / sec: 55779.62
Iteration:   2820, Loss function: 3.553, Average Loss: 4.253, avg. samples / sec: 55812.38
Iteration:   2820, Loss function: 3.689, Average Loss: 4.247, avg. samples / sec: 55994.49
Iteration:   2820, Loss function: 4.229, Average Loss: 4.238, avg. samples / sec: 55732.07
Iteration:   2840, Loss function: 3.826, Average Loss: 4.238, avg. samples / sec: 57297.79
Iteration:   2840, Loss function: 4.614, Average Loss: 4.237, avg. samples / sec: 57240.96
Iteration:   2840, Loss function: 3.859, Average Loss: 4.227, avg. samples / sec: 57134.28
Iteration:   2840, Loss function: 4.722, Average Loss: 4.263, avg. samples / sec: 57217.74
Iteration:   2840, Loss function: 4.420, Average Loss: 4.251, avg. samples / sec: 57121.08
Iteration:   2840, Loss function: 3.918, Average Loss: 4.248, avg. samples / sec: 57190.92
Iteration:   2840, Loss function: 4.461, Average Loss: 4.257, avg. samples / sec: 57155.50
Iteration:   2840, Loss function: 3.499, Average Loss: 4.258, avg. samples / sec: 57180.62
Iteration:   2840, Loss function: 3.544, Average Loss: 4.230, avg. samples / sec: 57345.40
Iteration:   2840, Loss function: 3.490, Average Loss: 4.258, avg. samples / sec: 57053.86
Iteration:   2840, Loss function: 5.005, Average Loss: 4.234, avg. samples / sec: 57098.33
Iteration:   2840, Loss function: 4.317, Average Loss: 4.243, avg. samples / sec: 57137.22
Iteration:   2840, Loss function: 3.636, Average Loss: 4.247, avg. samples / sec: 57132.31
Iteration:   2840, Loss function: 5.769, Average Loss: 4.260, avg. samples / sec: 57081.40
Iteration:   2840, Loss function: 3.977, Average Loss: 4.238, avg. samples / sec: 57015.98
Iteration:   2860, Loss function: 3.967, Average Loss: 4.252, avg. samples / sec: 59218.16
Iteration:   2860, Loss function: 3.195, Average Loss: 4.247, avg. samples / sec: 59173.33
Iteration:   2860, Loss function: 3.191, Average Loss: 4.230, avg. samples / sec: 59326.27
Iteration:   2860, Loss function: 3.801, Average Loss: 4.245, avg. samples / sec: 59210.82
Iteration:   2860, Loss function: 3.947, Average Loss: 4.232, avg. samples / sec: 59024.38
Iteration:   2860, Loss function: 3.554, Average Loss: 4.251, avg. samples / sec: 59089.00
Iteration:   2860, Loss function: 3.595, Average Loss: 4.233, avg. samples / sec: 58954.63
Iteration:   2860, Loss function: 3.489, Average Loss: 4.258, avg. samples / sec: 59018.94
Iteration:   2860, Loss function: 5.936, Average Loss: 4.226, avg. samples / sec: 58992.23
Iteration:   2860, Loss function: 2.839, Average Loss: 4.222, avg. samples / sec: 58969.20
Iteration:   2860, Loss function: 4.257, Average Loss: 4.245, avg. samples / sec: 58941.16
Iteration:   2860, Loss function: 4.931, Average Loss: 4.254, avg. samples / sec: 58940.37
Iteration:   2860, Loss function: 3.883, Average Loss: 4.262, avg. samples / sec: 59047.48
Iteration:   2860, Loss function: 3.180, Average Loss: 4.231, avg. samples / sec: 58958.92
Iteration:   2860, Loss function: 3.932, Average Loss: 4.242, avg. samples / sec: 58893.01
:::MLL 1558638805.800 epoch_stop: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 819}}
:::MLL 1558638805.801 epoch_start: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 673}}
Iteration:   2880, Loss function: 4.335, Average Loss: 4.237, avg. samples / sec: 57179.99
Iteration:   2880, Loss function: 3.354, Average Loss: 4.218, avg. samples / sec: 57153.09
Iteration:   2880, Loss function: 3.505, Average Loss: 4.242, avg. samples / sec: 56947.87
Iteration:   2880, Loss function: 4.538, Average Loss: 4.235, avg. samples / sec: 57048.06
Iteration:   2880, Loss function: 3.366, Average Loss: 4.256, avg. samples / sec: 57162.81
Iteration:   2880, Loss function: 4.423, Average Loss: 4.227, avg. samples / sec: 57002.28
Iteration:   2880, Loss function: 4.455, Average Loss: 4.224, avg. samples / sec: 56989.74
Iteration:   2880, Loss function: 4.399, Average Loss: 4.228, avg. samples / sec: 57119.02
Iteration:   2880, Loss function: 3.569, Average Loss: 4.254, avg. samples / sec: 57075.41
Iteration:   2880, Loss function: 3.841, Average Loss: 4.236, avg. samples / sec: 57222.76
Iteration:   2880, Loss function: 3.628, Average Loss: 4.226, avg. samples / sec: 56836.87
Iteration:   2880, Loss function: 4.105, Average Loss: 4.248, avg. samples / sec: 56871.48
Iteration:   2880, Loss function: 3.962, Average Loss: 4.242, avg. samples / sec: 56754.72
Iteration:   2880, Loss function: 3.327, Average Loss: 4.237, avg. samples / sec: 56816.25
Iteration:   2880, Loss function: 3.990, Average Loss: 4.252, avg. samples / sec: 56779.58
Iteration:   2900, Loss function: 5.293, Average Loss: 4.221, avg. samples / sec: 58612.14
Iteration:   2900, Loss function: 4.393, Average Loss: 4.230, avg. samples / sec: 58468.86
Iteration:   2900, Loss function: 2.445, Average Loss: 4.221, avg. samples / sec: 58609.07
Iteration:   2900, Loss function: 3.706, Average Loss: 4.236, avg. samples / sec: 58442.87
Iteration:   2900, Loss function: 3.374, Average Loss: 4.217, avg. samples / sec: 58367.96
Iteration:   2900, Loss function: 3.711, Average Loss: 4.249, avg. samples / sec: 58310.45
Iteration:   2900, Loss function: 3.466, Average Loss: 4.247, avg. samples / sec: 58576.33
Iteration:   2900, Loss function: 4.672, Average Loss: 4.250, avg. samples / sec: 58353.75
Iteration:   2900, Loss function: 5.665, Average Loss: 4.242, avg. samples / sec: 58416.83
Iteration:   2900, Loss function: 3.807, Average Loss: 4.229, avg. samples / sec: 58365.85
Iteration:   2900, Loss function: 4.441, Average Loss: 4.225, avg. samples / sec: 58237.88
Iteration:   2900, Loss function: 2.869, Average Loss: 4.227, avg. samples / sec: 58124.12
Iteration:   2900, Loss function: 3.760, Average Loss: 4.218, avg. samples / sec: 58172.73
Iteration:   2900, Loss function: 4.799, Average Loss: 4.231, avg. samples / sec: 58227.84
Iteration:   2900, Loss function: 5.617, Average Loss: 4.239, avg. samples / sec: 58306.60
Iteration:   2920, Loss function: 4.558, Average Loss: 4.243, avg. samples / sec: 58864.01
Iteration:   2920, Loss function: 4.140, Average Loss: 4.222, avg. samples / sec: 58939.04
Iteration:   2920, Loss function: 5.271, Average Loss: 4.220, avg. samples / sec: 58521.16
Iteration:   2920, Loss function: 3.561, Average Loss: 4.206, avg. samples / sec: 58694.72
Iteration:   2920, Loss function: 3.515, Average Loss: 4.244, avg. samples / sec: 58766.78
Iteration:   2920, Loss function: 4.387, Average Loss: 4.223, avg. samples / sec: 58883.36
Iteration:   2920, Loss function: 2.328, Average Loss: 4.227, avg. samples / sec: 58545.67
Iteration:   2920, Loss function: 4.124, Average Loss: 4.215, avg. samples / sec: 58532.39
Iteration:   2920, Loss function: 4.432, Average Loss: 4.238, avg. samples / sec: 58714.36
Iteration:   2920, Loss function: 3.682, Average Loss: 4.232, avg. samples / sec: 58831.28
Iteration:   2920, Loss function: 3.678, Average Loss: 4.218, avg. samples / sec: 58783.45
Iteration:   2920, Loss function: 4.118, Average Loss: 4.219, avg. samples / sec: 58779.72
Iteration:   2920, Loss function: 3.777, Average Loss: 4.217, avg. samples / sec: 58794.21
Iteration:   2920, Loss function: 4.231, Average Loss: 4.244, avg. samples / sec: 58617.09
Iteration:   2920, Loss function: 4.594, Average Loss: 4.229, avg. samples / sec: 58278.82
:::MLL 1558638807.838 epoch_stop: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 819}}
:::MLL 1558638807.838 epoch_start: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 673}}
Iteration:   2940, Loss function: 3.608, Average Loss: 4.224, avg. samples / sec: 57758.08
Iteration:   2940, Loss function: 3.540, Average Loss: 4.238, avg. samples / sec: 57721.88
Iteration:   2940, Loss function: 3.726, Average Loss: 4.220, avg. samples / sec: 57607.26
Iteration:   2940, Loss function: 3.460, Average Loss: 4.222, avg. samples / sec: 57678.23
Iteration:   2940, Loss function: 3.782, Average Loss: 4.235, avg. samples / sec: 57686.49
Iteration:   2940, Loss function: 4.364, Average Loss: 4.239, avg. samples / sec: 57443.36
Iteration:   2940, Loss function: 3.820, Average Loss: 4.240, avg. samples / sec: 57673.58
Iteration:   2940, Loss function: 4.505, Average Loss: 4.212, avg. samples / sec: 57619.70
Iteration:   2940, Loss function: 4.415, Average Loss: 4.215, avg. samples / sec: 57457.69
Iteration:   2940, Loss function: 3.568, Average Loss: 4.216, avg. samples / sec: 57553.62
Iteration:   2940, Loss function: 5.324, Average Loss: 4.213, avg. samples / sec: 57523.60
Iteration:   2940, Loss function: 3.382, Average Loss: 4.223, avg. samples / sec: 57488.66
Iteration:   2940, Loss function: 4.141, Average Loss: 4.205, avg. samples / sec: 57412.54
Iteration:   2940, Loss function: 4.210, Average Loss: 4.220, avg. samples / sec: 57637.77
Iteration:   2940, Loss function: 4.193, Average Loss: 4.204, avg. samples / sec: 57266.75
Iteration:   2960, Loss function: 3.492, Average Loss: 4.208, avg. samples / sec: 59665.53
Iteration:   2960, Loss function: 4.560, Average Loss: 4.216, avg. samples / sec: 59652.95
Iteration:   2960, Loss function: 3.579, Average Loss: 4.212, avg. samples / sec: 59384.67
Iteration:   2960, Loss function: 4.501, Average Loss: 4.211, avg. samples / sec: 59571.23
Iteration:   2960, Loss function: 3.688, Average Loss: 4.197, avg. samples / sec: 59662.32
Iteration:   2960, Loss function: 3.419, Average Loss: 4.206, avg. samples / sec: 59441.40
Iteration:   2960, Loss function: 4.497, Average Loss: 4.235, avg. samples / sec: 59233.56
Iteration:   2960, Loss function: 4.300, Average Loss: 4.231, avg. samples / sec: 59377.87
Iteration:   2960, Loss function: 3.548, Average Loss: 4.200, avg. samples / sec: 59639.20
Iteration:   2960, Loss function: 4.042, Average Loss: 4.223, avg. samples / sec: 59263.43
Iteration:   2960, Loss function: 2.776, Average Loss: 4.212, avg. samples / sec: 59612.99
Iteration:   2960, Loss function: 3.909, Average Loss: 4.229, avg. samples / sec: 59289.93
Iteration:   2960, Loss function: 4.489, Average Loss: 4.210, avg. samples / sec: 59422.96
Iteration:   2960, Loss function: 3.693, Average Loss: 4.217, avg. samples / sec: 59055.99
Iteration:   2960, Loss function: 4.018, Average Loss: 4.238, avg. samples / sec: 59060.64
Iteration:   2980, Loss function: 3.298, Average Loss: 4.206, avg. samples / sec: 55818.68
Iteration:   2980, Loss function: 4.892, Average Loss: 4.219, avg. samples / sec: 55925.81
Iteration:   2980, Loss function: 3.699, Average Loss: 4.230, avg. samples / sec: 55881.65
Iteration:   2980, Loss function: 4.805, Average Loss: 4.214, avg. samples / sec: 55760.56
Iteration:   2980, Loss function: 5.396, Average Loss: 4.203, avg. samples / sec: 55834.92
Iteration:   2980, Loss function: 4.989, Average Loss: 4.209, avg. samples / sec: 55777.90
Iteration:   2980, Loss function: 4.302, Average Loss: 4.189, avg. samples / sec: 55788.46
Iteration:   2980, Loss function: 3.139, Average Loss: 4.225, avg. samples / sec: 55849.45
Iteration:   2980, Loss function: 3.050, Average Loss: 4.203, avg. samples / sec: 55699.95
Iteration:   2980, Loss function: 4.029, Average Loss: 4.221, avg. samples / sec: 55840.45
Iteration:   2980, Loss function: 3.982, Average Loss: 4.205, avg. samples / sec: 55817.65
Iteration:   2980, Loss function: 4.200, Average Loss: 4.232, avg. samples / sec: 56050.75
Iteration:   2980, Loss function: 4.014, Average Loss: 4.208, avg. samples / sec: 55776.58
Iteration:   2980, Loss function: 4.178, Average Loss: 4.209, avg. samples / sec: 55860.06
Iteration:   2980, Loss function: 3.606, Average Loss: 4.199, avg. samples / sec: 55719.48
Iteration:   3000, Loss function: 3.352, Average Loss: 4.211, avg. samples / sec: 59087.26
Iteration:   3000, Loss function: 3.338, Average Loss: 4.197, avg. samples / sec: 59137.10
Iteration:   3000, Loss function: 3.989, Average Loss: 4.200, avg. samples / sec: 59142.71
Iteration:   3000, Loss function: 4.054, Average Loss: 4.183, avg. samples / sec: 59051.26
Iteration:   3000, Loss function: 4.113, Average Loss: 4.220, avg. samples / sec: 59060.84
Iteration:   3000, Loss function: 4.532, Average Loss: 4.203, avg. samples / sec: 59125.16
Iteration:   3000, Loss function: 3.924, Average Loss: 4.209, avg. samples / sec: 58967.01
Iteration:   3000, Loss function: 3.890, Average Loss: 4.195, avg. samples / sec: 59138.41
Iteration:   3000, Loss function: 4.170, Average Loss: 4.203, avg. samples / sec: 59045.50
Iteration:   3000, Loss function: 4.054, Average Loss: 4.224, avg. samples / sec: 58898.25
Iteration:   3000, Loss function: 4.664, Average Loss: 4.207, avg. samples / sec: 58866.56
Iteration:   3000, Loss function: 3.830, Average Loss: 4.218, avg. samples / sec: 58965.92
Iteration:   3000, Loss function: 3.909, Average Loss: 4.224, avg. samples / sec: 58990.16
Iteration:   3000, Loss function: 4.494, Average Loss: 4.201, avg. samples / sec: 58838.60
Iteration:   3000, Loss function: 3.888, Average Loss: 4.198, avg. samples / sec: 58789.73
:::MLL 1558638809.858 epoch_stop: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 819}}
:::MLL 1558638809.859 epoch_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 673}}
Iteration:   3020, Loss function: 3.928, Average Loss: 4.211, avg. samples / sec: 59185.78
Iteration:   3020, Loss function: 3.799, Average Loss: 4.196, avg. samples / sec: 59054.78
Iteration:   3020, Loss function: 3.215, Average Loss: 4.179, avg. samples / sec: 59122.71
Iteration:   3020, Loss function: 2.436, Average Loss: 4.211, avg. samples / sec: 59090.09
Iteration:   3020, Loss function: 3.937, Average Loss: 4.216, avg. samples / sec: 59196.91
Iteration:   3020, Loss function: 3.274, Average Loss: 4.206, avg. samples / sec: 59148.32
Iteration:   3020, Loss function: 4.395, Average Loss: 4.189, avg. samples / sec: 59290.43
Iteration:   3020, Loss function: 3.474, Average Loss: 4.201, avg. samples / sec: 59140.65
Iteration:   3020, Loss function: 3.584, Average Loss: 4.216, avg. samples / sec: 59112.47
Iteration:   3020, Loss function: 4.042, Average Loss: 4.197, avg. samples / sec: 59043.84
Iteration:   3020, Loss function: 3.607, Average Loss: 4.189, avg. samples / sec: 59067.75
Iteration:   3020, Loss function: 3.741, Average Loss: 4.219, avg. samples / sec: 59103.67
Iteration:   3020, Loss function: 4.252, Average Loss: 4.197, avg. samples / sec: 58803.54
Iteration:   3020, Loss function: 3.459, Average Loss: 4.204, avg. samples / sec: 58715.63
Iteration:   3020, Loss function: 4.996, Average Loss: 4.200, avg. samples / sec: 58912.88
Iteration:   3040, Loss function: 3.377, Average Loss: 4.213, avg. samples / sec: 59595.19
Iteration:   3040, Loss function: 3.772, Average Loss: 4.184, avg. samples / sec: 59463.40
Iteration:   3040, Loss function: 3.677, Average Loss: 4.187, avg. samples / sec: 59619.19
Iteration:   3040, Loss function: 3.637, Average Loss: 4.188, avg. samples / sec: 59382.54
Iteration:   3040, Loss function: 3.621, Average Loss: 4.202, avg. samples / sec: 59322.85
Iteration:   3040, Loss function: 4.132, Average Loss: 4.197, avg. samples / sec: 59356.48
Iteration:   3040, Loss function: 6.390, Average Loss: 4.195, avg. samples / sec: 59377.19
Iteration:   3040, Loss function: 4.360, Average Loss: 4.205, avg. samples / sec: 59226.00
Iteration:   3040, Loss function: 2.358, Average Loss: 4.211, avg. samples / sec: 59275.34
Iteration:   3040, Loss function: 3.454, Average Loss: 4.207, avg. samples / sec: 59321.83
Iteration:   3040, Loss function: 3.548, Average Loss: 4.203, avg. samples / sec: 59503.67
Iteration:   3040, Loss function: 3.626, Average Loss: 4.203, avg. samples / sec: 59230.45
Iteration:   3040, Loss function: 3.273, Average Loss: 4.173, avg. samples / sec: 59140.87
Iteration:   3040, Loss function: 5.101, Average Loss: 4.195, avg. samples / sec: 59115.67
Iteration:   3040, Loss function: 3.662, Average Loss: 4.196, avg. samples / sec: 59348.56
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
:::MLL 1558638810.959 eval_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.45s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.48s)
DONE (t=0.48s)
DONE (t=0.51s)
DONE (t=2.77s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.17097
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.31591
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.17104
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.04303
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.18605
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.27140
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.18107
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.26609
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.28013
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.07258
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.30812
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.43328
Current AP: 0.17097 AP goal: 0.23000
:::MLL 1558638814.835 eval_accuracy: {"value": 0.17096637098357703, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 389}}
:::MLL 1558638815.001 eval_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 392}}
:::MLL 1558638815.011 block_stop: {"value": null, "metadata": {"first_epoch_num": 33, "file": "train.py", "lineno": 804}}
:::MLL 1558638815.012 block_start: {"value": null, "metadata": {"first_epoch_num": 44, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3060, Loss function: 4.311, Average Loss: 4.198, avg. samples / sec: 7252.17
Iteration:   3060, Loss function: 3.257, Average Loss: 4.202, avg. samples / sec: 7252.55
Iteration:   3060, Loss function: 3.008, Average Loss: 4.195, avg. samples / sec: 7251.63
Iteration:   3060, Loss function: 3.507, Average Loss: 4.186, avg. samples / sec: 7253.35
Iteration:   3060, Loss function: 3.515, Average Loss: 4.194, avg. samples / sec: 7252.99
Iteration:   3060, Loss function: 4.249, Average Loss: 4.176, avg. samples / sec: 7252.07
Iteration:   3060, Loss function: 3.320, Average Loss: 4.202, avg. samples / sec: 7250.46
Iteration:   3060, Loss function: 4.728, Average Loss: 4.209, avg. samples / sec: 7247.40
Iteration:   3060, Loss function: 4.308, Average Loss: 4.189, avg. samples / sec: 7248.99
Iteration:   3060, Loss function: 3.561, Average Loss: 4.192, avg. samples / sec: 7253.93
Iteration:   3060, Loss function: 3.692, Average Loss: 4.198, avg. samples / sec: 7249.93
Iteration:   3060, Loss function: 3.634, Average Loss: 4.199, avg. samples / sec: 7249.22
Iteration:   3060, Loss function: 3.808, Average Loss: 4.177, avg. samples / sec: 7247.57
Iteration:   3060, Loss function: 4.876, Average Loss: 4.188, avg. samples / sec: 7247.69
Iteration:   3060, Loss function: 3.457, Average Loss: 4.180, avg. samples / sec: 7247.09
:::MLL 1558638815.911 epoch_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 819}}
:::MLL 1558638815.912 epoch_start: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 673}}
Iteration:   3080, Loss function: 3.306, Average Loss: 4.169, avg. samples / sec: 59512.04
Iteration:   3080, Loss function: 3.611, Average Loss: 4.167, avg. samples / sec: 59448.32
Iteration:   3080, Loss function: 3.880, Average Loss: 4.184, avg. samples / sec: 59215.15
Iteration:   3080, Loss function: 3.028, Average Loss: 4.184, avg. samples / sec: 59372.66
Iteration:   3080, Loss function: 3.914, Average Loss: 4.182, avg. samples / sec: 59225.40
Iteration:   3080, Loss function: 2.384, Average Loss: 4.200, avg. samples / sec: 59277.84
Iteration:   3080, Loss function: 2.740, Average Loss: 4.186, avg. samples / sec: 59081.56
Iteration:   3080, Loss function: 4.004, Average Loss: 4.167, avg. samples / sec: 59209.65
Iteration:   3080, Loss function: 4.426, Average Loss: 4.188, avg. samples / sec: 59318.06
Iteration:   3080, Loss function: 2.989, Average Loss: 4.179, avg. samples / sec: 59292.45
Iteration:   3080, Loss function: 3.294, Average Loss: 4.193, avg. samples / sec: 59205.52
Iteration:   3080, Loss function: 3.187, Average Loss: 4.193, avg. samples / sec: 58967.95
Iteration:   3080, Loss function: 3.584, Average Loss: 4.183, avg. samples / sec: 59173.08
Iteration:   3080, Loss function: 4.068, Average Loss: 4.173, avg. samples / sec: 59184.83
Iteration:   3080, Loss function: 4.018, Average Loss: 4.174, avg. samples / sec: 58994.85
Iteration:   3100, Loss function: 4.286, Average Loss: 4.176, avg. samples / sec: 59862.18
Iteration:   3100, Loss function: 3.195, Average Loss: 4.173, avg. samples / sec: 59599.55
Iteration:   3100, Loss function: 3.588, Average Loss: 4.164, avg. samples / sec: 59780.11
Iteration:   3100, Loss function: 4.202, Average Loss: 4.178, avg. samples / sec: 59555.07
Iteration:   3100, Loss function: 3.977, Average Loss: 4.158, avg. samples / sec: 59456.63
Iteration:   3100, Loss function: 3.194, Average Loss: 4.185, avg. samples / sec: 59582.74
Iteration:   3100, Loss function: 3.920, Average Loss: 4.158, avg. samples / sec: 59490.84
Iteration:   3100, Loss function: 4.531, Average Loss: 4.167, avg. samples / sec: 59421.10
Iteration:   3100, Loss function: 3.414, Average Loss: 4.150, avg. samples / sec: 59305.23
Iteration:   3100, Loss function: 2.681, Average Loss: 4.168, avg. samples / sec: 59331.54
Iteration:   3100, Loss function: 2.654, Average Loss: 4.172, avg. samples / sec: 59280.21
Iteration:   3100, Loss function: 3.405, Average Loss: 4.167, avg. samples / sec: 59493.47
Iteration:   3100, Loss function: 3.648, Average Loss: 4.183, avg. samples / sec: 59294.70
Iteration:   3100, Loss function: 4.233, Average Loss: 4.185, avg. samples / sec: 59415.67
Iteration:   3100, Loss function: 3.177, Average Loss: 4.180, avg. samples / sec: 59214.23
Iteration:   3120, Loss function: 3.309, Average Loss: 4.147, avg. samples / sec: 58507.87
Iteration:   3120, Loss function: 2.673, Average Loss: 4.143, avg. samples / sec: 58359.64
Iteration:   3120, Loss function: 3.558, Average Loss: 4.166, avg. samples / sec: 58293.52
Iteration:   3120, Loss function: 4.153, Average Loss: 4.162, avg. samples / sec: 58519.92
Iteration:   3120, Loss function: 4.301, Average Loss: 4.157, avg. samples / sec: 58458.00
Iteration:   3120, Loss function: 3.219, Average Loss: 4.166, avg. samples / sec: 58251.16
Iteration:   3120, Loss function: 3.822, Average Loss: 4.171, avg. samples / sec: 58315.79
Iteration:   3120, Loss function: 3.769, Average Loss: 4.157, avg. samples / sec: 58405.98
Iteration:   3120, Loss function: 3.282, Average Loss: 4.165, avg. samples / sec: 58576.18
Iteration:   3120, Loss function: 3.713, Average Loss: 4.155, avg. samples / sec: 58467.09
Iteration:   3120, Loss function: 2.953, Average Loss: 4.163, avg. samples / sec: 58043.78
Iteration:   3120, Loss function: 3.106, Average Loss: 4.177, avg. samples / sec: 58421.91
Iteration:   3120, Loss function: 5.295, Average Loss: 4.185, avg. samples / sec: 58464.72
Iteration:   3120, Loss function: 2.694, Average Loss: 4.131, avg. samples / sec: 58282.82
Iteration:   3120, Loss function: 5.193, Average Loss: 4.152, avg. samples / sec: 58068.03
Iteration:   3140, Loss function: 1.856, Average Loss: 4.156, avg. samples / sec: 58950.28
Iteration:   3140, Loss function: 3.632, Average Loss: 4.161, avg. samples / sec: 58965.13
Iteration:   3140, Loss function: 3.166, Average Loss: 4.141, avg. samples / sec: 58998.26
Iteration:   3140, Loss function: 4.157, Average Loss: 4.142, avg. samples / sec: 59134.44
Iteration:   3140, Loss function: 2.570, Average Loss: 4.173, avg. samples / sec: 59047.38
Iteration:   3140, Loss function: 4.197, Average Loss: 4.154, avg. samples / sec: 58919.48
Iteration:   3140, Loss function: 3.983, Average Loss: 4.130, avg. samples / sec: 58757.13
Iteration:   3140, Loss function: 2.971, Average Loss: 4.139, avg. samples / sec: 58931.70
Iteration:   3140, Loss function: 2.524, Average Loss: 4.146, avg. samples / sec: 58925.34
Iteration:   3140, Loss function: 2.110, Average Loss: 4.159, avg. samples / sec: 58875.69
Iteration:   3140, Loss function: 2.862, Average Loss: 4.132, avg. samples / sec: 58711.72
Iteration:   3140, Loss function: 3.647, Average Loss: 4.146, avg. samples / sec: 58750.12
Iteration:   3140, Loss function: 4.794, Average Loss: 4.164, avg. samples / sec: 58890.62
Iteration:   3140, Loss function: 3.623, Average Loss: 4.119, avg. samples / sec: 58950.21
Iteration:   3140, Loss function: 2.951, Average Loss: 4.148, avg. samples / sec: 58729.41
:::MLL 1558638817.905 epoch_stop: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 819}}
:::MLL 1558638817.905 epoch_start: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 673}}
Iteration:   3160, Loss function: 3.310, Average Loss: 4.118, avg. samples / sec: 58636.24
Iteration:   3160, Loss function: 3.338, Average Loss: 4.130, avg. samples / sec: 58688.86
Iteration:   3160, Loss function: 2.538, Average Loss: 4.150, avg. samples / sec: 58496.26
Iteration:   3160, Loss function: 4.267, Average Loss: 4.145, avg. samples / sec: 58629.70
Iteration:   3160, Loss function: 4.494, Average Loss: 4.137, avg. samples / sec: 58659.64
Iteration:   3160, Loss function: 2.834, Average Loss: 4.142, avg. samples / sec: 58507.75
Iteration:   3160, Loss function: 3.230, Average Loss: 4.110, avg. samples / sec: 58648.95
Iteration:   3160, Loss function: 3.356, Average Loss: 4.122, avg. samples / sec: 58451.16
Iteration:   3160, Loss function: 4.073, Average Loss: 4.148, avg. samples / sec: 58601.76
Iteration:   3160, Loss function: 2.867, Average Loss: 4.126, avg. samples / sec: 58390.30
Iteration:   3160, Loss function: 3.207, Average Loss: 4.126, avg. samples / sec: 58385.71
Iteration:   3160, Loss function: 3.341, Average Loss: 4.168, avg. samples / sec: 58315.64
Iteration:   3160, Loss function: 4.999, Average Loss: 4.148, avg. samples / sec: 58226.52
Iteration:   3160, Loss function: 2.893, Average Loss: 4.133, avg. samples / sec: 58449.49
Iteration:   3160, Loss function: 4.200, Average Loss: 4.122, avg. samples / sec: 58390.35
Iteration:   3180, Loss function: 3.595, Average Loss: 4.157, avg. samples / sec: 58252.06
Iteration:   3180, Loss function: 2.080, Average Loss: 4.099, avg. samples / sec: 57948.53
Iteration:   3180, Loss function: 3.996, Average Loss: 4.139, avg. samples / sec: 57951.43
Iteration:   3180, Loss function: 2.525, Average Loss: 4.099, avg. samples / sec: 57935.92
Iteration:   3180, Loss function: 3.254, Average Loss: 4.119, avg. samples / sec: 57856.53
Iteration:   3180, Loss function: 3.504, Average Loss: 4.115, avg. samples / sec: 58077.29
Iteration:   3180, Loss function: 4.338, Average Loss: 4.112, avg. samples / sec: 57977.68
Iteration:   3180, Loss function: 3.708, Average Loss: 4.127, avg. samples / sec: 57848.24
Iteration:   3180, Loss function: 3.344, Average Loss: 4.139, avg. samples / sec: 57899.03
Iteration:   3180, Loss function: 3.351, Average Loss: 4.129, avg. samples / sec: 57833.07
Iteration:   3180, Loss function: 3.834, Average Loss: 4.121, avg. samples / sec: 58052.84
Iteration:   3180, Loss function: 3.357, Average Loss: 4.111, avg. samples / sec: 58020.70
Iteration:   3180, Loss function: 3.581, Average Loss: 4.127, avg. samples / sec: 57762.79
Iteration:   3180, Loss function: 4.281, Average Loss: 4.133, avg. samples / sec: 57948.65
Iteration:   3180, Loss function: 3.231, Average Loss: 4.108, avg. samples / sec: 57783.21
Iteration:   3200, Loss function: 4.321, Average Loss: 4.140, avg. samples / sec: 56928.30
Iteration:   3200, Loss function: 3.852, Average Loss: 4.115, avg. samples / sec: 57130.55
Iteration:   3200, Loss function: 3.218, Average Loss: 4.098, avg. samples / sec: 57209.01
Iteration:   3200, Loss function: 4.107, Average Loss: 4.132, avg. samples / sec: 56962.65
Iteration:   3200, Loss function: 3.886, Average Loss: 4.110, avg. samples / sec: 56993.86
Iteration:   3200, Loss function: 5.079, Average Loss: 4.103, avg. samples / sec: 57004.22
Iteration:   3200, Loss function: 3.344, Average Loss: 4.120, avg. samples / sec: 57126.94
Iteration:   3200, Loss function: 4.085, Average Loss: 4.126, avg. samples / sec: 57022.34
Iteration:   3200, Loss function: 3.404, Average Loss: 4.082, avg. samples / sec: 56925.86
Iteration:   3200, Loss function: 3.620, Average Loss: 4.111, avg. samples / sec: 56965.53
Iteration:   3200, Loss function: 3.657, Average Loss: 4.086, avg. samples / sec: 56753.05
Iteration:   3200, Loss function: 2.380, Average Loss: 4.116, avg. samples / sec: 57002.30
Iteration:   3200, Loss function: 3.441, Average Loss: 4.101, avg. samples / sec: 56987.09
Iteration:   3200, Loss function: 3.621, Average Loss: 4.115, avg. samples / sec: 56923.88
Iteration:   3200, Loss function: 4.388, Average Loss: 4.096, avg. samples / sec: 56882.29
:::MLL 1558638819.942 epoch_stop: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 819}}
:::MLL 1558638819.942 epoch_start: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 673}}
Iteration:   3220, Loss function: 3.523, Average Loss: 4.112, avg. samples / sec: 58564.96
Iteration:   3220, Loss function: 3.941, Average Loss: 4.099, avg. samples / sec: 58407.97
Iteration:   3220, Loss function: 3.208, Average Loss: 4.096, avg. samples / sec: 58608.75
Iteration:   3220, Loss function: 2.703, Average Loss: 4.115, avg. samples / sec: 58489.27
Iteration:   3220, Loss function: 2.538, Average Loss: 4.107, avg. samples / sec: 58596.54
Iteration:   3220, Loss function: 3.731, Average Loss: 4.090, avg. samples / sec: 58562.36
Iteration:   3220, Loss function: 3.413, Average Loss: 4.127, avg. samples / sec: 58326.00
Iteration:   3220, Loss function: 3.610, Average Loss: 4.094, avg. samples / sec: 58383.60
Iteration:   3220, Loss function: 2.954, Average Loss: 4.072, avg. samples / sec: 58478.16
Iteration:   3220, Loss function: 2.733, Average Loss: 4.070, avg. samples / sec: 58377.29
Iteration:   3220, Loss function: 3.878, Average Loss: 4.083, avg. samples / sec: 58267.88
Iteration:   3220, Loss function: 2.740, Average Loss: 4.102, avg. samples / sec: 58447.16
Iteration:   3220, Loss function: 5.253, Average Loss: 4.096, avg. samples / sec: 58266.99
Iteration:   3220, Loss function: 4.155, Average Loss: 4.091, avg. samples / sec: 58417.31
Iteration:   3220, Loss function: 3.658, Average Loss: 4.123, avg. samples / sec: 58190.89
Iteration:   3240, Loss function: 2.684, Average Loss: 4.086, avg. samples / sec: 59430.90
Iteration:   3240, Loss function: 4.011, Average Loss: 4.083, avg. samples / sec: 59408.98
Iteration:   3240, Loss function: 3.757, Average Loss: 4.099, avg. samples / sec: 59365.51
Iteration:   3240, Loss function: 3.734, Average Loss: 4.074, avg. samples / sec: 59556.60
Iteration:   3240, Loss function: 3.554, Average Loss: 4.084, avg. samples / sec: 59467.84
Iteration:   3240, Loss function: 3.046, Average Loss: 4.088, avg. samples / sec: 59480.14
Iteration:   3240, Loss function: 3.826, Average Loss: 4.057, avg. samples / sec: 59432.81
Iteration:   3240, Loss function: 4.314, Average Loss: 4.086, avg. samples / sec: 59398.71
Iteration:   3240, Loss function: 6.180, Average Loss: 4.115, avg. samples / sec: 59270.56
Iteration:   3240, Loss function: 2.578, Average Loss: 4.092, avg. samples / sec: 59231.40
Iteration:   3240, Loss function: 3.246, Average Loss: 4.079, avg. samples / sec: 59399.86
Iteration:   3240, Loss function: 3.211, Average Loss: 4.111, avg. samples / sec: 59415.74
Iteration:   3240, Loss function: 4.025, Average Loss: 4.082, avg. samples / sec: 59163.51
Iteration:   3240, Loss function: 3.294, Average Loss: 4.060, avg. samples / sec: 59229.78
Iteration:   3240, Loss function: 4.486, Average Loss: 4.105, avg. samples / sec: 58995.03
Iteration:   3260, Loss function: 3.750, Average Loss: 4.076, avg. samples / sec: 56659.96
Iteration:   3260, Loss function: 3.704, Average Loss: 4.084, avg. samples / sec: 56864.35
Iteration:   3260, Loss function: 4.760, Average Loss: 4.045, avg. samples / sec: 56787.45
Iteration:   3260, Loss function: 4.904, Average Loss: 4.078, avg. samples / sec: 56688.20
Iteration:   3260, Loss function: 3.809, Average Loss: 4.094, avg. samples / sec: 56894.90
Iteration:   3260, Loss function: 2.427, Average Loss: 4.099, avg. samples / sec: 56812.52
Iteration:   3260, Loss function: 3.808, Average Loss: 4.063, avg. samples / sec: 56649.05
Iteration:   3260, Loss function: 3.307, Average Loss: 4.067, avg. samples / sec: 56596.13
Iteration:   3260, Loss function: 4.275, Average Loss: 4.072, avg. samples / sec: 56805.76
Iteration:   3260, Loss function: 3.592, Average Loss: 4.048, avg. samples / sec: 56894.56
Iteration:   3260, Loss function: 3.559, Average Loss: 4.078, avg. samples / sec: 56689.06
Iteration:   3260, Loss function: 3.352, Average Loss: 4.097, avg. samples / sec: 56995.87
Iteration:   3260, Loss function: 3.677, Average Loss: 4.072, avg. samples / sec: 56856.32
Iteration:   3260, Loss function: 4.607, Average Loss: 4.074, avg. samples / sec: 56758.72
Iteration:   3260, Loss function: 2.904, Average Loss: 4.089, avg. samples / sec: 56507.81
Iteration:   3280, Loss function: 3.339, Average Loss: 4.082, avg. samples / sec: 59266.07
Iteration:   3280, Loss function: 3.125, Average Loss: 4.080, avg. samples / sec: 59272.80
Iteration:   3280, Loss function: 4.639, Average Loss: 4.066, avg. samples / sec: 59259.99
Iteration:   3280, Loss function: 3.533, Average Loss: 4.039, avg. samples / sec: 59194.01
Iteration:   3280, Loss function: 4.265, Average Loss: 4.064, avg. samples / sec: 59187.02
Iteration:   3280, Loss function: 3.922, Average Loss: 4.082, avg. samples / sec: 59197.49
Iteration:   3280, Loss function: 2.971, Average Loss: 4.047, avg. samples / sec: 59123.45
Iteration:   3280, Loss function: 4.129, Average Loss: 4.067, avg. samples / sec: 59047.65
Iteration:   3280, Loss function: 2.828, Average Loss: 4.080, avg. samples / sec: 59244.32
Iteration:   3280, Loss function: 5.660, Average Loss: 4.065, avg. samples / sec: 59159.79
Iteration:   3280, Loss function: 2.794, Average Loss: 4.093, avg. samples / sec: 59070.94
Iteration:   3280, Loss function: 4.281, Average Loss: 4.067, avg. samples / sec: 59111.85
Iteration:   3280, Loss function: 4.416, Average Loss: 4.063, avg. samples / sec: 59061.88
Iteration:   3280, Loss function: 3.891, Average Loss: 4.053, avg. samples / sec: 59021.31
Iteration:   3280, Loss function: 4.116, Average Loss: 4.037, avg. samples / sec: 58822.93
:::MLL 1558638821.954 epoch_stop: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 819}}
:::MLL 1558638821.954 epoch_start: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 673}}
Iteration:   3300, Loss function: 2.909, Average Loss: 4.027, avg. samples / sec: 58371.24
Iteration:   3300, Loss function: 3.665, Average Loss: 4.055, avg. samples / sec: 58435.07
Iteration:   3300, Loss function: 2.915, Average Loss: 4.071, avg. samples / sec: 58375.43
Iteration:   3300, Loss function: 2.995, Average Loss: 4.052, avg. samples / sec: 58499.42
Iteration:   3300, Loss function: 3.385, Average Loss: 4.067, avg. samples / sec: 58150.38
Iteration:   3300, Loss function: 5.432, Average Loss: 4.077, avg. samples / sec: 58172.85
Iteration:   3300, Loss function: 2.482, Average Loss: 4.054, avg. samples / sec: 58353.99
Iteration:   3300, Loss function: 2.644, Average Loss: 4.055, avg. samples / sec: 58337.71
Iteration:   3300, Loss function: 3.225, Average Loss: 4.065, avg. samples / sec: 58293.98
Iteration:   3300, Loss function: 3.517, Average Loss: 4.029, avg. samples / sec: 58522.52
Iteration:   3300, Loss function: 3.500, Average Loss: 4.032, avg. samples / sec: 58243.46
Iteration:   3300, Loss function: 3.416, Average Loss: 4.043, avg. samples / sec: 58364.09
Iteration:   3300, Loss function: 2.900, Average Loss: 4.048, avg. samples / sec: 58194.18
Iteration:   3300, Loss function: 3.733, Average Loss: 4.059, avg. samples / sec: 57952.74
Iteration:   3300, Loss function: 3.212, Average Loss: 4.081, avg. samples / sec: 58075.83
Iteration:   3320, Loss function: 4.424, Average Loss: 4.014, avg. samples / sec: 58447.21
Iteration:   3320, Loss function: 3.860, Average Loss: 4.071, avg. samples / sec: 58675.47
Iteration:   3320, Loss function: 4.091, Average Loss: 4.054, avg. samples / sec: 58335.63
Iteration:   3320, Loss function: 2.315, Average Loss: 4.058, avg. samples / sec: 58253.16
Iteration:   3320, Loss function: 3.486, Average Loss: 4.042, avg. samples / sec: 58315.59
Iteration:   3320, Loss function: 3.670, Average Loss: 4.031, avg. samples / sec: 58343.02
Iteration:   3320, Loss function: 3.342, Average Loss: 4.045, avg. samples / sec: 58460.93
Iteration:   3320, Loss function: 3.476, Average Loss: 4.067, avg. samples / sec: 58218.39
Iteration:   3320, Loss function: 2.921, Average Loss: 4.053, avg. samples / sec: 58255.74
Iteration:   3320, Loss function: 3.219, Average Loss: 4.012, avg. samples / sec: 58087.99
Iteration:   3320, Loss function: 3.472, Average Loss: 4.035, avg. samples / sec: 58263.33
Iteration:   3320, Loss function: 3.560, Average Loss: 4.040, avg. samples / sec: 58110.72
Iteration:   3320, Loss function: 3.176, Average Loss: 4.043, avg. samples / sec: 58008.95
Iteration:   3320, Loss function: 4.781, Average Loss: 4.019, avg. samples / sec: 58046.22
Iteration:   3320, Loss function: 2.067, Average Loss: 4.044, avg. samples / sec: 57842.12
Iteration:   3340, Loss function: 3.526, Average Loss: 4.029, avg. samples / sec: 58848.91
Iteration:   3340, Loss function: 2.588, Average Loss: 4.018, avg. samples / sec: 58599.66
Iteration:   3340, Loss function: 4.848, Average Loss: 4.023, avg. samples / sec: 58667.31
Iteration:   3340, Loss function: 4.087, Average Loss: 4.004, avg. samples / sec: 58441.68
Iteration:   3340, Loss function: 3.038, Average Loss: 4.058, avg. samples / sec: 58574.58
Iteration:   3340, Loss function: 2.258, Average Loss: 4.048, avg. samples / sec: 58453.59
Iteration:   3340, Loss function: 3.965, Average Loss: 4.060, avg. samples / sec: 58404.43
Iteration:   3340, Loss function: 3.162, Average Loss: 4.033, avg. samples / sec: 58525.37
Iteration:   3340, Loss function: 3.728, Average Loss: 4.000, avg. samples / sec: 58581.22
Iteration:   3340, Loss function: 4.341, Average Loss: 4.043, avg. samples / sec: 58367.18
Iteration:   3340, Loss function: 3.088, Average Loss: 4.032, avg. samples / sec: 58392.96
Iteration:   3340, Loss function: 3.814, Average Loss: 4.026, avg. samples / sec: 58545.94
Iteration:   3340, Loss function: 2.368, Average Loss: 4.008, avg. samples / sec: 58674.10
Iteration:   3340, Loss function: 3.119, Average Loss: 4.035, avg. samples / sec: 58380.07
Iteration:   3340, Loss function: 3.983, Average Loss: 4.037, avg. samples / sec: 58600.69
:::MLL 1558638823.971 epoch_stop: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 819}}
:::MLL 1558638823.972 epoch_start: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 673}}
Iteration:   3360, Loss function: 3.042, Average Loss: 4.049, avg. samples / sec: 58497.04
Iteration:   3360, Loss function: 4.286, Average Loss: 4.028, avg. samples / sec: 58528.45
Iteration:   3360, Loss function: 3.346, Average Loss: 4.012, avg. samples / sec: 58363.51
Iteration:   3360, Loss function: 3.578, Average Loss: 4.040, avg. samples / sec: 58381.52
Iteration:   3360, Loss function: 3.726, Average Loss: 3.990, avg. samples / sec: 58415.67
Iteration:   3360, Loss function: 2.534, Average Loss: 4.007, avg. samples / sec: 58302.98
Iteration:   3360, Loss function: 2.857, Average Loss: 4.022, avg. samples / sec: 58370.35
Iteration:   3360, Loss function: 2.979, Average Loss: 4.029, avg. samples / sec: 58369.24
Iteration:   3360, Loss function: 3.568, Average Loss: 4.046, avg. samples / sec: 58263.54
Iteration:   3360, Loss function: 2.829, Average Loss: 3.997, avg. samples / sec: 58424.14
Iteration:   3360, Loss function: 3.066, Average Loss: 3.994, avg. samples / sec: 58176.31
Iteration:   3360, Loss function: 2.010, Average Loss: 4.018, avg. samples / sec: 58044.54
Iteration:   3360, Loss function: 3.930, Average Loss: 4.025, avg. samples / sec: 58359.18
Iteration:   3360, Loss function: 3.370, Average Loss: 4.011, avg. samples / sec: 58231.60
Iteration:   3360, Loss function: 4.124, Average Loss: 4.023, avg. samples / sec: 58341.64
Iteration:   3380, Loss function: 4.109, Average Loss: 4.010, avg. samples / sec: 59721.06
Iteration:   3380, Loss function: 2.982, Average Loss: 3.998, avg. samples / sec: 59870.52
Iteration:   3380, Loss function: 3.434, Average Loss: 3.986, avg. samples / sec: 59623.05
Iteration:   3380, Loss function: 2.634, Average Loss: 4.009, avg. samples / sec: 59760.18
Iteration:   3380, Loss function: 3.134, Average Loss: 4.001, avg. samples / sec: 59466.41
Iteration:   3380, Loss function: 2.670, Average Loss: 4.017, avg. samples / sec: 59691.87
Iteration:   3380, Loss function: 3.974, Average Loss: 4.001, avg. samples / sec: 59342.51
Iteration:   3380, Loss function: 3.005, Average Loss: 4.006, avg. samples / sec: 59594.99
Iteration:   3380, Loss function: 2.679, Average Loss: 3.978, avg. samples / sec: 59339.46
Iteration:   3380, Loss function: 4.395, Average Loss: 4.026, avg. samples / sec: 59316.66
Iteration:   3380, Loss function: 2.644, Average Loss: 4.032, avg. samples / sec: 59196.94
Iteration:   3380, Loss function: 3.484, Average Loss: 4.017, avg. samples / sec: 59385.82
Iteration:   3380, Loss function: 3.699, Average Loss: 3.984, avg. samples / sec: 59478.46
Iteration:   3380, Loss function: 3.101, Average Loss: 4.019, avg. samples / sec: 59214.67
Iteration:   3380, Loss function: 3.157, Average Loss: 4.032, avg. samples / sec: 59299.44
Iteration:   3400, Loss function: 5.420, Average Loss: 3.976, avg. samples / sec: 57400.20
Iteration:   3400, Loss function: 3.002, Average Loss: 4.022, avg. samples / sec: 57521.11
Iteration:   3400, Loss function: 3.657, Average Loss: 3.985, avg. samples / sec: 57351.16
Iteration:   3400, Loss function: 2.810, Average Loss: 3.993, avg. samples / sec: 57420.28
Iteration:   3400, Loss function: 4.612, Average Loss: 3.981, avg. samples / sec: 57478.34
Iteration:   3400, Loss function: 3.778, Average Loss: 4.000, avg. samples / sec: 57293.64
Iteration:   3400, Loss function: 4.175, Average Loss: 4.012, avg. samples / sec: 57449.80
Iteration:   3400, Loss function: 4.507, Average Loss: 4.002, avg. samples / sec: 57086.90
Iteration:   3400, Loss function: 2.062, Average Loss: 3.984, avg. samples / sec: 57167.72
Iteration:   3400, Loss function: 2.986, Average Loss: 3.969, avg. samples / sec: 57384.61
Iteration:   3400, Loss function: 3.841, Average Loss: 4.018, avg. samples / sec: 57510.85
Iteration:   3400, Loss function: 3.450, Average Loss: 3.996, avg. samples / sec: 57328.14
Iteration:   3400, Loss function: 4.542, Average Loss: 4.013, avg. samples / sec: 57304.80
Iteration:   3400, Loss function: 3.763, Average Loss: 4.014, avg. samples / sec: 57375.73
Iteration:   3400, Loss function: 3.261, Average Loss: 4.002, avg. samples / sec: 57174.50
Iteration:   3420, Loss function: 2.427, Average Loss: 3.969, avg. samples / sec: 57094.77
Iteration:   3420, Loss function: 3.263, Average Loss: 3.987, avg. samples / sec: 57149.83
Iteration:   3420, Loss function: 2.538, Average Loss: 3.969, avg. samples / sec: 57023.47
Iteration:   3420, Loss function: 3.397, Average Loss: 3.968, avg. samples / sec: 56863.20
Iteration:   3420, Loss function: 2.693, Average Loss: 3.990, avg. samples / sec: 57003.25
Iteration:   3420, Loss function: 2.640, Average Loss: 3.975, avg. samples / sec: 56947.60
Iteration:   3420, Loss function: 3.776, Average Loss: 4.001, avg. samples / sec: 57115.18
Iteration:   3420, Loss function: 4.001, Average Loss: 4.010, avg. samples / sec: 56899.79
Iteration:   3420, Loss function: 2.521, Average Loss: 3.960, avg. samples / sec: 56981.56
Iteration:   3420, Loss function: 3.692, Average Loss: 3.999, avg. samples / sec: 57075.30
Iteration:   3420, Loss function: 3.096, Average Loss: 3.973, avg. samples / sec: 56873.07
Iteration:   3420, Loss function: 5.236, Average Loss: 4.006, avg. samples / sec: 56939.54
Iteration:   3420, Loss function: 3.591, Average Loss: 3.993, avg. samples / sec: 56912.25
Iteration:   3420, Loss function: 2.772, Average Loss: 3.997, avg. samples / sec: 56892.01
Iteration:   3420, Loss function: 3.893, Average Loss: 3.992, avg. samples / sec: 57009.43
:::MLL 1558638825.972 eval_start: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.50s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.56s)
DONE (t=2.80s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22325
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.38292
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.22479
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05546
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.23451
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.36282
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.21750
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.31767
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33390
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09600
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36116
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.52021
Current AP: 0.22325 AP goal: 0.23000
:::MLL 1558638829.928 eval_accuracy: {"value": 0.2232484672812133, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 389}}
:::MLL 1558638829.976 eval_stop: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 392}}
:::MLL 1558638829.985 block_stop: {"value": null, "metadata": {"first_epoch_num": 44, "file": "train.py", "lineno": 804}}
:::MLL 1558638829.986 block_start: {"value": null, "metadata": {"first_epoch_num": 49, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
:::MLL 1558638830.020 epoch_stop: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 819}}
:::MLL 1558638830.021 epoch_start: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 673}}
Iteration:   3440, Loss function: 3.091, Average Loss: 3.993, avg. samples / sec: 7293.48
Iteration:   3440, Loss function: 4.406, Average Loss: 3.962, avg. samples / sec: 7293.21
Iteration:   3440, Loss function: 2.319, Average Loss: 3.947, avg. samples / sec: 7293.27
Iteration:   3440, Loss function: 4.655, Average Loss: 3.959, avg. samples / sec: 7292.21
Iteration:   3440, Loss function: 3.075, Average Loss: 3.980, avg. samples / sec: 7292.03
Iteration:   3440, Loss function: 3.197, Average Loss: 3.970, avg. samples / sec: 7290.70
Iteration:   3440, Loss function: 3.976, Average Loss: 3.960, avg. samples / sec: 7292.88
Iteration:   3440, Loss function: 2.391, Average Loss: 3.997, avg. samples / sec: 7291.80
Iteration:   3440, Loss function: 3.852, Average Loss: 3.953, avg. samples / sec: 7291.02
Iteration:   3440, Loss function: 4.322, Average Loss: 3.984, avg. samples / sec: 7292.56
Iteration:   3440, Loss function: 3.015, Average Loss: 3.984, avg. samples / sec: 7292.10
Iteration:   3440, Loss function: 2.333, Average Loss: 3.959, avg. samples / sec: 7287.32
Iteration:   3440, Loss function: 3.422, Average Loss: 3.982, avg. samples / sec: 7290.92
Iteration:   3440, Loss function: 2.854, Average Loss: 3.993, avg. samples / sec: 7290.35
Iteration:   3440, Loss function: 3.522, Average Loss: 3.991, avg. samples / sec: 7288.64
Iteration:   3460, Loss function: 3.436, Average Loss: 3.930, avg. samples / sec: 60506.15
Iteration:   3460, Loss function: 2.569, Average Loss: 3.983, avg. samples / sec: 60554.68
Iteration:   3460, Loss function: 2.215, Average Loss: 3.976, avg. samples / sec: 60613.10
Iteration:   3460, Loss function: 3.251, Average Loss: 3.957, avg. samples / sec: 60423.91
Iteration:   3460, Loss function: 4.064, Average Loss: 3.983, avg. samples / sec: 60764.34
Iteration:   3460, Loss function: 4.492, Average Loss: 3.948, avg. samples / sec: 60484.75
Iteration:   3460, Loss function: 2.262, Average Loss: 3.979, avg. samples / sec: 60357.66
Iteration:   3460, Loss function: 3.763, Average Loss: 3.972, avg. samples / sec: 60514.23
Iteration:   3460, Loss function: 3.681, Average Loss: 3.945, avg. samples / sec: 60590.40
Iteration:   3460, Loss function: 3.910, Average Loss: 3.947, avg. samples / sec: 60359.78
Iteration:   3460, Loss function: 3.881, Average Loss: 3.958, avg. samples / sec: 60382.02
Iteration:   3460, Loss function: 3.226, Average Loss: 3.972, avg. samples / sec: 60561.37
Iteration:   3460, Loss function: 3.913, Average Loss: 3.980, avg. samples / sec: 60553.88
Iteration:   3460, Loss function: 4.327, Average Loss: 3.945, avg. samples / sec: 60382.98
Iteration:   3460, Loss function: 4.552, Average Loss: 3.968, avg. samples / sec: 60235.40
Iteration:   3480, Loss function: 3.462, Average Loss: 3.971, avg. samples / sec: 56127.49
Iteration:   3480, Loss function: 2.709, Average Loss: 3.970, avg. samples / sec: 55969.12
Iteration:   3480, Loss function: 3.483, Average Loss: 3.949, avg. samples / sec: 56034.81
Iteration:   3480, Loss function: 3.918, Average Loss: 3.970, avg. samples / sec: 55953.27
Iteration:   3480, Loss function: 3.507, Average Loss: 3.967, avg. samples / sec: 55971.96
Iteration:   3480, Loss function: 3.364, Average Loss: 3.962, avg. samples / sec: 56043.12
Iteration:   3480, Loss function: 2.780, Average Loss: 3.973, avg. samples / sec: 55907.92
Iteration:   3480, Loss function: 3.510, Average Loss: 3.965, avg. samples / sec: 55967.89
Iteration:   3480, Loss function: 3.974, Average Loss: 3.935, avg. samples / sec: 55974.67
Iteration:   3480, Loss function: 2.843, Average Loss: 3.944, avg. samples / sec: 55884.42
Iteration:   3480, Loss function: 2.755, Average Loss: 3.923, avg. samples / sec: 55811.72
Iteration:   3480, Loss function: 2.967, Average Loss: 3.935, avg. samples / sec: 55905.80
Iteration:   3480, Loss function: 3.410, Average Loss: 3.958, avg. samples / sec: 56012.16
Iteration:   3480, Loss function: 3.360, Average Loss: 3.937, avg. samples / sec: 55941.52
Iteration:   3480, Loss function: 3.151, Average Loss: 3.933, avg. samples / sec: 55820.96
:::MLL 1558638832.050 epoch_stop: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 819}}
:::MLL 1558638832.050 epoch_start: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 673}}
Iteration:   3500, Loss function: 3.751, Average Loss: 3.961, avg. samples / sec: 58810.82
Iteration:   3500, Loss function: 2.637, Average Loss: 3.960, avg. samples / sec: 58761.32
Iteration:   3500, Loss function: 3.665, Average Loss: 3.959, avg. samples / sec: 58706.83
Iteration:   3500, Loss function: 5.074, Average Loss: 3.963, avg. samples / sec: 58666.58
Iteration:   3500, Loss function: 2.903, Average Loss: 3.956, avg. samples / sec: 58693.43
Iteration:   3500, Loss function: 3.130, Average Loss: 3.922, avg. samples / sec: 58809.11
Iteration:   3500, Loss function: 4.415, Average Loss: 3.954, avg. samples / sec: 58611.09
Iteration:   3500, Loss function: 3.096, Average Loss: 3.967, avg. samples / sec: 58575.14
Iteration:   3500, Loss function: 3.296, Average Loss: 3.927, avg. samples / sec: 58671.14
Iteration:   3500, Loss function: 3.179, Average Loss: 3.942, avg. samples / sec: 58517.95
Iteration:   3500, Loss function: 3.754, Average Loss: 3.925, avg. samples / sec: 58555.81
Iteration:   3500, Loss function: 3.259, Average Loss: 3.944, avg. samples / sec: 58653.32
Iteration:   3500, Loss function: 3.175, Average Loss: 3.930, avg. samples / sec: 58633.16
Iteration:   3500, Loss function: 4.731, Average Loss: 3.934, avg. samples / sec: 58517.32
Iteration:   3500, Loss function: 4.188, Average Loss: 3.914, avg. samples / sec: 58412.01
Iteration:   3520, Loss function: 2.658, Average Loss: 3.925, avg. samples / sec: 59634.38
Iteration:   3520, Loss function: 2.830, Average Loss: 3.935, avg. samples / sec: 59456.02
Iteration:   3520, Loss function: 3.272, Average Loss: 3.932, avg. samples / sec: 59496.08
Iteration:   3520, Loss function: 3.051, Average Loss: 3.939, avg. samples / sec: 59363.26
Iteration:   3520, Loss function: 2.477, Average Loss: 3.943, avg. samples / sec: 59246.21
Iteration:   3520, Loss function: 3.389, Average Loss: 3.952, avg. samples / sec: 59083.45
Iteration:   3520, Loss function: 2.767, Average Loss: 3.954, avg. samples / sec: 59253.14
Iteration:   3520, Loss function: 4.209, Average Loss: 3.913, avg. samples / sec: 59156.34
Iteration:   3520, Loss function: 3.532, Average Loss: 3.915, avg. samples / sec: 59259.12
Iteration:   3520, Loss function: 3.141, Average Loss: 3.950, avg. samples / sec: 58906.03
Iteration:   3520, Loss function: 2.600, Average Loss: 3.903, avg. samples / sec: 59441.91
Iteration:   3520, Loss function: 3.416, Average Loss: 3.928, avg. samples / sec: 59276.04
Iteration:   3520, Loss function: 3.287, Average Loss: 3.917, avg. samples / sec: 59211.27
Iteration:   3520, Loss function: 2.502, Average Loss: 3.951, avg. samples / sec: 58965.58
Iteration:   3520, Loss function: 4.476, Average Loss: 3.946, avg. samples / sec: 58852.18
Iteration:   3540, Loss function: 2.873, Average Loss: 3.940, avg. samples / sec: 59120.68
Iteration:   3540, Loss function: 3.242, Average Loss: 3.890, avg. samples / sec: 59117.80
Iteration:   3540, Loss function: 2.997, Average Loss: 3.905, avg. samples / sec: 59029.03
Iteration:   3540, Loss function: 3.377, Average Loss: 3.942, avg. samples / sec: 58967.77
Iteration:   3540, Loss function: 3.444, Average Loss: 3.910, avg. samples / sec: 59078.17
Iteration:   3540, Loss function: 3.917, Average Loss: 3.922, avg. samples / sec: 58796.32
Iteration:   3540, Loss function: 2.481, Average Loss: 3.934, avg. samples / sec: 59217.29
Iteration:   3540, Loss function: 3.323, Average Loss: 3.926, avg. samples / sec: 58778.28
Iteration:   3540, Loss function: 1.860, Average Loss: 3.915, avg. samples / sec: 59020.97
Iteration:   3540, Loss function: 2.858, Average Loss: 3.923, avg. samples / sec: 58777.56
Iteration:   3540, Loss function: 2.621, Average Loss: 3.947, avg. samples / sec: 58916.40
Iteration:   3540, Loss function: 2.955, Average Loss: 3.900, avg. samples / sec: 58895.59
Iteration:   3540, Loss function: 2.987, Average Loss: 3.915, avg. samples / sec: 58519.85
Iteration:   3540, Loss function: 3.444, Average Loss: 3.939, avg. samples / sec: 58871.09
Iteration:   3540, Loss function: 3.542, Average Loss: 3.932, avg. samples / sec: 58535.14
:::MLL 1558638833.752 epoch_stop: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 819}}
:::MLL 1558638833.753 epoch_start: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 673}}
Iteration:   3560, Loss function: 3.642, Average Loss: 3.901, avg. samples / sec: 59625.70
Iteration:   3560, Loss function: 2.903, Average Loss: 3.929, avg. samples / sec: 59670.66
Iteration:   3560, Loss function: 2.110, Average Loss: 3.922, avg. samples / sec: 59833.03
Iteration:   3560, Loss function: 1.767, Average Loss: 3.907, avg. samples / sec: 59611.55
Iteration:   3560, Loss function: 3.335, Average Loss: 3.930, avg. samples / sec: 59520.06
Iteration:   3560, Loss function: 2.746, Average Loss: 3.892, avg. samples / sec: 59614.85
Iteration:   3560, Loss function: 4.730, Average Loss: 3.925, avg. samples / sec: 59417.87
Iteration:   3560, Loss function: 3.004, Average Loss: 3.893, avg. samples / sec: 59457.76
Iteration:   3560, Loss function: 3.228, Average Loss: 3.905, avg. samples / sec: 59465.13
Iteration:   3560, Loss function: 3.085, Average Loss: 3.922, avg. samples / sec: 59808.62
Iteration:   3560, Loss function: 3.460, Average Loss: 3.919, avg. samples / sec: 59441.98
Iteration:   3560, Loss function: 2.338, Average Loss: 3.902, avg. samples / sec: 59595.92
Iteration:   3560, Loss function: 4.358, Average Loss: 3.912, avg. samples / sec: 59493.52
Iteration:   3560, Loss function: 3.263, Average Loss: 3.879, avg. samples / sec: 59352.38
Iteration:   3560, Loss function: 2.897, Average Loss: 3.917, avg. samples / sec: 59437.24
Iteration:   3580, Loss function: 4.554, Average Loss: 3.887, avg. samples / sec: 57185.66
Iteration:   3580, Loss function: 3.565, Average Loss: 3.919, avg. samples / sec: 57013.58
Iteration:   3580, Loss function: 3.569, Average Loss: 3.871, avg. samples / sec: 57139.28
Iteration:   3580, Loss function: 3.392, Average Loss: 3.896, avg. samples / sec: 57050.83
Iteration:   3580, Loss function: 3.738, Average Loss: 3.907, avg. samples / sec: 57066.33
Iteration:   3580, Loss function: 3.388, Average Loss: 3.879, avg. samples / sec: 56965.23
Iteration:   3580, Loss function: 3.213, Average Loss: 3.915, avg. samples / sec: 56926.29
Iteration:   3580, Loss function: 3.639, Average Loss: 3.889, avg. samples / sec: 57030.65
Iteration:   3580, Loss function: 3.882, Average Loss: 3.889, avg. samples / sec: 56790.77
Iteration:   3580, Loss function: 4.848, Average Loss: 3.899, avg. samples / sec: 56815.24
Iteration:   3580, Loss function: 3.215, Average Loss: 3.905, avg. samples / sec: 56970.39
Iteration:   3580, Loss function: 4.819, Average Loss: 3.915, avg. samples / sec: 56911.15
Iteration:   3580, Loss function: 3.563, Average Loss: 3.912, avg. samples / sec: 56771.46
Iteration:   3580, Loss function: 3.885, Average Loss: 3.914, avg. samples / sec: 56820.03
Iteration:   3580, Loss function: 3.527, Average Loss: 3.914, avg. samples / sec: 56880.09
Iteration:   3600, Loss function: 3.552, Average Loss: 3.883, avg. samples / sec: 60336.68
Iteration:   3600, Loss function: 2.286, Average Loss: 3.862, avg. samples / sec: 60265.33
Iteration:   3600, Loss function: 1.905, Average Loss: 3.869, avg. samples / sec: 60353.29
Iteration:   3600, Loss function: 2.952, Average Loss: 3.882, avg. samples / sec: 60307.71
Iteration:   3600, Loss function: 3.095, Average Loss: 3.909, avg. samples / sec: 60253.35
Iteration:   3600, Loss function: 2.640, Average Loss: 3.900, avg. samples / sec: 60423.70
Iteration:   3600, Loss function: 3.506, Average Loss: 3.887, avg. samples / sec: 60306.36
Iteration:   3600, Loss function: 3.185, Average Loss: 3.907, avg. samples / sec: 60319.11
Iteration:   3600, Loss function: 2.928, Average Loss: 3.907, avg. samples / sec: 60297.87
Iteration:   3600, Loss function: 3.223, Average Loss: 3.898, avg. samples / sec: 60231.28
Iteration:   3600, Loss function: 3.997, Average Loss: 3.878, avg. samples / sec: 60185.68
Iteration:   3600, Loss function: 3.234, Average Loss: 3.908, avg. samples / sec: 59923.57
Iteration:   3600, Loss function: 3.558, Average Loss: 3.901, avg. samples / sec: 59998.47
Iteration:   3600, Loss function: 3.832, Average Loss: 3.909, avg. samples / sec: 60112.38
Iteration:   3600, Loss function: 3.031, Average Loss: 3.875, avg. samples / sec: 59657.17
Iteration:   3620, Loss function: 3.803, Average Loss: 3.879, avg. samples / sec: 58519.05
Iteration:   3620, Loss function: 3.636, Average Loss: 3.888, avg. samples / sec: 58768.94
Iteration:   3620, Loss function: 2.928, Average Loss: 3.899, avg. samples / sec: 58791.25
Iteration:   3620, Loss function: 4.143, Average Loss: 3.867, avg. samples / sec: 58643.73
Iteration:   3620, Loss function: 3.683, Average Loss: 3.878, avg. samples / sec: 58525.32
Iteration:   3620, Loss function: 2.986, Average Loss: 3.896, avg. samples / sec: 58546.86
Iteration:   3620, Loss function: 3.362, Average Loss: 3.873, avg. samples / sec: 58328.77
Iteration:   3620, Loss function: 2.525, Average Loss: 3.895, avg. samples / sec: 58557.22
Iteration:   3620, Loss function: 3.155, Average Loss: 3.889, avg. samples / sec: 58364.89
Iteration:   3620, Loss function: 2.706, Average Loss: 3.899, avg. samples / sec: 58334.01
Iteration:   3620, Loss function: 2.309, Average Loss: 3.859, avg. samples / sec: 58219.59
Iteration:   3620, Loss function: 2.691, Average Loss: 3.871, avg. samples / sec: 58648.73
Iteration:   3620, Loss function: 3.590, Average Loss: 3.899, avg. samples / sec: 58441.54
Iteration:   3620, Loss function: 2.760, Average Loss: 3.850, avg. samples / sec: 58143.33
Iteration:   3620, Loss function: 3.384, Average Loss: 3.888, avg. samples / sec: 58334.76
:::MLL 1558638835.765 epoch_stop: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 819}}
:::MLL 1558638835.765 epoch_start: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 673}}
Iteration:   3640, Loss function: 3.461, Average Loss: 3.863, avg. samples / sec: 58863.17
Iteration:   3640, Loss function: 2.881, Average Loss: 3.885, avg. samples / sec: 58731.86
Iteration:   3640, Loss function: 3.558, Average Loss: 3.870, avg. samples / sec: 58601.93
Iteration:   3640, Loss function: 3.144, Average Loss: 3.858, avg. samples / sec: 58510.91
Iteration:   3640, Loss function: 3.499, Average Loss: 3.871, avg. samples / sec: 58533.27
Iteration:   3640, Loss function: 3.394, Average Loss: 3.877, avg. samples / sec: 58794.29
Iteration:   3640, Loss function: 3.319, Average Loss: 3.877, avg. samples / sec: 58442.97
Iteration:   3640, Loss function: 3.641, Average Loss: 3.871, avg. samples / sec: 58371.15
Iteration:   3640, Loss function: 3.275, Average Loss: 3.851, avg. samples / sec: 58578.52
Iteration:   3640, Loss function: 3.205, Average Loss: 3.886, avg. samples / sec: 58592.57
Iteration:   3640, Loss function: 3.689, Average Loss: 3.890, avg. samples / sec: 58299.89
Iteration:   3640, Loss function: 2.849, Average Loss: 3.882, avg. samples / sec: 58371.53
Iteration:   3640, Loss function: 3.581, Average Loss: 3.881, avg. samples / sec: 58442.17
Iteration:   3640, Loss function: 3.201, Average Loss: 3.845, avg. samples / sec: 58503.45
Iteration:   3640, Loss function: 2.175, Average Loss: 3.885, avg. samples / sec: 58265.47
Iteration:   3660, Loss function: 2.566, Average Loss: 3.880, avg. samples / sec: 58997.89
Iteration:   3660, Loss function: 2.664, Average Loss: 3.842, avg. samples / sec: 58929.23
Iteration:   3660, Loss function: 2.794, Average Loss: 3.866, avg. samples / sec: 58833.93
Iteration:   3660, Loss function: 4.022, Average Loss: 3.863, avg. samples / sec: 58604.68
Iteration:   3660, Loss function: 3.347, Average Loss: 3.854, avg. samples / sec: 58508.87
Iteration:   3660, Loss function: 2.083, Average Loss: 3.865, avg. samples / sec: 58612.75
Iteration:   3660, Loss function: 2.397, Average Loss: 3.845, avg. samples / sec: 58568.37
Iteration:   3660, Loss function: 2.890, Average Loss: 3.871, avg. samples / sec: 58783.96
Iteration:   3660, Loss function: 2.309, Average Loss: 3.834, avg. samples / sec: 58804.35
Iteration:   3660, Loss function: 4.451, Average Loss: 3.864, avg. samples / sec: 58537.89
Iteration:   3660, Loss function: 4.665, Average Loss: 3.879, avg. samples / sec: 58470.08
Iteration:   3660, Loss function: 2.951, Average Loss: 3.881, avg. samples / sec: 58777.00
Iteration:   3660, Loss function: 3.222, Average Loss: 3.877, avg. samples / sec: 58658.62
Iteration:   3660, Loss function: 3.368, Average Loss: 3.871, avg. samples / sec: 58490.68
Iteration:   3660, Loss function: 2.302, Average Loss: 3.872, avg. samples / sec: 58667.41
Iteration:   3680, Loss function: 3.871, Average Loss: 3.865, avg. samples / sec: 58190.24
Iteration:   3680, Loss function: 3.430, Average Loss: 3.844, avg. samples / sec: 58045.74
Iteration:   3680, Loss function: 5.606, Average Loss: 3.857, avg. samples / sec: 58091.06
Iteration:   3680, Loss function: 3.514, Average Loss: 3.856, avg. samples / sec: 58019.10
Iteration:   3680, Loss function: 3.540, Average Loss: 3.870, avg. samples / sec: 58066.31
Iteration:   3680, Loss function: 3.051, Average Loss: 3.863, avg. samples / sec: 58013.55
Iteration:   3680, Loss function: 2.318, Average Loss: 3.862, avg. samples / sec: 57917.92
Iteration:   3680, Loss function: 2.817, Average Loss: 3.866, avg. samples / sec: 57948.60
Iteration:   3680, Loss function: 3.388, Average Loss: 3.872, avg. samples / sec: 57635.67
Iteration:   3680, Loss function: 3.563, Average Loss: 3.858, avg. samples / sec: 57973.77
Iteration:   3680, Loss function: 3.609, Average Loss: 3.838, avg. samples / sec: 57636.29
Iteration:   3680, Loss function: 3.577, Average Loss: 3.828, avg. samples / sec: 57877.13
Iteration:   3680, Loss function: 3.332, Average Loss: 3.856, avg. samples / sec: 57729.43
Iteration:   3680, Loss function: 2.897, Average Loss: 3.833, avg. samples / sec: 57725.71
Iteration:   3680, Loss function: 4.716, Average Loss: 3.856, avg. samples / sec: 57473.77
:::MLL 1558638837.769 epoch_stop: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 819}}
:::MLL 1558638837.770 epoch_start: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 673}}
Iteration:   3700, Loss function: 3.676, Average Loss: 3.860, avg. samples / sec: 59498.22
Iteration:   3700, Loss function: 3.633, Average Loss: 3.848, avg. samples / sec: 59533.66
Iteration:   3700, Loss function: 3.730, Average Loss: 3.850, avg. samples / sec: 59089.89
Iteration:   3700, Loss function: 4.797, Average Loss: 3.838, avg. samples / sec: 59012.84
Iteration:   3700, Loss function: 3.378, Average Loss: 3.849, avg. samples / sec: 59245.52
Iteration:   3700, Loss function: 4.022, Average Loss: 3.849, avg. samples / sec: 59329.55
Iteration:   3700, Loss function: 3.001, Average Loss: 3.826, avg. samples / sec: 59370.14
Iteration:   3700, Loss function: 4.574, Average Loss: 3.832, avg. samples / sec: 59216.14
Iteration:   3700, Loss function: 3.305, Average Loss: 3.854, avg. samples / sec: 59160.78
Iteration:   3700, Loss function: 2.394, Average Loss: 3.855, avg. samples / sec: 58934.31
Iteration:   3700, Loss function: 2.746, Average Loss: 3.848, avg. samples / sec: 59151.02
Iteration:   3700, Loss function: 3.656, Average Loss: 3.820, avg. samples / sec: 59205.22
Iteration:   3700, Loss function: 2.583, Average Loss: 3.852, avg. samples / sec: 59151.99
Iteration:   3700, Loss function: 4.297, Average Loss: 3.849, avg. samples / sec: 58989.22
Iteration:   3700, Loss function: 3.163, Average Loss: 3.861, avg. samples / sec: 58857.86
Iteration:   3720, Loss function: 2.205, Average Loss: 3.831, avg. samples / sec: 57255.61
Iteration:   3720, Loss function: 3.794, Average Loss: 3.843, avg. samples / sec: 57152.42
Iteration:   3720, Loss function: 3.145, Average Loss: 3.836, avg. samples / sec: 57137.38
Iteration:   3720, Loss function: 3.070, Average Loss: 3.826, avg. samples / sec: 57169.93
Iteration:   3720, Loss function: 3.489, Average Loss: 3.843, avg. samples / sec: 57223.27
Iteration:   3720, Loss function: 3.423, Average Loss: 3.839, avg. samples / sec: 57083.57
Iteration:   3720, Loss function: 4.478, Average Loss: 3.852, avg. samples / sec: 57177.33
Iteration:   3720, Loss function: 3.959, Average Loss: 3.860, avg. samples / sec: 57364.70
Iteration:   3720, Loss function: 3.634, Average Loss: 3.846, avg. samples / sec: 57153.70
Iteration:   3720, Loss function: 4.237, Average Loss: 3.810, avg. samples / sec: 57176.28
Iteration:   3720, Loss function: 2.189, Average Loss: 3.825, avg. samples / sec: 57090.81
Iteration:   3720, Loss function: 4.495, Average Loss: 3.820, avg. samples / sec: 57092.08
Iteration:   3720, Loss function: 3.362, Average Loss: 3.839, avg. samples / sec: 57042.38
Iteration:   3720, Loss function: 3.588, Average Loss: 3.852, avg. samples / sec: 56814.26
Iteration:   3720, Loss function: 3.439, Average Loss: 3.841, avg. samples / sec: 57043.44
Iteration:   3740, Loss function: 3.343, Average Loss: 3.841, avg. samples / sec: 57754.05
Iteration:   3740, Loss function: 2.506, Average Loss: 3.824, avg. samples / sec: 57691.54
Iteration:   3740, Loss function: 4.368, Average Loss: 3.820, avg. samples / sec: 57689.87
Iteration:   3740, Loss function: 3.544, Average Loss: 3.802, avg. samples / sec: 57707.84
Iteration:   3740, Loss function: 3.149, Average Loss: 3.844, avg. samples / sec: 57741.42
Iteration:   3740, Loss function: 3.422, Average Loss: 3.844, avg. samples / sec: 57653.73
Iteration:   3740, Loss function: 4.174, Average Loss: 3.828, avg. samples / sec: 57633.34
Iteration:   3740, Loss function: 3.615, Average Loss: 3.836, avg. samples / sec: 57575.02
Iteration:   3740, Loss function: 4.253, Average Loss: 3.824, avg. samples / sec: 57519.14
Iteration:   3740, Loss function: 3.535, Average Loss: 3.832, avg. samples / sec: 57727.72
Iteration:   3740, Loss function: 4.258, Average Loss: 3.836, avg. samples / sec: 57539.94
Iteration:   3740, Loss function: 3.759, Average Loss: 3.833, avg. samples / sec: 57611.45
Iteration:   3740, Loss function: 3.223, Average Loss: 3.812, avg. samples / sec: 57554.21
Iteration:   3740, Loss function: 2.483, Average Loss: 3.818, avg. samples / sec: 57449.94
Iteration:   3740, Loss function: 4.305, Average Loss: 3.853, avg. samples / sec: 57382.97
Iteration:   3760, Loss function: 4.768, Average Loss: 3.815, avg. samples / sec: 58390.96
Iteration:   3760, Loss function: 4.346, Average Loss: 3.791, avg. samples / sec: 58166.44
Iteration:   3760, Loss function: 3.171, Average Loss: 3.829, avg. samples / sec: 58210.81
Iteration:   3760, Loss function: 2.869, Average Loss: 3.811, avg. samples / sec: 58150.48
Iteration:   3760, Loss function: 4.194, Average Loss: 3.800, avg. samples / sec: 58356.09
Iteration:   3760, Loss function: 2.892, Average Loss: 3.829, avg. samples / sec: 58292.51
Iteration:   3760, Loss function: 3.421, Average Loss: 3.842, avg. samples / sec: 58452.59
Iteration:   3760, Loss function: 3.791, Average Loss: 3.834, avg. samples / sec: 58159.81
Iteration:   3760, Loss function: 3.686, Average Loss: 3.821, avg. samples / sec: 58280.92
Iteration:   3760, Loss function: 3.277, Average Loss: 3.822, avg. samples / sec: 58157.46
Iteration:   3760, Loss function: 3.845, Average Loss: 3.816, avg. samples / sec: 58109.12
Iteration:   3760, Loss function: 3.340, Average Loss: 3.808, avg. samples / sec: 58339.81
Iteration:   3760, Loss function: 4.024, Average Loss: 3.819, avg. samples / sec: 58151.63
Iteration:   3760, Loss function: 3.733, Average Loss: 3.830, avg. samples / sec: 58048.63
Iteration:   3760, Loss function: 2.242, Average Loss: 3.835, avg. samples / sec: 57905.81
:::MLL 1558638839.798 epoch_stop: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 819}}
:::MLL 1558638839.799 epoch_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 673}}
Iteration:   3780, Loss function: 3.591, Average Loss: 3.809, avg. samples / sec: 58255.86
Iteration:   3780, Loss function: 3.895, Average Loss: 3.831, avg. samples / sec: 58508.60
Iteration:   3780, Loss function: 3.845, Average Loss: 3.827, avg. samples / sec: 58299.55
Iteration:   3780, Loss function: 2.721, Average Loss: 3.804, avg. samples / sec: 58291.35
Iteration:   3780, Loss function: 4.428, Average Loss: 3.815, avg. samples / sec: 58280.24
Iteration:   3780, Loss function: 3.015, Average Loss: 3.837, avg. samples / sec: 58253.91
Iteration:   3780, Loss function: 4.231, Average Loss: 3.791, avg. samples / sec: 58233.50
Iteration:   3780, Loss function: 2.900, Average Loss: 3.809, avg. samples / sec: 58334.86
Iteration:   3780, Loss function: 3.310, Average Loss: 3.800, avg. samples / sec: 58269.42
Iteration:   3780, Loss function: 2.993, Average Loss: 3.819, avg. samples / sec: 58332.10
Iteration:   3780, Loss function: 2.452, Average Loss: 3.807, avg. samples / sec: 58121.27
Iteration:   3780, Loss function: 3.626, Average Loss: 3.821, avg. samples / sec: 58089.45
Iteration:   3780, Loss function: 2.174, Average Loss: 3.801, avg. samples / sec: 58054.95
Iteration:   3780, Loss function: 3.457, Average Loss: 3.782, avg. samples / sec: 58025.14
Iteration:   3780, Loss function: 2.677, Average Loss: 3.817, avg. samples / sec: 57972.65
Iteration:   3800, Loss function: 3.338, Average Loss: 3.796, avg. samples / sec: 58657.20
Iteration:   3800, Loss function: 2.829, Average Loss: 3.818, avg. samples / sec: 58582.22
Iteration:   3800, Loss function: 3.764, Average Loss: 3.794, avg. samples / sec: 58646.46
Iteration:   3800, Loss function: 2.532, Average Loss: 3.807, avg. samples / sec: 58850.19
Iteration:   3800, Loss function: 2.353, Average Loss: 3.800, avg. samples / sec: 58698.86
Iteration:   3800, Loss function: 2.498, Average Loss: 3.803, avg. samples / sec: 58562.36
Iteration:   3800, Loss function: 3.033, Average Loss: 3.805, avg. samples / sec: 58521.48
Iteration:   3800, Loss function: 3.225, Average Loss: 3.784, avg. samples / sec: 58537.13
Iteration:   3800, Loss function: 2.435, Average Loss: 3.787, avg. samples / sec: 58684.97
Iteration:   3800, Loss function: 3.524, Average Loss: 3.813, avg. samples / sec: 58536.48
Iteration:   3800, Loss function: 3.170, Average Loss: 3.827, avg. samples / sec: 58428.48
Iteration:   3800, Loss function: 2.360, Average Loss: 3.801, avg. samples / sec: 58210.29
Iteration:   3800, Loss function: 3.345, Average Loss: 3.823, avg. samples / sec: 58297.62
Iteration:   3800, Loss function: 3.706, Average Loss: 3.774, avg. samples / sec: 58547.20
Iteration:   3800, Loss function: 3.849, Average Loss: 3.814, avg. samples / sec: 58462.44
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
:::MLL 1558638840.959 eval_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.57s)
DONE (t=0.60s)
DONE (t=2.80s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22642
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.38702
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.22981
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05697
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.23758
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.36808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22037
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32192
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33819
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.10048
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36849
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.52147
Current AP: 0.22642 AP goal: 0.23000
:::MLL 1558638844.940 eval_accuracy: {"value": 0.226419788085869, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 389}}
:::MLL 1558638844.973 eval_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 392}}
:::MLL 1558638844.983 block_stop: {"value": null, "metadata": {"first_epoch_num": 49, "file": "train.py", "lineno": 804}}
:::MLL 1558638844.984 block_start: {"value": null, "metadata": {"first_epoch_num": 55, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3820, Loss function: 4.360, Average Loss: 3.792, avg. samples / sec: 7291.74
Iteration:   3820, Loss function: 2.485, Average Loss: 3.805, avg. samples / sec: 7295.02
Iteration:   3820, Loss function: 2.547, Average Loss: 3.814, avg. samples / sec: 7292.96
Iteration:   3820, Loss function: 5.060, Average Loss: 3.790, avg. samples / sec: 7291.09
Iteration:   3820, Loss function: 3.521, Average Loss: 3.779, avg. samples / sec: 7291.54
Iteration:   3820, Loss function: 3.144, Average Loss: 3.799, avg. samples / sec: 7290.82
Iteration:   3820, Loss function: 2.870, Average Loss: 3.785, avg. samples / sec: 7289.19
Iteration:   3820, Loss function: 2.205, Average Loss: 3.797, avg. samples / sec: 7289.36
Iteration:   3820, Loss function: 2.659, Average Loss: 3.791, avg. samples / sec: 7287.54
Iteration:   3820, Loss function: 2.825, Average Loss: 3.767, avg. samples / sec: 7292.31
Iteration:   3820, Loss function: 3.730, Average Loss: 3.812, avg. samples / sec: 7291.73
Iteration:   3820, Loss function: 2.192, Average Loss: 3.797, avg. samples / sec: 7291.74
Iteration:   3820, Loss function: 3.735, Average Loss: 3.808, avg. samples / sec: 7287.36
Iteration:   3820, Loss function: 4.073, Average Loss: 3.776, avg. samples / sec: 7283.50
Iteration:   3820, Loss function: 3.690, Average Loss: 3.802, avg. samples / sec: 7284.13
:::MLL 1558638845.843 epoch_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 819}}
:::MLL 1558638845.843 epoch_start: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 673}}
Iteration:   3840, Loss function: 2.634, Average Loss: 3.801, avg. samples / sec: 59831.83
Iteration:   3840, Loss function: 3.008, Average Loss: 3.795, avg. samples / sec: 60107.33
Iteration:   3840, Loss function: 4.129, Average Loss: 3.772, avg. samples / sec: 59522.29
Iteration:   3840, Loss function: 2.726, Average Loss: 3.804, avg. samples / sec: 59493.07
Iteration:   3840, Loss function: 4.243, Average Loss: 3.762, avg. samples / sec: 59613.36
Iteration:   3840, Loss function: 2.964, Average Loss: 3.787, avg. samples / sec: 59567.23
Iteration:   3840, Loss function: 3.866, Average Loss: 3.791, avg. samples / sec: 59521.34
Iteration:   3840, Loss function: 4.804, Average Loss: 3.776, avg. samples / sec: 59503.82
Iteration:   3840, Loss function: 2.968, Average Loss: 3.766, avg. samples / sec: 59937.89
Iteration:   3840, Loss function: 3.638, Average Loss: 3.795, avg. samples / sec: 59395.71
Iteration:   3840, Loss function: 3.536, Average Loss: 3.784, avg. samples / sec: 59322.05
Iteration:   3840, Loss function: 2.650, Average Loss: 3.776, avg. samples / sec: 59383.70
Iteration:   3840, Loss function: 3.370, Average Loss: 3.786, avg. samples / sec: 59502.47
Iteration:   3840, Loss function: 3.430, Average Loss: 3.803, avg. samples / sec: 59494.28
Iteration:   3840, Loss function: 4.117, Average Loss: 3.785, avg. samples / sec: 59400.97
Iteration:   3860, Loss function: 3.209, Average Loss: 3.759, avg. samples / sec: 59676.17
Iteration:   3860, Loss function: 3.342, Average Loss: 3.769, avg. samples / sec: 59711.57
Iteration:   3860, Loss function: 3.359, Average Loss: 3.777, avg. samples / sec: 59774.15
Iteration:   3860, Loss function: 3.469, Average Loss: 3.795, avg. samples / sec: 59396.46
Iteration:   3860, Loss function: 1.868, Average Loss: 3.789, avg. samples / sec: 59574.93
Iteration:   3860, Loss function: 3.180, Average Loss: 3.772, avg. samples / sec: 59541.96
Iteration:   3860, Loss function: 3.441, Average Loss: 3.775, avg. samples / sec: 59589.42
Iteration:   3860, Loss function: 2.769, Average Loss: 3.757, avg. samples / sec: 59527.90
Iteration:   3860, Loss function: 4.621, Average Loss: 3.799, avg. samples / sec: 59431.88
Iteration:   3860, Loss function: 3.114, Average Loss: 3.791, avg. samples / sec: 59329.37
Iteration:   3860, Loss function: 3.102, Average Loss: 3.772, avg. samples / sec: 59521.77
Iteration:   3860, Loss function: 3.059, Average Loss: 3.761, avg. samples / sec: 59350.23
Iteration:   3860, Loss function: 4.646, Average Loss: 3.779, avg. samples / sec: 59367.04
Iteration:   3860, Loss function: 3.256, Average Loss: 3.784, avg. samples / sec: 59347.36
Iteration:   3860, Loss function: 2.805, Average Loss: 3.792, avg. samples / sec: 59451.36
Iteration:   3880, Loss function: 3.155, Average Loss: 3.763, avg. samples / sec: 58688.44
Iteration:   3880, Loss function: 3.568, Average Loss: 3.749, avg. samples / sec: 58590.70
Iteration:   3880, Loss function: 3.672, Average Loss: 3.775, avg. samples / sec: 58898.79
Iteration:   3880, Loss function: 2.865, Average Loss: 3.790, avg. samples / sec: 58774.55
Iteration:   3880, Loss function: 1.869, Average Loss: 3.780, avg. samples / sec: 58758.30
Iteration:   3880, Loss function: 3.647, Average Loss: 3.749, avg. samples / sec: 58679.06
Iteration:   3880, Loss function: 2.968, Average Loss: 3.776, avg. samples / sec: 58574.21
Iteration:   3880, Loss function: 4.454, Average Loss: 3.749, avg. samples / sec: 58680.33
Iteration:   3880, Loss function: 3.889, Average Loss: 3.765, avg. samples / sec: 58569.49
Iteration:   3880, Loss function: 3.677, Average Loss: 3.770, avg. samples / sec: 58443.07
Iteration:   3880, Loss function: 3.890, Average Loss: 3.787, avg. samples / sec: 58586.78
Iteration:   3880, Loss function: 2.472, Average Loss: 3.791, avg. samples / sec: 58352.20
Iteration:   3880, Loss function: 2.537, Average Loss: 3.766, avg. samples / sec: 58276.70
Iteration:   3880, Loss function: 3.592, Average Loss: 3.762, avg. samples / sec: 58344.13
Iteration:   3880, Loss function: 3.484, Average Loss: 3.771, avg. samples / sec: 58393.28
Iteration:   3900, Loss function: 2.855, Average Loss: 3.740, avg. samples / sec: 56639.94
Iteration:   3900, Loss function: 3.418, Average Loss: 3.754, avg. samples / sec: 56360.08
Iteration:   3900, Loss function: 3.597, Average Loss: 3.777, avg. samples / sec: 56276.16
Iteration:   3900, Loss function: 1.941, Average Loss: 3.769, avg. samples / sec: 56313.35
Iteration:   3900, Loss function: 2.192, Average Loss: 3.764, avg. samples / sec: 56609.20
Iteration:   3900, Loss function: 4.661, Average Loss: 3.786, avg. samples / sec: 56446.47
Iteration:   3900, Loss function: 3.730, Average Loss: 3.777, avg. samples / sec: 56424.64
Iteration:   3900, Loss function: 2.908, Average Loss: 3.744, avg. samples / sec: 56215.26
Iteration:   3900, Loss function: 2.095, Average Loss: 3.743, avg. samples / sec: 56102.71
Iteration:   3900, Loss function: 2.994, Average Loss: 3.756, avg. samples / sec: 56361.28
Iteration:   3900, Loss function: 2.788, Average Loss: 3.770, avg. samples / sec: 56241.95
Iteration:   3900, Loss function: 3.084, Average Loss: 3.763, avg. samples / sec: 56415.63
Iteration:   3900, Loss function: 3.474, Average Loss: 3.765, avg. samples / sec: 56349.85
Iteration:   3900, Loss function: 4.180, Average Loss: 3.767, avg. samples / sec: 56045.66
Iteration:   3900, Loss function: 2.454, Average Loss: 3.753, avg. samples / sec: 56411.14
:::MLL 1558638847.858 epoch_stop: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 819}}
:::MLL 1558638847.858 epoch_start: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 673}}
Iteration:   3920, Loss function: 3.892, Average Loss: 3.782, avg. samples / sec: 58665.70
Iteration:   3920, Loss function: 4.054, Average Loss: 3.746, avg. samples / sec: 58358.39
Iteration:   3920, Loss function: 3.479, Average Loss: 3.746, avg. samples / sec: 58720.33
Iteration:   3920, Loss function: 3.098, Average Loss: 3.759, avg. samples / sec: 58700.74
Iteration:   3920, Loss function: 2.443, Average Loss: 3.746, avg. samples / sec: 58625.01
Iteration:   3920, Loss function: 2.430, Average Loss: 3.757, avg. samples / sec: 58671.24
Iteration:   3920, Loss function: 3.715, Average Loss: 3.759, avg. samples / sec: 58537.45
Iteration:   3920, Loss function: 2.658, Average Loss: 3.733, avg. samples / sec: 58551.48
Iteration:   3920, Loss function: 4.309, Average Loss: 3.765, avg. samples / sec: 58576.06
Iteration:   3920, Loss function: 1.943, Average Loss: 3.759, avg. samples / sec: 58380.07
Iteration:   3920, Loss function: 3.590, Average Loss: 3.734, avg. samples / sec: 58070.38
Iteration:   3920, Loss function: 3.772, Average Loss: 3.734, avg. samples / sec: 58441.42
Iteration:   3920, Loss function: 2.821, Average Loss: 3.755, avg. samples / sec: 58467.29
Iteration:   3920, Loss function: 4.262, Average Loss: 3.771, avg. samples / sec: 58155.71
Iteration:   3920, Loss function: 2.950, Average Loss: 3.767, avg. samples / sec: 58292.87
Iteration:   3940, Loss function: 3.774, Average Loss: 3.739, avg. samples / sec: 58897.27
Iteration:   3940, Loss function: 2.890, Average Loss: 3.735, avg. samples / sec: 58849.09
Iteration:   3940, Loss function: 2.726, Average Loss: 3.751, avg. samples / sec: 58880.68
Iteration:   3940, Loss function: 2.936, Average Loss: 3.726, avg. samples / sec: 59004.44
Iteration:   3940, Loss function: 3.652, Average Loss: 3.764, avg. samples / sec: 59105.20
Iteration:   3940, Loss function: 3.578, Average Loss: 3.760, avg. samples / sec: 58837.91
Iteration:   3940, Loss function: 3.048, Average Loss: 3.730, avg. samples / sec: 58823.25
Iteration:   3940, Loss function: 3.026, Average Loss: 3.776, avg. samples / sec: 58653.15
Iteration:   3940, Loss function: 2.884, Average Loss: 3.754, avg. samples / sec: 59034.10
Iteration:   3940, Loss function: 3.236, Average Loss: 3.750, avg. samples / sec: 58805.25
Iteration:   3940, Loss function: 2.419, Average Loss: 3.750, avg. samples / sec: 58920.64
Iteration:   3940, Loss function: 3.916, Average Loss: 3.753, avg. samples / sec: 58666.94
Iteration:   3940, Loss function: 3.101, Average Loss: 3.738, avg. samples / sec: 58616.14
Iteration:   3940, Loss function: 3.531, Average Loss: 3.749, avg. samples / sec: 58668.09
Iteration:   3940, Loss function: 3.201, Average Loss: 3.723, avg. samples / sec: 58770.58
Iteration:   3960, Loss function: 3.886, Average Loss: 3.737, avg. samples / sec: 59896.17
Iteration:   3960, Loss function: 3.409, Average Loss: 3.747, avg. samples / sec: 60048.81
Iteration:   3960, Loss function: 2.506, Average Loss: 3.750, avg. samples / sec: 59938.84
Iteration:   3960, Loss function: 2.867, Average Loss: 3.728, avg. samples / sec: 60033.85
Iteration:   3960, Loss function: 2.780, Average Loss: 3.741, avg. samples / sec: 59944.37
Iteration:   3960, Loss function: 3.312, Average Loss: 3.712, avg. samples / sec: 59822.33
Iteration:   3960, Loss function: 2.951, Average Loss: 3.745, avg. samples / sec: 59960.59
Iteration:   3960, Loss function: 3.222, Average Loss: 3.720, avg. samples / sec: 59813.29
Iteration:   3960, Loss function: 2.857, Average Loss: 3.743, avg. samples / sec: 59853.25
Iteration:   3960, Loss function: 2.203, Average Loss: 3.726, avg. samples / sec: 59680.82
Iteration:   3960, Loss function: 2.687, Average Loss: 3.739, avg. samples / sec: 59909.54
Iteration:   3960, Loss function: 3.182, Average Loss: 3.717, avg. samples / sec: 59937.46
Iteration:   3960, Loss function: 3.254, Average Loss: 3.761, avg. samples / sec: 59774.99
Iteration:   3960, Loss function: 3.618, Average Loss: 3.741, avg. samples / sec: 59616.11
Iteration:   3960, Loss function: 4.484, Average Loss: 3.757, avg. samples / sec: 59693.31
:::MLL 1558638849.854 epoch_stop: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 819}}
:::MLL 1558638849.854 epoch_start: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 673}}
Iteration:   3980, Loss function: 2.968, Average Loss: 3.718, avg. samples / sec: 59259.74
Iteration:   3980, Loss function: 3.880, Average Loss: 3.726, avg. samples / sec: 59187.64
Iteration:   3980, Loss function: 3.185, Average Loss: 3.735, avg. samples / sec: 58962.57
Iteration:   3980, Loss function: 3.504, Average Loss: 3.709, avg. samples / sec: 59150.90
Iteration:   3980, Loss function: 3.432, Average Loss: 3.719, avg. samples / sec: 59126.11
Iteration:   3980, Loss function: 3.727, Average Loss: 3.755, avg. samples / sec: 59184.19
Iteration:   3980, Loss function: 3.021, Average Loss: 3.733, avg. samples / sec: 58981.35
Iteration:   3980, Loss function: 3.211, Average Loss: 3.737, avg. samples / sec: 59056.09
Iteration:   3980, Loss function: 3.643, Average Loss: 3.710, avg. samples / sec: 59071.61
Iteration:   3980, Loss function: 3.433, Average Loss: 3.752, avg. samples / sec: 59087.66
Iteration:   3980, Loss function: 3.441, Average Loss: 3.730, avg. samples / sec: 58798.65
Iteration:   3980, Loss function: 2.130, Average Loss: 3.733, avg. samples / sec: 59063.91
Iteration:   3980, Loss function: 2.526, Average Loss: 3.743, avg. samples / sec: 58813.79
Iteration:   3980, Loss function: 5.039, Average Loss: 3.739, avg. samples / sec: 58654.32
Iteration:   3980, Loss function: 3.329, Average Loss: 3.701, avg. samples / sec: 58466.20
Iteration:   4000, Loss function: 3.392, Average Loss: 3.713, avg. samples / sec: 57668.10
Iteration:   4000, Loss function: 5.287, Average Loss: 3.750, avg. samples / sec: 57624.03
Iteration:   4000, Loss function: 2.720, Average Loss: 3.726, avg. samples / sec: 57721.60
Iteration:   4000, Loss function: 2.647, Average Loss: 3.737, avg. samples / sec: 57749.25
Iteration:   4000, Loss function: 1.925, Average Loss: 3.699, avg. samples / sec: 57656.38
Iteration:   4000, Loss function: 3.406, Average Loss: 3.719, avg. samples / sec: 57501.07
Iteration:   4000, Loss function: 4.309, Average Loss: 3.731, avg. samples / sec: 57618.21
Iteration:   4000, Loss function: 3.058, Average Loss: 3.728, avg. samples / sec: 57926.52
Iteration:   4000, Loss function: 3.660, Average Loss: 3.698, avg. samples / sec: 58016.47
Iteration:   4000, Loss function: 2.914, Average Loss: 3.728, avg. samples / sec: 57525.29
Iteration:   4000, Loss function: 2.696, Average Loss: 3.725, avg. samples / sec: 57367.16
Iteration:   4000, Loss function: 3.111, Average Loss: 3.743, avg. samples / sec: 57469.22
Iteration:   4000, Loss function: 3.968, Average Loss: 3.711, avg. samples / sec: 57116.96
Iteration:   4000, Loss function: 3.245, Average Loss: 3.702, avg. samples / sec: 57289.17
Iteration:   4000, Loss function: 3.521, Average Loss: 3.725, avg. samples / sec: 57277.86
Iteration:   4020, Loss function: 2.857, Average Loss: 3.694, avg. samples / sec: 58544.94
Iteration:   4020, Loss function: 3.522, Average Loss: 3.732, avg. samples / sec: 58532.95
Iteration:   4020, Loss function: 2.404, Average Loss: 3.715, avg. samples / sec: 58561.77
Iteration:   4020, Loss function: 3.458, Average Loss: 3.718, avg. samples / sec: 58623.97
Iteration:   4020, Loss function: 3.094, Average Loss: 3.694, avg. samples / sec: 58743.26
Iteration:   4020, Loss function: 3.548, Average Loss: 3.705, avg. samples / sec: 58695.19
Iteration:   4020, Loss function: 3.094, Average Loss: 3.739, avg. samples / sec: 58438.10
Iteration:   4020, Loss function: 3.571, Average Loss: 3.721, avg. samples / sec: 58593.72
Iteration:   4020, Loss function: 3.619, Average Loss: 3.686, avg. samples / sec: 58510.01
Iteration:   4020, Loss function: 3.363, Average Loss: 3.726, avg. samples / sec: 58428.31
Iteration:   4020, Loss function: 3.385, Average Loss: 3.715, avg. samples / sec: 58673.37
Iteration:   4020, Loss function: 3.055, Average Loss: 3.716, avg. samples / sec: 58296.83
Iteration:   4020, Loss function: 3.408, Average Loss: 3.725, avg. samples / sec: 58336.64
Iteration:   4020, Loss function: 2.735, Average Loss: 3.734, avg. samples / sec: 58483.50
Iteration:   4020, Loss function: 3.688, Average Loss: 3.708, avg. samples / sec: 58176.98
Iteration:   4040, Loss function: 5.257, Average Loss: 3.683, avg. samples / sec: 57873.47
Iteration:   4040, Loss function: 3.767, Average Loss: 3.712, avg. samples / sec: 57944.76
Iteration:   4040, Loss function: 3.266, Average Loss: 3.693, avg. samples / sec: 57747.88
Iteration:   4040, Loss function: 3.054, Average Loss: 3.705, avg. samples / sec: 57902.22
Iteration:   4040, Loss function: 2.240, Average Loss: 3.696, avg. samples / sec: 57729.92
Iteration:   4040, Loss function: 3.388, Average Loss: 3.700, avg. samples / sec: 57905.91
Iteration:   4040, Loss function: 2.971, Average Loss: 3.708, avg. samples / sec: 57857.36
Iteration:   4040, Loss function: 4.082, Average Loss: 3.724, avg. samples / sec: 57885.55
Iteration:   4040, Loss function: 3.273, Average Loss: 3.716, avg. samples / sec: 57775.81
Iteration:   4040, Loss function: 3.174, Average Loss: 3.729, avg. samples / sec: 57679.45
Iteration:   4040, Loss function: 2.215, Average Loss: 3.687, avg. samples / sec: 57601.16
Iteration:   4040, Loss function: 3.270, Average Loss: 3.709, avg. samples / sec: 57651.02
Iteration:   4040, Loss function: 3.022, Average Loss: 3.726, avg. samples / sec: 57588.19
Iteration:   4040, Loss function: 4.582, Average Loss: 3.714, avg. samples / sec: 57691.47
Iteration:   4040, Loss function: 3.154, Average Loss: 3.708, avg. samples / sec: 57571.98
:::MLL 1558638851.874 epoch_stop: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 819}}
:::MLL 1558638851.875 epoch_start: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 673}}
Iteration:   4060, Loss function: 3.408, Average Loss: 3.713, avg. samples / sec: 59720.85
Iteration:   4060, Loss function: 2.230, Average Loss: 3.715, avg. samples / sec: 59633.14
Iteration:   4060, Loss function: 3.314, Average Loss: 3.701, avg. samples / sec: 59640.38
Iteration:   4060, Loss function: 3.315, Average Loss: 3.697, avg. samples / sec: 59504.53
Iteration:   4060, Loss function: 3.217, Average Loss: 3.676, avg. samples / sec: 59437.89
Iteration:   4060, Loss function: 3.117, Average Loss: 3.721, avg. samples / sec: 59528.53
Iteration:   4060, Loss function: 2.552, Average Loss: 3.702, avg. samples / sec: 59548.98
Iteration:   4060, Loss function: 2.947, Average Loss: 3.680, avg. samples / sec: 59496.56
Iteration:   4060, Loss function: 3.227, Average Loss: 3.687, avg. samples / sec: 59419.35
Iteration:   4060, Loss function: 3.779, Average Loss: 3.687, avg. samples / sec: 59458.13
Iteration:   4060, Loss function: 4.377, Average Loss: 3.717, avg. samples / sec: 59507.34
Iteration:   4060, Loss function: 3.428, Average Loss: 3.690, avg. samples / sec: 59387.12
Iteration:   4060, Loss function: 3.047, Average Loss: 3.709, avg. samples / sec: 59362.68
Iteration:   4060, Loss function: 2.851, Average Loss: 3.701, avg. samples / sec: 59243.35
Iteration:   4060, Loss function: 3.791, Average Loss: 3.704, avg. samples / sec: 59271.28
Iteration:   4080, Loss function: 2.217, Average Loss: 3.677, avg. samples / sec: 58792.62
Iteration:   4080, Loss function: 2.788, Average Loss: 3.693, avg. samples / sec: 58592.57
Iteration:   4080, Loss function: 3.333, Average Loss: 3.712, avg. samples / sec: 58479.61
Iteration:   4080, Loss function: 4.342, Average Loss: 3.672, avg. samples / sec: 58647.09
Iteration:   4080, Loss function: 3.838, Average Loss: 3.705, avg. samples / sec: 58456.11
Iteration:   4080, Loss function: 3.268, Average Loss: 3.698, avg. samples / sec: 58808.86
Iteration:   4080, Loss function: 3.303, Average Loss: 3.698, avg. samples / sec: 58694.04
Iteration:   4080, Loss function: 4.577, Average Loss: 3.674, avg. samples / sec: 58517.64
Iteration:   4080, Loss function: 3.900, Average Loss: 3.694, avg. samples / sec: 58693.38
Iteration:   4080, Loss function: 2.371, Average Loss: 3.688, avg. samples / sec: 58476.43
Iteration:   4080, Loss function: 3.335, Average Loss: 3.713, avg. samples / sec: 58522.01
Iteration:   4080, Loss function: 2.710, Average Loss: 3.688, avg. samples / sec: 58594.23
Iteration:   4080, Loss function: 3.564, Average Loss: 3.707, avg. samples / sec: 58530.91
Iteration:   4080, Loss function: 2.376, Average Loss: 3.693, avg. samples / sec: 58330.29
Iteration:   4080, Loss function: 3.860, Average Loss: 3.682, avg. samples / sec: 58249.45
Iteration:   4100, Loss function: 2.734, Average Loss: 3.696, avg. samples / sec: 59381.92
Iteration:   4100, Loss function: 2.843, Average Loss: 3.663, avg. samples / sec: 59139.18
Iteration:   4100, Loss function: 3.065, Average Loss: 3.712, avg. samples / sec: 59262.46
Iteration:   4100, Loss function: 3.609, Average Loss: 3.683, avg. samples / sec: 59365.58
Iteration:   4100, Loss function: 3.822, Average Loss: 3.700, avg. samples / sec: 59400.27
Iteration:   4100, Loss function: 3.949, Average Loss: 3.673, avg. samples / sec: 59299.84
Iteration:   4100, Loss function: 2.620, Average Loss: 3.683, avg. samples / sec: 59217.56
Iteration:   4100, Loss function: 4.042, Average Loss: 3.689, avg. samples / sec: 59339.81
Iteration:   4100, Loss function: 2.720, Average Loss: 3.691, avg. samples / sec: 59211.36
Iteration:   4100, Loss function: 5.246, Average Loss: 3.709, avg. samples / sec: 59227.39
Iteration:   4100, Loss function: 3.888, Average Loss: 3.680, avg. samples / sec: 59506.16
Iteration:   4100, Loss function: 4.111, Average Loss: 3.696, avg. samples / sec: 59148.29
Iteration:   4100, Loss function: 3.584, Average Loss: 3.680, avg. samples / sec: 59132.51
Iteration:   4100, Loss function: 2.571, Average Loss: 3.689, avg. samples / sec: 59289.36
Iteration:   4100, Loss function: 3.939, Average Loss: 3.667, avg. samples / sec: 58965.33
:::MLL 1558638853.876 epoch_stop: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 819}}
:::MLL 1558638853.877 epoch_start: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 673}}
Iteration:   4120, Loss function: 3.762, Average Loss: 3.662, avg. samples / sec: 58420.00
Iteration:   4120, Loss function: 2.586, Average Loss: 3.679, avg. samples / sec: 58673.27
Iteration:   4120, Loss function: 3.185, Average Loss: 3.675, avg. samples / sec: 58371.00
Iteration:   4120, Loss function: 3.441, Average Loss: 3.660, avg. samples / sec: 58627.06
Iteration:   4120, Loss function: 3.218, Average Loss: 3.711, avg. samples / sec: 58328.77
Iteration:   4120, Loss function: 3.788, Average Loss: 3.696, avg. samples / sec: 58308.45
Iteration:   4120, Loss function: 3.484, Average Loss: 3.690, avg. samples / sec: 58158.45
Iteration:   4120, Loss function: 3.215, Average Loss: 3.687, avg. samples / sec: 58361.45
Iteration:   4120, Loss function: 2.530, Average Loss: 3.675, avg. samples / sec: 58351.57
Iteration:   4120, Loss function: 3.519, Average Loss: 3.703, avg. samples / sec: 58286.17
Iteration:   4120, Loss function: 3.730, Average Loss: 3.678, avg. samples / sec: 58168.00
Iteration:   4120, Loss function: 3.788, Average Loss: 3.685, avg. samples / sec: 58229.79
Iteration:   4120, Loss function: 3.455, Average Loss: 3.674, avg. samples / sec: 58157.44
Iteration:   4120, Loss function: 3.164, Average Loss: 3.678, avg. samples / sec: 58129.44
Iteration:   4120, Loss function: 3.066, Average Loss: 3.670, avg. samples / sec: 58329.50
Iteration:   4140, Loss function: 3.568, Average Loss: 3.665, avg. samples / sec: 57962.95
Iteration:   4140, Loss function: 3.324, Average Loss: 3.674, avg. samples / sec: 57896.84
Iteration:   4140, Loss function: 2.933, Average Loss: 3.680, avg. samples / sec: 57766.48
Iteration:   4140, Loss function: 2.875, Average Loss: 3.650, avg. samples / sec: 57678.30
Iteration:   4140, Loss function: 3.084, Average Loss: 3.690, avg. samples / sec: 57691.80
Iteration:   4140, Loss function: 3.328, Average Loss: 3.681, avg. samples / sec: 57801.19
Iteration:   4140, Loss function: 3.715, Average Loss: 3.696, avg. samples / sec: 57805.93
Iteration:   4140, Loss function: 3.086, Average Loss: 3.660, avg. samples / sec: 57861.28
Iteration:   4140, Loss function: 4.012, Average Loss: 3.674, avg. samples / sec: 57852.56
Iteration:   4140, Loss function: 3.131, Average Loss: 3.705, avg. samples / sec: 57636.19
Iteration:   4140, Loss function: 4.720, Average Loss: 3.681, avg. samples / sec: 57786.64
Iteration:   4140, Loss function: 3.170, Average Loss: 3.669, avg. samples / sec: 57714.93
Iteration:   4140, Loss function: 4.291, Average Loss: 3.677, avg. samples / sec: 57476.32
Iteration:   4140, Loss function: 4.336, Average Loss: 3.653, avg. samples / sec: 57439.62
Iteration:   4140, Loss function: 2.695, Average Loss: 3.667, avg. samples / sec: 57391.71
Iteration:   4160, Loss function: 3.019, Average Loss: 3.655, avg. samples / sec: 58513.68
Iteration:   4160, Loss function: 3.011, Average Loss: 3.660, avg. samples / sec: 58394.80
Iteration:   4160, Loss function: 3.782, Average Loss: 3.677, avg. samples / sec: 58576.09
Iteration:   4160, Loss function: 4.151, Average Loss: 3.664, avg. samples / sec: 58388.32
Iteration:   4160, Loss function: 2.591, Average Loss: 3.664, avg. samples / sec: 58715.02
Iteration:   4160, Loss function: 1.927, Average Loss: 3.686, avg. samples / sec: 58443.45
Iteration:   4160, Loss function: 3.609, Average Loss: 3.698, avg. samples / sec: 58431.63
Iteration:   4160, Loss function: 2.684, Average Loss: 3.671, avg. samples / sec: 58365.47
Iteration:   4160, Loss function: 2.746, Average Loss: 3.669, avg. samples / sec: 58337.83
Iteration:   4160, Loss function: 3.366, Average Loss: 3.653, avg. samples / sec: 58402.28
Iteration:   4160, Loss function: 4.065, Average Loss: 3.673, avg. samples / sec: 58279.57
Iteration:   4160, Loss function: 3.272, Average Loss: 3.684, avg. samples / sec: 58257.35
Iteration:   4160, Loss function: 3.877, Average Loss: 3.653, avg. samples / sec: 58215.58
Iteration:   4160, Loss function: 3.505, Average Loss: 3.666, avg. samples / sec: 58236.58
Iteration:   4160, Loss function: 3.016, Average Loss: 3.660, avg. samples / sec: 58269.04
Iteration:   4180, Loss function: 3.043, Average Loss: 3.681, avg. samples / sec: 58984.19
Iteration:   4180, Loss function: 3.834, Average Loss: 3.696, avg. samples / sec: 58977.32
Iteration:   4180, Loss function: 3.219, Average Loss: 3.657, avg. samples / sec: 58881.81
Iteration:   4180, Loss function: 3.759, Average Loss: 3.660, avg. samples / sec: 59040.40
Iteration:   4180, Loss function: 3.803, Average Loss: 3.680, avg. samples / sec: 59060.89
Iteration:   4180, Loss function: 4.317, Average Loss: 3.660, avg. samples / sec: 58826.78
Iteration:   4180, Loss function: 4.033, Average Loss: 3.651, avg. samples / sec: 58988.63
Iteration:   4180, Loss function: 3.538, Average Loss: 3.647, avg. samples / sec: 58993.84
Iteration:   4180, Loss function: 3.677, Average Loss: 3.649, avg. samples / sec: 58720.43
Iteration:   4180, Loss function: 3.807, Average Loss: 3.655, avg. samples / sec: 58679.45
Iteration:   4180, Loss function: 4.198, Average Loss: 3.667, avg. samples / sec: 58706.46
Iteration:   4180, Loss function: 2.967, Average Loss: 3.660, avg. samples / sec: 58926.06
Iteration:   4180, Loss function: 3.722, Average Loss: 3.653, avg. samples / sec: 58886.41
Iteration:   4180, Loss function: 3.813, Average Loss: 3.664, avg. samples / sec: 58769.18
Iteration:   4180, Loss function: 2.660, Average Loss: 3.666, avg. samples / sec: 58582.29
:::MLL 1558638855.891 epoch_stop: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 819}}
:::MLL 1558638855.891 epoch_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 673}}
:::MLL 1558638855.954 eval_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.56s)
DONE (t=2.83s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.23128
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.39368
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23456
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05923
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24291
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.37471
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22388
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32679
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.34332
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.10272
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.37382
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.52862
Current AP: 0.23128 AP goal: 0.23000
:::MLL 1558638860.013 eval_accuracy: {"value": 0.23128009984359066, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 389}}
:::MLL 1558638860.023 eval_stop: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 392}}
:::MLL 1558638860.033 block_stop: {"value": null, "metadata": {"first_epoch_num": 55, "file": "train.py", "lineno": 804}}
:::MLL 1558638861.092 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 849}}
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-05-23 07:14:30 PM
RESULT,SINGLE_STAGE_DETECTOR,,218,nvidia,2019-05-23 07:10:52 PM
ENDING TIMING RUN AT 2019-05-23 07:14:30 PM
RESULT,SINGLE_STAGE_DETECTOR,,218,nvidia,2019-05-23 07:10:52 PM
ENDING TIMING RUN AT 2019-05-23 07:14:30 PM
RESULT,SINGLE_STAGE_DETECTOR,,218,nvidia,2019-05-23 07:10:52 PM
ENDING TIMING RUN AT 2019-05-23 07:14:30 PM
RESULT,SINGLE_STAGE_DETECTOR,,218,nvidia,2019-05-23 07:10:52 PM
ENDING TIMING RUN AT 2019-05-23 07:14:30 PM
RESULT,SINGLE_STAGE_DETECTOR,,218,nvidia,2019-05-23 07:10:52 PM
ENDING TIMING RUN AT 2019-05-23 07:14:30 PM
RESULT,SINGLE_STAGE_DETECTOR,,218,nvidia,2019-05-23 07:10:52 PM
ENDING TIMING RUN AT 2019-05-23 07:14:30 PM
RESULT,SINGLE_STAGE_DETECTOR,,218,nvidia,2019-05-23 07:10:52 PM
ENDING TIMING RUN AT 2019-05-23 07:14:30 PM
RESULT,SINGLE_STAGE_DETECTOR,,218,nvidia,2019-05-23 07:10:52 PM
ENDING TIMING RUN AT 2019-05-23 07:14:30 PM
RESULT,SINGLE_STAGE_DETECTOR,,218,nvidia,2019-05-23 07:10:52 PM
ENDING TIMING RUN AT 2019-05-23 07:14:31 PM
RESULT,SINGLE_STAGE_DETECTOR,,219,nvidia,2019-05-23 07:10:52 PM
ENDING TIMING RUN AT 2019-05-23 07:14:31 PM
RESULT,SINGLE_STAGE_DETECTOR,,219,nvidia,2019-05-23 07:10:52 PM
ENDING TIMING RUN AT 2019-05-23 07:14:31 PM
RESULT,SINGLE_STAGE_DETECTOR,,219,nvidia,2019-05-23 07:10:52 PM
ENDING TIMING RUN AT 2019-05-23 07:14:31 PM
RESULT,SINGLE_STAGE_DETECTOR,,219,nvidia,2019-05-23 07:10:52 PM
ENDING TIMING RUN AT 2019-05-23 07:14:31 PM
RESULT,SINGLE_STAGE_DETECTOR,,219,nvidia,2019-05-23 07:10:52 PM
ENDING TIMING RUN AT 2019-05-23 07:14:31 PM
RESULT,SINGLE_STAGE_DETECTOR,,219,nvidia,2019-05-23 07:10:52 PM
