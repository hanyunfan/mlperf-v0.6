Beginning trial 2 of 5
Gathering sys log on circe-n016
:::MLL 1558638872.175 submission_benchmark: {"value": "ssd", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1558638872.176 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known ssd keys.
:::MLL 1558638872.176 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1558638872.176 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1558638872.177 submission_platform: {"value": "15xNVIDIA DGX-2H", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1558638872.177 submission_entry: {"value": "{'hardware': 'NVIDIA DGX-2H', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': 'Ubuntu 18.04.2 LTS / NVIDIA DGX Server 4.0.5', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.4-2.0.7', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '15', 'cpu': '2x Intel(R) Xeon(R) Platinum 8174 CPU @ 3.10GHz', 'num_cores': '48', 'num_vcpus': '96', 'accelerator': 'Tesla V100-SXM3-32GB-H', 'num_accelerators': '16', 'sys_mem_size': '1510 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '2x 894.3G + 8x 3.5T', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27800 Family [ConnectX-5]', 'num_network_cards': '10', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1558638872.177 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1558638872.178 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
:::MLL 1558638874.980 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638874.980 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638875.028 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638875.042 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638875.048 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638875.062 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638875.053 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638875.060 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638875.074 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638875.076 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638875.086 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638875.055 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638875.073 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638875.071 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558638875.102 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node circe-n016
+ pids+=($!)
+ set +x
Launching on node circe-n017
+ pids+=($!)
+ set +x
Launching on node circe-n018
+ pids+=($!)
+ set +x
Launching on node circe-n019
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n017
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n016
+ pids+=($!)
+ set +x
Launching on node circe-n020
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n018
+ srun --mem=0 -N 1 -n 1 -w circe-n017 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=1 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w circe-n016 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=0 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n021
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ srun --mem=0 -N 1 -n 1 -w circe-n018 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=2 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n019
+ pids+=($!)
+ set +x
Launching on node circe-n022
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n020
+ srun --mem=0 -N 1 -n 1 -w circe-n019 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=3 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n023
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n021
+ srun --mem=0 -N 1 -n 1 -w circe-n020 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=4 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n024
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n022
+ pids+=($!)
+ set +x
Launching on node circe-n025
+ srun --mem=0 -N 1 -n 1 -w circe-n021 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=5 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w circe-n022 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=6 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n023
+ pids+=($!)
+ set +x
Launching on node circe-n026
+ pids+=($!)
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ set +x
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n024
Launching on node circe-n027
+ srun --mem=0 -N 1 -n 1 -w circe-n023 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=7 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n025
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ pids+=($!)
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n026
+ srun --mem=0 -N 1 -n 1 -w circe-n024 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=8 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ set +x
Launching on node circe-n028
+ srun --mem=0 -N 1 -n 1 -w circe-n025 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=9 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
+ srun --mem=0 -N 1 -n 1 -w circe-n026 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=10 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
Launching on node circe-n029
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n027
+ pids+=($!)
+ set +x
Launching on node circe-n030
+ srun --mem=0 -N 1 -n 1 -w circe-n027 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=11 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n028
+ pids+=($!)
+ set +x
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n029
+ srun --mem=0 -N 1 -n 1 -w circe-n028 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=12 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n030
+ srun --mem=0 -N 1 -n 1 -w circe-n029 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=13 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w circe-n030 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=14 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=0 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=9 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=3 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=6 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=11 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=10 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=13 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=8 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=1 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=7 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=2 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=5 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=12 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=4 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=14 --master_addr=10.0.1.16 --master_port=5204
STARTING TIMING RUN AT 2019-05-23 07:14:35 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=0 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:14:35 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=9 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:14:35 PM
STARTING TIMING RUN AT 2019-05-23 07:14:35 PM
running benchmark
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
STARTING TIMING RUN AT 2019-05-23 07:14:35 PM
+ export TORCH_MODEL_ZOO=/data/torchvision
running benchmark
+ TORCH_MODEL_ZOO=/data/torchvision
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=11 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ export TORCH_MODEL_ZOO=/data/torchvision
+ NUMEPOCHS=80
+ TORCH_MODEL_ZOO=/data/torchvision
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=3 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=6 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:14:35 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=5 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:14:35 PM
STARTING TIMING RUN AT 2019-05-23 07:14:35 PM
running benchmark
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ NUMEPOCHS=80
+ DATASET_DIR=/data/coco2017
+ echo 'running benchmark'
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ export DATASET_DIR=/data/coco2017
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=1 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=10 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:14:35 PM
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
STARTING TIMING RUN AT 2019-05-23 07:14:35 PM
+ DATASET_DIR=/data/coco2017
running benchmark
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=2 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:14:35 PM
STARTING TIMING RUN AT 2019-05-23 07:14:35 PM
running benchmark
running benchmark
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ NUMEPOCHS=80
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=8 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=7 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=13 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:14:35 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=12 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
STARTING TIMING RUN AT 2019-05-23 07:14:35 PM
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=4 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:14:35 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=14 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding::::MLL 1558638885.553 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.553 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.554 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.554 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.554 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.554 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638885.555 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638885.556 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.557 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.557 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.557 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.557 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.558 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.558 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.558 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638885.559 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638885.557 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.557 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.557 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.558 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.558 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.558 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.559 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.559 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.559 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.559 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638885.559 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.559 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.559 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.559 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.560 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638885.560 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638885.586 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.586 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.586 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.587 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.587 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.588 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.588 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638885.589 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638885.590 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.590 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.590 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.591 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.591 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.591 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.591 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.591 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638885.598 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.601 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.602 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.602 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.602 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.603 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.603 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.603 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.603 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.603 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.603 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.604 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638885.604 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638885.605 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.605 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.605 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638885.663 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.663 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.663 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.663 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.664 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638885.665 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.666 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.666 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.666 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.667 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.667 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.667 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.668 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638885.668 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.668 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.668 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638885.665 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.665 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.665 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.667 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.667 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638885.667 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.667 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.667 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.667 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.667 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.668 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.668 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.668 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.668 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.668 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638885.669 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638885.667 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.667 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.667 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.669 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638885.669 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.670 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.670 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.671 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.671 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.671 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.671 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.672 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.672 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.672 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.672 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.673 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638885.705 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.707 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.709 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.710 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.711 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.711 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.712 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.712 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638885.712 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.713 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.713 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.713 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638885.714 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.714 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.714 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638885.716 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
Binding::::MLL 1558638885.736 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
Binding::::MLL 1558638885.736 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.736 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.740 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.736 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.737 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.737 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.738 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.738 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.741 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.741 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.741 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.738 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.738 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.738 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.738 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.739 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.739 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.739 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.743 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638885.743 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.739 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.740 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638885.743 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.744 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.744 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638885.744 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.744 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.744 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638885.744 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638885.745 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638885.745 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638885.760 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.760 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.761 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.762 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.762 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.762 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638885.762 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.763 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.763 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.763 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.764 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.764 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638885.764 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638885.764 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.764 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.765 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638885.776 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.778 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.778 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.778 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.778 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.778 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.778 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.779 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
Binding::::MLL 1558638885.776 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.777 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.777 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.777 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.779 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.779 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.779 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.778 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.780 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.780 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.778 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.778 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638885.778 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.778 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.780 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638885.781 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.781 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638885.779 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.779 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.779 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638885.780 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558638885.780 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.780 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638885.780 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638885.763 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.763 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.763 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.763 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.763 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.764 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558638885.765 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.765 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.766 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.766 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.766 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.766 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.767 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.767 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.767 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.767 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558638885.769 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.769 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.770 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.771 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.771 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.772 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.772 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.772 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.772 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.772 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.772 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.773 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.773 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.773 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558638885.773 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558638885.773 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
0 Using seed = 1670743875
1 Using seed = 1670743876
:::MLL 1558638919.128 max_samples: {"value": 1, "metadata": {"file": "utils.py", "lineno": 465}}
3 Using seed = 1670743878
4 Using seed = 1670743879
5 Using seed = 1670743880
2 Using seed = 1670743877
9 Using seed = 1670743884
11 Using seed = 1670743886
8 Using seed = 1670743883
7 Using seed = 1670743882
6 Using seed = 1670743881
25 Using seed = 1670743900
21 Using seed = 1670743896
29 Using seed = 1670743904
26 Using seed = 1670743901
22 Using seed = 1670743897
28 Using seed = 1670743903
18 Using seed = 1670743893
19 Using seed = 1670743894
27 Using seed = 1670743902
23 Using seed = 1670743898
16 Using seed = 1670743891
31 Using seed = 1670743906
20 Using seed = 1670743895
30 Using seed = 1670743905
17 Using seed = 1670743892
24 Using seed = 1670743899
42 Using seed = 1670743917
45 Using seed = 1670743920
32 Using seed = 1670743907
37 Using seed = 1670743912
44 Using seed = 1670743919
46 Using seed = 1670743921
35 Using seed = 1670743910
36 Using seed = 1670743911
47 Using seed = 1670743922
33 Using seed = 1670743908
34 Using seed = 1670743909
41 Using seed = 1670743916
43 Using seed = 1670743918
39 Using seed = 1670743914
40 Using seed = 1670743915
38 Using seed = 1670743913
51 Using seed = 1670743926
50 Using seed = 1670743925
53 Using seed = 1670743928
49 Using seed = 1670743924
52 Using seed = 1670743927
48 Using seed = 1670743923
58 Using seed = 1670743933
63 Using seed = 1670743938
60 Using seed = 1670743935
61 Using seed = 1670743936
62 Using seed = 1670743937
55 Using seed = 1670743930
56 Using seed = 1670743931
59 Using seed = 1670743934
57 Using seed = 1670743932
54 Using seed = 1670743929
74 Using seed = 1670743949
77 Using seed = 1670743952
78 Using seed = 1670743953
79 Using seed = 1670743954
65 Using seed = 1670743940
66 Using seed = 1670743941
68 Using seed = 1670743943
67 Using seed = 1670743942
64 Using seed = 1670743939
69 Using seed = 1670743944
76 Using seed = 1670743951
75 Using seed = 1670743950
73 Using seed = 1670743948
72 Using seed = 1670743947
71 Using seed = 1670743946
70 Using seed = 1670743945
82 Using seed = 1670743957
83 Using seed = 1670743958
81 Using seed = 1670743956
86 Using seed = 1670743961
84 Using seed = 1670743959
80 Using seed = 1670743955
95 Using seed = 1670743970
94 Using seed = 1670743969
90 Using seed = 1670743965
92 Using seed = 1670743967
87 Using seed = 1670743962
88 Using seed = 1670743963
89 Using seed = 1670743964
85 Using seed = 1670743960
91 Using seed = 1670743966
93 Using seed = 1670743968
109 Using seed = 1670743984
108 Using seed = 1670743983
110 Using seed = 1670743985
111 Using seed = 1670743986
105 Using seed = 1670743980
106 Using seed = 1670743981
102 Using seed = 1670743977
104 Using seed = 1670743979
98 Using seed = 1670743973
97 Using seed = 1670743972
107 Using seed = 1670743982
99 Using seed = 1670743974
96 Using seed = 1670743971
100 Using seed = 1670743975
103 Using seed = 1670743978
101 Using seed = 1670743976
118 Using seed = 1670743993
126 Using seed = 1670744001
113 Using seed = 1670743988
127 Using seed = 1670744002
122 Using seed = 1670743997
121 Using seed = 1670743996
125 Using seed = 1670744000
115 Using seed = 1670743990
124 Using seed = 1670743999
116 Using seed = 1670743991
123 Using seed = 1670743998
114 Using seed = 1670743989
119 Using seed = 1670743994
120 Using seed = 1670743995
117 Using seed = 1670743992
112 Using seed = 1670743987
129 Using seed = 1670744004
136 Using seed = 1670744011
134 Using seed = 1670744009
138 Using seed = 1670744013
128 Using seed = 1670744003
142 Using seed = 1670744017
140 Using seed = 1670744015
139 Using seed = 1670744014
143 Using seed = 1670744018
131 Using seed = 1670744006
135 Using seed = 1670744010
137 Using seed = 1670744012
130 Using seed = 1670744005
132 Using seed = 1670744007
141 Using seed = 1670744016
133 Using seed = 1670744008
155 Using seed = 1670744030
153 Using seed = 1670744028
154 Using seed = 1670744029
146 Using seed = 1670744021
147 Using seed = 1670744022
157 Using seed = 1670744032
145 Using seed = 1670744020
158 Using seed = 1670744033
156 Using seed = 1670744031
159 Using seed = 1670744034
150 Using seed = 1670744025
148 Using seed = 1670744023
152 Using seed = 1670744027
149 Using seed = 1670744024
144 Using seed = 1670744019
151 Using seed = 1670744026
162 Using seed = 1670744037
164 Using seed = 1670744039
169 Using seed = 1670744044
161 Using seed = 1670744036
170 Using seed = 1670744045
174 Using seed = 1670744049
171 Using seed = 1670744046
167 Using seed = 1670744042
172 Using seed = 1670744047
168 Using seed = 1670744043
175 Using seed = 1670744050
173 Using seed = 1670744048
166 Using seed = 1670744041
160 Using seed = 1670744035
165 Using seed = 1670744040
163 Using seed = 1670744038
179 Using seed = 1670744054
176 Using seed = 1670744051
190 Using seed = 1670744065
182 Using seed = 1670744057
178 Using seed = 1670744053
186 Using seed = 1670744061
177 Using seed = 1670744052
189 Using seed = 1670744064
188 Using seed = 1670744063
185 Using seed = 1670744060
180 Using seed = 1670744055
187 Using seed = 1670744062
184 Using seed = 1670744059
191 Using seed = 1670744066
181 Using seed = 1670744056
183 Using seed = 1670744058
205 Using seed = 1670744080
198 Using seed = 1670744073
206 Using seed = 1670744081
192 Using seed = 1670744067
196 Using seed = 1670744071
202 Using seed = 1670744077
194 Using seed = 1670744069
204 Using seed = 1670744079
193 Using seed = 1670744068
203 Using seed = 1670744078
207 Using seed = 1670744082
195 Using seed = 1670744070
197 Using seed = 1670744072
201 Using seed = 1670744076
200 Using seed = 1670744075
199 Using seed = 1670744074
219 Using seed = 1670744094
211 Using seed = 1670744086
222 Using seed = 1670744097
221 Using seed = 1670744096
220 Using seed = 1670744095
218 Using seed = 1670744093
223 Using seed = 1670744098
208 Using seed = 1670744083
210 Using seed = 1670744085
209 Using seed = 1670744084
212 Using seed = 1670744087
214 Using seed = 1670744089
215 Using seed = 1670744090
213 Using seed = 1670744088
216 Using seed = 1670744091
217 Using seed = 1670744092
227 Using seed = 1670744102
238 Using seed = 1670744113
233 Using seed = 1670744108
237 Using seed = 1670744112
234 Using seed = 1670744109
230 Using seed = 1670744105
236 Using seed = 1670744111
226 Using seed = 1670744101
224 Using seed = 1670744099
225 Using seed = 1670744100
228 Using seed = 1670744103
239 Using seed = 1670744114
235 Using seed = 1670744110
231 Using seed = 1670744106
232 Using seed = 1670744107
229 Using seed = 1670744104
12 Using seed = 1670743887
13 Using seed = 1670743888
14 Using seed = 1670743889
10 Using seed = 1670743885
15 Using seed = 1670743890
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
:::MLL 1558638924.193 model_bn_span: {"value": 28, "metadata": {"file": "train.py", "lineno": 480}}
:::MLL 1558638924.193 global_batch_size: {"value": 1680, "metadata": {"file": "train.py", "lineno": 481}}
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
:::MLL 1558638924.230 opt_base_learning_rate: {"value": 0.1625, "metadata": {"file": "train.py", "lineno": 511}}
:::MLL 1558638924.230 opt_weight_decay: {"value": 0.0002, "metadata": {"file": "train.py", "lineno": 513}}
:::MLL 1558638924.230 opt_learning_rate_warmup_steps: {"value": 1250, "metadata": {"file": "train.py", "lineno": 516}}
:::MLL 1558638924.230 opt_learning_rate_warmup_factor: {"value": 0, "metadata": {"file": "train.py", "lineno": 518}}
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
:::MLL 1558638931.375 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 604}}
:::MLL 1558638931.375 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 610}}
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
time_check a: 1558638933.074806690
time_check a: 1558638933.088802814
time_check a: 1558638933.074030876
time_check a: 1558638933.077226162
time_check a: 1558638933.096714020
time_check a: 1558638933.086256027
time_check a: 1558638933.090070248
time_check a: 1558638933.084456444
time_check a: 1558638933.083582878
time_check a: 1558638933.098198175
time_check a: 1558638933.091386795
time_check a: 1558638933.070427418
time_check a: 1558638933.078626633
time_check a: 1558638933.099239826
time_check a: 1558638933.173763990
time_check b: 1558638939.521898746
time_check b: 1558638939.628139019
time_check b: 1558638939.881364107
time_check b: 1558638939.884678125
time_check b: 1558638939.916849136
time_check b: 1558638939.937528610
time_check b: 1558638939.960351229
time_check b: 1558638939.953493595
time_check b: 1558638939.970991611
time_check b: 1558638939.966277122
time_check b: 1558638939.998761654
time_check b: 1558638940.018430948
time_check b: 1558638940.050539732
time_check b: 1558638940.062476397
time_check b: 1558638940.074515343
:::MLL 1558638941.192 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 32.74606450292497, "file": "train.py", "lineno": 669}}
:::MLL 1558638941.193 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 673}}
Iteration:      0, Loss function: 22.429, Average Loss: 0.022, avg. samples / sec: 117.52
Iteration:      0, Loss function: 23.175, Average Loss: 0.023, avg. samples / sec: 117.31
Iteration:      0, Loss function: 22.563, Average Loss: 0.023, avg. samples / sec: 118.37
Iteration:      0, Loss function: 22.511, Average Loss: 0.023, avg. samples / sec: 117.18
Iteration:      0, Loss function: 22.131, Average Loss: 0.022, avg. samples / sec: 117.87
Iteration:      0, Loss function: 22.314, Average Loss: 0.022, avg. samples / sec: 117.35
Iteration:      0, Loss function: 22.335, Average Loss: 0.022, avg. samples / sec: 115.55
Iteration:      0, Loss function: 22.394, Average Loss: 0.022, avg. samples / sec: 117.29
Iteration:      0, Loss function: 21.762, Average Loss: 0.022, avg. samples / sec: 111.57
Iteration:      0, Loss function: 22.891, Average Loss: 0.023, avg. samples / sec: 116.95
Iteration:      0, Loss function: 22.455, Average Loss: 0.022, avg. samples / sec: 116.20
Iteration:      0, Loss function: 22.287, Average Loss: 0.022, avg. samples / sec: 114.00
Iteration:      0, Loss function: 21.991, Average Loss: 0.022, avg. samples / sec: 116.43
Iteration:      0, Loss function: 22.190, Average Loss: 0.022, avg. samples / sec: 108.23
Iteration:      0, Loss function: 22.004, Average Loss: 0.022, avg. samples / sec: 111.04
Iteration:     20, Loss function: 20.227, Average Loss: 0.442, avg. samples / sec: 38395.56
Iteration:     20, Loss function: 20.643, Average Loss: 0.445, avg. samples / sec: 37969.77
Iteration:     20, Loss function: 20.356, Average Loss: 0.439, avg. samples / sec: 38755.75
Iteration:     20, Loss function: 20.948, Average Loss: 0.441, avg. samples / sec: 38386.19
Iteration:     20, Loss function: 20.647, Average Loss: 0.441, avg. samples / sec: 38428.25
Iteration:     20, Loss function: 21.428, Average Loss: 0.442, avg. samples / sec: 38649.33
Iteration:     20, Loss function: 20.430, Average Loss: 0.439, avg. samples / sec: 38044.53
Iteration:     20, Loss function: 20.685, Average Loss: 0.441, avg. samples / sec: 38480.15
Iteration:     20, Loss function: 20.663, Average Loss: 0.441, avg. samples / sec: 38104.87
Iteration:     20, Loss function: 22.192, Average Loss: 0.445, avg. samples / sec: 38357.86
Iteration:     20, Loss function: 20.081, Average Loss: 0.441, avg. samples / sec: 37841.87
Iteration:     20, Loss function: 20.954, Average Loss: 0.443, avg. samples / sec: 37332.77
Iteration:     20, Loss function: 21.211, Average Loss: 0.440, avg. samples / sec: 38258.38
Iteration:     20, Loss function: 21.863, Average Loss: 0.444, avg. samples / sec: 37566.19
Iteration:     20, Loss function: 20.302, Average Loss: 0.442, avg. samples / sec: 37636.88
Iteration:     40, Loss function: 17.057, Average Loss: 0.828, avg. samples / sec: 56280.79
Iteration:     40, Loss function: 17.221, Average Loss: 0.830, avg. samples / sec: 55587.62
Iteration:     40, Loss function: 17.641, Average Loss: 0.829, avg. samples / sec: 56032.16
Iteration:     40, Loss function: 18.892, Average Loss: 0.830, avg. samples / sec: 56165.01
Iteration:     40, Loss function: 17.353, Average Loss: 0.826, avg. samples / sec: 57031.94
Iteration:     40, Loss function: 17.663, Average Loss: 0.824, avg. samples / sec: 55780.90
Iteration:     40, Loss function: 17.184, Average Loss: 0.828, avg. samples / sec: 55654.33
Iteration:     40, Loss function: 19.475, Average Loss: 0.833, avg. samples / sec: 55385.23
Iteration:     40, Loss function: 17.787, Average Loss: 0.828, avg. samples / sec: 55151.32
Iteration:     40, Loss function: 18.167, Average Loss: 0.826, avg. samples / sec: 56475.59
Iteration:     40, Loss function: 16.922, Average Loss: 0.827, avg. samples / sec: 56645.38
Iteration:     40, Loss function: 16.900, Average Loss: 0.823, avg. samples / sec: 56031.83
Iteration:     40, Loss function: 17.795, Average Loss: 0.822, avg. samples / sec: 54960.52
Iteration:     40, Loss function: 17.563, Average Loss: 0.828, avg. samples / sec: 55173.82
Iteration:     40, Loss function: 17.266, Average Loss: 0.823, avg. samples / sec: 55061.60
Iteration:     60, Loss function: 14.072, Average Loss: 1.088, avg. samples / sec: 57007.97
Iteration:     60, Loss function: 13.102, Average Loss: 1.096, avg. samples / sec: 56764.69
Iteration:     60, Loss function: 12.265, Average Loss: 1.096, avg. samples / sec: 56826.10
Iteration:     60, Loss function: 12.968, Average Loss: 1.097, avg. samples / sec: 57456.57
Iteration:     60, Loss function: 11.335, Average Loss: 1.099, avg. samples / sec: 56435.62
Iteration:     60, Loss function: 13.122, Average Loss: 1.089, avg. samples / sec: 56853.89
Iteration:     60, Loss function: 13.618, Average Loss: 1.099, avg. samples / sec: 56471.11
Iteration:     60, Loss function: 10.777, Average Loss: 1.085, avg. samples / sec: 56859.21
Iteration:     60, Loss function: 12.009, Average Loss: 1.093, avg. samples / sec: 56466.31
Iteration:     60, Loss function: 11.500, Average Loss: 1.093, avg. samples / sec: 56392.97
Iteration:     60, Loss function: 10.342, Average Loss: 1.084, avg. samples / sec: 57470.44
Iteration:     60, Loss function: 11.192, Average Loss: 1.089, avg. samples / sec: 56822.75
Iteration:     60, Loss function: 12.592, Average Loss: 1.089, avg. samples / sec: 56271.10
Iteration:     60, Loss function: 11.727, Average Loss: 1.092, avg. samples / sec: 55740.31
Iteration:     60, Loss function: 10.831, Average Loss: 1.087, avg. samples / sec: 55993.00
:::MLL 1558638944.309 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 819}}
:::MLL 1558638944.310 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 673}}
Iteration:     80, Loss function: 10.940, Average Loss: 1.307, avg. samples / sec: 58659.45
Iteration:     80, Loss function: 11.306, Average Loss: 1.308, avg. samples / sec: 58811.14
Iteration:     80, Loss function: 10.489, Average Loss: 1.309, avg. samples / sec: 58154.39
Iteration:     80, Loss function: 10.599, Average Loss: 1.306, avg. samples / sec: 58183.61
Iteration:     80, Loss function: 10.386, Average Loss: 1.301, avg. samples / sec: 58107.42
Iteration:     80, Loss function: 10.556, Average Loss: 1.317, avg. samples / sec: 57929.61
Iteration:     80, Loss function: 11.078, Average Loss: 1.316, avg. samples / sec: 57952.93
Iteration:     80, Loss function: 10.281, Average Loss: 1.308, avg. samples / sec: 57992.24
Iteration:     80, Loss function: 11.002, Average Loss: 1.316, avg. samples / sec: 57735.55
Iteration:     80, Loss function: 10.910, Average Loss: 1.328, avg. samples / sec: 57824.96
Iteration:     80, Loss function: 10.656, Average Loss: 1.319, avg. samples / sec: 57714.96
Iteration:     80, Loss function: 9.793, Average Loss: 1.310, avg. samples / sec: 58042.44
Iteration:     80, Loss function: 10.349, Average Loss: 1.311, avg. samples / sec: 57872.35
Iteration:     80, Loss function: 10.771, Average Loss: 1.299, avg. samples / sec: 57293.74
Iteration:     80, Loss function: 10.294, Average Loss: 1.310, avg. samples / sec: 58242.57
Iteration:    100, Loss function: 9.376, Average Loss: 1.487, avg. samples / sec: 58506.08
Iteration:    100, Loss function: 8.760, Average Loss: 1.487, avg. samples / sec: 58306.72
Iteration:    100, Loss function: 9.711, Average Loss: 1.469, avg. samples / sec: 58648.88
Iteration:    100, Loss function: 9.344, Average Loss: 1.480, avg. samples / sec: 57977.90
Iteration:    100, Loss function: 9.521, Average Loss: 1.502, avg. samples / sec: 58199.23
Iteration:    100, Loss function: 9.223, Average Loss: 1.480, avg. samples / sec: 58655.56
Iteration:    100, Loss function: 10.078, Average Loss: 1.476, avg. samples / sec: 57882.43
Iteration:    100, Loss function: 9.408, Average Loss: 1.479, avg. samples / sec: 57884.36
Iteration:    100, Loss function: 10.031, Average Loss: 1.483, avg. samples / sec: 58236.14
Iteration:    100, Loss function: 9.491, Average Loss: 1.478, avg. samples / sec: 57916.09
Iteration:    100, Loss function: 9.288, Average Loss: 1.488, avg. samples / sec: 57992.43
Iteration:    100, Loss function: 8.941, Average Loss: 1.473, avg. samples / sec: 57862.35
Iteration:    100, Loss function: 10.362, Average Loss: 1.494, avg. samples / sec: 57958.61
Iteration:    100, Loss function: 10.049, Average Loss: 1.482, avg. samples / sec: 57884.19
Iteration:    100, Loss function: 9.191, Average Loss: 1.490, avg. samples / sec: 57691.52
Iteration:    120, Loss function: 8.648, Average Loss: 1.625, avg. samples / sec: 57511.32
Iteration:    120, Loss function: 9.813, Average Loss: 1.640, avg. samples / sec: 57129.11
Iteration:    120, Loss function: 9.917, Average Loss: 1.634, avg. samples / sec: 57255.70
Iteration:    120, Loss function: 9.254, Average Loss: 1.634, avg. samples / sec: 57218.67
Iteration:    120, Loss function: 9.420, Average Loss: 1.631, avg. samples / sec: 57133.03
Iteration:    120, Loss function: 8.673, Average Loss: 1.636, avg. samples / sec: 57356.02
Iteration:    120, Loss function: 8.623, Average Loss: 1.648, avg. samples / sec: 57230.15
Iteration:    120, Loss function: 8.862, Average Loss: 1.629, avg. samples / sec: 57038.27
Iteration:    120, Loss function: 9.618, Average Loss: 1.637, avg. samples / sec: 56916.39
Iteration:    120, Loss function: 8.304, Average Loss: 1.657, avg. samples / sec: 56961.52
Iteration:    120, Loss function: 9.566, Average Loss: 1.632, avg. samples / sec: 57015.86
Iteration:    120, Loss function: 9.312, Average Loss: 1.648, avg. samples / sec: 57336.74
Iteration:    120, Loss function: 9.107, Average Loss: 1.641, avg. samples / sec: 56912.34
Iteration:    120, Loss function: 8.664, Average Loss: 1.634, avg. samples / sec: 56563.79
Iteration:    120, Loss function: 9.416, Average Loss: 1.629, avg. samples / sec: 56686.99
:::MLL 1558638946.349 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 819}}
:::MLL 1558638946.350 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 673}}
Iteration:    140, Loss function: 7.317, Average Loss: 1.786, avg. samples / sec: 57600.01
Iteration:    140, Loss function: 8.805, Average Loss: 1.793, avg. samples / sec: 57750.39
Iteration:    140, Loss function: 8.890, Average Loss: 1.773, avg. samples / sec: 57700.80
Iteration:    140, Loss function: 8.617, Average Loss: 1.778, avg. samples / sec: 57572.99
Iteration:    140, Loss function: 8.949, Average Loss: 1.778, avg. samples / sec: 57686.32
Iteration:    140, Loss function: 9.233, Average Loss: 1.785, avg. samples / sec: 57950.77
Iteration:    140, Loss function: 8.517, Average Loss: 1.788, avg. samples / sec: 57929.99
Iteration:    140, Loss function: 8.661, Average Loss: 1.771, avg. samples / sec: 57681.22
Iteration:    140, Loss function: 8.218, Average Loss: 1.800, avg. samples / sec: 57709.71
Iteration:    140, Loss function: 9.435, Average Loss: 1.779, avg. samples / sec: 57698.56
Iteration:    140, Loss function: 10.114, Average Loss: 1.769, avg. samples / sec: 57264.96
Iteration:    140, Loss function: 9.557, Average Loss: 1.777, avg. samples / sec: 57622.95
Iteration:    140, Loss function: 8.315, Average Loss: 1.776, avg. samples / sec: 57742.32
Iteration:    140, Loss function: 8.991, Average Loss: 1.776, avg. samples / sec: 57219.37
Iteration:    140, Loss function: 8.865, Average Loss: 1.775, avg. samples / sec: 57592.45
Iteration:    160, Loss function: 8.496, Average Loss: 1.909, avg. samples / sec: 55794.44
Iteration:    160, Loss function: 8.483, Average Loss: 1.933, avg. samples / sec: 55754.00
Iteration:    160, Loss function: 7.587, Average Loss: 1.907, avg. samples / sec: 56088.54
Iteration:    160, Loss function: 8.350, Average Loss: 1.925, avg. samples / sec: 55657.98
Iteration:    160, Loss function: 8.126, Average Loss: 1.915, avg. samples / sec: 55619.36
Iteration:    160, Loss function: 8.926, Average Loss: 1.912, avg. samples / sec: 55639.23
Iteration:    160, Loss function: 7.865, Average Loss: 1.928, avg. samples / sec: 55555.89
Iteration:    160, Loss function: 7.648, Average Loss: 1.922, avg. samples / sec: 55480.52
Iteration:    160, Loss function: 8.432, Average Loss: 1.912, avg. samples / sec: 55515.42
Iteration:    160, Loss function: 8.258, Average Loss: 1.937, avg. samples / sec: 55620.19
Iteration:    160, Loss function: 9.178, Average Loss: 1.912, avg. samples / sec: 55578.30
Iteration:    160, Loss function: 8.344, Average Loss: 1.911, avg. samples / sec: 55573.48
Iteration:    160, Loss function: 9.525, Average Loss: 1.907, avg. samples / sec: 55444.24
Iteration:    160, Loss function: 7.494, Average Loss: 1.912, avg. samples / sec: 55577.58
Iteration:    160, Loss function: 8.662, Average Loss: 1.911, avg. samples / sec: 55567.08
Iteration:    180, Loss function: 8.009, Average Loss: 2.037, avg. samples / sec: 59536.40
Iteration:    180, Loss function: 8.367, Average Loss: 2.040, avg. samples / sec: 59932.64
Iteration:    180, Loss function: 8.698, Average Loss: 2.035, avg. samples / sec: 59485.31
Iteration:    180, Loss function: 8.135, Average Loss: 2.052, avg. samples / sec: 59536.80
Iteration:    180, Loss function: 8.079, Average Loss: 2.051, avg. samples / sec: 59654.97
Iteration:    180, Loss function: 8.467, Average Loss: 2.039, avg. samples / sec: 59932.49
Iteration:    180, Loss function: 8.037, Average Loss: 2.057, avg. samples / sec: 59579.87
Iteration:    180, Loss function: 8.426, Average Loss: 2.041, avg. samples / sec: 59574.20
Iteration:    180, Loss function: 8.816, Average Loss: 2.037, avg. samples / sec: 59758.92
Iteration:    180, Loss function: 9.045, Average Loss: 2.065, avg. samples / sec: 59534.62
Iteration:    180, Loss function: 8.027, Average Loss: 2.040, avg. samples / sec: 59506.03
Iteration:    180, Loss function: 8.885, Average Loss: 2.040, avg. samples / sec: 59729.43
Iteration:    180, Loss function: 8.109, Average Loss: 2.046, avg. samples / sec: 59296.32
Iteration:    180, Loss function: 9.100, Average Loss: 2.038, avg. samples / sec: 59504.70
Iteration:    180, Loss function: 9.233, Average Loss: 2.066, avg. samples / sec: 59101.73
Iteration:    200, Loss function: 8.084, Average Loss: 2.165, avg. samples / sec: 58607.63
Iteration:    200, Loss function: 8.343, Average Loss: 2.165, avg. samples / sec: 58317.33
Iteration:    200, Loss function: 8.266, Average Loss: 2.168, avg. samples / sec: 58719.47
Iteration:    200, Loss function: 7.658, Average Loss: 2.193, avg. samples / sec: 58603.03
Iteration:    200, Loss function: 8.420, Average Loss: 2.177, avg. samples / sec: 58430.44
Iteration:    200, Loss function: 8.929, Average Loss: 2.170, avg. samples / sec: 58575.38
Iteration:    200, Loss function: 7.458, Average Loss: 2.162, avg. samples / sec: 58465.66
Iteration:    200, Loss function: 8.593, Average Loss: 2.180, avg. samples / sec: 58382.46
Iteration:    200, Loss function: 8.567, Average Loss: 2.168, avg. samples / sec: 58419.98
Iteration:    200, Loss function: 8.696, Average Loss: 2.185, avg. samples / sec: 58423.80
Iteration:    200, Loss function: 9.132, Average Loss: 2.164, avg. samples / sec: 58178.11
Iteration:    200, Loss function: 8.080, Average Loss: 2.163, avg. samples / sec: 58404.43
Iteration:    200, Loss function: 8.703, Average Loss: 2.170, avg. samples / sec: 58444.67
Iteration:    200, Loss function: 8.775, Average Loss: 2.194, avg. samples / sec: 58425.35
Iteration:    200, Loss function: 7.665, Average Loss: 2.159, avg. samples / sec: 58122.59
:::MLL 1558638948.374 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 819}}
:::MLL 1558638948.375 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 673}}
Iteration:    220, Loss function: 7.789, Average Loss: 2.301, avg. samples / sec: 59559.95
Iteration:    220, Loss function: 7.703, Average Loss: 2.299, avg. samples / sec: 59573.57
Iteration:    220, Loss function: 7.748, Average Loss: 2.291, avg. samples / sec: 59464.75
Iteration:    220, Loss function: 7.666, Average Loss: 2.285, avg. samples / sec: 59360.46
Iteration:    220, Loss function: 8.657, Average Loss: 2.297, avg. samples / sec: 59430.80
Iteration:    220, Loss function: 8.270, Average Loss: 2.318, avg. samples / sec: 59678.22
Iteration:    220, Loss function: 7.804, Average Loss: 2.281, avg. samples / sec: 59571.94
Iteration:    220, Loss function: 9.027, Average Loss: 2.282, avg. samples / sec: 59359.78
Iteration:    220, Loss function: 8.624, Average Loss: 2.285, avg. samples / sec: 59425.89
Iteration:    220, Loss function: 8.298, Average Loss: 2.283, avg. samples / sec: 59210.87
Iteration:    220, Loss function: 7.630, Average Loss: 2.291, avg. samples / sec: 59454.77
Iteration:    220, Loss function: 7.997, Average Loss: 2.282, avg. samples / sec: 59365.83
Iteration:    220, Loss function: 8.442, Average Loss: 2.286, avg. samples / sec: 59112.24
Iteration:    220, Loss function: 8.336, Average Loss: 2.278, avg. samples / sec: 59446.44
Iteration:    220, Loss function: 7.916, Average Loss: 2.313, avg. samples / sec: 58633.45
Iteration:    240, Loss function: 8.630, Average Loss: 2.411, avg. samples / sec: 57491.50
Iteration:    240, Loss function: 7.834, Average Loss: 2.421, avg. samples / sec: 58193.10
Iteration:    240, Loss function: 8.240, Average Loss: 2.396, avg. samples / sec: 57532.71
Iteration:    240, Loss function: 8.516, Average Loss: 2.394, avg. samples / sec: 57683.96
Iteration:    240, Loss function: 8.369, Average Loss: 2.402, avg. samples / sec: 57356.53
Iteration:    240, Loss function: 7.692, Average Loss: 2.404, avg. samples / sec: 57546.62
Iteration:    240, Loss function: 8.359, Average Loss: 2.408, avg. samples / sec: 57279.28
Iteration:    240, Loss function: 8.667, Average Loss: 2.394, avg. samples / sec: 57426.77
Iteration:    240, Loss function: 7.604, Average Loss: 2.393, avg. samples / sec: 57280.60
Iteration:    240, Loss function: 9.017, Average Loss: 2.404, avg. samples / sec: 57513.32
Iteration:    240, Loss function: 7.574, Average Loss: 2.398, avg. samples / sec: 57195.98
Iteration:    240, Loss function: 7.362, Average Loss: 2.400, avg. samples / sec: 57291.52
Iteration:    240, Loss function: 7.324, Average Loss: 2.428, avg. samples / sec: 57203.78
Iteration:    240, Loss function: 8.544, Average Loss: 2.400, avg. samples / sec: 57358.96
Iteration:    240, Loss function: 8.616, Average Loss: 2.413, avg. samples / sec: 57001.24
Iteration:    260, Loss function: 7.244, Average Loss: 2.538, avg. samples / sec: 57997.80
Iteration:    260, Loss function: 8.064, Average Loss: 2.511, avg. samples / sec: 57678.98
Iteration:    260, Loss function: 7.105, Average Loss: 2.503, avg. samples / sec: 57877.32
Iteration:    260, Loss function: 8.222, Average Loss: 2.507, avg. samples / sec: 57813.19
Iteration:    260, Loss function: 7.834, Average Loss: 2.516, avg. samples / sec: 57699.10
Iteration:    260, Loss function: 6.843, Average Loss: 2.517, avg. samples / sec: 57705.20
Iteration:    260, Loss function: 7.215, Average Loss: 2.509, avg. samples / sec: 57814.33
Iteration:    260, Loss function: 8.861, Average Loss: 2.506, avg. samples / sec: 57726.80
Iteration:    260, Loss function: 7.851, Average Loss: 2.512, avg. samples / sec: 57665.88
Iteration:    260, Loss function: 8.368, Average Loss: 2.533, avg. samples / sec: 57389.77
Iteration:    260, Loss function: 7.304, Average Loss: 2.513, avg. samples / sec: 57472.69
Iteration:    260, Loss function: 8.869, Average Loss: 2.507, avg. samples / sec: 57433.74
Iteration:    260, Loss function: 8.254, Average Loss: 2.524, avg. samples / sec: 57686.18
Iteration:    260, Loss function: 9.037, Average Loss: 2.522, avg. samples / sec: 57235.03
Iteration:    260, Loss function: 7.130, Average Loss: 2.510, avg. samples / sec: 57502.36
:::MLL 1558638950.390 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 819}}
:::MLL 1558638950.391 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 673}}
Iteration:    280, Loss function: 7.365, Average Loss: 2.611, avg. samples / sec: 60198.97
Iteration:    280, Loss function: 8.077, Average Loss: 2.640, avg. samples / sec: 59597.51
Iteration:    280, Loss function: 7.612, Average Loss: 2.610, avg. samples / sec: 59860.73
Iteration:    280, Loss function: 8.036, Average Loss: 2.637, avg. samples / sec: 59862.99
Iteration:    280, Loss function: 8.194, Average Loss: 2.601, avg. samples / sec: 59569.64
Iteration:    280, Loss function: 7.700, Average Loss: 2.609, avg. samples / sec: 59665.63
Iteration:    280, Loss function: 8.016, Average Loss: 2.616, avg. samples / sec: 59629.96
Iteration:    280, Loss function: 7.964, Average Loss: 2.623, avg. samples / sec: 59978.76
Iteration:    280, Loss function: 8.406, Average Loss: 2.611, avg. samples / sec: 59583.22
Iteration:    280, Loss function: 7.292, Average Loss: 2.609, avg. samples / sec: 59720.27
Iteration:    280, Loss function: 6.415, Average Loss: 2.607, avg. samples / sec: 59816.72
Iteration:    280, Loss function: 8.163, Average Loss: 2.618, avg. samples / sec: 59563.95
Iteration:    280, Loss function: 6.353, Average Loss: 2.611, avg. samples / sec: 59637.33
Iteration:    280, Loss function: 7.465, Average Loss: 2.611, avg. samples / sec: 59264.58
Iteration:    280, Loss function: 7.367, Average Loss: 2.630, avg. samples / sec: 59349.06
Iteration:    300, Loss function: 7.893, Average Loss: 2.707, avg. samples / sec: 57921.57
Iteration:    300, Loss function: 7.767, Average Loss: 2.698, avg. samples / sec: 57964.04
Iteration:    300, Loss function: 7.858, Average Loss: 2.737, avg. samples / sec: 57913.62
Iteration:    300, Loss function: 7.116, Average Loss: 2.724, avg. samples / sec: 58445.22
Iteration:    300, Loss function: 7.335, Average Loss: 2.709, avg. samples / sec: 57779.63
Iteration:    300, Loss function: 8.730, Average Loss: 2.708, avg. samples / sec: 57864.42
Iteration:    300, Loss function: 7.338, Average Loss: 2.717, avg. samples / sec: 57808.09
Iteration:    300, Loss function: 8.342, Average Loss: 2.718, avg. samples / sec: 57848.46
Iteration:    300, Loss function: 6.055, Average Loss: 2.704, avg. samples / sec: 57775.62
Iteration:    300, Loss function: 7.313, Average Loss: 2.707, avg. samples / sec: 58017.78
Iteration:    300, Loss function: 6.866, Average Loss: 2.712, avg. samples / sec: 57937.47
Iteration:    300, Loss function: 7.988, Average Loss: 2.706, avg. samples / sec: 57795.57
Iteration:    300, Loss function: 7.918, Average Loss: 2.713, avg. samples / sec: 57716.33
Iteration:    300, Loss function: 7.411, Average Loss: 2.735, avg. samples / sec: 57606.67
Iteration:    300, Loss function: 7.562, Average Loss: 2.708, avg. samples / sec: 57610.34
Iteration:    320, Loss function: 7.566, Average Loss: 2.830, avg. samples / sec: 57868.79
Iteration:    320, Loss function: 6.583, Average Loss: 2.796, avg. samples / sec: 57806.05
Iteration:    320, Loss function: 7.401, Average Loss: 2.805, avg. samples / sec: 57678.60
Iteration:    320, Loss function: 6.757, Average Loss: 2.804, avg. samples / sec: 57902.46
Iteration:    320, Loss function: 7.545, Average Loss: 2.805, avg. samples / sec: 57909.36
Iteration:    320, Loss function: 6.864, Average Loss: 2.801, avg. samples / sec: 57900.72
Iteration:    320, Loss function: 8.067, Average Loss: 2.812, avg. samples / sec: 57839.86
Iteration:    320, Loss function: 7.786, Average Loss: 2.808, avg. samples / sec: 57766.41
Iteration:    320, Loss function: 7.260, Average Loss: 2.795, avg. samples / sec: 57837.08
Iteration:    320, Loss function: 7.255, Average Loss: 2.828, avg. samples / sec: 57875.63
Iteration:    320, Loss function: 6.452, Average Loss: 2.813, avg. samples / sec: 57765.20
Iteration:    320, Loss function: 7.847, Average Loss: 2.801, avg. samples / sec: 57915.38
Iteration:    320, Loss function: 7.996, Average Loss: 2.801, avg. samples / sec: 57617.86
Iteration:    320, Loss function: 7.521, Average Loss: 2.816, avg. samples / sec: 57551.46
Iteration:    320, Loss function: 7.697, Average Loss: 2.810, avg. samples / sec: 57688.73
Iteration:    340, Loss function: 7.288, Average Loss: 2.887, avg. samples / sec: 58230.97
Iteration:    340, Loss function: 6.849, Average Loss: 2.878, avg. samples / sec: 58260.56
Iteration:    340, Loss function: 8.358, Average Loss: 2.895, avg. samples / sec: 58185.51
Iteration:    340, Loss function: 7.612, Average Loss: 2.917, avg. samples / sec: 58261.25
Iteration:    340, Loss function: 7.001, Average Loss: 2.884, avg. samples / sec: 57997.15
Iteration:    340, Loss function: 6.488, Average Loss: 2.891, avg. samples / sec: 58122.32
Iteration:    340, Loss function: 7.170, Average Loss: 2.893, avg. samples / sec: 58017.26
Iteration:    340, Loss function: 7.112, Average Loss: 2.900, avg. samples / sec: 58143.78
Iteration:    340, Loss function: 7.563, Average Loss: 2.898, avg. samples / sec: 58313.66
Iteration:    340, Loss function: 7.099, Average Loss: 2.886, avg. samples / sec: 58186.61
Iteration:    340, Loss function: 7.959, Average Loss: 2.893, avg. samples / sec: 58058.92
Iteration:    340, Loss function: 7.378, Average Loss: 2.903, avg. samples / sec: 58261.66
Iteration:    340, Loss function: 5.984, Average Loss: 2.892, avg. samples / sec: 58046.77
Iteration:    340, Loss function: 6.894, Average Loss: 2.914, avg. samples / sec: 57857.86
Iteration:    340, Loss function: 8.017, Average Loss: 2.890, avg. samples / sec: 57855.01
:::MLL 1558638952.412 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 819}}
:::MLL 1558638952.412 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 673}}
Iteration:    360, Loss function: 7.338, Average Loss: 2.999, avg. samples / sec: 57216.98
Iteration:    360, Loss function: 6.077, Average Loss: 2.979, avg. samples / sec: 57152.40
Iteration:    360, Loss function: 7.875, Average Loss: 2.977, avg. samples / sec: 57067.35
Iteration:    360, Loss function: 6.898, Average Loss: 2.991, avg. samples / sec: 57160.47
Iteration:    360, Loss function: 7.784, Average Loss: 2.972, avg. samples / sec: 57047.78
Iteration:    360, Loss function: 6.974, Average Loss: 2.970, avg. samples / sec: 57145.35
Iteration:    360, Loss function: 6.518, Average Loss: 2.979, avg. samples / sec: 56978.89
Iteration:    360, Loss function: 7.649, Average Loss: 3.005, avg. samples / sec: 56982.16
Iteration:    360, Loss function: 7.718, Average Loss: 2.982, avg. samples / sec: 57053.23
Iteration:    360, Loss function: 7.656, Average Loss: 2.980, avg. samples / sec: 57111.96
Iteration:    360, Loss function: 7.809, Average Loss: 2.975, avg. samples / sec: 57424.61
Iteration:    360, Loss function: 6.294, Average Loss: 2.977, avg. samples / sec: 57080.68
Iteration:    360, Loss function: 7.052, Average Loss: 2.966, avg. samples / sec: 56882.09
Iteration:    360, Loss function: 4.987, Average Loss: 2.984, avg. samples / sec: 56943.39
Iteration:    360, Loss function: 7.350, Average Loss: 2.965, avg. samples / sec: 56744.71
Iteration:    380, Loss function: 6.233, Average Loss: 3.042, avg. samples / sec: 59516.81
Iteration:    380, Loss function: 7.311, Average Loss: 3.084, avg. samples / sec: 59169.92
Iteration:    380, Loss function: 6.837, Average Loss: 3.061, avg. samples / sec: 59097.05
Iteration:    380, Loss function: 6.041, Average Loss: 3.051, avg. samples / sec: 59122.34
Iteration:    380, Loss function: 6.649, Average Loss: 3.048, avg. samples / sec: 59107.31
Iteration:    380, Loss function: 7.705, Average Loss: 3.055, avg. samples / sec: 59067.57
Iteration:    380, Loss function: 7.341, Average Loss: 3.050, avg. samples / sec: 59140.05
Iteration:    380, Loss function: 6.843, Average Loss: 3.065, avg. samples / sec: 59067.00
Iteration:    380, Loss function: 6.696, Average Loss: 3.065, avg. samples / sec: 59217.63
Iteration:    380, Loss function: 7.398, Average Loss: 3.073, avg. samples / sec: 58999.08
Iteration:    380, Loss function: 8.292, Average Loss: 3.048, avg. samples / sec: 59098.56
Iteration:    380, Loss function: 6.584, Average Loss: 3.064, avg. samples / sec: 59027.74
Iteration:    380, Loss function: 6.437, Average Loss: 3.080, avg. samples / sec: 58944.81
Iteration:    380, Loss function: 7.424, Average Loss: 3.057, avg. samples / sec: 59061.93
Iteration:    380, Loss function: 7.364, Average Loss: 3.061, avg. samples / sec: 59020.08
Iteration:    400, Loss function: 7.651, Average Loss: 3.141, avg. samples / sec: 56136.48
Iteration:    400, Loss function: 6.183, Average Loss: 3.134, avg. samples / sec: 56188.07
Iteration:    400, Loss function: 7.469, Average Loss: 3.131, avg. samples / sec: 56209.18
Iteration:    400, Loss function: 6.134, Average Loss: 3.124, avg. samples / sec: 56084.27
Iteration:    400, Loss function: 7.765, Average Loss: 3.128, avg. samples / sec: 56069.52
Iteration:    400, Loss function: 7.784, Average Loss: 3.149, avg. samples / sec: 56169.26
Iteration:    400, Loss function: 7.350, Average Loss: 3.143, avg. samples / sec: 56138.38
Iteration:    400, Loss function: 6.691, Average Loss: 3.140, avg. samples / sec: 56083.74
Iteration:    400, Loss function: 7.933, Average Loss: 3.119, avg. samples / sec: 55854.15
Iteration:    400, Loss function: 5.885, Average Loss: 3.120, avg. samples / sec: 56086.91
Iteration:    400, Loss function: 6.657, Average Loss: 3.156, avg. samples / sec: 56098.00
Iteration:    400, Loss function: 7.492, Average Loss: 3.129, avg. samples / sec: 56010.91
Iteration:    400, Loss function: 6.400, Average Loss: 3.165, avg. samples / sec: 55955.58
Iteration:    400, Loss function: 6.836, Average Loss: 3.137, avg. samples / sec: 56023.87
Iteration:    400, Loss function: 6.600, Average Loss: 3.123, avg. samples / sec: 55893.93
:::MLL 1558638954.450 epoch_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 819}}
:::MLL 1558638954.450 epoch_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 673}}
Iteration:    420, Loss function: 7.065, Average Loss: 3.203, avg. samples / sec: 59834.02
Iteration:    420, Loss function: 8.179, Average Loss: 3.192, avg. samples / sec: 59917.18
Iteration:    420, Loss function: 6.185, Average Loss: 3.212, avg. samples / sec: 60007.70
Iteration:    420, Loss function: 7.618, Average Loss: 3.210, avg. samples / sec: 59805.07
Iteration:    420, Loss function: 6.001, Average Loss: 3.209, avg. samples / sec: 59657.17
Iteration:    420, Loss function: 7.406, Average Loss: 3.196, avg. samples / sec: 59957.50
Iteration:    420, Loss function: 6.179, Average Loss: 3.226, avg. samples / sec: 59807.91
Iteration:    420, Loss function: 8.188, Average Loss: 3.223, avg. samples / sec: 59696.72
Iteration:    420, Loss function: 7.212, Average Loss: 3.211, avg. samples / sec: 59564.74
Iteration:    420, Loss function: 7.114, Average Loss: 3.218, avg. samples / sec: 59704.16
Iteration:    420, Loss function: 6.980, Average Loss: 3.196, avg. samples / sec: 59716.40
Iteration:    420, Loss function: 5.754, Average Loss: 3.241, avg. samples / sec: 59699.75
Iteration:    420, Loss function: 6.978, Average Loss: 3.203, avg. samples / sec: 59545.63
Iteration:    420, Loss function: 7.001, Average Loss: 3.205, avg. samples / sec: 59378.04
Iteration:    420, Loss function: 6.503, Average Loss: 3.203, avg. samples / sec: 59486.11
Iteration:    440, Loss function: 6.376, Average Loss: 3.260, avg. samples / sec: 58382.10
Iteration:    440, Loss function: 5.945, Average Loss: 3.275, avg. samples / sec: 58464.13
Iteration:    440, Loss function: 6.523, Average Loss: 3.277, avg. samples / sec: 58264.34
Iteration:    440, Loss function: 5.380, Average Loss: 3.273, avg. samples / sec: 58656.30
Iteration:    440, Loss function: 7.992, Average Loss: 3.294, avg. samples / sec: 58379.85
Iteration:    440, Loss function: 6.007, Average Loss: 3.283, avg. samples / sec: 58343.50
Iteration:    440, Loss function: 6.645, Average Loss: 3.265, avg. samples / sec: 58340.68
Iteration:    440, Loss function: 6.376, Average Loss: 3.279, avg. samples / sec: 58650.71
Iteration:    440, Loss function: 5.822, Average Loss: 3.288, avg. samples / sec: 58355.97
Iteration:    440, Loss function: 6.492, Average Loss: 3.313, avg. samples / sec: 58385.95
Iteration:    440, Loss function: 5.798, Average Loss: 3.267, avg. samples / sec: 58348.58
Iteration:    440, Loss function: 6.915, Average Loss: 3.277, avg. samples / sec: 58421.55
Iteration:    440, Loss function: 5.926, Average Loss: 3.283, avg. samples / sec: 58186.83
Iteration:    440, Loss function: 6.111, Average Loss: 3.286, avg. samples / sec: 58087.73
Iteration:    440, Loss function: 7.855, Average Loss: 3.297, avg. samples / sec: 58142.32
Iteration:    460, Loss function: 6.693, Average Loss: 3.343, avg. samples / sec: 58744.39
Iteration:    460, Loss function: 6.945, Average Loss: 3.339, avg. samples / sec: 58478.69
Iteration:    460, Loss function: 7.534, Average Loss: 3.356, avg. samples / sec: 58555.74
Iteration:    460, Loss function: 6.058, Average Loss: 3.352, avg. samples / sec: 58655.86
Iteration:    460, Loss function: 6.982, Average Loss: 3.326, avg. samples / sec: 58486.97
Iteration:    460, Loss function: 6.485, Average Loss: 3.363, avg. samples / sec: 58665.23
Iteration:    460, Loss function: 6.693, Average Loss: 3.382, avg. samples / sec: 58523.03
Iteration:    460, Loss function: 6.251, Average Loss: 3.347, avg. samples / sec: 58592.65
Iteration:    460, Loss function: 6.414, Average Loss: 3.365, avg. samples / sec: 58415.28
Iteration:    460, Loss function: 6.311, Average Loss: 3.342, avg. samples / sec: 58383.05
Iteration:    460, Loss function: 6.844, Average Loss: 3.322, avg. samples / sec: 58261.69
Iteration:    460, Loss function: 6.618, Average Loss: 3.350, avg. samples / sec: 58367.26
Iteration:    460, Loss function: 5.228, Average Loss: 3.330, avg. samples / sec: 58377.75
Iteration:    460, Loss function: 6.860, Average Loss: 3.336, avg. samples / sec: 58280.29
Iteration:    460, Loss function: 6.891, Average Loss: 3.342, avg. samples / sec: 58123.78
Iteration:    480, Loss function: 7.551, Average Loss: 3.415, avg. samples / sec: 60015.77
Iteration:    480, Loss function: 6.953, Average Loss: 3.381, avg. samples / sec: 60113.95
Iteration:    480, Loss function: 5.928, Average Loss: 3.405, avg. samples / sec: 60343.01
Iteration:    480, Loss function: 6.583, Average Loss: 3.391, avg. samples / sec: 59901.51
Iteration:    480, Loss function: 5.838, Average Loss: 3.401, avg. samples / sec: 60069.77
Iteration:    480, Loss function: 6.710, Average Loss: 3.409, avg. samples / sec: 59650.23
Iteration:    480, Loss function: 7.036, Average Loss: 3.409, avg. samples / sec: 59739.66
Iteration:    480, Loss function: 7.389, Average Loss: 3.434, avg. samples / sec: 59801.57
Iteration:    480, Loss function: 7.355, Average Loss: 3.428, avg. samples / sec: 59776.61
Iteration:    480, Loss function: 5.766, Average Loss: 3.411, avg. samples / sec: 59832.16
Iteration:    480, Loss function: 5.984, Average Loss: 3.399, avg. samples / sec: 59890.59
Iteration:    480, Loss function: 6.655, Average Loss: 3.413, avg. samples / sec: 59720.30
Iteration:    480, Loss function: 6.335, Average Loss: 3.421, avg. samples / sec: 59633.27
Iteration:    480, Loss function: 5.638, Average Loss: 3.404, avg. samples / sec: 59646.90
Iteration:    480, Loss function: 6.087, Average Loss: 3.446, avg. samples / sec: 58930.54
:::MLL 1558638956.452 epoch_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 819}}
:::MLL 1558638956.452 epoch_start: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 673}}
Iteration:    500, Loss function: 5.941, Average Loss: 3.439, avg. samples / sec: 58207.54
Iteration:    500, Loss function: 6.347, Average Loss: 3.479, avg. samples / sec: 58153.91
Iteration:    500, Loss function: 7.105, Average Loss: 3.471, avg. samples / sec: 58371.61
Iteration:    500, Loss function: 4.883, Average Loss: 3.492, avg. samples / sec: 58330.90
Iteration:    500, Loss function: 6.576, Average Loss: 3.472, avg. samples / sec: 58216.03
Iteration:    500, Loss function: 6.945, Average Loss: 3.487, avg. samples / sec: 58317.04
Iteration:    500, Loss function: 6.138, Average Loss: 3.472, avg. samples / sec: 58264.94
Iteration:    500, Loss function: 6.531, Average Loss: 3.465, avg. samples / sec: 58089.07
Iteration:    500, Loss function: 6.244, Average Loss: 3.452, avg. samples / sec: 58129.08
Iteration:    500, Loss function: 5.841, Average Loss: 3.507, avg. samples / sec: 59100.15
Iteration:    500, Loss function: 6.543, Average Loss: 3.459, avg. samples / sec: 58147.60
Iteration:    500, Loss function: 6.650, Average Loss: 3.475, avg. samples / sec: 58325.34
Iteration:    500, Loss function: 5.958, Average Loss: 3.464, avg. samples / sec: 58253.96
Iteration:    500, Loss function: 5.998, Average Loss: 3.482, avg. samples / sec: 58143.74
Iteration:    500, Loss function: 6.119, Average Loss: 3.465, avg. samples / sec: 57641.38
Iteration:    520, Loss function: 5.907, Average Loss: 3.557, avg. samples / sec: 57122.52
Iteration:    520, Loss function: 6.117, Average Loss: 3.520, avg. samples / sec: 57812.34
Iteration:    520, Loss function: 6.475, Average Loss: 3.529, avg. samples / sec: 56931.68
Iteration:    520, Loss function: 6.241, Average Loss: 3.507, avg. samples / sec: 56961.66
Iteration:    520, Loss function: 6.321, Average Loss: 3.525, avg. samples / sec: 56901.33
Iteration:    520, Loss function: 6.404, Average Loss: 3.540, avg. samples / sec: 56797.68
Iteration:    520, Loss function: 7.107, Average Loss: 3.530, avg. samples / sec: 56885.05
Iteration:    520, Loss function: 6.584, Average Loss: 3.514, avg. samples / sec: 56920.96
Iteration:    520, Loss function: 7.174, Average Loss: 3.526, avg. samples / sec: 56911.61
Iteration:    520, Loss function: 6.609, Average Loss: 3.552, avg. samples / sec: 56846.89
Iteration:    520, Loss function: 6.747, Average Loss: 3.523, avg. samples / sec: 56954.52
Iteration:    520, Loss function: 6.664, Average Loss: 3.542, avg. samples / sec: 57098.98
Iteration:    520, Loss function: 7.196, Average Loss: 3.498, avg. samples / sec: 56682.02
Iteration:    520, Loss function: 5.664, Average Loss: 3.532, avg. samples / sec: 56887.83
Iteration:    520, Loss function: 6.061, Average Loss: 3.551, avg. samples / sec: 56797.91
Iteration:    540, Loss function: 6.246, Average Loss: 3.578, avg. samples / sec: 58324.81
Iteration:    540, Loss function: 6.908, Average Loss: 3.555, avg. samples / sec: 58274.91
Iteration:    540, Loss function: 6.369, Average Loss: 3.556, avg. samples / sec: 58158.93
Iteration:    540, Loss function: 6.316, Average Loss: 3.581, avg. samples / sec: 58183.54
Iteration:    540, Loss function: 6.504, Average Loss: 3.608, avg. samples / sec: 57949.62
Iteration:    540, Loss function: 6.579, Average Loss: 3.588, avg. samples / sec: 58143.62
Iteration:    540, Loss function: 8.123, Average Loss: 3.606, avg. samples / sec: 58170.88
Iteration:    540, Loss function: 6.766, Average Loss: 3.575, avg. samples / sec: 58024.57
Iteration:    540, Loss function: 6.750, Average Loss: 3.607, avg. samples / sec: 58230.75
Iteration:    540, Loss function: 6.410, Average Loss: 3.582, avg. samples / sec: 58033.86
Iteration:    540, Loss function: 5.684, Average Loss: 3.585, avg. samples / sec: 58167.69
Iteration:    540, Loss function: 6.376, Average Loss: 3.566, avg. samples / sec: 58087.06
Iteration:    540, Loss function: 5.078, Average Loss: 3.582, avg. samples / sec: 58051.96
Iteration:    540, Loss function: 7.283, Average Loss: 3.598, avg. samples / sec: 57965.81
Iteration:    540, Loss function: 5.596, Average Loss: 3.589, avg. samples / sec: 57768.00
:::MLL 1558638958.489 epoch_stop: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 819}}
:::MLL 1558638958.489 epoch_start: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 673}}
Iteration:    560, Loss function: 5.335, Average Loss: 3.646, avg. samples / sec: 58004.27
Iteration:    560, Loss function: 5.582, Average Loss: 3.663, avg. samples / sec: 57943.48
Iteration:    560, Loss function: 5.605, Average Loss: 3.632, avg. samples / sec: 58023.75
Iteration:    560, Loss function: 6.862, Average Loss: 3.612, avg. samples / sec: 57842.00
Iteration:    560, Loss function: 5.638, Average Loss: 3.643, avg. samples / sec: 57874.83
Iteration:    560, Loss function: 6.543, Average Loss: 3.660, avg. samples / sec: 57842.47
Iteration:    560, Loss function: 5.505, Average Loss: 3.631, avg. samples / sec: 57703.80
Iteration:    560, Loss function: 7.336, Average Loss: 3.611, avg. samples / sec: 57770.32
Iteration:    560, Loss function: 6.427, Average Loss: 3.634, avg. samples / sec: 57799.49
Iteration:    560, Loss function: 6.232, Average Loss: 3.661, avg. samples / sec: 57802.38
Iteration:    560, Loss function: 6.152, Average Loss: 3.636, avg. samples / sec: 57753.11
Iteration:    560, Loss function: 5.472, Average Loss: 3.653, avg. samples / sec: 57971.53
Iteration:    560, Loss function: 6.563, Average Loss: 3.641, avg. samples / sec: 57780.72
Iteration:    560, Loss function: 5.127, Average Loss: 3.643, avg. samples / sec: 58101.55
Iteration:    560, Loss function: 5.754, Average Loss: 3.618, avg. samples / sec: 57585.72
Iteration:    580, Loss function: 5.115, Average Loss: 3.660, avg. samples / sec: 58960.22
Iteration:    580, Loss function: 5.389, Average Loss: 3.687, avg. samples / sec: 59024.18
Iteration:    580, Loss function: 6.576, Average Loss: 3.678, avg. samples / sec: 58809.47
Iteration:    580, Loss function: 6.604, Average Loss: 3.686, avg. samples / sec: 58947.92
Iteration:    580, Loss function: 5.903, Average Loss: 3.691, avg. samples / sec: 58838.82
Iteration:    580, Loss function: 6.099, Average Loss: 3.667, avg. samples / sec: 59196.69
Iteration:    580, Loss function: 6.055, Average Loss: 3.682, avg. samples / sec: 58960.42
Iteration:    580, Loss function: 6.122, Average Loss: 3.709, avg. samples / sec: 58859.19
Iteration:    580, Loss function: 4.905, Average Loss: 3.713, avg. samples / sec: 58848.87
Iteration:    580, Loss function: 5.968, Average Loss: 3.689, avg. samples / sec: 58846.80
Iteration:    580, Loss function: 6.296, Average Loss: 3.701, avg. samples / sec: 58864.08
Iteration:    580, Loss function: 6.445, Average Loss: 3.658, avg. samples / sec: 58743.46
Iteration:    580, Loss function: 5.043, Average Loss: 3.693, avg. samples / sec: 58545.42
Iteration:    580, Loss function: 6.836, Average Loss: 3.710, avg. samples / sec: 58549.95
Iteration:    580, Loss function: 6.781, Average Loss: 3.681, avg. samples / sec: 58618.94
Iteration:    600, Loss function: 6.721, Average Loss: 3.739, avg. samples / sec: 58955.19
Iteration:    600, Loss function: 6.098, Average Loss: 3.726, avg. samples / sec: 58853.85
Iteration:    600, Loss function: 7.397, Average Loss: 3.735, avg. samples / sec: 58857.96
Iteration:    600, Loss function: 6.011, Average Loss: 3.760, avg. samples / sec: 58844.74
Iteration:    600, Loss function: 6.956, Average Loss: 3.739, avg. samples / sec: 58809.72
Iteration:    600, Loss function: 5.854, Average Loss: 3.710, avg. samples / sec: 58869.61
Iteration:    600, Loss function: 6.370, Average Loss: 3.711, avg. samples / sec: 58709.13
Iteration:    600, Loss function: 6.245, Average Loss: 3.761, avg. samples / sec: 59009.53
Iteration:    600, Loss function: 6.227, Average Loss: 3.748, avg. samples / sec: 58873.25
Iteration:    600, Loss function: 6.433, Average Loss: 3.734, avg. samples / sec: 58715.95
Iteration:    600, Loss function: 5.159, Average Loss: 3.738, avg. samples / sec: 58892.84
Iteration:    600, Loss function: 5.998, Average Loss: 3.767, avg. samples / sec: 58716.56
Iteration:    600, Loss function: 5.919, Average Loss: 3.735, avg. samples / sec: 58606.78
Iteration:    600, Loss function: 6.545, Average Loss: 3.731, avg. samples / sec: 58881.74
Iteration:    600, Loss function: 5.672, Average Loss: 3.716, avg. samples / sec: 58228.18
Iteration:    620, Loss function: 5.186, Average Loss: 3.777, avg. samples / sec: 59560.13
Iteration:    620, Loss function: 5.686, Average Loss: 3.780, avg. samples / sec: 59233.96
Iteration:    620, Loss function: 5.518, Average Loss: 3.760, avg. samples / sec: 59874.16
Iteration:    620, Loss function: 6.399, Average Loss: 3.782, avg. samples / sec: 59287.41
Iteration:    620, Loss function: 5.890, Average Loss: 3.769, avg. samples / sec: 59117.23
Iteration:    620, Loss function: 7.364, Average Loss: 3.808, avg. samples / sec: 59149.58
Iteration:    620, Loss function: 6.130, Average Loss: 3.785, avg. samples / sec: 59158.75
Iteration:    620, Loss function: 6.556, Average Loss: 3.809, avg. samples / sec: 59327.62
Iteration:    620, Loss function: 5.870, Average Loss: 3.802, avg. samples / sec: 59173.72
Iteration:    620, Loss function: 4.847, Average Loss: 3.759, avg. samples / sec: 59156.98
Iteration:    620, Loss function: 5.449, Average Loss: 3.792, avg. samples / sec: 59169.38
Iteration:    620, Loss function: 7.616, Average Loss: 3.779, avg. samples / sec: 59321.55
Iteration:    620, Loss function: 6.060, Average Loss: 3.752, avg. samples / sec: 59112.67
Iteration:    620, Loss function: 5.870, Average Loss: 3.789, avg. samples / sec: 58973.77
Iteration:    620, Loss function: 7.024, Average Loss: 3.785, avg. samples / sec: 59136.48
:::MLL 1558638960.479 epoch_stop: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 819}}
:::MLL 1558638960.480 epoch_start: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 673}}
Iteration:    640, Loss function: 5.618, Average Loss: 3.810, avg. samples / sec: 58774.50
Iteration:    640, Loss function: 5.928, Average Loss: 3.844, avg. samples / sec: 58795.66
Iteration:    640, Loss function: 5.522, Average Loss: 3.823, avg. samples / sec: 58868.21
Iteration:    640, Loss function: 4.751, Average Loss: 3.815, avg. samples / sec: 58746.67
Iteration:    640, Loss function: 5.420, Average Loss: 3.843, avg. samples / sec: 58690.37
Iteration:    640, Loss function: 4.704, Average Loss: 3.795, avg. samples / sec: 58719.55
Iteration:    640, Loss function: 5.631, Average Loss: 3.794, avg. samples / sec: 58743.39
Iteration:    640, Loss function: 7.051, Average Loss: 3.817, avg. samples / sec: 58564.43
Iteration:    640, Loss function: 6.158, Average Loss: 3.821, avg. samples / sec: 58630.72
Iteration:    640, Loss function: 6.705, Average Loss: 3.825, avg. samples / sec: 58391.92
Iteration:    640, Loss function: 6.669, Average Loss: 3.823, avg. samples / sec: 58541.63
Iteration:    640, Loss function: 5.608, Average Loss: 3.851, avg. samples / sec: 58549.34
Iteration:    640, Loss function: 5.998, Average Loss: 3.798, avg. samples / sec: 58351.11
Iteration:    640, Loss function: 6.037, Average Loss: 3.826, avg. samples / sec: 58526.51
Iteration:    640, Loss function: 5.961, Average Loss: 3.830, avg. samples / sec: 58393.50
Iteration:    660, Loss function: 6.329, Average Loss: 3.905, avg. samples / sec: 60722.87
Iteration:    660, Loss function: 6.326, Average Loss: 3.875, avg. samples / sec: 60479.40
Iteration:    660, Loss function: 6.237, Average Loss: 3.852, avg. samples / sec: 60746.35
Iteration:    660, Loss function: 6.963, Average Loss: 3.869, avg. samples / sec: 60482.13
Iteration:    660, Loss function: 5.555, Average Loss: 3.871, avg. samples / sec: 60576.16
Iteration:    660, Loss function: 6.108, Average Loss: 3.876, avg. samples / sec: 60516.15
Iteration:    660, Loss function: 5.873, Average Loss: 3.879, avg. samples / sec: 60695.93
Iteration:    660, Loss function: 5.719, Average Loss: 3.869, avg. samples / sec: 60223.00
Iteration:    660, Loss function: 5.221, Average Loss: 3.839, avg. samples / sec: 60297.51
Iteration:    660, Loss function: 6.026, Average Loss: 3.845, avg. samples / sec: 60274.61
Iteration:    660, Loss function: 6.890, Average Loss: 3.879, avg. samples / sec: 60609.22
Iteration:    660, Loss function: 6.047, Average Loss: 3.872, avg. samples / sec: 60330.92
Iteration:    660, Loss function: 6.635, Average Loss: 3.867, avg. samples / sec: 60211.52
Iteration:    660, Loss function: 5.941, Average Loss: 3.890, avg. samples / sec: 60138.68
Iteration:    660, Loss function: 4.761, Average Loss: 3.886, avg. samples / sec: 59964.16
Iteration:    680, Loss function: 5.200, Average Loss: 3.924, avg. samples / sec: 57152.86
Iteration:    680, Loss function: 4.185, Average Loss: 3.924, avg. samples / sec: 57321.24
Iteration:    680, Loss function: 5.059, Average Loss: 3.903, avg. samples / sec: 57068.76
Iteration:    680, Loss function: 6.039, Average Loss: 3.919, avg. samples / sec: 56862.74
Iteration:    680, Loss function: 5.034, Average Loss: 3.911, avg. samples / sec: 56992.00
Iteration:    680, Loss function: 5.218, Average Loss: 3.887, avg. samples / sec: 56981.44
Iteration:    680, Loss function: 5.724, Average Loss: 3.909, avg. samples / sec: 56843.38
Iteration:    680, Loss function: 6.194, Average Loss: 3.946, avg. samples / sec: 56701.13
Iteration:    680, Loss function: 4.615, Average Loss: 3.902, avg. samples / sec: 56907.49
Iteration:    680, Loss function: 5.214, Average Loss: 3.919, avg. samples / sec: 56845.10
Iteration:    680, Loss function: 4.962, Average Loss: 3.917, avg. samples / sec: 56941.84
Iteration:    680, Loss function: 7.902, Average Loss: 3.913, avg. samples / sec: 56744.92
Iteration:    680, Loss function: 5.274, Average Loss: 3.897, avg. samples / sec: 56715.60
Iteration:    680, Loss function: 6.770, Average Loss: 3.918, avg. samples / sec: 56547.88
Iteration:    680, Loss function: 4.428, Average Loss: 3.879, avg. samples / sec: 56762.29
:::MLL 1558638962.480 epoch_stop: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 819}}
:::MLL 1558638962.480 epoch_start: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 673}}
Iteration:    700, Loss function: 6.045, Average Loss: 3.911, avg. samples / sec: 60329.31
Iteration:    700, Loss function: 4.590, Average Loss: 3.958, avg. samples / sec: 59831.68
Iteration:    700, Loss function: 7.044, Average Loss: 3.941, avg. samples / sec: 59826.95
Iteration:    700, Loss function: 5.490, Average Loss: 3.950, avg. samples / sec: 59891.92
Iteration:    700, Loss function: 6.163, Average Loss: 3.944, avg. samples / sec: 59933.05
Iteration:    700, Loss function: 5.187, Average Loss: 3.939, avg. samples / sec: 59833.69
Iteration:    700, Loss function: 5.762, Average Loss: 3.938, avg. samples / sec: 59937.10
Iteration:    700, Loss function: 5.623, Average Loss: 3.951, avg. samples / sec: 60044.49
Iteration:    700, Loss function: 5.118, Average Loss: 3.919, avg. samples / sec: 59806.24
Iteration:    700, Loss function: 5.064, Average Loss: 3.955, avg. samples / sec: 59655.86
Iteration:    700, Loss function: 7.207, Average Loss: 3.953, avg. samples / sec: 59754.71
Iteration:    700, Loss function: 5.732, Average Loss: 3.934, avg. samples / sec: 59763.35
Iteration:    700, Loss function: 5.560, Average Loss: 3.940, avg. samples / sec: 59687.77
Iteration:    700, Loss function: 6.055, Average Loss: 3.977, avg. samples / sec: 59632.58
Iteration:    700, Loss function: 5.615, Average Loss: 3.952, avg. samples / sec: 59648.16
Iteration:    720, Loss function: 5.391, Average Loss: 4.007, avg. samples / sec: 59933.71
Iteration:    720, Loss function: 5.955, Average Loss: 3.975, avg. samples / sec: 59646.24
Iteration:    720, Loss function: 6.711, Average Loss: 3.987, avg. samples / sec: 59573.75
Iteration:    720, Loss function: 6.481, Average Loss: 3.999, avg. samples / sec: 59628.60
Iteration:    720, Loss function: 6.270, Average Loss: 3.983, avg. samples / sec: 59551.70
Iteration:    720, Loss function: 5.950, Average Loss: 3.976, avg. samples / sec: 59546.82
Iteration:    720, Loss function: 5.673, Average Loss: 3.957, avg. samples / sec: 59587.15
Iteration:    720, Loss function: 5.229, Average Loss: 3.985, avg. samples / sec: 59558.82
Iteration:    720, Loss function: 4.297, Average Loss: 3.992, avg. samples / sec: 59732.19
Iteration:    720, Loss function: 5.149, Average Loss: 3.974, avg. samples / sec: 59604.26
Iteration:    720, Loss function: 6.065, Average Loss: 3.993, avg. samples / sec: 59385.90
Iteration:    720, Loss function: 5.481, Average Loss: 3.970, avg. samples / sec: 59549.99
Iteration:    720, Loss function: 5.518, Average Loss: 3.989, avg. samples / sec: 59511.23
Iteration:    720, Loss function: 5.394, Average Loss: 3.975, avg. samples / sec: 59365.48
Iteration:    720, Loss function: 4.960, Average Loss: 3.943, avg. samples / sec: 59008.59
Iteration:    740, Loss function: 5.773, Average Loss: 3.972, avg. samples / sec: 58647.51
Iteration:    740, Loss function: 4.745, Average Loss: 4.017, avg. samples / sec: 58282.26
Iteration:    740, Loss function: 5.485, Average Loss: 4.014, avg. samples / sec: 58294.22
Iteration:    740, Loss function: 6.201, Average Loss: 4.006, avg. samples / sec: 58150.12
Iteration:    740, Loss function: 5.719, Average Loss: 4.040, avg. samples / sec: 58121.58
Iteration:    740, Loss function: 5.961, Average Loss: 4.022, avg. samples / sec: 58340.48
Iteration:    740, Loss function: 5.034, Average Loss: 4.003, avg. samples / sec: 58339.47
Iteration:    740, Loss function: 5.922, Average Loss: 4.007, avg. samples / sec: 58419.40
Iteration:    740, Loss function: 5.333, Average Loss: 4.000, avg. samples / sec: 58312.17
Iteration:    740, Loss function: 4.483, Average Loss: 4.018, avg. samples / sec: 58330.46
Iteration:    740, Loss function: 6.267, Average Loss: 4.022, avg. samples / sec: 58222.65
Iteration:    740, Loss function: 7.026, Average Loss: 4.010, avg. samples / sec: 58165.24
Iteration:    740, Loss function: 6.397, Average Loss: 4.016, avg. samples / sec: 58150.74
Iteration:    740, Loss function: 6.313, Average Loss: 3.986, avg. samples / sec: 58103.92
Iteration:    740, Loss function: 6.788, Average Loss: 4.032, avg. samples / sec: 58073.25
Iteration:    760, Loss function: 5.154, Average Loss: 4.005, avg. samples / sec: 57389.72
Iteration:    760, Loss function: 4.964, Average Loss: 4.054, avg. samples / sec: 57488.42
Iteration:    760, Loss function: 7.111, Average Loss: 4.062, avg. samples / sec: 57595.49
Iteration:    760, Loss function: 5.914, Average Loss: 4.071, avg. samples / sec: 57398.98
Iteration:    760, Loss function: 5.941, Average Loss: 4.041, avg. samples / sec: 57514.07
Iteration:    760, Loss function: 6.396, Average Loss: 4.039, avg. samples / sec: 57408.50
Iteration:    760, Loss function: 5.442, Average Loss: 4.049, avg. samples / sec: 57315.85
Iteration:    760, Loss function: 5.124, Average Loss: 4.038, avg. samples / sec: 57473.70
Iteration:    760, Loss function: 5.169, Average Loss: 4.050, avg. samples / sec: 57461.72
Iteration:    760, Loss function: 5.634, Average Loss: 4.034, avg. samples / sec: 57371.41
Iteration:    760, Loss function: 5.547, Average Loss: 4.053, avg. samples / sec: 57370.36
Iteration:    760, Loss function: 5.845, Average Loss: 4.036, avg. samples / sec: 57361.41
Iteration:    760, Loss function: 6.238, Average Loss: 4.016, avg. samples / sec: 57440.46
Iteration:    760, Loss function: 5.236, Average Loss: 4.038, avg. samples / sec: 57239.59
Iteration:    760, Loss function: 4.267, Average Loss: 4.045, avg. samples / sec: 57141.09
:::MLL 1558638964.487 epoch_stop: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 819}}
:::MLL 1558638964.488 epoch_start: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 673}}
Iteration:    780, Loss function: 5.321, Average Loss: 4.089, avg. samples / sec: 58588.58
Iteration:    780, Loss function: 5.303, Average Loss: 4.062, avg. samples / sec: 58603.00
Iteration:    780, Loss function: 5.459, Average Loss: 4.071, avg. samples / sec: 58774.23
Iteration:    780, Loss function: 4.646, Average Loss: 4.074, avg. samples / sec: 58552.65
Iteration:    780, Loss function: 5.292, Average Loss: 4.062, avg. samples / sec: 58557.95
Iteration:    780, Loss function: 4.712, Average Loss: 4.040, avg. samples / sec: 58641.41
Iteration:    780, Loss function: 4.983, Average Loss: 4.075, avg. samples / sec: 58527.43
Iteration:    780, Loss function: 5.793, Average Loss: 4.033, avg. samples / sec: 58397.27
Iteration:    780, Loss function: 5.112, Average Loss: 4.078, avg. samples / sec: 58431.07
Iteration:    780, Loss function: 4.861, Average Loss: 4.098, avg. samples / sec: 58391.46
Iteration:    780, Loss function: 5.022, Average Loss: 4.077, avg. samples / sec: 58465.40
Iteration:    780, Loss function: 4.810, Average Loss: 4.057, avg. samples / sec: 58444.64
Iteration:    780, Loss function: 7.186, Average Loss: 4.071, avg. samples / sec: 58482.04
Iteration:    780, Loss function: 5.182, Average Loss: 4.069, avg. samples / sec: 58343.21
Iteration:    780, Loss function: 5.476, Average Loss: 4.092, avg. samples / sec: 58264.75
Iteration:    800, Loss function: 5.149, Average Loss: 4.091, avg. samples / sec: 57818.08
Iteration:    800, Loss function: 6.050, Average Loss: 4.095, avg. samples / sec: 57753.25
Iteration:    800, Loss function: 5.908, Average Loss: 4.126, avg. samples / sec: 57877.63
Iteration:    800, Loss function: 6.090, Average Loss: 4.102, avg. samples / sec: 57757.94
Iteration:    800, Loss function: 6.305, Average Loss: 4.120, avg. samples / sec: 57929.33
Iteration:    800, Loss function: 6.737, Average Loss: 4.120, avg. samples / sec: 57574.03
Iteration:    800, Loss function: 4.561, Average Loss: 4.094, avg. samples / sec: 57867.86
Iteration:    800, Loss function: 5.224, Average Loss: 4.089, avg. samples / sec: 57823.72
Iteration:    800, Loss function: 4.376, Average Loss: 4.062, avg. samples / sec: 57700.35
Iteration:    800, Loss function: 6.152, Average Loss: 4.103, avg. samples / sec: 57646.19
Iteration:    800, Loss function: 5.216, Average Loss: 4.110, avg. samples / sec: 57763.14
Iteration:    800, Loss function: 5.101, Average Loss: 4.106, avg. samples / sec: 57750.55
Iteration:    800, Loss function: 6.689, Average Loss: 4.094, avg. samples / sec: 57591.23
Iteration:    800, Loss function: 5.699, Average Loss: 4.098, avg. samples / sec: 57741.30
Iteration:    800, Loss function: 5.860, Average Loss: 4.072, avg. samples / sec: 57525.66
Iteration:    820, Loss function: 5.540, Average Loss: 4.138, avg. samples / sec: 56659.68
Iteration:    820, Loss function: 5.565, Average Loss: 4.124, avg. samples / sec: 56617.78
Iteration:    820, Loss function: 4.654, Average Loss: 4.124, avg. samples / sec: 56729.59
Iteration:    820, Loss function: 5.659, Average Loss: 4.156, avg. samples / sec: 56530.07
Iteration:    820, Loss function: 5.479, Average Loss: 4.118, avg. samples / sec: 56607.54
Iteration:    820, Loss function: 5.686, Average Loss: 4.146, avg. samples / sec: 56560.27
Iteration:    820, Loss function: 4.886, Average Loss: 4.096, avg. samples / sec: 56723.11
Iteration:    820, Loss function: 5.755, Average Loss: 4.135, avg. samples / sec: 56610.11
Iteration:    820, Loss function: 5.566, Average Loss: 4.121, avg. samples / sec: 56402.67
Iteration:    820, Loss function: 5.877, Average Loss: 4.122, avg. samples / sec: 56606.16
Iteration:    820, Loss function: 5.096, Average Loss: 4.146, avg. samples / sec: 56479.59
Iteration:    820, Loss function: 5.451, Average Loss: 4.129, avg. samples / sec: 56552.25
Iteration:    820, Loss function: 5.253, Average Loss: 4.124, avg. samples / sec: 56400.03
Iteration:    820, Loss function: 6.406, Average Loss: 4.091, avg. samples / sec: 56400.82
Iteration:    820, Loss function: 5.367, Average Loss: 4.133, avg. samples / sec: 56187.67
:::MLL 1558638966.528 epoch_stop: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 819}}
:::MLL 1558638966.528 epoch_start: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 673}}
Iteration:    840, Loss function: 4.344, Average Loss: 4.176, avg. samples / sec: 59059.01
Iteration:    840, Loss function: 5.854, Average Loss: 4.150, avg. samples / sec: 59073.81
Iteration:    840, Loss function: 5.041, Average Loss: 4.158, avg. samples / sec: 58933.74
Iteration:    840, Loss function: 6.113, Average Loss: 4.172, avg. samples / sec: 58988.60
Iteration:    840, Loss function: 5.077, Average Loss: 4.116, avg. samples / sec: 58920.86
Iteration:    840, Loss function: 5.327, Average Loss: 4.149, avg. samples / sec: 58980.16
Iteration:    840, Loss function: 4.944, Average Loss: 4.144, avg. samples / sec: 58914.43
Iteration:    840, Loss function: 4.235, Average Loss: 4.153, avg. samples / sec: 59231.55
Iteration:    840, Loss function: 5.817, Average Loss: 4.142, avg. samples / sec: 58901.40
Iteration:    840, Loss function: 6.471, Average Loss: 4.156, avg. samples / sec: 58790.36
Iteration:    840, Loss function: 5.471, Average Loss: 4.144, avg. samples / sec: 58777.71
Iteration:    840, Loss function: 5.602, Average Loss: 4.144, avg. samples / sec: 58814.63
Iteration:    840, Loss function: 5.277, Average Loss: 4.151, avg. samples / sec: 58781.90
Iteration:    840, Loss function: 5.552, Average Loss: 4.168, avg. samples / sec: 58783.64
Iteration:    840, Loss function: 4.892, Average Loss: 4.114, avg. samples / sec: 58953.64
Iteration:    860, Loss function: 5.528, Average Loss: 4.165, avg. samples / sec: 59965.18
Iteration:    860, Loss function: 6.567, Average Loss: 4.179, avg. samples / sec: 59796.54
Iteration:    860, Loss function: 4.164, Average Loss: 4.133, avg. samples / sec: 59849.95
Iteration:    860, Loss function: 6.332, Average Loss: 4.180, avg. samples / sec: 59861.03
Iteration:    860, Loss function: 5.113, Average Loss: 4.196, avg. samples / sec: 59737.66
Iteration:    860, Loss function: 6.468, Average Loss: 4.180, avg. samples / sec: 59718.70
Iteration:    860, Loss function: 4.670, Average Loss: 4.177, avg. samples / sec: 59658.66
Iteration:    860, Loss function: 4.533, Average Loss: 4.172, avg. samples / sec: 59791.88
Iteration:    860, Loss function: 6.081, Average Loss: 4.194, avg. samples / sec: 59466.56
Iteration:    860, Loss function: 6.060, Average Loss: 4.166, avg. samples / sec: 59674.12
Iteration:    860, Loss function: 6.828, Average Loss: 4.198, avg. samples / sec: 59700.49
Iteration:    860, Loss function: 6.463, Average Loss: 4.138, avg. samples / sec: 59701.40
Iteration:    860, Loss function: 4.556, Average Loss: 4.168, avg. samples / sec: 59593.52
Iteration:    860, Loss function: 5.261, Average Loss: 4.163, avg. samples / sec: 59382.79
Iteration:    860, Loss function: 5.159, Average Loss: 4.175, avg. samples / sec: 58674.10
Iteration:    880, Loss function: 4.290, Average Loss: 4.159, avg. samples / sec: 56861.46
Iteration:    880, Loss function: 4.472, Average Loss: 4.190, avg. samples / sec: 57181.27
Iteration:    880, Loss function: 6.049, Average Loss: 4.195, avg. samples / sec: 57017.27
Iteration:    880, Loss function: 5.977, Average Loss: 4.191, avg. samples / sec: 57054.32
Iteration:    880, Loss function: 5.194, Average Loss: 4.195, avg. samples / sec: 57945.05
Iteration:    880, Loss function: 5.581, Average Loss: 4.221, avg. samples / sec: 56894.33
Iteration:    880, Loss function: 5.621, Average Loss: 4.194, avg. samples / sec: 57221.39
Iteration:    880, Loss function: 5.767, Average Loss: 4.226, avg. samples / sec: 57007.03
Iteration:    880, Loss function: 5.182, Average Loss: 4.196, avg. samples / sec: 56912.32
Iteration:    880, Loss function: 4.312, Average Loss: 4.200, avg. samples / sec: 56911.10
Iteration:    880, Loss function: 6.375, Average Loss: 4.206, avg. samples / sec: 56790.33
Iteration:    880, Loss function: 5.356, Average Loss: 4.219, avg. samples / sec: 56932.04
Iteration:    880, Loss function: 6.321, Average Loss: 4.164, avg. samples / sec: 56976.01
Iteration:    880, Loss function: 4.030, Average Loss: 4.192, avg. samples / sec: 56607.02
Iteration:    880, Loss function: 5.448, Average Loss: 4.202, avg. samples / sec: 56605.63
Iteration:    900, Loss function: 3.812, Average Loss: 4.215, avg. samples / sec: 58794.93
Iteration:    900, Loss function: 4.172, Average Loss: 4.215, avg. samples / sec: 58696.44
Iteration:    900, Loss function: 4.155, Average Loss: 4.218, avg. samples / sec: 58792.52
Iteration:    900, Loss function: 5.209, Average Loss: 4.216, avg. samples / sec: 58737.53
Iteration:    900, Loss function: 5.804, Average Loss: 4.215, avg. samples / sec: 58865.09
Iteration:    900, Loss function: 6.163, Average Loss: 4.252, avg. samples / sec: 58718.03
Iteration:    900, Loss function: 3.541, Average Loss: 4.217, avg. samples / sec: 58589.94
Iteration:    900, Loss function: 4.882, Average Loss: 4.244, avg. samples / sec: 58728.97
Iteration:    900, Loss function: 5.252, Average Loss: 4.211, avg. samples / sec: 58686.78
Iteration:    900, Loss function: 5.011, Average Loss: 4.230, avg. samples / sec: 58699.69
Iteration:    900, Loss function: 3.050, Average Loss: 4.218, avg. samples / sec: 58820.62
Iteration:    900, Loss function: 5.446, Average Loss: 4.177, avg. samples / sec: 58512.76
Iteration:    900, Loss function: 6.914, Average Loss: 4.242, avg. samples / sec: 58591.99
Iteration:    900, Loss function: 5.348, Average Loss: 4.189, avg. samples / sec: 58677.37
Iteration:    900, Loss function: 5.080, Average Loss: 4.212, avg. samples / sec: 58496.05
:::MLL 1558638968.537 epoch_stop: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 819}}
:::MLL 1558638968.537 epoch_start: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 673}}
Iteration:    920, Loss function: 4.417, Average Loss: 4.195, avg. samples / sec: 59598.87
Iteration:    920, Loss function: 5.637, Average Loss: 4.233, avg. samples / sec: 59447.15
Iteration:    920, Loss function: 6.080, Average Loss: 4.230, avg. samples / sec: 59584.20
Iteration:    920, Loss function: 4.684, Average Loss: 4.257, avg. samples / sec: 59533.08
Iteration:    920, Loss function: 5.315, Average Loss: 4.229, avg. samples / sec: 59455.10
Iteration:    920, Loss function: 5.344, Average Loss: 4.273, avg. samples / sec: 59396.28
Iteration:    920, Loss function: 5.657, Average Loss: 4.235, avg. samples / sec: 59428.77
Iteration:    920, Loss function: 4.238, Average Loss: 4.255, avg. samples / sec: 59391.03
Iteration:    920, Loss function: 5.643, Average Loss: 4.213, avg. samples / sec: 59479.24
Iteration:    920, Loss function: 4.413, Average Loss: 4.246, avg. samples / sec: 59363.28
Iteration:    920, Loss function: 5.194, Average Loss: 4.233, avg. samples / sec: 59234.36
Iteration:    920, Loss function: 5.052, Average Loss: 4.237, avg. samples / sec: 59257.30
Iteration:    920, Loss function: 4.606, Average Loss: 4.236, avg. samples / sec: 59229.58
Iteration:    920, Loss function: 4.667, Average Loss: 4.234, avg. samples / sec: 59089.47
Iteration:    920, Loss function: 4.184, Average Loss: 4.235, avg. samples / sec: 59132.38
Iteration:    940, Loss function: 5.099, Average Loss: 4.277, avg. samples / sec: 56732.10
Iteration:    940, Loss function: 5.687, Average Loss: 4.296, avg. samples / sec: 56715.37
Iteration:    940, Loss function: 4.933, Average Loss: 4.267, avg. samples / sec: 56744.99
Iteration:    940, Loss function: 5.640, Average Loss: 4.262, avg. samples / sec: 56816.57
Iteration:    940, Loss function: 4.574, Average Loss: 4.256, avg. samples / sec: 56739.59
Iteration:    940, Loss function: 6.770, Average Loss: 4.214, avg. samples / sec: 56518.91
Iteration:    940, Loss function: 5.364, Average Loss: 4.260, avg. samples / sec: 56727.26
Iteration:    940, Loss function: 5.158, Average Loss: 4.236, avg. samples / sec: 56618.82
Iteration:    940, Loss function: 5.690, Average Loss: 4.257, avg. samples / sec: 56576.43
Iteration:    940, Loss function: 4.606, Average Loss: 4.255, avg. samples / sec: 56532.61
Iteration:    940, Loss function: 5.760, Average Loss: 4.254, avg. samples / sec: 56500.76
Iteration:    940, Loss function: 4.514, Average Loss: 4.253, avg. samples / sec: 56456.04
Iteration:    940, Loss function: 6.726, Average Loss: 4.259, avg. samples / sec: 56651.05
Iteration:    940, Loss function: 5.528, Average Loss: 4.265, avg. samples / sec: 56652.83
Iteration:    940, Loss function: 6.192, Average Loss: 4.283, avg. samples / sec: 56412.22
Iteration:    960, Loss function: 5.736, Average Loss: 4.274, avg. samples / sec: 59904.77
Iteration:    960, Loss function: 4.656, Average Loss: 4.278, avg. samples / sec: 59720.07
Iteration:    960, Loss function: 4.515, Average Loss: 4.277, avg. samples / sec: 59701.63
Iteration:    960, Loss function: 5.198, Average Loss: 4.285, avg. samples / sec: 59759.85
Iteration:    960, Loss function: 5.476, Average Loss: 4.252, avg. samples / sec: 59732.27
Iteration:    960, Loss function: 5.755, Average Loss: 4.272, avg. samples / sec: 59791.11
Iteration:    960, Loss function: 5.261, Average Loss: 4.297, avg. samples / sec: 59567.93
Iteration:    960, Loss function: 6.666, Average Loss: 4.283, avg. samples / sec: 59819.49
Iteration:    960, Loss function: 5.157, Average Loss: 4.275, avg. samples / sec: 59672.13
Iteration:    960, Loss function: 4.975, Average Loss: 4.279, avg. samples / sec: 59650.40
Iteration:    960, Loss function: 5.685, Average Loss: 4.283, avg. samples / sec: 59736.37
Iteration:    960, Loss function: 4.663, Average Loss: 4.301, avg. samples / sec: 59769.66
Iteration:    960, Loss function: 6.347, Average Loss: 4.234, avg. samples / sec: 59574.20
Iteration:    960, Loss function: 5.647, Average Loss: 4.314, avg. samples / sec: 59447.82
Iteration:    960, Loss function: 4.895, Average Loss: 4.289, avg. samples / sec: 59383.05
:::MLL 1558638970.545 epoch_stop: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 819}}
:::MLL 1558638970.546 epoch_start: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 673}}
Iteration:    980, Loss function: 3.425, Average Loss: 4.293, avg. samples / sec: 59314.26
Iteration:    980, Loss function: 5.085, Average Loss: 4.314, avg. samples / sec: 59230.30
Iteration:    980, Loss function: 5.437, Average Loss: 4.295, avg. samples / sec: 59195.27
Iteration:    980, Loss function: 4.201, Average Loss: 4.290, avg. samples / sec: 58976.93
Iteration:    980, Loss function: 4.564, Average Loss: 4.311, avg. samples / sec: 59100.22
Iteration:    980, Loss function: 5.689, Average Loss: 4.291, avg. samples / sec: 59004.66
Iteration:    980, Loss function: 6.012, Average Loss: 4.296, avg. samples / sec: 59061.63
Iteration:    980, Loss function: 5.023, Average Loss: 4.302, avg. samples / sec: 59015.43
Iteration:    980, Loss function: 6.612, Average Loss: 4.287, avg. samples / sec: 59046.12
Iteration:    980, Loss function: 5.869, Average Loss: 4.270, avg. samples / sec: 59041.19
Iteration:    980, Loss function: 5.580, Average Loss: 4.305, avg. samples / sec: 59270.88
Iteration:    980, Loss function: 5.375, Average Loss: 4.249, avg. samples / sec: 59129.38
Iteration:    980, Loss function: 5.146, Average Loss: 4.326, avg. samples / sec: 59149.83
Iteration:    980, Loss function: 6.183, Average Loss: 4.293, avg. samples / sec: 59080.25
Iteration:    980, Loss function: 5.642, Average Loss: 4.299, avg. samples / sec: 58771.98
Iteration:   1000, Loss function: 4.252, Average Loss: 4.311, avg. samples / sec: 55514.44
Iteration:   1000, Loss function: 4.814, Average Loss: 4.318, avg. samples / sec: 55688.46
Iteration:   1000, Loss function: 4.254, Average Loss: 4.301, avg. samples / sec: 55469.12
Iteration:   1000, Loss function: 4.172, Average Loss: 4.301, avg. samples / sec: 55518.68
Iteration:   1000, Loss function: 5.366, Average Loss: 4.282, avg. samples / sec: 55511.09
Iteration:   1000, Loss function: 4.908, Average Loss: 4.310, avg. samples / sec: 55455.83
Iteration:   1000, Loss function: 4.918, Average Loss: 4.310, avg. samples / sec: 55472.39
Iteration:   1000, Loss function: 5.468, Average Loss: 4.319, avg. samples / sec: 55491.29
Iteration:   1000, Loss function: 5.324, Average Loss: 4.326, avg. samples / sec: 55419.67
Iteration:   1000, Loss function: 5.696, Average Loss: 4.329, avg. samples / sec: 55391.13
Iteration:   1000, Loss function: 5.167, Average Loss: 4.312, avg. samples / sec: 55434.88
Iteration:   1000, Loss function: 5.600, Average Loss: 4.304, avg. samples / sec: 55433.14
Iteration:   1000, Loss function: 4.153, Average Loss: 4.303, avg. samples / sec: 55201.59
Iteration:   1000, Loss function: 6.402, Average Loss: 4.263, avg. samples / sec: 55372.29
Iteration:   1000, Loss function: 5.292, Average Loss: 4.340, avg. samples / sec: 55325.72
Iteration:   1020, Loss function: 6.282, Average Loss: 4.324, avg. samples / sec: 58638.28
Iteration:   1020, Loss function: 4.694, Average Loss: 4.317, avg. samples / sec: 58747.99
Iteration:   1020, Loss function: 5.812, Average Loss: 4.321, avg. samples / sec: 58741.62
Iteration:   1020, Loss function: 5.167, Average Loss: 4.343, avg. samples / sec: 58654.37
Iteration:   1020, Loss function: 5.695, Average Loss: 4.327, avg. samples / sec: 58648.58
Iteration:   1020, Loss function: 4.860, Average Loss: 4.331, avg. samples / sec: 58541.19
Iteration:   1020, Loss function: 4.847, Average Loss: 4.298, avg. samples / sec: 58508.16
Iteration:   1020, Loss function: 5.514, Average Loss: 4.346, avg. samples / sec: 58558.44
Iteration:   1020, Loss function: 4.753, Average Loss: 4.275, avg. samples / sec: 58655.30
Iteration:   1020, Loss function: 5.737, Average Loss: 4.319, avg. samples / sec: 58468.48
Iteration:   1020, Loss function: 5.129, Average Loss: 4.357, avg. samples / sec: 58702.96
Iteration:   1020, Loss function: 5.764, Average Loss: 4.318, avg. samples / sec: 58435.77
Iteration:   1020, Loss function: 4.432, Average Loss: 4.328, avg. samples / sec: 58351.74
Iteration:   1020, Loss function: 6.381, Average Loss: 4.340, avg. samples / sec: 58396.11
Iteration:   1020, Loss function: 5.666, Average Loss: 4.326, avg. samples / sec: 58385.75
Iteration:   1040, Loss function: 5.988, Average Loss: 4.343, avg. samples / sec: 58437.93
Iteration:   1040, Loss function: 4.684, Average Loss: 4.363, avg. samples / sec: 58297.26
Iteration:   1040, Loss function: 4.885, Average Loss: 4.334, avg. samples / sec: 58286.58
Iteration:   1040, Loss function: 4.851, Average Loss: 4.363, avg. samples / sec: 58095.89
Iteration:   1040, Loss function: 4.799, Average Loss: 4.361, avg. samples / sec: 58275.64
Iteration:   1040, Loss function: 4.890, Average Loss: 4.342, avg. samples / sec: 58028.48
Iteration:   1040, Loss function: 4.172, Average Loss: 4.340, avg. samples / sec: 58032.21
Iteration:   1040, Loss function: 6.042, Average Loss: 4.353, avg. samples / sec: 58047.01
Iteration:   1040, Loss function: 4.864, Average Loss: 4.291, avg. samples / sec: 58134.02
Iteration:   1040, Loss function: 5.406, Average Loss: 4.367, avg. samples / sec: 58088.88
Iteration:   1040, Loss function: 4.379, Average Loss: 4.329, avg. samples / sec: 57976.78
Iteration:   1040, Loss function: 5.487, Average Loss: 4.344, avg. samples / sec: 57994.98
Iteration:   1040, Loss function: 4.798, Average Loss: 4.343, avg. samples / sec: 58113.31
Iteration:   1040, Loss function: 4.251, Average Loss: 4.322, avg. samples / sec: 58025.52
Iteration:   1040, Loss function: 5.738, Average Loss: 4.340, avg. samples / sec: 58048.44
:::MLL 1558638972.580 epoch_stop: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 819}}
:::MLL 1558638972.581 epoch_start: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 673}}
Iteration:   1060, Loss function: 4.673, Average Loss: 4.303, avg. samples / sec: 60269.30
Iteration:   1060, Loss function: 5.381, Average Loss: 4.356, avg. samples / sec: 60322.94
Iteration:   1060, Loss function: 4.760, Average Loss: 4.363, avg. samples / sec: 60221.55
Iteration:   1060, Loss function: 3.666, Average Loss: 4.381, avg. samples / sec: 60214.04
Iteration:   1060, Loss function: 5.818, Average Loss: 4.377, avg. samples / sec: 60016.05
Iteration:   1060, Loss function: 5.615, Average Loss: 4.374, avg. samples / sec: 60119.77
Iteration:   1060, Loss function: 6.012, Average Loss: 4.372, avg. samples / sec: 60046.56
Iteration:   1060, Loss function: 5.022, Average Loss: 4.344, avg. samples / sec: 60103.39
Iteration:   1060, Loss function: 4.788, Average Loss: 4.335, avg. samples / sec: 60152.44
Iteration:   1060, Loss function: 4.086, Average Loss: 4.358, avg. samples / sec: 60097.49
Iteration:   1060, Loss function: 4.856, Average Loss: 4.354, avg. samples / sec: 59827.36
Iteration:   1060, Loss function: 4.829, Average Loss: 4.345, avg. samples / sec: 59897.13
Iteration:   1060, Loss function: 5.721, Average Loss: 4.358, avg. samples / sec: 59965.44
Iteration:   1060, Loss function: 5.791, Average Loss: 4.347, avg. samples / sec: 59996.02
Iteration:   1060, Loss function: 4.049, Average Loss: 4.350, avg. samples / sec: 59154.90
Iteration:   1080, Loss function: 4.507, Average Loss: 4.362, avg. samples / sec: 59126.60
Iteration:   1080, Loss function: 5.407, Average Loss: 4.389, avg. samples / sec: 58954.35
Iteration:   1080, Loss function: 4.804, Average Loss: 4.371, avg. samples / sec: 58788.33
Iteration:   1080, Loss function: 4.876, Average Loss: 4.361, avg. samples / sec: 58979.20
Iteration:   1080, Loss function: 5.135, Average Loss: 4.373, avg. samples / sec: 58933.64
Iteration:   1080, Loss function: 4.718, Average Loss: 4.373, avg. samples / sec: 58962.74
Iteration:   1080, Loss function: 4.779, Average Loss: 4.397, avg. samples / sec: 58768.79
Iteration:   1080, Loss function: 4.471, Average Loss: 4.353, avg. samples / sec: 58876.35
Iteration:   1080, Loss function: 4.894, Average Loss: 4.314, avg. samples / sec: 58666.58
Iteration:   1080, Loss function: 5.496, Average Loss: 4.386, avg. samples / sec: 58725.91
Iteration:   1080, Loss function: 4.289, Average Loss: 4.384, avg. samples / sec: 58761.71
Iteration:   1080, Loss function: 4.814, Average Loss: 4.380, avg. samples / sec: 58665.89
Iteration:   1080, Loss function: 5.011, Average Loss: 4.358, avg. samples / sec: 58813.30
Iteration:   1080, Loss function: 4.013, Average Loss: 4.361, avg. samples / sec: 59593.20
Iteration:   1080, Loss function: 4.870, Average Loss: 4.356, avg. samples / sec: 58339.08
Iteration:   1100, Loss function: 5.934, Average Loss: 4.374, avg. samples / sec: 59668.87
Iteration:   1100, Loss function: 5.174, Average Loss: 4.326, avg. samples / sec: 59737.54
Iteration:   1100, Loss function: 5.633, Average Loss: 4.395, avg. samples / sec: 59750.50
Iteration:   1100, Loss function: 4.363, Average Loss: 4.396, avg. samples / sec: 59516.56
Iteration:   1100, Loss function: 5.899, Average Loss: 4.401, avg. samples / sec: 59726.07
Iteration:   1100, Loss function: 5.347, Average Loss: 4.396, avg. samples / sec: 59712.76
Iteration:   1100, Loss function: 4.430, Average Loss: 4.388, avg. samples / sec: 59570.73
Iteration:   1100, Loss function: 4.261, Average Loss: 4.404, avg. samples / sec: 59587.33
Iteration:   1100, Loss function: 4.652, Average Loss: 4.376, avg. samples / sec: 59399.51
Iteration:   1100, Loss function: 4.432, Average Loss: 4.383, avg. samples / sec: 59512.74
Iteration:   1100, Loss function: 5.453, Average Loss: 4.378, avg. samples / sec: 59642.12
Iteration:   1100, Loss function: 4.670, Average Loss: 4.391, avg. samples / sec: 59396.64
Iteration:   1100, Loss function: 4.306, Average Loss: 4.371, avg. samples / sec: 60137.34
Iteration:   1100, Loss function: 5.751, Average Loss: 4.363, avg. samples / sec: 59383.87
Iteration:   1100, Loss function: 4.220, Average Loss: 4.373, avg. samples / sec: 59437.17
:::MLL 1558638974.563 epoch_stop: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 819}}
:::MLL 1558638974.564 epoch_start: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 673}}
Iteration:   1120, Loss function: 5.033, Average Loss: 4.335, avg. samples / sec: 59399.09
Iteration:   1120, Loss function: 4.460, Average Loss: 4.413, avg. samples / sec: 59416.14
Iteration:   1120, Loss function: 5.712, Average Loss: 4.385, avg. samples / sec: 59658.13
Iteration:   1120, Loss function: 6.246, Average Loss: 4.384, avg. samples / sec: 59423.61
Iteration:   1120, Loss function: 3.638, Average Loss: 4.388, avg. samples / sec: 59491.71
Iteration:   1120, Loss function: 4.302, Average Loss: 4.391, avg. samples / sec: 59533.06
Iteration:   1120, Loss function: 4.374, Average Loss: 4.406, avg. samples / sec: 59558.92
Iteration:   1120, Loss function: 4.520, Average Loss: 4.402, avg. samples / sec: 59297.44
Iteration:   1120, Loss function: 5.811, Average Loss: 4.388, avg. samples / sec: 59223.01
Iteration:   1120, Loss function: 3.597, Average Loss: 4.402, avg. samples / sec: 59233.84
Iteration:   1120, Loss function: 4.272, Average Loss: 4.380, avg. samples / sec: 59451.21
Iteration:   1120, Loss function: 5.102, Average Loss: 4.402, avg. samples / sec: 59160.19
Iteration:   1120, Loss function: 4.950, Average Loss: 4.404, avg. samples / sec: 59163.22
Iteration:   1120, Loss function: 5.118, Average Loss: 4.418, avg. samples / sec: 59176.46
Iteration:   1120, Loss function: 4.441, Average Loss: 4.373, avg. samples / sec: 59151.64
Iteration:   1140, Loss function: 5.711, Average Loss: 4.395, avg. samples / sec: 59448.37
Iteration:   1140, Loss function: 4.750, Average Loss: 4.421, avg. samples / sec: 59469.35
Iteration:   1140, Loss function: 4.725, Average Loss: 4.418, avg. samples / sec: 59591.79
Iteration:   1140, Loss function: 4.637, Average Loss: 4.412, avg. samples / sec: 59472.06
Iteration:   1140, Loss function: 3.980, Average Loss: 4.419, avg. samples / sec: 59317.36
Iteration:   1140, Loss function: 4.516, Average Loss: 4.413, avg. samples / sec: 59499.10
Iteration:   1140, Loss function: 4.212, Average Loss: 4.395, avg. samples / sec: 59318.63
Iteration:   1140, Loss function: 4.455, Average Loss: 4.395, avg. samples / sec: 59298.37
Iteration:   1140, Loss function: 4.899, Average Loss: 4.387, avg. samples / sec: 59796.11
Iteration:   1140, Loss function: 5.567, Average Loss: 4.344, avg. samples / sec: 59204.40
Iteration:   1140, Loss function: 5.538, Average Loss: 4.392, avg. samples / sec: 59460.11
Iteration:   1140, Loss function: 4.455, Average Loss: 4.411, avg. samples / sec: 59413.36
Iteration:   1140, Loss function: 5.019, Average Loss: 4.394, avg. samples / sec: 59338.99
Iteration:   1140, Loss function: 4.980, Average Loss: 4.404, avg. samples / sec: 59253.76
Iteration:   1140, Loss function: 5.220, Average Loss: 4.429, avg. samples / sec: 59419.35
Iteration:   1160, Loss function: 4.637, Average Loss: 4.431, avg. samples / sec: 58089.72
Iteration:   1160, Loss function: 4.034, Average Loss: 4.355, avg. samples / sec: 58192.40
Iteration:   1160, Loss function: 4.541, Average Loss: 4.400, avg. samples / sec: 58168.53
Iteration:   1160, Loss function: 5.425, Average Loss: 4.428, avg. samples / sec: 58108.35
Iteration:   1160, Loss function: 5.035, Average Loss: 4.407, avg. samples / sec: 57987.63
Iteration:   1160, Loss function: 4.938, Average Loss: 4.427, avg. samples / sec: 58032.57
Iteration:   1160, Loss function: 6.050, Average Loss: 4.408, avg. samples / sec: 58095.66
Iteration:   1160, Loss function: 4.534, Average Loss: 4.440, avg. samples / sec: 58231.11
Iteration:   1160, Loss function: 5.161, Average Loss: 4.431, avg. samples / sec: 57938.19
Iteration:   1160, Loss function: 4.681, Average Loss: 4.417, avg. samples / sec: 58145.15
Iteration:   1160, Loss function: 5.224, Average Loss: 4.424, avg. samples / sec: 58093.76
Iteration:   1160, Loss function: 4.406, Average Loss: 4.426, avg. samples / sec: 57951.00
Iteration:   1160, Loss function: 4.518, Average Loss: 4.399, avg. samples / sec: 58005.22
Iteration:   1160, Loss function: 5.722, Average Loss: 4.402, avg. samples / sec: 58003.65
Iteration:   1160, Loss function: 4.599, Average Loss: 4.409, avg. samples / sec: 58029.89
Iteration:   1180, Loss function: 5.687, Average Loss: 4.436, avg. samples / sec: 59063.27
Iteration:   1180, Loss function: 4.052, Average Loss: 4.408, avg. samples / sec: 58875.47
Iteration:   1180, Loss function: 4.974, Average Loss: 4.414, avg. samples / sec: 59014.62
Iteration:   1180, Loss function: 5.152, Average Loss: 4.409, avg. samples / sec: 58924.68
Iteration:   1180, Loss function: 5.554, Average Loss: 4.436, avg. samples / sec: 58800.71
Iteration:   1180, Loss function: 4.714, Average Loss: 4.437, avg. samples / sec: 58865.11
Iteration:   1180, Loss function: 5.199, Average Loss: 4.438, avg. samples / sec: 58708.91
Iteration:   1180, Loss function: 4.952, Average Loss: 4.365, avg. samples / sec: 58694.75
Iteration:   1180, Loss function: 6.819, Average Loss: 4.411, avg. samples / sec: 58889.49
Iteration:   1180, Loss function: 4.923, Average Loss: 4.415, avg. samples / sec: 58749.29
Iteration:   1180, Loss function: 4.930, Average Loss: 4.448, avg. samples / sec: 58759.80
Iteration:   1180, Loss function: 4.210, Average Loss: 4.415, avg. samples / sec: 58745.49
Iteration:   1180, Loss function: 5.222, Average Loss: 4.431, avg. samples / sec: 58719.60
Iteration:   1180, Loss function: 6.079, Average Loss: 4.439, avg. samples / sec: 58734.87
Iteration:   1180, Loss function: 4.371, Average Loss: 4.423, avg. samples / sec: 58743.36
:::MLL 1558638976.559 epoch_stop: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 819}}
:::MLL 1558638976.560 epoch_start: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 673}}
Iteration:   1200, Loss function: 5.847, Average Loss: 4.427, avg. samples / sec: 58903.42
Iteration:   1200, Loss function: 5.871, Average Loss: 4.443, avg. samples / sec: 58863.49
Iteration:   1200, Loss function: 5.299, Average Loss: 4.422, avg. samples / sec: 58840.44
Iteration:   1200, Loss function: 6.532, Average Loss: 4.448, avg. samples / sec: 58755.07
Iteration:   1200, Loss function: 5.107, Average Loss: 4.419, avg. samples / sec: 58575.33
Iteration:   1200, Loss function: 5.732, Average Loss: 4.428, avg. samples / sec: 58745.64
Iteration:   1200, Loss function: 4.201, Average Loss: 4.446, avg. samples / sec: 58794.85
Iteration:   1200, Loss function: 5.618, Average Loss: 4.378, avg. samples / sec: 58682.26
Iteration:   1200, Loss function: 5.708, Average Loss: 4.424, avg. samples / sec: 58559.22
Iteration:   1200, Loss function: 5.156, Average Loss: 4.459, avg. samples / sec: 58650.63
Iteration:   1200, Loss function: 4.631, Average Loss: 4.440, avg. samples / sec: 58588.85
Iteration:   1200, Loss function: 5.892, Average Loss: 4.422, avg. samples / sec: 58620.58
Iteration:   1200, Loss function: 5.257, Average Loss: 4.445, avg. samples / sec: 58575.16
Iteration:   1200, Loss function: 4.410, Average Loss: 4.444, avg. samples / sec: 58369.09
Iteration:   1200, Loss function: 5.718, Average Loss: 4.422, avg. samples / sec: 58404.56
Iteration:   1220, Loss function: 4.939, Average Loss: 4.460, avg. samples / sec: 56547.92
Iteration:   1220, Loss function: 4.856, Average Loss: 4.433, avg. samples / sec: 56338.99
Iteration:   1220, Loss function: 4.951, Average Loss: 4.430, avg. samples / sec: 56510.87
Iteration:   1220, Loss function: 6.187, Average Loss: 4.436, avg. samples / sec: 56441.41
Iteration:   1220, Loss function: 4.758, Average Loss: 4.472, avg. samples / sec: 56469.66
Iteration:   1220, Loss function: 4.320, Average Loss: 4.457, avg. samples / sec: 56492.20
Iteration:   1220, Loss function: 5.725, Average Loss: 4.432, avg. samples / sec: 56339.06
Iteration:   1220, Loss function: 4.331, Average Loss: 4.434, avg. samples / sec: 56345.57
Iteration:   1220, Loss function: 5.758, Average Loss: 4.436, avg. samples / sec: 56607.27
Iteration:   1220, Loss function: 4.486, Average Loss: 4.455, avg. samples / sec: 56450.27
Iteration:   1220, Loss function: 5.370, Average Loss: 4.441, avg. samples / sec: 56209.25
Iteration:   1220, Loss function: 3.583, Average Loss: 4.448, avg. samples / sec: 56226.58
Iteration:   1220, Loss function: 4.867, Average Loss: 4.457, avg. samples / sec: 56217.57
Iteration:   1220, Loss function: 5.329, Average Loss: 4.453, avg. samples / sec: 56347.46
Iteration:   1220, Loss function: 3.413, Average Loss: 4.390, avg. samples / sec: 56180.93
Iteration:   1240, Loss function: 5.276, Average Loss: 4.457, avg. samples / sec: 60318.62
Iteration:   1240, Loss function: 4.662, Average Loss: 4.468, avg. samples / sec: 60007.65
Iteration:   1240, Loss function: 5.641, Average Loss: 4.455, avg. samples / sec: 60254.05
Iteration:   1240, Loss function: 4.447, Average Loss: 4.438, avg. samples / sec: 60093.34
Iteration:   1240, Loss function: 5.088, Average Loss: 4.448, avg. samples / sec: 60198.09
Iteration:   1240, Loss function: 4.486, Average Loss: 4.388, avg. samples / sec: 60364.20
Iteration:   1240, Loss function: 4.219, Average Loss: 4.478, avg. samples / sec: 60065.40
Iteration:   1240, Loss function: 4.340, Average Loss: 4.440, avg. samples / sec: 60002.46
Iteration:   1240, Loss function: 4.258, Average Loss: 4.439, avg. samples / sec: 60029.91
Iteration:   1240, Loss function: 5.434, Average Loss: 4.464, avg. samples / sec: 60104.95
Iteration:   1240, Loss function: 4.416, Average Loss: 4.444, avg. samples / sec: 59993.72
Iteration:   1240, Loss function: 4.678, Average Loss: 4.458, avg. samples / sec: 60014.72
Iteration:   1240, Loss function: 5.829, Average Loss: 4.446, avg. samples / sec: 59993.47
Iteration:   1240, Loss function: 4.620, Average Loss: 4.438, avg. samples / sec: 59822.54
Iteration:   1240, Loss function: 4.999, Average Loss: 4.464, avg. samples / sec: 59838.28
:::MLL 1558638978.571 epoch_stop: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 819}}
:::MLL 1558638978.571 epoch_start: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 673}}
Iteration:   1260, Loss function: 4.259, Average Loss: 4.453, avg. samples / sec: 59679.35
Iteration:   1260, Loss function: 4.609, Average Loss: 4.450, avg. samples / sec: 59814.11
Iteration:   1260, Loss function: 4.051, Average Loss: 4.473, avg. samples / sec: 59521.77
Iteration:   1260, Loss function: 4.048, Average Loss: 4.482, avg. samples / sec: 59624.08
Iteration:   1260, Loss function: 4.722, Average Loss: 4.458, avg. samples / sec: 59717.77
Iteration:   1260, Loss function: 5.291, Average Loss: 4.472, avg. samples / sec: 59691.31
Iteration:   1260, Loss function: 4.949, Average Loss: 4.468, avg. samples / sec: 59815.99
Iteration:   1260, Loss function: 4.603, Average Loss: 4.456, avg. samples / sec: 59694.77
Iteration:   1260, Loss function: 5.130, Average Loss: 4.449, avg. samples / sec: 59503.67
Iteration:   1260, Loss function: 3.709, Average Loss: 4.442, avg. samples / sec: 59617.25
Iteration:   1260, Loss function: 5.523, Average Loss: 4.391, avg. samples / sec: 59517.09
Iteration:   1260, Loss function: 4.659, Average Loss: 4.464, avg. samples / sec: 59379.64
Iteration:   1260, Loss function: 5.073, Average Loss: 4.446, avg. samples / sec: 59537.91
Iteration:   1260, Loss function: 5.233, Average Loss: 4.459, avg. samples / sec: 59366.33
Iteration:   1260, Loss function: 3.905, Average Loss: 4.445, avg. samples / sec: 59654.65
Iteration:   1280, Loss function: 4.876, Average Loss: 4.473, avg. samples / sec: 59830.76
Iteration:   1280, Loss function: 5.126, Average Loss: 4.457, avg. samples / sec: 59733.84
Iteration:   1280, Loss function: 4.214, Average Loss: 4.493, avg. samples / sec: 59670.13
Iteration:   1280, Loss function: 4.534, Average Loss: 4.478, avg. samples / sec: 59635.08
Iteration:   1280, Loss function: 4.783, Average Loss: 4.478, avg. samples / sec: 59584.05
Iteration:   1280, Loss function: 5.450, Average Loss: 4.472, avg. samples / sec: 59588.79
Iteration:   1280, Loss function: 4.458, Average Loss: 4.394, avg. samples / sec: 59670.91
Iteration:   1280, Loss function: 5.832, Average Loss: 4.461, avg. samples / sec: 59477.30
Iteration:   1280, Loss function: 4.774, Average Loss: 4.459, avg. samples / sec: 59457.33
Iteration:   1280, Loss function: 4.845, Average Loss: 4.455, avg. samples / sec: 59736.88
Iteration:   1280, Loss function: 4.666, Average Loss: 4.474, avg. samples / sec: 59585.92
Iteration:   1280, Loss function: 6.139, Average Loss: 4.453, avg. samples / sec: 59689.46
Iteration:   1280, Loss function: 6.167, Average Loss: 4.459, avg. samples / sec: 59601.87
Iteration:   1280, Loss function: 5.691, Average Loss: 4.471, avg. samples / sec: 59574.03
Iteration:   1280, Loss function: 4.608, Average Loss: 4.462, avg. samples / sec: 59391.13
Iteration:   1300, Loss function: 4.407, Average Loss: 4.501, avg. samples / sec: 60283.40
Iteration:   1300, Loss function: 4.038, Average Loss: 4.463, avg. samples / sec: 60366.32
Iteration:   1300, Loss function: 4.335, Average Loss: 4.468, avg. samples / sec: 60501.91
Iteration:   1300, Loss function: 4.279, Average Loss: 4.466, avg. samples / sec: 60277.24
Iteration:   1300, Loss function: 5.319, Average Loss: 4.490, avg. samples / sec: 60224.98
Iteration:   1300, Loss function: 5.084, Average Loss: 4.402, avg. samples / sec: 60227.55
Iteration:   1300, Loss function: 4.692, Average Loss: 4.484, avg. samples / sec: 60189.61
Iteration:   1300, Loss function: 4.074, Average Loss: 4.470, avg. samples / sec: 60207.28
Iteration:   1300, Loss function: 3.222, Average Loss: 4.465, avg. samples / sec: 60250.98
Iteration:   1300, Loss function: 5.934, Average Loss: 4.475, avg. samples / sec: 60174.86
Iteration:   1300, Loss function: 4.598, Average Loss: 4.481, avg. samples / sec: 59984.63
Iteration:   1300, Loss function: 5.147, Average Loss: 4.465, avg. samples / sec: 60120.97
Iteration:   1300, Loss function: 4.891, Average Loss: 4.462, avg. samples / sec: 59991.40
Iteration:   1300, Loss function: 4.509, Average Loss: 4.476, avg. samples / sec: 60231.05
Iteration:   1300, Loss function: 4.131, Average Loss: 4.481, avg. samples / sec: 60089.19
Iteration:   1320, Loss function: 4.529, Average Loss: 4.482, avg. samples / sec: 57683.07
Iteration:   1320, Loss function: 5.300, Average Loss: 4.470, avg. samples / sec: 57695.37
Iteration:   1320, Loss function: 5.141, Average Loss: 4.473, avg. samples / sec: 57550.75
Iteration:   1320, Loss function: 4.866, Average Loss: 4.503, avg. samples / sec: 57439.80
Iteration:   1320, Loss function: 4.375, Average Loss: 4.485, avg. samples / sec: 57720.11
Iteration:   1320, Loss function: 4.254, Average Loss: 4.490, avg. samples / sec: 57674.95
Iteration:   1320, Loss function: 4.915, Average Loss: 4.412, avg. samples / sec: 57556.89
Iteration:   1320, Loss function: 3.651, Average Loss: 4.472, avg. samples / sec: 57578.71
Iteration:   1320, Loss function: 5.864, Average Loss: 4.495, avg. samples / sec: 57521.93
Iteration:   1320, Loss function: 4.462, Average Loss: 4.489, avg. samples / sec: 57535.57
Iteration:   1320, Loss function: 4.684, Average Loss: 4.485, avg. samples / sec: 57673.27
Iteration:   1320, Loss function: 4.900, Average Loss: 4.472, avg. samples / sec: 57416.03
Iteration:   1320, Loss function: 4.289, Average Loss: 4.466, avg. samples / sec: 57418.37
Iteration:   1320, Loss function: 4.884, Average Loss: 4.469, avg. samples / sec: 57557.66
Iteration:   1320, Loss function: 3.822, Average Loss: 4.475, avg. samples / sec: 57452.47
:::MLL 1558638980.555 epoch_stop: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 819}}
:::MLL 1558638980.555 epoch_start: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 673}}
Iteration:   1340, Loss function: 4.688, Average Loss: 4.414, avg. samples / sec: 60348.51
Iteration:   1340, Loss function: 5.273, Average Loss: 4.474, avg. samples / sec: 60294.26
Iteration:   1340, Loss function: 5.083, Average Loss: 4.490, avg. samples / sec: 60143.68
Iteration:   1340, Loss function: 4.412, Average Loss: 4.491, avg. samples / sec: 60210.00
Iteration:   1340, Loss function: 4.450, Average Loss: 4.483, avg. samples / sec: 60330.81
Iteration:   1340, Loss function: 3.918, Average Loss: 4.474, avg. samples / sec: 60193.41
Iteration:   1340, Loss function: 5.493, Average Loss: 4.491, avg. samples / sec: 60175.76
Iteration:   1340, Loss function: 3.904, Average Loss: 4.494, avg. samples / sec: 60178.30
Iteration:   1340, Loss function: 3.710, Average Loss: 4.474, avg. samples / sec: 60163.86
Iteration:   1340, Loss function: 3.778, Average Loss: 4.500, avg. samples / sec: 60112.56
Iteration:   1340, Loss function: 5.876, Average Loss: 4.488, avg. samples / sec: 60116.64
Iteration:   1340, Loss function: 5.043, Average Loss: 4.482, avg. samples / sec: 59994.08
Iteration:   1340, Loss function: 4.320, Average Loss: 4.472, avg. samples / sec: 60146.86
Iteration:   1340, Loss function: 4.178, Average Loss: 4.505, avg. samples / sec: 59952.58
Iteration:   1340, Loss function: 6.246, Average Loss: 4.471, avg. samples / sec: 59902.97
Iteration:   1360, Loss function: 5.117, Average Loss: 4.493, avg. samples / sec: 59068.46
Iteration:   1360, Loss function: 5.246, Average Loss: 4.472, avg. samples / sec: 59475.60
Iteration:   1360, Loss function: 4.040, Average Loss: 4.488, avg. samples / sec: 59081.84
Iteration:   1360, Loss function: 4.972, Average Loss: 4.486, avg. samples / sec: 59239.24
Iteration:   1360, Loss function: 6.513, Average Loss: 4.492, avg. samples / sec: 58968.59
Iteration:   1360, Loss function: 4.899, Average Loss: 4.419, avg. samples / sec: 58854.03
Iteration:   1360, Loss function: 4.412, Average Loss: 4.491, avg. samples / sec: 58959.58
Iteration:   1360, Loss function: 3.348, Average Loss: 4.475, avg. samples / sec: 58815.19
Iteration:   1360, Loss function: 5.100, Average Loss: 4.476, avg. samples / sec: 58932.46
Iteration:   1360, Loss function: 4.352, Average Loss: 4.496, avg. samples / sec: 58964.57
Iteration:   1360, Loss function: 5.204, Average Loss: 4.490, avg. samples / sec: 59000.36
Iteration:   1360, Loss function: 4.998, Average Loss: 4.506, avg. samples / sec: 58963.88
Iteration:   1360, Loss function: 4.122, Average Loss: 4.507, avg. samples / sec: 59026.80
Iteration:   1360, Loss function: 4.739, Average Loss: 4.486, avg. samples / sec: 58967.77
Iteration:   1360, Loss function: 4.653, Average Loss: 4.472, avg. samples / sec: 58752.20
Iteration:   1380, Loss function: 5.020, Average Loss: 4.488, avg. samples / sec: 59784.65
Iteration:   1380, Loss function: 4.918, Average Loss: 4.504, avg. samples / sec: 59440.55
Iteration:   1380, Loss function: 4.786, Average Loss: 4.482, avg. samples / sec: 59590.10
Iteration:   1380, Loss function: 4.806, Average Loss: 4.482, avg. samples / sec: 59395.13
Iteration:   1380, Loss function: 4.494, Average Loss: 4.498, avg. samples / sec: 59444.46
Iteration:   1380, Loss function: 4.506, Average Loss: 4.492, avg. samples / sec: 59540.25
Iteration:   1380, Loss function: 5.503, Average Loss: 4.498, avg. samples / sec: 59447.72
Iteration:   1380, Loss function: 5.223, Average Loss: 4.498, avg. samples / sec: 59469.02
Iteration:   1380, Loss function: 4.780, Average Loss: 4.507, avg. samples / sec: 59511.06
Iteration:   1380, Loss function: 4.240, Average Loss: 4.510, avg. samples / sec: 59561.24
Iteration:   1380, Loss function: 4.249, Average Loss: 4.479, avg. samples / sec: 59414.92
Iteration:   1380, Loss function: 4.208, Average Loss: 4.491, avg. samples / sec: 59276.57
Iteration:   1380, Loss function: 4.860, Average Loss: 4.476, avg. samples / sec: 59658.81
Iteration:   1380, Loss function: 5.247, Average Loss: 4.488, avg. samples / sec: 59242.55
Iteration:   1380, Loss function: 3.607, Average Loss: 4.422, avg. samples / sec: 59308.50
:::MLL 1558638982.547 epoch_stop: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 819}}
:::MLL 1558638982.547 epoch_start: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 673}}
Iteration:   1400, Loss function: 4.181, Average Loss: 4.485, avg. samples / sec: 58421.38
Iteration:   1400, Loss function: 4.569, Average Loss: 4.492, avg. samples / sec: 58504.35
Iteration:   1400, Loss function: 4.285, Average Loss: 4.512, avg. samples / sec: 58417.34
Iteration:   1400, Loss function: 4.139, Average Loss: 4.492, avg. samples / sec: 58194.49
Iteration:   1400, Loss function: 4.446, Average Loss: 4.486, avg. samples / sec: 58265.61
Iteration:   1400, Loss function: 3.715, Average Loss: 4.424, avg. samples / sec: 58412.98
Iteration:   1400, Loss function: 5.175, Average Loss: 4.498, avg. samples / sec: 58326.91
Iteration:   1400, Loss function: 5.791, Average Loss: 4.511, avg. samples / sec: 58124.84
Iteration:   1400, Loss function: 6.133, Average Loss: 4.494, avg. samples / sec: 58219.74
Iteration:   1400, Loss function: 4.684, Average Loss: 4.515, avg. samples / sec: 58253.67
Iteration:   1400, Loss function: 4.776, Average Loss: 4.501, avg. samples / sec: 58133.85
Iteration:   1400, Loss function: 4.867, Average Loss: 4.503, avg. samples / sec: 58173.55
Iteration:   1400, Loss function: 6.285, Average Loss: 4.502, avg. samples / sec: 58189.78
Iteration:   1400, Loss function: 4.404, Average Loss: 4.484, avg. samples / sec: 58175.59
Iteration:   1400, Loss function: 4.988, Average Loss: 4.478, avg. samples / sec: 58171.94
Iteration:   1420, Loss function: 4.193, Average Loss: 4.501, avg. samples / sec: 56163.69
Iteration:   1420, Loss function: 4.386, Average Loss: 4.505, avg. samples / sec: 56045.93
Iteration:   1420, Loss function: 3.887, Average Loss: 4.504, avg. samples / sec: 56109.75
Iteration:   1420, Loss function: 4.271, Average Loss: 4.516, avg. samples / sec: 56022.47
Iteration:   1420, Loss function: 4.771, Average Loss: 4.496, avg. samples / sec: 56029.26
Iteration:   1420, Loss function: 4.159, Average Loss: 4.488, avg. samples / sec: 55752.00
Iteration:   1420, Loss function: 4.629, Average Loss: 4.428, avg. samples / sec: 55906.64
Iteration:   1420, Loss function: 3.864, Average Loss: 4.494, avg. samples / sec: 55816.23
Iteration:   1420, Loss function: 4.430, Average Loss: 4.516, avg. samples / sec: 55833.68
Iteration:   1420, Loss function: 3.484, Average Loss: 4.486, avg. samples / sec: 55874.39
Iteration:   1420, Loss function: 3.953, Average Loss: 4.475, avg. samples / sec: 56034.86
Iteration:   1420, Loss function: 4.158, Average Loss: 4.506, avg. samples / sec: 55916.91
Iteration:   1420, Loss function: 4.724, Average Loss: 4.483, avg. samples / sec: 55886.71
Iteration:   1420, Loss function: 4.100, Average Loss: 4.493, avg. samples / sec: 55703.91
Iteration:   1420, Loss function: 5.023, Average Loss: 4.518, avg. samples / sec: 55794.35
Iteration:   1440, Loss function: 4.181, Average Loss: 4.523, avg. samples / sec: 58578.03
Iteration:   1440, Loss function: 3.787, Average Loss: 4.510, avg. samples / sec: 58467.97
Iteration:   1440, Loss function: 5.006, Average Loss: 4.520, avg. samples / sec: 58564.35
Iteration:   1440, Loss function: 6.165, Average Loss: 4.502, avg. samples / sec: 58755.22
Iteration:   1440, Loss function: 4.741, Average Loss: 4.489, avg. samples / sec: 58513.85
Iteration:   1440, Loss function: 4.122, Average Loss: 4.492, avg. samples / sec: 58566.69
Iteration:   1440, Loss function: 4.789, Average Loss: 4.505, avg. samples / sec: 58667.99
Iteration:   1440, Loss function: 5.387, Average Loss: 4.514, avg. samples / sec: 58748.01
Iteration:   1440, Loss function: 3.828, Average Loss: 4.435, avg. samples / sec: 58493.11
Iteration:   1440, Loss function: 4.754, Average Loss: 4.504, avg. samples / sec: 58502.16
Iteration:   1440, Loss function: 5.417, Average Loss: 4.500, avg. samples / sec: 58417.68
Iteration:   1440, Loss function: 5.814, Average Loss: 4.492, avg. samples / sec: 58649.61
Iteration:   1440, Loss function: 4.246, Average Loss: 4.497, avg. samples / sec: 58279.28
Iteration:   1440, Loss function: 3.311, Average Loss: 4.474, avg. samples / sec: 58457.59
Iteration:   1440, Loss function: 6.066, Average Loss: 4.509, avg. samples / sec: 58245.92
Iteration:   1460, Loss function: 5.198, Average Loss: 4.507, avg. samples / sec: 57574.41
Iteration:   1460, Loss function: 4.346, Average Loss: 4.510, avg. samples / sec: 57481.11
Iteration:   1460, Loss function: 4.765, Average Loss: 4.504, avg. samples / sec: 57521.95
Iteration:   1460, Loss function: 3.579, Average Loss: 4.523, avg. samples / sec: 57439.85
Iteration:   1460, Loss function: 4.523, Average Loss: 4.491, avg. samples / sec: 57412.68
Iteration:   1460, Loss function: 3.625, Average Loss: 4.509, avg. samples / sec: 57391.80
Iteration:   1460, Loss function: 4.054, Average Loss: 4.524, avg. samples / sec: 57314.22
Iteration:   1460, Loss function: 5.513, Average Loss: 4.514, avg. samples / sec: 57609.10
Iteration:   1460, Loss function: 4.427, Average Loss: 4.499, avg. samples / sec: 57511.09
Iteration:   1460, Loss function: 3.611, Average Loss: 4.473, avg. samples / sec: 57525.12
Iteration:   1460, Loss function: 4.758, Average Loss: 4.505, avg. samples / sec: 57366.34
Iteration:   1460, Loss function: 4.973, Average Loss: 4.495, avg. samples / sec: 57357.02
Iteration:   1460, Loss function: 4.416, Average Loss: 4.523, avg. samples / sec: 57318.72
Iteration:   1460, Loss function: 4.172, Average Loss: 4.438, avg. samples / sec: 57323.26
Iteration:   1460, Loss function: 5.485, Average Loss: 4.493, avg. samples / sec: 57328.77
:::MLL 1558638984.585 epoch_stop: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 819}}
:::MLL 1558638984.585 epoch_start: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 673}}
Iteration:   1480, Loss function: 4.575, Average Loss: 4.527, avg. samples / sec: 59772.25
Iteration:   1480, Loss function: 4.149, Average Loss: 4.525, avg. samples / sec: 59518.42
Iteration:   1480, Loss function: 5.276, Average Loss: 4.510, avg. samples / sec: 59454.49
Iteration:   1480, Loss function: 4.394, Average Loss: 4.512, avg. samples / sec: 59561.39
Iteration:   1480, Loss function: 4.579, Average Loss: 4.518, avg. samples / sec: 59433.13
Iteration:   1480, Loss function: 5.088, Average Loss: 4.504, avg. samples / sec: 59476.57
Iteration:   1480, Loss function: 5.095, Average Loss: 4.503, avg. samples / sec: 59598.99
Iteration:   1480, Loss function: 5.580, Average Loss: 4.526, avg. samples / sec: 59506.33
Iteration:   1480, Loss function: 3.211, Average Loss: 4.493, avg. samples / sec: 59509.48
Iteration:   1480, Loss function: 5.036, Average Loss: 4.497, avg. samples / sec: 59525.54
Iteration:   1480, Loss function: 4.589, Average Loss: 4.477, avg. samples / sec: 59504.83
Iteration:   1480, Loss function: 4.862, Average Loss: 4.497, avg. samples / sec: 59451.84
Iteration:   1480, Loss function: 3.146, Average Loss: 4.436, avg. samples / sec: 59458.83
Iteration:   1480, Loss function: 4.019, Average Loss: 4.496, avg. samples / sec: 59490.26
Iteration:   1480, Loss function: 4.685, Average Loss: 4.506, avg. samples / sec: 59240.11
Iteration:   1500, Loss function: 5.868, Average Loss: 4.497, avg. samples / sec: 58628.58
Iteration:   1500, Loss function: 4.860, Average Loss: 4.501, avg. samples / sec: 58824.69
Iteration:   1500, Loss function: 4.560, Average Loss: 4.498, avg. samples / sec: 58687.05
Iteration:   1500, Loss function: 4.044, Average Loss: 4.437, avg. samples / sec: 58731.12
Iteration:   1500, Loss function: 4.683, Average Loss: 4.523, avg. samples / sec: 58531.35
Iteration:   1500, Loss function: 4.902, Average Loss: 4.509, avg. samples / sec: 58798.16
Iteration:   1500, Loss function: 3.783, Average Loss: 4.526, avg. samples / sec: 58374.15
Iteration:   1500, Loss function: 3.991, Average Loss: 4.501, avg. samples / sec: 58504.91
Iteration:   1500, Loss function: 5.833, Average Loss: 4.508, avg. samples / sec: 58444.76
Iteration:   1500, Loss function: 4.524, Average Loss: 4.502, avg. samples / sec: 58438.53
Iteration:   1500, Loss function: 4.941, Average Loss: 4.477, avg. samples / sec: 58491.14
Iteration:   1500, Loss function: 3.977, Average Loss: 4.519, avg. samples / sec: 58394.85
Iteration:   1500, Loss function: 4.566, Average Loss: 4.508, avg. samples / sec: 58386.58
Iteration:   1500, Loss function: 4.308, Average Loss: 4.524, avg. samples / sec: 58327.95
Iteration:   1500, Loss function: 4.893, Average Loss: 4.515, avg. samples / sec: 58297.62
Iteration:   1520, Loss function: 4.181, Average Loss: 4.496, avg. samples / sec: 60110.59
Iteration:   1520, Loss function: 3.614, Average Loss: 4.500, avg. samples / sec: 60024.36
Iteration:   1520, Loss function: 4.120, Average Loss: 4.508, avg. samples / sec: 60076.23
Iteration:   1520, Loss function: 4.534, Average Loss: 4.495, avg. samples / sec: 59926.48
Iteration:   1520, Loss function: 3.987, Average Loss: 4.439, avg. samples / sec: 60004.53
Iteration:   1520, Loss function: 4.275, Average Loss: 4.503, avg. samples / sec: 60047.76
Iteration:   1520, Loss function: 3.795, Average Loss: 4.510, avg. samples / sec: 60025.59
Iteration:   1520, Loss function: 5.814, Average Loss: 4.516, avg. samples / sec: 60029.78
Iteration:   1520, Loss function: 5.930, Average Loss: 4.511, avg. samples / sec: 60032.95
Iteration:   1520, Loss function: 4.673, Average Loss: 4.525, avg. samples / sec: 59923.57
Iteration:   1520, Loss function: 3.737, Average Loss: 4.521, avg. samples / sec: 59841.16
Iteration:   1520, Loss function: 2.970, Average Loss: 4.509, avg. samples / sec: 60043.39
Iteration:   1520, Loss function: 4.617, Average Loss: 4.526, avg. samples / sec: 59942.89
Iteration:   1520, Loss function: 4.962, Average Loss: 4.504, avg. samples / sec: 59844.28
Iteration:   1520, Loss function: 5.214, Average Loss: 4.481, avg. samples / sec: 59799.23
:::MLL 1558638986.574 epoch_stop: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 819}}
:::MLL 1558638986.575 epoch_start: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 673}}
Iteration:   1540, Loss function: 4.422, Average Loss: 4.522, avg. samples / sec: 59406.17
Iteration:   1540, Loss function: 4.263, Average Loss: 4.495, avg. samples / sec: 58996.93
Iteration:   1540, Loss function: 4.899, Average Loss: 4.514, avg. samples / sec: 59187.72
Iteration:   1540, Loss function: 5.171, Average Loss: 4.509, avg. samples / sec: 59249.90
Iteration:   1540, Loss function: 3.448, Average Loss: 4.506, avg. samples / sec: 59117.60
Iteration:   1540, Loss function: 3.682, Average Loss: 4.503, avg. samples / sec: 59015.95
Iteration:   1540, Loss function: 3.836, Average Loss: 4.526, avg. samples / sec: 59169.75
Iteration:   1540, Loss function: 5.641, Average Loss: 4.497, avg. samples / sec: 59044.48
Iteration:   1540, Loss function: 4.771, Average Loss: 4.500, avg. samples / sec: 59017.88
Iteration:   1540, Loss function: 5.256, Average Loss: 4.508, avg. samples / sec: 59220.45
Iteration:   1540, Loss function: 3.977, Average Loss: 4.478, avg. samples / sec: 59292.30
Iteration:   1540, Loss function: 4.533, Average Loss: 4.513, avg. samples / sec: 59081.59
Iteration:   1540, Loss function: 4.591, Average Loss: 4.497, avg. samples / sec: 58874.61
Iteration:   1540, Loss function: 5.020, Average Loss: 4.445, avg. samples / sec: 58934.31
Iteration:   1540, Loss function: 4.471, Average Loss: 4.524, avg. samples / sec: 59112.99
Iteration:   1560, Loss function: 4.408, Average Loss: 4.498, avg. samples / sec: 58307.56
Iteration:   1560, Loss function: 4.165, Average Loss: 4.502, avg. samples / sec: 58226.25
Iteration:   1560, Loss function: 6.102, Average Loss: 4.480, avg. samples / sec: 58281.08
Iteration:   1560, Loss function: 4.200, Average Loss: 4.451, avg. samples / sec: 58342.37
Iteration:   1560, Loss function: 5.546, Average Loss: 4.514, avg. samples / sec: 58285.28
Iteration:   1560, Loss function: 3.172, Average Loss: 4.500, avg. samples / sec: 58175.23
Iteration:   1560, Loss function: 4.852, Average Loss: 4.495, avg. samples / sec: 58262.24
Iteration:   1560, Loss function: 3.573, Average Loss: 4.500, avg. samples / sec: 58188.73
Iteration:   1560, Loss function: 4.972, Average Loss: 4.525, avg. samples / sec: 57972.24
Iteration:   1560, Loss function: 4.854, Average Loss: 4.523, avg. samples / sec: 58245.12
Iteration:   1560, Loss function: 4.179, Average Loss: 4.510, avg. samples / sec: 58125.63
Iteration:   1560, Loss function: 4.982, Average Loss: 4.496, avg. samples / sec: 57973.68
Iteration:   1560, Loss function: 4.382, Average Loss: 4.528, avg. samples / sec: 58043.73
Iteration:   1560, Loss function: 4.706, Average Loss: 4.506, avg. samples / sec: 58026.33
Iteration:   1560, Loss function: 4.511, Average Loss: 4.511, avg. samples / sec: 57843.30
Iteration:   1580, Loss function: 4.652, Average Loss: 4.533, avg. samples / sec: 60847.48
Iteration:   1580, Loss function: 3.851, Average Loss: 4.497, avg. samples / sec: 60695.02
Iteration:   1580, Loss function: 4.829, Average Loss: 4.496, avg. samples / sec: 60728.99
Iteration:   1580, Loss function: 3.801, Average Loss: 4.510, avg. samples / sec: 60547.09
Iteration:   1580, Loss function: 5.153, Average Loss: 4.514, avg. samples / sec: 60888.44
Iteration:   1580, Loss function: 4.712, Average Loss: 4.523, avg. samples / sec: 60604.58
Iteration:   1580, Loss function: 4.238, Average Loss: 4.500, avg. samples / sec: 60520.15
Iteration:   1580, Loss function: 3.889, Average Loss: 4.518, avg. samples / sec: 60616.10
Iteration:   1580, Loss function: 4.728, Average Loss: 4.501, avg. samples / sec: 60399.69
Iteration:   1580, Loss function: 5.552, Average Loss: 4.497, avg. samples / sec: 60452.83
Iteration:   1580, Loss function: 5.091, Average Loss: 4.447, avg. samples / sec: 60376.48
Iteration:   1580, Loss function: 4.291, Average Loss: 4.514, avg. samples / sec: 60449.20
Iteration:   1580, Loss function: 4.837, Average Loss: 4.497, avg. samples / sec: 60193.57
Iteration:   1580, Loss function: 4.804, Average Loss: 4.504, avg. samples / sec: 60383.57
Iteration:   1580, Loss function: 4.672, Average Loss: 4.484, avg. samples / sec: 60190.59
Iteration:   1600, Loss function: 4.016, Average Loss: 4.485, avg. samples / sec: 58735.63
Iteration:   1600, Loss function: 4.042, Average Loss: 4.514, avg. samples / sec: 58622.23
Iteration:   1600, Loss function: 4.470, Average Loss: 4.500, avg. samples / sec: 58465.23
Iteration:   1600, Loss function: 3.667, Average Loss: 4.521, avg. samples / sec: 58353.87
Iteration:   1600, Loss function: 4.840, Average Loss: 4.499, avg. samples / sec: 58573.16
Iteration:   1600, Loss function: 4.178, Average Loss: 4.529, avg. samples / sec: 58208.00
Iteration:   1600, Loss function: 5.933, Average Loss: 4.450, avg. samples / sec: 58453.46
Iteration:   1600, Loss function: 3.888, Average Loss: 4.493, avg. samples / sec: 58248.95
Iteration:   1600, Loss function: 4.369, Average Loss: 4.500, avg. samples / sec: 58424.55
Iteration:   1600, Loss function: 5.367, Average Loss: 4.506, avg. samples / sec: 58276.22
Iteration:   1600, Loss function: 4.825, Average Loss: 4.504, avg. samples / sec: 58307.56
Iteration:   1600, Loss function: 5.369, Average Loss: 4.523, avg. samples / sec: 58301.77
Iteration:   1600, Loss function: 3.771, Average Loss: 4.519, avg. samples / sec: 58246.16
Iteration:   1600, Loss function: 4.731, Average Loss: 4.501, avg. samples / sec: 58481.29
Iteration:   1600, Loss function: 5.116, Average Loss: 4.494, avg. samples / sec: 58084.64
:::MLL 1558638988.562 epoch_stop: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 819}}
:::MLL 1558638988.562 epoch_start: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 673}}
Iteration:   1620, Loss function: 3.484, Average Loss: 4.497, avg. samples / sec: 59994.23
Iteration:   1620, Loss function: 3.847, Average Loss: 4.501, avg. samples / sec: 59937.92
Iteration:   1620, Loss function: 4.567, Average Loss: 4.496, avg. samples / sec: 60190.97
Iteration:   1620, Loss function: 4.577, Average Loss: 4.520, avg. samples / sec: 60026.66
Iteration:   1620, Loss function: 4.490, Average Loss: 4.531, avg. samples / sec: 59911.27
Iteration:   1620, Loss function: 3.836, Average Loss: 4.502, avg. samples / sec: 60087.65
Iteration:   1620, Loss function: 4.467, Average Loss: 4.521, avg. samples / sec: 59896.57
Iteration:   1620, Loss function: 6.005, Average Loss: 4.510, avg. samples / sec: 59938.56
Iteration:   1620, Loss function: 4.371, Average Loss: 4.515, avg. samples / sec: 59815.63
Iteration:   1620, Loss function: 4.029, Average Loss: 4.495, avg. samples / sec: 59889.04
Iteration:   1620, Loss function: 5.006, Average Loss: 4.526, avg. samples / sec: 59819.03
Iteration:   1620, Loss function: 4.640, Average Loss: 4.500, avg. samples / sec: 59711.26
Iteration:   1620, Loss function: 4.961, Average Loss: 4.501, avg. samples / sec: 59695.53
Iteration:   1620, Loss function: 4.690, Average Loss: 4.454, avg. samples / sec: 59542.16
Iteration:   1620, Loss function: 4.840, Average Loss: 4.489, avg. samples / sec: 58715.34
Iteration:   1640, Loss function: 5.982, Average Loss: 4.463, avg. samples / sec: 57807.95
Iteration:   1640, Loss function: 4.344, Average Loss: 4.499, avg. samples / sec: 57685.76
Iteration:   1640, Loss function: 3.811, Average Loss: 4.505, avg. samples / sec: 57430.16
Iteration:   1640, Loss function: 3.917, Average Loss: 4.487, avg. samples / sec: 58463.87
Iteration:   1640, Loss function: 4.204, Average Loss: 4.522, avg. samples / sec: 57568.17
Iteration:   1640, Loss function: 4.208, Average Loss: 4.503, avg. samples / sec: 57396.76
Iteration:   1640, Loss function: 4.616, Average Loss: 4.514, avg. samples / sec: 57407.16
Iteration:   1640, Loss function: 5.226, Average Loss: 4.493, avg. samples / sec: 57331.33
Iteration:   1640, Loss function: 5.695, Average Loss: 4.495, avg. samples / sec: 57364.17
Iteration:   1640, Loss function: 4.524, Average Loss: 4.498, avg. samples / sec: 57258.33
Iteration:   1640, Loss function: 4.440, Average Loss: 4.501, avg. samples / sec: 57480.52
Iteration:   1640, Loss function: 6.096, Average Loss: 4.519, avg. samples / sec: 57261.03
Iteration:   1640, Loss function: 4.179, Average Loss: 4.518, avg. samples / sec: 57250.89
Iteration:   1640, Loss function: 2.948, Average Loss: 4.493, avg. samples / sec: 57131.32
Iteration:   1640, Loss function: 4.989, Average Loss: 4.526, avg. samples / sec: 56161.69
Iteration:   1660, Loss function: 4.429, Average Loss: 4.517, avg. samples / sec: 58251.74
Iteration:   1660, Loss function: 4.860, Average Loss: 4.502, avg. samples / sec: 58235.21
Iteration:   1660, Loss function: 3.065, Average Loss: 4.499, avg. samples / sec: 58298.56
Iteration:   1660, Loss function: 3.809, Average Loss: 4.489, avg. samples / sec: 58187.84
Iteration:   1660, Loss function: 4.755, Average Loss: 4.494, avg. samples / sec: 58231.26
Iteration:   1660, Loss function: 4.852, Average Loss: 4.499, avg. samples / sec: 58292.44
Iteration:   1660, Loss function: 5.530, Average Loss: 4.460, avg. samples / sec: 58084.52
Iteration:   1660, Loss function: 3.510, Average Loss: 4.498, avg. samples / sec: 58083.47
Iteration:   1660, Loss function: 4.229, Average Loss: 4.507, avg. samples / sec: 58099.61
Iteration:   1660, Loss function: 4.009, Average Loss: 4.520, avg. samples / sec: 59416.09
Iteration:   1660, Loss function: 3.962, Average Loss: 4.519, avg. samples / sec: 58102.00
Iteration:   1660, Loss function: 5.403, Average Loss: 4.486, avg. samples / sec: 58222.89
Iteration:   1660, Loss function: 4.340, Average Loss: 4.494, avg. samples / sec: 58044.88
Iteration:   1660, Loss function: 3.674, Average Loss: 4.519, avg. samples / sec: 58132.58
Iteration:   1660, Loss function: 6.277, Average Loss: 4.519, avg. samples / sec: 58136.09
:::MLL 1558638990.567 epoch_stop: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 819}}
:::MLL 1558638990.567 epoch_start: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 673}}
Iteration:   1680, Loss function: 4.646, Average Loss: 4.510, avg. samples / sec: 60294.03
Iteration:   1680, Loss function: 5.488, Average Loss: 4.458, avg. samples / sec: 60161.47
Iteration:   1680, Loss function: 3.776, Average Loss: 4.517, avg. samples / sec: 60307.27
Iteration:   1680, Loss function: 4.323, Average Loss: 4.502, avg. samples / sec: 59985.73
Iteration:   1680, Loss function: 3.800, Average Loss: 4.513, avg. samples / sec: 60108.72
Iteration:   1680, Loss function: 5.198, Average Loss: 4.518, avg. samples / sec: 60206.38
Iteration:   1680, Loss function: 5.454, Average Loss: 4.495, avg. samples / sec: 60022.42
Iteration:   1680, Loss function: 4.403, Average Loss: 4.522, avg. samples / sec: 59912.74
Iteration:   1680, Loss function: 4.230, Average Loss: 4.496, avg. samples / sec: 60021.24
Iteration:   1680, Loss function: 4.166, Average Loss: 4.498, avg. samples / sec: 59917.38
Iteration:   1680, Loss function: 5.584, Average Loss: 4.489, avg. samples / sec: 60140.14
Iteration:   1680, Loss function: 4.824, Average Loss: 4.522, avg. samples / sec: 60049.89
Iteration:   1680, Loss function: 3.720, Average Loss: 4.484, avg. samples / sec: 60002.05
Iteration:   1680, Loss function: 4.444, Average Loss: 4.492, avg. samples / sec: 59856.07
Iteration:   1680, Loss function: 4.150, Average Loss: 4.487, avg. samples / sec: 59717.26
Iteration:   1700, Loss function: 4.512, Average Loss: 4.511, avg. samples / sec: 57107.98
Iteration:   1700, Loss function: 4.335, Average Loss: 4.506, avg. samples / sec: 57008.41
Iteration:   1700, Loss function: 4.958, Average Loss: 4.492, avg. samples / sec: 57187.44
Iteration:   1700, Loss function: 3.467, Average Loss: 4.498, avg. samples / sec: 57181.04
Iteration:   1700, Loss function: 5.054, Average Loss: 4.484, avg. samples / sec: 57161.79
Iteration:   1700, Loss function: 4.506, Average Loss: 4.485, avg. samples / sec: 57248.91
Iteration:   1700, Loss function: 3.675, Average Loss: 4.456, avg. samples / sec: 57010.51
Iteration:   1700, Loss function: 3.841, Average Loss: 4.515, avg. samples / sec: 57084.80
Iteration:   1700, Loss function: 5.121, Average Loss: 4.520, avg. samples / sec: 57144.33
Iteration:   1700, Loss function: 4.015, Average Loss: 4.514, avg. samples / sec: 57059.93
Iteration:   1700, Loss function: 3.866, Average Loss: 4.481, avg. samples / sec: 57303.71
Iteration:   1700, Loss function: 4.866, Average Loss: 4.486, avg. samples / sec: 57187.26
Iteration:   1700, Loss function: 3.724, Average Loss: 4.488, avg. samples / sec: 57047.41
Iteration:   1700, Loss function: 4.085, Average Loss: 4.499, avg. samples / sec: 56930.64
Iteration:   1700, Loss function: 4.136, Average Loss: 4.522, avg. samples / sec: 56982.11
Iteration:   1720, Loss function: 3.652, Average Loss: 4.508, avg. samples / sec: 58049.09
Iteration:   1720, Loss function: 4.433, Average Loss: 4.489, avg. samples / sec: 58013.48
Iteration:   1720, Loss function: 5.354, Average Loss: 4.460, avg. samples / sec: 58022.11
Iteration:   1720, Loss function: 5.363, Average Loss: 4.506, avg. samples / sec: 57928.35
Iteration:   1720, Loss function: 4.443, Average Loss: 4.485, avg. samples / sec: 58011.00
Iteration:   1720, Loss function: 4.046, Average Loss: 4.480, avg. samples / sec: 58044.16
Iteration:   1720, Loss function: 4.886, Average Loss: 4.491, avg. samples / sec: 58052.05
Iteration:   1720, Loss function: 4.001, Average Loss: 4.485, avg. samples / sec: 58035.94
Iteration:   1720, Loss function: 4.057, Average Loss: 4.505, avg. samples / sec: 58085.55
Iteration:   1720, Loss function: 4.964, Average Loss: 4.522, avg. samples / sec: 58085.38
Iteration:   1720, Loss function: 3.154, Average Loss: 4.511, avg. samples / sec: 57833.29
Iteration:   1720, Loss function: 4.997, Average Loss: 4.520, avg. samples / sec: 57934.23
Iteration:   1720, Loss function: 3.829, Average Loss: 4.516, avg. samples / sec: 57891.75
Iteration:   1720, Loss function: 5.022, Average Loss: 4.503, avg. samples / sec: 57794.53
Iteration:   1720, Loss function: 4.549, Average Loss: 4.496, avg. samples / sec: 57771.20
Iteration:   1740, Loss function: 4.497, Average Loss: 4.517, avg. samples / sec: 57674.83
Iteration:   1740, Loss function: 3.739, Average Loss: 4.517, avg. samples / sec: 57731.27
Iteration:   1740, Loss function: 5.919, Average Loss: 4.491, avg. samples / sec: 57532.24
Iteration:   1740, Loss function: 3.713, Average Loss: 4.457, avg. samples / sec: 57513.27
Iteration:   1740, Loss function: 3.329, Average Loss: 4.482, avg. samples / sec: 57521.39
Iteration:   1740, Loss function: 5.011, Average Loss: 4.508, avg. samples / sec: 57474.68
Iteration:   1740, Loss function: 4.186, Average Loss: 4.507, avg. samples / sec: 57573.79
Iteration:   1740, Loss function: 4.657, Average Loss: 4.506, avg. samples / sec: 57555.48
Iteration:   1740, Loss function: 2.797, Average Loss: 4.498, avg. samples / sec: 57648.66
Iteration:   1740, Loss function: 4.887, Average Loss: 4.509, avg. samples / sec: 57434.93
Iteration:   1740, Loss function: 5.710, Average Loss: 4.499, avg. samples / sec: 57463.55
Iteration:   1740, Loss function: 4.756, Average Loss: 4.523, avg. samples / sec: 57496.07
Iteration:   1740, Loss function: 3.612, Average Loss: 4.497, avg. samples / sec: 57589.42
Iteration:   1740, Loss function: 5.124, Average Loss: 4.478, avg. samples / sec: 57420.43
Iteration:   1740, Loss function: 3.878, Average Loss: 4.486, avg. samples / sec: 57372.53
:::MLL 1558638992.596 epoch_stop: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 819}}
:::MLL 1558638992.596 epoch_start: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 673}}
Iteration:   1760, Loss function: 4.482, Average Loss: 4.502, avg. samples / sec: 60112.13
Iteration:   1760, Loss function: 4.172, Average Loss: 4.488, avg. samples / sec: 59916.85
Iteration:   1760, Loss function: 4.466, Average Loss: 4.475, avg. samples / sec: 60036.35
Iteration:   1760, Loss function: 4.770, Average Loss: 4.506, avg. samples / sec: 59876.57
Iteration:   1760, Loss function: 6.158, Average Loss: 4.516, avg. samples / sec: 59781.07
Iteration:   1760, Loss function: 5.158, Average Loss: 4.507, avg. samples / sec: 59905.49
Iteration:   1760, Loss function: 5.346, Average Loss: 4.518, avg. samples / sec: 59914.93
Iteration:   1760, Loss function: 5.188, Average Loss: 4.498, avg. samples / sec: 59854.50
Iteration:   1760, Loss function: 3.370, Average Loss: 4.506, avg. samples / sec: 59783.96
Iteration:   1760, Loss function: 4.094, Average Loss: 4.506, avg. samples / sec: 59805.55
Iteration:   1760, Loss function: 4.981, Average Loss: 4.515, avg. samples / sec: 59723.44
Iteration:   1760, Loss function: 4.370, Average Loss: 4.488, avg. samples / sec: 59911.95
Iteration:   1760, Loss function: 5.638, Average Loss: 4.477, avg. samples / sec: 59668.94
Iteration:   1760, Loss function: 3.773, Average Loss: 4.495, avg. samples / sec: 59764.44
Iteration:   1760, Loss function: 4.131, Average Loss: 4.461, avg. samples / sec: 59635.76
Iteration:   1780, Loss function: 4.728, Average Loss: 4.502, avg. samples / sec: 59828.81
Iteration:   1780, Loss function: 4.777, Average Loss: 4.475, avg. samples / sec: 59673.74
Iteration:   1780, Loss function: 3.501, Average Loss: 4.488, avg. samples / sec: 59857.09
Iteration:   1780, Loss function: 3.477, Average Loss: 4.486, avg. samples / sec: 59525.46
Iteration:   1780, Loss function: 4.168, Average Loss: 4.509, avg. samples / sec: 59618.05
Iteration:   1780, Loss function: 4.028, Average Loss: 4.516, avg. samples / sec: 59627.29
Iteration:   1780, Loss function: 3.500, Average Loss: 4.501, avg. samples / sec: 59669.09
Iteration:   1780, Loss function: 3.921, Average Loss: 4.498, avg. samples / sec: 59799.54
Iteration:   1780, Loss function: 4.210, Average Loss: 4.478, avg. samples / sec: 59746.35
Iteration:   1780, Loss function: 3.885, Average Loss: 4.500, avg. samples / sec: 59565.16
Iteration:   1780, Loss function: 4.368, Average Loss: 4.516, avg. samples / sec: 59641.34
Iteration:   1780, Loss function: 4.014, Average Loss: 4.464, avg. samples / sec: 59743.82
Iteration:   1780, Loss function: 3.281, Average Loss: 4.514, avg. samples / sec: 59497.72
Iteration:   1780, Loss function: 3.983, Average Loss: 4.499, avg. samples / sec: 59328.37
Iteration:   1780, Loss function: 4.107, Average Loss: 4.502, avg. samples / sec: 59550.64
Iteration:   1800, Loss function: 4.265, Average Loss: 4.456, avg. samples / sec: 56779.51
Iteration:   1800, Loss function: 4.479, Average Loss: 4.484, avg. samples / sec: 56682.34
Iteration:   1800, Loss function: 3.833, Average Loss: 4.516, avg. samples / sec: 56676.18
Iteration:   1800, Loss function: 4.105, Average Loss: 4.471, avg. samples / sec: 56549.35
Iteration:   1800, Loss function: 5.975, Average Loss: 4.490, avg. samples / sec: 56556.48
Iteration:   1800, Loss function: 4.296, Average Loss: 4.511, avg. samples / sec: 56683.61
Iteration:   1800, Loss function: 4.617, Average Loss: 4.498, avg. samples / sec: 56507.86
Iteration:   1800, Loss function: 3.930, Average Loss: 4.502, avg. samples / sec: 56642.76
Iteration:   1800, Loss function: 3.871, Average Loss: 4.489, avg. samples / sec: 56634.82
Iteration:   1800, Loss function: 4.535, Average Loss: 4.517, avg. samples / sec: 56710.35
Iteration:   1800, Loss function: 3.518, Average Loss: 4.508, avg. samples / sec: 56720.25
Iteration:   1800, Loss function: 4.968, Average Loss: 4.472, avg. samples / sec: 56622.08
Iteration:   1800, Loss function: 4.251, Average Loss: 4.497, avg. samples / sec: 56662.81
Iteration:   1800, Loss function: 5.072, Average Loss: 4.497, avg. samples / sec: 56566.92
Iteration:   1800, Loss function: 4.441, Average Loss: 4.502, avg. samples / sec: 56521.32
:::MLL 1558638994.612 epoch_stop: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 819}}
:::MLL 1558638994.613 epoch_start: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 673}}
Iteration:   1820, Loss function: 4.142, Average Loss: 4.483, avg. samples / sec: 58627.19
Iteration:   1820, Loss function: 3.670, Average Loss: 4.467, avg. samples / sec: 58616.09
Iteration:   1820, Loss function: 5.551, Average Loss: 4.492, avg. samples / sec: 58622.04
Iteration:   1820, Loss function: 2.976, Average Loss: 4.466, avg. samples / sec: 58471.80
Iteration:   1820, Loss function: 3.882, Average Loss: 4.512, avg. samples / sec: 58457.61
Iteration:   1820, Loss function: 4.908, Average Loss: 4.492, avg. samples / sec: 58463.46
Iteration:   1820, Loss function: 5.346, Average Loss: 4.496, avg. samples / sec: 58535.80
Iteration:   1820, Loss function: 3.808, Average Loss: 4.486, avg. samples / sec: 58446.68
Iteration:   1820, Loss function: 4.719, Average Loss: 4.456, avg. samples / sec: 58350.82
Iteration:   1820, Loss function: 3.807, Average Loss: 4.495, avg. samples / sec: 58532.39
Iteration:   1820, Loss function: 5.666, Average Loss: 4.502, avg. samples / sec: 58394.80
Iteration:   1820, Loss function: 4.842, Average Loss: 4.506, avg. samples / sec: 58406.95
Iteration:   1820, Loss function: 4.094, Average Loss: 4.492, avg. samples / sec: 58342.85
Iteration:   1820, Loss function: 3.813, Average Loss: 4.510, avg. samples / sec: 58370.21
Iteration:   1820, Loss function: 4.938, Average Loss: 4.508, avg. samples / sec: 58324.11
Iteration:   1840, Loss function: 4.144, Average Loss: 4.481, avg. samples / sec: 59391.60
Iteration:   1840, Loss function: 3.899, Average Loss: 4.508, avg. samples / sec: 59673.36
Iteration:   1840, Loss function: 3.892, Average Loss: 4.452, avg. samples / sec: 59551.75
Iteration:   1840, Loss function: 4.694, Average Loss: 4.490, avg. samples / sec: 59481.32
Iteration:   1840, Loss function: 4.876, Average Loss: 4.492, avg. samples / sec: 59408.58
Iteration:   1840, Loss function: 4.017, Average Loss: 4.491, avg. samples / sec: 59474.87
Iteration:   1840, Loss function: 3.046, Average Loss: 4.509, avg. samples / sec: 59533.23
Iteration:   1840, Loss function: 2.858, Average Loss: 4.462, avg. samples / sec: 59299.29
Iteration:   1840, Loss function: 4.520, Average Loss: 4.509, avg. samples / sec: 59451.46
Iteration:   1840, Loss function: 4.696, Average Loss: 4.482, avg. samples / sec: 59351.53
Iteration:   1840, Loss function: 4.422, Average Loss: 4.498, avg. samples / sec: 59351.36
Iteration:   1840, Loss function: 4.466, Average Loss: 4.486, avg. samples / sec: 59381.74
Iteration:   1840, Loss function: 4.562, Average Loss: 4.491, avg. samples / sec: 59167.46
Iteration:   1840, Loss function: 3.696, Average Loss: 4.467, avg. samples / sec: 59172.63
Iteration:   1840, Loss function: 3.580, Average Loss: 4.506, avg. samples / sec: 59160.66
Iteration:   1860, Loss function: 5.250, Average Loss: 4.460, avg. samples / sec: 58414.94
Iteration:   1860, Loss function: 6.524, Average Loss: 4.496, avg. samples / sec: 58524.18
Iteration:   1860, Loss function: 4.919, Average Loss: 4.489, avg. samples / sec: 58353.72
Iteration:   1860, Loss function: 4.010, Average Loss: 4.507, avg. samples / sec: 58192.71
Iteration:   1860, Loss function: 4.978, Average Loss: 4.482, avg. samples / sec: 58292.15
Iteration:   1860, Loss function: 4.198, Average Loss: 4.465, avg. samples / sec: 58548.22
Iteration:   1860, Loss function: 2.936, Average Loss: 4.479, avg. samples / sec: 58454.99
Iteration:   1860, Loss function: 2.595, Average Loss: 4.486, avg. samples / sec: 58536.06
Iteration:   1860, Loss function: 3.366, Average Loss: 4.501, avg. samples / sec: 58331.69
Iteration:   1860, Loss function: 4.183, Average Loss: 4.451, avg. samples / sec: 58213.63
Iteration:   1860, Loss function: 4.763, Average Loss: 4.507, avg. samples / sec: 58424.00
Iteration:   1860, Loss function: 4.195, Average Loss: 4.485, avg. samples / sec: 58506.29
Iteration:   1860, Loss function: 3.510, Average Loss: 4.497, avg. samples / sec: 58468.35
Iteration:   1860, Loss function: 5.352, Average Loss: 4.486, avg. samples / sec: 58219.86
Iteration:   1860, Loss function: 3.327, Average Loss: 4.475, avg. samples / sec: 57989.80
Iteration:   1880, Loss function: 5.001, Average Loss: 4.461, avg. samples / sec: 57077.17
Iteration:   1880, Loss function: 4.298, Average Loss: 4.481, avg. samples / sec: 57124.48
Iteration:   1880, Loss function: 4.967, Average Loss: 4.499, avg. samples / sec: 57107.17
Iteration:   1880, Loss function: 3.976, Average Loss: 4.476, avg. samples / sec: 57075.95
Iteration:   1880, Loss function: 4.509, Average Loss: 4.474, avg. samples / sec: 57210.19
Iteration:   1880, Loss function: 4.443, Average Loss: 4.503, avg. samples / sec: 57098.58
Iteration:   1880, Loss function: 4.746, Average Loss: 4.489, avg. samples / sec: 57030.91
Iteration:   1880, Loss function: 4.149, Average Loss: 4.502, avg. samples / sec: 57031.30
Iteration:   1880, Loss function: 5.759, Average Loss: 4.491, avg. samples / sec: 56976.40
Iteration:   1880, Loss function: 3.961, Average Loss: 4.462, avg. samples / sec: 57017.57
Iteration:   1880, Loss function: 4.004, Average Loss: 4.482, avg. samples / sec: 57014.55
Iteration:   1880, Loss function: 4.531, Average Loss: 4.482, avg. samples / sec: 57119.32
Iteration:   1880, Loss function: 3.973, Average Loss: 4.446, avg. samples / sec: 56967.62
Iteration:   1880, Loss function: 3.284, Average Loss: 4.471, avg. samples / sec: 56870.36
Iteration:   1880, Loss function: 5.217, Average Loss: 4.499, avg. samples / sec: 56876.39
:::MLL 1558638996.621 epoch_stop: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 819}}
:::MLL 1558638996.621 epoch_start: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 673}}
Iteration:   1900, Loss function: 3.973, Average Loss: 4.487, avg. samples / sec: 58129.47
Iteration:   1900, Loss function: 5.364, Average Loss: 4.491, avg. samples / sec: 58364.84
Iteration:   1900, Loss function: 4.329, Average Loss: 4.484, avg. samples / sec: 58066.67
Iteration:   1900, Loss function: 3.324, Average Loss: 4.447, avg. samples / sec: 58173.67
Iteration:   1900, Loss function: 3.379, Average Loss: 4.498, avg. samples / sec: 58024.54
Iteration:   1900, Loss function: 4.092, Average Loss: 4.463, avg. samples / sec: 58194.93
Iteration:   1900, Loss function: 4.259, Average Loss: 4.482, avg. samples / sec: 57943.19
Iteration:   1900, Loss function: 3.114, Average Loss: 4.478, avg. samples / sec: 58046.34
Iteration:   1900, Loss function: 4.126, Average Loss: 4.469, avg. samples / sec: 57973.08
Iteration:   1900, Loss function: 2.970, Average Loss: 4.494, avg. samples / sec: 58010.28
Iteration:   1900, Loss function: 4.342, Average Loss: 4.458, avg. samples / sec: 57876.92
Iteration:   1900, Loss function: 3.364, Average Loss: 4.455, avg. samples / sec: 57965.04
Iteration:   1900, Loss function: 3.562, Average Loss: 4.471, avg. samples / sec: 57907.31
Iteration:   1900, Loss function: 4.882, Average Loss: 4.473, avg. samples / sec: 57980.78
Iteration:   1900, Loss function: 4.464, Average Loss: 4.494, avg. samples / sec: 57846.03
Iteration:   1920, Loss function: 4.987, Average Loss: 4.473, avg. samples / sec: 59631.22
Iteration:   1920, Loss function: 4.623, Average Loss: 4.478, avg. samples / sec: 59474.77
Iteration:   1920, Loss function: 6.488, Average Loss: 4.484, avg. samples / sec: 59474.87
Iteration:   1920, Loss function: 4.004, Average Loss: 4.464, avg. samples / sec: 59449.75
Iteration:   1920, Loss function: 3.724, Average Loss: 4.491, avg. samples / sec: 59365.66
Iteration:   1920, Loss function: 4.470, Average Loss: 4.468, avg. samples / sec: 59518.07
Iteration:   1920, Loss function: 4.002, Average Loss: 4.480, avg. samples / sec: 59342.09
Iteration:   1920, Loss function: 5.147, Average Loss: 4.496, avg. samples / sec: 59357.23
Iteration:   1920, Loss function: 3.278, Average Loss: 4.463, avg. samples / sec: 59347.88
Iteration:   1920, Loss function: 5.431, Average Loss: 4.455, avg. samples / sec: 59402.14
Iteration:   1920, Loss function: 4.147, Average Loss: 4.447, avg. samples / sec: 59228.83
Iteration:   1920, Loss function: 2.722, Average Loss: 4.483, avg. samples / sec: 59182.12
Iteration:   1920, Loss function: 4.037, Average Loss: 4.490, avg. samples / sec: 59288.21
Iteration:   1920, Loss function: 4.643, Average Loss: 4.458, avg. samples / sec: 59304.15
Iteration:   1920, Loss function: 5.022, Average Loss: 4.492, avg. samples / sec: 59355.96
Iteration:   1940, Loss function: 3.983, Average Loss: 4.475, avg. samples / sec: 56583.57
Iteration:   1940, Loss function: 3.347, Average Loss: 4.460, avg. samples / sec: 56668.41
Iteration:   1940, Loss function: 4.149, Average Loss: 4.467, avg. samples / sec: 56484.87
Iteration:   1940, Loss function: 4.227, Average Loss: 4.483, avg. samples / sec: 56629.36
Iteration:   1940, Loss function: 5.495, Average Loss: 4.479, avg. samples / sec: 56604.48
Iteration:   1940, Loss function: 3.349, Average Loss: 4.457, avg. samples / sec: 56422.61
Iteration:   1940, Loss function: 4.429, Average Loss: 4.485, avg. samples / sec: 56399.01
Iteration:   1940, Loss function: 5.238, Average Loss: 4.492, avg. samples / sec: 56469.18
Iteration:   1940, Loss function: 4.939, Average Loss: 4.461, avg. samples / sec: 56506.88
Iteration:   1940, Loss function: 4.699, Average Loss: 4.486, avg. samples / sec: 56398.79
Iteration:   1940, Loss function: 4.536, Average Loss: 4.445, avg. samples / sec: 56551.78
Iteration:   1940, Loss function: 3.355, Average Loss: 4.473, avg. samples / sec: 56371.65
Iteration:   1940, Loss function: 4.003, Average Loss: 4.465, avg. samples / sec: 56258.34
Iteration:   1940, Loss function: 3.648, Average Loss: 4.457, avg. samples / sec: 56401.32
Iteration:   1940, Loss function: 3.488, Average Loss: 4.485, avg. samples / sec: 56413.69
:::MLL 1558638998.648 epoch_stop: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 819}}
:::MLL 1558638998.648 epoch_start: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 673}}
Iteration:   1960, Loss function: 4.431, Average Loss: 4.466, avg. samples / sec: 59792.74
Iteration:   1960, Loss function: 4.546, Average Loss: 4.469, avg. samples / sec: 59557.33
Iteration:   1960, Loss function: 5.878, Average Loss: 4.476, avg. samples / sec: 59627.87
Iteration:   1960, Loss function: 4.173, Average Loss: 4.481, avg. samples / sec: 59763.83
Iteration:   1960, Loss function: 3.997, Average Loss: 4.478, avg. samples / sec: 59522.44
Iteration:   1960, Loss function: 3.626, Average Loss: 4.452, avg. samples / sec: 59495.63
Iteration:   1960, Loss function: 4.162, Average Loss: 4.463, avg. samples / sec: 59443.66
Iteration:   1960, Loss function: 4.178, Average Loss: 4.475, avg. samples / sec: 59446.42
Iteration:   1960, Loss function: 4.552, Average Loss: 4.467, avg. samples / sec: 59500.96
Iteration:   1960, Loss function: 4.619, Average Loss: 4.458, avg. samples / sec: 59462.10
Iteration:   1960, Loss function: 3.364, Average Loss: 4.440, avg. samples / sec: 59466.89
Iteration:   1960, Loss function: 3.480, Average Loss: 4.450, avg. samples / sec: 59566.22
Iteration:   1960, Loss function: 4.151, Average Loss: 4.480, avg. samples / sec: 59413.69
Iteration:   1960, Loss function: 3.683, Average Loss: 4.486, avg. samples / sec: 59381.17
Iteration:   1960, Loss function: 4.427, Average Loss: 4.455, avg. samples / sec: 59292.75
Iteration:   1980, Loss function: 2.895, Average Loss: 4.473, avg. samples / sec: 59657.42
Iteration:   1980, Loss function: 4.090, Average Loss: 4.459, avg. samples / sec: 59530.01
Iteration:   1980, Loss function: 4.950, Average Loss: 4.456, avg. samples / sec: 59542.82
Iteration:   1980, Loss function: 5.431, Average Loss: 4.463, avg. samples / sec: 59277.34
Iteration:   1980, Loss function: 4.673, Average Loss: 4.483, avg. samples / sec: 59507.92
Iteration:   1980, Loss function: 3.534, Average Loss: 4.470, avg. samples / sec: 59347.36
Iteration:   1980, Loss function: 4.072, Average Loss: 4.470, avg. samples / sec: 59373.89
Iteration:   1980, Loss function: 4.605, Average Loss: 4.449, avg. samples / sec: 59406.48
Iteration:   1980, Loss function: 4.096, Average Loss: 4.460, avg. samples / sec: 59332.12
Iteration:   1980, Loss function: 3.619, Average Loss: 4.434, avg. samples / sec: 59364.03
Iteration:   1980, Loss function: 4.952, Average Loss: 4.467, avg. samples / sec: 59096.50
Iteration:   1980, Loss function: 2.998, Average Loss: 4.471, avg. samples / sec: 59058.56
Iteration:   1980, Loss function: 4.294, Average Loss: 4.453, avg. samples / sec: 59334.59
Iteration:   1980, Loss function: 4.313, Average Loss: 4.448, avg. samples / sec: 59151.00
Iteration:   1980, Loss function: 4.499, Average Loss: 4.480, avg. samples / sec: 58939.44
Iteration:   2000, Loss function: 4.690, Average Loss: 4.450, avg. samples / sec: 58663.74
Iteration:   2000, Loss function: 4.831, Average Loss: 4.469, avg. samples / sec: 58736.97
Iteration:   2000, Loss function: 4.753, Average Loss: 4.474, avg. samples / sec: 59104.48
Iteration:   2000, Loss function: 4.290, Average Loss: 4.458, avg. samples / sec: 58640.58
Iteration:   2000, Loss function: 3.720, Average Loss: 4.428, avg. samples / sec: 58786.07
Iteration:   2000, Loss function: 3.852, Average Loss: 4.468, avg. samples / sec: 58732.69
Iteration:   2000, Loss function: 4.527, Average Loss: 4.471, avg. samples / sec: 58459.79
Iteration:   2000, Loss function: 3.618, Average Loss: 4.454, avg. samples / sec: 58489.42
Iteration:   2000, Loss function: 3.893, Average Loss: 4.465, avg. samples / sec: 58620.82
Iteration:   2000, Loss function: 4.490, Average Loss: 4.465, avg. samples / sec: 58723.29
Iteration:   2000, Loss function: 4.141, Average Loss: 4.481, avg. samples / sec: 58541.68
Iteration:   2000, Loss function: 4.570, Average Loss: 4.450, avg. samples / sec: 58690.15
Iteration:   2000, Loss function: 3.307, Average Loss: 4.446, avg. samples / sec: 58661.84
Iteration:   2000, Loss function: 4.578, Average Loss: 4.451, avg. samples / sec: 58468.48
Iteration:   2000, Loss function: 2.566, Average Loss: 4.468, avg. samples / sec: 58373.03
Iteration:   2020, Loss function: 5.343, Average Loss: 4.458, avg. samples / sec: 60727.37
Iteration:   2020, Loss function: 4.987, Average Loss: 4.467, avg. samples / sec: 60432.43
Iteration:   2020, Loss function: 4.045, Average Loss: 4.468, avg. samples / sec: 60824.27
Iteration:   2020, Loss function: 3.880, Average Loss: 4.469, avg. samples / sec: 60550.83
Iteration:   2020, Loss function: 4.819, Average Loss: 4.465, avg. samples / sec: 60511.96
Iteration:   2020, Loss function: 3.806, Average Loss: 4.447, avg. samples / sec: 60431.63
Iteration:   2020, Loss function: 5.593, Average Loss: 4.446, avg. samples / sec: 60595.93
Iteration:   2020, Loss function: 4.402, Average Loss: 4.447, avg. samples / sec: 60324.56
Iteration:   2020, Loss function: 5.126, Average Loss: 4.470, avg. samples / sec: 60365.78
Iteration:   2020, Loss function: 5.378, Average Loss: 4.429, avg. samples / sec: 60342.85
Iteration:   2020, Loss function: 4.006, Average Loss: 4.437, avg. samples / sec: 60596.06
Iteration:   2020, Loss function: 3.865, Average Loss: 4.477, avg. samples / sec: 60501.31
Iteration:   2020, Loss function: 3.601, Average Loss: 4.448, avg. samples / sec: 60412.46
Iteration:   2020, Loss function: 5.249, Average Loss: 4.461, avg. samples / sec: 60400.57
Iteration:   2020, Loss function: 3.145, Average Loss: 4.443, avg. samples / sec: 60392.19
:::MLL 1558639000.624 epoch_stop: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 819}}
:::MLL 1558639000.625 epoch_start: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 673}}
Iteration:   2040, Loss function: 5.410, Average Loss: 4.465, avg. samples / sec: 58977.18
Iteration:   2040, Loss function: 3.460, Average Loss: 4.438, avg. samples / sec: 59113.26
Iteration:   2040, Loss function: 4.514, Average Loss: 4.427, avg. samples / sec: 58996.75
Iteration:   2040, Loss function: 4.519, Average Loss: 4.464, avg. samples / sec: 58935.76
Iteration:   2040, Loss function: 4.401, Average Loss: 4.447, avg. samples / sec: 58920.27
Iteration:   2040, Loss function: 3.870, Average Loss: 4.433, avg. samples / sec: 58979.82
Iteration:   2040, Loss function: 4.456, Average Loss: 4.456, avg. samples / sec: 59014.10
Iteration:   2040, Loss function: 4.266, Average Loss: 4.457, avg. samples / sec: 58685.75
Iteration:   2040, Loss function: 3.654, Average Loss: 4.466, avg. samples / sec: 58743.90
Iteration:   2040, Loss function: 4.006, Average Loss: 4.444, avg. samples / sec: 58709.69
Iteration:   2040, Loss function: 3.677, Average Loss: 4.439, avg. samples / sec: 58701.45
Iteration:   2040, Loss function: 4.356, Average Loss: 4.466, avg. samples / sec: 58641.38
Iteration:   2040, Loss function: 3.136, Average Loss: 4.473, avg. samples / sec: 58709.93
Iteration:   2040, Loss function: 5.147, Average Loss: 4.458, avg. samples / sec: 58577.98
Iteration:   2040, Loss function: 4.873, Average Loss: 4.436, avg. samples / sec: 58912.33
Iteration:   2060, Loss function: 5.264, Average Loss: 4.437, avg. samples / sec: 60723.11
Iteration:   2060, Loss function: 5.273, Average Loss: 4.428, avg. samples / sec: 60502.61
Iteration:   2060, Loss function: 4.208, Average Loss: 4.458, avg. samples / sec: 60477.22
Iteration:   2060, Loss function: 3.141, Average Loss: 4.448, avg. samples / sec: 60484.67
Iteration:   2060, Loss function: 4.558, Average Loss: 4.462, avg. samples / sec: 60531.72
Iteration:   2060, Loss function: 4.193, Average Loss: 4.450, avg. samples / sec: 60484.90
Iteration:   2060, Loss function: 3.601, Average Loss: 4.432, avg. samples / sec: 60287.79
Iteration:   2060, Loss function: 5.677, Average Loss: 4.467, avg. samples / sec: 60630.99
Iteration:   2060, Loss function: 3.691, Average Loss: 4.440, avg. samples / sec: 60297.10
Iteration:   2060, Loss function: 4.066, Average Loss: 4.462, avg. samples / sec: 60181.20
Iteration:   2060, Loss function: 4.774, Average Loss: 4.436, avg. samples / sec: 60519.76
Iteration:   2060, Loss function: 4.633, Average Loss: 4.430, avg. samples / sec: 60599.47
Iteration:   2060, Loss function: 3.938, Average Loss: 4.427, avg. samples / sec: 60167.56
Iteration:   2060, Loss function: 3.539, Average Loss: 4.455, avg. samples / sec: 60460.30
Iteration:   2060, Loss function: 4.461, Average Loss: 4.465, avg. samples / sec: 60299.81
Iteration:   2080, Loss function: 3.656, Average Loss: 4.440, avg. samples / sec: 58635.04
Iteration:   2080, Loss function: 5.232, Average Loss: 4.432, avg. samples / sec: 58763.50
Iteration:   2080, Loss function: 4.232, Average Loss: 4.445, avg. samples / sec: 58846.26
Iteration:   2080, Loss function: 4.315, Average Loss: 4.431, avg. samples / sec: 58683.90
Iteration:   2080, Loss function: 3.700, Average Loss: 4.424, avg. samples / sec: 58687.24
Iteration:   2080, Loss function: 3.302, Average Loss: 4.453, avg. samples / sec: 58569.73
Iteration:   2080, Loss function: 3.779, Average Loss: 4.446, avg. samples / sec: 58564.01
Iteration:   2080, Loss function: 4.465, Average Loss: 4.454, avg. samples / sec: 58647.51
Iteration:   2080, Loss function: 4.144, Average Loss: 4.430, avg. samples / sec: 58410.44
Iteration:   2080, Loss function: 5.397, Average Loss: 4.430, avg. samples / sec: 58456.40
Iteration:   2080, Loss function: 4.281, Average Loss: 4.428, avg. samples / sec: 58526.85
Iteration:   2080, Loss function: 3.783, Average Loss: 4.423, avg. samples / sec: 58710.08
Iteration:   2080, Loss function: 4.244, Average Loss: 4.449, avg. samples / sec: 58423.51
Iteration:   2080, Loss function: 4.233, Average Loss: 4.457, avg. samples / sec: 58798.29
Iteration:   2080, Loss function: 3.195, Average Loss: 4.460, avg. samples / sec: 58409.95
:::MLL 1558639002.601 epoch_stop: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 819}}
:::MLL 1558639002.601 epoch_start: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 673}}
Iteration:   2100, Loss function: 2.851, Average Loss: 4.444, avg. samples / sec: 60528.68
Iteration:   2100, Loss function: 3.885, Average Loss: 4.444, avg. samples / sec: 60398.76
Iteration:   2100, Loss function: 4.227, Average Loss: 4.423, avg. samples / sec: 60344.79
Iteration:   2100, Loss function: 3.474, Average Loss: 4.448, avg. samples / sec: 60472.16
Iteration:   2100, Loss function: 4.280, Average Loss: 4.421, avg. samples / sec: 60363.42
Iteration:   2100, Loss function: 4.848, Average Loss: 4.425, avg. samples / sec: 60331.92
Iteration:   2100, Loss function: 3.981, Average Loss: 4.416, avg. samples / sec: 60361.15
Iteration:   2100, Loss function: 3.966, Average Loss: 4.429, avg. samples / sec: 60305.56
Iteration:   2100, Loss function: 3.393, Average Loss: 4.432, avg. samples / sec: 60222.43
Iteration:   2100, Loss function: 3.976, Average Loss: 4.447, avg. samples / sec: 60183.75
Iteration:   2100, Loss function: 4.773, Average Loss: 4.436, avg. samples / sec: 60090.16
Iteration:   2100, Loss function: 2.563, Average Loss: 4.426, avg. samples / sec: 60084.94
Iteration:   2100, Loss function: 4.987, Average Loss: 4.448, avg. samples / sec: 60145.12
Iteration:   2100, Loss function: 4.325, Average Loss: 4.441, avg. samples / sec: 60094.03
Iteration:   2100, Loss function: 3.755, Average Loss: 4.453, avg. samples / sec: 60187.30
Iteration:   2120, Loss function: 4.549, Average Loss: 4.422, avg. samples / sec: 59412.89
Iteration:   2120, Loss function: 5.925, Average Loss: 4.448, avg. samples / sec: 59282.25
Iteration:   2120, Loss function: 3.930, Average Loss: 4.416, avg. samples / sec: 59159.79
Iteration:   2120, Loss function: 4.834, Average Loss: 4.440, avg. samples / sec: 59100.44
Iteration:   2120, Loss function: 3.508, Average Loss: 4.454, avg. samples / sec: 59468.92
Iteration:   2120, Loss function: 3.471, Average Loss: 4.427, avg. samples / sec: 59244.05
Iteration:   2120, Loss function: 4.462, Average Loss: 4.405, avg. samples / sec: 59173.53
Iteration:   2120, Loss function: 3.861, Average Loss: 4.416, avg. samples / sec: 59149.31
Iteration:   2120, Loss function: 3.964, Average Loss: 4.431, avg. samples / sec: 59174.22
Iteration:   2120, Loss function: 4.095, Average Loss: 4.444, avg. samples / sec: 59052.57
Iteration:   2120, Loss function: 4.869, Average Loss: 4.441, avg. samples / sec: 58900.84
Iteration:   2120, Loss function: 5.449, Average Loss: 4.439, avg. samples / sec: 59241.38
Iteration:   2120, Loss function: 5.214, Average Loss: 4.447, avg. samples / sec: 59162.40
Iteration:   2120, Loss function: 4.456, Average Loss: 4.421, avg. samples / sec: 58979.32
Iteration:   2120, Loss function: 4.912, Average Loss: 4.425, avg. samples / sec: 58965.97
Iteration:   2140, Loss function: 3.081, Average Loss: 4.437, avg. samples / sec: 57315.59
Iteration:   2140, Loss function: 4.180, Average Loss: 4.433, avg. samples / sec: 57203.95
Iteration:   2140, Loss function: 3.680, Average Loss: 4.431, avg. samples / sec: 57195.17
Iteration:   2140, Loss function: 4.380, Average Loss: 4.443, avg. samples / sec: 57033.33
Iteration:   2140, Loss function: 4.813, Average Loss: 4.437, avg. samples / sec: 57196.08
Iteration:   2140, Loss function: 3.443, Average Loss: 4.441, avg. samples / sec: 57202.83
Iteration:   2140, Loss function: 5.468, Average Loss: 4.426, avg. samples / sec: 57017.59
Iteration:   2140, Loss function: 4.612, Average Loss: 4.419, avg. samples / sec: 57201.23
Iteration:   2140, Loss function: 5.535, Average Loss: 4.425, avg. samples / sec: 57249.63
Iteration:   2140, Loss function: 3.456, Average Loss: 4.433, avg. samples / sec: 56986.58
Iteration:   2140, Loss function: 4.760, Average Loss: 4.417, avg. samples / sec: 56894.86
Iteration:   2140, Loss function: 4.885, Average Loss: 4.450, avg. samples / sec: 56980.61
Iteration:   2140, Loss function: 4.188, Average Loss: 4.404, avg. samples / sec: 56980.20
Iteration:   2140, Loss function: 3.990, Average Loss: 4.411, avg. samples / sec: 56955.95
Iteration:   2140, Loss function: 4.987, Average Loss: 4.410, avg. samples / sec: 56880.04
Iteration:   2160, Loss function: 3.880, Average Loss: 4.410, avg. samples / sec: 58686.05
Iteration:   2160, Loss function: 3.842, Average Loss: 4.402, avg. samples / sec: 58568.83
Iteration:   2160, Loss function: 4.146, Average Loss: 4.419, avg. samples / sec: 58519.00
Iteration:   2160, Loss function: 3.093, Average Loss: 4.421, avg. samples / sec: 58489.18
Iteration:   2160, Loss function: 4.635, Average Loss: 4.436, avg. samples / sec: 58383.07
Iteration:   2160, Loss function: 3.892, Average Loss: 4.426, avg. samples / sec: 58340.77
Iteration:   2160, Loss function: 3.916, Average Loss: 4.426, avg. samples / sec: 58396.25
Iteration:   2160, Loss function: 5.065, Average Loss: 4.413, avg. samples / sec: 58394.42
Iteration:   2160, Loss function: 3.901, Average Loss: 4.424, avg. samples / sec: 58376.93
Iteration:   2160, Loss function: 4.091, Average Loss: 4.454, avg. samples / sec: 58358.46
Iteration:   2160, Loss function: 4.348, Average Loss: 4.436, avg. samples / sec: 58309.68
Iteration:   2160, Loss function: 4.075, Average Loss: 4.437, avg. samples / sec: 58217.93
Iteration:   2160, Loss function: 4.736, Average Loss: 4.435, avg. samples / sec: 58259.69
Iteration:   2160, Loss function: 4.226, Average Loss: 4.437, avg. samples / sec: 58117.50
Iteration:   2160, Loss function: 5.011, Average Loss: 4.413, avg. samples / sec: 58322.35
:::MLL 1558639004.610 epoch_stop: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 819}}
:::MLL 1558639004.610 epoch_start: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 673}}
Iteration:   2180, Loss function: 3.726, Average Loss: 4.448, avg. samples / sec: 59310.67
Iteration:   2180, Loss function: 5.049, Average Loss: 4.429, avg. samples / sec: 59256.35
Iteration:   2180, Loss function: 4.198, Average Loss: 4.435, avg. samples / sec: 59121.00
Iteration:   2180, Loss function: 3.988, Average Loss: 4.421, avg. samples / sec: 59168.98
Iteration:   2180, Loss function: 3.915, Average Loss: 4.415, avg. samples / sec: 59016.02
Iteration:   2180, Loss function: 4.996, Average Loss: 4.437, avg. samples / sec: 59178.89
Iteration:   2180, Loss function: 3.554, Average Loss: 4.426, avg. samples / sec: 59166.12
Iteration:   2180, Loss function: 4.736, Average Loss: 4.421, avg. samples / sec: 59080.47
Iteration:   2180, Loss function: 5.680, Average Loss: 4.421, avg. samples / sec: 59038.32
Iteration:   2180, Loss function: 5.759, Average Loss: 4.429, avg. samples / sec: 59135.61
Iteration:   2180, Loss function: 3.484, Average Loss: 4.414, avg. samples / sec: 58940.20
Iteration:   2180, Loss function: 3.775, Average Loss: 4.404, avg. samples / sec: 59236.68
Iteration:   2180, Loss function: 4.157, Average Loss: 4.411, avg. samples / sec: 58822.56
Iteration:   2180, Loss function: 4.425, Average Loss: 4.405, avg. samples / sec: 58951.64
Iteration:   2180, Loss function: 3.055, Average Loss: 4.396, avg. samples / sec: 58801.35
Iteration:   2200, Loss function: 4.451, Average Loss: 4.414, avg. samples / sec: 60484.44
Iteration:   2200, Loss function: 4.224, Average Loss: 4.437, avg. samples / sec: 60351.92
Iteration:   2200, Loss function: 4.017, Average Loss: 4.412, avg. samples / sec: 60550.26
Iteration:   2200, Loss function: 3.255, Average Loss: 4.422, avg. samples / sec: 60399.00
Iteration:   2200, Loss function: 4.168, Average Loss: 4.419, avg. samples / sec: 60461.21
Iteration:   2200, Loss function: 4.329, Average Loss: 4.392, avg. samples / sec: 60584.26
Iteration:   2200, Loss function: 4.015, Average Loss: 4.417, avg. samples / sec: 60424.50
Iteration:   2200, Loss function: 3.311, Average Loss: 4.409, avg. samples / sec: 60305.67
Iteration:   2200, Loss function: 4.204, Average Loss: 4.406, avg. samples / sec: 60411.71
Iteration:   2200, Loss function: 4.993, Average Loss: 4.416, avg. samples / sec: 60386.94
Iteration:   2200, Loss function: 3.864, Average Loss: 4.428, avg. samples / sec: 60268.89
Iteration:   2200, Loss function: 4.267, Average Loss: 4.426, avg. samples / sec: 60355.25
Iteration:   2200, Loss function: 4.452, Average Loss: 4.396, avg. samples / sec: 60358.51
Iteration:   2200, Loss function: 3.404, Average Loss: 4.437, avg. samples / sec: 60262.45
Iteration:   2200, Loss function: 4.911, Average Loss: 4.396, avg. samples / sec: 60246.86
Iteration:   2220, Loss function: 5.187, Average Loss: 4.403, avg. samples / sec: 59755.88
Iteration:   2220, Loss function: 3.562, Average Loss: 4.428, avg. samples / sec: 59681.75
Iteration:   2220, Loss function: 3.453, Average Loss: 4.404, avg. samples / sec: 59795.78
Iteration:   2220, Loss function: 4.120, Average Loss: 4.412, avg. samples / sec: 59789.92
Iteration:   2220, Loss function: 5.219, Average Loss: 4.417, avg. samples / sec: 59717.56
Iteration:   2220, Loss function: 5.322, Average Loss: 4.412, avg. samples / sec: 59762.34
Iteration:   2220, Loss function: 5.309, Average Loss: 4.418, avg. samples / sec: 59640.71
Iteration:   2220, Loss function: 4.562, Average Loss: 4.392, avg. samples / sec: 60006.50
Iteration:   2220, Loss function: 3.970, Average Loss: 4.422, avg. samples / sec: 59779.63
Iteration:   2220, Loss function: 3.998, Average Loss: 4.406, avg. samples / sec: 59582.82
Iteration:   2220, Loss function: 4.621, Average Loss: 4.422, avg. samples / sec: 59740.98
Iteration:   2220, Loss function: 4.224, Average Loss: 4.391, avg. samples / sec: 59744.73
Iteration:   2220, Loss function: 2.502, Average Loss: 4.400, avg. samples / sec: 59675.23
Iteration:   2220, Loss function: 4.740, Average Loss: 4.432, avg. samples / sec: 59694.24
Iteration:   2220, Loss function: 2.423, Average Loss: 4.384, avg. samples / sec: 59437.99
:::MLL 1558639006.599 epoch_stop: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 819}}
:::MLL 1558639006.599 epoch_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 673}}
Iteration:   2240, Loss function: 3.687, Average Loss: 4.419, avg. samples / sec: 58394.56
Iteration:   2240, Loss function: 4.942, Average Loss: 4.416, avg. samples / sec: 58198.17
Iteration:   2240, Loss function: 3.848, Average Loss: 4.413, avg. samples / sec: 58170.66
Iteration:   2240, Loss function: 4.626, Average Loss: 4.412, avg. samples / sec: 58171.91
Iteration:   2240, Loss function: 4.885, Average Loss: 4.386, avg. samples / sec: 58152.49
Iteration:   2240, Loss function: 3.426, Average Loss: 4.409, avg. samples / sec: 58113.45
Iteration:   2240, Loss function: 3.528, Average Loss: 4.392, avg. samples / sec: 58182.98
Iteration:   2240, Loss function: 4.426, Average Loss: 4.404, avg. samples / sec: 58116.86
Iteration:   2240, Loss function: 2.406, Average Loss: 4.396, avg. samples / sec: 57967.91
Iteration:   2240, Loss function: 3.600, Average Loss: 4.399, avg. samples / sec: 58030.87
Iteration:   2240, Loss function: 3.853, Average Loss: 4.414, avg. samples / sec: 58052.84
Iteration:   2240, Loss function: 3.984, Average Loss: 4.379, avg. samples / sec: 58281.35
Iteration:   2240, Loss function: 5.051, Average Loss: 4.425, avg. samples / sec: 57926.95
Iteration:   2240, Loss function: 5.561, Average Loss: 4.427, avg. samples / sec: 58087.30
Iteration:   2240, Loss function: 5.092, Average Loss: 4.387, avg. samples / sec: 57971.41
Iteration:   2260, Loss function: 3.776, Average Loss: 4.428, avg. samples / sec: 57972.46
Iteration:   2260, Loss function: 5.312, Average Loss: 4.394, avg. samples / sec: 57784.58
Iteration:   2260, Loss function: 4.239, Average Loss: 4.387, avg. samples / sec: 57753.39
Iteration:   2260, Loss function: 3.956, Average Loss: 4.412, avg. samples / sec: 57622.33
Iteration:   2260, Loss function: 5.401, Average Loss: 4.383, avg. samples / sec: 57923.14
Iteration:   2260, Loss function: 4.128, Average Loss: 4.411, avg. samples / sec: 57784.98
Iteration:   2260, Loss function: 3.820, Average Loss: 4.398, avg. samples / sec: 57727.51
Iteration:   2260, Loss function: 4.256, Average Loss: 4.403, avg. samples / sec: 57652.32
Iteration:   2260, Loss function: 4.169, Average Loss: 4.402, avg. samples / sec: 57673.98
Iteration:   2260, Loss function: 4.479, Average Loss: 4.416, avg. samples / sec: 57418.83
Iteration:   2260, Loss function: 3.035, Average Loss: 4.372, avg. samples / sec: 57704.11
Iteration:   2260, Loss function: 5.149, Average Loss: 4.422, avg. samples / sec: 57706.12
Iteration:   2260, Loss function: 5.024, Average Loss: 4.379, avg. samples / sec: 57509.28
Iteration:   2260, Loss function: 4.134, Average Loss: 4.406, avg. samples / sec: 57342.25
Iteration:   2260, Loss function: 4.498, Average Loss: 4.403, avg. samples / sec: 57334.29
Iteration:   2280, Loss function: 3.215, Average Loss: 4.405, avg. samples / sec: 57246.10
Iteration:   2280, Loss function: 5.144, Average Loss: 4.404, avg. samples / sec: 56961.54
Iteration:   2280, Loss function: 4.049, Average Loss: 4.402, avg. samples / sec: 56940.92
Iteration:   2280, Loss function: 3.611, Average Loss: 4.413, avg. samples / sec: 56977.50
Iteration:   2280, Loss function: 4.635, Average Loss: 4.404, avg. samples / sec: 56911.42
Iteration:   2280, Loss function: 3.528, Average Loss: 4.428, avg. samples / sec: 56829.03
Iteration:   2280, Loss function: 4.215, Average Loss: 4.372, avg. samples / sec: 56990.48
Iteration:   2280, Loss function: 3.935, Average Loss: 4.403, avg. samples / sec: 57207.64
Iteration:   2280, Loss function: 4.325, Average Loss: 4.392, avg. samples / sec: 56830.20
Iteration:   2280, Loss function: 4.116, Average Loss: 4.396, avg. samples / sec: 56912.11
Iteration:   2280, Loss function: 3.784, Average Loss: 4.376, avg. samples / sec: 57011.50
Iteration:   2280, Loss function: 4.040, Average Loss: 4.418, avg. samples / sec: 56956.64
Iteration:   2280, Loss function: 4.046, Average Loss: 4.389, avg. samples / sec: 56805.30
Iteration:   2280, Loss function: 4.302, Average Loss: 4.381, avg. samples / sec: 56826.44
Iteration:   2280, Loss function: 4.919, Average Loss: 4.412, avg. samples / sec: 56740.69
:::MLL 1558639007.952 eval_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.01 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.01 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.01 s
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.01 s
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.01 s
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.01 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.01 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.01 s
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.01 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.01 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.01 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.01 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.01 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.01 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 1.01 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.40s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.41s)
DONE (t=0.42s)
DONE (t=0.42s)
DONE (t=0.44s)
DONE (t=2.90s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.14689
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.27595
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.14430
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.03232
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.14909
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.24032
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.16415
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.23829
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.24902
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05632
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24952
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.40146
Current AP: 0.14689 AP goal: 0.23000
:::MLL 1558639012.329 eval_accuracy: {"value": 0.1468949387516648, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 389}}
:::MLL 1558639012.369 eval_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 392}}
:::MLL 1558639012.378 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 804}}
:::MLL 1558639012.378 block_start: {"value": null, "metadata": {"first_epoch_num": 33, "epoch_count": 10.915354834308324, "file": "train.py", "lineno": 813}}
Iteration:   2300, Loss function: 3.448, Average Loss: 4.385, avg. samples / sec: 6550.02
Iteration:   2300, Loss function: 3.758, Average Loss: 4.399, avg. samples / sec: 6547.92
Iteration:   2300, Loss function: 4.430, Average Loss: 4.399, avg. samples / sec: 6548.10
Iteration:   2300, Loss function: 4.125, Average Loss: 4.389, avg. samples / sec: 6548.92
Iteration:   2300, Loss function: 3.911, Average Loss: 4.363, avg. samples / sec: 6548.56
Iteration:   2300, Loss function: 3.948, Average Loss: 4.376, avg. samples / sec: 6548.05
Iteration:   2300, Loss function: 3.575, Average Loss: 4.427, avg. samples / sec: 6546.77
Iteration:   2300, Loss function: 3.818, Average Loss: 4.369, avg. samples / sec: 6547.03
Iteration:   2300, Loss function: 3.285, Average Loss: 4.395, avg. samples / sec: 6545.59
Iteration:   2300, Loss function: 4.742, Average Loss: 4.386, avg. samples / sec: 6546.49
Iteration:   2300, Loss function: 4.756, Average Loss: 4.398, avg. samples / sec: 6545.45
Iteration:   2300, Loss function: 4.685, Average Loss: 4.414, avg. samples / sec: 6546.65
Iteration:   2300, Loss function: 5.100, Average Loss: 4.400, avg. samples / sec: 6546.21
Iteration:   2300, Loss function: 5.234, Average Loss: 4.410, avg. samples / sec: 6547.14
Iteration:   2300, Loss function: 4.129, Average Loss: 4.409, avg. samples / sec: 6544.79
:::MLL 1558639013.188 epoch_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 819}}
:::MLL 1558639013.189 epoch_start: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 673}}
Iteration:   2320, Loss function: 4.563, Average Loss: 4.395, avg. samples / sec: 59358.98
Iteration:   2320, Loss function: 4.080, Average Loss: 4.372, avg. samples / sec: 59218.21
Iteration:   2320, Loss function: 4.565, Average Loss: 4.355, avg. samples / sec: 59148.47
Iteration:   2320, Loss function: 3.310, Average Loss: 4.408, avg. samples / sec: 59296.99
Iteration:   2320, Loss function: 3.739, Average Loss: 4.395, avg. samples / sec: 59260.22
Iteration:   2320, Loss function: 4.040, Average Loss: 4.397, avg. samples / sec: 59089.99
Iteration:   2320, Loss function: 5.333, Average Loss: 4.370, avg. samples / sec: 59174.62
Iteration:   2320, Loss function: 4.672, Average Loss: 4.378, avg. samples / sec: 59227.19
Iteration:   2320, Loss function: 5.677, Average Loss: 4.405, avg. samples / sec: 59268.84
Iteration:   2320, Loss function: 3.838, Average Loss: 4.377, avg. samples / sec: 58889.86
Iteration:   2320, Loss function: 4.290, Average Loss: 4.390, avg. samples / sec: 59169.35
Iteration:   2320, Loss function: 4.727, Average Loss: 4.422, avg. samples / sec: 59083.03
Iteration:   2320, Loss function: 3.684, Average Loss: 4.404, avg. samples / sec: 59145.88
Iteration:   2320, Loss function: 4.558, Average Loss: 4.388, avg. samples / sec: 58874.38
Iteration:   2320, Loss function: 3.807, Average Loss: 4.383, avg. samples / sec: 58890.77
Iteration:   2340, Loss function: 4.714, Average Loss: 4.355, avg. samples / sec: 58805.28
Iteration:   2340, Loss function: 4.317, Average Loss: 4.378, avg. samples / sec: 58827.74
Iteration:   2340, Loss function: 4.001, Average Loss: 4.386, avg. samples / sec: 58857.81
Iteration:   2340, Loss function: 4.327, Average Loss: 4.381, avg. samples / sec: 58816.89
Iteration:   2340, Loss function: 4.324, Average Loss: 4.388, avg. samples / sec: 58538.88
Iteration:   2340, Loss function: 4.596, Average Loss: 4.417, avg. samples / sec: 58708.76
Iteration:   2340, Loss function: 3.124, Average Loss: 4.402, avg. samples / sec: 58751.61
Iteration:   2340, Loss function: 4.500, Average Loss: 4.400, avg. samples / sec: 58623.60
Iteration:   2340, Loss function: 3.827, Average Loss: 4.401, avg. samples / sec: 58495.58
Iteration:   2340, Loss function: 4.874, Average Loss: 4.369, avg. samples / sec: 58520.80
Iteration:   2340, Loss function: 3.619, Average Loss: 4.372, avg. samples / sec: 58426.20
Iteration:   2340, Loss function: 3.292, Average Loss: 4.390, avg. samples / sec: 58438.07
Iteration:   2340, Loss function: 4.259, Average Loss: 4.374, avg. samples / sec: 58477.23
Iteration:   2340, Loss function: 4.587, Average Loss: 4.389, avg. samples / sec: 58307.56
Iteration:   2340, Loss function: 4.467, Average Loss: 4.382, avg. samples / sec: 58409.18
Iteration:   2360, Loss function: 4.766, Average Loss: 4.385, avg. samples / sec: 56982.96
Iteration:   2360, Loss function: 5.546, Average Loss: 4.399, avg. samples / sec: 56887.80
Iteration:   2360, Loss function: 4.402, Average Loss: 4.369, avg. samples / sec: 56928.18
Iteration:   2360, Loss function: 3.657, Average Loss: 4.382, avg. samples / sec: 56711.85
Iteration:   2360, Loss function: 3.203, Average Loss: 4.378, avg. samples / sec: 56967.69
Iteration:   2360, Loss function: 3.877, Average Loss: 4.375, avg. samples / sec: 56669.98
Iteration:   2360, Loss function: 3.887, Average Loss: 4.347, avg. samples / sec: 56451.11
Iteration:   2360, Loss function: 3.860, Average Loss: 4.414, avg. samples / sec: 56670.48
Iteration:   2360, Loss function: 3.427, Average Loss: 4.366, avg. samples / sec: 56784.11
Iteration:   2360, Loss function: 3.653, Average Loss: 4.364, avg. samples / sec: 56729.34
Iteration:   2360, Loss function: 4.936, Average Loss: 4.395, avg. samples / sec: 56684.07
Iteration:   2360, Loss function: 5.134, Average Loss: 4.377, avg. samples / sec: 56431.58
Iteration:   2360, Loss function: 3.755, Average Loss: 4.381, avg. samples / sec: 56513.59
Iteration:   2360, Loss function: 4.142, Average Loss: 4.394, avg. samples / sec: 56563.38
Iteration:   2360, Loss function: 2.428, Average Loss: 4.384, avg. samples / sec: 56787.72
:::MLL 1558639015.206 epoch_stop: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 819}}
:::MLL 1558639015.207 epoch_start: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 673}}
Iteration:   2380, Loss function: 3.549, Average Loss: 4.372, avg. samples / sec: 59563.60
Iteration:   2380, Loss function: 4.297, Average Loss: 4.359, avg. samples / sec: 59245.47
Iteration:   2380, Loss function: 4.309, Average Loss: 4.374, avg. samples / sec: 59375.79
Iteration:   2380, Loss function: 4.108, Average Loss: 4.344, avg. samples / sec: 59277.96
Iteration:   2380, Loss function: 4.038, Average Loss: 4.372, avg. samples / sec: 59205.35
Iteration:   2380, Loss function: 4.964, Average Loss: 4.360, avg. samples / sec: 59231.82
Iteration:   2380, Loss function: 3.686, Average Loss: 4.374, avg. samples / sec: 58968.54
Iteration:   2380, Loss function: 2.767, Average Loss: 4.381, avg. samples / sec: 59205.62
Iteration:   2380, Loss function: 3.980, Average Loss: 4.390, avg. samples / sec: 59274.20
Iteration:   2380, Loss function: 4.193, Average Loss: 4.390, avg. samples / sec: 58913.62
Iteration:   2380, Loss function: 3.620, Average Loss: 4.388, avg. samples / sec: 59154.45
Iteration:   2380, Loss function: 4.435, Average Loss: 4.406, avg. samples / sec: 59083.05
Iteration:   2380, Loss function: 3.891, Average Loss: 4.362, avg. samples / sec: 58984.31
Iteration:   2380, Loss function: 4.594, Average Loss: 4.376, avg. samples / sec: 58866.91
Iteration:   2380, Loss function: 4.539, Average Loss: 4.374, avg. samples / sec: 58869.88
Iteration:   2400, Loss function: 2.906, Average Loss: 4.355, avg. samples / sec: 59351.41
Iteration:   2400, Loss function: 4.123, Average Loss: 4.368, avg. samples / sec: 59471.35
Iteration:   2400, Loss function: 4.726, Average Loss: 4.386, avg. samples / sec: 59496.96
Iteration:   2400, Loss function: 5.336, Average Loss: 4.368, avg. samples / sec: 59335.69
Iteration:   2400, Loss function: 3.496, Average Loss: 4.380, avg. samples / sec: 59393.53
Iteration:   2400, Loss function: 4.924, Average Loss: 4.385, avg. samples / sec: 59427.57
Iteration:   2400, Loss function: 3.440, Average Loss: 4.370, avg. samples / sec: 59618.38
Iteration:   2400, Loss function: 4.960, Average Loss: 4.372, avg. samples / sec: 59091.03
Iteration:   2400, Loss function: 2.912, Average Loss: 4.343, avg. samples / sec: 59164.58
Iteration:   2400, Loss function: 4.494, Average Loss: 4.372, avg. samples / sec: 59495.83
Iteration:   2400, Loss function: 3.167, Average Loss: 4.353, avg. samples / sec: 59388.43
Iteration:   2400, Loss function: 3.417, Average Loss: 4.364, avg. samples / sec: 59070.84
Iteration:   2400, Loss function: 3.926, Average Loss: 4.359, avg. samples / sec: 59180.01
Iteration:   2400, Loss function: 4.317, Average Loss: 4.368, avg. samples / sec: 59200.17
Iteration:   2400, Loss function: 4.340, Average Loss: 4.395, avg. samples / sec: 59233.84
Iteration:   2420, Loss function: 4.092, Average Loss: 4.357, avg. samples / sec: 57482.07
Iteration:   2420, Loss function: 3.989, Average Loss: 4.338, avg. samples / sec: 57348.34
Iteration:   2420, Loss function: 3.828, Average Loss: 4.350, avg. samples / sec: 57378.79
Iteration:   2420, Loss function: 4.555, Average Loss: 4.372, avg. samples / sec: 57232.38
Iteration:   2420, Loss function: 3.933, Average Loss: 4.350, avg. samples / sec: 57109.51
Iteration:   2420, Loss function: 4.635, Average Loss: 4.361, avg. samples / sec: 57364.28
Iteration:   2420, Loss function: 4.341, Average Loss: 4.374, avg. samples / sec: 57295.81
Iteration:   2420, Loss function: 3.588, Average Loss: 4.376, avg. samples / sec: 57129.72
Iteration:   2420, Loss function: 4.012, Average Loss: 4.381, avg. samples / sec: 57153.16
Iteration:   2420, Loss function: 3.481, Average Loss: 4.362, avg. samples / sec: 57031.34
Iteration:   2420, Loss function: 5.535, Average Loss: 4.363, avg. samples / sec: 57060.95
Iteration:   2420, Loss function: 5.042, Average Loss: 4.374, avg. samples / sec: 57112.79
Iteration:   2420, Loss function: 4.491, Average Loss: 4.363, avg. samples / sec: 57073.33
Iteration:   2420, Loss function: 3.799, Average Loss: 4.361, avg. samples / sec: 57197.28
Iteration:   2420, Loss function: 3.021, Average Loss: 4.387, avg. samples / sec: 57221.34
Iteration:   2440, Loss function: 4.401, Average Loss: 4.357, avg. samples / sec: 60224.80
Iteration:   2440, Loss function: 4.469, Average Loss: 4.379, avg. samples / sec: 60301.74
Iteration:   2440, Loss function: 5.162, Average Loss: 4.341, avg. samples / sec: 60160.96
Iteration:   2440, Loss function: 2.385, Average Loss: 4.348, avg. samples / sec: 60083.60
Iteration:   2440, Loss function: 4.400, Average Loss: 4.359, avg. samples / sec: 60205.24
Iteration:   2440, Loss function: 3.454, Average Loss: 4.344, avg. samples / sec: 60033.03
Iteration:   2440, Loss function: 4.247, Average Loss: 4.367, avg. samples / sec: 60031.01
Iteration:   2440, Loss function: 4.356, Average Loss: 4.349, avg. samples / sec: 59916.03
Iteration:   2440, Loss function: 4.569, Average Loss: 4.355, avg. samples / sec: 60182.59
Iteration:   2440, Loss function: 5.039, Average Loss: 4.367, avg. samples / sec: 60053.67
Iteration:   2440, Loss function: 5.165, Average Loss: 4.355, avg. samples / sec: 60198.07
Iteration:   2440, Loss function: 4.477, Average Loss: 4.364, avg. samples / sec: 60147.69
Iteration:   2440, Loss function: 3.853, Average Loss: 4.368, avg. samples / sec: 60015.34
Iteration:   2440, Loss function: 4.200, Average Loss: 4.361, avg. samples / sec: 60030.96
Iteration:   2440, Loss function: 4.415, Average Loss: 4.382, avg. samples / sec: 60028.09
:::MLL 1558639017.203 epoch_stop: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 819}}
:::MLL 1558639017.203 epoch_start: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 673}}
Iteration:   2460, Loss function: 3.737, Average Loss: 4.363, avg. samples / sec: 59615.91
Iteration:   2460, Loss function: 5.184, Average Loss: 4.339, avg. samples / sec: 59328.87
Iteration:   2460, Loss function: 3.606, Average Loss: 4.353, avg. samples / sec: 59409.03
Iteration:   2460, Loss function: 3.557, Average Loss: 4.350, avg. samples / sec: 59387.35
Iteration:   2460, Loss function: 4.047, Average Loss: 4.369, avg. samples / sec: 59421.60
Iteration:   2460, Loss function: 5.817, Average Loss: 4.358, avg. samples / sec: 59441.15
Iteration:   2460, Loss function: 4.616, Average Loss: 4.373, avg. samples / sec: 59220.25
Iteration:   2460, Loss function: 4.065, Average Loss: 4.345, avg. samples / sec: 59379.32
Iteration:   2460, Loss function: 3.997, Average Loss: 4.376, avg. samples / sec: 59464.13
Iteration:   2460, Loss function: 3.564, Average Loss: 4.369, avg. samples / sec: 59267.09
Iteration:   2460, Loss function: 3.318, Average Loss: 4.351, avg. samples / sec: 59387.95
Iteration:   2460, Loss function: 5.525, Average Loss: 4.356, avg. samples / sec: 59010.66
Iteration:   2460, Loss function: 4.116, Average Loss: 4.348, avg. samples / sec: 59191.59
Iteration:   2460, Loss function: 4.037, Average Loss: 4.339, avg. samples / sec: 59103.59
Iteration:   2460, Loss function: 5.538, Average Loss: 4.354, avg. samples / sec: 59155.05
Iteration:   2480, Loss function: 5.044, Average Loss: 4.339, avg. samples / sec: 57171.36
Iteration:   2480, Loss function: 2.912, Average Loss: 4.341, avg. samples / sec: 57283.12
Iteration:   2480, Loss function: 4.288, Average Loss: 4.358, avg. samples / sec: 56920.55
Iteration:   2480, Loss function: 3.601, Average Loss: 4.339, avg. samples / sec: 57116.70
Iteration:   2480, Loss function: 4.079, Average Loss: 4.348, avg. samples / sec: 57344.30
Iteration:   2480, Loss function: 6.156, Average Loss: 4.343, avg. samples / sec: 57263.12
Iteration:   2480, Loss function: 3.076, Average Loss: 4.349, avg. samples / sec: 57043.63
Iteration:   2480, Loss function: 5.274, Average Loss: 4.371, avg. samples / sec: 57153.14
Iteration:   2480, Loss function: 3.633, Average Loss: 4.336, avg. samples / sec: 56917.17
Iteration:   2480, Loss function: 4.466, Average Loss: 4.366, avg. samples / sec: 57183.47
Iteration:   2480, Loss function: 4.016, Average Loss: 4.354, avg. samples / sec: 57197.65
Iteration:   2480, Loss function: 4.553, Average Loss: 4.352, avg. samples / sec: 56944.70
Iteration:   2480, Loss function: 4.129, Average Loss: 4.368, avg. samples / sec: 56950.89
Iteration:   2480, Loss function: 5.717, Average Loss: 4.367, avg. samples / sec: 56894.79
Iteration:   2480, Loss function: 4.146, Average Loss: 4.334, avg. samples / sec: 57166.47
Iteration:   2500, Loss function: 5.834, Average Loss: 4.336, avg. samples / sec: 59577.15
Iteration:   2500, Loss function: 3.733, Average Loss: 4.332, avg. samples / sec: 59454.34
Iteration:   2500, Loss function: 4.025, Average Loss: 4.338, avg. samples / sec: 59355.16
Iteration:   2500, Loss function: 3.612, Average Loss: 4.340, avg. samples / sec: 59209.62
Iteration:   2500, Loss function: 3.015, Average Loss: 4.365, avg. samples / sec: 59389.00
Iteration:   2500, Loss function: 3.949, Average Loss: 4.354, avg. samples / sec: 59290.03
Iteration:   2500, Loss function: 3.765, Average Loss: 4.330, avg. samples / sec: 59500.05
Iteration:   2500, Loss function: 4.513, Average Loss: 4.365, avg. samples / sec: 59456.75
Iteration:   2500, Loss function: 4.664, Average Loss: 4.346, avg. samples / sec: 59337.94
Iteration:   2500, Loss function: 3.768, Average Loss: 4.344, avg. samples / sec: 59336.74
Iteration:   2500, Loss function: 5.196, Average Loss: 4.367, avg. samples / sec: 59393.63
Iteration:   2500, Loss function: 3.714, Average Loss: 4.334, avg. samples / sec: 59203.38
Iteration:   2500, Loss function: 3.195, Average Loss: 4.336, avg. samples / sec: 59206.56
Iteration:   2500, Loss function: 3.527, Average Loss: 4.350, avg. samples / sec: 59294.52
Iteration:   2500, Loss function: 4.314, Average Loss: 4.367, avg. samples / sec: 59205.94
:::MLL 1558639019.206 epoch_stop: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 819}}
:::MLL 1558639019.207 epoch_start: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 673}}
Iteration:   2520, Loss function: 3.601, Average Loss: 4.326, avg. samples / sec: 59525.66
Iteration:   2520, Loss function: 3.972, Average Loss: 4.363, avg. samples / sec: 59742.96
Iteration:   2520, Loss function: 3.810, Average Loss: 4.349, avg. samples / sec: 59516.96
Iteration:   2520, Loss function: 4.428, Average Loss: 4.329, avg. samples / sec: 59186.70
Iteration:   2520, Loss function: 3.897, Average Loss: 4.337, avg. samples / sec: 59459.34
Iteration:   2520, Loss function: 5.102, Average Loss: 4.360, avg. samples / sec: 59515.18
Iteration:   2520, Loss function: 5.069, Average Loss: 4.337, avg. samples / sec: 59450.66
Iteration:   2520, Loss function: 4.804, Average Loss: 4.335, avg. samples / sec: 59349.58
Iteration:   2520, Loss function: 3.933, Average Loss: 4.330, avg. samples / sec: 59476.88
Iteration:   2520, Loss function: 3.203, Average Loss: 4.328, avg. samples / sec: 59435.99
Iteration:   2520, Loss function: 3.418, Average Loss: 4.342, avg. samples / sec: 59393.83
Iteration:   2520, Loss function: 4.066, Average Loss: 4.329, avg. samples / sec: 59188.24
Iteration:   2520, Loss function: 3.999, Average Loss: 4.359, avg. samples / sec: 59254.29
Iteration:   2520, Loss function: 2.760, Average Loss: 4.321, avg. samples / sec: 59243.28
Iteration:   2520, Loss function: 4.225, Average Loss: 4.360, avg. samples / sec: 59122.56
Iteration:   2540, Loss function: 3.129, Average Loss: 4.337, avg. samples / sec: 59219.63
Iteration:   2540, Loss function: 4.382, Average Loss: 4.331, avg. samples / sec: 59299.94
Iteration:   2540, Loss function: 2.953, Average Loss: 4.333, avg. samples / sec: 59229.90
Iteration:   2540, Loss function: 3.248, Average Loss: 4.318, avg. samples / sec: 59324.48
Iteration:   2540, Loss function: 4.329, Average Loss: 4.325, avg. samples / sec: 59114.67
Iteration:   2540, Loss function: 3.306, Average Loss: 4.352, avg. samples / sec: 59288.34
Iteration:   2540, Loss function: 4.033, Average Loss: 4.357, avg. samples / sec: 58977.42
Iteration:   2540, Loss function: 2.961, Average Loss: 4.327, avg. samples / sec: 59117.85
Iteration:   2540, Loss function: 4.221, Average Loss: 4.325, avg. samples / sec: 58872.81
Iteration:   2540, Loss function: 5.039, Average Loss: 4.353, avg. samples / sec: 59337.01
Iteration:   2540, Loss function: 4.366, Average Loss: 4.326, avg. samples / sec: 59103.32
Iteration:   2540, Loss function: 4.421, Average Loss: 4.337, avg. samples / sec: 59144.07
Iteration:   2540, Loss function: 4.731, Average Loss: 4.352, avg. samples / sec: 58905.93
Iteration:   2540, Loss function: 3.440, Average Loss: 4.328, avg. samples / sec: 58922.75
Iteration:   2540, Loss function: 3.707, Average Loss: 4.324, avg. samples / sec: 58971.62
Iteration:   2560, Loss function: 4.014, Average Loss: 4.326, avg. samples / sec: 57470.11
Iteration:   2560, Loss function: 3.634, Average Loss: 4.322, avg. samples / sec: 57705.22
Iteration:   2560, Loss function: 3.911, Average Loss: 4.350, avg. samples / sec: 57418.20
Iteration:   2560, Loss function: 4.974, Average Loss: 4.327, avg. samples / sec: 57270.01
Iteration:   2560, Loss function: 4.983, Average Loss: 4.349, avg. samples / sec: 57378.09
Iteration:   2560, Loss function: 3.545, Average Loss: 4.320, avg. samples / sec: 57357.35
Iteration:   2560, Loss function: 4.707, Average Loss: 4.334, avg. samples / sec: 57447.58
Iteration:   2560, Loss function: 4.888, Average Loss: 4.327, avg. samples / sec: 57423.09
Iteration:   2560, Loss function: 4.283, Average Loss: 4.314, avg. samples / sec: 57307.04
Iteration:   2560, Loss function: 3.804, Average Loss: 4.343, avg. samples / sec: 57505.64
Iteration:   2560, Loss function: 3.830, Average Loss: 4.332, avg. samples / sec: 57107.77
Iteration:   2560, Loss function: 4.933, Average Loss: 4.346, avg. samples / sec: 57329.58
Iteration:   2560, Loss function: 4.513, Average Loss: 4.314, avg. samples / sec: 57208.36
Iteration:   2560, Loss function: 4.385, Average Loss: 4.319, avg. samples / sec: 57522.73
Iteration:   2560, Loss function: 4.409, Average Loss: 4.321, avg. samples / sec: 57272.08
Iteration:   2580, Loss function: 3.546, Average Loss: 4.315, avg. samples / sec: 59498.52
Iteration:   2580, Loss function: 3.880, Average Loss: 4.319, avg. samples / sec: 59286.32
Iteration:   2580, Loss function: 3.724, Average Loss: 4.348, avg. samples / sec: 59410.71
Iteration:   2580, Loss function: 3.723, Average Loss: 4.313, avg. samples / sec: 59556.53
Iteration:   2580, Loss function: 4.029, Average Loss: 4.324, avg. samples / sec: 59359.56
Iteration:   2580, Loss function: 3.893, Average Loss: 4.320, avg. samples / sec: 59560.91
Iteration:   2580, Loss function: 3.362, Average Loss: 4.314, avg. samples / sec: 59413.04
Iteration:   2580, Loss function: 3.982, Average Loss: 4.336, avg. samples / sec: 59431.98
Iteration:   2580, Loss function: 2.586, Average Loss: 4.311, avg. samples / sec: 59428.55
Iteration:   2580, Loss function: 2.626, Average Loss: 4.318, avg. samples / sec: 59237.92
Iteration:   2580, Loss function: 3.238, Average Loss: 4.306, avg. samples / sec: 59419.80
Iteration:   2580, Loss function: 4.349, Average Loss: 4.330, avg. samples / sec: 59292.93
Iteration:   2580, Loss function: 3.377, Average Loss: 4.342, avg. samples / sec: 59208.28
Iteration:   2580, Loss function: 4.045, Average Loss: 4.332, avg. samples / sec: 59267.72
Iteration:   2580, Loss function: 3.384, Average Loss: 4.338, avg. samples / sec: 59284.30
:::MLL 1558639021.213 epoch_stop: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 819}}
:::MLL 1558639021.213 epoch_start: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 673}}
Iteration:   2600, Loss function: 3.670, Average Loss: 4.308, avg. samples / sec: 59031.57
Iteration:   2600, Loss function: 3.936, Average Loss: 4.338, avg. samples / sec: 59219.73
Iteration:   2600, Loss function: 2.841, Average Loss: 4.315, avg. samples / sec: 59053.74
Iteration:   2600, Loss function: 3.347, Average Loss: 4.311, avg. samples / sec: 59077.01
Iteration:   2600, Loss function: 3.431, Average Loss: 4.346, avg. samples / sec: 59009.80
Iteration:   2600, Loss function: 4.092, Average Loss: 4.325, avg. samples / sec: 59218.43
Iteration:   2600, Loss function: 4.903, Average Loss: 4.332, avg. samples / sec: 59038.92
Iteration:   2600, Loss function: 4.650, Average Loss: 4.334, avg. samples / sec: 59196.52
Iteration:   2600, Loss function: 3.941, Average Loss: 4.313, avg. samples / sec: 59000.04
Iteration:   2600, Loss function: 4.648, Average Loss: 4.316, avg. samples / sec: 58954.53
Iteration:   2600, Loss function: 5.003, Average Loss: 4.304, avg. samples / sec: 59051.36
Iteration:   2600, Loss function: 3.618, Average Loss: 4.312, avg. samples / sec: 58956.99
Iteration:   2600, Loss function: 4.598, Average Loss: 4.327, avg. samples / sec: 59035.09
Iteration:   2600, Loss function: 4.602, Average Loss: 4.308, avg. samples / sec: 58812.96
Iteration:   2600, Loss function: 3.888, Average Loss: 4.306, avg. samples / sec: 58683.85
Iteration:   2620, Loss function: 3.750, Average Loss: 4.321, avg. samples / sec: 59176.98
Iteration:   2620, Loss function: 4.541, Average Loss: 4.345, avg. samples / sec: 58961.38
Iteration:   2620, Loss function: 4.699, Average Loss: 4.301, avg. samples / sec: 59213.80
Iteration:   2620, Loss function: 3.474, Average Loss: 4.311, avg. samples / sec: 59026.73
Iteration:   2620, Loss function: 2.645, Average Loss: 4.300, avg. samples / sec: 59090.78
Iteration:   2620, Loss function: 4.344, Average Loss: 4.334, avg. samples / sec: 58886.78
Iteration:   2620, Loss function: 3.279, Average Loss: 4.335, avg. samples / sec: 58721.82
Iteration:   2620, Loss function: 4.018, Average Loss: 4.300, avg. samples / sec: 58848.08
Iteration:   2620, Loss function: 4.682, Average Loss: 4.309, avg. samples / sec: 58831.35
Iteration:   2620, Loss function: 3.168, Average Loss: 4.298, avg. samples / sec: 58704.55
Iteration:   2620, Loss function: 4.202, Average Loss: 4.326, avg. samples / sec: 58692.94
Iteration:   2620, Loss function: 3.823, Average Loss: 4.309, avg. samples / sec: 58524.95
Iteration:   2620, Loss function: 3.016, Average Loss: 4.316, avg. samples / sec: 58554.62
Iteration:   2620, Loss function: 4.801, Average Loss: 4.313, avg. samples / sec: 58657.49
Iteration:   2620, Loss function: 3.076, Average Loss: 4.309, avg. samples / sec: 58464.76
Iteration:   2640, Loss function: 4.355, Average Loss: 4.306, avg. samples / sec: 59386.10
Iteration:   2640, Loss function: 4.247, Average Loss: 4.329, avg. samples / sec: 59031.55
Iteration:   2640, Loss function: 4.400, Average Loss: 4.319, avg. samples / sec: 58746.01
Iteration:   2640, Loss function: 3.052, Average Loss: 4.302, avg. samples / sec: 59034.02
Iteration:   2640, Loss function: 4.787, Average Loss: 4.305, avg. samples / sec: 59125.64
Iteration:   2640, Loss function: 4.061, Average Loss: 4.343, avg. samples / sec: 58681.92
Iteration:   2640, Loss function: 3.872, Average Loss: 4.295, avg. samples / sec: 58867.89
Iteration:   2640, Loss function: 4.361, Average Loss: 4.296, avg. samples / sec: 58667.87
Iteration:   2640, Loss function: 3.077, Average Loss: 4.292, avg. samples / sec: 58713.31
Iteration:   2640, Loss function: 3.261, Average Loss: 4.330, avg. samples / sec: 58759.28
Iteration:   2640, Loss function: 4.415, Average Loss: 4.316, avg. samples / sec: 58637.14
Iteration:   2640, Loss function: 3.649, Average Loss: 4.320, avg. samples / sec: 58894.53
Iteration:   2640, Loss function: 4.546, Average Loss: 4.308, avg. samples / sec: 58885.87
Iteration:   2640, Loss function: 3.561, Average Loss: 4.308, avg. samples / sec: 58955.02
Iteration:   2640, Loss function: 4.130, Average Loss: 4.291, avg. samples / sec: 58777.96
:::MLL 1558639023.219 epoch_stop: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 819}}
:::MLL 1558639023.220 epoch_start: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 673}}
Iteration:   2660, Loss function: 4.461, Average Loss: 4.345, avg. samples / sec: 57873.38
Iteration:   2660, Loss function: 2.588, Average Loss: 4.312, avg. samples / sec: 57917.35
Iteration:   2660, Loss function: 3.775, Average Loss: 4.296, avg. samples / sec: 57837.58
Iteration:   2660, Loss function: 4.144, Average Loss: 4.329, avg. samples / sec: 57647.06
Iteration:   2660, Loss function: 5.504, Average Loss: 4.305, avg. samples / sec: 57918.00
Iteration:   2660, Loss function: 4.271, Average Loss: 4.306, avg. samples / sec: 57557.87
Iteration:   2660, Loss function: 4.864, Average Loss: 4.315, avg. samples / sec: 57627.33
Iteration:   2660, Loss function: 4.042, Average Loss: 4.305, avg. samples / sec: 57892.35
Iteration:   2660, Loss function: 5.037, Average Loss: 4.291, avg. samples / sec: 57831.67
Iteration:   2660, Loss function: 4.366, Average Loss: 4.292, avg. samples / sec: 57740.66
Iteration:   2660, Loss function: 4.245, Average Loss: 4.299, avg. samples / sec: 57672.94
Iteration:   2660, Loss function: 3.856, Average Loss: 4.295, avg. samples / sec: 57534.85
Iteration:   2660, Loss function: 4.134, Average Loss: 4.325, avg. samples / sec: 57707.28
Iteration:   2660, Loss function: 3.419, Average Loss: 4.298, avg. samples / sec: 57582.62
Iteration:   2660, Loss function: 4.206, Average Loss: 4.327, avg. samples / sec: 57479.21
Iteration:   2680, Loss function: 2.735, Average Loss: 4.298, avg. samples / sec: 59011.92
Iteration:   2680, Loss function: 5.878, Average Loss: 4.300, avg. samples / sec: 58990.88
Iteration:   2680, Loss function: 4.454, Average Loss: 4.319, avg. samples / sec: 58966.54
Iteration:   2680, Loss function: 4.394, Average Loss: 4.317, avg. samples / sec: 59361.86
Iteration:   2680, Loss function: 4.456, Average Loss: 4.307, avg. samples / sec: 58975.52
Iteration:   2680, Loss function: 4.334, Average Loss: 4.292, avg. samples / sec: 58853.19
Iteration:   2680, Loss function: 3.632, Average Loss: 4.300, avg. samples / sec: 58782.98
Iteration:   2680, Loss function: 3.190, Average Loss: 4.297, avg. samples / sec: 58827.13
Iteration:   2680, Loss function: 3.534, Average Loss: 4.296, avg. samples / sec: 58937.39
Iteration:   2680, Loss function: 3.728, Average Loss: 4.289, avg. samples / sec: 58869.15
Iteration:   2680, Loss function: 4.264, Average Loss: 4.291, avg. samples / sec: 58963.73
Iteration:   2680, Loss function: 4.096, Average Loss: 4.291, avg. samples / sec: 58901.48
Iteration:   2680, Loss function: 3.238, Average Loss: 4.339, avg. samples / sec: 58652.78
Iteration:   2680, Loss function: 3.568, Average Loss: 4.286, avg. samples / sec: 58851.77
Iteration:   2680, Loss function: 4.843, Average Loss: 4.323, avg. samples / sec: 58768.01
Iteration:   2700, Loss function: 3.630, Average Loss: 4.294, avg. samples / sec: 57895.61
Iteration:   2700, Loss function: 3.981, Average Loss: 4.319, avg. samples / sec: 57754.84
Iteration:   2700, Loss function: 3.107, Average Loss: 4.279, avg. samples / sec: 57937.14
Iteration:   2700, Loss function: 4.499, Average Loss: 4.291, avg. samples / sec: 57903.38
Iteration:   2700, Loss function: 4.310, Average Loss: 4.289, avg. samples / sec: 57795.55
Iteration:   2700, Loss function: 4.435, Average Loss: 4.299, avg. samples / sec: 57654.96
Iteration:   2700, Loss function: 3.105, Average Loss: 4.338, avg. samples / sec: 57891.11
Iteration:   2700, Loss function: 4.708, Average Loss: 4.283, avg. samples / sec: 57882.19
Iteration:   2700, Loss function: 2.984, Average Loss: 4.321, avg. samples / sec: 57972.03
Iteration:   2700, Loss function: 4.037, Average Loss: 4.298, avg. samples / sec: 57576.69
Iteration:   2700, Loss function: 3.931, Average Loss: 4.289, avg. samples / sec: 57812.67
Iteration:   2700, Loss function: 4.019, Average Loss: 4.303, avg. samples / sec: 57593.61
Iteration:   2700, Loss function: 4.399, Average Loss: 4.299, avg. samples / sec: 57658.43
Iteration:   2700, Loss function: 3.726, Average Loss: 4.312, avg. samples / sec: 57465.61
Iteration:   2700, Loss function: 5.184, Average Loss: 4.297, avg. samples / sec: 57578.45
Iteration:   2720, Loss function: 4.369, Average Loss: 4.322, avg. samples / sec: 57637.70
Iteration:   2720, Loss function: 3.519, Average Loss: 4.294, avg. samples / sec: 57788.80
Iteration:   2720, Loss function: 2.918, Average Loss: 4.312, avg. samples / sec: 57705.24
Iteration:   2720, Loss function: 4.101, Average Loss: 4.294, avg. samples / sec: 57898.39
Iteration:   2720, Loss function: 4.532, Average Loss: 4.291, avg. samples / sec: 57580.03
Iteration:   2720, Loss function: 5.270, Average Loss: 4.287, avg. samples / sec: 57658.31
Iteration:   2720, Loss function: 3.952, Average Loss: 4.336, avg. samples / sec: 57555.08
Iteration:   2720, Loss function: 2.965, Average Loss: 4.276, avg. samples / sec: 57551.08
Iteration:   2720, Loss function: 3.120, Average Loss: 4.297, avg. samples / sec: 57722.38
Iteration:   2720, Loss function: 3.943, Average Loss: 4.305, avg. samples / sec: 57724.39
Iteration:   2720, Loss function: 4.829, Average Loss: 4.277, avg. samples / sec: 57470.09
Iteration:   2720, Loss function: 4.124, Average Loss: 4.296, avg. samples / sec: 57514.18
Iteration:   2720, Loss function: 4.329, Average Loss: 4.288, avg. samples / sec: 57472.32
Iteration:   2720, Loss function: 5.366, Average Loss: 4.303, avg. samples / sec: 57582.00
Iteration:   2720, Loss function: 4.121, Average Loss: 4.294, avg. samples / sec: 57346.33
:::MLL 1558639025.238 epoch_stop: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 819}}
:::MLL 1558639025.238 epoch_start: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 673}}
Iteration:   2740, Loss function: 4.269, Average Loss: 4.285, avg. samples / sec: 59239.32
Iteration:   2740, Loss function: 3.282, Average Loss: 4.288, avg. samples / sec: 59156.63
Iteration:   2740, Loss function: 3.788, Average Loss: 4.288, avg. samples / sec: 59215.94
Iteration:   2740, Loss function: 3.985, Average Loss: 4.315, avg. samples / sec: 58815.29
Iteration:   2740, Loss function: 4.125, Average Loss: 4.289, avg. samples / sec: 58825.50
Iteration:   2740, Loss function: 4.026, Average Loss: 4.273, avg. samples / sec: 59028.58
Iteration:   2740, Loss function: 4.494, Average Loss: 4.283, avg. samples / sec: 58929.31
Iteration:   2740, Loss function: 4.365, Average Loss: 4.287, avg. samples / sec: 59007.01
Iteration:   2740, Loss function: 3.947, Average Loss: 4.299, avg. samples / sec: 59048.37
Iteration:   2740, Loss function: 3.635, Average Loss: 4.269, avg. samples / sec: 58943.70
Iteration:   2740, Loss function: 4.400, Average Loss: 4.301, avg. samples / sec: 58928.25
Iteration:   2740, Loss function: 4.844, Average Loss: 4.307, avg. samples / sec: 58789.31
Iteration:   2740, Loss function: 4.185, Average Loss: 4.284, avg. samples / sec: 58797.11
Iteration:   2740, Loss function: 4.494, Average Loss: 4.331, avg. samples / sec: 58822.75
Iteration:   2740, Loss function: 2.916, Average Loss: 4.289, avg. samples / sec: 58034.34
Iteration:   2760, Loss function: 3.692, Average Loss: 4.283, avg. samples / sec: 58803.07
Iteration:   2760, Loss function: 3.636, Average Loss: 4.306, avg. samples / sec: 58953.27
Iteration:   2760, Loss function: 4.408, Average Loss: 4.283, avg. samples / sec: 58626.50
Iteration:   2760, Loss function: 3.647, Average Loss: 4.277, avg. samples / sec: 58826.44
Iteration:   2760, Loss function: 3.643, Average Loss: 4.294, avg. samples / sec: 58771.41
Iteration:   2760, Loss function: 4.313, Average Loss: 4.300, avg. samples / sec: 58828.01
Iteration:   2760, Loss function: 3.955, Average Loss: 4.279, avg. samples / sec: 58874.24
Iteration:   2760, Loss function: 3.189, Average Loss: 4.308, avg. samples / sec: 58702.74
Iteration:   2760, Loss function: 4.970, Average Loss: 4.282, avg. samples / sec: 58684.07
Iteration:   2760, Loss function: 4.089, Average Loss: 4.284, avg. samples / sec: 59550.46
Iteration:   2760, Loss function: 5.027, Average Loss: 4.272, avg. samples / sec: 58694.60
Iteration:   2760, Loss function: 4.527, Average Loss: 4.286, avg. samples / sec: 58516.16
Iteration:   2760, Loss function: 3.478, Average Loss: 4.268, avg. samples / sec: 58587.75
Iteration:   2760, Loss function: 3.928, Average Loss: 4.327, avg. samples / sec: 58746.67
Iteration:   2760, Loss function: 4.037, Average Loss: 4.290, avg. samples / sec: 58555.69
Iteration:   2780, Loss function: 3.038, Average Loss: 4.302, avg. samples / sec: 58161.88
Iteration:   2780, Loss function: 3.766, Average Loss: 4.280, avg. samples / sec: 58141.96
Iteration:   2780, Loss function: 4.241, Average Loss: 4.269, avg. samples / sec: 58113.74
Iteration:   2780, Loss function: 4.128, Average Loss: 4.284, avg. samples / sec: 57838.94
Iteration:   2780, Loss function: 4.296, Average Loss: 4.283, avg. samples / sec: 57924.38
Iteration:   2780, Loss function: 4.664, Average Loss: 4.288, avg. samples / sec: 58167.47
Iteration:   2780, Loss function: 4.979, Average Loss: 4.302, avg. samples / sec: 57891.82
Iteration:   2780, Loss function: 2.864, Average Loss: 4.279, avg. samples / sec: 58047.96
Iteration:   2780, Loss function: 3.466, Average Loss: 4.292, avg. samples / sec: 57940.62
Iteration:   2780, Loss function: 5.415, Average Loss: 4.261, avg. samples / sec: 58105.57
Iteration:   2780, Loss function: 3.921, Average Loss: 4.270, avg. samples / sec: 57879.53
Iteration:   2780, Loss function: 3.552, Average Loss: 4.297, avg. samples / sec: 57948.86
Iteration:   2780, Loss function: 4.715, Average Loss: 4.275, avg. samples / sec: 57930.16
Iteration:   2780, Loss function: 3.366, Average Loss: 4.281, avg. samples / sec: 58005.75
Iteration:   2780, Loss function: 4.181, Average Loss: 4.327, avg. samples / sec: 58024.45
:::MLL 1558639027.253 epoch_stop: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 819}}
:::MLL 1558639027.253 epoch_start: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 673}}
Iteration:   2800, Loss function: 3.781, Average Loss: 4.278, avg. samples / sec: 58655.47
Iteration:   2800, Loss function: 5.496, Average Loss: 4.323, avg. samples / sec: 58526.90
Iteration:   2800, Loss function: 3.010, Average Loss: 4.265, avg. samples / sec: 58341.09
Iteration:   2800, Loss function: 4.599, Average Loss: 4.297, avg. samples / sec: 58232.65
Iteration:   2800, Loss function: 4.110, Average Loss: 4.278, avg. samples / sec: 58376.23
Iteration:   2800, Loss function: 4.037, Average Loss: 4.301, avg. samples / sec: 58359.01
Iteration:   2800, Loss function: 4.508, Average Loss: 4.262, avg. samples / sec: 58346.62
Iteration:   2800, Loss function: 2.580, Average Loss: 4.281, avg. samples / sec: 58271.54
Iteration:   2800, Loss function: 4.180, Average Loss: 4.256, avg. samples / sec: 58319.43
Iteration:   2800, Loss function: 4.536, Average Loss: 4.287, avg. samples / sec: 58300.95
Iteration:   2800, Loss function: 3.675, Average Loss: 4.272, avg. samples / sec: 58326.14
Iteration:   2800, Loss function: 3.537, Average Loss: 4.272, avg. samples / sec: 58166.05
Iteration:   2800, Loss function: 4.373, Average Loss: 4.288, avg. samples / sec: 58182.50
Iteration:   2800, Loss function: 5.165, Average Loss: 4.291, avg. samples / sec: 58119.47
Iteration:   2800, Loss function: 2.376, Average Loss: 4.273, avg. samples / sec: 58107.70
Iteration:   2820, Loss function: 3.748, Average Loss: 4.275, avg. samples / sec: 59256.38
Iteration:   2820, Loss function: 4.325, Average Loss: 4.261, avg. samples / sec: 59297.32
Iteration:   2820, Loss function: 3.667, Average Loss: 4.276, avg. samples / sec: 59240.79
Iteration:   2820, Loss function: 3.818, Average Loss: 4.275, avg. samples / sec: 58939.93
Iteration:   2820, Loss function: 3.779, Average Loss: 4.273, avg. samples / sec: 59254.26
Iteration:   2820, Loss function: 3.242, Average Loss: 4.270, avg. samples / sec: 59376.21
Iteration:   2820, Loss function: 4.157, Average Loss: 4.282, avg. samples / sec: 59336.76
Iteration:   2820, Loss function: 3.949, Average Loss: 4.259, avg. samples / sec: 59054.60
Iteration:   2820, Loss function: 4.116, Average Loss: 4.292, avg. samples / sec: 59059.60
Iteration:   2820, Loss function: 4.610, Average Loss: 4.267, avg. samples / sec: 59171.21
Iteration:   2820, Loss function: 4.687, Average Loss: 4.288, avg. samples / sec: 59271.46
Iteration:   2820, Loss function: 3.318, Average Loss: 4.280, avg. samples / sec: 59047.01
Iteration:   2820, Loss function: 2.222, Average Loss: 4.289, avg. samples / sec: 58925.12
Iteration:   2820, Loss function: 3.774, Average Loss: 4.320, avg. samples / sec: 58843.48
Iteration:   2820, Loss function: 4.935, Average Loss: 4.249, avg. samples / sec: 58902.09
Iteration:   2840, Loss function: 3.723, Average Loss: 4.271, avg. samples / sec: 58253.02
Iteration:   2840, Loss function: 3.847, Average Loss: 4.251, avg. samples / sec: 58154.10
Iteration:   2840, Loss function: 3.210, Average Loss: 4.255, avg. samples / sec: 58261.98
Iteration:   2840, Loss function: 4.306, Average Loss: 4.270, avg. samples / sec: 58065.71
Iteration:   2840, Loss function: 3.782, Average Loss: 4.275, avg. samples / sec: 58154.63
Iteration:   2840, Loss function: 5.782, Average Loss: 4.286, avg. samples / sec: 58270.02
Iteration:   2840, Loss function: 4.515, Average Loss: 4.245, avg. samples / sec: 58382.83
Iteration:   2840, Loss function: 5.730, Average Loss: 4.266, avg. samples / sec: 58060.04
Iteration:   2840, Loss function: 3.819, Average Loss: 4.273, avg. samples / sec: 58214.59
Iteration:   2840, Loss function: 2.671, Average Loss: 4.283, avg. samples / sec: 58149.28
Iteration:   2840, Loss function: 4.326, Average Loss: 4.288, avg. samples / sec: 58095.77
Iteration:   2840, Loss function: 4.666, Average Loss: 4.262, avg. samples / sec: 58075.45
Iteration:   2840, Loss function: 4.007, Average Loss: 4.267, avg. samples / sec: 57962.56
Iteration:   2840, Loss function: 4.510, Average Loss: 4.313, avg. samples / sec: 58146.90
Iteration:   2840, Loss function: 4.155, Average Loss: 4.275, avg. samples / sec: 57801.48
Iteration:   2860, Loss function: 3.242, Average Loss: 4.268, avg. samples / sec: 56599.79
Iteration:   2860, Loss function: 5.302, Average Loss: 4.246, avg. samples / sec: 56618.60
Iteration:   2860, Loss function: 3.178, Average Loss: 4.261, avg. samples / sec: 56643.24
Iteration:   2860, Loss function: 4.180, Average Loss: 4.239, avg. samples / sec: 56733.09
Iteration:   2860, Loss function: 4.935, Average Loss: 4.272, avg. samples / sec: 56681.97
Iteration:   2860, Loss function: 3.638, Average Loss: 4.262, avg. samples / sec: 56833.20
Iteration:   2860, Loss function: 3.198, Average Loss: 4.275, avg. samples / sec: 56749.10
Iteration:   2860, Loss function: 3.848, Average Loss: 4.308, avg. samples / sec: 56880.69
Iteration:   2860, Loss function: 3.494, Average Loss: 4.281, avg. samples / sec: 56703.00
Iteration:   2860, Loss function: 4.952, Average Loss: 4.275, avg. samples / sec: 56959.10
Iteration:   2860, Loss function: 3.134, Average Loss: 4.242, avg. samples / sec: 56590.04
Iteration:   2860, Loss function: 3.565, Average Loss: 4.261, avg. samples / sec: 56721.30
Iteration:   2860, Loss function: 4.002, Average Loss: 4.263, avg. samples / sec: 56728.75
Iteration:   2860, Loss function: 3.811, Average Loss: 4.282, avg. samples / sec: 56741.65
Iteration:   2860, Loss function: 4.615, Average Loss: 4.259, avg. samples / sec: 56768.53
:::MLL 1558639029.271 epoch_stop: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 819}}
:::MLL 1558639029.271 epoch_start: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 673}}
Iteration:   2880, Loss function: 3.612, Average Loss: 4.276, avg. samples / sec: 58158.21
Iteration:   2880, Loss function: 4.189, Average Loss: 4.242, avg. samples / sec: 58099.44
Iteration:   2880, Loss function: 4.811, Average Loss: 4.258, avg. samples / sec: 58070.50
Iteration:   2880, Loss function: 3.440, Average Loss: 4.240, avg. samples / sec: 58108.47
Iteration:   2880, Loss function: 3.082, Average Loss: 4.264, avg. samples / sec: 57978.78
Iteration:   2880, Loss function: 4.302, Average Loss: 4.269, avg. samples / sec: 58041.53
Iteration:   2880, Loss function: 4.054, Average Loss: 4.263, avg. samples / sec: 58030.16
Iteration:   2880, Loss function: 3.880, Average Loss: 4.269, avg. samples / sec: 58012.38
Iteration:   2880, Loss function: 4.357, Average Loss: 4.302, avg. samples / sec: 58017.76
Iteration:   2880, Loss function: 2.876, Average Loss: 4.261, avg. samples / sec: 57980.66
Iteration:   2880, Loss function: 3.181, Average Loss: 4.260, avg. samples / sec: 57983.31
Iteration:   2880, Loss function: 4.431, Average Loss: 4.278, avg. samples / sec: 57972.03
Iteration:   2880, Loss function: 4.088, Average Loss: 4.240, avg. samples / sec: 57850.64
Iteration:   2880, Loss function: 3.755, Average Loss: 4.254, avg. samples / sec: 57921.04
Iteration:   2880, Loss function: 5.092, Average Loss: 4.255, avg. samples / sec: 57892.01
Iteration:   2900, Loss function: 3.827, Average Loss: 4.261, avg. samples / sec: 57627.85
Iteration:   2900, Loss function: 3.856, Average Loss: 4.274, avg. samples / sec: 57523.22
Iteration:   2900, Loss function: 3.960, Average Loss: 4.255, avg. samples / sec: 57676.74
Iteration:   2900, Loss function: 5.162, Average Loss: 4.236, avg. samples / sec: 57736.59
Iteration:   2900, Loss function: 3.001, Average Loss: 4.255, avg. samples / sec: 57662.18
Iteration:   2900, Loss function: 3.257, Average Loss: 4.243, avg. samples / sec: 57521.09
Iteration:   2900, Loss function: 4.085, Average Loss: 4.301, avg. samples / sec: 57540.81
Iteration:   2900, Loss function: 4.206, Average Loss: 4.238, avg. samples / sec: 57444.53
Iteration:   2900, Loss function: 6.153, Average Loss: 4.260, avg. samples / sec: 57523.88
Iteration:   2900, Loss function: 3.037, Average Loss: 4.258, avg. samples / sec: 57513.69
Iteration:   2900, Loss function: 3.219, Average Loss: 4.252, avg. samples / sec: 57416.96
Iteration:   2900, Loss function: 4.470, Average Loss: 4.266, avg. samples / sec: 57499.33
Iteration:   2900, Loss function: 5.541, Average Loss: 4.271, avg. samples / sec: 57563.94
Iteration:   2900, Loss function: 4.371, Average Loss: 4.251, avg. samples / sec: 57571.14
Iteration:   2900, Loss function: 5.224, Average Loss: 4.250, avg. samples / sec: 57496.47
Iteration:   2920, Loss function: 4.072, Average Loss: 4.259, avg. samples / sec: 59531.90
Iteration:   2920, Loss function: 3.222, Average Loss: 4.242, avg. samples / sec: 59466.99
Iteration:   2920, Loss function: 4.594, Average Loss: 4.273, avg. samples / sec: 59339.36
Iteration:   2920, Loss function: 4.142, Average Loss: 4.241, avg. samples / sec: 59327.00
Iteration:   2920, Loss function: 4.638, Average Loss: 4.244, avg. samples / sec: 59566.52
Iteration:   2920, Loss function: 4.337, Average Loss: 4.255, avg. samples / sec: 59253.56
Iteration:   2920, Loss function: 3.293, Average Loss: 4.249, avg. samples / sec: 59414.61
Iteration:   2920, Loss function: 4.566, Average Loss: 4.273, avg. samples / sec: 59422.08
Iteration:   2920, Loss function: 5.012, Average Loss: 4.250, avg. samples / sec: 59185.63
Iteration:   2920, Loss function: 3.881, Average Loss: 4.301, avg. samples / sec: 59286.84
Iteration:   2920, Loss function: 3.806, Average Loss: 4.249, avg. samples / sec: 59373.86
Iteration:   2920, Loss function: 5.891, Average Loss: 4.240, avg. samples / sec: 59142.21
Iteration:   2920, Loss function: 4.096, Average Loss: 4.252, avg. samples / sec: 59161.01
Iteration:   2920, Loss function: 3.441, Average Loss: 4.264, avg. samples / sec: 59215.79
Iteration:   2920, Loss function: 4.275, Average Loss: 4.253, avg. samples / sec: 59110.06
:::MLL 1558639031.287 epoch_stop: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 819}}
:::MLL 1558639031.288 epoch_start: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 673}}
Iteration:   2940, Loss function: 4.959, Average Loss: 4.240, avg. samples / sec: 59574.81
Iteration:   2940, Loss function: 3.785, Average Loss: 4.268, avg. samples / sec: 59592.62
Iteration:   2940, Loss function: 4.499, Average Loss: 4.297, avg. samples / sec: 59551.50
Iteration:   2940, Loss function: 3.812, Average Loss: 4.249, avg. samples / sec: 59292.45
Iteration:   2940, Loss function: 3.522, Average Loss: 4.246, avg. samples / sec: 59545.86
Iteration:   2940, Loss function: 4.952, Average Loss: 4.265, avg. samples / sec: 59339.89
Iteration:   2940, Loss function: 4.460, Average Loss: 4.241, avg. samples / sec: 59417.70
Iteration:   2940, Loss function: 3.560, Average Loss: 4.236, avg. samples / sec: 59280.06
Iteration:   2940, Loss function: 3.168, Average Loss: 4.256, avg. samples / sec: 59562.49
Iteration:   2940, Loss function: 4.393, Average Loss: 4.242, avg. samples / sec: 59403.09
Iteration:   2940, Loss function: 4.626, Average Loss: 4.252, avg. samples / sec: 59310.97
Iteration:   2940, Loss function: 4.061, Average Loss: 4.234, avg. samples / sec: 59293.18
Iteration:   2940, Loss function: 3.275, Average Loss: 4.249, avg. samples / sec: 59570.78
Iteration:   2940, Loss function: 4.450, Average Loss: 4.235, avg. samples / sec: 59385.75
Iteration:   2940, Loss function: 4.113, Average Loss: 4.242, avg. samples / sec: 59372.36
Iteration:   2960, Loss function: 3.373, Average Loss: 4.236, avg. samples / sec: 58434.95
Iteration:   2960, Loss function: 3.800, Average Loss: 4.235, avg. samples / sec: 58275.93
Iteration:   2960, Loss function: 3.286, Average Loss: 4.240, avg. samples / sec: 58382.25
Iteration:   2960, Loss function: 3.608, Average Loss: 4.235, avg. samples / sec: 58290.68
Iteration:   2960, Loss function: 4.281, Average Loss: 4.241, avg. samples / sec: 58219.76
Iteration:   2960, Loss function: 3.879, Average Loss: 4.225, avg. samples / sec: 58317.24
Iteration:   2960, Loss function: 3.689, Average Loss: 4.242, avg. samples / sec: 58109.43
Iteration:   2960, Loss function: 4.085, Average Loss: 4.237, avg. samples / sec: 58294.56
Iteration:   2960, Loss function: 3.757, Average Loss: 4.259, avg. samples / sec: 57946.69
Iteration:   2960, Loss function: 4.134, Average Loss: 4.287, avg. samples / sec: 58057.74
Iteration:   2960, Loss function: 3.992, Average Loss: 4.242, avg. samples / sec: 58151.49
Iteration:   2960, Loss function: 4.070, Average Loss: 4.234, avg. samples / sec: 58206.82
Iteration:   2960, Loss function: 4.015, Average Loss: 4.250, avg. samples / sec: 57996.29
Iteration:   2960, Loss function: 3.335, Average Loss: 4.261, avg. samples / sec: 57904.26
Iteration:   2960, Loss function: 3.694, Average Loss: 4.236, avg. samples / sec: 57709.83
Iteration:   2980, Loss function: 4.127, Average Loss: 4.230, avg. samples / sec: 59485.66
Iteration:   2980, Loss function: 4.571, Average Loss: 4.240, avg. samples / sec: 59100.79
Iteration:   2980, Loss function: 4.429, Average Loss: 4.256, avg. samples / sec: 59349.26
Iteration:   2980, Loss function: 3.588, Average Loss: 4.234, avg. samples / sec: 59170.02
Iteration:   2980, Loss function: 4.516, Average Loss: 4.230, avg. samples / sec: 59107.56
Iteration:   2980, Loss function: 3.614, Average Loss: 4.224, avg. samples / sec: 58994.88
Iteration:   2980, Loss function: 3.711, Average Loss: 4.283, avg. samples / sec: 59109.84
Iteration:   2980, Loss function: 4.186, Average Loss: 4.231, avg. samples / sec: 58953.54
Iteration:   2980, Loss function: 4.819, Average Loss: 4.233, avg. samples / sec: 58924.21
Iteration:   2980, Loss function: 4.040, Average Loss: 4.235, avg. samples / sec: 58794.83
Iteration:   2980, Loss function: 3.639, Average Loss: 4.245, avg. samples / sec: 59202.81
Iteration:   2980, Loss function: 4.427, Average Loss: 4.255, avg. samples / sec: 59017.73
Iteration:   2980, Loss function: 4.476, Average Loss: 4.230, avg. samples / sec: 59063.49
Iteration:   2980, Loss function: 4.525, Average Loss: 4.239, avg. samples / sec: 58973.94
Iteration:   2980, Loss function: 3.527, Average Loss: 4.238, avg. samples / sec: 58798.92
Iteration:   3000, Loss function: 3.818, Average Loss: 4.225, avg. samples / sec: 59738.02
Iteration:   3000, Loss function: 3.128, Average Loss: 4.248, avg. samples / sec: 59691.31
Iteration:   3000, Loss function: 3.046, Average Loss: 4.221, avg. samples / sec: 59726.85
Iteration:   3000, Loss function: 4.908, Average Loss: 4.235, avg. samples / sec: 59794.92
Iteration:   3000, Loss function: 4.408, Average Loss: 4.238, avg. samples / sec: 59836.89
Iteration:   3000, Loss function: 4.984, Average Loss: 4.227, avg. samples / sec: 59625.62
Iteration:   3000, Loss function: 4.198, Average Loss: 4.219, avg. samples / sec: 59607.89
Iteration:   3000, Loss function: 3.553, Average Loss: 4.223, avg. samples / sec: 59579.79
Iteration:   3000, Loss function: 4.454, Average Loss: 4.225, avg. samples / sec: 59678.19
Iteration:   3000, Loss function: 3.339, Average Loss: 4.240, avg. samples / sec: 59646.42
Iteration:   3000, Loss function: 4.363, Average Loss: 4.223, avg. samples / sec: 59431.10
Iteration:   3000, Loss function: 4.126, Average Loss: 4.228, avg. samples / sec: 59444.99
Iteration:   3000, Loss function: 3.676, Average Loss: 4.255, avg. samples / sec: 59634.58
Iteration:   3000, Loss function: 2.793, Average Loss: 4.228, avg. samples / sec: 59610.56
Iteration:   3000, Loss function: 4.857, Average Loss: 4.280, avg. samples / sec: 59540.22
:::MLL 1558639033.284 epoch_stop: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 819}}
:::MLL 1558639033.285 epoch_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 673}}
Iteration:   3020, Loss function: 4.496, Average Loss: 4.222, avg. samples / sec: 57953.94
Iteration:   3020, Loss function: 3.566, Average Loss: 4.222, avg. samples / sec: 57962.66
Iteration:   3020, Loss function: 3.576, Average Loss: 4.214, avg. samples / sec: 57792.66
Iteration:   3020, Loss function: 2.881, Average Loss: 4.222, avg. samples / sec: 57702.50
Iteration:   3020, Loss function: 3.663, Average Loss: 4.241, avg. samples / sec: 57719.14
Iteration:   3020, Loss function: 4.290, Average Loss: 4.209, avg. samples / sec: 57796.05
Iteration:   3020, Loss function: 4.491, Average Loss: 4.227, avg. samples / sec: 57855.72
Iteration:   3020, Loss function: 4.465, Average Loss: 4.237, avg. samples / sec: 57725.00
Iteration:   3020, Loss function: 3.941, Average Loss: 4.233, avg. samples / sec: 57782.83
Iteration:   3020, Loss function: 2.675, Average Loss: 4.275, avg. samples / sec: 57806.95
Iteration:   3020, Loss function: 4.598, Average Loss: 4.220, avg. samples / sec: 57789.63
Iteration:   3020, Loss function: 4.001, Average Loss: 4.216, avg. samples / sec: 57762.17
Iteration:   3020, Loss function: 4.557, Average Loss: 4.237, avg. samples / sec: 57624.74
Iteration:   3020, Loss function: 4.742, Average Loss: 4.217, avg. samples / sec: 57690.79
Iteration:   3020, Loss function: 3.912, Average Loss: 4.250, avg. samples / sec: 57720.25
Iteration:   3040, Loss function: 3.754, Average Loss: 4.239, avg. samples / sec: 59420.10
Iteration:   3040, Loss function: 3.582, Average Loss: 4.208, avg. samples / sec: 59502.87
Iteration:   3040, Loss function: 4.172, Average Loss: 4.221, avg. samples / sec: 59175.54
Iteration:   3040, Loss function: 4.521, Average Loss: 4.232, avg. samples / sec: 59430.78
Iteration:   3040, Loss function: 3.981, Average Loss: 4.247, avg. samples / sec: 59445.67
Iteration:   3040, Loss function: 3.611, Average Loss: 4.226, avg. samples / sec: 59351.26
Iteration:   3040, Loss function: 4.129, Average Loss: 4.216, avg. samples / sec: 59359.23
Iteration:   3040, Loss function: 2.703, Average Loss: 4.217, avg. samples / sec: 59219.82
Iteration:   3040, Loss function: 3.436, Average Loss: 4.267, avg. samples / sec: 59295.50
Iteration:   3040, Loss function: 4.283, Average Loss: 4.224, avg. samples / sec: 59196.57
Iteration:   3040, Loss function: 3.501, Average Loss: 4.212, avg. samples / sec: 59073.14
Iteration:   3040, Loss function: 4.149, Average Loss: 4.200, avg. samples / sec: 59122.61
Iteration:   3040, Loss function: 3.480, Average Loss: 4.217, avg. samples / sec: 59101.93
Iteration:   3040, Loss function: 3.511, Average Loss: 4.232, avg. samples / sec: 59035.60
Iteration:   3040, Loss function: 4.127, Average Loss: 4.221, avg. samples / sec: 58852.82
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
:::MLL 1558639034.388 eval_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.45s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.46s)
DONE (t=0.47s)
DONE (t=0.48s)
DONE (t=0.49s)
DONE (t=2.84s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.16970
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.31354
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.16776
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.04023
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.18186
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.26800
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.18011
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.26435
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.27892
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.07174
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.30432
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.42726
Current AP: 0.16970 AP goal: 0.23000
:::MLL 1558639038.320 eval_accuracy: {"value": 0.16970227493050064, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 389}}
:::MLL 1558639038.376 eval_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 392}}
:::MLL 1558639038.386 block_stop: {"value": null, "metadata": {"first_epoch_num": 33, "file": "train.py", "lineno": 804}}
:::MLL 1558639038.386 block_start: {"value": null, "metadata": {"first_epoch_num": 44, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3060, Loss function: 3.969, Average Loss: 4.210, avg. samples / sec: 7342.46
Iteration:   3060, Loss function: 5.393, Average Loss: 4.212, avg. samples / sec: 7346.59
Iteration:   3060, Loss function: 3.326, Average Loss: 4.220, avg. samples / sec: 7346.25
Iteration:   3060, Loss function: 3.753, Average Loss: 4.264, avg. samples / sec: 7342.47
Iteration:   3060, Loss function: 3.632, Average Loss: 4.201, avg. samples / sec: 7340.09
Iteration:   3060, Loss function: 3.513, Average Loss: 4.218, avg. samples / sec: 7342.73
Iteration:   3060, Loss function: 4.266, Average Loss: 4.216, avg. samples / sec: 7341.06
Iteration:   3060, Loss function: 4.114, Average Loss: 4.219, avg. samples / sec: 7340.61
Iteration:   3060, Loss function: 4.696, Average Loss: 4.237, avg. samples / sec: 7337.64
Iteration:   3060, Loss function: 3.598, Average Loss: 4.192, avg. samples / sec: 7342.22
Iteration:   3060, Loss function: 4.279, Average Loss: 4.214, avg. samples / sec: 7340.07
Iteration:   3060, Loss function: 4.497, Average Loss: 4.201, avg. samples / sec: 7341.11
Iteration:   3060, Loss function: 4.484, Average Loss: 4.216, avg. samples / sec: 7343.09
Iteration:   3060, Loss function: 3.019, Average Loss: 4.236, avg. samples / sec: 7338.39
Iteration:   3060, Loss function: 3.730, Average Loss: 4.221, avg. samples / sec: 7336.88
:::MLL 1558639039.288 epoch_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 819}}
:::MLL 1558639039.288 epoch_start: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 673}}
Iteration:   3080, Loss function: 3.677, Average Loss: 4.201, avg. samples / sec: 59475.50
Iteration:   3080, Loss function: 3.373, Average Loss: 4.250, avg. samples / sec: 59522.34
Iteration:   3080, Loss function: 4.556, Average Loss: 4.211, avg. samples / sec: 59777.72
Iteration:   3080, Loss function: 4.412, Average Loss: 4.207, avg. samples / sec: 59535.12
Iteration:   3080, Loss function: 3.951, Average Loss: 4.185, avg. samples / sec: 59596.95
Iteration:   3080, Loss function: 2.316, Average Loss: 4.225, avg. samples / sec: 59526.07
Iteration:   3080, Loss function: 3.103, Average Loss: 4.224, avg. samples / sec: 59566.45
Iteration:   3080, Loss function: 4.050, Average Loss: 4.209, avg. samples / sec: 59496.41
Iteration:   3080, Loss function: 5.364, Average Loss: 4.198, avg. samples / sec: 59365.01
Iteration:   3080, Loss function: 2.705, Average Loss: 4.210, avg. samples / sec: 59359.43
Iteration:   3080, Loss function: 3.070, Average Loss: 4.192, avg. samples / sec: 59443.46
Iteration:   3080, Loss function: 3.988, Average Loss: 4.207, avg. samples / sec: 59253.91
Iteration:   3080, Loss function: 3.627, Average Loss: 4.207, avg. samples / sec: 59415.74
Iteration:   3080, Loss function: 3.202, Average Loss: 4.204, avg. samples / sec: 59240.74
Iteration:   3080, Loss function: 4.105, Average Loss: 4.198, avg. samples / sec: 59140.37
Iteration:   3100, Loss function: 2.477, Average Loss: 4.198, avg. samples / sec: 57146.03
Iteration:   3100, Loss function: 2.965, Average Loss: 4.177, avg. samples / sec: 57275.88
Iteration:   3100, Loss function: 3.355, Average Loss: 4.239, avg. samples / sec: 57059.52
Iteration:   3100, Loss function: 3.295, Average Loss: 4.203, avg. samples / sec: 57090.79
Iteration:   3100, Loss function: 3.884, Average Loss: 4.216, avg. samples / sec: 57192.87
Iteration:   3100, Loss function: 3.722, Average Loss: 4.198, avg. samples / sec: 57315.13
Iteration:   3100, Loss function: 4.461, Average Loss: 4.191, avg. samples / sec: 56969.33
Iteration:   3100, Loss function: 4.036, Average Loss: 4.178, avg. samples / sec: 57064.97
Iteration:   3100, Loss function: 4.851, Average Loss: 4.205, avg. samples / sec: 57197.70
Iteration:   3100, Loss function: 3.931, Average Loss: 4.181, avg. samples / sec: 57160.61
Iteration:   3100, Loss function: 3.771, Average Loss: 4.196, avg. samples / sec: 57080.99
Iteration:   3100, Loss function: 2.781, Average Loss: 4.189, avg. samples / sec: 57216.00
Iteration:   3100, Loss function: 3.925, Average Loss: 4.201, avg. samples / sec: 57128.56
Iteration:   3100, Loss function: 4.127, Average Loss: 4.208, avg. samples / sec: 56990.08
Iteration:   3100, Loss function: 2.487, Average Loss: 4.188, avg. samples / sec: 57190.62
Iteration:   3120, Loss function: 4.264, Average Loss: 4.174, avg. samples / sec: 59668.21
Iteration:   3120, Loss function: 3.097, Average Loss: 4.184, avg. samples / sec: 59623.40
Iteration:   3120, Loss function: 4.715, Average Loss: 4.168, avg. samples / sec: 59453.19
Iteration:   3120, Loss function: 4.800, Average Loss: 4.180, avg. samples / sec: 59545.03
Iteration:   3120, Loss function: 3.651, Average Loss: 4.200, avg. samples / sec: 59437.52
Iteration:   3120, Loss function: 3.004, Average Loss: 4.226, avg. samples / sec: 59370.89
Iteration:   3120, Loss function: 4.698, Average Loss: 4.190, avg. samples / sec: 59519.91
Iteration:   3120, Loss function: 4.602, Average Loss: 4.203, avg. samples / sec: 59355.58
Iteration:   3120, Loss function: 3.013, Average Loss: 4.184, avg. samples / sec: 59342.79
Iteration:   3120, Loss function: 4.050, Average Loss: 4.162, avg. samples / sec: 59370.94
Iteration:   3120, Loss function: 3.106, Average Loss: 4.191, avg. samples / sec: 59275.25
Iteration:   3120, Loss function: 2.975, Average Loss: 4.161, avg. samples / sec: 59314.06
Iteration:   3120, Loss function: 3.324, Average Loss: 4.198, avg. samples / sec: 59318.98
Iteration:   3120, Loss function: 4.086, Average Loss: 4.186, avg. samples / sec: 59072.45
Iteration:   3120, Loss function: 4.284, Average Loss: 4.180, avg. samples / sec: 59111.95
Iteration:   3140, Loss function: 3.169, Average Loss: 4.149, avg. samples / sec: 59531.52
Iteration:   3140, Loss function: 2.510, Average Loss: 4.179, avg. samples / sec: 59602.32
Iteration:   3140, Loss function: 3.339, Average Loss: 4.166, avg. samples / sec: 59437.92
Iteration:   3140, Loss function: 4.474, Average Loss: 4.164, avg. samples / sec: 59395.28
Iteration:   3140, Loss function: 2.176, Average Loss: 4.182, avg. samples / sec: 59546.56
Iteration:   3140, Loss function: 2.878, Average Loss: 4.169, avg. samples / sec: 59769.66
Iteration:   3140, Loss function: 2.708, Average Loss: 4.152, avg. samples / sec: 59510.30
Iteration:   3140, Loss function: 4.422, Average Loss: 4.213, avg. samples / sec: 59421.68
Iteration:   3140, Loss function: 2.669, Average Loss: 4.168, avg. samples / sec: 59442.56
Iteration:   3140, Loss function: 3.347, Average Loss: 4.190, avg. samples / sec: 59405.47
Iteration:   3140, Loss function: 4.089, Average Loss: 4.192, avg. samples / sec: 59395.78
Iteration:   3140, Loss function: 3.616, Average Loss: 4.163, avg. samples / sec: 59302.91
Iteration:   3140, Loss function: 3.326, Average Loss: 4.169, avg. samples / sec: 59505.43
Iteration:   3140, Loss function: 3.551, Average Loss: 4.186, avg. samples / sec: 59457.60
Iteration:   3140, Loss function: 4.012, Average Loss: 4.149, avg. samples / sec: 59274.57
:::MLL 1558639041.289 epoch_stop: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 819}}
:::MLL 1558639041.289 epoch_start: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 673}}
Iteration:   3160, Loss function: 4.330, Average Loss: 4.152, avg. samples / sec: 60115.79
Iteration:   3160, Loss function: 2.542, Average Loss: 4.145, avg. samples / sec: 59871.13
Iteration:   3160, Loss function: 3.192, Average Loss: 4.158, avg. samples / sec: 60018.00
Iteration:   3160, Loss function: 3.520, Average Loss: 4.168, avg. samples / sec: 59706.71
Iteration:   3160, Loss function: 2.895, Average Loss: 4.154, avg. samples / sec: 59745.89
Iteration:   3160, Loss function: 3.224, Average Loss: 4.196, avg. samples / sec: 59869.98
Iteration:   3160, Loss function: 3.241, Average Loss: 4.155, avg. samples / sec: 59801.80
Iteration:   3160, Loss function: 3.247, Average Loss: 4.137, avg. samples / sec: 60052.98
Iteration:   3160, Loss function: 3.095, Average Loss: 4.155, avg. samples / sec: 59749.64
Iteration:   3160, Loss function: 3.689, Average Loss: 4.167, avg. samples / sec: 59668.39
Iteration:   3160, Loss function: 3.969, Average Loss: 4.182, avg. samples / sec: 59658.03
Iteration:   3160, Loss function: 2.944, Average Loss: 4.137, avg. samples / sec: 59381.94
Iteration:   3160, Loss function: 3.307, Average Loss: 4.180, avg. samples / sec: 59563.25
Iteration:   3160, Loss function: 3.726, Average Loss: 4.178, avg. samples / sec: 59661.69
Iteration:   3160, Loss function: 4.729, Average Loss: 4.144, avg. samples / sec: 59425.84
Iteration:   3180, Loss function: 3.429, Average Loss: 4.144, avg. samples / sec: 58732.05
Iteration:   3180, Loss function: 3.553, Average Loss: 4.164, avg. samples / sec: 58918.15
Iteration:   3180, Loss function: 2.456, Average Loss: 4.126, avg. samples / sec: 58673.61
Iteration:   3180, Loss function: 4.581, Average Loss: 4.124, avg. samples / sec: 58837.49
Iteration:   3180, Loss function: 3.032, Average Loss: 4.137, avg. samples / sec: 58390.59
Iteration:   3180, Loss function: 3.291, Average Loss: 4.167, avg. samples / sec: 58907.93
Iteration:   3180, Loss function: 3.688, Average Loss: 4.170, avg. samples / sec: 58776.58
Iteration:   3180, Loss function: 3.071, Average Loss: 4.178, avg. samples / sec: 58514.55
Iteration:   3180, Loss function: 3.670, Average Loss: 4.130, avg. samples / sec: 58894.19
Iteration:   3180, Loss function: 4.077, Average Loss: 4.154, avg. samples / sec: 58490.24
Iteration:   3180, Loss function: 1.836, Average Loss: 4.142, avg. samples / sec: 58482.09
Iteration:   3180, Loss function: 3.635, Average Loss: 4.146, avg. samples / sec: 58462.15
Iteration:   3180, Loss function: 3.115, Average Loss: 4.129, avg. samples / sec: 58313.78
Iteration:   3180, Loss function: 4.002, Average Loss: 4.146, avg. samples / sec: 58419.57
Iteration:   3180, Loss function: 3.444, Average Loss: 4.151, avg. samples / sec: 58440.86
Iteration:   3200, Loss function: 3.249, Average Loss: 4.153, avg. samples / sec: 60048.53
Iteration:   3200, Loss function: 3.282, Average Loss: 4.119, avg. samples / sec: 60094.31
Iteration:   3200, Loss function: 4.216, Average Loss: 4.148, avg. samples / sec: 59895.33
Iteration:   3200, Loss function: 4.287, Average Loss: 4.167, avg. samples / sec: 59942.28
Iteration:   3200, Loss function: 3.508, Average Loss: 4.134, avg. samples / sec: 59974.57
Iteration:   3200, Loss function: 3.536, Average Loss: 4.159, avg. samples / sec: 59896.50
Iteration:   3200, Loss function: 3.732, Average Loss: 4.125, avg. samples / sec: 59930.81
Iteration:   3200, Loss function: 5.408, Average Loss: 4.138, avg. samples / sec: 60111.00
Iteration:   3200, Loss function: 4.397, Average Loss: 4.113, avg. samples / sec: 59791.32
Iteration:   3200, Loss function: 3.651, Average Loss: 4.110, avg. samples / sec: 59755.01
Iteration:   3200, Loss function: 2.709, Average Loss: 4.124, avg. samples / sec: 59767.58
Iteration:   3200, Loss function: 3.934, Average Loss: 4.116, avg. samples / sec: 59686.40
Iteration:   3200, Loss function: 3.534, Average Loss: 4.137, avg. samples / sec: 59910.22
Iteration:   3200, Loss function: 3.161, Average Loss: 4.141, avg. samples / sec: 59640.36
Iteration:   3200, Loss function: 2.334, Average Loss: 4.129, avg. samples / sec: 59388.60
:::MLL 1558639043.282 epoch_stop: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 819}}
:::MLL 1558639043.283 epoch_start: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 673}}
Iteration:   3220, Loss function: 3.813, Average Loss: 4.135, avg. samples / sec: 58510.52
Iteration:   3220, Loss function: 4.415, Average Loss: 4.136, avg. samples / sec: 58098.48
Iteration:   3220, Loss function: 2.700, Average Loss: 4.142, avg. samples / sec: 57988.66
Iteration:   3220, Loss function: 3.576, Average Loss: 4.150, avg. samples / sec: 58143.23
Iteration:   3220, Loss function: 3.536, Average Loss: 4.110, avg. samples / sec: 58105.04
Iteration:   3220, Loss function: 2.858, Average Loss: 4.154, avg. samples / sec: 58068.41
Iteration:   3220, Loss function: 3.147, Average Loss: 4.116, avg. samples / sec: 58375.48
Iteration:   3220, Loss function: 3.831, Average Loss: 4.125, avg. samples / sec: 58083.06
Iteration:   3220, Loss function: 3.040, Average Loss: 4.100, avg. samples / sec: 58098.98
Iteration:   3220, Loss function: 3.443, Average Loss: 4.106, avg. samples / sec: 57931.99
Iteration:   3220, Loss function: 3.517, Average Loss: 4.107, avg. samples / sec: 58216.06
Iteration:   3220, Loss function: 4.251, Average Loss: 4.104, avg. samples / sec: 58096.80
Iteration:   3220, Loss function: 3.102, Average Loss: 4.110, avg. samples / sec: 58099.94
Iteration:   3220, Loss function: 4.676, Average Loss: 4.121, avg. samples / sec: 57936.23
Iteration:   3220, Loss function: 3.628, Average Loss: 4.120, avg. samples / sec: 58152.83
Iteration:   3240, Loss function: 3.461, Average Loss: 4.086, avg. samples / sec: 56474.93
Iteration:   3240, Loss function: 3.153, Average Loss: 4.137, avg. samples / sec: 56400.03
Iteration:   3240, Loss function: 2.613, Average Loss: 4.093, avg. samples / sec: 56478.55
Iteration:   3240, Loss function: 3.841, Average Loss: 4.096, avg. samples / sec: 56440.12
Iteration:   3240, Loss function: 2.919, Average Loss: 4.098, avg. samples / sec: 56335.66
Iteration:   3240, Loss function: 3.182, Average Loss: 4.122, avg. samples / sec: 56266.68
Iteration:   3240, Loss function: 3.597, Average Loss: 4.100, avg. samples / sec: 56432.80
Iteration:   3240, Loss function: 2.720, Average Loss: 4.136, avg. samples / sec: 56274.65
Iteration:   3240, Loss function: 3.404, Average Loss: 4.111, avg. samples / sec: 56448.58
Iteration:   3240, Loss function: 4.109, Average Loss: 4.102, avg. samples / sec: 56307.82
Iteration:   3240, Loss function: 4.101, Average Loss: 4.131, avg. samples / sec: 56166.73
Iteration:   3240, Loss function: 3.538, Average Loss: 4.100, avg. samples / sec: 56275.01
Iteration:   3240, Loss function: 2.516, Average Loss: 4.109, avg. samples / sec: 56343.70
Iteration:   3240, Loss function: 3.570, Average Loss: 4.125, avg. samples / sec: 56047.31
Iteration:   3240, Loss function: 4.645, Average Loss: 4.113, avg. samples / sec: 56107.23
Iteration:   3260, Loss function: 4.843, Average Loss: 4.100, avg. samples / sec: 58845.35
Iteration:   3260, Loss function: 3.325, Average Loss: 4.085, avg. samples / sec: 58815.22
Iteration:   3260, Loss function: 4.026, Average Loss: 4.088, avg. samples / sec: 58681.62
Iteration:   3260, Loss function: 3.692, Average Loss: 4.121, avg. samples / sec: 58684.12
Iteration:   3260, Loss function: 3.141, Average Loss: 4.122, avg. samples / sec: 58799.76
Iteration:   3260, Loss function: 2.825, Average Loss: 4.113, avg. samples / sec: 58808.74
Iteration:   3260, Loss function: 3.390, Average Loss: 4.089, avg. samples / sec: 58781.17
Iteration:   3260, Loss function: 3.716, Average Loss: 4.084, avg. samples / sec: 58538.45
Iteration:   3260, Loss function: 3.570, Average Loss: 4.074, avg. samples / sec: 58415.16
Iteration:   3260, Loss function: 3.230, Average Loss: 4.131, avg. samples / sec: 58429.18
Iteration:   3260, Loss function: 3.978, Average Loss: 4.085, avg. samples / sec: 58434.80
Iteration:   3260, Loss function: 3.540, Average Loss: 4.097, avg. samples / sec: 58758.01
Iteration:   3260, Loss function: 4.401, Average Loss: 4.085, avg. samples / sec: 58417.51
Iteration:   3260, Loss function: 2.905, Average Loss: 4.095, avg. samples / sec: 58572.39
Iteration:   3260, Loss function: 3.157, Average Loss: 4.109, avg. samples / sec: 58382.76
Iteration:   3280, Loss function: 4.568, Average Loss: 4.084, avg. samples / sec: 58987.42
Iteration:   3280, Loss function: 2.341, Average Loss: 4.111, avg. samples / sec: 58700.69
Iteration:   3280, Loss function: 3.858, Average Loss: 4.104, avg. samples / sec: 58674.44
Iteration:   3280, Loss function: 4.454, Average Loss: 4.091, avg. samples / sec: 58429.52
Iteration:   3280, Loss function: 5.087, Average Loss: 4.086, avg. samples / sec: 58759.45
Iteration:   3280, Loss function: 2.500, Average Loss: 4.101, avg. samples / sec: 58822.14
Iteration:   3280, Loss function: 3.977, Average Loss: 4.065, avg. samples / sec: 58679.72
Iteration:   3280, Loss function: 4.253, Average Loss: 4.083, avg. samples / sec: 58572.65
Iteration:   3280, Loss function: 2.545, Average Loss: 4.102, avg. samples / sec: 58536.45
Iteration:   3280, Loss function: 4.070, Average Loss: 4.079, avg. samples / sec: 58478.50
Iteration:   3280, Loss function: 4.163, Average Loss: 4.074, avg. samples / sec: 58609.92
Iteration:   3280, Loss function: 5.181, Average Loss: 4.077, avg. samples / sec: 58676.59
Iteration:   3280, Loss function: 3.558, Average Loss: 4.074, avg. samples / sec: 58291.81
Iteration:   3280, Loss function: 2.959, Average Loss: 4.127, avg. samples / sec: 58538.40
Iteration:   3280, Loss function: 2.736, Average Loss: 4.069, avg. samples / sec: 57979.88
:::MLL 1558639045.312 epoch_stop: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 819}}
:::MLL 1558639045.312 epoch_start: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 673}}
Iteration:   3300, Loss function: 2.582, Average Loss: 4.057, avg. samples / sec: 58141.03
Iteration:   3300, Loss function: 3.253, Average Loss: 4.060, avg. samples / sec: 58141.99
Iteration:   3300, Loss function: 3.369, Average Loss: 4.078, avg. samples / sec: 58095.75
Iteration:   3300, Loss function: 2.692, Average Loss: 4.070, avg. samples / sec: 58055.90
Iteration:   3300, Loss function: 2.188, Average Loss: 4.073, avg. samples / sec: 57982.29
Iteration:   3300, Loss function: 3.324, Average Loss: 4.071, avg. samples / sec: 57820.45
Iteration:   3300, Loss function: 3.521, Average Loss: 4.052, avg. samples / sec: 58746.72
Iteration:   3300, Loss function: 3.456, Average Loss: 4.064, avg. samples / sec: 58107.78
Iteration:   3300, Loss function: 2.431, Average Loss: 4.082, avg. samples / sec: 57972.91
Iteration:   3300, Loss function: 3.017, Average Loss: 4.093, avg. samples / sec: 57799.60
Iteration:   3300, Loss function: 2.716, Average Loss: 4.061, avg. samples / sec: 57964.45
Iteration:   3300, Loss function: 3.129, Average Loss: 4.114, avg. samples / sec: 58061.67
Iteration:   3300, Loss function: 4.718, Average Loss: 4.104, avg. samples / sec: 57683.28
Iteration:   3300, Loss function: 4.136, Average Loss: 4.085, avg. samples / sec: 57802.26
Iteration:   3300, Loss function: 3.295, Average Loss: 4.090, avg. samples / sec: 57801.93
Iteration:   3320, Loss function: 3.605, Average Loss: 4.048, avg. samples / sec: 60044.03
Iteration:   3320, Loss function: 3.432, Average Loss: 4.050, avg. samples / sec: 59898.51
Iteration:   3320, Loss function: 3.905, Average Loss: 4.101, avg. samples / sec: 59950.85
Iteration:   3320, Loss function: 3.936, Average Loss: 4.076, avg. samples / sec: 60043.90
Iteration:   3320, Loss function: 4.442, Average Loss: 4.049, avg. samples / sec: 59664.19
Iteration:   3320, Loss function: 3.937, Average Loss: 4.073, avg. samples / sec: 59867.14
Iteration:   3320, Loss function: 1.952, Average Loss: 4.081, avg. samples / sec: 59849.72
Iteration:   3320, Loss function: 2.803, Average Loss: 4.070, avg. samples / sec: 59886.75
Iteration:   3320, Loss function: 3.583, Average Loss: 4.094, avg. samples / sec: 59881.94
Iteration:   3320, Loss function: 3.007, Average Loss: 4.057, avg. samples / sec: 59672.33
Iteration:   3320, Loss function: 2.608, Average Loss: 4.071, avg. samples / sec: 59598.39
Iteration:   3320, Loss function: 4.202, Average Loss: 4.047, avg. samples / sec: 59483.13
Iteration:   3320, Loss function: 4.338, Average Loss: 4.060, avg. samples / sec: 59476.42
Iteration:   3320, Loss function: 4.677, Average Loss: 4.039, avg. samples / sec: 59469.25
Iteration:   3320, Loss function: 2.040, Average Loss: 4.061, avg. samples / sec: 59374.64
Iteration:   3340, Loss function: 3.581, Average Loss: 4.048, avg. samples / sec: 59644.72
Iteration:   3340, Loss function: 3.495, Average Loss: 4.057, avg. samples / sec: 59419.50
Iteration:   3340, Loss function: 3.958, Average Loss: 4.057, avg. samples / sec: 59389.28
Iteration:   3340, Loss function: 3.946, Average Loss: 4.092, avg. samples / sec: 59261.41
Iteration:   3340, Loss function: 4.454, Average Loss: 4.038, avg. samples / sec: 59122.91
Iteration:   3340, Loss function: 3.431, Average Loss: 4.055, avg. samples / sec: 59635.79
Iteration:   3340, Loss function: 3.546, Average Loss: 4.065, avg. samples / sec: 59156.81
Iteration:   3340, Loss function: 3.578, Average Loss: 4.029, avg. samples / sec: 59486.42
Iteration:   3340, Loss function: 3.450, Average Loss: 4.048, avg. samples / sec: 59259.69
Iteration:   3340, Loss function: 3.181, Average Loss: 4.082, avg. samples / sec: 59152.56
Iteration:   3340, Loss function: 4.001, Average Loss: 4.032, avg. samples / sec: 59274.47
Iteration:   3340, Loss function: 3.573, Average Loss: 4.040, avg. samples / sec: 59018.03
Iteration:   3340, Loss function: 2.201, Average Loss: 4.070, avg. samples / sec: 59081.91
Iteration:   3340, Loss function: 4.171, Average Loss: 4.040, avg. samples / sec: 59018.77
Iteration:   3340, Loss function: 3.376, Average Loss: 4.057, avg. samples / sec: 58984.28
:::MLL 1558639047.304 epoch_stop: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 819}}
:::MLL 1558639047.305 epoch_start: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 673}}
Iteration:   3360, Loss function: 5.961, Average Loss: 4.022, avg. samples / sec: 59291.75
Iteration:   3360, Loss function: 3.495, Average Loss: 4.055, avg. samples / sec: 59084.66
Iteration:   3360, Loss function: 3.631, Average Loss: 4.023, avg. samples / sec: 58981.94
Iteration:   3360, Loss function: 3.534, Average Loss: 4.048, avg. samples / sec: 58949.45
Iteration:   3360, Loss function: 4.238, Average Loss: 4.059, avg. samples / sec: 59189.21
Iteration:   3360, Loss function: 3.742, Average Loss: 4.070, avg. samples / sec: 59167.54
Iteration:   3360, Loss function: 2.391, Average Loss: 4.021, avg. samples / sec: 59166.92
Iteration:   3360, Loss function: 3.972, Average Loss: 4.042, avg. samples / sec: 59028.24
Iteration:   3360, Loss function: 3.159, Average Loss: 4.074, avg. samples / sec: 58831.05
Iteration:   3360, Loss function: 4.461, Average Loss: 4.035, avg. samples / sec: 58755.09
Iteration:   3360, Loss function: 2.034, Average Loss: 4.049, avg. samples / sec: 58737.93
Iteration:   3360, Loss function: 3.379, Average Loss: 4.048, avg. samples / sec: 59023.71
Iteration:   3360, Loss function: 3.844, Average Loss: 4.029, avg. samples / sec: 58964.99
Iteration:   3360, Loss function: 4.227, Average Loss: 4.042, avg. samples / sec: 58731.51
Iteration:   3360, Loss function: 3.161, Average Loss: 4.017, avg. samples / sec: 58771.71
Iteration:   3380, Loss function: 4.196, Average Loss: 4.035, avg. samples / sec: 58377.51
Iteration:   3380, Loss function: 3.930, Average Loss: 4.041, avg. samples / sec: 58535.75
Iteration:   3380, Loss function: 3.158, Average Loss: 4.021, avg. samples / sec: 58444.37
Iteration:   3380, Loss function: 2.983, Average Loss: 4.009, avg. samples / sec: 58605.83
Iteration:   3380, Loss function: 2.766, Average Loss: 4.011, avg. samples / sec: 58270.48
Iteration:   3380, Loss function: 3.200, Average Loss: 4.011, avg. samples / sec: 58177.03
Iteration:   3380, Loss function: 3.445, Average Loss: 4.032, avg. samples / sec: 58340.89
Iteration:   3380, Loss function: 3.537, Average Loss: 4.061, avg. samples / sec: 58367.42
Iteration:   3380, Loss function: 4.011, Average Loss: 4.043, avg. samples / sec: 58266.87
Iteration:   3380, Loss function: 3.161, Average Loss: 4.037, avg. samples / sec: 58430.39
Iteration:   3380, Loss function: 2.786, Average Loss: 4.026, avg. samples / sec: 58520.51
Iteration:   3380, Loss function: 3.244, Average Loss: 4.011, avg. samples / sec: 58237.23
Iteration:   3380, Loss function: 3.627, Average Loss: 4.022, avg. samples / sec: 58433.49
Iteration:   3380, Loss function: 3.014, Average Loss: 4.039, avg. samples / sec: 58095.44
Iteration:   3380, Loss function: 3.240, Average Loss: 4.056, avg. samples / sec: 58098.72
Iteration:   3400, Loss function: 4.520, Average Loss: 4.052, avg. samples / sec: 57833.55
Iteration:   3400, Loss function: 2.917, Average Loss: 4.005, avg. samples / sec: 57826.59
Iteration:   3400, Loss function: 2.487, Average Loss: 4.004, avg. samples / sec: 57806.46
Iteration:   3400, Loss function: 3.906, Average Loss: 4.000, avg. samples / sec: 57789.48
Iteration:   3400, Loss function: 3.889, Average Loss: 4.026, avg. samples / sec: 57607.33
Iteration:   3400, Loss function: 3.878, Average Loss: 4.012, avg. samples / sec: 57763.95
Iteration:   3400, Loss function: 4.536, Average Loss: 4.031, avg. samples / sec: 57708.03
Iteration:   3400, Loss function: 3.403, Average Loss: 4.027, avg. samples / sec: 57841.67
Iteration:   3400, Loss function: 4.048, Average Loss: 4.044, avg. samples / sec: 57860.83
Iteration:   3400, Loss function: 3.475, Average Loss: 4.026, avg. samples / sec: 57612.56
Iteration:   3400, Loss function: 3.474, Average Loss: 4.024, avg. samples / sec: 57615.08
Iteration:   3400, Loss function: 3.514, Average Loss: 4.015, avg. samples / sec: 57684.48
Iteration:   3400, Loss function: 2.490, Average Loss: 4.007, avg. samples / sec: 57579.53
Iteration:   3400, Loss function: 2.966, Average Loss: 4.028, avg. samples / sec: 57629.17
Iteration:   3400, Loss function: 4.047, Average Loss: 3.998, avg. samples / sec: 57461.09
Iteration:   3420, Loss function: 3.727, Average Loss: 4.013, avg. samples / sec: 59310.74
Iteration:   3420, Loss function: 3.810, Average Loss: 4.018, avg. samples / sec: 59256.48
Iteration:   3420, Loss function: 2.863, Average Loss: 3.989, avg. samples / sec: 59147.85
Iteration:   3420, Loss function: 2.621, Average Loss: 3.994, avg. samples / sec: 59368.11
Iteration:   3420, Loss function: 3.334, Average Loss: 3.994, avg. samples / sec: 59485.54
Iteration:   3420, Loss function: 2.424, Average Loss: 3.994, avg. samples / sec: 59102.40
Iteration:   3420, Loss function: 3.503, Average Loss: 4.018, avg. samples / sec: 59354.13
Iteration:   3420, Loss function: 3.649, Average Loss: 4.014, avg. samples / sec: 59302.51
Iteration:   3420, Loss function: 2.942, Average Loss: 3.993, avg. samples / sec: 59196.52
Iteration:   3420, Loss function: 2.397, Average Loss: 4.003, avg. samples / sec: 59304.63
Iteration:   3420, Loss function: 3.677, Average Loss: 4.017, avg. samples / sec: 59131.39
Iteration:   3420, Loss function: 4.185, Average Loss: 4.031, avg. samples / sec: 59140.00
Iteration:   3420, Loss function: 2.777, Average Loss: 4.013, avg. samples / sec: 59166.79
Iteration:   3420, Loss function: 3.808, Average Loss: 4.004, avg. samples / sec: 59033.77
Iteration:   3420, Loss function: 2.395, Average Loss: 4.037, avg. samples / sec: 58903.37
:::MLL 1558639049.299 eval_start: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.56s)
DONE (t=2.83s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22265
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.38228
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.22754
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05739
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.23562
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.35962
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.21692
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.31491
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33122
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09433
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.35894
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.51430
Current AP: 0.22265 AP goal: 0.23000
:::MLL 1558639053.292 eval_accuracy: {"value": 0.22265093040067008, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 389}}
:::MLL 1558639053.327 eval_stop: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 392}}
:::MLL 1558639053.336 block_stop: {"value": null, "metadata": {"first_epoch_num": 44, "file": "train.py", "lineno": 804}}
:::MLL 1558639053.337 block_start: {"value": null, "metadata": {"first_epoch_num": 49, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
:::MLL 1558639053.363 epoch_stop: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 819}}
:::MLL 1558639053.364 epoch_start: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 673}}
Iteration:   3440, Loss function: 2.374, Average Loss: 4.002, avg. samples / sec: 7281.62
Iteration:   3440, Loss function: 3.826, Average Loss: 4.012, avg. samples / sec: 7283.58
Iteration:   3440, Loss function: 4.789, Average Loss: 3.976, avg. samples / sec: 7280.21
Iteration:   3440, Loss function: 4.270, Average Loss: 3.982, avg. samples / sec: 7280.43
Iteration:   3440, Loss function: 3.364, Average Loss: 3.996, avg. samples / sec: 7283.07
Iteration:   3440, Loss function: 2.413, Average Loss: 3.989, avg. samples / sec: 7280.36
Iteration:   3440, Loss function: 3.772, Average Loss: 4.023, avg. samples / sec: 7282.73
Iteration:   3440, Loss function: 2.674, Average Loss: 4.016, avg. samples / sec: 7281.53
Iteration:   3440, Loss function: 3.413, Average Loss: 4.007, avg. samples / sec: 7279.33
Iteration:   3440, Loss function: 3.180, Average Loss: 3.982, avg. samples / sec: 7277.29
Iteration:   3440, Loss function: 2.804, Average Loss: 3.979, avg. samples / sec: 7276.26
Iteration:   3440, Loss function: 3.161, Average Loss: 4.005, avg. samples / sec: 7278.77
Iteration:   3440, Loss function: 2.669, Average Loss: 4.009, avg. samples / sec: 7274.82
Iteration:   3440, Loss function: 3.810, Average Loss: 3.982, avg. samples / sec: 7274.64
Iteration:   3440, Loss function: 2.533, Average Loss: 4.005, avg. samples / sec: 7271.98
Iteration:   3460, Loss function: 4.780, Average Loss: 3.997, avg. samples / sec: 59300.11
Iteration:   3460, Loss function: 4.270, Average Loss: 3.966, avg. samples / sec: 59058.69
Iteration:   3460, Loss function: 2.790, Average Loss: 3.989, avg. samples / sec: 58711.16
Iteration:   3460, Loss function: 2.100, Average Loss: 3.989, avg. samples / sec: 59101.24
Iteration:   3460, Loss function: 3.339, Average Loss: 4.003, avg. samples / sec: 58817.23
Iteration:   3460, Loss function: 2.975, Average Loss: 3.996, avg. samples / sec: 58816.62
Iteration:   3460, Loss function: 4.150, Average Loss: 3.995, avg. samples / sec: 59262.38
Iteration:   3460, Loss function: 3.809, Average Loss: 3.974, avg. samples / sec: 58738.56
Iteration:   3460, Loss function: 2.301, Average Loss: 4.016, avg. samples / sec: 58737.41
Iteration:   3460, Loss function: 3.915, Average Loss: 3.969, avg. samples / sec: 58671.17
Iteration:   3460, Loss function: 4.554, Average Loss: 3.974, avg. samples / sec: 58676.37
Iteration:   3460, Loss function: 4.521, Average Loss: 4.001, avg. samples / sec: 58514.43
Iteration:   3460, Loss function: 4.516, Average Loss: 3.981, avg. samples / sec: 58619.58
Iteration:   3460, Loss function: 4.794, Average Loss: 3.975, avg. samples / sec: 58855.72
Iteration:   3460, Loss function: 4.568, Average Loss: 3.975, avg. samples / sec: 58909.77
Iteration:   3480, Loss function: 3.531, Average Loss: 3.967, avg. samples / sec: 55907.41
Iteration:   3480, Loss function: 2.348, Average Loss: 4.004, avg. samples / sec: 55807.99
Iteration:   3480, Loss function: 3.425, Average Loss: 3.985, avg. samples / sec: 55553.96
Iteration:   3480, Loss function: 3.676, Average Loss: 3.962, avg. samples / sec: 55781.28
Iteration:   3480, Loss function: 3.396, Average Loss: 3.956, avg. samples / sec: 55798.15
Iteration:   3480, Loss function: 2.805, Average Loss: 3.985, avg. samples / sec: 55704.46
Iteration:   3480, Loss function: 3.653, Average Loss: 3.956, avg. samples / sec: 55601.52
Iteration:   3480, Loss function: 3.190, Average Loss: 3.978, avg. samples / sec: 55589.72
Iteration:   3480, Loss function: 2.720, Average Loss: 3.992, avg. samples / sec: 55784.48
Iteration:   3480, Loss function: 3.147, Average Loss: 3.959, avg. samples / sec: 55750.74
Iteration:   3480, Loss function: 2.833, Average Loss: 3.969, avg. samples / sec: 55797.95
Iteration:   3480, Loss function: 4.503, Average Loss: 3.982, avg. samples / sec: 55551.26
Iteration:   3480, Loss function: 3.927, Average Loss: 3.993, avg. samples / sec: 55612.93
Iteration:   3480, Loss function: 2.818, Average Loss: 3.961, avg. samples / sec: 55781.79
Iteration:   3480, Loss function: 3.672, Average Loss: 3.986, avg. samples / sec: 55629.75
:::MLL 1558639055.391 epoch_stop: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 819}}
:::MLL 1558639055.392 epoch_start: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 673}}
Iteration:   3500, Loss function: 2.926, Average Loss: 3.971, avg. samples / sec: 59064.83
Iteration:   3500, Loss function: 2.586, Average Loss: 3.977, avg. samples / sec: 58899.61
Iteration:   3500, Loss function: 4.071, Average Loss: 3.946, avg. samples / sec: 58909.95
Iteration:   3500, Loss function: 3.571, Average Loss: 3.994, avg. samples / sec: 58879.87
Iteration:   3500, Loss function: 2.823, Average Loss: 3.981, avg. samples / sec: 58965.43
Iteration:   3500, Loss function: 4.519, Average Loss: 3.973, avg. samples / sec: 58938.62
Iteration:   3500, Loss function: 3.085, Average Loss: 3.950, avg. samples / sec: 58887.15
Iteration:   3500, Loss function: 3.648, Average Loss: 3.947, avg. samples / sec: 58919.03
Iteration:   3500, Loss function: 2.594, Average Loss: 3.967, avg. samples / sec: 58893.57
Iteration:   3500, Loss function: 2.821, Average Loss: 3.951, avg. samples / sec: 58958.37
Iteration:   3500, Loss function: 3.429, Average Loss: 3.973, avg. samples / sec: 58984.38
Iteration:   3500, Loss function: 4.800, Average Loss: 3.988, avg. samples / sec: 58935.72
Iteration:   3500, Loss function: 3.561, Average Loss: 3.952, avg. samples / sec: 58784.82
Iteration:   3500, Loss function: 3.314, Average Loss: 3.958, avg. samples / sec: 58758.74
Iteration:   3500, Loss function: 3.125, Average Loss: 3.959, avg. samples / sec: 58564.21
Iteration:   3520, Loss function: 3.010, Average Loss: 3.960, avg. samples / sec: 58425.18
Iteration:   3520, Loss function: 3.071, Average Loss: 3.955, avg. samples / sec: 58674.83
Iteration:   3520, Loss function: 3.987, Average Loss: 3.961, avg. samples / sec: 58313.83
Iteration:   3520, Loss function: 3.692, Average Loss: 3.939, avg. samples / sec: 58348.16
Iteration:   3520, Loss function: 3.357, Average Loss: 3.941, avg. samples / sec: 58315.21
Iteration:   3520, Loss function: 3.088, Average Loss: 3.940, avg. samples / sec: 58370.33
Iteration:   3520, Loss function: 2.542, Average Loss: 3.945, avg. samples / sec: 58505.32
Iteration:   3520, Loss function: 2.855, Average Loss: 3.964, avg. samples / sec: 58342.83
Iteration:   3520, Loss function: 2.865, Average Loss: 3.981, avg. samples / sec: 58244.95
Iteration:   3520, Loss function: 3.817, Average Loss: 3.942, avg. samples / sec: 58389.17
Iteration:   3520, Loss function: 3.877, Average Loss: 3.936, avg. samples / sec: 58223.66
Iteration:   3520, Loss function: 2.841, Average Loss: 3.956, avg. samples / sec: 58261.93
Iteration:   3520, Loss function: 2.523, Average Loss: 3.976, avg. samples / sec: 58261.45
Iteration:   3520, Loss function: 2.405, Average Loss: 3.967, avg. samples / sec: 58169.08
Iteration:   3520, Loss function: 3.394, Average Loss: 3.967, avg. samples / sec: 58081.15
Iteration:   3540, Loss function: 3.381, Average Loss: 3.969, avg. samples / sec: 58246.08
Iteration:   3540, Loss function: 2.425, Average Loss: 3.946, avg. samples / sec: 58235.64
Iteration:   3540, Loss function: 4.035, Average Loss: 3.943, avg. samples / sec: 58034.74
Iteration:   3540, Loss function: 3.121, Average Loss: 3.956, avg. samples / sec: 58244.93
Iteration:   3540, Loss function: 3.658, Average Loss: 3.926, avg. samples / sec: 58214.11
Iteration:   3540, Loss function: 3.516, Average Loss: 3.965, avg. samples / sec: 58221.59
Iteration:   3540, Loss function: 2.559, Average Loss: 3.953, avg. samples / sec: 58028.58
Iteration:   3540, Loss function: 2.710, Average Loss: 3.932, avg. samples / sec: 58147.17
Iteration:   3540, Loss function: 2.331, Average Loss: 3.929, avg. samples / sec: 58011.07
Iteration:   3540, Loss function: 2.539, Average Loss: 3.931, avg. samples / sec: 58021.29
Iteration:   3540, Loss function: 3.223, Average Loss: 3.954, avg. samples / sec: 58021.63
Iteration:   3540, Loss function: 3.196, Average Loss: 3.943, avg. samples / sec: 57829.44
Iteration:   3540, Loss function: 3.096, Average Loss: 3.936, avg. samples / sec: 57914.71
Iteration:   3540, Loss function: 3.032, Average Loss: 3.954, avg. samples / sec: 58115.52
Iteration:   3540, Loss function: 3.092, Average Loss: 3.931, avg. samples / sec: 57780.29
:::MLL 1558639057.116 epoch_stop: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 819}}
:::MLL 1558639057.116 epoch_start: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 673}}
Iteration:   3560, Loss function: 1.929, Average Loss: 3.948, avg. samples / sec: 59154.45
Iteration:   3560, Loss function: 3.451, Average Loss: 3.922, avg. samples / sec: 59397.21
Iteration:   3560, Loss function: 2.894, Average Loss: 3.938, avg. samples / sec: 59099.60
Iteration:   3560, Loss function: 2.381, Average Loss: 3.924, avg. samples / sec: 59058.22
Iteration:   3560, Loss function: 3.226, Average Loss: 3.950, avg. samples / sec: 59009.67
Iteration:   3560, Loss function: 2.470, Average Loss: 3.930, avg. samples / sec: 58971.03
Iteration:   3560, Loss function: 3.102, Average Loss: 3.928, avg. samples / sec: 58919.38
Iteration:   3560, Loss function: 3.882, Average Loss: 3.935, avg. samples / sec: 59081.17
Iteration:   3560, Loss function: 3.015, Average Loss: 3.919, avg. samples / sec: 59023.79
Iteration:   3560, Loss function: 3.380, Average Loss: 3.945, avg. samples / sec: 59136.43
Iteration:   3560, Loss function: 3.693, Average Loss: 3.953, avg. samples / sec: 58788.48
Iteration:   3560, Loss function: 2.751, Average Loss: 3.945, avg. samples / sec: 58963.95
Iteration:   3560, Loss function: 2.046, Average Loss: 3.919, avg. samples / sec: 58903.15
Iteration:   3560, Loss function: 3.382, Average Loss: 3.922, avg. samples / sec: 58987.74
Iteration:   3560, Loss function: 3.378, Average Loss: 3.920, avg. samples / sec: 58698.00
Iteration:   3580, Loss function: 4.033, Average Loss: 3.916, avg. samples / sec: 59349.73
Iteration:   3580, Loss function: 3.640, Average Loss: 3.913, avg. samples / sec: 59409.86
Iteration:   3580, Loss function: 3.818, Average Loss: 3.944, avg. samples / sec: 59200.57
Iteration:   3580, Loss function: 3.210, Average Loss: 3.913, avg. samples / sec: 59019.78
Iteration:   3580, Loss function: 3.274, Average Loss: 3.914, avg. samples / sec: 58974.53
Iteration:   3580, Loss function: 4.139, Average Loss: 3.912, avg. samples / sec: 59050.22
Iteration:   3580, Loss function: 2.698, Average Loss: 3.929, avg. samples / sec: 59047.97
Iteration:   3580, Loss function: 3.601, Average Loss: 3.925, avg. samples / sec: 59014.00
Iteration:   3580, Loss function: 4.948, Average Loss: 3.914, avg. samples / sec: 59064.06
Iteration:   3580, Loss function: 4.093, Average Loss: 3.932, avg. samples / sec: 58791.30
Iteration:   3580, Loss function: 3.293, Average Loss: 3.919, avg. samples / sec: 58886.54
Iteration:   3580, Loss function: 4.386, Average Loss: 3.941, avg. samples / sec: 58759.70
Iteration:   3580, Loss function: 3.354, Average Loss: 3.936, avg. samples / sec: 58888.09
Iteration:   3580, Loss function: 3.502, Average Loss: 3.911, avg. samples / sec: 58599.59
Iteration:   3580, Loss function: 2.901, Average Loss: 3.937, avg. samples / sec: 58514.09
Iteration:   3600, Loss function: 2.580, Average Loss: 3.938, avg. samples / sec: 59904.37
Iteration:   3600, Loss function: 3.793, Average Loss: 3.904, avg. samples / sec: 59658.08
Iteration:   3600, Loss function: 3.266, Average Loss: 3.906, avg. samples / sec: 59889.52
Iteration:   3600, Loss function: 2.932, Average Loss: 3.921, avg. samples / sec: 59757.40
Iteration:   3600, Loss function: 3.243, Average Loss: 3.928, avg. samples / sec: 59996.10
Iteration:   3600, Loss function: 3.163, Average Loss: 3.906, avg. samples / sec: 59890.19
Iteration:   3600, Loss function: 2.800, Average Loss: 3.903, avg. samples / sec: 59576.67
Iteration:   3600, Loss function: 3.269, Average Loss: 3.908, avg. samples / sec: 59665.73
Iteration:   3600, Loss function: 3.181, Average Loss: 3.928, avg. samples / sec: 59988.26
Iteration:   3600, Loss function: 2.690, Average Loss: 3.907, avg. samples / sec: 59627.59
Iteration:   3600, Loss function: 4.243, Average Loss: 3.920, avg. samples / sec: 59727.21
Iteration:   3600, Loss function: 4.708, Average Loss: 3.898, avg. samples / sec: 59907.42
Iteration:   3600, Loss function: 3.620, Average Loss: 3.901, avg. samples / sec: 59522.97
Iteration:   3600, Loss function: 4.749, Average Loss: 3.933, avg. samples / sec: 59607.92
Iteration:   3600, Loss function: 3.759, Average Loss: 3.919, avg. samples / sec: 59297.04
Iteration:   3620, Loss function: 3.664, Average Loss: 3.896, avg. samples / sec: 59686.66
Iteration:   3620, Loss function: 4.132, Average Loss: 3.907, avg. samples / sec: 59843.04
Iteration:   3620, Loss function: 3.520, Average Loss: 3.905, avg. samples / sec: 60211.75
Iteration:   3620, Loss function: 3.800, Average Loss: 3.891, avg. samples / sec: 59835.82
Iteration:   3620, Loss function: 4.645, Average Loss: 3.903, avg. samples / sec: 59703.62
Iteration:   3620, Loss function: 3.335, Average Loss: 3.899, avg. samples / sec: 59652.85
Iteration:   3620, Loss function: 2.781, Average Loss: 3.918, avg. samples / sec: 59734.75
Iteration:   3620, Loss function: 2.457, Average Loss: 3.925, avg. samples / sec: 59347.46
Iteration:   3620, Loss function: 3.352, Average Loss: 3.915, avg. samples / sec: 59570.93
Iteration:   3620, Loss function: 2.410, Average Loss: 3.897, avg. samples / sec: 59672.20
Iteration:   3620, Loss function: 2.478, Average Loss: 3.910, avg. samples / sec: 59519.50
Iteration:   3620, Loss function: 3.123, Average Loss: 3.892, avg. samples / sec: 59561.44
Iteration:   3620, Loss function: 3.700, Average Loss: 3.923, avg. samples / sec: 59804.94
Iteration:   3620, Loss function: 3.268, Average Loss: 3.897, avg. samples / sec: 59305.13
Iteration:   3620, Loss function: 2.519, Average Loss: 3.891, avg. samples / sec: 59446.12
:::MLL 1558639059.096 epoch_stop: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 819}}
:::MLL 1558639059.096 epoch_start: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 673}}
Iteration:   3640, Loss function: 1.870, Average Loss: 3.906, avg. samples / sec: 59222.01
Iteration:   3640, Loss function: 2.997, Average Loss: 3.886, avg. samples / sec: 59216.56
Iteration:   3640, Loss function: 2.441, Average Loss: 3.889, avg. samples / sec: 59281.10
Iteration:   3640, Loss function: 2.966, Average Loss: 3.911, avg. samples / sec: 59028.71
Iteration:   3640, Loss function: 3.134, Average Loss: 3.884, avg. samples / sec: 58969.50
Iteration:   3640, Loss function: 3.457, Average Loss: 3.898, avg. samples / sec: 59042.60
Iteration:   3640, Loss function: 3.425, Average Loss: 3.885, avg. samples / sec: 58819.88
Iteration:   3640, Loss function: 3.341, Average Loss: 3.880, avg. samples / sec: 58852.58
Iteration:   3640, Loss function: 3.529, Average Loss: 3.906, avg. samples / sec: 58961.65
Iteration:   3640, Loss function: 2.796, Average Loss: 3.884, avg. samples / sec: 59016.94
Iteration:   3640, Loss function: 3.807, Average Loss: 3.882, avg. samples / sec: 59235.95
Iteration:   3640, Loss function: 3.519, Average Loss: 3.893, avg. samples / sec: 58706.24
Iteration:   3640, Loss function: 3.292, Average Loss: 3.898, avg. samples / sec: 58701.86
Iteration:   3640, Loss function: 3.702, Average Loss: 3.895, avg. samples / sec: 58717.69
Iteration:   3640, Loss function: 4.160, Average Loss: 3.919, avg. samples / sec: 58863.37
Iteration:   3660, Loss function: 2.844, Average Loss: 3.887, avg. samples / sec: 59145.19
Iteration:   3660, Loss function: 4.241, Average Loss: 3.877, avg. samples / sec: 59031.90
Iteration:   3660, Loss function: 3.997, Average Loss: 3.892, avg. samples / sec: 59118.10
Iteration:   3660, Loss function: 3.217, Average Loss: 3.896, avg. samples / sec: 58948.04
Iteration:   3660, Loss function: 3.756, Average Loss: 3.892, avg. samples / sec: 58912.33
Iteration:   3660, Loss function: 2.248, Average Loss: 3.878, avg. samples / sec: 59055.87
Iteration:   3660, Loss function: 2.941, Average Loss: 3.896, avg. samples / sec: 58585.58
Iteration:   3660, Loss function: 2.586, Average Loss: 3.906, avg. samples / sec: 59064.68
Iteration:   3660, Loss function: 2.904, Average Loss: 3.881, avg. samples / sec: 58595.23
Iteration:   3660, Loss function: 3.556, Average Loss: 3.868, avg. samples / sec: 58807.39
Iteration:   3660, Loss function: 2.706, Average Loss: 3.902, avg. samples / sec: 58724.44
Iteration:   3660, Loss function: 3.707, Average Loss: 3.878, avg. samples / sec: 58772.29
Iteration:   3660, Loss function: 3.830, Average Loss: 3.883, avg. samples / sec: 58669.58
Iteration:   3660, Loss function: 3.025, Average Loss: 3.875, avg. samples / sec: 58699.88
Iteration:   3660, Loss function: 1.984, Average Loss: 3.870, avg. samples / sec: 58636.24
Iteration:   3680, Loss function: 3.253, Average Loss: 3.881, avg. samples / sec: 58629.84
Iteration:   3680, Loss function: 3.447, Average Loss: 3.901, avg. samples / sec: 58706.07
Iteration:   3680, Loss function: 3.513, Average Loss: 3.861, avg. samples / sec: 58914.85
Iteration:   3680, Loss function: 4.246, Average Loss: 3.869, avg. samples / sec: 58745.54
Iteration:   3680, Loss function: 2.784, Average Loss: 3.855, avg. samples / sec: 58690.93
Iteration:   3680, Loss function: 3.438, Average Loss: 3.882, avg. samples / sec: 58608.61
Iteration:   3680, Loss function: 4.463, Average Loss: 3.870, avg. samples / sec: 58482.82
Iteration:   3680, Loss function: 3.071, Average Loss: 3.868, avg. samples / sec: 58430.05
Iteration:   3680, Loss function: 3.621, Average Loss: 3.880, avg. samples / sec: 58543.67
Iteration:   3680, Loss function: 3.006, Average Loss: 3.866, avg. samples / sec: 58553.77
Iteration:   3680, Loss function: 3.145, Average Loss: 3.876, avg. samples / sec: 58542.58
Iteration:   3680, Loss function: 3.101, Average Loss: 3.879, avg. samples / sec: 58364.11
Iteration:   3680, Loss function: 3.371, Average Loss: 3.891, avg. samples / sec: 58443.55
Iteration:   3680, Loss function: 4.026, Average Loss: 3.884, avg. samples / sec: 58207.74
Iteration:   3680, Loss function: 2.800, Average Loss: 3.877, avg. samples / sec: 57606.36
:::MLL 1558639061.096 epoch_stop: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 819}}
:::MLL 1558639061.096 epoch_start: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 673}}
Iteration:   3700, Loss function: 4.115, Average Loss: 3.891, avg. samples / sec: 58932.39
Iteration:   3700, Loss function: 4.301, Average Loss: 3.872, avg. samples / sec: 59140.28
Iteration:   3700, Loss function: 4.202, Average Loss: 3.871, avg. samples / sec: 59032.39
Iteration:   3700, Loss function: 3.262, Average Loss: 3.860, avg. samples / sec: 58863.64
Iteration:   3700, Loss function: 4.622, Average Loss: 3.874, avg. samples / sec: 59781.71
Iteration:   3700, Loss function: 2.490, Average Loss: 3.876, avg. samples / sec: 59102.03
Iteration:   3700, Loss function: 3.839, Average Loss: 3.874, avg. samples / sec: 59206.04
Iteration:   3700, Loss function: 3.497, Average Loss: 3.873, avg. samples / sec: 58774.50
Iteration:   3700, Loss function: 3.771, Average Loss: 3.863, avg. samples / sec: 58927.31
Iteration:   3700, Loss function: 2.713, Average Loss: 3.871, avg. samples / sec: 58790.17
Iteration:   3700, Loss function: 4.273, Average Loss: 3.867, avg. samples / sec: 58937.39
Iteration:   3700, Loss function: 3.175, Average Loss: 3.854, avg. samples / sec: 58710.18
Iteration:   3700, Loss function: 3.169, Average Loss: 3.847, avg. samples / sec: 58739.62
Iteration:   3700, Loss function: 3.025, Average Loss: 3.851, avg. samples / sec: 58905.32
Iteration:   3700, Loss function: 3.302, Average Loss: 3.864, avg. samples / sec: 58660.23
Iteration:   3720, Loss function: 3.806, Average Loss: 3.864, avg. samples / sec: 57949.77
Iteration:   3720, Loss function: 2.935, Average Loss: 3.862, avg. samples / sec: 57803.61
Iteration:   3720, Loss function: 3.855, Average Loss: 3.857, avg. samples / sec: 57989.18
Iteration:   3720, Loss function: 4.553, Average Loss: 3.839, avg. samples / sec: 57940.28
Iteration:   3720, Loss function: 3.204, Average Loss: 3.882, avg. samples / sec: 57714.01
Iteration:   3720, Loss function: 3.265, Average Loss: 3.856, avg. samples / sec: 58112.35
Iteration:   3720, Loss function: 3.682, Average Loss: 3.843, avg. samples / sec: 57950.10
Iteration:   3720, Loss function: 3.859, Average Loss: 3.861, avg. samples / sec: 57835.33
Iteration:   3720, Loss function: 2.216, Average Loss: 3.864, avg. samples / sec: 57812.24
Iteration:   3720, Loss function: 3.973, Average Loss: 3.844, avg. samples / sec: 57871.09
Iteration:   3720, Loss function: 2.830, Average Loss: 3.865, avg. samples / sec: 57677.66
Iteration:   3720, Loss function: 3.760, Average Loss: 3.871, avg. samples / sec: 57725.05
Iteration:   3720, Loss function: 4.110, Average Loss: 3.865, avg. samples / sec: 57769.56
Iteration:   3720, Loss function: 3.588, Average Loss: 3.853, avg. samples / sec: 57718.05
Iteration:   3720, Loss function: 3.599, Average Loss: 3.853, avg. samples / sec: 57551.60
Iteration:   3740, Loss function: 3.187, Average Loss: 3.873, avg. samples / sec: 58305.61
Iteration:   3740, Loss function: 5.088, Average Loss: 3.845, avg. samples / sec: 58522.91
Iteration:   3740, Loss function: 3.705, Average Loss: 3.834, avg. samples / sec: 58326.84
Iteration:   3740, Loss function: 3.499, Average Loss: 3.831, avg. samples / sec: 58261.86
Iteration:   3740, Loss function: 3.887, Average Loss: 3.854, avg. samples / sec: 58258.00
Iteration:   3740, Loss function: 3.762, Average Loss: 3.859, avg. samples / sec: 58296.90
Iteration:   3740, Loss function: 3.009, Average Loss: 3.851, avg. samples / sec: 58131.67
Iteration:   3740, Loss function: 4.065, Average Loss: 3.852, avg. samples / sec: 58111.35
Iteration:   3740, Loss function: 2.853, Average Loss: 3.846, avg. samples / sec: 58290.15
Iteration:   3740, Loss function: 3.754, Average Loss: 3.864, avg. samples / sec: 58260.70
Iteration:   3740, Loss function: 3.803, Average Loss: 3.850, avg. samples / sec: 58127.86
Iteration:   3740, Loss function: 5.491, Average Loss: 3.859, avg. samples / sec: 58232.03
Iteration:   3740, Loss function: 4.324, Average Loss: 3.853, avg. samples / sec: 58089.93
Iteration:   3740, Loss function: 3.136, Average Loss: 3.862, avg. samples / sec: 57935.85
Iteration:   3740, Loss function: 3.299, Average Loss: 3.832, avg. samples / sec: 58051.77
Iteration:   3760, Loss function: 4.117, Average Loss: 3.818, avg. samples / sec: 60039.68
Iteration:   3760, Loss function: 2.392, Average Loss: 3.849, avg. samples / sec: 60110.69
Iteration:   3760, Loss function: 4.570, Average Loss: 3.819, avg. samples / sec: 60156.75
Iteration:   3760, Loss function: 4.237, Average Loss: 3.824, avg. samples / sec: 59924.90
Iteration:   3760, Loss function: 3.253, Average Loss: 3.841, avg. samples / sec: 60036.46
Iteration:   3760, Loss function: 2.214, Average Loss: 3.855, avg. samples / sec: 60067.98
Iteration:   3760, Loss function: 3.631, Average Loss: 3.854, avg. samples / sec: 59955.95
Iteration:   3760, Loss function: 3.577, Average Loss: 3.839, avg. samples / sec: 59777.09
Iteration:   3760, Loss function: 3.057, Average Loss: 3.846, avg. samples / sec: 59910.81
Iteration:   3760, Loss function: 3.684, Average Loss: 3.861, avg. samples / sec: 59649.57
Iteration:   3760, Loss function: 3.056, Average Loss: 3.838, avg. samples / sec: 59828.78
Iteration:   3760, Loss function: 3.596, Average Loss: 3.843, avg. samples / sec: 59695.56
Iteration:   3760, Loss function: 4.114, Average Loss: 3.841, avg. samples / sec: 59667.60
Iteration:   3760, Loss function: 3.188, Average Loss: 3.842, avg. samples / sec: 59731.00
Iteration:   3760, Loss function: 3.870, Average Loss: 3.838, avg. samples / sec: 59548.48
:::MLL 1558639063.101 epoch_stop: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 819}}
:::MLL 1558639063.102 epoch_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 673}}
Iteration:   3780, Loss function: 3.140, Average Loss: 3.833, avg. samples / sec: 58695.97
Iteration:   3780, Loss function: 4.208, Average Loss: 3.810, avg. samples / sec: 58609.00
Iteration:   3780, Loss function: 3.721, Average Loss: 3.832, avg. samples / sec: 58744.66
Iteration:   3780, Loss function: 2.897, Average Loss: 3.815, avg. samples / sec: 58586.73
Iteration:   3780, Loss function: 3.264, Average Loss: 3.831, avg. samples / sec: 58770.82
Iteration:   3780, Loss function: 2.846, Average Loss: 3.848, avg. samples / sec: 58609.12
Iteration:   3780, Loss function: 3.054, Average Loss: 3.830, avg. samples / sec: 58838.62
Iteration:   3780, Loss function: 3.496, Average Loss: 3.809, avg. samples / sec: 58396.76
Iteration:   3780, Loss function: 3.405, Average Loss: 3.829, avg. samples / sec: 58880.39
Iteration:   3780, Loss function: 2.634, Average Loss: 3.832, avg. samples / sec: 58742.70
Iteration:   3780, Loss function: 3.573, Average Loss: 3.836, avg. samples / sec: 58601.12
Iteration:   3780, Loss function: 3.828, Average Loss: 3.839, avg. samples / sec: 58646.12
Iteration:   3780, Loss function: 3.578, Average Loss: 3.854, avg. samples / sec: 58590.45
Iteration:   3780, Loss function: 2.661, Average Loss: 3.836, avg. samples / sec: 58209.59
Iteration:   3780, Loss function: 3.558, Average Loss: 3.849, avg. samples / sec: 58356.57
Iteration:   3800, Loss function: 3.523, Average Loss: 3.826, avg. samples / sec: 58487.23
Iteration:   3800, Loss function: 3.085, Average Loss: 3.819, avg. samples / sec: 58616.94
Iteration:   3800, Loss function: 3.222, Average Loss: 3.823, avg. samples / sec: 58597.88
Iteration:   3800, Loss function: 3.292, Average Loss: 3.843, avg. samples / sec: 58644.21
Iteration:   3800, Loss function: 3.755, Average Loss: 3.804, avg. samples / sec: 58338.19
Iteration:   3800, Loss function: 2.615, Average Loss: 3.820, avg. samples / sec: 58370.28
Iteration:   3800, Loss function: 2.532, Average Loss: 3.825, avg. samples / sec: 58464.09
Iteration:   3800, Loss function: 4.150, Average Loss: 3.844, avg. samples / sec: 58550.14
Iteration:   3800, Loss function: 3.731, Average Loss: 3.802, avg. samples / sec: 58201.87
Iteration:   3800, Loss function: 3.319, Average Loss: 3.827, avg. samples / sec: 58183.30
Iteration:   3800, Loss function: 2.462, Average Loss: 3.804, avg. samples / sec: 58272.24
Iteration:   3800, Loss function: 4.084, Average Loss: 3.839, avg. samples / sec: 58227.58
Iteration:   3800, Loss function: 3.541, Average Loss: 3.826, avg. samples / sec: 58111.13
Iteration:   3800, Loss function: 2.724, Average Loss: 3.825, avg. samples / sec: 58329.21
Iteration:   3800, Loss function: 2.785, Average Loss: 3.832, avg. samples / sec: 58199.54
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
:::MLL 1558639064.256 eval_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.55s)
DONE (t=0.56s)
DONE (t=2.83s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22737
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.38950
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23368
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05880
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24088
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.36357
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.21976
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32069
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33659
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09675
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36806
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.52069
Current AP: 0.22737 AP goal: 0.23000
:::MLL 1558639068.301 eval_accuracy: {"value": 0.22736960862266492, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 389}}
:::MLL 1558639068.348 eval_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 392}}
:::MLL 1558639068.358 block_stop: {"value": null, "metadata": {"first_epoch_num": 49, "file": "train.py", "lineno": 804}}
:::MLL 1558639068.358 block_start: {"value": null, "metadata": {"first_epoch_num": 55, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3820, Loss function: 2.748, Average Loss: 3.818, avg. samples / sec: 7190.97
Iteration:   3820, Loss function: 2.910, Average Loss: 3.817, avg. samples / sec: 7185.63
Iteration:   3820, Loss function: 4.487, Average Loss: 3.792, avg. samples / sec: 7189.17
Iteration:   3820, Loss function: 2.850, Average Loss: 3.797, avg. samples / sec: 7186.14
Iteration:   3820, Loss function: 2.950, Average Loss: 3.818, avg. samples / sec: 7186.87
Iteration:   3820, Loss function: 2.429, Average Loss: 3.832, avg. samples / sec: 7184.57
Iteration:   3820, Loss function: 3.567, Average Loss: 3.820, avg. samples / sec: 7189.21
Iteration:   3820, Loss function: 3.464, Average Loss: 3.831, avg. samples / sec: 7185.90
Iteration:   3820, Loss function: 3.679, Average Loss: 3.815, avg. samples / sec: 7186.12
Iteration:   3820, Loss function: 3.414, Average Loss: 3.816, avg. samples / sec: 7182.19
Iteration:   3820, Loss function: 2.548, Average Loss: 3.826, avg. samples / sec: 7188.57
Iteration:   3820, Loss function: 4.037, Average Loss: 3.793, avg. samples / sec: 7184.89
Iteration:   3820, Loss function: 2.833, Average Loss: 3.809, avg. samples / sec: 7183.97
Iteration:   3820, Loss function: 2.141, Average Loss: 3.825, avg. samples / sec: 7185.32
Iteration:   3820, Loss function: 3.428, Average Loss: 3.810, avg. samples / sec: 7179.71
:::MLL 1558639069.216 epoch_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 819}}
:::MLL 1558639069.217 epoch_start: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 673}}
Iteration:   3840, Loss function: 3.588, Average Loss: 3.809, avg. samples / sec: 58542.58
Iteration:   3840, Loss function: 2.332, Average Loss: 3.814, avg. samples / sec: 58676.47
Iteration:   3840, Loss function: 2.705, Average Loss: 3.785, avg. samples / sec: 58589.80
Iteration:   3840, Loss function: 3.993, Average Loss: 3.807, avg. samples / sec: 58273.78
Iteration:   3840, Loss function: 2.560, Average Loss: 3.822, avg. samples / sec: 58455.28
Iteration:   3840, Loss function: 4.701, Average Loss: 3.808, avg. samples / sec: 58259.35
Iteration:   3840, Loss function: 3.172, Average Loss: 3.803, avg. samples / sec: 58645.34
Iteration:   3840, Loss function: 4.868, Average Loss: 3.790, avg. samples / sec: 58309.39
Iteration:   3840, Loss function: 3.287, Average Loss: 3.826, avg. samples / sec: 58343.96
Iteration:   3840, Loss function: 3.196, Average Loss: 3.801, avg. samples / sec: 58503.18
Iteration:   3840, Loss function: 4.661, Average Loss: 3.804, avg. samples / sec: 58448.52
Iteration:   3840, Loss function: 3.468, Average Loss: 3.809, avg. samples / sec: 58363.03
Iteration:   3840, Loss function: 3.949, Average Loss: 3.815, avg. samples / sec: 58241.01
Iteration:   3840, Loss function: 3.059, Average Loss: 3.780, avg. samples / sec: 58030.37
Iteration:   3840, Loss function: 3.162, Average Loss: 3.819, avg. samples / sec: 58182.07
Iteration:   3860, Loss function: 4.016, Average Loss: 3.807, avg. samples / sec: 58907.71
Iteration:   3860, Loss function: 4.236, Average Loss: 3.790, avg. samples / sec: 58809.77
Iteration:   3860, Loss function: 3.867, Average Loss: 3.794, avg. samples / sec: 58736.60
Iteration:   3860, Loss function: 3.230, Average Loss: 3.797, avg. samples / sec: 58767.86
Iteration:   3860, Loss function: 3.509, Average Loss: 3.797, avg. samples / sec: 58781.76
Iteration:   3860, Loss function: 4.887, Average Loss: 3.809, avg. samples / sec: 58612.82
Iteration:   3860, Loss function: 2.254, Average Loss: 3.819, avg. samples / sec: 58729.58
Iteration:   3860, Loss function: 3.018, Average Loss: 3.801, avg. samples / sec: 58701.30
Iteration:   3860, Loss function: 2.112, Average Loss: 3.801, avg. samples / sec: 58642.12
Iteration:   3860, Loss function: 2.850, Average Loss: 3.785, avg. samples / sec: 58673.17
Iteration:   3860, Loss function: 3.456, Average Loss: 3.773, avg. samples / sec: 58911.37
Iteration:   3860, Loss function: 3.245, Average Loss: 3.810, avg. samples / sec: 58740.15
Iteration:   3860, Loss function: 2.642, Average Loss: 3.811, avg. samples / sec: 58587.73
Iteration:   3860, Loss function: 3.084, Average Loss: 3.775, avg. samples / sec: 58504.54
Iteration:   3860, Loss function: 3.201, Average Loss: 3.807, avg. samples / sec: 58832.01
Iteration:   3880, Loss function: 3.001, Average Loss: 3.799, avg. samples / sec: 59414.31
Iteration:   3880, Loss function: 3.617, Average Loss: 3.766, avg. samples / sec: 59293.05
Iteration:   3880, Loss function: 3.322, Average Loss: 3.768, avg. samples / sec: 59213.38
Iteration:   3880, Loss function: 3.542, Average Loss: 3.792, avg. samples / sec: 59137.59
Iteration:   3880, Loss function: 3.637, Average Loss: 3.775, avg. samples / sec: 59163.59
Iteration:   3880, Loss function: 2.043, Average Loss: 3.798, avg. samples / sec: 58758.28
Iteration:   3880, Loss function: 2.724, Average Loss: 3.798, avg. samples / sec: 59018.15
Iteration:   3880, Loss function: 2.640, Average Loss: 3.783, avg. samples / sec: 59018.37
Iteration:   3880, Loss function: 3.523, Average Loss: 3.804, avg. samples / sec: 59094.89
Iteration:   3880, Loss function: 4.061, Average Loss: 3.802, avg. samples / sec: 59047.38
Iteration:   3880, Loss function: 3.050, Average Loss: 3.813, avg. samples / sec: 58948.36
Iteration:   3880, Loss function: 3.540, Average Loss: 3.783, avg. samples / sec: 58917.43
Iteration:   3880, Loss function: 3.420, Average Loss: 3.787, avg. samples / sec: 58849.85
Iteration:   3880, Loss function: 3.536, Average Loss: 3.793, avg. samples / sec: 58843.41
Iteration:   3880, Loss function: 3.589, Average Loss: 3.784, avg. samples / sec: 57931.61
Iteration:   3900, Loss function: 3.153, Average Loss: 3.760, avg. samples / sec: 58350.44
Iteration:   3900, Loss function: 4.188, Average Loss: 3.797, avg. samples / sec: 58484.59
Iteration:   3900, Loss function: 3.725, Average Loss: 3.787, avg. samples / sec: 58434.51
Iteration:   3900, Loss function: 2.991, Average Loss: 3.779, avg. samples / sec: 58380.14
Iteration:   3900, Loss function: 3.864, Average Loss: 3.787, avg. samples / sec: 58201.05
Iteration:   3900, Loss function: 2.635, Average Loss: 3.792, avg. samples / sec: 58379.18
Iteration:   3900, Loss function: 2.428, Average Loss: 3.784, avg. samples / sec: 58483.81
Iteration:   3900, Loss function: 3.423, Average Loss: 3.783, avg. samples / sec: 58440.59
Iteration:   3900, Loss function: 2.825, Average Loss: 3.761, avg. samples / sec: 58168.10
Iteration:   3900, Loss function: 2.116, Average Loss: 3.777, avg. samples / sec: 59296.99
Iteration:   3900, Loss function: 3.909, Average Loss: 3.810, avg. samples / sec: 58351.26
Iteration:   3900, Loss function: 2.547, Average Loss: 3.768, avg. samples / sec: 58145.01
Iteration:   3900, Loss function: 3.147, Average Loss: 3.785, avg. samples / sec: 58134.05
Iteration:   3900, Loss function: 2.773, Average Loss: 3.776, avg. samples / sec: 58260.34
Iteration:   3900, Loss function: 2.015, Average Loss: 3.788, avg. samples / sec: 58110.10
:::MLL 1558639071.218 epoch_stop: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 819}}
:::MLL 1558639071.219 epoch_start: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 673}}
Iteration:   3920, Loss function: 2.623, Average Loss: 3.774, avg. samples / sec: 58397.80
Iteration:   3920, Loss function: 2.534, Average Loss: 3.751, avg. samples / sec: 58476.70
Iteration:   3920, Loss function: 2.418, Average Loss: 3.775, avg. samples / sec: 58434.05
Iteration:   3920, Loss function: 4.226, Average Loss: 3.804, avg. samples / sec: 58426.81
Iteration:   3920, Loss function: 3.636, Average Loss: 3.771, avg. samples / sec: 58504.84
Iteration:   3920, Loss function: 2.177, Average Loss: 3.777, avg. samples / sec: 58527.89
Iteration:   3920, Loss function: 4.370, Average Loss: 3.780, avg. samples / sec: 58439.16
Iteration:   3920, Loss function: 3.112, Average Loss: 3.776, avg. samples / sec: 58358.00
Iteration:   3920, Loss function: 3.764, Average Loss: 3.771, avg. samples / sec: 58371.39
Iteration:   3920, Loss function: 2.674, Average Loss: 3.774, avg. samples / sec: 58246.85
Iteration:   3920, Loss function: 3.146, Average Loss: 3.759, avg. samples / sec: 58409.18
Iteration:   3920, Loss function: 4.143, Average Loss: 3.779, avg. samples / sec: 58157.03
Iteration:   3920, Loss function: 3.787, Average Loss: 3.751, avg. samples / sec: 58099.32
Iteration:   3920, Loss function: 3.472, Average Loss: 3.782, avg. samples / sec: 58161.28
Iteration:   3920, Loss function: 2.959, Average Loss: 3.791, avg. samples / sec: 57977.13
Iteration:   3940, Loss function: 2.816, Average Loss: 3.776, avg. samples / sec: 59204.52
Iteration:   3940, Loss function: 3.355, Average Loss: 3.770, avg. samples / sec: 59166.22
Iteration:   3940, Loss function: 2.020, Average Loss: 3.766, avg. samples / sec: 59183.32
Iteration:   3940, Loss function: 3.527, Average Loss: 3.753, avg. samples / sec: 59162.37
Iteration:   3940, Loss function: 2.855, Average Loss: 3.767, avg. samples / sec: 59140.10
Iteration:   3940, Loss function: 3.998, Average Loss: 3.799, avg. samples / sec: 59080.62
Iteration:   3940, Loss function: 3.550, Average Loss: 3.769, avg. samples / sec: 59186.10
Iteration:   3940, Loss function: 2.983, Average Loss: 3.763, avg. samples / sec: 59022.01
Iteration:   3940, Loss function: 3.418, Average Loss: 3.744, avg. samples / sec: 59172.75
Iteration:   3940, Loss function: 4.356, Average Loss: 3.765, avg. samples / sec: 59097.77
Iteration:   3940, Loss function: 2.813, Average Loss: 3.771, avg. samples / sec: 59056.78
Iteration:   3940, Loss function: 3.824, Average Loss: 3.786, avg. samples / sec: 59155.22
Iteration:   3940, Loss function: 3.116, Average Loss: 3.768, avg. samples / sec: 58788.89
Iteration:   3940, Loss function: 3.043, Average Loss: 3.750, avg. samples / sec: 58752.81
Iteration:   3940, Loss function: 3.035, Average Loss: 3.760, avg. samples / sec: 58800.08
Iteration:   3960, Loss function: 3.809, Average Loss: 3.759, avg. samples / sec: 59673.19
Iteration:   3960, Loss function: 2.699, Average Loss: 3.760, avg. samples / sec: 59582.34
Iteration:   3960, Loss function: 3.450, Average Loss: 3.769, avg. samples / sec: 59530.87
Iteration:   3960, Loss function: 2.941, Average Loss: 3.752, avg. samples / sec: 59599.22
Iteration:   3960, Loss function: 2.788, Average Loss: 3.778, avg. samples / sec: 59791.98
Iteration:   3960, Loss function: 4.134, Average Loss: 3.756, avg. samples / sec: 59556.13
Iteration:   3960, Loss function: 2.420, Average Loss: 3.735, avg. samples / sec: 59597.73
Iteration:   3960, Loss function: 2.917, Average Loss: 3.742, avg. samples / sec: 59823.58
Iteration:   3960, Loss function: 3.303, Average Loss: 3.786, avg. samples / sec: 59545.51
Iteration:   3960, Loss function: 3.509, Average Loss: 3.761, avg. samples / sec: 59451.81
Iteration:   3960, Loss function: 3.480, Average Loss: 3.737, avg. samples / sec: 59408.75
Iteration:   3960, Loss function: 3.040, Average Loss: 3.764, avg. samples / sec: 59578.58
Iteration:   3960, Loss function: 3.896, Average Loss: 3.761, avg. samples / sec: 59571.16
Iteration:   3960, Loss function: 3.438, Average Loss: 3.752, avg. samples / sec: 59370.71
Iteration:   3960, Loss function: 3.027, Average Loss: 3.750, avg. samples / sec: 59607.44
:::MLL 1558639073.207 epoch_stop: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 819}}
:::MLL 1558639073.208 epoch_start: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 673}}
Iteration:   3980, Loss function: 4.755, Average Loss: 3.726, avg. samples / sec: 59668.31
Iteration:   3980, Loss function: 3.340, Average Loss: 3.743, avg. samples / sec: 59927.47
Iteration:   3980, Loss function: 3.357, Average Loss: 3.757, avg. samples / sec: 59710.73
Iteration:   3980, Loss function: 2.782, Average Loss: 3.753, avg. samples / sec: 59787.34
Iteration:   3980, Loss function: 3.383, Average Loss: 3.776, avg. samples / sec: 59583.67
Iteration:   3980, Loss function: 3.029, Average Loss: 3.729, avg. samples / sec: 59562.85
Iteration:   3980, Loss function: 4.583, Average Loss: 3.752, avg. samples / sec: 59451.08
Iteration:   3980, Loss function: 3.588, Average Loss: 3.741, avg. samples / sec: 59743.41
Iteration:   3980, Loss function: 4.167, Average Loss: 3.748, avg. samples / sec: 59462.65
Iteration:   3980, Loss function: 3.120, Average Loss: 3.736, avg. samples / sec: 59626.13
Iteration:   3980, Loss function: 3.874, Average Loss: 3.756, avg. samples / sec: 59364.96
Iteration:   3980, Loss function: 4.353, Average Loss: 3.756, avg. samples / sec: 59686.08
Iteration:   3980, Loss function: 2.318, Average Loss: 3.746, avg. samples / sec: 59424.01
Iteration:   3980, Loss function: 2.933, Average Loss: 3.762, avg. samples / sec: 59358.31
Iteration:   3980, Loss function: 4.245, Average Loss: 3.772, avg. samples / sec: 59404.50
Iteration:   4000, Loss function: 2.878, Average Loss: 3.756, avg. samples / sec: 57625.09
Iteration:   4000, Loss function: 4.934, Average Loss: 3.751, avg. samples / sec: 57564.83
Iteration:   4000, Loss function: 3.321, Average Loss: 3.740, avg. samples / sec: 57519.91
Iteration:   4000, Loss function: 3.324, Average Loss: 3.732, avg. samples / sec: 57506.25
Iteration:   4000, Loss function: 2.906, Average Loss: 3.763, avg. samples / sec: 57435.38
Iteration:   4000, Loss function: 3.146, Average Loss: 3.745, avg. samples / sec: 57409.10
Iteration:   4000, Loss function: 3.500, Average Loss: 3.722, avg. samples / sec: 57273.04
Iteration:   4000, Loss function: 2.632, Average Loss: 3.737, avg. samples / sec: 57500.41
Iteration:   4000, Loss function: 2.067, Average Loss: 3.742, avg. samples / sec: 57330.66
Iteration:   4000, Loss function: 2.705, Average Loss: 3.748, avg. samples / sec: 57434.21
Iteration:   4000, Loss function: 1.893, Average Loss: 3.733, avg. samples / sec: 57246.56
Iteration:   4000, Loss function: 3.098, Average Loss: 3.763, avg. samples / sec: 57463.22
Iteration:   4000, Loss function: 3.931, Average Loss: 3.731, avg. samples / sec: 57388.81
Iteration:   4000, Loss function: 3.921, Average Loss: 3.747, avg. samples / sec: 57212.45
Iteration:   4000, Loss function: 2.746, Average Loss: 3.726, avg. samples / sec: 57242.66
Iteration:   4020, Loss function: 3.439, Average Loss: 3.720, avg. samples / sec: 58262.68
Iteration:   4020, Loss function: 4.802, Average Loss: 3.752, avg. samples / sec: 57931.95
Iteration:   4020, Loss function: 3.069, Average Loss: 3.738, avg. samples / sec: 57927.11
Iteration:   4020, Loss function: 3.840, Average Loss: 3.758, avg. samples / sec: 58065.95
Iteration:   4020, Loss function: 2.848, Average Loss: 3.739, avg. samples / sec: 58015.61
Iteration:   4020, Loss function: 2.322, Average Loss: 3.728, avg. samples / sec: 58042.73
Iteration:   4020, Loss function: 3.729, Average Loss: 3.735, avg. samples / sec: 57947.48
Iteration:   4020, Loss function: 3.481, Average Loss: 3.740, avg. samples / sec: 57907.40
Iteration:   4020, Loss function: 3.426, Average Loss: 3.729, avg. samples / sec: 57839.86
Iteration:   4020, Loss function: 3.397, Average Loss: 3.715, avg. samples / sec: 57792.19
Iteration:   4020, Loss function: 2.325, Average Loss: 3.725, avg. samples / sec: 57697.76
Iteration:   4020, Loss function: 3.873, Average Loss: 3.736, avg. samples / sec: 57666.12
Iteration:   4020, Loss function: 3.468, Average Loss: 3.756, avg. samples / sec: 57684.91
Iteration:   4020, Loss function: 3.767, Average Loss: 3.719, avg. samples / sec: 57797.45
Iteration:   4020, Loss function: 3.910, Average Loss: 3.744, avg. samples / sec: 57805.72
Iteration:   4040, Loss function: 2.741, Average Loss: 3.724, avg. samples / sec: 57951.24
Iteration:   4040, Loss function: 2.906, Average Loss: 3.744, avg. samples / sec: 57842.21
Iteration:   4040, Loss function: 3.979, Average Loss: 3.713, avg. samples / sec: 58144.43
Iteration:   4040, Loss function: 2.614, Average Loss: 3.706, avg. samples / sec: 58089.45
Iteration:   4040, Loss function: 3.627, Average Loss: 3.752, avg. samples / sec: 57845.89
Iteration:   4040, Loss function: 3.085, Average Loss: 3.716, avg. samples / sec: 57683.68
Iteration:   4040, Loss function: 3.422, Average Loss: 3.735, avg. samples / sec: 58092.97
Iteration:   4040, Loss function: 3.857, Average Loss: 3.725, avg. samples / sec: 58053.63
Iteration:   4040, Loss function: 3.245, Average Loss: 3.732, avg. samples / sec: 57925.95
Iteration:   4040, Loss function: 3.191, Average Loss: 3.746, avg. samples / sec: 58033.05
Iteration:   4040, Loss function: 3.901, Average Loss: 3.734, avg. samples / sec: 57791.59
Iteration:   4040, Loss function: 3.050, Average Loss: 3.731, avg. samples / sec: 57733.09
Iteration:   4040, Loss function: 4.177, Average Loss: 3.722, avg. samples / sec: 57978.26
Iteration:   4040, Loss function: 3.171, Average Loss: 3.722, avg. samples / sec: 57872.26
Iteration:   4040, Loss function: 3.307, Average Loss: 3.726, avg. samples / sec: 57709.19
:::MLL 1558639075.233 epoch_stop: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 819}}
:::MLL 1558639075.234 epoch_start: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 673}}
Iteration:   4060, Loss function: 3.644, Average Loss: 3.711, avg. samples / sec: 59911.80
Iteration:   4060, Loss function: 3.395, Average Loss: 3.706, avg. samples / sec: 59666.44
Iteration:   4060, Loss function: 3.371, Average Loss: 3.734, avg. samples / sec: 59623.73
Iteration:   4060, Loss function: 3.184, Average Loss: 3.716, avg. samples / sec: 59553.11
Iteration:   4060, Loss function: 2.904, Average Loss: 3.743, avg. samples / sec: 59639.52
Iteration:   4060, Loss function: 2.702, Average Loss: 3.727, avg. samples / sec: 59637.66
Iteration:   4060, Loss function: 4.550, Average Loss: 3.728, avg. samples / sec: 59633.17
Iteration:   4060, Loss function: 3.132, Average Loss: 3.720, avg. samples / sec: 59639.93
Iteration:   4060, Loss function: 3.596, Average Loss: 3.720, avg. samples / sec: 59739.56
Iteration:   4060, Loss function: 3.523, Average Loss: 3.718, avg. samples / sec: 59636.42
Iteration:   4060, Loss function: 3.122, Average Loss: 3.716, avg. samples / sec: 59585.18
Iteration:   4060, Loss function: 3.159, Average Loss: 3.698, avg. samples / sec: 59454.82
Iteration:   4060, Loss function: 2.781, Average Loss: 3.740, avg. samples / sec: 59528.40
Iteration:   4060, Loss function: 3.261, Average Loss: 3.724, avg. samples / sec: 59507.57
Iteration:   4060, Loss function: 3.371, Average Loss: 3.715, avg. samples / sec: 59464.91
Iteration:   4080, Loss function: 2.732, Average Loss: 3.702, avg. samples / sec: 60044.90
Iteration:   4080, Loss function: 2.822, Average Loss: 3.708, avg. samples / sec: 60025.87
Iteration:   4080, Loss function: 3.658, Average Loss: 3.717, avg. samples / sec: 59863.45
Iteration:   4080, Loss function: 3.083, Average Loss: 3.739, avg. samples / sec: 59980.01
Iteration:   4080, Loss function: 3.342, Average Loss: 3.736, avg. samples / sec: 59659.62
Iteration:   4080, Loss function: 4.341, Average Loss: 3.715, avg. samples / sec: 59811.19
Iteration:   4080, Loss function: 1.994, Average Loss: 3.718, avg. samples / sec: 59696.39
Iteration:   4080, Loss function: 3.869, Average Loss: 3.702, avg. samples / sec: 59566.45
Iteration:   4080, Loss function: 3.961, Average Loss: 3.708, avg. samples / sec: 59570.10
Iteration:   4080, Loss function: 5.743, Average Loss: 3.730, avg. samples / sec: 59556.25
Iteration:   4080, Loss function: 3.899, Average Loss: 3.715, avg. samples / sec: 59736.35
Iteration:   4080, Loss function: 3.557, Average Loss: 3.704, avg. samples / sec: 59320.73
Iteration:   4080, Loss function: 2.844, Average Loss: 3.695, avg. samples / sec: 59689.59
Iteration:   4080, Loss function: 2.549, Average Loss: 3.710, avg. samples / sec: 59540.63
Iteration:   4080, Loss function: 3.295, Average Loss: 3.713, avg. samples / sec: 59485.11
Iteration:   4100, Loss function: 4.063, Average Loss: 3.707, avg. samples / sec: 56973.31
Iteration:   4100, Loss function: 3.137, Average Loss: 3.704, avg. samples / sec: 56912.85
Iteration:   4100, Loss function: 6.167, Average Loss: 3.708, avg. samples / sec: 56573.96
Iteration:   4100, Loss function: 3.431, Average Loss: 3.700, avg. samples / sec: 56672.31
Iteration:   4100, Loss function: 3.685, Average Loss: 3.695, avg. samples / sec: 56707.29
Iteration:   4100, Loss function: 3.508, Average Loss: 3.712, avg. samples / sec: 56595.36
Iteration:   4100, Loss function: 4.474, Average Loss: 3.723, avg. samples / sec: 56637.05
Iteration:   4100, Loss function: 3.998, Average Loss: 3.704, avg. samples / sec: 56606.25
Iteration:   4100, Loss function: 3.540, Average Loss: 3.738, avg. samples / sec: 56399.33
Iteration:   4100, Loss function: 3.054, Average Loss: 3.691, avg. samples / sec: 56295.76
Iteration:   4100, Loss function: 3.795, Average Loss: 3.733, avg. samples / sec: 56473.07
Iteration:   4100, Loss function: 2.715, Average Loss: 3.696, avg. samples / sec: 56294.41
Iteration:   4100, Loss function: 3.751, Average Loss: 3.689, avg. samples / sec: 56513.61
Iteration:   4100, Loss function: 2.363, Average Loss: 3.708, avg. samples / sec: 56322.60
Iteration:   4100, Loss function: 3.471, Average Loss: 3.712, avg. samples / sec: 56420.40
:::MLL 1558639077.241 epoch_stop: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 819}}
:::MLL 1558639077.242 epoch_start: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 673}}
Iteration:   4120, Loss function: 3.474, Average Loss: 3.702, avg. samples / sec: 59199.53
Iteration:   4120, Loss function: 3.845, Average Loss: 3.697, avg. samples / sec: 59185.25
Iteration:   4120, Loss function: 2.831, Average Loss: 3.735, avg. samples / sec: 59338.54
Iteration:   4120, Loss function: 3.945, Average Loss: 3.704, avg. samples / sec: 59544.43
Iteration:   4120, Loss function: 3.620, Average Loss: 3.725, avg. samples / sec: 59354.06
Iteration:   4120, Loss function: 4.133, Average Loss: 3.697, avg. samples / sec: 59298.96
Iteration:   4120, Loss function: 3.692, Average Loss: 3.700, avg. samples / sec: 59069.43
Iteration:   4120, Loss function: 2.513, Average Loss: 3.689, avg. samples / sec: 59168.76
Iteration:   4120, Loss function: 3.903, Average Loss: 3.719, avg. samples / sec: 59194.33
Iteration:   4120, Loss function: 2.608, Average Loss: 3.683, avg. samples / sec: 59278.16
Iteration:   4120, Loss function: 3.563, Average Loss: 3.681, avg. samples / sec: 59368.49
Iteration:   4120, Loss function: 3.478, Average Loss: 3.697, avg. samples / sec: 59075.52
Iteration:   4120, Loss function: 3.320, Average Loss: 3.695, avg. samples / sec: 59328.37
Iteration:   4120, Loss function: 3.238, Average Loss: 3.701, avg. samples / sec: 59387.95
Iteration:   4120, Loss function: 2.809, Average Loss: 3.705, avg. samples / sec: 59103.52
Iteration:   4140, Loss function: 3.428, Average Loss: 3.685, avg. samples / sec: 56759.93
Iteration:   4140, Loss function: 2.759, Average Loss: 3.715, avg. samples / sec: 56709.18
Iteration:   4140, Loss function: 2.475, Average Loss: 3.687, avg. samples / sec: 56636.37
Iteration:   4140, Loss function: 3.923, Average Loss: 3.678, avg. samples / sec: 56779.81
Iteration:   4140, Loss function: 2.604, Average Loss: 3.710, avg. samples / sec: 56755.22
Iteration:   4140, Loss function: 2.955, Average Loss: 3.691, avg. samples / sec: 56689.41
Iteration:   4140, Loss function: 3.421, Average Loss: 3.686, avg. samples / sec: 56769.22
Iteration:   4140, Loss function: 2.380, Average Loss: 3.686, avg. samples / sec: 56670.78
Iteration:   4140, Loss function: 4.408, Average Loss: 3.687, avg. samples / sec: 56767.43
Iteration:   4140, Loss function: 4.296, Average Loss: 3.692, avg. samples / sec: 56790.47
Iteration:   4140, Loss function: 2.779, Average Loss: 3.673, avg. samples / sec: 56715.37
Iteration:   4140, Loss function: 2.323, Average Loss: 3.728, avg. samples / sec: 56603.20
Iteration:   4140, Loss function: 2.696, Average Loss: 3.688, avg. samples / sec: 56534.58
Iteration:   4140, Loss function: 4.396, Average Loss: 3.704, avg. samples / sec: 56726.51
Iteration:   4140, Loss function: 3.418, Average Loss: 3.700, avg. samples / sec: 56546.70
Iteration:   4160, Loss function: 4.604, Average Loss: 3.671, avg. samples / sec: 58700.13
Iteration:   4160, Loss function: 3.321, Average Loss: 3.685, avg. samples / sec: 58600.73
Iteration:   4160, Loss function: 3.044, Average Loss: 3.683, avg. samples / sec: 58660.47
Iteration:   4160, Loss function: 4.048, Average Loss: 3.682, avg. samples / sec: 58545.45
Iteration:   4160, Loss function: 3.224, Average Loss: 3.700, avg. samples / sec: 58648.92
Iteration:   4160, Loss function: 3.324, Average Loss: 3.675, avg. samples / sec: 58492.65
Iteration:   4160, Loss function: 2.390, Average Loss: 3.684, avg. samples / sec: 58509.91
Iteration:   4160, Loss function: 3.295, Average Loss: 3.701, avg. samples / sec: 58484.73
Iteration:   4160, Loss function: 2.947, Average Loss: 3.669, avg. samples / sec: 58520.77
Iteration:   4160, Loss function: 4.103, Average Loss: 3.692, avg. samples / sec: 58625.19
Iteration:   4160, Loss function: 3.384, Average Loss: 3.678, avg. samples / sec: 58446.22
Iteration:   4160, Loss function: 3.971, Average Loss: 3.686, avg. samples / sec: 58430.73
Iteration:   4160, Loss function: 4.526, Average Loss: 3.723, avg. samples / sec: 58457.68
Iteration:   4160, Loss function: 3.952, Average Loss: 3.706, avg. samples / sec: 58288.12
Iteration:   4160, Loss function: 2.873, Average Loss: 3.679, avg. samples / sec: 58329.57
Iteration:   4180, Loss function: 4.009, Average Loss: 3.670, avg. samples / sec: 60613.02
Iteration:   4180, Loss function: 3.056, Average Loss: 3.678, avg. samples / sec: 60283.02
Iteration:   4180, Loss function: 3.701, Average Loss: 3.669, avg. samples / sec: 60164.45
Iteration:   4180, Loss function: 3.917, Average Loss: 3.675, avg. samples / sec: 60251.60
Iteration:   4180, Loss function: 3.632, Average Loss: 3.674, avg. samples / sec: 60098.31
Iteration:   4180, Loss function: 2.655, Average Loss: 3.685, avg. samples / sec: 60128.62
Iteration:   4180, Loss function: 3.626, Average Loss: 3.685, avg. samples / sec: 60172.78
Iteration:   4180, Loss function: 3.712, Average Loss: 3.665, avg. samples / sec: 60091.19
Iteration:   4180, Loss function: 3.805, Average Loss: 3.694, avg. samples / sec: 60074.66
Iteration:   4180, Loss function: 3.868, Average Loss: 3.690, avg. samples / sec: 60007.49
Iteration:   4180, Loss function: 3.951, Average Loss: 3.719, avg. samples / sec: 60135.60
Iteration:   4180, Loss function: 4.100, Average Loss: 3.666, avg. samples / sec: 59778.43
Iteration:   4180, Loss function: 3.358, Average Loss: 3.675, avg. samples / sec: 59925.92
Iteration:   4180, Loss function: 3.010, Average Loss: 3.677, avg. samples / sec: 59866.27
Iteration:   4180, Loss function: 3.755, Average Loss: 3.697, avg. samples / sec: 60077.71
:::MLL 1558639079.252 epoch_stop: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 819}}
:::MLL 1558639079.252 epoch_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 673}}
:::MLL 1558639079.314 eval_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.54s)
DONE (t=0.54s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.57s)
DONE (t=2.86s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.23041
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.39310
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23565
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05896
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24483
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.36943
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22172
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32361
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.34024
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09762
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.37070
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.53096
Current AP: 0.23041 AP goal: 0.23000
:::MLL 1558639083.406 eval_accuracy: {"value": 0.23041388738371285, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 389}}
:::MLL 1558639083.489 eval_stop: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 392}}
:::MLL 1558639083.498 block_stop: {"value": null, "metadata": {"first_epoch_num": 55, "file": "train.py", "lineno": 804}}
:::MLL 1558639084.561 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 849}}
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-05-23 07:18:13 PM
RESULT,SINGLE_STAGE_DETECTOR,,218,nvidia,2019-05-23 07:14:35 PM
ENDING TIMING RUN AT 2019-05-23 07:18:13 PM
RESULT,SINGLE_STAGE_DETECTOR,,218,nvidia,2019-05-23 07:14:35 PM
ENDING TIMING RUN AT 2019-05-23 07:18:13 PM
RESULT,SINGLE_STAGE_DETECTOR,,218,nvidia,2019-05-23 07:14:35 PM
ENDING TIMING RUN AT 2019-05-23 07:18:13 PM
RESULT,SINGLE_STAGE_DETECTOR,,218,nvidia,2019-05-23 07:14:35 PM
ENDING TIMING RUN AT 2019-05-23 07:18:14 PM
RESULT,SINGLE_STAGE_DETECTOR,,219,nvidia,2019-05-23 07:14:35 PM
ENDING TIMING RUN AT 2019-05-23 07:18:14 PM
RESULT,SINGLE_STAGE_DETECTOR,,219,nvidia,2019-05-23 07:14:35 PM
ENDING TIMING RUN AT 2019-05-23 07:18:14 PM
RESULT,SINGLE_STAGE_DETECTOR,,219,nvidia,2019-05-23 07:14:35 PM
ENDING TIMING RUN AT 2019-05-23 07:18:14 PM
RESULT,SINGLE_STAGE_DETECTOR,,219,nvidia,2019-05-23 07:14:35 PM
ENDING TIMING RUN AT 2019-05-23 07:18:14 PM
RESULT,SINGLE_STAGE_DETECTOR,,219,nvidia,2019-05-23 07:14:35 PM
ENDING TIMING RUN AT 2019-05-23 07:18:14 PM
RESULT,SINGLE_STAGE_DETECTOR,,219,nvidia,2019-05-23 07:14:35 PM
ENDING TIMING RUN AT 2019-05-23 07:18:14 PM
RESULT,SINGLE_STAGE_DETECTOR,,219,nvidia,2019-05-23 07:14:35 PM
ENDING TIMING RUN AT 2019-05-23 07:18:14 PM
RESULT,SINGLE_STAGE_DETECTOR,,219,nvidia,2019-05-23 07:14:35 PM
ENDING TIMING RUN AT 2019-05-23 07:18:14 PM
RESULT,SINGLE_STAGE_DETECTOR,,219,nvidia,2019-05-23 07:14:35 PM
ENDING TIMING RUN AT 2019-05-23 07:18:14 PM
RESULT,SINGLE_STAGE_DETECTOR,,219,nvidia,2019-05-23 07:14:35 PM
ENDING TIMING RUN AT 2019-05-23 07:18:14 PM
RESULT,SINGLE_STAGE_DETECTOR,,219,nvidia,2019-05-23 07:14:35 PM
