Beginning trial 4 of 5
Gathering sys log on circe-n001
:::MLL 1558639311.674 submission_benchmark: {"value": "ssd", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1558639311.674 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known ssd keys.
:::MLL 1558639311.674 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1558639311.675 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1558639311.675 submission_platform: {"value": "15xNVIDIA DGX-2H", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1558639311.676 submission_entry: {"value": "{'hardware': 'NVIDIA DGX-2H', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': 'Ubuntu 18.04.1 LTS / NVIDIA DGX Server 4.0.4 4.0.5', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.4-2.0.7', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '15', 'cpu': '2x Intel(R) Xeon(R) Platinum 8174 CPU @ 3.10GHz', 'num_cores': '48', 'num_vcpus': '96', 'accelerator': 'Tesla V100-SXM3-32GB-H', 'num_accelerators': '16', 'sys_mem_size': '1510 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '2x 894.3G + 8x 3.5T', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27800 Family [ConnectX-5]', 'num_network_cards': '10', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1558639311.676 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1558639311.676 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
:::MLL 1558639314.470 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639314.530 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639314.555 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639314.546 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639314.553 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639314.560 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639314.566 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639314.561 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639314.572 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639314.581 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639314.571 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639314.601 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639314.595 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639314.595 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639314.658 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node circe-n001
+ pids+=($!)
+ set +x
Launching on node circe-n002
+ pids+=($!)
+ set +x
Launching on node circe-n003
+ pids+=($!)
+ set +x
Launching on node circe-n004
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n001
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n002
+ pids+=($!)
+ set +x
Launching on node circe-n005
+ srun --mem=0 -N 1 -n 1 -w circe-n002 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=1 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w circe-n001 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=0 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n003
+ pids+=($!)
+ set +x
Launching on node circe-n006
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ srun --mem=0 -N 1 -n 1 -w circe-n003 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=2 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n004
+ pids+=($!)
+ set +x
Launching on node circe-n007
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n005
+ srun --mem=0 -N 1 -n 1 -w circe-n004 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=3 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n008
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n006
+ srun --mem=0 -N 1 -n 1 -w circe-n005 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=4 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n009
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n007
+ srun --mem=0 -N 1 -n 1 -w circe-n006 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=5 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n010
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n008
+ srun --mem=0 -N 1 -n 1 -w circe-n007 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=6 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n011
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n009
+ srun --mem=0 -N 1 -n 1 -w circe-n008 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=7 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n012
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n010
+ srun --mem=0 -N 1 -n 1 -w circe-n009 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=8 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+ pids+=($!)
+ set +x
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n011
Launching on node circe-n013
+ srun --mem=0 -N 1 -n 1 -w circe-n010 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=9 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w circe-n011 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=10 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n014
+ pids+=($!)
+ set +x
Launching on node circe-n015
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n013
+ pids+=($!)
+ set +x
+ srun --mem=0 -N 1 -n 1 -w circe-n013 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=12 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n014
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n015
+ srun --mem=0 -N 1 -n 1 -w circe-n014 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=13 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w circe-n015 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=14 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n012
+ srun --mem=0 -N 1 -n 1 -w circe-n012 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=11 --master_addr=10.0.1.1 --master_port=4715' -e SLURM_JOB_ID=89738 -e SLURM_NTASKS_PER_NODE=16 cont_89738 ./run_and_time.sh
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=0 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=7 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=6 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=13 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=8 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=12 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=14 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=10 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=9 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=2 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=1 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=3 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=5 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=4 --master_addr=10.0.1.1 --master_port=4715
Run vars: id 89738 gpus 16 mparams  --nnodes=15 --node_rank=11 --master_addr=10.0.1.1 --master_port=4715
STARTING TIMING RUN AT 2019-05-23 07:21:54 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=0 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:21:54 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=7 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:21:54 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=13 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:21:54 PM
running benchmark
STARTING TIMING RUN AT 2019-05-23 07:21:54 PM
+ NUMEPOCHS=80
+ echo 'running benchmark'
running benchmark
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=14 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=6 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:21:54 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
STARTING TIMING RUN AT 2019-05-23 07:21:54 PM
+ export DATASET_DIR=/data/coco2017
running benchmark
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=8 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=12 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:21:54 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=3 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:21:54 PM
STARTING TIMING RUN AT 2019-05-23 07:21:54 PM
running benchmark
STARTING TIMING RUN AT 2019-05-23 07:21:54 PM
running benchmark
STARTING TIMING RUN AT 2019-05-23 07:21:54 PM
running benchmark
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ NUMEPOCHS=80
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ NUMEPOCHS=80
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=10 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ TORCH_MODEL_ZOO=/data/torchvision
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=2 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=1 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=9 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
STARTING TIMING RUN AT 2019-05-23 07:21:54 PM
STARTING TIMING RUN AT 2019-05-23 07:21:54 PM
running benchmark
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=4 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=5 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:21:54 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=11 --master_addr=10.0.1.1 --master_port=4715 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding::::MLL 1558639324.957 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639324.958 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639324.958 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639324.959 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639324.959 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639324.959 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639324.960 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639324.960 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639324.960 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639324.960 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639324.961 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639324.961 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639324.961 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639324.961 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639324.961 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639324.962 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639325.090 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.091 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.092 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.092 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.092 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639325.095 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.095 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.096 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.096 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.096 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.096 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.097 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.097 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.097 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.097 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639325.098 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639325.165 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.166 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.169 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.169 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.169 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.170 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.170 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.171 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.171 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
Binding::::MLL 1558639325.165 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.165 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.166 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.166 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.172 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.166 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.166 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.166 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.172 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.172 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.167 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.173 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.173 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.167 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.173 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.173 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639325.168 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.169 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.169 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639325.169 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.169 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.169 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.170 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639325.204 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.208 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.209 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.210 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.210 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.210 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.211 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.211 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.211 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.211 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639325.211 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.212 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.212 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.212 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.212 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.212 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639325.218 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.219 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.219 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.220 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.221 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.221 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.221 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.221 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.221 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.221 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.222 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
Binding::::MLL 1558639325.223 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.222 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.222 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.223 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.224 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.223 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639325.223 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.225 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639325.226 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639325.228 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.229 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.229 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.229 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.230 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.230 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.230 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.230 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.231 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.231 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.231 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639325.232 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639325.225 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.225 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.227 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.227 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.227 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.228 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.228 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.229 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.229 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.229 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639325.229 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.229 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.229 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.230 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.230 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.230 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639325.251 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
Binding::::MLL 1558639325.246 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.247 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.247 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.247 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.248 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.248 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.253 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.248 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.254 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.254 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.254 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.254 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.254 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.254 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.254 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.254 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.254 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.254 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.255 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639325.255 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.255 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.250 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639325.250 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639325.256 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639325.251 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.251 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.252 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639325.252 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.252 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.253 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.253 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639325.254 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.254 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.255 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.255 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.255 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639325.256 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.256 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.256 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.256 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639325.257 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.257 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.257 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.257 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.257 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.258 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.258 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639325.291 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.291 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
Binding::::MLL 1558639325.264 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.292 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.265 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.265 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.292 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.292 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.265 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.292 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.266 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.293 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.266 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.266 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.293 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.293 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.266 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.293 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.266 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.294 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.267 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.294 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.267 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.267 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.267 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.294 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.267 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.294 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639325.268 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.294 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.295 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.268 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639325.276 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.277 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.278 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.279 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.279 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.279 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.280 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.280 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.281 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.281 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.281 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.281 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.281 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.281 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.282 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.282 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639325.337 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.340 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.340 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.341 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.341 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.341 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639325.342 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.342 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.343 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.343 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639325.343 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.343 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.344 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.344 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639325.345 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639325.346 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
0 Using seed = 3391229488
1 Using seed = 3391229489
3 Using seed = 3391229491
4 Using seed = 3391229492
5 Using seed = 3391229493
2 Using seed = 3391229490
:::MLL 1558639358.293 max_samples: {"value": 1, "metadata": {"file": "utils.py", "lineno": 465}}
11 Using seed = 3391229499
7 Using seed = 3391229495
9 Using seed = 3391229497
8 Using seed = 3391229496
6 Using seed = 3391229494
16 Using seed = 3391229504
23 Using seed = 3391229511
31 Using seed = 3391229519
17 Using seed = 3391229505
20 Using seed = 3391229508
22 Using seed = 3391229510
25 Using seed = 3391229513
26 Using seed = 3391229514
28 Using seed = 3391229516
19 Using seed = 3391229507
27 Using seed = 3391229515
29 Using seed = 3391229517
18 Using seed = 3391229506
21 Using seed = 3391229509
24 Using seed = 3391229512
30 Using seed = 3391229518
46 Using seed = 3391229534
44 Using seed = 3391229532
45 Using seed = 3391229533
47 Using seed = 3391229535
42 Using seed = 3391229530
36 Using seed = 3391229524
34 Using seed = 3391229522
35 Using seed = 3391229523
33 Using seed = 3391229521
32 Using seed = 3391229520
37 Using seed = 3391229525
41 Using seed = 3391229529
43 Using seed = 3391229531
40 Using seed = 3391229528
39 Using seed = 3391229527
38 Using seed = 3391229526
61 Using seed = 3391229549
63 Using seed = 3391229551
58 Using seed = 3391229546
54 Using seed = 3391229542
57 Using seed = 3391229545
50 Using seed = 3391229538
60 Using seed = 3391229548
51 Using seed = 3391229539
48 Using seed = 3391229536
49 Using seed = 3391229537
55 Using seed = 3391229543
59 Using seed = 3391229547
53 Using seed = 3391229541
56 Using seed = 3391229544
52 Using seed = 3391229540
62 Using seed = 3391229550
74 Using seed = 3391229562
78 Using seed = 3391229566
77 Using seed = 3391229565
68 Using seed = 3391229556
79 Using seed = 3391229567
67 Using seed = 3391229555
65 Using seed = 3391229553
64 Using seed = 3391229552
66 Using seed = 3391229554
69 Using seed = 3391229557
76 Using seed = 3391229564
73 Using seed = 3391229561
75 Using seed = 3391229563
72 Using seed = 3391229560
71 Using seed = 3391229559
70 Using seed = 3391229558
86 Using seed = 3391229574
84 Using seed = 3391229572
82 Using seed = 3391229570
83 Using seed = 3391229571
87 Using seed = 3391229575
85 Using seed = 3391229573
80 Using seed = 3391229568
92 Using seed = 3391229580
81 Using seed = 3391229569
95 Using seed = 3391229583
88 Using seed = 3391229576
94 Using seed = 3391229582
91 Using seed = 3391229579
89 Using seed = 3391229577
93 Using seed = 3391229581
90 Using seed = 3391229578
106 Using seed = 3391229594
110 Using seed = 3391229598
109 Using seed = 3391229597
111 Using seed = 3391229599
99 Using seed = 3391229587
98 Using seed = 3391229586
100 Using seed = 3391229588
97 Using seed = 3391229585
101 Using seed = 3391229589
96 Using seed = 3391229584
108 Using seed = 3391229596
105 Using seed = 3391229593
103 Using seed = 3391229591
107 Using seed = 3391229595
104 Using seed = 3391229592
102 Using seed = 3391229590
124 Using seed = 3391229612
125 Using seed = 3391229613
123 Using seed = 3391229611
126 Using seed = 3391229614
122 Using seed = 3391229610
120 Using seed = 3391229608
127 Using seed = 3391229615
121 Using seed = 3391229609
118 Using seed = 3391229606
113 Using seed = 3391229601
116 Using seed = 3391229604
119 Using seed = 3391229607
114 Using seed = 3391229602
117 Using seed = 3391229605
112 Using seed = 3391229600
115 Using seed = 3391229603
143 Using seed = 3391229631
142 Using seed = 3391229630
141 Using seed = 3391229629
138 Using seed = 3391229626
140 Using seed = 3391229628
137 Using seed = 3391229625
139 Using seed = 3391229627
134 Using seed = 3391229622
135 Using seed = 3391229623
131 Using seed = 3391229619
128 Using seed = 3391229616
132 Using seed = 3391229620
129 Using seed = 3391229617
130 Using seed = 3391229618
136 Using seed = 3391229624
133 Using seed = 3391229621
150 Using seed = 3391229638
153 Using seed = 3391229641
157 Using seed = 3391229645
158 Using seed = 3391229646
154 Using seed = 3391229642
156 Using seed = 3391229644
159 Using seed = 3391229647
155 Using seed = 3391229643
147 Using seed = 3391229635
152 Using seed = 3391229640
144 Using seed = 3391229632
148 Using seed = 3391229636
145 Using seed = 3391229633
146 Using seed = 3391229634
149 Using seed = 3391229637
151 Using seed = 3391229639
166 Using seed = 3391229654
164 Using seed = 3391229652
161 Using seed = 3391229649
160 Using seed = 3391229648
163 Using seed = 3391229651
174 Using seed = 3391229662
162 Using seed = 3391229650
172 Using seed = 3391229660
175 Using seed = 3391229663
170 Using seed = 3391229658
165 Using seed = 3391229653
173 Using seed = 3391229661
169 Using seed = 3391229657
168 Using seed = 3391229656
171 Using seed = 3391229659
167 Using seed = 3391229655
188 Using seed = 3391229676
187 Using seed = 3391229675
179 Using seed = 3391229667
178 Using seed = 3391229666
190 Using seed = 3391229678
177 Using seed = 3391229665
186 Using seed = 3391229674
185 Using seed = 3391229673
184 Using seed = 3391229672
182 Using seed = 3391229670
191 Using seed = 3391229679
181 Using seed = 3391229669
189 Using seed = 3391229677
176 Using seed = 3391229664
183 Using seed = 3391229671
180 Using seed = 3391229668
207 Using seed = 3391229695
203 Using seed = 3391229691
193 Using seed = 3391229681
202 Using seed = 3391229690
206 Using seed = 3391229694
204 Using seed = 3391229692
205 Using seed = 3391229693
201 Using seed = 3391229689
195 Using seed = 3391229683
192 Using seed = 3391229680
194 Using seed = 3391229682
198 Using seed = 3391229686
200 Using seed = 3391229688
196 Using seed = 3391229684
197 Using seed = 3391229685
199 Using seed = 3391229687
220 Using seed = 3391229708
211 Using seed = 3391229699
219 Using seed = 3391229707
222 Using seed = 3391229710
218 Using seed = 3391229706
212 Using seed = 3391229700
221 Using seed = 3391229709
208 Using seed = 3391229696
223 Using seed = 3391229711
217 Using seed = 3391229705
209 Using seed = 3391229697
215 Using seed = 3391229703
214 Using seed = 3391229702
210 Using seed = 3391229698
216 Using seed = 3391229704
213 Using seed = 3391229701
12 Using seed = 3391229500
15 Using seed = 3391229503
10 Using seed = 3391229498
14 Using seed = 3391229502
13 Using seed = 3391229501
236 Using seed = 3391229724
234 Using seed = 3391229722
230 Using seed = 3391229718
237 Using seed = 3391229725
233 Using seed = 3391229721
227 Using seed = 3391229715
225 Using seed = 3391229713
228 Using seed = 3391229716
229 Using seed = 3391229717
226 Using seed = 3391229714
224 Using seed = 3391229712
235 Using seed = 3391229723
239 Using seed = 3391229727
238 Using seed = 3391229726
232 Using seed = 3391229720
231 Using seed = 3391229719
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
:::MLL 1558639363.971 model_bn_span: {"value": 28, "metadata": {"file": "train.py", "lineno": 480}}
:::MLL 1558639363.972 global_batch_size: {"value": 1680, "metadata": {"file": "train.py", "lineno": 481}}
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
:::MLL 1558639363.985 opt_base_learning_rate: {"value": 0.1625, "metadata": {"file": "train.py", "lineno": 511}}
:::MLL 1558639363.985 opt_weight_decay: {"value": 0.0002, "metadata": {"file": "train.py", "lineno": 513}}
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
:::MLL 1558639363.985 opt_learning_rate_warmup_steps: {"value": 1250, "metadata": {"file": "train.py", "lineno": 516}}
:::MLL 1558639363.986 opt_learning_rate_warmup_factor: {"value": 0, "metadata": {"file": "train.py", "lineno": 518}}
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
:::MLL 1558639371.031 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 604}}
:::MLL 1558639371.032 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 610}}
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
time_check a: 1558639372.750322342
time_check a: 1558639372.751046181
time_check a: 1558639372.750203848
time_check a: 1558639372.753809929
time_check a: 1558639372.780100822
time_check a: 1558639372.737776756
time_check a: 1558639372.758548737
time_check a: 1558639372.760869741
time_check a: 1558639372.758434772
time_check a: 1558639372.764174938
time_check a: 1558639372.751665115
time_check a: 1558639372.757926941
time_check a: 1558639372.780037165
time_check a: 1558639372.766335011
time_check a: 1558639372.827573776
time_check b: 1558639379.468398333
time_check b: 1558639379.477718115
time_check b: 1558639379.528901577
time_check b: 1558639379.646983147
time_check b: 1558639379.665870905
time_check b: 1558639379.653054714
time_check b: 1558639379.667972088
time_check b: 1558639379.666605234
time_check b: 1558639379.675853729
time_check b: 1558639379.703881025
time_check b: 1558639379.694566250
time_check b: 1558639379.725284338
time_check b: 1558639379.729423285
time_check b: 1558639379.727744341
time_check b: 1558639379.790979147
:::MLL 1558639381.102 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 32.74606450292497, "file": "train.py", "lineno": 669}}
:::MLL 1558639381.102 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 673}}
Iteration:      0, Loss function: 22.609, Average Loss: 0.023, avg. samples / sec: 117.82
Iteration:      0, Loss function: 22.500, Average Loss: 0.022, avg. samples / sec: 118.17
Iteration:      0, Loss function: 22.473, Average Loss: 0.022, avg. samples / sec: 116.52
Iteration:      0, Loss function: 22.721, Average Loss: 0.023, avg. samples / sec: 118.00
Iteration:      0, Loss function: 22.557, Average Loss: 0.023, avg. samples / sec: 117.48
Iteration:      0, Loss function: 23.013, Average Loss: 0.023, avg. samples / sec: 117.42
Iteration:      0, Loss function: 23.357, Average Loss: 0.023, avg. samples / sec: 118.32
Iteration:      0, Loss function: 22.260, Average Loss: 0.022, avg. samples / sec: 115.13
Iteration:      0, Loss function: 22.705, Average Loss: 0.023, avg. samples / sec: 113.37
Iteration:      0, Loss function: 22.322, Average Loss: 0.022, avg. samples / sec: 110.78
Iteration:      0, Loss function: 23.395, Average Loss: 0.023, avg. samples / sec: 114.54
Iteration:      0, Loss function: 22.702, Average Loss: 0.023, avg. samples / sec: 113.09
Iteration:      0, Loss function: 22.397, Average Loss: 0.022, avg. samples / sec: 118.19
Iteration:      0, Loss function: 23.270, Average Loss: 0.023, avg. samples / sec: 117.52
Iteration:      0, Loss function: 23.215, Average Loss: 0.023, avg. samples / sec: 97.21
Iteration:     20, Loss function: 20.687, Average Loss: 0.443, avg. samples / sec: 37248.78
Iteration:     20, Loss function: 20.779, Average Loss: 0.445, avg. samples / sec: 36823.63
Iteration:     20, Loss function: 20.595, Average Loss: 0.445, avg. samples / sec: 36754.85
Iteration:     20, Loss function: 20.201, Average Loss: 0.443, avg. samples / sec: 36773.04
Iteration:     20, Loss function: 20.695, Average Loss: 0.447, avg. samples / sec: 36352.55
Iteration:     20, Loss function: 20.883, Average Loss: 0.447, avg. samples / sec: 37007.95
Iteration:     20, Loss function: 20.713, Average Loss: 0.446, avg. samples / sec: 37163.12
Iteration:     20, Loss function: 20.167, Average Loss: 0.441, avg. samples / sec: 36476.84
Iteration:     20, Loss function: 21.030, Average Loss: 0.446, avg. samples / sec: 36906.80
Iteration:     20, Loss function: 22.138, Average Loss: 0.446, avg. samples / sec: 36747.49
Iteration:     20, Loss function: 20.344, Average Loss: 0.443, avg. samples / sec: 35864.42
Iteration:     20, Loss function: 22.668, Average Loss: 0.447, avg. samples / sec: 36382.14
Iteration:     20, Loss function: 20.112, Average Loss: 0.442, avg. samples / sec: 36216.58
Iteration:     20, Loss function: 20.411, Average Loss: 0.444, avg. samples / sec: 35981.30
Iteration:     20, Loss function: 20.453, Average Loss: 0.444, avg. samples / sec: 36407.05
Iteration:     40, Loss function: 18.504, Average Loss: 0.838, avg. samples / sec: 56189.21
Iteration:     40, Loss function: 18.920, Average Loss: 0.834, avg. samples / sec: 55485.02
Iteration:     40, Loss function: 19.164, Average Loss: 0.836, avg. samples / sec: 56069.57
Iteration:     40, Loss function: 19.665, Average Loss: 0.838, avg. samples / sec: 55662.35
Iteration:     40, Loss function: 19.867, Average Loss: 0.841, avg. samples / sec: 55308.09
Iteration:     40, Loss function: 19.327, Average Loss: 0.836, avg. samples / sec: 56609.50
Iteration:     40, Loss function: 19.924, Average Loss: 0.835, avg. samples / sec: 55068.85
Iteration:     40, Loss function: 18.615, Average Loss: 0.836, avg. samples / sec: 55606.41
Iteration:     40, Loss function: 18.858, Average Loss: 0.833, avg. samples / sec: 56067.72
Iteration:     40, Loss function: 19.009, Average Loss: 0.829, avg. samples / sec: 55655.30
Iteration:     40, Loss function: 18.573, Average Loss: 0.835, avg. samples / sec: 55578.63
Iteration:     40, Loss function: 19.296, Average Loss: 0.834, avg. samples / sec: 56301.68
Iteration:     40, Loss function: 18.684, Average Loss: 0.839, avg. samples / sec: 55801.62
Iteration:     40, Loss function: 20.875, Average Loss: 0.842, avg. samples / sec: 55566.86
Iteration:     40, Loss function: 19.067, Average Loss: 0.838, avg. samples / sec: 55312.24
Iteration:     60, Loss function: 11.637, Average Loss: 1.101, avg. samples / sec: 56551.62
Iteration:     60, Loss function: 10.655, Average Loss: 1.101, avg. samples / sec: 56640.83
Iteration:     60, Loss function: 11.211, Average Loss: 1.091, avg. samples / sec: 56507.27
Iteration:     60, Loss function: 11.595, Average Loss: 1.100, avg. samples / sec: 56411.48
Iteration:     60, Loss function: 11.867, Average Loss: 1.104, avg. samples / sec: 56441.89
Iteration:     60, Loss function: 12.346, Average Loss: 1.102, avg. samples / sec: 56540.34
Iteration:     60, Loss function: 11.156, Average Loss: 1.110, avg. samples / sec: 55938.88
Iteration:     60, Loss function: 10.964, Average Loss: 1.098, avg. samples / sec: 56096.46
Iteration:     60, Loss function: 12.214, Average Loss: 1.107, avg. samples / sec: 56025.37
Iteration:     60, Loss function: 11.846, Average Loss: 1.099, avg. samples / sec: 56283.67
Iteration:     60, Loss function: 10.601, Average Loss: 1.105, avg. samples / sec: 56220.89
Iteration:     60, Loss function: 12.783, Average Loss: 1.110, avg. samples / sec: 55709.33
Iteration:     60, Loss function: 12.008, Average Loss: 1.104, avg. samples / sec: 56023.50
Iteration:     60, Loss function: 11.382, Average Loss: 1.108, avg. samples / sec: 55261.20
Iteration:     60, Loss function: 13.043, Average Loss: 1.111, avg. samples / sec: 55007.03
:::MLL 1558639384.267 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 819}}
:::MLL 1558639384.268 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 673}}
Iteration:     80, Loss function: 10.056, Average Loss: 1.290, avg. samples / sec: 57852.09
Iteration:     80, Loss function: 9.724, Average Loss: 1.295, avg. samples / sec: 58911.99
Iteration:     80, Loss function: 10.521, Average Loss: 1.297, avg. samples / sec: 57854.25
Iteration:     80, Loss function: 10.055, Average Loss: 1.291, avg. samples / sec: 58117.55
Iteration:     80, Loss function: 10.102, Average Loss: 1.297, avg. samples / sec: 58729.36
Iteration:     80, Loss function: 9.742, Average Loss: 1.281, avg. samples / sec: 57828.18
Iteration:     80, Loss function: 9.948, Average Loss: 1.287, avg. samples / sec: 57742.98
Iteration:     80, Loss function: 10.842, Average Loss: 1.279, avg. samples / sec: 57538.21
Iteration:     80, Loss function: 8.894, Average Loss: 1.288, avg. samples / sec: 57470.16
Iteration:     80, Loss function: 9.707, Average Loss: 1.294, avg. samples / sec: 57690.93
Iteration:     80, Loss function: 9.595, Average Loss: 1.295, avg. samples / sec: 57953.46
Iteration:     80, Loss function: 9.772, Average Loss: 1.285, avg. samples / sec: 57412.45
Iteration:     80, Loss function: 9.827, Average Loss: 1.292, avg. samples / sec: 57963.07
Iteration:     80, Loss function: 10.322, Average Loss: 1.284, avg. samples / sec: 57196.84
Iteration:     80, Loss function: 10.028, Average Loss: 1.295, avg. samples / sec: 57123.42
Iteration:    100, Loss function: 9.118, Average Loss: 1.470, avg. samples / sec: 58889.54
Iteration:    100, Loss function: 9.192, Average Loss: 1.467, avg. samples / sec: 58881.07
Iteration:    100, Loss function: 9.819, Average Loss: 1.463, avg. samples / sec: 59252.42
Iteration:    100, Loss function: 9.798, Average Loss: 1.458, avg. samples / sec: 58920.51
Iteration:    100, Loss function: 10.361, Average Loss: 1.453, avg. samples / sec: 58980.11
Iteration:    100, Loss function: 9.247, Average Loss: 1.461, avg. samples / sec: 58946.49
Iteration:    100, Loss function: 10.354, Average Loss: 1.473, avg. samples / sec: 58933.64
Iteration:    100, Loss function: 9.394, Average Loss: 1.458, avg. samples / sec: 59035.88
Iteration:    100, Loss function: 10.223, Average Loss: 1.473, avg. samples / sec: 58707.78
Iteration:    100, Loss function: 8.883, Average Loss: 1.469, avg. samples / sec: 58875.84
Iteration:    100, Loss function: 9.593, Average Loss: 1.470, avg. samples / sec: 58877.16
Iteration:    100, Loss function: 9.123, Average Loss: 1.471, avg. samples / sec: 59289.16
Iteration:    100, Loss function: 9.002, Average Loss: 1.457, avg. samples / sec: 58837.07
Iteration:    100, Loss function: 10.437, Average Loss: 1.469, avg. samples / sec: 58261.11
Iteration:    100, Loss function: 10.143, Average Loss: 1.465, avg. samples / sec: 57834.43
Iteration:    120, Loss function: 9.678, Average Loss: 1.606, avg. samples / sec: 59027.30
Iteration:    120, Loss function: 9.007, Average Loss: 1.617, avg. samples / sec: 59160.51
Iteration:    120, Loss function: 7.246, Average Loss: 1.601, avg. samples / sec: 59451.94
Iteration:    120, Loss function: 9.032, Average Loss: 1.613, avg. samples / sec: 58861.47
Iteration:    120, Loss function: 8.644, Average Loss: 1.625, avg. samples / sec: 59077.16
Iteration:    120, Loss function: 10.279, Average Loss: 1.619, avg. samples / sec: 59680.46
Iteration:    120, Loss function: 8.684, Average Loss: 1.627, avg. samples / sec: 58787.10
Iteration:    120, Loss function: 8.328, Average Loss: 1.625, avg. samples / sec: 58907.95
Iteration:    120, Loss function: 8.772, Average Loss: 1.623, avg. samples / sec: 58814.21
Iteration:    120, Loss function: 8.975, Average Loss: 1.621, avg. samples / sec: 58489.59
Iteration:    120, Loss function: 8.966, Average Loss: 1.613, avg. samples / sec: 58614.94
Iteration:    120, Loss function: 8.982, Average Loss: 1.620, avg. samples / sec: 59063.34
Iteration:    120, Loss function: 9.068, Average Loss: 1.613, avg. samples / sec: 58638.67
Iteration:    120, Loss function: 8.866, Average Loss: 1.616, avg. samples / sec: 58394.92
Iteration:    120, Loss function: 9.597, Average Loss: 1.606, avg. samples / sec: 58479.01
:::MLL 1558639386.277 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 819}}
:::MLL 1558639386.278 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 673}}
Iteration:    140, Loss function: 8.787, Average Loss: 1.759, avg. samples / sec: 58146.57
Iteration:    140, Loss function: 9.094, Average Loss: 1.748, avg. samples / sec: 58358.92
Iteration:    140, Loss function: 8.830, Average Loss: 1.763, avg. samples / sec: 58239.54
Iteration:    140, Loss function: 8.035, Average Loss: 1.762, avg. samples / sec: 58141.34
Iteration:    140, Loss function: 9.885, Average Loss: 1.742, avg. samples / sec: 57734.72
Iteration:    140, Loss function: 8.880, Average Loss: 1.761, avg. samples / sec: 57887.40
Iteration:    140, Loss function: 8.587, Average Loss: 1.769, avg. samples / sec: 58014.39
Iteration:    140, Loss function: 7.492, Average Loss: 1.755, avg. samples / sec: 58207.06
Iteration:    140, Loss function: 9.284, Average Loss: 1.751, avg. samples / sec: 57745.82
Iteration:    140, Loss function: 8.970, Average Loss: 1.760, avg. samples / sec: 58120.14
Iteration:    140, Loss function: 8.565, Average Loss: 1.745, avg. samples / sec: 57582.22
Iteration:    140, Loss function: 7.777, Average Loss: 1.755, avg. samples / sec: 57514.32
Iteration:    140, Loss function: 8.758, Average Loss: 1.755, avg. samples / sec: 57743.88
Iteration:    140, Loss function: 9.126, Average Loss: 1.759, avg. samples / sec: 57589.53
Iteration:    140, Loss function: 8.815, Average Loss: 1.753, avg. samples / sec: 57472.41
Iteration:    160, Loss function: 9.251, Average Loss: 1.898, avg. samples / sec: 57691.80
Iteration:    160, Loss function: 9.345, Average Loss: 1.903, avg. samples / sec: 57720.56
Iteration:    160, Loss function: 8.658, Average Loss: 1.899, avg. samples / sec: 57842.23
Iteration:    160, Loss function: 7.799, Average Loss: 1.901, avg. samples / sec: 57693.41
Iteration:    160, Loss function: 8.751, Average Loss: 1.913, avg. samples / sec: 57724.70
Iteration:    160, Loss function: 8.986, Average Loss: 1.885, avg. samples / sec: 57602.22
Iteration:    160, Loss function: 8.882, Average Loss: 1.902, avg. samples / sec: 57559.40
Iteration:    160, Loss function: 9.242, Average Loss: 1.888, avg. samples / sec: 57711.77
Iteration:    160, Loss function: 9.578, Average Loss: 1.881, avg. samples / sec: 57712.45
Iteration:    160, Loss function: 8.158, Average Loss: 1.890, avg. samples / sec: 58294.92
Iteration:    160, Loss function: 9.656, Average Loss: 1.893, avg. samples / sec: 57599.44
Iteration:    160, Loss function: 8.767, Average Loss: 1.889, avg. samples / sec: 57889.35
Iteration:    160, Loss function: 7.579, Average Loss: 1.891, avg. samples / sec: 57663.12
Iteration:    160, Loss function: 9.286, Average Loss: 1.884, avg. samples / sec: 57350.02
Iteration:    160, Loss function: 8.705, Average Loss: 1.889, avg. samples / sec: 57804.35
Iteration:    180, Loss function: 8.174, Average Loss: 2.034, avg. samples / sec: 59053.14
Iteration:    180, Loss function: 8.056, Average Loss: 2.023, avg. samples / sec: 59490.53
Iteration:    180, Loss function: 8.411, Average Loss: 2.031, avg. samples / sec: 59052.23
Iteration:    180, Loss function: 8.834, Average Loss: 2.025, avg. samples / sec: 59160.86
Iteration:    180, Loss function: 9.982, Average Loss: 2.055, avg. samples / sec: 59064.85
Iteration:    180, Loss function: 8.680, Average Loss: 2.024, avg. samples / sec: 59231.95
Iteration:    180, Loss function: 9.124, Average Loss: 2.017, avg. samples / sec: 59405.72
Iteration:    180, Loss function: 8.533, Average Loss: 2.025, avg. samples / sec: 59020.89
Iteration:    180, Loss function: 8.247, Average Loss: 2.029, avg. samples / sec: 59151.30
Iteration:    180, Loss function: 8.679, Average Loss: 2.035, avg. samples / sec: 58834.66
Iteration:    180, Loss function: 8.738, Average Loss: 2.024, avg. samples / sec: 59137.20
Iteration:    180, Loss function: 9.126, Average Loss: 2.040, avg. samples / sec: 58938.75
Iteration:    180, Loss function: 8.208, Average Loss: 2.018, avg. samples / sec: 58787.42
Iteration:    180, Loss function: 8.215, Average Loss: 2.037, avg. samples / sec: 58628.70
Iteration:    180, Loss function: 9.380, Average Loss: 2.026, avg. samples / sec: 58368.10
Iteration:    200, Loss function: 8.149, Average Loss: 2.155, avg. samples / sec: 59391.60
Iteration:    200, Loss function: 8.140, Average Loss: 2.143, avg. samples / sec: 59615.46
Iteration:    200, Loss function: 7.407, Average Loss: 2.144, avg. samples / sec: 59410.03
Iteration:    200, Loss function: 8.122, Average Loss: 2.151, avg. samples / sec: 60395.63
Iteration:    200, Loss function: 7.657, Average Loss: 2.138, avg. samples / sec: 59702.94
Iteration:    200, Loss function: 8.652, Average Loss: 2.162, avg. samples / sec: 59732.02
Iteration:    200, Loss function: 8.098, Average Loss: 2.142, avg. samples / sec: 59320.36
Iteration:    200, Loss function: 8.958, Average Loss: 2.152, avg. samples / sec: 59344.66
Iteration:    200, Loss function: 6.904, Average Loss: 2.147, avg. samples / sec: 59248.36
Iteration:    200, Loss function: 7.372, Average Loss: 2.144, avg. samples / sec: 59190.62
Iteration:    200, Loss function: 7.816, Average Loss: 2.163, avg. samples / sec: 59418.57
Iteration:    200, Loss function: 8.463, Average Loss: 2.152, avg. samples / sec: 59187.91
Iteration:    200, Loss function: 8.490, Average Loss: 2.157, avg. samples / sec: 59277.74
Iteration:    200, Loss function: 8.659, Average Loss: 2.177, avg. samples / sec: 59134.72
Iteration:    200, Loss function: 7.661, Average Loss: 2.142, avg. samples / sec: 59145.61
:::MLL 1558639388.276 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 819}}
:::MLL 1558639388.277 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 673}}
Iteration:    220, Loss function: 8.063, Average Loss: 2.261, avg. samples / sec: 56848.43
Iteration:    220, Loss function: 8.337, Average Loss: 2.264, avg. samples / sec: 57009.13
Iteration:    220, Loss function: 7.138, Average Loss: 2.284, avg. samples / sec: 56996.19
Iteration:    220, Loss function: 7.725, Average Loss: 2.271, avg. samples / sec: 56803.26
Iteration:    220, Loss function: 7.955, Average Loss: 2.260, avg. samples / sec: 57039.65
Iteration:    220, Loss function: 7.519, Average Loss: 2.267, avg. samples / sec: 56908.41
Iteration:    220, Loss function: 8.390, Average Loss: 2.261, avg. samples / sec: 56864.44
Iteration:    220, Loss function: 7.484, Average Loss: 2.271, avg. samples / sec: 56844.53
Iteration:    220, Loss function: 8.842, Average Loss: 2.278, avg. samples / sec: 56788.64
Iteration:    220, Loss function: 8.595, Average Loss: 2.271, avg. samples / sec: 56785.87
Iteration:    220, Loss function: 8.055, Average Loss: 2.262, avg. samples / sec: 56558.70
Iteration:    220, Loss function: 8.786, Average Loss: 2.301, avg. samples / sec: 56719.13
Iteration:    220, Loss function: 7.744, Average Loss: 2.272, avg. samples / sec: 56346.14
Iteration:    220, Loss function: 7.771, Average Loss: 2.270, avg. samples / sec: 56621.49
Iteration:    220, Loss function: 8.917, Average Loss: 2.259, avg. samples / sec: 56399.10
Iteration:    240, Loss function: 8.007, Average Loss: 2.393, avg. samples / sec: 59540.02
Iteration:    240, Loss function: 8.374, Average Loss: 2.382, avg. samples / sec: 59567.43
Iteration:    240, Loss function: 7.933, Average Loss: 2.387, avg. samples / sec: 59681.02
Iteration:    240, Loss function: 8.816, Average Loss: 2.375, avg. samples / sec: 59431.60
Iteration:    240, Loss function: 8.887, Average Loss: 2.385, avg. samples / sec: 59900.78
Iteration:    240, Loss function: 7.540, Average Loss: 2.384, avg. samples / sec: 59643.26
Iteration:    240, Loss function: 6.670, Average Loss: 2.379, avg. samples / sec: 59489.83
Iteration:    240, Loss function: 9.552, Average Loss: 2.377, avg. samples / sec: 59456.07
Iteration:    240, Loss function: 8.737, Average Loss: 2.382, avg. samples / sec: 59499.88
Iteration:    240, Loss function: 8.177, Average Loss: 2.372, avg. samples / sec: 59354.68
Iteration:    240, Loss function: 8.523, Average Loss: 2.384, avg. samples / sec: 59532.13
Iteration:    240, Loss function: 7.988, Average Loss: 2.386, avg. samples / sec: 59831.20
Iteration:    240, Loss function: 8.220, Average Loss: 2.372, avg. samples / sec: 59891.13
Iteration:    240, Loss function: 8.060, Average Loss: 2.375, avg. samples / sec: 59442.78
Iteration:    240, Loss function: 6.974, Average Loss: 2.408, avg. samples / sec: 59579.82
Iteration:    260, Loss function: 7.321, Average Loss: 2.484, avg. samples / sec: 57665.06
Iteration:    260, Loss function: 7.960, Average Loss: 2.519, avg. samples / sec: 57730.66
Iteration:    260, Loss function: 8.005, Average Loss: 2.481, avg. samples / sec: 57493.44
Iteration:    260, Loss function: 8.103, Average Loss: 2.490, avg. samples / sec: 57440.29
Iteration:    260, Loss function: 7.518, Average Loss: 2.493, avg. samples / sec: 57320.44
Iteration:    260, Loss function: 7.065, Average Loss: 2.497, avg. samples / sec: 57334.41
Iteration:    260, Loss function: 8.182, Average Loss: 2.498, avg. samples / sec: 57341.83
Iteration:    260, Loss function: 6.496, Average Loss: 2.482, avg. samples / sec: 57385.82
Iteration:    260, Loss function: 6.613, Average Loss: 2.494, avg. samples / sec: 57372.67
Iteration:    260, Loss function: 8.375, Average Loss: 2.504, avg. samples / sec: 57194.82
Iteration:    260, Loss function: 7.471, Average Loss: 2.496, avg. samples / sec: 57352.17
Iteration:    260, Loss function: 7.412, Average Loss: 2.488, avg. samples / sec: 57484.16
Iteration:    260, Loss function: 8.364, Average Loss: 2.498, avg. samples / sec: 57213.84
Iteration:    260, Loss function: 8.277, Average Loss: 2.486, avg. samples / sec: 57216.51
Iteration:    260, Loss function: 7.412, Average Loss: 2.496, avg. samples / sec: 57259.61
:::MLL 1558639390.303 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 819}}
:::MLL 1558639390.304 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 673}}
Iteration:    280, Loss function: 9.168, Average Loss: 2.583, avg. samples / sec: 59441.78
Iteration:    280, Loss function: 7.165, Average Loss: 2.595, avg. samples / sec: 59487.55
Iteration:    280, Loss function: 7.833, Average Loss: 2.621, avg. samples / sec: 59231.22
Iteration:    280, Loss function: 6.347, Average Loss: 2.584, avg. samples / sec: 59465.21
Iteration:    280, Loss function: 7.882, Average Loss: 2.598, avg. samples / sec: 59285.64
Iteration:    280, Loss function: 8.136, Average Loss: 2.597, avg. samples / sec: 59461.12
Iteration:    280, Loss function: 7.138, Average Loss: 2.592, avg. samples / sec: 59195.57
Iteration:    280, Loss function: 7.316, Average Loss: 2.595, avg. samples / sec: 59250.87
Iteration:    280, Loss function: 6.943, Average Loss: 2.589, avg. samples / sec: 59151.47
Iteration:    280, Loss function: 7.856, Average Loss: 2.588, avg. samples / sec: 58984.26
Iteration:    280, Loss function: 7.087, Average Loss: 2.601, avg. samples / sec: 59066.88
Iteration:    280, Loss function: 8.120, Average Loss: 2.590, avg. samples / sec: 59154.62
Iteration:    280, Loss function: 7.830, Average Loss: 2.609, avg. samples / sec: 58998.78
Iteration:    280, Loss function: 7.827, Average Loss: 2.593, avg. samples / sec: 59058.41
Iteration:    280, Loss function: 6.845, Average Loss: 2.578, avg. samples / sec: 58535.16
Iteration:    300, Loss function: 7.851, Average Loss: 2.699, avg. samples / sec: 59909.94
Iteration:    300, Loss function: 8.788, Average Loss: 2.678, avg. samples / sec: 59403.29
Iteration:    300, Loss function: 6.817, Average Loss: 2.689, avg. samples / sec: 59519.30
Iteration:    300, Loss function: 7.942, Average Loss: 2.680, avg. samples / sec: 59581.91
Iteration:    300, Loss function: 8.405, Average Loss: 2.690, avg. samples / sec: 59845.35
Iteration:    300, Loss function: 7.439, Average Loss: 2.682, avg. samples / sec: 59686.91
Iteration:    300, Loss function: 7.706, Average Loss: 2.687, avg. samples / sec: 59619.34
Iteration:    300, Loss function: 8.312, Average Loss: 2.688, avg. samples / sec: 59624.49
Iteration:    300, Loss function: 7.537, Average Loss: 2.720, avg. samples / sec: 59456.45
Iteration:    300, Loss function: 7.693, Average Loss: 2.685, avg. samples / sec: 59938.66
Iteration:    300, Loss function: 7.708, Average Loss: 2.708, avg. samples / sec: 59881.51
Iteration:    300, Loss function: 7.617, Average Loss: 2.674, avg. samples / sec: 60294.55
Iteration:    300, Loss function: 7.623, Average Loss: 2.690, avg. samples / sec: 59416.64
Iteration:    300, Loss function: 8.085, Average Loss: 2.693, avg. samples / sec: 59303.43
Iteration:    300, Loss function: 7.369, Average Loss: 2.682, avg. samples / sec: 59285.49
Iteration:    320, Loss function: 7.440, Average Loss: 2.790, avg. samples / sec: 55938.84
Iteration:    320, Loss function: 6.692, Average Loss: 2.779, avg. samples / sec: 55939.81
Iteration:    320, Loss function: 6.481, Average Loss: 2.781, avg. samples / sec: 55968.72
Iteration:    320, Loss function: 6.746, Average Loss: 2.776, avg. samples / sec: 55818.95
Iteration:    320, Loss function: 7.277, Average Loss: 2.790, avg. samples / sec: 55758.68
Iteration:    320, Loss function: 6.541, Average Loss: 2.788, avg. samples / sec: 56052.40
Iteration:    320, Loss function: 7.814, Average Loss: 2.801, avg. samples / sec: 55874.45
Iteration:    320, Loss function: 7.700, Average Loss: 2.816, avg. samples / sec: 55843.79
Iteration:    320, Loss function: 8.074, Average Loss: 2.771, avg. samples / sec: 55737.84
Iteration:    320, Loss function: 9.138, Average Loss: 2.783, avg. samples / sec: 55751.91
Iteration:    320, Loss function: 7.289, Average Loss: 2.771, avg. samples / sec: 55811.88
Iteration:    320, Loss function: 6.830, Average Loss: 2.777, avg. samples / sec: 55719.26
Iteration:    320, Loss function: 7.670, Average Loss: 2.780, avg. samples / sec: 55683.75
Iteration:    320, Loss function: 7.814, Average Loss: 2.780, avg. samples / sec: 55618.37
Iteration:    320, Loss function: 6.990, Average Loss: 2.781, avg. samples / sec: 55834.47
Iteration:    340, Loss function: 7.753, Average Loss: 2.908, avg. samples / sec: 59555.17
Iteration:    340, Loss function: 7.757, Average Loss: 2.871, avg. samples / sec: 59725.54
Iteration:    340, Loss function: 7.052, Average Loss: 2.866, avg. samples / sec: 59494.08
Iteration:    340, Loss function: 8.130, Average Loss: 2.882, avg. samples / sec: 59335.82
Iteration:    340, Loss function: 6.653, Average Loss: 2.869, avg. samples / sec: 59398.24
Iteration:    340, Loss function: 7.122, Average Loss: 2.866, avg. samples / sec: 59270.21
Iteration:    340, Loss function: 6.899, Average Loss: 2.870, avg. samples / sec: 59243.42
Iteration:    340, Loss function: 7.547, Average Loss: 2.883, avg. samples / sec: 59276.07
Iteration:    340, Loss function: 7.790, Average Loss: 2.862, avg. samples / sec: 59348.58
Iteration:    340, Loss function: 8.406, Average Loss: 2.877, avg. samples / sec: 59186.25
Iteration:    340, Loss function: 6.885, Average Loss: 2.880, avg. samples / sec: 59128.22
Iteration:    340, Loss function: 7.317, Average Loss: 2.887, avg. samples / sec: 59195.60
Iteration:    340, Loss function: 9.044, Average Loss: 2.863, avg. samples / sec: 59211.81
Iteration:    340, Loss function: 8.018, Average Loss: 2.871, avg. samples / sec: 59274.30
Iteration:    340, Loss function: 6.396, Average Loss: 2.870, avg. samples / sec: 59550.72
:::MLL 1558639392.321 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 819}}
:::MLL 1558639392.321 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 673}}
Iteration:    360, Loss function: 7.045, Average Loss: 2.952, avg. samples / sec: 59510.51
Iteration:    360, Loss function: 7.993, Average Loss: 2.972, avg. samples / sec: 59494.20
Iteration:    360, Loss function: 6.786, Average Loss: 2.993, avg. samples / sec: 59138.49
Iteration:    360, Loss function: 7.155, Average Loss: 2.956, avg. samples / sec: 59515.56
Iteration:    360, Loss function: 7.735, Average Loss: 2.957, avg. samples / sec: 59351.51
Iteration:    360, Loss function: 6.567, Average Loss: 2.943, avg. samples / sec: 59381.42
Iteration:    360, Loss function: 6.652, Average Loss: 2.955, avg. samples / sec: 59100.92
Iteration:    360, Loss function: 6.350, Average Loss: 2.963, avg. samples / sec: 59289.38
Iteration:    360, Loss function: 7.161, Average Loss: 2.956, avg. samples / sec: 59229.26
Iteration:    360, Loss function: 7.026, Average Loss: 2.958, avg. samples / sec: 59389.70
Iteration:    360, Loss function: 7.828, Average Loss: 2.970, avg. samples / sec: 59335.74
Iteration:    360, Loss function: 5.473, Average Loss: 2.958, avg. samples / sec: 59334.49
Iteration:    360, Loss function: 7.937, Average Loss: 2.951, avg. samples / sec: 59280.83
Iteration:    360, Loss function: 6.752, Average Loss: 2.969, avg. samples / sec: 59032.64
Iteration:    360, Loss function: 7.259, Average Loss: 2.946, avg. samples / sec: 59019.81
Iteration:    380, Loss function: 6.128, Average Loss: 3.031, avg. samples / sec: 58099.75
Iteration:    380, Loss function: 6.478, Average Loss: 3.031, avg. samples / sec: 57999.92
Iteration:    380, Loss function: 6.385, Average Loss: 3.026, avg. samples / sec: 58306.84
Iteration:    380, Loss function: 7.205, Average Loss: 3.071, avg. samples / sec: 57949.93
Iteration:    380, Loss function: 6.824, Average Loss: 3.029, avg. samples / sec: 58120.21
Iteration:    380, Loss function: 5.923, Average Loss: 3.026, avg. samples / sec: 57702.20
Iteration:    380, Loss function: 6.502, Average Loss: 3.050, avg. samples / sec: 57870.19
Iteration:    380, Loss function: 7.282, Average Loss: 3.035, avg. samples / sec: 57931.11
Iteration:    380, Loss function: 6.772, Average Loss: 3.042, avg. samples / sec: 57940.78
Iteration:    380, Loss function: 7.048, Average Loss: 3.047, avg. samples / sec: 58100.33
Iteration:    380, Loss function: 6.702, Average Loss: 3.049, avg. samples / sec: 57887.97
Iteration:    380, Loss function: 7.017, Average Loss: 3.015, avg. samples / sec: 57782.12
Iteration:    380, Loss function: 6.783, Average Loss: 3.033, avg. samples / sec: 57741.68
Iteration:    380, Loss function: 6.454, Average Loss: 3.039, avg. samples / sec: 57833.45
Iteration:    380, Loss function: 6.547, Average Loss: 3.043, avg. samples / sec: 57799.96
Iteration:    400, Loss function: 7.096, Average Loss: 3.124, avg. samples / sec: 57965.95
Iteration:    400, Loss function: 7.626, Average Loss: 3.101, avg. samples / sec: 57849.14
Iteration:    400, Loss function: 6.924, Average Loss: 3.107, avg. samples / sec: 57709.90
Iteration:    400, Loss function: 6.623, Average Loss: 3.125, avg. samples / sec: 57954.87
Iteration:    400, Loss function: 6.410, Average Loss: 3.109, avg. samples / sec: 57873.14
Iteration:    400, Loss function: 5.901, Average Loss: 3.099, avg. samples / sec: 57708.29
Iteration:    400, Loss function: 6.688, Average Loss: 3.086, avg. samples / sec: 57908.69
Iteration:    400, Loss function: 7.412, Average Loss: 3.112, avg. samples / sec: 57942.07
Iteration:    400, Loss function: 7.670, Average Loss: 3.123, avg. samples / sec: 57806.72
Iteration:    400, Loss function: 7.615, Average Loss: 3.103, avg. samples / sec: 57705.50
Iteration:    400, Loss function: 6.905, Average Loss: 3.118, avg. samples / sec: 57867.10
Iteration:    400, Loss function: 6.573, Average Loss: 3.148, avg. samples / sec: 57601.59
Iteration:    400, Loss function: 7.284, Average Loss: 3.106, avg. samples / sec: 57790.36
Iteration:    400, Loss function: 6.575, Average Loss: 3.113, avg. samples / sec: 57673.06
Iteration:    400, Loss function: 5.984, Average Loss: 3.105, avg. samples / sec: 57516.32
:::MLL 1558639394.332 epoch_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 819}}
:::MLL 1558639394.332 epoch_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 673}}
Iteration:    420, Loss function: 7.725, Average Loss: 3.193, avg. samples / sec: 59481.19
Iteration:    420, Loss function: 7.825, Average Loss: 3.201, avg. samples / sec: 59284.12
Iteration:    420, Loss function: 6.766, Average Loss: 3.181, avg. samples / sec: 59214.10
Iteration:    420, Loss function: 6.786, Average Loss: 3.178, avg. samples / sec: 59200.27
Iteration:    420, Loss function: 5.585, Average Loss: 3.197, avg. samples / sec: 59114.13
Iteration:    420, Loss function: 6.572, Average Loss: 3.181, avg. samples / sec: 59415.69
Iteration:    420, Loss function: 8.306, Average Loss: 3.176, avg. samples / sec: 59226.77
Iteration:    420, Loss function: 7.055, Average Loss: 3.186, avg. samples / sec: 59236.35
Iteration:    420, Loss function: 6.611, Average Loss: 3.175, avg. samples / sec: 59266.15
Iteration:    420, Loss function: 6.252, Average Loss: 3.184, avg. samples / sec: 59130.13
Iteration:    420, Loss function: 5.279, Average Loss: 3.227, avg. samples / sec: 59298.14
Iteration:    420, Loss function: 7.126, Average Loss: 3.182, avg. samples / sec: 59335.49
Iteration:    420, Loss function: 5.722, Average Loss: 3.188, avg. samples / sec: 59337.19
Iteration:    420, Loss function: 8.144, Average Loss: 3.197, avg. samples / sec: 58994.80
Iteration:    420, Loss function: 7.008, Average Loss: 3.159, avg. samples / sec: 58883.78
Iteration:    440, Loss function: 7.205, Average Loss: 3.224, avg. samples / sec: 60122.80
Iteration:    440, Loss function: 5.343, Average Loss: 3.268, avg. samples / sec: 59617.52
Iteration:    440, Loss function: 7.205, Average Loss: 3.259, avg. samples / sec: 59775.75
Iteration:    440, Loss function: 6.741, Average Loss: 3.255, avg. samples / sec: 59642.96
Iteration:    440, Loss function: 7.293, Average Loss: 3.267, avg. samples / sec: 59951.10
Iteration:    440, Loss function: 5.444, Average Loss: 3.248, avg. samples / sec: 59603.98
Iteration:    440, Loss function: 7.026, Average Loss: 3.258, avg. samples / sec: 59637.05
Iteration:    440, Loss function: 6.362, Average Loss: 3.302, avg. samples / sec: 59686.05
Iteration:    440, Loss function: 6.475, Average Loss: 3.261, avg. samples / sec: 59618.00
Iteration:    440, Loss function: 7.370, Average Loss: 3.270, avg. samples / sec: 59493.17
Iteration:    440, Loss function: 6.222, Average Loss: 3.260, avg. samples / sec: 59380.17
Iteration:    440, Loss function: 5.683, Average Loss: 3.243, avg. samples / sec: 59527.88
Iteration:    440, Loss function: 7.464, Average Loss: 3.257, avg. samples / sec: 59496.03
Iteration:    440, Loss function: 5.785, Average Loss: 3.243, avg. samples / sec: 59431.80
Iteration:    440, Loss function: 7.261, Average Loss: 3.258, avg. samples / sec: 59459.86
Iteration:    460, Loss function: 6.664, Average Loss: 3.331, avg. samples / sec: 58809.57
Iteration:    460, Loss function: 5.758, Average Loss: 3.323, avg. samples / sec: 58801.52
Iteration:    460, Loss function: 6.381, Average Loss: 3.283, avg. samples / sec: 58753.72
Iteration:    460, Loss function: 4.862, Average Loss: 3.308, avg. samples / sec: 58835.40
Iteration:    460, Loss function: 6.464, Average Loss: 3.367, avg. samples / sec: 58805.74
Iteration:    460, Loss function: 7.137, Average Loss: 3.322, avg. samples / sec: 59026.48
Iteration:    460, Loss function: 6.557, Average Loss: 3.306, avg. samples / sec: 58927.26
Iteration:    460, Loss function: 7.108, Average Loss: 3.316, avg. samples / sec: 58733.81
Iteration:    460, Loss function: 6.878, Average Loss: 3.322, avg. samples / sec: 58882.11
Iteration:    460, Loss function: 5.918, Average Loss: 3.333, avg. samples / sec: 58869.32
Iteration:    460, Loss function: 6.498, Average Loss: 3.324, avg. samples / sec: 58866.37
Iteration:    460, Loss function: 5.830, Average Loss: 3.332, avg. samples / sec: 58692.25
Iteration:    460, Loss function: 5.015, Average Loss: 3.320, avg. samples / sec: 58683.33
Iteration:    460, Loss function: 6.278, Average Loss: 3.303, avg. samples / sec: 58846.29
Iteration:    460, Loss function: 6.237, Average Loss: 3.323, avg. samples / sec: 58776.80
Iteration:    480, Loss function: 8.073, Average Loss: 3.399, avg. samples / sec: 56895.32
Iteration:    480, Loss function: 7.048, Average Loss: 3.394, avg. samples / sec: 56923.95
Iteration:    480, Loss function: 6.539, Average Loss: 3.395, avg. samples / sec: 56745.65
Iteration:    480, Loss function: 7.349, Average Loss: 3.381, avg. samples / sec: 56743.27
Iteration:    480, Loss function: 6.408, Average Loss: 3.389, avg. samples / sec: 56828.25
Iteration:    480, Loss function: 5.763, Average Loss: 3.382, avg. samples / sec: 56913.15
Iteration:    480, Loss function: 6.924, Average Loss: 3.362, avg. samples / sec: 56912.78
Iteration:    480, Loss function: 7.420, Average Loss: 3.389, avg. samples / sec: 56922.52
Iteration:    480, Loss function: 6.459, Average Loss: 3.365, avg. samples / sec: 56749.28
Iteration:    480, Loss function: 6.026, Average Loss: 3.429, avg. samples / sec: 56722.56
Iteration:    480, Loss function: 5.774, Average Loss: 3.378, avg. samples / sec: 56708.18
Iteration:    480, Loss function: 7.584, Average Loss: 3.388, avg. samples / sec: 56717.60
Iteration:    480, Loss function: 5.617, Average Loss: 3.383, avg. samples / sec: 56668.50
Iteration:    480, Loss function: 6.255, Average Loss: 3.371, avg. samples / sec: 56558.93
Iteration:    480, Loss function: 6.408, Average Loss: 3.343, avg. samples / sec: 56326.86
:::MLL 1558639396.341 epoch_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 819}}
:::MLL 1558639396.341 epoch_start: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 673}}
Iteration:    500, Loss function: 6.028, Average Loss: 3.433, avg. samples / sec: 58247.10
Iteration:    500, Loss function: 6.069, Average Loss: 3.447, avg. samples / sec: 58053.61
Iteration:    500, Loss function: 6.378, Average Loss: 3.418, avg. samples / sec: 58071.36
Iteration:    500, Loss function: 6.566, Average Loss: 3.440, avg. samples / sec: 57995.10
Iteration:    500, Loss function: 4.933, Average Loss: 3.417, avg. samples / sec: 58018.71
Iteration:    500, Loss function: 6.255, Average Loss: 3.450, avg. samples / sec: 57968.81
Iteration:    500, Loss function: 6.086, Average Loss: 3.455, avg. samples / sec: 57905.41
Iteration:    500, Loss function: 5.850, Average Loss: 3.443, avg. samples / sec: 57899.24
Iteration:    500, Loss function: 6.288, Average Loss: 3.398, avg. samples / sec: 58357.01
Iteration:    500, Loss function: 4.416, Average Loss: 3.452, avg. samples / sec: 57857.31
Iteration:    500, Loss function: 5.631, Average Loss: 3.487, avg. samples / sec: 57977.23
Iteration:    500, Loss function: 6.241, Average Loss: 3.452, avg. samples / sec: 57995.53
Iteration:    500, Loss function: 6.579, Average Loss: 3.440, avg. samples / sec: 57986.20
Iteration:    500, Loss function: 5.823, Average Loss: 3.453, avg. samples / sec: 57739.60
Iteration:    500, Loss function: 6.151, Average Loss: 3.432, avg. samples / sec: 57926.18
Iteration:    520, Loss function: 6.644, Average Loss: 3.452, avg. samples / sec: 57788.56
Iteration:    520, Loss function: 6.919, Average Loss: 3.501, avg. samples / sec: 57792.68
Iteration:    520, Loss function: 6.774, Average Loss: 3.476, avg. samples / sec: 57627.45
Iteration:    520, Loss function: 6.160, Average Loss: 3.537, avg. samples / sec: 57710.30
Iteration:    520, Loss function: 5.670, Average Loss: 3.512, avg. samples / sec: 57692.16
Iteration:    520, Loss function: 7.250, Average Loss: 3.516, avg. samples / sec: 57623.39
Iteration:    520, Loss function: 6.787, Average Loss: 3.503, avg. samples / sec: 57512.89
Iteration:    520, Loss function: 6.314, Average Loss: 3.487, avg. samples / sec: 57708.93
Iteration:    520, Loss function: 6.407, Average Loss: 3.512, avg. samples / sec: 57683.85
Iteration:    520, Loss function: 6.220, Average Loss: 3.497, avg. samples / sec: 57455.52
Iteration:    520, Loss function: 6.780, Average Loss: 3.510, avg. samples / sec: 57527.68
Iteration:    520, Loss function: 6.647, Average Loss: 3.477, avg. samples / sec: 57445.73
Iteration:    520, Loss function: 5.984, Average Loss: 3.491, avg. samples / sec: 57366.81
Iteration:    520, Loss function: 6.017, Average Loss: 3.501, avg. samples / sec: 57252.96
Iteration:    520, Loss function: 6.762, Average Loss: 3.504, avg. samples / sec: 56465.22
Iteration:    540, Loss function: 6.784, Average Loss: 3.504, avg. samples / sec: 57792.80
Iteration:    540, Loss function: 6.870, Average Loss: 3.537, avg. samples / sec: 58049.95
Iteration:    540, Loss function: 6.376, Average Loss: 3.530, avg. samples / sec: 57832.72
Iteration:    540, Loss function: 6.531, Average Loss: 3.551, avg. samples / sec: 58047.32
Iteration:    540, Loss function: 6.273, Average Loss: 3.573, avg. samples / sec: 57951.36
Iteration:    540, Loss function: 7.022, Average Loss: 3.552, avg. samples / sec: 57995.91
Iteration:    540, Loss function: 7.099, Average Loss: 3.567, avg. samples / sec: 57958.70
Iteration:    540, Loss function: 6.450, Average Loss: 3.543, avg. samples / sec: 57851.35
Iteration:    540, Loss function: 6.225, Average Loss: 3.565, avg. samples / sec: 59007.99
Iteration:    540, Loss function: 5.993, Average Loss: 3.554, avg. samples / sec: 57740.16
Iteration:    540, Loss function: 6.029, Average Loss: 3.567, avg. samples / sec: 57730.54
Iteration:    540, Loss function: 5.604, Average Loss: 3.560, avg. samples / sec: 57757.44
Iteration:    540, Loss function: 7.648, Average Loss: 3.574, avg. samples / sec: 57712.00
Iteration:    540, Loss function: 5.583, Average Loss: 3.549, avg. samples / sec: 58023.92
Iteration:    540, Loss function: 6.702, Average Loss: 3.587, avg. samples / sec: 57586.52
:::MLL 1558639398.371 epoch_stop: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 819}}
:::MLL 1558639398.372 epoch_start: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 673}}
Iteration:    560, Loss function: 6.576, Average Loss: 3.553, avg. samples / sec: 59057.85
Iteration:    560, Loss function: 5.589, Average Loss: 3.624, avg. samples / sec: 59281.33
Iteration:    560, Loss function: 5.568, Average Loss: 3.617, avg. samples / sec: 59164.18
Iteration:    560, Loss function: 6.568, Average Loss: 3.601, avg. samples / sec: 59152.71
Iteration:    560, Loss function: 4.805, Average Loss: 3.598, avg. samples / sec: 59315.76
Iteration:    560, Loss function: 5.248, Average Loss: 3.597, avg. samples / sec: 59024.43
Iteration:    560, Loss function: 5.821, Average Loss: 3.604, avg. samples / sec: 59196.84
Iteration:    560, Loss function: 5.003, Average Loss: 3.606, avg. samples / sec: 59035.41
Iteration:    560, Loss function: 6.077, Average Loss: 3.580, avg. samples / sec: 58997.84
Iteration:    560, Loss function: 5.384, Average Loss: 3.621, avg. samples / sec: 59072.28
Iteration:    560, Loss function: 6.598, Average Loss: 3.619, avg. samples / sec: 59154.90
Iteration:    560, Loss function: 6.192, Average Loss: 3.598, avg. samples / sec: 59053.71
Iteration:    560, Loss function: 5.646, Average Loss: 3.635, avg. samples / sec: 59143.68
Iteration:    560, Loss function: 5.405, Average Loss: 3.624, avg. samples / sec: 58864.33
Iteration:    560, Loss function: 6.836, Average Loss: 3.591, avg. samples / sec: 58366.10
Iteration:    580, Loss function: 6.474, Average Loss: 3.646, avg. samples / sec: 58423.54
Iteration:    580, Loss function: 6.995, Average Loss: 3.650, avg. samples / sec: 58329.64
Iteration:    580, Loss function: 6.171, Average Loss: 3.661, avg. samples / sec: 58377.72
Iteration:    580, Loss function: 5.603, Average Loss: 3.655, avg. samples / sec: 58339.64
Iteration:    580, Loss function: 6.387, Average Loss: 3.634, avg. samples / sec: 58335.03
Iteration:    580, Loss function: 6.125, Average Loss: 3.640, avg. samples / sec: 58936.80
Iteration:    580, Loss function: 6.280, Average Loss: 3.666, avg. samples / sec: 58226.33
Iteration:    580, Loss function: 7.156, Average Loss: 3.652, avg. samples / sec: 58254.13
Iteration:    580, Loss function: 7.005, Average Loss: 3.673, avg. samples / sec: 58395.75
Iteration:    580, Loss function: 6.136, Average Loss: 3.646, avg. samples / sec: 58203.91
Iteration:    580, Loss function: 6.016, Average Loss: 3.673, avg. samples / sec: 58115.68
Iteration:    580, Loss function: 6.696, Average Loss: 3.602, avg. samples / sec: 58103.80
Iteration:    580, Loss function: 6.694, Average Loss: 3.686, avg. samples / sec: 58268.00
Iteration:    580, Loss function: 5.152, Average Loss: 3.672, avg. samples / sec: 58094.89
Iteration:    580, Loss function: 5.849, Average Loss: 3.648, avg. samples / sec: 58005.94
Iteration:    600, Loss function: 6.134, Average Loss: 3.703, avg. samples / sec: 57110.32
Iteration:    600, Loss function: 6.019, Average Loss: 3.717, avg. samples / sec: 57061.92
Iteration:    600, Loss function: 5.811, Average Loss: 3.696, avg. samples / sec: 57381.90
Iteration:    600, Loss function: 5.854, Average Loss: 3.718, avg. samples / sec: 57142.34
Iteration:    600, Loss function: 4.782, Average Loss: 3.680, avg. samples / sec: 57006.57
Iteration:    600, Loss function: 7.383, Average Loss: 3.714, avg. samples / sec: 56934.57
Iteration:    600, Loss function: 5.726, Average Loss: 3.657, avg. samples / sec: 57078.19
Iteration:    600, Loss function: 6.065, Average Loss: 3.735, avg. samples / sec: 57136.04
Iteration:    600, Loss function: 5.944, Average Loss: 3.702, avg. samples / sec: 56897.68
Iteration:    600, Loss function: 5.089, Average Loss: 3.696, avg. samples / sec: 57004.40
Iteration:    600, Loss function: 5.740, Average Loss: 3.697, avg. samples / sec: 56858.11
Iteration:    600, Loss function: 5.278, Average Loss: 3.720, avg. samples / sec: 57128.44
Iteration:    600, Loss function: 6.000, Average Loss: 3.699, avg. samples / sec: 56797.95
Iteration:    600, Loss function: 5.797, Average Loss: 3.723, avg. samples / sec: 56929.33
Iteration:    600, Loss function: 6.193, Average Loss: 3.694, avg. samples / sec: 56788.52
Iteration:    620, Loss function: 5.960, Average Loss: 3.738, avg. samples / sec: 59291.80
Iteration:    620, Loss function: 6.227, Average Loss: 3.740, avg. samples / sec: 59400.44
Iteration:    620, Loss function: 5.258, Average Loss: 3.737, avg. samples / sec: 59379.77
Iteration:    620, Loss function: 5.579, Average Loss: 3.748, avg. samples / sec: 59287.84
Iteration:    620, Loss function: 5.585, Average Loss: 3.758, avg. samples / sec: 59211.79
Iteration:    620, Loss function: 5.609, Average Loss: 3.764, avg. samples / sec: 59382.39
Iteration:    620, Loss function: 8.025, Average Loss: 3.779, avg. samples / sec: 59275.12
Iteration:    620, Loss function: 6.372, Average Loss: 3.699, avg. samples / sec: 59254.93
Iteration:    620, Loss function: 6.593, Average Loss: 3.743, avg. samples / sec: 59265.60
Iteration:    620, Loss function: 5.554, Average Loss: 3.720, avg. samples / sec: 59140.50
Iteration:    620, Loss function: 5.273, Average Loss: 3.743, avg. samples / sec: 59079.73
Iteration:    620, Loss function: 4.742, Average Loss: 3.735, avg. samples / sec: 59338.79
Iteration:    620, Loss function: 6.133, Average Loss: 3.759, avg. samples / sec: 59021.34
Iteration:    620, Loss function: 5.894, Average Loss: 3.740, avg. samples / sec: 59152.46
Iteration:    620, Loss function: 7.093, Average Loss: 3.765, avg. samples / sec: 59122.46
:::MLL 1558639400.385 epoch_stop: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 819}}
:::MLL 1558639400.386 epoch_start: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 673}}
Iteration:    640, Loss function: 5.919, Average Loss: 3.764, avg. samples / sec: 59591.86
Iteration:    640, Loss function: 5.957, Average Loss: 3.809, avg. samples / sec: 59445.44
Iteration:    640, Loss function: 6.591, Average Loss: 3.793, avg. samples / sec: 59408.25
Iteration:    640, Loss function: 6.529, Average Loss: 3.820, avg. samples / sec: 59417.65
Iteration:    640, Loss function: 5.925, Average Loss: 3.786, avg. samples / sec: 59299.74
Iteration:    640, Loss function: 5.532, Average Loss: 3.777, avg. samples / sec: 59434.21
Iteration:    640, Loss function: 6.526, Average Loss: 3.783, avg. samples / sec: 59208.55
Iteration:    640, Loss function: 7.309, Average Loss: 3.785, avg. samples / sec: 59424.49
Iteration:    640, Loss function: 5.736, Average Loss: 3.811, avg. samples / sec: 59446.52
Iteration:    640, Loss function: 6.428, Average Loss: 3.788, avg. samples / sec: 59278.54
Iteration:    640, Loss function: 5.667, Average Loss: 3.747, avg. samples / sec: 59210.64
Iteration:    640, Loss function: 6.656, Average Loss: 3.789, avg. samples / sec: 59117.80
Iteration:    640, Loss function: 8.085, Average Loss: 3.806, avg. samples / sec: 59298.57
Iteration:    640, Loss function: 6.331, Average Loss: 3.803, avg. samples / sec: 59120.73
Iteration:    640, Loss function: 5.387, Average Loss: 3.788, avg. samples / sec: 59196.79
Iteration:    660, Loss function: 7.851, Average Loss: 3.815, avg. samples / sec: 60281.39
Iteration:    660, Loss function: 4.797, Average Loss: 3.826, avg. samples / sec: 60554.37
Iteration:    660, Loss function: 5.742, Average Loss: 3.826, avg. samples / sec: 60464.89
Iteration:    660, Loss function: 5.631, Average Loss: 3.784, avg. samples / sec: 60592.77
Iteration:    660, Loss function: 5.727, Average Loss: 3.836, avg. samples / sec: 60639.05
Iteration:    660, Loss function: 6.255, Average Loss: 3.817, avg. samples / sec: 60426.34
Iteration:    660, Loss function: 6.955, Average Loss: 3.837, avg. samples / sec: 60284.20
Iteration:    660, Loss function: 5.234, Average Loss: 3.833, avg. samples / sec: 60562.39
Iteration:    660, Loss function: 5.068, Average Loss: 3.851, avg. samples / sec: 60260.59
Iteration:    660, Loss function: 5.342, Average Loss: 3.848, avg. samples / sec: 60569.36
Iteration:    660, Loss function: 5.515, Average Loss: 3.834, avg. samples / sec: 60319.09
Iteration:    660, Loss function: 6.658, Average Loss: 3.857, avg. samples / sec: 60418.86
Iteration:    660, Loss function: 6.232, Average Loss: 3.834, avg. samples / sec: 60417.07
Iteration:    660, Loss function: 4.520, Average Loss: 3.857, avg. samples / sec: 60172.47
Iteration:    660, Loss function: 6.013, Average Loss: 3.846, avg. samples / sec: 60192.82
Iteration:    680, Loss function: 5.763, Average Loss: 3.853, avg. samples / sec: 57617.43
Iteration:    680, Loss function: 7.139, Average Loss: 3.877, avg. samples / sec: 57499.14
Iteration:    680, Loss function: 5.204, Average Loss: 3.881, avg. samples / sec: 57819.93
Iteration:    680, Loss function: 5.041, Average Loss: 3.862, avg. samples / sec: 57349.41
Iteration:    680, Loss function: 6.198, Average Loss: 3.891, avg. samples / sec: 57473.42
Iteration:    680, Loss function: 5.405, Average Loss: 3.851, avg. samples / sec: 57319.04
Iteration:    680, Loss function: 4.561, Average Loss: 3.862, avg. samples / sec: 57329.49
Iteration:    680, Loss function: 6.116, Average Loss: 3.874, avg. samples / sec: 57352.91
Iteration:    680, Loss function: 4.685, Average Loss: 3.883, avg. samples / sec: 57362.56
Iteration:    680, Loss function: 4.386, Average Loss: 3.884, avg. samples / sec: 57338.52
Iteration:    680, Loss function: 4.506, Average Loss: 3.864, avg. samples / sec: 57360.17
Iteration:    680, Loss function: 6.147, Average Loss: 3.865, avg. samples / sec: 57256.68
Iteration:    680, Loss function: 3.813, Average Loss: 3.893, avg. samples / sec: 57382.71
Iteration:    680, Loss function: 6.245, Average Loss: 3.872, avg. samples / sec: 57327.30
Iteration:    680, Loss function: 4.352, Average Loss: 3.822, avg. samples / sec: 57183.43
:::MLL 1558639402.380 epoch_stop: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 819}}
:::MLL 1558639402.380 epoch_start: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 673}}
Iteration:    700, Loss function: 5.885, Average Loss: 3.852, avg. samples / sec: 59672.35
Iteration:    700, Loss function: 5.136, Average Loss: 3.907, avg. samples / sec: 59628.45
Iteration:    700, Loss function: 5.739, Average Loss: 3.887, avg. samples / sec: 59438.40
Iteration:    700, Loss function: 5.569, Average Loss: 3.893, avg. samples / sec: 59384.40
Iteration:    700, Loss function: 5.893, Average Loss: 3.897, avg. samples / sec: 59490.08
Iteration:    700, Loss function: 4.473, Average Loss: 3.929, avg. samples / sec: 59489.30
Iteration:    700, Loss function: 6.546, Average Loss: 3.906, avg. samples / sec: 59184.73
Iteration:    700, Loss function: 5.575, Average Loss: 3.891, avg. samples / sec: 59321.10
Iteration:    700, Loss function: 5.962, Average Loss: 3.912, avg. samples / sec: 59327.55
Iteration:    700, Loss function: 5.670, Average Loss: 3.916, avg. samples / sec: 59179.69
Iteration:    700, Loss function: 5.406, Average Loss: 3.925, avg. samples / sec: 59210.07
Iteration:    700, Loss function: 5.978, Average Loss: 3.915, avg. samples / sec: 59298.07
Iteration:    700, Loss function: 6.009, Average Loss: 3.891, avg. samples / sec: 58930.39
Iteration:    700, Loss function: 6.747, Average Loss: 3.903, avg. samples / sec: 59218.13
Iteration:    700, Loss function: 6.336, Average Loss: 3.907, avg. samples / sec: 59090.43
Iteration:    720, Loss function: 5.376, Average Loss: 3.950, avg. samples / sec: 57527.00
Iteration:    720, Loss function: 5.941, Average Loss: 3.938, avg. samples / sec: 57636.64
Iteration:    720, Loss function: 5.146, Average Loss: 3.937, avg. samples / sec: 57176.19
Iteration:    720, Loss function: 5.967, Average Loss: 3.940, avg. samples / sec: 57325.34
Iteration:    720, Loss function: 5.544, Average Loss: 3.927, avg. samples / sec: 57206.31
Iteration:    720, Loss function: 5.265, Average Loss: 3.929, avg. samples / sec: 57241.63
Iteration:    720, Loss function: 6.124, Average Loss: 3.934, avg. samples / sec: 57440.01
Iteration:    720, Loss function: 5.243, Average Loss: 3.962, avg. samples / sec: 57258.73
Iteration:    720, Loss function: 6.272, Average Loss: 3.925, avg. samples / sec: 57289.13
Iteration:    720, Loss function: 5.484, Average Loss: 3.925, avg. samples / sec: 57390.10
Iteration:    720, Loss function: 6.110, Average Loss: 3.925, avg. samples / sec: 57108.33
Iteration:    720, Loss function: 4.121, Average Loss: 3.883, avg. samples / sec: 57052.45
Iteration:    720, Loss function: 5.784, Average Loss: 3.959, avg. samples / sec: 57299.98
Iteration:    720, Loss function: 3.917, Average Loss: 3.953, avg. samples / sec: 57241.21
Iteration:    720, Loss function: 4.738, Average Loss: 3.944, avg. samples / sec: 57131.73
Iteration:    740, Loss function: 5.326, Average Loss: 3.959, avg. samples / sec: 59141.57
Iteration:    740, Loss function: 4.590, Average Loss: 3.956, avg. samples / sec: 59122.11
Iteration:    740, Loss function: 5.971, Average Loss: 3.984, avg. samples / sec: 58919.21
Iteration:    740, Loss function: 4.980, Average Loss: 3.991, avg. samples / sec: 59064.83
Iteration:    740, Loss function: 4.945, Average Loss: 3.956, avg. samples / sec: 59024.08
Iteration:    740, Loss function: 6.506, Average Loss: 3.987, avg. samples / sec: 59062.10
Iteration:    740, Loss function: 5.752, Average Loss: 3.982, avg. samples / sec: 59110.58
Iteration:    740, Loss function: 6.788, Average Loss: 3.954, avg. samples / sec: 59038.13
Iteration:    740, Loss function: 4.642, Average Loss: 3.963, avg. samples / sec: 58800.49
Iteration:    740, Loss function: 6.170, Average Loss: 3.969, avg. samples / sec: 59110.21
Iteration:    740, Loss function: 6.819, Average Loss: 3.969, avg. samples / sec: 58830.96
Iteration:    740, Loss function: 6.336, Average Loss: 3.911, avg. samples / sec: 58933.64
Iteration:    740, Loss function: 4.821, Average Loss: 3.971, avg. samples / sec: 58823.32
Iteration:    740, Loss function: 4.903, Average Loss: 3.952, avg. samples / sec: 58845.06
Iteration:    740, Loss function: 6.716, Average Loss: 3.964, avg. samples / sec: 58745.79
Iteration:    760, Loss function: 6.207, Average Loss: 4.016, avg. samples / sec: 58537.04
Iteration:    760, Loss function: 4.131, Average Loss: 4.005, avg. samples / sec: 58645.19
Iteration:    760, Loss function: 5.129, Average Loss: 4.023, avg. samples / sec: 58435.19
Iteration:    760, Loss function: 5.580, Average Loss: 3.988, avg. samples / sec: 58458.92
Iteration:    760, Loss function: 5.574, Average Loss: 3.995, avg. samples / sec: 58551.97
Iteration:    760, Loss function: 5.462, Average Loss: 4.010, avg. samples / sec: 58459.11
Iteration:    760, Loss function: 4.405, Average Loss: 3.994, avg. samples / sec: 58685.88
Iteration:    760, Loss function: 6.340, Average Loss: 4.005, avg. samples / sec: 58494.20
Iteration:    760, Loss function: 6.035, Average Loss: 4.018, avg. samples / sec: 58313.45
Iteration:    760, Loss function: 6.087, Average Loss: 3.989, avg. samples / sec: 58407.82
Iteration:    760, Loss function: 5.919, Average Loss: 3.947, avg. samples / sec: 58507.53
Iteration:    760, Loss function: 4.811, Average Loss: 3.988, avg. samples / sec: 58243.32
Iteration:    760, Loss function: 5.365, Average Loss: 3.983, avg. samples / sec: 58540.12
Iteration:    760, Loss function: 5.812, Average Loss: 3.988, avg. samples / sec: 58154.39
Iteration:    760, Loss function: 5.676, Average Loss: 3.996, avg. samples / sec: 58380.77
:::MLL 1558639404.389 epoch_stop: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 819}}
:::MLL 1558639404.389 epoch_start: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 673}}
Iteration:    780, Loss function: 4.298, Average Loss: 4.007, avg. samples / sec: 60205.89
Iteration:    780, Loss function: 5.610, Average Loss: 4.029, avg. samples / sec: 60287.09
Iteration:    780, Loss function: 4.060, Average Loss: 4.015, avg. samples / sec: 60271.11
Iteration:    780, Loss function: 5.067, Average Loss: 3.970, avg. samples / sec: 60142.06
Iteration:    780, Loss function: 5.183, Average Loss: 4.009, avg. samples / sec: 60133.96
Iteration:    780, Loss function: 5.166, Average Loss: 4.037, avg. samples / sec: 60065.52
Iteration:    780, Loss function: 5.404, Average Loss: 4.043, avg. samples / sec: 59978.07
Iteration:    780, Loss function: 6.418, Average Loss: 4.025, avg. samples / sec: 60017.54
Iteration:    780, Loss function: 4.642, Average Loss: 4.043, avg. samples / sec: 60053.75
Iteration:    780, Loss function: 5.815, Average Loss: 4.028, avg. samples / sec: 59915.55
Iteration:    780, Loss function: 5.057, Average Loss: 4.020, avg. samples / sec: 59917.99
Iteration:    780, Loss function: 5.583, Average Loss: 4.028, avg. samples / sec: 59990.45
Iteration:    780, Loss function: 5.338, Average Loss: 4.053, avg. samples / sec: 59880.19
Iteration:    780, Loss function: 4.634, Average Loss: 4.023, avg. samples / sec: 59968.27
Iteration:    780, Loss function: 4.802, Average Loss: 4.012, avg. samples / sec: 59879.22
Iteration:    800, Loss function: 4.538, Average Loss: 4.064, avg. samples / sec: 60322.14
Iteration:    800, Loss function: 5.686, Average Loss: 4.041, avg. samples / sec: 60232.80
Iteration:    800, Loss function: 4.878, Average Loss: 4.047, avg. samples / sec: 60064.47
Iteration:    800, Loss function: 4.661, Average Loss: 3.999, avg. samples / sec: 59898.82
Iteration:    800, Loss function: 5.951, Average Loss: 4.069, avg. samples / sec: 59897.06
Iteration:    800, Loss function: 7.187, Average Loss: 4.041, avg. samples / sec: 59874.66
Iteration:    800, Loss function: 6.818, Average Loss: 4.081, avg. samples / sec: 60004.20
Iteration:    800, Loss function: 6.008, Average Loss: 4.051, avg. samples / sec: 59982.49
Iteration:    800, Loss function: 5.072, Average Loss: 4.059, avg. samples / sec: 59801.29
Iteration:    800, Loss function: 4.293, Average Loss: 4.045, avg. samples / sec: 59834.73
Iteration:    800, Loss function: 5.125, Average Loss: 4.072, avg. samples / sec: 59844.56
Iteration:    800, Loss function: 5.030, Average Loss: 4.036, avg. samples / sec: 59689.14
Iteration:    800, Loss function: 4.641, Average Loss: 4.065, avg. samples / sec: 59717.82
Iteration:    800, Loss function: 5.816, Average Loss: 4.045, avg. samples / sec: 59671.77
Iteration:    800, Loss function: 3.951, Average Loss: 4.048, avg. samples / sec: 59793.70
Iteration:    820, Loss function: 5.239, Average Loss: 4.071, avg. samples / sec: 59023.69
Iteration:    820, Loss function: 5.577, Average Loss: 4.096, avg. samples / sec: 59202.46
Iteration:    820, Loss function: 5.581, Average Loss: 4.025, avg. samples / sec: 59168.83
Iteration:    820, Loss function: 5.136, Average Loss: 4.099, avg. samples / sec: 59294.92
Iteration:    820, Loss function: 4.419, Average Loss: 4.090, avg. samples / sec: 59200.89
Iteration:    820, Loss function: 4.968, Average Loss: 4.089, avg. samples / sec: 59303.01
Iteration:    820, Loss function: 5.007, Average Loss: 4.071, avg. samples / sec: 59229.16
Iteration:    820, Loss function: 5.462, Average Loss: 4.074, avg. samples / sec: 59040.18
Iteration:    820, Loss function: 4.421, Average Loss: 4.091, avg. samples / sec: 58792.57
Iteration:    820, Loss function: 5.184, Average Loss: 4.062, avg. samples / sec: 59201.69
Iteration:    820, Loss function: 5.321, Average Loss: 4.070, avg. samples / sec: 59249.73
Iteration:    820, Loss function: 5.650, Average Loss: 4.108, avg. samples / sec: 59044.21
Iteration:    820, Loss function: 5.021, Average Loss: 4.080, avg. samples / sec: 59023.09
Iteration:    820, Loss function: 5.776, Average Loss: 4.070, avg. samples / sec: 58921.25
Iteration:    820, Loss function: 5.340, Average Loss: 4.070, avg. samples / sec: 59084.64
:::MLL 1558639406.370 epoch_stop: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 819}}
:::MLL 1558639406.371 epoch_start: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 673}}
Iteration:    840, Loss function: 3.945, Average Loss: 4.118, avg. samples / sec: 58985.40
Iteration:    840, Loss function: 6.038, Average Loss: 4.086, avg. samples / sec: 58920.41
Iteration:    840, Loss function: 5.097, Average Loss: 4.095, avg. samples / sec: 58988.60
Iteration:    840, Loss function: 4.480, Average Loss: 4.118, avg. samples / sec: 58867.62
Iteration:    840, Loss function: 4.970, Average Loss: 4.082, avg. samples / sec: 59015.04
Iteration:    840, Loss function: 6.037, Average Loss: 4.114, avg. samples / sec: 58974.04
Iteration:    840, Loss function: 4.835, Average Loss: 4.095, avg. samples / sec: 59200.47
Iteration:    840, Loss function: 5.056, Average Loss: 4.093, avg. samples / sec: 58919.18
Iteration:    840, Loss function: 5.036, Average Loss: 4.090, avg. samples / sec: 59108.70
Iteration:    840, Loss function: 5.816, Average Loss: 4.095, avg. samples / sec: 58954.58
Iteration:    840, Loss function: 5.452, Average Loss: 4.049, avg. samples / sec: 58812.42
Iteration:    840, Loss function: 5.067, Average Loss: 4.112, avg. samples / sec: 58846.58
Iteration:    840, Loss function: 5.797, Average Loss: 4.105, avg. samples / sec: 58953.61
Iteration:    840, Loss function: 4.173, Average Loss: 4.115, avg. samples / sec: 58717.81
Iteration:    840, Loss function: 6.450, Average Loss: 4.131, avg. samples / sec: 58523.96
Iteration:    860, Loss function: 4.421, Average Loss: 4.140, avg. samples / sec: 59821.49
Iteration:    860, Loss function: 6.091, Average Loss: 4.126, avg. samples / sec: 59781.88
Iteration:    860, Loss function: 4.433, Average Loss: 4.117, avg. samples / sec: 59694.32
Iteration:    860, Loss function: 6.647, Average Loss: 4.142, avg. samples / sec: 59917.61
Iteration:    860, Loss function: 5.866, Average Loss: 4.071, avg. samples / sec: 59754.71
Iteration:    860, Loss function: 5.953, Average Loss: 4.145, avg. samples / sec: 59652.12
Iteration:    860, Loss function: 4.996, Average Loss: 4.158, avg. samples / sec: 60173.21
Iteration:    860, Loss function: 5.804, Average Loss: 4.115, avg. samples / sec: 59588.33
Iteration:    860, Loss function: 6.003, Average Loss: 4.131, avg. samples / sec: 59771.84
Iteration:    860, Loss function: 5.714, Average Loss: 4.108, avg. samples / sec: 59533.01
Iteration:    860, Loss function: 3.819, Average Loss: 4.098, avg. samples / sec: 59587.68
Iteration:    860, Loss function: 5.229, Average Loss: 4.132, avg. samples / sec: 59669.62
Iteration:    860, Loss function: 5.329, Average Loss: 4.113, avg. samples / sec: 59553.16
Iteration:    860, Loss function: 6.014, Average Loss: 4.137, avg. samples / sec: 59399.44
Iteration:    860, Loss function: 5.719, Average Loss: 4.113, avg. samples / sec: 59299.24
Iteration:    880, Loss function: 4.296, Average Loss: 4.160, avg. samples / sec: 58677.79
Iteration:    880, Loss function: 4.542, Average Loss: 4.119, avg. samples / sec: 58778.52
Iteration:    880, Loss function: 5.309, Average Loss: 4.142, avg. samples / sec: 58702.77
Iteration:    880, Loss function: 5.253, Average Loss: 4.176, avg. samples / sec: 58661.08
Iteration:    880, Loss function: 4.361, Average Loss: 4.163, avg. samples / sec: 58452.76
Iteration:    880, Loss function: 5.957, Average Loss: 4.154, avg. samples / sec: 58589.84
Iteration:    880, Loss function: 5.736, Average Loss: 4.170, avg. samples / sec: 58485.49
Iteration:    880, Loss function: 5.481, Average Loss: 4.098, avg. samples / sec: 58456.52
Iteration:    880, Loss function: 5.596, Average Loss: 4.133, avg. samples / sec: 58848.57
Iteration:    880, Loss function: 6.321, Average Loss: 4.164, avg. samples / sec: 58615.97
Iteration:    880, Loss function: 5.407, Average Loss: 4.144, avg. samples / sec: 58394.92
Iteration:    880, Loss function: 6.366, Average Loss: 4.135, avg. samples / sec: 58490.61
Iteration:    880, Loss function: 4.769, Average Loss: 4.150, avg. samples / sec: 58437.39
Iteration:    880, Loss function: 5.400, Average Loss: 4.135, avg. samples / sec: 58340.24
Iteration:    880, Loss function: 4.135, Average Loss: 4.137, avg. samples / sec: 58432.23
Iteration:    900, Loss function: 3.744, Average Loss: 4.155, avg. samples / sec: 57357.93
Iteration:    900, Loss function: 5.860, Average Loss: 4.200, avg. samples / sec: 57202.46
Iteration:    900, Loss function: 4.704, Average Loss: 4.189, avg. samples / sec: 57226.15
Iteration:    900, Loss function: 6.694, Average Loss: 4.138, avg. samples / sec: 56962.03
Iteration:    900, Loss function: 4.901, Average Loss: 4.175, avg. samples / sec: 57142.36
Iteration:    900, Loss function: 6.113, Average Loss: 4.196, avg. samples / sec: 57027.86
Iteration:    900, Loss function: 3.816, Average Loss: 4.185, avg. samples / sec: 57026.04
Iteration:    900, Loss function: 3.589, Average Loss: 4.162, avg. samples / sec: 57149.53
Iteration:    900, Loss function: 5.698, Average Loss: 4.154, avg. samples / sec: 57145.38
Iteration:    900, Loss function: 4.153, Average Loss: 4.180, avg. samples / sec: 56900.28
Iteration:    900, Loss function: 5.066, Average Loss: 4.166, avg. samples / sec: 57185.28
Iteration:    900, Loss function: 4.889, Average Loss: 4.158, avg. samples / sec: 57147.86
Iteration:    900, Loss function: 5.506, Average Loss: 4.159, avg. samples / sec: 57259.17
Iteration:    900, Loss function: 4.867, Average Loss: 4.160, avg. samples / sec: 56843.06
Iteration:    900, Loss function: 5.791, Average Loss: 4.123, avg. samples / sec: 56999.88
:::MLL 1558639408.381 epoch_stop: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 819}}
:::MLL 1558639408.381 epoch_start: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 673}}
Iteration:    920, Loss function: 5.108, Average Loss: 4.187, avg. samples / sec: 59255.66
Iteration:    920, Loss function: 4.779, Average Loss: 4.182, avg. samples / sec: 58933.97
Iteration:    920, Loss function: 4.094, Average Loss: 4.207, avg. samples / sec: 58931.90
Iteration:    920, Loss function: 5.040, Average Loss: 4.224, avg. samples / sec: 58939.83
Iteration:    920, Loss function: 5.617, Average Loss: 4.229, avg. samples / sec: 58894.44
Iteration:    920, Loss function: 4.931, Average Loss: 4.187, avg. samples / sec: 59022.65
Iteration:    920, Loss function: 4.421, Average Loss: 4.197, avg. samples / sec: 59001.62
Iteration:    920, Loss function: 5.627, Average Loss: 4.156, avg. samples / sec: 59080.38
Iteration:    920, Loss function: 5.380, Average Loss: 4.204, avg. samples / sec: 58871.36
Iteration:    920, Loss function: 5.789, Average Loss: 4.214, avg. samples / sec: 58905.71
Iteration:    920, Loss function: 5.277, Average Loss: 4.188, avg. samples / sec: 58901.62
Iteration:    920, Loss function: 5.356, Average Loss: 4.186, avg. samples / sec: 58928.00
Iteration:    920, Loss function: 5.166, Average Loss: 4.169, avg. samples / sec: 58790.88
Iteration:    920, Loss function: 5.123, Average Loss: 4.206, avg. samples / sec: 58833.76
Iteration:    920, Loss function: 5.903, Average Loss: 4.179, avg. samples / sec: 58814.04
Iteration:    940, Loss function: 5.347, Average Loss: 4.205, avg. samples / sec: 59401.79
Iteration:    940, Loss function: 5.729, Average Loss: 4.236, avg. samples / sec: 59322.95
Iteration:    940, Loss function: 5.315, Average Loss: 4.202, avg. samples / sec: 59149.34
Iteration:    940, Loss function: 6.778, Average Loss: 4.187, avg. samples / sec: 59367.99
Iteration:    940, Loss function: 4.937, Average Loss: 4.176, avg. samples / sec: 59245.97
Iteration:    940, Loss function: 5.879, Average Loss: 4.199, avg. samples / sec: 59375.34
Iteration:    940, Loss function: 4.794, Average Loss: 4.221, avg. samples / sec: 59212.34
Iteration:    940, Loss function: 5.407, Average Loss: 4.211, avg. samples / sec: 59195.42
Iteration:    940, Loss function: 4.292, Average Loss: 4.217, avg. samples / sec: 59236.20
Iteration:    940, Loss function: 4.921, Average Loss: 4.245, avg. samples / sec: 59174.89
Iteration:    940, Loss function: 4.705, Average Loss: 4.202, avg. samples / sec: 59220.89
Iteration:    940, Loss function: 6.374, Average Loss: 4.245, avg. samples / sec: 59112.76
Iteration:    940, Loss function: 4.972, Average Loss: 4.224, avg. samples / sec: 59029.55
Iteration:    940, Loss function: 5.043, Average Loss: 4.207, avg. samples / sec: 58905.74
Iteration:    940, Loss function: 4.420, Average Loss: 4.223, avg. samples / sec: 59173.43
Iteration:    960, Loss function: 5.806, Average Loss: 4.209, avg. samples / sec: 60563.58
Iteration:    960, Loss function: 4.884, Average Loss: 4.244, avg. samples / sec: 60656.10
Iteration:    960, Loss function: 6.238, Average Loss: 4.236, avg. samples / sec: 60447.18
Iteration:    960, Loss function: 4.741, Average Loss: 4.249, avg. samples / sec: 60397.44
Iteration:    960, Loss function: 5.192, Average Loss: 4.244, avg. samples / sec: 60544.51
Iteration:    960, Loss function: 6.222, Average Loss: 4.224, avg. samples / sec: 60342.46
Iteration:    960, Loss function: 4.475, Average Loss: 4.262, avg. samples / sec: 60458.25
Iteration:    960, Loss function: 4.619, Average Loss: 4.192, avg. samples / sec: 60366.89
Iteration:    960, Loss function: 5.549, Average Loss: 4.231, avg. samples / sec: 60533.10
Iteration:    960, Loss function: 6.713, Average Loss: 4.219, avg. samples / sec: 60361.72
Iteration:    960, Loss function: 6.076, Average Loss: 4.237, avg. samples / sec: 60302.65
Iteration:    960, Loss function: 5.388, Average Loss: 4.225, avg. samples / sec: 60177.43
Iteration:    960, Loss function: 4.816, Average Loss: 4.221, avg. samples / sec: 60328.57
Iteration:    960, Loss function: 5.723, Average Loss: 4.263, avg. samples / sec: 60216.95
Iteration:    960, Loss function: 4.859, Average Loss: 4.234, avg. samples / sec: 60147.51
:::MLL 1558639410.354 epoch_stop: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 819}}
:::MLL 1558639410.354 epoch_start: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 673}}
Iteration:    980, Loss function: 5.965, Average Loss: 4.248, avg. samples / sec: 60016.10
Iteration:    980, Loss function: 5.190, Average Loss: 4.210, avg. samples / sec: 59734.68
Iteration:    980, Loss function: 4.185, Average Loss: 4.247, avg. samples / sec: 59700.56
Iteration:    980, Loss function: 4.977, Average Loss: 4.235, avg. samples / sec: 59705.17
Iteration:    980, Loss function: 5.214, Average Loss: 4.249, avg. samples / sec: 59578.03
Iteration:    980, Loss function: 4.385, Average Loss: 4.254, avg. samples / sec: 59725.31
Iteration:    980, Loss function: 5.254, Average Loss: 4.276, avg. samples / sec: 59622.42
Iteration:    980, Loss function: 4.436, Average Loss: 4.275, avg. samples / sec: 59772.07
Iteration:    980, Loss function: 5.492, Average Loss: 4.237, avg. samples / sec: 59540.93
Iteration:    980, Loss function: 6.346, Average Loss: 4.242, avg. samples / sec: 59632.94
Iteration:    980, Loss function: 5.235, Average Loss: 4.268, avg. samples / sec: 59455.47
Iteration:    980, Loss function: 5.579, Average Loss: 4.243, avg. samples / sec: 59583.90
Iteration:    980, Loss function: 4.851, Average Loss: 4.258, avg. samples / sec: 59376.01
Iteration:    980, Loss function: 5.358, Average Loss: 4.223, avg. samples / sec: 59211.71
Iteration:    980, Loss function: 4.624, Average Loss: 4.259, avg. samples / sec: 59393.36
Iteration:   1000, Loss function: 4.280, Average Loss: 4.252, avg. samples / sec: 57908.71
Iteration:   1000, Loss function: 5.609, Average Loss: 4.291, avg. samples / sec: 57867.62
Iteration:   1000, Loss function: 5.697, Average Loss: 4.289, avg. samples / sec: 57928.18
Iteration:   1000, Loss function: 5.195, Average Loss: 4.273, avg. samples / sec: 57947.45
Iteration:   1000, Loss function: 5.278, Average Loss: 4.225, avg. samples / sec: 57680.33
Iteration:   1000, Loss function: 4.780, Average Loss: 4.263, avg. samples / sec: 57735.67
Iteration:   1000, Loss function: 5.790, Average Loss: 4.277, avg. samples / sec: 57947.76
Iteration:   1000, Loss function: 4.791, Average Loss: 4.264, avg. samples / sec: 57614.96
Iteration:   1000, Loss function: 4.774, Average Loss: 4.259, avg. samples / sec: 57659.18
Iteration:   1000, Loss function: 6.758, Average Loss: 4.243, avg. samples / sec: 57899.63
Iteration:   1000, Loss function: 6.072, Average Loss: 4.292, avg. samples / sec: 57715.22
Iteration:   1000, Loss function: 4.851, Average Loss: 4.262, avg. samples / sec: 57666.36
Iteration:   1000, Loss function: 5.367, Average Loss: 4.258, avg. samples / sec: 57775.20
Iteration:   1000, Loss function: 6.946, Average Loss: 4.258, avg. samples / sec: 57762.58
Iteration:   1000, Loss function: 4.469, Average Loss: 4.245, avg. samples / sec: 57499.49
Iteration:   1020, Loss function: 5.116, Average Loss: 4.259, avg. samples / sec: 60691.41
Iteration:   1020, Loss function: 4.577, Average Loss: 4.243, avg. samples / sec: 60527.17
Iteration:   1020, Loss function: 4.678, Average Loss: 4.310, avg. samples / sec: 60462.98
Iteration:   1020, Loss function: 4.465, Average Loss: 4.282, avg. samples / sec: 60620.32
Iteration:   1020, Loss function: 4.222, Average Loss: 4.303, avg. samples / sec: 60428.16
Iteration:   1020, Loss function: 5.586, Average Loss: 4.281, avg. samples / sec: 60554.09
Iteration:   1020, Loss function: 3.864, Average Loss: 4.309, avg. samples / sec: 60500.59
Iteration:   1020, Loss function: 4.803, Average Loss: 4.274, avg. samples / sec: 60351.17
Iteration:   1020, Loss function: 4.683, Average Loss: 4.281, avg. samples / sec: 60437.90
Iteration:   1020, Loss function: 4.618, Average Loss: 4.266, avg. samples / sec: 60635.35
Iteration:   1020, Loss function: 4.772, Average Loss: 4.297, avg. samples / sec: 60387.45
Iteration:   1020, Loss function: 5.660, Average Loss: 4.279, avg. samples / sec: 60339.65
Iteration:   1020, Loss function: 5.017, Average Loss: 4.277, avg. samples / sec: 60442.07
Iteration:   1020, Loss function: 5.919, Average Loss: 4.294, avg. samples / sec: 60262.58
Iteration:   1020, Loss function: 6.033, Average Loss: 4.277, avg. samples / sec: 60230.18
Iteration:   1040, Loss function: 3.705, Average Loss: 4.263, avg. samples / sec: 58104.90
Iteration:   1040, Loss function: 6.214, Average Loss: 4.322, avg. samples / sec: 58075.23
Iteration:   1040, Loss function: 5.802, Average Loss: 4.284, avg. samples / sec: 58150.19
Iteration:   1040, Loss function: 5.144, Average Loss: 4.276, avg. samples / sec: 57924.38
Iteration:   1040, Loss function: 4.910, Average Loss: 4.295, avg. samples / sec: 58069.06
Iteration:   1040, Loss function: 3.735, Average Loss: 4.287, avg. samples / sec: 58193.94
Iteration:   1040, Loss function: 4.931, Average Loss: 4.283, avg. samples / sec: 58034.89
Iteration:   1040, Loss function: 3.202, Average Loss: 4.293, avg. samples / sec: 58228.44
Iteration:   1040, Loss function: 4.871, Average Loss: 4.316, avg. samples / sec: 57900.67
Iteration:   1040, Loss function: 5.141, Average Loss: 4.296, avg. samples / sec: 58009.97
Iteration:   1040, Loss function: 4.443, Average Loss: 4.311, avg. samples / sec: 58039.84
Iteration:   1040, Loss function: 4.644, Average Loss: 4.296, avg. samples / sec: 58060.69
Iteration:   1040, Loss function: 5.480, Average Loss: 4.329, avg. samples / sec: 57935.47
Iteration:   1040, Loss function: 5.476, Average Loss: 4.296, avg. samples / sec: 57852.42
Iteration:   1040, Loss function: 5.374, Average Loss: 4.309, avg. samples / sec: 58001.02
:::MLL 1558639412.351 epoch_stop: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 819}}
:::MLL 1558639412.351 epoch_start: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 673}}
Iteration:   1060, Loss function: 5.619, Average Loss: 4.332, avg. samples / sec: 59583.80
Iteration:   1060, Loss function: 4.643, Average Loss: 4.301, avg. samples / sec: 59473.31
Iteration:   1060, Loss function: 5.260, Average Loss: 4.286, avg. samples / sec: 59358.28
Iteration:   1060, Loss function: 3.804, Average Loss: 4.308, avg. samples / sec: 59347.21
Iteration:   1060, Loss function: 5.249, Average Loss: 4.316, avg. samples / sec: 59529.28
Iteration:   1060, Loss function: 4.938, Average Loss: 4.321, avg. samples / sec: 59591.23
Iteration:   1060, Loss function: 5.088, Average Loss: 4.293, avg. samples / sec: 59312.79
Iteration:   1060, Loss function: 3.740, Average Loss: 4.342, avg. samples / sec: 59451.81
Iteration:   1060, Loss function: 4.419, Average Loss: 4.303, avg. samples / sec: 59345.36
Iteration:   1060, Loss function: 5.052, Average Loss: 4.319, avg. samples / sec: 59316.41
Iteration:   1060, Loss function: 4.692, Average Loss: 4.333, avg. samples / sec: 59152.89
Iteration:   1060, Loss function: 5.853, Average Loss: 4.277, avg. samples / sec: 59051.78
Iteration:   1060, Loss function: 5.288, Average Loss: 4.299, avg. samples / sec: 59222.19
Iteration:   1060, Loss function: 5.944, Average Loss: 4.310, avg. samples / sec: 59260.12
Iteration:   1060, Loss function: 4.481, Average Loss: 4.302, avg. samples / sec: 59223.56
Iteration:   1080, Loss function: 4.978, Average Loss: 4.319, avg. samples / sec: 58027.81
Iteration:   1080, Loss function: 4.038, Average Loss: 4.317, avg. samples / sec: 58078.39
Iteration:   1080, Loss function: 3.155, Average Loss: 4.295, avg. samples / sec: 58031.64
Iteration:   1080, Loss function: 5.035, Average Loss: 4.323, avg. samples / sec: 58067.05
Iteration:   1080, Loss function: 5.010, Average Loss: 4.299, avg. samples / sec: 57835.16
Iteration:   1080, Loss function: 5.377, Average Loss: 4.344, avg. samples / sec: 57723.28
Iteration:   1080, Loss function: 4.783, Average Loss: 4.314, avg. samples / sec: 57725.24
Iteration:   1080, Loss function: 5.753, Average Loss: 4.330, avg. samples / sec: 57820.24
Iteration:   1080, Loss function: 4.954, Average Loss: 4.332, avg. samples / sec: 57792.11
Iteration:   1080, Loss function: 5.256, Average Loss: 4.323, avg. samples / sec: 57789.63
Iteration:   1080, Loss function: 4.583, Average Loss: 4.352, avg. samples / sec: 57819.88
Iteration:   1080, Loss function: 5.406, Average Loss: 4.316, avg. samples / sec: 57952.63
Iteration:   1080, Loss function: 4.085, Average Loss: 4.327, avg. samples / sec: 57889.59
Iteration:   1080, Loss function: 5.338, Average Loss: 4.347, avg. samples / sec: 57849.03
Iteration:   1080, Loss function: 4.695, Average Loss: 4.301, avg. samples / sec: 57684.18
Iteration:   1100, Loss function: 3.737, Average Loss: 4.306, avg. samples / sec: 56915.72
Iteration:   1100, Loss function: 5.106, Average Loss: 4.331, avg. samples / sec: 56895.87
Iteration:   1100, Loss function: 4.272, Average Loss: 4.360, avg. samples / sec: 57036.65
Iteration:   1100, Loss function: 5.372, Average Loss: 4.304, avg. samples / sec: 56866.34
Iteration:   1100, Loss function: 3.841, Average Loss: 4.338, avg. samples / sec: 56913.86
Iteration:   1100, Loss function: 5.080, Average Loss: 4.345, avg. samples / sec: 56885.62
Iteration:   1100, Loss function: 4.924, Average Loss: 4.339, avg. samples / sec: 56957.33
Iteration:   1100, Loss function: 4.511, Average Loss: 4.338, avg. samples / sec: 56836.71
Iteration:   1100, Loss function: 6.115, Average Loss: 4.359, avg. samples / sec: 56830.89
Iteration:   1100, Loss function: 4.326, Average Loss: 4.318, avg. samples / sec: 56996.05
Iteration:   1100, Loss function: 5.026, Average Loss: 4.364, avg. samples / sec: 56889.09
Iteration:   1100, Loss function: 4.665, Average Loss: 4.333, avg. samples / sec: 56838.25
Iteration:   1100, Loss function: 5.212, Average Loss: 4.338, avg. samples / sec: 56797.88
Iteration:   1100, Loss function: 4.034, Average Loss: 4.329, avg. samples / sec: 56745.06
Iteration:   1100, Loss function: 5.576, Average Loss: 4.328, avg. samples / sec: 56483.53
:::MLL 1558639414.376 epoch_stop: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 819}}
:::MLL 1558639414.376 epoch_start: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 673}}
Iteration:   1120, Loss function: 4.031, Average Loss: 4.349, avg. samples / sec: 59343.11
Iteration:   1120, Loss function: 3.866, Average Loss: 4.372, avg. samples / sec: 59345.56
Iteration:   1120, Loss function: 5.987, Average Loss: 4.343, avg. samples / sec: 59659.19
Iteration:   1120, Loss function: 5.247, Average Loss: 4.346, avg. samples / sec: 59308.17
Iteration:   1120, Loss function: 4.031, Average Loss: 4.346, avg. samples / sec: 59245.64
Iteration:   1120, Loss function: 4.392, Average Loss: 4.376, avg. samples / sec: 59250.75
Iteration:   1120, Loss function: 4.038, Average Loss: 4.327, avg. samples / sec: 59228.93
Iteration:   1120, Loss function: 6.569, Average Loss: 4.341, avg. samples / sec: 59331.02
Iteration:   1120, Loss function: 3.758, Average Loss: 4.346, avg. samples / sec: 59251.72
Iteration:   1120, Loss function: 5.103, Average Loss: 4.318, avg. samples / sec: 59088.35
Iteration:   1120, Loss function: 5.922, Average Loss: 4.358, avg. samples / sec: 59111.35
Iteration:   1120, Loss function: 4.606, Average Loss: 4.316, avg. samples / sec: 59021.51
Iteration:   1120, Loss function: 5.085, Average Loss: 4.373, avg. samples / sec: 59025.27
Iteration:   1120, Loss function: 4.127, Average Loss: 4.354, avg. samples / sec: 59032.02
Iteration:   1120, Loss function: 3.742, Average Loss: 4.345, avg. samples / sec: 58895.35
Iteration:   1140, Loss function: 3.705, Average Loss: 4.377, avg. samples / sec: 56119.22
Iteration:   1140, Loss function: 4.373, Average Loss: 4.357, avg. samples / sec: 56105.97
Iteration:   1140, Loss function: 5.271, Average Loss: 4.351, avg. samples / sec: 56108.36
Iteration:   1140, Loss function: 5.115, Average Loss: 4.385, avg. samples / sec: 56310.02
Iteration:   1140, Loss function: 5.659, Average Loss: 4.370, avg. samples / sec: 56450.86
Iteration:   1140, Loss function: 5.574, Average Loss: 4.337, avg. samples / sec: 56154.24
Iteration:   1140, Loss function: 4.453, Average Loss: 4.353, avg. samples / sec: 56430.47
Iteration:   1140, Loss function: 3.916, Average Loss: 4.357, avg. samples / sec: 56005.22
Iteration:   1140, Loss function: 5.627, Average Loss: 4.383, avg. samples / sec: 56101.82
Iteration:   1140, Loss function: 4.072, Average Loss: 4.357, avg. samples / sec: 56152.83
Iteration:   1140, Loss function: 4.275, Average Loss: 4.367, avg. samples / sec: 56178.91
Iteration:   1140, Loss function: 4.415, Average Loss: 4.360, avg. samples / sec: 55994.85
Iteration:   1140, Loss function: 5.973, Average Loss: 4.323, avg. samples / sec: 56189.93
Iteration:   1140, Loss function: 5.181, Average Loss: 4.326, avg. samples / sec: 56135.23
Iteration:   1140, Loss function: 4.733, Average Loss: 4.353, avg. samples / sec: 56028.26
Iteration:   1160, Loss function: 4.292, Average Loss: 4.362, avg. samples / sec: 57106.71
Iteration:   1160, Loss function: 4.505, Average Loss: 4.370, avg. samples / sec: 57100.78
Iteration:   1160, Loss function: 4.987, Average Loss: 4.386, avg. samples / sec: 57059.17
Iteration:   1160, Loss function: 4.893, Average Loss: 4.368, avg. samples / sec: 57196.40
Iteration:   1160, Loss function: 4.674, Average Loss: 4.377, avg. samples / sec: 57159.77
Iteration:   1160, Loss function: 4.194, Average Loss: 4.367, avg. samples / sec: 57136.53
Iteration:   1160, Loss function: 3.792, Average Loss: 4.337, avg. samples / sec: 57160.77
Iteration:   1160, Loss function: 4.925, Average Loss: 4.335, avg. samples / sec: 57189.93
Iteration:   1160, Loss function: 5.041, Average Loss: 4.394, avg. samples / sec: 57047.50
Iteration:   1160, Loss function: 4.323, Average Loss: 4.360, avg. samples / sec: 57198.12
Iteration:   1160, Loss function: 5.240, Average Loss: 4.378, avg. samples / sec: 57024.12
Iteration:   1160, Loss function: 5.382, Average Loss: 4.365, avg. samples / sec: 57058.54
Iteration:   1160, Loss function: 4.821, Average Loss: 4.365, avg. samples / sec: 57047.62
Iteration:   1160, Loss function: 5.083, Average Loss: 4.343, avg. samples / sec: 57006.48
Iteration:   1160, Loss function: 5.252, Average Loss: 4.393, avg. samples / sec: 56996.49
Iteration:   1180, Loss function: 4.232, Average Loss: 4.376, avg. samples / sec: 61176.44
Iteration:   1180, Loss function: 5.400, Average Loss: 4.384, avg. samples / sec: 61098.78
Iteration:   1180, Loss function: 5.379, Average Loss: 4.343, avg. samples / sec: 61013.10
Iteration:   1180, Loss function: 4.716, Average Loss: 4.380, avg. samples / sec: 60977.89
Iteration:   1180, Loss function: 5.804, Average Loss: 4.379, avg. samples / sec: 61011.86
Iteration:   1180, Loss function: 6.755, Average Loss: 4.351, avg. samples / sec: 61056.30
Iteration:   1180, Loss function: 4.379, Average Loss: 4.373, avg. samples / sec: 60989.81
Iteration:   1180, Loss function: 5.032, Average Loss: 4.379, avg. samples / sec: 60854.18
Iteration:   1180, Loss function: 4.376, Average Loss: 4.349, avg. samples / sec: 60931.12
Iteration:   1180, Loss function: 5.022, Average Loss: 4.366, avg. samples / sec: 60828.65
Iteration:   1180, Loss function: 5.383, Average Loss: 4.403, avg. samples / sec: 60927.30
Iteration:   1180, Loss function: 4.997, Average Loss: 4.367, avg. samples / sec: 60952.67
Iteration:   1180, Loss function: 4.855, Average Loss: 4.402, avg. samples / sec: 61024.52
Iteration:   1180, Loss function: 5.234, Average Loss: 4.395, avg. samples / sec: 60803.38
Iteration:   1180, Loss function: 4.723, Average Loss: 4.385, avg. samples / sec: 60717.32
:::MLL 1558639416.399 epoch_stop: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 819}}
:::MLL 1558639416.399 epoch_start: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 673}}
Iteration:   1200, Loss function: 5.187, Average Loss: 4.405, avg. samples / sec: 57669.87
Iteration:   1200, Loss function: 5.472, Average Loss: 4.388, avg. samples / sec: 57453.60
Iteration:   1200, Loss function: 4.547, Average Loss: 4.387, avg. samples / sec: 57398.18
Iteration:   1200, Loss function: 5.187, Average Loss: 4.411, avg. samples / sec: 57505.97
Iteration:   1200, Loss function: 4.919, Average Loss: 4.377, avg. samples / sec: 57456.95
Iteration:   1200, Loss function: 5.883, Average Loss: 4.361, avg. samples / sec: 57379.35
Iteration:   1200, Loss function: 4.314, Average Loss: 4.388, avg. samples / sec: 57293.97
Iteration:   1200, Loss function: 4.365, Average Loss: 4.357, avg. samples / sec: 57385.84
Iteration:   1200, Loss function: 4.113, Average Loss: 4.408, avg. samples / sec: 57390.52
Iteration:   1200, Loss function: 4.462, Average Loss: 4.383, avg. samples / sec: 57363.89
Iteration:   1200, Loss function: 6.444, Average Loss: 4.377, avg. samples / sec: 57371.31
Iteration:   1200, Loss function: 4.631, Average Loss: 4.386, avg. samples / sec: 57327.95
Iteration:   1200, Loss function: 5.022, Average Loss: 4.385, avg. samples / sec: 57171.76
Iteration:   1200, Loss function: 4.517, Average Loss: 4.394, avg. samples / sec: 57414.18
Iteration:   1200, Loss function: 5.456, Average Loss: 4.361, avg. samples / sec: 57132.98
Iteration:   1220, Loss function: 5.088, Average Loss: 4.388, avg. samples / sec: 57560.44
Iteration:   1220, Loss function: 6.232, Average Loss: 4.390, avg. samples / sec: 57640.08
Iteration:   1220, Loss function: 5.459, Average Loss: 4.401, avg. samples / sec: 57449.29
Iteration:   1220, Loss function: 4.889, Average Loss: 4.402, avg. samples / sec: 57561.42
Iteration:   1220, Loss function: 3.833, Average Loss: 4.365, avg. samples / sec: 57535.36
Iteration:   1220, Loss function: 4.946, Average Loss: 4.371, avg. samples / sec: 57494.59
Iteration:   1220, Loss function: 4.023, Average Loss: 4.425, avg. samples / sec: 57436.81
Iteration:   1220, Loss function: 4.105, Average Loss: 4.398, avg. samples / sec: 57639.82
Iteration:   1220, Loss function: 4.540, Average Loss: 4.397, avg. samples / sec: 57385.42
Iteration:   1220, Loss function: 5.861, Average Loss: 4.395, avg. samples / sec: 57528.43
Iteration:   1220, Loss function: 5.059, Average Loss: 4.395, avg. samples / sec: 57484.13
Iteration:   1220, Loss function: 3.454, Average Loss: 4.414, avg. samples / sec: 57241.40
Iteration:   1220, Loss function: 5.312, Average Loss: 4.397, avg. samples / sec: 57483.40
Iteration:   1220, Loss function: 5.468, Average Loss: 4.421, avg. samples / sec: 57419.30
Iteration:   1220, Loss function: 5.581, Average Loss: 4.373, avg. samples / sec: 57545.54
Iteration:   1240, Loss function: 4.590, Average Loss: 4.364, avg. samples / sec: 57276.88
Iteration:   1240, Loss function: 4.668, Average Loss: 4.409, avg. samples / sec: 57215.77
Iteration:   1240, Loss function: 5.014, Average Loss: 4.398, avg. samples / sec: 57165.73
Iteration:   1240, Loss function: 5.575, Average Loss: 4.405, avg. samples / sec: 57331.64
Iteration:   1240, Loss function: 4.899, Average Loss: 4.407, avg. samples / sec: 57252.24
Iteration:   1240, Loss function: 5.274, Average Loss: 4.425, avg. samples / sec: 57331.99
Iteration:   1240, Loss function: 4.714, Average Loss: 4.434, avg. samples / sec: 57218.76
Iteration:   1240, Loss function: 4.104, Average Loss: 4.382, avg. samples / sec: 57200.93
Iteration:   1240, Loss function: 5.313, Average Loss: 4.407, avg. samples / sec: 57248.82
Iteration:   1240, Loss function: 4.114, Average Loss: 4.399, avg. samples / sec: 57088.59
Iteration:   1240, Loss function: 6.101, Average Loss: 4.424, avg. samples / sec: 57233.98
Iteration:   1240, Loss function: 4.709, Average Loss: 4.396, avg. samples / sec: 57195.75
Iteration:   1240, Loss function: 5.035, Average Loss: 4.405, avg. samples / sec: 57186.49
Iteration:   1240, Loss function: 5.855, Average Loss: 4.414, avg. samples / sec: 57049.24
Iteration:   1240, Loss function: 5.004, Average Loss: 4.384, avg. samples / sec: 57197.47
:::MLL 1558639418.444 epoch_stop: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 819}}
:::MLL 1558639418.445 epoch_start: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 673}}
Iteration:   1260, Loss function: 3.815, Average Loss: 4.414, avg. samples / sec: 59011.01
Iteration:   1260, Loss function: 5.087, Average Loss: 4.421, avg. samples / sec: 58897.07
Iteration:   1260, Loss function: 4.021, Average Loss: 4.411, avg. samples / sec: 58821.97
Iteration:   1260, Loss function: 4.613, Average Loss: 4.422, avg. samples / sec: 58891.06
Iteration:   1260, Loss function: 3.273, Average Loss: 4.410, avg. samples / sec: 58774.33
Iteration:   1260, Loss function: 6.152, Average Loss: 4.371, avg. samples / sec: 58672.41
Iteration:   1260, Loss function: 4.333, Average Loss: 4.400, avg. samples / sec: 58836.85
Iteration:   1260, Loss function: 4.915, Average Loss: 4.405, avg. samples / sec: 58675.59
Iteration:   1260, Loss function: 4.428, Average Loss: 4.441, avg. samples / sec: 58639.94
Iteration:   1260, Loss function: 4.511, Average Loss: 4.436, avg. samples / sec: 58622.06
Iteration:   1260, Loss function: 4.032, Average Loss: 4.427, avg. samples / sec: 58640.68
Iteration:   1260, Loss function: 3.785, Average Loss: 4.401, avg. samples / sec: 58738.69
Iteration:   1260, Loss function: 4.530, Average Loss: 4.420, avg. samples / sec: 58447.89
Iteration:   1260, Loss function: 4.460, Average Loss: 4.390, avg. samples / sec: 58544.74
Iteration:   1260, Loss function: 4.516, Average Loss: 4.415, avg. samples / sec: 58485.80
Iteration:   1280, Loss function: 4.332, Average Loss: 4.421, avg. samples / sec: 57673.79
Iteration:   1280, Loss function: 5.783, Average Loss: 4.414, avg. samples / sec: 57659.82
Iteration:   1280, Loss function: 4.011, Average Loss: 4.424, avg. samples / sec: 57905.33
Iteration:   1280, Loss function: 4.678, Average Loss: 4.444, avg. samples / sec: 57752.54
Iteration:   1280, Loss function: 5.227, Average Loss: 4.436, avg. samples / sec: 57777.12
Iteration:   1280, Loss function: 4.354, Average Loss: 4.372, avg. samples / sec: 57619.77
Iteration:   1280, Loss function: 5.525, Average Loss: 4.424, avg. samples / sec: 57592.17
Iteration:   1280, Loss function: 6.342, Average Loss: 4.411, avg. samples / sec: 57596.48
Iteration:   1280, Loss function: 4.097, Average Loss: 4.409, avg. samples / sec: 57794.15
Iteration:   1280, Loss function: 3.831, Average Loss: 4.417, avg. samples / sec: 57424.03
Iteration:   1280, Loss function: 4.668, Average Loss: 4.450, avg. samples / sec: 57696.53
Iteration:   1280, Loss function: 4.833, Average Loss: 4.426, avg. samples / sec: 57440.29
Iteration:   1280, Loss function: 5.270, Average Loss: 4.412, avg. samples / sec: 57564.25
Iteration:   1280, Loss function: 4.063, Average Loss: 4.418, avg. samples / sec: 57750.32
Iteration:   1280, Loss function: 5.352, Average Loss: 4.396, avg. samples / sec: 57738.06
Iteration:   1300, Loss function: 5.208, Average Loss: 4.435, avg. samples / sec: 58150.60
Iteration:   1300, Loss function: 5.090, Average Loss: 4.439, avg. samples / sec: 58100.06
Iteration:   1300, Loss function: 4.836, Average Loss: 4.414, avg. samples / sec: 58113.69
Iteration:   1300, Loss function: 5.691, Average Loss: 4.412, avg. samples / sec: 58091.51
Iteration:   1300, Loss function: 4.843, Average Loss: 4.430, avg. samples / sec: 58138.80
Iteration:   1300, Loss function: 3.838, Average Loss: 4.375, avg. samples / sec: 58061.26
Iteration:   1300, Loss function: 5.589, Average Loss: 4.425, avg. samples / sec: 57920.21
Iteration:   1300, Loss function: 5.182, Average Loss: 4.421, avg. samples / sec: 58057.12
Iteration:   1300, Loss function: 4.194, Average Loss: 4.407, avg. samples / sec: 58051.26
Iteration:   1300, Loss function: 3.958, Average Loss: 4.449, avg. samples / sec: 57928.26
Iteration:   1300, Loss function: 4.054, Average Loss: 4.418, avg. samples / sec: 58014.18
Iteration:   1300, Loss function: 3.399, Average Loss: 4.426, avg. samples / sec: 57833.29
Iteration:   1300, Loss function: 4.332, Average Loss: 4.421, avg. samples / sec: 57855.49
Iteration:   1300, Loss function: 4.381, Average Loss: 4.456, avg. samples / sec: 57942.17
Iteration:   1300, Loss function: 4.828, Average Loss: 4.422, avg. samples / sec: 57744.40
Iteration:   1320, Loss function: 4.706, Average Loss: 4.416, avg. samples / sec: 57468.61
Iteration:   1320, Loss function: 5.251, Average Loss: 4.385, avg. samples / sec: 57463.60
Iteration:   1320, Loss function: 5.135, Average Loss: 4.439, avg. samples / sec: 57475.50
Iteration:   1320, Loss function: 4.557, Average Loss: 4.422, avg. samples / sec: 57445.09
Iteration:   1320, Loss function: 3.987, Average Loss: 4.427, avg. samples / sec: 57543.33
Iteration:   1320, Loss function: 3.766, Average Loss: 4.457, avg. samples / sec: 57544.34
Iteration:   1320, Loss function: 5.243, Average Loss: 4.415, avg. samples / sec: 57528.13
Iteration:   1320, Loss function: 4.189, Average Loss: 4.446, avg. samples / sec: 57352.17
Iteration:   1320, Loss function: 4.056, Average Loss: 4.426, avg. samples / sec: 57713.02
Iteration:   1320, Loss function: 3.819, Average Loss: 4.438, avg. samples / sec: 57519.42
Iteration:   1320, Loss function: 5.228, Average Loss: 4.460, avg. samples / sec: 57511.79
Iteration:   1320, Loss function: 4.892, Average Loss: 4.425, avg. samples / sec: 57488.07
Iteration:   1320, Loss function: 5.544, Average Loss: 4.429, avg. samples / sec: 57416.05
Iteration:   1320, Loss function: 5.577, Average Loss: 4.426, avg. samples / sec: 57461.75
Iteration:   1320, Loss function: 6.098, Average Loss: 4.441, avg. samples / sec: 57185.82
:::MLL 1558639420.468 epoch_stop: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 819}}
:::MLL 1558639420.468 epoch_start: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 673}}
Iteration:   1340, Loss function: 4.552, Average Loss: 4.458, avg. samples / sec: 59618.43
Iteration:   1340, Loss function: 4.413, Average Loss: 4.442, avg. samples / sec: 59661.97
Iteration:   1340, Loss function: 4.957, Average Loss: 4.433, avg. samples / sec: 59658.08
Iteration:   1340, Loss function: 4.326, Average Loss: 4.467, avg. samples / sec: 59590.10
Iteration:   1340, Loss function: 4.518, Average Loss: 4.385, avg. samples / sec: 59453.04
Iteration:   1340, Loss function: 4.785, Average Loss: 4.442, avg. samples / sec: 59642.30
Iteration:   1340, Loss function: 4.174, Average Loss: 4.431, avg. samples / sec: 59445.01
Iteration:   1340, Loss function: 3.534, Average Loss: 4.423, avg. samples / sec: 59529.99
Iteration:   1340, Loss function: 4.245, Average Loss: 4.440, avg. samples / sec: 59366.41
Iteration:   1340, Loss function: 5.514, Average Loss: 4.452, avg. samples / sec: 59443.43
Iteration:   1340, Loss function: 4.897, Average Loss: 4.427, avg. samples / sec: 59361.51
Iteration:   1340, Loss function: 4.190, Average Loss: 4.427, avg. samples / sec: 59501.84
Iteration:   1340, Loss function: 4.114, Average Loss: 4.431, avg. samples / sec: 59394.71
Iteration:   1340, Loss function: 4.608, Average Loss: 4.420, avg. samples / sec: 59334.29
Iteration:   1340, Loss function: 5.862, Average Loss: 4.423, avg. samples / sec: 59119.16
Iteration:   1360, Loss function: 4.285, Average Loss: 4.436, avg. samples / sec: 60202.36
Iteration:   1360, Loss function: 4.412, Average Loss: 4.427, avg. samples / sec: 60335.77
Iteration:   1360, Loss function: 4.710, Average Loss: 4.392, avg. samples / sec: 60132.13
Iteration:   1360, Loss function: 5.463, Average Loss: 4.432, avg. samples / sec: 60406.94
Iteration:   1360, Loss function: 4.607, Average Loss: 4.462, avg. samples / sec: 59984.07
Iteration:   1360, Loss function: 5.111, Average Loss: 4.435, avg. samples / sec: 60245.21
Iteration:   1360, Loss function: 4.420, Average Loss: 4.452, avg. samples / sec: 60155.62
Iteration:   1360, Loss function: 5.161, Average Loss: 4.433, avg. samples / sec: 60152.87
Iteration:   1360, Loss function: 4.439, Average Loss: 4.451, avg. samples / sec: 60068.03
Iteration:   1360, Loss function: 3.921, Average Loss: 4.447, avg. samples / sec: 59886.98
Iteration:   1360, Loss function: 4.379, Average Loss: 4.470, avg. samples / sec: 59944.75
Iteration:   1360, Loss function: 6.576, Average Loss: 4.431, avg. samples / sec: 60056.97
Iteration:   1360, Loss function: 4.765, Average Loss: 4.451, avg. samples / sec: 59945.72
Iteration:   1360, Loss function: 3.648, Average Loss: 4.424, avg. samples / sec: 59946.61
Iteration:   1360, Loss function: 3.976, Average Loss: 4.436, avg. samples / sec: 59825.96
Iteration:   1380, Loss function: 4.733, Average Loss: 4.441, avg. samples / sec: 57889.92
Iteration:   1380, Loss function: 4.449, Average Loss: 4.438, avg. samples / sec: 58057.51
Iteration:   1380, Loss function: 4.514, Average Loss: 4.441, avg. samples / sec: 57889.90
Iteration:   1380, Loss function: 4.983, Average Loss: 4.427, avg. samples / sec: 57796.67
Iteration:   1380, Loss function: 4.890, Average Loss: 4.473, avg. samples / sec: 57954.06
Iteration:   1380, Loss function: 4.462, Average Loss: 4.438, avg. samples / sec: 57820.81
Iteration:   1380, Loss function: 4.286, Average Loss: 4.449, avg. samples / sec: 57982.14
Iteration:   1380, Loss function: 3.986, Average Loss: 4.436, avg. samples / sec: 57857.29
Iteration:   1380, Loss function: 4.337, Average Loss: 4.441, avg. samples / sec: 57976.99
Iteration:   1380, Loss function: 3.492, Average Loss: 4.454, avg. samples / sec: 57839.05
Iteration:   1380, Loss function: 4.517, Average Loss: 4.398, avg. samples / sec: 57682.76
Iteration:   1380, Loss function: 4.495, Average Loss: 4.452, avg. samples / sec: 57757.06
Iteration:   1380, Loss function: 4.326, Average Loss: 4.455, avg. samples / sec: 57798.61
Iteration:   1380, Loss function: 4.893, Average Loss: 4.467, avg. samples / sec: 57660.95
Iteration:   1380, Loss function: 4.482, Average Loss: 4.428, avg. samples / sec: 57784.08
:::MLL 1558639422.464 epoch_stop: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 819}}
:::MLL 1558639422.465 epoch_start: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 673}}
Iteration:   1400, Loss function: 4.191, Average Loss: 4.474, avg. samples / sec: 59265.90
Iteration:   1400, Loss function: 4.292, Average Loss: 4.442, avg. samples / sec: 59287.02
Iteration:   1400, Loss function: 5.161, Average Loss: 4.432, avg. samples / sec: 59240.51
Iteration:   1400, Loss function: 4.608, Average Loss: 4.445, avg. samples / sec: 59269.14
Iteration:   1400, Loss function: 5.687, Average Loss: 4.457, avg. samples / sec: 59288.79
Iteration:   1400, Loss function: 4.301, Average Loss: 4.462, avg. samples / sec: 59191.25
Iteration:   1400, Loss function: 5.072, Average Loss: 4.469, avg. samples / sec: 59281.95
Iteration:   1400, Loss function: 3.233, Average Loss: 4.401, avg. samples / sec: 59214.50
Iteration:   1400, Loss function: 4.946, Average Loss: 4.456, avg. samples / sec: 59111.53
Iteration:   1400, Loss function: 5.300, Average Loss: 4.443, avg. samples / sec: 59024.35
Iteration:   1400, Loss function: 5.362, Average Loss: 4.458, avg. samples / sec: 59200.02
Iteration:   1400, Loss function: 4.090, Average Loss: 4.444, avg. samples / sec: 59077.75
Iteration:   1400, Loss function: 4.987, Average Loss: 4.435, avg. samples / sec: 59279.41
Iteration:   1400, Loss function: 3.994, Average Loss: 4.439, avg. samples / sec: 58898.77
Iteration:   1400, Loss function: 4.749, Average Loss: 4.446, avg. samples / sec: 58777.71
Iteration:   1420, Loss function: 4.296, Average Loss: 4.459, avg. samples / sec: 56906.55
Iteration:   1420, Loss function: 4.466, Average Loss: 4.461, avg. samples / sec: 56800.47
Iteration:   1420, Loss function: 5.028, Average Loss: 4.442, avg. samples / sec: 56958.80
Iteration:   1420, Loss function: 4.348, Average Loss: 4.472, avg. samples / sec: 56681.79
Iteration:   1420, Loss function: 4.410, Average Loss: 4.446, avg. samples / sec: 56691.32
Iteration:   1420, Loss function: 3.605, Average Loss: 4.434, avg. samples / sec: 56685.87
Iteration:   1420, Loss function: 3.892, Average Loss: 4.458, avg. samples / sec: 56841.82
Iteration:   1420, Loss function: 4.138, Average Loss: 4.449, avg. samples / sec: 56984.30
Iteration:   1420, Loss function: 3.473, Average Loss: 4.433, avg. samples / sec: 56844.12
Iteration:   1420, Loss function: 5.055, Average Loss: 4.443, avg. samples / sec: 56648.73
Iteration:   1420, Loss function: 3.739, Average Loss: 4.467, avg. samples / sec: 56753.26
Iteration:   1420, Loss function: 4.218, Average Loss: 4.467, avg. samples / sec: 56752.21
Iteration:   1420, Loss function: 4.637, Average Loss: 4.404, avg. samples / sec: 56721.26
Iteration:   1420, Loss function: 4.211, Average Loss: 4.449, avg. samples / sec: 56697.36
Iteration:   1420, Loss function: 4.359, Average Loss: 4.444, avg. samples / sec: 56613.78
Iteration:   1440, Loss function: 4.823, Average Loss: 4.466, avg. samples / sec: 58428.75
Iteration:   1440, Loss function: 4.832, Average Loss: 4.451, avg. samples / sec: 58485.56
Iteration:   1440, Loss function: 4.156, Average Loss: 4.469, avg. samples / sec: 58492.86
Iteration:   1440, Loss function: 4.217, Average Loss: 4.453, avg. samples / sec: 58543.48
Iteration:   1440, Loss function: 4.560, Average Loss: 4.466, avg. samples / sec: 58446.07
Iteration:   1440, Loss function: 5.362, Average Loss: 4.454, avg. samples / sec: 58388.87
Iteration:   1440, Loss function: 4.603, Average Loss: 4.449, avg. samples / sec: 58350.07
Iteration:   1440, Loss function: 4.294, Average Loss: 4.415, avg. samples / sec: 58450.87
Iteration:   1440, Loss function: 5.091, Average Loss: 4.440, avg. samples / sec: 58323.24
Iteration:   1440, Loss function: 4.243, Average Loss: 4.444, avg. samples / sec: 58272.00
Iteration:   1440, Loss function: 4.588, Average Loss: 4.474, avg. samples / sec: 58274.53
Iteration:   1440, Loss function: 3.901, Average Loss: 4.432, avg. samples / sec: 58320.23
Iteration:   1440, Loss function: 4.665, Average Loss: 4.449, avg. samples / sec: 58527.55
Iteration:   1440, Loss function: 5.935, Average Loss: 4.459, avg. samples / sec: 58178.30
Iteration:   1440, Loss function: 5.677, Average Loss: 4.464, avg. samples / sec: 58213.39
Iteration:   1460, Loss function: 4.570, Average Loss: 4.462, avg. samples / sec: 57715.69
Iteration:   1460, Loss function: 4.691, Average Loss: 4.415, avg. samples / sec: 57527.50
Iteration:   1460, Loss function: 4.506, Average Loss: 4.467, avg. samples / sec: 57509.68
Iteration:   1460, Loss function: 5.420, Average Loss: 4.450, avg. samples / sec: 57448.72
Iteration:   1460, Loss function: 3.619, Average Loss: 4.444, avg. samples / sec: 57525.41
Iteration:   1460, Loss function: 4.891, Average Loss: 4.454, avg. samples / sec: 57471.19
Iteration:   1460, Loss function: 3.964, Average Loss: 4.432, avg. samples / sec: 57520.03
Iteration:   1460, Loss function: 4.464, Average Loss: 4.449, avg. samples / sec: 57466.67
Iteration:   1460, Loss function: 4.699, Average Loss: 4.466, avg. samples / sec: 57269.94
Iteration:   1460, Loss function: 3.570, Average Loss: 4.477, avg. samples / sec: 57436.29
Iteration:   1460, Loss function: 3.804, Average Loss: 4.469, avg. samples / sec: 57478.15
Iteration:   1460, Loss function: 3.912, Average Loss: 4.452, avg. samples / sec: 57308.18
Iteration:   1460, Loss function: 4.297, Average Loss: 4.467, avg. samples / sec: 57286.59
Iteration:   1460, Loss function: 4.948, Average Loss: 4.455, avg. samples / sec: 57288.68
Iteration:   1460, Loss function: 4.849, Average Loss: 4.442, avg. samples / sec: 57277.37
:::MLL 1558639424.493 epoch_stop: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 819}}
:::MLL 1558639424.493 epoch_start: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 673}}
Iteration:   1480, Loss function: 5.508, Average Loss: 4.474, avg. samples / sec: 60552.89
Iteration:   1480, Loss function: 5.183, Average Loss: 4.465, avg. samples / sec: 60249.59
Iteration:   1480, Loss function: 5.447, Average Loss: 4.458, avg. samples / sec: 60400.16
Iteration:   1480, Loss function: 4.596, Average Loss: 4.435, avg. samples / sec: 60429.27
Iteration:   1480, Loss function: 4.816, Average Loss: 4.444, avg. samples / sec: 60621.78
Iteration:   1480, Loss function: 4.775, Average Loss: 4.475, avg. samples / sec: 60527.87
Iteration:   1480, Loss function: 4.273, Average Loss: 4.453, avg. samples / sec: 60320.87
Iteration:   1480, Loss function: 5.389, Average Loss: 4.473, avg. samples / sec: 60503.26
Iteration:   1480, Loss function: 2.998, Average Loss: 4.447, avg. samples / sec: 60426.01
Iteration:   1480, Loss function: 4.052, Average Loss: 4.413, avg. samples / sec: 60269.56
Iteration:   1480, Loss function: 3.783, Average Loss: 4.440, avg. samples / sec: 60290.16
Iteration:   1480, Loss function: 4.788, Average Loss: 4.467, avg. samples / sec: 60162.12
Iteration:   1480, Loss function: 5.372, Average Loss: 4.456, avg. samples / sec: 60311.73
Iteration:   1480, Loss function: 3.133, Average Loss: 4.451, avg. samples / sec: 60346.49
Iteration:   1480, Loss function: 4.686, Average Loss: 4.481, avg. samples / sec: 60189.51
Iteration:   1500, Loss function: 4.173, Average Loss: 4.476, avg. samples / sec: 60678.16
Iteration:   1500, Loss function: 3.743, Average Loss: 4.440, avg. samples / sec: 60677.95
Iteration:   1500, Loss function: 4.041, Average Loss: 4.465, avg. samples / sec: 60547.76
Iteration:   1500, Loss function: 5.943, Average Loss: 4.451, avg. samples / sec: 60579.73
Iteration:   1500, Loss function: 3.913, Average Loss: 4.415, avg. samples / sec: 60587.69
Iteration:   1500, Loss function: 4.438, Average Loss: 4.487, avg. samples / sec: 60819.26
Iteration:   1500, Loss function: 4.085, Average Loss: 4.467, avg. samples / sec: 60685.56
Iteration:   1500, Loss function: 4.734, Average Loss: 4.452, avg. samples / sec: 60717.14
Iteration:   1500, Loss function: 5.118, Average Loss: 4.473, avg. samples / sec: 60417.72
Iteration:   1500, Loss function: 4.429, Average Loss: 4.454, avg. samples / sec: 60678.74
Iteration:   1500, Loss function: 4.213, Average Loss: 4.447, avg. samples / sec: 60428.00
Iteration:   1500, Loss function: 4.987, Average Loss: 4.459, avg. samples / sec: 60452.13
Iteration:   1500, Loss function: 6.045, Average Loss: 4.436, avg. samples / sec: 60405.42
Iteration:   1500, Loss function: 4.196, Average Loss: 4.475, avg. samples / sec: 60389.34
Iteration:   1500, Loss function: 4.404, Average Loss: 4.459, avg. samples / sec: 60327.33
Iteration:   1520, Loss function: 3.775, Average Loss: 4.415, avg. samples / sec: 57751.50
Iteration:   1520, Loss function: 3.910, Average Loss: 4.441, avg. samples / sec: 57632.87
Iteration:   1520, Loss function: 2.930, Average Loss: 4.474, avg. samples / sec: 57555.92
Iteration:   1520, Loss function: 4.783, Average Loss: 4.450, avg. samples / sec: 57632.56
Iteration:   1520, Loss function: 4.311, Average Loss: 4.452, avg. samples / sec: 57688.78
Iteration:   1520, Loss function: 4.786, Average Loss: 4.439, avg. samples / sec: 57723.92
Iteration:   1520, Loss function: 5.297, Average Loss: 4.469, avg. samples / sec: 57615.95
Iteration:   1520, Loss function: 3.842, Average Loss: 4.459, avg. samples / sec: 57762.10
Iteration:   1520, Loss function: 4.565, Average Loss: 4.486, avg. samples / sec: 57595.25
Iteration:   1520, Loss function: 3.812, Average Loss: 4.459, avg. samples / sec: 57643.62
Iteration:   1520, Loss function: 2.969, Average Loss: 4.446, avg. samples / sec: 57587.25
Iteration:   1520, Loss function: 5.276, Average Loss: 4.449, avg. samples / sec: 57637.42
Iteration:   1520, Loss function: 4.888, Average Loss: 4.474, avg. samples / sec: 57632.26
Iteration:   1520, Loss function: 3.930, Average Loss: 4.477, avg. samples / sec: 57521.74
Iteration:   1520, Loss function: 4.244, Average Loss: 4.467, avg. samples / sec: 57370.99
:::MLL 1558639426.471 epoch_stop: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 819}}
:::MLL 1558639426.472 epoch_start: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 673}}
Iteration:   1540, Loss function: 4.334, Average Loss: 4.454, avg. samples / sec: 60165.22
Iteration:   1540, Loss function: 4.063, Average Loss: 4.477, avg. samples / sec: 60243.44
Iteration:   1540, Loss function: 4.148, Average Loss: 4.489, avg. samples / sec: 60121.95
Iteration:   1540, Loss function: 3.959, Average Loss: 4.476, avg. samples / sec: 60004.04
Iteration:   1540, Loss function: 3.543, Average Loss: 4.467, avg. samples / sec: 60261.13
Iteration:   1540, Loss function: 4.132, Average Loss: 4.450, avg. samples / sec: 60122.18
Iteration:   1540, Loss function: 4.371, Average Loss: 4.420, avg. samples / sec: 59835.90
Iteration:   1540, Loss function: 4.574, Average Loss: 4.440, avg. samples / sec: 59833.20
Iteration:   1540, Loss function: 4.245, Average Loss: 4.478, avg. samples / sec: 60028.25
Iteration:   1540, Loss function: 5.438, Average Loss: 4.474, avg. samples / sec: 59916.67
Iteration:   1540, Loss function: 4.302, Average Loss: 4.438, avg. samples / sec: 59877.51
Iteration:   1540, Loss function: 4.304, Average Loss: 4.459, avg. samples / sec: 59943.61
Iteration:   1540, Loss function: 4.807, Average Loss: 4.451, avg. samples / sec: 59847.25
Iteration:   1540, Loss function: 4.822, Average Loss: 4.448, avg. samples / sec: 59903.17
Iteration:   1540, Loss function: 4.166, Average Loss: 4.457, avg. samples / sec: 59787.36
Iteration:   1560, Loss function: 3.477, Average Loss: 4.472, avg. samples / sec: 56459.73
Iteration:   1560, Loss function: 4.010, Average Loss: 4.422, avg. samples / sec: 56524.29
Iteration:   1560, Loss function: 3.887, Average Loss: 4.468, avg. samples / sec: 56445.77
Iteration:   1560, Loss function: 4.325, Average Loss: 4.486, avg. samples / sec: 56400.07
Iteration:   1560, Loss function: 4.271, Average Loss: 4.479, avg. samples / sec: 56533.18
Iteration:   1560, Loss function: 4.156, Average Loss: 4.475, avg. samples / sec: 56398.43
Iteration:   1560, Loss function: 5.222, Average Loss: 4.455, avg. samples / sec: 56540.82
Iteration:   1560, Loss function: 4.896, Average Loss: 4.452, avg. samples / sec: 56261.62
Iteration:   1560, Loss function: 3.201, Average Loss: 4.440, avg. samples / sec: 56458.89
Iteration:   1560, Loss function: 5.323, Average Loss: 4.453, avg. samples / sec: 56358.89
Iteration:   1560, Loss function: 5.127, Average Loss: 4.439, avg. samples / sec: 56490.14
Iteration:   1560, Loss function: 4.935, Average Loss: 4.450, avg. samples / sec: 56526.03
Iteration:   1560, Loss function: 3.912, Average Loss: 4.474, avg. samples / sec: 56466.81
Iteration:   1560, Loss function: 4.477, Average Loss: 4.456, avg. samples / sec: 56477.15
Iteration:   1560, Loss function: 4.599, Average Loss: 4.457, avg. samples / sec: 56497.00
Iteration:   1580, Loss function: 4.762, Average Loss: 4.476, avg. samples / sec: 59113.88
Iteration:   1580, Loss function: 5.061, Average Loss: 4.483, avg. samples / sec: 58950.01
Iteration:   1580, Loss function: 4.100, Average Loss: 4.460, avg. samples / sec: 59136.58
Iteration:   1580, Loss function: 4.700, Average Loss: 4.457, avg. samples / sec: 58984.38
Iteration:   1580, Loss function: 5.506, Average Loss: 4.446, avg. samples / sec: 58919.50
Iteration:   1580, Loss function: 4.353, Average Loss: 4.482, avg. samples / sec: 58786.49
Iteration:   1580, Loss function: 5.107, Average Loss: 4.416, avg. samples / sec: 58735.31
Iteration:   1580, Loss function: 5.476, Average Loss: 4.470, avg. samples / sec: 58678.57
Iteration:   1580, Loss function: 4.982, Average Loss: 4.457, avg. samples / sec: 58815.44
Iteration:   1580, Loss function: 3.406, Average Loss: 4.438, avg. samples / sec: 58847.19
Iteration:   1580, Loss function: 5.454, Average Loss: 4.455, avg. samples / sec: 58795.17
Iteration:   1580, Loss function: 4.292, Average Loss: 4.470, avg. samples / sec: 58715.90
Iteration:   1580, Loss function: 4.076, Average Loss: 4.451, avg. samples / sec: 58817.97
Iteration:   1580, Loss function: 4.818, Average Loss: 4.448, avg. samples / sec: 58824.52
Iteration:   1580, Loss function: 4.504, Average Loss: 4.477, avg. samples / sec: 58794.61
Iteration:   1600, Loss function: 3.538, Average Loss: 4.446, avg. samples / sec: 60434.32
Iteration:   1600, Loss function: 4.320, Average Loss: 4.438, avg. samples / sec: 60492.28
Iteration:   1600, Loss function: 4.810, Average Loss: 4.475, avg. samples / sec: 60454.23
Iteration:   1600, Loss function: 4.946, Average Loss: 4.479, avg. samples / sec: 60083.09
Iteration:   1600, Loss function: 4.745, Average Loss: 4.418, avg. samples / sec: 60387.25
Iteration:   1600, Loss function: 3.674, Average Loss: 4.455, avg. samples / sec: 60414.27
Iteration:   1600, Loss function: 3.890, Average Loss: 4.481, avg. samples / sec: 60426.27
Iteration:   1600, Loss function: 5.412, Average Loss: 4.451, avg. samples / sec: 60376.22
Iteration:   1600, Loss function: 5.109, Average Loss: 4.487, avg. samples / sec: 60302.88
Iteration:   1600, Loss function: 4.812, Average Loss: 4.456, avg. samples / sec: 60218.18
Iteration:   1600, Loss function: 5.818, Average Loss: 4.449, avg. samples / sec: 60343.08
Iteration:   1600, Loss function: 3.981, Average Loss: 4.489, avg. samples / sec: 60071.36
Iteration:   1600, Loss function: 4.823, Average Loss: 4.458, avg. samples / sec: 60204.63
Iteration:   1600, Loss function: 4.054, Average Loss: 4.470, avg. samples / sec: 60157.67
Iteration:   1600, Loss function: 4.968, Average Loss: 4.461, avg. samples / sec: 59844.43
:::MLL 1558639428.475 epoch_stop: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 819}}
:::MLL 1558639428.475 epoch_start: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 673}}
Iteration:   1620, Loss function: 4.829, Average Loss: 4.448, avg. samples / sec: 60140.55
Iteration:   1620, Loss function: 4.101, Average Loss: 4.459, avg. samples / sec: 60315.16
Iteration:   1620, Loss function: 3.766, Average Loss: 4.448, avg. samples / sec: 59925.38
Iteration:   1620, Loss function: 4.238, Average Loss: 4.479, avg. samples / sec: 59873.88
Iteration:   1620, Loss function: 4.121, Average Loss: 4.478, avg. samples / sec: 59927.19
Iteration:   1620, Loss function: 5.304, Average Loss: 4.487, avg. samples / sec: 59949.75
Iteration:   1620, Loss function: 3.863, Average Loss: 4.448, avg. samples / sec: 59864.72
Iteration:   1620, Loss function: 4.708, Average Loss: 4.445, avg. samples / sec: 59668.16
Iteration:   1620, Loss function: 4.112, Average Loss: 4.440, avg. samples / sec: 59678.04
Iteration:   1620, Loss function: 4.564, Average Loss: 4.457, avg. samples / sec: 59931.09
Iteration:   1620, Loss function: 3.778, Average Loss: 4.453, avg. samples / sec: 59802.63
Iteration:   1620, Loss function: 4.158, Average Loss: 4.487, avg. samples / sec: 59872.32
Iteration:   1620, Loss function: 5.187, Average Loss: 4.471, avg. samples / sec: 59620.58
Iteration:   1620, Loss function: 4.248, Average Loss: 4.422, avg. samples / sec: 59673.44
Iteration:   1620, Loss function: 3.452, Average Loss: 4.469, avg. samples / sec: 59854.65
Iteration:   1640, Loss function: 6.570, Average Loss: 4.483, avg. samples / sec: 60043.69
Iteration:   1640, Loss function: 4.506, Average Loss: 4.462, avg. samples / sec: 60194.03
Iteration:   1640, Loss function: 4.156, Average Loss: 4.469, avg. samples / sec: 60037.56
Iteration:   1640, Loss function: 3.648, Average Loss: 4.456, avg. samples / sec: 59737.46
Iteration:   1640, Loss function: 4.156, Average Loss: 4.447, avg. samples / sec: 59568.31
Iteration:   1640, Loss function: 4.433, Average Loss: 4.448, avg. samples / sec: 59687.39
Iteration:   1640, Loss function: 4.318, Average Loss: 4.453, avg. samples / sec: 59785.00
Iteration:   1640, Loss function: 3.903, Average Loss: 4.476, avg. samples / sec: 59667.50
Iteration:   1640, Loss function: 5.393, Average Loss: 4.427, avg. samples / sec: 59798.07
Iteration:   1640, Loss function: 5.211, Average Loss: 4.438, avg. samples / sec: 59726.52
Iteration:   1640, Loss function: 3.956, Average Loss: 4.454, avg. samples / sec: 59732.12
Iteration:   1640, Loss function: 4.878, Average Loss: 4.479, avg. samples / sec: 59601.54
Iteration:   1640, Loss function: 4.202, Average Loss: 4.441, avg. samples / sec: 59687.14
Iteration:   1640, Loss function: 4.372, Average Loss: 4.483, avg. samples / sec: 59529.96
Iteration:   1640, Loss function: 3.678, Average Loss: 4.448, avg. samples / sec: 59077.18
Iteration:   1660, Loss function: 4.168, Average Loss: 4.441, avg. samples / sec: 59263.70
Iteration:   1660, Loss function: 5.010, Average Loss: 4.444, avg. samples / sec: 59896.27
Iteration:   1660, Loss function: 4.989, Average Loss: 4.421, avg. samples / sec: 59168.68
Iteration:   1660, Loss function: 4.435, Average Loss: 4.455, avg. samples / sec: 59000.48
Iteration:   1660, Loss function: 5.306, Average Loss: 4.457, avg. samples / sec: 58809.28
Iteration:   1660, Loss function: 3.463, Average Loss: 4.475, avg. samples / sec: 58999.00
Iteration:   1660, Loss function: 5.281, Average Loss: 4.480, avg. samples / sec: 58808.03
Iteration:   1660, Loss function: 4.671, Average Loss: 4.451, avg. samples / sec: 58935.74
Iteration:   1660, Loss function: 5.816, Average Loss: 4.481, avg. samples / sec: 59099.21
Iteration:   1660, Loss function: 3.971, Average Loss: 4.448, avg. samples / sec: 58921.20
Iteration:   1660, Loss function: 3.439, Average Loss: 4.455, avg. samples / sec: 58824.65
Iteration:   1660, Loss function: 3.978, Average Loss: 4.452, avg. samples / sec: 58991.77
Iteration:   1660, Loss function: 3.141, Average Loss: 4.482, avg. samples / sec: 58965.63
Iteration:   1660, Loss function: 5.056, Average Loss: 4.438, avg. samples / sec: 58977.94
Iteration:   1660, Loss function: 2.930, Average Loss: 4.465, avg. samples / sec: 58520.65
:::MLL 1558639430.449 epoch_stop: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 819}}
:::MLL 1558639430.450 epoch_start: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 673}}
Iteration:   1680, Loss function: 3.712, Average Loss: 4.448, avg. samples / sec: 60131.31
Iteration:   1680, Loss function: 3.730, Average Loss: 4.443, avg. samples / sec: 59936.24
Iteration:   1680, Loss function: 5.850, Average Loss: 4.416, avg. samples / sec: 59926.81
Iteration:   1680, Loss function: 4.587, Average Loss: 4.471, avg. samples / sec: 60006.42
Iteration:   1680, Loss function: 5.403, Average Loss: 4.479, avg. samples / sec: 59996.84
Iteration:   1680, Loss function: 4.532, Average Loss: 4.484, avg. samples / sec: 60022.03
Iteration:   1680, Loss function: 3.648, Average Loss: 4.446, avg. samples / sec: 59948.68
Iteration:   1680, Loss function: 3.830, Average Loss: 4.448, avg. samples / sec: 59972.20
Iteration:   1680, Loss function: 4.361, Average Loss: 4.448, avg. samples / sec: 59891.33
Iteration:   1680, Loss function: 4.587, Average Loss: 4.461, avg. samples / sec: 60208.54
Iteration:   1680, Loss function: 4.120, Average Loss: 4.435, avg. samples / sec: 59689.82
Iteration:   1680, Loss function: 4.393, Average Loss: 4.438, avg. samples / sec: 59952.07
Iteration:   1680, Loss function: 4.760, Average Loss: 4.482, avg. samples / sec: 59953.32
Iteration:   1680, Loss function: 3.261, Average Loss: 4.452, avg. samples / sec: 59774.53
Iteration:   1680, Loss function: 5.183, Average Loss: 4.450, avg. samples / sec: 59710.63
Iteration:   1700, Loss function: 4.296, Average Loss: 4.478, avg. samples / sec: 59920.49
Iteration:   1700, Loss function: 3.068, Average Loss: 4.412, avg. samples / sec: 59706.33
Iteration:   1700, Loss function: 4.307, Average Loss: 4.448, avg. samples / sec: 60017.59
Iteration:   1700, Loss function: 4.006, Average Loss: 4.448, avg. samples / sec: 59761.98
Iteration:   1700, Loss function: 3.440, Average Loss: 4.471, avg. samples / sec: 59579.92
Iteration:   1700, Loss function: 4.625, Average Loss: 4.436, avg. samples / sec: 59720.85
Iteration:   1700, Loss function: 3.509, Average Loss: 4.440, avg. samples / sec: 59639.75
Iteration:   1700, Loss function: 4.240, Average Loss: 4.477, avg. samples / sec: 59529.69
Iteration:   1700, Loss function: 3.994, Average Loss: 4.430, avg. samples / sec: 59681.60
Iteration:   1700, Loss function: 3.748, Average Loss: 4.449, avg. samples / sec: 59382.82
Iteration:   1700, Loss function: 4.306, Average Loss: 4.457, avg. samples / sec: 59649.60
Iteration:   1700, Loss function: 3.734, Average Loss: 4.452, avg. samples / sec: 59682.64
Iteration:   1700, Loss function: 4.669, Average Loss: 4.445, avg. samples / sec: 59582.69
Iteration:   1700, Loss function: 4.790, Average Loss: 4.443, avg. samples / sec: 59285.69
Iteration:   1700, Loss function: 4.960, Average Loss: 4.485, avg. samples / sec: 59313.07
Iteration:   1720, Loss function: 3.026, Average Loss: 4.430, avg. samples / sec: 60240.84
Iteration:   1720, Loss function: 4.028, Average Loss: 4.477, avg. samples / sec: 59986.70
Iteration:   1720, Loss function: 5.179, Average Loss: 4.487, avg. samples / sec: 60391.31
Iteration:   1720, Loss function: 5.920, Average Loss: 4.420, avg. samples / sec: 59905.72
Iteration:   1720, Loss function: 4.967, Average Loss: 4.479, avg. samples / sec: 60043.08
Iteration:   1720, Loss function: 3.302, Average Loss: 4.442, avg. samples / sec: 60036.63
Iteration:   1720, Loss function: 3.113, Average Loss: 4.442, avg. samples / sec: 60002.54
Iteration:   1720, Loss function: 4.790, Average Loss: 4.461, avg. samples / sec: 60012.09
Iteration:   1720, Loss function: 5.571, Average Loss: 4.448, avg. samples / sec: 60031.95
Iteration:   1720, Loss function: 4.477, Average Loss: 4.436, avg. samples / sec: 59907.29
Iteration:   1720, Loss function: 4.444, Average Loss: 4.449, avg. samples / sec: 59838.46
Iteration:   1720, Loss function: 4.529, Average Loss: 4.454, avg. samples / sec: 59801.16
Iteration:   1720, Loss function: 4.375, Average Loss: 4.478, avg. samples / sec: 59765.33
Iteration:   1720, Loss function: 4.525, Average Loss: 4.455, avg. samples / sec: 59754.23
Iteration:   1720, Loss function: 3.837, Average Loss: 4.440, avg. samples / sec: 59868.84
Iteration:   1740, Loss function: 5.007, Average Loss: 4.437, avg. samples / sec: 56994.35
Iteration:   1740, Loss function: 5.441, Average Loss: 4.461, avg. samples / sec: 57077.10
Iteration:   1740, Loss function: 4.127, Average Loss: 4.453, avg. samples / sec: 57241.98
Iteration:   1740, Loss function: 4.947, Average Loss: 4.452, avg. samples / sec: 57012.98
Iteration:   1740, Loss function: 4.687, Average Loss: 4.444, avg. samples / sec: 56930.89
Iteration:   1740, Loss function: 3.983, Average Loss: 4.421, avg. samples / sec: 56864.60
Iteration:   1740, Loss function: 4.328, Average Loss: 4.481, avg. samples / sec: 56886.31
Iteration:   1740, Loss function: 5.136, Average Loss: 4.429, avg. samples / sec: 56694.40
Iteration:   1740, Loss function: 4.923, Average Loss: 4.476, avg. samples / sec: 56708.91
Iteration:   1740, Loss function: 3.501, Average Loss: 4.483, avg. samples / sec: 56750.81
Iteration:   1740, Loss function: 5.284, Average Loss: 4.455, avg. samples / sec: 56914.11
Iteration:   1740, Loss function: 4.046, Average Loss: 4.441, avg. samples / sec: 56881.61
Iteration:   1740, Loss function: 3.690, Average Loss: 4.473, avg. samples / sec: 56928.48
Iteration:   1740, Loss function: 3.540, Average Loss: 4.463, avg. samples / sec: 56727.63
Iteration:   1740, Loss function: 3.362, Average Loss: 4.437, avg. samples / sec: 56935.63
:::MLL 1558639432.440 epoch_stop: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 819}}
:::MLL 1558639432.441 epoch_start: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 673}}
Iteration:   1760, Loss function: 5.024, Average Loss: 4.477, avg. samples / sec: 57907.93
Iteration:   1760, Loss function: 3.676, Average Loss: 4.456, avg. samples / sec: 57629.47
Iteration:   1760, Loss function: 4.578, Average Loss: 4.474, avg. samples / sec: 57767.36
Iteration:   1760, Loss function: 5.162, Average Loss: 4.442, avg. samples / sec: 57796.03
Iteration:   1760, Loss function: 4.326, Average Loss: 4.454, avg. samples / sec: 57771.36
Iteration:   1760, Loss function: 4.743, Average Loss: 4.428, avg. samples / sec: 57691.00
Iteration:   1760, Loss function: 4.503, Average Loss: 4.437, avg. samples / sec: 57576.66
Iteration:   1760, Loss function: 3.959, Average Loss: 4.472, avg. samples / sec: 57675.54
Iteration:   1760, Loss function: 3.990, Average Loss: 4.425, avg. samples / sec: 57617.36
Iteration:   1760, Loss function: 5.092, Average Loss: 4.446, avg. samples / sec: 57580.78
Iteration:   1760, Loss function: 4.657, Average Loss: 4.432, avg. samples / sec: 57872.57
Iteration:   1760, Loss function: 4.792, Average Loss: 4.481, avg. samples / sec: 57580.85
Iteration:   1760, Loss function: 4.995, Average Loss: 4.451, avg. samples / sec: 57532.64
Iteration:   1760, Loss function: 4.099, Average Loss: 4.445, avg. samples / sec: 57541.47
Iteration:   1760, Loss function: 5.035, Average Loss: 4.462, avg. samples / sec: 57714.58
Iteration:   1780, Loss function: 5.148, Average Loss: 4.445, avg. samples / sec: 57083.67
Iteration:   1780, Loss function: 4.511, Average Loss: 4.463, avg. samples / sec: 57135.92
Iteration:   1780, Loss function: 3.478, Average Loss: 4.436, avg. samples / sec: 56907.70
Iteration:   1780, Loss function: 3.303, Average Loss: 4.430, avg. samples / sec: 56943.36
Iteration:   1780, Loss function: 4.565, Average Loss: 4.481, avg. samples / sec: 57002.09
Iteration:   1780, Loss function: 4.787, Average Loss: 4.473, avg. samples / sec: 56867.93
Iteration:   1780, Loss function: 4.956, Average Loss: 4.427, avg. samples / sec: 56898.30
Iteration:   1780, Loss function: 3.965, Average Loss: 4.452, avg. samples / sec: 56863.41
Iteration:   1780, Loss function: 3.588, Average Loss: 4.431, avg. samples / sec: 56947.50
Iteration:   1780, Loss function: 4.480, Average Loss: 4.421, avg. samples / sec: 56901.24
Iteration:   1780, Loss function: 5.147, Average Loss: 4.446, avg. samples / sec: 56951.99
Iteration:   1780, Loss function: 3.344, Average Loss: 4.451, avg. samples / sec: 56822.07
Iteration:   1780, Loss function: 3.215, Average Loss: 4.471, avg. samples / sec: 56866.59
Iteration:   1780, Loss function: 3.507, Average Loss: 4.480, avg. samples / sec: 56775.96
Iteration:   1780, Loss function: 3.898, Average Loss: 4.443, avg. samples / sec: 56725.37
Iteration:   1800, Loss function: 3.261, Average Loss: 4.441, avg. samples / sec: 60658.18
Iteration:   1800, Loss function: 3.831, Average Loss: 4.451, avg. samples / sec: 60764.63
Iteration:   1800, Loss function: 3.454, Average Loss: 4.442, avg. samples / sec: 60931.38
Iteration:   1800, Loss function: 4.586, Average Loss: 4.427, avg. samples / sec: 60624.21
Iteration:   1800, Loss function: 3.896, Average Loss: 4.469, avg. samples / sec: 60508.09
Iteration:   1800, Loss function: 3.360, Average Loss: 4.478, avg. samples / sec: 60585.04
Iteration:   1800, Loss function: 4.507, Average Loss: 4.442, avg. samples / sec: 60596.03
Iteration:   1800, Loss function: 4.122, Average Loss: 4.427, avg. samples / sec: 60517.94
Iteration:   1800, Loss function: 4.866, Average Loss: 4.440, avg. samples / sec: 60498.33
Iteration:   1800, Loss function: 3.079, Average Loss: 4.473, avg. samples / sec: 60589.80
Iteration:   1800, Loss function: 4.269, Average Loss: 4.414, avg. samples / sec: 60549.35
Iteration:   1800, Loss function: 4.231, Average Loss: 4.471, avg. samples / sec: 60572.98
Iteration:   1800, Loss function: 4.283, Average Loss: 4.476, avg. samples / sec: 60469.56
Iteration:   1800, Loss function: 4.504, Average Loss: 4.451, avg. samples / sec: 60493.16
Iteration:   1800, Loss function: 4.682, Average Loss: 4.425, avg. samples / sec: 60411.32
:::MLL 1558639434.456 epoch_stop: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 819}}
:::MLL 1558639434.456 epoch_start: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 673}}
Iteration:   1820, Loss function: 4.634, Average Loss: 4.449, avg. samples / sec: 59809.21
Iteration:   1820, Loss function: 4.985, Average Loss: 4.416, avg. samples / sec: 59775.92
Iteration:   1820, Loss function: 3.907, Average Loss: 4.435, avg. samples / sec: 59644.85
Iteration:   1820, Loss function: 3.093, Average Loss: 4.417, avg. samples / sec: 59805.83
Iteration:   1820, Loss function: 3.950, Average Loss: 4.437, avg. samples / sec: 59350.26
Iteration:   1820, Loss function: 4.392, Average Loss: 4.471, avg. samples / sec: 59641.49
Iteration:   1820, Loss function: 4.474, Average Loss: 4.469, avg. samples / sec: 59574.05
Iteration:   1820, Loss function: 3.646, Average Loss: 4.464, avg. samples / sec: 59584.68
Iteration:   1820, Loss function: 4.479, Average Loss: 4.439, avg. samples / sec: 59556.28
Iteration:   1820, Loss function: 3.429, Average Loss: 4.472, avg. samples / sec: 59499.58
Iteration:   1820, Loss function: 5.028, Average Loss: 4.466, avg. samples / sec: 59490.81
Iteration:   1820, Loss function: 3.659, Average Loss: 4.421, avg. samples / sec: 59471.10
Iteration:   1820, Loss function: 5.129, Average Loss: 4.449, avg. samples / sec: 59350.33
Iteration:   1820, Loss function: 5.184, Average Loss: 4.423, avg. samples / sec: 59457.50
Iteration:   1820, Loss function: 4.508, Average Loss: 4.437, avg. samples / sec: 59409.43
Iteration:   1840, Loss function: 5.334, Average Loss: 4.447, avg. samples / sec: 56675.50
Iteration:   1840, Loss function: 4.876, Average Loss: 4.443, avg. samples / sec: 56326.38
Iteration:   1840, Loss function: 3.849, Average Loss: 4.417, avg. samples / sec: 56533.15
Iteration:   1840, Loss function: 4.390, Average Loss: 4.467, avg. samples / sec: 56499.81
Iteration:   1840, Loss function: 4.060, Average Loss: 4.432, avg. samples / sec: 56437.07
Iteration:   1840, Loss function: 3.792, Average Loss: 4.418, avg. samples / sec: 56370.77
Iteration:   1840, Loss function: 4.342, Average Loss: 4.468, avg. samples / sec: 56482.92
Iteration:   1840, Loss function: 3.806, Average Loss: 4.436, avg. samples / sec: 56469.68
Iteration:   1840, Loss function: 5.096, Average Loss: 4.433, avg. samples / sec: 56316.84
Iteration:   1840, Loss function: 4.570, Average Loss: 4.462, avg. samples / sec: 56438.88
Iteration:   1840, Loss function: 5.377, Average Loss: 4.425, avg. samples / sec: 56522.41
Iteration:   1840, Loss function: 4.074, Average Loss: 4.412, avg. samples / sec: 56238.25
Iteration:   1840, Loss function: 3.287, Average Loss: 4.469, avg. samples / sec: 56385.81
Iteration:   1840, Loss function: 4.067, Average Loss: 4.463, avg. samples / sec: 56402.20
Iteration:   1840, Loss function: 4.752, Average Loss: 4.434, avg. samples / sec: 56572.10
Iteration:   1860, Loss function: 5.799, Average Loss: 4.430, avg. samples / sec: 59042.65
Iteration:   1860, Loss function: 4.671, Average Loss: 4.462, avg. samples / sec: 59106.37
Iteration:   1860, Loss function: 3.892, Average Loss: 4.423, avg. samples / sec: 59005.57
Iteration:   1860, Loss function: 6.556, Average Loss: 4.431, avg. samples / sec: 58995.87
Iteration:   1860, Loss function: 3.323, Average Loss: 4.460, avg. samples / sec: 59019.39
Iteration:   1860, Loss function: 5.242, Average Loss: 4.440, avg. samples / sec: 58965.13
Iteration:   1860, Loss function: 4.169, Average Loss: 4.433, avg. samples / sec: 58951.64
Iteration:   1860, Loss function: 4.579, Average Loss: 4.435, avg. samples / sec: 58965.75
Iteration:   1860, Loss function: 3.878, Average Loss: 4.464, avg. samples / sec: 58983.64
Iteration:   1860, Loss function: 3.959, Average Loss: 4.464, avg. samples / sec: 58930.96
Iteration:   1860, Loss function: 4.122, Average Loss: 4.441, avg. samples / sec: 58733.35
Iteration:   1860, Loss function: 2.393, Average Loss: 4.421, avg. samples / sec: 58945.48
Iteration:   1860, Loss function: 4.988, Average Loss: 4.414, avg. samples / sec: 58867.25
Iteration:   1860, Loss function: 4.157, Average Loss: 4.466, avg. samples / sec: 58872.54
Iteration:   1860, Loss function: 4.293, Average Loss: 4.410, avg. samples / sec: 58932.78
Iteration:   1880, Loss function: 5.492, Average Loss: 4.421, avg. samples / sec: 57746.51
Iteration:   1880, Loss function: 4.806, Average Loss: 4.428, avg. samples / sec: 57720.28
Iteration:   1880, Loss function: 5.011, Average Loss: 4.415, avg. samples / sec: 57811.53
Iteration:   1880, Loss function: 4.307, Average Loss: 4.458, avg. samples / sec: 57792.02
Iteration:   1880, Loss function: 3.992, Average Loss: 4.456, avg. samples / sec: 57676.46
Iteration:   1880, Loss function: 4.741, Average Loss: 4.431, avg. samples / sec: 57773.04
Iteration:   1880, Loss function: 3.499, Average Loss: 4.450, avg. samples / sec: 57708.53
Iteration:   1880, Loss function: 5.083, Average Loss: 4.462, avg. samples / sec: 57797.42
Iteration:   1880, Loss function: 3.199, Average Loss: 4.405, avg. samples / sec: 57807.62
Iteration:   1880, Loss function: 4.909, Average Loss: 4.416, avg. samples / sec: 57757.01
Iteration:   1880, Loss function: 3.912, Average Loss: 4.427, avg. samples / sec: 57703.17
Iteration:   1880, Loss function: 3.819, Average Loss: 4.464, avg. samples / sec: 57712.92
Iteration:   1880, Loss function: 4.113, Average Loss: 4.415, avg. samples / sec: 57602.15
Iteration:   1880, Loss function: 3.518, Average Loss: 4.437, avg. samples / sec: 57610.98
Iteration:   1880, Loss function: 3.780, Average Loss: 4.438, avg. samples / sec: 57684.18
:::MLL 1558639436.480 epoch_stop: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 819}}
:::MLL 1558639436.480 epoch_start: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 673}}
Iteration:   1900, Loss function: 5.040, Average Loss: 4.419, avg. samples / sec: 59229.46
Iteration:   1900, Loss function: 4.300, Average Loss: 4.434, avg. samples / sec: 59348.36
Iteration:   1900, Loss function: 4.890, Average Loss: 4.426, avg. samples / sec: 59217.88
Iteration:   1900, Loss function: 3.940, Average Loss: 4.447, avg. samples / sec: 59223.81
Iteration:   1900, Loss function: 2.921, Average Loss: 4.449, avg. samples / sec: 59197.76
Iteration:   1900, Loss function: 4.663, Average Loss: 4.451, avg. samples / sec: 59152.56
Iteration:   1900, Loss function: 4.266, Average Loss: 4.409, avg. samples / sec: 59192.49
Iteration:   1900, Loss function: 4.344, Average Loss: 4.420, avg. samples / sec: 59136.80
Iteration:   1900, Loss function: 4.533, Average Loss: 4.404, avg. samples / sec: 59053.19
Iteration:   1900, Loss function: 3.501, Average Loss: 4.419, avg. samples / sec: 58934.93
Iteration:   1900, Loss function: 4.932, Average Loss: 4.421, avg. samples / sec: 58945.45
Iteration:   1900, Loss function: 3.190, Average Loss: 4.408, avg. samples / sec: 59062.70
Iteration:   1900, Loss function: 3.517, Average Loss: 4.433, avg. samples / sec: 59067.28
Iteration:   1900, Loss function: 4.696, Average Loss: 4.452, avg. samples / sec: 58919.38
Iteration:   1900, Loss function: 3.678, Average Loss: 4.458, avg. samples / sec: 58867.89
Iteration:   1920, Loss function: 3.806, Average Loss: 4.425, avg. samples / sec: 60770.19
Iteration:   1920, Loss function: 5.932, Average Loss: 4.420, avg. samples / sec: 60945.27
Iteration:   1920, Loss function: 4.085, Average Loss: 4.433, avg. samples / sec: 60577.04
Iteration:   1920, Loss function: 3.871, Average Loss: 4.417, avg. samples / sec: 60524.78
Iteration:   1920, Loss function: 3.805, Average Loss: 4.413, avg. samples / sec: 60744.36
Iteration:   1920, Loss function: 3.637, Average Loss: 4.417, avg. samples / sec: 60824.40
Iteration:   1920, Loss function: 4.266, Average Loss: 4.447, avg. samples / sec: 60525.22
Iteration:   1920, Loss function: 3.706, Average Loss: 4.406, avg. samples / sec: 60586.13
Iteration:   1920, Loss function: 5.290, Average Loss: 4.404, avg. samples / sec: 60762.51
Iteration:   1920, Loss function: 4.941, Average Loss: 4.432, avg. samples / sec: 60726.69
Iteration:   1920, Loss function: 3.607, Average Loss: 4.445, avg. samples / sec: 60471.61
Iteration:   1920, Loss function: 4.724, Average Loss: 4.453, avg. samples / sec: 60894.28
Iteration:   1920, Loss function: 4.512, Average Loss: 4.451, avg. samples / sec: 60442.28
Iteration:   1920, Loss function: 5.073, Average Loss: 4.400, avg. samples / sec: 60550.55
Iteration:   1920, Loss function: 3.993, Average Loss: 4.452, avg. samples / sec: 60597.67
Iteration:   1940, Loss function: 4.320, Average Loss: 4.418, avg. samples / sec: 56704.64
Iteration:   1940, Loss function: 4.111, Average Loss: 4.407, avg. samples / sec: 56840.22
Iteration:   1940, Loss function: 4.987, Average Loss: 4.399, avg. samples / sec: 56922.13
Iteration:   1940, Loss function: 3.800, Average Loss: 4.419, avg. samples / sec: 56544.86
Iteration:   1940, Loss function: 5.459, Average Loss: 4.414, avg. samples / sec: 56717.83
Iteration:   1940, Loss function: 3.981, Average Loss: 4.429, avg. samples / sec: 56804.91
Iteration:   1940, Loss function: 4.355, Average Loss: 4.410, avg. samples / sec: 56674.43
Iteration:   1940, Loss function: 4.295, Average Loss: 4.447, avg. samples / sec: 56829.40
Iteration:   1940, Loss function: 4.926, Average Loss: 4.442, avg. samples / sec: 56759.25
Iteration:   1940, Loss function: 4.794, Average Loss: 4.409, avg. samples / sec: 56624.58
Iteration:   1940, Loss function: 4.397, Average Loss: 4.443, avg. samples / sec: 56679.60
Iteration:   1940, Loss function: 4.978, Average Loss: 4.450, avg. samples / sec: 56702.91
Iteration:   1940, Loss function: 4.076, Average Loss: 4.430, avg. samples / sec: 56467.37
Iteration:   1940, Loss function: 4.851, Average Loss: 4.452, avg. samples / sec: 56719.98
Iteration:   1940, Loss function: 4.339, Average Loss: 4.406, avg. samples / sec: 56476.90
:::MLL 1558639438.493 epoch_stop: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 819}}
:::MLL 1558639438.493 epoch_start: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 673}}
Iteration:   1960, Loss function: 4.832, Average Loss: 4.426, avg. samples / sec: 58810.68
Iteration:   1960, Loss function: 5.023, Average Loss: 4.441, avg. samples / sec: 58652.32
Iteration:   1960, Loss function: 3.939, Average Loss: 4.412, avg. samples / sec: 58541.48
Iteration:   1960, Loss function: 3.853, Average Loss: 4.446, avg. samples / sec: 58606.80
Iteration:   1960, Loss function: 5.461, Average Loss: 4.418, avg. samples / sec: 58417.24
Iteration:   1960, Loss function: 5.329, Average Loss: 4.432, avg. samples / sec: 58525.03
Iteration:   1960, Loss function: 4.783, Average Loss: 4.450, avg. samples / sec: 58769.13
Iteration:   1960, Loss function: 5.407, Average Loss: 4.409, avg. samples / sec: 58410.78
Iteration:   1960, Loss function: 3.403, Average Loss: 4.395, avg. samples / sec: 58463.48
Iteration:   1960, Loss function: 3.868, Average Loss: 4.404, avg. samples / sec: 58803.49
Iteration:   1960, Loss function: 3.741, Average Loss: 4.448, avg. samples / sec: 58521.06
Iteration:   1960, Loss function: 5.436, Average Loss: 4.419, avg. samples / sec: 58415.18
Iteration:   1960, Loss function: 4.768, Average Loss: 4.419, avg. samples / sec: 58406.85
Iteration:   1960, Loss function: 4.230, Average Loss: 4.408, avg. samples / sec: 58396.74
Iteration:   1960, Loss function: 3.949, Average Loss: 4.449, avg. samples / sec: 58437.37
Iteration:   1980, Loss function: 5.142, Average Loss: 4.440, avg. samples / sec: 58326.79
Iteration:   1980, Loss function: 4.820, Average Loss: 4.415, avg. samples / sec: 58390.57
Iteration:   1980, Loss function: 3.327, Average Loss: 4.412, avg. samples / sec: 58296.13
Iteration:   1980, Loss function: 3.486, Average Loss: 4.448, avg. samples / sec: 58291.28
Iteration:   1980, Loss function: 4.831, Average Loss: 4.447, avg. samples / sec: 58307.32
Iteration:   1980, Loss function: 5.139, Average Loss: 4.416, avg. samples / sec: 58236.14
Iteration:   1980, Loss function: 4.166, Average Loss: 4.430, avg. samples / sec: 58208.87
Iteration:   1980, Loss function: 4.139, Average Loss: 4.394, avg. samples / sec: 58215.12
Iteration:   1980, Loss function: 3.815, Average Loss: 4.428, avg. samples / sec: 58115.95
Iteration:   1980, Loss function: 3.290, Average Loss: 4.419, avg. samples / sec: 58249.29
Iteration:   1980, Loss function: 4.274, Average Loss: 4.438, avg. samples / sec: 58065.33
Iteration:   1980, Loss function: 4.271, Average Loss: 4.404, avg. samples / sec: 58316.37
Iteration:   1980, Loss function: 4.314, Average Loss: 4.406, avg. samples / sec: 58138.89
Iteration:   1980, Loss function: 5.561, Average Loss: 4.405, avg. samples / sec: 58090.27
Iteration:   1980, Loss function: 3.893, Average Loss: 4.445, avg. samples / sec: 58197.86
Iteration:   2000, Loss function: 4.792, Average Loss: 4.405, avg. samples / sec: 59876.14
Iteration:   2000, Loss function: 3.959, Average Loss: 4.433, avg. samples / sec: 59909.71
Iteration:   2000, Loss function: 4.164, Average Loss: 4.412, avg. samples / sec: 59950.52
Iteration:   2000, Loss function: 4.845, Average Loss: 4.444, avg. samples / sec: 59819.92
Iteration:   2000, Loss function: 3.437, Average Loss: 4.411, avg. samples / sec: 59733.13
Iteration:   2000, Loss function: 5.055, Average Loss: 4.396, avg. samples / sec: 59864.29
Iteration:   2000, Loss function: 3.246, Average Loss: 4.432, avg. samples / sec: 59856.10
Iteration:   2000, Loss function: 3.653, Average Loss: 4.434, avg. samples / sec: 59607.69
Iteration:   2000, Loss function: 4.496, Average Loss: 4.438, avg. samples / sec: 59945.75
Iteration:   2000, Loss function: 4.584, Average Loss: 4.402, avg. samples / sec: 59823.96
Iteration:   2000, Loss function: 3.328, Average Loss: 4.387, avg. samples / sec: 59752.30
Iteration:   2000, Loss function: 3.332, Average Loss: 4.419, avg. samples / sec: 59680.14
Iteration:   2000, Loss function: 4.332, Average Loss: 4.401, avg. samples / sec: 59748.86
Iteration:   2000, Loss function: 3.613, Average Loss: 4.415, avg. samples / sec: 59542.49
Iteration:   2000, Loss function: 4.402, Average Loss: 4.443, avg. samples / sec: 59505.41
Iteration:   2020, Loss function: 4.133, Average Loss: 4.427, avg. samples / sec: 59735.26
Iteration:   2020, Loss function: 5.029, Average Loss: 4.396, avg. samples / sec: 59728.62
Iteration:   2020, Loss function: 5.073, Average Loss: 4.411, avg. samples / sec: 59517.12
Iteration:   2020, Loss function: 4.559, Average Loss: 4.434, avg. samples / sec: 59655.68
Iteration:   2020, Loss function: 3.889, Average Loss: 4.434, avg. samples / sec: 59683.07
Iteration:   2020, Loss function: 4.003, Average Loss: 4.427, avg. samples / sec: 59463.83
Iteration:   2020, Loss function: 5.271, Average Loss: 4.392, avg. samples / sec: 59745.36
Iteration:   2020, Loss function: 4.290, Average Loss: 4.412, avg. samples / sec: 59729.41
Iteration:   2020, Loss function: 4.843, Average Loss: 4.393, avg. samples / sec: 59530.49
Iteration:   2020, Loss function: 5.118, Average Loss: 4.383, avg. samples / sec: 59562.44
Iteration:   2020, Loss function: 5.275, Average Loss: 4.442, avg. samples / sec: 59382.34
Iteration:   2020, Loss function: 2.683, Average Loss: 4.413, avg. samples / sec: 59606.93
Iteration:   2020, Loss function: 4.002, Average Loss: 4.400, avg. samples / sec: 59247.34
Iteration:   2020, Loss function: 5.145, Average Loss: 4.404, avg. samples / sec: 59357.68
Iteration:   2020, Loss function: 4.020, Average Loss: 4.440, avg. samples / sec: 59555.67
:::MLL 1558639440.474 epoch_stop: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 819}}
:::MLL 1558639440.475 epoch_start: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 673}}
Iteration:   2040, Loss function: 4.500, Average Loss: 4.438, avg. samples / sec: 59055.10
Iteration:   2040, Loss function: 5.627, Average Loss: 4.436, avg. samples / sec: 59171.41
Iteration:   2040, Loss function: 4.785, Average Loss: 4.383, avg. samples / sec: 58969.85
Iteration:   2040, Loss function: 4.830, Average Loss: 4.407, avg. samples / sec: 58919.58
Iteration:   2040, Loss function: 4.498, Average Loss: 4.421, avg. samples / sec: 58834.47
Iteration:   2040, Loss function: 4.053, Average Loss: 4.423, avg. samples / sec: 58690.13
Iteration:   2040, Loss function: 4.239, Average Loss: 4.388, avg. samples / sec: 58857.91
Iteration:   2040, Loss function: 3.189, Average Loss: 4.427, avg. samples / sec: 58720.77
Iteration:   2040, Loss function: 3.810, Average Loss: 4.404, avg. samples / sec: 58942.54
Iteration:   2040, Loss function: 4.519, Average Loss: 4.394, avg. samples / sec: 58667.63
Iteration:   2040, Loss function: 4.333, Average Loss: 4.381, avg. samples / sec: 58818.46
Iteration:   2040, Loss function: 4.167, Average Loss: 4.407, avg. samples / sec: 58822.14
Iteration:   2040, Loss function: 4.323, Average Loss: 4.407, avg. samples / sec: 58655.42
Iteration:   2040, Loss function: 4.193, Average Loss: 4.432, avg. samples / sec: 58583.63
Iteration:   2040, Loss function: 3.163, Average Loss: 4.393, avg. samples / sec: 58790.88
Iteration:   2060, Loss function: 5.284, Average Loss: 4.380, avg. samples / sec: 60245.55
Iteration:   2060, Loss function: 3.559, Average Loss: 4.437, avg. samples / sec: 60231.64
Iteration:   2060, Loss function: 3.706, Average Loss: 4.403, avg. samples / sec: 60266.08
Iteration:   2060, Loss function: 3.896, Average Loss: 4.382, avg. samples / sec: 60373.66
Iteration:   2060, Loss function: 3.803, Average Loss: 4.437, avg. samples / sec: 60147.17
Iteration:   2060, Loss function: 5.090, Average Loss: 4.400, avg. samples / sec: 60337.68
Iteration:   2060, Loss function: 4.680, Average Loss: 4.403, avg. samples / sec: 60384.66
Iteration:   2060, Loss function: 3.526, Average Loss: 4.416, avg. samples / sec: 60231.26
Iteration:   2060, Loss function: 6.385, Average Loss: 4.435, avg. samples / sec: 60424.01
Iteration:   2060, Loss function: 4.634, Average Loss: 4.392, avg. samples / sec: 60404.43
Iteration:   2060, Loss function: 3.224, Average Loss: 4.387, avg. samples / sec: 60283.74
Iteration:   2060, Loss function: 3.980, Average Loss: 4.402, avg. samples / sec: 60290.16
Iteration:   2060, Loss function: 4.870, Average Loss: 4.421, avg. samples / sec: 60230.41
Iteration:   2060, Loss function: 5.553, Average Loss: 4.384, avg. samples / sec: 60135.78
Iteration:   2060, Loss function: 4.428, Average Loss: 4.420, avg. samples / sec: 59447.02
Iteration:   2080, Loss function: 4.011, Average Loss: 4.431, avg. samples / sec: 59979.12
Iteration:   2080, Loss function: 4.072, Average Loss: 4.397, avg. samples / sec: 59875.66
Iteration:   2080, Loss function: 4.538, Average Loss: 4.418, avg. samples / sec: 60004.73
Iteration:   2080, Loss function: 3.250, Average Loss: 4.430, avg. samples / sec: 59891.94
Iteration:   2080, Loss function: 4.561, Average Loss: 4.380, avg. samples / sec: 59859.33
Iteration:   2080, Loss function: 4.235, Average Loss: 4.429, avg. samples / sec: 59703.88
Iteration:   2080, Loss function: 3.462, Average Loss: 4.379, avg. samples / sec: 59768.95
Iteration:   2080, Loss function: 4.927, Average Loss: 4.382, avg. samples / sec: 59936.77
Iteration:   2080, Loss function: 3.982, Average Loss: 4.398, avg. samples / sec: 59885.55
Iteration:   2080, Loss function: 3.883, Average Loss: 4.404, avg. samples / sec: 59753.01
Iteration:   2080, Loss function: 5.560, Average Loss: 4.422, avg. samples / sec: 60545.61
Iteration:   2080, Loss function: 4.358, Average Loss: 4.377, avg. samples / sec: 59602.17
Iteration:   2080, Loss function: 3.658, Average Loss: 4.384, avg. samples / sec: 59731.51
Iteration:   2080, Loss function: 3.232, Average Loss: 4.393, avg. samples / sec: 59492.84
Iteration:   2080, Loss function: 3.589, Average Loss: 4.410, avg. samples / sec: 59474.82
:::MLL 1558639442.453 epoch_stop: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 819}}
:::MLL 1558639442.453 epoch_start: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 673}}
Iteration:   2100, Loss function: 3.599, Average Loss: 4.380, avg. samples / sec: 59496.76
Iteration:   2100, Loss function: 4.326, Average Loss: 4.390, avg. samples / sec: 59415.89
Iteration:   2100, Loss function: 5.187, Average Loss: 4.374, avg. samples / sec: 59365.78
Iteration:   2100, Loss function: 3.946, Average Loss: 4.422, avg. samples / sec: 59143.23
Iteration:   2100, Loss function: 4.305, Average Loss: 4.379, avg. samples / sec: 59429.70
Iteration:   2100, Loss function: 3.200, Average Loss: 4.377, avg. samples / sec: 59318.41
Iteration:   2100, Loss function: 3.683, Average Loss: 4.423, avg. samples / sec: 59247.31
Iteration:   2100, Loss function: 3.900, Average Loss: 4.411, avg. samples / sec: 59192.09
Iteration:   2100, Loss function: 4.260, Average Loss: 4.397, avg. samples / sec: 59165.45
Iteration:   2100, Loss function: 3.980, Average Loss: 4.372, avg. samples / sec: 59300.16
Iteration:   2100, Loss function: 4.302, Average Loss: 4.402, avg. samples / sec: 59229.78
Iteration:   2100, Loss function: 4.512, Average Loss: 4.386, avg. samples / sec: 59470.28
Iteration:   2100, Loss function: 4.487, Average Loss: 4.416, avg. samples / sec: 59208.93
Iteration:   2100, Loss function: 4.350, Average Loss: 4.404, avg. samples / sec: 59432.58
Iteration:   2100, Loss function: 3.483, Average Loss: 4.424, avg. samples / sec: 59089.99
Iteration:   2120, Loss function: 3.940, Average Loss: 4.411, avg. samples / sec: 59530.47
Iteration:   2120, Loss function: 6.156, Average Loss: 4.401, avg. samples / sec: 59364.21
Iteration:   2120, Loss function: 4.661, Average Loss: 4.368, avg. samples / sec: 59250.85
Iteration:   2120, Loss function: 4.370, Average Loss: 4.376, avg. samples / sec: 59314.29
Iteration:   2120, Loss function: 3.757, Average Loss: 4.373, avg. samples / sec: 59383.70
Iteration:   2120, Loss function: 4.653, Average Loss: 4.402, avg. samples / sec: 59470.38
Iteration:   2120, Loss function: 3.337, Average Loss: 4.421, avg. samples / sec: 59247.68
Iteration:   2120, Loss function: 3.920, Average Loss: 4.412, avg. samples / sec: 59237.70
Iteration:   2120, Loss function: 4.076, Average Loss: 4.387, avg. samples / sec: 59101.44
Iteration:   2120, Loss function: 5.086, Average Loss: 4.419, avg. samples / sec: 59171.76
Iteration:   2120, Loss function: 4.672, Average Loss: 4.401, avg. samples / sec: 59224.83
Iteration:   2120, Loss function: 3.865, Average Loss: 4.377, avg. samples / sec: 58897.69
Iteration:   2120, Loss function: 5.037, Average Loss: 4.378, avg. samples / sec: 59063.86
Iteration:   2120, Loss function: 4.934, Average Loss: 4.385, avg. samples / sec: 59117.35
Iteration:   2120, Loss function: 5.613, Average Loss: 4.426, avg. samples / sec: 59165.72
Iteration:   2140, Loss function: 4.117, Average Loss: 4.375, avg. samples / sec: 58067.31
Iteration:   2140, Loss function: 3.873, Average Loss: 4.386, avg. samples / sec: 58147.96
Iteration:   2140, Loss function: 5.175, Average Loss: 4.373, avg. samples / sec: 57828.52
Iteration:   2140, Loss function: 4.359, Average Loss: 4.395, avg. samples / sec: 58022.85
Iteration:   2140, Loss function: 3.224, Average Loss: 4.370, avg. samples / sec: 58045.84
Iteration:   2140, Loss function: 4.006, Average Loss: 4.403, avg. samples / sec: 57892.92
Iteration:   2140, Loss function: 2.534, Average Loss: 4.410, avg. samples / sec: 57909.45
Iteration:   2140, Loss function: 3.333, Average Loss: 4.424, avg. samples / sec: 58148.75
Iteration:   2140, Loss function: 4.177, Average Loss: 4.415, avg. samples / sec: 57867.17
Iteration:   2140, Loss function: 3.586, Average Loss: 4.397, avg. samples / sec: 57742.46
Iteration:   2140, Loss function: 3.109, Average Loss: 4.399, avg. samples / sec: 57794.63
Iteration:   2140, Loss function: 5.181, Average Loss: 4.366, avg. samples / sec: 57775.84
Iteration:   2140, Loss function: 3.849, Average Loss: 4.379, avg. samples / sec: 57824.86
Iteration:   2140, Loss function: 4.078, Average Loss: 4.369, avg. samples / sec: 57735.50
Iteration:   2140, Loss function: 5.072, Average Loss: 4.409, avg. samples / sec: 57663.12
Iteration:   2160, Loss function: 4.066, Average Loss: 4.377, avg. samples / sec: 57992.16
Iteration:   2160, Loss function: 4.158, Average Loss: 4.369, avg. samples / sec: 57859.90
Iteration:   2160, Loss function: 2.834, Average Loss: 4.408, avg. samples / sec: 58002.45
Iteration:   2160, Loss function: 3.276, Average Loss: 4.400, avg. samples / sec: 57936.90
Iteration:   2160, Loss function: 5.478, Average Loss: 4.399, avg. samples / sec: 57864.77
Iteration:   2160, Loss function: 4.097, Average Loss: 4.369, avg. samples / sec: 57936.83
Iteration:   2160, Loss function: 6.016, Average Loss: 4.394, avg. samples / sec: 57827.26
Iteration:   2160, Loss function: 4.653, Average Loss: 4.379, avg. samples / sec: 57805.37
Iteration:   2160, Loss function: 4.678, Average Loss: 4.368, avg. samples / sec: 57857.91
Iteration:   2160, Loss function: 4.537, Average Loss: 4.420, avg. samples / sec: 57781.45
Iteration:   2160, Loss function: 3.745, Average Loss: 4.369, avg. samples / sec: 57728.86
Iteration:   2160, Loss function: 3.225, Average Loss: 4.364, avg. samples / sec: 57747.55
Iteration:   2160, Loss function: 5.775, Average Loss: 4.393, avg. samples / sec: 57786.50
Iteration:   2160, Loss function: 4.618, Average Loss: 4.413, avg. samples / sec: 57732.26
Iteration:   2160, Loss function: 3.498, Average Loss: 4.403, avg. samples / sec: 57731.58
:::MLL 1558639444.459 epoch_stop: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 819}}
:::MLL 1558639444.460 epoch_start: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 673}}
Iteration:   2180, Loss function: 4.963, Average Loss: 4.370, avg. samples / sec: 58463.24
Iteration:   2180, Loss function: 3.709, Average Loss: 4.363, avg. samples / sec: 58356.31
Iteration:   2180, Loss function: 5.553, Average Loss: 4.396, avg. samples / sec: 58474.42
Iteration:   2180, Loss function: 3.444, Average Loss: 4.364, avg. samples / sec: 58320.37
Iteration:   2180, Loss function: 3.142, Average Loss: 4.388, avg. samples / sec: 58442.60
Iteration:   2180, Loss function: 5.295, Average Loss: 4.401, avg. samples / sec: 58304.01
Iteration:   2180, Loss function: 3.841, Average Loss: 4.400, avg. samples / sec: 58262.60
Iteration:   2180, Loss function: 4.460, Average Loss: 4.379, avg. samples / sec: 58228.80
Iteration:   2180, Loss function: 3.860, Average Loss: 4.406, avg. samples / sec: 58406.73
Iteration:   2180, Loss function: 4.859, Average Loss: 4.357, avg. samples / sec: 58327.23
Iteration:   2180, Loss function: 4.906, Average Loss: 4.401, avg. samples / sec: 58213.29
Iteration:   2180, Loss function: 3.887, Average Loss: 4.362, avg. samples / sec: 58251.41
Iteration:   2180, Loss function: 5.584, Average Loss: 4.414, avg. samples / sec: 58271.32
Iteration:   2180, Loss function: 3.728, Average Loss: 4.388, avg. samples / sec: 58194.57
Iteration:   2180, Loss function: 3.997, Average Loss: 4.371, avg. samples / sec: 58177.08
Iteration:   2200, Loss function: 3.656, Average Loss: 4.385, avg. samples / sec: 58081.36
Iteration:   2200, Loss function: 3.142, Average Loss: 4.352, avg. samples / sec: 58133.83
Iteration:   2200, Loss function: 3.273, Average Loss: 4.401, avg. samples / sec: 57983.91
Iteration:   2200, Loss function: 4.808, Average Loss: 4.390, avg. samples / sec: 57937.21
Iteration:   2200, Loss function: 3.770, Average Loss: 4.379, avg. samples / sec: 57993.79
Iteration:   2200, Loss function: 4.451, Average Loss: 4.364, avg. samples / sec: 57876.49
Iteration:   2200, Loss function: 4.252, Average Loss: 4.364, avg. samples / sec: 58082.37
Iteration:   2200, Loss function: 4.661, Average Loss: 4.355, avg. samples / sec: 58039.12
Iteration:   2200, Loss function: 3.114, Average Loss: 4.362, avg. samples / sec: 57834.88
Iteration:   2200, Loss function: 2.868, Average Loss: 4.397, avg. samples / sec: 57988.06
Iteration:   2200, Loss function: 3.796, Average Loss: 4.381, avg. samples / sec: 58024.45
Iteration:   2200, Loss function: 3.393, Average Loss: 4.406, avg. samples / sec: 58002.12
Iteration:   2200, Loss function: 5.277, Average Loss: 4.401, avg. samples / sec: 57926.95
Iteration:   2200, Loss function: 3.638, Average Loss: 4.356, avg. samples / sec: 57823.25
Iteration:   2200, Loss function: 3.645, Average Loss: 4.398, avg. samples / sec: 57857.98
Iteration:   2220, Loss function: 4.636, Average Loss: 4.406, avg. samples / sec: 60618.58
Iteration:   2220, Loss function: 3.574, Average Loss: 4.357, avg. samples / sec: 60493.19
Iteration:   2220, Loss function: 4.038, Average Loss: 4.360, avg. samples / sec: 60447.34
Iteration:   2220, Loss function: 2.974, Average Loss: 4.352, avg. samples / sec: 60282.48
Iteration:   2220, Loss function: 2.752, Average Loss: 4.360, avg. samples / sec: 60391.15
Iteration:   2220, Loss function: 2.243, Average Loss: 4.354, avg. samples / sec: 60472.13
Iteration:   2220, Loss function: 3.916, Average Loss: 4.350, avg. samples / sec: 60391.33
Iteration:   2220, Loss function: 4.114, Average Loss: 4.390, avg. samples / sec: 60292.59
Iteration:   2220, Loss function: 4.826, Average Loss: 4.382, avg. samples / sec: 60325.08
Iteration:   2220, Loss function: 3.400, Average Loss: 4.389, avg. samples / sec: 60302.05
Iteration:   2220, Loss function: 3.436, Average Loss: 4.395, avg. samples / sec: 60314.42
Iteration:   2220, Loss function: 5.751, Average Loss: 4.397, avg. samples / sec: 60173.93
Iteration:   2220, Loss function: 4.082, Average Loss: 4.380, avg. samples / sec: 60045.41
Iteration:   2220, Loss function: 4.954, Average Loss: 4.370, avg. samples / sec: 60152.95
Iteration:   2220, Loss function: 4.346, Average Loss: 4.398, avg. samples / sec: 60187.60
:::MLL 1558639446.455 epoch_stop: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 819}}
:::MLL 1558639446.455 epoch_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 673}}
Iteration:   2240, Loss function: 4.127, Average Loss: 4.342, avg. samples / sec: 60193.85
Iteration:   2240, Loss function: 4.897, Average Loss: 4.376, avg. samples / sec: 60313.56
Iteration:   2240, Loss function: 3.236, Average Loss: 4.354, avg. samples / sec: 60052.09
Iteration:   2240, Loss function: 3.903, Average Loss: 4.350, avg. samples / sec: 60088.60
Iteration:   2240, Loss function: 4.010, Average Loss: 4.378, avg. samples / sec: 60084.01
Iteration:   2240, Loss function: 3.748, Average Loss: 4.378, avg. samples / sec: 60124.92
Iteration:   2240, Loss function: 4.542, Average Loss: 4.389, avg. samples / sec: 60105.87
Iteration:   2240, Loss function: 3.807, Average Loss: 4.353, avg. samples / sec: 59986.22
Iteration:   2240, Loss function: 5.113, Average Loss: 4.346, avg. samples / sec: 59983.69
Iteration:   2240, Loss function: 5.339, Average Loss: 4.393, avg. samples / sec: 60002.61
Iteration:   2240, Loss function: 4.480, Average Loss: 4.402, avg. samples / sec: 59692.95
Iteration:   2240, Loss function: 2.430, Average Loss: 4.367, avg. samples / sec: 60049.32
Iteration:   2240, Loss function: 4.021, Average Loss: 4.351, avg. samples / sec: 59801.67
Iteration:   2240, Loss function: 5.460, Average Loss: 4.393, avg. samples / sec: 60091.34
Iteration:   2240, Loss function: 4.043, Average Loss: 4.399, avg. samples / sec: 59935.01
Iteration:   2260, Loss function: 3.942, Average Loss: 4.392, avg. samples / sec: 58864.13
Iteration:   2260, Loss function: 4.112, Average Loss: 4.346, avg. samples / sec: 58482.28
Iteration:   2260, Loss function: 2.257, Average Loss: 4.347, avg. samples / sec: 58592.57
Iteration:   2260, Loss function: 4.846, Average Loss: 4.343, avg. samples / sec: 58689.25
Iteration:   2260, Loss function: 4.088, Average Loss: 4.387, avg. samples / sec: 58640.99
Iteration:   2260, Loss function: 5.050, Average Loss: 4.351, avg. samples / sec: 58402.16
Iteration:   2260, Loss function: 4.151, Average Loss: 4.389, avg. samples / sec: 58688.08
Iteration:   2260, Loss function: 4.430, Average Loss: 4.376, avg. samples / sec: 58461.90
Iteration:   2260, Loss function: 4.384, Average Loss: 4.373, avg. samples / sec: 58442.68
Iteration:   2260, Loss function: 4.640, Average Loss: 4.375, avg. samples / sec: 58304.47
Iteration:   2260, Loss function: 4.125, Average Loss: 4.389, avg. samples / sec: 58562.38
Iteration:   2260, Loss function: 5.660, Average Loss: 4.343, avg. samples / sec: 58431.10
Iteration:   2260, Loss function: 4.282, Average Loss: 4.341, avg. samples / sec: 58211.70
Iteration:   2260, Loss function: 3.178, Average Loss: 4.368, avg. samples / sec: 58503.79
Iteration:   2260, Loss function: 3.990, Average Loss: 4.383, avg. samples / sec: 58349.98
Iteration:   2280, Loss function: 3.699, Average Loss: 4.384, avg. samples / sec: 60010.15
Iteration:   2280, Loss function: 3.589, Average Loss: 4.383, avg. samples / sec: 59927.47
Iteration:   2280, Loss function: 4.012, Average Loss: 4.346, avg. samples / sec: 59815.53
Iteration:   2280, Loss function: 2.869, Average Loss: 4.341, avg. samples / sec: 59783.51
Iteration:   2280, Loss function: 5.095, Average Loss: 4.385, avg. samples / sec: 59931.90
Iteration:   2280, Loss function: 4.731, Average Loss: 4.374, avg. samples / sec: 60009.54
Iteration:   2280, Loss function: 4.398, Average Loss: 4.369, avg. samples / sec: 59846.01
Iteration:   2280, Loss function: 4.785, Average Loss: 4.370, avg. samples / sec: 59883.87
Iteration:   2280, Loss function: 4.185, Average Loss: 4.337, avg. samples / sec: 59904.98
Iteration:   2280, Loss function: 4.141, Average Loss: 4.385, avg. samples / sec: 59599.02
Iteration:   2280, Loss function: 4.070, Average Loss: 4.336, avg. samples / sec: 59707.85
Iteration:   2280, Loss function: 3.002, Average Loss: 4.370, avg. samples / sec: 59743.92
Iteration:   2280, Loss function: 3.791, Average Loss: 4.365, avg. samples / sec: 59773.82
Iteration:   2280, Loss function: 4.278, Average Loss: 4.346, avg. samples / sec: 59563.22
Iteration:   2280, Loss function: 4.447, Average Loss: 4.344, avg. samples / sec: 59637.63
:::MLL 1558639447.778 eval_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.96 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.96 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.96 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.96 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.96 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.96 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.96 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.96 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.96 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.96 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.96 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.96 s
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.96 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.96 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.96 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.38s)
DONE (t=0.40s)
DONE (t=2.63s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.15507
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.29520
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.15042
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.03373
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.17263
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.24901
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.17121
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.24941
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.26256
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.06400
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.27301
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.42020
Current AP: 0.15507 AP goal: 0.23000
:::MLL 1558639451.785 eval_accuracy: {"value": 0.15506551311965458, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 389}}
:::MLL 1558639451.830 eval_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 392}}
:::MLL 1558639451.839 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 804}}
:::MLL 1558639451.839 block_start: {"value": null, "metadata": {"first_epoch_num": 33, "epoch_count": 10.915354834308324, "file": "train.py", "lineno": 813}}
Iteration:   2300, Loss function: 3.485, Average Loss: 4.362, avg. samples / sec: 7054.82
Iteration:   2300, Loss function: 4.760, Average Loss: 4.385, avg. samples / sec: 7052.33
Iteration:   2300, Loss function: 4.227, Average Loss: 4.364, avg. samples / sec: 7054.49
Iteration:   2300, Loss function: 5.732, Average Loss: 4.383, avg. samples / sec: 7052.37
Iteration:   2300, Loss function: 6.141, Average Loss: 4.370, avg. samples / sec: 7053.85
Iteration:   2300, Loss function: 4.095, Average Loss: 4.364, avg. samples / sec: 7055.64
Iteration:   2300, Loss function: 3.798, Average Loss: 4.326, avg. samples / sec: 7054.38
Iteration:   2300, Loss function: 4.120, Average Loss: 4.341, avg. samples / sec: 7052.12
Iteration:   2300, Loss function: 4.877, Average Loss: 4.342, avg. samples / sec: 7051.41
Iteration:   2300, Loss function: 5.465, Average Loss: 4.345, avg. samples / sec: 7055.95
Iteration:   2300, Loss function: 4.015, Average Loss: 4.364, avg. samples / sec: 7054.27
Iteration:   2300, Loss function: 3.854, Average Loss: 4.382, avg. samples / sec: 7051.59
Iteration:   2300, Loss function: 2.887, Average Loss: 4.373, avg. samples / sec: 7050.21
Iteration:   2300, Loss function: 3.800, Average Loss: 4.340, avg. samples / sec: 7051.69
Iteration:   2300, Loss function: 3.685, Average Loss: 4.334, avg. samples / sec: 7047.67
:::MLL 1558639452.645 epoch_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 819}}
:::MLL 1558639452.646 epoch_start: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 673}}
Iteration:   2320, Loss function: 4.174, Average Loss: 4.358, avg. samples / sec: 58876.38
Iteration:   2320, Loss function: 4.115, Average Loss: 4.318, avg. samples / sec: 58861.82
Iteration:   2320, Loss function: 4.179, Average Loss: 4.361, avg. samples / sec: 58689.79
Iteration:   2320, Loss function: 4.039, Average Loss: 4.338, avg. samples / sec: 58755.61
Iteration:   2320, Loss function: 4.525, Average Loss: 4.382, avg. samples / sec: 58601.42
Iteration:   2320, Loss function: 5.048, Average Loss: 4.342, avg. samples / sec: 58726.99
Iteration:   2320, Loss function: 3.489, Average Loss: 4.373, avg. samples / sec: 58871.70
Iteration:   2320, Loss function: 2.936, Average Loss: 4.363, avg. samples / sec: 58617.82
Iteration:   2320, Loss function: 3.986, Average Loss: 4.329, avg. samples / sec: 59014.02
Iteration:   2320, Loss function: 4.555, Average Loss: 4.371, avg. samples / sec: 58742.50
Iteration:   2320, Loss function: 5.351, Average Loss: 4.347, avg. samples / sec: 58602.78
Iteration:   2320, Loss function: 4.063, Average Loss: 4.362, avg. samples / sec: 58430.17
Iteration:   2320, Loss function: 5.065, Average Loss: 4.360, avg. samples / sec: 58640.16
Iteration:   2320, Loss function: 4.594, Average Loss: 4.335, avg. samples / sec: 58898.01
Iteration:   2320, Loss function: 4.557, Average Loss: 4.376, avg. samples / sec: 58309.18
Iteration:   2340, Loss function: 4.948, Average Loss: 4.319, avg. samples / sec: 56372.91
Iteration:   2340, Loss function: 4.698, Average Loss: 4.339, avg. samples / sec: 56511.75
Iteration:   2340, Loss function: 3.094, Average Loss: 4.323, avg. samples / sec: 56560.63
Iteration:   2340, Loss function: 3.518, Average Loss: 4.357, avg. samples / sec: 56454.25
Iteration:   2340, Loss function: 4.539, Average Loss: 4.351, avg. samples / sec: 56212.79
Iteration:   2340, Loss function: 4.368, Average Loss: 4.353, avg. samples / sec: 56571.01
Iteration:   2340, Loss function: 5.422, Average Loss: 4.330, avg. samples / sec: 56378.23
Iteration:   2340, Loss function: 3.999, Average Loss: 4.363, avg. samples / sec: 56512.59
Iteration:   2340, Loss function: 4.315, Average Loss: 4.358, avg. samples / sec: 56549.85
Iteration:   2340, Loss function: 2.906, Average Loss: 4.371, avg. samples / sec: 56347.78
Iteration:   2340, Loss function: 4.687, Average Loss: 4.368, avg. samples / sec: 56641.15
Iteration:   2340, Loss function: 4.650, Average Loss: 4.373, avg. samples / sec: 56307.48
Iteration:   2340, Loss function: 4.572, Average Loss: 4.347, avg. samples / sec: 56411.54
Iteration:   2340, Loss function: 4.154, Average Loss: 4.353, avg. samples / sec: 56177.25
Iteration:   2340, Loss function: 3.971, Average Loss: 4.326, avg. samples / sec: 56419.90
Iteration:   2360, Loss function: 3.188, Average Loss: 4.350, avg. samples / sec: 58330.03
Iteration:   2360, Loss function: 4.160, Average Loss: 4.366, avg. samples / sec: 58362.08
Iteration:   2360, Loss function: 4.791, Average Loss: 4.353, avg. samples / sec: 58275.78
Iteration:   2360, Loss function: 4.157, Average Loss: 4.351, avg. samples / sec: 58234.15
Iteration:   2360, Loss function: 3.110, Average Loss: 4.348, avg. samples / sec: 58376.13
Iteration:   2360, Loss function: 4.093, Average Loss: 4.350, avg. samples / sec: 58223.32
Iteration:   2360, Loss function: 3.730, Average Loss: 4.364, avg. samples / sec: 58285.64
Iteration:   2360, Loss function: 4.288, Average Loss: 4.348, avg. samples / sec: 58340.56
Iteration:   2360, Loss function: 3.864, Average Loss: 4.320, avg. samples / sec: 58367.91
Iteration:   2360, Loss function: 3.312, Average Loss: 4.370, avg. samples / sec: 58274.38
Iteration:   2360, Loss function: 3.755, Average Loss: 4.325, avg. samples / sec: 58083.73
Iteration:   2360, Loss function: 4.126, Average Loss: 4.334, avg. samples / sec: 58040.07
Iteration:   2360, Loss function: 3.807, Average Loss: 4.316, avg. samples / sec: 57996.65
Iteration:   2360, Loss function: 3.905, Average Loss: 4.325, avg. samples / sec: 58132.37
Iteration:   2360, Loss function: 3.901, Average Loss: 4.358, avg. samples / sec: 58090.67
:::MLL 1558639454.673 epoch_stop: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 819}}
:::MLL 1558639454.673 epoch_start: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 673}}
Iteration:   2380, Loss function: 3.219, Average Loss: 4.339, avg. samples / sec: 58765.02
Iteration:   2380, Loss function: 4.174, Average Loss: 4.346, avg. samples / sec: 58694.21
Iteration:   2380, Loss function: 4.238, Average Loss: 4.348, avg. samples / sec: 58693.21
Iteration:   2380, Loss function: 3.717, Average Loss: 4.320, avg. samples / sec: 58792.60
Iteration:   2380, Loss function: 4.015, Average Loss: 4.308, avg. samples / sec: 58800.67
Iteration:   2380, Loss function: 4.476, Average Loss: 4.356, avg. samples / sec: 58859.46
Iteration:   2380, Loss function: 4.132, Average Loss: 4.360, avg. samples / sec: 58719.25
Iteration:   2380, Loss function: 5.000, Average Loss: 4.320, avg. samples / sec: 58749.95
Iteration:   2380, Loss function: 3.799, Average Loss: 4.361, avg. samples / sec: 58691.08
Iteration:   2380, Loss function: 3.692, Average Loss: 4.348, avg. samples / sec: 58599.47
Iteration:   2380, Loss function: 4.321, Average Loss: 4.340, avg. samples / sec: 58592.31
Iteration:   2380, Loss function: 4.083, Average Loss: 4.344, avg. samples / sec: 58427.75
Iteration:   2380, Loss function: 4.297, Average Loss: 4.317, avg. samples / sec: 58539.90
Iteration:   2380, Loss function: 4.929, Average Loss: 4.325, avg. samples / sec: 58586.36
Iteration:   2380, Loss function: 3.779, Average Loss: 4.359, avg. samples / sec: 58410.99
Iteration:   2400, Loss function: 4.253, Average Loss: 4.341, avg. samples / sec: 57865.11
Iteration:   2400, Loss function: 3.769, Average Loss: 4.341, avg. samples / sec: 57709.71
Iteration:   2400, Loss function: 5.712, Average Loss: 4.340, avg. samples / sec: 57626.13
Iteration:   2400, Loss function: 2.992, Average Loss: 4.313, avg. samples / sec: 57643.00
Iteration:   2400, Loss function: 3.734, Average Loss: 4.342, avg. samples / sec: 57592.17
Iteration:   2400, Loss function: 4.535, Average Loss: 4.327, avg. samples / sec: 57793.89
Iteration:   2400, Loss function: 5.021, Average Loss: 4.357, avg. samples / sec: 57669.49
Iteration:   2400, Loss function: 3.027, Average Loss: 4.313, avg. samples / sec: 57587.44
Iteration:   2400, Loss function: 4.264, Average Loss: 4.356, avg. samples / sec: 57576.92
Iteration:   2400, Loss function: 5.585, Average Loss: 4.316, avg. samples / sec: 57756.63
Iteration:   2400, Loss function: 3.184, Average Loss: 4.347, avg. samples / sec: 57514.21
Iteration:   2400, Loss function: 3.581, Average Loss: 4.353, avg. samples / sec: 57730.70
Iteration:   2400, Loss function: 3.970, Average Loss: 4.341, avg. samples / sec: 57664.85
Iteration:   2400, Loss function: 4.309, Average Loss: 4.319, avg. samples / sec: 57539.80
Iteration:   2400, Loss function: 3.694, Average Loss: 4.337, avg. samples / sec: 57381.61
Iteration:   2420, Loss function: 5.337, Average Loss: 4.310, avg. samples / sec: 58301.96
Iteration:   2420, Loss function: 4.760, Average Loss: 4.318, avg. samples / sec: 58362.69
Iteration:   2420, Loss function: 3.436, Average Loss: 4.339, avg. samples / sec: 58100.21
Iteration:   2420, Loss function: 4.392, Average Loss: 4.325, avg. samples / sec: 58221.47
Iteration:   2420, Loss function: 4.740, Average Loss: 4.334, avg. samples / sec: 58324.14
Iteration:   2420, Loss function: 4.614, Average Loss: 4.340, avg. samples / sec: 58283.64
Iteration:   2420, Loss function: 4.774, Average Loss: 4.337, avg. samples / sec: 58089.26
Iteration:   2420, Loss function: 4.798, Average Loss: 4.335, avg. samples / sec: 58053.73
Iteration:   2420, Loss function: 3.094, Average Loss: 4.347, avg. samples / sec: 58263.74
Iteration:   2420, Loss function: 3.688, Average Loss: 4.330, avg. samples / sec: 58500.54
Iteration:   2420, Loss function: 4.682, Average Loss: 4.308, avg. samples / sec: 58156.00
Iteration:   2420, Loss function: 4.129, Average Loss: 4.310, avg. samples / sec: 58105.98
Iteration:   2420, Loss function: 3.639, Average Loss: 4.339, avg. samples / sec: 58084.28
Iteration:   2420, Loss function: 3.002, Average Loss: 4.352, avg. samples / sec: 58052.24
Iteration:   2420, Loss function: 3.622, Average Loss: 4.346, avg. samples / sec: 58025.57
Iteration:   2440, Loss function: 3.970, Average Loss: 4.326, avg. samples / sec: 57691.05
Iteration:   2440, Loss function: 2.749, Average Loss: 4.318, avg. samples / sec: 57601.89
Iteration:   2440, Loss function: 4.982, Average Loss: 4.343, avg. samples / sec: 57811.70
Iteration:   2440, Loss function: 4.936, Average Loss: 4.330, avg. samples / sec: 57635.67
Iteration:   2440, Loss function: 3.624, Average Loss: 4.301, avg. samples / sec: 57550.85
Iteration:   2440, Loss function: 5.114, Average Loss: 4.335, avg. samples / sec: 57566.36
Iteration:   2440, Loss function: 3.827, Average Loss: 4.340, avg. samples / sec: 57624.62
Iteration:   2440, Loss function: 4.193, Average Loss: 4.304, avg. samples / sec: 57628.72
Iteration:   2440, Loss function: 4.757, Average Loss: 4.331, avg. samples / sec: 57558.77
Iteration:   2440, Loss function: 2.337, Average Loss: 4.335, avg. samples / sec: 57677.71
Iteration:   2440, Loss function: 4.272, Average Loss: 4.334, avg. samples / sec: 57586.00
Iteration:   2440, Loss function: 3.151, Average Loss: 4.314, avg. samples / sec: 57459.62
Iteration:   2440, Loss function: 4.804, Average Loss: 4.337, avg. samples / sec: 57504.89
Iteration:   2440, Loss function: 4.305, Average Loss: 4.347, avg. samples / sec: 57630.35
Iteration:   2440, Loss function: 5.475, Average Loss: 4.310, avg. samples / sec: 57510.55
:::MLL 1558639456.696 epoch_stop: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 819}}
:::MLL 1558639456.696 epoch_start: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 673}}
Iteration:   2460, Loss function: 2.944, Average Loss: 4.343, avg. samples / sec: 59998.73
Iteration:   2460, Loss function: 4.709, Average Loss: 4.330, avg. samples / sec: 59991.99
Iteration:   2460, Loss function: 4.051, Average Loss: 4.300, avg. samples / sec: 59964.16
Iteration:   2460, Loss function: 4.633, Average Loss: 4.297, avg. samples / sec: 59916.77
Iteration:   2460, Loss function: 4.172, Average Loss: 4.334, avg. samples / sec: 59945.16
Iteration:   2460, Loss function: 4.292, Average Loss: 4.313, avg. samples / sec: 59877.79
Iteration:   2460, Loss function: 3.067, Average Loss: 4.341, avg. samples / sec: 60024.92
Iteration:   2460, Loss function: 5.336, Average Loss: 4.319, avg. samples / sec: 59804.79
Iteration:   2460, Loss function: 4.938, Average Loss: 4.307, avg. samples / sec: 59972.81
Iteration:   2460, Loss function: 4.111, Average Loss: 4.335, avg. samples / sec: 59853.58
Iteration:   2460, Loss function: 3.356, Average Loss: 4.312, avg. samples / sec: 59907.96
Iteration:   2460, Loss function: 6.463, Average Loss: 4.333, avg. samples / sec: 59898.99
Iteration:   2460, Loss function: 5.846, Average Loss: 4.336, avg. samples / sec: 59797.69
Iteration:   2460, Loss function: 4.447, Average Loss: 4.322, avg. samples / sec: 59716.93
Iteration:   2460, Loss function: 3.780, Average Loss: 4.323, avg. samples / sec: 59618.48
Iteration:   2480, Loss function: 4.257, Average Loss: 4.312, avg. samples / sec: 59139.95
Iteration:   2480, Loss function: 4.873, Average Loss: 4.329, avg. samples / sec: 59121.37
Iteration:   2480, Loss function: 4.033, Average Loss: 4.327, avg. samples / sec: 58827.05
Iteration:   2480, Loss function: 4.902, Average Loss: 4.332, avg. samples / sec: 58992.21
Iteration:   2480, Loss function: 4.991, Average Loss: 4.335, avg. samples / sec: 58886.49
Iteration:   2480, Loss function: 3.509, Average Loss: 4.302, avg. samples / sec: 58944.66
Iteration:   2480, Loss function: 5.413, Average Loss: 4.325, avg. samples / sec: 59080.52
Iteration:   2480, Loss function: 4.113, Average Loss: 4.297, avg. samples / sec: 58794.68
Iteration:   2480, Loss function: 4.222, Average Loss: 4.335, avg. samples / sec: 58940.35
Iteration:   2480, Loss function: 3.317, Average Loss: 4.315, avg. samples / sec: 59022.23
Iteration:   2480, Loss function: 4.476, Average Loss: 4.339, avg. samples / sec: 58680.55
Iteration:   2480, Loss function: 4.376, Average Loss: 4.294, avg. samples / sec: 58723.58
Iteration:   2480, Loss function: 3.201, Average Loss: 4.316, avg. samples / sec: 58789.11
Iteration:   2480, Loss function: 4.524, Average Loss: 4.325, avg. samples / sec: 58688.86
Iteration:   2480, Loss function: 3.168, Average Loss: 4.307, avg. samples / sec: 58657.42
Iteration:   2500, Loss function: 3.996, Average Loss: 4.313, avg. samples / sec: 57645.31
Iteration:   2500, Loss function: 4.244, Average Loss: 4.331, avg. samples / sec: 57695.18
Iteration:   2500, Loss function: 4.144, Average Loss: 4.305, avg. samples / sec: 57934.26
Iteration:   2500, Loss function: 2.987, Average Loss: 4.304, avg. samples / sec: 57756.68
Iteration:   2500, Loss function: 3.571, Average Loss: 4.321, avg. samples / sec: 57649.87
Iteration:   2500, Loss function: 3.173, Average Loss: 4.320, avg. samples / sec: 57551.18
Iteration:   2500, Loss function: 5.642, Average Loss: 4.284, avg. samples / sec: 57764.97
Iteration:   2500, Loss function: 4.907, Average Loss: 4.312, avg. samples / sec: 57783.11
Iteration:   2500, Loss function: 4.332, Average Loss: 4.330, avg. samples / sec: 57588.66
Iteration:   2500, Loss function: 3.820, Average Loss: 4.329, avg. samples / sec: 57743.88
Iteration:   2500, Loss function: 4.293, Average Loss: 4.302, avg. samples / sec: 57581.06
Iteration:   2500, Loss function: 4.240, Average Loss: 4.334, avg. samples / sec: 57625.42
Iteration:   2500, Loss function: 4.129, Average Loss: 4.331, avg. samples / sec: 57556.32
Iteration:   2500, Loss function: 4.512, Average Loss: 4.321, avg. samples / sec: 57421.50
Iteration:   2500, Loss function: 3.922, Average Loss: 4.292, avg. samples / sec: 57025.21
:::MLL 1558639458.696 epoch_stop: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 819}}
:::MLL 1558639458.696 epoch_start: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 673}}
Iteration:   2520, Loss function: 3.015, Average Loss: 4.296, avg. samples / sec: 59664.82
Iteration:   2520, Loss function: 5.580, Average Loss: 4.317, avg. samples / sec: 59925.43
Iteration:   2520, Loss function: 3.785, Average Loss: 4.296, avg. samples / sec: 59559.02
Iteration:   2520, Loss function: 5.104, Average Loss: 4.322, avg. samples / sec: 59698.16
Iteration:   2520, Loss function: 3.825, Average Loss: 4.324, avg. samples / sec: 59775.26
Iteration:   2520, Loss function: 4.049, Average Loss: 4.299, avg. samples / sec: 59696.37
Iteration:   2520, Loss function: 3.606, Average Loss: 4.315, avg. samples / sec: 59537.73
Iteration:   2520, Loss function: 3.240, Average Loss: 4.286, avg. samples / sec: 60252.55
Iteration:   2520, Loss function: 4.211, Average Loss: 4.304, avg. samples / sec: 59546.97
Iteration:   2520, Loss function: 5.724, Average Loss: 4.327, avg. samples / sec: 59436.01
Iteration:   2520, Loss function: 4.195, Average Loss: 4.315, avg. samples / sec: 59443.26
Iteration:   2520, Loss function: 4.812, Average Loss: 4.331, avg. samples / sec: 59528.03
Iteration:   2520, Loss function: 5.265, Average Loss: 4.330, avg. samples / sec: 59485.96
Iteration:   2520, Loss function: 4.659, Average Loss: 4.306, avg. samples / sec: 59174.57
Iteration:   2520, Loss function: 5.308, Average Loss: 4.278, avg. samples / sec: 59341.41
Iteration:   2540, Loss function: 6.031, Average Loss: 4.285, avg. samples / sec: 57129.60
Iteration:   2540, Loss function: 3.467, Average Loss: 4.295, avg. samples / sec: 57040.81
Iteration:   2540, Loss function: 3.936, Average Loss: 4.329, avg. samples / sec: 57160.00
Iteration:   2540, Loss function: 5.209, Average Loss: 4.324, avg. samples / sec: 57097.24
Iteration:   2540, Loss function: 4.012, Average Loss: 4.293, avg. samples / sec: 56940.49
Iteration:   2540, Loss function: 2.499, Average Loss: 4.312, avg. samples / sec: 56918.75
Iteration:   2540, Loss function: 3.852, Average Loss: 4.275, avg. samples / sec: 57195.38
Iteration:   2540, Loss function: 3.997, Average Loss: 4.318, avg. samples / sec: 56911.38
Iteration:   2540, Loss function: 4.650, Average Loss: 4.301, avg. samples / sec: 57157.45
Iteration:   2540, Loss function: 4.070, Average Loss: 4.312, avg. samples / sec: 56950.17
Iteration:   2540, Loss function: 3.080, Average Loss: 4.297, avg. samples / sec: 56969.01
Iteration:   2540, Loss function: 3.444, Average Loss: 4.298, avg. samples / sec: 56794.31
Iteration:   2540, Loss function: 4.456, Average Loss: 4.318, avg. samples / sec: 56856.68
Iteration:   2540, Loss function: 4.263, Average Loss: 4.312, avg. samples / sec: 56973.57
Iteration:   2540, Loss function: 3.110, Average Loss: 4.321, avg. samples / sec: 57024.58
Iteration:   2560, Loss function: 3.914, Average Loss: 4.290, avg. samples / sec: 57385.14
Iteration:   2560, Loss function: 4.365, Average Loss: 4.306, avg. samples / sec: 57283.68
Iteration:   2560, Loss function: 3.769, Average Loss: 4.320, avg. samples / sec: 57196.22
Iteration:   2560, Loss function: 3.792, Average Loss: 4.314, avg. samples / sec: 57403.12
Iteration:   2560, Loss function: 3.759, Average Loss: 4.316, avg. samples / sec: 57203.37
Iteration:   2560, Loss function: 4.082, Average Loss: 4.319, avg. samples / sec: 57267.22
Iteration:   2560, Loss function: 4.301, Average Loss: 4.271, avg. samples / sec: 57250.59
Iteration:   2560, Loss function: 4.987, Average Loss: 4.312, avg. samples / sec: 57314.03
Iteration:   2560, Loss function: 3.482, Average Loss: 4.309, avg. samples / sec: 57263.43
Iteration:   2560, Loss function: 5.189, Average Loss: 4.298, avg. samples / sec: 57257.94
Iteration:   2560, Loss function: 3.744, Average Loss: 4.287, avg. samples / sec: 57180.60
Iteration:   2560, Loss function: 4.024, Average Loss: 4.291, avg. samples / sec: 57053.49
Iteration:   2560, Loss function: 4.234, Average Loss: 4.296, avg. samples / sec: 57207.41
Iteration:   2560, Loss function: 4.065, Average Loss: 4.281, avg. samples / sec: 57035.59
Iteration:   2560, Loss function: 4.481, Average Loss: 4.317, avg. samples / sec: 57181.22
Iteration:   2580, Loss function: 4.459, Average Loss: 4.286, avg. samples / sec: 58901.50
Iteration:   2580, Loss function: 3.255, Average Loss: 4.261, avg. samples / sec: 58736.46
Iteration:   2580, Loss function: 4.192, Average Loss: 4.303, avg. samples / sec: 58653.76
Iteration:   2580, Loss function: 4.571, Average Loss: 4.288, avg. samples / sec: 58797.04
Iteration:   2580, Loss function: 4.964, Average Loss: 4.317, avg. samples / sec: 58785.73
Iteration:   2580, Loss function: 3.321, Average Loss: 4.279, avg. samples / sec: 58702.82
Iteration:   2580, Loss function: 3.139, Average Loss: 4.285, avg. samples / sec: 58683.95
Iteration:   2580, Loss function: 3.579, Average Loss: 4.295, avg. samples / sec: 58587.51
Iteration:   2580, Loss function: 2.980, Average Loss: 4.310, avg. samples / sec: 58558.12
Iteration:   2580, Loss function: 3.126, Average Loss: 4.318, avg. samples / sec: 58540.49
Iteration:   2580, Loss function: 3.817, Average Loss: 4.309, avg. samples / sec: 58552.33
Iteration:   2580, Loss function: 2.932, Average Loss: 4.313, avg. samples / sec: 58493.71
Iteration:   2580, Loss function: 4.040, Average Loss: 4.286, avg. samples / sec: 58425.45
Iteration:   2580, Loss function: 4.147, Average Loss: 4.311, avg. samples / sec: 58461.52
Iteration:   2580, Loss function: 4.270, Average Loss: 4.310, avg. samples / sec: 58269.66
:::MLL 1558639460.731 epoch_stop: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 819}}
:::MLL 1558639460.732 epoch_start: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 673}}
Iteration:   2600, Loss function: 4.688, Average Loss: 4.282, avg. samples / sec: 57624.62
Iteration:   2600, Loss function: 2.347, Average Loss: 4.288, avg. samples / sec: 57444.35
Iteration:   2600, Loss function: 4.127, Average Loss: 4.280, avg. samples / sec: 57581.20
Iteration:   2600, Loss function: 3.970, Average Loss: 4.260, avg. samples / sec: 57366.01
Iteration:   2600, Loss function: 4.300, Average Loss: 4.281, avg. samples / sec: 57367.69
Iteration:   2600, Loss function: 4.755, Average Loss: 4.302, avg. samples / sec: 57774.51
Iteration:   2600, Loss function: 4.596, Average Loss: 4.308, avg. samples / sec: 57500.27
Iteration:   2600, Loss function: 4.777, Average Loss: 4.308, avg. samples / sec: 57549.93
Iteration:   2600, Loss function: 3.622, Average Loss: 4.282, avg. samples / sec: 57495.18
Iteration:   2600, Loss function: 4.748, Average Loss: 4.284, avg. samples / sec: 57148.27
Iteration:   2600, Loss function: 4.985, Average Loss: 4.288, avg. samples / sec: 57436.15
Iteration:   2600, Loss function: 3.367, Average Loss: 4.314, avg. samples / sec: 57453.69
Iteration:   2600, Loss function: 5.002, Average Loss: 4.318, avg. samples / sec: 57341.60
Iteration:   2600, Loss function: 4.729, Average Loss: 4.309, avg. samples / sec: 57405.29
Iteration:   2600, Loss function: 3.893, Average Loss: 4.304, avg. samples / sec: 57338.70
Iteration:   2620, Loss function: 3.692, Average Loss: 4.276, avg. samples / sec: 59783.15
Iteration:   2620, Loss function: 4.738, Average Loss: 4.310, avg. samples / sec: 60012.96
Iteration:   2620, Loss function: 4.506, Average Loss: 4.287, avg. samples / sec: 59803.90
Iteration:   2620, Loss function: 2.292, Average Loss: 4.274, avg. samples / sec: 59729.21
Iteration:   2620, Loss function: 4.887, Average Loss: 4.283, avg. samples / sec: 59584.48
Iteration:   2620, Loss function: 3.905, Average Loss: 4.310, avg. samples / sec: 59775.24
Iteration:   2620, Loss function: 3.629, Average Loss: 4.279, avg. samples / sec: 59708.94
Iteration:   2620, Loss function: 3.478, Average Loss: 4.297, avg. samples / sec: 59837.14
Iteration:   2620, Loss function: 3.500, Average Loss: 4.301, avg. samples / sec: 59679.88
Iteration:   2620, Loss function: 4.673, Average Loss: 4.281, avg. samples / sec: 59686.18
Iteration:   2620, Loss function: 3.859, Average Loss: 4.272, avg. samples / sec: 59501.51
Iteration:   2620, Loss function: 3.667, Average Loss: 4.310, avg. samples / sec: 59668.01
Iteration:   2620, Loss function: 4.109, Average Loss: 4.258, avg. samples / sec: 59540.02
Iteration:   2620, Loss function: 3.887, Average Loss: 4.301, avg. samples / sec: 59541.23
Iteration:   2620, Loss function: 3.029, Average Loss: 4.293, avg. samples / sec: 59439.62
Iteration:   2640, Loss function: 3.814, Average Loss: 4.295, avg. samples / sec: 57315.76
Iteration:   2640, Loss function: 4.912, Average Loss: 4.307, avg. samples / sec: 57151.33
Iteration:   2640, Loss function: 2.997, Average Loss: 4.271, avg. samples / sec: 57169.58
Iteration:   2640, Loss function: 5.067, Average Loss: 4.309, avg. samples / sec: 56933.95
Iteration:   2640, Loss function: 3.695, Average Loss: 4.294, avg. samples / sec: 57153.70
Iteration:   2640, Loss function: 3.489, Average Loss: 4.266, avg. samples / sec: 57163.39
Iteration:   2640, Loss function: 4.275, Average Loss: 4.275, avg. samples / sec: 57086.93
Iteration:   2640, Loss function: 4.804, Average Loss: 4.283, avg. samples / sec: 57269.27
Iteration:   2640, Loss function: 4.395, Average Loss: 4.273, avg. samples / sec: 56798.23
Iteration:   2640, Loss function: 3.941, Average Loss: 4.307, avg. samples / sec: 57112.70
Iteration:   2640, Loss function: 3.902, Average Loss: 4.266, avg. samples / sec: 56980.22
Iteration:   2640, Loss function: 3.679, Average Loss: 4.294, avg. samples / sec: 57041.99
Iteration:   2640, Loss function: 3.403, Average Loss: 4.252, avg. samples / sec: 57092.78
Iteration:   2640, Loss function: 4.080, Average Loss: 4.283, avg. samples / sec: 56939.08
Iteration:   2640, Loss function: 4.057, Average Loss: 4.272, avg. samples / sec: 56851.27
:::MLL 1558639462.749 epoch_stop: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 819}}
:::MLL 1558639462.750 epoch_start: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 673}}
Iteration:   2660, Loss function: 3.871, Average Loss: 4.300, avg. samples / sec: 59470.02
Iteration:   2660, Loss function: 3.939, Average Loss: 4.289, avg. samples / sec: 59338.81
Iteration:   2660, Loss function: 3.115, Average Loss: 4.283, avg. samples / sec: 59338.54
Iteration:   2660, Loss function: 4.144, Average Loss: 4.275, avg. samples / sec: 59451.18
Iteration:   2660, Loss function: 4.556, Average Loss: 4.303, avg. samples / sec: 59276.94
Iteration:   2660, Loss function: 3.417, Average Loss: 4.276, avg. samples / sec: 59276.62
Iteration:   2660, Loss function: 4.622, Average Loss: 4.265, avg. samples / sec: 59299.04
Iteration:   2660, Loss function: 3.206, Average Loss: 4.301, avg. samples / sec: 59162.50
Iteration:   2660, Loss function: 4.175, Average Loss: 4.259, avg. samples / sec: 59195.42
Iteration:   2660, Loss function: 4.130, Average Loss: 4.262, avg. samples / sec: 59205.05
Iteration:   2660, Loss function: 4.121, Average Loss: 4.269, avg. samples / sec: 59441.53
Iteration:   2660, Loss function: 4.416, Average Loss: 4.295, avg. samples / sec: 59257.47
Iteration:   2660, Loss function: 3.951, Average Loss: 4.272, avg. samples / sec: 59062.23
Iteration:   2660, Loss function: 3.666, Average Loss: 4.251, avg. samples / sec: 59112.81
Iteration:   2660, Loss function: 4.392, Average Loss: 4.267, avg. samples / sec: 58414.65
Iteration:   2680, Loss function: 4.802, Average Loss: 4.262, avg. samples / sec: 58443.55
Iteration:   2680, Loss function: 4.016, Average Loss: 4.259, avg. samples / sec: 57603.33
Iteration:   2680, Loss function: 4.353, Average Loss: 4.284, avg. samples / sec: 57387.34
Iteration:   2680, Loss function: 4.255, Average Loss: 4.290, avg. samples / sec: 57608.04
Iteration:   2680, Loss function: 3.402, Average Loss: 4.276, avg. samples / sec: 57380.35
Iteration:   2680, Loss function: 3.979, Average Loss: 4.260, avg. samples / sec: 57512.92
Iteration:   2680, Loss function: 4.720, Average Loss: 4.292, avg. samples / sec: 57515.22
Iteration:   2680, Loss function: 4.991, Average Loss: 4.267, avg. samples / sec: 57380.21
Iteration:   2680, Loss function: 2.974, Average Loss: 4.296, avg. samples / sec: 57388.18
Iteration:   2680, Loss function: 3.299, Average Loss: 4.244, avg. samples / sec: 57643.50
Iteration:   2680, Loss function: 2.828, Average Loss: 4.286, avg. samples / sec: 57267.57
Iteration:   2680, Loss function: 4.263, Average Loss: 4.252, avg. samples / sec: 57406.86
Iteration:   2680, Loss function: 4.022, Average Loss: 4.271, avg. samples / sec: 57603.80
Iteration:   2680, Loss function: 4.078, Average Loss: 4.266, avg. samples / sec: 57421.71
Iteration:   2680, Loss function: 4.040, Average Loss: 4.270, avg. samples / sec: 57335.41
Iteration:   2700, Loss function: 4.122, Average Loss: 4.265, avg. samples / sec: 59627.89
Iteration:   2700, Loss function: 4.338, Average Loss: 4.272, avg. samples / sec: 59401.59
Iteration:   2700, Loss function: 3.360, Average Loss: 4.249, avg. samples / sec: 59399.26
Iteration:   2700, Loss function: 4.098, Average Loss: 4.259, avg. samples / sec: 59542.46
Iteration:   2700, Loss function: 3.689, Average Loss: 4.245, avg. samples / sec: 59436.97
Iteration:   2700, Loss function: 2.974, Average Loss: 4.259, avg. samples / sec: 59336.17
Iteration:   2700, Loss function: 3.419, Average Loss: 4.283, avg. samples / sec: 59262.16
Iteration:   2700, Loss function: 3.045, Average Loss: 4.286, avg. samples / sec: 59262.21
Iteration:   2700, Loss function: 4.476, Average Loss: 4.280, avg. samples / sec: 59393.38
Iteration:   2700, Loss function: 3.367, Average Loss: 4.261, avg. samples / sec: 59167.59
Iteration:   2700, Loss function: 3.295, Average Loss: 4.284, avg. samples / sec: 59272.58
Iteration:   2700, Loss function: 3.462, Average Loss: 4.247, avg. samples / sec: 59361.73
Iteration:   2700, Loss function: 4.891, Average Loss: 4.252, avg. samples / sec: 59168.93
Iteration:   2700, Loss function: 2.543, Average Loss: 4.287, avg. samples / sec: 59274.87
Iteration:   2700, Loss function: 4.161, Average Loss: 4.267, avg. samples / sec: 59236.40
Iteration:   2720, Loss function: 3.989, Average Loss: 4.246, avg. samples / sec: 60501.89
Iteration:   2720, Loss function: 2.852, Average Loss: 4.267, avg. samples / sec: 60408.55
Iteration:   2720, Loss function: 4.626, Average Loss: 4.287, avg. samples / sec: 60476.31
Iteration:   2720, Loss function: 4.809, Average Loss: 4.287, avg. samples / sec: 60549.32
Iteration:   2720, Loss function: 4.431, Average Loss: 4.246, avg. samples / sec: 60519.53
Iteration:   2720, Loss function: 5.200, Average Loss: 4.252, avg. samples / sec: 60452.50
Iteration:   2720, Loss function: 3.597, Average Loss: 4.255, avg. samples / sec: 60438.94
Iteration:   2720, Loss function: 4.491, Average Loss: 4.263, avg. samples / sec: 60609.45
Iteration:   2720, Loss function: 4.609, Average Loss: 4.255, avg. samples / sec: 60295.73
Iteration:   2720, Loss function: 2.574, Average Loss: 4.247, avg. samples / sec: 60395.42
Iteration:   2720, Loss function: 5.307, Average Loss: 4.279, avg. samples / sec: 60339.47
Iteration:   2720, Loss function: 3.993, Average Loss: 4.257, avg. samples / sec: 60100.33
Iteration:   2720, Loss function: 3.668, Average Loss: 4.245, avg. samples / sec: 60189.82
Iteration:   2720, Loss function: 3.677, Average Loss: 4.275, avg. samples / sec: 60217.77
Iteration:   2720, Loss function: 2.975, Average Loss: 4.280, avg. samples / sec: 60205.60
:::MLL 1558639464.743 epoch_stop: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 819}}
:::MLL 1558639464.743 epoch_start: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 673}}
Iteration:   2740, Loss function: 4.674, Average Loss: 4.244, avg. samples / sec: 58776.22
Iteration:   2740, Loss function: 4.237, Average Loss: 4.274, avg. samples / sec: 58906.06
Iteration:   2740, Loss function: 4.087, Average Loss: 4.282, avg. samples / sec: 58602.81
Iteration:   2740, Loss function: 4.065, Average Loss: 4.247, avg. samples / sec: 58500.10
Iteration:   2740, Loss function: 3.396, Average Loss: 4.237, avg. samples / sec: 58619.19
Iteration:   2740, Loss function: 3.251, Average Loss: 4.261, avg. samples / sec: 58451.26
Iteration:   2740, Loss function: 4.054, Average Loss: 4.265, avg. samples / sec: 58701.52
Iteration:   2740, Loss function: 3.824, Average Loss: 4.253, avg. samples / sec: 58688.12
Iteration:   2740, Loss function: 4.533, Average Loss: 4.271, avg. samples / sec: 58615.75
Iteration:   2740, Loss function: 3.727, Average Loss: 4.237, avg. samples / sec: 58649.19
Iteration:   2740, Loss function: 4.663, Average Loss: 4.284, avg. samples / sec: 58428.75
Iteration:   2740, Loss function: 3.962, Average Loss: 4.249, avg. samples / sec: 58399.59
Iteration:   2740, Loss function: 3.714, Average Loss: 4.260, avg. samples / sec: 58414.31
Iteration:   2740, Loss function: 3.270, Average Loss: 4.239, avg. samples / sec: 58159.74
Iteration:   2740, Loss function: 3.826, Average Loss: 4.245, avg. samples / sec: 58286.39
Iteration:   2760, Loss function: 3.505, Average Loss: 4.276, avg. samples / sec: 57099.90
Iteration:   2760, Loss function: 2.647, Average Loss: 4.270, avg. samples / sec: 57047.94
Iteration:   2760, Loss function: 4.244, Average Loss: 4.231, avg. samples / sec: 57143.11
Iteration:   2760, Loss function: 3.244, Average Loss: 4.278, avg. samples / sec: 57204.29
Iteration:   2760, Loss function: 3.966, Average Loss: 4.234, avg. samples / sec: 57150.82
Iteration:   2760, Loss function: 3.260, Average Loss: 4.240, avg. samples / sec: 57302.36
Iteration:   2760, Loss function: 3.117, Average Loss: 4.264, avg. samples / sec: 57051.68
Iteration:   2760, Loss function: 3.920, Average Loss: 4.241, avg. samples / sec: 57012.72
Iteration:   2760, Loss function: 3.851, Average Loss: 4.240, avg. samples / sec: 56749.88
Iteration:   2760, Loss function: 3.966, Average Loss: 4.263, avg. samples / sec: 57003.85
Iteration:   2760, Loss function: 3.770, Average Loss: 4.250, avg. samples / sec: 56987.27
Iteration:   2760, Loss function: 3.988, Average Loss: 4.251, avg. samples / sec: 57092.64
Iteration:   2760, Loss function: 3.683, Average Loss: 4.249, avg. samples / sec: 57053.05
Iteration:   2760, Loss function: 3.019, Average Loss: 4.254, avg. samples / sec: 56889.32
Iteration:   2760, Loss function: 3.108, Average Loss: 4.235, avg. samples / sec: 57075.18
Iteration:   2780, Loss function: 4.262, Average Loss: 4.265, avg. samples / sec: 60236.05
Iteration:   2780, Loss function: 3.871, Average Loss: 4.245, avg. samples / sec: 60496.38
Iteration:   2780, Loss function: 4.415, Average Loss: 4.253, avg. samples / sec: 60537.39
Iteration:   2780, Loss function: 4.880, Average Loss: 4.228, avg. samples / sec: 60524.21
Iteration:   2780, Loss function: 4.789, Average Loss: 4.235, avg. samples / sec: 60363.84
Iteration:   2780, Loss function: 3.653, Average Loss: 4.270, avg. samples / sec: 60144.30
Iteration:   2780, Loss function: 3.461, Average Loss: 4.261, avg. samples / sec: 60402.31
Iteration:   2780, Loss function: 3.840, Average Loss: 4.247, avg. samples / sec: 60411.27
Iteration:   2780, Loss function: 3.938, Average Loss: 4.235, avg. samples / sec: 60252.22
Iteration:   2780, Loss function: 4.102, Average Loss: 4.246, avg. samples / sec: 60298.78
Iteration:   2780, Loss function: 4.245, Average Loss: 4.235, avg. samples / sec: 60209.02
Iteration:   2780, Loss function: 3.661, Average Loss: 4.235, avg. samples / sec: 60276.60
Iteration:   2780, Loss function: 4.319, Average Loss: 4.231, avg. samples / sec: 60063.94
Iteration:   2780, Loss function: 4.376, Average Loss: 4.277, avg. samples / sec: 60023.59
Iteration:   2780, Loss function: 4.949, Average Loss: 4.258, avg. samples / sec: 59904.95
:::MLL 1558639466.751 epoch_stop: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 819}}
:::MLL 1558639466.751 epoch_start: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 673}}
Iteration:   2800, Loss function: 2.460, Average Loss: 4.248, avg. samples / sec: 58740.11
Iteration:   2800, Loss function: 4.549, Average Loss: 4.254, avg. samples / sec: 59026.46
Iteration:   2800, Loss function: 3.640, Average Loss: 4.233, avg. samples / sec: 58601.12
Iteration:   2800, Loss function: 4.148, Average Loss: 4.264, avg. samples / sec: 58536.57
Iteration:   2800, Loss function: 4.679, Average Loss: 4.256, avg. samples / sec: 58523.40
Iteration:   2800, Loss function: 2.641, Average Loss: 4.226, avg. samples / sec: 58639.43
Iteration:   2800, Loss function: 5.135, Average Loss: 4.224, avg. samples / sec: 58593.35
Iteration:   2800, Loss function: 2.084, Average Loss: 4.235, avg. samples / sec: 58555.62
Iteration:   2800, Loss function: 4.261, Average Loss: 4.226, avg. samples / sec: 58437.66
Iteration:   2800, Loss function: 5.766, Average Loss: 4.275, avg. samples / sec: 58575.89
Iteration:   2800, Loss function: 3.595, Average Loss: 4.239, avg. samples / sec: 58392.55
Iteration:   2800, Loss function: 3.297, Average Loss: 4.231, avg. samples / sec: 58492.91
Iteration:   2800, Loss function: 4.221, Average Loss: 4.241, avg. samples / sec: 58291.57
Iteration:   2800, Loss function: 3.948, Average Loss: 4.264, avg. samples / sec: 58273.54
Iteration:   2800, Loss function: 3.354, Average Loss: 4.228, avg. samples / sec: 58036.49
Iteration:   2820, Loss function: 3.268, Average Loss: 4.243, avg. samples / sec: 58806.90
Iteration:   2820, Loss function: 4.044, Average Loss: 4.228, avg. samples / sec: 59137.72
Iteration:   2820, Loss function: 4.562, Average Loss: 4.229, avg. samples / sec: 58898.05
Iteration:   2820, Loss function: 4.286, Average Loss: 4.237, avg. samples / sec: 59094.50
Iteration:   2820, Loss function: 2.668, Average Loss: 4.230, avg. samples / sec: 58981.62
Iteration:   2820, Loss function: 3.852, Average Loss: 4.258, avg. samples / sec: 59042.38
Iteration:   2820, Loss function: 4.533, Average Loss: 4.231, avg. samples / sec: 59314.91
Iteration:   2820, Loss function: 4.368, Average Loss: 4.219, avg. samples / sec: 58891.75
Iteration:   2820, Loss function: 3.539, Average Loss: 4.220, avg. samples / sec: 58818.34
Iteration:   2820, Loss function: 3.963, Average Loss: 4.250, avg. samples / sec: 58693.33
Iteration:   2820, Loss function: 3.698, Average Loss: 4.273, avg. samples / sec: 58864.20
Iteration:   2820, Loss function: 3.716, Average Loss: 4.234, avg. samples / sec: 58760.16
Iteration:   2820, Loss function: 3.453, Average Loss: 4.250, avg. samples / sec: 58584.97
Iteration:   2820, Loss function: 3.541, Average Loss: 4.258, avg. samples / sec: 58518.81
Iteration:   2820, Loss function: 4.559, Average Loss: 4.220, avg. samples / sec: 58564.28
Iteration:   2840, Loss function: 3.772, Average Loss: 4.256, avg. samples / sec: 58305.51
Iteration:   2840, Loss function: 4.034, Average Loss: 4.215, avg. samples / sec: 58574.97
Iteration:   2840, Loss function: 6.033, Average Loss: 4.231, avg. samples / sec: 58183.06
Iteration:   2840, Loss function: 4.190, Average Loss: 4.227, avg. samples / sec: 58118.27
Iteration:   2840, Loss function: 4.453, Average Loss: 4.230, avg. samples / sec: 58437.73
Iteration:   2840, Loss function: 3.979, Average Loss: 4.223, avg. samples / sec: 58195.91
Iteration:   2840, Loss function: 3.052, Average Loss: 4.244, avg. samples / sec: 58246.01
Iteration:   2840, Loss function: 3.669, Average Loss: 4.236, avg. samples / sec: 57952.53
Iteration:   2840, Loss function: 4.071, Average Loss: 4.224, avg. samples / sec: 58008.99
Iteration:   2840, Loss function: 3.533, Average Loss: 4.215, avg. samples / sec: 58131.12
Iteration:   2840, Loss function: 3.327, Average Loss: 4.210, avg. samples / sec: 58183.73
Iteration:   2840, Loss function: 3.926, Average Loss: 4.265, avg. samples / sec: 58202.09
Iteration:   2840, Loss function: 5.566, Average Loss: 4.256, avg. samples / sec: 58360.25
Iteration:   2840, Loss function: 4.419, Average Loss: 4.245, avg. samples / sec: 58285.57
Iteration:   2840, Loss function: 3.749, Average Loss: 4.231, avg. samples / sec: 57888.61
Iteration:   2860, Loss function: 2.913, Average Loss: 4.238, avg. samples / sec: 59906.40
Iteration:   2860, Loss function: 4.391, Average Loss: 4.251, avg. samples / sec: 59725.76
Iteration:   2860, Loss function: 4.429, Average Loss: 4.260, avg. samples / sec: 59979.48
Iteration:   2860, Loss function: 3.382, Average Loss: 4.225, avg. samples / sec: 59809.66
Iteration:   2860, Loss function: 2.595, Average Loss: 4.232, avg. samples / sec: 59880.34
Iteration:   2860, Loss function: 4.347, Average Loss: 4.225, avg. samples / sec: 60073.13
Iteration:   2860, Loss function: 3.822, Average Loss: 4.222, avg. samples / sec: 59758.00
Iteration:   2860, Loss function: 4.585, Average Loss: 4.257, avg. samples / sec: 59846.77
Iteration:   2860, Loss function: 4.054, Average Loss: 4.212, avg. samples / sec: 59791.32
Iteration:   2860, Loss function: 4.013, Average Loss: 4.239, avg. samples / sec: 59858.80
Iteration:   2860, Loss function: 4.738, Average Loss: 4.228, avg. samples / sec: 59636.42
Iteration:   2860, Loss function: 3.713, Average Loss: 4.218, avg. samples / sec: 59703.90
Iteration:   2860, Loss function: 2.986, Average Loss: 4.211, avg. samples / sec: 59571.03
Iteration:   2860, Loss function: 4.535, Average Loss: 4.229, avg. samples / sec: 59605.07
Iteration:   2860, Loss function: 6.135, Average Loss: 4.207, avg. samples / sec: 59667.28
:::MLL 1558639468.745 epoch_stop: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 819}}
:::MLL 1558639468.745 epoch_start: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 673}}
Iteration:   2880, Loss function: 3.424, Average Loss: 4.226, avg. samples / sec: 59540.20
Iteration:   2880, Loss function: 4.159, Average Loss: 4.229, avg. samples / sec: 59494.13
Iteration:   2880, Loss function: 4.315, Average Loss: 4.222, avg. samples / sec: 59706.51
Iteration:   2880, Loss function: 3.307, Average Loss: 4.252, avg. samples / sec: 59652.20
Iteration:   2880, Loss function: 3.832, Average Loss: 4.208, avg. samples / sec: 59718.88
Iteration:   2880, Loss function: 3.537, Average Loss: 4.227, avg. samples / sec: 59486.09
Iteration:   2880, Loss function: 4.369, Average Loss: 4.214, avg. samples / sec: 59532.48
Iteration:   2880, Loss function: 4.171, Average Loss: 4.249, avg. samples / sec: 59358.56
Iteration:   2880, Loss function: 3.773, Average Loss: 4.224, avg. samples / sec: 59593.58
Iteration:   2880, Loss function: 3.428, Average Loss: 4.218, avg. samples / sec: 59465.61
Iteration:   2880, Loss function: 4.334, Average Loss: 4.199, avg. samples / sec: 59623.81
Iteration:   2880, Loss function: 3.067, Average Loss: 4.252, avg. samples / sec: 59274.05
Iteration:   2880, Loss function: 3.880, Average Loss: 4.222, avg. samples / sec: 59281.55
Iteration:   2880, Loss function: 4.966, Average Loss: 4.213, avg. samples / sec: 59466.79
Iteration:   2880, Loss function: 2.808, Average Loss: 4.235, avg. samples / sec: 59440.88
Iteration:   2900, Loss function: 3.409, Average Loss: 4.215, avg. samples / sec: 56533.27
Iteration:   2900, Loss function: 4.858, Average Loss: 4.201, avg. samples / sec: 56552.64
Iteration:   2900, Loss function: 3.888, Average Loss: 4.224, avg. samples / sec: 56387.60
Iteration:   2900, Loss function: 5.173, Average Loss: 4.226, avg. samples / sec: 56517.40
Iteration:   2900, Loss function: 3.786, Average Loss: 4.250, avg. samples / sec: 56387.19
Iteration:   2900, Loss function: 5.690, Average Loss: 4.229, avg. samples / sec: 56358.01
Iteration:   2900, Loss function: 4.766, Average Loss: 4.246, avg. samples / sec: 56491.61
Iteration:   2900, Loss function: 2.096, Average Loss: 4.209, avg. samples / sec: 56348.68
Iteration:   2900, Loss function: 3.451, Average Loss: 4.249, avg. samples / sec: 56542.88
Iteration:   2900, Loss function: 4.959, Average Loss: 4.224, avg. samples / sec: 56545.86
Iteration:   2900, Loss function: 3.893, Average Loss: 4.211, avg. samples / sec: 56482.20
Iteration:   2900, Loss function: 3.540, Average Loss: 4.208, avg. samples / sec: 56306.69
Iteration:   2900, Loss function: 4.731, Average Loss: 4.206, avg. samples / sec: 56511.71
Iteration:   2900, Loss function: 2.594, Average Loss: 4.231, avg. samples / sec: 56465.63
Iteration:   2900, Loss function: 5.054, Average Loss: 4.228, avg. samples / sec: 56231.88
Iteration:   2920, Loss function: 4.237, Average Loss: 4.220, avg. samples / sec: 58780.56
Iteration:   2920, Loss function: 3.919, Average Loss: 4.247, avg. samples / sec: 58750.44
Iteration:   2920, Loss function: 4.178, Average Loss: 4.205, avg. samples / sec: 58715.66
Iteration:   2920, Loss function: 4.150, Average Loss: 4.224, avg. samples / sec: 58668.90
Iteration:   2920, Loss function: 3.317, Average Loss: 4.213, avg. samples / sec: 58652.83
Iteration:   2920, Loss function: 3.294, Average Loss: 4.221, avg. samples / sec: 58680.23
Iteration:   2920, Loss function: 3.071, Average Loss: 4.203, avg. samples / sec: 58745.35
Iteration:   2920, Loss function: 4.351, Average Loss: 4.219, avg. samples / sec: 58855.03
Iteration:   2920, Loss function: 4.063, Average Loss: 4.245, avg. samples / sec: 58641.41
Iteration:   2920, Loss function: 3.212, Average Loss: 4.198, avg. samples / sec: 58698.02
Iteration:   2920, Loss function: 4.280, Average Loss: 4.244, avg. samples / sec: 58604.56
Iteration:   2920, Loss function: 4.833, Average Loss: 4.206, avg. samples / sec: 58626.43
Iteration:   2920, Loss function: 4.609, Average Loss: 4.218, avg. samples / sec: 58553.77
Iteration:   2920, Loss function: 4.014, Average Loss: 4.228, avg. samples / sec: 58716.17
Iteration:   2920, Loss function: 4.701, Average Loss: 4.200, avg. samples / sec: 58443.89
:::MLL 1558639470.763 epoch_stop: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 819}}
:::MLL 1558639470.764 epoch_start: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 673}}
Iteration:   2940, Loss function: 4.677, Average Loss: 4.216, avg. samples / sec: 59247.14
Iteration:   2940, Loss function: 3.635, Average Loss: 4.200, avg. samples / sec: 59186.62
Iteration:   2940, Loss function: 5.046, Average Loss: 4.238, avg. samples / sec: 59272.68
Iteration:   2940, Loss function: 3.328, Average Loss: 4.216, avg. samples / sec: 59207.66
Iteration:   2940, Loss function: 3.809, Average Loss: 4.216, avg. samples / sec: 59259.52
Iteration:   2940, Loss function: 3.712, Average Loss: 4.193, avg. samples / sec: 59202.39
Iteration:   2940, Loss function: 3.260, Average Loss: 4.214, avg. samples / sec: 59096.78
Iteration:   2940, Loss function: 3.809, Average Loss: 4.212, avg. samples / sec: 59124.37
Iteration:   2940, Loss function: 5.447, Average Loss: 4.246, avg. samples / sec: 59083.12
Iteration:   2940, Loss function: 3.365, Average Loss: 4.239, avg. samples / sec: 59190.08
Iteration:   2940, Loss function: 3.845, Average Loss: 4.213, avg. samples / sec: 59208.35
Iteration:   2940, Loss function: 4.656, Average Loss: 4.219, avg. samples / sec: 59063.02
Iteration:   2940, Loss function: 4.793, Average Loss: 4.201, avg. samples / sec: 59173.13
Iteration:   2940, Loss function: 4.461, Average Loss: 4.198, avg. samples / sec: 59094.15
Iteration:   2940, Loss function: 4.655, Average Loss: 4.194, avg. samples / sec: 59222.39
Iteration:   2960, Loss function: 3.283, Average Loss: 4.196, avg. samples / sec: 59995.36
Iteration:   2960, Loss function: 5.524, Average Loss: 4.208, avg. samples / sec: 60021.09
Iteration:   2960, Loss function: 3.392, Average Loss: 4.212, avg. samples / sec: 59866.45
Iteration:   2960, Loss function: 3.804, Average Loss: 4.193, avg. samples / sec: 59978.30
Iteration:   2960, Loss function: 3.003, Average Loss: 4.197, avg. samples / sec: 59948.25
Iteration:   2960, Loss function: 3.740, Average Loss: 4.237, avg. samples / sec: 59793.17
Iteration:   2960, Loss function: 3.909, Average Loss: 4.236, avg. samples / sec: 59856.99
Iteration:   2960, Loss function: 4.081, Average Loss: 4.192, avg. samples / sec: 59945.49
Iteration:   2960, Loss function: 3.013, Average Loss: 4.200, avg. samples / sec: 59800.91
Iteration:   2960, Loss function: 3.878, Average Loss: 4.210, avg. samples / sec: 59701.15
Iteration:   2960, Loss function: 3.513, Average Loss: 4.233, avg. samples / sec: 59663.18
Iteration:   2960, Loss function: 3.970, Average Loss: 4.186, avg. samples / sec: 59637.03
Iteration:   2960, Loss function: 4.838, Average Loss: 4.214, avg. samples / sec: 59710.20
Iteration:   2960, Loss function: 3.870, Average Loss: 4.212, avg. samples / sec: 59621.91
Iteration:   2960, Loss function: 3.871, Average Loss: 4.212, avg. samples / sec: 59396.01
Iteration:   2980, Loss function: 3.693, Average Loss: 4.203, avg. samples / sec: 57855.53
Iteration:   2980, Loss function: 3.694, Average Loss: 4.236, avg. samples / sec: 57997.03
Iteration:   2980, Loss function: 3.670, Average Loss: 4.209, avg. samples / sec: 58173.71
Iteration:   2980, Loss function: 4.336, Average Loss: 4.231, avg. samples / sec: 57949.00
Iteration:   2980, Loss function: 3.602, Average Loss: 4.197, avg. samples / sec: 57918.85
Iteration:   2980, Loss function: 4.996, Average Loss: 4.210, avg. samples / sec: 57861.57
Iteration:   2980, Loss function: 4.383, Average Loss: 4.208, avg. samples / sec: 58276.19
Iteration:   2980, Loss function: 3.867, Average Loss: 4.195, avg. samples / sec: 57944.67
Iteration:   2980, Loss function: 5.544, Average Loss: 4.191, avg. samples / sec: 57806.48
Iteration:   2980, Loss function: 4.410, Average Loss: 4.208, avg. samples / sec: 58042.61
Iteration:   2980, Loss function: 3.472, Average Loss: 4.191, avg. samples / sec: 57608.74
Iteration:   2980, Loss function: 4.354, Average Loss: 4.228, avg. samples / sec: 57978.04
Iteration:   2980, Loss function: 3.266, Average Loss: 4.188, avg. samples / sec: 57972.41
Iteration:   2980, Loss function: 4.552, Average Loss: 4.188, avg. samples / sec: 57777.85
Iteration:   2980, Loss function: 4.079, Average Loss: 4.203, avg. samples / sec: 57828.16
Iteration:   3000, Loss function: 5.346, Average Loss: 4.206, avg. samples / sec: 59787.36
Iteration:   3000, Loss function: 2.337, Average Loss: 4.187, avg. samples / sec: 59647.17
Iteration:   3000, Loss function: 2.981, Average Loss: 4.222, avg. samples / sec: 59630.11
Iteration:   3000, Loss function: 3.753, Average Loss: 4.193, avg. samples / sec: 59546.29
Iteration:   3000, Loss function: 3.354, Average Loss: 4.189, avg. samples / sec: 59678.54
Iteration:   3000, Loss function: 3.579, Average Loss: 4.203, avg. samples / sec: 59691.64
Iteration:   3000, Loss function: 3.837, Average Loss: 4.187, avg. samples / sec: 59744.45
Iteration:   3000, Loss function: 3.034, Average Loss: 4.195, avg. samples / sec: 59825.86
Iteration:   3000, Loss function: 3.168, Average Loss: 4.228, avg. samples / sec: 59515.76
Iteration:   3000, Loss function: 3.670, Average Loss: 4.184, avg. samples / sec: 59759.73
Iteration:   3000, Loss function: 4.180, Average Loss: 4.230, avg. samples / sec: 59705.60
Iteration:   3000, Loss function: 3.506, Average Loss: 4.203, avg. samples / sec: 59501.01
Iteration:   3000, Loss function: 4.710, Average Loss: 4.186, avg. samples / sec: 59598.04
Iteration:   3000, Loss function: 4.741, Average Loss: 4.190, avg. samples / sec: 59635.08
Iteration:   3000, Loss function: 4.603, Average Loss: 4.202, avg. samples / sec: 59411.18
:::MLL 1558639472.754 epoch_stop: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 819}}
:::MLL 1558639472.755 epoch_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 673}}
Iteration:   3020, Loss function: 2.620, Average Loss: 4.219, avg. samples / sec: 57642.82
Iteration:   3020, Loss function: 3.612, Average Loss: 4.178, avg. samples / sec: 57614.54
Iteration:   3020, Loss function: 4.918, Average Loss: 4.183, avg. samples / sec: 57629.07
Iteration:   3020, Loss function: 3.851, Average Loss: 4.198, avg. samples / sec: 57590.38
Iteration:   3020, Loss function: 4.868, Average Loss: 4.206, avg. samples / sec: 57348.36
Iteration:   3020, Loss function: 3.672, Average Loss: 4.190, avg. samples / sec: 57505.03
Iteration:   3020, Loss function: 4.273, Average Loss: 4.186, avg. samples / sec: 57456.29
Iteration:   3020, Loss function: 3.658, Average Loss: 4.226, avg. samples / sec: 57541.89
Iteration:   3020, Loss function: 4.533, Average Loss: 4.181, avg. samples / sec: 57531.11
Iteration:   3020, Loss function: 3.296, Average Loss: 4.188, avg. samples / sec: 57524.14
Iteration:   3020, Loss function: 3.246, Average Loss: 4.198, avg. samples / sec: 57449.31
Iteration:   3020, Loss function: 3.744, Average Loss: 4.217, avg. samples / sec: 57384.44
Iteration:   3020, Loss function: 4.673, Average Loss: 4.190, avg. samples / sec: 57386.94
Iteration:   3020, Loss function: 3.966, Average Loss: 4.199, avg. samples / sec: 57599.80
Iteration:   3020, Loss function: 4.019, Average Loss: 4.191, avg. samples / sec: 57489.10
Iteration:   3040, Loss function: 3.671, Average Loss: 4.171, avg. samples / sec: 60460.54
Iteration:   3040, Loss function: 4.367, Average Loss: 4.214, avg. samples / sec: 60513.52
Iteration:   3040, Loss function: 3.930, Average Loss: 4.192, avg. samples / sec: 60376.30
Iteration:   3040, Loss function: 4.417, Average Loss: 4.195, avg. samples / sec: 60402.41
Iteration:   3040, Loss function: 3.255, Average Loss: 4.192, avg. samples / sec: 60253.97
Iteration:   3040, Loss function: 4.482, Average Loss: 4.181, avg. samples / sec: 60277.55
Iteration:   3040, Loss function: 4.140, Average Loss: 4.213, avg. samples / sec: 60137.55
Iteration:   3040, Loss function: 3.477, Average Loss: 4.225, avg. samples / sec: 60243.64
Iteration:   3040, Loss function: 4.624, Average Loss: 4.188, avg. samples / sec: 60212.96
Iteration:   3040, Loss function: 4.058, Average Loss: 4.189, avg. samples / sec: 60296.89
Iteration:   3040, Loss function: 4.716, Average Loss: 4.204, avg. samples / sec: 60200.69
Iteration:   3040, Loss function: 2.984, Average Loss: 4.190, avg. samples / sec: 60187.01
Iteration:   3040, Loss function: 3.536, Average Loss: 4.179, avg. samples / sec: 60046.76
Iteration:   3040, Loss function: 3.965, Average Loss: 4.191, avg. samples / sec: 60199.20
Iteration:   3040, Loss function: 4.774, Average Loss: 4.185, avg. samples / sec: 59925.51
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
:::MLL 1558639473.853 eval_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.53s)
DONE (t=0.55s)
DONE (t=2.95s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.17153
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.31770
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.16808
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.04191
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.18365
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.27179
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.18430
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.26953
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.28459
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.07142
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.30771
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.44172
Current AP: 0.17153 AP goal: 0.23000
:::MLL 1558639477.950 eval_accuracy: {"value": 0.1715267856532591, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 389}}
:::MLL 1558639478.328 eval_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 392}}
:::MLL 1558639478.338 block_stop: {"value": null, "metadata": {"first_epoch_num": 33, "file": "train.py", "lineno": 804}}
:::MLL 1558639478.338 block_start: {"value": null, "metadata": {"first_epoch_num": 44, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3060, Loss function: 3.923, Average Loss: 4.214, avg. samples / sec: 6645.11
Iteration:   3060, Loss function: 3.061, Average Loss: 4.182, avg. samples / sec: 6646.61
Iteration:   3060, Loss function: 4.046, Average Loss: 4.185, avg. samples / sec: 6642.58
Iteration:   3060, Loss function: 3.539, Average Loss: 4.195, avg. samples / sec: 6643.27
Iteration:   3060, Loss function: 3.697, Average Loss: 4.190, avg. samples / sec: 6642.28
Iteration:   3060, Loss function: 5.113, Average Loss: 4.189, avg. samples / sec: 6643.35
Iteration:   3060, Loss function: 5.413, Average Loss: 4.208, avg. samples / sec: 6640.53
Iteration:   3060, Loss function: 3.549, Average Loss: 4.178, avg. samples / sec: 6646.06
Iteration:   3060, Loss function: 4.529, Average Loss: 4.208, avg. samples / sec: 6642.12
Iteration:   3060, Loss function: 3.309, Average Loss: 4.177, avg. samples / sec: 6641.83
Iteration:   3060, Loss function: 4.749, Average Loss: 4.177, avg. samples / sec: 6641.87
Iteration:   3060, Loss function: 3.761, Average Loss: 4.181, avg. samples / sec: 6642.27
Iteration:   3060, Loss function: 4.554, Average Loss: 4.190, avg. samples / sec: 6640.13
Iteration:   3060, Loss function: 3.819, Average Loss: 4.162, avg. samples / sec: 6636.51
Iteration:   3060, Loss function: 3.975, Average Loss: 4.176, avg. samples / sec: 6641.39
:::MLL 1558639479.239 epoch_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 819}}
:::MLL 1558639479.240 epoch_start: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 673}}
Iteration:   3080, Loss function: 4.305, Average Loss: 4.185, avg. samples / sec: 59080.15
Iteration:   3080, Loss function: 3.009, Average Loss: 4.174, avg. samples / sec: 59033.38
Iteration:   3080, Loss function: 3.295, Average Loss: 4.181, avg. samples / sec: 59216.44
Iteration:   3080, Loss function: 3.250, Average Loss: 4.167, avg. samples / sec: 59109.99
Iteration:   3080, Loss function: 3.667, Average Loss: 4.174, avg. samples / sec: 59038.57
Iteration:   3080, Loss function: 3.644, Average Loss: 4.201, avg. samples / sec: 58837.83
Iteration:   3080, Loss function: 3.799, Average Loss: 4.171, avg. samples / sec: 58859.83
Iteration:   3080, Loss function: 2.773, Average Loss: 4.173, avg. samples / sec: 59094.57
Iteration:   3080, Loss function: 2.298, Average Loss: 4.196, avg. samples / sec: 58954.45
Iteration:   3080, Loss function: 4.096, Average Loss: 4.153, avg. samples / sec: 59151.54
Iteration:   3080, Loss function: 3.840, Average Loss: 4.172, avg. samples / sec: 59129.31
Iteration:   3080, Loss function: 2.829, Average Loss: 4.165, avg. samples / sec: 58934.70
Iteration:   3080, Loss function: 3.527, Average Loss: 4.165, avg. samples / sec: 58855.18
Iteration:   3080, Loss function: 3.722, Average Loss: 4.178, avg. samples / sec: 58740.37
Iteration:   3080, Loss function: 3.527, Average Loss: 4.193, avg. samples / sec: 58710.42
Iteration:   3100, Loss function: 4.412, Average Loss: 4.164, avg. samples / sec: 59037.29
Iteration:   3100, Loss function: 3.202, Average Loss: 4.163, avg. samples / sec: 58782.25
Iteration:   3100, Loss function: 5.287, Average Loss: 4.155, avg. samples / sec: 58947.72
Iteration:   3100, Loss function: 2.775, Average Loss: 4.158, avg. samples / sec: 58935.27
Iteration:   3100, Loss function: 3.337, Average Loss: 4.179, avg. samples / sec: 58791.88
Iteration:   3100, Loss function: 4.299, Average Loss: 4.164, avg. samples / sec: 58756.86
Iteration:   3100, Loss function: 2.990, Average Loss: 4.167, avg. samples / sec: 58989.05
Iteration:   3100, Loss function: 2.550, Average Loss: 4.157, avg. samples / sec: 58726.18
Iteration:   3100, Loss function: 3.868, Average Loss: 4.157, avg. samples / sec: 58654.47
Iteration:   3100, Loss function: 3.147, Average Loss: 4.183, avg. samples / sec: 59025.02
Iteration:   3100, Loss function: 3.136, Average Loss: 4.177, avg. samples / sec: 58586.58
Iteration:   3100, Loss function: 3.112, Average Loss: 4.164, avg. samples / sec: 58592.57
Iteration:   3100, Loss function: 3.747, Average Loss: 4.135, avg. samples / sec: 58688.86
Iteration:   3100, Loss function: 3.373, Average Loss: 4.192, avg. samples / sec: 58565.55
Iteration:   3100, Loss function: 3.499, Average Loss: 4.170, avg. samples / sec: 58494.90
Iteration:   3120, Loss function: 2.304, Average Loss: 4.156, avg. samples / sec: 59354.48
Iteration:   3120, Loss function: 3.058, Average Loss: 4.153, avg. samples / sec: 59114.55
Iteration:   3120, Loss function: 4.440, Average Loss: 4.161, avg. samples / sec: 59087.58
Iteration:   3120, Loss function: 2.282, Average Loss: 4.173, avg. samples / sec: 59047.80
Iteration:   3120, Loss function: 3.827, Average Loss: 4.145, avg. samples / sec: 59016.44
Iteration:   3120, Loss function: 5.092, Average Loss: 4.157, avg. samples / sec: 58976.39
Iteration:   3120, Loss function: 2.926, Average Loss: 4.141, avg. samples / sec: 58997.00
Iteration:   3120, Loss function: 3.774, Average Loss: 4.178, avg. samples / sec: 59148.86
Iteration:   3120, Loss function: 7.582, Average Loss: 4.163, avg. samples / sec: 59029.25
Iteration:   3120, Loss function: 2.337, Average Loss: 4.120, avg. samples / sec: 59054.85
Iteration:   3120, Loss function: 3.088, Average Loss: 4.147, avg. samples / sec: 58721.82
Iteration:   3120, Loss function: 4.317, Average Loss: 4.146, avg. samples / sec: 58853.39
Iteration:   3120, Loss function: 5.208, Average Loss: 4.150, avg. samples / sec: 58771.51
Iteration:   3120, Loss function: 2.977, Average Loss: 4.171, avg. samples / sec: 58825.31
Iteration:   3120, Loss function: 4.253, Average Loss: 4.146, avg. samples / sec: 58697.39
Iteration:   3140, Loss function: 2.834, Average Loss: 4.133, avg. samples / sec: 59782.80
Iteration:   3140, Loss function: 3.323, Average Loss: 4.139, avg. samples / sec: 59497.87
Iteration:   3140, Loss function: 2.721, Average Loss: 4.152, avg. samples / sec: 59694.44
Iteration:   3140, Loss function: 2.829, Average Loss: 4.137, avg. samples / sec: 59722.14
Iteration:   3140, Loss function: 3.126, Average Loss: 4.127, avg. samples / sec: 59635.26
Iteration:   3140, Loss function: 4.270, Average Loss: 4.163, avg. samples / sec: 59570.50
Iteration:   3140, Loss function: 3.782, Average Loss: 4.136, avg. samples / sec: 59703.83
Iteration:   3140, Loss function: 3.870, Average Loss: 4.139, avg. samples / sec: 59588.31
Iteration:   3140, Loss function: 2.420, Average Loss: 4.152, avg. samples / sec: 59491.49
Iteration:   3140, Loss function: 3.776, Average Loss: 4.109, avg. samples / sec: 59515.63
Iteration:   3140, Loss function: 2.918, Average Loss: 4.130, avg. samples / sec: 59457.35
Iteration:   3140, Loss function: 2.059, Average Loss: 4.142, avg. samples / sec: 59318.38
Iteration:   3140, Loss function: 3.133, Average Loss: 4.129, avg. samples / sec: 59693.69
Iteration:   3140, Loss function: 4.331, Average Loss: 4.158, avg. samples / sec: 59529.86
Iteration:   3140, Loss function: 3.778, Average Loss: 4.167, avg. samples / sec: 59358.76
:::MLL 1558639481.237 epoch_stop: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 819}}
:::MLL 1558639481.238 epoch_start: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 673}}
Iteration:   3160, Loss function: 2.940, Average Loss: 4.119, avg. samples / sec: 58188.51
Iteration:   3160, Loss function: 3.415, Average Loss: 4.143, avg. samples / sec: 58082.20
Iteration:   3160, Loss function: 3.381, Average Loss: 4.118, avg. samples / sec: 58017.42
Iteration:   3160, Loss function: 3.577, Average Loss: 4.121, avg. samples / sec: 57980.02
Iteration:   3160, Loss function: 3.553, Average Loss: 4.125, avg. samples / sec: 57910.64
Iteration:   3160, Loss function: 3.346, Average Loss: 4.148, avg. samples / sec: 57999.56
Iteration:   3160, Loss function: 4.543, Average Loss: 4.130, avg. samples / sec: 57976.30
Iteration:   3160, Loss function: 3.212, Average Loss: 4.155, avg. samples / sec: 58186.06
Iteration:   3160, Loss function: 3.757, Average Loss: 4.131, avg. samples / sec: 58076.55
Iteration:   3160, Loss function: 4.347, Average Loss: 4.144, avg. samples / sec: 58163.75
Iteration:   3160, Loss function: 3.247, Average Loss: 4.142, avg. samples / sec: 57987.92
Iteration:   3160, Loss function: 2.958, Average Loss: 4.097, avg. samples / sec: 58039.31
Iteration:   3160, Loss function: 3.030, Average Loss: 4.115, avg. samples / sec: 58091.78
Iteration:   3160, Loss function: 4.173, Average Loss: 4.119, avg. samples / sec: 57893.13
Iteration:   3160, Loss function: 3.165, Average Loss: 4.117, avg. samples / sec: 57895.91
Iteration:   3180, Loss function: 3.726, Average Loss: 4.117, avg. samples / sec: 59162.50
Iteration:   3180, Loss function: 3.179, Average Loss: 4.132, avg. samples / sec: 58913.10
Iteration:   3180, Loss function: 4.043, Average Loss: 4.110, avg. samples / sec: 59029.92
Iteration:   3180, Loss function: 4.475, Average Loss: 4.138, avg. samples / sec: 59100.49
Iteration:   3180, Loss function: 2.315, Average Loss: 4.110, avg. samples / sec: 59016.40
Iteration:   3180, Loss function: 3.459, Average Loss: 4.108, avg. samples / sec: 59105.28
Iteration:   3180, Loss function: 3.050, Average Loss: 4.133, avg. samples / sec: 58943.55
Iteration:   3180, Loss function: 3.964, Average Loss: 4.106, avg. samples / sec: 59008.39
Iteration:   3180, Loss function: 4.338, Average Loss: 4.106, avg. samples / sec: 59170.52
Iteration:   3180, Loss function: 3.735, Average Loss: 4.142, avg. samples / sec: 58957.24
Iteration:   3180, Loss function: 4.083, Average Loss: 4.116, avg. samples / sec: 58882.77
Iteration:   3180, Loss function: 3.077, Average Loss: 4.127, avg. samples / sec: 58841.99
Iteration:   3180, Loss function: 4.209, Average Loss: 4.107, avg. samples / sec: 58641.63
Iteration:   3180, Loss function: 3.921, Average Loss: 4.106, avg. samples / sec: 58572.02
Iteration:   3180, Loss function: 3.362, Average Loss: 4.085, avg. samples / sec: 57984.39
Iteration:   3200, Loss function: 3.750, Average Loss: 4.070, avg. samples / sec: 58319.55
Iteration:   3200, Loss function: 4.714, Average Loss: 4.098, avg. samples / sec: 57436.10
Iteration:   3200, Loss function: 4.349, Average Loss: 4.123, avg. samples / sec: 57126.13
Iteration:   3200, Loss function: 3.476, Average Loss: 4.124, avg. samples / sec: 57205.90
Iteration:   3200, Loss function: 3.100, Average Loss: 4.094, avg. samples / sec: 57385.31
Iteration:   3200, Loss function: 3.801, Average Loss: 4.116, avg. samples / sec: 57357.96
Iteration:   3200, Loss function: 2.916, Average Loss: 4.105, avg. samples / sec: 57308.46
Iteration:   3200, Loss function: 3.608, Average Loss: 4.098, avg. samples / sec: 57116.29
Iteration:   3200, Loss function: 4.248, Average Loss: 4.093, avg. samples / sec: 57185.19
Iteration:   3200, Loss function: 3.772, Average Loss: 4.115, avg. samples / sec: 57069.22
Iteration:   3200, Loss function: 2.820, Average Loss: 4.098, avg. samples / sec: 57058.59
Iteration:   3200, Loss function: 3.587, Average Loss: 4.129, avg. samples / sec: 57168.91
Iteration:   3200, Loss function: 4.021, Average Loss: 4.097, avg. samples / sec: 57066.68
Iteration:   3200, Loss function: 2.919, Average Loss: 4.109, avg. samples / sec: 56933.54
Iteration:   3200, Loss function: 3.725, Average Loss: 4.090, avg. samples / sec: 57092.52
:::MLL 1558639483.243 epoch_stop: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 819}}
:::MLL 1558639483.244 epoch_start: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 673}}
Iteration:   3220, Loss function: 2.991, Average Loss: 4.086, avg. samples / sec: 60032.01
Iteration:   3220, Loss function: 4.131, Average Loss: 4.100, avg. samples / sec: 59976.51
Iteration:   3220, Loss function: 3.818, Average Loss: 4.103, avg. samples / sec: 59856.56
Iteration:   3220, Loss function: 3.029, Average Loss: 4.084, avg. samples / sec: 59910.30
Iteration:   3220, Loss function: 3.555, Average Loss: 4.095, avg. samples / sec: 59821.37
Iteration:   3220, Loss function: 3.685, Average Loss: 4.079, avg. samples / sec: 59813.85
Iteration:   3220, Loss function: 2.399, Average Loss: 4.095, avg. samples / sec: 59912.21
Iteration:   3220, Loss function: 2.530, Average Loss: 4.055, avg. samples / sec: 59668.79
Iteration:   3220, Loss function: 3.218, Average Loss: 4.112, avg. samples / sec: 59718.02
Iteration:   3220, Loss function: 3.604, Average Loss: 4.084, avg. samples / sec: 59753.82
Iteration:   3220, Loss function: 4.439, Average Loss: 4.082, avg. samples / sec: 59613.49
Iteration:   3220, Loss function: 3.105, Average Loss: 4.113, avg. samples / sec: 59701.10
Iteration:   3220, Loss function: 3.823, Average Loss: 4.123, avg. samples / sec: 59686.03
Iteration:   3220, Loss function: 3.399, Average Loss: 4.083, avg. samples / sec: 59718.37
Iteration:   3220, Loss function: 4.023, Average Loss: 4.084, avg. samples / sec: 59575.86
Iteration:   3240, Loss function: 3.154, Average Loss: 4.112, avg. samples / sec: 58331.19
Iteration:   3240, Loss function: 2.778, Average Loss: 4.086, avg. samples / sec: 57974.51
Iteration:   3240, Loss function: 3.832, Average Loss: 4.073, avg. samples / sec: 58115.13
Iteration:   3240, Loss function: 2.781, Average Loss: 4.075, avg. samples / sec: 58297.91
Iteration:   3240, Loss function: 2.791, Average Loss: 4.081, avg. samples / sec: 58091.18
Iteration:   3240, Loss function: 3.084, Average Loss: 4.092, avg. samples / sec: 58044.59
Iteration:   3240, Loss function: 3.423, Average Loss: 4.078, avg. samples / sec: 58138.05
Iteration:   3240, Loss function: 3.628, Average Loss: 4.085, avg. samples / sec: 58062.29
Iteration:   3240, Loss function: 4.147, Average Loss: 4.100, avg. samples / sec: 58119.66
Iteration:   3240, Loss function: 3.319, Average Loss: 4.069, avg. samples / sec: 58018.45
Iteration:   3240, Loss function: 4.207, Average Loss: 4.074, avg. samples / sec: 57813.29
Iteration:   3240, Loss function: 2.808, Average Loss: 4.069, avg. samples / sec: 58199.95
Iteration:   3240, Loss function: 4.194, Average Loss: 4.046, avg. samples / sec: 57944.26
Iteration:   3240, Loss function: 4.662, Average Loss: 4.074, avg. samples / sec: 57978.37
Iteration:   3240, Loss function: 2.983, Average Loss: 4.096, avg. samples / sec: 57927.23
Iteration:   3260, Loss function: 3.296, Average Loss: 4.073, avg. samples / sec: 57441.72
Iteration:   3260, Loss function: 5.046, Average Loss: 4.067, avg. samples / sec: 57466.01
Iteration:   3260, Loss function: 3.931, Average Loss: 4.057, avg. samples / sec: 57537.74
Iteration:   3260, Loss function: 4.055, Average Loss: 4.073, avg. samples / sec: 57465.47
Iteration:   3260, Loss function: 4.423, Average Loss: 4.067, avg. samples / sec: 57429.76
Iteration:   3260, Loss function: 3.836, Average Loss: 4.033, avg. samples / sec: 57568.48
Iteration:   3260, Loss function: 3.313, Average Loss: 4.058, avg. samples / sec: 57539.78
Iteration:   3260, Loss function: 3.665, Average Loss: 4.061, avg. samples / sec: 57601.44
Iteration:   3260, Loss function: 4.295, Average Loss: 4.057, avg. samples / sec: 57443.85
Iteration:   3260, Loss function: 3.756, Average Loss: 4.062, avg. samples / sec: 57342.67
Iteration:   3260, Loss function: 3.112, Average Loss: 4.088, avg. samples / sec: 57628.98
Iteration:   3260, Loss function: 2.462, Average Loss: 4.077, avg. samples / sec: 57358.82
Iteration:   3260, Loss function: 3.607, Average Loss: 4.090, avg. samples / sec: 57347.24
Iteration:   3260, Loss function: 3.820, Average Loss: 4.097, avg. samples / sec: 57255.26
Iteration:   3260, Loss function: 2.451, Average Loss: 4.074, avg. samples / sec: 57321.12
Iteration:   3280, Loss function: 4.139, Average Loss: 4.060, avg. samples / sec: 58904.33
Iteration:   3280, Loss function: 4.500, Average Loss: 4.063, avg. samples / sec: 58926.40
Iteration:   3280, Loss function: 2.711, Average Loss: 4.067, avg. samples / sec: 58878.57
Iteration:   3280, Loss function: 3.636, Average Loss: 4.056, avg. samples / sec: 58774.92
Iteration:   3280, Loss function: 3.704, Average Loss: 4.026, avg. samples / sec: 58792.69
Iteration:   3280, Loss function: 2.962, Average Loss: 4.061, avg. samples / sec: 58733.76
Iteration:   3280, Loss function: 3.026, Average Loss: 4.040, avg. samples / sec: 58798.75
Iteration:   3280, Loss function: 3.142, Average Loss: 4.083, avg. samples / sec: 58803.90
Iteration:   3280, Loss function: 2.774, Average Loss: 4.083, avg. samples / sec: 58843.88
Iteration:   3280, Loss function: 5.282, Average Loss: 4.051, avg. samples / sec: 58711.38
Iteration:   3280, Loss function: 2.930, Average Loss: 4.060, avg. samples / sec: 58586.68
Iteration:   3280, Loss function: 3.736, Average Loss: 4.073, avg. samples / sec: 58714.85
Iteration:   3280, Loss function: 3.546, Average Loss: 4.045, avg. samples / sec: 58528.28
Iteration:   3280, Loss function: 4.001, Average Loss: 4.046, avg. samples / sec: 58517.32
Iteration:   3280, Loss function: 4.961, Average Loss: 4.050, avg. samples / sec: 58457.47
:::MLL 1558639485.261 epoch_stop: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 819}}
:::MLL 1558639485.261 epoch_start: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 673}}
Iteration:   3300, Loss function: 4.609, Average Loss: 4.078, avg. samples / sec: 57260.40
Iteration:   3300, Loss function: 2.250, Average Loss: 4.013, avg. samples / sec: 57213.10
Iteration:   3300, Loss function: 3.508, Average Loss: 4.045, avg. samples / sec: 57295.83
Iteration:   3300, Loss function: 3.847, Average Loss: 4.060, avg. samples / sec: 57157.78
Iteration:   3300, Loss function: 4.219, Average Loss: 4.038, avg. samples / sec: 57350.21
Iteration:   3300, Loss function: 2.677, Average Loss: 4.065, avg. samples / sec: 57271.48
Iteration:   3300, Loss function: 2.153, Average Loss: 4.038, avg. samples / sec: 57435.50
Iteration:   3300, Loss function: 2.821, Average Loss: 4.048, avg. samples / sec: 56967.81
Iteration:   3300, Loss function: 2.894, Average Loss: 4.035, avg. samples / sec: 57345.91
Iteration:   3300, Loss function: 4.047, Average Loss: 4.054, avg. samples / sec: 57075.16
Iteration:   3300, Loss function: 3.488, Average Loss: 4.050, avg. samples / sec: 57200.09
Iteration:   3300, Loss function: 3.485, Average Loss: 4.044, avg. samples / sec: 57072.99
Iteration:   3300, Loss function: 3.057, Average Loss: 4.025, avg. samples / sec: 57057.97
Iteration:   3300, Loss function: 4.353, Average Loss: 4.073, avg. samples / sec: 57050.41
Iteration:   3300, Loss function: 2.794, Average Loss: 4.050, avg. samples / sec: 56901.72
Iteration:   3320, Loss function: 4.426, Average Loss: 4.004, avg. samples / sec: 59762.31
Iteration:   3320, Loss function: 3.538, Average Loss: 4.062, avg. samples / sec: 59957.43
Iteration:   3320, Loss function: 2.794, Average Loss: 4.021, avg. samples / sec: 59847.92
Iteration:   3320, Loss function: 3.433, Average Loss: 4.065, avg. samples / sec: 59705.57
Iteration:   3320, Loss function: 3.859, Average Loss: 4.023, avg. samples / sec: 59833.64
Iteration:   3320, Loss function: 4.069, Average Loss: 4.050, avg. samples / sec: 59711.57
Iteration:   3320, Loss function: 3.235, Average Loss: 4.031, avg. samples / sec: 59789.26
Iteration:   3320, Loss function: 3.016, Average Loss: 4.040, avg. samples / sec: 59791.39
Iteration:   3320, Loss function: 3.539, Average Loss: 4.026, avg. samples / sec: 59721.99
Iteration:   3320, Loss function: 2.976, Average Loss: 4.039, avg. samples / sec: 59925.79
Iteration:   3320, Loss function: 3.266, Average Loss: 4.009, avg. samples / sec: 59783.56
Iteration:   3320, Loss function: 2.678, Average Loss: 4.036, avg. samples / sec: 59618.48
Iteration:   3320, Loss function: 2.573, Average Loss: 4.051, avg. samples / sec: 59704.54
Iteration:   3320, Loss function: 2.778, Average Loss: 4.031, avg. samples / sec: 59735.11
Iteration:   3320, Loss function: 1.785, Average Loss: 4.037, avg. samples / sec: 59528.83
Iteration:   3340, Loss function: 3.772, Average Loss: 4.012, avg. samples / sec: 59810.32
Iteration:   3340, Loss function: 3.447, Average Loss: 4.023, avg. samples / sec: 59885.96
Iteration:   3340, Loss function: 3.066, Average Loss: 4.027, avg. samples / sec: 60075.36
Iteration:   3340, Loss function: 3.673, Average Loss: 4.015, avg. samples / sec: 59862.71
Iteration:   3340, Loss function: 4.398, Average Loss: 3.992, avg. samples / sec: 59647.68
Iteration:   3340, Loss function: 4.265, Average Loss: 4.053, avg. samples / sec: 59627.79
Iteration:   3340, Loss function: 3.227, Average Loss: 4.029, avg. samples / sec: 59697.58
Iteration:   3340, Loss function: 2.557, Average Loss: 4.015, avg. samples / sec: 59706.69
Iteration:   3340, Loss function: 3.029, Average Loss: 4.029, avg. samples / sec: 59723.38
Iteration:   3340, Loss function: 1.952, Average Loss: 4.039, avg. samples / sec: 59747.57
Iteration:   3340, Loss function: 3.968, Average Loss: 4.017, avg. samples / sec: 59642.48
Iteration:   3340, Loss function: 3.024, Average Loss: 4.053, avg. samples / sec: 59572.99
Iteration:   3340, Loss function: 2.663, Average Loss: 3.995, avg. samples / sec: 59656.89
Iteration:   3340, Loss function: 4.196, Average Loss: 4.011, avg. samples / sec: 59469.37
Iteration:   3340, Loss function: 3.498, Average Loss: 4.043, avg. samples / sec: 59428.87
:::MLL 1558639487.264 epoch_stop: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 819}}
:::MLL 1558639487.264 epoch_start: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 673}}
Iteration:   3360, Loss function: 3.475, Average Loss: 4.028, avg. samples / sec: 58961.28
Iteration:   3360, Loss function: 2.205, Average Loss: 4.007, avg. samples / sec: 58958.60
Iteration:   3360, Loss function: 3.630, Average Loss: 4.017, avg. samples / sec: 58840.34
Iteration:   3360, Loss function: 3.656, Average Loss: 4.003, avg. samples / sec: 58757.22
Iteration:   3360, Loss function: 4.488, Average Loss: 4.007, avg. samples / sec: 58862.63
Iteration:   3360, Loss function: 4.092, Average Loss: 4.045, avg. samples / sec: 58875.42
Iteration:   3360, Loss function: 4.351, Average Loss: 3.996, avg. samples / sec: 58979.89
Iteration:   3360, Loss function: 1.880, Average Loss: 4.013, avg. samples / sec: 58625.33
Iteration:   3360, Loss function: 2.512, Average Loss: 3.979, avg. samples / sec: 58678.84
Iteration:   3360, Loss function: 3.438, Average Loss: 4.011, avg. samples / sec: 58619.65
Iteration:   3360, Loss function: 3.731, Average Loss: 4.034, avg. samples / sec: 58961.36
Iteration:   3360, Loss function: 3.080, Average Loss: 4.003, avg. samples / sec: 58535.38
Iteration:   3360, Loss function: 2.560, Average Loss: 3.988, avg. samples / sec: 58808.89
Iteration:   3360, Loss function: 5.241, Average Loss: 4.023, avg. samples / sec: 58652.00
Iteration:   3360, Loss function: 3.202, Average Loss: 4.038, avg. samples / sec: 58454.53
Iteration:   3380, Loss function: 2.819, Average Loss: 3.984, avg. samples / sec: 59417.92
Iteration:   3380, Loss function: 3.335, Average Loss: 3.991, avg. samples / sec: 59441.86
Iteration:   3380, Loss function: 4.206, Average Loss: 4.007, avg. samples / sec: 59276.39
Iteration:   3380, Loss function: 3.203, Average Loss: 3.998, avg. samples / sec: 59383.07
Iteration:   3380, Loss function: 3.395, Average Loss: 4.004, avg. samples / sec: 59346.41
Iteration:   3380, Loss function: 3.292, Average Loss: 3.993, avg. samples / sec: 59235.58
Iteration:   3380, Loss function: 4.725, Average Loss: 4.015, avg. samples / sec: 59056.78
Iteration:   3380, Loss function: 3.438, Average Loss: 3.971, avg. samples / sec: 59256.65
Iteration:   3380, Loss function: 3.574, Average Loss: 4.031, avg. samples / sec: 59176.33
Iteration:   3380, Loss function: 2.733, Average Loss: 3.995, avg. samples / sec: 59030.29
Iteration:   3380, Loss function: 3.381, Average Loss: 3.978, avg. samples / sec: 59214.18
Iteration:   3380, Loss function: 3.211, Average Loss: 4.026, avg. samples / sec: 59430.37
Iteration:   3380, Loss function: 2.677, Average Loss: 4.021, avg. samples / sec: 59154.05
Iteration:   3380, Loss function: 3.242, Average Loss: 4.014, avg. samples / sec: 59230.50
Iteration:   3380, Loss function: 3.385, Average Loss: 3.998, avg. samples / sec: 58878.32
Iteration:   3400, Loss function: 3.777, Average Loss: 3.984, avg. samples / sec: 59379.37
Iteration:   3400, Loss function: 4.573, Average Loss: 3.999, avg. samples / sec: 59230.38
Iteration:   3400, Loss function: 4.565, Average Loss: 4.004, avg. samples / sec: 59303.98
Iteration:   3400, Loss function: 4.114, Average Loss: 4.018, avg. samples / sec: 59327.10
Iteration:   3400, Loss function: 4.441, Average Loss: 4.019, avg. samples / sec: 59395.58
Iteration:   3400, Loss function: 3.040, Average Loss: 4.008, avg. samples / sec: 59404.47
Iteration:   3400, Loss function: 4.032, Average Loss: 4.005, avg. samples / sec: 59396.64
Iteration:   3400, Loss function: 3.277, Average Loss: 3.977, avg. samples / sec: 59191.64
Iteration:   3400, Loss function: 3.806, Average Loss: 3.968, avg. samples / sec: 59272.95
Iteration:   3400, Loss function: 2.200, Average Loss: 3.976, avg. samples / sec: 59025.27
Iteration:   3400, Loss function: 2.438, Average Loss: 3.988, avg. samples / sec: 59423.93
Iteration:   3400, Loss function: 2.596, Average Loss: 3.975, avg. samples / sec: 58866.59
Iteration:   3400, Loss function: 3.996, Average Loss: 3.966, avg. samples / sec: 59106.94
Iteration:   3400, Loss function: 3.274, Average Loss: 3.994, avg. samples / sec: 58875.96
Iteration:   3400, Loss function: 4.201, Average Loss: 3.989, avg. samples / sec: 58859.41
Iteration:   3420, Loss function: 3.993, Average Loss: 3.979, avg. samples / sec: 60097.29
Iteration:   3420, Loss function: 3.832, Average Loss: 4.007, avg. samples / sec: 59683.83
Iteration:   3420, Loss function: 3.144, Average Loss: 3.983, avg. samples / sec: 60028.66
Iteration:   3420, Loss function: 3.451, Average Loss: 3.999, avg. samples / sec: 59696.34
Iteration:   3420, Loss function: 3.375, Average Loss: 3.962, avg. samples / sec: 59766.01
Iteration:   3420, Loss function: 2.312, Average Loss: 3.954, avg. samples / sec: 59811.36
Iteration:   3420, Loss function: 2.779, Average Loss: 3.973, avg. samples / sec: 59532.50
Iteration:   3420, Loss function: 3.447, Average Loss: 3.990, avg. samples / sec: 59524.38
Iteration:   3420, Loss function: 2.403, Average Loss: 3.966, avg. samples / sec: 59665.48
Iteration:   3420, Loss function: 3.692, Average Loss: 3.988, avg. samples / sec: 59481.95
Iteration:   3420, Loss function: 2.277, Average Loss: 3.966, avg. samples / sec: 59729.56
Iteration:   3420, Loss function: 2.986, Average Loss: 3.991, avg. samples / sec: 59497.01
Iteration:   3420, Loss function: 4.252, Average Loss: 4.004, avg. samples / sec: 59440.50
Iteration:   3420, Loss function: 3.451, Average Loss: 3.967, avg. samples / sec: 59481.82
Iteration:   3420, Loss function: 3.585, Average Loss: 3.972, avg. samples / sec: 59557.81
:::MLL 1558639489.232 eval_start: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.58 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.50s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.56s)
DONE (t=2.79s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22091
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.38099
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.22272
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05498
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.23129
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.36052
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.21547
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.31514
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33072
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09441
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.35686
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.51994
Current AP: 0.22091 AP goal: 0.23000
:::MLL 1558639493.166 eval_accuracy: {"value": 0.2209122101664696, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 389}}
:::MLL 1558639493.183 eval_stop: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 392}}
:::MLL 1558639493.193 block_stop: {"value": null, "metadata": {"first_epoch_num": 44, "file": "train.py", "lineno": 804}}
:::MLL 1558639493.193 block_start: {"value": null, "metadata": {"first_epoch_num": 49, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
:::MLL 1558639493.220 epoch_stop: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 819}}
:::MLL 1558639493.220 epoch_start: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 673}}
Iteration:   3440, Loss function: 3.053, Average Loss: 3.992, avg. samples / sec: 7401.16
Iteration:   3440, Loss function: 4.337, Average Loss: 3.960, avg. samples / sec: 7405.40
Iteration:   3440, Loss function: 2.407, Average Loss: 3.947, avg. samples / sec: 7401.59
Iteration:   3440, Loss function: 3.848, Average Loss: 3.985, avg. samples / sec: 7402.21
Iteration:   3440, Loss function: 2.372, Average Loss: 3.954, avg. samples / sec: 7402.37
Iteration:   3440, Loss function: 3.099, Average Loss: 3.968, avg. samples / sec: 7399.70
Iteration:   3440, Loss function: 3.688, Average Loss: 3.951, avg. samples / sec: 7401.77
Iteration:   3440, Loss function: 3.560, Average Loss: 3.953, avg. samples / sec: 7399.91
Iteration:   3440, Loss function: 3.595, Average Loss: 3.960, avg. samples / sec: 7403.15
Iteration:   3440, Loss function: 3.338, Average Loss: 3.974, avg. samples / sec: 7397.46
Iteration:   3440, Loss function: 4.174, Average Loss: 3.960, avg. samples / sec: 7399.11
Iteration:   3440, Loss function: 4.182, Average Loss: 3.981, avg. samples / sec: 7399.53
Iteration:   3440, Loss function: 2.190, Average Loss: 3.984, avg. samples / sec: 7400.79
Iteration:   3440, Loss function: 2.910, Average Loss: 3.989, avg. samples / sec: 7395.91
Iteration:   3440, Loss function: 2.783, Average Loss: 3.989, avg. samples / sec: 7399.24
Iteration:   3460, Loss function: 3.360, Average Loss: 3.973, avg. samples / sec: 59271.43
Iteration:   3460, Loss function: 4.324, Average Loss: 3.939, avg. samples / sec: 59228.09
Iteration:   3460, Loss function: 3.941, Average Loss: 3.976, avg. samples / sec: 59433.86
Iteration:   3460, Loss function: 3.637, Average Loss: 3.957, avg. samples / sec: 59079.21
Iteration:   3460, Loss function: 3.148, Average Loss: 3.977, avg. samples / sec: 59352.31
Iteration:   3460, Loss function: 2.333, Average Loss: 3.985, avg. samples / sec: 58958.99
Iteration:   3460, Loss function: 4.552, Average Loss: 3.961, avg. samples / sec: 59117.70
Iteration:   3460, Loss function: 3.652, Average Loss: 3.933, avg. samples / sec: 58984.51
Iteration:   3460, Loss function: 3.530, Average Loss: 3.958, avg. samples / sec: 58979.79
Iteration:   3460, Loss function: 4.079, Average Loss: 3.970, avg. samples / sec: 59130.05
Iteration:   3460, Loss function: 2.036, Average Loss: 3.969, avg. samples / sec: 59126.28
Iteration:   3460, Loss function: 3.359, Average Loss: 3.944, avg. samples / sec: 58914.08
Iteration:   3460, Loss function: 4.206, Average Loss: 3.951, avg. samples / sec: 58897.07
Iteration:   3460, Loss function: 4.028, Average Loss: 3.944, avg. samples / sec: 58840.04
Iteration:   3460, Loss function: 3.920, Average Loss: 3.952, avg. samples / sec: 58873.57
Iteration:   3480, Loss function: 2.576, Average Loss: 3.930, avg. samples / sec: 59262.98
Iteration:   3480, Loss function: 3.326, Average Loss: 3.964, avg. samples / sec: 59197.19
Iteration:   3480, Loss function: 3.227, Average Loss: 3.945, avg. samples / sec: 59027.89
Iteration:   3480, Loss function: 3.562, Average Loss: 3.950, avg. samples / sec: 59160.09
Iteration:   3480, Loss function: 2.807, Average Loss: 3.950, avg. samples / sec: 59097.52
Iteration:   3480, Loss function: 3.122, Average Loss: 3.944, avg. samples / sec: 59245.54
Iteration:   3480, Loss function: 4.121, Average Loss: 3.921, avg. samples / sec: 59073.49
Iteration:   3480, Loss function: 2.833, Average Loss: 3.967, avg. samples / sec: 58984.63
Iteration:   3480, Loss function: 3.252, Average Loss: 3.941, avg. samples / sec: 59323.65
Iteration:   3480, Loss function: 3.917, Average Loss: 3.957, avg. samples / sec: 59068.76
Iteration:   3480, Loss function: 3.368, Average Loss: 3.936, avg. samples / sec: 59153.88
Iteration:   3480, Loss function: 2.477, Average Loss: 3.931, avg. samples / sec: 58723.98
Iteration:   3480, Loss function: 1.722, Average Loss: 3.974, avg. samples / sec: 58905.14
Iteration:   3480, Loss function: 3.335, Average Loss: 3.970, avg. samples / sec: 58700.15
Iteration:   3480, Loss function: 3.668, Average Loss: 3.957, avg. samples / sec: 58436.30
:::MLL 1558639495.210 epoch_stop: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 819}}
:::MLL 1558639495.210 epoch_start: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 673}}
Iteration:   3500, Loss function: 2.236, Average Loss: 3.953, avg. samples / sec: 59071.56
Iteration:   3500, Loss function: 4.288, Average Loss: 3.933, avg. samples / sec: 59031.60
Iteration:   3500, Loss function: 3.449, Average Loss: 3.909, avg. samples / sec: 59053.02
Iteration:   3500, Loss function: 4.175, Average Loss: 3.960, avg. samples / sec: 59006.63
Iteration:   3500, Loss function: 3.950, Average Loss: 3.947, avg. samples / sec: 59041.52
Iteration:   3500, Loss function: 3.499, Average Loss: 3.937, avg. samples / sec: 58945.45
Iteration:   3500, Loss function: 5.206, Average Loss: 3.966, avg. samples / sec: 59145.51
Iteration:   3500, Loss function: 3.323, Average Loss: 3.965, avg. samples / sec: 59055.07
Iteration:   3500, Loss function: 5.830, Average Loss: 3.953, avg. samples / sec: 59271.23
Iteration:   3500, Loss function: 3.574, Average Loss: 3.921, avg. samples / sec: 59012.69
Iteration:   3500, Loss function: 3.494, Average Loss: 3.920, avg. samples / sec: 58826.76
Iteration:   3500, Loss function: 3.966, Average Loss: 3.935, avg. samples / sec: 58805.72
Iteration:   3500, Loss function: 4.240, Average Loss: 3.936, avg. samples / sec: 58681.84
Iteration:   3500, Loss function: 3.669, Average Loss: 3.932, avg. samples / sec: 58660.01
Iteration:   3500, Loss function: 3.674, Average Loss: 3.931, avg. samples / sec: 58336.19
Iteration:   3520, Loss function: 3.206, Average Loss: 3.948, avg. samples / sec: 56429.70
Iteration:   3520, Loss function: 3.194, Average Loss: 3.944, avg. samples / sec: 56507.74
Iteration:   3520, Loss function: 2.140, Average Loss: 3.926, avg. samples / sec: 56540.66
Iteration:   3520, Loss function: 3.872, Average Loss: 3.937, avg. samples / sec: 56392.97
Iteration:   3520, Loss function: 2.489, Average Loss: 3.952, avg. samples / sec: 56438.34
Iteration:   3520, Loss function: 2.881, Average Loss: 3.951, avg. samples / sec: 56399.67
Iteration:   3520, Loss function: 3.413, Average Loss: 3.909, avg. samples / sec: 56413.12
Iteration:   3520, Loss function: 3.958, Average Loss: 3.929, avg. samples / sec: 56520.37
Iteration:   3520, Loss function: 2.778, Average Loss: 3.928, avg. samples / sec: 57005.94
Iteration:   3520, Loss function: 2.673, Average Loss: 3.924, avg. samples / sec: 56240.72
Iteration:   3520, Loss function: 4.086, Average Loss: 3.921, avg. samples / sec: 56623.47
Iteration:   3520, Loss function: 3.217, Average Loss: 3.910, avg. samples / sec: 56359.48
Iteration:   3520, Loss function: 2.725, Average Loss: 3.923, avg. samples / sec: 56276.97
Iteration:   3520, Loss function: 2.285, Average Loss: 3.943, avg. samples / sec: 56069.08
Iteration:   3520, Loss function: 3.460, Average Loss: 3.895, avg. samples / sec: 56135.27
Iteration:   3540, Loss function: 2.891, Average Loss: 3.943, avg. samples / sec: 57052.75
Iteration:   3540, Loss function: 3.366, Average Loss: 3.937, avg. samples / sec: 57090.51
Iteration:   3540, Loss function: 3.135, Average Loss: 3.931, avg. samples / sec: 57013.44
Iteration:   3540, Loss function: 2.406, Average Loss: 3.926, avg. samples / sec: 57037.05
Iteration:   3540, Loss function: 3.505, Average Loss: 3.916, avg. samples / sec: 57115.25
Iteration:   3540, Loss function: 2.699, Average Loss: 3.906, avg. samples / sec: 57125.11
Iteration:   3540, Loss function: 3.314, Average Loss: 3.901, avg. samples / sec: 57136.60
Iteration:   3540, Loss function: 3.561, Average Loss: 3.931, avg. samples / sec: 57195.59
Iteration:   3540, Loss function: 3.138, Average Loss: 3.914, avg. samples / sec: 57122.45
Iteration:   3540, Loss function: 2.552, Average Loss: 3.917, avg. samples / sec: 57067.95
Iteration:   3540, Loss function: 3.264, Average Loss: 3.945, avg. samples / sec: 57028.16
Iteration:   3540, Loss function: 3.107, Average Loss: 3.898, avg. samples / sec: 57015.33
Iteration:   3540, Loss function: 3.242, Average Loss: 3.916, avg. samples / sec: 56873.78
Iteration:   3540, Loss function: 3.075, Average Loss: 3.909, avg. samples / sec: 56931.93
Iteration:   3540, Loss function: 3.142, Average Loss: 3.887, avg. samples / sec: 56984.07
:::MLL 1558639496.968 epoch_stop: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 819}}
:::MLL 1558639496.968 epoch_start: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 673}}
Iteration:   3560, Loss function: 3.467, Average Loss: 3.924, avg. samples / sec: 58807.81
Iteration:   3560, Loss function: 2.952, Average Loss: 3.891, avg. samples / sec: 58772.76
Iteration:   3560, Loss function: 2.224, Average Loss: 3.920, avg. samples / sec: 58663.84
Iteration:   3560, Loss function: 2.957, Average Loss: 3.897, avg. samples / sec: 58703.35
Iteration:   3560, Loss function: 2.872, Average Loss: 3.912, avg. samples / sec: 58689.25
Iteration:   3560, Loss function: 3.570, Average Loss: 3.881, avg. samples / sec: 58940.67
Iteration:   3560, Loss function: 3.851, Average Loss: 3.922, avg. samples / sec: 58642.36
Iteration:   3560, Loss function: 3.182, Average Loss: 3.926, avg. samples / sec: 58537.93
Iteration:   3560, Loss function: 2.558, Average Loss: 3.910, avg. samples / sec: 58673.93
Iteration:   3560, Loss function: 2.312, Average Loss: 3.903, avg. samples / sec: 58610.53
Iteration:   3560, Loss function: 2.578, Average Loss: 3.908, avg. samples / sec: 58723.80
Iteration:   3560, Loss function: 3.752, Average Loss: 3.899, avg. samples / sec: 58793.38
Iteration:   3560, Loss function: 3.818, Average Loss: 3.899, avg. samples / sec: 58584.05
Iteration:   3560, Loss function: 3.758, Average Loss: 3.890, avg. samples / sec: 58648.24
Iteration:   3560, Loss function: 3.469, Average Loss: 3.927, avg. samples / sec: 58470.37
Iteration:   3580, Loss function: 4.591, Average Loss: 3.883, avg. samples / sec: 59692.02
Iteration:   3580, Loss function: 3.553, Average Loss: 3.888, avg. samples / sec: 59822.84
Iteration:   3580, Loss function: 4.520, Average Loss: 3.902, avg. samples / sec: 59759.42
Iteration:   3580, Loss function: 3.776, Average Loss: 3.883, avg. samples / sec: 59837.65
Iteration:   3580, Loss function: 3.813, Average Loss: 3.919, avg. samples / sec: 59983.53
Iteration:   3580, Loss function: 3.545, Average Loss: 3.907, avg. samples / sec: 59670.00
Iteration:   3580, Loss function: 3.632, Average Loss: 3.908, avg. samples / sec: 59591.16
Iteration:   3580, Loss function: 4.090, Average Loss: 3.890, avg. samples / sec: 59666.01
Iteration:   3580, Loss function: 2.853, Average Loss: 3.886, avg. samples / sec: 59542.51
Iteration:   3580, Loss function: 3.892, Average Loss: 3.898, avg. samples / sec: 59640.53
Iteration:   3580, Loss function: 3.767, Average Loss: 3.887, avg. samples / sec: 59591.46
Iteration:   3580, Loss function: 4.017, Average Loss: 3.873, avg. samples / sec: 59411.01
Iteration:   3580, Loss function: 4.350, Average Loss: 3.914, avg. samples / sec: 59238.12
Iteration:   3580, Loss function: 4.624, Average Loss: 3.908, avg. samples / sec: 59258.89
Iteration:   3580, Loss function: 3.670, Average Loss: 3.916, avg. samples / sec: 58713.11
Iteration:   3600, Loss function: 3.929, Average Loss: 3.906, avg. samples / sec: 57574.78
Iteration:   3600, Loss function: 2.567, Average Loss: 3.909, avg. samples / sec: 57240.84
Iteration:   3600, Loss function: 2.289, Average Loss: 3.895, avg. samples / sec: 57613.24
Iteration:   3600, Loss function: 2.860, Average Loss: 3.876, avg. samples / sec: 57341.04
Iteration:   3600, Loss function: 3.219, Average Loss: 3.883, avg. samples / sec: 57169.07
Iteration:   3600, Loss function: 3.696, Average Loss: 3.878, avg. samples / sec: 57428.73
Iteration:   3600, Loss function: 3.567, Average Loss: 3.890, avg. samples / sec: 57315.64
Iteration:   3600, Loss function: 3.291, Average Loss: 3.876, avg. samples / sec: 57190.92
Iteration:   3600, Loss function: 4.025, Average Loss: 3.870, avg. samples / sec: 57055.63
Iteration:   3600, Loss function: 3.194, Average Loss: 3.890, avg. samples / sec: 57268.99
Iteration:   3600, Loss function: 3.496, Average Loss: 3.901, avg. samples / sec: 58115.32
Iteration:   3600, Loss function: 3.223, Average Loss: 3.901, avg. samples / sec: 57175.75
Iteration:   3600, Loss function: 3.633, Average Loss: 3.900, avg. samples / sec: 57160.74
Iteration:   3600, Loss function: 3.916, Average Loss: 3.859, avg. samples / sec: 57354.74
Iteration:   3600, Loss function: 3.756, Average Loss: 3.892, avg. samples / sec: 57094.58
Iteration:   3620, Loss function: 2.384, Average Loss: 3.868, avg. samples / sec: 58605.68
Iteration:   3620, Loss function: 2.851, Average Loss: 3.893, avg. samples / sec: 58493.98
Iteration:   3620, Loss function: 2.465, Average Loss: 3.896, avg. samples / sec: 58335.22
Iteration:   3620, Loss function: 3.002, Average Loss: 3.897, avg. samples / sec: 58308.84
Iteration:   3620, Loss function: 2.961, Average Loss: 3.865, avg. samples / sec: 58352.05
Iteration:   3620, Loss function: 2.163, Average Loss: 3.863, avg. samples / sec: 58316.80
Iteration:   3620, Loss function: 3.809, Average Loss: 3.878, avg. samples / sec: 58324.14
Iteration:   3620, Loss function: 3.573, Average Loss: 3.880, avg. samples / sec: 58375.09
Iteration:   3620, Loss function: 3.284, Average Loss: 3.876, avg. samples / sec: 58264.75
Iteration:   3620, Loss function: 3.532, Average Loss: 3.880, avg. samples / sec: 58355.12
Iteration:   3620, Loss function: 2.329, Average Loss: 3.892, avg. samples / sec: 58335.46
Iteration:   3620, Loss function: 3.665, Average Loss: 3.882, avg. samples / sec: 58212.59
Iteration:   3620, Loss function: 4.529, Average Loss: 3.851, avg. samples / sec: 58320.08
Iteration:   3620, Loss function: 3.365, Average Loss: 3.889, avg. samples / sec: 58264.36
Iteration:   3620, Loss function: 3.402, Average Loss: 3.872, avg. samples / sec: 58028.17
:::MLL 1558639498.983 epoch_stop: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 819}}
:::MLL 1558639498.984 epoch_start: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 673}}
Iteration:   3640, Loss function: 2.928, Average Loss: 3.883, avg. samples / sec: 58747.48
Iteration:   3640, Loss function: 4.120, Average Loss: 3.870, avg. samples / sec: 58604.85
Iteration:   3640, Loss function: 3.795, Average Loss: 3.860, avg. samples / sec: 58249.00
Iteration:   3640, Loss function: 2.794, Average Loss: 3.844, avg. samples / sec: 58561.29
Iteration:   3640, Loss function: 2.593, Average Loss: 3.877, avg. samples / sec: 58587.14
Iteration:   3640, Loss function: 3.319, Average Loss: 3.853, avg. samples / sec: 58434.05
Iteration:   3640, Loss function: 2.616, Average Loss: 3.867, avg. samples / sec: 58747.30
Iteration:   3640, Loss function: 3.865, Average Loss: 3.879, avg. samples / sec: 58327.98
Iteration:   3640, Loss function: 3.698, Average Loss: 3.868, avg. samples / sec: 58448.74
Iteration:   3640, Loss function: 4.028, Average Loss: 3.889, avg. samples / sec: 58369.38
Iteration:   3640, Loss function: 2.941, Average Loss: 3.886, avg. samples / sec: 58284.22
Iteration:   3640, Loss function: 3.150, Average Loss: 3.861, avg. samples / sec: 58302.18
Iteration:   3640, Loss function: 3.746, Average Loss: 3.874, avg. samples / sec: 58341.43
Iteration:   3640, Loss function: 3.318, Average Loss: 3.873, avg. samples / sec: 58393.06
Iteration:   3640, Loss function: 3.565, Average Loss: 3.866, avg. samples / sec: 57865.06
Iteration:   3660, Loss function: 4.093, Average Loss: 3.860, avg. samples / sec: 59439.77
Iteration:   3660, Loss function: 3.251, Average Loss: 3.846, avg. samples / sec: 59436.04
Iteration:   3660, Loss function: 3.439, Average Loss: 3.850, avg. samples / sec: 59406.93
Iteration:   3660, Loss function: 2.242, Average Loss: 3.855, avg. samples / sec: 59990.02
Iteration:   3660, Loss function: 3.241, Average Loss: 3.863, avg. samples / sec: 59386.10
Iteration:   3660, Loss function: 3.274, Average Loss: 3.868, avg. samples / sec: 59356.48
Iteration:   3660, Loss function: 2.975, Average Loss: 3.879, avg. samples / sec: 59329.75
Iteration:   3660, Loss function: 2.618, Average Loss: 3.828, avg. samples / sec: 59288.39
Iteration:   3660, Loss function: 2.361, Average Loss: 3.859, avg. samples / sec: 59434.26
Iteration:   3660, Loss function: 2.661, Average Loss: 3.877, avg. samples / sec: 59386.90
Iteration:   3660, Loss function: 4.397, Average Loss: 3.875, avg. samples / sec: 59035.06
Iteration:   3660, Loss function: 2.499, Average Loss: 3.847, avg. samples / sec: 59321.70
Iteration:   3660, Loss function: 3.830, Average Loss: 3.862, avg. samples / sec: 59047.70
Iteration:   3660, Loss function: 3.428, Average Loss: 3.870, avg. samples / sec: 59120.70
Iteration:   3660, Loss function: 3.459, Average Loss: 3.869, avg. samples / sec: 59205.69
Iteration:   3680, Loss function: 3.220, Average Loss: 3.842, avg. samples / sec: 55525.61
Iteration:   3680, Loss function: 3.363, Average Loss: 3.865, avg. samples / sec: 55666.71
Iteration:   3680, Loss function: 3.791, Average Loss: 3.856, avg. samples / sec: 55565.00
Iteration:   3680, Loss function: 2.514, Average Loss: 3.848, avg. samples / sec: 55617.78
Iteration:   3680, Loss function: 3.320, Average Loss: 3.870, avg. samples / sec: 55592.33
Iteration:   3680, Loss function: 3.749, Average Loss: 3.845, avg. samples / sec: 55496.71
Iteration:   3680, Loss function: 2.680, Average Loss: 3.840, avg. samples / sec: 55647.63
Iteration:   3680, Loss function: 3.533, Average Loss: 3.858, avg. samples / sec: 55741.68
Iteration:   3680, Loss function: 3.479, Average Loss: 3.857, avg. samples / sec: 55756.76
Iteration:   3680, Loss function: 3.263, Average Loss: 3.854, avg. samples / sec: 55415.72
Iteration:   3680, Loss function: 3.826, Average Loss: 3.855, avg. samples / sec: 55501.93
Iteration:   3680, Loss function: 3.187, Average Loss: 3.840, avg. samples / sec: 55442.89
Iteration:   3680, Loss function: 3.853, Average Loss: 3.868, avg. samples / sec: 55553.67
Iteration:   3680, Loss function: 4.867, Average Loss: 3.856, avg. samples / sec: 55676.01
Iteration:   3680, Loss function: 2.821, Average Loss: 3.817, avg. samples / sec: 55445.94
:::MLL 1558639501.005 epoch_stop: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 819}}
:::MLL 1558639501.005 epoch_start: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 673}}
Iteration:   3700, Loss function: 2.787, Average Loss: 3.840, avg. samples / sec: 59693.99
Iteration:   3700, Loss function: 4.327, Average Loss: 3.859, avg. samples / sec: 59622.24
Iteration:   3700, Loss function: 3.121, Average Loss: 3.830, avg. samples / sec: 59640.43
Iteration:   3700, Loss function: 4.014, Average Loss: 3.856, avg. samples / sec: 59567.43
Iteration:   3700, Loss function: 3.331, Average Loss: 3.845, avg. samples / sec: 59676.35
Iteration:   3700, Loss function: 3.410, Average Loss: 3.844, avg. samples / sec: 59635.26
Iteration:   3700, Loss function: 4.050, Average Loss: 3.836, avg. samples / sec: 59516.96
Iteration:   3700, Loss function: 3.668, Average Loss: 3.840, avg. samples / sec: 59579.92
Iteration:   3700, Loss function: 3.775, Average Loss: 3.834, avg. samples / sec: 59607.09
Iteration:   3700, Loss function: 2.763, Average Loss: 3.849, avg. samples / sec: 59560.93
Iteration:   3700, Loss function: 3.977, Average Loss: 3.846, avg. samples / sec: 59579.97
Iteration:   3700, Loss function: 3.261, Average Loss: 3.846, avg. samples / sec: 59471.45
Iteration:   3700, Loss function: 2.601, Average Loss: 3.855, avg. samples / sec: 59528.73
Iteration:   3700, Loss function: 2.595, Average Loss: 3.844, avg. samples / sec: 59460.31
Iteration:   3700, Loss function: 2.992, Average Loss: 3.810, avg. samples / sec: 59464.60
Iteration:   3720, Loss function: 2.932, Average Loss: 3.835, avg. samples / sec: 55858.62
Iteration:   3720, Loss function: 3.545, Average Loss: 3.832, avg. samples / sec: 55694.34
Iteration:   3720, Loss function: 2.596, Average Loss: 3.823, avg. samples / sec: 55721.25
Iteration:   3720, Loss function: 3.581, Average Loss: 3.851, avg. samples / sec: 55641.23
Iteration:   3720, Loss function: 4.017, Average Loss: 3.819, avg. samples / sec: 55566.22
Iteration:   3720, Loss function: 3.465, Average Loss: 3.828, avg. samples / sec: 55580.14
Iteration:   3720, Loss function: 3.631, Average Loss: 3.839, avg. samples / sec: 55613.59
Iteration:   3720, Loss function: 3.019, Average Loss: 3.833, avg. samples / sec: 55569.45
Iteration:   3720, Loss function: 4.318, Average Loss: 3.805, avg. samples / sec: 55776.44
Iteration:   3720, Loss function: 2.676, Average Loss: 3.840, avg. samples / sec: 55614.18
Iteration:   3720, Loss function: 4.305, Average Loss: 3.837, avg. samples / sec: 55550.45
Iteration:   3720, Loss function: 4.257, Average Loss: 3.832, avg. samples / sec: 55297.13
Iteration:   3720, Loss function: 3.568, Average Loss: 3.837, avg. samples / sec: 55381.06
Iteration:   3720, Loss function: 3.204, Average Loss: 3.842, avg. samples / sec: 55426.38
Iteration:   3720, Loss function: 3.694, Average Loss: 3.853, avg. samples / sec: 55330.98
Iteration:   3740, Loss function: 4.073, Average Loss: 3.831, avg. samples / sec: 54761.05
Iteration:   3740, Loss function: 3.731, Average Loss: 3.827, avg. samples / sec: 54633.68
Iteration:   3740, Loss function: 3.132, Average Loss: 3.829, avg. samples / sec: 54641.56
Iteration:   3740, Loss function: 3.753, Average Loss: 3.798, avg. samples / sec: 54515.73
Iteration:   3740, Loss function: 2.958, Average Loss: 3.809, avg. samples / sec: 54458.83
Iteration:   3740, Loss function: 4.227, Average Loss: 3.826, avg. samples / sec: 54319.45
Iteration:   3740, Loss function: 4.376, Average Loss: 3.848, avg. samples / sec: 54637.49
Iteration:   3740, Loss function: 2.598, Average Loss: 3.826, avg. samples / sec: 54176.12
Iteration:   3740, Loss function: 4.003, Average Loss: 3.835, avg. samples / sec: 54423.88
Iteration:   3740, Loss function: 3.815, Average Loss: 3.823, avg. samples / sec: 54326.59
Iteration:   3740, Loss function: 3.117, Average Loss: 3.842, avg. samples / sec: 54202.56
Iteration:   3740, Loss function: 3.823, Average Loss: 3.834, avg. samples / sec: 54308.25
Iteration:   3740, Loss function: 2.144, Average Loss: 3.812, avg. samples / sec: 54169.52
Iteration:   3740, Loss function: 4.635, Average Loss: 3.826, avg. samples / sec: 54218.33
Iteration:   3740, Loss function: 3.997, Average Loss: 3.830, avg. samples / sec: 54365.76
Iteration:   3760, Loss function: 3.936, Average Loss: 3.818, avg. samples / sec: 59267.82
Iteration:   3760, Loss function: 2.621, Average Loss: 3.822, avg. samples / sec: 59082.48
Iteration:   3760, Loss function: 4.068, Average Loss: 3.824, avg. samples / sec: 58900.66
Iteration:   3760, Loss function: 2.502, Average Loss: 3.813, avg. samples / sec: 59191.27
Iteration:   3760, Loss function: 3.829, Average Loss: 3.805, avg. samples / sec: 59238.02
Iteration:   3760, Loss function: 4.138, Average Loss: 3.815, avg. samples / sec: 59304.15
Iteration:   3760, Loss function: 3.864, Average Loss: 3.785, avg. samples / sec: 58932.71
Iteration:   3760, Loss function: 3.120, Average Loss: 3.823, avg. samples / sec: 59057.87
Iteration:   3760, Loss function: 3.617, Average Loss: 3.832, avg. samples / sec: 59028.11
Iteration:   3760, Loss function: 3.964, Average Loss: 3.799, avg. samples / sec: 58829.36
Iteration:   3760, Loss function: 3.344, Average Loss: 3.812, avg. samples / sec: 58805.13
Iteration:   3760, Loss function: 3.356, Average Loss: 3.821, avg. samples / sec: 59129.88
Iteration:   3760, Loss function: 3.237, Average Loss: 3.818, avg. samples / sec: 58795.98
Iteration:   3760, Loss function: 3.974, Average Loss: 3.820, avg. samples / sec: 58950.38
Iteration:   3760, Loss function: 3.837, Average Loss: 3.840, avg. samples / sec: 58720.75
:::MLL 1558639503.087 epoch_stop: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 819}}
:::MLL 1558639503.087 epoch_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 673}}
Iteration:   3780, Loss function: 2.841, Average Loss: 3.805, avg. samples / sec: 57051.06
Iteration:   3780, Loss function: 3.492, Average Loss: 3.805, avg. samples / sec: 57179.99
Iteration:   3780, Loss function: 3.144, Average Loss: 3.819, avg. samples / sec: 56948.06
Iteration:   3780, Loss function: 3.099, Average Loss: 3.833, avg. samples / sec: 57343.46
Iteration:   3780, Loss function: 2.981, Average Loss: 3.817, avg. samples / sec: 56967.65
Iteration:   3780, Loss function: 2.545, Average Loss: 3.805, avg. samples / sec: 57197.28
Iteration:   3780, Loss function: 4.125, Average Loss: 3.824, avg. samples / sec: 57153.91
Iteration:   3780, Loss function: 3.404, Average Loss: 3.813, avg. samples / sec: 57048.80
Iteration:   3780, Loss function: 3.398, Average Loss: 3.791, avg. samples / sec: 57149.64
Iteration:   3780, Loss function: 3.820, Average Loss: 3.808, avg. samples / sec: 57202.14
Iteration:   3780, Loss function: 4.266, Average Loss: 3.778, avg. samples / sec: 57030.14
Iteration:   3780, Loss function: 2.430, Average Loss: 3.807, avg. samples / sec: 57156.41
Iteration:   3780, Loss function: 3.382, Average Loss: 3.795, avg. samples / sec: 56924.29
Iteration:   3780, Loss function: 2.116, Average Loss: 3.802, avg. samples / sec: 56913.12
Iteration:   3780, Loss function: 2.182, Average Loss: 3.811, avg. samples / sec: 57133.31
Iteration:   3800, Loss function: 2.971, Average Loss: 3.802, avg. samples / sec: 57258.70
Iteration:   3800, Loss function: 3.108, Average Loss: 3.797, avg. samples / sec: 57122.72
Iteration:   3800, Loss function: 4.533, Average Loss: 3.815, avg. samples / sec: 57120.22
Iteration:   3800, Loss function: 3.000, Average Loss: 3.799, avg. samples / sec: 56930.69
Iteration:   3800, Loss function: 3.655, Average Loss: 3.769, avg. samples / sec: 57170.30
Iteration:   3800, Loss function: 3.266, Average Loss: 3.824, avg. samples / sec: 57089.19
Iteration:   3800, Loss function: 3.560, Average Loss: 3.789, avg. samples / sec: 57125.48
Iteration:   3800, Loss function: 3.832, Average Loss: 3.813, avg. samples / sec: 57014.96
Iteration:   3800, Loss function: 3.062, Average Loss: 3.798, avg. samples / sec: 56890.35
Iteration:   3800, Loss function: 2.562, Average Loss: 3.801, avg. samples / sec: 57058.71
Iteration:   3800, Loss function: 2.907, Average Loss: 3.795, avg. samples / sec: 57018.95
Iteration:   3800, Loss function: 2.926, Average Loss: 3.804, avg. samples / sec: 57008.92
Iteration:   3800, Loss function: 3.545, Average Loss: 3.813, avg. samples / sec: 56973.70
Iteration:   3800, Loss function: 2.945, Average Loss: 3.796, avg. samples / sec: 57031.67
Iteration:   3800, Loss function: 3.756, Average Loss: 3.781, avg. samples / sec: 56920.50
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
:::MLL 1558639504.259 eval_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.63 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.50s)
DONE (t=0.49s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.49s)
DONE (t=0.50s)
DONE (t=0.49s)
DONE (t=0.49s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.50s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.52s)
DONE (t=0.55s)
DONE (t=2.69s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22493
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.38741
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.22767
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05787
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.23633
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.36673
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.21749
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.31856
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33428
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09774
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36447
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.51729
Current AP: 0.22493 AP goal: 0.23000
:::MLL 1558639508.150 eval_accuracy: {"value": 0.22493091348064773, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 389}}
:::MLL 1558639508.220 eval_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 392}}
:::MLL 1558639508.230 block_stop: {"value": null, "metadata": {"first_epoch_num": 49, "file": "train.py", "lineno": 804}}
:::MLL 1558639508.230 block_start: {"value": null, "metadata": {"first_epoch_num": 55, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3820, Loss function: 2.417, Average Loss: 3.809, avg. samples / sec: 7401.38
Iteration:   3820, Loss function: 3.414, Average Loss: 3.784, avg. samples / sec: 7402.35
Iteration:   3820, Loss function: 3.029, Average Loss: 3.790, avg. samples / sec: 7400.75
Iteration:   3820, Loss function: 2.388, Average Loss: 3.814, avg. samples / sec: 7398.69
Iteration:   3820, Loss function: 4.256, Average Loss: 3.792, avg. samples / sec: 7397.93
Iteration:   3820, Loss function: 2.554, Average Loss: 3.792, avg. samples / sec: 7400.12
Iteration:   3820, Loss function: 3.047, Average Loss: 3.785, avg. samples / sec: 7400.76
Iteration:   3820, Loss function: 2.566, Average Loss: 3.773, avg. samples / sec: 7401.09
Iteration:   3820, Loss function: 2.899, Average Loss: 3.803, avg. samples / sec: 7400.07
Iteration:   3820, Loss function: 3.463, Average Loss: 3.803, avg. samples / sec: 7399.06
Iteration:   3820, Loss function: 4.398, Average Loss: 3.787, avg. samples / sec: 7398.67
Iteration:   3820, Loss function: 2.921, Average Loss: 3.788, avg. samples / sec: 7397.98
Iteration:   3820, Loss function: 3.744, Average Loss: 3.784, avg. samples / sec: 7395.81
Iteration:   3820, Loss function: 2.598, Average Loss: 3.794, avg. samples / sec: 7398.07
Iteration:   3820, Loss function: 4.168, Average Loss: 3.765, avg. samples / sec: 7394.76
:::MLL 1558639509.093 epoch_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 819}}
:::MLL 1558639509.094 epoch_start: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 673}}
Iteration:   3840, Loss function: 3.664, Average Loss: 3.778, avg. samples / sec: 58208.72
Iteration:   3840, Loss function: 2.503, Average Loss: 3.809, avg. samples / sec: 58224.35
Iteration:   3840, Loss function: 3.046, Average Loss: 3.792, avg. samples / sec: 58230.30
Iteration:   3840, Loss function: 4.039, Average Loss: 3.764, avg. samples / sec: 58224.91
Iteration:   3840, Loss function: 3.938, Average Loss: 3.783, avg. samples / sec: 58297.65
Iteration:   3840, Loss function: 3.253, Average Loss: 3.773, avg. samples / sec: 58188.58
Iteration:   3840, Loss function: 2.984, Average Loss: 3.794, avg. samples / sec: 58167.40
Iteration:   3840, Loss function: 4.382, Average Loss: 3.787, avg. samples / sec: 58071.72
Iteration:   3840, Loss function: 3.509, Average Loss: 3.779, avg. samples / sec: 58207.69
Iteration:   3840, Loss function: 3.439, Average Loss: 3.785, avg. samples / sec: 58055.97
Iteration:   3840, Loss function: 3.717, Average Loss: 3.786, avg. samples / sec: 58210.36
Iteration:   3840, Loss function: 3.107, Average Loss: 3.757, avg. samples / sec: 58246.71
Iteration:   3840, Loss function: 4.321, Average Loss: 3.773, avg. samples / sec: 57858.03
Iteration:   3840, Loss function: 3.891, Average Loss: 3.798, avg. samples / sec: 57816.91
Iteration:   3840, Loss function: 3.064, Average Loss: 3.776, avg. samples / sec: 57914.31
Iteration:   3860, Loss function: 3.263, Average Loss: 3.779, avg. samples / sec: 59349.96
Iteration:   3860, Loss function: 2.178, Average Loss: 3.793, avg. samples / sec: 59481.22
Iteration:   3860, Loss function: 3.199, Average Loss: 3.789, avg. samples / sec: 59239.86
Iteration:   3860, Loss function: 4.477, Average Loss: 3.774, avg. samples / sec: 59342.61
Iteration:   3860, Loss function: 3.004, Average Loss: 3.767, avg. samples / sec: 59282.48
Iteration:   3860, Loss function: 4.290, Average Loss: 3.800, avg. samples / sec: 59159.24
Iteration:   3860, Loss function: 4.777, Average Loss: 3.779, avg. samples / sec: 59253.81
Iteration:   3860, Loss function: 2.838, Average Loss: 3.763, avg. samples / sec: 59147.82
Iteration:   3860, Loss function: 2.204, Average Loss: 3.769, avg. samples / sec: 59282.03
Iteration:   3860, Loss function: 3.434, Average Loss: 3.773, avg. samples / sec: 59202.21
Iteration:   3860, Loss function: 2.748, Average Loss: 3.770, avg. samples / sec: 59198.23
Iteration:   3860, Loss function: 2.648, Average Loss: 3.782, avg. samples / sec: 59108.55
Iteration:   3860, Loss function: 3.641, Average Loss: 3.771, avg. samples / sec: 58859.14
Iteration:   3860, Loss function: 2.953, Average Loss: 3.749, avg. samples / sec: 59155.39
Iteration:   3860, Loss function: 3.509, Average Loss: 3.767, avg. samples / sec: 59140.13
Iteration:   3880, Loss function: 3.462, Average Loss: 3.763, avg. samples / sec: 59076.93
Iteration:   3880, Loss function: 2.704, Average Loss: 3.791, avg. samples / sec: 59071.46
Iteration:   3880, Loss function: 3.737, Average Loss: 3.754, avg. samples / sec: 59425.84
Iteration:   3880, Loss function: 2.140, Average Loss: 3.766, avg. samples / sec: 59131.89
Iteration:   3880, Loss function: 3.804, Average Loss: 3.777, avg. samples / sec: 59108.70
Iteration:   3880, Loss function: 3.616, Average Loss: 3.743, avg. samples / sec: 59110.24
Iteration:   3880, Loss function: 2.460, Average Loss: 3.784, avg. samples / sec: 58857.76
Iteration:   3880, Loss function: 3.164, Average Loss: 3.765, avg. samples / sec: 58991.54
Iteration:   3880, Loss function: 3.918, Average Loss: 3.771, avg. samples / sec: 58716.95
Iteration:   3880, Loss function: 3.118, Average Loss: 3.783, avg. samples / sec: 58723.14
Iteration:   3880, Loss function: 4.393, Average Loss: 3.753, avg. samples / sec: 58884.94
Iteration:   3880, Loss function: 2.163, Average Loss: 3.760, avg. samples / sec: 58939.04
Iteration:   3880, Loss function: 2.635, Average Loss: 3.759, avg. samples / sec: 58813.72
Iteration:   3880, Loss function: 3.033, Average Loss: 3.767, avg. samples / sec: 58684.73
Iteration:   3880, Loss function: 3.353, Average Loss: 3.774, avg. samples / sec: 58704.85
Iteration:   3900, Loss function: 2.183, Average Loss: 3.743, avg. samples / sec: 58938.48
Iteration:   3900, Loss function: 3.032, Average Loss: 3.779, avg. samples / sec: 58898.77
Iteration:   3900, Loss function: 3.222, Average Loss: 3.755, avg. samples / sec: 58583.59
Iteration:   3900, Loss function: 3.187, Average Loss: 3.736, avg. samples / sec: 58667.41
Iteration:   3900, Loss function: 2.990, Average Loss: 3.765, avg. samples / sec: 58619.80
Iteration:   3900, Loss function: 3.150, Average Loss: 3.749, avg. samples / sec: 58504.81
Iteration:   3900, Loss function: 3.675, Average Loss: 3.778, avg. samples / sec: 58474.54
Iteration:   3900, Loss function: 2.358, Average Loss: 3.750, avg. samples / sec: 58806.46
Iteration:   3900, Loss function: 3.299, Average Loss: 3.757, avg. samples / sec: 58746.96
Iteration:   3900, Loss function: 2.577, Average Loss: 3.768, avg. samples / sec: 58866.49
Iteration:   3900, Loss function: 4.588, Average Loss: 3.760, avg. samples / sec: 58427.58
Iteration:   3900, Loss function: 3.735, Average Loss: 3.755, avg. samples / sec: 58558.46
Iteration:   3900, Loss function: 4.030, Average Loss: 3.780, avg. samples / sec: 58515.16
Iteration:   3900, Loss function: 2.340, Average Loss: 3.761, avg. samples / sec: 58614.09
Iteration:   3900, Loss function: 4.194, Average Loss: 3.765, avg. samples / sec: 58407.68
:::MLL 1558639511.088 epoch_stop: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 819}}
:::MLL 1558639511.088 epoch_start: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 673}}
Iteration:   3920, Loss function: 3.310, Average Loss: 3.746, avg. samples / sec: 59406.98
Iteration:   3920, Loss function: 4.169, Average Loss: 3.780, avg. samples / sec: 59414.34
Iteration:   3920, Loss function: 3.865, Average Loss: 3.748, avg. samples / sec: 59096.31
Iteration:   3920, Loss function: 2.293, Average Loss: 3.751, avg. samples / sec: 59212.83
Iteration:   3920, Loss function: 3.528, Average Loss: 3.737, avg. samples / sec: 58981.00
Iteration:   3920, Loss function: 3.200, Average Loss: 3.753, avg. samples / sec: 59156.51
Iteration:   3920, Loss function: 4.612, Average Loss: 3.772, avg. samples / sec: 59159.81
Iteration:   3920, Loss function: 3.372, Average Loss: 3.744, avg. samples / sec: 59156.56
Iteration:   3920, Loss function: 3.691, Average Loss: 3.745, avg. samples / sec: 59100.54
Iteration:   3920, Loss function: 4.056, Average Loss: 3.758, avg. samples / sec: 59342.86
Iteration:   3920, Loss function: 4.103, Average Loss: 3.771, avg. samples / sec: 58912.19
Iteration:   3920, Loss function: 3.106, Average Loss: 3.762, avg. samples / sec: 59128.96
Iteration:   3920, Loss function: 2.376, Average Loss: 3.749, avg. samples / sec: 59195.85
Iteration:   3920, Loss function: 2.740, Average Loss: 3.730, avg. samples / sec: 59000.85
Iteration:   3920, Loss function: 3.065, Average Loss: 3.755, avg. samples / sec: 59317.36
Iteration:   3940, Loss function: 2.637, Average Loss: 3.733, avg. samples / sec: 58591.48
Iteration:   3940, Loss function: 2.696, Average Loss: 3.745, avg. samples / sec: 58553.18
Iteration:   3940, Loss function: 3.095, Average Loss: 3.741, avg. samples / sec: 58447.28
Iteration:   3940, Loss function: 4.237, Average Loss: 3.751, avg. samples / sec: 58545.18
Iteration:   3940, Loss function: 2.439, Average Loss: 3.728, avg. samples / sec: 58594.11
Iteration:   3940, Loss function: 2.747, Average Loss: 3.729, avg. samples / sec: 58438.87
Iteration:   3940, Loss function: 3.796, Average Loss: 3.749, avg. samples / sec: 58586.70
Iteration:   3940, Loss function: 2.973, Average Loss: 3.766, avg. samples / sec: 58465.40
Iteration:   3940, Loss function: 2.793, Average Loss: 3.774, avg. samples / sec: 58296.44
Iteration:   3940, Loss function: 3.226, Average Loss: 3.738, avg. samples / sec: 58306.09
Iteration:   3940, Loss function: 3.024, Average Loss: 3.741, avg. samples / sec: 58467.72
Iteration:   3940, Loss function: 3.120, Average Loss: 3.734, avg. samples / sec: 58400.15
Iteration:   3940, Loss function: 3.084, Average Loss: 3.754, avg. samples / sec: 58404.31
Iteration:   3940, Loss function: 3.732, Average Loss: 3.763, avg. samples / sec: 58284.60
Iteration:   3940, Loss function: 2.838, Average Loss: 3.740, avg. samples / sec: 58197.23
Iteration:   3960, Loss function: 2.743, Average Loss: 3.743, avg. samples / sec: 59903.73
Iteration:   3960, Loss function: 2.492, Average Loss: 3.760, avg. samples / sec: 59886.04
Iteration:   3960, Loss function: 3.157, Average Loss: 3.726, avg. samples / sec: 59906.89
Iteration:   3960, Loss function: 3.272, Average Loss: 3.725, avg. samples / sec: 59663.28
Iteration:   3960, Loss function: 3.240, Average Loss: 3.720, avg. samples / sec: 59752.30
Iteration:   3960, Loss function: 2.638, Average Loss: 3.732, avg. samples / sec: 59813.90
Iteration:   3960, Loss function: 4.076, Average Loss: 3.743, avg. samples / sec: 59857.98
Iteration:   3960, Loss function: 3.416, Average Loss: 3.731, avg. samples / sec: 59950.13
Iteration:   3960, Loss function: 2.903, Average Loss: 3.730, avg. samples / sec: 59618.68
Iteration:   3960, Loss function: 3.253, Average Loss: 3.713, avg. samples / sec: 59641.64
Iteration:   3960, Loss function: 2.992, Average Loss: 3.741, avg. samples / sec: 59593.70
Iteration:   3960, Loss function: 3.291, Average Loss: 3.763, avg. samples / sec: 59644.95
Iteration:   3960, Loss function: 2.727, Average Loss: 3.735, avg. samples / sec: 59436.26
Iteration:   3960, Loss function: 3.795, Average Loss: 3.754, avg. samples / sec: 59731.21
Iteration:   3960, Loss function: 3.516, Average Loss: 3.732, avg. samples / sec: 59506.46
:::MLL 1558639513.088 epoch_stop: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 819}}
:::MLL 1558639513.089 epoch_start: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 673}}
Iteration:   3980, Loss function: 3.736, Average Loss: 3.724, avg. samples / sec: 58226.23
Iteration:   3980, Loss function: 3.348, Average Loss: 3.750, avg. samples / sec: 58195.53
Iteration:   3980, Loss function: 3.211, Average Loss: 3.719, avg. samples / sec: 57965.52
Iteration:   3980, Loss function: 3.381, Average Loss: 3.706, avg. samples / sec: 58078.63
Iteration:   3980, Loss function: 2.914, Average Loss: 3.718, avg. samples / sec: 58021.72
Iteration:   3980, Loss function: 2.840, Average Loss: 3.727, avg. samples / sec: 58078.99
Iteration:   3980, Loss function: 3.109, Average Loss: 3.704, avg. samples / sec: 57934.64
Iteration:   3980, Loss function: 3.278, Average Loss: 3.725, avg. samples / sec: 58229.82
Iteration:   3980, Loss function: 2.582, Average Loss: 3.750, avg. samples / sec: 57847.39
Iteration:   3980, Loss function: 3.784, Average Loss: 3.724, avg. samples / sec: 57933.28
Iteration:   3980, Loss function: 4.151, Average Loss: 3.731, avg. samples / sec: 58085.00
Iteration:   3980, Loss function: 4.080, Average Loss: 3.751, avg. samples / sec: 58074.97
Iteration:   3980, Loss function: 3.417, Average Loss: 3.719, avg. samples / sec: 57830.18
Iteration:   3980, Loss function: 2.079, Average Loss: 3.733, avg. samples / sec: 57818.08
Iteration:   3980, Loss function: 3.924, Average Loss: 3.734, avg. samples / sec: 57682.92
Iteration:   4000, Loss function: 3.435, Average Loss: 3.725, avg. samples / sec: 59769.23
Iteration:   4000, Loss function: 3.072, Average Loss: 3.744, avg. samples / sec: 59634.50
Iteration:   4000, Loss function: 2.692, Average Loss: 3.739, avg. samples / sec: 59501.61
Iteration:   4000, Loss function: 3.251, Average Loss: 3.718, avg. samples / sec: 59545.63
Iteration:   4000, Loss function: 3.078, Average Loss: 3.712, avg. samples / sec: 59502.72
Iteration:   4000, Loss function: 3.302, Average Loss: 3.702, avg. samples / sec: 59501.31
Iteration:   4000, Loss function: 2.810, Average Loss: 3.698, avg. samples / sec: 59507.87
Iteration:   4000, Loss function: 2.029, Average Loss: 3.710, avg. samples / sec: 59570.27
Iteration:   4000, Loss function: 6.195, Average Loss: 3.749, avg. samples / sec: 59540.15
Iteration:   4000, Loss function: 3.085, Average Loss: 3.717, avg. samples / sec: 59478.93
Iteration:   4000, Loss function: 3.939, Average Loss: 3.715, avg. samples / sec: 59220.12
Iteration:   4000, Loss function: 3.550, Average Loss: 3.708, avg. samples / sec: 59345.14
Iteration:   4000, Loss function: 3.156, Average Loss: 3.719, avg. samples / sec: 59341.36
Iteration:   4000, Loss function: 3.792, Average Loss: 3.724, avg. samples / sec: 59332.02
Iteration:   4000, Loss function: 2.931, Average Loss: 3.723, avg. samples / sec: 59346.29
Iteration:   4020, Loss function: 3.522, Average Loss: 3.692, avg. samples / sec: 60074.61
Iteration:   4020, Loss function: 3.254, Average Loss: 3.709, avg. samples / sec: 60258.53
Iteration:   4020, Loss function: 3.458, Average Loss: 3.738, avg. samples / sec: 59900.57
Iteration:   4020, Loss function: 3.539, Average Loss: 3.718, avg. samples / sec: 60302.83
Iteration:   4020, Loss function: 3.364, Average Loss: 3.704, avg. samples / sec: 60099.44
Iteration:   4020, Loss function: 2.446, Average Loss: 3.703, avg. samples / sec: 59940.44
Iteration:   4020, Loss function: 3.905, Average Loss: 3.706, avg. samples / sec: 59964.98
Iteration:   4020, Loss function: 2.718, Average Loss: 3.739, avg. samples / sec: 59931.98
Iteration:   4020, Loss function: 2.282, Average Loss: 3.711, avg. samples / sec: 59782.64
Iteration:   4020, Loss function: 3.073, Average Loss: 3.717, avg. samples / sec: 60035.41
Iteration:   4020, Loss function: 4.383, Average Loss: 3.710, avg. samples / sec: 59743.31
Iteration:   4020, Loss function: 3.906, Average Loss: 3.693, avg. samples / sec: 59739.41
Iteration:   4020, Loss function: 2.784, Average Loss: 3.732, avg. samples / sec: 59678.44
Iteration:   4020, Loss function: 3.675, Average Loss: 3.718, avg. samples / sec: 59648.76
Iteration:   4020, Loss function: 3.387, Average Loss: 3.708, avg. samples / sec: 59742.40
Iteration:   4040, Loss function: 3.317, Average Loss: 3.712, avg. samples / sec: 58701.47
Iteration:   4040, Loss function: 3.402, Average Loss: 3.686, avg. samples / sec: 58859.51
Iteration:   4040, Loss function: 3.059, Average Loss: 3.689, avg. samples / sec: 58565.06
Iteration:   4040, Loss function: 3.799, Average Loss: 3.699, avg. samples / sec: 58676.96
Iteration:   4040, Loss function: 2.182, Average Loss: 3.698, avg. samples / sec: 58715.80
Iteration:   4040, Loss function: 2.980, Average Loss: 3.709, avg. samples / sec: 58823.42
Iteration:   4040, Loss function: 3.068, Average Loss: 3.707, avg. samples / sec: 58766.07
Iteration:   4040, Loss function: 4.525, Average Loss: 3.702, avg. samples / sec: 58771.41
Iteration:   4040, Loss function: 3.124, Average Loss: 3.696, avg. samples / sec: 58622.38
Iteration:   4040, Loss function: 3.363, Average Loss: 3.729, avg. samples / sec: 58649.02
Iteration:   4040, Loss function: 2.721, Average Loss: 3.697, avg. samples / sec: 58483.13
Iteration:   4040, Loss function: 3.646, Average Loss: 3.710, avg. samples / sec: 58682.97
Iteration:   4040, Loss function: 2.942, Average Loss: 3.732, avg. samples / sec: 58506.32
Iteration:   4040, Loss function: 4.453, Average Loss: 3.725, avg. samples / sec: 58703.60
Iteration:   4040, Loss function: 3.435, Average Loss: 3.700, avg. samples / sec: 58515.45
:::MLL 1558639515.070 epoch_stop: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 819}}
:::MLL 1558639515.071 epoch_start: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 673}}
Iteration:   4060, Loss function: 3.641, Average Loss: 3.694, avg. samples / sec: 58037.78
Iteration:   4060, Loss function: 3.602, Average Loss: 3.683, avg. samples / sec: 57998.37
Iteration:   4060, Loss function: 2.723, Average Loss: 3.683, avg. samples / sec: 58056.81
Iteration:   4060, Loss function: 2.669, Average Loss: 3.678, avg. samples / sec: 57977.97
Iteration:   4060, Loss function: 3.358, Average Loss: 3.694, avg. samples / sec: 57963.42
Iteration:   4060, Loss function: 3.663, Average Loss: 3.706, avg. samples / sec: 57913.95
Iteration:   4060, Loss function: 4.289, Average Loss: 3.693, avg. samples / sec: 58333.99
Iteration:   4060, Loss function: 2.992, Average Loss: 3.704, avg. samples / sec: 58034.10
Iteration:   4060, Loss function: 3.402, Average Loss: 3.691, avg. samples / sec: 57989.92
Iteration:   4060, Loss function: 2.174, Average Loss: 3.719, avg. samples / sec: 58031.42
Iteration:   4060, Loss function: 4.201, Average Loss: 3.699, avg. samples / sec: 57955.08
Iteration:   4060, Loss function: 2.739, Average Loss: 3.695, avg. samples / sec: 57901.12
Iteration:   4060, Loss function: 4.147, Average Loss: 3.721, avg. samples / sec: 57923.85
Iteration:   4060, Loss function: 3.467, Average Loss: 3.706, avg. samples / sec: 57880.67
Iteration:   4060, Loss function: 3.558, Average Loss: 3.723, avg. samples / sec: 57966.43
Iteration:   4080, Loss function: 3.509, Average Loss: 3.698, avg. samples / sec: 59955.82
Iteration:   4080, Loss function: 2.563, Average Loss: 3.683, avg. samples / sec: 59894.05
Iteration:   4080, Loss function: 2.611, Average Loss: 3.697, avg. samples / sec: 59776.66
Iteration:   4080, Loss function: 2.668, Average Loss: 3.684, avg. samples / sec: 59721.08
Iteration:   4080, Loss function: 3.432, Average Loss: 3.696, avg. samples / sec: 59705.85
Iteration:   4080, Loss function: 3.550, Average Loss: 3.682, avg. samples / sec: 59672.91
Iteration:   4080, Loss function: 2.321, Average Loss: 3.691, avg. samples / sec: 59737.41
Iteration:   4080, Loss function: 2.627, Average Loss: 3.715, avg. samples / sec: 59724.75
Iteration:   4080, Loss function: 3.955, Average Loss: 3.675, avg. samples / sec: 59593.35
Iteration:   4080, Loss function: 4.075, Average Loss: 3.716, avg. samples / sec: 59740.42
Iteration:   4080, Loss function: 3.749, Average Loss: 3.673, avg. samples / sec: 59511.74
Iteration:   4080, Loss function: 2.583, Average Loss: 3.689, avg. samples / sec: 59443.31
Iteration:   4080, Loss function: 3.454, Average Loss: 3.676, avg. samples / sec: 59457.86
Iteration:   4080, Loss function: 3.151, Average Loss: 3.690, avg. samples / sec: 59578.71
Iteration:   4080, Loss function: 3.220, Average Loss: 3.713, avg. samples / sec: 59332.97
Iteration:   4100, Loss function: 3.254, Average Loss: 3.717, avg. samples / sec: 57638.57
Iteration:   4100, Loss function: 3.628, Average Loss: 3.686, avg. samples / sec: 57750.22
Iteration:   4100, Loss function: 2.771, Average Loss: 3.677, avg. samples / sec: 57514.82
Iteration:   4100, Loss function: 3.781, Average Loss: 3.688, avg. samples / sec: 57680.35
Iteration:   4100, Loss function: 2.666, Average Loss: 3.689, avg. samples / sec: 57385.70
Iteration:   4100, Loss function: 3.851, Average Loss: 3.709, avg. samples / sec: 57943.50
Iteration:   4100, Loss function: 3.873, Average Loss: 3.708, avg. samples / sec: 57557.00
Iteration:   4100, Loss function: 3.842, Average Loss: 3.670, avg. samples / sec: 57631.01
Iteration:   4100, Loss function: 4.275, Average Loss: 3.696, avg. samples / sec: 57447.37
Iteration:   4100, Loss function: 2.753, Average Loss: 3.681, avg. samples / sec: 57465.36
Iteration:   4100, Loss function: 3.106, Average Loss: 3.685, avg. samples / sec: 57400.99
Iteration:   4100, Loss function: 4.213, Average Loss: 3.676, avg. samples / sec: 57311.70
Iteration:   4100, Loss function: 3.425, Average Loss: 3.672, avg. samples / sec: 57554.63
Iteration:   4100, Loss function: 3.314, Average Loss: 3.673, avg. samples / sec: 57399.19
Iteration:   4100, Loss function: 3.414, Average Loss: 3.672, avg. samples / sec: 57429.71
:::MLL 1558639517.076 epoch_stop: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 819}}
:::MLL 1558639517.076 epoch_start: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 673}}
Iteration:   4120, Loss function: 2.581, Average Loss: 3.669, avg. samples / sec: 60040.32
Iteration:   4120, Loss function: 3.462, Average Loss: 3.665, avg. samples / sec: 60191.46
Iteration:   4120, Loss function: 4.029, Average Loss: 3.702, avg. samples / sec: 59970.92
Iteration:   4120, Loss function: 3.722, Average Loss: 3.703, avg. samples / sec: 59940.47
Iteration:   4120, Loss function: 3.697, Average Loss: 3.675, avg. samples / sec: 59876.19
Iteration:   4120, Loss function: 3.880, Average Loss: 3.680, avg. samples / sec: 59993.52
Iteration:   4120, Loss function: 3.821, Average Loss: 3.666, avg. samples / sec: 60049.91
Iteration:   4120, Loss function: 3.159, Average Loss: 3.665, avg. samples / sec: 59984.22
Iteration:   4120, Loss function: 2.455, Average Loss: 3.677, avg. samples / sec: 59962.15
Iteration:   4120, Loss function: 3.146, Average Loss: 3.689, avg. samples / sec: 59909.66
Iteration:   4120, Loss function: 3.097, Average Loss: 3.669, avg. samples / sec: 59949.21
Iteration:   4120, Loss function: 2.374, Average Loss: 3.664, avg. samples / sec: 59845.22
Iteration:   4120, Loss function: 3.189, Average Loss: 3.679, avg. samples / sec: 59752.53
Iteration:   4120, Loss function: 3.229, Average Loss: 3.715, avg. samples / sec: 59633.32
Iteration:   4120, Loss function: 3.593, Average Loss: 3.685, avg. samples / sec: 59718.20
Iteration:   4140, Loss function: 4.727, Average Loss: 3.678, avg. samples / sec: 56334.81
Iteration:   4140, Loss function: 2.760, Average Loss: 3.694, avg. samples / sec: 56117.88
Iteration:   4140, Loss function: 3.285, Average Loss: 3.674, avg. samples / sec: 56228.29
Iteration:   4140, Loss function: 2.945, Average Loss: 3.696, avg. samples / sec: 56069.90
Iteration:   4140, Loss function: 4.777, Average Loss: 3.667, avg. samples / sec: 55933.00
Iteration:   4140, Loss function: 2.601, Average Loss: 3.704, avg. samples / sec: 56203.85
Iteration:   4140, Loss function: 3.369, Average Loss: 3.684, avg. samples / sec: 56097.82
Iteration:   4140, Loss function: 3.130, Average Loss: 3.668, avg. samples / sec: 56024.12
Iteration:   4140, Loss function: 3.735, Average Loss: 3.661, avg. samples / sec: 56150.59
Iteration:   4140, Loss function: 2.369, Average Loss: 3.657, avg. samples / sec: 56042.10
Iteration:   4140, Loss function: 3.231, Average Loss: 3.658, avg. samples / sec: 56038.26
Iteration:   4140, Loss function: 4.005, Average Loss: 3.670, avg. samples / sec: 56014.14
Iteration:   4140, Loss function: 2.090, Average Loss: 3.654, avg. samples / sec: 55871.88
Iteration:   4140, Loss function: 2.565, Average Loss: 3.667, avg. samples / sec: 55956.05
Iteration:   4140, Loss function: 4.547, Average Loss: 3.659, avg. samples / sec: 55930.22
Iteration:   4160, Loss function: 4.374, Average Loss: 3.667, avg. samples / sec: 59195.42
Iteration:   4160, Loss function: 3.304, Average Loss: 3.649, avg. samples / sec: 59266.22
Iteration:   4160, Loss function: 2.297, Average Loss: 3.687, avg. samples / sec: 59128.89
Iteration:   4160, Loss function: 4.136, Average Loss: 3.660, avg. samples / sec: 59243.85
Iteration:   4160, Loss function: 3.137, Average Loss: 3.654, avg. samples / sec: 59407.73
Iteration:   4160, Loss function: 3.212, Average Loss: 3.647, avg. samples / sec: 59257.15
Iteration:   4160, Loss function: 3.192, Average Loss: 3.687, avg. samples / sec: 59159.14
Iteration:   4160, Loss function: 3.662, Average Loss: 3.654, avg. samples / sec: 59196.77
Iteration:   4160, Loss function: 4.725, Average Loss: 3.677, avg. samples / sec: 59184.04
Iteration:   4160, Loss function: 2.931, Average Loss: 3.663, avg. samples / sec: 59130.00
Iteration:   4160, Loss function: 3.393, Average Loss: 3.668, avg. samples / sec: 58999.67
Iteration:   4160, Loss function: 3.905, Average Loss: 3.698, avg. samples / sec: 59107.36
Iteration:   4160, Loss function: 3.871, Average Loss: 3.671, avg. samples / sec: 59093.75
Iteration:   4160, Loss function: 3.382, Average Loss: 3.662, avg. samples / sec: 59037.31
Iteration:   4160, Loss function: 2.708, Average Loss: 3.670, avg. samples / sec: 59113.01
Iteration:   4180, Loss function: 3.901, Average Loss: 3.648, avg. samples / sec: 59175.12
Iteration:   4180, Loss function: 2.896, Average Loss: 3.681, avg. samples / sec: 59103.37
Iteration:   4180, Loss function: 4.267, Average Loss: 3.658, avg. samples / sec: 59009.92
Iteration:   4180, Loss function: 4.232, Average Loss: 3.659, avg. samples / sec: 59069.28
Iteration:   4180, Loss function: 4.376, Average Loss: 3.653, avg. samples / sec: 59044.41
Iteration:   4180, Loss function: 3.002, Average Loss: 3.653, avg. samples / sec: 58946.04
Iteration:   4180, Loss function: 3.844, Average Loss: 3.642, avg. samples / sec: 58918.39
Iteration:   4180, Loss function: 3.264, Average Loss: 3.664, avg. samples / sec: 59157.11
Iteration:   4180, Loss function: 4.037, Average Loss: 3.695, avg. samples / sec: 59008.93
Iteration:   4180, Loss function: 4.345, Average Loss: 3.650, avg. samples / sec: 58888.31
Iteration:   4180, Loss function: 3.340, Average Loss: 3.658, avg. samples / sec: 59085.01
Iteration:   4180, Loss function: 4.155, Average Loss: 3.683, avg. samples / sec: 58869.02
Iteration:   4180, Loss function: 3.987, Average Loss: 3.673, avg. samples / sec: 58971.62
Iteration:   4180, Loss function: 3.581, Average Loss: 3.668, avg. samples / sec: 58799.93
Iteration:   4180, Loss function: 2.901, Average Loss: 3.640, avg. samples / sec: 58734.50
:::MLL 1558639519.098 epoch_stop: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 819}}
:::MLL 1558639519.098 epoch_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 673}}
:::MLL 1558639519.159 eval_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.51s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.51s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.54s)
DONE (t=0.55s)
DONE (t=0.57s)
DONE (t=2.78s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22954
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.39324
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23380
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05901
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.23955
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.37785
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22183
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32425
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.34056
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09963
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.37029
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.53150
Current AP: 0.22954 AP goal: 0.23000
:::MLL 1558639523.160 eval_accuracy: {"value": 0.22954348336983227, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 389}}
:::MLL 1558639523.227 eval_stop: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 392}}
:::MLL 1558639523.237 block_stop: {"value": null, "metadata": {"first_epoch_num": 55, "file": "train.py", "lineno": 804}}
:::MLL 1558639523.237 block_start: {"value": null, "metadata": {"first_epoch_num": 61, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   4200, Loss function: 3.823, Average Loss: 3.663, avg. samples / sec: 7230.53
Iteration:   4200, Loss function: 3.661, Average Loss: 3.639, avg. samples / sec: 7232.01
Iteration:   4200, Loss function: 3.754, Average Loss: 3.673, avg. samples / sec: 7227.95
Iteration:   4200, Loss function: 3.058, Average Loss: 3.644, avg. samples / sec: 7223.04
Iteration:   4200, Loss function: 2.667, Average Loss: 3.649, avg. samples / sec: 7225.03
Iteration:   4200, Loss function: 2.702, Average Loss: 3.651, avg. samples / sec: 7224.63
Iteration:   4200, Loss function: 3.695, Average Loss: 3.687, avg. samples / sec: 7225.76
Iteration:   4200, Loss function: 3.329, Average Loss: 3.636, avg. samples / sec: 7224.58
Iteration:   4200, Loss function: 2.458, Average Loss: 3.658, avg. samples / sec: 7224.80
Iteration:   4200, Loss function: 3.827, Average Loss: 3.636, avg. samples / sec: 7225.52
Iteration:   4200, Loss function: 3.718, Average Loss: 3.651, avg. samples / sec: 7222.21
Iteration:   4200, Loss function: 2.301, Average Loss: 3.654, avg. samples / sec: 7224.63
Iteration:   4200, Loss function: 3.893, Average Loss: 3.663, avg. samples / sec: 7225.56
Iteration:   4200, Loss function: 2.772, Average Loss: 3.648, avg. samples / sec: 7221.61
Iteration:   4200, Loss function: 2.813, Average Loss: 3.680, avg. samples / sec: 7217.76
Iteration:   4220, Loss function: 3.698, Average Loss: 3.643, avg. samples / sec: 59907.91
Iteration:   4220, Loss function: 3.174, Average Loss: 3.676, avg. samples / sec: 59816.77
Iteration:   4220, Loss function: 3.621, Average Loss: 3.668, avg. samples / sec: 59650.56
Iteration:   4220, Loss function: 2.603, Average Loss: 3.648, avg. samples / sec: 59832.37
Iteration:   4220, Loss function: 2.655, Average Loss: 3.658, avg. samples / sec: 59861.80
Iteration:   4220, Loss function: 3.275, Average Loss: 3.650, avg. samples / sec: 59466.46
Iteration:   4220, Loss function: 3.716, Average Loss: 3.626, avg. samples / sec: 59738.22
Iteration:   4220, Loss function: 2.921, Average Loss: 3.672, avg. samples / sec: 59996.69
Iteration:   4220, Loss function: 4.083, Average Loss: 3.627, avg. samples / sec: 59698.47
Iteration:   4220, Loss function: 2.245, Average Loss: 3.643, avg. samples / sec: 59688.28
Iteration:   4220, Loss function: 3.311, Average Loss: 3.646, avg. samples / sec: 59585.71
Iteration:   4220, Loss function: 3.530, Average Loss: 3.641, avg. samples / sec: 59780.54
Iteration:   4220, Loss function: 3.812, Average Loss: 3.638, avg. samples / sec: 59518.67
Iteration:   4220, Loss function: 3.560, Average Loss: 3.633, avg. samples / sec: 59341.81
Iteration:   4220, Loss function: 3.553, Average Loss: 3.647, avg. samples / sec: 59603.73
Iteration:   4240, Loss function: 3.372, Average Loss: 3.658, avg. samples / sec: 58118.99
Iteration:   4240, Loss function: 2.894, Average Loss: 3.640, avg. samples / sec: 58062.86
Iteration:   4240, Loss function: 2.429, Average Loss: 3.620, avg. samples / sec: 58082.53
Iteration:   4240, Loss function: 4.144, Average Loss: 3.638, avg. samples / sec: 58185.84
Iteration:   4240, Loss function: 3.404, Average Loss: 3.640, avg. samples / sec: 58148.32
Iteration:   4240, Loss function: 2.902, Average Loss: 3.628, avg. samples / sec: 58166.70
Iteration:   4240, Loss function: 2.570, Average Loss: 3.669, avg. samples / sec: 58037.76
Iteration:   4240, Loss function: 3.088, Average Loss: 3.660, avg. samples / sec: 57942.24
Iteration:   4240, Loss function: 3.085, Average Loss: 3.622, avg. samples / sec: 57988.70
Iteration:   4240, Loss function: 5.618, Average Loss: 3.641, avg. samples / sec: 58091.70
Iteration:   4240, Loss function: 3.024, Average Loss: 3.639, avg. samples / sec: 57951.70
Iteration:   4240, Loss function: 4.310, Average Loss: 3.644, avg. samples / sec: 57742.84
Iteration:   4240, Loss function: 3.419, Average Loss: 3.633, avg. samples / sec: 57951.36
Iteration:   4240, Loss function: 2.358, Average Loss: 3.662, avg. samples / sec: 57735.03
Iteration:   4240, Loss function: 4.129, Average Loss: 3.631, avg. samples / sec: 57956.65
:::MLL 1558639525.182 epoch_stop: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 819}}
:::MLL 1558639525.182 epoch_start: {"value": null, "metadata": {"epoch_num": 62, "file": "train.py", "lineno": 673}}
Iteration:   4260, Loss function: 2.492, Average Loss: 3.634, avg. samples / sec: 58481.68
Iteration:   4260, Loss function: 4.184, Average Loss: 3.654, avg. samples / sec: 58366.05
Iteration:   4260, Loss function: 3.224, Average Loss: 3.616, avg. samples / sec: 58375.40
Iteration:   4260, Loss function: 2.982, Average Loss: 3.630, avg. samples / sec: 58368.20
Iteration:   4260, Loss function: 3.860, Average Loss: 3.664, avg. samples / sec: 58306.40
Iteration:   4260, Loss function: 3.000, Average Loss: 3.626, avg. samples / sec: 58469.30
Iteration:   4260, Loss function: 3.676, Average Loss: 3.649, avg. samples / sec: 58469.11
Iteration:   4260, Loss function: 4.928, Average Loss: 3.638, avg. samples / sec: 58192.76
Iteration:   4260, Loss function: 4.290, Average Loss: 3.636, avg. samples / sec: 58280.75
Iteration:   4260, Loss function: 3.609, Average Loss: 3.617, avg. samples / sec: 58136.18
Iteration:   4260, Loss function: 4.423, Average Loss: 3.622, avg. samples / sec: 58131.89
Iteration:   4260, Loss function: 2.637, Average Loss: 3.653, avg. samples / sec: 58171.94
Iteration:   4260, Loss function: 3.954, Average Loss: 3.626, avg. samples / sec: 58180.49
Iteration:   4260, Loss function: 2.726, Average Loss: 3.630, avg. samples / sec: 57943.09
Iteration:   4260, Loss function: 3.228, Average Loss: 3.628, avg. samples / sec: 58140.45
Iteration:   4280, Loss function: 3.458, Average Loss: 3.643, avg. samples / sec: 56059.80
Iteration:   4280, Loss function: 2.872, Average Loss: 3.638, avg. samples / sec: 55910.74
Iteration:   4280, Loss function: 3.557, Average Loss: 3.623, avg. samples / sec: 55867.70
Iteration:   4280, Loss function: 4.035, Average Loss: 3.622, avg. samples / sec: 56150.24
Iteration:   4280, Loss function: 3.140, Average Loss: 3.613, avg. samples / sec: 55760.14
Iteration:   4280, Loss function: 3.327, Average Loss: 3.621, avg. samples / sec: 55932.07
Iteration:   4280, Loss function: 4.590, Average Loss: 3.620, avg. samples / sec: 55808.78
Iteration:   4280, Loss function: 3.407, Average Loss: 3.608, avg. samples / sec: 55905.42
Iteration:   4280, Loss function: 3.147, Average Loss: 3.657, avg. samples / sec: 55771.57
Iteration:   4280, Loss function: 3.282, Average Loss: 3.634, avg. samples / sec: 55840.96
Iteration:   4280, Loss function: 3.220, Average Loss: 3.647, avg. samples / sec: 55581.54
Iteration:   4280, Loss function: 2.602, Average Loss: 3.631, avg. samples / sec: 55818.29
Iteration:   4280, Loss function: 3.204, Average Loss: 3.622, avg. samples / sec: 55911.27
Iteration:   4280, Loss function: 3.311, Average Loss: 3.628, avg. samples / sec: 55518.20
Iteration:   4280, Loss function: 3.611, Average Loss: 3.623, avg. samples / sec: 55907.92
Iteration:   4300, Loss function: 3.276, Average Loss: 3.610, avg. samples / sec: 59150.13
Iteration:   4300, Loss function: 4.288, Average Loss: 3.640, avg. samples / sec: 59027.20
Iteration:   4300, Loss function: 2.877, Average Loss: 3.619, avg. samples / sec: 59137.79
Iteration:   4300, Loss function: 3.568, Average Loss: 3.601, avg. samples / sec: 59127.07
Iteration:   4300, Loss function: 3.056, Average Loss: 3.618, avg. samples / sec: 59008.83
Iteration:   4300, Loss function: 3.020, Average Loss: 3.646, avg. samples / sec: 59129.53
Iteration:   4300, Loss function: 3.573, Average Loss: 3.611, avg. samples / sec: 59028.90
Iteration:   4300, Loss function: 3.065, Average Loss: 3.620, avg. samples / sec: 59243.70
Iteration:   4300, Loss function: 4.099, Average Loss: 3.619, avg. samples / sec: 59030.91
Iteration:   4300, Loss function: 2.397, Average Loss: 3.615, avg. samples / sec: 59078.00
Iteration:   4300, Loss function: 3.306, Average Loss: 3.618, avg. samples / sec: 59002.83
Iteration:   4300, Loss function: 2.460, Average Loss: 3.630, avg. samples / sec: 58921.74
Iteration:   4300, Loss function: 2.456, Average Loss: 3.620, avg. samples / sec: 58906.35
Iteration:   4300, Loss function: 3.032, Average Loss: 3.632, avg. samples / sec: 58726.52
Iteration:   4300, Loss function: 3.998, Average Loss: 3.642, avg. samples / sec: 58860.39
Iteration:   4320, Loss function: 2.740, Average Loss: 3.635, avg. samples / sec: 57401.95
Iteration:   4320, Loss function: 3.661, Average Loss: 3.612, avg. samples / sec: 57368.49
Iteration:   4320, Loss function: 2.765, Average Loss: 3.624, avg. samples / sec: 57524.07
Iteration:   4320, Loss function: 2.928, Average Loss: 3.616, avg. samples / sec: 57202.46
Iteration:   4320, Loss function: 3.280, Average Loss: 3.622, avg. samples / sec: 57469.15
Iteration:   4320, Loss function: 3.255, Average Loss: 3.605, avg. samples / sec: 57166.59
Iteration:   4320, Loss function: 3.171, Average Loss: 3.596, avg. samples / sec: 57182.78
Iteration:   4320, Loss function: 4.143, Average Loss: 3.615, avg. samples / sec: 57256.91
Iteration:   4320, Loss function: 3.675, Average Loss: 3.617, avg. samples / sec: 57107.98
Iteration:   4320, Loss function: 3.326, Average Loss: 3.640, avg. samples / sec: 57112.86
Iteration:   4320, Loss function: 3.662, Average Loss: 3.633, avg. samples / sec: 57372.74
Iteration:   4320, Loss function: 3.894, Average Loss: 3.614, avg. samples / sec: 57100.09
Iteration:   4320, Loss function: 4.312, Average Loss: 3.617, avg. samples / sec: 57305.60
Iteration:   4320, Loss function: 4.206, Average Loss: 3.616, avg. samples / sec: 57225.34
Iteration:   4320, Loss function: 3.660, Average Loss: 3.612, avg. samples / sec: 56529.41
:::MLL 1558639527.226 epoch_stop: {"value": null, "metadata": {"epoch_num": 62, "file": "train.py", "lineno": 819}}
:::MLL 1558639527.226 epoch_start: {"value": null, "metadata": {"epoch_num": 63, "file": "train.py", "lineno": 673}}
Iteration:   4340, Loss function: 3.201, Average Loss: 3.616, avg. samples / sec: 59230.40
Iteration:   4340, Loss function: 3.351, Average Loss: 3.626, avg. samples / sec: 59133.75
Iteration:   4340, Loss function: 2.949, Average Loss: 3.611, avg. samples / sec: 58980.70
Iteration:   4340, Loss function: 3.132, Average Loss: 3.609, avg. samples / sec: 59127.25
Iteration:   4340, Loss function: 2.568, Average Loss: 3.596, avg. samples / sec: 58937.34
Iteration:   4340, Loss function: 2.350, Average Loss: 3.609, avg. samples / sec: 59763.63
Iteration:   4340, Loss function: 3.893, Average Loss: 3.610, avg. samples / sec: 59040.60
Iteration:   4340, Loss function: 3.916, Average Loss: 3.616, avg. samples / sec: 58671.88
Iteration:   4340, Loss function: 3.997, Average Loss: 3.613, avg. samples / sec: 58831.10
Iteration:   4340, Loss function: 3.073, Average Loss: 3.616, avg. samples / sec: 58712.60
Iteration:   4340, Loss function: 4.310, Average Loss: 3.622, avg. samples / sec: 58784.65
Iteration:   4340, Loss function: 2.870, Average Loss: 3.611, avg. samples / sec: 58940.23
Iteration:   4340, Loss function: 3.999, Average Loss: 3.632, avg. samples / sec: 58842.43
Iteration:   4340, Loss function: 3.191, Average Loss: 3.629, avg. samples / sec: 58461.83
Iteration:   4340, Loss function: 3.666, Average Loss: 3.598, avg. samples / sec: 58679.21
Iteration:   4360, Loss function: 3.888, Average Loss: 3.607, avg. samples / sec: 58076.84
Iteration:   4360, Loss function: 2.864, Average Loss: 3.588, avg. samples / sec: 58050.07
Iteration:   4360, Loss function: 4.226, Average Loss: 3.608, avg. samples / sec: 58126.47
Iteration:   4360, Loss function: 2.985, Average Loss: 3.590, avg. samples / sec: 58234.65
Iteration:   4360, Loss function: 3.530, Average Loss: 3.624, avg. samples / sec: 57909.36
Iteration:   4360, Loss function: 4.110, Average Loss: 3.607, avg. samples / sec: 58106.63
Iteration:   4360, Loss function: 3.386, Average Loss: 3.608, avg. samples / sec: 57886.95
Iteration:   4360, Loss function: 3.777, Average Loss: 3.626, avg. samples / sec: 58115.90
Iteration:   4360, Loss function: 3.244, Average Loss: 3.606, avg. samples / sec: 58026.05
Iteration:   4360, Loss function: 3.342, Average Loss: 3.607, avg. samples / sec: 57922.14
Iteration:   4360, Loss function: 3.486, Average Loss: 3.604, avg. samples / sec: 57887.23
Iteration:   4360, Loss function: 3.226, Average Loss: 3.608, avg. samples / sec: 58026.19
Iteration:   4360, Loss function: 2.478, Average Loss: 3.607, avg. samples / sec: 57695.28
Iteration:   4360, Loss function: 3.001, Average Loss: 3.626, avg. samples / sec: 58076.86
Iteration:   4360, Loss function: 3.270, Average Loss: 3.619, avg. samples / sec: 57985.87
Iteration:   4380, Loss function: 2.685, Average Loss: 3.601, avg. samples / sec: 59780.36
Iteration:   4380, Loss function: 3.381, Average Loss: 3.612, avg. samples / sec: 59793.40
Iteration:   4380, Loss function: 3.412, Average Loss: 3.600, avg. samples / sec: 59688.07
Iteration:   4380, Loss function: 2.868, Average Loss: 3.601, avg. samples / sec: 59546.09
Iteration:   4380, Loss function: 3.505, Average Loss: 3.599, avg. samples / sec: 59653.91
Iteration:   4380, Loss function: 3.442, Average Loss: 3.606, avg. samples / sec: 59604.97
Iteration:   4380, Loss function: 3.573, Average Loss: 3.618, avg. samples / sec: 59567.35
Iteration:   4380, Loss function: 2.468, Average Loss: 3.603, avg. samples / sec: 59614.09
Iteration:   4380, Loss function: 2.874, Average Loss: 3.600, avg. samples / sec: 59415.62
Iteration:   4380, Loss function: 3.565, Average Loss: 3.582, avg. samples / sec: 59365.46
Iteration:   4380, Loss function: 3.567, Average Loss: 3.586, avg. samples / sec: 59406.02
Iteration:   4380, Loss function: 3.850, Average Loss: 3.603, avg. samples / sec: 59491.84
Iteration:   4380, Loss function: 4.806, Average Loss: 3.628, avg. samples / sec: 59367.66
Iteration:   4380, Loss function: 3.488, Average Loss: 3.601, avg. samples / sec: 59318.06
Iteration:   4380, Loss function: 4.186, Average Loss: 3.619, avg. samples / sec: 59401.79
:::MLL 1558639529.222 epoch_stop: {"value": null, "metadata": {"epoch_num": 63, "file": "train.py", "lineno": 819}}
:::MLL 1558639529.223 epoch_start: {"value": null, "metadata": {"epoch_num": 64, "file": "train.py", "lineno": 673}}
Iteration:   4400, Loss function: 3.873, Average Loss: 3.596, avg. samples / sec: 59249.13
Iteration:   4400, Loss function: 2.765, Average Loss: 3.619, avg. samples / sec: 59461.09
Iteration:   4400, Loss function: 3.411, Average Loss: 3.573, avg. samples / sec: 59330.34
Iteration:   4400, Loss function: 3.239, Average Loss: 3.594, avg. samples / sec: 59134.96
Iteration:   4400, Loss function: 2.594, Average Loss: 3.592, avg. samples / sec: 59280.03
Iteration:   4400, Loss function: 4.702, Average Loss: 3.614, avg. samples / sec: 59132.01
Iteration:   4400, Loss function: 2.773, Average Loss: 3.602, avg. samples / sec: 59287.31
Iteration:   4400, Loss function: 3.922, Average Loss: 3.589, avg. samples / sec: 59097.54
Iteration:   4400, Loss function: 3.210, Average Loss: 3.594, avg. samples / sec: 59039.09
Iteration:   4400, Loss function: 3.408, Average Loss: 3.594, avg. samples / sec: 58877.16
Iteration:   4400, Loss function: 3.956, Average Loss: 3.607, avg. samples / sec: 58940.28
Iteration:   4400, Loss function: 3.524, Average Loss: 3.579, avg. samples / sec: 59144.17
Iteration:   4400, Loss function: 3.092, Average Loss: 3.611, avg. samples / sec: 59277.74
Iteration:   4400, Loss function: 2.334, Average Loss: 3.592, avg. samples / sec: 59255.21
Iteration:   4400, Loss function: 4.359, Average Loss: 3.598, avg. samples / sec: 58867.47
Iteration:   4420, Loss function: 4.075, Average Loss: 3.593, avg. samples / sec: 59368.44
Iteration:   4420, Loss function: 3.599, Average Loss: 3.573, avg. samples / sec: 59452.94
Iteration:   4420, Loss function: 3.151, Average Loss: 3.612, avg. samples / sec: 59534.34
Iteration:   4420, Loss function: 4.358, Average Loss: 3.590, avg. samples / sec: 59577.50
Iteration:   4420, Loss function: 2.819, Average Loss: 3.572, avg. samples / sec: 59593.55
Iteration:   4420, Loss function: 3.270, Average Loss: 3.596, avg. samples / sec: 59521.04
Iteration:   4420, Loss function: 3.156, Average Loss: 3.600, avg. samples / sec: 59553.74
Iteration:   4420, Loss function: 3.078, Average Loss: 3.612, avg. samples / sec: 59329.05
Iteration:   4420, Loss function: 3.274, Average Loss: 3.592, avg. samples / sec: 59361.03
Iteration:   4420, Loss function: 2.152, Average Loss: 3.587, avg. samples / sec: 59509.38
Iteration:   4420, Loss function: 3.577, Average Loss: 3.595, avg. samples / sec: 59324.35
Iteration:   4420, Loss function: 3.576, Average Loss: 3.597, avg. samples / sec: 59372.39
Iteration:   4420, Loss function: 3.623, Average Loss: 3.593, avg. samples / sec: 59665.03
Iteration:   4420, Loss function: 2.218, Average Loss: 3.598, avg. samples / sec: 59413.84
Iteration:   4420, Loss function: 3.706, Average Loss: 3.585, avg. samples / sec: 59278.29
Iteration:   4440, Loss function: 2.730, Average Loss: 3.585, avg. samples / sec: 57284.91
Iteration:   4440, Loss function: 3.386, Average Loss: 3.580, avg. samples / sec: 57497.36
Iteration:   4440, Loss function: 3.485, Average Loss: 3.607, avg. samples / sec: 57242.14
Iteration:   4440, Loss function: 3.171, Average Loss: 3.595, avg. samples / sec: 57413.55
Iteration:   4440, Loss function: 2.367, Average Loss: 3.577, avg. samples / sec: 57364.14
Iteration:   4440, Loss function: 2.536, Average Loss: 3.591, avg. samples / sec: 57279.37
Iteration:   4440, Loss function: 2.085, Average Loss: 3.597, avg. samples / sec: 57277.55
Iteration:   4440, Loss function: 3.650, Average Loss: 3.588, avg. samples / sec: 57190.60
Iteration:   4440, Loss function: 4.149, Average Loss: 3.594, avg. samples / sec: 57308.42
Iteration:   4440, Loss function: 3.175, Average Loss: 3.567, avg. samples / sec: 57206.87
Iteration:   4440, Loss function: 3.435, Average Loss: 3.601, avg. samples / sec: 57239.45
Iteration:   4440, Loss function: 3.264, Average Loss: 3.592, avg. samples / sec: 57267.08
Iteration:   4440, Loss function: 3.722, Average Loss: 3.590, avg. samples / sec: 57247.47
Iteration:   4440, Loss function: 3.054, Average Loss: 3.563, avg. samples / sec: 57067.90
Iteration:   4440, Loss function: 2.351, Average Loss: 3.582, avg. samples / sec: 57057.44
Iteration:   4460, Loss function: 2.861, Average Loss: 3.598, avg. samples / sec: 57808.02
Iteration:   4460, Loss function: 3.697, Average Loss: 3.589, avg. samples / sec: 57674.17
Iteration:   4460, Loss function: 2.593, Average Loss: 3.584, avg. samples / sec: 57775.29
Iteration:   4460, Loss function: 3.095, Average Loss: 3.587, avg. samples / sec: 57800.70
Iteration:   4460, Loss function: 3.016, Average Loss: 3.559, avg. samples / sec: 57711.29
Iteration:   4460, Loss function: 3.818, Average Loss: 3.588, avg. samples / sec: 57621.96
Iteration:   4460, Loss function: 2.765, Average Loss: 3.579, avg. samples / sec: 57517.42
Iteration:   4460, Loss function: 3.079, Average Loss: 3.569, avg. samples / sec: 57575.68
Iteration:   4460, Loss function: 3.876, Average Loss: 3.589, avg. samples / sec: 57599.04
Iteration:   4460, Loss function: 3.111, Average Loss: 3.577, avg. samples / sec: 57826.78
Iteration:   4460, Loss function: 2.983, Average Loss: 3.590, avg. samples / sec: 57600.36
Iteration:   4460, Loss function: 3.177, Average Loss: 3.578, avg. samples / sec: 57499.31
Iteration:   4460, Loss function: 3.144, Average Loss: 3.581, avg. samples / sec: 57558.46
Iteration:   4460, Loss function: 2.985, Average Loss: 3.599, avg. samples / sec: 57441.07
Iteration:   4460, Loss function: 4.213, Average Loss: 3.563, avg. samples / sec: 57560.27
:::MLL 1558639531.238 epoch_stop: {"value": null, "metadata": {"epoch_num": 64, "file": "train.py", "lineno": 819}}
:::MLL 1558639531.238 epoch_start: {"value": null, "metadata": {"epoch_num": 65, "file": "train.py", "lineno": 673}}
Iteration:   4480, Loss function: 2.473, Average Loss: 3.581, avg. samples / sec: 59078.82
Iteration:   4480, Loss function: 3.786, Average Loss: 3.583, avg. samples / sec: 59147.77
Iteration:   4480, Loss function: 3.044, Average Loss: 3.576, avg. samples / sec: 59114.57
Iteration:   4480, Loss function: 4.316, Average Loss: 3.573, avg. samples / sec: 59137.45
Iteration:   4480, Loss function: 3.200, Average Loss: 3.594, avg. samples / sec: 58905.93
Iteration:   4480, Loss function: 3.390, Average Loss: 3.580, avg. samples / sec: 58930.24
Iteration:   4480, Loss function: 2.574, Average Loss: 3.550, avg. samples / sec: 58939.44
Iteration:   4480, Loss function: 2.790, Average Loss: 3.583, avg. samples / sec: 58999.57
Iteration:   4480, Loss function: 2.813, Average Loss: 3.592, avg. samples / sec: 59038.92
Iteration:   4480, Loss function: 3.946, Average Loss: 3.580, avg. samples / sec: 58813.75
Iteration:   4480, Loss function: 3.401, Average Loss: 3.586, avg. samples / sec: 58923.81
Iteration:   4480, Loss function: 3.818, Average Loss: 3.576, avg. samples / sec: 58896.45
Iteration:   4480, Loss function: 3.331, Average Loss: 3.558, avg. samples / sec: 58945.48
Iteration:   4480, Loss function: 3.969, Average Loss: 3.573, avg. samples / sec: 58764.67
Iteration:   4480, Loss function: 3.853, Average Loss: 3.563, avg. samples / sec: 58019.21
Iteration:   4500, Loss function: 3.645, Average Loss: 3.560, avg. samples / sec: 60003.86
Iteration:   4500, Loss function: 3.154, Average Loss: 3.594, avg. samples / sec: 59009.38
Iteration:   4500, Loss function: 3.266, Average Loss: 3.579, avg. samples / sec: 58904.36
Iteration:   4500, Loss function: 2.473, Average Loss: 3.568, avg. samples / sec: 58763.67
Iteration:   4500, Loss function: 3.302, Average Loss: 3.578, avg. samples / sec: 58780.73
Iteration:   4500, Loss function: 3.643, Average Loss: 3.578, avg. samples / sec: 58644.51
Iteration:   4500, Loss function: 5.051, Average Loss: 3.548, avg. samples / sec: 58790.02
Iteration:   4500, Loss function: 3.420, Average Loss: 3.574, avg. samples / sec: 58638.28
Iteration:   4500, Loss function: 2.992, Average Loss: 3.575, avg. samples / sec: 58834.91
Iteration:   4500, Loss function: 2.919, Average Loss: 3.575, avg. samples / sec: 58865.36
Iteration:   4500, Loss function: 2.667, Average Loss: 3.555, avg. samples / sec: 58916.84
Iteration:   4500, Loss function: 2.879, Average Loss: 3.582, avg. samples / sec: 58726.35
Iteration:   4500, Loss function: 3.933, Average Loss: 3.586, avg. samples / sec: 58559.58
Iteration:   4500, Loss function: 3.000, Average Loss: 3.568, avg. samples / sec: 58846.70
Iteration:   4500, Loss function: 3.786, Average Loss: 3.577, avg. samples / sec: 58413.51
Iteration:   4520, Loss function: 3.408, Average Loss: 3.572, avg. samples / sec: 58560.29
Iteration:   4520, Loss function: 2.847, Average Loss: 3.572, avg. samples / sec: 58590.70
Iteration:   4520, Loss function: 3.204, Average Loss: 3.582, avg. samples / sec: 58664.94
Iteration:   4520, Loss function: 3.770, Average Loss: 3.574, avg. samples / sec: 58490.41
Iteration:   4520, Loss function: 3.943, Average Loss: 3.570, avg. samples / sec: 58374.90
Iteration:   4520, Loss function: 3.606, Average Loss: 3.543, avg. samples / sec: 58402.35
Iteration:   4520, Loss function: 3.502, Average Loss: 3.573, avg. samples / sec: 58370.88
Iteration:   4520, Loss function: 3.735, Average Loss: 3.575, avg. samples / sec: 58600.13
Iteration:   4520, Loss function: 3.280, Average Loss: 3.557, avg. samples / sec: 58168.41
Iteration:   4520, Loss function: 3.221, Average Loss: 3.547, avg. samples / sec: 58461.49
Iteration:   4520, Loss function: 2.490, Average Loss: 3.562, avg. samples / sec: 58535.07
Iteration:   4520, Loss function: 3.144, Average Loss: 3.562, avg. samples / sec: 58223.66
Iteration:   4520, Loss function: 3.875, Average Loss: 3.576, avg. samples / sec: 58163.01
Iteration:   4520, Loss function: 2.370, Average Loss: 3.575, avg. samples / sec: 58392.19
Iteration:   4520, Loss function: 2.671, Average Loss: 3.586, avg. samples / sec: 58032.38
:::MLL 1558639533.239 epoch_stop: {"value": null, "metadata": {"epoch_num": 65, "file": "train.py", "lineno": 819}}
:::MLL 1558639533.240 epoch_start: {"value": null, "metadata": {"epoch_num": 66, "file": "train.py", "lineno": 673}}
Iteration:   4540, Loss function: 2.776, Average Loss: 3.577, avg. samples / sec: 59487.04
Iteration:   4540, Loss function: 3.317, Average Loss: 3.570, avg. samples / sec: 59683.52
Iteration:   4540, Loss function: 4.195, Average Loss: 3.549, avg. samples / sec: 59564.79
Iteration:   4540, Loss function: 3.165, Average Loss: 3.554, avg. samples / sec: 59514.53
Iteration:   4540, Loss function: 2.862, Average Loss: 3.566, avg. samples / sec: 59326.05
Iteration:   4540, Loss function: 2.445, Average Loss: 3.566, avg. samples / sec: 59600.46
Iteration:   4540, Loss function: 4.000, Average Loss: 3.564, avg. samples / sec: 59314.74
Iteration:   4540, Loss function: 3.208, Average Loss: 3.556, avg. samples / sec: 59527.70
Iteration:   4540, Loss function: 3.655, Average Loss: 3.571, avg. samples / sec: 59389.08
Iteration:   4540, Loss function: 4.298, Average Loss: 3.539, avg. samples / sec: 59348.38
Iteration:   4540, Loss function: 4.685, Average Loss: 3.575, avg. samples / sec: 59289.81
Iteration:   4540, Loss function: 2.801, Average Loss: 3.567, avg. samples / sec: 59326.07
Iteration:   4540, Loss function: 3.906, Average Loss: 3.582, avg. samples / sec: 59579.14
Iteration:   4540, Loss function: 2.759, Average Loss: 3.566, avg. samples / sec: 59258.22
Iteration:   4540, Loss function: 2.553, Average Loss: 3.559, avg. samples / sec: 59295.52
Iteration:   4560, Loss function: 3.169, Average Loss: 3.567, avg. samples / sec: 57521.18
Iteration:   4560, Loss function: 3.082, Average Loss: 3.550, avg. samples / sec: 57385.73
Iteration:   4560, Loss function: 3.777, Average Loss: 3.558, avg. samples / sec: 57368.44
Iteration:   4560, Loss function: 3.838, Average Loss: 3.569, avg. samples / sec: 57413.20
Iteration:   4560, Loss function: 2.713, Average Loss: 3.547, avg. samples / sec: 57259.03
Iteration:   4560, Loss function: 3.107, Average Loss: 3.558, avg. samples / sec: 57530.22
Iteration:   4560, Loss function: 2.999, Average Loss: 3.562, avg. samples / sec: 57301.63
Iteration:   4560, Loss function: 3.169, Average Loss: 3.565, avg. samples / sec: 57452.49
Iteration:   4560, Loss function: 2.753, Average Loss: 3.547, avg. samples / sec: 57222.92
Iteration:   4560, Loss function: 2.044, Average Loss: 3.567, avg. samples / sec: 57185.38
Iteration:   4560, Loss function: 3.900, Average Loss: 3.569, avg. samples / sec: 57114.09
Iteration:   4560, Loss function: 3.990, Average Loss: 3.582, avg. samples / sec: 57349.83
Iteration:   4560, Loss function: 3.313, Average Loss: 3.556, avg. samples / sec: 57187.28
Iteration:   4560, Loss function: 1.981, Average Loss: 3.531, avg. samples / sec: 57251.21
Iteration:   4560, Loss function: 2.722, Average Loss: 3.561, avg. samples / sec: 57243.54
:::MLL 1558639534.200 eval_start: {"value": null, "metadata": {"epoch_num": 66, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.66 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.66 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.66 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.66 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.66 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.66 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.66 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.66 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.66 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.66 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.66 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.66 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.66 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.66 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.66 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.55s)
DONE (t=0.57s)
DONE (t=2.77s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22998
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.39366
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23405
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05946
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24112
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.37506
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22185
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32379
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33984
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.09914
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.37051
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.52881
Current AP: 0.22998 AP goal: 0.23000
:::MLL 1558639538.207 eval_accuracy: {"value": 0.22997673128021598, "metadata": {"epoch_num": 66, "file": "train.py", "lineno": 389}}
:::MLL 1558639538.271 eval_stop: {"value": null, "metadata": {"epoch_num": 66, "file": "train.py", "lineno": 392}}
:::MLL 1558639538.281 block_stop: {"value": null, "metadata": {"first_epoch_num": 61, "file": "train.py", "lineno": 804}}
:::MLL 1558639538.281 block_start: {"value": null, "metadata": {"first_epoch_num": 66, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   4580, Loss function: 3.768, Average Loss: 3.552, avg. samples / sec: 7219.78
Iteration:   4580, Loss function: 4.226, Average Loss: 3.546, avg. samples / sec: 7219.79
Iteration:   4580, Loss function: 4.966, Average Loss: 3.562, avg. samples / sec: 7217.49
Iteration:   4580, Loss function: 4.767, Average Loss: 3.557, avg. samples / sec: 7218.71
Iteration:   4580, Loss function: 2.974, Average Loss: 3.563, avg. samples / sec: 7219.95
Iteration:   4580, Loss function: 2.829, Average Loss: 3.565, avg. samples / sec: 7220.01
Iteration:   4580, Loss function: 2.312, Average Loss: 3.566, avg. samples / sec: 7218.26
Iteration:   4580, Loss function: 3.785, Average Loss: 3.558, avg. samples / sec: 7217.77
Iteration:   4580, Loss function: 3.347, Average Loss: 3.575, avg. samples / sec: 7218.59
Iteration:   4580, Loss function: 3.611, Average Loss: 3.526, avg. samples / sec: 7219.10
Iteration:   4580, Loss function: 2.935, Average Loss: 3.554, avg. samples / sec: 7219.57
Iteration:   4580, Loss function: 3.580, Average Loss: 3.540, avg. samples / sec: 7217.62
Iteration:   4580, Loss function: 3.762, Average Loss: 3.550, avg. samples / sec: 7215.80
Iteration:   4580, Loss function: 3.444, Average Loss: 3.553, avg. samples / sec: 7218.39
Iteration:   4580, Loss function: 4.903, Average Loss: 3.566, avg. samples / sec: 7216.46
Iteration:   4600, Loss function: 3.173, Average Loss: 3.567, avg. samples / sec: 59288.91
Iteration:   4600, Loss function: 2.929, Average Loss: 3.540, avg. samples / sec: 59152.26
Iteration:   4600, Loss function: 3.067, Average Loss: 3.548, avg. samples / sec: 59355.46
Iteration:   4600, Loss function: 2.950, Average Loss: 3.555, avg. samples / sec: 59061.31
Iteration:   4600, Loss function: 3.014, Average Loss: 3.565, avg. samples / sec: 59270.98
Iteration:   4600, Loss function: 5.213, Average Loss: 3.557, avg. samples / sec: 59142.61
Iteration:   4600, Loss function: 3.660, Average Loss: 3.534, avg. samples / sec: 59185.21
Iteration:   4600, Loss function: 3.258, Average Loss: 3.560, avg. samples / sec: 59030.34
Iteration:   4600, Loss function: 5.162, Average Loss: 3.574, avg. samples / sec: 59074.53
Iteration:   4600, Loss function: 3.568, Average Loss: 3.562, avg. samples / sec: 58963.11
Iteration:   4600, Loss function: 4.210, Average Loss: 3.546, avg. samples / sec: 59135.26
Iteration:   4600, Loss function: 4.452, Average Loss: 3.550, avg. samples / sec: 59086.59
Iteration:   4600, Loss function: 2.995, Average Loss: 3.547, avg. samples / sec: 58829.61
Iteration:   4600, Loss function: 4.121, Average Loss: 3.533, avg. samples / sec: 58916.62
Iteration:   4600, Loss function: 4.048, Average Loss: 3.551, avg. samples / sec: 58708.78
:::MLL 1558639539.334 epoch_stop: {"value": null, "metadata": {"epoch_num": 66, "file": "train.py", "lineno": 819}}
:::MLL 1558639539.334 epoch_start: {"value": null, "metadata": {"epoch_num": 67, "file": "train.py", "lineno": 673}}
Iteration:   4620, Loss function: 4.561, Average Loss: 3.551, avg. samples / sec: 59446.29
Iteration:   4620, Loss function: 3.429, Average Loss: 3.544, avg. samples / sec: 59565.57
Iteration:   4620, Loss function: 4.724, Average Loss: 3.558, avg. samples / sec: 59414.69
Iteration:   4620, Loss function: 3.872, Average Loss: 3.547, avg. samples / sec: 59492.22
Iteration:   4620, Loss function: 3.156, Average Loss: 3.562, avg. samples / sec: 59463.68
Iteration:   4620, Loss function: 1.938, Average Loss: 3.550, avg. samples / sec: 59270.96
Iteration:   4620, Loss function: 3.111, Average Loss: 3.530, avg. samples / sec: 59289.28
Iteration:   4620, Loss function: 3.430, Average Loss: 3.571, avg. samples / sec: 59342.56
Iteration:   4620, Loss function: 3.493, Average Loss: 3.534, avg. samples / sec: 59068.96
Iteration:   4620, Loss function: 3.116, Average Loss: 3.528, avg. samples / sec: 59458.61
Iteration:   4620, Loss function: 3.497, Average Loss: 3.545, avg. samples / sec: 59524.73
Iteration:   4620, Loss function: 3.739, Average Loss: 3.542, avg. samples / sec: 59269.16
Iteration:   4620, Loss function: 3.457, Average Loss: 3.544, avg. samples / sec: 59046.81
Iteration:   4620, Loss function: 3.349, Average Loss: 3.565, avg. samples / sec: 59143.40
Iteration:   4620, Loss function: 3.457, Average Loss: 3.563, avg. samples / sec: 58922.19
Iteration:   4640, Loss function: 3.331, Average Loss: 3.546, avg. samples / sec: 57916.31
Iteration:   4640, Loss function: 3.256, Average Loss: 3.548, avg. samples / sec: 57909.07
Iteration:   4640, Loss function: 2.777, Average Loss: 3.524, avg. samples / sec: 58052.17
Iteration:   4640, Loss function: 3.581, Average Loss: 3.569, avg. samples / sec: 57956.49
Iteration:   4640, Loss function: 4.065, Average Loss: 3.562, avg. samples / sec: 58044.66
Iteration:   4640, Loss function: 3.289, Average Loss: 3.552, avg. samples / sec: 57797.68
Iteration:   4640, Loss function: 4.141, Average Loss: 3.531, avg. samples / sec: 57897.67
Iteration:   4640, Loss function: 4.975, Average Loss: 3.556, avg. samples / sec: 57982.55
Iteration:   4640, Loss function: 3.216, Average Loss: 3.536, avg. samples / sec: 57708.53
Iteration:   4640, Loss function: 2.911, Average Loss: 3.539, avg. samples / sec: 57907.98
Iteration:   4640, Loss function: 2.761, Average Loss: 3.564, avg. samples / sec: 57730.75
Iteration:   4640, Loss function: 3.536, Average Loss: 3.542, avg. samples / sec: 57860.00
Iteration:   4640, Loss function: 3.229, Average Loss: 3.540, avg. samples / sec: 57856.29
Iteration:   4640, Loss function: 2.637, Average Loss: 3.525, avg. samples / sec: 57731.29
Iteration:   4640, Loss function: 2.522, Average Loss: 3.553, avg. samples / sec: 57499.73
Iteration:   4660, Loss function: 3.633, Average Loss: 3.525, avg. samples / sec: 59686.40
Iteration:   4660, Loss function: 4.894, Average Loss: 3.533, avg. samples / sec: 59727.99
Iteration:   4660, Loss function: 3.402, Average Loss: 3.541, avg. samples / sec: 59489.33
Iteration:   4660, Loss function: 3.368, Average Loss: 3.523, avg. samples / sec: 59550.92
Iteration:   4660, Loss function: 2.779, Average Loss: 3.550, avg. samples / sec: 59626.20
Iteration:   4660, Loss function: 3.592, Average Loss: 3.531, avg. samples / sec: 59702.06
Iteration:   4660, Loss function: 4.523, Average Loss: 3.524, avg. samples / sec: 59717.24
Iteration:   4660, Loss function: 4.043, Average Loss: 3.529, avg. samples / sec: 59606.96
Iteration:   4660, Loss function: 3.325, Average Loss: 3.542, avg. samples / sec: 59648.01
Iteration:   4660, Loss function: 3.539, Average Loss: 3.570, avg. samples / sec: 59408.78
Iteration:   4660, Loss function: 2.770, Average Loss: 3.556, avg. samples / sec: 59469.12
Iteration:   4660, Loss function: 5.036, Average Loss: 3.546, avg. samples / sec: 59361.46
Iteration:   4660, Loss function: 2.773, Average Loss: 3.547, avg. samples / sec: 59290.38
Iteration:   4660, Loss function: 2.671, Average Loss: 3.555, avg. samples / sec: 59291.55
Iteration:   4660, Loss function: 2.700, Average Loss: 3.549, avg. samples / sec: 59452.64
:::MLL 1558639541.342 epoch_stop: {"value": null, "metadata": {"epoch_num": 67, "file": "train.py", "lineno": 819}}
:::MLL 1558639541.343 epoch_start: {"value": null, "metadata": {"epoch_num": 68, "file": "train.py", "lineno": 673}}
Iteration:   4680, Loss function: 4.456, Average Loss: 3.570, avg. samples / sec: 57980.28
Iteration:   4680, Loss function: 3.921, Average Loss: 3.541, avg. samples / sec: 58051.12
Iteration:   4680, Loss function: 4.074, Average Loss: 3.530, avg. samples / sec: 57804.18
Iteration:   4680, Loss function: 2.609, Average Loss: 3.519, avg. samples / sec: 57792.61
Iteration:   4680, Loss function: 3.354, Average Loss: 3.548, avg. samples / sec: 58030.66
Iteration:   4680, Loss function: 2.811, Average Loss: 3.520, avg. samples / sec: 57752.56
Iteration:   4680, Loss function: 2.401, Average Loss: 3.545, avg. samples / sec: 57794.18
Iteration:   4680, Loss function: 3.036, Average Loss: 3.553, avg. samples / sec: 57936.04
Iteration:   4680, Loss function: 3.281, Average Loss: 3.528, avg. samples / sec: 57778.49
Iteration:   4680, Loss function: 3.975, Average Loss: 3.526, avg. samples / sec: 57732.15
Iteration:   4680, Loss function: 3.443, Average Loss: 3.539, avg. samples / sec: 57671.62
Iteration:   4680, Loss function: 3.580, Average Loss: 3.534, avg. samples / sec: 57771.17
Iteration:   4680, Loss function: 4.918, Average Loss: 3.527, avg. samples / sec: 57708.98
Iteration:   4680, Loss function: 3.732, Average Loss: 3.542, avg. samples / sec: 57858.60
Iteration:   4680, Loss function: 2.939, Average Loss: 3.544, avg. samples / sec: 57944.52
Iteration:   4700, Loss function: 3.929, Average Loss: 3.515, avg. samples / sec: 57335.11
Iteration:   4700, Loss function: 3.732, Average Loss: 3.519, avg. samples / sec: 57429.88
Iteration:   4700, Loss function: 3.046, Average Loss: 3.540, avg. samples / sec: 57503.36
Iteration:   4700, Loss function: 3.165, Average Loss: 3.563, avg. samples / sec: 57223.74
Iteration:   4700, Loss function: 3.318, Average Loss: 3.537, avg. samples / sec: 57407.21
Iteration:   4700, Loss function: 3.128, Average Loss: 3.549, avg. samples / sec: 57321.10
Iteration:   4700, Loss function: 3.555, Average Loss: 3.535, avg. samples / sec: 57350.28
Iteration:   4700, Loss function: 4.261, Average Loss: 3.525, avg. samples / sec: 57307.97
Iteration:   4700, Loss function: 2.636, Average Loss: 3.525, avg. samples / sec: 57169.02
Iteration:   4700, Loss function: 3.105, Average Loss: 3.524, avg. samples / sec: 57354.50
Iteration:   4700, Loss function: 2.681, Average Loss: 3.513, avg. samples / sec: 57215.26
Iteration:   4700, Loss function: 3.707, Average Loss: 3.542, avg. samples / sec: 57134.09
Iteration:   4700, Loss function: 3.403, Average Loss: 3.531, avg. samples / sec: 57264.71
Iteration:   4700, Loss function: 3.359, Average Loss: 3.540, avg. samples / sec: 57171.43
Iteration:   4700, Loss function: 3.837, Average Loss: 3.547, avg. samples / sec: 57127.22
Iteration:   4720, Loss function: 3.223, Average Loss: 3.526, avg. samples / sec: 60341.38
Iteration:   4720, Loss function: 3.767, Average Loss: 3.523, avg. samples / sec: 60312.45
Iteration:   4720, Loss function: 2.585, Average Loss: 3.516, avg. samples / sec: 60194.67
Iteration:   4720, Loss function: 3.841, Average Loss: 3.540, avg. samples / sec: 60221.48
Iteration:   4720, Loss function: 3.731, Average Loss: 3.532, avg. samples / sec: 60225.98
Iteration:   4720, Loss function: 4.327, Average Loss: 3.535, avg. samples / sec: 60230.51
Iteration:   4720, Loss function: 3.744, Average Loss: 3.517, avg. samples / sec: 60209.33
Iteration:   4720, Loss function: 3.162, Average Loss: 3.547, avg. samples / sec: 60148.07
Iteration:   4720, Loss function: 2.858, Average Loss: 3.540, avg. samples / sec: 60289.08
Iteration:   4720, Loss function: 2.439, Average Loss: 3.534, avg. samples / sec: 60240.24
Iteration:   4720, Loss function: 3.243, Average Loss: 3.531, avg. samples / sec: 60195.19
Iteration:   4720, Loss function: 3.130, Average Loss: 3.536, avg. samples / sec: 60119.82
Iteration:   4720, Loss function: 2.520, Average Loss: 3.511, avg. samples / sec: 60092.72
Iteration:   4720, Loss function: 2.748, Average Loss: 3.554, avg. samples / sec: 60018.51
Iteration:   4720, Loss function: 2.750, Average Loss: 3.508, avg. samples / sec: 59826.93
Iteration:   4740, Loss function: 2.747, Average Loss: 3.518, avg. samples / sec: 58619.45
Iteration:   4740, Loss function: 2.662, Average Loss: 3.526, avg. samples / sec: 58642.80
Iteration:   4740, Loss function: 3.409, Average Loss: 3.549, avg. samples / sec: 58850.19
Iteration:   4740, Loss function: 3.897, Average Loss: 3.545, avg. samples / sec: 58719.20
Iteration:   4740, Loss function: 3.512, Average Loss: 3.509, avg. samples / sec: 58818.36
Iteration:   4740, Loss function: 3.580, Average Loss: 3.534, avg. samples / sec: 58805.01
Iteration:   4740, Loss function: 3.917, Average Loss: 3.535, avg. samples / sec: 58615.87
Iteration:   4740, Loss function: 3.033, Average Loss: 3.528, avg. samples / sec: 58601.42
Iteration:   4740, Loss function: 3.808, Average Loss: 3.513, avg. samples / sec: 58641.26
Iteration:   4740, Loss function: 3.556, Average Loss: 3.535, avg. samples / sec: 58652.63
Iteration:   4740, Loss function: 2.607, Average Loss: 3.533, avg. samples / sec: 58516.89
Iteration:   4740, Loss function: 2.723, Average Loss: 3.531, avg. samples / sec: 58614.14
Iteration:   4740, Loss function: 2.913, Average Loss: 3.512, avg. samples / sec: 58449.25
Iteration:   4740, Loss function: 2.741, Average Loss: 3.506, avg. samples / sec: 58725.49
Iteration:   4740, Loss function: 5.099, Average Loss: 3.529, avg. samples / sec: 58551.17
:::MLL 1558639543.348 epoch_stop: {"value": null, "metadata": {"epoch_num": 68, "file": "train.py", "lineno": 819}}
:::MLL 1558639543.348 epoch_start: {"value": null, "metadata": {"epoch_num": 69, "file": "train.py", "lineno": 673}}
Iteration:   4760, Loss function: 2.755, Average Loss: 3.510, avg. samples / sec: 59176.06
Iteration:   4760, Loss function: 3.009, Average Loss: 3.525, avg. samples / sec: 59217.66
Iteration:   4760, Loss function: 2.400, Average Loss: 3.528, avg. samples / sec: 59198.83
Iteration:   4760, Loss function: 2.475, Average Loss: 3.530, avg. samples / sec: 59086.84
Iteration:   4760, Loss function: 3.053, Average Loss: 3.540, avg. samples / sec: 59050.50
Iteration:   4760, Loss function: 2.933, Average Loss: 3.524, avg. samples / sec: 59094.84
Iteration:   4760, Loss function: 4.088, Average Loss: 3.542, avg. samples / sec: 58988.23
Iteration:   4760, Loss function: 4.210, Average Loss: 3.524, avg. samples / sec: 58956.62
Iteration:   4760, Loss function: 2.732, Average Loss: 3.527, avg. samples / sec: 59007.97
Iteration:   4760, Loss function: 3.158, Average Loss: 3.517, avg. samples / sec: 58894.41
Iteration:   4760, Loss function: 2.518, Average Loss: 3.528, avg. samples / sec: 59036.62
Iteration:   4760, Loss function: 2.628, Average Loss: 3.504, avg. samples / sec: 58898.40
Iteration:   4760, Loss function: 5.140, Average Loss: 3.532, avg. samples / sec: 59062.94
Iteration:   4760, Loss function: 2.522, Average Loss: 3.507, avg. samples / sec: 58297.67
Iteration:   4760, Loss function: 3.215, Average Loss: 3.502, avg. samples / sec: 58320.64
Iteration:   4780, Loss function: 4.051, Average Loss: 3.499, avg. samples / sec: 60522.13
Iteration:   4780, Loss function: 3.122, Average Loss: 3.530, avg. samples / sec: 59751.47
Iteration:   4780, Loss function: 3.736, Average Loss: 3.528, avg. samples / sec: 59429.22
Iteration:   4780, Loss function: 2.613, Average Loss: 3.522, avg. samples / sec: 59524.66
Iteration:   4780, Loss function: 2.689, Average Loss: 3.536, avg. samples / sec: 59467.09
Iteration:   4780, Loss function: 3.211, Average Loss: 3.513, avg. samples / sec: 59495.48
Iteration:   4780, Loss function: 3.452, Average Loss: 3.537, avg. samples / sec: 59388.00
Iteration:   4780, Loss function: 3.494, Average Loss: 3.519, avg. samples / sec: 59345.14
Iteration:   4780, Loss function: 3.391, Average Loss: 3.507, avg. samples / sec: 59294.55
Iteration:   4780, Loss function: 3.582, Average Loss: 3.521, avg. samples / sec: 59391.55
Iteration:   4780, Loss function: 3.505, Average Loss: 3.520, avg. samples / sec: 59344.06
Iteration:   4780, Loss function: 4.431, Average Loss: 3.522, avg. samples / sec: 59393.23
Iteration:   4780, Loss function: 2.297, Average Loss: 3.527, avg. samples / sec: 59315.86
Iteration:   4780, Loss function: 3.711, Average Loss: 3.500, avg. samples / sec: 59465.41
Iteration:   4780, Loss function: 3.535, Average Loss: 3.502, avg. samples / sec: 60175.19
Iteration:   4800, Loss function: 2.333, Average Loss: 3.524, avg. samples / sec: 59466.49
Iteration:   4800, Loss function: 2.937, Average Loss: 3.512, avg. samples / sec: 59580.25
Iteration:   4800, Loss function: 4.553, Average Loss: 3.525, avg. samples / sec: 59631.98
Iteration:   4800, Loss function: 4.958, Average Loss: 3.513, avg. samples / sec: 59522.07
Iteration:   4800, Loss function: 2.737, Average Loss: 3.491, avg. samples / sec: 59624.56
Iteration:   4800, Loss function: 3.298, Average Loss: 3.496, avg. samples / sec: 59298.79
Iteration:   4800, Loss function: 3.068, Average Loss: 3.520, avg. samples / sec: 59363.98
Iteration:   4800, Loss function: 2.781, Average Loss: 3.519, avg. samples / sec: 59469.62
Iteration:   4800, Loss function: 3.744, Average Loss: 3.513, avg. samples / sec: 59361.08
Iteration:   4800, Loss function: 3.323, Average Loss: 3.508, avg. samples / sec: 59312.69
Iteration:   4800, Loss function: 3.482, Average Loss: 3.497, avg. samples / sec: 59362.01
Iteration:   4800, Loss function: 3.274, Average Loss: 3.517, avg. samples / sec: 59209.13
Iteration:   4800, Loss function: 3.820, Average Loss: 3.535, avg. samples / sec: 59189.01
Iteration:   4800, Loss function: 4.045, Average Loss: 3.530, avg. samples / sec: 59154.75
Iteration:   4800, Loss function: 3.280, Average Loss: 3.521, avg. samples / sec: 59224.58
:::MLL 1558639545.327 epoch_stop: {"value": null, "metadata": {"epoch_num": 69, "file": "train.py", "lineno": 819}}
:::MLL 1558639545.327 epoch_start: {"value": null, "metadata": {"epoch_num": 70, "file": "train.py", "lineno": 673}}
Iteration:   4820, Loss function: 3.667, Average Loss: 3.521, avg. samples / sec: 59355.78
Iteration:   4820, Loss function: 3.908, Average Loss: 3.514, avg. samples / sec: 59546.59
Iteration:   4820, Loss function: 2.870, Average Loss: 3.513, avg. samples / sec: 59384.42
Iteration:   4820, Loss function: 5.198, Average Loss: 3.528, avg. samples / sec: 59199.97
Iteration:   4820, Loss function: 3.061, Average Loss: 3.489, avg. samples / sec: 59309.55
Iteration:   4820, Loss function: 2.659, Average Loss: 3.523, avg. samples / sec: 59586.29
Iteration:   4820, Loss function: 3.584, Average Loss: 3.512, avg. samples / sec: 59345.54
Iteration:   4820, Loss function: 3.707, Average Loss: 3.533, avg. samples / sec: 59571.11
Iteration:   4820, Loss function: 3.938, Average Loss: 3.506, avg. samples / sec: 59144.20
Iteration:   4820, Loss function: 3.605, Average Loss: 3.509, avg. samples / sec: 59457.88
Iteration:   4820, Loss function: 3.614, Average Loss: 3.516, avg. samples / sec: 59503.17
Iteration:   4820, Loss function: 2.948, Average Loss: 3.508, avg. samples / sec: 59169.85
Iteration:   4820, Loss function: 2.552, Average Loss: 3.497, avg. samples / sec: 59185.93
Iteration:   4820, Loss function: 5.355, Average Loss: 3.514, avg. samples / sec: 59355.36
Iteration:   4820, Loss function: 3.028, Average Loss: 3.496, avg. samples / sec: 59219.68
Iteration:   4840, Loss function: 2.473, Average Loss: 3.507, avg. samples / sec: 59856.28
Iteration:   4840, Loss function: 2.789, Average Loss: 3.491, avg. samples / sec: 59962.58
Iteration:   4840, Loss function: 2.892, Average Loss: 3.507, avg. samples / sec: 59983.79
Iteration:   4840, Loss function: 3.291, Average Loss: 3.484, avg. samples / sec: 59723.61
Iteration:   4840, Loss function: 1.742, Average Loss: 3.523, avg. samples / sec: 59711.57
Iteration:   4840, Loss function: 3.334, Average Loss: 3.528, avg. samples / sec: 59709.75
Iteration:   4840, Loss function: 3.101, Average Loss: 3.513, avg. samples / sec: 59528.40
Iteration:   4840, Loss function: 3.500, Average Loss: 3.506, avg. samples / sec: 59639.25
Iteration:   4840, Loss function: 3.105, Average Loss: 3.496, avg. samples / sec: 59876.88
Iteration:   4840, Loss function: 2.165, Average Loss: 3.503, avg. samples / sec: 59609.18
Iteration:   4840, Loss function: 2.350, Average Loss: 3.510, avg. samples / sec: 59560.38
Iteration:   4840, Loss function: 4.188, Average Loss: 3.504, avg. samples / sec: 59592.16
Iteration:   4840, Loss function: 2.948, Average Loss: 3.519, avg. samples / sec: 59344.94
Iteration:   4840, Loss function: 3.371, Average Loss: 3.522, avg. samples / sec: 59472.28
Iteration:   4840, Loss function: 2.358, Average Loss: 3.513, avg. samples / sec: 59495.13
Iteration:   4860, Loss function: 2.519, Average Loss: 3.500, avg. samples / sec: 56233.70
Iteration:   4860, Loss function: 3.862, Average Loss: 3.506, avg. samples / sec: 56261.26
Iteration:   4860, Loss function: 3.148, Average Loss: 3.508, avg. samples / sec: 56061.07
Iteration:   4860, Loss function: 3.604, Average Loss: 3.497, avg. samples / sec: 56090.68
Iteration:   4860, Loss function: 3.105, Average Loss: 3.525, avg. samples / sec: 55954.98
Iteration:   4860, Loss function: 4.118, Average Loss: 3.485, avg. samples / sec: 55919.57
Iteration:   4860, Loss function: 2.612, Average Loss: 3.502, avg. samples / sec: 56017.42
Iteration:   4860, Loss function: 3.390, Average Loss: 3.494, avg. samples / sec: 56019.69
Iteration:   4860, Loss function: 3.499, Average Loss: 3.514, avg. samples / sec: 56118.24
Iteration:   4860, Loss function: 3.444, Average Loss: 3.490, avg. samples / sec: 55760.62
Iteration:   4860, Loss function: 4.565, Average Loss: 3.500, avg. samples / sec: 55702.06
Iteration:   4860, Loss function: 3.142, Average Loss: 3.518, avg. samples / sec: 55816.03
Iteration:   4860, Loss function: 2.669, Average Loss: 3.502, avg. samples / sec: 55713.84
Iteration:   4860, Loss function: 3.582, Average Loss: 3.507, avg. samples / sec: 55939.77
Iteration:   4860, Loss function: 3.483, Average Loss: 3.514, avg. samples / sec: 55979.61
Iteration:   4880, Loss function: 3.308, Average Loss: 3.501, avg. samples / sec: 60348.15
Iteration:   4880, Loss function: 3.713, Average Loss: 3.503, avg. samples / sec: 60464.53
Iteration:   4880, Loss function: 2.188, Average Loss: 3.484, avg. samples / sec: 60311.29
Iteration:   4880, Loss function: 2.892, Average Loss: 3.519, avg. samples / sec: 60241.04
Iteration:   4880, Loss function: 2.680, Average Loss: 3.510, avg. samples / sec: 60245.65
Iteration:   4880, Loss function: 3.757, Average Loss: 3.485, avg. samples / sec: 60166.05
Iteration:   4880, Loss function: 3.195, Average Loss: 3.515, avg. samples / sec: 60284.62
Iteration:   4880, Loss function: 3.413, Average Loss: 3.501, avg. samples / sec: 60186.14
Iteration:   4880, Loss function: 2.692, Average Loss: 3.502, avg. samples / sec: 60100.08
Iteration:   4880, Loss function: 2.621, Average Loss: 3.496, avg. samples / sec: 60011.55
Iteration:   4880, Loss function: 3.960, Average Loss: 3.501, avg. samples / sec: 60239.83
Iteration:   4880, Loss function: 3.865, Average Loss: 3.508, avg. samples / sec: 60193.70
Iteration:   4880, Loss function: 3.416, Average Loss: 3.499, avg. samples / sec: 60081.71
Iteration:   4880, Loss function: 3.189, Average Loss: 3.496, avg. samples / sec: 59901.00
Iteration:   4880, Loss function: 3.401, Average Loss: 3.490, avg. samples / sec: 60021.62
:::MLL 1558639547.331 epoch_stop: {"value": null, "metadata": {"epoch_num": 70, "file": "train.py", "lineno": 819}}
:::MLL 1558639547.331 epoch_start: {"value": null, "metadata": {"epoch_num": 71, "file": "train.py", "lineno": 673}}
Iteration:   4900, Loss function: 3.220, Average Loss: 3.500, avg. samples / sec: 59579.19
Iteration:   4900, Loss function: 3.278, Average Loss: 3.484, avg. samples / sec: 59231.22
Iteration:   4900, Loss function: 2.420, Average Loss: 3.482, avg. samples / sec: 59304.73
Iteration:   4900, Loss function: 4.016, Average Loss: 3.509, avg. samples / sec: 59252.52
Iteration:   4900, Loss function: 2.756, Average Loss: 3.489, avg. samples / sec: 59531.75
Iteration:   4900, Loss function: 2.726, Average Loss: 3.501, avg. samples / sec: 59299.04
Iteration:   4900, Loss function: 3.101, Average Loss: 3.492, avg. samples / sec: 59062.82
Iteration:   4900, Loss function: 5.046, Average Loss: 3.512, avg. samples / sec: 59163.04
Iteration:   4900, Loss function: 3.169, Average Loss: 3.499, avg. samples / sec: 59310.42
Iteration:   4900, Loss function: 3.132, Average Loss: 3.514, avg. samples / sec: 59134.37
Iteration:   4900, Loss function: 4.355, Average Loss: 3.505, avg. samples / sec: 59244.10
Iteration:   4900, Loss function: 3.321, Average Loss: 3.493, avg. samples / sec: 59130.00
Iteration:   4900, Loss function: 3.372, Average Loss: 3.499, avg. samples / sec: 58954.23
Iteration:   4900, Loss function: 2.366, Average Loss: 3.489, avg. samples / sec: 59274.87
Iteration:   4900, Loss function: 3.873, Average Loss: 3.501, avg. samples / sec: 59026.83
Iteration:   4920, Loss function: 2.655, Average Loss: 3.497, avg. samples / sec: 60086.29
Iteration:   4920, Loss function: 4.288, Average Loss: 3.474, avg. samples / sec: 59770.52
Iteration:   4920, Loss function: 2.855, Average Loss: 3.495, avg. samples / sec: 60047.25
Iteration:   4920, Loss function: 2.920, Average Loss: 3.488, avg. samples / sec: 60001.59
Iteration:   4920, Loss function: 3.227, Average Loss: 3.480, avg. samples / sec: 59658.61
Iteration:   4920, Loss function: 2.600, Average Loss: 3.496, avg. samples / sec: 59686.10
Iteration:   4920, Loss function: 4.021, Average Loss: 3.508, avg. samples / sec: 59830.71
Iteration:   4920, Loss function: 3.853, Average Loss: 3.489, avg. samples / sec: 59834.50
Iteration:   4920, Loss function: 4.356, Average Loss: 3.509, avg. samples / sec: 59598.31
Iteration:   4920, Loss function: 2.711, Average Loss: 3.500, avg. samples / sec: 59484.31
Iteration:   4920, Loss function: 2.443, Average Loss: 3.488, avg. samples / sec: 59599.90
Iteration:   4920, Loss function: 3.132, Average Loss: 3.508, avg. samples / sec: 59615.46
Iteration:   4920, Loss function: 2.967, Average Loss: 3.485, avg. samples / sec: 59540.05
Iteration:   4920, Loss function: 2.955, Average Loss: 3.497, avg. samples / sec: 59578.31
Iteration:   4920, Loss function: 3.053, Average Loss: 3.497, avg. samples / sec: 59629.61
Iteration:   4940, Loss function: 2.551, Average Loss: 3.495, avg. samples / sec: 57116.68
Iteration:   4940, Loss function: 3.005, Average Loss: 3.469, avg. samples / sec: 56785.64
Iteration:   4940, Loss function: 3.629, Average Loss: 3.481, avg. samples / sec: 57004.45
Iteration:   4940, Loss function: 2.750, Average Loss: 3.493, avg. samples / sec: 56706.65
Iteration:   4940, Loss function: 3.105, Average Loss: 3.490, avg. samples / sec: 56846.32
Iteration:   4940, Loss function: 2.953, Average Loss: 3.506, avg. samples / sec: 56838.61
Iteration:   4940, Loss function: 3.109, Average Loss: 3.505, avg. samples / sec: 56905.81
Iteration:   4940, Loss function: 3.200, Average Loss: 3.484, avg. samples / sec: 56873.89
Iteration:   4940, Loss function: 4.550, Average Loss: 3.488, avg. samples / sec: 56698.82
Iteration:   4940, Loss function: 2.992, Average Loss: 3.494, avg. samples / sec: 56686.46
Iteration:   4940, Loss function: 3.608, Average Loss: 3.478, avg. samples / sec: 56719.61
Iteration:   4940, Loss function: 3.591, Average Loss: 3.493, avg. samples / sec: 56823.56
Iteration:   4940, Loss function: 3.081, Average Loss: 3.489, avg. samples / sec: 56739.50
Iteration:   4940, Loss function: 4.467, Average Loss: 3.504, avg. samples / sec: 56764.53
Iteration:   4940, Loss function: 4.424, Average Loss: 3.499, avg. samples / sec: 56782.44
:::MLL 1558639549.160 eval_start: {"value": null, "metadata": {"epoch_num": 71, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.65 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.55s)
DONE (t=0.57s)
DONE (t=2.80s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.23117
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.39416
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23633
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.06052
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24148
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.37862
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22277
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32528
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.34122
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.10134
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36960
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.53585
Current AP: 0.23117 AP goal: 0.23000
:::MLL 1558639553.199 eval_accuracy: {"value": 0.2311682579915304, "metadata": {"epoch_num": 71, "file": "train.py", "lineno": 389}}
:::MLL 1558639553.208 eval_stop: {"value": null, "metadata": {"epoch_num": 71, "file": "train.py", "lineno": 392}}
:::MLL 1558639553.218 block_stop: {"value": null, "metadata": {"first_epoch_num": 66, "file": "train.py", "lineno": 804}}
:::MLL 1558639554.286 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 849}}
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-05-23 07:26:03 PM
RESULT,SINGLE_STAGE_DETECTOR,,249,nvidia,2019-05-23 07:21:54 PM
ENDING TIMING RUN AT 2019-05-23 07:26:03 PM
RESULT,SINGLE_STAGE_DETECTOR,,249,nvidia,2019-05-23 07:21:54 PM
ENDING TIMING RUN AT 2019-05-23 07:26:03 PM
RESULT,SINGLE_STAGE_DETECTOR,,249,nvidia,2019-05-23 07:21:54 PM
ENDING TIMING RUN AT 2019-05-23 07:26:03 PM
RESULT,SINGLE_STAGE_DETECTOR,,249,nvidia,2019-05-23 07:21:54 PM
ENDING TIMING RUN AT 2019-05-23 07:26:03 PM
RESULT,SINGLE_STAGE_DETECTOR,,249,nvidia,2019-05-23 07:21:54 PM
ENDING TIMING RUN AT 2019-05-23 07:26:03 PM
RESULT,SINGLE_STAGE_DETECTOR,,249,nvidia,2019-05-23 07:21:54 PM
ENDING TIMING RUN AT 2019-05-23 07:26:03 PM
RESULT,SINGLE_STAGE_DETECTOR,,249,nvidia,2019-05-23 07:21:54 PM
ENDING TIMING RUN AT 2019-05-23 07:26:03 PM
RESULT,SINGLE_STAGE_DETECTOR,,249,nvidia,2019-05-23 07:21:54 PM
ENDING TIMING RUN AT 2019-05-23 07:26:03 PM
RESULT,SINGLE_STAGE_DETECTOR,,249,nvidia,2019-05-23 07:21:54 PM
ENDING TIMING RUN AT 2019-05-23 07:26:04 PM
RESULT,SINGLE_STAGE_DETECTOR,,250,nvidia,2019-05-23 07:21:54 PM
ENDING TIMING RUN AT 2019-05-23 07:26:04 PM
RESULT,SINGLE_STAGE_DETECTOR,,250,nvidia,2019-05-23 07:21:54 PM
ENDING TIMING RUN AT 2019-05-23 07:26:04 PM
RESULT,SINGLE_STAGE_DETECTOR,,250,nvidia,2019-05-23 07:21:54 PM
ENDING TIMING RUN AT 2019-05-23 07:26:04 PM
RESULT,SINGLE_STAGE_DETECTOR,,250,nvidia,2019-05-23 07:21:54 PM
ENDING TIMING RUN AT 2019-05-23 07:26:04 PM
RESULT,SINGLE_STAGE_DETECTOR,,250,nvidia,2019-05-23 07:21:54 PM
ENDING TIMING RUN AT 2019-05-23 07:26:04 PM
RESULT,SINGLE_STAGE_DETECTOR,,250,nvidia,2019-05-23 07:21:54 PM
