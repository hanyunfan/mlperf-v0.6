Beginning trial 3 of 5
Gathering sys log on circe-n016
:::MLL 1558639095.693 submission_benchmark: {"value": "ssd", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1558639095.694 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known ssd keys.
:::MLL 1558639095.694 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1558639095.694 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1558639095.695 submission_platform: {"value": "15xNVIDIA DGX-2H", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1558639095.695 submission_entry: {"value": "{'hardware': 'NVIDIA DGX-2H', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': 'Ubuntu 18.04.2 LTS / NVIDIA DGX Server 4.0.5', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.4-2.0.7', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '15', 'cpu': '2x Intel(R) Xeon(R) Platinum 8174 CPU @ 3.10GHz', 'num_cores': '48', 'num_vcpus': '96', 'accelerator': 'Tesla V100-SXM3-32GB-H', 'num_accelerators': '16', 'sys_mem_size': '1510 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '2x 894.3G + 8x 3.5T', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27800 Family [ConnectX-5]', 'num_network_cards': '10', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1558639095.696 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1558639095.696 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
:::MLL 1558639098.521 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639098.516 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639098.538 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639098.538 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639098.542 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639098.568 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639098.556 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639098.576 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639098.566 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639098.588 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639098.570 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639098.598 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639098.617 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639098.595 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
:::MLL 1558639098.672 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node circe-n016
+ pids+=($!)
+ set +x
Launching on node circe-n017
+ pids+=($!)
+ set +x
Launching on node circe-n018
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n016
+ pids+=($!)
+ set +x
Launching on node circe-n019
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n017
+ srun --mem=0 -N 1 -n 1 -w circe-n016 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=0 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n020
+ srun --mem=0 -N 1 -n 1 -w circe-n017 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=1 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n018
+ pids+=($!)
+ set +x
Launching on node circe-n021
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n019
+ pids+=($!)
+ set +x
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
Launching on node circe-n022
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n020
+ srun --mem=0 -N 1 -n 1 -w circe-n018 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=2 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n023
+ srun --mem=0 -N 1 -n 1 -w circe-n019 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=3 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n021
+ srun --mem=0 -N 1 -n 1 -w circe-n020 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=4 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n024
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n022
+ srun --mem=0 -N 1 -n 1 -w circe-n021 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=5 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n025
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n023
+ srun --mem=0 -N 1 -n 1 -w circe-n022 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=6 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n026
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n024
+ srun --mem=0 -N 1 -n 1 -w circe-n023 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=7 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n027
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n025
+ srun --mem=0 -N 1 -n 1 -w circe-n024 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=8 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n028
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n026
+ srun --mem=0 -N 1 -n 1 -w circe-n025 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=9 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n029
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n027
+ srun --mem=0 -N 1 -n 1 -w circe-n026 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=10 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n030
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n028
+ srun --mem=0 -N 1 -n 1 -w circe-n027 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=11 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ pids+=($!)
+ set +x
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n029
+ srun --mem=0 -N 1 -n 1 -w circe-n028 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=12 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
++ eval echo srun --mem=0 -N 1 -n 1 -w '$hostn'
+++ echo srun --mem=0 -N 1 -n 1 -w circe-n030
+ srun --mem=0 -N 1 -n 1 -w circe-n029 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=13 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
+ srun --mem=0 -N 1 -n 1 -w circe-n030 docker exec -e DGXSYSTEM=DGX2_multi_full_15x16x7 -e 'MULTI_NODE= --nnodes=15 --node_rank=14 --master_addr=10.0.1.16 --master_port=5204' -e SLURM_JOB_ID=89739 -e SLURM_NTASKS_PER_NODE=16 cont_89739 ./run_and_time.sh
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=0 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=2 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=3 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=5 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=11 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=1 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=7 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=6 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=13 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=10 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=8 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=12 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=4 --master_addr=10.0.1.16 --master_port=5204
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=9 --master_addr=10.0.1.16 --master_port=5204
STARTING TIMING RUN AT 2019-05-23 07:18:18 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=0 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
Run vars: id 89739 gpus 16 mparams  --nnodes=15 --node_rank=14 --master_addr=10.0.1.16 --master_port=5204
STARTING TIMING RUN AT 2019-05-23 07:18:18 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=3 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:18:18 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=5 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:18:18 PM
+ NUMEPOCHS=80
STARTING TIMING RUN AT 2019-05-23 07:18:18 PM
running benchmark
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ NUMEPOCHS=80
+ DATASET_DIR=/data/coco2017
+ echo 'running benchmark'
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=2 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=11 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
STARTING TIMING RUN AT 2019-05-23 07:18:18 PM
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=1 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:18:18 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
STARTING TIMING RUN AT 2019-05-23 07:18:18 PM
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=6 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=13 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:18:18 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=7 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:18:18 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=8 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:18:18 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=10 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:18:18 PM
running benchmark
+ NUMEPOCHS=80
STARTING TIMING RUN AT 2019-05-23 07:18:18 PM
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
running benchmark
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=12 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=4 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:18:18 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=9 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
STARTING TIMING RUN AT 2019-05-23 07:18:18 PM
running benchmark
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --nnodes=15 --node_rank=14 --master_addr=10.0.1.16 --master_port=5204 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 7 --eval-batch-size 40 --warmup 1250 --num-workers 3 --bn-group 4 --lr 3.1e-3 --wd 2e-4 --input-batch-multiplier 10 --use-nvjpeg --use-roi-decode
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding::::MLL 1558639109.021 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.021 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.021 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.022 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.022 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.022 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.023 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.023 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.024 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.024 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.024 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.024 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.024 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.024 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639109.027 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.030 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
Binding::::MLL 1558639109.147 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.148 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.148 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.149 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.149 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.150 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.150 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.150 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.150 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.150 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.150 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.150 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.151 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.151 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.151 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.151 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639109.137 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.137 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.137 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.137 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.137 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.138 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.138 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.138 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.139 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.139 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639109.139 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.140 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.140 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.141 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.141 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.141 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639109.156 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.156 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.156 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.156 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.157 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.157 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.157 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.157 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.157 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.157 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.157 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.157 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.157 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.158 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.158 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.158 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639109.204 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.204 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.205 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639109.207 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.207 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.207 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639109.209 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.209 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.209 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.210 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.210 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.211 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.211 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639109.211 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.212 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.212 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639109.214 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.214 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.214 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.215 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.215 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.215 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.215 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.216 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.216 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.216 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.216 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.216 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639109.236 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.236 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.236 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.236 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.236 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.236 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.217 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.217 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639109.217 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.217 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639109.237 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.237 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.237 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.237 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
Binding::::MLL 1558639109.227 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639109.238 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.238 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.238 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.239 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.239 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639109.239 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639109.231 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.232 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.233 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.233 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.233 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.234 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.235 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.235 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.236 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.237 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.237 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.237 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.237 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.237 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.237 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639109.236 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.236 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.237 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.237 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.238 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.238 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.238 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.238 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.239 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.239 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.239 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.239 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.240 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.240 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639109.240 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639109.241 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639109.236 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.236 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.236 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.236 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.237 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.237 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.237 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.237 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
Binding::::MLL 1558639109.262 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.263 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.263 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.263 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.238 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.238 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.263 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.239 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.239 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.239 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.239 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.264 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.264 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.239 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.240 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.265 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.265 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.265 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639109.266 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639109.266 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.266 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.266 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.267 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.267 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639109.263 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.263 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.263 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.263 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.264 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.264 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.264 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.264 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.264 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
:::MLL 1558639109.265 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.265 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.266 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.266 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.266 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.266 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.266 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639109.310 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.310 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.310 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.311 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.312 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.312 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.312 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.313 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.313 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.313 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.314 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.314 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.314 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.314 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.314 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.315 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639109.307 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.307 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.307 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.308 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.308 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.308 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.308 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.309 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.309 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.309 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.310 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.310 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.310 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.310 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.310 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.310 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
Binding::::MLL 1558639109.371 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.374 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.374 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.374 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.375 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.376 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.376 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.377 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.377 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.377 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.377 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.377 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.378 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
:::MLL 1558639109.378 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1558639109.378 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
:::MLL 1558639109.378 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
BN group: 4
0 Using seed = 2520238392
1 Using seed = 2520238393
3 Using seed = 2520238395
4 Using seed = 2520238396
5 Using seed = 2520238397
2 Using seed = 2520238394
:::MLL 1558639142.460 max_samples: {"value": 1, "metadata": {"file": "utils.py", "lineno": 465}}
11 Using seed = 2520238403
9 Using seed = 2520238401
8 Using seed = 2520238400
7 Using seed = 2520238399
6 Using seed = 2520238398
22 Using seed = 2520238414
20 Using seed = 2520238412
27 Using seed = 2520238419
21 Using seed = 2520238413
26 Using seed = 2520238418
30 Using seed = 2520238422
31 Using seed = 2520238423
17 Using seed = 2520238409
28 Using seed = 2520238420
23 Using seed = 2520238415
25 Using seed = 2520238417
16 Using seed = 2520238408
18 Using seed = 2520238410
29 Using seed = 2520238421
19 Using seed = 2520238411
24 Using seed = 2520238416
44 Using seed = 2520238436
46 Using seed = 2520238438
37 Using seed = 2520238429
33 Using seed = 2520238425
47 Using seed = 2520238439
42 Using seed = 2520238434
35 Using seed = 2520238427
34 Using seed = 2520238426
45 Using seed = 2520238437
36 Using seed = 2520238428
32 Using seed = 2520238424
41 Using seed = 2520238433
43 Using seed = 2520238435
39 Using seed = 2520238431
40 Using seed = 2520238432
38 Using seed = 2520238430
61 Using seed = 2520238453
62 Using seed = 2520238454
60 Using seed = 2520238452
50 Using seed = 2520238442
52 Using seed = 2520238444
53 Using seed = 2520238445
48 Using seed = 2520238440
49 Using seed = 2520238441
63 Using seed = 2520238455
58 Using seed = 2520238450
51 Using seed = 2520238443
55 Using seed = 2520238447
57 Using seed = 2520238449
59 Using seed = 2520238451
56 Using seed = 2520238448
54 Using seed = 2520238446
74 Using seed = 2520238466
77 Using seed = 2520238469
78 Using seed = 2520238470
79 Using seed = 2520238471
67 Using seed = 2520238459
69 Using seed = 2520238461
65 Using seed = 2520238457
64 Using seed = 2520238456
66 Using seed = 2520238458
68 Using seed = 2520238460
76 Using seed = 2520238468
73 Using seed = 2520238465
75 Using seed = 2520238467
71 Using seed = 2520238463
72 Using seed = 2520238464
70 Using seed = 2520238462
83 Using seed = 2520238475
81 Using seed = 2520238473
86 Using seed = 2520238478
84 Using seed = 2520238476
80 Using seed = 2520238472
82 Using seed = 2520238474
94 Using seed = 2520238486
85 Using seed = 2520238477
95 Using seed = 2520238487
93 Using seed = 2520238485
90 Using seed = 2520238482
89 Using seed = 2520238481
87 Using seed = 2520238479
92 Using seed = 2520238484
91 Using seed = 2520238483
88 Using seed = 2520238480
111 Using seed = 2520238503
105 Using seed = 2520238497
109 Using seed = 2520238501
108 Using seed = 2520238500
107 Using seed = 2520238499
106 Using seed = 2520238498
110 Using seed = 2520238502
104 Using seed = 2520238496
100 Using seed = 2520238492
102 Using seed = 2520238494
99 Using seed = 2520238491
97 Using seed = 2520238489
98 Using seed = 2520238490
96 Using seed = 2520238488
101 Using seed = 2520238493
103 Using seed = 2520238495
118 Using seed = 2520238510
113 Using seed = 2520238505
114 Using seed = 2520238506
115 Using seed = 2520238507
116 Using seed = 2520238508
125 Using seed = 2520238517
112 Using seed = 2520238504
126 Using seed = 2520238518
120 Using seed = 2520238512
123 Using seed = 2520238515
122 Using seed = 2520238514
124 Using seed = 2520238516
127 Using seed = 2520238519
117 Using seed = 2520238509
121 Using seed = 2520238513
119 Using seed = 2520238511
138 Using seed = 2520238530
142 Using seed = 2520238534
143 Using seed = 2520238535
141 Using seed = 2520238533
130 Using seed = 2520238522
129 Using seed = 2520238521
128 Using seed = 2520238520
132 Using seed = 2520238524
131 Using seed = 2520238523
133 Using seed = 2520238525
140 Using seed = 2520238532
139 Using seed = 2520238531
137 Using seed = 2520238529
136 Using seed = 2520238528
135 Using seed = 2520238527
134 Using seed = 2520238526
157 Using seed = 2520238549
150 Using seed = 2520238542
154 Using seed = 2520238546
158 Using seed = 2520238550
152 Using seed = 2520238544
155 Using seed = 2520238547
156 Using seed = 2520238548
145 Using seed = 2520238537
148 Using seed = 2520238540
147 Using seed = 2520238539
144 Using seed = 2520238536
149 Using seed = 2520238541
146 Using seed = 2520238538
153 Using seed = 2520238545
151 Using seed = 2520238543
159 Using seed = 2520238551
172 Using seed = 2520238564
174 Using seed = 2520238566
166 Using seed = 2520238558
169 Using seed = 2520238561
160 Using seed = 2520238552
170 Using seed = 2520238562
175 Using seed = 2520238567
161 Using seed = 2520238553
163 Using seed = 2520238555
162 Using seed = 2520238554
164 Using seed = 2520238556
171 Using seed = 2520238563
168 Using seed = 2520238560
173 Using seed = 2520238565
165 Using seed = 2520238557
167 Using seed = 2520238559
190 Using seed = 2520238582
191 Using seed = 2520238583
186 Using seed = 2520238578
189 Using seed = 2520238581
188 Using seed = 2520238580
178 Using seed = 2520238570
176 Using seed = 2520238568
179 Using seed = 2520238571
177 Using seed = 2520238569
181 Using seed = 2520238573
180 Using seed = 2520238572
183 Using seed = 2520238575
187 Using seed = 2520238579
185 Using seed = 2520238577
184 Using seed = 2520238576
182 Using seed = 2520238574
202 Using seed = 2520238594
205 Using seed = 2520238597
206 Using seed = 2520238598
207 Using seed = 2520238599
195 Using seed = 2520238587
196 Using seed = 2520238588
194 Using seed = 2520238586
193 Using seed = 2520238585
192 Using seed = 2520238584
197 Using seed = 2520238589
204 Using seed = 2520238596
201 Using seed = 2520238593
203 Using seed = 2520238595
200 Using seed = 2520238592
199 Using seed = 2520238591
198 Using seed = 2520238590
214 Using seed = 2520238606
210 Using seed = 2520238602
218 Using seed = 2520238610
208 Using seed = 2520238600
209 Using seed = 2520238601
220 Using seed = 2520238612
223 Using seed = 2520238615
216 Using seed = 2520238608
221 Using seed = 2520238613
217 Using seed = 2520238609
219 Using seed = 2520238611
211 Using seed = 2520238603
222 Using seed = 2520238614
212 Using seed = 2520238604
213 Using seed = 2520238605
215 Using seed = 2520238607
235 Using seed = 2520238627
227 Using seed = 2520238619
236 Using seed = 2520238628
230 Using seed = 2520238622
238 Using seed = 2520238630
225 Using seed = 2520238617
228 Using seed = 2520238620
233 Using seed = 2520238625
234 Using seed = 2520238626
226 Using seed = 2520238618
239 Using seed = 2520238631
231 Using seed = 2520238623
237 Using seed = 2520238629
232 Using seed = 2520238624
224 Using seed = 2520238616
229 Using seed = 2520238621
14 Using seed = 2520238406
12 Using seed = 2520238404
15 Using seed = 2520238407
10 Using seed = 2520238402
13 Using seed = 2520238405
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
:::MLL 1558639147.846 model_bn_span: {"value": 28, "metadata": {"file": "train.py", "lineno": 480}}
:::MLL 1558639147.846 global_batch_size: {"value": 1680, "metadata": {"file": "train.py", "lineno": 481}}
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
:::MLL 1558639147.852 opt_base_learning_rate: {"value": 0.1625, "metadata": {"file": "train.py", "lineno": 511}}
:::MLL 1558639147.852 opt_weight_decay: {"value": 0.0002, "metadata": {"file": "train.py", "lineno": 513}}
:::MLL 1558639147.853 opt_learning_rate_warmup_steps: {"value": 1250, "metadata": {"file": "train.py", "lineno": 516}}
:::MLL 1558639147.853 opt_learning_rate_warmup_factor: {"value": 0, "metadata": {"file": "train.py", "lineno": 518}}
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
:::MLL 1558639154.826 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 604}}
:::MLL 1558639154.827 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 610}}
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
Done (t=0.45s)
creating index...
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.49s)
creating index...
time_check a: 1558639156.529141188
time_check a: 1558639156.508333683
time_check a: 1558639156.534944773
time_check a: 1558639156.539253712
time_check a: 1558639156.531191349
time_check a: 1558639156.532869816
time_check a: 1558639156.541682482
time_check a: 1558639156.548605442
time_check a: 1558639156.529715776
time_check a: 1558639156.553200483
time_check a: 1558639156.544839382
time_check a: 1558639156.536795855
time_check a: 1558639156.537956238
time_check a: 1558639156.519102097
time_check a: 1558639156.551903486
time_check b: 1558639163.210885525
time_check b: 1558639163.278058052
time_check b: 1558639163.273566723
time_check b: 1558639163.341804504
time_check b: 1558639163.371544600
time_check b: 1558639163.387259245
time_check b: 1558639163.390764713
time_check b: 1558639163.413009644
time_check b: 1558639163.419379234
time_check b: 1558639163.433703661
time_check b: 1558639163.471363306
time_check b: 1558639163.483530283
time_check b: 1558639163.481578588
time_check b: 1558639163.518803596
time_check b: 1558639163.512347460
:::MLL 1558639164.773 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 32.74606450292497, "file": "train.py", "lineno": 669}}
:::MLL 1558639164.774 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 673}}
Iteration:      0, Loss function: 22.542, Average Loss: 0.023, avg. samples / sec: 108.94
Iteration:      0, Loss function: 22.879, Average Loss: 0.023, avg. samples / sec: 117.00
Iteration:      0, Loss function: 22.830, Average Loss: 0.023, avg. samples / sec: 116.25
Iteration:      0, Loss function: 22.721, Average Loss: 0.023, avg. samples / sec: 117.12
Iteration:      0, Loss function: 23.044, Average Loss: 0.023, avg. samples / sec: 118.31
Iteration:      0, Loss function: 22.453, Average Loss: 0.022, avg. samples / sec: 108.05
Iteration:      0, Loss function: 22.161, Average Loss: 0.022, avg. samples / sec: 112.87
Iteration:      0, Loss function: 23.122, Average Loss: 0.023, avg. samples / sec: 107.97
Iteration:      0, Loss function: 22.260, Average Loss: 0.022, avg. samples / sec: 117.34
Iteration:      0, Loss function: 22.389, Average Loss: 0.022, avg. samples / sec: 119.54
Iteration:      0, Loss function: 22.508, Average Loss: 0.023, avg. samples / sec: 114.64
Iteration:      0, Loss function: 22.260, Average Loss: 0.022, avg. samples / sec: 118.10
Iteration:      0, Loss function: 22.416, Average Loss: 0.022, avg. samples / sec: 108.85
Iteration:      0, Loss function: 22.185, Average Loss: 0.022, avg. samples / sec: 115.50
Iteration:      0, Loss function: 22.241, Average Loss: 0.022, avg. samples / sec: 108.94
Iteration:     20, Loss function: 20.750, Average Loss: 0.445, avg. samples / sec: 37239.18
Iteration:     20, Loss function: 20.145, Average Loss: 0.442, avg. samples / sec: 37370.55
Iteration:     20, Loss function: 20.105, Average Loss: 0.443, avg. samples / sec: 37726.14
Iteration:     20, Loss function: 20.343, Average Loss: 0.441, avg. samples / sec: 37450.42
Iteration:     20, Loss function: 20.366, Average Loss: 0.442, avg. samples / sec: 37932.80
Iteration:     20, Loss function: 21.941, Average Loss: 0.446, avg. samples / sec: 36646.06
Iteration:     20, Loss function: 20.515, Average Loss: 0.444, avg. samples / sec: 37170.69
Iteration:     20, Loss function: 20.855, Average Loss: 0.444, avg. samples / sec: 36906.88
Iteration:     20, Loss function: 20.908, Average Loss: 0.444, avg. samples / sec: 36830.09
Iteration:     20, Loss function: 20.520, Average Loss: 0.444, avg. samples / sec: 36823.20
Iteration:     20, Loss function: 20.173, Average Loss: 0.444, avg. samples / sec: 37042.85
Iteration:     20, Loss function: 20.778, Average Loss: 0.445, avg. samples / sec: 37164.91
Iteration:     20, Loss function: 20.689, Average Loss: 0.448, avg. samples / sec: 36874.68
Iteration:     20, Loss function: 21.443, Average Loss: 0.447, avg. samples / sec: 36478.59
Iteration:     20, Loss function: 20.796, Average Loss: 0.444, avg. samples / sec: 36210.40
Iteration:     40, Loss function: 19.805, Average Loss: 0.839, avg. samples / sec: 55772.82
Iteration:     40, Loss function: 19.183, Average Loss: 0.840, avg. samples / sec: 54707.63
Iteration:     40, Loss function: 19.872, Average Loss: 0.835, avg. samples / sec: 54039.80
Iteration:     40, Loss function: 19.032, Average Loss: 0.832, avg. samples / sec: 54398.29
Iteration:     40, Loss function: 21.025, Average Loss: 0.841, avg. samples / sec: 53491.16
Iteration:     40, Loss function: 19.258, Average Loss: 0.838, avg. samples / sec: 54925.43
Iteration:     40, Loss function: 19.260, Average Loss: 0.832, avg. samples / sec: 53909.84
Iteration:     40, Loss function: 20.002, Average Loss: 0.840, avg. samples / sec: 54164.42
Iteration:     40, Loss function: 19.513, Average Loss: 0.834, avg. samples / sec: 53595.89
Iteration:     40, Loss function: 19.461, Average Loss: 0.838, avg. samples / sec: 54245.42
Iteration:     40, Loss function: 19.071, Average Loss: 0.839, avg. samples / sec: 53822.48
Iteration:     40, Loss function: 19.153, Average Loss: 0.836, avg. samples / sec: 54008.01
Iteration:     40, Loss function: 19.581, Average Loss: 0.833, avg. samples / sec: 53458.05
Iteration:     40, Loss function: 19.409, Average Loss: 0.834, avg. samples / sec: 53246.68
Iteration:     40, Loss function: 19.255, Average Loss: 0.839, avg. samples / sec: 53302.56
Iteration:     60, Loss function: 12.599, Average Loss: 1.113, avg. samples / sec: 57332.17
Iteration:     60, Loss function: 15.664, Average Loss: 1.119, avg. samples / sec: 57705.98
Iteration:     60, Loss function: 13.486, Average Loss: 1.114, avg. samples / sec: 57524.11
Iteration:     60, Loss function: 14.265, Average Loss: 1.114, avg. samples / sec: 57262.43
Iteration:     60, Loss function: 13.437, Average Loss: 1.118, avg. samples / sec: 57484.20
Iteration:     60, Loss function: 14.113, Average Loss: 1.114, avg. samples / sec: 57662.72
Iteration:     60, Loss function: 12.267, Average Loss: 1.112, avg. samples / sec: 58525.93
Iteration:     60, Loss function: 15.993, Average Loss: 1.118, avg. samples / sec: 57635.51
Iteration:     60, Loss function: 13.568, Average Loss: 1.128, avg. samples / sec: 57071.92
Iteration:     60, Loss function: 14.057, Average Loss: 1.115, avg. samples / sec: 58316.05
Iteration:     60, Loss function: 12.494, Average Loss: 1.115, avg. samples / sec: 57248.73
Iteration:     60, Loss function: 11.728, Average Loss: 1.115, avg. samples / sec: 56842.53
Iteration:     60, Loss function: 13.909, Average Loss: 1.111, avg. samples / sec: 57056.40
Iteration:     60, Loss function: 14.106, Average Loss: 1.119, avg. samples / sec: 57028.78
Iteration:     60, Loss function: 16.789, Average Loss: 1.119, avg. samples / sec: 56797.93
:::MLL 1558639167.923 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 819}}
:::MLL 1558639167.923 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 673}}
Iteration:     80, Loss function: 10.858, Average Loss: 1.343, avg. samples / sec: 59379.14
Iteration:     80, Loss function: 11.811, Average Loss: 1.338, avg. samples / sec: 58904.65
Iteration:     80, Loss function: 11.758, Average Loss: 1.338, avg. samples / sec: 58922.09
Iteration:     80, Loss function: 12.295, Average Loss: 1.335, avg. samples / sec: 58882.90
Iteration:     80, Loss function: 11.521, Average Loss: 1.332, avg. samples / sec: 59128.44
Iteration:     80, Loss function: 12.207, Average Loss: 1.352, avg. samples / sec: 58838.69
Iteration:     80, Loss function: 10.937, Average Loss: 1.333, avg. samples / sec: 58698.20
Iteration:     80, Loss function: 11.077, Average Loss: 1.343, avg. samples / sec: 58667.75
Iteration:     80, Loss function: 10.808, Average Loss: 1.335, avg. samples / sec: 58933.42
Iteration:     80, Loss function: 11.635, Average Loss: 1.346, avg. samples / sec: 58542.36
Iteration:     80, Loss function: 11.562, Average Loss: 1.337, avg. samples / sec: 58468.57
Iteration:     80, Loss function: 11.102, Average Loss: 1.332, avg. samples / sec: 58319.48
Iteration:     80, Loss function: 11.202, Average Loss: 1.332, avg. samples / sec: 58236.24
Iteration:     80, Loss function: 11.949, Average Loss: 1.344, avg. samples / sec: 58637.16
Iteration:     80, Loss function: 10.694, Average Loss: 1.329, avg. samples / sec: 58000.23
Iteration:    100, Loss function: 8.918, Average Loss: 1.527, avg. samples / sec: 58056.29
Iteration:    100, Loss function: 9.732, Average Loss: 1.517, avg. samples / sec: 57777.66
Iteration:    100, Loss function: 10.204, Average Loss: 1.520, avg. samples / sec: 58567.15
Iteration:    100, Loss function: 9.109, Average Loss: 1.520, avg. samples / sec: 58094.36
Iteration:    100, Loss function: 9.972, Average Loss: 1.505, avg. samples / sec: 57900.15
Iteration:    100, Loss function: 9.556, Average Loss: 1.504, avg. samples / sec: 58224.38
Iteration:    100, Loss function: 9.553, Average Loss: 1.511, avg. samples / sec: 58133.66
Iteration:    100, Loss function: 9.509, Average Loss: 1.511, avg. samples / sec: 57777.66
Iteration:    100, Loss function: 10.368, Average Loss: 1.514, avg. samples / sec: 57892.78
Iteration:    100, Loss function: 9.719, Average Loss: 1.515, avg. samples / sec: 57623.51
Iteration:    100, Loss function: 9.288, Average Loss: 1.518, avg. samples / sec: 57729.47
Iteration:    100, Loss function: 10.173, Average Loss: 1.514, avg. samples / sec: 57588.22
Iteration:    100, Loss function: 9.174, Average Loss: 1.506, avg. samples / sec: 58360.27
Iteration:    100, Loss function: 9.203, Average Loss: 1.505, avg. samples / sec: 57564.79
Iteration:    100, Loss function: 9.533, Average Loss: 1.513, avg. samples / sec: 57987.85
Iteration:    120, Loss function: 8.424, Average Loss: 1.655, avg. samples / sec: 58503.94
Iteration:    120, Loss function: 9.484, Average Loss: 1.663, avg. samples / sec: 58904.36
Iteration:    120, Loss function: 9.601, Average Loss: 1.669, avg. samples / sec: 58448.33
Iteration:    120, Loss function: 10.078, Average Loss: 1.663, avg. samples / sec: 58522.28
Iteration:    120, Loss function: 9.294, Average Loss: 1.672, avg. samples / sec: 58298.37
Iteration:    120, Loss function: 9.114, Average Loss: 1.662, avg. samples / sec: 58574.80
Iteration:    120, Loss function: 9.120, Average Loss: 1.671, avg. samples / sec: 58245.22
Iteration:    120, Loss function: 8.623, Average Loss: 1.672, avg. samples / sec: 58271.52
Iteration:    120, Loss function: 9.394, Average Loss: 1.663, avg. samples / sec: 58668.60
Iteration:    120, Loss function: 9.142, Average Loss: 1.655, avg. samples / sec: 58272.02
Iteration:    120, Loss function: 8.666, Average Loss: 1.675, avg. samples / sec: 58489.22
Iteration:    120, Loss function: 8.876, Average Loss: 1.660, avg. samples / sec: 58555.11
Iteration:    120, Loss function: 8.859, Average Loss: 1.667, avg. samples / sec: 58311.93
Iteration:    120, Loss function: 8.428, Average Loss: 1.680, avg. samples / sec: 57864.92
Iteration:    120, Loss function: 8.539, Average Loss: 1.669, avg. samples / sec: 58023.97
:::MLL 1558639169.933 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 819}}
:::MLL 1558639169.933 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 673}}
Iteration:    140, Loss function: 9.029, Average Loss: 1.804, avg. samples / sec: 59567.08
Iteration:    140, Loss function: 8.870, Average Loss: 1.812, avg. samples / sec: 59445.44
Iteration:    140, Loss function: 8.699, Average Loss: 1.812, avg. samples / sec: 59326.12
Iteration:    140, Loss function: 8.630, Average Loss: 1.805, avg. samples / sec: 59136.55
Iteration:    140, Loss function: 7.361, Average Loss: 1.815, avg. samples / sec: 59294.35
Iteration:    140, Loss function: 8.808, Average Loss: 1.798, avg. samples / sec: 59314.39
Iteration:    140, Loss function: 8.480, Average Loss: 1.822, avg. samples / sec: 59546.66
Iteration:    140, Loss function: 9.308, Average Loss: 1.803, avg. samples / sec: 59217.06
Iteration:    140, Loss function: 8.211, Average Loss: 1.805, avg. samples / sec: 59200.05
Iteration:    140, Loss function: 9.006, Average Loss: 1.809, avg. samples / sec: 58961.14
Iteration:    140, Loss function: 8.474, Average Loss: 1.802, avg. samples / sec: 58915.49
Iteration:    140, Loss function: 9.154, Average Loss: 1.810, avg. samples / sec: 58918.42
Iteration:    140, Loss function: 8.821, Average Loss: 1.812, avg. samples / sec: 58954.58
Iteration:    140, Loss function: 9.970, Average Loss: 1.799, avg. samples / sec: 58557.95
Iteration:    140, Loss function: 8.841, Average Loss: 1.814, avg. samples / sec: 59148.07
Iteration:    160, Loss function: 8.434, Average Loss: 1.954, avg. samples / sec: 58811.88
Iteration:    160, Loss function: 8.589, Average Loss: 1.954, avg. samples / sec: 58896.09
Iteration:    160, Loss function: 8.304, Average Loss: 1.936, avg. samples / sec: 58794.43
Iteration:    160, Loss function: 9.025, Average Loss: 1.951, avg. samples / sec: 58924.06
Iteration:    160, Loss function: 8.542, Average Loss: 1.954, avg. samples / sec: 59227.32
Iteration:    160, Loss function: 8.041, Average Loss: 1.943, avg. samples / sec: 58773.79
Iteration:    160, Loss function: 8.728, Average Loss: 1.943, avg. samples / sec: 58475.68
Iteration:    160, Loss function: 8.304, Average Loss: 1.962, avg. samples / sec: 58718.01
Iteration:    160, Loss function: 7.367, Average Loss: 1.945, avg. samples / sec: 58664.18
Iteration:    160, Loss function: 9.028, Average Loss: 1.944, avg. samples / sec: 58903.82
Iteration:    160, Loss function: 8.847, Average Loss: 1.945, avg. samples / sec: 59175.31
Iteration:    160, Loss function: 8.816, Average Loss: 1.960, avg. samples / sec: 59198.75
Iteration:    160, Loss function: 8.398, Average Loss: 1.948, avg. samples / sec: 58913.69
Iteration:    160, Loss function: 8.570, Average Loss: 1.952, avg. samples / sec: 58345.82
Iteration:    160, Loss function: 8.806, Average Loss: 1.944, avg. samples / sec: 58422.79
Iteration:    180, Loss function: 8.258, Average Loss: 2.079, avg. samples / sec: 58854.42
Iteration:    180, Loss function: 8.321, Average Loss: 2.072, avg. samples / sec: 58981.32
Iteration:    180, Loss function: 8.254, Average Loss: 2.085, avg. samples / sec: 58420.05
Iteration:    180, Loss function: 8.686, Average Loss: 2.071, avg. samples / sec: 58650.68
Iteration:    180, Loss function: 8.194, Average Loss: 2.065, avg. samples / sec: 58513.31
Iteration:    180, Loss function: 7.918, Average Loss: 2.074, avg. samples / sec: 58613.16
Iteration:    180, Loss function: 8.588, Average Loss: 2.074, avg. samples / sec: 58608.80
Iteration:    180, Loss function: 8.924, Average Loss: 2.081, avg. samples / sec: 58862.02
Iteration:    180, Loss function: 9.291, Average Loss: 2.071, avg. samples / sec: 58537.11
Iteration:    180, Loss function: 8.461, Average Loss: 2.076, avg. samples / sec: 58495.24
Iteration:    180, Loss function: 9.585, Average Loss: 2.092, avg. samples / sec: 58429.79
Iteration:    180, Loss function: 8.014, Average Loss: 2.082, avg. samples / sec: 58341.72
Iteration:    180, Loss function: 8.280, Average Loss: 2.079, avg. samples / sec: 58344.40
Iteration:    180, Loss function: 8.747, Average Loss: 2.080, avg. samples / sec: 58080.26
Iteration:    180, Loss function: 9.451, Average Loss: 2.092, avg. samples / sec: 58244.04
Iteration:    200, Loss function: 7.753, Average Loss: 2.206, avg. samples / sec: 58986.95
Iteration:    200, Loss function: 8.150, Average Loss: 2.182, avg. samples / sec: 58952.31
Iteration:    200, Loss function: 7.676, Average Loss: 2.199, avg. samples / sec: 58853.41
Iteration:    200, Loss function: 8.250, Average Loss: 2.196, avg. samples / sec: 59069.26
Iteration:    200, Loss function: 8.365, Average Loss: 2.193, avg. samples / sec: 58857.39
Iteration:    200, Loss function: 8.272, Average Loss: 2.199, avg. samples / sec: 59076.02
Iteration:    200, Loss function: 9.035, Average Loss: 2.200, avg. samples / sec: 58806.55
Iteration:    200, Loss function: 8.503, Average Loss: 2.213, avg. samples / sec: 59231.10
Iteration:    200, Loss function: 8.630, Average Loss: 2.193, avg. samples / sec: 58703.26
Iteration:    200, Loss function: 8.510, Average Loss: 2.209, avg. samples / sec: 58688.64
Iteration:    200, Loss function: 8.165, Average Loss: 2.197, avg. samples / sec: 58727.52
Iteration:    200, Loss function: 8.676, Average Loss: 2.203, avg. samples / sec: 58910.24
Iteration:    200, Loss function: 7.801, Average Loss: 2.219, avg. samples / sec: 58879.35
Iteration:    200, Loss function: 8.462, Average Loss: 2.203, avg. samples / sec: 58943.06
Iteration:    200, Loss function: 7.566, Average Loss: 2.194, avg. samples / sec: 58748.99
:::MLL 1558639171.934 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 819}}
:::MLL 1558639171.935 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 673}}
Iteration:    220, Loss function: 8.443, Average Loss: 2.322, avg. samples / sec: 58683.09
Iteration:    220, Loss function: 8.479, Average Loss: 2.317, avg. samples / sec: 58637.33
Iteration:    220, Loss function: 8.896, Average Loss: 2.343, avg. samples / sec: 58687.05
Iteration:    220, Loss function: 8.432, Average Loss: 2.305, avg. samples / sec: 58501.97
Iteration:    220, Loss function: 8.936, Average Loss: 2.323, avg. samples / sec: 58490.19
Iteration:    220, Loss function: 8.601, Average Loss: 2.332, avg. samples / sec: 58719.86
Iteration:    220, Loss function: 7.762, Average Loss: 2.329, avg. samples / sec: 58452.08
Iteration:    220, Loss function: 9.065, Average Loss: 2.321, avg. samples / sec: 58797.57
Iteration:    220, Loss function: 7.619, Average Loss: 2.324, avg. samples / sec: 58662.67
Iteration:    220, Loss function: 8.702, Average Loss: 2.325, avg. samples / sec: 58367.01
Iteration:    220, Loss function: 8.371, Average Loss: 2.323, avg. samples / sec: 58415.33
Iteration:    220, Loss function: 8.536, Average Loss: 2.321, avg. samples / sec: 58459.33
Iteration:    220, Loss function: 7.832, Average Loss: 2.348, avg. samples / sec: 58438.41
Iteration:    220, Loss function: 7.317, Average Loss: 2.327, avg. samples / sec: 58469.76
Iteration:    220, Loss function: 7.665, Average Loss: 2.316, avg. samples / sec: 57911.76
Iteration:    240, Loss function: 6.820, Average Loss: 2.451, avg. samples / sec: 56576.41
Iteration:    240, Loss function: 8.164, Average Loss: 2.442, avg. samples / sec: 56853.17
Iteration:    240, Loss function: 8.506, Average Loss: 2.433, avg. samples / sec: 56476.88
Iteration:    240, Loss function: 8.082, Average Loss: 2.435, avg. samples / sec: 56692.64
Iteration:    240, Loss function: 7.446, Average Loss: 2.431, avg. samples / sec: 56390.82
Iteration:    240, Loss function: 8.196, Average Loss: 2.423, avg. samples / sec: 56412.29
Iteration:    240, Loss function: 8.354, Average Loss: 2.438, avg. samples / sec: 56573.25
Iteration:    240, Loss function: 6.613, Average Loss: 2.439, avg. samples / sec: 56409.53
Iteration:    240, Loss function: 7.914, Average Loss: 2.432, avg. samples / sec: 56318.78
Iteration:    240, Loss function: 7.827, Average Loss: 2.438, avg. samples / sec: 56427.67
Iteration:    240, Loss function: 7.502, Average Loss: 2.433, avg. samples / sec: 56368.36
Iteration:    240, Loss function: 7.611, Average Loss: 2.458, avg. samples / sec: 56624.38
Iteration:    240, Loss function: 8.762, Average Loss: 2.441, avg. samples / sec: 56434.83
Iteration:    240, Loss function: 7.150, Average Loss: 2.441, avg. samples / sec: 56284.79
Iteration:    240, Loss function: 6.884, Average Loss: 2.431, avg. samples / sec: 56972.64
Iteration:    260, Loss function: 7.657, Average Loss: 2.549, avg. samples / sec: 59153.26
Iteration:    260, Loss function: 8.277, Average Loss: 2.542, avg. samples / sec: 58977.62
Iteration:    260, Loss function: 7.888, Average Loss: 2.540, avg. samples / sec: 58994.01
Iteration:    260, Loss function: 7.314, Average Loss: 2.543, avg. samples / sec: 59098.88
Iteration:    260, Loss function: 8.541, Average Loss: 2.545, avg. samples / sec: 58943.41
Iteration:    260, Loss function: 9.114, Average Loss: 2.549, avg. samples / sec: 59018.15
Iteration:    260, Loss function: 8.636, Average Loss: 2.546, avg. samples / sec: 59061.48
Iteration:    260, Loss function: 7.523, Average Loss: 2.546, avg. samples / sec: 59079.90
Iteration:    260, Loss function: 9.260, Average Loss: 2.565, avg. samples / sec: 58972.24
Iteration:    260, Loss function: 8.248, Average Loss: 2.559, avg. samples / sec: 58662.64
Iteration:    260, Loss function: 6.962, Average Loss: 2.530, avg. samples / sec: 58784.87
Iteration:    260, Loss function: 7.840, Average Loss: 2.540, avg. samples / sec: 58976.34
Iteration:    260, Loss function: 8.374, Average Loss: 2.553, avg. samples / sec: 58636.80
Iteration:    260, Loss function: 8.939, Average Loss: 2.551, avg. samples / sec: 58717.39
Iteration:    260, Loss function: 7.941, Average Loss: 2.541, avg. samples / sec: 58702.35
:::MLL 1558639173.954 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 819}}
:::MLL 1558639173.954 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 673}}
Iteration:    280, Loss function: 7.216, Average Loss: 2.635, avg. samples / sec: 59742.85
Iteration:    280, Loss function: 8.642, Average Loss: 2.649, avg. samples / sec: 59477.65
Iteration:    280, Loss function: 7.859, Average Loss: 2.651, avg. samples / sec: 59510.71
Iteration:    280, Loss function: 7.309, Average Loss: 2.652, avg. samples / sec: 59819.87
Iteration:    280, Loss function: 8.046, Average Loss: 2.651, avg. samples / sec: 59481.55
Iteration:    280, Loss function: 7.439, Average Loss: 2.648, avg. samples / sec: 59435.26
Iteration:    280, Loss function: 8.063, Average Loss: 2.669, avg. samples / sec: 59534.36
Iteration:    280, Loss function: 8.569, Average Loss: 2.647, avg. samples / sec: 59344.54
Iteration:    280, Loss function: 6.309, Average Loss: 2.645, avg. samples / sec: 59347.53
Iteration:    280, Loss function: 7.487, Average Loss: 2.655, avg. samples / sec: 59326.60
Iteration:    280, Loss function: 7.573, Average Loss: 2.655, avg. samples / sec: 59218.83
Iteration:    280, Loss function: 7.621, Average Loss: 2.661, avg. samples / sec: 59534.41
Iteration:    280, Loss function: 8.115, Average Loss: 2.640, avg. samples / sec: 59465.33
Iteration:    280, Loss function: 6.870, Average Loss: 2.640, avg. samples / sec: 59414.66
Iteration:    280, Loss function: 7.582, Average Loss: 2.669, avg. samples / sec: 59103.82
Iteration:    300, Loss function: 8.994, Average Loss: 2.746, avg. samples / sec: 59658.06
Iteration:    300, Loss function: 8.158, Average Loss: 2.745, avg. samples / sec: 59706.74
Iteration:    300, Loss function: 9.024, Average Loss: 2.747, avg. samples / sec: 59692.52
Iteration:    300, Loss function: 8.592, Average Loss: 2.745, avg. samples / sec: 59645.30
Iteration:    300, Loss function: 8.471, Average Loss: 2.746, avg. samples / sec: 59518.30
Iteration:    300, Loss function: 8.281, Average Loss: 2.736, avg. samples / sec: 59742.07
Iteration:    300, Loss function: 7.454, Average Loss: 2.756, avg. samples / sec: 59677.61
Iteration:    300, Loss function: 8.622, Average Loss: 2.770, avg. samples / sec: 59888.63
Iteration:    300, Loss function: 7.677, Average Loss: 2.745, avg. samples / sec: 59307.20
Iteration:    300, Loss function: 7.988, Average Loss: 2.746, avg. samples / sec: 59407.70
Iteration:    300, Loss function: 8.693, Average Loss: 2.729, avg. samples / sec: 59277.21
Iteration:    300, Loss function: 7.161, Average Loss: 2.765, avg. samples / sec: 59419.10
Iteration:    300, Loss function: 7.910, Average Loss: 2.755, avg. samples / sec: 59365.58
Iteration:    300, Loss function: 7.148, Average Loss: 2.737, avg. samples / sec: 59535.42
Iteration:    300, Loss function: 6.568, Average Loss: 2.751, avg. samples / sec: 59280.86
Iteration:    320, Loss function: 6.643, Average Loss: 2.841, avg. samples / sec: 58888.78
Iteration:    320, Loss function: 8.049, Average Loss: 2.850, avg. samples / sec: 59259.99
Iteration:    320, Loss function: 6.010, Average Loss: 2.831, avg. samples / sec: 58943.83
Iteration:    320, Loss function: 7.651, Average Loss: 2.866, avg. samples / sec: 59065.64
Iteration:    320, Loss function: 7.312, Average Loss: 2.840, avg. samples / sec: 59333.87
Iteration:    320, Loss function: 7.677, Average Loss: 2.841, avg. samples / sec: 59086.57
Iteration:    320, Loss function: 7.491, Average Loss: 2.841, avg. samples / sec: 59054.70
Iteration:    320, Loss function: 7.277, Average Loss: 2.860, avg. samples / sec: 59067.57
Iteration:    320, Loss function: 6.269, Average Loss: 2.840, avg. samples / sec: 58789.43
Iteration:    320, Loss function: 6.609, Average Loss: 2.830, avg. samples / sec: 59186.17
Iteration:    320, Loss function: 8.179, Average Loss: 2.845, avg. samples / sec: 58747.62
Iteration:    320, Loss function: 8.158, Average Loss: 2.838, avg. samples / sec: 58708.25
Iteration:    320, Loss function: 7.660, Average Loss: 2.830, avg. samples / sec: 58895.15
Iteration:    320, Loss function: 7.628, Average Loss: 2.849, avg. samples / sec: 58735.99
Iteration:    320, Loss function: 7.990, Average Loss: 2.840, avg. samples / sec: 58388.15
Iteration:    340, Loss function: 7.666, Average Loss: 2.936, avg. samples / sec: 59300.71
Iteration:    340, Loss function: 7.313, Average Loss: 2.920, avg. samples / sec: 59273.18
Iteration:    340, Loss function: 7.135, Average Loss: 2.928, avg. samples / sec: 59261.61
Iteration:    340, Loss function: 7.697, Average Loss: 2.926, avg. samples / sec: 59358.26
Iteration:    340, Loss function: 7.747, Average Loss: 2.920, avg. samples / sec: 59350.33
Iteration:    340, Loss function: 8.393, Average Loss: 2.930, avg. samples / sec: 59504.78
Iteration:    340, Loss function: 8.588, Average Loss: 2.940, avg. samples / sec: 59363.53
Iteration:    340, Loss function: 6.945, Average Loss: 2.927, avg. samples / sec: 59103.64
Iteration:    340, Loss function: 7.934, Average Loss: 2.928, avg. samples / sec: 59222.01
Iteration:    340, Loss function: 6.795, Average Loss: 2.923, avg. samples / sec: 59112.91
Iteration:    340, Loss function: 7.059, Average Loss: 2.949, avg. samples / sec: 59126.48
Iteration:    340, Loss function: 8.925, Average Loss: 2.934, avg. samples / sec: 59228.88
Iteration:    340, Loss function: 7.379, Average Loss: 2.948, avg. samples / sec: 59053.34
Iteration:    340, Loss function: 6.210, Average Loss: 2.929, avg. samples / sec: 58939.26
Iteration:    340, Loss function: 8.301, Average Loss: 2.922, avg. samples / sec: 59033.53
:::MLL 1558639175.942 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 819}}
:::MLL 1558639175.943 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 673}}
Iteration:    360, Loss function: 6.508, Average Loss: 3.009, avg. samples / sec: 57152.79
Iteration:    360, Loss function: 6.480, Average Loss: 3.027, avg. samples / sec: 57246.00
Iteration:    360, Loss function: 7.044, Average Loss: 3.006, avg. samples / sec: 57243.05
Iteration:    360, Loss function: 7.797, Average Loss: 3.011, avg. samples / sec: 57203.32
Iteration:    360, Loss function: 6.741, Average Loss: 3.007, avg. samples / sec: 57430.28
Iteration:    360, Loss function: 7.116, Average Loss: 3.039, avg. samples / sec: 57193.20
Iteration:    360, Loss function: 7.840, Average Loss: 3.017, avg. samples / sec: 57360.38
Iteration:    360, Loss function: 6.498, Average Loss: 3.013, avg. samples / sec: 57154.02
Iteration:    360, Loss function: 6.929, Average Loss: 3.035, avg. samples / sec: 57154.67
Iteration:    360, Loss function: 8.185, Average Loss: 3.014, avg. samples / sec: 57020.13
Iteration:    360, Loss function: 6.823, Average Loss: 3.019, avg. samples / sec: 57102.03
Iteration:    360, Loss function: 7.731, Average Loss: 3.011, avg. samples / sec: 56928.85
Iteration:    360, Loss function: 6.812, Average Loss: 3.012, avg. samples / sec: 57065.20
Iteration:    360, Loss function: 7.563, Average Loss: 3.017, avg. samples / sec: 57026.75
Iteration:    360, Loss function: 7.198, Average Loss: 3.020, avg. samples / sec: 56754.90
Iteration:    380, Loss function: 6.984, Average Loss: 3.094, avg. samples / sec: 57930.28
Iteration:    380, Loss function: 6.794, Average Loss: 3.110, avg. samples / sec: 57728.22
Iteration:    380, Loss function: 6.176, Average Loss: 3.086, avg. samples / sec: 57868.48
Iteration:    380, Loss function: 7.031, Average Loss: 3.089, avg. samples / sec: 57697.45
Iteration:    380, Loss function: 7.906, Average Loss: 3.083, avg. samples / sec: 57669.99
Iteration:    380, Loss function: 6.725, Average Loss: 3.115, avg. samples / sec: 57759.74
Iteration:    380, Loss function: 6.627, Average Loss: 3.090, avg. samples / sec: 57618.82
Iteration:    380, Loss function: 6.814, Average Loss: 3.094, avg. samples / sec: 57733.23
Iteration:    380, Loss function: 6.908, Average Loss: 3.094, avg. samples / sec: 57758.86
Iteration:    380, Loss function: 8.423, Average Loss: 3.120, avg. samples / sec: 57615.43
Iteration:    380, Loss function: 5.898, Average Loss: 3.100, avg. samples / sec: 57843.02
Iteration:    380, Loss function: 7.454, Average Loss: 3.095, avg. samples / sec: 57596.10
Iteration:    380, Loss function: 6.951, Average Loss: 3.087, avg. samples / sec: 57446.45
Iteration:    380, Loss function: 6.353, Average Loss: 3.093, avg. samples / sec: 57515.87
Iteration:    380, Loss function: 6.662, Average Loss: 3.089, avg. samples / sec: 57557.66
Iteration:    400, Loss function: 6.807, Average Loss: 3.166, avg. samples / sec: 55684.13
Iteration:    400, Loss function: 7.322, Average Loss: 3.172, avg. samples / sec: 55631.28
Iteration:    400, Loss function: 8.366, Average Loss: 3.186, avg. samples / sec: 55511.40
Iteration:    400, Loss function: 6.136, Average Loss: 3.165, avg. samples / sec: 55837.57
Iteration:    400, Loss function: 7.528, Average Loss: 3.189, avg. samples / sec: 55609.86
Iteration:    400, Loss function: 6.644, Average Loss: 3.161, avg. samples / sec: 55538.52
Iteration:    400, Loss function: 7.998, Average Loss: 3.165, avg. samples / sec: 55697.62
Iteration:    400, Loss function: 7.170, Average Loss: 3.177, avg. samples / sec: 55651.76
Iteration:    400, Loss function: 6.363, Average Loss: 3.158, avg. samples / sec: 55458.12
Iteration:    400, Loss function: 6.802, Average Loss: 3.161, avg. samples / sec: 55431.64
Iteration:    400, Loss function: 6.821, Average Loss: 3.200, avg. samples / sec: 55558.16
Iteration:    400, Loss function: 7.574, Average Loss: 3.171, avg. samples / sec: 55576.33
Iteration:    400, Loss function: 6.998, Average Loss: 3.171, avg. samples / sec: 55352.67
Iteration:    400, Loss function: 7.400, Average Loss: 3.168, avg. samples / sec: 55601.59
Iteration:    400, Loss function: 6.906, Average Loss: 3.173, avg. samples / sec: 55442.84
:::MLL 1558639178.014 epoch_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 819}}
:::MLL 1558639178.015 epoch_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 673}}
Iteration:    420, Loss function: 6.800, Average Loss: 3.245, avg. samples / sec: 57901.31
Iteration:    420, Loss function: 7.719, Average Loss: 3.281, avg. samples / sec: 57890.23
Iteration:    420, Loss function: 6.412, Average Loss: 3.262, avg. samples / sec: 57707.84
Iteration:    420, Loss function: 6.152, Average Loss: 3.249, avg. samples / sec: 57939.04
Iteration:    420, Loss function: 5.931, Average Loss: 3.236, avg. samples / sec: 57698.02
Iteration:    420, Loss function: 6.190, Average Loss: 3.238, avg. samples / sec: 57575.16
Iteration:    420, Loss function: 6.799, Average Loss: 3.245, avg. samples / sec: 57584.71
Iteration:    420, Loss function: 7.223, Average Loss: 3.245, avg. samples / sec: 57714.20
Iteration:    420, Loss function: 7.958, Average Loss: 3.233, avg. samples / sec: 57676.13
Iteration:    420, Loss function: 6.253, Average Loss: 3.252, avg. samples / sec: 57605.78
Iteration:    420, Loss function: 7.248, Average Loss: 3.239, avg. samples / sec: 57762.46
Iteration:    420, Loss function: 6.748, Average Loss: 3.240, avg. samples / sec: 57493.65
Iteration:    420, Loss function: 8.624, Average Loss: 3.261, avg. samples / sec: 57453.88
Iteration:    420, Loss function: 6.464, Average Loss: 3.237, avg. samples / sec: 57565.14
Iteration:    420, Loss function: 7.836, Average Loss: 3.242, avg. samples / sec: 57459.38
Iteration:    440, Loss function: 5.999, Average Loss: 3.351, avg. samples / sec: 56579.93
Iteration:    440, Loss function: 6.124, Average Loss: 3.307, avg. samples / sec: 56899.20
Iteration:    440, Loss function: 7.531, Average Loss: 3.310, avg. samples / sec: 56568.35
Iteration:    440, Loss function: 7.516, Average Loss: 3.322, avg. samples / sec: 56744.44
Iteration:    440, Loss function: 8.000, Average Loss: 3.330, avg. samples / sec: 56803.15
Iteration:    440, Loss function: 6.814, Average Loss: 3.318, avg. samples / sec: 56690.55
Iteration:    440, Loss function: 6.746, Average Loss: 3.321, avg. samples / sec: 56559.31
Iteration:    440, Loss function: 6.182, Average Loss: 3.314, avg. samples / sec: 56568.98
Iteration:    440, Loss function: 4.975, Average Loss: 3.302, avg. samples / sec: 56555.68
Iteration:    440, Loss function: 7.638, Average Loss: 3.330, avg. samples / sec: 56435.74
Iteration:    440, Loss function: 5.664, Average Loss: 3.306, avg. samples / sec: 56679.17
Iteration:    440, Loss function: 6.451, Average Loss: 3.310, avg. samples / sec: 56594.84
Iteration:    440, Loss function: 6.842, Average Loss: 3.295, avg. samples / sec: 56486.38
Iteration:    440, Loss function: 6.685, Average Loss: 3.310, avg. samples / sec: 56500.65
Iteration:    440, Loss function: 6.256, Average Loss: 3.307, avg. samples / sec: 56337.01
Iteration:    460, Loss function: 7.206, Average Loss: 3.375, avg. samples / sec: 59281.73
Iteration:    460, Loss function: 7.518, Average Loss: 3.395, avg. samples / sec: 59436.59
Iteration:    460, Loss function: 7.747, Average Loss: 3.385, avg. samples / sec: 59287.54
Iteration:    460, Loss function: 7.597, Average Loss: 3.382, avg. samples / sec: 59329.25
Iteration:    460, Loss function: 7.696, Average Loss: 3.376, avg. samples / sec: 59146.36
Iteration:    460, Loss function: 7.151, Average Loss: 3.388, avg. samples / sec: 59199.77
Iteration:    460, Loss function: 6.671, Average Loss: 3.389, avg. samples / sec: 59156.68
Iteration:    460, Loss function: 7.019, Average Loss: 3.373, avg. samples / sec: 59440.43
Iteration:    460, Loss function: 7.140, Average Loss: 3.360, avg. samples / sec: 59377.49
Iteration:    460, Loss function: 7.140, Average Loss: 3.400, avg. samples / sec: 59106.07
Iteration:    460, Loss function: 7.463, Average Loss: 3.370, avg. samples / sec: 59185.11
Iteration:    460, Loss function: 5.786, Average Loss: 3.371, avg. samples / sec: 59210.52
Iteration:    460, Loss function: 7.107, Average Loss: 3.376, avg. samples / sec: 59219.75
Iteration:    460, Loss function: 6.821, Average Loss: 3.416, avg. samples / sec: 58964.37
Iteration:    460, Loss function: 6.949, Average Loss: 3.376, avg. samples / sec: 59068.66
Iteration:    480, Loss function: 6.877, Average Loss: 3.470, avg. samples / sec: 56785.73
Iteration:    480, Loss function: 7.251, Average Loss: 3.426, avg. samples / sec: 56667.00
Iteration:    480, Loss function: 6.960, Average Loss: 3.444, avg. samples / sec: 56760.89
Iteration:    480, Loss function: 7.703, Average Loss: 3.463, avg. samples / sec: 56497.50
Iteration:    480, Loss function: 7.885, Average Loss: 3.456, avg. samples / sec: 56561.29
Iteration:    480, Loss function: 7.167, Average Loss: 3.445, avg. samples / sec: 56578.91
Iteration:    480, Loss function: 5.852, Average Loss: 3.483, avg. samples / sec: 56698.78
Iteration:    480, Loss function: 7.179, Average Loss: 3.451, avg. samples / sec: 56475.72
Iteration:    480, Loss function: 5.675, Average Loss: 3.452, avg. samples / sec: 56406.19
Iteration:    480, Loss function: 6.635, Average Loss: 3.441, avg. samples / sec: 56633.61
Iteration:    480, Loss function: 6.451, Average Loss: 3.440, avg. samples / sec: 56482.20
Iteration:    480, Loss function: 6.584, Average Loss: 3.436, avg. samples / sec: 56602.09
Iteration:    480, Loss function: 5.843, Average Loss: 3.453, avg. samples / sec: 56443.63
Iteration:    480, Loss function: 6.117, Average Loss: 3.446, avg. samples / sec: 56306.38
Iteration:    480, Loss function: 5.930, Average Loss: 3.444, avg. samples / sec: 56609.00
:::MLL 1558639180.058 epoch_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 819}}
:::MLL 1558639180.059 epoch_start: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 673}}
Iteration:    500, Loss function: 5.622, Average Loss: 3.541, avg. samples / sec: 58712.01
Iteration:    500, Loss function: 6.913, Average Loss: 3.509, avg. samples / sec: 58673.17
Iteration:    500, Loss function: 5.616, Average Loss: 3.481, avg. samples / sec: 58540.73
Iteration:    500, Loss function: 6.104, Average Loss: 3.505, avg. samples / sec: 58597.10
Iteration:    500, Loss function: 6.788, Average Loss: 3.510, avg. samples / sec: 58672.78
Iteration:    500, Loss function: 6.455, Average Loss: 3.505, avg. samples / sec: 58449.32
Iteration:    500, Loss function: 6.425, Average Loss: 3.496, avg. samples / sec: 58556.71
Iteration:    500, Loss function: 6.133, Average Loss: 3.508, avg. samples / sec: 58600.10
Iteration:    500, Loss function: 6.510, Average Loss: 3.517, avg. samples / sec: 58434.63
Iteration:    500, Loss function: 6.595, Average Loss: 3.505, avg. samples / sec: 58495.12
Iteration:    500, Loss function: 5.204, Average Loss: 3.526, avg. samples / sec: 58232.08
Iteration:    500, Loss function: 6.812, Average Loss: 3.491, avg. samples / sec: 58480.66
Iteration:    500, Loss function: 6.376, Average Loss: 3.512, avg. samples / sec: 58384.86
Iteration:    500, Loss function: 7.087, Average Loss: 3.502, avg. samples / sec: 58553.82
Iteration:    500, Loss function: 6.344, Average Loss: 3.517, avg. samples / sec: 57835.59
Iteration:    520, Loss function: 6.562, Average Loss: 3.558, avg. samples / sec: 59715.97
Iteration:    520, Loss function: 6.389, Average Loss: 3.584, avg. samples / sec: 59530.47
Iteration:    520, Loss function: 6.551, Average Loss: 3.566, avg. samples / sec: 59309.77
Iteration:    520, Loss function: 6.472, Average Loss: 3.548, avg. samples / sec: 59453.92
Iteration:    520, Loss function: 5.570, Average Loss: 3.591, avg. samples / sec: 59070.57
Iteration:    520, Loss function: 6.131, Average Loss: 3.576, avg. samples / sec: 59309.50
Iteration:    520, Loss function: 7.524, Average Loss: 3.573, avg. samples / sec: 59408.55
Iteration:    520, Loss function: 6.563, Average Loss: 3.547, avg. samples / sec: 59233.37
Iteration:    520, Loss function: 6.598, Average Loss: 3.561, avg. samples / sec: 59123.21
Iteration:    520, Loss function: 6.961, Average Loss: 3.564, avg. samples / sec: 59200.32
Iteration:    520, Loss function: 6.502, Average Loss: 3.578, avg. samples / sec: 59840.16
Iteration:    520, Loss function: 6.433, Average Loss: 3.567, avg. samples / sec: 59008.76
Iteration:    520, Loss function: 6.993, Average Loss: 3.562, avg. samples / sec: 59111.80
Iteration:    520, Loss function: 6.623, Average Loss: 3.564, avg. samples / sec: 59151.74
Iteration:    520, Loss function: 6.537, Average Loss: 3.538, avg. samples / sec: 58908.91
Iteration:    540, Loss function: 6.725, Average Loss: 3.622, avg. samples / sec: 54972.89
Iteration:    540, Loss function: 6.522, Average Loss: 3.612, avg. samples / sec: 54682.65
Iteration:    540, Loss function: 6.405, Average Loss: 3.599, avg. samples / sec: 54872.31
Iteration:    540, Loss function: 6.333, Average Loss: 3.600, avg. samples / sec: 54752.95
Iteration:    540, Loss function: 6.502, Average Loss: 3.621, avg. samples / sec: 54972.06
Iteration:    540, Loss function: 5.736, Average Loss: 3.617, avg. samples / sec: 54844.96
Iteration:    540, Loss function: 6.311, Average Loss: 3.620, avg. samples / sec: 54701.56
Iteration:    540, Loss function: 7.503, Average Loss: 3.630, avg. samples / sec: 54803.58
Iteration:    540, Loss function: 6.720, Average Loss: 3.622, avg. samples / sec: 54901.08
Iteration:    540, Loss function: 6.728, Average Loss: 3.598, avg. samples / sec: 54936.61
Iteration:    540, Loss function: 5.590, Average Loss: 3.627, avg. samples / sec: 54732.43
Iteration:    540, Loss function: 6.314, Average Loss: 3.643, avg. samples / sec: 54655.84
Iteration:    540, Loss function: 6.536, Average Loss: 3.640, avg. samples / sec: 54509.03
Iteration:    540, Loss function: 7.400, Average Loss: 3.640, avg. samples / sec: 54669.54
Iteration:    540, Loss function: 5.907, Average Loss: 3.618, avg. samples / sec: 54601.91
:::MLL 1558639182.107 epoch_stop: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 819}}
:::MLL 1558639182.107 epoch_start: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 673}}
Iteration:    560, Loss function: 5.867, Average Loss: 3.684, avg. samples / sec: 57905.60
Iteration:    560, Loss function: 6.838, Average Loss: 3.674, avg. samples / sec: 58139.95
Iteration:    560, Loss function: 6.212, Average Loss: 3.671, avg. samples / sec: 57911.43
Iteration:    560, Loss function: 5.637, Average Loss: 3.678, avg. samples / sec: 57891.18
Iteration:    560, Loss function: 6.643, Average Loss: 3.675, avg. samples / sec: 57779.44
Iteration:    560, Loss function: 5.919, Average Loss: 3.663, avg. samples / sec: 57711.25
Iteration:    560, Loss function: 7.152, Average Loss: 3.694, avg. samples / sec: 57847.79
Iteration:    560, Loss function: 6.489, Average Loss: 3.692, avg. samples / sec: 57834.78
Iteration:    560, Loss function: 5.981, Average Loss: 3.673, avg. samples / sec: 57614.23
Iteration:    560, Loss function: 7.359, Average Loss: 3.651, avg. samples / sec: 57576.12
Iteration:    560, Loss function: 5.654, Average Loss: 3.692, avg. samples / sec: 57812.95
Iteration:    560, Loss function: 6.033, Average Loss: 3.670, avg. samples / sec: 57493.14
Iteration:    560, Loss function: 6.556, Average Loss: 3.652, avg. samples / sec: 57592.24
Iteration:    560, Loss function: 5.047, Average Loss: 3.664, avg. samples / sec: 57461.96
Iteration:    560, Loss function: 6.307, Average Loss: 3.649, avg. samples / sec: 57453.67
Iteration:    580, Loss function: 6.010, Average Loss: 3.721, avg. samples / sec: 56194.37
Iteration:    580, Loss function: 5.813, Average Loss: 3.718, avg. samples / sec: 56148.18
Iteration:    580, Loss function: 6.702, Average Loss: 3.712, avg. samples / sec: 56213.44
Iteration:    580, Loss function: 6.489, Average Loss: 3.705, avg. samples / sec: 56407.23
Iteration:    580, Loss function: 6.440, Average Loss: 3.719, avg. samples / sec: 56353.46
Iteration:    580, Loss function: 6.143, Average Loss: 3.703, avg. samples / sec: 56409.26
Iteration:    580, Loss function: 5.556, Average Loss: 3.723, avg. samples / sec: 56258.41
Iteration:    580, Loss function: 5.938, Average Loss: 3.740, avg. samples / sec: 56195.49
Iteration:    580, Loss function: 5.077, Average Loss: 3.748, avg. samples / sec: 56190.06
Iteration:    580, Loss function: 6.676, Average Loss: 3.737, avg. samples / sec: 56261.80
Iteration:    580, Loss function: 6.320, Average Loss: 3.732, avg. samples / sec: 55967.54
Iteration:    580, Loss function: 5.399, Average Loss: 3.712, avg. samples / sec: 56312.27
Iteration:    580, Loss function: 5.565, Average Loss: 3.725, avg. samples / sec: 56038.40
Iteration:    580, Loss function: 6.392, Average Loss: 3.702, avg. samples / sec: 56189.93
Iteration:    580, Loss function: 4.930, Average Loss: 3.728, avg. samples / sec: 55984.13
Iteration:    600, Loss function: 5.250, Average Loss: 3.787, avg. samples / sec: 59574.38
Iteration:    600, Loss function: 6.439, Average Loss: 3.756, avg. samples / sec: 59467.84
Iteration:    600, Loss function: 6.330, Average Loss: 3.771, avg. samples / sec: 59662.30
Iteration:    600, Loss function: 4.942, Average Loss: 3.748, avg. samples / sec: 59440.73
Iteration:    600, Loss function: 6.737, Average Loss: 3.752, avg. samples / sec: 59540.58
Iteration:    600, Loss function: 7.472, Average Loss: 3.767, avg. samples / sec: 59309.82
Iteration:    600, Loss function: 5.406, Average Loss: 3.773, avg. samples / sec: 59515.46
Iteration:    600, Loss function: 5.688, Average Loss: 3.756, avg. samples / sec: 59504.45
Iteration:    600, Loss function: 6.489, Average Loss: 3.768, avg. samples / sec: 59281.00
Iteration:    600, Loss function: 5.898, Average Loss: 3.770, avg. samples / sec: 59356.86
Iteration:    600, Loss function: 6.456, Average Loss: 3.789, avg. samples / sec: 59346.56
Iteration:    600, Loss function: 6.869, Average Loss: 3.756, avg. samples / sec: 59268.56
Iteration:    600, Loss function: 6.171, Average Loss: 3.775, avg. samples / sec: 59342.14
Iteration:    600, Loss function: 4.803, Average Loss: 3.794, avg. samples / sec: 59297.92
Iteration:    600, Loss function: 5.696, Average Loss: 3.766, avg. samples / sec: 59233.51
Iteration:    620, Loss function: 5.781, Average Loss: 3.799, avg. samples / sec: 60046.07
Iteration:    620, Loss function: 7.086, Average Loss: 3.818, avg. samples / sec: 59894.11
Iteration:    620, Loss function: 7.367, Average Loss: 3.832, avg. samples / sec: 59998.93
Iteration:    620, Loss function: 4.741, Average Loss: 3.816, avg. samples / sec: 59790.00
Iteration:    620, Loss function: 5.245, Average Loss: 3.806, avg. samples / sec: 59852.44
Iteration:    620, Loss function: 6.225, Average Loss: 3.833, avg. samples / sec: 59725.49
Iteration:    620, Loss function: 5.731, Average Loss: 3.790, avg. samples / sec: 59796.75
Iteration:    620, Loss function: 5.239, Average Loss: 3.820, avg. samples / sec: 59962.40
Iteration:    620, Loss function: 5.378, Average Loss: 3.815, avg. samples / sec: 59738.63
Iteration:    620, Loss function: 6.406, Average Loss: 3.841, avg. samples / sec: 59898.03
Iteration:    620, Loss function: 5.066, Average Loss: 3.801, avg. samples / sec: 59654.32
Iteration:    620, Loss function: 6.346, Average Loss: 3.802, avg. samples / sec: 59718.73
Iteration:    620, Loss function: 6.301, Average Loss: 3.797, avg. samples / sec: 59703.75
Iteration:    620, Loss function: 6.834, Average Loss: 3.812, avg. samples / sec: 59882.55
Iteration:    620, Loss function: 6.321, Average Loss: 3.815, avg. samples / sec: 59731.31
:::MLL 1558639184.115 epoch_stop: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 819}}
:::MLL 1558639184.116 epoch_start: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 673}}
Iteration:    640, Loss function: 5.795, Average Loss: 3.870, avg. samples / sec: 58213.89
Iteration:    640, Loss function: 7.197, Average Loss: 3.848, avg. samples / sec: 58207.78
Iteration:    640, Loss function: 5.852, Average Loss: 3.842, avg. samples / sec: 58332.85
Iteration:    640, Loss function: 5.608, Average Loss: 3.882, avg. samples / sec: 58302.59
Iteration:    640, Loss function: 6.335, Average Loss: 3.831, avg. samples / sec: 58228.44
Iteration:    640, Loss function: 7.118, Average Loss: 3.856, avg. samples / sec: 58356.77
Iteration:    640, Loss function: 5.496, Average Loss: 3.843, avg. samples / sec: 58268.00
Iteration:    640, Loss function: 5.426, Average Loss: 3.858, avg. samples / sec: 58106.00
Iteration:    640, Loss function: 5.091, Average Loss: 3.844, avg. samples / sec: 58085.98
Iteration:    640, Loss function: 5.772, Average Loss: 3.877, avg. samples / sec: 58116.69
Iteration:    640, Loss function: 6.411, Average Loss: 3.863, avg. samples / sec: 58123.35
Iteration:    640, Loss function: 6.257, Average Loss: 3.844, avg. samples / sec: 58183.97
Iteration:    640, Loss function: 6.228, Average Loss: 3.862, avg. samples / sec: 58160.46
Iteration:    640, Loss function: 5.669, Average Loss: 3.858, avg. samples / sec: 58051.53
Iteration:    640, Loss function: 6.463, Average Loss: 3.855, avg. samples / sec: 58113.57
Iteration:    660, Loss function: 5.948, Average Loss: 3.901, avg. samples / sec: 58997.35
Iteration:    660, Loss function: 5.172, Average Loss: 3.888, avg. samples / sec: 58876.16
Iteration:    660, Loss function: 5.120, Average Loss: 3.893, avg. samples / sec: 58957.91
Iteration:    660, Loss function: 6.361, Average Loss: 3.876, avg. samples / sec: 58704.38
Iteration:    660, Loss function: 5.541, Average Loss: 3.890, avg. samples / sec: 58716.12
Iteration:    660, Loss function: 5.519, Average Loss: 3.921, avg. samples / sec: 58621.36
Iteration:    660, Loss function: 5.230, Average Loss: 3.891, avg. samples / sec: 58630.43
Iteration:    660, Loss function: 5.877, Average Loss: 3.905, avg. samples / sec: 58746.64
Iteration:    660, Loss function: 4.645, Average Loss: 3.905, avg. samples / sec: 58503.38
Iteration:    660, Loss function: 5.456, Average Loss: 3.899, avg. samples / sec: 58588.85
Iteration:    660, Loss function: 7.102, Average Loss: 3.890, avg. samples / sec: 58504.42
Iteration:    660, Loss function: 5.958, Average Loss: 3.881, avg. samples / sec: 58552.09
Iteration:    660, Loss function: 4.998, Average Loss: 3.879, avg. samples / sec: 58490.63
Iteration:    660, Loss function: 6.339, Average Loss: 3.906, avg. samples / sec: 58552.65
Iteration:    660, Loss function: 5.350, Average Loss: 3.915, avg. samples / sec: 58350.70
Iteration:    680, Loss function: 5.408, Average Loss: 3.942, avg. samples / sec: 59512.24
Iteration:    680, Loss function: 5.231, Average Loss: 3.919, avg. samples / sec: 59380.39
Iteration:    680, Loss function: 5.081, Average Loss: 3.929, avg. samples / sec: 59271.63
Iteration:    680, Loss function: 5.657, Average Loss: 3.921, avg. samples / sec: 59068.59
Iteration:    680, Loss function: 5.162, Average Loss: 3.945, avg. samples / sec: 59479.89
Iteration:    680, Loss function: 5.274, Average Loss: 3.913, avg. samples / sec: 58993.02
Iteration:    680, Loss function: 5.760, Average Loss: 3.934, avg. samples / sec: 58967.70
Iteration:    680, Loss function: 7.787, Average Loss: 3.929, avg. samples / sec: 59161.48
Iteration:    680, Loss function: 4.314, Average Loss: 3.944, avg. samples / sec: 59103.86
Iteration:    680, Loss function: 6.476, Average Loss: 3.923, avg. samples / sec: 58922.80
Iteration:    680, Loss function: 4.812, Average Loss: 3.918, avg. samples / sec: 59040.40
Iteration:    680, Loss function: 6.498, Average Loss: 3.940, avg. samples / sec: 58642.19
Iteration:    680, Loss function: 5.593, Average Loss: 3.932, avg. samples / sec: 58941.41
Iteration:    680, Loss function: 6.971, Average Loss: 3.958, avg. samples / sec: 58841.35
Iteration:    680, Loss function: 5.084, Average Loss: 3.940, avg. samples / sec: 58644.26
:::MLL 1558639186.120 epoch_stop: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 819}}
:::MLL 1558639186.120 epoch_start: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 673}}
Iteration:    700, Loss function: 5.481, Average Loss: 3.947, avg. samples / sec: 59459.66
Iteration:    700, Loss function: 5.357, Average Loss: 3.990, avg. samples / sec: 59323.93
Iteration:    700, Loss function: 5.470, Average Loss: 3.967, avg. samples / sec: 59268.14
Iteration:    700, Loss function: 6.014, Average Loss: 3.957, avg. samples / sec: 59150.75
Iteration:    700, Loss function: 5.812, Average Loss: 3.980, avg. samples / sec: 59086.59
Iteration:    700, Loss function: 5.923, Average Loss: 3.961, avg. samples / sec: 58919.23
Iteration:    700, Loss function: 6.331, Average Loss: 3.960, avg. samples / sec: 59112.86
Iteration:    700, Loss function: 6.417, Average Loss: 3.955, avg. samples / sec: 59049.21
Iteration:    700, Loss function: 6.008, Average Loss: 3.991, avg. samples / sec: 59270.46
Iteration:    700, Loss function: 6.490, Average Loss: 3.971, avg. samples / sec: 59244.84
Iteration:    700, Loss function: 6.509, Average Loss: 3.976, avg. samples / sec: 59230.58
Iteration:    700, Loss function: 5.388, Average Loss: 3.979, avg. samples / sec: 58756.88
Iteration:    700, Loss function: 6.205, Average Loss: 3.975, avg. samples / sec: 59449.13
Iteration:    700, Loss function: 5.222, Average Loss: 3.953, avg. samples / sec: 58711.99
Iteration:    700, Loss function: 5.400, Average Loss: 3.963, avg. samples / sec: 58694.04
Iteration:    720, Loss function: 6.192, Average Loss: 4.020, avg. samples / sec: 59613.51
Iteration:    720, Loss function: 5.869, Average Loss: 4.001, avg. samples / sec: 59657.27
Iteration:    720, Loss function: 4.852, Average Loss: 4.013, avg. samples / sec: 59665.78
Iteration:    720, Loss function: 5.818, Average Loss: 3.989, avg. samples / sec: 59589.87
Iteration:    720, Loss function: 4.684, Average Loss: 3.978, avg. samples / sec: 59338.81
Iteration:    720, Loss function: 5.002, Average Loss: 3.991, avg. samples / sec: 59477.00
Iteration:    720, Loss function: 5.705, Average Loss: 3.988, avg. samples / sec: 59616.59
Iteration:    720, Loss function: 5.790, Average Loss: 3.998, avg. samples / sec: 59255.13
Iteration:    720, Loss function: 5.752, Average Loss: 4.016, avg. samples / sec: 59470.40
Iteration:    720, Loss function: 6.013, Average Loss: 3.992, avg. samples / sec: 59332.29
Iteration:    720, Loss function: 5.772, Average Loss: 4.015, avg. samples / sec: 59176.90
Iteration:    720, Loss function: 6.390, Average Loss: 4.008, avg. samples / sec: 59467.29
Iteration:    720, Loss function: 5.311, Average Loss: 3.994, avg. samples / sec: 59761.58
Iteration:    720, Loss function: 6.391, Average Loss: 4.009, avg. samples / sec: 59359.58
Iteration:    720, Loss function: 6.110, Average Loss: 3.994, avg. samples / sec: 59142.19
Iteration:    740, Loss function: 5.267, Average Loss: 4.011, avg. samples / sec: 58897.91
Iteration:    740, Loss function: 4.839, Average Loss: 4.025, avg. samples / sec: 58830.86
Iteration:    740, Loss function: 4.825, Average Loss: 4.046, avg. samples / sec: 59022.28
Iteration:    740, Loss function: 5.380, Average Loss: 4.046, avg. samples / sec: 58797.33
Iteration:    740, Loss function: 5.502, Average Loss: 4.020, avg. samples / sec: 58922.24
Iteration:    740, Loss function: 5.295, Average Loss: 4.048, avg. samples / sec: 58973.79
Iteration:    740, Loss function: 6.648, Average Loss: 4.054, avg. samples / sec: 58706.39
Iteration:    740, Loss function: 6.261, Average Loss: 4.024, avg. samples / sec: 58899.16
Iteration:    740, Loss function: 5.846, Average Loss: 4.053, avg. samples / sec: 58868.63
Iteration:    740, Loss function: 4.690, Average Loss: 4.040, avg. samples / sec: 58993.15
Iteration:    740, Loss function: 6.041, Average Loss: 4.026, avg. samples / sec: 58795.78
Iteration:    740, Loss function: 5.941, Average Loss: 4.037, avg. samples / sec: 58670.04
Iteration:    740, Loss function: 5.466, Average Loss: 4.037, avg. samples / sec: 58823.12
Iteration:    740, Loss function: 4.863, Average Loss: 4.025, avg. samples / sec: 59078.94
Iteration:    740, Loss function: 6.677, Average Loss: 4.031, avg. samples / sec: 58836.41
Iteration:    760, Loss function: 4.679, Average Loss: 4.079, avg. samples / sec: 57687.46
Iteration:    760, Loss function: 5.712, Average Loss: 4.047, avg. samples / sec: 57591.98
Iteration:    760, Loss function: 5.592, Average Loss: 4.081, avg. samples / sec: 57679.50
Iteration:    760, Loss function: 5.599, Average Loss: 4.075, avg. samples / sec: 57581.67
Iteration:    760, Loss function: 3.478, Average Loss: 4.060, avg. samples / sec: 57742.44
Iteration:    760, Loss function: 6.409, Average Loss: 4.059, avg. samples / sec: 57550.17
Iteration:    760, Loss function: 6.192, Average Loss: 4.072, avg. samples / sec: 57626.55
Iteration:    760, Loss function: 5.361, Average Loss: 4.083, avg. samples / sec: 57619.44
Iteration:    760, Loss function: 5.177, Average Loss: 4.064, avg. samples / sec: 57727.35
Iteration:    760, Loss function: 6.092, Average Loss: 4.056, avg. samples / sec: 57589.13
Iteration:    760, Loss function: 5.345, Average Loss: 4.060, avg. samples / sec: 57582.36
Iteration:    760, Loss function: 5.851, Average Loss: 4.082, avg. samples / sec: 57535.55
Iteration:    760, Loss function: 4.542, Average Loss: 4.068, avg. samples / sec: 57593.30
Iteration:    760, Loss function: 4.375, Average Loss: 4.066, avg. samples / sec: 57605.14
Iteration:    760, Loss function: 6.039, Average Loss: 4.051, avg. samples / sec: 57431.03
:::MLL 1558639188.118 epoch_stop: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 819}}
:::MLL 1558639188.118 epoch_start: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 673}}
Iteration:    780, Loss function: 4.913, Average Loss: 4.097, avg. samples / sec: 60040.04
Iteration:    780, Loss function: 5.286, Average Loss: 4.106, avg. samples / sec: 59817.97
Iteration:    780, Loss function: 4.520, Average Loss: 4.091, avg. samples / sec: 60028.40
Iteration:    780, Loss function: 5.667, Average Loss: 4.088, avg. samples / sec: 59890.13
Iteration:    780, Loss function: 6.969, Average Loss: 4.099, avg. samples / sec: 59976.26
Iteration:    780, Loss function: 5.115, Average Loss: 4.107, avg. samples / sec: 59914.25
Iteration:    780, Loss function: 5.659, Average Loss: 4.090, avg. samples / sec: 59853.15
Iteration:    780, Loss function: 4.711, Average Loss: 4.085, avg. samples / sec: 59896.57
Iteration:    780, Loss function: 5.304, Average Loss: 4.103, avg. samples / sec: 59798.55
Iteration:    780, Loss function: 5.794, Average Loss: 4.076, avg. samples / sec: 59720.78
Iteration:    780, Loss function: 4.823, Average Loss: 4.079, avg. samples / sec: 59933.48
Iteration:    780, Loss function: 5.331, Average Loss: 4.110, avg. samples / sec: 59765.96
Iteration:    780, Loss function: 6.162, Average Loss: 4.113, avg. samples / sec: 59555.40
Iteration:    780, Loss function: 5.161, Average Loss: 4.084, avg. samples / sec: 59712.71
Iteration:    780, Loss function: 5.297, Average Loss: 4.095, avg. samples / sec: 59563.95
Iteration:    800, Loss function: 4.912, Average Loss: 4.110, avg. samples / sec: 56839.62
Iteration:    800, Loss function: 4.307, Average Loss: 4.117, avg. samples / sec: 57056.26
Iteration:    800, Loss function: 6.469, Average Loss: 4.117, avg. samples / sec: 56792.60
Iteration:    800, Loss function: 5.555, Average Loss: 4.118, avg. samples / sec: 56783.92
Iteration:    800, Loss function: 4.583, Average Loss: 4.119, avg. samples / sec: 56754.40
Iteration:    800, Loss function: 4.802, Average Loss: 4.104, avg. samples / sec: 56811.71
Iteration:    800, Loss function: 5.553, Average Loss: 4.115, avg. samples / sec: 56777.18
Iteration:    800, Loss function: 5.939, Average Loss: 4.137, avg. samples / sec: 56907.15
Iteration:    800, Loss function: 5.030, Average Loss: 4.126, avg. samples / sec: 56610.80
Iteration:    800, Loss function: 5.877, Average Loss: 4.127, avg. samples / sec: 56760.21
Iteration:    800, Loss function: 7.147, Average Loss: 4.142, avg. samples / sec: 56890.38
Iteration:    800, Loss function: 4.779, Average Loss: 4.108, avg. samples / sec: 56777.02
Iteration:    800, Loss function: 5.093, Average Loss: 4.135, avg. samples / sec: 56672.81
Iteration:    800, Loss function: 7.256, Average Loss: 4.138, avg. samples / sec: 56606.59
Iteration:    800, Loss function: 5.661, Average Loss: 4.121, avg. samples / sec: 56944.83
Iteration:    820, Loss function: 5.103, Average Loss: 4.149, avg. samples / sec: 60553.93
Iteration:    820, Loss function: 3.411, Average Loss: 4.136, avg. samples / sec: 60392.01
Iteration:    820, Loss function: 5.148, Average Loss: 4.142, avg. samples / sec: 60279.00
Iteration:    820, Loss function: 5.212, Average Loss: 4.144, avg. samples / sec: 60326.45
Iteration:    820, Loss function: 5.996, Average Loss: 4.131, avg. samples / sec: 60291.73
Iteration:    820, Loss function: 5.745, Average Loss: 4.159, avg. samples / sec: 60365.03
Iteration:    820, Loss function: 4.737, Average Loss: 4.130, avg. samples / sec: 60291.32
Iteration:    820, Loss function: 5.222, Average Loss: 4.167, avg. samples / sec: 60273.50
Iteration:    820, Loss function: 5.659, Average Loss: 4.155, avg. samples / sec: 60188.45
Iteration:    820, Loss function: 5.239, Average Loss: 4.144, avg. samples / sec: 60117.38
Iteration:    820, Loss function: 5.242, Average Loss: 4.164, avg. samples / sec: 60238.34
Iteration:    820, Loss function: 5.732, Average Loss: 4.158, avg. samples / sec: 60144.89
Iteration:    820, Loss function: 5.776, Average Loss: 4.142, avg. samples / sec: 59973.30
Iteration:    820, Loss function: 5.623, Average Loss: 4.137, avg. samples / sec: 59924.87
Iteration:    820, Loss function: 5.413, Average Loss: 4.151, avg. samples / sec: 59976.23
:::MLL 1558639190.123 epoch_stop: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 819}}
:::MLL 1558639190.123 epoch_start: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 673}}
Iteration:    840, Loss function: 5.576, Average Loss: 4.174, avg. samples / sec: 59126.80
Iteration:    840, Loss function: 5.054, Average Loss: 4.163, avg. samples / sec: 59015.98
Iteration:    840, Loss function: 4.668, Average Loss: 4.185, avg. samples / sec: 58850.59
Iteration:    840, Loss function: 5.707, Average Loss: 4.174, avg. samples / sec: 58790.61
Iteration:    840, Loss function: 5.105, Average Loss: 4.148, avg. samples / sec: 58754.70
Iteration:    840, Loss function: 5.757, Average Loss: 4.176, avg. samples / sec: 58773.20
Iteration:    840, Loss function: 5.092, Average Loss: 4.183, avg. samples / sec: 58662.38
Iteration:    840, Loss function: 4.889, Average Loss: 4.161, avg. samples / sec: 58809.92
Iteration:    840, Loss function: 5.323, Average Loss: 4.169, avg. samples / sec: 58502.60
Iteration:    840, Loss function: 4.745, Average Loss: 4.173, avg. samples / sec: 58490.58
Iteration:    840, Loss function: 5.189, Average Loss: 4.165, avg. samples / sec: 58512.39
Iteration:    840, Loss function: 5.510, Average Loss: 4.165, avg. samples / sec: 58470.22
Iteration:    840, Loss function: 5.481, Average Loss: 4.156, avg. samples / sec: 58508.41
Iteration:    840, Loss function: 4.322, Average Loss: 4.188, avg. samples / sec: 58656.98
Iteration:    840, Loss function: 5.712, Average Loss: 4.194, avg. samples / sec: 58600.25
Iteration:    860, Loss function: 6.365, Average Loss: 4.207, avg. samples / sec: 58484.03
Iteration:    860, Loss function: 6.352, Average Loss: 4.199, avg. samples / sec: 58439.43
Iteration:    860, Loss function: 6.143, Average Loss: 4.196, avg. samples / sec: 58217.23
Iteration:    860, Loss function: 6.554, Average Loss: 4.213, avg. samples / sec: 58306.79
Iteration:    860, Loss function: 5.083, Average Loss: 4.194, avg. samples / sec: 58419.95
Iteration:    860, Loss function: 4.456, Average Loss: 4.164, avg. samples / sec: 58324.21
Iteration:    860, Loss function: 6.513, Average Loss: 4.178, avg. samples / sec: 58457.76
Iteration:    860, Loss function: 6.712, Average Loss: 4.222, avg. samples / sec: 58418.86
Iteration:    860, Loss function: 7.099, Average Loss: 4.200, avg. samples / sec: 58332.08
Iteration:    860, Loss function: 6.612, Average Loss: 4.208, avg. samples / sec: 58368.75
Iteration:    860, Loss function: 5.882, Average Loss: 4.182, avg. samples / sec: 58037.30
Iteration:    860, Loss function: 5.509, Average Loss: 4.189, avg. samples / sec: 58286.43
Iteration:    860, Loss function: 5.633, Average Loss: 4.179, avg. samples / sec: 58198.48
Iteration:    860, Loss function: 5.436, Average Loss: 4.206, avg. samples / sec: 58138.94
Iteration:    860, Loss function: 5.534, Average Loss: 4.186, avg. samples / sec: 58180.05
Iteration:    880, Loss function: 5.326, Average Loss: 4.211, avg. samples / sec: 57343.21
Iteration:    880, Loss function: 4.889, Average Loss: 4.202, avg. samples / sec: 57072.62
Iteration:    880, Loss function: 4.831, Average Loss: 4.218, avg. samples / sec: 57038.04
Iteration:    880, Loss function: 5.288, Average Loss: 4.237, avg. samples / sec: 57051.98
Iteration:    880, Loss function: 4.329, Average Loss: 4.185, avg. samples / sec: 57028.81
Iteration:    880, Loss function: 5.521, Average Loss: 4.244, avg. samples / sec: 57068.73
Iteration:    880, Loss function: 5.470, Average Loss: 4.208, avg. samples / sec: 57165.73
Iteration:    880, Loss function: 5.945, Average Loss: 4.206, avg. samples / sec: 57128.56
Iteration:    880, Loss function: 4.854, Average Loss: 4.219, avg. samples / sec: 56957.86
Iteration:    880, Loss function: 4.614, Average Loss: 4.221, avg. samples / sec: 57017.52
Iteration:    880, Loss function: 6.182, Average Loss: 4.225, avg. samples / sec: 56848.52
Iteration:    880, Loss function: 5.452, Average Loss: 4.228, avg. samples / sec: 56794.27
Iteration:    880, Loss function: 4.203, Average Loss: 4.218, avg. samples / sec: 57024.14
Iteration:    880, Loss function: 5.741, Average Loss: 4.235, avg. samples / sec: 57070.38
Iteration:    880, Loss function: 5.709, Average Loss: 4.233, avg. samples / sec: 56890.47
Iteration:    900, Loss function: 6.015, Average Loss: 4.238, avg. samples / sec: 60103.36
Iteration:    900, Loss function: 4.698, Average Loss: 4.243, avg. samples / sec: 59971.56
Iteration:    900, Loss function: 4.741, Average Loss: 4.256, avg. samples / sec: 60060.63
Iteration:    900, Loss function: 3.904, Average Loss: 4.242, avg. samples / sec: 59931.65
Iteration:    900, Loss function: 4.404, Average Loss: 4.226, avg. samples / sec: 59679.81
Iteration:    900, Loss function: 4.752, Average Loss: 4.225, avg. samples / sec: 59786.27
Iteration:    900, Loss function: 5.329, Average Loss: 4.203, avg. samples / sec: 59737.23
Iteration:    900, Loss function: 4.445, Average Loss: 4.248, avg. samples / sec: 59829.37
Iteration:    900, Loss function: 6.137, Average Loss: 4.260, avg. samples / sec: 59725.97
Iteration:    900, Loss function: 4.000, Average Loss: 4.224, avg. samples / sec: 59722.22
Iteration:    900, Loss function: 3.223, Average Loss: 4.234, avg. samples / sec: 59581.36
Iteration:    900, Loss function: 4.729, Average Loss: 4.254, avg. samples / sec: 59876.67
Iteration:    900, Loss function: 4.372, Average Loss: 4.235, avg. samples / sec: 59677.94
Iteration:    900, Loss function: 6.175, Average Loss: 4.262, avg. samples / sec: 59545.43
Iteration:    900, Loss function: 4.758, Average Loss: 4.222, avg. samples / sec: 59481.97
:::MLL 1558639192.139 epoch_stop: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 819}}
:::MLL 1558639192.140 epoch_start: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 673}}
Iteration:    920, Loss function: 5.668, Average Loss: 4.274, avg. samples / sec: 58769.50
Iteration:    920, Loss function: 4.694, Average Loss: 4.222, avg. samples / sec: 58775.06
Iteration:    920, Loss function: 5.200, Average Loss: 4.264, avg. samples / sec: 58573.68
Iteration:    920, Loss function: 4.937, Average Loss: 4.248, avg. samples / sec: 58684.09
Iteration:    920, Loss function: 6.927, Average Loss: 4.247, avg. samples / sec: 58911.25
Iteration:    920, Loss function: 5.940, Average Loss: 4.259, avg. samples / sec: 58474.57
Iteration:    920, Loss function: 5.110, Average Loss: 4.264, avg. samples / sec: 58685.04
Iteration:    920, Loss function: 4.976, Average Loss: 4.265, avg. samples / sec: 58717.49
Iteration:    920, Loss function: 4.915, Average Loss: 4.279, avg. samples / sec: 58627.55
Iteration:    920, Loss function: 5.558, Average Loss: 4.253, avg. samples / sec: 58730.68
Iteration:    920, Loss function: 5.791, Average Loss: 4.259, avg. samples / sec: 58497.36
Iteration:    920, Loss function: 5.051, Average Loss: 4.241, avg. samples / sec: 58549.63
Iteration:    920, Loss function: 5.032, Average Loss: 4.282, avg. samples / sec: 58689.40
Iteration:    920, Loss function: 5.968, Average Loss: 4.243, avg. samples / sec: 58587.12
Iteration:    920, Loss function: 5.502, Average Loss: 4.257, avg. samples / sec: 58638.19
Iteration:    940, Loss function: 4.620, Average Loss: 4.269, avg. samples / sec: 58209.13
Iteration:    940, Loss function: 5.303, Average Loss: 4.285, avg. samples / sec: 58182.74
Iteration:    940, Loss function: 5.701, Average Loss: 4.280, avg. samples / sec: 58144.98
Iteration:    940, Loss function: 4.277, Average Loss: 4.263, avg. samples / sec: 58282.58
Iteration:    940, Loss function: 6.487, Average Loss: 4.286, avg. samples / sec: 58073.03
Iteration:    940, Loss function: 4.478, Average Loss: 4.292, avg. samples / sec: 57885.90
Iteration:    940, Loss function: 5.944, Average Loss: 4.303, avg. samples / sec: 58191.03
Iteration:    940, Loss function: 4.810, Average Loss: 4.302, avg. samples / sec: 58212.02
Iteration:    940, Loss function: 4.847, Average Loss: 4.280, avg. samples / sec: 58144.48
Iteration:    940, Loss function: 5.440, Average Loss: 4.284, avg. samples / sec: 58062.86
Iteration:    940, Loss function: 5.345, Average Loss: 4.282, avg. samples / sec: 58187.52
Iteration:    940, Loss function: 5.782, Average Loss: 4.243, avg. samples / sec: 57947.14
Iteration:    940, Loss function: 4.969, Average Loss: 4.286, avg. samples / sec: 58100.18
Iteration:    940, Loss function: 5.163, Average Loss: 4.263, avg. samples / sec: 58110.99
Iteration:    940, Loss function: 4.799, Average Loss: 4.272, avg. samples / sec: 58035.80
Iteration:    960, Loss function: 5.291, Average Loss: 4.298, avg. samples / sec: 56121.17
Iteration:    960, Loss function: 5.388, Average Loss: 4.303, avg. samples / sec: 56149.36
Iteration:    960, Loss function: 5.079, Average Loss: 4.291, avg. samples / sec: 56210.26
Iteration:    960, Loss function: 5.196, Average Loss: 4.283, avg. samples / sec: 55939.88
Iteration:    960, Loss function: 5.289, Average Loss: 4.319, avg. samples / sec: 56030.51
Iteration:    960, Loss function: 5.605, Average Loss: 4.299, avg. samples / sec: 56054.80
Iteration:    960, Loss function: 6.604, Average Loss: 4.279, avg. samples / sec: 56100.17
Iteration:    960, Loss function: 4.236, Average Loss: 4.317, avg. samples / sec: 55958.03
Iteration:    960, Loss function: 5.907, Average Loss: 4.259, avg. samples / sec: 56017.99
Iteration:    960, Loss function: 4.644, Average Loss: 4.299, avg. samples / sec: 55940.50
Iteration:    960, Loss function: 4.505, Average Loss: 4.304, avg. samples / sec: 55969.70
Iteration:    960, Loss function: 5.197, Average Loss: 4.307, avg. samples / sec: 55875.65
Iteration:    960, Loss function: 5.914, Average Loss: 4.283, avg. samples / sec: 55835.03
Iteration:    960, Loss function: 4.424, Average Loss: 4.303, avg. samples / sec: 55776.16
Iteration:    960, Loss function: 6.119, Average Loss: 4.290, avg. samples / sec: 55730.15
:::MLL 1558639194.183 epoch_stop: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 819}}
:::MLL 1558639194.183 epoch_start: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 673}}
Iteration:    980, Loss function: 5.511, Average Loss: 4.319, avg. samples / sec: 58108.11
Iteration:    980, Loss function: 5.317, Average Loss: 4.313, avg. samples / sec: 57801.57
Iteration:    980, Loss function: 4.848, Average Loss: 4.319, avg. samples / sec: 57940.66
Iteration:    980, Loss function: 5.855, Average Loss: 4.322, avg. samples / sec: 57967.24
Iteration:    980, Loss function: 5.117, Average Loss: 4.320, avg. samples / sec: 57758.43
Iteration:    980, Loss function: 6.430, Average Loss: 4.298, avg. samples / sec: 57791.90
Iteration:    980, Loss function: 4.969, Average Loss: 4.309, avg. samples / sec: 57813.90
Iteration:    980, Loss function: 5.094, Average Loss: 4.274, avg. samples / sec: 57840.88
Iteration:    980, Loss function: 4.956, Average Loss: 4.330, avg. samples / sec: 57797.64
Iteration:    980, Loss function: 5.093, Average Loss: 4.332, avg. samples / sec: 57725.08
Iteration:    980, Loss function: 3.837, Average Loss: 4.301, avg. samples / sec: 57812.01
Iteration:    980, Loss function: 4.919, Average Loss: 4.295, avg. samples / sec: 57619.84
Iteration:    980, Loss function: 5.567, Average Loss: 4.304, avg. samples / sec: 57797.19
Iteration:    980, Loss function: 6.014, Average Loss: 4.318, avg. samples / sec: 57603.73
Iteration:    980, Loss function: 5.058, Average Loss: 4.305, avg. samples / sec: 57381.61
Iteration:   1000, Loss function: 5.834, Average Loss: 4.323, avg. samples / sec: 60058.61
Iteration:   1000, Loss function: 4.321, Average Loss: 4.318, avg. samples / sec: 59794.41
Iteration:   1000, Loss function: 5.911, Average Loss: 4.337, avg. samples / sec: 59527.42
Iteration:   1000, Loss function: 6.395, Average Loss: 4.291, avg. samples / sec: 59666.34
Iteration:   1000, Loss function: 4.870, Average Loss: 4.323, avg. samples / sec: 59906.61
Iteration:   1000, Loss function: 5.993, Average Loss: 4.352, avg. samples / sec: 59726.93
Iteration:   1000, Loss function: 4.572, Average Loss: 4.317, avg. samples / sec: 59605.88
Iteration:   1000, Loss function: 5.341, Average Loss: 4.332, avg. samples / sec: 59622.72
Iteration:   1000, Loss function: 5.626, Average Loss: 4.332, avg. samples / sec: 59504.22
Iteration:   1000, Loss function: 5.295, Average Loss: 4.341, avg. samples / sec: 59528.40
Iteration:   1000, Loss function: 6.001, Average Loss: 4.342, avg. samples / sec: 59810.45
Iteration:   1000, Loss function: 4.702, Average Loss: 4.309, avg. samples / sec: 59773.01
Iteration:   1000, Loss function: 5.721, Average Loss: 4.354, avg. samples / sec: 59585.44
Iteration:   1000, Loss function: 5.016, Average Loss: 4.327, avg. samples / sec: 59418.75
Iteration:   1000, Loss function: 4.573, Average Loss: 4.338, avg. samples / sec: 59394.98
Iteration:   1020, Loss function: 5.660, Average Loss: 4.334, avg. samples / sec: 58439.14
Iteration:   1020, Loss function: 5.487, Average Loss: 4.357, avg. samples / sec: 58425.45
Iteration:   1020, Loss function: 4.357, Average Loss: 4.369, avg. samples / sec: 58553.33
Iteration:   1020, Loss function: 5.307, Average Loss: 4.369, avg. samples / sec: 58434.97
Iteration:   1020, Loss function: 4.151, Average Loss: 4.354, avg. samples / sec: 58524.42
Iteration:   1020, Loss function: 4.257, Average Loss: 4.338, avg. samples / sec: 58524.98
Iteration:   1020, Loss function: 4.254, Average Loss: 4.356, avg. samples / sec: 58464.50
Iteration:   1020, Loss function: 5.034, Average Loss: 4.305, avg. samples / sec: 58329.47
Iteration:   1020, Loss function: 5.333, Average Loss: 4.343, avg. samples / sec: 58405.89
Iteration:   1020, Loss function: 4.247, Average Loss: 4.331, avg. samples / sec: 58320.49
Iteration:   1020, Loss function: 5.285, Average Loss: 4.325, avg. samples / sec: 58413.49
Iteration:   1020, Loss function: 4.869, Average Loss: 4.350, avg. samples / sec: 58297.79
Iteration:   1020, Loss function: 4.684, Average Loss: 4.354, avg. samples / sec: 58447.23
Iteration:   1020, Loss function: 5.549, Average Loss: 4.334, avg. samples / sec: 58203.17
Iteration:   1020, Loss function: 4.946, Average Loss: 4.340, avg. samples / sec: 58189.86
Iteration:   1040, Loss function: 5.886, Average Loss: 4.363, avg. samples / sec: 60038.30
Iteration:   1040, Loss function: 5.068, Average Loss: 4.377, avg. samples / sec: 59814.23
Iteration:   1040, Loss function: 3.937, Average Loss: 4.355, avg. samples / sec: 59894.92
Iteration:   1040, Loss function: 5.368, Average Loss: 4.343, avg. samples / sec: 59899.55
Iteration:   1040, Loss function: 4.147, Average Loss: 4.349, avg. samples / sec: 59981.01
Iteration:   1040, Loss function: 5.143, Average Loss: 4.378, avg. samples / sec: 59743.51
Iteration:   1040, Loss function: 5.424, Average Loss: 4.354, avg. samples / sec: 59889.60
Iteration:   1040, Loss function: 4.496, Average Loss: 4.389, avg. samples / sec: 59631.53
Iteration:   1040, Loss function: 7.208, Average Loss: 4.375, avg. samples / sec: 59645.05
Iteration:   1040, Loss function: 4.539, Average Loss: 4.372, avg. samples / sec: 59679.86
Iteration:   1040, Loss function: 4.235, Average Loss: 4.352, avg. samples / sec: 59646.31
Iteration:   1040, Loss function: 4.944, Average Loss: 4.362, avg. samples / sec: 59760.23
Iteration:   1040, Loss function: 5.958, Average Loss: 4.321, avg. samples / sec: 59681.00
Iteration:   1040, Loss function: 4.844, Average Loss: 4.372, avg. samples / sec: 59740.27
Iteration:   1040, Loss function: 4.971, Average Loss: 4.352, avg. samples / sec: 59427.42
:::MLL 1558639196.167 epoch_stop: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 819}}
:::MLL 1558639196.167 epoch_start: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 673}}
Iteration:   1060, Loss function: 5.666, Average Loss: 4.381, avg. samples / sec: 58339.57
Iteration:   1060, Loss function: 4.250, Average Loss: 4.332, avg. samples / sec: 58287.35
Iteration:   1060, Loss function: 4.541, Average Loss: 4.365, avg. samples / sec: 58220.89
Iteration:   1060, Loss function: 4.662, Average Loss: 4.362, avg. samples / sec: 58266.05
Iteration:   1060, Loss function: 5.977, Average Loss: 4.363, avg. samples / sec: 58402.30
Iteration:   1060, Loss function: 4.191, Average Loss: 4.407, avg. samples / sec: 58156.38
Iteration:   1060, Loss function: 4.172, Average Loss: 4.357, avg. samples / sec: 58000.88
Iteration:   1060, Loss function: 5.048, Average Loss: 4.394, avg. samples / sec: 57949.07
Iteration:   1060, Loss function: 6.116, Average Loss: 4.394, avg. samples / sec: 58016.06
Iteration:   1060, Loss function: 5.191, Average Loss: 4.370, avg. samples / sec: 57979.26
Iteration:   1060, Loss function: 4.392, Average Loss: 4.381, avg. samples / sec: 58076.33
Iteration:   1060, Loss function: 3.501, Average Loss: 4.376, avg. samples / sec: 57778.14
Iteration:   1060, Loss function: 6.216, Average Loss: 4.381, avg. samples / sec: 58110.20
Iteration:   1060, Loss function: 4.889, Average Loss: 4.376, avg. samples / sec: 58046.03
Iteration:   1060, Loss function: 6.218, Average Loss: 4.350, avg. samples / sec: 57760.37
Iteration:   1080, Loss function: 5.625, Average Loss: 4.391, avg. samples / sec: 60341.04
Iteration:   1080, Loss function: 5.045, Average Loss: 4.374, avg. samples / sec: 60102.31
Iteration:   1080, Loss function: 3.963, Average Loss: 4.368, avg. samples / sec: 60230.69
Iteration:   1080, Loss function: 5.097, Average Loss: 4.393, avg. samples / sec: 60349.08
Iteration:   1080, Loss function: 4.667, Average Loss: 4.359, avg. samples / sec: 60449.64
Iteration:   1080, Loss function: 4.620, Average Loss: 4.380, avg. samples / sec: 60090.93
Iteration:   1080, Loss function: 5.382, Average Loss: 4.394, avg. samples / sec: 60197.09
Iteration:   1080, Loss function: 4.836, Average Loss: 4.395, avg. samples / sec: 60194.44
Iteration:   1080, Loss function: 5.011, Average Loss: 4.421, avg. samples / sec: 60055.82
Iteration:   1080, Loss function: 5.100, Average Loss: 4.404, avg. samples / sec: 60094.08
Iteration:   1080, Loss function: 3.958, Average Loss: 4.392, avg. samples / sec: 59853.46
Iteration:   1080, Loss function: 4.257, Average Loss: 4.404, avg. samples / sec: 60053.14
Iteration:   1080, Loss function: 4.010, Average Loss: 4.388, avg. samples / sec: 60055.00
Iteration:   1080, Loss function: 4.491, Average Loss: 4.342, avg. samples / sec: 59913.08
Iteration:   1080, Loss function: 5.690, Average Loss: 4.376, avg. samples / sec: 59913.30
Iteration:   1100, Loss function: 4.774, Average Loss: 4.377, avg. samples / sec: 56977.00
Iteration:   1100, Loss function: 4.659, Average Loss: 4.413, avg. samples / sec: 56923.12
Iteration:   1100, Loss function: 4.418, Average Loss: 4.409, avg. samples / sec: 56963.85
Iteration:   1100, Loss function: 5.664, Average Loss: 4.419, avg. samples / sec: 57049.77
Iteration:   1100, Loss function: 5.205, Average Loss: 4.399, avg. samples / sec: 57044.59
Iteration:   1100, Loss function: 5.677, Average Loss: 4.404, avg. samples / sec: 57000.27
Iteration:   1100, Loss function: 4.140, Average Loss: 4.391, avg. samples / sec: 56893.80
Iteration:   1100, Loss function: 4.834, Average Loss: 4.431, avg. samples / sec: 56976.88
Iteration:   1100, Loss function: 5.445, Average Loss: 4.405, avg. samples / sec: 56972.44
Iteration:   1100, Loss function: 3.768, Average Loss: 4.413, avg. samples / sec: 56942.00
Iteration:   1100, Loss function: 5.545, Average Loss: 4.386, avg. samples / sec: 56783.08
Iteration:   1100, Loss function: 4.579, Average Loss: 4.387, avg. samples / sec: 56770.13
Iteration:   1100, Loss function: 5.032, Average Loss: 4.354, avg. samples / sec: 56930.20
Iteration:   1100, Loss function: 4.685, Average Loss: 4.406, avg. samples / sec: 56731.08
Iteration:   1100, Loss function: 4.263, Average Loss: 4.389, avg. samples / sec: 56860.86
:::MLL 1558639198.172 epoch_stop: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 819}}
:::MLL 1558639198.172 epoch_start: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 673}}
Iteration:   1120, Loss function: 4.675, Average Loss: 4.440, avg. samples / sec: 60161.47
Iteration:   1120, Loss function: 4.345, Average Loss: 4.398, avg. samples / sec: 60247.38
Iteration:   1120, Loss function: 4.108, Average Loss: 4.429, avg. samples / sec: 60126.34
Iteration:   1120, Loss function: 5.359, Average Loss: 4.399, avg. samples / sec: 60335.15
Iteration:   1120, Loss function: 4.556, Average Loss: 4.408, avg. samples / sec: 60103.10
Iteration:   1120, Loss function: 5.763, Average Loss: 4.417, avg. samples / sec: 60061.94
Iteration:   1120, Loss function: 3.971, Average Loss: 4.417, avg. samples / sec: 60148.64
Iteration:   1120, Loss function: 4.975, Average Loss: 4.410, avg. samples / sec: 60019.73
Iteration:   1120, Loss function: 4.168, Average Loss: 4.424, avg. samples / sec: 59975.47
Iteration:   1120, Loss function: 5.021, Average Loss: 4.361, avg. samples / sec: 60123.36
Iteration:   1120, Loss function: 3.727, Average Loss: 4.390, avg. samples / sec: 59901.87
Iteration:   1120, Loss function: 6.085, Average Loss: 4.396, avg. samples / sec: 60086.11
Iteration:   1120, Loss function: 4.919, Average Loss: 4.418, avg. samples / sec: 59886.32
Iteration:   1120, Loss function: 5.320, Average Loss: 4.421, avg. samples / sec: 59708.48
Iteration:   1120, Loss function: 3.356, Average Loss: 4.397, avg. samples / sec: 59686.83
Iteration:   1140, Loss function: 5.902, Average Loss: 4.371, avg. samples / sec: 56251.18
Iteration:   1140, Loss function: 5.051, Average Loss: 4.431, avg. samples / sec: 56197.23
Iteration:   1140, Loss function: 4.100, Average Loss: 4.415, avg. samples / sec: 56116.27
Iteration:   1140, Loss function: 3.879, Average Loss: 4.420, avg. samples / sec: 56139.70
Iteration:   1140, Loss function: 5.557, Average Loss: 4.401, avg. samples / sec: 56186.57
Iteration:   1140, Loss function: 4.740, Average Loss: 4.429, avg. samples / sec: 56155.65
Iteration:   1140, Loss function: 4.893, Average Loss: 4.409, avg. samples / sec: 56044.17
Iteration:   1140, Loss function: 5.654, Average Loss: 4.449, avg. samples / sec: 56008.02
Iteration:   1140, Loss function: 4.116, Average Loss: 4.411, avg. samples / sec: 55985.84
Iteration:   1140, Loss function: 3.915, Average Loss: 4.435, avg. samples / sec: 55991.51
Iteration:   1140, Loss function: 5.197, Average Loss: 4.419, avg. samples / sec: 56080.10
Iteration:   1140, Loss function: 5.822, Average Loss: 4.404, avg. samples / sec: 56362.88
Iteration:   1140, Loss function: 4.596, Average Loss: 4.431, avg. samples / sec: 56301.79
Iteration:   1140, Loss function: 4.351, Average Loss: 4.406, avg. samples / sec: 56024.85
Iteration:   1140, Loss function: 5.812, Average Loss: 4.428, avg. samples / sec: 56127.99
Iteration:   1160, Loss function: 4.923, Average Loss: 4.440, avg. samples / sec: 56785.80
Iteration:   1160, Loss function: 5.694, Average Loss: 4.431, avg. samples / sec: 56893.87
Iteration:   1160, Loss function: 4.608, Average Loss: 4.415, avg. samples / sec: 56838.38
Iteration:   1160, Loss function: 5.256, Average Loss: 4.443, avg. samples / sec: 56846.89
Iteration:   1160, Loss function: 5.948, Average Loss: 4.456, avg. samples / sec: 56819.25
Iteration:   1160, Loss function: 3.920, Average Loss: 4.431, avg. samples / sec: 56744.76
Iteration:   1160, Loss function: 4.678, Average Loss: 4.424, avg. samples / sec: 56679.31
Iteration:   1160, Loss function: 5.534, Average Loss: 4.417, avg. samples / sec: 56793.67
Iteration:   1160, Loss function: 5.470, Average Loss: 4.415, avg. samples / sec: 56848.72
Iteration:   1160, Loss function: 4.527, Average Loss: 4.411, avg. samples / sec: 56656.91
Iteration:   1160, Loss function: 4.031, Average Loss: 4.378, avg. samples / sec: 56574.30
Iteration:   1160, Loss function: 6.601, Average Loss: 4.443, avg. samples / sec: 56752.82
Iteration:   1160, Loss function: 5.263, Average Loss: 4.420, avg. samples / sec: 56659.16
Iteration:   1160, Loss function: 4.642, Average Loss: 4.440, avg. samples / sec: 56782.96
Iteration:   1160, Loss function: 5.076, Average Loss: 4.438, avg. samples / sec: 56565.26
Iteration:   1180, Loss function: 5.530, Average Loss: 4.437, avg. samples / sec: 57236.29
Iteration:   1180, Loss function: 4.671, Average Loss: 4.466, avg. samples / sec: 57241.26
Iteration:   1180, Loss function: 4.045, Average Loss: 4.427, avg. samples / sec: 57426.95
Iteration:   1180, Loss function: 4.770, Average Loss: 4.455, avg. samples / sec: 57358.61
Iteration:   1180, Loss function: 4.794, Average Loss: 4.434, avg. samples / sec: 57282.05
Iteration:   1180, Loss function: 4.045, Average Loss: 4.424, avg. samples / sec: 57251.07
Iteration:   1180, Loss function: 4.521, Average Loss: 4.423, avg. samples / sec: 57114.00
Iteration:   1180, Loss function: 5.706, Average Loss: 4.453, avg. samples / sec: 57090.97
Iteration:   1180, Loss function: 4.541, Average Loss: 4.420, avg. samples / sec: 57230.85
Iteration:   1180, Loss function: 6.600, Average Loss: 4.421, avg. samples / sec: 57216.42
Iteration:   1180, Loss function: 5.661, Average Loss: 4.442, avg. samples / sec: 57304.15
Iteration:   1180, Loss function: 5.109, Average Loss: 4.449, avg. samples / sec: 57075.23
Iteration:   1180, Loss function: 5.404, Average Loss: 4.448, avg. samples / sec: 57275.46
Iteration:   1180, Loss function: 5.184, Average Loss: 4.437, avg. samples / sec: 57072.02
Iteration:   1180, Loss function: 4.197, Average Loss: 4.389, avg. samples / sec: 57143.22
:::MLL 1558639200.231 epoch_stop: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 819}}
:::MLL 1558639200.232 epoch_start: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 673}}
Iteration:   1200, Loss function: 4.427, Average Loss: 4.436, avg. samples / sec: 57180.32
Iteration:   1200, Loss function: 4.657, Average Loss: 4.457, avg. samples / sec: 57075.28
Iteration:   1200, Loss function: 5.184, Average Loss: 4.459, avg. samples / sec: 57147.28
Iteration:   1200, Loss function: 5.508, Average Loss: 4.458, avg. samples / sec: 57179.74
Iteration:   1200, Loss function: 5.379, Average Loss: 4.442, avg. samples / sec: 57049.26
Iteration:   1200, Loss function: 4.324, Average Loss: 4.430, avg. samples / sec: 57088.94
Iteration:   1200, Loss function: 4.705, Average Loss: 4.474, avg. samples / sec: 57000.69
Iteration:   1200, Loss function: 4.667, Average Loss: 4.448, avg. samples / sec: 56956.83
Iteration:   1200, Loss function: 4.526, Average Loss: 4.399, avg. samples / sec: 57189.34
Iteration:   1200, Loss function: 5.809, Average Loss: 4.433, avg. samples / sec: 57112.95
Iteration:   1200, Loss function: 5.719, Average Loss: 4.434, avg. samples / sec: 56971.15
Iteration:   1200, Loss function: 4.699, Average Loss: 4.427, avg. samples / sec: 57038.22
Iteration:   1200, Loss function: 4.848, Average Loss: 4.455, avg. samples / sec: 57090.14
Iteration:   1200, Loss function: 4.407, Average Loss: 4.450, avg. samples / sec: 57061.02
Iteration:   1200, Loss function: 5.496, Average Loss: 4.447, avg. samples / sec: 57090.88
Iteration:   1220, Loss function: 4.795, Average Loss: 4.486, avg. samples / sec: 58398.58
Iteration:   1220, Loss function: 4.039, Average Loss: 4.463, avg. samples / sec: 58321.87
Iteration:   1220, Loss function: 5.621, Average Loss: 4.446, avg. samples / sec: 58267.23
Iteration:   1220, Loss function: 4.360, Average Loss: 4.439, avg. samples / sec: 58126.23
Iteration:   1220, Loss function: 5.234, Average Loss: 4.439, avg. samples / sec: 58238.04
Iteration:   1220, Loss function: 4.650, Average Loss: 4.458, avg. samples / sec: 58296.13
Iteration:   1220, Loss function: 5.920, Average Loss: 4.455, avg. samples / sec: 58276.50
Iteration:   1220, Loss function: 5.138, Average Loss: 4.456, avg. samples / sec: 58152.37
Iteration:   1220, Loss function: 5.822, Average Loss: 4.464, avg. samples / sec: 58083.32
Iteration:   1220, Loss function: 4.105, Average Loss: 4.405, avg. samples / sec: 58167.59
Iteration:   1220, Loss function: 5.021, Average Loss: 4.439, avg. samples / sec: 58148.51
Iteration:   1220, Loss function: 5.383, Average Loss: 4.440, avg. samples / sec: 58082.20
Iteration:   1220, Loss function: 4.132, Average Loss: 4.466, avg. samples / sec: 58054.75
Iteration:   1220, Loss function: 5.057, Average Loss: 4.464, avg. samples / sec: 58150.79
Iteration:   1220, Loss function: 4.840, Average Loss: 4.432, avg. samples / sec: 58139.75
Iteration:   1240, Loss function: 4.716, Average Loss: 4.463, avg. samples / sec: 59253.99
Iteration:   1240, Loss function: 4.953, Average Loss: 4.409, avg. samples / sec: 59288.96
Iteration:   1240, Loss function: 4.789, Average Loss: 4.494, avg. samples / sec: 58985.00
Iteration:   1240, Loss function: 2.771, Average Loss: 4.446, avg. samples / sec: 59234.63
Iteration:   1240, Loss function: 4.977, Average Loss: 4.447, avg. samples / sec: 59062.82
Iteration:   1240, Loss function: 4.500, Average Loss: 4.447, avg. samples / sec: 59073.54
Iteration:   1240, Loss function: 4.893, Average Loss: 4.442, avg. samples / sec: 59198.11
Iteration:   1240, Loss function: 5.256, Average Loss: 4.470, avg. samples / sec: 59118.15
Iteration:   1240, Loss function: 4.964, Average Loss: 4.474, avg. samples / sec: 59167.96
Iteration:   1240, Loss function: 4.639, Average Loss: 4.448, avg. samples / sec: 59018.40
Iteration:   1240, Loss function: 4.947, Average Loss: 4.469, avg. samples / sec: 59077.33
Iteration:   1240, Loss function: 5.651, Average Loss: 4.471, avg. samples / sec: 59012.39
Iteration:   1240, Loss function: 4.717, Average Loss: 4.453, avg. samples / sec: 59103.15
Iteration:   1240, Loss function: 5.129, Average Loss: 4.478, avg. samples / sec: 59089.76
Iteration:   1240, Loss function: 6.336, Average Loss: 4.475, avg. samples / sec: 58220.91
:::MLL 1558639202.262 epoch_stop: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 819}}
:::MLL 1558639202.262 epoch_start: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 673}}
Iteration:   1260, Loss function: 5.444, Average Loss: 4.474, avg. samples / sec: 58326.53
Iteration:   1260, Loss function: 4.526, Average Loss: 4.457, avg. samples / sec: 58437.10
Iteration:   1260, Loss function: 3.901, Average Loss: 4.456, avg. samples / sec: 58469.25
Iteration:   1260, Loss function: 5.093, Average Loss: 4.486, avg. samples / sec: 58455.28
Iteration:   1260, Loss function: 3.577, Average Loss: 4.449, avg. samples / sec: 58411.07
Iteration:   1260, Loss function: 4.712, Average Loss: 4.452, avg. samples / sec: 58293.96
Iteration:   1260, Loss function: 4.082, Average Loss: 4.483, avg. samples / sec: 58380.09
Iteration:   1260, Loss function: 4.788, Average Loss: 4.483, avg. samples / sec: 58306.16
Iteration:   1260, Loss function: 3.318, Average Loss: 4.479, avg. samples / sec: 58329.81
Iteration:   1260, Loss function: 4.724, Average Loss: 4.476, avg. samples / sec: 58951.49
Iteration:   1260, Loss function: 3.691, Average Loss: 4.458, avg. samples / sec: 58150.45
Iteration:   1260, Loss function: 5.536, Average Loss: 4.463, avg. samples / sec: 58281.11
Iteration:   1260, Loss function: 5.375, Average Loss: 4.409, avg. samples / sec: 58006.89
Iteration:   1260, Loss function: 3.862, Average Loss: 4.482, avg. samples / sec: 58182.82
Iteration:   1260, Loss function: 3.972, Average Loss: 4.501, avg. samples / sec: 57706.59
Iteration:   1280, Loss function: 4.917, Average Loss: 4.487, avg. samples / sec: 57508.10
Iteration:   1280, Loss function: 5.887, Average Loss: 4.491, avg. samples / sec: 57427.66
Iteration:   1280, Loss function: 5.002, Average Loss: 4.465, avg. samples / sec: 57491.40
Iteration:   1280, Loss function: 4.607, Average Loss: 4.467, avg. samples / sec: 57269.76
Iteration:   1280, Loss function: 4.512, Average Loss: 4.468, avg. samples / sec: 57490.39
Iteration:   1280, Loss function: 6.326, Average Loss: 4.455, avg. samples / sec: 57303.64
Iteration:   1280, Loss function: 5.465, Average Loss: 4.463, avg. samples / sec: 57316.11
Iteration:   1280, Loss function: 3.536, Average Loss: 4.487, avg. samples / sec: 57362.60
Iteration:   1280, Loss function: 5.213, Average Loss: 4.481, avg. samples / sec: 57187.91
Iteration:   1280, Loss function: 3.753, Average Loss: 4.461, avg. samples / sec: 57211.40
Iteration:   1280, Loss function: 4.561, Average Loss: 4.481, avg. samples / sec: 57359.87
Iteration:   1280, Loss function: 4.094, Average Loss: 4.489, avg. samples / sec: 57502.03
Iteration:   1280, Loss function: 5.052, Average Loss: 4.492, avg. samples / sec: 57185.89
Iteration:   1280, Loss function: 3.982, Average Loss: 4.410, avg. samples / sec: 57401.15
Iteration:   1280, Loss function: 3.999, Average Loss: 4.509, avg. samples / sec: 57605.31
Iteration:   1300, Loss function: 3.779, Average Loss: 4.473, avg. samples / sec: 58523.64
Iteration:   1300, Loss function: 4.668, Average Loss: 4.492, avg. samples / sec: 58599.42
Iteration:   1300, Loss function: 3.961, Average Loss: 4.417, avg. samples / sec: 58648.95
Iteration:   1300, Loss function: 5.453, Average Loss: 4.469, avg. samples / sec: 58490.56
Iteration:   1300, Loss function: 5.058, Average Loss: 4.516, avg. samples / sec: 58745.86
Iteration:   1300, Loss function: 3.988, Average Loss: 4.475, avg. samples / sec: 58462.58
Iteration:   1300, Loss function: 5.177, Average Loss: 4.476, avg. samples / sec: 58423.63
Iteration:   1300, Loss function: 4.224, Average Loss: 4.489, avg. samples / sec: 58437.47
Iteration:   1300, Loss function: 4.474, Average Loss: 4.494, avg. samples / sec: 58410.97
Iteration:   1300, Loss function: 5.288, Average Loss: 4.496, avg. samples / sec: 58409.93
Iteration:   1300, Loss function: 4.638, Average Loss: 4.463, avg. samples / sec: 58278.29
Iteration:   1300, Loss function: 4.660, Average Loss: 4.497, avg. samples / sec: 58210.55
Iteration:   1300, Loss function: 5.517, Average Loss: 4.496, avg. samples / sec: 58237.68
Iteration:   1300, Loss function: 5.403, Average Loss: 4.467, avg. samples / sec: 58248.11
Iteration:   1300, Loss function: 5.469, Average Loss: 4.493, avg. samples / sec: 57976.32
Iteration:   1320, Loss function: 3.150, Average Loss: 4.483, avg. samples / sec: 58031.85
Iteration:   1320, Loss function: 4.514, Average Loss: 4.484, avg. samples / sec: 58037.80
Iteration:   1320, Loss function: 4.460, Average Loss: 4.492, avg. samples / sec: 57922.35
Iteration:   1320, Loss function: 4.489, Average Loss: 4.427, avg. samples / sec: 57932.47
Iteration:   1320, Loss function: 4.996, Average Loss: 4.494, avg. samples / sec: 58044.23
Iteration:   1320, Loss function: 6.119, Average Loss: 4.496, avg. samples / sec: 58067.27
Iteration:   1320, Loss function: 5.765, Average Loss: 4.483, avg. samples / sec: 57851.73
Iteration:   1320, Loss function: 4.803, Average Loss: 4.476, avg. samples / sec: 57899.03
Iteration:   1320, Loss function: 4.566, Average Loss: 4.502, avg. samples / sec: 58119.54
Iteration:   1320, Loss function: 3.680, Average Loss: 4.465, avg. samples / sec: 58046.39
Iteration:   1320, Loss function: 5.860, Average Loss: 4.518, avg. samples / sec: 57849.05
Iteration:   1320, Loss function: 5.008, Average Loss: 4.502, avg. samples / sec: 57975.08
Iteration:   1320, Loss function: 5.230, Average Loss: 4.471, avg. samples / sec: 58081.17
Iteration:   1320, Loss function: 4.043, Average Loss: 4.500, avg. samples / sec: 58217.55
Iteration:   1320, Loss function: 4.203, Average Loss: 4.499, avg. samples / sec: 57882.74
:::MLL 1558639204.280 epoch_stop: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 819}}
:::MLL 1558639204.281 epoch_start: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 673}}
Iteration:   1340, Loss function: 5.888, Average Loss: 4.503, avg. samples / sec: 58357.56
Iteration:   1340, Loss function: 5.335, Average Loss: 4.488, avg. samples / sec: 58296.54
Iteration:   1340, Loss function: 4.038, Average Loss: 4.519, avg. samples / sec: 58370.74
Iteration:   1340, Loss function: 4.513, Average Loss: 4.487, avg. samples / sec: 58108.71
Iteration:   1340, Loss function: 4.499, Average Loss: 4.502, avg. samples / sec: 58533.63
Iteration:   1340, Loss function: 5.318, Average Loss: 4.482, avg. samples / sec: 58233.93
Iteration:   1340, Loss function: 4.750, Average Loss: 4.506, avg. samples / sec: 58312.05
Iteration:   1340, Loss function: 4.600, Average Loss: 4.429, avg. samples / sec: 58149.47
Iteration:   1340, Loss function: 3.920, Average Loss: 4.466, avg. samples / sec: 58254.05
Iteration:   1340, Loss function: 3.810, Average Loss: 4.492, avg. samples / sec: 58141.39
Iteration:   1340, Loss function: 6.187, Average Loss: 4.501, avg. samples / sec: 58300.11
Iteration:   1340, Loss function: 6.059, Average Loss: 4.500, avg. samples / sec: 58063.41
Iteration:   1340, Loss function: 4.235, Average Loss: 4.483, avg. samples / sec: 58036.32
Iteration:   1340, Loss function: 4.507, Average Loss: 4.499, avg. samples / sec: 58063.10
Iteration:   1340, Loss function: 5.166, Average Loss: 4.476, avg. samples / sec: 58149.30
Iteration:   1360, Loss function: 3.245, Average Loss: 4.491, avg. samples / sec: 57666.52
Iteration:   1360, Loss function: 4.766, Average Loss: 4.504, avg. samples / sec: 57586.05
Iteration:   1360, Loss function: 4.943, Average Loss: 4.435, avg. samples / sec: 57600.15
Iteration:   1360, Loss function: 6.093, Average Loss: 4.484, avg. samples / sec: 57570.55
Iteration:   1360, Loss function: 3.990, Average Loss: 4.511, avg. samples / sec: 57561.00
Iteration:   1360, Loss function: 4.251, Average Loss: 4.501, avg. samples / sec: 57670.04
Iteration:   1360, Loss function: 4.821, Average Loss: 4.505, avg. samples / sec: 57581.86
Iteration:   1360, Loss function: 3.834, Average Loss: 4.507, avg. samples / sec: 57425.97
Iteration:   1360, Loss function: 4.528, Average Loss: 4.493, avg. samples / sec: 57596.05
Iteration:   1360, Loss function: 3.811, Average Loss: 4.519, avg. samples / sec: 57409.69
Iteration:   1360, Loss function: 5.137, Average Loss: 4.507, avg. samples / sec: 57565.59
Iteration:   1360, Loss function: 4.180, Average Loss: 4.476, avg. samples / sec: 57632.40
Iteration:   1360, Loss function: 4.373, Average Loss: 4.491, avg. samples / sec: 57334.57
Iteration:   1360, Loss function: 5.106, Average Loss: 4.469, avg. samples / sec: 57429.60
Iteration:   1360, Loss function: 3.848, Average Loss: 4.492, avg. samples / sec: 57388.25
Iteration:   1380, Loss function: 4.779, Average Loss: 4.501, avg. samples / sec: 60532.94
Iteration:   1380, Loss function: 4.599, Average Loss: 4.477, avg. samples / sec: 60629.84
Iteration:   1380, Loss function: 4.094, Average Loss: 4.512, avg. samples / sec: 60571.21
Iteration:   1380, Loss function: 4.563, Average Loss: 4.493, avg. samples / sec: 60368.18
Iteration:   1380, Loss function: 4.676, Average Loss: 4.525, avg. samples / sec: 60518.62
Iteration:   1380, Loss function: 5.969, Average Loss: 4.510, avg. samples / sec: 60432.64
Iteration:   1380, Loss function: 4.287, Average Loss: 4.440, avg. samples / sec: 60400.32
Iteration:   1380, Loss function: 4.118, Average Loss: 4.496, avg. samples / sec: 60601.22
Iteration:   1380, Loss function: 4.044, Average Loss: 4.485, avg. samples / sec: 60399.23
Iteration:   1380, Loss function: 4.657, Average Loss: 4.509, avg. samples / sec: 60402.75
Iteration:   1380, Loss function: 4.222, Average Loss: 4.513, avg. samples / sec: 60329.52
Iteration:   1380, Loss function: 4.975, Average Loss: 4.490, avg. samples / sec: 60423.18
Iteration:   1380, Loss function: 4.530, Average Loss: 4.471, avg. samples / sec: 60402.54
Iteration:   1380, Loss function: 5.136, Average Loss: 4.510, avg. samples / sec: 60252.35
Iteration:   1380, Loss function: 4.320, Average Loss: 4.497, avg. samples / sec: 60264.07
:::MLL 1558639206.295 epoch_stop: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 819}}
:::MLL 1558639206.296 epoch_start: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 673}}
Iteration:   1400, Loss function: 3.660, Average Loss: 4.443, avg. samples / sec: 58556.30
Iteration:   1400, Loss function: 5.567, Average Loss: 4.515, avg. samples / sec: 58430.25
Iteration:   1400, Loss function: 4.200, Average Loss: 4.492, avg. samples / sec: 58580.74
Iteration:   1400, Loss function: 5.460, Average Loss: 4.519, avg. samples / sec: 58515.60
Iteration:   1400, Loss function: 4.889, Average Loss: 4.482, avg. samples / sec: 58276.46
Iteration:   1400, Loss function: 4.712, Average Loss: 4.487, avg. samples / sec: 58367.16
Iteration:   1400, Loss function: 5.139, Average Loss: 4.499, avg. samples / sec: 58337.42
Iteration:   1400, Loss function: 4.460, Average Loss: 4.515, avg. samples / sec: 58264.82
Iteration:   1400, Loss function: 4.722, Average Loss: 4.498, avg. samples / sec: 58286.41
Iteration:   1400, Loss function: 4.952, Average Loss: 4.476, avg. samples / sec: 58481.68
Iteration:   1400, Loss function: 4.860, Average Loss: 4.514, avg. samples / sec: 58495.20
Iteration:   1400, Loss function: 3.823, Average Loss: 4.525, avg. samples / sec: 58271.59
Iteration:   1400, Loss function: 3.761, Average Loss: 4.500, avg. samples / sec: 58431.19
Iteration:   1400, Loss function: 5.292, Average Loss: 4.514, avg. samples / sec: 58276.55
Iteration:   1400, Loss function: 5.382, Average Loss: 4.507, avg. samples / sec: 58149.54
Iteration:   1420, Loss function: 4.288, Average Loss: 4.447, avg. samples / sec: 58233.30
Iteration:   1420, Loss function: 3.918, Average Loss: 4.513, avg. samples / sec: 58408.36
Iteration:   1420, Loss function: 4.598, Average Loss: 4.515, avg. samples / sec: 58482.84
Iteration:   1420, Loss function: 4.007, Average Loss: 4.479, avg. samples / sec: 58423.54
Iteration:   1420, Loss function: 4.602, Average Loss: 4.522, avg. samples / sec: 58297.72
Iteration:   1420, Loss function: 4.858, Average Loss: 4.508, avg. samples / sec: 58465.06
Iteration:   1420, Loss function: 4.873, Average Loss: 4.498, avg. samples / sec: 58257.23
Iteration:   1420, Loss function: 5.946, Average Loss: 4.483, avg. samples / sec: 58331.33
Iteration:   1420, Loss function: 4.105, Average Loss: 4.499, avg. samples / sec: 58438.19
Iteration:   1420, Loss function: 5.551, Average Loss: 4.527, avg. samples / sec: 58341.98
Iteration:   1420, Loss function: 4.366, Average Loss: 4.523, avg. samples / sec: 58142.15
Iteration:   1420, Loss function: 3.691, Average Loss: 4.494, avg. samples / sec: 58231.88
Iteration:   1420, Loss function: 3.985, Average Loss: 4.506, avg. samples / sec: 58190.50
Iteration:   1420, Loss function: 3.617, Average Loss: 4.515, avg. samples / sec: 58180.05
Iteration:   1420, Loss function: 5.087, Average Loss: 4.487, avg. samples / sec: 58120.48
Iteration:   1440, Loss function: 4.220, Average Loss: 4.518, avg. samples / sec: 57618.57
Iteration:   1440, Loss function: 3.513, Average Loss: 4.493, avg. samples / sec: 57793.63
Iteration:   1440, Loss function: 5.913, Average Loss: 4.507, avg. samples / sec: 57675.68
Iteration:   1440, Loss function: 4.563, Average Loss: 4.517, avg. samples / sec: 57611.03
Iteration:   1440, Loss function: 5.868, Average Loss: 4.490, avg. samples / sec: 57627.73
Iteration:   1440, Loss function: 3.619, Average Loss: 4.508, avg. samples / sec: 57777.28
Iteration:   1440, Loss function: 4.408, Average Loss: 4.514, avg. samples / sec: 57805.15
Iteration:   1440, Loss function: 5.011, Average Loss: 4.530, avg. samples / sec: 57567.11
Iteration:   1440, Loss function: 5.261, Average Loss: 4.504, avg. samples / sec: 57590.05
Iteration:   1440, Loss function: 5.500, Average Loss: 4.509, avg. samples / sec: 57566.62
Iteration:   1440, Loss function: 3.476, Average Loss: 4.451, avg. samples / sec: 57456.99
Iteration:   1440, Loss function: 4.982, Average Loss: 4.485, avg. samples / sec: 57728.24
Iteration:   1440, Loss function: 4.368, Average Loss: 4.478, avg. samples / sec: 57463.95
Iteration:   1440, Loss function: 4.180, Average Loss: 4.531, avg. samples / sec: 57522.38
Iteration:   1440, Loss function: 3.500, Average Loss: 4.523, avg. samples / sec: 57503.74
Iteration:   1460, Loss function: 4.287, Average Loss: 4.451, avg. samples / sec: 59456.12
Iteration:   1460, Loss function: 4.298, Average Loss: 4.483, avg. samples / sec: 59470.07
Iteration:   1460, Loss function: 3.772, Average Loss: 4.510, avg. samples / sec: 59325.42
Iteration:   1460, Loss function: 4.298, Average Loss: 4.504, avg. samples / sec: 59341.74
Iteration:   1460, Loss function: 3.388, Average Loss: 4.492, avg. samples / sec: 59276.29
Iteration:   1460, Loss function: 5.028, Average Loss: 4.531, avg. samples / sec: 59342.21
Iteration:   1460, Loss function: 3.906, Average Loss: 4.486, avg. samples / sec: 59287.11
Iteration:   1460, Loss function: 4.250, Average Loss: 4.517, avg. samples / sec: 59248.66
Iteration:   1460, Loss function: 3.600, Average Loss: 4.517, avg. samples / sec: 59479.11
Iteration:   1460, Loss function: 4.566, Average Loss: 4.513, avg. samples / sec: 59275.34
Iteration:   1460, Loss function: 4.520, Average Loss: 4.509, avg. samples / sec: 59293.63
Iteration:   1460, Loss function: 4.257, Average Loss: 4.515, avg. samples / sec: 59202.91
Iteration:   1460, Loss function: 4.264, Average Loss: 4.503, avg. samples / sec: 59200.07
Iteration:   1460, Loss function: 4.552, Average Loss: 4.532, avg. samples / sec: 59271.08
Iteration:   1460, Loss function: 5.396, Average Loss: 4.481, avg. samples / sec: 59182.77
:::MLL 1558639208.302 epoch_stop: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 819}}
:::MLL 1558639208.302 epoch_start: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 673}}
Iteration:   1480, Loss function: 4.390, Average Loss: 4.486, avg. samples / sec: 58595.37
Iteration:   1480, Loss function: 5.867, Average Loss: 4.519, avg. samples / sec: 58699.42
Iteration:   1480, Loss function: 4.829, Average Loss: 4.517, avg. samples / sec: 58578.74
Iteration:   1480, Loss function: 4.550, Average Loss: 4.535, avg. samples / sec: 58597.71
Iteration:   1480, Loss function: 5.767, Average Loss: 4.510, avg. samples / sec: 58737.22
Iteration:   1480, Loss function: 6.274, Average Loss: 4.490, avg. samples / sec: 58776.17
Iteration:   1480, Loss function: 5.340, Average Loss: 4.526, avg. samples / sec: 58544.91
Iteration:   1480, Loss function: 5.192, Average Loss: 4.528, avg. samples / sec: 58599.13
Iteration:   1480, Loss function: 3.145, Average Loss: 4.521, avg. samples / sec: 58500.68
Iteration:   1480, Loss function: 4.960, Average Loss: 4.501, avg. samples / sec: 58433.54
Iteration:   1480, Loss function: 4.239, Average Loss: 4.494, avg. samples / sec: 58448.20
Iteration:   1480, Loss function: 4.724, Average Loss: 4.541, avg. samples / sec: 58633.55
Iteration:   1480, Loss function: 3.814, Average Loss: 4.452, avg. samples / sec: 58320.64
Iteration:   1480, Loss function: 4.931, Average Loss: 4.514, avg. samples / sec: 58320.98
Iteration:   1480, Loss function: 4.962, Average Loss: 4.517, avg. samples / sec: 58352.78
Iteration:   1500, Loss function: 4.116, Average Loss: 4.520, avg. samples / sec: 59930.30
Iteration:   1500, Loss function: 3.345, Average Loss: 4.488, avg. samples / sec: 59907.09
Iteration:   1500, Loss function: 4.731, Average Loss: 4.540, avg. samples / sec: 59935.06
Iteration:   1500, Loss function: 4.265, Average Loss: 4.517, avg. samples / sec: 59891.51
Iteration:   1500, Loss function: 4.315, Average Loss: 4.519, avg. samples / sec: 60102.08
Iteration:   1500, Loss function: 4.280, Average Loss: 4.548, avg. samples / sec: 60019.81
Iteration:   1500, Loss function: 4.393, Average Loss: 4.526, avg. samples / sec: 59937.59
Iteration:   1500, Loss function: 4.994, Average Loss: 4.503, avg. samples / sec: 60015.85
Iteration:   1500, Loss function: 6.235, Average Loss: 4.525, avg. samples / sec: 59955.69
Iteration:   1500, Loss function: 5.457, Average Loss: 4.529, avg. samples / sec: 59933.28
Iteration:   1500, Loss function: 4.576, Average Loss: 4.496, avg. samples / sec: 59910.63
Iteration:   1500, Loss function: 4.696, Average Loss: 4.502, avg. samples / sec: 59913.66
Iteration:   1500, Loss function: 4.342, Average Loss: 4.520, avg. samples / sec: 59712.53
Iteration:   1500, Loss function: 4.780, Average Loss: 4.456, avg. samples / sec: 59923.09
Iteration:   1500, Loss function: 5.156, Average Loss: 4.520, avg. samples / sec: 60019.45
Iteration:   1520, Loss function: 3.316, Average Loss: 4.452, avg. samples / sec: 60282.30
Iteration:   1520, Loss function: 4.380, Average Loss: 4.489, avg. samples / sec: 59960.39
Iteration:   1520, Loss function: 2.969, Average Loss: 4.517, avg. samples / sec: 60088.37
Iteration:   1520, Loss function: 4.626, Average Loss: 4.546, avg. samples / sec: 60073.79
Iteration:   1520, Loss function: 5.277, Average Loss: 4.524, avg. samples / sec: 60193.85
Iteration:   1520, Loss function: 5.107, Average Loss: 4.520, avg. samples / sec: 59998.47
Iteration:   1520, Loss function: 2.758, Average Loss: 4.530, avg. samples / sec: 59909.13
Iteration:   1520, Loss function: 4.209, Average Loss: 4.493, avg. samples / sec: 60014.77
Iteration:   1520, Loss function: 4.524, Average Loss: 4.504, avg. samples / sec: 59928.16
Iteration:   1520, Loss function: 3.753, Average Loss: 4.522, avg. samples / sec: 59925.86
Iteration:   1520, Loss function: 4.653, Average Loss: 4.523, avg. samples / sec: 59901.51
Iteration:   1520, Loss function: 3.039, Average Loss: 4.528, avg. samples / sec: 59889.45
Iteration:   1520, Loss function: 4.160, Average Loss: 4.506, avg. samples / sec: 59964.75
Iteration:   1520, Loss function: 5.095, Average Loss: 4.520, avg. samples / sec: 59999.14
Iteration:   1520, Loss function: 4.515, Average Loss: 4.518, avg. samples / sec: 59735.43
:::MLL 1558639210.297 epoch_stop: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 819}}
:::MLL 1558639210.297 epoch_start: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 673}}
Iteration:   1540, Loss function: 4.738, Average Loss: 4.522, avg. samples / sec: 58008.42
Iteration:   1540, Loss function: 4.187, Average Loss: 4.514, avg. samples / sec: 58029.13
Iteration:   1540, Loss function: 4.771, Average Loss: 4.485, avg. samples / sec: 57751.45
Iteration:   1540, Loss function: 5.537, Average Loss: 4.527, avg. samples / sec: 57820.43
Iteration:   1540, Loss function: 4.541, Average Loss: 4.502, avg. samples / sec: 57868.69
Iteration:   1540, Loss function: 5.002, Average Loss: 4.455, avg. samples / sec: 57653.97
Iteration:   1540, Loss function: 4.132, Average Loss: 4.546, avg. samples / sec: 57696.31
Iteration:   1540, Loss function: 5.708, Average Loss: 4.527, avg. samples / sec: 57721.01
Iteration:   1540, Loss function: 4.798, Average Loss: 4.518, avg. samples / sec: 57720.61
Iteration:   1540, Loss function: 4.053, Average Loss: 4.518, avg. samples / sec: 57859.24
Iteration:   1540, Loss function: 4.103, Average Loss: 4.517, avg. samples / sec: 57649.37
Iteration:   1540, Loss function: 4.419, Average Loss: 4.503, avg. samples / sec: 57834.64
Iteration:   1540, Loss function: 4.777, Average Loss: 4.522, avg. samples / sec: 57779.72
Iteration:   1540, Loss function: 4.700, Average Loss: 4.525, avg. samples / sec: 57794.39
Iteration:   1540, Loss function: 4.570, Average Loss: 4.490, avg. samples / sec: 57686.66
Iteration:   1560, Loss function: 3.896, Average Loss: 4.500, avg. samples / sec: 56175.21
Iteration:   1560, Loss function: 4.868, Average Loss: 4.491, avg. samples / sec: 56275.39
Iteration:   1560, Loss function: 5.274, Average Loss: 4.504, avg. samples / sec: 56190.29
Iteration:   1560, Loss function: 4.781, Average Loss: 4.533, avg. samples / sec: 56008.15
Iteration:   1560, Loss function: 5.528, Average Loss: 4.520, avg. samples / sec: 56137.80
Iteration:   1560, Loss function: 4.032, Average Loss: 4.486, avg. samples / sec: 56076.75
Iteration:   1560, Loss function: 4.494, Average Loss: 4.516, avg. samples / sec: 56016.64
Iteration:   1560, Loss function: 4.257, Average Loss: 4.458, avg. samples / sec: 56113.61
Iteration:   1560, Loss function: 4.330, Average Loss: 4.521, avg. samples / sec: 56195.78
Iteration:   1560, Loss function: 4.316, Average Loss: 4.523, avg. samples / sec: 56111.40
Iteration:   1560, Loss function: 4.859, Average Loss: 4.522, avg. samples / sec: 56113.24
Iteration:   1560, Loss function: 4.191, Average Loss: 4.545, avg. samples / sec: 56054.11
Iteration:   1560, Loss function: 4.513, Average Loss: 4.513, avg. samples / sec: 56070.06
Iteration:   1560, Loss function: 4.697, Average Loss: 4.528, avg. samples / sec: 55983.66
Iteration:   1560, Loss function: 4.198, Average Loss: 4.516, avg. samples / sec: 55982.21
Iteration:   1580, Loss function: 4.532, Average Loss: 4.531, avg. samples / sec: 57835.49
Iteration:   1580, Loss function: 4.966, Average Loss: 4.494, avg. samples / sec: 57753.01
Iteration:   1580, Loss function: 5.665, Average Loss: 4.508, avg. samples / sec: 57778.85
Iteration:   1580, Loss function: 5.620, Average Loss: 4.501, avg. samples / sec: 57713.94
Iteration:   1580, Loss function: 4.751, Average Loss: 4.516, avg. samples / sec: 57878.91
Iteration:   1580, Loss function: 4.824, Average Loss: 4.513, avg. samples / sec: 57965.23
Iteration:   1580, Loss function: 4.444, Average Loss: 4.517, avg. samples / sec: 57781.48
Iteration:   1580, Loss function: 3.753, Average Loss: 4.519, avg. samples / sec: 57738.86
Iteration:   1580, Loss function: 4.670, Average Loss: 4.521, avg. samples / sec: 57814.09
Iteration:   1580, Loss function: 3.705, Average Loss: 4.538, avg. samples / sec: 57815.47
Iteration:   1580, Loss function: 2.991, Average Loss: 4.481, avg. samples / sec: 57665.06
Iteration:   1580, Loss function: 4.496, Average Loss: 4.524, avg. samples / sec: 57621.56
Iteration:   1580, Loss function: 5.180, Average Loss: 4.454, avg. samples / sec: 57567.42
Iteration:   1580, Loss function: 4.750, Average Loss: 4.523, avg. samples / sec: 57672.61
Iteration:   1580, Loss function: 4.445, Average Loss: 4.515, avg. samples / sec: 57534.31
Iteration:   1600, Loss function: 5.074, Average Loss: 4.541, avg. samples / sec: 58492.74
Iteration:   1600, Loss function: 5.054, Average Loss: 4.511, avg. samples / sec: 58647.90
Iteration:   1600, Loss function: 5.024, Average Loss: 4.511, avg. samples / sec: 58380.24
Iteration:   1600, Loss function: 4.642, Average Loss: 4.456, avg. samples / sec: 58592.99
Iteration:   1600, Loss function: 3.720, Average Loss: 4.519, avg. samples / sec: 58421.75
Iteration:   1600, Loss function: 4.657, Average Loss: 4.527, avg. samples / sec: 58277.03
Iteration:   1600, Loss function: 4.666, Average Loss: 4.521, avg. samples / sec: 58554.96
Iteration:   1600, Loss function: 3.762, Average Loss: 4.506, avg. samples / sec: 58308.45
Iteration:   1600, Loss function: 4.420, Average Loss: 4.521, avg. samples / sec: 58339.52
Iteration:   1600, Loss function: 5.111, Average Loss: 4.492, avg. samples / sec: 58270.22
Iteration:   1600, Loss function: 3.592, Average Loss: 4.524, avg. samples / sec: 58468.09
Iteration:   1600, Loss function: 3.896, Average Loss: 4.522, avg. samples / sec: 58271.90
Iteration:   1600, Loss function: 4.888, Average Loss: 4.479, avg. samples / sec: 58372.65
Iteration:   1600, Loss function: 5.125, Average Loss: 4.501, avg. samples / sec: 58230.34
Iteration:   1600, Loss function: 5.091, Average Loss: 4.512, avg. samples / sec: 58164.85
:::MLL 1558639212.332 epoch_stop: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 819}}
:::MLL 1558639212.333 epoch_start: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 673}}
Iteration:   1620, Loss function: 4.027, Average Loss: 4.518, avg. samples / sec: 59474.27
Iteration:   1620, Loss function: 4.501, Average Loss: 4.504, avg. samples / sec: 59615.03
Iteration:   1620, Loss function: 4.881, Average Loss: 4.542, avg. samples / sec: 59357.16
Iteration:   1620, Loss function: 3.822, Average Loss: 4.460, avg. samples / sec: 59417.30
Iteration:   1620, Loss function: 4.106, Average Loss: 4.480, avg. samples / sec: 59528.86
Iteration:   1620, Loss function: 4.364, Average Loss: 4.524, avg. samples / sec: 59477.91
Iteration:   1620, Loss function: 3.969, Average Loss: 4.505, avg. samples / sec: 59413.76
Iteration:   1620, Loss function: 4.585, Average Loss: 4.518, avg. samples / sec: 59397.71
Iteration:   1620, Loss function: 3.249, Average Loss: 4.490, avg. samples / sec: 59409.48
Iteration:   1620, Loss function: 3.947, Average Loss: 4.505, avg. samples / sec: 59313.34
Iteration:   1620, Loss function: 3.056, Average Loss: 4.525, avg. samples / sec: 59319.93
Iteration:   1620, Loss function: 3.577, Average Loss: 4.517, avg. samples / sec: 59314.96
Iteration:   1620, Loss function: 4.316, Average Loss: 4.516, avg. samples / sec: 59343.86
Iteration:   1620, Loss function: 5.374, Average Loss: 4.512, avg. samples / sec: 59491.09
Iteration:   1620, Loss function: 3.940, Average Loss: 4.512, avg. samples / sec: 58989.30
Iteration:   1640, Loss function: 4.746, Average Loss: 4.504, avg. samples / sec: 59942.61
Iteration:   1640, Loss function: 3.674, Average Loss: 4.486, avg. samples / sec: 60055.64
Iteration:   1640, Loss function: 4.114, Average Loss: 4.514, avg. samples / sec: 60051.11
Iteration:   1640, Loss function: 5.104, Average Loss: 4.481, avg. samples / sec: 59949.60
Iteration:   1640, Loss function: 3.722, Average Loss: 4.503, avg. samples / sec: 60016.26
Iteration:   1640, Loss function: 4.325, Average Loss: 4.523, avg. samples / sec: 59959.01
Iteration:   1640, Loss function: 5.586, Average Loss: 4.513, avg. samples / sec: 60090.44
Iteration:   1640, Loss function: 6.021, Average Loss: 4.463, avg. samples / sec: 59899.53
Iteration:   1640, Loss function: 3.849, Average Loss: 4.505, avg. samples / sec: 59952.28
Iteration:   1640, Loss function: 4.689, Average Loss: 4.510, avg. samples / sec: 60083.55
Iteration:   1640, Loss function: 3.141, Average Loss: 4.518, avg. samples / sec: 60019.78
Iteration:   1640, Loss function: 4.270, Average Loss: 4.519, avg. samples / sec: 60004.02
Iteration:   1640, Loss function: 5.240, Average Loss: 4.509, avg. samples / sec: 60259.69
Iteration:   1640, Loss function: 4.299, Average Loss: 4.516, avg. samples / sec: 59773.87
Iteration:   1640, Loss function: 4.517, Average Loss: 4.537, avg. samples / sec: 59788.07
Iteration:   1660, Loss function: 5.066, Average Loss: 4.511, avg. samples / sec: 59474.19
Iteration:   1660, Loss function: 3.478, Average Loss: 4.510, avg. samples / sec: 59388.83
Iteration:   1660, Loss function: 3.922, Average Loss: 4.532, avg. samples / sec: 59554.04
Iteration:   1660, Loss function: 4.903, Average Loss: 4.511, avg. samples / sec: 59463.63
Iteration:   1660, Loss function: 4.085, Average Loss: 4.515, avg. samples / sec: 59474.52
Iteration:   1660, Loss function: 4.142, Average Loss: 4.508, avg. samples / sec: 59369.59
Iteration:   1660, Loss function: 4.955, Average Loss: 4.456, avg. samples / sec: 59353.03
Iteration:   1660, Loss function: 4.426, Average Loss: 4.518, avg. samples / sec: 59377.19
Iteration:   1660, Loss function: 4.844, Average Loss: 4.479, avg. samples / sec: 59248.83
Iteration:   1660, Loss function: 4.221, Average Loss: 4.504, avg. samples / sec: 59315.96
Iteration:   1660, Loss function: 3.197, Average Loss: 4.504, avg. samples / sec: 59235.33
Iteration:   1660, Loss function: 3.057, Average Loss: 4.524, avg. samples / sec: 59199.58
Iteration:   1660, Loss function: 4.643, Average Loss: 4.511, avg. samples / sec: 59190.60
Iteration:   1660, Loss function: 3.764, Average Loss: 4.500, avg. samples / sec: 59104.71
Iteration:   1660, Loss function: 2.937, Average Loss: 4.482, avg. samples / sec: 58408.14
:::MLL 1558639214.322 epoch_stop: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 819}}
:::MLL 1558639214.322 epoch_start: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 673}}
Iteration:   1680, Loss function: 4.449, Average Loss: 4.510, avg. samples / sec: 58518.03
Iteration:   1680, Loss function: 3.512, Average Loss: 4.496, avg. samples / sec: 58632.67
Iteration:   1680, Loss function: 4.161, Average Loss: 4.505, avg. samples / sec: 58508.46
Iteration:   1680, Loss function: 4.186, Average Loss: 4.506, avg. samples / sec: 58397.97
Iteration:   1680, Loss function: 4.887, Average Loss: 4.496, avg. samples / sec: 58625.36
Iteration:   1680, Loss function: 4.037, Average Loss: 4.501, avg. samples / sec: 58551.68
Iteration:   1680, Loss function: 5.274, Average Loss: 4.451, avg. samples / sec: 58431.99
Iteration:   1680, Loss function: 4.603, Average Loss: 4.510, avg. samples / sec: 58588.24
Iteration:   1680, Loss function: 4.970, Average Loss: 4.508, avg. samples / sec: 58301.05
Iteration:   1680, Loss function: 4.306, Average Loss: 4.534, avg. samples / sec: 58256.73
Iteration:   1680, Loss function: 5.822, Average Loss: 4.480, avg. samples / sec: 59169.92
Iteration:   1680, Loss function: 4.183, Average Loss: 4.515, avg. samples / sec: 58217.36
Iteration:   1680, Loss function: 4.542, Average Loss: 4.516, avg. samples / sec: 58267.18
Iteration:   1680, Loss function: 3.882, Average Loss: 4.526, avg. samples / sec: 58349.23
Iteration:   1680, Loss function: 4.660, Average Loss: 4.478, avg. samples / sec: 58220.67
Iteration:   1700, Loss function: 4.326, Average Loss: 4.473, avg. samples / sec: 57922.45
Iteration:   1700, Loss function: 3.628, Average Loss: 4.494, avg. samples / sec: 57680.37
Iteration:   1700, Loss function: 4.548, Average Loss: 4.508, avg. samples / sec: 57706.40
Iteration:   1700, Loss function: 4.382, Average Loss: 4.504, avg. samples / sec: 57588.59
Iteration:   1700, Loss function: 4.509, Average Loss: 4.504, avg. samples / sec: 57742.39
Iteration:   1700, Loss function: 4.928, Average Loss: 4.539, avg. samples / sec: 57785.83
Iteration:   1700, Loss function: 3.930, Average Loss: 4.494, avg. samples / sec: 57634.52
Iteration:   1700, Loss function: 3.121, Average Loss: 4.511, avg. samples / sec: 57827.61
Iteration:   1700, Loss function: 4.998, Average Loss: 4.502, avg. samples / sec: 57623.42
Iteration:   1700, Loss function: 3.647, Average Loss: 4.502, avg. samples / sec: 57555.41
Iteration:   1700, Loss function: 3.433, Average Loss: 4.508, avg. samples / sec: 57781.69
Iteration:   1700, Loss function: 4.470, Average Loss: 4.522, avg. samples / sec: 57818.50
Iteration:   1700, Loss function: 4.674, Average Loss: 4.476, avg. samples / sec: 57880.44
Iteration:   1700, Loss function: 3.105, Average Loss: 4.448, avg. samples / sec: 57579.27
Iteration:   1700, Loss function: 3.823, Average Loss: 4.510, avg. samples / sec: 57531.82
Iteration:   1720, Loss function: 3.660, Average Loss: 4.497, avg. samples / sec: 56653.76
Iteration:   1720, Loss function: 3.002, Average Loss: 4.485, avg. samples / sec: 56475.32
Iteration:   1720, Loss function: 4.737, Average Loss: 4.474, avg. samples / sec: 56643.77
Iteration:   1720, Loss function: 3.953, Average Loss: 4.519, avg. samples / sec: 56627.90
Iteration:   1720, Loss function: 4.587, Average Loss: 4.452, avg. samples / sec: 56637.16
Iteration:   1720, Loss function: 4.954, Average Loss: 4.539, avg. samples / sec: 56498.70
Iteration:   1720, Loss function: 4.847, Average Loss: 4.494, avg. samples / sec: 56525.31
Iteration:   1720, Loss function: 4.855, Average Loss: 4.503, avg. samples / sec: 56417.32
Iteration:   1720, Loss function: 2.949, Average Loss: 4.510, avg. samples / sec: 56555.02
Iteration:   1720, Loss function: 3.777, Average Loss: 4.510, avg. samples / sec: 56605.52
Iteration:   1720, Loss function: 3.721, Average Loss: 4.509, avg. samples / sec: 56500.81
Iteration:   1720, Loss function: 3.616, Average Loss: 4.506, avg. samples / sec: 56320.22
Iteration:   1720, Loss function: 4.693, Average Loss: 4.502, avg. samples / sec: 56432.71
Iteration:   1720, Loss function: 4.267, Average Loss: 4.476, avg. samples / sec: 56254.14
Iteration:   1720, Loss function: 4.062, Average Loss: 4.505, avg. samples / sec: 56281.35
Iteration:   1740, Loss function: 3.454, Average Loss: 4.508, avg. samples / sec: 60541.44
Iteration:   1740, Loss function: 4.233, Average Loss: 4.507, avg. samples / sec: 60363.89
Iteration:   1740, Loss function: 4.180, Average Loss: 4.501, avg. samples / sec: 60426.08
Iteration:   1740, Loss function: 4.396, Average Loss: 4.538, avg. samples / sec: 60290.47
Iteration:   1740, Loss function: 4.416, Average Loss: 4.509, avg. samples / sec: 60340.29
Iteration:   1740, Loss function: 5.164, Average Loss: 4.485, avg. samples / sec: 60211.55
Iteration:   1740, Loss function: 3.526, Average Loss: 4.503, avg. samples / sec: 60367.46
Iteration:   1740, Loss function: 4.000, Average Loss: 4.450, avg. samples / sec: 60171.75
Iteration:   1740, Loss function: 4.442, Average Loss: 4.503, avg. samples / sec: 60195.37
Iteration:   1740, Loss function: 4.789, Average Loss: 4.498, avg. samples / sec: 59961.43
Iteration:   1740, Loss function: 5.588, Average Loss: 4.502, avg. samples / sec: 60052.57
Iteration:   1740, Loss function: 6.545, Average Loss: 4.485, avg. samples / sec: 60210.03
Iteration:   1740, Loss function: 5.529, Average Loss: 4.518, avg. samples / sec: 59924.08
Iteration:   1740, Loss function: 4.293, Average Loss: 4.477, avg. samples / sec: 59908.57
Iteration:   1740, Loss function: 4.886, Average Loss: 4.505, avg. samples / sec: 59858.29
:::MLL 1558639216.340 epoch_stop: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 819}}
:::MLL 1558639216.340 epoch_start: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 673}}
Iteration:   1760, Loss function: 4.515, Average Loss: 4.499, avg. samples / sec: 59162.12
Iteration:   1760, Loss function: 5.185, Average Loss: 4.498, avg. samples / sec: 58683.65
Iteration:   1760, Loss function: 3.669, Average Loss: 4.482, avg. samples / sec: 58651.98
Iteration:   1760, Loss function: 4.915, Average Loss: 4.504, avg. samples / sec: 58631.55
Iteration:   1760, Loss function: 4.495, Average Loss: 4.495, avg. samples / sec: 58776.80
Iteration:   1760, Loss function: 4.810, Average Loss: 4.508, avg. samples / sec: 58537.08
Iteration:   1760, Loss function: 4.406, Average Loss: 4.516, avg. samples / sec: 58863.61
Iteration:   1760, Loss function: 4.165, Average Loss: 4.452, avg. samples / sec: 58641.99
Iteration:   1760, Loss function: 4.108, Average Loss: 4.503, avg. samples / sec: 58775.06
Iteration:   1760, Loss function: 4.856, Average Loss: 4.478, avg. samples / sec: 58838.99
Iteration:   1760, Loss function: 3.776, Average Loss: 4.481, avg. samples / sec: 58758.67
Iteration:   1760, Loss function: 4.898, Average Loss: 4.533, avg. samples / sec: 58489.03
Iteration:   1760, Loss function: 4.197, Average Loss: 4.502, avg. samples / sec: 58578.72
Iteration:   1760, Loss function: 3.834, Average Loss: 4.497, avg. samples / sec: 58441.83
Iteration:   1760, Loss function: 5.088, Average Loss: 4.507, avg. samples / sec: 58397.80
Iteration:   1780, Loss function: 3.857, Average Loss: 4.499, avg. samples / sec: 58996.90
Iteration:   1780, Loss function: 4.570, Average Loss: 4.500, avg. samples / sec: 59146.28
Iteration:   1780, Loss function: 3.303, Average Loss: 4.473, avg. samples / sec: 59038.45
Iteration:   1780, Loss function: 4.213, Average Loss: 4.502, avg. samples / sec: 58932.39
Iteration:   1780, Loss function: 4.222, Average Loss: 4.495, avg. samples / sec: 58980.06
Iteration:   1780, Loss function: 3.873, Average Loss: 4.479, avg. samples / sec: 58906.38
Iteration:   1780, Loss function: 4.961, Average Loss: 4.501, avg. samples / sec: 58979.00
Iteration:   1780, Loss function: 4.559, Average Loss: 4.504, avg. samples / sec: 58973.23
Iteration:   1780, Loss function: 3.613, Average Loss: 4.476, avg. samples / sec: 58996.88
Iteration:   1780, Loss function: 3.736, Average Loss: 4.494, avg. samples / sec: 59042.60
Iteration:   1780, Loss function: 4.344, Average Loss: 4.529, avg. samples / sec: 58984.90
Iteration:   1780, Loss function: 5.117, Average Loss: 4.453, avg. samples / sec: 58921.67
Iteration:   1780, Loss function: 4.982, Average Loss: 4.498, avg. samples / sec: 58639.72
Iteration:   1780, Loss function: 3.454, Average Loss: 4.514, avg. samples / sec: 58761.83
Iteration:   1780, Loss function: 4.608, Average Loss: 4.512, avg. samples / sec: 58806.11
Iteration:   1800, Loss function: 4.768, Average Loss: 4.499, avg. samples / sec: 59684.61
Iteration:   1800, Loss function: 4.768, Average Loss: 4.492, avg. samples / sec: 59764.03
Iteration:   1800, Loss function: 3.890, Average Loss: 4.500, avg. samples / sec: 59706.31
Iteration:   1800, Loss function: 3.566, Average Loss: 4.497, avg. samples / sec: 59579.37
Iteration:   1800, Loss function: 5.210, Average Loss: 4.520, avg. samples / sec: 59959.24
Iteration:   1800, Loss function: 3.503, Average Loss: 4.532, avg. samples / sec: 59670.61
Iteration:   1800, Loss function: 4.253, Average Loss: 4.514, avg. samples / sec: 59815.22
Iteration:   1800, Loss function: 4.782, Average Loss: 4.497, avg. samples / sec: 59572.62
Iteration:   1800, Loss function: 4.088, Average Loss: 4.473, avg. samples / sec: 59543.77
Iteration:   1800, Loss function: 4.542, Average Loss: 4.501, avg. samples / sec: 59578.06
Iteration:   1800, Loss function: 3.722, Average Loss: 4.484, avg. samples / sec: 59544.65
Iteration:   1800, Loss function: 4.300, Average Loss: 4.491, avg. samples / sec: 59535.92
Iteration:   1800, Loss function: 4.853, Average Loss: 4.449, avg. samples / sec: 59590.68
Iteration:   1800, Loss function: 3.793, Average Loss: 4.495, avg. samples / sec: 59715.57
Iteration:   1800, Loss function: 4.354, Average Loss: 4.476, avg. samples / sec: 59462.30
:::MLL 1558639218.328 epoch_stop: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 819}}
:::MLL 1558639218.329 epoch_start: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 673}}
Iteration:   1820, Loss function: 4.746, Average Loss: 4.477, avg. samples / sec: 59741.92
Iteration:   1820, Loss function: 4.687, Average Loss: 4.496, avg. samples / sec: 59558.14
Iteration:   1820, Loss function: 4.894, Average Loss: 4.494, avg. samples / sec: 59440.43
Iteration:   1820, Loss function: 5.062, Average Loss: 4.502, avg. samples / sec: 59568.84
Iteration:   1820, Loss function: 5.021, Average Loss: 4.491, avg. samples / sec: 59388.80
Iteration:   1820, Loss function: 4.263, Average Loss: 4.532, avg. samples / sec: 59507.26
Iteration:   1820, Loss function: 2.752, Average Loss: 4.486, avg. samples / sec: 59534.51
Iteration:   1820, Loss function: 4.704, Average Loss: 4.495, avg. samples / sec: 59547.34
Iteration:   1820, Loss function: 4.349, Average Loss: 4.493, avg. samples / sec: 59397.86
Iteration:   1820, Loss function: 4.247, Average Loss: 4.495, avg. samples / sec: 59203.48
Iteration:   1820, Loss function: 4.005, Average Loss: 4.507, avg. samples / sec: 59346.59
Iteration:   1820, Loss function: 5.007, Average Loss: 4.451, avg. samples / sec: 59408.23
Iteration:   1820, Loss function: 4.981, Average Loss: 4.521, avg. samples / sec: 59288.14
Iteration:   1820, Loss function: 3.886, Average Loss: 4.484, avg. samples / sec: 59363.13
Iteration:   1820, Loss function: 4.161, Average Loss: 4.472, avg. samples / sec: 59179.36
Iteration:   1840, Loss function: 3.737, Average Loss: 4.489, avg. samples / sec: 59685.39
Iteration:   1840, Loss function: 5.146, Average Loss: 4.498, avg. samples / sec: 59484.21
Iteration:   1840, Loss function: 2.879, Average Loss: 4.491, avg. samples / sec: 59451.13
Iteration:   1840, Loss function: 4.157, Average Loss: 4.488, avg. samples / sec: 59563.43
Iteration:   1840, Loss function: 4.893, Average Loss: 4.493, avg. samples / sec: 59486.74
Iteration:   1840, Loss function: 4.072, Average Loss: 4.467, avg. samples / sec: 59811.69
Iteration:   1840, Loss function: 4.170, Average Loss: 4.490, avg. samples / sec: 59469.62
Iteration:   1840, Loss function: 4.043, Average Loss: 4.477, avg. samples / sec: 59578.38
Iteration:   1840, Loss function: 4.572, Average Loss: 4.516, avg. samples / sec: 59600.83
Iteration:   1840, Loss function: 5.423, Average Loss: 4.491, avg. samples / sec: 59405.72
Iteration:   1840, Loss function: 4.769, Average Loss: 4.446, avg. samples / sec: 59505.23
Iteration:   1840, Loss function: 5.235, Average Loss: 4.489, avg. samples / sec: 59313.94
Iteration:   1840, Loss function: 4.464, Average Loss: 4.472, avg. samples / sec: 59096.01
Iteration:   1840, Loss function: 3.632, Average Loss: 4.523, avg. samples / sec: 59168.31
Iteration:   1840, Loss function: 4.101, Average Loss: 4.507, avg. samples / sec: 59298.14
Iteration:   1860, Loss function: 5.791, Average Loss: 4.491, avg. samples / sec: 59881.23
Iteration:   1860, Loss function: 2.878, Average Loss: 4.481, avg. samples / sec: 59994.51
Iteration:   1860, Loss function: 4.707, Average Loss: 4.485, avg. samples / sec: 59750.15
Iteration:   1860, Loss function: 3.578, Average Loss: 4.465, avg. samples / sec: 59778.18
Iteration:   1860, Loss function: 4.645, Average Loss: 4.504, avg. samples / sec: 60107.18
Iteration:   1860, Loss function: 4.465, Average Loss: 4.490, avg. samples / sec: 59766.01
Iteration:   1860, Loss function: 4.744, Average Loss: 4.511, avg. samples / sec: 59786.55
Iteration:   1860, Loss function: 3.463, Average Loss: 4.488, avg. samples / sec: 59685.72
Iteration:   1860, Loss function: 5.549, Average Loss: 4.473, avg. samples / sec: 59783.33
Iteration:   1860, Loss function: 4.117, Average Loss: 4.467, avg. samples / sec: 59994.67
Iteration:   1860, Loss function: 5.144, Average Loss: 4.488, avg. samples / sec: 59698.21
Iteration:   1860, Loss function: 4.122, Average Loss: 4.441, avg. samples / sec: 59787.36
Iteration:   1860, Loss function: 4.792, Average Loss: 4.495, avg. samples / sec: 59611.62
Iteration:   1860, Loss function: 3.577, Average Loss: 4.485, avg. samples / sec: 59612.38
Iteration:   1860, Loss function: 4.233, Average Loss: 4.519, avg. samples / sec: 59905.41
Iteration:   1880, Loss function: 4.960, Average Loss: 4.474, avg. samples / sec: 60189.74
Iteration:   1880, Loss function: 4.295, Average Loss: 4.468, avg. samples / sec: 60310.57
Iteration:   1880, Loss function: 4.174, Average Loss: 4.485, avg. samples / sec: 60221.79
Iteration:   1880, Loss function: 3.821, Average Loss: 4.459, avg. samples / sec: 60149.97
Iteration:   1880, Loss function: 5.963, Average Loss: 4.487, avg. samples / sec: 60029.68
Iteration:   1880, Loss function: 4.910, Average Loss: 4.486, avg. samples / sec: 60208.72
Iteration:   1880, Loss function: 4.183, Average Loss: 4.488, avg. samples / sec: 60154.67
Iteration:   1880, Loss function: 4.712, Average Loss: 4.483, avg. samples / sec: 60005.47
Iteration:   1880, Loss function: 4.433, Average Loss: 4.499, avg. samples / sec: 60070.80
Iteration:   1880, Loss function: 3.241, Average Loss: 4.477, avg. samples / sec: 60169.21
Iteration:   1880, Loss function: 3.265, Average Loss: 4.439, avg. samples / sec: 60140.37
Iteration:   1880, Loss function: 4.887, Average Loss: 4.507, avg. samples / sec: 60052.29
Iteration:   1880, Loss function: 3.701, Average Loss: 4.493, avg. samples / sec: 60112.95
Iteration:   1880, Loss function: 5.142, Average Loss: 4.512, avg. samples / sec: 60144.53
Iteration:   1880, Loss function: 3.861, Average Loss: 4.470, avg. samples / sec: 59968.88
:::MLL 1558639220.297 epoch_stop: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 819}}
:::MLL 1558639220.298 epoch_start: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 673}}
Iteration:   1900, Loss function: 4.251, Average Loss: 4.453, avg. samples / sec: 57723.52
Iteration:   1900, Loss function: 3.618, Average Loss: 4.479, avg. samples / sec: 57619.20
Iteration:   1900, Loss function: 3.189, Average Loss: 4.464, avg. samples / sec: 57586.64
Iteration:   1900, Loss function: 3.793, Average Loss: 4.470, avg. samples / sec: 57735.13
Iteration:   1900, Loss function: 4.120, Average Loss: 4.478, avg. samples / sec: 57645.93
Iteration:   1900, Loss function: 3.824, Average Loss: 4.483, avg. samples / sec: 57642.39
Iteration:   1900, Loss function: 3.908, Average Loss: 4.484, avg. samples / sec: 57599.80
Iteration:   1900, Loss function: 5.001, Average Loss: 4.498, avg. samples / sec: 57726.97
Iteration:   1900, Loss function: 3.329, Average Loss: 4.492, avg. samples / sec: 57668.13
Iteration:   1900, Loss function: 4.762, Average Loss: 4.478, avg. samples / sec: 57646.64
Iteration:   1900, Loss function: 3.579, Average Loss: 4.438, avg. samples / sec: 57683.77
Iteration:   1900, Loss function: 5.305, Average Loss: 4.463, avg. samples / sec: 57791.81
Iteration:   1900, Loss function: 4.238, Average Loss: 4.494, avg. samples / sec: 57696.95
Iteration:   1900, Loss function: 4.900, Average Loss: 4.472, avg. samples / sec: 57420.78
Iteration:   1900, Loss function: 5.113, Average Loss: 4.506, avg. samples / sec: 57669.99
Iteration:   1920, Loss function: 3.738, Average Loss: 4.493, avg. samples / sec: 60987.39
Iteration:   1920, Loss function: 5.748, Average Loss: 4.469, avg. samples / sec: 60978.18
Iteration:   1920, Loss function: 3.776, Average Loss: 4.480, avg. samples / sec: 60857.71
Iteration:   1920, Loss function: 3.391, Average Loss: 4.485, avg. samples / sec: 60845.17
Iteration:   1920, Loss function: 4.247, Average Loss: 4.460, avg. samples / sec: 60864.43
Iteration:   1920, Loss function: 3.567, Average Loss: 4.468, avg. samples / sec: 60779.46
Iteration:   1920, Loss function: 6.159, Average Loss: 4.476, avg. samples / sec: 60719.84
Iteration:   1920, Loss function: 3.224, Average Loss: 4.476, avg. samples / sec: 60777.94
Iteration:   1920, Loss function: 2.818, Average Loss: 4.445, avg. samples / sec: 60623.04
Iteration:   1920, Loss function: 4.621, Average Loss: 4.461, avg. samples / sec: 60680.23
Iteration:   1920, Loss function: 4.520, Average Loss: 4.476, avg. samples / sec: 60751.40
Iteration:   1920, Loss function: 4.310, Average Loss: 4.435, avg. samples / sec: 60765.34
Iteration:   1920, Loss function: 3.895, Average Loss: 4.493, avg. samples / sec: 60672.89
Iteration:   1920, Loss function: 4.524, Average Loss: 4.478, avg. samples / sec: 60656.59
Iteration:   1920, Loss function: 4.300, Average Loss: 4.500, avg. samples / sec: 60679.13
Iteration:   1940, Loss function: 4.125, Average Loss: 4.472, avg. samples / sec: 56324.20
Iteration:   1940, Loss function: 4.115, Average Loss: 4.500, avg. samples / sec: 56576.66
Iteration:   1940, Loss function: 3.920, Average Loss: 4.488, avg. samples / sec: 56406.49
Iteration:   1940, Loss function: 4.787, Average Loss: 4.477, avg. samples / sec: 56269.51
Iteration:   1940, Loss function: 4.153, Average Loss: 4.477, avg. samples / sec: 56273.96
Iteration:   1940, Loss function: 3.393, Average Loss: 4.488, avg. samples / sec: 56150.77
Iteration:   1940, Loss function: 4.678, Average Loss: 4.474, avg. samples / sec: 56367.48
Iteration:   1940, Loss function: 3.542, Average Loss: 4.460, avg. samples / sec: 56282.56
Iteration:   1940, Loss function: 5.015, Average Loss: 4.434, avg. samples / sec: 56281.10
Iteration:   1940, Loss function: 5.081, Average Loss: 4.476, avg. samples / sec: 56156.68
Iteration:   1940, Loss function: 4.235, Average Loss: 4.454, avg. samples / sec: 56160.89
Iteration:   1940, Loss function: 4.497, Average Loss: 4.465, avg. samples / sec: 56169.68
Iteration:   1940, Loss function: 4.058, Average Loss: 4.479, avg. samples / sec: 56114.51
Iteration:   1940, Loss function: 5.713, Average Loss: 4.444, avg. samples / sec: 56165.92
Iteration:   1940, Loss function: 4.201, Average Loss: 4.474, avg. samples / sec: 56163.44
:::MLL 1558639222.307 epoch_stop: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 819}}
:::MLL 1558639222.307 epoch_start: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 673}}
Iteration:   1960, Loss function: 5.138, Average Loss: 4.475, avg. samples / sec: 60224.69
Iteration:   1960, Loss function: 4.190, Average Loss: 4.482, avg. samples / sec: 60157.60
Iteration:   1960, Loss function: 4.470, Average Loss: 4.464, avg. samples / sec: 59966.82
Iteration:   1960, Loss function: 4.031, Average Loss: 4.472, avg. samples / sec: 60113.33
Iteration:   1960, Loss function: 4.605, Average Loss: 4.475, avg. samples / sec: 60188.14
Iteration:   1960, Loss function: 3.593, Average Loss: 4.430, avg. samples / sec: 60135.01
Iteration:   1960, Loss function: 4.839, Average Loss: 4.473, avg. samples / sec: 60088.34
Iteration:   1960, Loss function: 4.150, Average Loss: 4.451, avg. samples / sec: 60157.57
Iteration:   1960, Loss function: 4.522, Average Loss: 4.473, avg. samples / sec: 60229.69
Iteration:   1960, Loss function: 4.075, Average Loss: 4.440, avg. samples / sec: 60151.72
Iteration:   1960, Loss function: 3.899, Average Loss: 4.467, avg. samples / sec: 60104.41
Iteration:   1960, Loss function: 3.830, Average Loss: 4.493, avg. samples / sec: 59770.93
Iteration:   1960, Loss function: 3.913, Average Loss: 4.459, avg. samples / sec: 59901.49
Iteration:   1960, Loss function: 4.337, Average Loss: 4.475, avg. samples / sec: 59997.27
Iteration:   1960, Loss function: 4.379, Average Loss: 4.484, avg. samples / sec: 59793.98
Iteration:   1980, Loss function: 4.712, Average Loss: 4.450, avg. samples / sec: 55183.52
Iteration:   1980, Loss function: 3.989, Average Loss: 4.473, avg. samples / sec: 55101.08
Iteration:   1980, Loss function: 5.335, Average Loss: 4.470, avg. samples / sec: 54997.15
Iteration:   1980, Loss function: 3.648, Average Loss: 4.458, avg. samples / sec: 55066.27
Iteration:   1980, Loss function: 4.273, Average Loss: 4.427, avg. samples / sec: 55098.28
Iteration:   1980, Loss function: 4.184, Average Loss: 4.469, avg. samples / sec: 55085.64
Iteration:   1980, Loss function: 3.984, Average Loss: 4.469, avg. samples / sec: 55246.86
Iteration:   1980, Loss function: 4.581, Average Loss: 4.467, avg. samples / sec: 55085.70
Iteration:   1980, Loss function: 2.929, Average Loss: 4.484, avg. samples / sec: 55201.72
Iteration:   1980, Loss function: 4.471, Average Loss: 4.470, avg. samples / sec: 55012.10
Iteration:   1980, Loss function: 4.352, Average Loss: 4.482, avg. samples / sec: 55261.27
Iteration:   1980, Loss function: 3.803, Average Loss: 4.460, avg. samples / sec: 55195.13
Iteration:   1980, Loss function: 4.988, Average Loss: 4.468, avg. samples / sec: 55009.61
Iteration:   1980, Loss function: 3.374, Average Loss: 4.478, avg. samples / sec: 54876.31
Iteration:   1980, Loss function: 4.479, Average Loss: 4.435, avg. samples / sec: 54910.78
Iteration:   2000, Loss function: 4.521, Average Loss: 4.476, avg. samples / sec: 59076.07
Iteration:   2000, Loss function: 4.329, Average Loss: 4.466, avg. samples / sec: 59056.58
Iteration:   2000, Loss function: 4.164, Average Loss: 4.470, avg. samples / sec: 59123.45
Iteration:   2000, Loss function: 4.704, Average Loss: 4.428, avg. samples / sec: 59227.12
Iteration:   2000, Loss function: 4.095, Average Loss: 4.461, avg. samples / sec: 58904.18
Iteration:   2000, Loss function: 3.516, Average Loss: 4.446, avg. samples / sec: 58783.77
Iteration:   2000, Loss function: 4.461, Average Loss: 4.450, avg. samples / sec: 58856.39
Iteration:   2000, Loss function: 4.482, Average Loss: 4.419, avg. samples / sec: 58846.58
Iteration:   2000, Loss function: 2.809, Average Loss: 4.464, avg. samples / sec: 58855.82
Iteration:   2000, Loss function: 4.252, Average Loss: 4.467, avg. samples / sec: 58873.70
Iteration:   2000, Loss function: 3.975, Average Loss: 4.463, avg. samples / sec: 58786.15
Iteration:   2000, Loss function: 4.188, Average Loss: 4.479, avg. samples / sec: 58755.09
Iteration:   2000, Loss function: 3.987, Average Loss: 4.465, avg. samples / sec: 58733.99
Iteration:   2000, Loss function: 3.598, Average Loss: 4.461, avg. samples / sec: 58864.47
Iteration:   2000, Loss function: 4.598, Average Loss: 4.462, avg. samples / sec: 58722.48
Iteration:   2020, Loss function: 4.999, Average Loss: 4.443, avg. samples / sec: 57262.82
Iteration:   2020, Loss function: 5.754, Average Loss: 4.420, avg. samples / sec: 57249.33
Iteration:   2020, Loss function: 3.872, Average Loss: 4.454, avg. samples / sec: 57372.53
Iteration:   2020, Loss function: 4.456, Average Loss: 4.464, avg. samples / sec: 57216.37
Iteration:   2020, Loss function: 3.023, Average Loss: 4.466, avg. samples / sec: 57103.21
Iteration:   2020, Loss function: 3.730, Average Loss: 4.453, avg. samples / sec: 57115.39
Iteration:   2020, Loss function: 3.654, Average Loss: 4.445, avg. samples / sec: 57165.43
Iteration:   2020, Loss function: 4.639, Average Loss: 4.461, avg. samples / sec: 57194.01
Iteration:   2020, Loss function: 4.378, Average Loss: 4.456, avg. samples / sec: 57287.73
Iteration:   2020, Loss function: 4.256, Average Loss: 4.422, avg. samples / sec: 57053.86
Iteration:   2020, Loss function: 4.986, Average Loss: 4.471, avg. samples / sec: 56945.34
Iteration:   2020, Loss function: 3.263, Average Loss: 4.477, avg. samples / sec: 57214.21
Iteration:   2020, Loss function: 3.328, Average Loss: 4.460, avg. samples / sec: 57188.11
Iteration:   2020, Loss function: 4.071, Average Loss: 4.460, avg. samples / sec: 56875.66
Iteration:   2020, Loss function: 3.638, Average Loss: 4.458, avg. samples / sec: 56999.47
:::MLL 1558639224.354 epoch_stop: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 819}}
:::MLL 1558639224.354 epoch_start: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 673}}
Iteration:   2040, Loss function: 3.298, Average Loss: 4.413, avg. samples / sec: 58832.21
Iteration:   2040, Loss function: 4.241, Average Loss: 4.451, avg. samples / sec: 59146.93
Iteration:   2040, Loss function: 4.851, Average Loss: 4.472, avg. samples / sec: 59005.87
Iteration:   2040, Loss function: 4.018, Average Loss: 4.461, avg. samples / sec: 58881.49
Iteration:   2040, Loss function: 3.054, Average Loss: 4.439, avg. samples / sec: 58896.90
Iteration:   2040, Loss function: 5.069, Average Loss: 4.437, avg. samples / sec: 58778.89
Iteration:   2040, Loss function: 3.994, Average Loss: 4.447, avg. samples / sec: 58862.29
Iteration:   2040, Loss function: 3.789, Average Loss: 4.459, avg. samples / sec: 58834.71
Iteration:   2040, Loss function: 5.264, Average Loss: 4.464, avg. samples / sec: 58927.31
Iteration:   2040, Loss function: 5.068, Average Loss: 4.453, avg. samples / sec: 58854.39
Iteration:   2040, Loss function: 4.594, Average Loss: 4.456, avg. samples / sec: 58838.89
Iteration:   2040, Loss function: 5.369, Average Loss: 4.454, avg. samples / sec: 59013.41
Iteration:   2040, Loss function: 3.121, Average Loss: 4.417, avg. samples / sec: 58755.48
Iteration:   2040, Loss function: 3.820, Average Loss: 4.448, avg. samples / sec: 58658.44
Iteration:   2040, Loss function: 3.174, Average Loss: 4.456, avg. samples / sec: 58842.80
Iteration:   2060, Loss function: 5.042, Average Loss: 4.441, avg. samples / sec: 59031.15
Iteration:   2060, Loss function: 4.230, Average Loss: 4.433, avg. samples / sec: 58837.44
Iteration:   2060, Loss function: 4.962, Average Loss: 4.442, avg. samples / sec: 58808.22
Iteration:   2060, Loss function: 4.487, Average Loss: 4.470, avg. samples / sec: 58761.32
Iteration:   2060, Loss function: 3.686, Average Loss: 4.453, avg. samples / sec: 58811.39
Iteration:   2060, Loss function: 4.959, Average Loss: 4.459, avg. samples / sec: 58810.24
Iteration:   2060, Loss function: 4.194, Average Loss: 4.410, avg. samples / sec: 58737.51
Iteration:   2060, Loss function: 6.155, Average Loss: 4.462, avg. samples / sec: 58747.65
Iteration:   2060, Loss function: 4.522, Average Loss: 4.446, avg. samples / sec: 58719.94
Iteration:   2060, Loss function: 4.318, Average Loss: 4.411, avg. samples / sec: 58895.94
Iteration:   2060, Loss function: 5.550, Average Loss: 4.448, avg. samples / sec: 58900.81
Iteration:   2060, Loss function: 4.615, Average Loss: 4.451, avg. samples / sec: 58765.46
Iteration:   2060, Loss function: 3.949, Average Loss: 4.449, avg. samples / sec: 58703.55
Iteration:   2060, Loss function: 4.528, Average Loss: 4.433, avg. samples / sec: 58532.51
Iteration:   2060, Loss function: 3.195, Average Loss: 4.446, avg. samples / sec: 58603.71
Iteration:   2080, Loss function: 4.951, Average Loss: 4.451, avg. samples / sec: 59382.92
Iteration:   2080, Loss function: 4.780, Average Loss: 4.465, avg. samples / sec: 59353.46
Iteration:   2080, Loss function: 3.780, Average Loss: 4.442, avg. samples / sec: 59358.91
Iteration:   2080, Loss function: 5.752, Average Loss: 4.412, avg. samples / sec: 59357.43
Iteration:   2080, Loss function: 3.170, Average Loss: 4.427, avg. samples / sec: 59548.55
Iteration:   2080, Loss function: 3.266, Average Loss: 4.427, avg. samples / sec: 59186.50
Iteration:   2080, Loss function: 4.226, Average Loss: 4.445, avg. samples / sec: 59377.34
Iteration:   2080, Loss function: 3.955, Average Loss: 4.455, avg. samples / sec: 59233.17
Iteration:   2080, Loss function: 4.192, Average Loss: 4.451, avg. samples / sec: 59288.26
Iteration:   2080, Loss function: 4.145, Average Loss: 4.409, avg. samples / sec: 59193.09
Iteration:   2080, Loss function: 5.549, Average Loss: 4.440, avg. samples / sec: 59078.52
Iteration:   2080, Loss function: 4.063, Average Loss: 4.443, avg. samples / sec: 59209.47
Iteration:   2080, Loss function: 3.505, Average Loss: 4.445, avg. samples / sec: 59380.34
Iteration:   2080, Loss function: 4.475, Average Loss: 4.449, avg. samples / sec: 59097.82
Iteration:   2080, Loss function: 3.093, Average Loss: 4.438, avg. samples / sec: 58936.01
:::MLL 1558639226.360 epoch_stop: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 819}}
:::MLL 1558639226.361 epoch_start: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 673}}
Iteration:   2100, Loss function: 2.777, Average Loss: 4.407, avg. samples / sec: 58557.56
Iteration:   2100, Loss function: 5.018, Average Loss: 4.459, avg. samples / sec: 58418.31
Iteration:   2100, Loss function: 4.198, Average Loss: 4.433, avg. samples / sec: 58809.18
Iteration:   2100, Loss function: 3.642, Average Loss: 4.445, avg. samples / sec: 58525.90
Iteration:   2100, Loss function: 4.536, Average Loss: 4.429, avg. samples / sec: 58357.73
Iteration:   2100, Loss function: 3.465, Average Loss: 4.445, avg. samples / sec: 58262.34
Iteration:   2100, Loss function: 4.119, Average Loss: 4.440, avg. samples / sec: 58303.12
Iteration:   2100, Loss function: 4.332, Average Loss: 4.438, avg. samples / sec: 58533.36
Iteration:   2100, Loss function: 3.902, Average Loss: 4.401, avg. samples / sec: 58372.21
Iteration:   2100, Loss function: 3.850, Average Loss: 4.446, avg. samples / sec: 58345.60
Iteration:   2100, Loss function: 4.253, Average Loss: 4.437, avg. samples / sec: 58384.11
Iteration:   2100, Loss function: 4.410, Average Loss: 4.420, avg. samples / sec: 58312.55
Iteration:   2100, Loss function: 4.078, Average Loss: 4.436, avg. samples / sec: 58391.00
Iteration:   2100, Loss function: 4.025, Average Loss: 4.445, avg. samples / sec: 58330.75
Iteration:   2100, Loss function: 5.064, Average Loss: 4.436, avg. samples / sec: 58114.13
Iteration:   2120, Loss function: 4.865, Average Loss: 4.430, avg. samples / sec: 58660.35
Iteration:   2120, Loss function: 5.370, Average Loss: 4.446, avg. samples / sec: 58740.77
Iteration:   2120, Loss function: 3.773, Average Loss: 4.440, avg. samples / sec: 58633.99
Iteration:   2120, Loss function: 3.796, Average Loss: 4.435, avg. samples / sec: 58647.00
Iteration:   2120, Loss function: 3.991, Average Loss: 4.390, avg. samples / sec: 58726.25
Iteration:   2120, Loss function: 3.398, Average Loss: 4.444, avg. samples / sec: 58709.79
Iteration:   2120, Loss function: 5.477, Average Loss: 4.431, avg. samples / sec: 58612.63
Iteration:   2120, Loss function: 4.066, Average Loss: 4.431, avg. samples / sec: 58694.65
Iteration:   2120, Loss function: 4.824, Average Loss: 4.403, avg. samples / sec: 58359.62
Iteration:   2120, Loss function: 4.443, Average Loss: 4.458, avg. samples / sec: 58436.55
Iteration:   2120, Loss function: 4.105, Average Loss: 4.433, avg. samples / sec: 58555.76
Iteration:   2120, Loss function: 4.981, Average Loss: 4.417, avg. samples / sec: 58612.75
Iteration:   2120, Loss function: 3.599, Average Loss: 4.433, avg. samples / sec: 58820.05
Iteration:   2120, Loss function: 3.440, Average Loss: 4.431, avg. samples / sec: 58548.81
Iteration:   2120, Loss function: 5.017, Average Loss: 4.447, avg. samples / sec: 58489.22
Iteration:   2140, Loss function: 3.015, Average Loss: 4.449, avg. samples / sec: 59792.26
Iteration:   2140, Loss function: 4.761, Average Loss: 4.437, avg. samples / sec: 59737.28
Iteration:   2140, Loss function: 3.757, Average Loss: 4.393, avg. samples / sec: 59693.05
Iteration:   2140, Loss function: 3.821, Average Loss: 4.427, avg. samples / sec: 59555.85
Iteration:   2140, Loss function: 4.623, Average Loss: 4.442, avg. samples / sec: 59950.85
Iteration:   2140, Loss function: 4.569, Average Loss: 4.396, avg. samples / sec: 59714.50
Iteration:   2140, Loss function: 4.886, Average Loss: 4.425, avg. samples / sec: 59598.97
Iteration:   2140, Loss function: 5.911, Average Loss: 4.430, avg. samples / sec: 59783.45
Iteration:   2140, Loss function: 3.902, Average Loss: 4.428, avg. samples / sec: 59680.01
Iteration:   2140, Loss function: 4.077, Average Loss: 4.428, avg. samples / sec: 59570.43
Iteration:   2140, Loss function: 3.693, Average Loss: 4.441, avg. samples / sec: 59444.89
Iteration:   2140, Loss function: 3.512, Average Loss: 4.432, avg. samples / sec: 59644.42
Iteration:   2140, Loss function: 3.688, Average Loss: 4.419, avg. samples / sec: 59538.24
Iteration:   2140, Loss function: 4.147, Average Loss: 4.410, avg. samples / sec: 59592.69
Iteration:   2140, Loss function: 4.444, Average Loss: 4.441, avg. samples / sec: 59397.39
Iteration:   2160, Loss function: 5.554, Average Loss: 4.423, avg. samples / sec: 60270.72
Iteration:   2160, Loss function: 4.675, Average Loss: 4.439, avg. samples / sec: 60093.62
Iteration:   2160, Loss function: 3.569, Average Loss: 4.424, avg. samples / sec: 60223.41
Iteration:   2160, Loss function: 4.274, Average Loss: 4.393, avg. samples / sec: 60074.72
Iteration:   2160, Loss function: 4.311, Average Loss: 4.396, avg. samples / sec: 60011.50
Iteration:   2160, Loss function: 5.225, Average Loss: 4.442, avg. samples / sec: 59970.90
Iteration:   2160, Loss function: 3.994, Average Loss: 4.445, avg. samples / sec: 59920.69
Iteration:   2160, Loss function: 5.108, Average Loss: 4.440, avg. samples / sec: 60089.62
Iteration:   2160, Loss function: 3.623, Average Loss: 4.403, avg. samples / sec: 60145.45
Iteration:   2160, Loss function: 3.246, Average Loss: 4.434, avg. samples / sec: 60097.11
Iteration:   2160, Loss function: 4.083, Average Loss: 4.412, avg. samples / sec: 60131.90
Iteration:   2160, Loss function: 3.340, Average Loss: 4.432, avg. samples / sec: 60003.20
Iteration:   2160, Loss function: 5.285, Average Loss: 4.421, avg. samples / sec: 59894.82
Iteration:   2160, Loss function: 3.553, Average Loss: 4.441, avg. samples / sec: 60111.33
Iteration:   2160, Loss function: 4.166, Average Loss: 4.426, avg. samples / sec: 59791.95
:::MLL 1558639228.339 epoch_stop: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 819}}
:::MLL 1558639228.340 epoch_start: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 673}}
Iteration:   2180, Loss function: 3.782, Average Loss: 4.397, avg. samples / sec: 58455.70
Iteration:   2180, Loss function: 5.188, Average Loss: 4.407, avg. samples / sec: 58425.21
Iteration:   2180, Loss function: 4.715, Average Loss: 4.437, avg. samples / sec: 58452.74
Iteration:   2180, Loss function: 4.529, Average Loss: 4.438, avg. samples / sec: 58332.80
Iteration:   2180, Loss function: 3.571, Average Loss: 4.394, avg. samples / sec: 58305.58
Iteration:   2180, Loss function: 4.152, Average Loss: 4.436, avg. samples / sec: 58147.48
Iteration:   2180, Loss function: 5.809, Average Loss: 4.417, avg. samples / sec: 58325.37
Iteration:   2180, Loss function: 5.291, Average Loss: 4.438, avg. samples / sec: 58301.36
Iteration:   2180, Loss function: 4.049, Average Loss: 4.434, avg. samples / sec: 58275.85
Iteration:   2180, Loss function: 4.087, Average Loss: 4.420, avg. samples / sec: 58488.64
Iteration:   2180, Loss function: 5.171, Average Loss: 4.423, avg. samples / sec: 58085.48
Iteration:   2180, Loss function: 3.630, Average Loss: 4.436, avg. samples / sec: 58229.70
Iteration:   2180, Loss function: 4.179, Average Loss: 4.426, avg. samples / sec: 58245.94
Iteration:   2180, Loss function: 4.268, Average Loss: 4.424, avg. samples / sec: 58038.11
Iteration:   2180, Loss function: 3.821, Average Loss: 4.387, avg. samples / sec: 58015.46
Iteration:   2200, Loss function: 5.069, Average Loss: 4.433, avg. samples / sec: 60035.71
Iteration:   2200, Loss function: 3.914, Average Loss: 4.427, avg. samples / sec: 60008.16
Iteration:   2200, Loss function: 3.692, Average Loss: 4.384, avg. samples / sec: 59970.90
Iteration:   2200, Loss function: 3.290, Average Loss: 4.424, avg. samples / sec: 60084.37
Iteration:   2200, Loss function: 3.603, Average Loss: 4.435, avg. samples / sec: 60066.09
Iteration:   2200, Loss function: 3.164, Average Loss: 4.426, avg. samples / sec: 59966.18
Iteration:   2200, Loss function: 3.632, Average Loss: 4.404, avg. samples / sec: 59832.09
Iteration:   2200, Loss function: 4.146, Average Loss: 4.394, avg. samples / sec: 59787.97
Iteration:   2200, Loss function: 3.737, Average Loss: 4.419, avg. samples / sec: 59971.23
Iteration:   2200, Loss function: 4.471, Average Loss: 4.412, avg. samples / sec: 59952.20
Iteration:   2200, Loss function: 4.097, Average Loss: 4.384, avg. samples / sec: 60048.63
Iteration:   2200, Loss function: 4.118, Average Loss: 4.425, avg. samples / sec: 59938.94
Iteration:   2200, Loss function: 3.264, Average Loss: 4.429, avg. samples / sec: 59754.73
Iteration:   2200, Loss function: 4.392, Average Loss: 4.414, avg. samples / sec: 59740.47
Iteration:   2200, Loss function: 4.062, Average Loss: 4.426, avg. samples / sec: 58454.82
Iteration:   2220, Loss function: 3.664, Average Loss: 4.419, avg. samples / sec: 58568.22
Iteration:   2220, Loss function: 4.717, Average Loss: 4.408, avg. samples / sec: 58560.07
Iteration:   2220, Loss function: 2.651, Average Loss: 4.375, avg. samples / sec: 58589.80
Iteration:   2220, Loss function: 5.071, Average Loss: 4.423, avg. samples / sec: 60042.01
Iteration:   2220, Loss function: 3.537, Average Loss: 4.423, avg. samples / sec: 58637.70
Iteration:   2220, Loss function: 5.721, Average Loss: 4.401, avg. samples / sec: 58493.69
Iteration:   2220, Loss function: 3.988, Average Loss: 4.412, avg. samples / sec: 58499.54
Iteration:   2220, Loss function: 4.728, Average Loss: 4.409, avg. samples / sec: 58672.58
Iteration:   2220, Loss function: 4.764, Average Loss: 4.429, avg. samples / sec: 58340.17
Iteration:   2220, Loss function: 3.926, Average Loss: 4.415, avg. samples / sec: 58398.38
Iteration:   2220, Loss function: 3.900, Average Loss: 4.391, avg. samples / sec: 58433.30
Iteration:   2220, Loss function: 5.145, Average Loss: 4.421, avg. samples / sec: 58399.06
Iteration:   2220, Loss function: 3.038, Average Loss: 4.383, avg. samples / sec: 58294.73
Iteration:   2220, Loss function: 4.836, Average Loss: 4.432, avg. samples / sec: 58256.46
Iteration:   2220, Loss function: 5.254, Average Loss: 4.417, avg. samples / sec: 58340.34
:::MLL 1558639230.341 epoch_stop: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 819}}
:::MLL 1558639230.342 epoch_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 673}}
Iteration:   2240, Loss function: 4.021, Average Loss: 4.418, avg. samples / sec: 58921.18
Iteration:   2240, Loss function: 4.570, Average Loss: 4.412, avg. samples / sec: 58575.18
Iteration:   2240, Loss function: 3.093, Average Loss: 4.420, avg. samples / sec: 58692.18
Iteration:   2240, Loss function: 3.686, Average Loss: 4.399, avg. samples / sec: 58704.63
Iteration:   2240, Loss function: 5.288, Average Loss: 4.425, avg. samples / sec: 58905.69
Iteration:   2240, Loss function: 5.626, Average Loss: 4.410, avg. samples / sec: 58708.20
Iteration:   2240, Loss function: 4.981, Average Loss: 4.414, avg. samples / sec: 58711.50
Iteration:   2240, Loss function: 3.508, Average Loss: 4.387, avg. samples / sec: 58689.76
Iteration:   2240, Loss function: 4.686, Average Loss: 4.375, avg. samples / sec: 58756.22
Iteration:   2240, Loss function: 4.248, Average Loss: 4.417, avg. samples / sec: 58581.93
Iteration:   2240, Loss function: 3.627, Average Loss: 4.404, avg. samples / sec: 58585.24
Iteration:   2240, Loss function: 4.816, Average Loss: 4.404, avg. samples / sec: 58510.35
Iteration:   2240, Loss function: 3.802, Average Loss: 4.412, avg. samples / sec: 58600.22
Iteration:   2240, Loss function: 2.349, Average Loss: 4.410, avg. samples / sec: 58758.84
Iteration:   2240, Loss function: 4.057, Average Loss: 4.371, avg. samples / sec: 58405.79
Iteration:   2260, Loss function: 4.063, Average Loss: 4.416, avg. samples / sec: 59115.62
Iteration:   2260, Loss function: 4.567, Average Loss: 4.409, avg. samples / sec: 59071.26
Iteration:   2260, Loss function: 5.491, Average Loss: 4.399, avg. samples / sec: 59151.27
Iteration:   2260, Loss function: 4.331, Average Loss: 4.392, avg. samples / sec: 58954.92
Iteration:   2260, Loss function: 3.622, Average Loss: 4.411, avg. samples / sec: 59181.13
Iteration:   2260, Loss function: 3.060, Average Loss: 4.410, avg. samples / sec: 58991.22
Iteration:   2260, Loss function: 4.045, Average Loss: 4.419, avg. samples / sec: 58937.05
Iteration:   2260, Loss function: 4.046, Average Loss: 4.411, avg. samples / sec: 58920.44
Iteration:   2260, Loss function: 4.156, Average Loss: 4.380, avg. samples / sec: 58975.69
Iteration:   2260, Loss function: 4.313, Average Loss: 4.414, avg. samples / sec: 58999.37
Iteration:   2260, Loss function: 4.980, Average Loss: 4.397, avg. samples / sec: 59040.72
Iteration:   2260, Loss function: 2.755, Average Loss: 4.368, avg. samples / sec: 59066.11
Iteration:   2260, Loss function: 4.048, Average Loss: 4.399, avg. samples / sec: 58933.08
Iteration:   2260, Loss function: 4.757, Average Loss: 4.413, avg. samples / sec: 58549.49
Iteration:   2260, Loss function: 3.737, Average Loss: 4.374, avg. samples / sec: 58676.35
Iteration:   2280, Loss function: 4.346, Average Loss: 4.406, avg. samples / sec: 59888.38
Iteration:   2280, Loss function: 2.971, Average Loss: 4.390, avg. samples / sec: 59894.23
Iteration:   2280, Loss function: 4.309, Average Loss: 4.367, avg. samples / sec: 59897.47
Iteration:   2280, Loss function: 3.584, Average Loss: 4.391, avg. samples / sec: 59661.04
Iteration:   2280, Loss function: 3.923, Average Loss: 4.398, avg. samples / sec: 59698.34
Iteration:   2280, Loss function: 4.150, Average Loss: 4.407, avg. samples / sec: 59526.90
Iteration:   2280, Loss function: 4.368, Average Loss: 4.404, avg. samples / sec: 59885.83
Iteration:   2280, Loss function: 4.516, Average Loss: 4.375, avg. samples / sec: 59688.43
Iteration:   2280, Loss function: 5.402, Average Loss: 4.398, avg. samples / sec: 59842.60
Iteration:   2280, Loss function: 4.290, Average Loss: 4.408, avg. samples / sec: 59619.95
Iteration:   2280, Loss function: 4.851, Average Loss: 4.415, avg. samples / sec: 59392.91
Iteration:   2280, Loss function: 4.498, Average Loss: 4.372, avg. samples / sec: 59958.17
Iteration:   2280, Loss function: 4.162, Average Loss: 4.414, avg. samples / sec: 59511.96
Iteration:   2280, Loss function: 3.915, Average Loss: 4.408, avg. samples / sec: 59444.04
Iteration:   2280, Loss function: 3.599, Average Loss: 4.387, avg. samples / sec: 59090.31
:::MLL 1558639231.662 eval_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.97 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.97 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.97 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.97 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.97 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.97 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.97 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.97 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.97 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.97 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.97 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.97 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.97 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.97 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.97 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.36s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.36s)
DONE (t=0.36s)
DONE (t=0.37s)
DONE (t=0.36s)
DONE (t=0.37s)
DONE (t=0.37s)
DONE (t=0.37s)
DONE (t=0.37s)
DONE (t=0.37s)
DONE (t=0.37s)
DONE (t=0.37s)
DONE (t=0.37s)
DONE (t=0.37s)
DONE (t=0.37s)
DONE (t=0.37s)
DONE (t=0.37s)
DONE (t=0.37s)
DONE (t=0.37s)
DONE (t=0.37s)
DONE (t=0.37s)
DONE (t=0.37s)
DONE (t=0.37s)
DONE (t=0.38s)
DONE (t=0.40s)
DONE (t=2.81s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.15655
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.29841
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.15276
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.03590
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.16808
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.24461
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.17244
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.25106
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.26442
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.06591
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.28094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.40100
Current AP: 0.15655 AP goal: 0.23000
:::MLL 1558639235.868 eval_accuracy: {"value": 0.15655011106789843, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 389}}
:::MLL 1558639235.908 eval_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 392}}
:::MLL 1558639235.917 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 804}}
:::MLL 1558639235.918 block_start: {"value": null, "metadata": {"first_epoch_num": 33, "epoch_count": 10.915354834308324, "file": "train.py", "lineno": 813}}
Iteration:   2300, Loss function: 3.214, Average Loss: 4.385, avg. samples / sec: 6774.94
Iteration:   2300, Loss function: 4.060, Average Loss: 4.388, avg. samples / sec: 6773.48
Iteration:   2300, Loss function: 4.775, Average Loss: 4.394, avg. samples / sec: 6773.92
Iteration:   2300, Loss function: 6.081, Average Loss: 4.412, avg. samples / sec: 6774.47
Iteration:   2300, Loss function: 4.430, Average Loss: 4.405, avg. samples / sec: 6776.70
Iteration:   2300, Loss function: 3.210, Average Loss: 4.379, avg. samples / sec: 6770.90
Iteration:   2300, Loss function: 3.286, Average Loss: 4.387, avg. samples / sec: 6773.36
Iteration:   2300, Loss function: 3.839, Average Loss: 4.364, avg. samples / sec: 6771.57
Iteration:   2300, Loss function: 3.941, Average Loss: 4.385, avg. samples / sec: 6779.94
Iteration:   2300, Loss function: 4.483, Average Loss: 4.367, avg. samples / sec: 6773.46
Iteration:   2300, Loss function: 4.633, Average Loss: 4.398, avg. samples / sec: 6769.87
Iteration:   2300, Loss function: 4.202, Average Loss: 4.406, avg. samples / sec: 6771.79
Iteration:   2300, Loss function: 3.884, Average Loss: 4.410, avg. samples / sec: 6773.66
Iteration:   2300, Loss function: 4.519, Average Loss: 4.406, avg. samples / sec: 6771.20
Iteration:   2300, Loss function: 5.208, Average Loss: 4.371, avg. samples / sec: 6770.32
:::MLL 1558639236.724 epoch_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 819}}
:::MLL 1558639236.724 epoch_start: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 673}}
Iteration:   2320, Loss function: 3.674, Average Loss: 4.379, avg. samples / sec: 58675.37
Iteration:   2320, Loss function: 3.402, Average Loss: 4.387, avg. samples / sec: 58759.67
Iteration:   2320, Loss function: 4.467, Average Loss: 4.390, avg. samples / sec: 58798.73
Iteration:   2320, Loss function: 4.333, Average Loss: 4.403, avg. samples / sec: 58812.47
Iteration:   2320, Loss function: 4.562, Average Loss: 4.376, avg. samples / sec: 58773.45
Iteration:   2320, Loss function: 4.277, Average Loss: 4.403, avg. samples / sec: 58683.95
Iteration:   2320, Loss function: 3.903, Average Loss: 4.370, avg. samples / sec: 58658.44
Iteration:   2320, Loss function: 3.854, Average Loss: 4.366, avg. samples / sec: 58886.76
Iteration:   2320, Loss function: 4.857, Average Loss: 4.363, avg. samples / sec: 58699.00
Iteration:   2320, Loss function: 5.033, Average Loss: 4.366, avg. samples / sec: 58618.36
Iteration:   2320, Loss function: 4.419, Average Loss: 4.390, avg. samples / sec: 58522.16
Iteration:   2320, Loss function: 4.987, Average Loss: 4.408, avg. samples / sec: 58498.62
Iteration:   2320, Loss function: 4.081, Average Loss: 4.399, avg. samples / sec: 58670.97
Iteration:   2320, Loss function: 4.742, Average Loss: 4.404, avg. samples / sec: 58607.68
Iteration:   2320, Loss function: 3.787, Average Loss: 4.379, avg. samples / sec: 58216.51
Iteration:   2340, Loss function: 4.763, Average Loss: 4.376, avg. samples / sec: 57199.98
Iteration:   2340, Loss function: 3.196, Average Loss: 4.407, avg. samples / sec: 57333.78
Iteration:   2340, Loss function: 5.840, Average Loss: 4.369, avg. samples / sec: 57276.62
Iteration:   2340, Loss function: 4.910, Average Loss: 4.369, avg. samples / sec: 57199.07
Iteration:   2340, Loss function: 5.380, Average Loss: 4.360, avg. samples / sec: 57181.04
Iteration:   2340, Loss function: 3.057, Average Loss: 4.383, avg. samples / sec: 57073.31
Iteration:   2340, Loss function: 3.907, Average Loss: 4.396, avg. samples / sec: 57085.82
Iteration:   2340, Loss function: 4.282, Average Loss: 4.359, avg. samples / sec: 57153.37
Iteration:   2340, Loss function: 3.044, Average Loss: 4.399, avg. samples / sec: 57113.95
Iteration:   2340, Loss function: 4.417, Average Loss: 4.389, avg. samples / sec: 57052.68
Iteration:   2340, Loss function: 4.379, Average Loss: 4.395, avg. samples / sec: 57282.42
Iteration:   2340, Loss function: 4.074, Average Loss: 4.376, avg. samples / sec: 57387.29
Iteration:   2340, Loss function: 3.388, Average Loss: 4.371, avg. samples / sec: 56998.89
Iteration:   2340, Loss function: 5.291, Average Loss: 4.399, avg. samples / sec: 57251.84
Iteration:   2340, Loss function: 4.189, Average Loss: 4.378, avg. samples / sec: 57040.00
Iteration:   2360, Loss function: 4.907, Average Loss: 4.372, avg. samples / sec: 57895.91
Iteration:   2360, Loss function: 3.951, Average Loss: 4.405, avg. samples / sec: 57774.18
Iteration:   2360, Loss function: 4.406, Average Loss: 4.356, avg. samples / sec: 57805.32
Iteration:   2360, Loss function: 4.336, Average Loss: 4.386, avg. samples / sec: 57833.45
Iteration:   2360, Loss function: 3.279, Average Loss: 4.393, avg. samples / sec: 57869.91
Iteration:   2360, Loss function: 4.428, Average Loss: 4.398, avg. samples / sec: 57786.83
Iteration:   2360, Loss function: 4.453, Average Loss: 4.370, avg. samples / sec: 57612.16
Iteration:   2360, Loss function: 3.793, Average Loss: 4.364, avg. samples / sec: 57669.26
Iteration:   2360, Loss function: 3.669, Average Loss: 4.347, avg. samples / sec: 57717.65
Iteration:   2360, Loss function: 4.040, Average Loss: 4.377, avg. samples / sec: 57694.64
Iteration:   2360, Loss function: 4.231, Average Loss: 4.389, avg. samples / sec: 57642.93
Iteration:   2360, Loss function: 3.686, Average Loss: 4.388, avg. samples / sec: 57683.96
Iteration:   2360, Loss function: 3.598, Average Loss: 4.367, avg. samples / sec: 57709.71
Iteration:   2360, Loss function: 2.930, Average Loss: 4.373, avg. samples / sec: 57795.05
Iteration:   2360, Loss function: 3.343, Average Loss: 4.363, avg. samples / sec: 57551.97
:::MLL 1558639238.740 epoch_stop: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 819}}
:::MLL 1558639238.741 epoch_start: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 673}}
Iteration:   2380, Loss function: 5.264, Average Loss: 4.366, avg. samples / sec: 59733.89
Iteration:   2380, Loss function: 4.206, Average Loss: 4.397, avg. samples / sec: 59808.01
Iteration:   2380, Loss function: 4.970, Average Loss: 4.364, avg. samples / sec: 59814.59
Iteration:   2380, Loss function: 3.818, Average Loss: 4.365, avg. samples / sec: 59929.81
Iteration:   2380, Loss function: 4.263, Average Loss: 4.358, avg. samples / sec: 59939.09
Iteration:   2380, Loss function: 4.293, Average Loss: 4.387, avg. samples / sec: 59685.09
Iteration:   2380, Loss function: 4.771, Average Loss: 4.402, avg. samples / sec: 59645.20
Iteration:   2380, Loss function: 4.792, Average Loss: 4.362, avg. samples / sec: 59834.50
Iteration:   2380, Loss function: 4.310, Average Loss: 4.376, avg. samples / sec: 59658.43
Iteration:   2380, Loss function: 4.437, Average Loss: 4.344, avg. samples / sec: 59715.54
Iteration:   2380, Loss function: 3.986, Average Loss: 4.383, avg. samples / sec: 59751.85
Iteration:   2380, Loss function: 4.800, Average Loss: 4.354, avg. samples / sec: 59550.24
Iteration:   2380, Loss function: 5.147, Average Loss: 4.359, avg. samples / sec: 59594.10
Iteration:   2380, Loss function: 4.389, Average Loss: 4.384, avg. samples / sec: 59581.56
Iteration:   2380, Loss function: 3.706, Average Loss: 4.372, avg. samples / sec: 59470.00
Iteration:   2400, Loss function: 5.264, Average Loss: 4.384, avg. samples / sec: 59728.07
Iteration:   2400, Loss function: 5.362, Average Loss: 4.363, avg. samples / sec: 59432.25
Iteration:   2400, Loss function: 3.847, Average Loss: 4.356, avg. samples / sec: 59385.55
Iteration:   2400, Loss function: 3.600, Average Loss: 4.351, avg. samples / sec: 59453.87
Iteration:   2400, Loss function: 3.523, Average Loss: 4.362, avg. samples / sec: 59315.61
Iteration:   2400, Loss function: 4.573, Average Loss: 4.340, avg. samples / sec: 59477.08
Iteration:   2400, Loss function: 2.835, Average Loss: 4.358, avg. samples / sec: 59361.26
Iteration:   2400, Loss function: 4.723, Average Loss: 4.358, avg. samples / sec: 59456.10
Iteration:   2400, Loss function: 3.009, Average Loss: 4.366, avg. samples / sec: 59596.57
Iteration:   2400, Loss function: 3.343, Average Loss: 4.385, avg. samples / sec: 59185.95
Iteration:   2400, Loss function: 3.833, Average Loss: 4.392, avg. samples / sec: 59264.53
Iteration:   2400, Loss function: 5.795, Average Loss: 4.353, avg. samples / sec: 59365.36
Iteration:   2400, Loss function: 6.615, Average Loss: 4.376, avg. samples / sec: 59270.26
Iteration:   2400, Loss function: 4.894, Average Loss: 4.383, avg. samples / sec: 59152.12
Iteration:   2400, Loss function: 3.834, Average Loss: 4.378, avg. samples / sec: 59229.46
Iteration:   2420, Loss function: 3.367, Average Loss: 4.347, avg. samples / sec: 57152.12
Iteration:   2420, Loss function: 4.167, Average Loss: 4.378, avg. samples / sec: 57284.07
Iteration:   2420, Loss function: 3.454, Average Loss: 4.372, avg. samples / sec: 57253.73
Iteration:   2420, Loss function: 4.451, Average Loss: 4.356, avg. samples / sec: 57206.18
Iteration:   2420, Loss function: 5.270, Average Loss: 4.357, avg. samples / sec: 57029.77
Iteration:   2420, Loss function: 4.903, Average Loss: 4.353, avg. samples / sec: 57198.26
Iteration:   2420, Loss function: 5.568, Average Loss: 4.333, avg. samples / sec: 57018.63
Iteration:   2420, Loss function: 4.299, Average Loss: 4.382, avg. samples / sec: 57112.35
Iteration:   2420, Loss function: 3.555, Average Loss: 4.355, avg. samples / sec: 56958.92
Iteration:   2420, Loss function: 3.249, Average Loss: 4.375, avg. samples / sec: 57232.26
Iteration:   2420, Loss function: 4.070, Average Loss: 4.354, avg. samples / sec: 56936.53
Iteration:   2420, Loss function: 3.689, Average Loss: 4.377, avg. samples / sec: 56906.78
Iteration:   2420, Loss function: 5.104, Average Loss: 4.373, avg. samples / sec: 57199.77
Iteration:   2420, Loss function: 3.079, Average Loss: 4.356, avg. samples / sec: 57063.07
Iteration:   2420, Loss function: 4.497, Average Loss: 4.353, avg. samples / sec: 56962.37
Iteration:   2440, Loss function: 4.775, Average Loss: 4.374, avg. samples / sec: 57658.69
Iteration:   2440, Loss function: 2.308, Average Loss: 4.351, avg. samples / sec: 57800.96
Iteration:   2440, Loss function: 3.786, Average Loss: 4.329, avg. samples / sec: 57764.90
Iteration:   2440, Loss function: 4.595, Average Loss: 4.371, avg. samples / sec: 57651.07
Iteration:   2440, Loss function: 4.619, Average Loss: 4.372, avg. samples / sec: 57795.55
Iteration:   2440, Loss function: 4.129, Average Loss: 4.342, avg. samples / sec: 57573.93
Iteration:   2440, Loss function: 4.368, Average Loss: 4.350, avg. samples / sec: 57799.79
Iteration:   2440, Loss function: 4.079, Average Loss: 4.352, avg. samples / sec: 57633.48
Iteration:   2440, Loss function: 4.166, Average Loss: 4.348, avg. samples / sec: 57761.39
Iteration:   2440, Loss function: 4.268, Average Loss: 4.379, avg. samples / sec: 57668.53
Iteration:   2440, Loss function: 4.623, Average Loss: 4.369, avg. samples / sec: 57660.41
Iteration:   2440, Loss function: 3.496, Average Loss: 4.352, avg. samples / sec: 57537.85
Iteration:   2440, Loss function: 3.524, Average Loss: 4.343, avg. samples / sec: 57576.52
Iteration:   2440, Loss function: 4.752, Average Loss: 4.369, avg. samples / sec: 57687.69
Iteration:   2440, Loss function: 4.719, Average Loss: 4.351, avg. samples / sec: 57511.60
:::MLL 1558639240.758 epoch_stop: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 819}}
:::MLL 1558639240.758 epoch_start: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 673}}
Iteration:   2460, Loss function: 3.320, Average Loss: 4.336, avg. samples / sec: 58665.92
Iteration:   2460, Loss function: 4.406, Average Loss: 4.343, avg. samples / sec: 58800.79
Iteration:   2460, Loss function: 3.892, Average Loss: 4.365, avg. samples / sec: 58681.21
Iteration:   2460, Loss function: 3.957, Average Loss: 4.382, avg. samples / sec: 58782.47
Iteration:   2460, Loss function: 4.190, Average Loss: 4.364, avg. samples / sec: 58789.33
Iteration:   2460, Loss function: 5.766, Average Loss: 4.346, avg. samples / sec: 58599.27
Iteration:   2460, Loss function: 4.783, Average Loss: 4.349, avg. samples / sec: 58843.46
Iteration:   2460, Loss function: 5.506, Average Loss: 4.346, avg. samples / sec: 58624.77
Iteration:   2460, Loss function: 3.893, Average Loss: 4.349, avg. samples / sec: 58533.05
Iteration:   2460, Loss function: 5.409, Average Loss: 4.343, avg. samples / sec: 58582.78
Iteration:   2460, Loss function: 3.874, Average Loss: 4.367, avg. samples / sec: 58625.45
Iteration:   2460, Loss function: 3.706, Average Loss: 4.370, avg. samples / sec: 58410.34
Iteration:   2460, Loss function: 3.608, Average Loss: 4.325, avg. samples / sec: 58468.28
Iteration:   2460, Loss function: 3.902, Average Loss: 4.343, avg. samples / sec: 58593.33
Iteration:   2460, Loss function: 3.826, Average Loss: 4.369, avg. samples / sec: 58403.64
Iteration:   2480, Loss function: 3.409, Average Loss: 4.349, avg. samples / sec: 56722.90
Iteration:   2480, Loss function: 5.331, Average Loss: 4.358, avg. samples / sec: 56760.41
Iteration:   2480, Loss function: 3.192, Average Loss: 4.343, avg. samples / sec: 56684.39
Iteration:   2480, Loss function: 4.330, Average Loss: 4.380, avg. samples / sec: 56549.06
Iteration:   2480, Loss function: 3.328, Average Loss: 4.354, avg. samples / sec: 56554.96
Iteration:   2480, Loss function: 4.648, Average Loss: 4.318, avg. samples / sec: 56724.00
Iteration:   2480, Loss function: 4.421, Average Loss: 4.344, avg. samples / sec: 56609.98
Iteration:   2480, Loss function: 5.329, Average Loss: 4.369, avg. samples / sec: 56694.19
Iteration:   2480, Loss function: 4.286, Average Loss: 4.335, avg. samples / sec: 56507.31
Iteration:   2480, Loss function: 3.936, Average Loss: 4.335, avg. samples / sec: 56634.77
Iteration:   2480, Loss function: 4.566, Average Loss: 4.362, avg. samples / sec: 56755.18
Iteration:   2480, Loss function: 4.336, Average Loss: 4.342, avg. samples / sec: 56665.54
Iteration:   2480, Loss function: 3.433, Average Loss: 4.338, avg. samples / sec: 56455.79
Iteration:   2480, Loss function: 3.940, Average Loss: 4.358, avg. samples / sec: 56422.65
Iteration:   2480, Loss function: 4.429, Average Loss: 4.337, avg. samples / sec: 56534.26
Iteration:   2500, Loss function: 2.999, Average Loss: 4.327, avg. samples / sec: 58874.06
Iteration:   2500, Loss function: 4.607, Average Loss: 4.372, avg. samples / sec: 58697.00
Iteration:   2500, Loss function: 3.403, Average Loss: 4.341, avg. samples / sec: 58775.85
Iteration:   2500, Loss function: 3.713, Average Loss: 4.338, avg. samples / sec: 58776.12
Iteration:   2500, Loss function: 4.259, Average Loss: 4.338, avg. samples / sec: 58565.35
Iteration:   2500, Loss function: 3.116, Average Loss: 4.345, avg. samples / sec: 58618.65
Iteration:   2500, Loss function: 4.165, Average Loss: 4.363, avg. samples / sec: 58615.41
Iteration:   2500, Loss function: 5.708, Average Loss: 4.309, avg. samples / sec: 58555.76
Iteration:   2500, Loss function: 4.838, Average Loss: 4.353, avg. samples / sec: 58478.69
Iteration:   2500, Loss function: 3.707, Average Loss: 4.334, avg. samples / sec: 58610.65
Iteration:   2500, Loss function: 3.703, Average Loss: 4.345, avg. samples / sec: 58445.20
Iteration:   2500, Loss function: 3.829, Average Loss: 4.357, avg. samples / sec: 58557.34
Iteration:   2500, Loss function: 4.110, Average Loss: 4.354, avg. samples / sec: 58621.31
Iteration:   2500, Loss function: 3.488, Average Loss: 4.331, avg. samples / sec: 58534.38
Iteration:   2500, Loss function: 3.504, Average Loss: 4.343, avg. samples / sec: 58397.88
:::MLL 1558639242.793 epoch_stop: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 819}}
:::MLL 1558639242.793 epoch_start: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 673}}
Iteration:   2520, Loss function: 4.918, Average Loss: 4.329, avg. samples / sec: 58594.79
Iteration:   2520, Loss function: 4.696, Average Loss: 4.322, avg. samples / sec: 58292.68
Iteration:   2520, Loss function: 4.315, Average Loss: 4.301, avg. samples / sec: 58552.72
Iteration:   2520, Loss function: 4.112, Average Loss: 4.325, avg. samples / sec: 58522.43
Iteration:   2520, Loss function: 3.584, Average Loss: 4.351, avg. samples / sec: 58531.47
Iteration:   2520, Loss function: 5.583, Average Loss: 4.366, avg. samples / sec: 58337.75
Iteration:   2520, Loss function: 3.308, Average Loss: 4.339, avg. samples / sec: 58401.92
Iteration:   2520, Loss function: 3.975, Average Loss: 4.349, avg. samples / sec: 58455.43
Iteration:   2520, Loss function: 3.960, Average Loss: 4.338, avg. samples / sec: 58476.92
Iteration:   2520, Loss function: 3.159, Average Loss: 4.322, avg. samples / sec: 58455.19
Iteration:   2520, Loss function: 4.593, Average Loss: 4.349, avg. samples / sec: 58452.59
Iteration:   2520, Loss function: 4.608, Average Loss: 4.358, avg. samples / sec: 58343.14
Iteration:   2520, Loss function: 4.617, Average Loss: 4.333, avg. samples / sec: 58277.16
Iteration:   2520, Loss function: 4.414, Average Loss: 4.338, avg. samples / sec: 58194.52
Iteration:   2520, Loss function: 3.558, Average Loss: 4.339, avg. samples / sec: 58456.08
Iteration:   2540, Loss function: 4.574, Average Loss: 4.300, avg. samples / sec: 58922.66
Iteration:   2540, Loss function: 3.201, Average Loss: 4.342, avg. samples / sec: 59041.69
Iteration:   2540, Loss function: 3.581, Average Loss: 4.342, avg. samples / sec: 58940.55
Iteration:   2540, Loss function: 3.433, Average Loss: 4.324, avg. samples / sec: 58763.37
Iteration:   2540, Loss function: 3.711, Average Loss: 4.321, avg. samples / sec: 58998.53
Iteration:   2540, Loss function: 2.945, Average Loss: 4.315, avg. samples / sec: 58802.26
Iteration:   2540, Loss function: 3.234, Average Loss: 4.338, avg. samples / sec: 58876.35
Iteration:   2540, Loss function: 4.122, Average Loss: 4.334, avg. samples / sec: 58893.94
Iteration:   2540, Loss function: 4.533, Average Loss: 4.326, avg. samples / sec: 58958.77
Iteration:   2540, Loss function: 4.571, Average Loss: 4.335, avg. samples / sec: 59001.05
Iteration:   2540, Loss function: 4.247, Average Loss: 4.323, avg. samples / sec: 58792.45
Iteration:   2540, Loss function: 4.329, Average Loss: 4.334, avg. samples / sec: 59002.78
Iteration:   2540, Loss function: 4.390, Average Loss: 4.344, avg. samples / sec: 58788.11
Iteration:   2540, Loss function: 5.050, Average Loss: 4.360, avg. samples / sec: 58720.21
Iteration:   2540, Loss function: 3.515, Average Loss: 4.350, avg. samples / sec: 58709.61
Iteration:   2560, Loss function: 4.865, Average Loss: 4.316, avg. samples / sec: 56949.00
Iteration:   2560, Loss function: 3.759, Average Loss: 4.311, avg. samples / sec: 56964.35
Iteration:   2560, Loss function: 3.463, Average Loss: 4.325, avg. samples / sec: 57031.74
Iteration:   2560, Loss function: 3.682, Average Loss: 4.315, avg. samples / sec: 56846.80
Iteration:   2560, Loss function: 3.653, Average Loss: 4.338, avg. samples / sec: 56771.71
Iteration:   2560, Loss function: 4.459, Average Loss: 4.295, avg. samples / sec: 56708.66
Iteration:   2560, Loss function: 4.239, Average Loss: 4.325, avg. samples / sec: 56827.50
Iteration:   2560, Loss function: 4.720, Average Loss: 4.326, avg. samples / sec: 56837.33
Iteration:   2560, Loss function: 4.045, Average Loss: 4.337, avg. samples / sec: 56916.43
Iteration:   2560, Loss function: 4.628, Average Loss: 4.334, avg. samples / sec: 56812.10
Iteration:   2560, Loss function: 3.801, Average Loss: 4.354, avg. samples / sec: 56897.66
Iteration:   2560, Loss function: 4.433, Average Loss: 4.317, avg. samples / sec: 56811.71
Iteration:   2560, Loss function: 4.091, Average Loss: 4.346, avg. samples / sec: 56966.22
Iteration:   2560, Loss function: 4.799, Average Loss: 4.334, avg. samples / sec: 56715.89
Iteration:   2560, Loss function: 5.003, Average Loss: 4.336, avg. samples / sec: 56600.02
Iteration:   2580, Loss function: 4.376, Average Loss: 4.352, avg. samples / sec: 58192.11
Iteration:   2580, Loss function: 3.188, Average Loss: 4.323, avg. samples / sec: 58199.04
Iteration:   2580, Loss function: 3.755, Average Loss: 4.337, avg. samples / sec: 58194.42
Iteration:   2580, Loss function: 4.015, Average Loss: 4.338, avg. samples / sec: 58099.94
Iteration:   2580, Loss function: 2.850, Average Loss: 4.329, avg. samples / sec: 57988.06
Iteration:   2580, Loss function: 3.578, Average Loss: 4.338, avg. samples / sec: 57839.67
Iteration:   2580, Loss function: 4.386, Average Loss: 4.320, avg. samples / sec: 57874.90
Iteration:   2580, Loss function: 3.231, Average Loss: 4.309, avg. samples / sec: 57789.79
Iteration:   2580, Loss function: 4.331, Average Loss: 4.330, avg. samples / sec: 57847.17
Iteration:   2580, Loss function: 4.088, Average Loss: 4.291, avg. samples / sec: 57807.88
Iteration:   2580, Loss function: 3.764, Average Loss: 4.309, avg. samples / sec: 57597.77
Iteration:   2580, Loss function: 4.248, Average Loss: 4.321, avg. samples / sec: 57656.92
Iteration:   2580, Loss function: 3.257, Average Loss: 4.309, avg. samples / sec: 57634.31
Iteration:   2580, Loss function: 4.199, Average Loss: 4.316, avg. samples / sec: 57866.96
Iteration:   2580, Loss function: 3.251, Average Loss: 4.319, avg. samples / sec: 57556.11
:::MLL 1558639244.819 epoch_stop: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 819}}
:::MLL 1558639244.819 epoch_start: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 673}}
Iteration:   2600, Loss function: 3.443, Average Loss: 4.324, avg. samples / sec: 59400.19
Iteration:   2600, Loss function: 5.541, Average Loss: 4.309, avg. samples / sec: 59438.02
Iteration:   2600, Loss function: 3.154, Average Loss: 4.290, avg. samples / sec: 59470.38
Iteration:   2600, Loss function: 3.040, Average Loss: 4.316, avg. samples / sec: 59368.46
Iteration:   2600, Loss function: 3.727, Average Loss: 4.329, avg. samples / sec: 59365.11
Iteration:   2600, Loss function: 3.536, Average Loss: 4.334, avg. samples / sec: 59093.13
Iteration:   2600, Loss function: 3.774, Average Loss: 4.302, avg. samples / sec: 59334.54
Iteration:   2600, Loss function: 4.894, Average Loss: 4.321, avg. samples / sec: 59321.33
Iteration:   2600, Loss function: 5.618, Average Loss: 4.312, avg. samples / sec: 59293.25
Iteration:   2600, Loss function: 4.319, Average Loss: 4.347, avg. samples / sec: 58943.38
Iteration:   2600, Loss function: 4.142, Average Loss: 4.322, avg. samples / sec: 59074.09
Iteration:   2600, Loss function: 4.234, Average Loss: 4.330, avg. samples / sec: 59213.78
Iteration:   2600, Loss function: 4.599, Average Loss: 4.302, avg. samples / sec: 59226.07
Iteration:   2600, Loss function: 4.558, Average Loss: 4.314, avg. samples / sec: 59478.61
Iteration:   2600, Loss function: 3.699, Average Loss: 4.315, avg. samples / sec: 58913.74
Iteration:   2620, Loss function: 4.262, Average Loss: 4.339, avg. samples / sec: 59620.53
Iteration:   2620, Loss function: 3.515, Average Loss: 4.295, avg. samples / sec: 59665.46
Iteration:   2620, Loss function: 3.939, Average Loss: 4.329, avg. samples / sec: 59504.85
Iteration:   2620, Loss function: 4.541, Average Loss: 4.323, avg. samples / sec: 59570.43
Iteration:   2620, Loss function: 3.501, Average Loss: 4.303, avg. samples / sec: 59299.66
Iteration:   2620, Loss function: 3.719, Average Loss: 4.318, avg. samples / sec: 59195.42
Iteration:   2620, Loss function: 4.211, Average Loss: 4.313, avg. samples / sec: 59560.78
Iteration:   2620, Loss function: 3.478, Average Loss: 4.319, avg. samples / sec: 59505.56
Iteration:   2620, Loss function: 3.522, Average Loss: 4.305, avg. samples / sec: 59541.58
Iteration:   2620, Loss function: 3.919, Average Loss: 4.299, avg. samples / sec: 59363.48
Iteration:   2620, Loss function: 3.191, Average Loss: 4.320, avg. samples / sec: 59219.55
Iteration:   2620, Loss function: 2.246, Average Loss: 4.314, avg. samples / sec: 59327.17
Iteration:   2620, Loss function: 4.031, Average Loss: 4.292, avg. samples / sec: 59139.28
Iteration:   2620, Loss function: 4.445, Average Loss: 4.307, avg. samples / sec: 59213.50
Iteration:   2620, Loss function: 3.031, Average Loss: 4.313, avg. samples / sec: 59031.52
Iteration:   2640, Loss function: 4.113, Average Loss: 4.310, avg. samples / sec: 59741.71
Iteration:   2640, Loss function: 4.716, Average Loss: 4.318, avg. samples / sec: 59396.91
Iteration:   2640, Loss function: 3.331, Average Loss: 4.300, avg. samples / sec: 59619.21
Iteration:   2640, Loss function: 5.141, Average Loss: 4.311, avg. samples / sec: 59357.81
Iteration:   2640, Loss function: 3.853, Average Loss: 4.292, avg. samples / sec: 59129.38
Iteration:   2640, Loss function: 4.725, Average Loss: 4.298, avg. samples / sec: 59188.54
Iteration:   2640, Loss function: 3.483, Average Loss: 4.333, avg. samples / sec: 59084.44
Iteration:   2640, Loss function: 3.278, Average Loss: 4.306, avg. samples / sec: 59317.78
Iteration:   2640, Loss function: 3.166, Average Loss: 4.310, avg. samples / sec: 59207.29
Iteration:   2640, Loss function: 3.456, Average Loss: 4.287, avg. samples / sec: 59289.31
Iteration:   2640, Loss function: 3.693, Average Loss: 4.308, avg. samples / sec: 59149.53
Iteration:   2640, Loss function: 3.689, Average Loss: 4.300, avg. samples / sec: 59199.85
Iteration:   2640, Loss function: 5.146, Average Loss: 4.326, avg. samples / sec: 59043.64
Iteration:   2640, Loss function: 3.365, Average Loss: 4.301, avg. samples / sec: 59115.07
Iteration:   2640, Loss function: 3.961, Average Loss: 4.310, avg. samples / sec: 58857.79
:::MLL 1558639246.811 epoch_stop: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 819}}
:::MLL 1558639246.811 epoch_start: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 673}}
Iteration:   2660, Loss function: 4.476, Average Loss: 4.306, avg. samples / sec: 58680.38
Iteration:   2660, Loss function: 3.876, Average Loss: 4.301, avg. samples / sec: 58718.45
Iteration:   2660, Loss function: 3.912, Average Loss: 4.322, avg. samples / sec: 58616.14
Iteration:   2660, Loss function: 4.137, Average Loss: 4.311, avg. samples / sec: 58841.00
Iteration:   2660, Loss function: 4.366, Average Loss: 4.336, avg. samples / sec: 58465.69
Iteration:   2660, Loss function: 2.944, Average Loss: 4.283, avg. samples / sec: 58422.54
Iteration:   2660, Loss function: 4.178, Average Loss: 4.305, avg. samples / sec: 58231.40
Iteration:   2660, Loss function: 3.601, Average Loss: 4.315, avg. samples / sec: 58219.52
Iteration:   2660, Loss function: 4.723, Average Loss: 4.300, avg. samples / sec: 58446.68
Iteration:   2660, Loss function: 4.195, Average Loss: 4.299, avg. samples / sec: 58388.08
Iteration:   2660, Loss function: 4.464, Average Loss: 4.295, avg. samples / sec: 58475.05
Iteration:   2660, Loss function: 4.872, Average Loss: 4.292, avg. samples / sec: 58256.29
Iteration:   2660, Loss function: 4.847, Average Loss: 4.297, avg. samples / sec: 58392.92
Iteration:   2660, Loss function: 4.064, Average Loss: 4.285, avg. samples / sec: 58319.82
Iteration:   2660, Loss function: 3.523, Average Loss: 4.302, avg. samples / sec: 58144.36
Iteration:   2680, Loss function: 4.895, Average Loss: 4.293, avg. samples / sec: 58479.18
Iteration:   2680, Loss function: 4.036, Average Loss: 4.294, avg. samples / sec: 58496.82
Iteration:   2680, Loss function: 4.285, Average Loss: 4.296, avg. samples / sec: 58242.47
Iteration:   2680, Loss function: 4.051, Average Loss: 4.300, avg. samples / sec: 57973.41
Iteration:   2680, Loss function: 3.484, Average Loss: 4.316, avg. samples / sec: 58078.78
Iteration:   2680, Loss function: 3.283, Average Loss: 4.286, avg. samples / sec: 58216.87
Iteration:   2680, Loss function: 3.996, Average Loss: 4.306, avg. samples / sec: 58157.80
Iteration:   2680, Loss function: 3.360, Average Loss: 4.277, avg. samples / sec: 58307.03
Iteration:   2680, Loss function: 4.723, Average Loss: 4.298, avg. samples / sec: 58350.60
Iteration:   2680, Loss function: 4.397, Average Loss: 4.307, avg. samples / sec: 57965.19
Iteration:   2680, Loss function: 4.817, Average Loss: 4.293, avg. samples / sec: 57787.11
Iteration:   2680, Loss function: 3.834, Average Loss: 4.275, avg. samples / sec: 57987.39
Iteration:   2680, Loss function: 3.881, Average Loss: 4.297, avg. samples / sec: 57942.47
Iteration:   2680, Loss function: 5.013, Average Loss: 4.327, avg. samples / sec: 57925.88
Iteration:   2680, Loss function: 3.845, Average Loss: 4.298, avg. samples / sec: 57923.76
Iteration:   2700, Loss function: 4.582, Average Loss: 4.292, avg. samples / sec: 56159.68
Iteration:   2700, Loss function: 4.439, Average Loss: 4.294, avg. samples / sec: 56464.36
Iteration:   2700, Loss function: 5.081, Average Loss: 4.286, avg. samples / sec: 56199.23
Iteration:   2700, Loss function: 4.307, Average Loss: 4.300, avg. samples / sec: 56337.49
Iteration:   2700, Loss function: 3.971, Average Loss: 4.285, avg. samples / sec: 56594.04
Iteration:   2700, Loss function: 4.228, Average Loss: 4.295, avg. samples / sec: 56312.88
Iteration:   2700, Loss function: 3.531, Average Loss: 4.322, avg. samples / sec: 56497.75
Iteration:   2700, Loss function: 3.835, Average Loss: 4.291, avg. samples / sec: 56485.46
Iteration:   2700, Loss function: 3.645, Average Loss: 4.278, avg. samples / sec: 56288.16
Iteration:   2700, Loss function: 3.641, Average Loss: 4.269, avg. samples / sec: 56420.17
Iteration:   2700, Loss function: 3.294, Average Loss: 4.313, avg. samples / sec: 56236.43
Iteration:   2700, Loss function: 4.072, Average Loss: 4.302, avg. samples / sec: 56337.85
Iteration:   2700, Loss function: 3.453, Average Loss: 4.285, avg. samples / sec: 56374.29
Iteration:   2700, Loss function: 4.977, Average Loss: 4.296, avg. samples / sec: 56150.84
Iteration:   2700, Loss function: 5.543, Average Loss: 4.280, avg. samples / sec: 56197.70
Iteration:   2720, Loss function: 4.673, Average Loss: 4.276, avg. samples / sec: 58482.77
Iteration:   2720, Loss function: 3.113, Average Loss: 4.311, avg. samples / sec: 58466.92
Iteration:   2720, Loss function: 4.411, Average Loss: 4.280, avg. samples / sec: 58539.64
Iteration:   2720, Loss function: 4.130, Average Loss: 4.297, avg. samples / sec: 58505.13
Iteration:   2720, Loss function: 3.992, Average Loss: 4.288, avg. samples / sec: 58266.65
Iteration:   2720, Loss function: 3.419, Average Loss: 4.289, avg. samples / sec: 58362.06
Iteration:   2720, Loss function: 4.470, Average Loss: 4.286, avg. samples / sec: 58264.96
Iteration:   2720, Loss function: 3.752, Average Loss: 4.289, avg. samples / sec: 58194.52
Iteration:   2720, Loss function: 3.674, Average Loss: 4.278, avg. samples / sec: 58427.12
Iteration:   2720, Loss function: 4.725, Average Loss: 4.298, avg. samples / sec: 58169.94
Iteration:   2720, Loss function: 3.770, Average Loss: 4.314, avg. samples / sec: 58278.55
Iteration:   2720, Loss function: 3.343, Average Loss: 4.278, avg. samples / sec: 58157.17
Iteration:   2720, Loss function: 4.360, Average Loss: 4.290, avg. samples / sec: 58133.62
Iteration:   2720, Loss function: 5.758, Average Loss: 4.289, avg. samples / sec: 58244.50
Iteration:   2720, Loss function: 3.417, Average Loss: 4.271, avg. samples / sec: 58095.61
:::MLL 1558639248.845 epoch_stop: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 819}}
:::MLL 1558639248.846 epoch_start: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 673}}
Iteration:   2740, Loss function: 3.696, Average Loss: 4.282, avg. samples / sec: 58376.85
Iteration:   2740, Loss function: 3.541, Average Loss: 4.291, avg. samples / sec: 58259.23
Iteration:   2740, Loss function: 5.093, Average Loss: 4.289, avg. samples / sec: 58279.35
Iteration:   2740, Loss function: 3.665, Average Loss: 4.286, avg. samples / sec: 58468.99
Iteration:   2740, Loss function: 4.037, Average Loss: 4.273, avg. samples / sec: 58397.22
Iteration:   2740, Loss function: 4.647, Average Loss: 4.282, avg. samples / sec: 58263.57
Iteration:   2740, Loss function: 4.363, Average Loss: 4.273, avg. samples / sec: 58117.07
Iteration:   2740, Loss function: 3.635, Average Loss: 4.268, avg. samples / sec: 58523.18
Iteration:   2740, Loss function: 3.614, Average Loss: 4.273, avg. samples / sec: 58270.22
Iteration:   2740, Loss function: 4.584, Average Loss: 4.283, avg. samples / sec: 58235.23
Iteration:   2740, Loss function: 3.966, Average Loss: 4.303, avg. samples / sec: 58093.67
Iteration:   2740, Loss function: 4.733, Average Loss: 4.280, avg. samples / sec: 58050.52
Iteration:   2740, Loss function: 4.710, Average Loss: 4.292, avg. samples / sec: 58241.87
Iteration:   2740, Loss function: 3.652, Average Loss: 4.288, avg. samples / sec: 58339.03
Iteration:   2740, Loss function: 4.074, Average Loss: 4.314, avg. samples / sec: 58164.66
Iteration:   2760, Loss function: 2.977, Average Loss: 4.297, avg. samples / sec: 58235.16
Iteration:   2760, Loss function: 3.632, Average Loss: 4.286, avg. samples / sec: 58089.19
Iteration:   2760, Loss function: 3.440, Average Loss: 4.285, avg. samples / sec: 58279.01
Iteration:   2760, Loss function: 3.717, Average Loss: 4.281, avg. samples / sec: 58259.71
Iteration:   2760, Loss function: 3.697, Average Loss: 4.268, avg. samples / sec: 58115.90
Iteration:   2760, Loss function: 3.945, Average Loss: 4.277, avg. samples / sec: 58197.02
Iteration:   2760, Loss function: 3.677, Average Loss: 4.269, avg. samples / sec: 58144.55
Iteration:   2760, Loss function: 4.320, Average Loss: 4.284, avg. samples / sec: 58041.27
Iteration:   2760, Loss function: 3.362, Average Loss: 4.262, avg. samples / sec: 58112.38
Iteration:   2760, Loss function: 3.951, Average Loss: 4.276, avg. samples / sec: 58137.07
Iteration:   2760, Loss function: 4.142, Average Loss: 4.270, avg. samples / sec: 58054.68
Iteration:   2760, Loss function: 3.847, Average Loss: 4.280, avg. samples / sec: 58045.05
Iteration:   2760, Loss function: 3.903, Average Loss: 4.276, avg. samples / sec: 57997.63
Iteration:   2760, Loss function: 3.107, Average Loss: 4.275, avg. samples / sec: 57733.35
Iteration:   2760, Loss function: 4.099, Average Loss: 4.304, avg. samples / sec: 58016.47
Iteration:   2780, Loss function: 3.526, Average Loss: 4.264, avg. samples / sec: 60080.94
Iteration:   2780, Loss function: 3.728, Average Loss: 4.270, avg. samples / sec: 59952.50
Iteration:   2780, Loss function: 3.779, Average Loss: 4.266, avg. samples / sec: 59939.04
Iteration:   2780, Loss function: 3.249, Average Loss: 4.273, avg. samples / sec: 59980.85
Iteration:   2780, Loss function: 4.601, Average Loss: 4.264, avg. samples / sec: 60132.80
Iteration:   2780, Loss function: 3.629, Average Loss: 4.274, avg. samples / sec: 59877.74
Iteration:   2780, Loss function: 4.190, Average Loss: 4.290, avg. samples / sec: 59723.92
Iteration:   2780, Loss function: 3.207, Average Loss: 4.299, avg. samples / sec: 60071.82
Iteration:   2780, Loss function: 4.030, Average Loss: 4.263, avg. samples / sec: 59721.11
Iteration:   2780, Loss function: 3.899, Average Loss: 4.258, avg. samples / sec: 59729.18
Iteration:   2780, Loss function: 3.624, Average Loss: 4.273, avg. samples / sec: 59732.90
Iteration:   2780, Loss function: 3.491, Average Loss: 4.279, avg. samples / sec: 59719.08
Iteration:   2780, Loss function: 3.096, Average Loss: 4.278, avg. samples / sec: 59655.61
Iteration:   2780, Loss function: 4.397, Average Loss: 4.279, avg. samples / sec: 59637.05
Iteration:   2780, Loss function: 5.222, Average Loss: 4.280, avg. samples / sec: 59538.97
:::MLL 1558639250.848 epoch_stop: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 819}}
:::MLL 1558639250.849 epoch_start: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 673}}
Iteration:   2800, Loss function: 4.106, Average Loss: 4.274, avg. samples / sec: 59216.91
Iteration:   2800, Loss function: 3.839, Average Loss: 4.290, avg. samples / sec: 59065.91
Iteration:   2800, Loss function: 4.855, Average Loss: 4.295, avg. samples / sec: 59118.62
Iteration:   2800, Loss function: 2.628, Average Loss: 4.267, avg. samples / sec: 59163.76
Iteration:   2800, Loss function: 4.023, Average Loss: 4.263, avg. samples / sec: 58856.80
Iteration:   2800, Loss function: 3.534, Average Loss: 4.256, avg. samples / sec: 59059.92
Iteration:   2800, Loss function: 4.106, Average Loss: 4.269, avg. samples / sec: 59043.00
Iteration:   2800, Loss function: 2.513, Average Loss: 4.255, avg. samples / sec: 59034.79
Iteration:   2800, Loss function: 4.359, Average Loss: 4.259, avg. samples / sec: 58808.81
Iteration:   2800, Loss function: 4.840, Average Loss: 4.269, avg. samples / sec: 58892.93
Iteration:   2800, Loss function: 4.165, Average Loss: 4.261, avg. samples / sec: 58874.41
Iteration:   2800, Loss function: 4.857, Average Loss: 4.268, avg. samples / sec: 59063.61
Iteration:   2800, Loss function: 2.627, Average Loss: 4.256, avg. samples / sec: 58569.59
Iteration:   2800, Loss function: 4.660, Average Loss: 4.268, avg. samples / sec: 58666.01
Iteration:   2800, Loss function: 3.542, Average Loss: 4.271, avg. samples / sec: 58804.47
Iteration:   2820, Loss function: 4.944, Average Loss: 4.253, avg. samples / sec: 59262.18
Iteration:   2820, Loss function: 3.449, Average Loss: 4.257, avg. samples / sec: 59199.48
Iteration:   2820, Loss function: 4.687, Average Loss: 4.254, avg. samples / sec: 59256.35
Iteration:   2820, Loss function: 3.385, Average Loss: 4.252, avg. samples / sec: 59065.62
Iteration:   2820, Loss function: 3.008, Average Loss: 4.260, avg. samples / sec: 58931.62
Iteration:   2820, Loss function: 3.939, Average Loss: 4.293, avg. samples / sec: 58896.33
Iteration:   2820, Loss function: 4.352, Average Loss: 4.268, avg. samples / sec: 59040.16
Iteration:   2820, Loss function: 4.449, Average Loss: 4.269, avg. samples / sec: 59215.10
Iteration:   2820, Loss function: 3.725, Average Loss: 4.254, avg. samples / sec: 58914.30
Iteration:   2820, Loss function: 4.339, Average Loss: 4.251, avg. samples / sec: 58977.89
Iteration:   2820, Loss function: 3.595, Average Loss: 4.262, avg. samples / sec: 58897.66
Iteration:   2820, Loss function: 3.149, Average Loss: 4.261, avg. samples / sec: 59062.65
Iteration:   2820, Loss function: 4.687, Average Loss: 4.284, avg. samples / sec: 58731.81
Iteration:   2820, Loss function: 3.018, Average Loss: 4.266, avg. samples / sec: 58694.60
Iteration:   2820, Loss function: 3.876, Average Loss: 4.262, avg. samples / sec: 58888.23
Iteration:   2840, Loss function: 4.264, Average Loss: 4.257, avg. samples / sec: 57088.48
Iteration:   2840, Loss function: 3.926, Average Loss: 4.251, avg. samples / sec: 56818.29
Iteration:   2840, Loss function: 3.514, Average Loss: 4.255, avg. samples / sec: 56749.28
Iteration:   2840, Loss function: 4.480, Average Loss: 4.284, avg. samples / sec: 56888.10
Iteration:   2840, Loss function: 3.639, Average Loss: 4.247, avg. samples / sec: 56621.19
Iteration:   2840, Loss function: 5.559, Average Loss: 4.257, avg. samples / sec: 56861.36
Iteration:   2840, Loss function: 2.924, Average Loss: 4.243, avg. samples / sec: 56937.17
Iteration:   2840, Loss function: 3.272, Average Loss: 4.253, avg. samples / sec: 57029.27
Iteration:   2840, Loss function: 3.945, Average Loss: 4.245, avg. samples / sec: 56891.07
Iteration:   2840, Loss function: 3.790, Average Loss: 4.280, avg. samples / sec: 56985.52
Iteration:   2840, Loss function: 4.290, Average Loss: 4.263, avg. samples / sec: 56884.61
Iteration:   2840, Loss function: 3.804, Average Loss: 4.265, avg. samples / sec: 56809.58
Iteration:   2840, Loss function: 4.212, Average Loss: 4.246, avg. samples / sec: 56760.99
Iteration:   2840, Loss function: 5.758, Average Loss: 4.264, avg. samples / sec: 56882.85
Iteration:   2840, Loss function: 3.830, Average Loss: 4.256, avg. samples / sec: 56792.55
Iteration:   2860, Loss function: 3.653, Average Loss: 4.251, avg. samples / sec: 59783.15
Iteration:   2860, Loss function: 4.889, Average Loss: 4.241, avg. samples / sec: 59635.41
Iteration:   2860, Loss function: 3.907, Average Loss: 4.257, avg. samples / sec: 59611.50
Iteration:   2860, Loss function: 3.856, Average Loss: 4.280, avg. samples / sec: 59522.14
Iteration:   2860, Loss function: 4.439, Average Loss: 4.252, avg. samples / sec: 59432.61
Iteration:   2860, Loss function: 2.583, Average Loss: 4.245, avg. samples / sec: 59455.67
Iteration:   2860, Loss function: 4.326, Average Loss: 4.252, avg. samples / sec: 59679.76
Iteration:   2860, Loss function: 6.240, Average Loss: 4.255, avg. samples / sec: 59476.73
Iteration:   2860, Loss function: 5.274, Average Loss: 4.245, avg. samples / sec: 59482.22
Iteration:   2860, Loss function: 4.342, Average Loss: 4.267, avg. samples / sec: 59578.46
Iteration:   2860, Loss function: 4.711, Average Loss: 4.254, avg. samples / sec: 59390.75
Iteration:   2860, Loss function: 3.143, Average Loss: 4.247, avg. samples / sec: 59499.90
Iteration:   2860, Loss function: 2.847, Average Loss: 4.262, avg. samples / sec: 59465.98
Iteration:   2860, Loss function: 5.137, Average Loss: 4.280, avg. samples / sec: 59367.89
Iteration:   2860, Loss function: 4.227, Average Loss: 4.262, avg. samples / sec: 59350.71
:::MLL 1558639252.858 epoch_stop: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 819}}
:::MLL 1558639252.859 epoch_start: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 673}}
Iteration:   2880, Loss function: 3.535, Average Loss: 4.240, avg. samples / sec: 56552.78
Iteration:   2880, Loss function: 4.481, Average Loss: 4.245, avg. samples / sec: 56555.36
Iteration:   2880, Loss function: 3.661, Average Loss: 4.246, avg. samples / sec: 56445.34
Iteration:   2880, Loss function: 4.042, Average Loss: 4.241, avg. samples / sec: 56337.04
Iteration:   2880, Loss function: 4.350, Average Loss: 4.255, avg. samples / sec: 56544.34
Iteration:   2880, Loss function: 3.771, Average Loss: 4.273, avg. samples / sec: 56395.74
Iteration:   2880, Loss function: 4.147, Average Loss: 4.257, avg. samples / sec: 56602.63
Iteration:   2880, Loss function: 3.733, Average Loss: 4.237, avg. samples / sec: 56448.53
Iteration:   2880, Loss function: 4.141, Average Loss: 4.248, avg. samples / sec: 56427.94
Iteration:   2880, Loss function: 4.072, Average Loss: 4.245, avg. samples / sec: 56440.30
Iteration:   2880, Loss function: 3.264, Average Loss: 4.245, avg. samples / sec: 56131.03
Iteration:   2880, Loss function: 3.928, Average Loss: 4.251, avg. samples / sec: 56281.17
Iteration:   2880, Loss function: 3.391, Average Loss: 4.244, avg. samples / sec: 56405.51
Iteration:   2880, Loss function: 3.626, Average Loss: 4.261, avg. samples / sec: 56391.75
Iteration:   2880, Loss function: 3.591, Average Loss: 4.271, avg. samples / sec: 56470.92
Iteration:   2900, Loss function: 3.628, Average Loss: 4.237, avg. samples / sec: 60285.62
Iteration:   2900, Loss function: 5.734, Average Loss: 4.241, avg. samples / sec: 60209.69
Iteration:   2900, Loss function: 2.659, Average Loss: 4.239, avg. samples / sec: 60139.50
Iteration:   2900, Loss function: 5.744, Average Loss: 4.256, avg. samples / sec: 60101.33
Iteration:   2900, Loss function: 3.722, Average Loss: 4.240, avg. samples / sec: 60199.46
Iteration:   2900, Loss function: 5.986, Average Loss: 4.236, avg. samples / sec: 59970.57
Iteration:   2900, Loss function: 3.575, Average Loss: 4.255, avg. samples / sec: 60151.07
Iteration:   2900, Loss function: 4.459, Average Loss: 4.268, avg. samples / sec: 60086.50
Iteration:   2900, Loss function: 3.065, Average Loss: 4.237, avg. samples / sec: 59955.64
Iteration:   2900, Loss function: 3.916, Average Loss: 4.236, avg. samples / sec: 59949.27
Iteration:   2900, Loss function: 4.648, Average Loss: 4.242, avg. samples / sec: 59929.08
Iteration:   2900, Loss function: 4.758, Average Loss: 4.251, avg. samples / sec: 59930.66
Iteration:   2900, Loss function: 4.262, Average Loss: 4.268, avg. samples / sec: 59871.71
Iteration:   2900, Loss function: 2.533, Average Loss: 4.233, avg. samples / sec: 59695.46
Iteration:   2900, Loss function: 4.848, Average Loss: 4.249, avg. samples / sec: 59671.12
Iteration:   2920, Loss function: 4.226, Average Loss: 4.268, avg. samples / sec: 56777.45
Iteration:   2920, Loss function: 4.861, Average Loss: 4.269, avg. samples / sec: 56695.02
Iteration:   2920, Loss function: 3.580, Average Loss: 4.236, avg. samples / sec: 56334.90
Iteration:   2920, Loss function: 3.580, Average Loss: 4.233, avg. samples / sec: 56449.30
Iteration:   2920, Loss function: 4.384, Average Loss: 4.254, avg. samples / sec: 56559.77
Iteration:   2920, Loss function: 4.023, Average Loss: 4.232, avg. samples / sec: 56506.18
Iteration:   2920, Loss function: 3.954, Average Loss: 4.231, avg. samples / sec: 56627.63
Iteration:   2920, Loss function: 3.739, Average Loss: 4.244, avg. samples / sec: 56842.67
Iteration:   2920, Loss function: 3.692, Average Loss: 4.249, avg. samples / sec: 56457.01
Iteration:   2920, Loss function: 5.112, Average Loss: 4.238, avg. samples / sec: 56386.78
Iteration:   2920, Loss function: 4.380, Average Loss: 4.244, avg. samples / sec: 56619.71
Iteration:   2920, Loss function: 3.151, Average Loss: 4.237, avg. samples / sec: 56601.06
Iteration:   2920, Loss function: 4.433, Average Loss: 4.227, avg. samples / sec: 56628.43
Iteration:   2920, Loss function: 4.727, Average Loss: 4.231, avg. samples / sec: 56448.46
Iteration:   2920, Loss function: 4.009, Average Loss: 4.236, avg. samples / sec: 56291.13
:::MLL 1558639254.899 epoch_stop: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 819}}
:::MLL 1558639254.900 epoch_start: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 673}}
Iteration:   2940, Loss function: 3.415, Average Loss: 4.227, avg. samples / sec: 58688.66
Iteration:   2940, Loss function: 3.874, Average Loss: 4.232, avg. samples / sec: 58454.02
Iteration:   2940, Loss function: 4.381, Average Loss: 4.246, avg. samples / sec: 58448.54
Iteration:   2940, Loss function: 4.564, Average Loss: 4.244, avg. samples / sec: 58424.31
Iteration:   2940, Loss function: 3.596, Average Loss: 4.263, avg. samples / sec: 58310.96
Iteration:   2940, Loss function: 5.038, Average Loss: 4.238, avg. samples / sec: 58416.08
Iteration:   2940, Loss function: 3.801, Average Loss: 4.265, avg. samples / sec: 58242.59
Iteration:   2940, Loss function: 4.287, Average Loss: 4.225, avg. samples / sec: 58566.45
Iteration:   2940, Loss function: 4.251, Average Loss: 4.232, avg. samples / sec: 58384.21
Iteration:   2940, Loss function: 4.395, Average Loss: 4.223, avg. samples / sec: 58291.04
Iteration:   2940, Loss function: 5.056, Average Loss: 4.230, avg. samples / sec: 58292.15
Iteration:   2940, Loss function: 4.347, Average Loss: 4.226, avg. samples / sec: 58264.05
Iteration:   2940, Loss function: 3.767, Average Loss: 4.243, avg. samples / sec: 58314.41
Iteration:   2940, Loss function: 4.189, Average Loss: 4.230, avg. samples / sec: 58482.77
Iteration:   2940, Loss function: 4.563, Average Loss: 4.234, avg. samples / sec: 58280.51
Iteration:   2960, Loss function: 3.725, Average Loss: 4.241, avg. samples / sec: 55472.20
Iteration:   2960, Loss function: 4.275, Average Loss: 4.235, avg. samples / sec: 55635.26
Iteration:   2960, Loss function: 3.399, Average Loss: 4.221, avg. samples / sec: 55389.13
Iteration:   2960, Loss function: 4.417, Average Loss: 4.226, avg. samples / sec: 55619.80
Iteration:   2960, Loss function: 3.702, Average Loss: 4.238, avg. samples / sec: 55423.33
Iteration:   2960, Loss function: 3.865, Average Loss: 4.229, avg. samples / sec: 55358.80
Iteration:   2960, Loss function: 3.718, Average Loss: 4.255, avg. samples / sec: 55488.88
Iteration:   2960, Loss function: 4.182, Average Loss: 4.226, avg. samples / sec: 55461.85
Iteration:   2960, Loss function: 3.921, Average Loss: 4.216, avg. samples / sec: 55463.16
Iteration:   2960, Loss function: 3.545, Average Loss: 4.224, avg. samples / sec: 55496.34
Iteration:   2960, Loss function: 4.569, Average Loss: 4.230, avg. samples / sec: 55403.07
Iteration:   2960, Loss function: 3.156, Average Loss: 4.214, avg. samples / sec: 55486.61
Iteration:   2960, Loss function: 3.529, Average Loss: 4.216, avg. samples / sec: 55446.64
Iteration:   2960, Loss function: 3.790, Average Loss: 4.255, avg. samples / sec: 55372.05
Iteration:   2960, Loss function: 4.466, Average Loss: 4.230, avg. samples / sec: 55505.89
Iteration:   2980, Loss function: 4.704, Average Loss: 4.250, avg. samples / sec: 57936.04
Iteration:   2980, Loss function: 3.656, Average Loss: 4.222, avg. samples / sec: 57841.12
Iteration:   2980, Loss function: 3.823, Average Loss: 4.232, avg. samples / sec: 57856.20
Iteration:   2980, Loss function: 3.654, Average Loss: 4.216, avg. samples / sec: 57749.20
Iteration:   2980, Loss function: 4.005, Average Loss: 4.223, avg. samples / sec: 57738.96
Iteration:   2980, Loss function: 3.942, Average Loss: 4.230, avg. samples / sec: 57699.88
Iteration:   2980, Loss function: 5.457, Average Loss: 4.227, avg. samples / sec: 57859.74
Iteration:   2980, Loss function: 3.401, Average Loss: 4.222, avg. samples / sec: 57773.04
Iteration:   2980, Loss function: 3.785, Average Loss: 4.210, avg. samples / sec: 57745.63
Iteration:   2980, Loss function: 4.264, Average Loss: 4.221, avg. samples / sec: 57727.04
Iteration:   2980, Loss function: 4.328, Average Loss: 4.239, avg. samples / sec: 57584.66
Iteration:   2980, Loss function: 3.918, Average Loss: 4.207, avg. samples / sec: 57715.57
Iteration:   2980, Loss function: 4.628, Average Loss: 4.212, avg. samples / sec: 57721.20
Iteration:   2980, Loss function: 3.943, Average Loss: 4.232, avg. samples / sec: 57622.45
Iteration:   2980, Loss function: 4.483, Average Loss: 4.252, avg. samples / sec: 57622.66
Iteration:   3000, Loss function: 3.996, Average Loss: 4.217, avg. samples / sec: 59755.39
Iteration:   3000, Loss function: 2.815, Average Loss: 4.202, avg. samples / sec: 59769.05
Iteration:   3000, Loss function: 3.486, Average Loss: 4.250, avg. samples / sec: 59616.57
Iteration:   3000, Loss function: 4.538, Average Loss: 4.219, avg. samples / sec: 59716.07
Iteration:   3000, Loss function: 3.735, Average Loss: 4.216, avg. samples / sec: 59673.24
Iteration:   3000, Loss function: 2.386, Average Loss: 4.214, avg. samples / sec: 59587.63
Iteration:   3000, Loss function: 4.296, Average Loss: 4.207, avg. samples / sec: 59762.92
Iteration:   3000, Loss function: 3.912, Average Loss: 4.246, avg. samples / sec: 59786.70
Iteration:   3000, Loss function: 4.169, Average Loss: 4.227, avg. samples / sec: 59570.93
Iteration:   3000, Loss function: 3.770, Average Loss: 4.219, avg. samples / sec: 59593.85
Iteration:   3000, Loss function: 3.376, Average Loss: 4.225, avg. samples / sec: 59738.60
Iteration:   3000, Loss function: 5.010, Average Loss: 4.236, avg. samples / sec: 59693.91
Iteration:   3000, Loss function: 3.844, Average Loss: 4.200, avg. samples / sec: 59695.41
Iteration:   3000, Loss function: 3.857, Average Loss: 4.220, avg. samples / sec: 59477.88
Iteration:   3000, Loss function: 4.262, Average Loss: 4.216, avg. samples / sec: 59486.69
:::MLL 1558639256.938 epoch_stop: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 819}}
:::MLL 1558639256.939 epoch_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 673}}
Iteration:   3020, Loss function: 4.039, Average Loss: 4.231, avg. samples / sec: 58792.47
Iteration:   3020, Loss function: 3.145, Average Loss: 4.207, avg. samples / sec: 58935.72
Iteration:   3020, Loss function: 4.103, Average Loss: 4.201, avg. samples / sec: 58648.66
Iteration:   3020, Loss function: 3.293, Average Loss: 4.240, avg. samples / sec: 58650.83
Iteration:   3020, Loss function: 2.642, Average Loss: 4.202, avg. samples / sec: 58643.31
Iteration:   3020, Loss function: 4.851, Average Loss: 4.215, avg. samples / sec: 58546.86
Iteration:   3020, Loss function: 4.473, Average Loss: 4.216, avg. samples / sec: 58431.65
Iteration:   3020, Loss function: 4.047, Average Loss: 4.209, avg. samples / sec: 58550.95
Iteration:   3020, Loss function: 4.187, Average Loss: 4.211, avg. samples / sec: 58527.46
Iteration:   3020, Loss function: 3.734, Average Loss: 4.250, avg. samples / sec: 58492.65
Iteration:   3020, Loss function: 3.796, Average Loss: 4.201, avg. samples / sec: 58664.13
Iteration:   3020, Loss function: 3.714, Average Loss: 4.229, avg. samples / sec: 58573.38
Iteration:   3020, Loss function: 3.527, Average Loss: 4.219, avg. samples / sec: 58582.12
Iteration:   3020, Loss function: 4.244, Average Loss: 4.216, avg. samples / sec: 58586.00
Iteration:   3020, Loss function: 4.439, Average Loss: 4.211, avg. samples / sec: 58610.82
Iteration:   3040, Loss function: 3.879, Average Loss: 4.210, avg. samples / sec: 60165.84
Iteration:   3040, Loss function: 4.813, Average Loss: 4.226, avg. samples / sec: 59914.43
Iteration:   3040, Loss function: 4.637, Average Loss: 4.225, avg. samples / sec: 60081.78
Iteration:   3040, Loss function: 3.743, Average Loss: 4.207, avg. samples / sec: 60062.48
Iteration:   3040, Loss function: 4.476, Average Loss: 4.200, avg. samples / sec: 60065.93
Iteration:   3040, Loss function: 3.557, Average Loss: 4.205, avg. samples / sec: 60203.93
Iteration:   3040, Loss function: 4.043, Average Loss: 4.231, avg. samples / sec: 59960.64
Iteration:   3040, Loss function: 3.983, Average Loss: 4.205, avg. samples / sec: 59977.48
Iteration:   3040, Loss function: 3.864, Average Loss: 4.216, avg. samples / sec: 59989.02
Iteration:   3040, Loss function: 2.731, Average Loss: 4.197, avg. samples / sec: 59882.42
Iteration:   3040, Loss function: 3.615, Average Loss: 4.212, avg. samples / sec: 59847.46
Iteration:   3040, Loss function: 3.835, Average Loss: 4.199, avg. samples / sec: 59710.33
Iteration:   3040, Loss function: 4.281, Average Loss: 4.199, avg. samples / sec: 59705.12
Iteration:   3040, Loss function: 3.638, Average Loss: 4.245, avg. samples / sec: 59858.11
Iteration:   3040, Loss function: 3.732, Average Loss: 4.213, avg. samples / sec: 59709.01
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
:::MLL 1558639258.031 eval_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.61 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.57s)
DONE (t=3.03s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.17033
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.31144
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.17142
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.04111
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.17859
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.27381
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.18049
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.26224
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.27602
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.07185
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.29498
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.42842
Current AP: 0.17033 AP goal: 0.23000
:::MLL 1558639262.259 eval_accuracy: {"value": 0.1703303593550433, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 389}}
:::MLL 1558639262.409 eval_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 392}}
:::MLL 1558639262.419 block_stop: {"value": null, "metadata": {"first_epoch_num": 33, "file": "train.py", "lineno": 804}}
:::MLL 1558639262.419 block_start: {"value": null, "metadata": {"first_epoch_num": 44, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3060, Loss function: 3.721, Average Loss: 4.195, avg. samples / sec: 6746.63
Iteration:   3060, Loss function: 5.779, Average Loss: 4.205, avg. samples / sec: 6744.71
Iteration:   3060, Loss function: 3.952, Average Loss: 4.202, avg. samples / sec: 6749.68
Iteration:   3060, Loss function: 3.880, Average Loss: 4.199, avg. samples / sec: 6746.52
Iteration:   3060, Loss function: 2.871, Average Loss: 4.224, avg. samples / sec: 6745.43
Iteration:   3060, Loss function: 3.219, Average Loss: 4.193, avg. samples / sec: 6747.51
Iteration:   3060, Loss function: 4.471, Average Loss: 4.215, avg. samples / sec: 6744.08
Iteration:   3060, Loss function: 3.660, Average Loss: 4.233, avg. samples / sec: 6746.85
Iteration:   3060, Loss function: 3.503, Average Loss: 4.223, avg. samples / sec: 6743.98
Iteration:   3060, Loss function: 3.634, Average Loss: 4.198, avg. samples / sec: 6744.10
Iteration:   3060, Loss function: 3.564, Average Loss: 4.207, avg. samples / sec: 6745.90
Iteration:   3060, Loss function: 3.865, Average Loss: 4.191, avg. samples / sec: 6745.93
Iteration:   3060, Loss function: 3.264, Average Loss: 4.188, avg. samples / sec: 6743.00
Iteration:   3060, Loss function: 4.613, Average Loss: 4.211, avg. samples / sec: 6744.10
Iteration:   3060, Loss function: 3.961, Average Loss: 4.192, avg. samples / sec: 6743.30
:::MLL 1558639263.326 epoch_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 819}}
:::MLL 1558639263.327 epoch_start: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 673}}
Iteration:   3080, Loss function: 3.869, Average Loss: 4.212, avg. samples / sec: 60063.32
Iteration:   3080, Loss function: 3.838, Average Loss: 4.204, avg. samples / sec: 60046.07
Iteration:   3080, Loss function: 3.014, Average Loss: 4.175, avg. samples / sec: 60112.02
Iteration:   3080, Loss function: 2.531, Average Loss: 4.220, avg. samples / sec: 60012.81
Iteration:   3080, Loss function: 3.097, Average Loss: 4.183, avg. samples / sec: 59777.67
Iteration:   3080, Loss function: 4.265, Average Loss: 4.181, avg. samples / sec: 59902.81
Iteration:   3080, Loss function: 3.594, Average Loss: 4.187, avg. samples / sec: 59855.29
Iteration:   3080, Loss function: 3.253, Average Loss: 4.210, avg. samples / sec: 59858.87
Iteration:   3080, Loss function: 4.798, Average Loss: 4.182, avg. samples / sec: 59991.19
Iteration:   3080, Loss function: 3.253, Average Loss: 4.182, avg. samples / sec: 60057.13
Iteration:   3080, Loss function: 3.695, Average Loss: 4.201, avg. samples / sec: 59892.68
Iteration:   3080, Loss function: 2.743, Average Loss: 4.184, avg. samples / sec: 59866.78
Iteration:   3080, Loss function: 2.854, Average Loss: 4.187, avg. samples / sec: 59734.17
Iteration:   3080, Loss function: 2.769, Average Loss: 4.195, avg. samples / sec: 59664.70
Iteration:   3080, Loss function: 3.887, Average Loss: 4.195, avg. samples / sec: 59865.02
Iteration:   3100, Loss function: 4.388, Average Loss: 4.174, avg. samples / sec: 58967.40
Iteration:   3100, Loss function: 2.760, Average Loss: 4.172, avg. samples / sec: 58837.91
Iteration:   3100, Loss function: 3.979, Average Loss: 4.182, avg. samples / sec: 58843.41
Iteration:   3100, Loss function: 3.647, Average Loss: 4.201, avg. samples / sec: 58853.41
Iteration:   3100, Loss function: 4.335, Average Loss: 4.190, avg. samples / sec: 58920.27
Iteration:   3100, Loss function: 2.985, Average Loss: 4.178, avg. samples / sec: 58907.46
Iteration:   3100, Loss function: 3.891, Average Loss: 4.168, avg. samples / sec: 58741.77
Iteration:   3100, Loss function: 2.520, Average Loss: 4.199, avg. samples / sec: 58606.66
Iteration:   3100, Loss function: 3.381, Average Loss: 4.173, avg. samples / sec: 58778.67
Iteration:   3100, Loss function: 3.241, Average Loss: 4.182, avg. samples / sec: 58889.42
Iteration:   3100, Loss function: 3.492, Average Loss: 4.212, avg. samples / sec: 58651.12
Iteration:   3100, Loss function: 5.143, Average Loss: 4.165, avg. samples / sec: 58602.39
Iteration:   3100, Loss function: 3.011, Average Loss: 4.169, avg. samples / sec: 58606.17
Iteration:   3100, Loss function: 3.738, Average Loss: 4.179, avg. samples / sec: 58736.73
Iteration:   3100, Loss function: 3.477, Average Loss: 4.195, avg. samples / sec: 58475.85
Iteration:   3120, Loss function: 3.176, Average Loss: 4.169, avg. samples / sec: 58410.32
Iteration:   3120, Loss function: 2.919, Average Loss: 4.180, avg. samples / sec: 58657.49
Iteration:   3120, Loss function: 5.127, Average Loss: 4.185, avg. samples / sec: 58490.75
Iteration:   3120, Loss function: 4.267, Average Loss: 4.155, avg. samples / sec: 58561.09
Iteration:   3120, Loss function: 3.071, Average Loss: 4.193, avg. samples / sec: 58472.94
Iteration:   3120, Loss function: 2.693, Average Loss: 4.160, avg. samples / sec: 58389.60
Iteration:   3120, Loss function: 2.382, Average Loss: 4.187, avg. samples / sec: 58263.33
Iteration:   3120, Loss function: 3.271, Average Loss: 4.159, avg. samples / sec: 58242.09
Iteration:   3120, Loss function: 4.510, Average Loss: 4.160, avg. samples / sec: 58230.01
Iteration:   3120, Loss function: 4.037, Average Loss: 4.171, avg. samples / sec: 58433.61
Iteration:   3120, Loss function: 4.286, Average Loss: 4.157, avg. samples / sec: 58339.35
Iteration:   3120, Loss function: 3.230, Average Loss: 4.174, avg. samples / sec: 58174.91
Iteration:   3120, Loss function: 2.950, Average Loss: 4.170, avg. samples / sec: 58243.92
Iteration:   3120, Loss function: 3.125, Average Loss: 4.148, avg. samples / sec: 58219.18
Iteration:   3120, Loss function: 5.179, Average Loss: 4.168, avg. samples / sec: 58077.67
Iteration:   3140, Loss function: 1.809, Average Loss: 4.169, avg. samples / sec: 57721.34
Iteration:   3140, Loss function: 2.476, Average Loss: 4.138, avg. samples / sec: 57762.51
Iteration:   3140, Loss function: 3.503, Average Loss: 4.155, avg. samples / sec: 57907.43
Iteration:   3140, Loss function: 4.251, Average Loss: 4.173, avg. samples / sec: 57764.59
Iteration:   3140, Loss function: 3.372, Average Loss: 4.139, avg. samples / sec: 57911.88
Iteration:   3140, Loss function: 3.147, Average Loss: 4.144, avg. samples / sec: 57764.78
Iteration:   3140, Loss function: 5.239, Average Loss: 4.182, avg. samples / sec: 57665.10
Iteration:   3140, Loss function: 4.323, Average Loss: 4.168, avg. samples / sec: 57612.09
Iteration:   3140, Loss function: 3.202, Average Loss: 4.163, avg. samples / sec: 57763.48
Iteration:   3140, Loss function: 3.403, Average Loss: 4.148, avg. samples / sec: 57705.86
Iteration:   3140, Loss function: 2.368, Average Loss: 4.159, avg. samples / sec: 57781.93
Iteration:   3140, Loss function: 4.022, Average Loss: 4.155, avg. samples / sec: 57983.03
Iteration:   3140, Loss function: 2.144, Average Loss: 4.147, avg. samples / sec: 57676.20
Iteration:   3140, Loss function: 3.091, Average Loss: 4.148, avg. samples / sec: 57744.54
Iteration:   3140, Loss function: 2.307, Average Loss: 4.156, avg. samples / sec: 57497.38
:::MLL 1558639265.338 epoch_stop: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 819}}
:::MLL 1558639265.338 epoch_start: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 673}}
Iteration:   3160, Loss function: 3.848, Average Loss: 4.144, avg. samples / sec: 59055.67
Iteration:   3160, Loss function: 3.603, Average Loss: 4.134, avg. samples / sec: 59076.14
Iteration:   3160, Loss function: 3.229, Average Loss: 4.155, avg. samples / sec: 58963.31
Iteration:   3160, Loss function: 4.364, Average Loss: 4.158, avg. samples / sec: 58915.68
Iteration:   3160, Loss function: 3.403, Average Loss: 4.144, avg. samples / sec: 58975.77
Iteration:   3160, Loss function: 2.750, Average Loss: 4.134, avg. samples / sec: 58933.97
Iteration:   3160, Loss function: 2.991, Average Loss: 4.171, avg. samples / sec: 58839.04
Iteration:   3160, Loss function: 3.961, Average Loss: 4.134, avg. samples / sec: 58837.19
Iteration:   3160, Loss function: 3.020, Average Loss: 4.127, avg. samples / sec: 58677.18
Iteration:   3160, Loss function: 3.464, Average Loss: 4.159, avg. samples / sec: 58747.11
Iteration:   3160, Loss function: 3.103, Average Loss: 4.131, avg. samples / sec: 58730.34
Iteration:   3160, Loss function: 4.864, Average Loss: 4.138, avg. samples / sec: 58803.63
Iteration:   3160, Loss function: 5.331, Average Loss: 4.145, avg. samples / sec: 58666.41
Iteration:   3160, Loss function: 3.769, Average Loss: 4.154, avg. samples / sec: 58541.63
Iteration:   3160, Loss function: 2.498, Average Loss: 4.134, avg. samples / sec: 58683.31
Iteration:   3180, Loss function: 3.811, Average Loss: 4.123, avg. samples / sec: 59923.39
Iteration:   3180, Loss function: 5.039, Average Loss: 4.117, avg. samples / sec: 60073.33
Iteration:   3180, Loss function: 1.721, Average Loss: 4.117, avg. samples / sec: 59981.77
Iteration:   3180, Loss function: 3.076, Average Loss: 4.143, avg. samples / sec: 60025.05
Iteration:   3180, Loss function: 3.421, Average Loss: 4.159, avg. samples / sec: 59927.95
Iteration:   3180, Loss function: 3.875, Average Loss: 4.126, avg. samples / sec: 60037.38
Iteration:   3180, Loss function: 3.936, Average Loss: 4.131, avg. samples / sec: 59710.91
Iteration:   3180, Loss function: 2.596, Average Loss: 4.117, avg. samples / sec: 59931.83
Iteration:   3180, Loss function: 3.030, Average Loss: 4.145, avg. samples / sec: 59780.41
Iteration:   3180, Loss function: 3.129, Average Loss: 4.136, avg. samples / sec: 59951.46
Iteration:   3180, Loss function: 4.249, Average Loss: 4.131, avg. samples / sec: 59787.06
Iteration:   3180, Loss function: 3.362, Average Loss: 4.121, avg. samples / sec: 59770.90
Iteration:   3180, Loss function: 3.512, Average Loss: 4.120, avg. samples / sec: 59995.46
Iteration:   3180, Loss function: 3.750, Average Loss: 4.141, avg. samples / sec: 59649.67
Iteration:   3180, Loss function: 3.726, Average Loss: 4.141, avg. samples / sec: 59669.60
Iteration:   3200, Loss function: 2.283, Average Loss: 4.130, avg. samples / sec: 57074.67
Iteration:   3200, Loss function: 3.001, Average Loss: 4.112, avg. samples / sec: 57098.08
Iteration:   3200, Loss function: 4.579, Average Loss: 4.108, avg. samples / sec: 57110.11
Iteration:   3200, Loss function: 3.510, Average Loss: 4.132, avg. samples / sec: 56935.33
Iteration:   3200, Loss function: 4.043, Average Loss: 4.128, avg. samples / sec: 57091.95
Iteration:   3200, Loss function: 3.834, Average Loss: 4.101, avg. samples / sec: 56980.34
Iteration:   3200, Loss function: 3.825, Average Loss: 4.124, avg. samples / sec: 56973.59
Iteration:   3200, Loss function: 3.123, Average Loss: 4.112, avg. samples / sec: 56855.26
Iteration:   3200, Loss function: 4.019, Average Loss: 4.150, avg. samples / sec: 56840.13
Iteration:   3200, Loss function: 2.710, Average Loss: 4.111, avg. samples / sec: 56688.15
Iteration:   3200, Loss function: 2.686, Average Loss: 4.119, avg. samples / sec: 56917.74
Iteration:   3200, Loss function: 4.027, Average Loss: 4.100, avg. samples / sec: 56711.01
Iteration:   3200, Loss function: 4.440, Average Loss: 4.126, avg. samples / sec: 56862.58
Iteration:   3200, Loss function: 3.895, Average Loss: 4.105, avg. samples / sec: 56682.54
Iteration:   3200, Loss function: 3.320, Average Loss: 4.125, avg. samples / sec: 57104.62
:::MLL 1558639267.350 epoch_stop: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 819}}
:::MLL 1558639267.351 epoch_start: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 673}}
Iteration:   3220, Loss function: 2.968, Average Loss: 4.124, avg. samples / sec: 58884.64
Iteration:   3220, Loss function: 3.204, Average Loss: 4.116, avg. samples / sec: 58790.58
Iteration:   3220, Loss function: 3.303, Average Loss: 4.104, avg. samples / sec: 59087.39
Iteration:   3220, Loss function: 3.479, Average Loss: 4.145, avg. samples / sec: 58916.47
Iteration:   3220, Loss function: 3.620, Average Loss: 4.099, avg. samples / sec: 58781.36
Iteration:   3220, Loss function: 2.893, Average Loss: 4.114, avg. samples / sec: 58869.98
Iteration:   3220, Loss function: 3.301, Average Loss: 4.101, avg. samples / sec: 58875.17
Iteration:   3220, Loss function: 2.393, Average Loss: 4.090, avg. samples / sec: 58748.41
Iteration:   3220, Loss function: 2.755, Average Loss: 4.098, avg. samples / sec: 58828.06
Iteration:   3220, Loss function: 3.993, Average Loss: 4.111, avg. samples / sec: 58815.56
Iteration:   3220, Loss function: 3.133, Average Loss: 4.094, avg. samples / sec: 58615.80
Iteration:   3220, Loss function: 4.091, Average Loss: 4.109, avg. samples / sec: 58810.09
Iteration:   3220, Loss function: 3.962, Average Loss: 4.091, avg. samples / sec: 58805.43
Iteration:   3220, Loss function: 4.000, Average Loss: 4.118, avg. samples / sec: 58619.36
Iteration:   3220, Loss function: 4.333, Average Loss: 4.091, avg. samples / sec: 58811.71
Iteration:   3240, Loss function: 2.359, Average Loss: 4.092, avg. samples / sec: 59516.94
Iteration:   3240, Loss function: 3.605, Average Loss: 4.091, avg. samples / sec: 59526.64
Iteration:   3240, Loss function: 3.642, Average Loss: 4.101, avg. samples / sec: 59621.53
Iteration:   3240, Loss function: 4.414, Average Loss: 4.076, avg. samples / sec: 59515.43
Iteration:   3240, Loss function: 2.565, Average Loss: 4.105, avg. samples / sec: 59341.16
Iteration:   3240, Loss function: 2.516, Average Loss: 4.079, avg. samples / sec: 59550.36
Iteration:   3240, Loss function: 4.197, Average Loss: 4.083, avg. samples / sec: 59434.26
Iteration:   3240, Loss function: 3.512, Average Loss: 4.104, avg. samples / sec: 59508.60
Iteration:   3240, Loss function: 3.543, Average Loss: 4.082, avg. samples / sec: 59439.67
Iteration:   3240, Loss function: 3.105, Average Loss: 4.081, avg. samples / sec: 59482.73
Iteration:   3240, Loss function: 3.944, Average Loss: 4.097, avg. samples / sec: 59417.72
Iteration:   3240, Loss function: 3.139, Average Loss: 4.109, avg. samples / sec: 59068.19
Iteration:   3240, Loss function: 3.456, Average Loss: 4.132, avg. samples / sec: 59143.05
Iteration:   3240, Loss function: 4.448, Average Loss: 4.093, avg. samples / sec: 59144.22
Iteration:   3240, Loss function: 4.724, Average Loss: 4.100, avg. samples / sec: 59099.38
Iteration:   3260, Loss function: 3.414, Average Loss: 4.064, avg. samples / sec: 59392.98
Iteration:   3260, Loss function: 3.448, Average Loss: 4.094, avg. samples / sec: 59306.50
Iteration:   3260, Loss function: 3.612, Average Loss: 4.094, avg. samples / sec: 59564.79
Iteration:   3260, Loss function: 4.889, Average Loss: 4.078, avg. samples / sec: 59105.90
Iteration:   3260, Loss function: 3.010, Average Loss: 4.063, avg. samples / sec: 59172.56
Iteration:   3260, Loss function: 3.409, Average Loss: 4.068, avg. samples / sec: 59275.44
Iteration:   3260, Loss function: 3.509, Average Loss: 4.067, avg. samples / sec: 59127.30
Iteration:   3260, Loss function: 2.755, Average Loss: 4.080, avg. samples / sec: 58988.23
Iteration:   3260, Loss function: 3.779, Average Loss: 4.082, avg. samples / sec: 59358.86
Iteration:   3260, Loss function: 4.318, Average Loss: 4.083, avg. samples / sec: 59195.75
Iteration:   3260, Loss function: 4.086, Average Loss: 4.118, avg. samples / sec: 59280.03
Iteration:   3260, Loss function: 2.645, Average Loss: 4.088, avg. samples / sec: 58849.55
Iteration:   3260, Loss function: 2.861, Average Loss: 4.102, avg. samples / sec: 59134.17
Iteration:   3260, Loss function: 2.141, Average Loss: 4.088, avg. samples / sec: 58982.73
Iteration:   3260, Loss function: 3.748, Average Loss: 4.071, avg. samples / sec: 58961.46
Iteration:   3280, Loss function: 2.157, Average Loss: 4.076, avg. samples / sec: 57185.77
Iteration:   3280, Loss function: 3.051, Average Loss: 4.099, avg. samples / sec: 57142.94
Iteration:   3280, Loss function: 5.332, Average Loss: 4.074, avg. samples / sec: 56947.04
Iteration:   3280, Loss function: 4.098, Average Loss: 4.067, avg. samples / sec: 56768.00
Iteration:   3280, Loss function: 4.022, Average Loss: 4.086, avg. samples / sec: 56697.00
Iteration:   3280, Loss function: 3.230, Average Loss: 4.056, avg. samples / sec: 57096.55
Iteration:   3280, Loss function: 2.644, Average Loss: 4.074, avg. samples / sec: 56996.88
Iteration:   3280, Loss function: 4.226, Average Loss: 4.061, avg. samples / sec: 56831.32
Iteration:   3280, Loss function: 3.940, Average Loss: 4.073, avg. samples / sec: 56807.96
Iteration:   3280, Loss function: 3.996, Average Loss: 4.056, avg. samples / sec: 56652.78
Iteration:   3280, Loss function: 3.874, Average Loss: 4.050, avg. samples / sec: 56527.01
Iteration:   3280, Loss function: 2.280, Average Loss: 4.104, avg. samples / sec: 56740.67
Iteration:   3280, Loss function: 3.691, Average Loss: 4.078, avg. samples / sec: 56454.30
Iteration:   3280, Loss function: 3.922, Average Loss: 4.068, avg. samples / sec: 56552.48
Iteration:   3280, Loss function: 3.710, Average Loss: 4.056, avg. samples / sec: 56482.01
:::MLL 1558639269.359 epoch_stop: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 819}}
:::MLL 1558639269.359 epoch_start: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 673}}
Iteration:   3300, Loss function: 3.166, Average Loss: 4.086, avg. samples / sec: 58810.46
Iteration:   3300, Loss function: 3.293, Average Loss: 4.042, avg. samples / sec: 59004.16
Iteration:   3300, Loss function: 3.660, Average Loss: 4.053, avg. samples / sec: 59144.69
Iteration:   3300, Loss function: 2.698, Average Loss: 4.058, avg. samples / sec: 58800.98
Iteration:   3300, Loss function: 2.744, Average Loss: 4.067, avg. samples / sec: 59077.35
Iteration:   3300, Loss function: 3.673, Average Loss: 4.064, avg. samples / sec: 58876.84
Iteration:   3300, Loss function: 2.113, Average Loss: 4.042, avg. samples / sec: 58904.73
Iteration:   3300, Loss function: 2.404, Average Loss: 4.044, avg. samples / sec: 58783.74
Iteration:   3300, Loss function: 2.238, Average Loss: 4.063, avg. samples / sec: 58697.10
Iteration:   3300, Loss function: 4.685, Average Loss: 4.043, avg. samples / sec: 58730.22
Iteration:   3300, Loss function: 2.865, Average Loss: 4.043, avg. samples / sec: 59047.08
Iteration:   3300, Loss function: 4.858, Average Loss: 4.102, avg. samples / sec: 58856.75
Iteration:   3300, Loss function: 4.926, Average Loss: 4.063, avg. samples / sec: 58567.76
Iteration:   3300, Loss function: 2.552, Average Loss: 4.078, avg. samples / sec: 58562.31
Iteration:   3300, Loss function: 3.374, Average Loss: 4.065, avg. samples / sec: 58301.67
Iteration:   3320, Loss function: 3.300, Average Loss: 4.070, avg. samples / sec: 58085.86
Iteration:   3320, Loss function: 3.118, Average Loss: 4.076, avg. samples / sec: 57731.29
Iteration:   3320, Loss function: 3.667, Average Loss: 4.029, avg. samples / sec: 57753.32
Iteration:   3320, Loss function: 3.722, Average Loss: 4.032, avg. samples / sec: 57833.97
Iteration:   3320, Loss function: 3.653, Average Loss: 4.053, avg. samples / sec: 58128.99
Iteration:   3320, Loss function: 2.737, Average Loss: 4.044, avg. samples / sec: 57692.96
Iteration:   3320, Loss function: 3.204, Average Loss: 4.055, avg. samples / sec: 57685.45
Iteration:   3320, Loss function: 1.985, Average Loss: 4.041, avg. samples / sec: 57625.56
Iteration:   3320, Loss function: 3.775, Average Loss: 4.026, avg. samples / sec: 57692.70
Iteration:   3320, Loss function: 5.007, Average Loss: 4.033, avg. samples / sec: 57652.41
Iteration:   3320, Loss function: 3.768, Average Loss: 4.090, avg. samples / sec: 57757.49
Iteration:   3320, Loss function: 2.924, Average Loss: 4.054, avg. samples / sec: 57796.40
Iteration:   3320, Loss function: 2.518, Average Loss: 4.054, avg. samples / sec: 57605.31
Iteration:   3320, Loss function: 3.007, Average Loss: 4.048, avg. samples / sec: 57628.56
Iteration:   3320, Loss function: 3.828, Average Loss: 4.032, avg. samples / sec: 57645.34
Iteration:   3340, Loss function: 4.055, Average Loss: 4.017, avg. samples / sec: 59706.96
Iteration:   3340, Loss function: 3.784, Average Loss: 4.064, avg. samples / sec: 59605.62
Iteration:   3340, Loss function: 3.732, Average Loss: 4.043, avg. samples / sec: 59849.39
Iteration:   3340, Loss function: 4.124, Average Loss: 4.040, avg. samples / sec: 59758.51
Iteration:   3340, Loss function: 4.117, Average Loss: 4.030, avg. samples / sec: 59788.88
Iteration:   3340, Loss function: 3.474, Average Loss: 4.028, avg. samples / sec: 59710.58
Iteration:   3340, Loss function: 4.683, Average Loss: 4.024, avg. samples / sec: 59766.80
Iteration:   3340, Loss function: 3.467, Average Loss: 4.041, avg. samples / sec: 59814.44
Iteration:   3340, Loss function: 3.429, Average Loss: 4.042, avg. samples / sec: 59539.92
Iteration:   3340, Loss function: 3.464, Average Loss: 4.012, avg. samples / sec: 59676.65
Iteration:   3340, Loss function: 2.438, Average Loss: 4.019, avg. samples / sec: 59433.51
Iteration:   3340, Loss function: 3.365, Average Loss: 4.058, avg. samples / sec: 59339.91
Iteration:   3340, Loss function: 3.634, Average Loss: 4.076, avg. samples / sec: 59601.87
Iteration:   3340, Loss function: 2.244, Average Loss: 4.040, avg. samples / sec: 59627.56
Iteration:   3340, Loss function: 4.634, Average Loss: 4.020, avg. samples / sec: 59661.92
:::MLL 1558639271.374 epoch_stop: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 819}}
:::MLL 1558639271.375 epoch_start: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 673}}
Iteration:   3360, Loss function: 3.939, Average Loss: 4.005, avg. samples / sec: 58034.77
Iteration:   3360, Loss function: 4.221, Average Loss: 4.004, avg. samples / sec: 57664.04
Iteration:   3360, Loss function: 2.103, Average Loss: 4.030, avg. samples / sec: 57710.14
Iteration:   3360, Loss function: 3.273, Average Loss: 4.002, avg. samples / sec: 57805.29
Iteration:   3360, Loss function: 3.453, Average Loss: 4.030, avg. samples / sec: 57890.30
Iteration:   3360, Loss function: 3.572, Average Loss: 4.053, avg. samples / sec: 57813.57
Iteration:   3360, Loss function: 2.986, Average Loss: 4.029, avg. samples / sec: 57608.96
Iteration:   3360, Loss function: 2.979, Average Loss: 4.008, avg. samples / sec: 57757.89
Iteration:   3360, Loss function: 3.720, Average Loss: 4.016, avg. samples / sec: 57601.63
Iteration:   3360, Loss function: 3.650, Average Loss: 4.030, avg. samples / sec: 57665.67
Iteration:   3360, Loss function: 4.547, Average Loss: 4.066, avg. samples / sec: 57732.90
Iteration:   3360, Loss function: 4.000, Average Loss: 4.014, avg. samples / sec: 57549.65
Iteration:   3360, Loss function: 3.103, Average Loss: 4.026, avg. samples / sec: 57549.48
Iteration:   3360, Loss function: 3.251, Average Loss: 4.051, avg. samples / sec: 57469.15
Iteration:   3360, Loss function: 4.178, Average Loss: 4.018, avg. samples / sec: 57465.15
Iteration:   3380, Loss function: 3.467, Average Loss: 4.007, avg. samples / sec: 58674.29
Iteration:   3380, Loss function: 3.355, Average Loss: 3.993, avg. samples / sec: 58591.18
Iteration:   3380, Loss function: 4.071, Average Loss: 4.018, avg. samples / sec: 58528.04
Iteration:   3380, Loss function: 4.064, Average Loss: 4.020, avg. samples / sec: 58484.17
Iteration:   3380, Loss function: 3.426, Average Loss: 4.040, avg. samples / sec: 58616.68
Iteration:   3380, Loss function: 2.801, Average Loss: 4.011, avg. samples / sec: 58673.63
Iteration:   3380, Loss function: 3.234, Average Loss: 4.020, avg. samples / sec: 58381.21
Iteration:   3380, Loss function: 3.436, Average Loss: 4.017, avg. samples / sec: 58577.89
Iteration:   3380, Loss function: 3.151, Average Loss: 4.015, avg. samples / sec: 58517.42
Iteration:   3380, Loss function: 2.643, Average Loss: 3.990, avg. samples / sec: 58192.33
Iteration:   3380, Loss function: 2.811, Average Loss: 3.998, avg. samples / sec: 58400.08
Iteration:   3380, Loss function: 3.143, Average Loss: 3.991, avg. samples / sec: 58233.86
Iteration:   3380, Loss function: 3.497, Average Loss: 4.006, avg. samples / sec: 58451.99
Iteration:   3380, Loss function: 3.299, Average Loss: 4.045, avg. samples / sec: 58265.78
Iteration:   3380, Loss function: 3.308, Average Loss: 4.051, avg. samples / sec: 58255.98
Iteration:   3400, Loss function: 3.705, Average Loss: 3.986, avg. samples / sec: 58816.27
Iteration:   3400, Loss function: 3.729, Average Loss: 4.000, avg. samples / sec: 58622.92
Iteration:   3400, Loss function: 2.724, Average Loss: 4.003, avg. samples / sec: 58590.36
Iteration:   3400, Loss function: 2.333, Average Loss: 3.978, avg. samples / sec: 58575.53
Iteration:   3400, Loss function: 2.580, Average Loss: 3.981, avg. samples / sec: 58568.81
Iteration:   3400, Loss function: 4.421, Average Loss: 3.984, avg. samples / sec: 58268.22
Iteration:   3400, Loss function: 3.356, Average Loss: 4.012, avg. samples / sec: 58447.69
Iteration:   3400, Loss function: 4.587, Average Loss: 4.035, avg. samples / sec: 58636.28
Iteration:   3400, Loss function: 3.947, Average Loss: 3.997, avg. samples / sec: 58237.59
Iteration:   3400, Loss function: 4.394, Average Loss: 4.036, avg. samples / sec: 58729.78
Iteration:   3400, Loss function: 3.904, Average Loss: 4.011, avg. samples / sec: 58387.11
Iteration:   3400, Loss function: 4.252, Average Loss: 4.032, avg. samples / sec: 58379.97
Iteration:   3400, Loss function: 3.954, Average Loss: 4.000, avg. samples / sec: 58522.84
Iteration:   3400, Loss function: 4.670, Average Loss: 4.006, avg. samples / sec: 58182.62
Iteration:   3400, Loss function: 2.872, Average Loss: 4.001, avg. samples / sec: 58162.79
Iteration:   3420, Loss function: 3.653, Average Loss: 3.992, avg. samples / sec: 59885.53
Iteration:   3420, Loss function: 3.388, Average Loss: 3.990, avg. samples / sec: 59656.06
Iteration:   3420, Loss function: 2.907, Average Loss: 3.978, avg. samples / sec: 59840.44
Iteration:   3420, Loss function: 3.387, Average Loss: 4.001, avg. samples / sec: 59884.46
Iteration:   3420, Loss function: 2.638, Average Loss: 3.973, avg. samples / sec: 59497.59
Iteration:   3420, Loss function: 2.949, Average Loss: 3.999, avg. samples / sec: 59770.22
Iteration:   3420, Loss function: 2.251, Average Loss: 3.969, avg. samples / sec: 59714.30
Iteration:   3420, Loss function: 3.765, Average Loss: 4.019, avg. samples / sec: 59775.34
Iteration:   3420, Loss function: 2.096, Average Loss: 3.989, avg. samples / sec: 59797.56
Iteration:   3420, Loss function: 2.326, Average Loss: 4.020, avg. samples / sec: 59732.45
Iteration:   3420, Loss function: 2.776, Average Loss: 3.965, avg. samples / sec: 59689.59
Iteration:   3420, Loss function: 2.953, Average Loss: 3.988, avg. samples / sec: 60018.33
Iteration:   3420, Loss function: 3.097, Average Loss: 3.989, avg. samples / sec: 59660.00
Iteration:   3420, Loss function: 4.211, Average Loss: 4.021, avg. samples / sec: 59641.80
Iteration:   3420, Loss function: 2.543, Average Loss: 3.992, avg. samples / sec: 59691.08
:::MLL 1558639273.356 eval_start: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.59 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.53s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.55s)
DONE (t=0.58s)
DONE (t=2.86s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22478
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.38516
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23324
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05869
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.23840
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.36038
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22038
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32044
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33687
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.10084
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36660
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.51802
Current AP: 0.22478 AP goal: 0.23000
:::MLL 1558639277.407 eval_accuracy: {"value": 0.22477800080918894, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 389}}
:::MLL 1558639277.495 eval_stop: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 392}}
:::MLL 1558639277.505 block_stop: {"value": null, "metadata": {"first_epoch_num": 44, "file": "train.py", "lineno": 804}}
:::MLL 1558639277.505 block_start: {"value": null, "metadata": {"first_epoch_num": 49, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
:::MLL 1558639277.532 epoch_stop: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 819}}
:::MLL 1558639277.532 epoch_start: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 673}}
Iteration:   3440, Loss function: 3.606, Average Loss: 3.978, avg. samples / sec: 7107.38
Iteration:   3440, Loss function: 2.056, Average Loss: 3.979, avg. samples / sec: 7104.78
Iteration:   3440, Loss function: 2.499, Average Loss: 4.005, avg. samples / sec: 7109.69
Iteration:   3440, Loss function: 3.959, Average Loss: 4.004, avg. samples / sec: 7107.58
Iteration:   3440, Loss function: 4.132, Average Loss: 3.965, avg. samples / sec: 7105.84
Iteration:   3440, Loss function: 3.035, Average Loss: 3.972, avg. samples / sec: 7107.70
Iteration:   3440, Loss function: 2.529, Average Loss: 3.976, avg. samples / sec: 7107.30
Iteration:   3440, Loss function: 2.790, Average Loss: 3.958, avg. samples / sec: 7106.45
Iteration:   3440, Loss function: 4.480, Average Loss: 3.954, avg. samples / sec: 7106.51
Iteration:   3440, Loss function: 5.062, Average Loss: 3.964, avg. samples / sec: 7105.16
Iteration:   3440, Loss function: 2.990, Average Loss: 4.011, avg. samples / sec: 7105.28
Iteration:   3440, Loss function: 3.367, Average Loss: 3.982, avg. samples / sec: 7107.75
Iteration:   3440, Loss function: 4.114, Average Loss: 3.982, avg. samples / sec: 7106.37
Iteration:   3440, Loss function: 4.408, Average Loss: 3.992, avg. samples / sec: 7102.97
Iteration:   3440, Loss function: 2.962, Average Loss: 3.983, avg. samples / sec: 7102.74
Iteration:   3460, Loss function: 3.280, Average Loss: 3.967, avg. samples / sec: 57873.95
Iteration:   3460, Loss function: 2.966, Average Loss: 3.949, avg. samples / sec: 58076.53
Iteration:   3460, Loss function: 4.088, Average Loss: 3.979, avg. samples / sec: 58171.48
Iteration:   3460, Loss function: 3.317, Average Loss: 3.972, avg. samples / sec: 58137.72
Iteration:   3460, Loss function: 3.921, Average Loss: 3.956, avg. samples / sec: 58061.36
Iteration:   3460, Loss function: 3.988, Average Loss: 3.963, avg. samples / sec: 57920.69
Iteration:   3460, Loss function: 3.156, Average Loss: 3.992, avg. samples / sec: 57848.10
Iteration:   3460, Loss function: 2.798, Average Loss: 3.968, avg. samples / sec: 57818.81
Iteration:   3460, Loss function: 4.465, Average Loss: 3.954, avg. samples / sec: 57893.94
Iteration:   3460, Loss function: 3.258, Average Loss: 3.970, avg. samples / sec: 58155.88
Iteration:   3460, Loss function: 3.802, Average Loss: 3.943, avg. samples / sec: 57908.00
Iteration:   3460, Loss function: 2.258, Average Loss: 3.999, avg. samples / sec: 57956.49
Iteration:   3460, Loss function: 2.307, Average Loss: 4.001, avg. samples / sec: 57788.77
Iteration:   3460, Loss function: 4.280, Average Loss: 3.968, avg. samples / sec: 57934.00
Iteration:   3460, Loss function: 4.500, Average Loss: 3.962, avg. samples / sec: 57766.36
Iteration:   3480, Loss function: 3.004, Average Loss: 3.936, avg. samples / sec: 60296.64
Iteration:   3480, Loss function: 2.979, Average Loss: 3.974, avg. samples / sec: 60114.69
Iteration:   3480, Loss function: 3.485, Average Loss: 3.937, avg. samples / sec: 60213.71
Iteration:   3480, Loss function: 2.497, Average Loss: 3.993, avg. samples / sec: 60284.59
Iteration:   3480, Loss function: 2.939, Average Loss: 3.958, avg. samples / sec: 60288.18
Iteration:   3480, Loss function: 2.843, Average Loss: 3.958, avg. samples / sec: 60058.71
Iteration:   3480, Loss function: 4.433, Average Loss: 3.962, avg. samples / sec: 60022.70
Iteration:   3480, Loss function: 3.511, Average Loss: 3.986, avg. samples / sec: 60092.80
Iteration:   3480, Loss function: 2.717, Average Loss: 3.943, avg. samples / sec: 60000.03
Iteration:   3480, Loss function: 3.407, Average Loss: 3.981, avg. samples / sec: 59974.06
Iteration:   3480, Loss function: 2.507, Average Loss: 3.959, avg. samples / sec: 59804.94
Iteration:   3480, Loss function: 3.737, Average Loss: 3.952, avg. samples / sec: 59877.13
Iteration:   3480, Loss function: 3.994, Average Loss: 3.950, avg. samples / sec: 60014.72
Iteration:   3480, Loss function: 3.040, Average Loss: 3.942, avg. samples / sec: 59775.57
Iteration:   3480, Loss function: 4.046, Average Loss: 3.962, avg. samples / sec: 59569.49
:::MLL 1558639279.549 epoch_stop: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 819}}
:::MLL 1558639279.550 epoch_start: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 673}}
Iteration:   3500, Loss function: 5.399, Average Loss: 3.974, avg. samples / sec: 56738.75
Iteration:   3500, Loss function: 3.302, Average Loss: 3.974, avg. samples / sec: 56681.54
Iteration:   3500, Loss function: 3.115, Average Loss: 3.951, avg. samples / sec: 56565.83
Iteration:   3500, Loss function: 4.213, Average Loss: 3.947, avg. samples / sec: 56682.47
Iteration:   3500, Loss function: 3.142, Average Loss: 3.934, avg. samples / sec: 56589.22
Iteration:   3500, Loss function: 3.482, Average Loss: 3.937, avg. samples / sec: 56692.71
Iteration:   3500, Loss function: 2.775, Average Loss: 3.962, avg. samples / sec: 56379.14
Iteration:   3500, Loss function: 3.721, Average Loss: 3.949, avg. samples / sec: 56549.06
Iteration:   3500, Loss function: 4.305, Average Loss: 3.927, avg. samples / sec: 56176.15
Iteration:   3500, Loss function: 3.638, Average Loss: 3.985, avg. samples / sec: 56373.39
Iteration:   3500, Loss function: 3.092, Average Loss: 3.947, avg. samples / sec: 56607.75
Iteration:   3500, Loss function: 3.659, Average Loss: 3.961, avg. samples / sec: 56801.32
Iteration:   3500, Loss function: 3.228, Average Loss: 3.933, avg. samples / sec: 56558.45
Iteration:   3500, Loss function: 3.524, Average Loss: 3.928, avg. samples / sec: 56151.76
Iteration:   3500, Loss function: 3.988, Average Loss: 3.948, avg. samples / sec: 56165.72
Iteration:   3520, Loss function: 3.215, Average Loss: 3.942, avg. samples / sec: 58994.95
Iteration:   3520, Loss function: 2.276, Average Loss: 3.961, avg. samples / sec: 58673.00
Iteration:   3520, Loss function: 2.695, Average Loss: 3.940, avg. samples / sec: 58755.48
Iteration:   3520, Loss function: 3.685, Average Loss: 3.926, avg. samples / sec: 58770.70
Iteration:   3520, Loss function: 2.448, Average Loss: 3.916, avg. samples / sec: 59045.84
Iteration:   3520, Loss function: 3.101, Average Loss: 3.933, avg. samples / sec: 59061.46
Iteration:   3520, Loss function: 3.023, Average Loss: 3.936, avg. samples / sec: 58738.00
Iteration:   3520, Loss function: 3.829, Average Loss: 3.959, avg. samples / sec: 58625.62
Iteration:   3520, Loss function: 2.600, Average Loss: 3.949, avg. samples / sec: 58739.08
Iteration:   3520, Loss function: 2.666, Average Loss: 3.971, avg. samples / sec: 58743.56
Iteration:   3520, Loss function: 2.115, Average Loss: 3.941, avg. samples / sec: 58708.39
Iteration:   3520, Loss function: 3.264, Average Loss: 3.927, avg. samples / sec: 58534.00
Iteration:   3520, Loss function: 3.236, Average Loss: 3.920, avg. samples / sec: 58695.48
Iteration:   3520, Loss function: 3.722, Average Loss: 3.921, avg. samples / sec: 58583.41
Iteration:   3520, Loss function: 3.334, Average Loss: 3.950, avg. samples / sec: 58609.97
Iteration:   3540, Loss function: 2.351, Average Loss: 3.933, avg. samples / sec: 56910.55
Iteration:   3540, Loss function: 2.697, Average Loss: 3.947, avg. samples / sec: 56959.29
Iteration:   3540, Loss function: 3.549, Average Loss: 3.934, avg. samples / sec: 56776.74
Iteration:   3540, Loss function: 3.060, Average Loss: 3.902, avg. samples / sec: 56924.25
Iteration:   3540, Loss function: 2.843, Average Loss: 3.909, avg. samples / sec: 57147.18
Iteration:   3540, Loss function: 2.922, Average Loss: 3.914, avg. samples / sec: 56879.54
Iteration:   3540, Loss function: 4.014, Average Loss: 3.949, avg. samples / sec: 56812.97
Iteration:   3540, Loss function: 3.105, Average Loss: 3.921, avg. samples / sec: 57072.73
Iteration:   3540, Loss function: 3.073, Average Loss: 3.921, avg. samples / sec: 56853.89
Iteration:   3540, Loss function: 3.046, Average Loss: 3.909, avg. samples / sec: 57064.32
Iteration:   3540, Loss function: 3.206, Average Loss: 3.966, avg. samples / sec: 56893.45
Iteration:   3540, Loss function: 3.132, Average Loss: 3.924, avg. samples / sec: 56815.58
Iteration:   3540, Loss function: 3.060, Average Loss: 3.939, avg. samples / sec: 57026.59
Iteration:   3540, Loss function: 3.314, Average Loss: 3.938, avg. samples / sec: 56837.38
Iteration:   3540, Loss function: 3.261, Average Loss: 3.933, avg. samples / sec: 56803.15
:::MLL 1558639281.275 epoch_stop: {"value": null, "metadata": {"epoch_num": 51, "file": "train.py", "lineno": 819}}
:::MLL 1558639281.275 epoch_start: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 673}}
Iteration:   3560, Loss function: 3.059, Average Loss: 3.919, avg. samples / sec: 59625.90
Iteration:   3560, Loss function: 3.446, Average Loss: 3.930, avg. samples / sec: 59778.21
Iteration:   3560, Loss function: 2.323, Average Loss: 3.936, avg. samples / sec: 59646.04
Iteration:   3560, Loss function: 3.613, Average Loss: 3.908, avg. samples / sec: 59707.80
Iteration:   3560, Loss function: 3.339, Average Loss: 3.895, avg. samples / sec: 59564.53
Iteration:   3560, Loss function: 3.097, Average Loss: 3.930, avg. samples / sec: 59534.84
Iteration:   3560, Loss function: 4.169, Average Loss: 3.950, avg. samples / sec: 59658.46
Iteration:   3560, Loss function: 2.980, Average Loss: 3.925, avg. samples / sec: 59750.96
Iteration:   3560, Loss function: 3.023, Average Loss: 3.901, avg. samples / sec: 59621.66
Iteration:   3560, Loss function: 3.615, Average Loss: 3.916, avg. samples / sec: 59586.80
Iteration:   3560, Loss function: 3.022, Average Loss: 3.902, avg. samples / sec: 59510.18
Iteration:   3560, Loss function: 3.706, Average Loss: 3.912, avg. samples / sec: 59573.67
Iteration:   3560, Loss function: 2.332, Average Loss: 3.904, avg. samples / sec: 59453.11
Iteration:   3560, Loss function: 1.966, Average Loss: 3.921, avg. samples / sec: 59418.30
Iteration:   3560, Loss function: 3.263, Average Loss: 3.930, avg. samples / sec: 59435.31
Iteration:   3580, Loss function: 4.232, Average Loss: 3.942, avg. samples / sec: 59086.72
Iteration:   3580, Loss function: 3.095, Average Loss: 3.887, avg. samples / sec: 58948.16
Iteration:   3580, Loss function: 5.052, Average Loss: 3.895, avg. samples / sec: 59002.86
Iteration:   3580, Loss function: 3.829, Average Loss: 3.915, avg. samples / sec: 59090.06
Iteration:   3580, Loss function: 3.108, Average Loss: 3.907, avg. samples / sec: 58953.15
Iteration:   3580, Loss function: 3.815, Average Loss: 3.907, avg. samples / sec: 58857.30
Iteration:   3580, Loss function: 3.959, Average Loss: 3.905, avg. samples / sec: 58882.72
Iteration:   3580, Loss function: 3.254, Average Loss: 3.913, avg. samples / sec: 58720.35
Iteration:   3580, Loss function: 3.299, Average Loss: 3.886, avg. samples / sec: 58832.60
Iteration:   3580, Loss function: 3.390, Average Loss: 3.920, avg. samples / sec: 58797.08
Iteration:   3580, Loss function: 4.774, Average Loss: 3.896, avg. samples / sec: 58808.89
Iteration:   3580, Loss function: 3.651, Average Loss: 3.900, avg. samples / sec: 58525.76
Iteration:   3580, Loss function: 4.020, Average Loss: 3.924, avg. samples / sec: 58533.19
Iteration:   3580, Loss function: 4.054, Average Loss: 3.926, avg. samples / sec: 58544.16
Iteration:   3580, Loss function: 4.513, Average Loss: 3.922, avg. samples / sec: 58453.83
Iteration:   3600, Loss function: 3.468, Average Loss: 3.879, avg. samples / sec: 57699.48
Iteration:   3600, Loss function: 3.177, Average Loss: 3.900, avg. samples / sec: 57786.00
Iteration:   3600, Loss function: 4.087, Average Loss: 3.894, avg. samples / sec: 57697.90
Iteration:   3600, Loss function: 3.371, Average Loss: 3.914, avg. samples / sec: 57738.91
Iteration:   3600, Loss function: 3.353, Average Loss: 3.919, avg. samples / sec: 57903.67
Iteration:   3600, Loss function: 2.538, Average Loss: 3.933, avg. samples / sec: 57390.28
Iteration:   3600, Loss function: 3.847, Average Loss: 3.900, avg. samples / sec: 57630.82
Iteration:   3600, Loss function: 3.326, Average Loss: 3.884, avg. samples / sec: 57738.13
Iteration:   3600, Loss function: 2.845, Average Loss: 3.910, avg. samples / sec: 57592.76
Iteration:   3600, Loss function: 3.476, Average Loss: 3.887, avg. samples / sec: 57860.95
Iteration:   3600, Loss function: 3.362, Average Loss: 3.912, avg. samples / sec: 57865.51
Iteration:   3600, Loss function: 2.907, Average Loss: 3.882, avg. samples / sec: 57469.04
Iteration:   3600, Loss function: 3.485, Average Loss: 3.900, avg. samples / sec: 57572.15
Iteration:   3600, Loss function: 2.618, Average Loss: 3.912, avg. samples / sec: 57746.86
Iteration:   3600, Loss function: 2.190, Average Loss: 3.878, avg. samples / sec: 57469.93
Iteration:   3620, Loss function: 2.341, Average Loss: 3.866, avg. samples / sec: 60410.26
Iteration:   3620, Loss function: 3.593, Average Loss: 3.883, avg. samples / sec: 60101.36
Iteration:   3620, Loss function: 2.906, Average Loss: 3.920, avg. samples / sec: 60160.16
Iteration:   3620, Loss function: 3.816, Average Loss: 3.887, avg. samples / sec: 60248.92
Iteration:   3620, Loss function: 2.982, Average Loss: 3.894, avg. samples / sec: 60010.99
Iteration:   3620, Loss function: 3.823, Average Loss: 3.894, avg. samples / sec: 60076.43
Iteration:   3620, Loss function: 2.714, Average Loss: 3.905, avg. samples / sec: 60036.63
Iteration:   3620, Loss function: 3.726, Average Loss: 3.904, avg. samples / sec: 59992.65
Iteration:   3620, Loss function: 2.860, Average Loss: 3.870, avg. samples / sec: 59817.56
Iteration:   3620, Loss function: 3.206, Average Loss: 3.896, avg. samples / sec: 60182.05
Iteration:   3620, Loss function: 2.574, Average Loss: 3.879, avg. samples / sec: 60061.99
Iteration:   3620, Loss function: 2.913, Average Loss: 3.900, avg. samples / sec: 60017.28
Iteration:   3620, Loss function: 3.812, Average Loss: 3.875, avg. samples / sec: 59973.53
Iteration:   3620, Loss function: 3.395, Average Loss: 3.879, avg. samples / sec: 60002.05
Iteration:   3620, Loss function: 2.924, Average Loss: 3.905, avg. samples / sec: 59980.45
:::MLL 1558639283.273 epoch_stop: {"value": null, "metadata": {"epoch_num": 52, "file": "train.py", "lineno": 819}}
:::MLL 1558639283.274 epoch_start: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 673}}
Iteration:   3640, Loss function: 3.369, Average Loss: 3.881, avg. samples / sec: 57824.84
Iteration:   3640, Loss function: 3.522, Average Loss: 3.878, avg. samples / sec: 57794.46
Iteration:   3640, Loss function: 3.015, Average Loss: 3.885, avg. samples / sec: 57945.12
Iteration:   3640, Loss function: 3.634, Average Loss: 3.885, avg. samples / sec: 57835.85
Iteration:   3640, Loss function: 2.863, Average Loss: 3.876, avg. samples / sec: 57751.24
Iteration:   3640, Loss function: 3.192, Average Loss: 3.856, avg. samples / sec: 57704.11
Iteration:   3640, Loss function: 3.495, Average Loss: 3.872, avg. samples / sec: 57890.35
Iteration:   3640, Loss function: 2.920, Average Loss: 3.867, avg. samples / sec: 57915.90
Iteration:   3640, Loss function: 3.883, Average Loss: 3.898, avg. samples / sec: 57936.92
Iteration:   3640, Loss function: 3.083, Average Loss: 3.873, avg. samples / sec: 57893.35
Iteration:   3640, Loss function: 4.032, Average Loss: 3.894, avg. samples / sec: 57802.52
Iteration:   3640, Loss function: 3.335, Average Loss: 3.908, avg. samples / sec: 57675.87
Iteration:   3640, Loss function: 2.542, Average Loss: 3.867, avg. samples / sec: 57797.38
Iteration:   3640, Loss function: 3.509, Average Loss: 3.885, avg. samples / sec: 57768.26
Iteration:   3640, Loss function: 2.204, Average Loss: 3.893, avg. samples / sec: 57623.06
Iteration:   3660, Loss function: 3.461, Average Loss: 3.880, avg. samples / sec: 57014.48
Iteration:   3660, Loss function: 2.945, Average Loss: 3.872, avg. samples / sec: 56935.54
Iteration:   3660, Loss function: 2.729, Average Loss: 3.873, avg. samples / sec: 57080.92
Iteration:   3660, Loss function: 4.496, Average Loss: 3.863, avg. samples / sec: 56947.62
Iteration:   3660, Loss function: 2.815, Average Loss: 3.898, avg. samples / sec: 56996.98
Iteration:   3660, Loss function: 4.687, Average Loss: 3.880, avg. samples / sec: 56893.32
Iteration:   3660, Loss function: 2.836, Average Loss: 3.885, avg. samples / sec: 57146.26
Iteration:   3660, Loss function: 3.335, Average Loss: 3.861, avg. samples / sec: 56916.98
Iteration:   3660, Loss function: 2.617, Average Loss: 3.864, avg. samples / sec: 56852.33
Iteration:   3660, Loss function: 3.155, Average Loss: 3.890, avg. samples / sec: 56941.61
Iteration:   3660, Loss function: 2.263, Average Loss: 3.854, avg. samples / sec: 56939.93
Iteration:   3660, Loss function: 3.264, Average Loss: 3.849, avg. samples / sec: 56871.99
Iteration:   3660, Loss function: 3.320, Average Loss: 3.887, avg. samples / sec: 56889.55
Iteration:   3660, Loss function: 4.811, Average Loss: 3.867, avg. samples / sec: 56846.25
Iteration:   3660, Loss function: 3.658, Average Loss: 3.867, avg. samples / sec: 56793.95
Iteration:   3680, Loss function: 3.536, Average Loss: 3.886, avg. samples / sec: 59630.67
Iteration:   3680, Loss function: 3.820, Average Loss: 3.868, avg. samples / sec: 59566.07
Iteration:   3680, Loss function: 3.768, Average Loss: 3.873, avg. samples / sec: 59571.73
Iteration:   3680, Loss function: 3.484, Average Loss: 3.855, avg. samples / sec: 59524.91
Iteration:   3680, Loss function: 4.429, Average Loss: 3.853, avg. samples / sec: 59512.77
Iteration:   3680, Loss function: 3.246, Average Loss: 3.868, avg. samples / sec: 59362.46
Iteration:   3680, Loss function: 3.029, Average Loss: 3.860, avg. samples / sec: 59572.46
Iteration:   3680, Loss function: 3.299, Average Loss: 3.843, avg. samples / sec: 59483.10
Iteration:   3680, Loss function: 3.530, Average Loss: 3.854, avg. samples / sec: 59434.44
Iteration:   3680, Loss function: 4.015, Average Loss: 3.882, avg. samples / sec: 59473.51
Iteration:   3680, Loss function: 1.920, Average Loss: 3.864, avg. samples / sec: 59339.54
Iteration:   3680, Loss function: 3.366, Average Loss: 3.846, avg. samples / sec: 59459.79
Iteration:   3680, Loss function: 2.904, Average Loss: 3.854, avg. samples / sec: 59525.74
Iteration:   3680, Loss function: 3.973, Average Loss: 3.862, avg. samples / sec: 59230.13
Iteration:   3680, Loss function: 3.763, Average Loss: 3.877, avg. samples / sec: 59160.09
:::MLL 1558639285.293 epoch_stop: {"value": null, "metadata": {"epoch_num": 53, "file": "train.py", "lineno": 819}}
:::MLL 1558639285.293 epoch_start: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 673}}
Iteration:   3700, Loss function: 2.732, Average Loss: 3.849, avg. samples / sec: 59501.54
Iteration:   3700, Loss function: 3.309, Average Loss: 3.847, avg. samples / sec: 59360.16
Iteration:   3700, Loss function: 4.902, Average Loss: 3.874, avg. samples / sec: 59363.51
Iteration:   3700, Loss function: 3.019, Average Loss: 3.836, avg. samples / sec: 59335.27
Iteration:   3700, Loss function: 3.150, Average Loss: 3.865, avg. samples / sec: 59610.36
Iteration:   3700, Loss function: 3.761, Average Loss: 3.862, avg. samples / sec: 59128.12
Iteration:   3700, Loss function: 3.447, Average Loss: 3.860, avg. samples / sec: 59302.63
Iteration:   3700, Loss function: 4.096, Average Loss: 3.851, avg. samples / sec: 59239.52
Iteration:   3700, Loss function: 4.373, Average Loss: 3.848, avg. samples / sec: 59151.87
Iteration:   3700, Loss function: 3.063, Average Loss: 3.842, avg. samples / sec: 59267.99
Iteration:   3700, Loss function: 3.433, Average Loss: 3.861, avg. samples / sec: 59179.49
Iteration:   3700, Loss function: 3.671, Average Loss: 3.842, avg. samples / sec: 59160.81
Iteration:   3700, Loss function: 3.561, Average Loss: 3.836, avg. samples / sec: 59245.17
Iteration:   3700, Loss function: 2.723, Average Loss: 3.860, avg. samples / sec: 59048.17
Iteration:   3700, Loss function: 2.719, Average Loss: 3.875, avg. samples / sec: 58960.87
Iteration:   3720, Loss function: 2.763, Average Loss: 3.858, avg. samples / sec: 55828.39
Iteration:   3720, Loss function: 3.041, Average Loss: 3.840, avg. samples / sec: 55725.98
Iteration:   3720, Loss function: 3.864, Average Loss: 3.866, avg. samples / sec: 55730.59
Iteration:   3720, Loss function: 3.680, Average Loss: 3.853, avg. samples / sec: 55745.31
Iteration:   3720, Loss function: 2.367, Average Loss: 3.864, avg. samples / sec: 55840.43
Iteration:   3720, Loss function: 3.305, Average Loss: 3.851, avg. samples / sec: 55700.54
Iteration:   3720, Loss function: 3.102, Average Loss: 3.841, avg. samples / sec: 55628.27
Iteration:   3720, Loss function: 2.891, Average Loss: 3.835, avg. samples / sec: 55725.70
Iteration:   3720, Loss function: 3.010, Average Loss: 3.853, avg. samples / sec: 55731.03
Iteration:   3720, Loss function: 4.570, Average Loss: 3.856, avg. samples / sec: 55776.73
Iteration:   3720, Loss function: 2.877, Average Loss: 3.830, avg. samples / sec: 55729.29
Iteration:   3720, Loss function: 4.213, Average Loss: 3.845, avg. samples / sec: 55686.35
Iteration:   3720, Loss function: 4.329, Average Loss: 3.827, avg. samples / sec: 55630.47
Iteration:   3720, Loss function: 4.086, Average Loss: 3.833, avg. samples / sec: 55672.88
Iteration:   3720, Loss function: 3.844, Average Loss: 3.835, avg. samples / sec: 55648.09
Iteration:   3740, Loss function: 3.802, Average Loss: 3.830, avg. samples / sec: 59608.85
Iteration:   3740, Loss function: 4.867, Average Loss: 3.853, avg. samples / sec: 59377.07
Iteration:   3740, Loss function: 4.013, Average Loss: 3.854, avg. samples / sec: 59272.63
Iteration:   3740, Loss function: 3.484, Average Loss: 3.847, avg. samples / sec: 59398.81
Iteration:   3740, Loss function: 2.717, Average Loss: 3.823, avg. samples / sec: 59415.34
Iteration:   3740, Loss function: 3.905, Average Loss: 3.851, avg. samples / sec: 59264.50
Iteration:   3740, Loss function: 4.440, Average Loss: 3.817, avg. samples / sec: 59327.60
Iteration:   3740, Loss function: 3.559, Average Loss: 3.832, avg. samples / sec: 59279.53
Iteration:   3740, Loss function: 3.394, Average Loss: 3.846, avg. samples / sec: 59206.44
Iteration:   3740, Loss function: 3.951, Average Loss: 3.844, avg. samples / sec: 59246.36
Iteration:   3740, Loss function: 2.652, Average Loss: 3.840, avg. samples / sec: 59130.72
Iteration:   3740, Loss function: 3.650, Average Loss: 3.823, avg. samples / sec: 59067.55
Iteration:   3740, Loss function: 3.568, Average Loss: 3.855, avg. samples / sec: 58928.91
Iteration:   3740, Loss function: 4.666, Average Loss: 3.836, avg. samples / sec: 58909.63
Iteration:   3740, Loss function: 1.938, Average Loss: 3.825, avg. samples / sec: 58966.12
Iteration:   3760, Loss function: 4.419, Average Loss: 3.846, avg. samples / sec: 57703.80
Iteration:   3760, Loss function: 4.112, Average Loss: 3.809, avg. samples / sec: 57712.95
Iteration:   3760, Loss function: 4.143, Average Loss: 3.833, avg. samples / sec: 57735.72
Iteration:   3760, Loss function: 3.660, Average Loss: 3.833, avg. samples / sec: 57824.17
Iteration:   3760, Loss function: 3.423, Average Loss: 3.820, avg. samples / sec: 57635.58
Iteration:   3760, Loss function: 3.397, Average Loss: 3.839, avg. samples / sec: 57523.90
Iteration:   3760, Loss function: 4.041, Average Loss: 3.821, avg. samples / sec: 57455.35
Iteration:   3760, Loss function: 4.558, Average Loss: 3.809, avg. samples / sec: 57589.30
Iteration:   3760, Loss function: 3.593, Average Loss: 3.815, avg. samples / sec: 57872.59
Iteration:   3760, Loss function: 4.001, Average Loss: 3.846, avg. samples / sec: 57874.02
Iteration:   3760, Loss function: 3.151, Average Loss: 3.833, avg. samples / sec: 57520.50
Iteration:   3760, Loss function: 3.627, Average Loss: 3.815, avg. samples / sec: 57847.51
Iteration:   3760, Loss function: 2.608, Average Loss: 3.836, avg. samples / sec: 57606.27
Iteration:   3760, Loss function: 3.677, Average Loss: 3.847, avg. samples / sec: 57422.91
Iteration:   3760, Loss function: 3.034, Average Loss: 3.827, avg. samples / sec: 57809.33
:::MLL 1558639287.325 epoch_stop: {"value": null, "metadata": {"epoch_num": 54, "file": "train.py", "lineno": 819}}
:::MLL 1558639287.325 epoch_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 673}}
Iteration:   3780, Loss function: 3.835, Average Loss: 3.831, avg. samples / sec: 59701.83
Iteration:   3780, Loss function: 3.427, Average Loss: 3.813, avg. samples / sec: 59694.75
Iteration:   3780, Loss function: 2.022, Average Loss: 3.815, avg. samples / sec: 59740.63
Iteration:   3780, Loss function: 3.656, Average Loss: 3.801, avg. samples / sec: 59481.97
Iteration:   3780, Loss function: 2.894, Average Loss: 3.825, avg. samples / sec: 59590.15
Iteration:   3780, Loss function: 3.804, Average Loss: 3.841, avg. samples / sec: 59438.17
Iteration:   3780, Loss function: 2.985, Average Loss: 3.826, avg. samples / sec: 59623.65
Iteration:   3780, Loss function: 3.812, Average Loss: 3.837, avg. samples / sec: 59542.82
Iteration:   3780, Loss function: 3.195, Average Loss: 3.821, avg. samples / sec: 59485.29
Iteration:   3780, Loss function: 3.193, Average Loss: 3.811, avg. samples / sec: 59457.03
Iteration:   3780, Loss function: 2.842, Average Loss: 3.842, avg. samples / sec: 59548.93
Iteration:   3780, Loss function: 3.564, Average Loss: 3.807, avg. samples / sec: 59530.24
Iteration:   3780, Loss function: 2.551, Average Loss: 3.823, avg. samples / sec: 59406.80
Iteration:   3780, Loss function: 4.535, Average Loss: 3.799, avg. samples / sec: 59428.97
Iteration:   3780, Loss function: 2.293, Average Loss: 3.805, avg. samples / sec: 59196.91
Iteration:   3800, Loss function: 3.506, Average Loss: 3.816, avg. samples / sec: 60099.18
Iteration:   3800, Loss function: 3.871, Average Loss: 3.828, avg. samples / sec: 60034.33
Iteration:   3800, Loss function: 2.926, Average Loss: 3.803, avg. samples / sec: 60004.20
Iteration:   3800, Loss function: 2.826, Average Loss: 3.809, avg. samples / sec: 59815.43
Iteration:   3800, Loss function: 3.775, Average Loss: 3.834, avg. samples / sec: 59888.89
Iteration:   3800, Loss function: 3.200, Average Loss: 3.818, avg. samples / sec: 59848.70
Iteration:   3800, Loss function: 2.647, Average Loss: 3.813, avg. samples / sec: 59964.77
Iteration:   3800, Loss function: 2.270, Average Loss: 3.792, avg. samples / sec: 60204.55
Iteration:   3800, Loss function: 3.390, Average Loss: 3.806, avg. samples / sec: 59682.13
Iteration:   3800, Loss function: 3.007, Average Loss: 3.788, avg. samples / sec: 59882.09
Iteration:   3800, Loss function: 2.529, Average Loss: 3.815, avg. samples / sec: 59725.11
Iteration:   3800, Loss function: 3.893, Average Loss: 3.792, avg. samples / sec: 59670.91
Iteration:   3800, Loss function: 3.165, Average Loss: 3.829, avg. samples / sec: 59792.74
Iteration:   3800, Loss function: 4.031, Average Loss: 3.802, avg. samples / sec: 59719.18
Iteration:   3800, Loss function: 2.648, Average Loss: 3.823, avg. samples / sec: 59388.80
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
:::MLL 1558639288.465 eval_start: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.54s)
DONE (t=0.53s)
DONE (t=0.54s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.57s)
DONE (t=2.83s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.22853
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.39047
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23574
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.06076
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24061
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.37297
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22330
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32365
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.33965
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.10506
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36880
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.52934
Current AP: 0.22853 AP goal: 0.23000
:::MLL 1558639292.522 eval_accuracy: {"value": 0.22853057927762102, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 389}}
:::MLL 1558639292.606 eval_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 392}}
:::MLL 1558639292.616 block_stop: {"value": null, "metadata": {"first_epoch_num": 49, "file": "train.py", "lineno": 804}}
:::MLL 1558639292.616 block_start: {"value": null, "metadata": {"first_epoch_num": 55, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   3820, Loss function: 2.813, Average Loss: 3.810, avg. samples / sec: 7101.38
Iteration:   3820, Loss function: 2.203, Average Loss: 3.821, avg. samples / sec: 7105.72
Iteration:   3820, Loss function: 1.795, Average Loss: 3.795, avg. samples / sec: 7101.96
Iteration:   3820, Loss function: 4.569, Average Loss: 3.793, avg. samples / sec: 7101.38
Iteration:   3820, Loss function: 3.605, Average Loss: 3.823, avg. samples / sec: 7102.16
Iteration:   3820, Loss function: 2.715, Average Loss: 3.816, avg. samples / sec: 7100.44
Iteration:   3820, Loss function: 4.086, Average Loss: 3.806, avg. samples / sec: 7102.33
Iteration:   3820, Loss function: 2.597, Average Loss: 3.800, avg. samples / sec: 7102.71
Iteration:   3820, Loss function: 3.165, Average Loss: 3.798, avg. samples / sec: 7105.20
Iteration:   3820, Loss function: 2.597, Average Loss: 3.783, avg. samples / sec: 7104.28
Iteration:   3820, Loss function: 3.927, Average Loss: 3.806, avg. samples / sec: 7100.59
Iteration:   3820, Loss function: 3.412, Average Loss: 3.784, avg. samples / sec: 7100.80
Iteration:   3820, Loss function: 2.842, Average Loss: 3.814, avg. samples / sec: 7104.61
Iteration:   3820, Loss function: 4.102, Average Loss: 3.777, avg. samples / sec: 7099.58
Iteration:   3820, Loss function: 2.803, Average Loss: 3.808, avg. samples / sec: 7099.79
:::MLL 1558639293.473 epoch_stop: {"value": null, "metadata": {"epoch_num": 55, "file": "train.py", "lineno": 819}}
:::MLL 1558639293.474 epoch_start: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 673}}
Iteration:   3840, Loss function: 2.742, Average Loss: 3.813, avg. samples / sec: 59033.70
Iteration:   3840, Loss function: 3.896, Average Loss: 3.778, avg. samples / sec: 59057.30
Iteration:   3840, Loss function: 4.263, Average Loss: 3.798, avg. samples / sec: 58847.66
Iteration:   3840, Loss function: 3.582, Average Loss: 3.801, avg. samples / sec: 59258.02
Iteration:   3840, Loss function: 3.036, Average Loss: 3.807, avg. samples / sec: 58915.46
Iteration:   3840, Loss function: 3.813, Average Loss: 3.807, avg. samples / sec: 59069.78
Iteration:   3840, Loss function: 3.939, Average Loss: 3.787, avg. samples / sec: 58942.72
Iteration:   3840, Loss function: 2.985, Average Loss: 3.769, avg. samples / sec: 59179.99
Iteration:   3840, Loss function: 2.909, Average Loss: 3.781, avg. samples / sec: 58845.47
Iteration:   3840, Loss function: 3.934, Average Loss: 3.789, avg. samples / sec: 58843.90
Iteration:   3840, Loss function: 3.064, Average Loss: 3.788, avg. samples / sec: 58778.32
Iteration:   3840, Loss function: 3.373, Average Loss: 3.800, avg. samples / sec: 58908.76
Iteration:   3840, Loss function: 3.207, Average Loss: 3.774, avg. samples / sec: 58919.95
Iteration:   3840, Loss function: 3.571, Average Loss: 3.814, avg. samples / sec: 58722.07
Iteration:   3840, Loss function: 3.649, Average Loss: 3.800, avg. samples / sec: 58775.85
Iteration:   3860, Loss function: 3.436, Average Loss: 3.798, avg. samples / sec: 59820.23
Iteration:   3860, Loss function: 3.292, Average Loss: 3.801, avg. samples / sec: 59530.89
Iteration:   3860, Loss function: 3.970, Average Loss: 3.776, avg. samples / sec: 59517.82
Iteration:   3860, Loss function: 2.737, Average Loss: 3.774, avg. samples / sec: 59314.49
Iteration:   3860, Loss function: 3.986, Average Loss: 3.776, avg. samples / sec: 59542.19
Iteration:   3860, Loss function: 3.047, Average Loss: 3.791, avg. samples / sec: 59309.40
Iteration:   3860, Loss function: 2.803, Average Loss: 3.780, avg. samples / sec: 59350.58
Iteration:   3860, Loss function: 2.579, Average Loss: 3.783, avg. samples / sec: 59414.49
Iteration:   3860, Loss function: 3.278, Average Loss: 3.793, avg. samples / sec: 59304.95
Iteration:   3860, Loss function: 4.094, Average Loss: 3.808, avg. samples / sec: 59460.64
Iteration:   3860, Loss function: 3.115, Average Loss: 3.761, avg. samples / sec: 59351.81
Iteration:   3860, Loss function: 3.236, Average Loss: 3.804, avg. samples / sec: 59176.21
Iteration:   3860, Loss function: 3.572, Average Loss: 3.793, avg. samples / sec: 59291.75
Iteration:   3860, Loss function: 3.498, Average Loss: 3.762, avg. samples / sec: 59294.55
Iteration:   3860, Loss function: 4.042, Average Loss: 3.793, avg. samples / sec: 59296.52
Iteration:   3880, Loss function: 3.969, Average Loss: 3.783, avg. samples / sec: 58898.20
Iteration:   3880, Loss function: 3.056, Average Loss: 3.796, avg. samples / sec: 58892.47
Iteration:   3880, Loss function: 3.588, Average Loss: 3.764, avg. samples / sec: 58763.72
Iteration:   3880, Loss function: 3.861, Average Loss: 3.796, avg. samples / sec: 58865.24
Iteration:   3880, Loss function: 2.818, Average Loss: 3.787, avg. samples / sec: 58835.55
Iteration:   3880, Loss function: 3.890, Average Loss: 3.753, avg. samples / sec: 58810.73
Iteration:   3880, Loss function: 2.610, Average Loss: 3.793, avg. samples / sec: 58593.11
Iteration:   3880, Loss function: 3.493, Average Loss: 3.774, avg. samples / sec: 58725.81
Iteration:   3880, Loss function: 2.427, Average Loss: 3.778, avg. samples / sec: 58692.06
Iteration:   3880, Loss function: 3.664, Average Loss: 3.767, avg. samples / sec: 58579.76
Iteration:   3880, Loss function: 3.252, Average Loss: 3.788, avg. samples / sec: 58854.62
Iteration:   3880, Loss function: 3.899, Average Loss: 3.752, avg. samples / sec: 58832.72
Iteration:   3880, Loss function: 2.653, Average Loss: 3.790, avg. samples / sec: 58323.75
Iteration:   3880, Loss function: 2.832, Average Loss: 3.771, avg. samples / sec: 58515.60
Iteration:   3880, Loss function: 3.641, Average Loss: 3.771, avg. samples / sec: 58507.31
Iteration:   3900, Loss function: 2.821, Average Loss: 3.758, avg. samples / sec: 58407.80
Iteration:   3900, Loss function: 3.733, Average Loss: 3.776, avg. samples / sec: 58301.05
Iteration:   3900, Loss function: 4.300, Average Loss: 3.788, avg. samples / sec: 58182.62
Iteration:   3900, Loss function: 1.499, Average Loss: 3.777, avg. samples / sec: 58240.57
Iteration:   3900, Loss function: 2.611, Average Loss: 3.777, avg. samples / sec: 58074.37
Iteration:   3900, Loss function: 2.891, Average Loss: 3.769, avg. samples / sec: 58141.63
Iteration:   3900, Loss function: 2.516, Average Loss: 3.752, avg. samples / sec: 58002.48
Iteration:   3900, Loss function: 2.589, Average Loss: 3.762, avg. samples / sec: 58264.77
Iteration:   3900, Loss function: 4.181, Average Loss: 3.787, avg. samples / sec: 57981.09
Iteration:   3900, Loss function: 2.762, Average Loss: 3.745, avg. samples / sec: 58070.88
Iteration:   3900, Loss function: 2.882, Average Loss: 3.749, avg. samples / sec: 57904.86
Iteration:   3900, Loss function: 3.229, Average Loss: 3.784, avg. samples / sec: 57819.24
Iteration:   3900, Loss function: 2.540, Average Loss: 3.762, avg. samples / sec: 58073.63
Iteration:   3900, Loss function: 4.124, Average Loss: 3.784, avg. samples / sec: 57976.23
Iteration:   3900, Loss function: 4.501, Average Loss: 3.775, avg. samples / sec: 57700.00
:::MLL 1558639295.473 epoch_stop: {"value": null, "metadata": {"epoch_num": 56, "file": "train.py", "lineno": 819}}
:::MLL 1558639295.474 epoch_start: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 673}}
Iteration:   3920, Loss function: 3.079, Average Loss: 3.773, avg. samples / sec: 59181.60
Iteration:   3920, Loss function: 4.150, Average Loss: 3.762, avg. samples / sec: 59252.29
Iteration:   3920, Loss function: 2.370, Average Loss: 3.741, avg. samples / sec: 59403.65
Iteration:   3920, Loss function: 4.157, Average Loss: 3.751, avg. samples / sec: 59010.54
Iteration:   3920, Loss function: 3.147, Average Loss: 3.739, avg. samples / sec: 59372.29
Iteration:   3920, Loss function: 3.901, Average Loss: 3.783, avg. samples / sec: 59304.50
Iteration:   3920, Loss function: 3.645, Average Loss: 3.757, avg. samples / sec: 59381.94
Iteration:   3920, Loss function: 3.440, Average Loss: 3.775, avg. samples / sec: 59034.42
Iteration:   3920, Loss function: 2.850, Average Loss: 3.773, avg. samples / sec: 59399.44
Iteration:   3920, Loss function: 3.514, Average Loss: 3.768, avg. samples / sec: 59411.03
Iteration:   3920, Loss function: 2.400, Average Loss: 3.768, avg. samples / sec: 59042.18
Iteration:   3920, Loss function: 3.596, Average Loss: 3.777, avg. samples / sec: 59207.86
Iteration:   3920, Loss function: 2.830, Average Loss: 3.754, avg. samples / sec: 59083.32
Iteration:   3920, Loss function: 2.102, Average Loss: 3.765, avg. samples / sec: 58884.42
Iteration:   3920, Loss function: 3.337, Average Loss: 3.745, avg. samples / sec: 58944.74
Iteration:   3940, Loss function: 3.185, Average Loss: 3.765, avg. samples / sec: 57642.56
Iteration:   3940, Loss function: 3.048, Average Loss: 3.742, avg. samples / sec: 57785.15
Iteration:   3940, Loss function: 2.887, Average Loss: 3.776, avg. samples / sec: 57539.43
Iteration:   3940, Loss function: 2.902, Average Loss: 3.763, avg. samples / sec: 57567.94
Iteration:   3940, Loss function: 2.179, Average Loss: 3.734, avg. samples / sec: 57490.56
Iteration:   3940, Loss function: 3.699, Average Loss: 3.761, avg. samples / sec: 57610.82
Iteration:   3940, Loss function: 2.681, Average Loss: 3.765, avg. samples / sec: 57366.11
Iteration:   3940, Loss function: 3.889, Average Loss: 3.748, avg. samples / sec: 57485.91
Iteration:   3940, Loss function: 3.259, Average Loss: 3.765, avg. samples / sec: 57605.33
Iteration:   3940, Loss function: 2.641, Average Loss: 3.756, avg. samples / sec: 57417.71
Iteration:   3940, Loss function: 2.976, Average Loss: 3.744, avg. samples / sec: 57408.29
Iteration:   3940, Loss function: 2.692, Average Loss: 3.730, avg. samples / sec: 57392.90
Iteration:   3940, Loss function: 3.963, Average Loss: 3.770, avg. samples / sec: 57545.32
Iteration:   3940, Loss function: 3.315, Average Loss: 3.738, avg. samples / sec: 57572.97
Iteration:   3940, Loss function: 3.175, Average Loss: 3.756, avg. samples / sec: 57559.38
Iteration:   3960, Loss function: 2.722, Average Loss: 3.720, avg. samples / sec: 59063.41
Iteration:   3960, Loss function: 3.209, Average Loss: 3.738, avg. samples / sec: 59008.88
Iteration:   3960, Loss function: 3.874, Average Loss: 3.755, avg. samples / sec: 58874.09
Iteration:   3960, Loss function: 3.558, Average Loss: 3.732, avg. samples / sec: 58797.53
Iteration:   3960, Loss function: 3.900, Average Loss: 3.759, avg. samples / sec: 58898.10
Iteration:   3960, Loss function: 4.473, Average Loss: 3.756, avg. samples / sec: 58717.78
Iteration:   3960, Loss function: 2.910, Average Loss: 3.729, avg. samples / sec: 58746.89
Iteration:   3960, Loss function: 3.533, Average Loss: 3.752, avg. samples / sec: 58803.66
Iteration:   3960, Loss function: 2.384, Average Loss: 3.748, avg. samples / sec: 58970.22
Iteration:   3960, Loss function: 3.370, Average Loss: 3.757, avg. samples / sec: 58675.37
Iteration:   3960, Loss function: 3.297, Average Loss: 3.721, avg. samples / sec: 58913.79
Iteration:   3960, Loss function: 2.551, Average Loss: 3.761, avg. samples / sec: 58652.78
Iteration:   3960, Loss function: 2.720, Average Loss: 3.739, avg. samples / sec: 58734.65
Iteration:   3960, Loss function: 4.071, Average Loss: 3.759, avg. samples / sec: 58791.66
Iteration:   3960, Loss function: 2.946, Average Loss: 3.758, avg. samples / sec: 58674.12
:::MLL 1558639297.481 epoch_stop: {"value": null, "metadata": {"epoch_num": 57, "file": "train.py", "lineno": 819}}
:::MLL 1558639297.482 epoch_start: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 673}}
Iteration:   3980, Loss function: 3.763, Average Loss: 3.755, avg. samples / sec: 59427.84
Iteration:   3980, Loss function: 3.313, Average Loss: 3.730, avg. samples / sec: 59046.71
Iteration:   3980, Loss function: 2.427, Average Loss: 3.746, avg. samples / sec: 59193.83
Iteration:   3980, Loss function: 3.270, Average Loss: 3.714, avg. samples / sec: 59210.12
Iteration:   3980, Loss function: 3.396, Average Loss: 3.752, avg. samples / sec: 59260.34
Iteration:   3980, Loss function: 5.193, Average Loss: 3.741, avg. samples / sec: 59160.61
Iteration:   3980, Loss function: 3.088, Average Loss: 3.749, avg. samples / sec: 59058.39
Iteration:   3980, Loss function: 3.528, Average Loss: 3.717, avg. samples / sec: 59129.11
Iteration:   3980, Loss function: 2.466, Average Loss: 3.745, avg. samples / sec: 59169.82
Iteration:   3980, Loss function: 3.568, Average Loss: 3.714, avg. samples / sec: 58891.06
Iteration:   3980, Loss function: 2.714, Average Loss: 3.727, avg. samples / sec: 59169.72
Iteration:   3980, Loss function: 3.480, Average Loss: 3.751, avg. samples / sec: 59162.35
Iteration:   3980, Loss function: 3.509, Average Loss: 3.728, avg. samples / sec: 58908.03
Iteration:   3980, Loss function: 4.409, Average Loss: 3.749, avg. samples / sec: 58887.30
Iteration:   3980, Loss function: 2.629, Average Loss: 3.748, avg. samples / sec: 58826.44
Iteration:   4000, Loss function: 3.299, Average Loss: 3.741, avg. samples / sec: 59310.09
Iteration:   4000, Loss function: 2.741, Average Loss: 3.710, avg. samples / sec: 59242.80
Iteration:   4000, Loss function: 2.647, Average Loss: 3.733, avg. samples / sec: 59253.86
Iteration:   4000, Loss function: 4.445, Average Loss: 3.724, avg. samples / sec: 59114.18
Iteration:   4000, Loss function: 3.431, Average Loss: 3.718, avg. samples / sec: 59200.52
Iteration:   4000, Loss function: 3.976, Average Loss: 3.711, avg. samples / sec: 59070.02
Iteration:   4000, Loss function: 2.972, Average Loss: 3.736, avg. samples / sec: 59071.46
Iteration:   4000, Loss function: 3.665, Average Loss: 3.718, avg. samples / sec: 59192.07
Iteration:   4000, Loss function: 4.651, Average Loss: 3.750, avg. samples / sec: 58898.33
Iteration:   4000, Loss function: 2.908, Average Loss: 3.741, avg. samples / sec: 59020.10
Iteration:   4000, Loss function: 3.539, Average Loss: 3.740, avg. samples / sec: 59305.78
Iteration:   4000, Loss function: 4.116, Average Loss: 3.744, avg. samples / sec: 58964.74
Iteration:   4000, Loss function: 3.463, Average Loss: 3.740, avg. samples / sec: 59094.99
Iteration:   4000, Loss function: 3.388, Average Loss: 3.703, avg. samples / sec: 58963.90
Iteration:   4000, Loss function: 2.949, Average Loss: 3.741, avg. samples / sec: 58815.51
Iteration:   4020, Loss function: 3.151, Average Loss: 3.731, avg. samples / sec: 57771.98
Iteration:   4020, Loss function: 3.483, Average Loss: 3.728, avg. samples / sec: 58137.09
Iteration:   4020, Loss function: 3.208, Average Loss: 3.742, avg. samples / sec: 57858.26
Iteration:   4020, Loss function: 3.724, Average Loss: 3.721, avg. samples / sec: 57840.17
Iteration:   4020, Loss function: 2.429, Average Loss: 3.696, avg. samples / sec: 57914.81
Iteration:   4020, Loss function: 3.582, Average Loss: 3.699, avg. samples / sec: 57736.07
Iteration:   4020, Loss function: 3.063, Average Loss: 3.730, avg. samples / sec: 57569.68
Iteration:   4020, Loss function: 3.577, Average Loss: 3.704, avg. samples / sec: 57577.42
Iteration:   4020, Loss function: 4.978, Average Loss: 3.735, avg. samples / sec: 57809.14
Iteration:   4020, Loss function: 4.167, Average Loss: 3.718, avg. samples / sec: 57579.37
Iteration:   4020, Loss function: 3.538, Average Loss: 3.715, avg. samples / sec: 57669.00
Iteration:   4020, Loss function: 2.525, Average Loss: 3.734, avg. samples / sec: 57750.62
Iteration:   4020, Loss function: 3.240, Average Loss: 3.736, avg. samples / sec: 57656.14
Iteration:   4020, Loss function: 2.588, Average Loss: 3.712, avg. samples / sec: 57570.81
Iteration:   4020, Loss function: 3.989, Average Loss: 3.733, avg. samples / sec: 57623.58
Iteration:   4040, Loss function: 3.572, Average Loss: 3.699, avg. samples / sec: 60169.21
Iteration:   4040, Loss function: 3.544, Average Loss: 3.698, avg. samples / sec: 60096.29
Iteration:   4040, Loss function: 3.204, Average Loss: 3.703, avg. samples / sec: 60171.16
Iteration:   4040, Loss function: 3.060, Average Loss: 3.727, avg. samples / sec: 60067.70
Iteration:   4040, Loss function: 3.429, Average Loss: 3.721, avg. samples / sec: 59966.97
Iteration:   4040, Loss function: 3.468, Average Loss: 3.723, avg. samples / sec: 60115.33
Iteration:   4040, Loss function: 3.873, Average Loss: 3.726, avg. samples / sec: 60041.65
Iteration:   4040, Loss function: 3.686, Average Loss: 3.710, avg. samples / sec: 59983.07
Iteration:   4040, Loss function: 2.722, Average Loss: 3.691, avg. samples / sec: 59871.00
Iteration:   4040, Loss function: 3.345, Average Loss: 3.718, avg. samples / sec: 59730.93
Iteration:   4040, Loss function: 3.431, Average Loss: 3.704, avg. samples / sec: 59935.96
Iteration:   4040, Loss function: 3.289, Average Loss: 3.733, avg. samples / sec: 59716.83
Iteration:   4040, Loss function: 3.976, Average Loss: 3.729, avg. samples / sec: 59872.55
Iteration:   4040, Loss function: 3.731, Average Loss: 3.726, avg. samples / sec: 59554.14
Iteration:   4040, Loss function: 3.168, Average Loss: 3.715, avg. samples / sec: 59576.95
:::MLL 1558639299.478 epoch_stop: {"value": null, "metadata": {"epoch_num": 58, "file": "train.py", "lineno": 819}}
:::MLL 1558639299.478 epoch_start: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 673}}
Iteration:   4060, Loss function: 2.947, Average Loss: 3.695, avg. samples / sec: 59175.76
Iteration:   4060, Loss function: 3.592, Average Loss: 3.721, avg. samples / sec: 59421.70
Iteration:   4060, Loss function: 2.407, Average Loss: 3.689, avg. samples / sec: 59060.84
Iteration:   4060, Loss function: 3.265, Average Loss: 3.722, avg. samples / sec: 59270.93
Iteration:   4060, Loss function: 3.352, Average Loss: 3.718, avg. samples / sec: 59008.88
Iteration:   4060, Loss function: 2.600, Average Loss: 3.711, avg. samples / sec: 59115.27
Iteration:   4060, Loss function: 2.863, Average Loss: 3.704, avg. samples / sec: 59053.74
Iteration:   4060, Loss function: 3.167, Average Loss: 3.708, avg. samples / sec: 59241.31
Iteration:   4060, Loss function: 3.441, Average Loss: 3.693, avg. samples / sec: 58788.40
Iteration:   4060, Loss function: 2.091, Average Loss: 3.711, avg. samples / sec: 58946.73
Iteration:   4060, Loss function: 3.288, Average Loss: 3.717, avg. samples / sec: 58934.21
Iteration:   4060, Loss function: 3.962, Average Loss: 3.721, avg. samples / sec: 59143.08
Iteration:   4060, Loss function: 3.571, Average Loss: 3.695, avg. samples / sec: 59015.11
Iteration:   4060, Loss function: 3.390, Average Loss: 3.714, avg. samples / sec: 58889.98
Iteration:   4060, Loss function: 3.208, Average Loss: 3.682, avg. samples / sec: 58894.09
Iteration:   4080, Loss function: 4.442, Average Loss: 3.710, avg. samples / sec: 58573.53
Iteration:   4080, Loss function: 3.178, Average Loss: 3.689, avg. samples / sec: 58256.85
Iteration:   4080, Loss function: 3.576, Average Loss: 3.712, avg. samples / sec: 58575.94
Iteration:   4080, Loss function: 2.331, Average Loss: 3.684, avg. samples / sec: 58556.37
Iteration:   4080, Loss function: 2.942, Average Loss: 3.700, avg. samples / sec: 58504.40
Iteration:   4080, Loss function: 4.329, Average Loss: 3.712, avg. samples / sec: 58528.65
Iteration:   4080, Loss function: 4.323, Average Loss: 3.711, avg. samples / sec: 58233.71
Iteration:   4080, Loss function: 2.852, Average Loss: 3.702, avg. samples / sec: 58407.63
Iteration:   4080, Loss function: 4.105, Average Loss: 3.675, avg. samples / sec: 58592.77
Iteration:   4080, Loss function: 2.897, Average Loss: 3.714, avg. samples / sec: 58278.87
Iteration:   4080, Loss function: 3.314, Average Loss: 3.712, avg. samples / sec: 58529.86
Iteration:   4080, Loss function: 2.470, Average Loss: 3.699, avg. samples / sec: 58281.59
Iteration:   4080, Loss function: 3.547, Average Loss: 3.687, avg. samples / sec: 58316.17
Iteration:   4080, Loss function: 3.189, Average Loss: 3.709, avg. samples / sec: 58156.69
Iteration:   4080, Loss function: 5.274, Average Loss: 3.687, avg. samples / sec: 57955.39
Iteration:   4100, Loss function: 3.008, Average Loss: 3.698, avg. samples / sec: 59927.60
Iteration:   4100, Loss function: 3.482, Average Loss: 3.706, avg. samples / sec: 59758.99
Iteration:   4100, Loss function: 4.473, Average Loss: 3.708, avg. samples / sec: 59834.30
Iteration:   4100, Loss function: 3.818, Average Loss: 3.672, avg. samples / sec: 59808.47
Iteration:   4100, Loss function: 2.847, Average Loss: 3.680, avg. samples / sec: 59717.08
Iteration:   4100, Loss function: 3.789, Average Loss: 3.703, avg. samples / sec: 59743.44
Iteration:   4100, Loss function: 3.164, Average Loss: 3.685, avg. samples / sec: 60062.35
Iteration:   4100, Loss function: 3.327, Average Loss: 3.696, avg. samples / sec: 59860.07
Iteration:   4100, Loss function: 3.760, Average Loss: 3.708, avg. samples / sec: 59935.40
Iteration:   4100, Loss function: 3.164, Average Loss: 3.703, avg. samples / sec: 59661.29
Iteration:   4100, Loss function: 2.866, Average Loss: 3.690, avg. samples / sec: 59622.09
Iteration:   4100, Loss function: 2.777, Average Loss: 3.672, avg. samples / sec: 59624.71
Iteration:   4100, Loss function: 4.068, Average Loss: 3.712, avg. samples / sec: 59635.26
Iteration:   4100, Loss function: 2.532, Average Loss: 3.705, avg. samples / sec: 59535.24
Iteration:   4100, Loss function: 3.831, Average Loss: 3.681, avg. samples / sec: 59753.54
:::MLL 1558639301.468 epoch_stop: {"value": null, "metadata": {"epoch_num": 59, "file": "train.py", "lineno": 819}}
:::MLL 1558639301.468 epoch_start: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 673}}
Iteration:   4120, Loss function: 2.684, Average Loss: 3.691, avg. samples / sec: 59167.44
Iteration:   4120, Loss function: 3.401, Average Loss: 3.701, avg. samples / sec: 59406.37
Iteration:   4120, Loss function: 3.107, Average Loss: 3.674, avg. samples / sec: 59395.08
Iteration:   4120, Loss function: 3.290, Average Loss: 3.700, avg. samples / sec: 59291.31
Iteration:   4120, Loss function: 4.391, Average Loss: 3.673, avg. samples / sec: 59286.77
Iteration:   4120, Loss function: 2.444, Average Loss: 3.672, avg. samples / sec: 59107.36
Iteration:   4120, Loss function: 3.559, Average Loss: 3.705, avg. samples / sec: 59260.81
Iteration:   4120, Loss function: 3.138, Average Loss: 3.695, avg. samples / sec: 59052.13
Iteration:   4120, Loss function: 3.627, Average Loss: 3.668, avg. samples / sec: 59061.41
Iteration:   4120, Loss function: 3.297, Average Loss: 3.702, avg. samples / sec: 59136.63
Iteration:   4120, Loss function: 3.206, Average Loss: 3.688, avg. samples / sec: 59083.52
Iteration:   4120, Loss function: 4.099, Average Loss: 3.700, avg. samples / sec: 59031.25
Iteration:   4120, Loss function: 3.425, Average Loss: 3.679, avg. samples / sec: 59035.56
Iteration:   4120, Loss function: 2.831, Average Loss: 3.703, avg. samples / sec: 58911.47
Iteration:   4120, Loss function: 3.038, Average Loss: 3.682, avg. samples / sec: 59017.75
Iteration:   4140, Loss function: 4.180, Average Loss: 3.669, avg. samples / sec: 59704.26
Iteration:   4140, Loss function: 2.814, Average Loss: 3.696, avg. samples / sec: 59463.90
Iteration:   4140, Loss function: 2.830, Average Loss: 3.689, avg. samples / sec: 59301.83
Iteration:   4140, Loss function: 2.529, Average Loss: 3.694, avg. samples / sec: 59510.20
Iteration:   4140, Loss function: 3.564, Average Loss: 3.683, avg. samples / sec: 59481.85
Iteration:   4140, Loss function: 2.460, Average Loss: 3.663, avg. samples / sec: 59361.96
Iteration:   4140, Loss function: 4.109, Average Loss: 3.691, avg. samples / sec: 59082.83
Iteration:   4140, Loss function: 2.905, Average Loss: 3.695, avg. samples / sec: 59423.36
Iteration:   4140, Loss function: 4.930, Average Loss: 3.695, avg. samples / sec: 59159.54
Iteration:   4140, Loss function: 3.377, Average Loss: 3.661, avg. samples / sec: 59309.00
Iteration:   4140, Loss function: 5.340, Average Loss: 3.664, avg. samples / sec: 59192.22
Iteration:   4140, Loss function: 3.270, Average Loss: 3.691, avg. samples / sec: 59269.34
Iteration:   4140, Loss function: 3.103, Average Loss: 3.695, avg. samples / sec: 59276.94
Iteration:   4140, Loss function: 2.603, Average Loss: 3.676, avg. samples / sec: 59330.30
Iteration:   4140, Loss function: 3.735, Average Loss: 3.667, avg. samples / sec: 59000.09
Iteration:   4160, Loss function: 3.431, Average Loss: 3.687, avg. samples / sec: 59129.28
Iteration:   4160, Loss function: 3.322, Average Loss: 3.663, avg. samples / sec: 59132.26
Iteration:   4160, Loss function: 3.108, Average Loss: 3.657, avg. samples / sec: 59289.26
Iteration:   4160, Loss function: 3.518, Average Loss: 3.670, avg. samples / sec: 59250.85
Iteration:   4160, Loss function: 3.275, Average Loss: 3.665, avg. samples / sec: 58767.22
Iteration:   4160, Loss function: 4.283, Average Loss: 3.673, avg. samples / sec: 58921.87
Iteration:   4160, Loss function: 2.303, Average Loss: 3.691, avg. samples / sec: 58789.95
Iteration:   4160, Loss function: 3.555, Average Loss: 3.682, avg. samples / sec: 58843.88
Iteration:   4160, Loss function: 3.386, Average Loss: 3.660, avg. samples / sec: 58831.69
Iteration:   4160, Loss function: 2.893, Average Loss: 3.682, avg. samples / sec: 58916.00
Iteration:   4160, Loss function: 4.539, Average Loss: 3.689, avg. samples / sec: 58951.96
Iteration:   4160, Loss function: 3.941, Average Loss: 3.664, avg. samples / sec: 58910.12
Iteration:   4160, Loss function: 3.710, Average Loss: 3.683, avg. samples / sec: 58923.59
Iteration:   4160, Loss function: 2.643, Average Loss: 3.685, avg. samples / sec: 58655.66
Iteration:   4160, Loss function: 4.114, Average Loss: 3.691, avg. samples / sec: 58722.48
Iteration:   4180, Loss function: 3.399, Average Loss: 3.656, avg. samples / sec: 57120.39
Iteration:   4180, Loss function: 3.746, Average Loss: 3.688, avg. samples / sec: 57286.03
Iteration:   4180, Loss function: 2.981, Average Loss: 3.684, avg. samples / sec: 57037.48
Iteration:   4180, Loss function: 3.679, Average Loss: 3.662, avg. samples / sec: 56892.65
Iteration:   4180, Loss function: 3.741, Average Loss: 3.679, avg. samples / sec: 57181.83
Iteration:   4180, Loss function: 3.323, Average Loss: 3.677, avg. samples / sec: 57077.66
Iteration:   4180, Loss function: 5.141, Average Loss: 3.676, avg. samples / sec: 56815.40
Iteration:   4180, Loss function: 2.904, Average Loss: 3.677, avg. samples / sec: 56980.31
Iteration:   4180, Loss function: 3.922, Average Loss: 3.654, avg. samples / sec: 56827.06
Iteration:   4180, Loss function: 3.345, Average Loss: 3.657, avg. samples / sec: 56781.75
Iteration:   4180, Loss function: 3.583, Average Loss: 3.678, avg. samples / sec: 56967.23
Iteration:   4180, Loss function: 3.963, Average Loss: 3.660, avg. samples / sec: 56801.87
Iteration:   4180, Loss function: 3.286, Average Loss: 3.672, avg. samples / sec: 56917.12
Iteration:   4180, Loss function: 4.275, Average Loss: 3.665, avg. samples / sec: 56803.95
Iteration:   4180, Loss function: 4.023, Average Loss: 3.662, avg. samples / sec: 56935.38
:::MLL 1558639303.479 epoch_stop: {"value": null, "metadata": {"epoch_num": 60, "file": "train.py", "lineno": 819}}
:::MLL 1558639303.479 epoch_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 673}}
:::MLL 1558639303.540 eval_start: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 276}}
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 0.64 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.52s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.53s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.54s)
DONE (t=0.57s)
DONE (t=2.83s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.23069
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.39269
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23810
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.06180
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24449
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.37605
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22549
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32541
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.34150
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.10396
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.37329
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.53203
Current AP: 0.23069 AP goal: 0.23000
:::MLL 1558639307.608 eval_accuracy: {"value": 0.23068888550902758, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 389}}
:::MLL 1558639307.707 eval_stop: {"value": null, "metadata": {"epoch_num": 61, "file": "train.py", "lineno": 392}}
:::MLL 1558639307.719 block_stop: {"value": null, "metadata": {"first_epoch_num": 55, "file": "train.py", "lineno": 804}}
:::MLL 1558639308.788 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 849}}
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
 ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '7', '--eval-batch-size', '40', '--warmup', '1250', '--num-workers', '3', '--bn-group', '4', '--lr', '3.1e-3', '--wd', '2e-4', '--input-batch-multiplier', '10', '--use-nvjpeg', '--use-roi-decode']
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-05-23 07:21:58 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:18:18 PM
ENDING TIMING RUN AT 2019-05-23 07:21:58 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:18:18 PM
ENDING TIMING RUN AT 2019-05-23 07:21:58 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:18:18 PM
ENDING TIMING RUN AT 2019-05-23 07:21:58 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:18:18 PM
ENDING TIMING RUN AT 2019-05-23 07:21:58 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:18:18 PM
ENDING TIMING RUN AT 2019-05-23 07:21:58 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:18:18 PM
ENDING TIMING RUN AT 2019-05-23 07:21:58 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:18:18 PM
ENDING TIMING RUN AT 2019-05-23 07:21:58 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:18:18 PM
ENDING TIMING RUN AT 2019-05-23 07:21:58 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:18:18 PM
ENDING TIMING RUN AT 2019-05-23 07:21:58 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:18:18 PM
ENDING TIMING RUN AT 2019-05-23 07:21:58 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:18:18 PM
ENDING TIMING RUN AT 2019-05-23 07:21:58 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:18:18 PM
ENDING TIMING RUN AT 2019-05-23 07:21:58 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:18:18 PM
ENDING TIMING RUN AT 2019-05-23 07:21:58 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:18:18 PM
ENDING TIMING RUN AT 2019-05-23 07:21:58 PM
RESULT,SINGLE_STAGE_DETECTOR,,220,nvidia,2019-05-23 07:18:18 PM
