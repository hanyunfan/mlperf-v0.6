Beginning trial 1 of 1
Gathering sys log on XPL-CR-87
:::MLL 1560904704.965 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1560904704.966 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1560904704.966 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1560904704.966 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1560904704.967 submission_platform: {"value": "1xNVIDIA DGX-2", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1560904704.967 submission_entry: {"value": "{'hardware': 'NVIDIA DGX-2', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 10 Gb/sec (4X)', 'os': 'Ubuntu 18.04.2 LTS / NVIDIA DGX Server', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.4-2.0.7', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Platinum 8168 CPU @ 2.70GHz', 'num_cores': '48', 'num_vcpus': '96', 'accelerator': 'Tesla V100-SXM3-32GB', 'num_accelerators': '16', 'sys_mem_size': '1510 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '2x 894.3G + 8x 3.5T', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27800 Family [ConnectX-5]', 'num_network_cards': '8', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1560904704.968 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1560904704.968 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1560904709.014 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node XPL-CR-87
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX2 -e 'MULTI_NODE= --master_port=4989' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=128 -e TEST_BATCH_SIZE=64 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=1560904617 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_1560904617 ./run_and_time.sh
Run vars: id 1560904617 gpus 16 mparams  --master_port=4989
STARTING TIMING RUN AT 2019-06-19 12:38:29 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=128
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 24 --nproc_per_node 16  --master_port=4989'
running benchmark
+ echo 'running benchmark'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --master_port=4989 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 128 --test-batch-size 64 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1560904711.273 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560904711.273 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560904711.273 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560904711.273 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560904711.274 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560904711.275 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560904711.275 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560904711.277 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560904711.277 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560904711.278 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560904711.278 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560904711.279 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560904711.279 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560904711.280 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560904711.281 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1560904711.291 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=64, test_loader_workers=0, train_batch_size=128, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 3729880992
0: Worker 0 is using worker seed: 918934291
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1560904755.344 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1560904759.923 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1560904759.923 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1560904759.924 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1560904760.237 global_batch_size: {"value": 2048, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1560904760.238 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1560904760.238 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1560904760.239 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1560904760.239 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1560904760.239 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1560904760.239 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1560904760.240 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1560904760.241 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1560904760.241 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 675681949
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/1938]	Time 0.440 (0.440)	Data 3.44e-01 (3.44e-01)	Tok/s 19127 (19127)	Loss/tok 10.6930 (10.6930)	LR 2.000e-05
0: TRAIN [0][10/1938]	Time 0.092 (0.129)	Data 9.16e-05 (3.13e-02)	Tok/s 90261 (68531)	Loss/tok 9.6858 (10.1683)	LR 2.518e-05
0: TRAIN [0][20/1938]	Time 0.042 (0.107)	Data 8.68e-05 (1.65e-02)	Tok/s 62157 (72165)	Loss/tok 8.9434 (9.8664)	LR 3.170e-05
0: TRAIN [0][30/1938]	Time 0.068 (0.098)	Data 8.96e-05 (1.12e-02)	Tok/s 77904 (77115)	Loss/tok 9.0412 (9.6184)	LR 3.991e-05
0: TRAIN [0][40/1938]	Time 0.089 (0.093)	Data 8.75e-05 (8.48e-03)	Tok/s 92139 (78660)	Loss/tok 8.7830 (9.4543)	LR 5.024e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][50/1938]	Time 0.065 (0.090)	Data 9.27e-05 (6.83e-03)	Tok/s 78463 (80089)	Loss/tok 8.4313 (9.3042)	LR 6.181e-05
0: TRAIN [0][60/1938]	Time 0.092 (0.090)	Data 9.27e-05 (5.73e-03)	Tok/s 89802 (80937)	Loss/tok 8.5172 (9.1674)	LR 7.781e-05
0: TRAIN [0][70/1938]	Time 0.064 (0.089)	Data 9.66e-05 (4.93e-03)	Tok/s 79623 (81883)	Loss/tok 8.1929 (9.0492)	LR 9.796e-05
0: TRAIN [0][80/1938]	Time 0.066 (0.089)	Data 9.66e-05 (4.34e-03)	Tok/s 77821 (82071)	Loss/tok 7.9399 (8.9410)	LR 1.233e-04
0: TRAIN [0][90/1938]	Time 0.089 (0.087)	Data 8.89e-05 (3.87e-03)	Tok/s 95101 (82240)	Loss/tok 8.0425 (8.8491)	LR 1.552e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
0: TRAIN [0][100/1938]	Time 0.065 (0.087)	Data 9.42e-05 (3.50e-03)	Tok/s 79682 (82652)	Loss/tok 7.7891 (8.7663)	LR 1.910e-04
0: TRAIN [0][110/1938]	Time 0.090 (0.087)	Data 1.10e-04 (3.19e-03)	Tok/s 92540 (83303)	Loss/tok 8.0047 (8.6879)	LR 2.405e-04
0: TRAIN [0][120/1938]	Time 0.064 (0.085)	Data 8.49e-05 (2.93e-03)	Tok/s 79668 (83023)	Loss/tok 7.7836 (8.6332)	LR 3.027e-04
0: TRAIN [0][130/1938]	Time 0.090 (0.087)	Data 8.70e-05 (2.72e-03)	Tok/s 93256 (83828)	Loss/tok 7.8419 (8.5680)	LR 3.811e-04
0: TRAIN [0][140/1938]	Time 0.065 (0.087)	Data 9.97e-05 (2.53e-03)	Tok/s 79788 (84461)	Loss/tok 7.7198 (8.5123)	LR 4.798e-04
0: TRAIN [0][150/1938]	Time 0.116 (0.087)	Data 8.63e-05 (2.37e-03)	Tok/s 101492 (84748)	Loss/tok 7.8523 (8.4610)	LR 6.040e-04
0: TRAIN [0][160/1938]	Time 0.042 (0.086)	Data 8.65e-05 (2.23e-03)	Tok/s 63376 (84458)	Loss/tok 7.0148 (8.4196)	LR 7.604e-04
0: TRAIN [0][170/1938]	Time 0.089 (0.086)	Data 8.75e-05 (2.10e-03)	Tok/s 94896 (84586)	Loss/tok 7.6854 (8.3729)	LR 9.573e-04
0: TRAIN [0][180/1938]	Time 0.089 (0.086)	Data 9.04e-05 (1.99e-03)	Tok/s 94865 (84598)	Loss/tok 7.4652 (8.3248)	LR 1.205e-03
0: TRAIN [0][190/1938]	Time 0.065 (0.085)	Data 8.56e-05 (1.89e-03)	Tok/s 79894 (84552)	Loss/tok 6.9860 (8.2768)	LR 1.517e-03
0: TRAIN [0][200/1938]	Time 0.042 (0.084)	Data 9.80e-05 (1.80e-03)	Tok/s 65445 (84531)	Loss/tok 6.4232 (8.2251)	LR 1.910e-03
0: TRAIN [0][210/1938]	Time 0.043 (0.084)	Data 1.31e-04 (1.72e-03)	Tok/s 58596 (84448)	Loss/tok 6.1287 (8.1695)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.093 (0.083)	Data 9.58e-05 (1.65e-03)	Tok/s 91650 (84379)	Loss/tok 6.8913 (8.1150)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.090 (0.083)	Data 8.96e-05 (1.58e-03)	Tok/s 93401 (84539)	Loss/tok 6.6683 (8.0509)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.147 (0.084)	Data 8.99e-05 (1.52e-03)	Tok/s 101757 (84637)	Loss/tok 6.8725 (7.9876)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.091 (0.084)	Data 9.30e-05 (1.46e-03)	Tok/s 94226 (84777)	Loss/tok 6.4536 (7.9217)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.091 (0.084)	Data 8.63e-05 (1.41e-03)	Tok/s 93577 (84992)	Loss/tok 6.1089 (7.8536)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.065 (0.084)	Data 9.08e-05 (1.36e-03)	Tok/s 80292 (84990)	Loss/tok 5.9390 (7.7977)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.065 (0.084)	Data 9.18e-05 (1.32e-03)	Tok/s 82275 (85108)	Loss/tok 5.6677 (7.7333)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.089 (0.084)	Data 1.02e-04 (1.27e-03)	Tok/s 96073 (85301)	Loss/tok 5.8143 (7.6668)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.116 (0.084)	Data 9.04e-05 (1.23e-03)	Tok/s 99839 (85427)	Loss/tok 5.9362 (7.5994)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.065 (0.084)	Data 8.73e-05 (1.20e-03)	Tok/s 80663 (85480)	Loss/tok 5.5117 (7.5417)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.091 (0.084)	Data 9.35e-05 (1.16e-03)	Tok/s 91206 (85611)	Loss/tok 5.7759 (7.4836)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.092 (0.084)	Data 9.94e-05 (1.13e-03)	Tok/s 90794 (85669)	Loss/tok 5.6162 (7.4266)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.090 (0.084)	Data 8.73e-05 (1.10e-03)	Tok/s 95522 (85603)	Loss/tok 5.6417 (7.3785)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.090 (0.084)	Data 8.56e-05 (1.07e-03)	Tok/s 93073 (85507)	Loss/tok 5.8184 (7.3311)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.150 (0.084)	Data 8.92e-05 (1.04e-03)	Tok/s 99037 (85427)	Loss/tok 5.8349 (7.2839)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.151 (0.084)	Data 8.73e-05 (1.02e-03)	Tok/s 98127 (85483)	Loss/tok 5.8597 (7.2277)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.118 (0.084)	Data 8.94e-05 (9.95e-04)	Tok/s 97875 (85660)	Loss/tok 5.4355 (7.1661)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.117 (0.085)	Data 8.65e-05 (9.72e-04)	Tok/s 100912 (85743)	Loss/tok 5.3398 (7.1106)	LR 2.000e-03
0: TRAIN [0][400/1938]	Time 0.066 (0.084)	Data 8.61e-05 (9.50e-04)	Tok/s 79439 (85746)	Loss/tok 4.9606 (7.0629)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.065 (0.084)	Data 9.08e-05 (9.29e-04)	Tok/s 80428 (85716)	Loss/tok 4.5677 (7.0147)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.065 (0.084)	Data 9.04e-05 (9.09e-04)	Tok/s 79139 (85811)	Loss/tok 4.5838 (6.9613)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.065 (0.084)	Data 8.75e-05 (8.90e-04)	Tok/s 79367 (85769)	Loss/tok 4.4631 (6.9142)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.065 (0.084)	Data 9.11e-05 (8.72e-04)	Tok/s 79083 (85806)	Loss/tok 4.4118 (6.8637)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.090 (0.084)	Data 8.82e-05 (8.55e-04)	Tok/s 94214 (85761)	Loss/tok 4.6302 (6.8209)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.092 (0.084)	Data 8.51e-05 (8.38e-04)	Tok/s 93056 (85756)	Loss/tok 4.7448 (6.7756)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.065 (0.084)	Data 9.11e-05 (8.22e-04)	Tok/s 77327 (85763)	Loss/tok 4.2137 (6.7296)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.065 (0.084)	Data 8.68e-05 (8.07e-04)	Tok/s 78488 (85667)	Loss/tok 4.3393 (6.6920)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.090 (0.084)	Data 8.89e-05 (7.93e-04)	Tok/s 93214 (85548)	Loss/tok 4.4983 (6.6563)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][500/1938]	Time 0.065 (0.084)	Data 8.63e-05 (7.79e-04)	Tok/s 79887 (85523)	Loss/tok 4.2338 (6.6145)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][510/1938]	Time 0.150 (0.083)	Data 9.01e-05 (7.65e-04)	Tok/s 98268 (85417)	Loss/tok 5.1530 (6.5788)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.091 (0.083)	Data 1.00e-04 (7.52e-04)	Tok/s 92994 (85345)	Loss/tok 4.3298 (6.5425)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.067 (0.083)	Data 1.29e-04 (7.40e-04)	Tok/s 76769 (85383)	Loss/tok 3.9945 (6.5014)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.065 (0.083)	Data 8.46e-05 (7.28e-04)	Tok/s 79505 (85461)	Loss/tok 4.0549 (6.4592)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.091 (0.083)	Data 8.99e-05 (7.16e-04)	Tok/s 89947 (85507)	Loss/tok 4.5366 (6.4202)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.091 (0.083)	Data 9.78e-05 (7.05e-04)	Tok/s 92093 (85500)	Loss/tok 4.2466 (6.3831)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.066 (0.083)	Data 9.16e-05 (6.94e-04)	Tok/s 78047 (85440)	Loss/tok 3.9650 (6.3505)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.067 (0.083)	Data 8.87e-05 (6.84e-04)	Tok/s 77939 (85408)	Loss/tok 3.9739 (6.3155)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.091 (0.083)	Data 8.44e-05 (6.74e-04)	Tok/s 93707 (85399)	Loss/tok 4.3557 (6.2810)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.066 (0.083)	Data 8.80e-05 (6.64e-04)	Tok/s 77501 (85224)	Loss/tok 3.7543 (6.2555)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.116 (0.083)	Data 8.61e-05 (6.55e-04)	Tok/s 100391 (85252)	Loss/tok 4.3563 (6.2211)	LR 2.000e-03
0: TRAIN [0][620/1938]	Time 0.065 (0.083)	Data 9.49e-05 (6.46e-04)	Tok/s 79957 (85250)	Loss/tok 3.8955 (6.1891)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.069 (0.083)	Data 8.58e-05 (6.37e-04)	Tok/s 74278 (85248)	Loss/tok 3.7089 (6.1573)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.066 (0.083)	Data 8.70e-05 (6.28e-04)	Tok/s 78943 (85247)	Loss/tok 3.9476 (6.1244)	LR 2.000e-03
0: TRAIN [0][650/1938]	Time 0.092 (0.083)	Data 9.06e-05 (6.20e-04)	Tok/s 93062 (85277)	Loss/tok 4.4574 (6.0913)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.067 (0.083)	Data 9.20e-05 (6.12e-04)	Tok/s 77108 (85195)	Loss/tok 3.8240 (6.0651)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.065 (0.083)	Data 8.80e-05 (6.04e-04)	Tok/s 80536 (85229)	Loss/tok 3.7546 (6.0321)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.066 (0.083)	Data 8.94e-05 (5.97e-04)	Tok/s 78975 (85252)	Loss/tok 3.9119 (6.0026)	LR 2.000e-03
0: TRAIN [0][690/1938]	Time 0.091 (0.083)	Data 9.04e-05 (5.89e-04)	Tok/s 93857 (85187)	Loss/tok 4.1631 (5.9768)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.091 (0.083)	Data 8.94e-05 (5.82e-04)	Tok/s 92133 (85267)	Loss/tok 4.0796 (5.9452)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.065 (0.083)	Data 8.75e-05 (5.75e-04)	Tok/s 77692 (85188)	Loss/tok 3.6534 (5.9225)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.117 (0.083)	Data 8.73e-05 (5.69e-04)	Tok/s 98708 (85208)	Loss/tok 4.3512 (5.8954)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.092 (0.083)	Data 8.42e-05 (5.62e-04)	Tok/s 91546 (85182)	Loss/tok 3.9036 (5.8702)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.117 (0.083)	Data 9.44e-05 (5.56e-04)	Tok/s 101515 (85138)	Loss/tok 4.2691 (5.8468)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.044 (0.083)	Data 8.92e-05 (5.50e-04)	Tok/s 59902 (85097)	Loss/tok 3.0013 (5.8240)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.092 (0.083)	Data 9.01e-05 (5.44e-04)	Tok/s 90549 (85166)	Loss/tok 3.9562 (5.7967)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.117 (0.083)	Data 1.21e-04 (5.38e-04)	Tok/s 100346 (85150)	Loss/tok 4.1598 (5.7743)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.044 (0.083)	Data 8.77e-05 (5.32e-04)	Tok/s 58212 (85054)	Loss/tok 2.9670 (5.7546)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.090 (0.083)	Data 1.00e-04 (5.27e-04)	Tok/s 92184 (85018)	Loss/tok 4.0230 (5.7342)	LR 2.000e-03
0: TRAIN [0][800/1938]	Time 0.066 (0.083)	Data 8.85e-05 (5.21e-04)	Tok/s 75560 (85016)	Loss/tok 3.7406 (5.7129)	LR 2.000e-03
0: TRAIN [0][810/1938]	Time 0.067 (0.083)	Data 9.23e-05 (5.16e-04)	Tok/s 75899 (84976)	Loss/tok 3.7517 (5.6933)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.093 (0.083)	Data 8.89e-05 (5.11e-04)	Tok/s 90242 (85080)	Loss/tok 4.0235 (5.6676)	LR 2.000e-03
0: TRAIN [0][830/1938]	Time 0.067 (0.083)	Data 9.06e-05 (5.06e-04)	Tok/s 79204 (85058)	Loss/tok 3.7886 (5.6477)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.065 (0.083)	Data 9.63e-05 (5.01e-04)	Tok/s 79977 (85052)	Loss/tok 3.6306 (5.6274)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.117 (0.083)	Data 8.63e-05 (4.96e-04)	Tok/s 98779 (85060)	Loss/tok 4.1481 (5.6072)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.091 (0.083)	Data 8.63e-05 (4.91e-04)	Tok/s 90856 (85066)	Loss/tok 4.0635 (5.5872)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.092 (0.083)	Data 9.66e-05 (4.87e-04)	Tok/s 92028 (85073)	Loss/tok 3.9222 (5.5674)	LR 2.000e-03
0: TRAIN [0][880/1938]	Time 0.066 (0.083)	Data 9.82e-05 (4.82e-04)	Tok/s 77522 (85058)	Loss/tok 3.6819 (5.5494)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.149 (0.083)	Data 9.27e-05 (4.78e-04)	Tok/s 98034 (85030)	Loss/tok 4.3617 (5.5316)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.118 (0.083)	Data 8.82e-05 (4.73e-04)	Tok/s 99727 (85077)	Loss/tok 4.0796 (5.5109)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.120 (0.083)	Data 8.82e-05 (4.69e-04)	Tok/s 97477 (85089)	Loss/tok 4.1771 (5.4926)	LR 2.000e-03
0: TRAIN [0][920/1938]	Time 0.152 (0.083)	Data 8.75e-05 (4.65e-04)	Tok/s 97011 (85050)	Loss/tok 4.3390 (5.4764)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.117 (0.083)	Data 8.75e-05 (4.61e-04)	Tok/s 100607 (85094)	Loss/tok 3.7767 (5.4569)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.118 (0.083)	Data 8.92e-05 (4.57e-04)	Tok/s 96739 (85139)	Loss/tok 4.0872 (5.4374)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.092 (0.083)	Data 1.25e-04 (4.53e-04)	Tok/s 91791 (85089)	Loss/tok 3.9447 (5.4226)	LR 2.000e-03
0: TRAIN [0][960/1938]	Time 0.118 (0.083)	Data 8.85e-05 (4.50e-04)	Tok/s 98884 (85107)	Loss/tok 3.9078 (5.4043)	LR 2.000e-03
0: TRAIN [0][970/1938]	Time 0.091 (0.083)	Data 8.37e-05 (4.46e-04)	Tok/s 91312 (85081)	Loss/tok 3.9319 (5.3895)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.044 (0.083)	Data 9.16e-05 (4.42e-04)	Tok/s 60989 (85027)	Loss/tok 2.8254 (5.3760)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][990/1938]	Time 0.066 (0.083)	Data 8.89e-05 (4.39e-04)	Tok/s 77653 (85056)	Loss/tok 3.4534 (5.3594)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.066 (0.083)	Data 1.17e-04 (4.35e-04)	Tok/s 79158 (85061)	Loss/tok 3.3451 (5.3438)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.066 (0.083)	Data 9.06e-05 (4.32e-04)	Tok/s 79265 (85037)	Loss/tok 3.4797 (5.3290)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.065 (0.083)	Data 8.70e-05 (4.29e-04)	Tok/s 79541 (85002)	Loss/tok 3.5123 (5.3154)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.090 (0.083)	Data 8.54e-05 (4.25e-04)	Tok/s 92920 (84962)	Loss/tok 3.6810 (5.3020)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.065 (0.083)	Data 8.85e-05 (4.22e-04)	Tok/s 79977 (84880)	Loss/tok 3.5001 (5.2899)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.065 (0.083)	Data 8.70e-05 (4.19e-04)	Tok/s 79048 (84881)	Loss/tok 3.4378 (5.2758)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.092 (0.083)	Data 8.73e-05 (4.16e-04)	Tok/s 91981 (84858)	Loss/tok 3.8355 (5.2625)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.065 (0.083)	Data 9.01e-05 (4.13e-04)	Tok/s 75832 (84837)	Loss/tok 3.4922 (5.2489)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.065 (0.083)	Data 8.58e-05 (4.10e-04)	Tok/s 79023 (84823)	Loss/tok 3.7403 (5.2359)	LR 2.000e-03
0: TRAIN [0][1090/1938]	Time 0.065 (0.083)	Data 9.39e-05 (4.07e-04)	Tok/s 77574 (84794)	Loss/tok 3.6169 (5.2238)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.068 (0.083)	Data 9.25e-05 (4.04e-04)	Tok/s 74715 (84820)	Loss/tok 3.4294 (5.2092)	LR 2.000e-03
0: TRAIN [0][1110/1938]	Time 0.043 (0.083)	Data 8.87e-05 (4.01e-04)	Tok/s 60148 (84779)	Loss/tok 2.9292 (5.1973)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.118 (0.083)	Data 1.06e-04 (3.98e-04)	Tok/s 99134 (84810)	Loss/tok 3.9486 (5.1830)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.118 (0.083)	Data 9.04e-05 (3.96e-04)	Tok/s 96101 (84839)	Loss/tok 4.0694 (5.1689)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.044 (0.083)	Data 8.96e-05 (3.93e-04)	Tok/s 61516 (84824)	Loss/tok 3.1015 (5.1567)	LR 2.000e-03
0: TRAIN [0][1150/1938]	Time 0.091 (0.083)	Data 9.32e-05 (3.90e-04)	Tok/s 89595 (84844)	Loss/tok 3.6800 (5.1439)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.066 (0.083)	Data 8.82e-05 (3.88e-04)	Tok/s 79348 (84835)	Loss/tok 3.5478 (5.1320)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.044 (0.083)	Data 8.92e-05 (3.85e-04)	Tok/s 61977 (84808)	Loss/tok 3.0773 (5.1210)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.117 (0.083)	Data 8.58e-05 (3.83e-04)	Tok/s 99305 (84784)	Loss/tok 3.8233 (5.1095)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.092 (0.083)	Data 8.85e-05 (3.80e-04)	Tok/s 91682 (84785)	Loss/tok 3.7003 (5.0984)	LR 2.000e-03
0: TRAIN [0][1200/1938]	Time 0.067 (0.083)	Data 9.13e-05 (3.78e-04)	Tok/s 76329 (84811)	Loss/tok 3.3374 (5.0858)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.094 (0.083)	Data 8.73e-05 (3.76e-04)	Tok/s 89945 (84816)	Loss/tok 3.5129 (5.0744)	LR 2.000e-03
0: TRAIN [0][1220/1938]	Time 0.044 (0.083)	Data 8.77e-05 (3.73e-04)	Tok/s 60881 (84841)	Loss/tok 2.8526 (5.0618)	LR 2.000e-03
0: TRAIN [0][1230/1938]	Time 0.118 (0.083)	Data 9.01e-05 (3.71e-04)	Tok/s 98744 (84879)	Loss/tok 3.9437 (5.0496)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.065 (0.083)	Data 8.56e-05 (3.69e-04)	Tok/s 77287 (84901)	Loss/tok 3.4019 (5.0372)	LR 2.000e-03
0: TRAIN [0][1250/1938]	Time 0.091 (0.083)	Data 8.61e-05 (3.67e-04)	Tok/s 92668 (84910)	Loss/tok 3.6141 (5.0264)	LR 2.000e-03
0: TRAIN [0][1260/1938]	Time 0.091 (0.083)	Data 9.73e-05 (3.64e-04)	Tok/s 93360 (84920)	Loss/tok 3.7304 (5.0155)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.091 (0.083)	Data 9.73e-05 (3.62e-04)	Tok/s 91319 (84937)	Loss/tok 3.6378 (5.0046)	LR 2.000e-03
0: TRAIN [0][1280/1938]	Time 0.066 (0.083)	Data 8.73e-05 (3.60e-04)	Tok/s 77818 (84940)	Loss/tok 3.4208 (4.9939)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.067 (0.083)	Data 9.11e-05 (3.58e-04)	Tok/s 76420 (84920)	Loss/tok 3.4724 (4.9846)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.066 (0.083)	Data 9.08e-05 (3.56e-04)	Tok/s 75960 (84912)	Loss/tok 3.1630 (4.9745)	LR 2.000e-03
0: TRAIN [0][1310/1938]	Time 0.066 (0.083)	Data 8.54e-05 (3.54e-04)	Tok/s 78172 (84911)	Loss/tok 3.3409 (4.9643)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.066 (0.083)	Data 1.12e-04 (3.52e-04)	Tok/s 76861 (84946)	Loss/tok 3.4751 (4.9537)	LR 2.000e-03
0: TRAIN [0][1330/1938]	Time 0.092 (0.083)	Data 8.44e-05 (3.50e-04)	Tok/s 91574 (84966)	Loss/tok 3.5396 (4.9436)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.065 (0.083)	Data 8.87e-05 (3.48e-04)	Tok/s 81080 (84939)	Loss/tok 3.2778 (4.9344)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.119 (0.083)	Data 9.89e-05 (3.46e-04)	Tok/s 97539 (84966)	Loss/tok 3.9387 (4.9244)	LR 2.000e-03
0: TRAIN [0][1360/1938]	Time 0.118 (0.083)	Data 9.13e-05 (3.45e-04)	Tok/s 100239 (85010)	Loss/tok 3.8840 (4.9134)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.067 (0.083)	Data 9.89e-05 (3.43e-04)	Tok/s 78494 (84995)	Loss/tok 3.3483 (4.9045)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1380/1938]	Time 0.066 (0.083)	Data 9.01e-05 (3.41e-04)	Tok/s 77690 (85007)	Loss/tok 3.2492 (4.8946)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.092 (0.083)	Data 8.75e-05 (3.39e-04)	Tok/s 90490 (85025)	Loss/tok 3.6679 (4.8858)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.092 (0.083)	Data 8.75e-05 (3.37e-04)	Tok/s 93136 (85053)	Loss/tok 3.5860 (4.8762)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.066 (0.083)	Data 9.39e-05 (3.36e-04)	Tok/s 78587 (85055)	Loss/tok 3.4277 (4.8675)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.065 (0.083)	Data 8.77e-05 (3.34e-04)	Tok/s 79795 (85057)	Loss/tok 3.5267 (4.8591)	LR 2.000e-03
0: TRAIN [0][1430/1938]	Time 0.152 (0.084)	Data 9.16e-05 (3.32e-04)	Tok/s 98720 (85094)	Loss/tok 3.9921 (4.8494)	LR 2.000e-03
0: TRAIN [0][1440/1938]	Time 0.092 (0.084)	Data 9.37e-05 (3.31e-04)	Tok/s 89348 (85087)	Loss/tok 3.5797 (4.8410)	LR 2.000e-03
0: TRAIN [0][1450/1938]	Time 0.065 (0.084)	Data 9.06e-05 (3.29e-04)	Tok/s 81667 (85060)	Loss/tok 3.2500 (4.8332)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.067 (0.084)	Data 8.75e-05 (3.27e-04)	Tok/s 75111 (85039)	Loss/tok 3.2028 (4.8253)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.091 (0.083)	Data 8.73e-05 (3.26e-04)	Tok/s 92921 (85017)	Loss/tok 3.4934 (4.8180)	LR 2.000e-03
0: TRAIN [0][1480/1938]	Time 0.092 (0.083)	Data 9.89e-05 (3.24e-04)	Tok/s 92003 (85032)	Loss/tok 3.5604 (4.8097)	LR 2.000e-03
0: TRAIN [0][1490/1938]	Time 0.065 (0.084)	Data 1.24e-04 (3.23e-04)	Tok/s 79131 (85021)	Loss/tok 3.3640 (4.8021)	LR 2.000e-03
0: TRAIN [0][1500/1938]	Time 0.091 (0.084)	Data 9.01e-05 (3.21e-04)	Tok/s 91589 (85019)	Loss/tok 3.6498 (4.7945)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.066 (0.083)	Data 1.15e-04 (3.20e-04)	Tok/s 76864 (85013)	Loss/tok 3.3477 (4.7868)	LR 2.000e-03
0: TRAIN [0][1520/1938]	Time 0.117 (0.083)	Data 9.51e-05 (3.18e-04)	Tok/s 100481 (85019)	Loss/tok 3.8187 (4.7789)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.093 (0.084)	Data 9.04e-05 (3.17e-04)	Tok/s 91951 (85042)	Loss/tok 3.5318 (4.7705)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.151 (0.084)	Data 8.63e-05 (3.15e-04)	Tok/s 99577 (85050)	Loss/tok 4.0916 (4.7628)	LR 2.000e-03
0: TRAIN [0][1550/1938]	Time 0.044 (0.083)	Data 9.37e-05 (3.14e-04)	Tok/s 61912 (85003)	Loss/tok 2.7724 (4.7564)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.094 (0.084)	Data 9.01e-05 (3.12e-04)	Tok/s 86668 (85021)	Loss/tok 3.6762 (4.7486)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.151 (0.084)	Data 8.80e-05 (3.11e-04)	Tok/s 98605 (85024)	Loss/tok 4.0268 (4.7410)	LR 2.000e-03
0: TRAIN [0][1580/1938]	Time 0.066 (0.084)	Data 8.70e-05 (3.10e-04)	Tok/s 77819 (85021)	Loss/tok 3.2515 (4.7333)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1590/1938]	Time 0.091 (0.084)	Data 8.89e-05 (3.08e-04)	Tok/s 90458 (85060)	Loss/tok 3.6844 (4.7252)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.094 (0.084)	Data 8.80e-05 (3.07e-04)	Tok/s 91660 (85088)	Loss/tok 3.5044 (4.7178)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.068 (0.084)	Data 9.13e-05 (3.06e-04)	Tok/s 76605 (85087)	Loss/tok 3.4183 (4.7111)	LR 2.000e-03
0: TRAIN [0][1620/1938]	Time 0.065 (0.084)	Data 8.87e-05 (3.04e-04)	Tok/s 79679 (85083)	Loss/tok 3.3396 (4.7039)	LR 2.000e-03
0: TRAIN [0][1630/1938]	Time 0.065 (0.084)	Data 9.04e-05 (3.03e-04)	Tok/s 77295 (85095)	Loss/tok 3.3011 (4.6968)	LR 2.000e-03
0: TRAIN [0][1640/1938]	Time 0.066 (0.084)	Data 9.23e-05 (3.02e-04)	Tok/s 81046 (85065)	Loss/tok 3.4152 (4.6906)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.066 (0.084)	Data 8.80e-05 (3.00e-04)	Tok/s 78898 (85088)	Loss/tok 3.4397 (4.6832)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.118 (0.084)	Data 9.80e-05 (2.99e-04)	Tok/s 97801 (85109)	Loss/tok 3.7240 (4.6758)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.091 (0.084)	Data 1.05e-04 (2.98e-04)	Tok/s 92894 (85132)	Loss/tok 3.6474 (4.6688)	LR 2.000e-03
0: TRAIN [0][1680/1938]	Time 0.151 (0.084)	Data 9.35e-05 (2.97e-04)	Tok/s 98790 (85129)	Loss/tok 4.0843 (4.6626)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.043 (0.084)	Data 9.44e-05 (2.95e-04)	Tok/s 61012 (85082)	Loss/tok 2.8794 (4.6568)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.066 (0.084)	Data 8.82e-05 (2.94e-04)	Tok/s 79124 (85073)	Loss/tok 3.5378 (4.6509)	LR 2.000e-03
0: TRAIN [0][1710/1938]	Time 0.066 (0.084)	Data 8.96e-05 (2.93e-04)	Tok/s 77970 (85051)	Loss/tok 3.3446 (4.6448)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.067 (0.083)	Data 8.94e-05 (2.92e-04)	Tok/s 76263 (85027)	Loss/tok 3.3271 (4.6390)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.043 (0.083)	Data 8.51e-05 (2.91e-04)	Tok/s 61009 (84997)	Loss/tok 2.8152 (4.6332)	LR 2.000e-03
0: TRAIN [0][1740/1938]	Time 0.066 (0.083)	Data 8.58e-05 (2.90e-04)	Tok/s 78181 (84975)	Loss/tok 3.4810 (4.6272)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][1750/1938]	Time 0.151 (0.083)	Data 1.08e-04 (2.88e-04)	Tok/s 98456 (84948)	Loss/tok 3.8829 (4.6216)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][1760/1938]	Time 0.067 (0.083)	Data 8.85e-05 (2.87e-04)	Tok/s 76398 (84948)	Loss/tok 3.3049 (4.6154)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.091 (0.083)	Data 8.51e-05 (2.87e-04)	Tok/s 93172 (84940)	Loss/tok 3.4650 (4.6098)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.092 (0.083)	Data 9.27e-05 (2.86e-04)	Tok/s 89968 (84957)	Loss/tok 3.7400 (4.6036)	LR 2.000e-03
0: TRAIN [0][1790/1938]	Time 0.043 (0.083)	Data 8.94e-05 (2.85e-04)	Tok/s 60357 (84937)	Loss/tok 2.8783 (4.5984)	LR 2.000e-03
0: TRAIN [0][1800/1938]	Time 0.065 (0.083)	Data 8.68e-05 (2.84e-04)	Tok/s 77538 (84901)	Loss/tok 3.2774 (4.5932)	LR 2.000e-03
0: TRAIN [0][1810/1938]	Time 0.067 (0.083)	Data 8.80e-05 (2.83e-04)	Tok/s 78193 (84925)	Loss/tok 3.3944 (4.5869)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.066 (0.083)	Data 9.01e-05 (2.81e-04)	Tok/s 76337 (84923)	Loss/tok 3.3548 (4.5816)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.065 (0.083)	Data 8.92e-05 (2.80e-04)	Tok/s 79761 (84925)	Loss/tok 3.2952 (4.5753)	LR 2.000e-03
0: TRAIN [0][1840/1938]	Time 0.091 (0.083)	Data 8.54e-05 (2.79e-04)	Tok/s 91682 (84918)	Loss/tok 3.4596 (4.5700)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.065 (0.083)	Data 9.87e-05 (2.78e-04)	Tok/s 77053 (84913)	Loss/tok 3.2644 (4.5645)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.043 (0.083)	Data 1.32e-04 (2.77e-04)	Tok/s 59150 (84908)	Loss/tok 2.6972 (4.5590)	LR 2.000e-03
0: TRAIN [0][1870/1938]	Time 0.092 (0.083)	Data 8.99e-05 (2.76e-04)	Tok/s 90564 (84884)	Loss/tok 3.5635 (4.5540)	LR 2.000e-03
0: TRAIN [0][1880/1938]	Time 0.152 (0.083)	Data 8.85e-05 (2.75e-04)	Tok/s 99139 (84889)	Loss/tok 3.8791 (4.5485)	LR 2.000e-03
0: TRAIN [0][1890/1938]	Time 0.091 (0.083)	Data 8.73e-05 (2.74e-04)	Tok/s 92254 (84895)	Loss/tok 3.3525 (4.5428)	LR 2.000e-03
0: TRAIN [0][1900/1938]	Time 0.065 (0.083)	Data 8.89e-05 (2.73e-04)	Tok/s 78545 (84850)	Loss/tok 3.3607 (4.5387)	LR 2.000e-03
0: TRAIN [0][1910/1938]	Time 0.119 (0.083)	Data 8.65e-05 (2.72e-04)	Tok/s 101751 (84865)	Loss/tok 3.6232 (4.5328)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.043 (0.083)	Data 9.04e-05 (2.71e-04)	Tok/s 60754 (84861)	Loss/tok 2.7646 (4.5280)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.118 (0.083)	Data 1.21e-04 (2.71e-04)	Tok/s 100023 (84856)	Loss/tok 3.7183 (4.5232)	LR 2.000e-03
:::MLL 1560904921.976 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1560904921.976 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.509 (0.509)	Decoder iters 149.0 (149.0)	Tok/s 16795 (16795)
0: Running moses detokenizer
0: BLEU(score=19.857050564938167, counts=[33932, 15497, 8276, 4622], totals=[62982, 59979, 56976, 53978], precisions=[53.875710520466164, 25.837376415078612, 14.525414209491716, 8.562747786135093], bp=0.9734619179560914, sys_len=62982, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1560904923.224 eval_accuracy: {"value": 19.86, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1560904923.224 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 4.5209	Test BLEU: 19.86
0: Performance: Epoch: 0	Training: 1356877 Tok/s
0: Finished epoch 0
:::MLL 1560904923.225 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1560904923.225 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1560904923.225 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 1141214796
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [1][0/1938]	Time 0.459 (0.459)	Data 3.05e-01 (3.05e-01)	Tok/s 25593 (25593)	Loss/tok 3.6877 (3.6877)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.117 (0.113)	Data 9.11e-05 (2.78e-02)	Tok/s 98904 (78412)	Loss/tok 3.6468 (3.4665)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.066 (0.099)	Data 1.06e-04 (1.46e-02)	Tok/s 78616 (82605)	Loss/tok 3.2665 (3.4573)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.090 (0.096)	Data 9.89e-05 (9.91e-03)	Tok/s 94454 (84763)	Loss/tok 3.3470 (3.4667)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.091 (0.092)	Data 8.82e-05 (7.55e-03)	Tok/s 91597 (84943)	Loss/tok 3.4818 (3.4672)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.066 (0.087)	Data 9.08e-05 (6.09e-03)	Tok/s 78429 (83391)	Loss/tok 3.2490 (3.4306)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.092 (0.086)	Data 1.24e-04 (5.11e-03)	Tok/s 92187 (83828)	Loss/tok 3.2308 (3.4172)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.149 (0.085)	Data 9.18e-05 (4.40e-03)	Tok/s 100216 (83284)	Loss/tok 3.9077 (3.4329)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.120 (0.087)	Data 1.29e-04 (3.87e-03)	Tok/s 96667 (84126)	Loss/tok 3.6282 (3.4556)	LR 2.000e-03
0: TRAIN [1][90/1938]	Time 0.067 (0.088)	Data 8.51e-05 (3.45e-03)	Tok/s 75684 (84319)	Loss/tok 3.1725 (3.4673)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.065 (0.086)	Data 8.70e-05 (3.12e-03)	Tok/s 78706 (83915)	Loss/tok 3.2054 (3.4585)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.042 (0.086)	Data 8.68e-05 (2.85e-03)	Tok/s 62462 (84019)	Loss/tok 2.7849 (3.4598)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.092 (0.086)	Data 8.96e-05 (2.62e-03)	Tok/s 92146 (84536)	Loss/tok 3.4265 (3.4676)	LR 2.000e-03
0: TRAIN [1][130/1938]	Time 0.066 (0.086)	Data 8.70e-05 (2.43e-03)	Tok/s 78393 (84701)	Loss/tok 3.1090 (3.4633)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.066 (0.087)	Data 8.56e-05 (2.26e-03)	Tok/s 77522 (85065)	Loss/tok 3.3080 (3.4742)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.091 (0.087)	Data 8.34e-05 (2.12e-03)	Tok/s 93123 (84925)	Loss/tok 3.4570 (3.4692)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.067 (0.087)	Data 8.70e-05 (1.99e-03)	Tok/s 78225 (84859)	Loss/tok 3.1935 (3.4746)	LR 2.000e-03
0: TRAIN [1][170/1938]	Time 0.065 (0.087)	Data 8.58e-05 (1.88e-03)	Tok/s 78506 (84942)	Loss/tok 3.3140 (3.4757)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.067 (0.086)	Data 8.96e-05 (1.78e-03)	Tok/s 77446 (84779)	Loss/tok 3.1913 (3.4714)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.065 (0.086)	Data 8.65e-05 (1.69e-03)	Tok/s 78827 (84868)	Loss/tok 3.1852 (3.4701)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.043 (0.086)	Data 8.68e-05 (1.61e-03)	Tok/s 60785 (84697)	Loss/tok 2.9144 (3.4644)	LR 2.000e-03
0: TRAIN [1][210/1938]	Time 0.118 (0.086)	Data 1.25e-04 (1.54e-03)	Tok/s 99474 (84744)	Loss/tok 3.5081 (3.4653)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.092 (0.086)	Data 8.89e-05 (1.48e-03)	Tok/s 92310 (84967)	Loss/tok 3.5619 (3.4691)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.067 (0.086)	Data 1.29e-04 (1.42e-03)	Tok/s 75557 (84915)	Loss/tok 3.3818 (3.4643)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.119 (0.086)	Data 8.96e-05 (1.36e-03)	Tok/s 100429 (85148)	Loss/tok 3.7218 (3.4700)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.151 (0.086)	Data 9.01e-05 (1.31e-03)	Tok/s 97180 (85094)	Loss/tok 3.8614 (3.4719)	LR 2.000e-03
0: TRAIN [1][260/1938]	Time 0.091 (0.086)	Data 9.30e-05 (1.26e-03)	Tok/s 94485 (85184)	Loss/tok 3.4926 (3.4727)	LR 2.000e-03
0: TRAIN [1][270/1938]	Time 0.091 (0.087)	Data 8.96e-05 (1.22e-03)	Tok/s 93105 (85309)	Loss/tok 3.5109 (3.4759)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.065 (0.086)	Data 8.37e-05 (1.18e-03)	Tok/s 79565 (85210)	Loss/tok 3.1402 (3.4716)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.065 (0.086)	Data 8.82e-05 (1.14e-03)	Tok/s 78284 (85087)	Loss/tok 3.0346 (3.4709)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.152 (0.087)	Data 9.25e-05 (1.11e-03)	Tok/s 99305 (85294)	Loss/tok 3.6533 (3.4747)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][310/1938]	Time 0.065 (0.086)	Data 8.94e-05 (1.08e-03)	Tok/s 77949 (85255)	Loss/tok 3.2717 (3.4722)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.092 (0.086)	Data 8.68e-05 (1.05e-03)	Tok/s 89824 (85236)	Loss/tok 3.5681 (3.4705)	LR 2.000e-03
0: TRAIN [1][330/1938]	Time 0.068 (0.086)	Data 1.06e-04 (1.02e-03)	Tok/s 75681 (85182)	Loss/tok 3.5276 (3.4691)	LR 2.000e-03
0: TRAIN [1][340/1938]	Time 0.066 (0.086)	Data 9.73e-05 (9.89e-04)	Tok/s 78004 (85277)	Loss/tok 3.1453 (3.4684)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.066 (0.086)	Data 8.87e-05 (9.63e-04)	Tok/s 78791 (85335)	Loss/tok 3.2551 (3.4688)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.091 (0.086)	Data 9.82e-05 (9.39e-04)	Tok/s 93596 (85399)	Loss/tok 3.3479 (3.4684)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.092 (0.086)	Data 8.61e-05 (9.16e-04)	Tok/s 90657 (85267)	Loss/tok 3.4283 (3.4652)	LR 2.000e-03
0: TRAIN [1][380/1938]	Time 0.066 (0.086)	Data 1.04e-04 (8.95e-04)	Tok/s 76289 (85350)	Loss/tok 3.1985 (3.4700)	LR 2.000e-03
0: TRAIN [1][390/1938]	Time 0.151 (0.087)	Data 8.25e-05 (8.74e-04)	Tok/s 96088 (85485)	Loss/tok 3.8143 (3.4709)	LR 2.000e-03
0: TRAIN [1][400/1938]	Time 0.066 (0.087)	Data 8.85e-05 (8.55e-04)	Tok/s 78154 (85513)	Loss/tok 3.2064 (3.4694)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.043 (0.087)	Data 1.29e-04 (8.36e-04)	Tok/s 61826 (85506)	Loss/tok 2.8487 (3.4707)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.065 (0.087)	Data 8.94e-05 (8.18e-04)	Tok/s 78630 (85500)	Loss/tok 3.1497 (3.4711)	LR 2.000e-03
0: TRAIN [1][430/1938]	Time 0.066 (0.087)	Data 9.23e-05 (8.01e-04)	Tok/s 78680 (85503)	Loss/tok 3.3774 (3.4705)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.118 (0.087)	Data 8.80e-05 (7.85e-04)	Tok/s 99605 (85534)	Loss/tok 3.5523 (3.4712)	LR 2.000e-03
0: TRAIN [1][450/1938]	Time 0.150 (0.086)	Data 8.89e-05 (7.70e-04)	Tok/s 99077 (85390)	Loss/tok 3.8739 (3.4702)	LR 2.000e-03
0: TRAIN [1][460/1938]	Time 0.092 (0.086)	Data 9.08e-05 (7.55e-04)	Tok/s 89867 (85396)	Loss/tok 3.4620 (3.4699)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.067 (0.086)	Data 8.89e-05 (7.41e-04)	Tok/s 78736 (85404)	Loss/tok 3.1860 (3.4694)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.093 (0.086)	Data 9.18e-05 (7.28e-04)	Tok/s 91231 (85338)	Loss/tok 3.3574 (3.4691)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][490/1938]	Time 0.066 (0.086)	Data 8.96e-05 (7.15e-04)	Tok/s 77664 (85298)	Loss/tok 3.2313 (3.4683)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.092 (0.086)	Data 8.77e-05 (7.02e-04)	Tok/s 92887 (85305)	Loss/tok 3.2450 (3.4654)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.066 (0.086)	Data 1.22e-04 (6.90e-04)	Tok/s 74599 (85276)	Loss/tok 3.0783 (3.4634)	LR 2.000e-03
0: TRAIN [1][520/1938]	Time 0.091 (0.086)	Data 8.70e-05 (6.79e-04)	Tok/s 92000 (85324)	Loss/tok 3.4140 (3.4617)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.092 (0.086)	Data 1.23e-04 (6.68e-04)	Tok/s 89957 (85313)	Loss/tok 3.4176 (3.4602)	LR 2.000e-03
0: TRAIN [1][540/1938]	Time 0.091 (0.086)	Data 8.46e-05 (6.57e-04)	Tok/s 93409 (85220)	Loss/tok 3.4733 (3.4594)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.066 (0.086)	Data 8.92e-05 (6.47e-04)	Tok/s 78112 (85296)	Loss/tok 3.3131 (3.4632)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.119 (0.086)	Data 9.11e-05 (6.37e-04)	Tok/s 99554 (85346)	Loss/tok 3.4692 (3.4643)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.067 (0.086)	Data 9.35e-05 (6.28e-04)	Tok/s 77963 (85452)	Loss/tok 3.2103 (3.4658)	LR 2.000e-03
0: TRAIN [1][580/1938]	Time 0.092 (0.086)	Data 8.54e-05 (6.18e-04)	Tok/s 90910 (85498)	Loss/tok 3.3385 (3.4652)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.091 (0.086)	Data 8.75e-05 (6.09e-04)	Tok/s 90388 (85488)	Loss/tok 3.4452 (3.4636)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.065 (0.086)	Data 9.01e-05 (6.01e-04)	Tok/s 81871 (85423)	Loss/tok 3.2101 (3.4616)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.093 (0.086)	Data 9.87e-05 (5.92e-04)	Tok/s 89006 (85379)	Loss/tok 3.4377 (3.4588)	LR 2.000e-03
0: TRAIN [1][620/1938]	Time 0.043 (0.086)	Data 8.46e-05 (5.84e-04)	Tok/s 62208 (85335)	Loss/tok 2.7866 (3.4567)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.090 (0.085)	Data 8.68e-05 (5.76e-04)	Tok/s 91627 (85276)	Loss/tok 3.3955 (3.4549)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.066 (0.086)	Data 8.73e-05 (5.69e-04)	Tok/s 76576 (85300)	Loss/tok 3.3742 (3.4559)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.066 (0.086)	Data 1.16e-04 (5.62e-04)	Tok/s 81146 (85343)	Loss/tok 3.2915 (3.4573)	LR 2.000e-03
0: TRAIN [1][660/1938]	Time 0.118 (0.086)	Data 8.80e-05 (5.55e-04)	Tok/s 98861 (85419)	Loss/tok 3.7337 (3.4579)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.093 (0.086)	Data 9.42e-05 (5.48e-04)	Tok/s 89853 (85383)	Loss/tok 3.4179 (3.4560)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.065 (0.085)	Data 8.70e-05 (5.41e-04)	Tok/s 80157 (85230)	Loss/tok 3.2147 (3.4531)	LR 2.000e-03
0: TRAIN [1][690/1938]	Time 0.067 (0.086)	Data 9.20e-05 (5.34e-04)	Tok/s 75849 (85293)	Loss/tok 3.1959 (3.4523)	LR 2.000e-03
0: TRAIN [1][700/1938]	Time 0.093 (0.086)	Data 8.99e-05 (5.28e-04)	Tok/s 91466 (85341)	Loss/tok 3.4693 (3.4510)	LR 2.000e-03
0: TRAIN [1][710/1938]	Time 0.094 (0.086)	Data 9.06e-05 (5.22e-04)	Tok/s 89363 (85375)	Loss/tok 3.4303 (3.4503)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.065 (0.086)	Data 8.75e-05 (5.16e-04)	Tok/s 79409 (85339)	Loss/tok 3.2182 (3.4506)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.092 (0.085)	Data 8.30e-05 (5.10e-04)	Tok/s 91658 (85300)	Loss/tok 3.3529 (3.4502)	LR 2.000e-03
0: TRAIN [1][740/1938]	Time 0.119 (0.086)	Data 9.23e-05 (5.05e-04)	Tok/s 98717 (85404)	Loss/tok 3.5624 (3.4502)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][750/1938]	Time 0.066 (0.086)	Data 1.21e-04 (4.99e-04)	Tok/s 81118 (85376)	Loss/tok 3.1417 (3.4497)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.065 (0.086)	Data 8.70e-05 (4.94e-04)	Tok/s 76268 (85284)	Loss/tok 3.1254 (3.4490)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.091 (0.086)	Data 8.34e-05 (4.88e-04)	Tok/s 92516 (85296)	Loss/tok 3.4507 (3.4482)	LR 2.000e-03
0: TRAIN [1][780/1938]	Time 0.066 (0.086)	Data 1.22e-04 (4.83e-04)	Tok/s 77093 (85317)	Loss/tok 3.2832 (3.4477)	LR 2.000e-03
0: TRAIN [1][790/1938]	Time 0.066 (0.086)	Data 9.01e-05 (4.78e-04)	Tok/s 77095 (85324)	Loss/tok 3.0884 (3.4467)	LR 2.000e-03
0: TRAIN [1][800/1938]	Time 0.065 (0.085)	Data 8.30e-05 (4.74e-04)	Tok/s 79238 (85272)	Loss/tok 3.1630 (3.4451)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.116 (0.085)	Data 9.37e-05 (4.69e-04)	Tok/s 101094 (85302)	Loss/tok 3.5533 (3.4461)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.118 (0.085)	Data 9.13e-05 (4.64e-04)	Tok/s 98228 (85285)	Loss/tok 3.7521 (3.4465)	LR 2.000e-03
0: TRAIN [1][830/1938]	Time 0.151 (0.085)	Data 8.61e-05 (4.60e-04)	Tok/s 98055 (85236)	Loss/tok 3.7658 (3.4476)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.118 (0.085)	Data 9.27e-05 (4.55e-04)	Tok/s 100818 (85227)	Loss/tok 3.6109 (3.4478)	LR 2.000e-03
0: TRAIN [1][850/1938]	Time 0.067 (0.085)	Data 8.58e-05 (4.51e-04)	Tok/s 78189 (85191)	Loss/tok 3.3373 (3.4480)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.091 (0.085)	Data 1.01e-04 (4.47e-04)	Tok/s 92566 (85180)	Loss/tok 3.3028 (3.4463)	LR 2.000e-03
0: TRAIN [1][870/1938]	Time 0.092 (0.085)	Data 8.70e-05 (4.43e-04)	Tok/s 91895 (85068)	Loss/tok 3.2310 (3.4446)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.067 (0.085)	Data 8.82e-05 (4.39e-04)	Tok/s 79507 (85067)	Loss/tok 3.0561 (3.4435)	LR 2.000e-03
0: TRAIN [1][890/1938]	Time 0.068 (0.085)	Data 8.73e-05 (4.35e-04)	Tok/s 78547 (85100)	Loss/tok 3.1110 (3.4434)	LR 2.000e-03
0: TRAIN [1][900/1938]	Time 0.065 (0.085)	Data 1.03e-04 (4.31e-04)	Tok/s 79638 (85084)	Loss/tok 3.1420 (3.4427)	LR 2.000e-03
0: TRAIN [1][910/1938]	Time 0.065 (0.085)	Data 8.89e-05 (4.27e-04)	Tok/s 81156 (85077)	Loss/tok 3.2344 (3.4413)	LR 2.000e-03
0: TRAIN [1][920/1938]	Time 0.091 (0.085)	Data 9.54e-05 (4.28e-04)	Tok/s 91518 (85077)	Loss/tok 3.3432 (3.4410)	LR 2.000e-03
0: TRAIN [1][930/1938]	Time 0.091 (0.085)	Data 9.18e-05 (4.24e-04)	Tok/s 91631 (85138)	Loss/tok 3.6954 (3.4408)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.119 (0.085)	Data 1.03e-04 (4.21e-04)	Tok/s 99137 (85078)	Loss/tok 3.4676 (3.4393)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.092 (0.085)	Data 9.08e-05 (4.17e-04)	Tok/s 92302 (85120)	Loss/tok 3.3165 (3.4389)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][960/1938]	Time 0.065 (0.085)	Data 8.82e-05 (4.14e-04)	Tok/s 78959 (85148)	Loss/tok 3.1813 (3.4389)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.091 (0.085)	Data 8.77e-05 (4.10e-04)	Tok/s 91384 (85129)	Loss/tok 3.5022 (3.4383)	LR 2.000e-03
0: TRAIN [1][980/1938]	Time 0.093 (0.085)	Data 8.85e-05 (4.07e-04)	Tok/s 90321 (85108)	Loss/tok 3.5098 (3.4386)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.066 (0.085)	Data 8.65e-05 (4.04e-04)	Tok/s 75988 (85090)	Loss/tok 3.2390 (3.4380)	LR 2.000e-03
0: TRAIN [1][1000/1938]	Time 0.067 (0.085)	Data 8.34e-05 (4.01e-04)	Tok/s 80724 (85101)	Loss/tok 2.9822 (3.4387)	LR 2.000e-03
0: TRAIN [1][1010/1938]	Time 0.044 (0.085)	Data 8.75e-05 (3.98e-04)	Tok/s 60202 (85080)	Loss/tok 2.7387 (3.4378)	LR 2.000e-03
0: TRAIN [1][1020/1938]	Time 0.042 (0.085)	Data 9.01e-05 (3.95e-04)	Tok/s 62544 (85075)	Loss/tok 2.8287 (3.4365)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.151 (0.085)	Data 9.01e-05 (3.92e-04)	Tok/s 96904 (85087)	Loss/tok 3.7650 (3.4354)	LR 2.000e-03
0: TRAIN [1][1040/1938]	Time 0.066 (0.085)	Data 9.06e-05 (3.89e-04)	Tok/s 77757 (85071)	Loss/tok 3.0963 (3.4343)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.043 (0.085)	Data 1.02e-04 (3.86e-04)	Tok/s 61149 (85069)	Loss/tok 2.7860 (3.4336)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.065 (0.085)	Data 8.39e-05 (3.84e-04)	Tok/s 81217 (85061)	Loss/tok 3.2205 (3.4337)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.065 (0.085)	Data 9.37e-05 (3.81e-04)	Tok/s 79407 (85033)	Loss/tok 3.1164 (3.4322)	LR 2.000e-03
0: TRAIN [1][1080/1938]	Time 0.066 (0.085)	Data 8.54e-05 (3.78e-04)	Tok/s 77151 (85000)	Loss/tok 3.2099 (3.4319)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.043 (0.085)	Data 8.77e-05 (3.76e-04)	Tok/s 61265 (84959)	Loss/tok 2.6701 (3.4304)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.067 (0.085)	Data 1.01e-04 (3.73e-04)	Tok/s 79256 (84969)	Loss/tok 3.1485 (3.4301)	LR 2.000e-03
0: TRAIN [1][1110/1938]	Time 0.065 (0.085)	Data 8.39e-05 (3.70e-04)	Tok/s 79735 (84926)	Loss/tok 3.3949 (3.4288)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.065 (0.084)	Data 8.70e-05 (3.68e-04)	Tok/s 83152 (84883)	Loss/tok 3.0714 (3.4273)	LR 2.000e-03
0: TRAIN [1][1130/1938]	Time 0.043 (0.084)	Data 8.56e-05 (3.65e-04)	Tok/s 61251 (84844)	Loss/tok 2.7346 (3.4262)	LR 2.000e-03
0: TRAIN [1][1140/1938]	Time 0.121 (0.084)	Data 8.92e-05 (3.63e-04)	Tok/s 95264 (84849)	Loss/tok 3.6624 (3.4264)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.067 (0.084)	Data 1.20e-04 (3.61e-04)	Tok/s 74464 (84846)	Loss/tok 3.0440 (3.4256)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1160/1938]	Time 0.065 (0.084)	Data 1.22e-04 (3.58e-04)	Tok/s 77790 (84867)	Loss/tok 3.1719 (3.4260)	LR 2.000e-03
0: TRAIN [1][1170/1938]	Time 0.065 (0.084)	Data 8.63e-05 (3.56e-04)	Tok/s 78604 (84833)	Loss/tok 3.1207 (3.4248)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.092 (0.084)	Data 8.96e-05 (3.54e-04)	Tok/s 89796 (84857)	Loss/tok 3.2892 (3.4248)	LR 2.000e-03
0: TRAIN [1][1190/1938]	Time 0.043 (0.084)	Data 8.65e-05 (3.52e-04)	Tok/s 62113 (84806)	Loss/tok 2.7822 (3.4232)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.090 (0.084)	Data 8.63e-05 (3.50e-04)	Tok/s 92626 (84787)	Loss/tok 3.2553 (3.4229)	LR 2.000e-03
0: TRAIN [1][1210/1938]	Time 0.066 (0.084)	Data 8.68e-05 (3.47e-04)	Tok/s 79549 (84770)	Loss/tok 3.3369 (3.4220)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.065 (0.084)	Data 9.08e-05 (3.45e-04)	Tok/s 76655 (84766)	Loss/tok 3.1085 (3.4221)	LR 2.000e-03
0: TRAIN [1][1230/1938]	Time 0.092 (0.084)	Data 8.99e-05 (3.43e-04)	Tok/s 91950 (84803)	Loss/tok 3.3079 (3.4225)	LR 2.000e-03
0: TRAIN [1][1240/1938]	Time 0.065 (0.084)	Data 8.85e-05 (3.41e-04)	Tok/s 77886 (84744)	Loss/tok 3.2027 (3.4208)	LR 2.000e-03
0: TRAIN [1][1250/1938]	Time 0.092 (0.084)	Data 9.42e-05 (3.39e-04)	Tok/s 90592 (84708)	Loss/tok 3.4074 (3.4198)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.120 (0.084)	Data 8.75e-05 (3.37e-04)	Tok/s 97168 (84718)	Loss/tok 3.5279 (3.4192)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.091 (0.084)	Data 9.75e-05 (3.35e-04)	Tok/s 91448 (84667)	Loss/tok 3.3644 (3.4175)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.092 (0.084)	Data 8.65e-05 (3.33e-04)	Tok/s 90858 (84682)	Loss/tok 3.4423 (3.4175)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.093 (0.084)	Data 9.01e-05 (3.32e-04)	Tok/s 88457 (84667)	Loss/tok 3.5873 (3.4176)	LR 2.000e-03
0: TRAIN [1][1300/1938]	Time 0.066 (0.084)	Data 8.80e-05 (3.30e-04)	Tok/s 78780 (84653)	Loss/tok 3.1583 (3.4166)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.092 (0.084)	Data 9.99e-05 (3.28e-04)	Tok/s 91371 (84672)	Loss/tok 3.3773 (3.4166)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1320/1938]	Time 0.066 (0.084)	Data 1.03e-04 (3.26e-04)	Tok/s 79694 (84651)	Loss/tok 3.2643 (3.4160)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.092 (0.084)	Data 8.77e-05 (3.24e-04)	Tok/s 89033 (84647)	Loss/tok 3.4468 (3.4156)	LR 2.000e-03
0: TRAIN [1][1340/1938]	Time 0.043 (0.084)	Data 8.46e-05 (3.23e-04)	Tok/s 61719 (84633)	Loss/tok 2.7481 (3.4153)	LR 2.000e-03
0: TRAIN [1][1350/1938]	Time 0.044 (0.084)	Data 8.73e-05 (3.21e-04)	Tok/s 60214 (84639)	Loss/tok 2.7354 (3.4147)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.116 (0.084)	Data 1.20e-04 (3.19e-04)	Tok/s 100385 (84655)	Loss/tok 3.5269 (3.4152)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.119 (0.084)	Data 9.13e-05 (3.18e-04)	Tok/s 99823 (84675)	Loss/tok 3.4448 (3.4148)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.066 (0.084)	Data 9.16e-05 (3.16e-04)	Tok/s 79214 (84673)	Loss/tok 3.0566 (3.4148)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.066 (0.084)	Data 9.06e-05 (3.14e-04)	Tok/s 80912 (84675)	Loss/tok 3.2089 (3.4140)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.118 (0.084)	Data 9.16e-05 (3.13e-04)	Tok/s 99386 (84672)	Loss/tok 3.5133 (3.4139)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.043 (0.084)	Data 8.70e-05 (3.11e-04)	Tok/s 61391 (84648)	Loss/tok 2.7044 (3.4130)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.117 (0.084)	Data 9.27e-05 (3.10e-04)	Tok/s 100878 (84687)	Loss/tok 3.4312 (3.4128)	LR 2.000e-03
0: TRAIN [1][1430/1938]	Time 0.043 (0.084)	Data 8.77e-05 (3.08e-04)	Tok/s 61599 (84697)	Loss/tok 2.7063 (3.4123)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.065 (0.084)	Data 9.92e-05 (3.07e-04)	Tok/s 81188 (84703)	Loss/tok 3.0292 (3.4116)	LR 2.000e-03
0: TRAIN [1][1450/1938]	Time 0.066 (0.084)	Data 8.92e-05 (3.05e-04)	Tok/s 77371 (84733)	Loss/tok 3.0262 (3.4120)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.066 (0.084)	Data 9.04e-05 (3.04e-04)	Tok/s 79203 (84715)	Loss/tok 3.1788 (3.4114)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.043 (0.084)	Data 8.70e-05 (3.02e-04)	Tok/s 61541 (84707)	Loss/tok 2.5738 (3.4112)	LR 2.000e-03
0: TRAIN [1][1480/1938]	Time 0.066 (0.084)	Data 8.42e-05 (3.01e-04)	Tok/s 77498 (84673)	Loss/tok 3.2561 (3.4107)	LR 2.000e-03
0: TRAIN [1][1490/1938]	Time 0.065 (0.084)	Data 8.37e-05 (2.99e-04)	Tok/s 79201 (84671)	Loss/tok 3.2045 (3.4101)	LR 2.000e-03
0: TRAIN [1][1500/1938]	Time 0.065 (0.084)	Data 9.01e-05 (2.98e-04)	Tok/s 80235 (84656)	Loss/tok 3.0555 (3.4096)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.074 (0.084)	Data 9.37e-05 (2.97e-04)	Tok/s 69131 (84626)	Loss/tok 3.2308 (3.4087)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1520/1938]	Time 0.043 (0.084)	Data 9.25e-05 (2.95e-04)	Tok/s 61711 (84625)	Loss/tok 2.8462 (3.4087)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.043 (0.084)	Data 9.70e-05 (2.94e-04)	Tok/s 62446 (84638)	Loss/tok 2.7179 (3.4084)	LR 2.000e-03
0: TRAIN [1][1540/1938]	Time 0.091 (0.084)	Data 8.77e-05 (2.93e-04)	Tok/s 91462 (84649)	Loss/tok 3.3073 (3.4076)	LR 2.000e-03
0: TRAIN [1][1550/1938]	Time 0.066 (0.084)	Data 8.99e-05 (2.91e-04)	Tok/s 80577 (84672)	Loss/tok 3.2811 (3.4073)	LR 2.000e-03
0: TRAIN [1][1560/1938]	Time 0.090 (0.084)	Data 9.18e-05 (2.90e-04)	Tok/s 92861 (84646)	Loss/tok 3.4381 (3.4068)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.066 (0.084)	Data 9.25e-05 (2.89e-04)	Tok/s 77846 (84648)	Loss/tok 3.1498 (3.4072)	LR 2.000e-03
0: TRAIN [1][1580/1938]	Time 0.091 (0.084)	Data 8.75e-05 (2.88e-04)	Tok/s 93517 (84673)	Loss/tok 3.4096 (3.4071)	LR 2.000e-03
0: TRAIN [1][1590/1938]	Time 0.151 (0.084)	Data 8.99e-05 (2.86e-04)	Tok/s 98527 (84684)	Loss/tok 3.4999 (3.4067)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.092 (0.084)	Data 8.89e-05 (2.85e-04)	Tok/s 90976 (84662)	Loss/tok 3.3934 (3.4059)	LR 2.000e-03
0: TRAIN [1][1610/1938]	Time 0.066 (0.084)	Data 9.30e-05 (2.84e-04)	Tok/s 78013 (84677)	Loss/tok 3.0988 (3.4060)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.090 (0.084)	Data 8.73e-05 (2.83e-04)	Tok/s 92593 (84655)	Loss/tok 3.5241 (3.4054)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.066 (0.084)	Data 8.70e-05 (2.82e-04)	Tok/s 78679 (84644)	Loss/tok 3.2336 (3.4055)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.065 (0.084)	Data 8.73e-05 (2.81e-04)	Tok/s 78627 (84661)	Loss/tok 3.2115 (3.4055)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.090 (0.084)	Data 8.92e-05 (2.79e-04)	Tok/s 92937 (84643)	Loss/tok 3.1903 (3.4046)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.067 (0.084)	Data 8.70e-05 (2.78e-04)	Tok/s 76903 (84628)	Loss/tok 3.1986 (3.4038)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.117 (0.084)	Data 8.92e-05 (2.77e-04)	Tok/s 100963 (84642)	Loss/tok 3.5164 (3.4034)	LR 2.000e-03
0: TRAIN [1][1680/1938]	Time 0.044 (0.084)	Data 9.37e-05 (2.76e-04)	Tok/s 61713 (84649)	Loss/tok 2.8770 (3.4028)	LR 2.000e-03
0: TRAIN [1][1690/1938]	Time 0.092 (0.084)	Data 8.92e-05 (2.75e-04)	Tok/s 90133 (84637)	Loss/tok 3.3738 (3.4022)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.093 (0.084)	Data 9.01e-05 (2.74e-04)	Tok/s 91421 (84624)	Loss/tok 3.3789 (3.4023)	LR 2.000e-03
0: TRAIN [1][1710/1938]	Time 0.044 (0.084)	Data 9.49e-05 (2.73e-04)	Tok/s 62618 (84615)	Loss/tok 2.6590 (3.4015)	LR 2.000e-03
0: TRAIN [1][1720/1938]	Time 0.065 (0.084)	Data 8.87e-05 (2.72e-04)	Tok/s 77374 (84601)	Loss/tok 3.2515 (3.4011)	LR 2.000e-03
0: TRAIN [1][1730/1938]	Time 0.118 (0.083)	Data 9.39e-05 (2.71e-04)	Tok/s 98040 (84584)	Loss/tok 3.5553 (3.4003)	LR 2.000e-03
0: TRAIN [1][1740/1938]	Time 0.065 (0.083)	Data 9.47e-05 (2.70e-04)	Tok/s 79823 (84595)	Loss/tok 3.4048 (3.4004)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.067 (0.083)	Data 1.11e-04 (2.69e-04)	Tok/s 78540 (84599)	Loss/tok 3.0486 (3.4001)	LR 2.000e-03
0: TRAIN [1][1760/1938]	Time 0.091 (0.083)	Data 9.30e-05 (2.68e-04)	Tok/s 92547 (84560)	Loss/tok 3.4782 (3.3993)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.066 (0.083)	Data 9.39e-05 (2.67e-04)	Tok/s 76453 (84566)	Loss/tok 3.3067 (3.3990)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1780/1938]	Time 0.119 (0.083)	Data 1.31e-04 (2.66e-04)	Tok/s 98759 (84578)	Loss/tok 3.5576 (3.3984)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.119 (0.083)	Data 8.80e-05 (2.65e-04)	Tok/s 98605 (84579)	Loss/tok 3.5520 (3.3978)	LR 2.000e-03
0: TRAIN [1][1800/1938]	Time 0.044 (0.083)	Data 9.06e-05 (2.64e-04)	Tok/s 61094 (84570)	Loss/tok 2.6947 (3.3974)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.092 (0.083)	Data 8.85e-05 (2.63e-04)	Tok/s 92027 (84593)	Loss/tok 3.2643 (3.3970)	LR 2.000e-03
0: TRAIN [1][1820/1938]	Time 0.066 (0.083)	Data 8.82e-05 (2.62e-04)	Tok/s 80024 (84588)	Loss/tok 3.0150 (3.3968)	LR 2.000e-03
0: TRAIN [1][1830/1938]	Time 0.066 (0.083)	Data 9.13e-05 (2.61e-04)	Tok/s 75982 (84587)	Loss/tok 3.0042 (3.3961)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.118 (0.083)	Data 8.96e-05 (2.60e-04)	Tok/s 98415 (84595)	Loss/tok 3.4625 (3.3958)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1850/1938]	Time 0.065 (0.083)	Data 1.31e-04 (2.59e-04)	Tok/s 78948 (84578)	Loss/tok 2.9686 (3.3952)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.117 (0.083)	Data 9.11e-05 (2.58e-04)	Tok/s 98724 (84572)	Loss/tok 3.4083 (3.3949)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.066 (0.083)	Data 8.58e-05 (2.58e-04)	Tok/s 78739 (84559)	Loss/tok 3.2118 (3.3947)	LR 2.000e-03
0: TRAIN [1][1880/1938]	Time 0.043 (0.083)	Data 8.58e-05 (2.57e-04)	Tok/s 60389 (84558)	Loss/tok 2.6224 (3.3941)	LR 2.000e-03
0: TRAIN [1][1890/1938]	Time 0.094 (0.083)	Data 9.11e-05 (2.56e-04)	Tok/s 90415 (84572)	Loss/tok 3.4608 (3.3940)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.092 (0.083)	Data 9.16e-05 (2.55e-04)	Tok/s 91066 (84572)	Loss/tok 3.2643 (3.3933)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.118 (0.083)	Data 8.99e-05 (2.54e-04)	Tok/s 95851 (84564)	Loss/tok 3.5868 (3.3928)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.118 (0.083)	Data 9.08e-05 (2.53e-04)	Tok/s 97171 (84577)	Loss/tok 3.5082 (3.3925)	LR 2.000e-03
0: TRAIN [1][1930/1938]	Time 0.066 (0.083)	Data 8.99e-05 (2.52e-04)	Tok/s 80404 (84560)	Loss/tok 3.2698 (3.3920)	LR 2.000e-03
:::MLL 1560905085.449 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1560905085.450 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.453 (0.453)	Decoder iters 116.0 (116.0)	Tok/s 19901 (19901)
0: Running moses detokenizer
0: BLEU(score=22.15303376246992, counts=[36257, 17487, 9652, 5552], totals=[65881, 62878, 59875, 56877], precisions=[55.03407659264431, 27.810999077578803, 16.120250521920667, 9.76141498320938], bp=1.0, sys_len=65881, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1560905086.710 eval_accuracy: {"value": 22.15, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1560905086.711 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.3958	Test BLEU: 22.15
0: Performance: Epoch: 1	Training: 1352909 Tok/s
0: Finished epoch 1
:::MLL 1560905086.711 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1560905086.712 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1560905086.712 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 1704803886
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [2][0/1938]	Time 0.423 (0.423)	Data 3.05e-01 (3.05e-01)	Tok/s 19735 (19735)	Loss/tok 3.3513 (3.3513)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [2][10/1938]	Time 0.211 (0.125)	Data 8.70e-05 (2.78e-02)	Tok/s 54964 (72819)	Loss/tok 3.4879 (3.3623)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.091 (0.107)	Data 8.73e-05 (1.46e-02)	Tok/s 92267 (79372)	Loss/tok 3.1151 (3.3114)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.066 (0.099)	Data 8.54e-05 (9.93e-03)	Tok/s 78996 (81046)	Loss/tok 3.1670 (3.2874)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.068 (0.095)	Data 9.42e-05 (7.53e-03)	Tok/s 77607 (81698)	Loss/tok 2.9936 (3.2720)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.118 (0.094)	Data 8.68e-05 (6.07e-03)	Tok/s 97271 (83249)	Loss/tok 3.3614 (3.2706)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.149 (0.092)	Data 8.39e-05 (5.09e-03)	Tok/s 98453 (82695)	Loss/tok 3.6414 (3.2779)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.117 (0.092)	Data 8.96e-05 (4.39e-03)	Tok/s 100213 (83134)	Loss/tok 3.4449 (3.2876)	LR 2.000e-03
0: TRAIN [2][80/1938]	Time 0.043 (0.091)	Data 8.96e-05 (3.86e-03)	Tok/s 61394 (83332)	Loss/tok 2.4075 (3.2792)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.065 (0.089)	Data 8.63e-05 (3.44e-03)	Tok/s 80072 (82893)	Loss/tok 3.3021 (3.2674)	LR 2.000e-03
0: TRAIN [2][100/1938]	Time 0.092 (0.089)	Data 8.63e-05 (3.11e-03)	Tok/s 89814 (83616)	Loss/tok 3.1743 (3.2711)	LR 2.000e-03
0: TRAIN [2][110/1938]	Time 0.092 (0.088)	Data 8.94e-05 (2.84e-03)	Tok/s 91767 (83593)	Loss/tok 3.1173 (3.2629)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.151 (0.088)	Data 9.16e-05 (2.61e-03)	Tok/s 98502 (83816)	Loss/tok 3.6271 (3.2736)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.095 (0.089)	Data 9.04e-05 (2.42e-03)	Tok/s 88388 (84261)	Loss/tok 3.2703 (3.2848)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.091 (0.089)	Data 8.77e-05 (2.25e-03)	Tok/s 94856 (84428)	Loss/tok 3.2306 (3.2855)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.065 (0.090)	Data 1.27e-04 (2.11e-03)	Tok/s 79412 (84671)	Loss/tok 3.1906 (3.2922)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.093 (0.090)	Data 9.54e-05 (1.99e-03)	Tok/s 91181 (84953)	Loss/tok 3.2019 (3.2946)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.066 (0.089)	Data 8.75e-05 (1.88e-03)	Tok/s 76934 (84827)	Loss/tok 2.9211 (3.2915)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.068 (0.089)	Data 8.73e-05 (1.78e-03)	Tok/s 75711 (84711)	Loss/tok 3.0301 (3.2886)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.091 (0.088)	Data 8.61e-05 (1.69e-03)	Tok/s 91986 (84575)	Loss/tok 3.1665 (3.2815)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.118 (0.087)	Data 8.92e-05 (1.61e-03)	Tok/s 99665 (84524)	Loss/tok 3.3110 (3.2763)	LR 2.000e-03
0: TRAIN [2][210/1938]	Time 0.091 (0.087)	Data 8.73e-05 (1.54e-03)	Tok/s 92473 (84338)	Loss/tok 3.1524 (3.2713)	LR 2.000e-03
0: TRAIN [2][220/1938]	Time 0.092 (0.087)	Data 9.18e-05 (1.47e-03)	Tok/s 89853 (84416)	Loss/tok 3.1813 (3.2665)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.120 (0.086)	Data 1.25e-04 (1.41e-03)	Tok/s 97936 (84344)	Loss/tok 3.3385 (3.2659)	LR 2.000e-03
0: TRAIN [2][240/1938]	Time 0.065 (0.087)	Data 8.96e-05 (1.36e-03)	Tok/s 80176 (84449)	Loss/tok 3.0974 (3.2711)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.093 (0.086)	Data 9.63e-05 (1.31e-03)	Tok/s 89447 (84413)	Loss/tok 3.2161 (3.2662)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.093 (0.086)	Data 8.99e-05 (1.26e-03)	Tok/s 90576 (84456)	Loss/tok 3.2033 (3.2697)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][270/1938]	Time 0.120 (0.086)	Data 8.85e-05 (1.22e-03)	Tok/s 97196 (84555)	Loss/tok 3.3285 (3.2717)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.151 (0.087)	Data 9.01e-05 (1.18e-03)	Tok/s 99627 (84499)	Loss/tok 3.4581 (3.2737)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.116 (0.087)	Data 8.44e-05 (1.14e-03)	Tok/s 101491 (84601)	Loss/tok 3.3629 (3.2763)	LR 2.000e-03
0: TRAIN [2][300/1938]	Time 0.092 (0.087)	Data 9.23e-05 (1.11e-03)	Tok/s 89289 (84752)	Loss/tok 3.0959 (3.2763)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.091 (0.087)	Data 8.85e-05 (1.07e-03)	Tok/s 90488 (84786)	Loss/tok 3.3271 (3.2780)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.066 (0.087)	Data 9.35e-05 (1.04e-03)	Tok/s 78968 (84754)	Loss/tok 3.0465 (3.2733)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.067 (0.087)	Data 8.94e-05 (1.01e-03)	Tok/s 73997 (84744)	Loss/tok 2.9749 (3.2753)	LR 2.000e-03
0: TRAIN [2][340/1938]	Time 0.065 (0.086)	Data 8.80e-05 (9.86e-04)	Tok/s 77737 (84726)	Loss/tok 3.1139 (3.2745)	LR 2.000e-03
0: TRAIN [2][350/1938]	Time 0.043 (0.086)	Data 8.87e-05 (9.61e-04)	Tok/s 58222 (84606)	Loss/tok 2.5008 (3.2707)	LR 2.000e-03
0: TRAIN [2][360/1938]	Time 0.067 (0.086)	Data 9.08e-05 (9.37e-04)	Tok/s 76593 (84721)	Loss/tok 3.1455 (3.2716)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.065 (0.086)	Data 9.11e-05 (9.14e-04)	Tok/s 80944 (84802)	Loss/tok 2.7814 (3.2708)	LR 2.000e-03
0: TRAIN [2][380/1938]	Time 0.118 (0.086)	Data 1.04e-04 (8.93e-04)	Tok/s 97588 (84839)	Loss/tok 3.5915 (3.2741)	LR 2.000e-03
0: TRAIN [2][390/1938]	Time 0.043 (0.086)	Data 9.87e-05 (8.72e-04)	Tok/s 60999 (84658)	Loss/tok 2.7094 (3.2725)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.067 (0.086)	Data 9.06e-05 (8.53e-04)	Tok/s 75764 (84784)	Loss/tok 3.2655 (3.2736)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.116 (0.086)	Data 1.04e-04 (8.34e-04)	Tok/s 100134 (84674)	Loss/tok 3.4414 (3.2739)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.066 (0.086)	Data 9.73e-05 (8.17e-04)	Tok/s 75790 (84539)	Loss/tok 3.1486 (3.2723)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.090 (0.085)	Data 8.96e-05 (8.00e-04)	Tok/s 92611 (84355)	Loss/tok 3.1840 (3.2693)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.067 (0.085)	Data 9.06e-05 (7.84e-04)	Tok/s 74073 (84376)	Loss/tok 2.9692 (3.2708)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][450/1938]	Time 0.094 (0.085)	Data 8.85e-05 (7.68e-04)	Tok/s 90241 (84384)	Loss/tok 3.3212 (3.2741)	LR 2.000e-03
0: TRAIN [2][460/1938]	Time 0.093 (0.085)	Data 8.51e-05 (7.54e-04)	Tok/s 92201 (84387)	Loss/tok 3.3084 (3.2723)	LR 2.000e-03
0: TRAIN [2][470/1938]	Time 0.117 (0.085)	Data 8.70e-05 (7.40e-04)	Tok/s 100595 (84241)	Loss/tok 3.5137 (3.2708)	LR 2.000e-03
0: TRAIN [2][480/1938]	Time 0.092 (0.085)	Data 9.68e-05 (7.26e-04)	Tok/s 92750 (84229)	Loss/tok 3.3510 (3.2689)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.067 (0.084)	Data 9.08e-05 (7.13e-04)	Tok/s 76860 (84203)	Loss/tok 3.0098 (3.2676)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.066 (0.085)	Data 8.96e-05 (7.01e-04)	Tok/s 78484 (84254)	Loss/tok 3.0236 (3.2696)	LR 2.000e-03
0: TRAIN [2][510/1938]	Time 0.092 (0.085)	Data 8.68e-05 (6.89e-04)	Tok/s 90828 (84365)	Loss/tok 3.1781 (3.2695)	LR 2.000e-03
0: TRAIN [2][520/1938]	Time 0.066 (0.085)	Data 1.11e-04 (6.78e-04)	Tok/s 81635 (84300)	Loss/tok 3.0391 (3.2680)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.092 (0.084)	Data 8.94e-05 (6.67e-04)	Tok/s 91775 (84272)	Loss/tok 3.4347 (3.2664)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.151 (0.085)	Data 8.58e-05 (6.56e-04)	Tok/s 96775 (84280)	Loss/tok 3.6574 (3.2683)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.043 (0.084)	Data 8.73e-05 (6.46e-04)	Tok/s 57044 (84229)	Loss/tok 2.5538 (3.2683)	LR 2.000e-03
0: TRAIN [2][560/1938]	Time 0.066 (0.084)	Data 1.24e-04 (6.36e-04)	Tok/s 78794 (84165)	Loss/tok 2.9676 (3.2662)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.066 (0.084)	Data 8.39e-05 (6.26e-04)	Tok/s 78523 (84144)	Loss/tok 3.0319 (3.2658)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.066 (0.084)	Data 9.16e-05 (6.17e-04)	Tok/s 79348 (84187)	Loss/tok 2.8963 (3.2646)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.150 (0.084)	Data 8.68e-05 (6.08e-04)	Tok/s 99060 (84085)	Loss/tok 3.5877 (3.2634)	LR 2.000e-03
0: TRAIN [2][600/1938]	Time 0.066 (0.084)	Data 8.63e-05 (6.00e-04)	Tok/s 76522 (84134)	Loss/tok 2.9103 (3.2631)	LR 2.000e-03
0: TRAIN [2][610/1938]	Time 0.092 (0.084)	Data 8.82e-05 (5.91e-04)	Tok/s 90539 (84209)	Loss/tok 3.3343 (3.2625)	LR 2.000e-03
0: TRAIN [2][620/1938]	Time 0.093 (0.084)	Data 8.75e-05 (5.83e-04)	Tok/s 91310 (84272)	Loss/tok 3.2101 (3.2631)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.119 (0.084)	Data 8.80e-05 (5.76e-04)	Tok/s 98007 (84291)	Loss/tok 3.3695 (3.2627)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.117 (0.084)	Data 1.24e-04 (5.68e-04)	Tok/s 100142 (84279)	Loss/tok 3.5761 (3.2619)	LR 2.000e-03
0: TRAIN [2][650/1938]	Time 0.067 (0.084)	Data 8.73e-05 (5.61e-04)	Tok/s 76354 (84298)	Loss/tok 3.1252 (3.2617)	LR 2.000e-03
0: TRAIN [2][660/1938]	Time 0.092 (0.084)	Data 8.42e-05 (5.54e-04)	Tok/s 91251 (84269)	Loss/tok 3.2390 (3.2608)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.065 (0.084)	Data 9.58e-05 (5.47e-04)	Tok/s 79286 (84212)	Loss/tok 3.0596 (3.2594)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.045 (0.084)	Data 8.68e-05 (5.40e-04)	Tok/s 59445 (84266)	Loss/tok 2.8268 (3.2597)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.092 (0.084)	Data 8.42e-05 (5.34e-04)	Tok/s 91452 (84234)	Loss/tok 3.2596 (3.2596)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.095 (0.084)	Data 8.96e-05 (5.27e-04)	Tok/s 87363 (84260)	Loss/tok 3.4417 (3.2603)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][710/1938]	Time 0.066 (0.084)	Data 9.56e-05 (5.23e-04)	Tok/s 76263 (84227)	Loss/tok 3.1947 (3.2596)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.091 (0.084)	Data 8.56e-05 (5.17e-04)	Tok/s 92035 (84195)	Loss/tok 3.1556 (3.2582)	LR 2.000e-03
0: TRAIN [2][730/1938]	Time 0.153 (0.084)	Data 8.85e-05 (5.11e-04)	Tok/s 99184 (84255)	Loss/tok 3.5048 (3.2603)	LR 2.000e-03
0: TRAIN [2][740/1938]	Time 0.091 (0.084)	Data 8.99e-05 (5.05e-04)	Tok/s 91310 (84329)	Loss/tok 3.2287 (3.2601)	LR 2.000e-03
0: TRAIN [2][750/1938]	Time 0.066 (0.084)	Data 9.30e-05 (5.00e-04)	Tok/s 78112 (84258)	Loss/tok 3.0977 (3.2588)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.043 (0.084)	Data 8.56e-05 (4.95e-04)	Tok/s 61653 (84255)	Loss/tok 2.5912 (3.2582)	LR 2.000e-03
0: TRAIN [2][770/1938]	Time 0.067 (0.084)	Data 8.87e-05 (4.89e-04)	Tok/s 77641 (84261)	Loss/tok 3.0583 (3.2583)	LR 2.000e-03
0: TRAIN [2][780/1938]	Time 0.068 (0.084)	Data 8.65e-05 (4.84e-04)	Tok/s 78345 (84246)	Loss/tok 2.9557 (3.2571)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.117 (0.084)	Data 8.58e-05 (4.79e-04)	Tok/s 99482 (84260)	Loss/tok 3.4319 (3.2566)	LR 2.000e-03
0: TRAIN [2][800/1938]	Time 0.150 (0.084)	Data 8.70e-05 (4.74e-04)	Tok/s 98482 (84231)	Loss/tok 3.7983 (3.2564)	LR 2.000e-03
0: TRAIN [2][810/1938]	Time 0.067 (0.084)	Data 9.04e-05 (4.70e-04)	Tok/s 78425 (84220)	Loss/tok 3.0936 (3.2567)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.092 (0.084)	Data 8.63e-05 (4.65e-04)	Tok/s 91955 (84250)	Loss/tok 3.2242 (3.2567)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.043 (0.083)	Data 8.87e-05 (4.61e-04)	Tok/s 62547 (84195)	Loss/tok 2.6071 (3.2557)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][840/1938]	Time 0.066 (0.084)	Data 9.30e-05 (4.56e-04)	Tok/s 77690 (84220)	Loss/tok 2.9743 (3.2573)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.067 (0.084)	Data 9.01e-05 (4.52e-04)	Tok/s 75403 (84236)	Loss/tok 3.0161 (3.2565)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.065 (0.083)	Data 9.23e-05 (4.48e-04)	Tok/s 79902 (84249)	Loss/tok 3.1018 (3.2567)	LR 2.000e-03
0: TRAIN [2][870/1938]	Time 0.119 (0.084)	Data 1.32e-04 (4.44e-04)	Tok/s 99221 (84274)	Loss/tok 3.2208 (3.2570)	LR 2.000e-03
0: TRAIN [2][880/1938]	Time 0.117 (0.083)	Data 8.54e-05 (4.40e-04)	Tok/s 99972 (84205)	Loss/tok 3.5087 (3.2561)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][890/1938]	Time 0.153 (0.083)	Data 9.23e-05 (4.36e-04)	Tok/s 97278 (84173)	Loss/tok 3.6789 (3.2579)	LR 2.000e-03
0: TRAIN [2][900/1938]	Time 0.091 (0.083)	Data 8.49e-05 (4.32e-04)	Tok/s 91620 (84177)	Loss/tok 3.4181 (3.2589)	LR 2.000e-03
0: TRAIN [2][910/1938]	Time 0.066 (0.083)	Data 8.92e-05 (4.28e-04)	Tok/s 76648 (84166)	Loss/tok 3.0048 (3.2587)	LR 2.000e-03
0: TRAIN [2][920/1938]	Time 0.067 (0.083)	Data 8.96e-05 (4.25e-04)	Tok/s 76316 (84185)	Loss/tok 2.9483 (3.2579)	LR 2.000e-03
0: TRAIN [2][930/1938]	Time 0.118 (0.084)	Data 8.99e-05 (4.21e-04)	Tok/s 100083 (84241)	Loss/tok 3.4434 (3.2590)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.153 (0.084)	Data 8.80e-05 (4.18e-04)	Tok/s 99216 (84232)	Loss/tok 3.4759 (3.2598)	LR 2.000e-03
0: TRAIN [2][950/1938]	Time 0.065 (0.083)	Data 8.06e-05 (4.14e-04)	Tok/s 80077 (84146)	Loss/tok 2.9466 (3.2581)	LR 2.000e-03
0: TRAIN [2][960/1938]	Time 0.044 (0.084)	Data 9.06e-05 (4.11e-04)	Tok/s 60998 (84162)	Loss/tok 2.6273 (3.2581)	LR 2.000e-03
0: TRAIN [2][970/1938]	Time 0.091 (0.083)	Data 8.73e-05 (4.08e-04)	Tok/s 90841 (84153)	Loss/tok 3.5165 (3.2578)	LR 2.000e-03
0: TRAIN [2][980/1938]	Time 0.151 (0.084)	Data 9.32e-05 (4.04e-04)	Tok/s 98171 (84192)	Loss/tok 3.7685 (3.2589)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.066 (0.083)	Data 8.96e-05 (4.01e-04)	Tok/s 80066 (84149)	Loss/tok 3.0412 (3.2578)	LR 2.000e-03
0: TRAIN [2][1000/1938]	Time 0.065 (0.083)	Data 9.04e-05 (3.98e-04)	Tok/s 77039 (84118)	Loss/tok 2.9276 (3.2577)	LR 2.000e-03
0: TRAIN [2][1010/1938]	Time 0.070 (0.083)	Data 9.25e-05 (3.95e-04)	Tok/s 74187 (84158)	Loss/tok 2.9486 (3.2578)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][1020/1938]	Time 0.065 (0.084)	Data 9.49e-05 (3.92e-04)	Tok/s 78670 (84213)	Loss/tok 3.0590 (3.2590)	LR 2.000e-03
0: TRAIN [2][1030/1938]	Time 0.067 (0.083)	Data 9.23e-05 (3.89e-04)	Tok/s 75748 (84192)	Loss/tok 3.0114 (3.2581)	LR 2.000e-03
0: TRAIN [2][1040/1938]	Time 0.092 (0.084)	Data 1.27e-04 (3.86e-04)	Tok/s 94427 (84208)	Loss/tok 3.2760 (3.2584)	LR 2.000e-03
0: TRAIN [2][1050/1938]	Time 0.092 (0.084)	Data 9.66e-05 (3.84e-04)	Tok/s 91014 (84240)	Loss/tok 3.1769 (3.2585)	LR 2.000e-03
0: TRAIN [2][1060/1938]	Time 0.067 (0.084)	Data 9.25e-05 (3.81e-04)	Tok/s 75063 (84270)	Loss/tok 3.0509 (3.2594)	LR 2.000e-03
0: TRAIN [2][1070/1938]	Time 0.091 (0.084)	Data 9.32e-05 (3.78e-04)	Tok/s 91619 (84292)	Loss/tok 3.3096 (3.2601)	LR 2.000e-03
0: TRAIN [2][1080/1938]	Time 0.065 (0.084)	Data 9.73e-05 (3.75e-04)	Tok/s 82350 (84274)	Loss/tok 3.1939 (3.2594)	LR 2.000e-03
0: TRAIN [2][1090/1938]	Time 0.092 (0.084)	Data 8.61e-05 (3.73e-04)	Tok/s 91306 (84324)	Loss/tok 3.2453 (3.2599)	LR 2.000e-03
0: TRAIN [2][1100/1938]	Time 0.065 (0.084)	Data 1.21e-04 (3.70e-04)	Tok/s 79329 (84301)	Loss/tok 3.2325 (3.2602)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.066 (0.084)	Data 8.87e-05 (3.68e-04)	Tok/s 78094 (84313)	Loss/tok 2.9396 (3.2596)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.117 (0.084)	Data 1.29e-04 (3.65e-04)	Tok/s 100498 (84342)	Loss/tok 3.4326 (3.2590)	LR 2.000e-03
0: TRAIN [2][1130/1938]	Time 0.065 (0.084)	Data 8.63e-05 (3.63e-04)	Tok/s 78772 (84318)	Loss/tok 3.1430 (3.2581)	LR 2.000e-03
0: TRAIN [2][1140/1938]	Time 0.118 (0.084)	Data 1.12e-04 (3.61e-04)	Tok/s 98456 (84357)	Loss/tok 3.4443 (3.2578)	LR 2.000e-03
0: TRAIN [2][1150/1938]	Time 0.067 (0.084)	Data 8.92e-05 (3.58e-04)	Tok/s 76285 (84357)	Loss/tok 3.1079 (3.2579)	LR 2.000e-03
0: TRAIN [2][1160/1938]	Time 0.091 (0.084)	Data 8.68e-05 (3.56e-04)	Tok/s 92396 (84339)	Loss/tok 3.2011 (3.2577)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.065 (0.083)	Data 8.39e-05 (3.54e-04)	Tok/s 77909 (84258)	Loss/tok 3.0212 (3.2568)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][1180/1938]	Time 0.092 (0.083)	Data 9.49e-05 (3.51e-04)	Tok/s 90060 (84259)	Loss/tok 3.3952 (3.2567)	LR 2.000e-03
0: TRAIN [2][1190/1938]	Time 0.066 (0.083)	Data 9.35e-05 (3.49e-04)	Tok/s 76640 (84237)	Loss/tok 3.0216 (3.2559)	LR 2.000e-03
0: TRAIN [2][1200/1938]	Time 0.094 (0.083)	Data 1.27e-04 (3.47e-04)	Tok/s 89597 (84255)	Loss/tok 3.2584 (3.2560)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.092 (0.083)	Data 9.01e-05 (3.45e-04)	Tok/s 91688 (84246)	Loss/tok 3.1985 (3.2557)	LR 2.000e-03
0: TRAIN [2][1220/1938]	Time 0.091 (0.083)	Data 8.82e-05 (3.43e-04)	Tok/s 91765 (84241)	Loss/tok 3.1698 (3.2553)	LR 2.000e-03
0: TRAIN [2][1230/1938]	Time 0.066 (0.083)	Data 8.68e-05 (3.41e-04)	Tok/s 77478 (84230)	Loss/tok 3.0685 (3.2553)	LR 2.000e-03
0: TRAIN [2][1240/1938]	Time 0.044 (0.083)	Data 9.97e-05 (3.39e-04)	Tok/s 60745 (84203)	Loss/tok 2.8316 (3.2562)	LR 2.000e-03
0: TRAIN [2][1250/1938]	Time 0.043 (0.083)	Data 1.02e-04 (3.37e-04)	Tok/s 62847 (84182)	Loss/tok 2.7310 (3.2559)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [2][1260/1938]	Time 0.092 (0.083)	Data 8.23e-05 (3.35e-04)	Tok/s 91301 (84183)	Loss/tok 3.2913 (3.2558)	LR 2.000e-03
0: TRAIN [2][1270/1938]	Time 0.093 (0.083)	Data 9.16e-05 (3.33e-04)	Tok/s 88658 (84202)	Loss/tok 3.3350 (3.2558)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.092 (0.083)	Data 8.82e-05 (3.31e-04)	Tok/s 92207 (84211)	Loss/tok 3.2507 (3.2552)	LR 2.000e-03
0: TRAIN [2][1290/1938]	Time 0.043 (0.083)	Data 9.08e-05 (3.29e-04)	Tok/s 61957 (84155)	Loss/tok 2.6369 (3.2541)	LR 2.000e-03
0: TRAIN [2][1300/1938]	Time 0.118 (0.083)	Data 9.35e-05 (3.28e-04)	Tok/s 100175 (84189)	Loss/tok 3.4272 (3.2548)	LR 2.000e-03
0: TRAIN [2][1310/1938]	Time 0.095 (0.083)	Data 8.65e-05 (3.26e-04)	Tok/s 86417 (84188)	Loss/tok 3.0997 (3.2545)	LR 2.000e-03
0: TRAIN [2][1320/1938]	Time 0.092 (0.083)	Data 8.54e-05 (3.24e-04)	Tok/s 92744 (84193)	Loss/tok 3.3511 (3.2541)	LR 2.000e-03
0: TRAIN [2][1330/1938]	Time 0.150 (0.083)	Data 8.73e-05 (3.22e-04)	Tok/s 97378 (84234)	Loss/tok 3.6588 (3.2546)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.042 (0.083)	Data 8.34e-05 (3.21e-04)	Tok/s 62039 (84173)	Loss/tok 2.5949 (3.2536)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.068 (0.083)	Data 9.06e-05 (3.19e-04)	Tok/s 75062 (84203)	Loss/tok 3.1048 (3.2540)	LR 2.000e-03
0: TRAIN [2][1360/1938]	Time 0.067 (0.083)	Data 8.56e-05 (3.17e-04)	Tok/s 78097 (84136)	Loss/tok 2.9763 (3.2532)	LR 2.000e-03
0: TRAIN [2][1370/1938]	Time 0.066 (0.083)	Data 8.70e-05 (3.16e-04)	Tok/s 78378 (84151)	Loss/tok 2.8786 (3.2531)	LR 2.000e-03
0: TRAIN [2][1380/1938]	Time 0.066 (0.083)	Data 1.14e-04 (3.14e-04)	Tok/s 80999 (84156)	Loss/tok 3.1551 (3.2523)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.043 (0.083)	Data 8.68e-05 (3.12e-04)	Tok/s 62445 (84095)	Loss/tok 2.7192 (3.2512)	LR 2.000e-03
0: TRAIN [2][1400/1938]	Time 0.095 (0.083)	Data 8.82e-05 (3.11e-04)	Tok/s 88456 (84127)	Loss/tok 3.1131 (3.2518)	LR 2.000e-03
0: TRAIN [2][1410/1938]	Time 0.066 (0.083)	Data 1.02e-04 (3.09e-04)	Tok/s 76650 (84140)	Loss/tok 3.2112 (3.2520)	LR 2.000e-03
0: TRAIN [2][1420/1938]	Time 0.064 (0.083)	Data 1.25e-04 (3.08e-04)	Tok/s 78879 (84121)	Loss/tok 2.9727 (3.2514)	LR 2.000e-03
0: TRAIN [2][1430/1938]	Time 0.065 (0.083)	Data 8.82e-05 (3.06e-04)	Tok/s 79725 (84143)	Loss/tok 3.0986 (3.2521)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.064 (0.083)	Data 8.82e-05 (3.05e-04)	Tok/s 79667 (84146)	Loss/tok 3.0335 (3.2525)	LR 2.000e-03
0: TRAIN [2][1450/1938]	Time 0.066 (0.083)	Data 9.58e-05 (3.03e-04)	Tok/s 77535 (84129)	Loss/tok 2.9414 (3.2517)	LR 2.000e-03
0: TRAIN [2][1460/1938]	Time 0.117 (0.083)	Data 8.94e-05 (3.02e-04)	Tok/s 100353 (84145)	Loss/tok 3.4636 (3.2521)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.066 (0.083)	Data 8.68e-05 (3.00e-04)	Tok/s 80080 (84171)	Loss/tok 3.1897 (3.2531)	LR 2.000e-03
0: TRAIN [2][1480/1938]	Time 0.064 (0.083)	Data 8.42e-05 (2.99e-04)	Tok/s 80788 (84137)	Loss/tok 3.0542 (3.2523)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.068 (0.083)	Data 9.11e-05 (2.98e-04)	Tok/s 76563 (84156)	Loss/tok 3.2191 (3.2525)	LR 2.000e-03
0: TRAIN [2][1500/1938]	Time 0.153 (0.083)	Data 9.25e-05 (2.96e-04)	Tok/s 97671 (84171)	Loss/tok 3.5575 (3.2537)	LR 2.000e-03
0: TRAIN [2][1510/1938]	Time 0.065 (0.083)	Data 9.06e-05 (2.95e-04)	Tok/s 80559 (84148)	Loss/tok 3.0398 (3.2534)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.119 (0.083)	Data 8.58e-05 (2.95e-04)	Tok/s 97891 (84183)	Loss/tok 3.4483 (3.2535)	LR 2.000e-03
0: TRAIN [2][1530/1938]	Time 0.067 (0.083)	Data 1.24e-04 (2.93e-04)	Tok/s 77413 (84208)	Loss/tok 2.9376 (3.2536)	LR 2.000e-03
0: TRAIN [2][1540/1938]	Time 0.068 (0.083)	Data 8.77e-05 (2.92e-04)	Tok/s 77222 (84192)	Loss/tok 3.0683 (3.2533)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.067 (0.083)	Data 8.85e-05 (2.91e-04)	Tok/s 74494 (84190)	Loss/tok 2.9415 (3.2537)	LR 2.000e-03
0: TRAIN [2][1560/1938]	Time 0.094 (0.084)	Data 9.27e-05 (2.89e-04)	Tok/s 89345 (84231)	Loss/tok 3.0949 (3.2549)	LR 2.000e-03
0: TRAIN [2][1570/1938]	Time 0.119 (0.083)	Data 9.11e-05 (2.88e-04)	Tok/s 97701 (84233)	Loss/tok 3.4922 (3.2549)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.093 (0.084)	Data 8.82e-05 (2.87e-04)	Tok/s 89588 (84245)	Loss/tok 3.3257 (3.2552)	LR 2.000e-03
0: TRAIN [2][1590/1938]	Time 0.092 (0.083)	Data 1.22e-04 (2.86e-04)	Tok/s 88625 (84234)	Loss/tok 3.3718 (3.2550)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.151 (0.083)	Data 8.75e-05 (2.84e-04)	Tok/s 98600 (84223)	Loss/tok 3.6217 (3.2552)	LR 2.000e-03
0: TRAIN [2][1610/1938]	Time 0.092 (0.084)	Data 8.65e-05 (2.83e-04)	Tok/s 89527 (84239)	Loss/tok 3.1795 (3.2557)	LR 2.000e-03
0: TRAIN [2][1620/1938]	Time 0.066 (0.083)	Data 8.73e-05 (2.82e-04)	Tok/s 77605 (84224)	Loss/tok 3.1649 (3.2552)	LR 2.000e-03
0: TRAIN [2][1630/1938]	Time 0.091 (0.083)	Data 8.89e-05 (2.81e-04)	Tok/s 91767 (84241)	Loss/tok 3.2074 (3.2548)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1640/1938]	Time 0.149 (0.083)	Data 8.94e-05 (2.80e-04)	Tok/s 99992 (84247)	Loss/tok 3.6501 (3.2546)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][1650/1938]	Time 0.118 (0.084)	Data 8.87e-05 (2.79e-04)	Tok/s 99671 (84249)	Loss/tok 3.3830 (3.2549)	LR 2.000e-03
0: TRAIN [2][1660/1938]	Time 0.092 (0.084)	Data 8.56e-05 (2.77e-04)	Tok/s 90259 (84291)	Loss/tok 3.2725 (3.2557)	LR 2.000e-03
0: TRAIN [2][1670/1938]	Time 0.068 (0.084)	Data 8.94e-05 (2.76e-04)	Tok/s 76815 (84296)	Loss/tok 3.0465 (3.2554)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.042 (0.084)	Data 8.80e-05 (2.75e-04)	Tok/s 63790 (84267)	Loss/tok 2.6823 (3.2551)	LR 2.000e-03
0: TRAIN [2][1690/1938]	Time 0.043 (0.083)	Data 8.80e-05 (2.74e-04)	Tok/s 63690 (84260)	Loss/tok 2.6021 (3.2545)	LR 2.000e-03
0: TRAIN [2][1700/1938]	Time 0.065 (0.083)	Data 9.18e-05 (2.73e-04)	Tok/s 77106 (84257)	Loss/tok 3.0705 (3.2542)	LR 2.000e-03
0: TRAIN [2][1710/1938]	Time 0.092 (0.084)	Data 8.70e-05 (2.72e-04)	Tok/s 89621 (84264)	Loss/tok 3.3579 (3.2541)	LR 2.000e-03
0: TRAIN [2][1720/1938]	Time 0.090 (0.084)	Data 8.89e-05 (2.71e-04)	Tok/s 93334 (84261)	Loss/tok 3.2224 (3.2545)	LR 2.000e-03
0: TRAIN [2][1730/1938]	Time 0.066 (0.083)	Data 8.89e-05 (2.70e-04)	Tok/s 77935 (84242)	Loss/tok 3.1725 (3.2539)	LR 2.000e-03
0: TRAIN [2][1740/1938]	Time 0.067 (0.084)	Data 8.85e-05 (2.69e-04)	Tok/s 75269 (84267)	Loss/tok 2.9491 (3.2548)	LR 2.000e-03
0: TRAIN [2][1750/1938]	Time 0.044 (0.084)	Data 9.42e-05 (2.68e-04)	Tok/s 59816 (84269)	Loss/tok 2.6726 (3.2552)	LR 2.000e-03
0: TRAIN [2][1760/1938]	Time 0.064 (0.084)	Data 9.08e-05 (2.67e-04)	Tok/s 80525 (84263)	Loss/tok 2.9983 (3.2548)	LR 2.000e-03
0: TRAIN [2][1770/1938]	Time 0.120 (0.083)	Data 9.23e-05 (2.66e-04)	Tok/s 95364 (84245)	Loss/tok 3.6385 (3.2546)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.119 (0.084)	Data 8.92e-05 (2.65e-04)	Tok/s 96321 (84267)	Loss/tok 3.5298 (3.2549)	LR 2.000e-03
0: TRAIN [2][1790/1938]	Time 0.066 (0.083)	Data 1.22e-04 (2.64e-04)	Tok/s 79846 (84251)	Loss/tok 3.3603 (3.2543)	LR 2.000e-03
0: TRAIN [2][1800/1938]	Time 0.067 (0.083)	Data 9.23e-05 (2.63e-04)	Tok/s 76367 (84252)	Loss/tok 2.9739 (3.2543)	LR 2.000e-03
0: TRAIN [2][1810/1938]	Time 0.094 (0.083)	Data 8.85e-05 (2.62e-04)	Tok/s 89253 (84252)	Loss/tok 3.1779 (3.2541)	LR 2.000e-03
0: TRAIN [2][1820/1938]	Time 0.149 (0.083)	Data 8.94e-05 (2.61e-04)	Tok/s 99688 (84264)	Loss/tok 3.6928 (3.2546)	LR 2.000e-03
0: TRAIN [2][1830/1938]	Time 0.095 (0.083)	Data 8.87e-05 (2.60e-04)	Tok/s 89393 (84269)	Loss/tok 3.3418 (3.2547)	LR 2.000e-03
0: TRAIN [2][1840/1938]	Time 0.066 (0.084)	Data 9.06e-05 (2.60e-04)	Tok/s 78421 (84275)	Loss/tok 2.9868 (3.2549)	LR 2.000e-03
0: TRAIN [2][1850/1938]	Time 0.152 (0.084)	Data 8.77e-05 (2.59e-04)	Tok/s 98358 (84290)	Loss/tok 3.4956 (3.2554)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.094 (0.084)	Data 9.32e-05 (2.58e-04)	Tok/s 90450 (84306)	Loss/tok 3.2337 (3.2556)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][1870/1938]	Time 0.093 (0.084)	Data 8.75e-05 (2.57e-04)	Tok/s 91017 (84312)	Loss/tok 3.1836 (3.2555)	LR 2.000e-03
0: TRAIN [2][1880/1938]	Time 0.066 (0.084)	Data 8.65e-05 (2.56e-04)	Tok/s 79129 (84300)	Loss/tok 3.0589 (3.2550)	LR 2.000e-03
0: TRAIN [2][1890/1938]	Time 0.066 (0.084)	Data 8.94e-05 (2.56e-04)	Tok/s 76232 (84302)	Loss/tok 2.9673 (3.2546)	LR 2.000e-03
0: TRAIN [2][1900/1938]	Time 0.043 (0.084)	Data 8.65e-05 (2.56e-04)	Tok/s 62411 (84304)	Loss/tok 2.5859 (3.2543)	LR 2.000e-03
0: TRAIN [2][1910/1938]	Time 0.066 (0.084)	Data 8.87e-05 (2.55e-04)	Tok/s 75248 (84293)	Loss/tok 2.9751 (3.2537)	LR 2.000e-03
0: TRAIN [2][1920/1938]	Time 0.066 (0.084)	Data 8.80e-05 (2.54e-04)	Tok/s 76673 (84307)	Loss/tok 2.9880 (3.2536)	LR 2.000e-03
0: TRAIN [2][1930/1938]	Time 0.066 (0.084)	Data 9.23e-05 (2.53e-04)	Tok/s 76266 (84315)	Loss/tok 2.9782 (3.2545)	LR 2.000e-03
:::MLL 1560905249.412 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1560905249.413 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.522 (0.522)	Decoder iters 149.0 (149.0)	Tok/s 16105 (16105)
0: Running moses detokenizer
0: BLEU(score=22.636443641926068, counts=[35977, 17511, 9749, 5661], totals=[64921, 61918, 58915, 55915], precisions=[55.41658323192804, 28.28095222713912, 16.5475685309344, 10.124295806134311], bp=1.0, sys_len=64921, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1560905250.674 eval_accuracy: {"value": 22.64, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1560905250.674 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.2573	Test BLEU: 22.64
0: Performance: Epoch: 2	Training: 1349356 Tok/s
0: Finished epoch 2
:::MLL 1560905250.675 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1560905250.675 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1560905250.675 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 2245972799
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [3][0/1938]	Time 0.394 (0.394)	Data 2.96e-01 (2.96e-01)	Tok/s 21456 (21456)	Loss/tok 3.3150 (3.3150)	LR 2.000e-03
0: TRAIN [3][10/1938]	Time 0.067 (0.122)	Data 9.01e-05 (2.70e-02)	Tok/s 75914 (73708)	Loss/tok 3.0902 (3.2228)	LR 2.000e-03
0: TRAIN [3][20/1938]	Time 0.150 (0.106)	Data 1.26e-04 (1.42e-02)	Tok/s 99031 (78971)	Loss/tok 3.4991 (3.2156)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.066 (0.096)	Data 8.73e-05 (9.63e-03)	Tok/s 75206 (79118)	Loss/tok 2.8430 (3.1920)	LR 2.000e-03
0: TRAIN [3][40/1938]	Time 0.066 (0.093)	Data 9.49e-05 (7.30e-03)	Tok/s 80082 (80909)	Loss/tok 2.8718 (3.1914)	LR 2.000e-03
0: TRAIN [3][50/1938]	Time 0.092 (0.092)	Data 9.54e-05 (5.89e-03)	Tok/s 93316 (82409)	Loss/tok 3.2044 (3.1971)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.065 (0.091)	Data 9.01e-05 (4.94e-03)	Tok/s 76980 (82767)	Loss/tok 3.0097 (3.1920)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.118 (0.091)	Data 8.92e-05 (4.26e-03)	Tok/s 98858 (83684)	Loss/tok 3.3003 (3.1988)	LR 2.000e-03
0: TRAIN [3][80/1938]	Time 0.091 (0.089)	Data 8.56e-05 (3.74e-03)	Tok/s 91530 (83638)	Loss/tok 3.1932 (3.1876)	LR 2.000e-03
0: TRAIN [3][90/1938]	Time 0.065 (0.088)	Data 9.58e-05 (3.34e-03)	Tok/s 79501 (83501)	Loss/tok 2.9373 (3.1761)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.065 (0.086)	Data 8.37e-05 (3.02e-03)	Tok/s 80412 (83043)	Loss/tok 3.0602 (3.1651)	LR 2.000e-03
0: TRAIN [3][110/1938]	Time 0.066 (0.086)	Data 8.85e-05 (2.76e-03)	Tok/s 79542 (83016)	Loss/tok 2.9681 (3.1763)	LR 2.000e-03
0: TRAIN [3][120/1938]	Time 0.066 (0.085)	Data 8.51e-05 (2.54e-03)	Tok/s 78378 (82969)	Loss/tok 3.2239 (3.1724)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.067 (0.085)	Data 9.30e-05 (2.35e-03)	Tok/s 77732 (82825)	Loss/tok 2.8642 (3.1727)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.065 (0.084)	Data 8.80e-05 (2.19e-03)	Tok/s 79416 (82706)	Loss/tok 2.8442 (3.1692)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.066 (0.083)	Data 8.54e-05 (2.05e-03)	Tok/s 79187 (82640)	Loss/tok 3.0956 (3.1639)	LR 2.000e-03
0: TRAIN [3][160/1938]	Time 0.044 (0.084)	Data 8.75e-05 (1.93e-03)	Tok/s 60749 (82817)	Loss/tok 2.6736 (3.1721)	LR 2.000e-03
0: TRAIN [3][170/1938]	Time 0.120 (0.084)	Data 9.18e-05 (1.82e-03)	Tok/s 97195 (83056)	Loss/tok 3.1812 (3.1715)	LR 2.000e-03
0: TRAIN [3][180/1938]	Time 0.093 (0.084)	Data 9.63e-05 (1.73e-03)	Tok/s 89616 (83052)	Loss/tok 3.1077 (3.1713)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.066 (0.084)	Data 8.70e-05 (1.64e-03)	Tok/s 77216 (83068)	Loss/tok 3.0125 (3.1707)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.089 (0.084)	Data 9.47e-05 (1.56e-03)	Tok/s 93250 (83062)	Loss/tok 3.1448 (3.1667)	LR 2.000e-03
0: TRAIN [3][210/1938]	Time 0.118 (0.084)	Data 9.87e-05 (1.49e-03)	Tok/s 100156 (83218)	Loss/tok 3.2402 (3.1639)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.066 (0.083)	Data 8.58e-05 (1.43e-03)	Tok/s 80000 (83056)	Loss/tok 2.9627 (3.1606)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.044 (0.083)	Data 8.80e-05 (1.37e-03)	Tok/s 59754 (83023)	Loss/tok 2.6200 (3.1597)	LR 2.000e-03
0: TRAIN [3][240/1938]	Time 0.066 (0.084)	Data 1.25e-04 (1.32e-03)	Tok/s 77221 (83108)	Loss/tok 3.1038 (3.1650)	LR 2.000e-03
0: TRAIN [3][250/1938]	Time 0.066 (0.084)	Data 9.56e-05 (1.27e-03)	Tok/s 77314 (83082)	Loss/tok 2.9838 (3.1627)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.066 (0.084)	Data 8.85e-05 (1.23e-03)	Tok/s 78181 (83091)	Loss/tok 3.0122 (3.1646)	LR 2.000e-03
0: TRAIN [3][270/1938]	Time 0.067 (0.084)	Data 8.73e-05 (1.18e-03)	Tok/s 76332 (83187)	Loss/tok 2.9750 (3.1656)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.066 (0.084)	Data 9.30e-05 (1.14e-03)	Tok/s 79130 (83268)	Loss/tok 2.9171 (3.1665)	LR 2.000e-03
0: TRAIN [3][290/1938]	Time 0.044 (0.083)	Data 9.08e-05 (1.11e-03)	Tok/s 58030 (83153)	Loss/tok 2.5993 (3.1652)	LR 2.000e-03
0: TRAIN [3][300/1938]	Time 0.092 (0.083)	Data 8.44e-05 (1.07e-03)	Tok/s 92120 (83048)	Loss/tok 3.2056 (3.1634)	LR 2.000e-03
0: TRAIN [3][310/1938]	Time 0.067 (0.083)	Data 9.44e-05 (1.04e-03)	Tok/s 75422 (83104)	Loss/tok 2.9633 (3.1630)	LR 2.000e-03
0: TRAIN [3][320/1938]	Time 0.121 (0.083)	Data 1.26e-04 (1.01e-03)	Tok/s 97042 (83181)	Loss/tok 3.4119 (3.1674)	LR 2.000e-03
0: TRAIN [3][330/1938]	Time 0.067 (0.083)	Data 8.68e-05 (9.86e-04)	Tok/s 76351 (83200)	Loss/tok 2.8558 (3.1687)	LR 2.000e-03
0: TRAIN [3][340/1938]	Time 0.091 (0.083)	Data 8.89e-05 (9.59e-04)	Tok/s 92207 (83206)	Loss/tok 3.0931 (3.1682)	LR 2.000e-03
0: TRAIN [3][350/1938]	Time 0.065 (0.083)	Data 8.63e-05 (9.34e-04)	Tok/s 77529 (83239)	Loss/tok 3.0217 (3.1664)	LR 2.000e-03
0: TRAIN [3][360/1938]	Time 0.118 (0.084)	Data 1.01e-04 (9.11e-04)	Tok/s 98594 (83398)	Loss/tok 3.2969 (3.1680)	LR 2.000e-03
0: TRAIN [3][370/1938]	Time 0.066 (0.084)	Data 8.82e-05 (8.89e-04)	Tok/s 79223 (83535)	Loss/tok 3.0425 (3.1711)	LR 2.000e-03
0: TRAIN [3][380/1938]	Time 0.043 (0.084)	Data 9.08e-05 (8.68e-04)	Tok/s 60370 (83548)	Loss/tok 2.7024 (3.1709)	LR 2.000e-03
0: TRAIN [3][390/1938]	Time 0.093 (0.084)	Data 9.44e-05 (8.53e-04)	Tok/s 90434 (83633)	Loss/tok 3.1022 (3.1732)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][400/1938]	Time 0.089 (0.084)	Data 8.99e-05 (8.33e-04)	Tok/s 94408 (83695)	Loss/tok 3.1764 (3.1761)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][410/1938]	Time 0.069 (0.084)	Data 8.61e-05 (8.16e-04)	Tok/s 74844 (83613)	Loss/tok 2.9870 (3.1768)	LR 2.000e-03
0: TRAIN [3][420/1938]	Time 0.065 (0.084)	Data 1.37e-04 (7.98e-04)	Tok/s 78797 (83674)	Loss/tok 3.0568 (3.1775)	LR 2.000e-03
0: TRAIN [3][430/1938]	Time 0.066 (0.084)	Data 8.34e-05 (7.82e-04)	Tok/s 77947 (83636)	Loss/tok 2.8894 (3.1760)	LR 2.000e-03
0: TRAIN [3][440/1938]	Time 0.065 (0.084)	Data 8.77e-05 (7.69e-04)	Tok/s 76836 (83593)	Loss/tok 2.9931 (3.1767)	LR 2.000e-03
0: TRAIN [3][450/1938]	Time 0.066 (0.084)	Data 9.66e-05 (7.54e-04)	Tok/s 76038 (83531)	Loss/tok 3.1237 (3.1765)	LR 2.000e-03
0: TRAIN [3][460/1938]	Time 0.117 (0.084)	Data 8.56e-05 (7.43e-04)	Tok/s 97928 (83591)	Loss/tok 3.4781 (3.1772)	LR 2.000e-03
0: TRAIN [3][470/1938]	Time 0.066 (0.084)	Data 8.87e-05 (7.29e-04)	Tok/s 77527 (83660)	Loss/tok 2.8546 (3.1761)	LR 2.000e-03
0: TRAIN [3][480/1938]	Time 0.118 (0.084)	Data 9.23e-05 (7.16e-04)	Tok/s 99912 (83727)	Loss/tok 3.3889 (3.1783)	LR 2.000e-03
0: TRAIN [3][490/1938]	Time 0.041 (0.084)	Data 8.89e-05 (7.03e-04)	Tok/s 62752 (83720)	Loss/tok 2.4928 (3.1789)	LR 2.000e-03
0: TRAIN [3][500/1938]	Time 0.116 (0.084)	Data 9.20e-05 (6.91e-04)	Tok/s 99920 (83668)	Loss/tok 3.4207 (3.1779)	LR 2.000e-03
0: TRAIN [3][510/1938]	Time 0.092 (0.084)	Data 9.20e-05 (6.79e-04)	Tok/s 92695 (83774)	Loss/tok 3.2823 (3.1778)	LR 2.000e-03
0: TRAIN [3][520/1938]	Time 0.066 (0.084)	Data 8.96e-05 (6.68e-04)	Tok/s 80203 (83846)	Loss/tok 2.9435 (3.1802)	LR 2.000e-03
0: TRAIN [3][530/1938]	Time 0.044 (0.084)	Data 9.16e-05 (6.57e-04)	Tok/s 60865 (83826)	Loss/tok 2.5779 (3.1802)	LR 2.000e-03
0: TRAIN [3][540/1938]	Time 0.067 (0.084)	Data 9.04e-05 (6.46e-04)	Tok/s 78449 (83924)	Loss/tok 3.1012 (3.1851)	LR 2.000e-03
0: TRAIN [3][550/1938]	Time 0.120 (0.084)	Data 9.30e-05 (6.36e-04)	Tok/s 97734 (83977)	Loss/tok 3.4364 (3.1845)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][560/1938]	Time 0.119 (0.085)	Data 8.92e-05 (6.27e-04)	Tok/s 96522 (84058)	Loss/tok 3.4228 (3.1864)	LR 2.000e-03
0: TRAIN [3][570/1938]	Time 0.117 (0.085)	Data 8.77e-05 (6.17e-04)	Tok/s 99171 (84124)	Loss/tok 3.3775 (3.1860)	LR 2.000e-03
0: TRAIN [3][580/1938]	Time 0.092 (0.085)	Data 8.99e-05 (6.08e-04)	Tok/s 91800 (84150)	Loss/tok 3.1198 (3.1876)	LR 2.000e-03
0: TRAIN [3][590/1938]	Time 0.092 (0.085)	Data 9.18e-05 (6.00e-04)	Tok/s 92232 (84117)	Loss/tok 3.1830 (3.1877)	LR 2.000e-03
0: TRAIN [3][600/1938]	Time 0.065 (0.085)	Data 8.70e-05 (5.91e-04)	Tok/s 79244 (84097)	Loss/tok 2.9813 (3.1881)	LR 2.000e-03
0: TRAIN [3][610/1938]	Time 0.095 (0.085)	Data 9.16e-05 (5.83e-04)	Tok/s 89296 (84114)	Loss/tok 3.3174 (3.1881)	LR 2.000e-03
0: TRAIN [3][620/1938]	Time 0.092 (0.085)	Data 9.73e-05 (5.75e-04)	Tok/s 88930 (84138)	Loss/tok 3.1485 (3.1882)	LR 2.000e-03
0: TRAIN [3][630/1938]	Time 0.092 (0.085)	Data 8.77e-05 (5.67e-04)	Tok/s 91942 (84142)	Loss/tok 3.2233 (3.1893)	LR 2.000e-03
0: TRAIN [3][640/1938]	Time 0.120 (0.085)	Data 9.58e-05 (5.60e-04)	Tok/s 95804 (84192)	Loss/tok 3.5277 (3.1909)	LR 2.000e-03
0: TRAIN [3][650/1938]	Time 0.066 (0.085)	Data 8.37e-05 (5.53e-04)	Tok/s 76917 (84082)	Loss/tok 2.9098 (3.1913)	LR 2.000e-03
0: TRAIN [3][660/1938]	Time 0.042 (0.085)	Data 9.01e-05 (5.46e-04)	Tok/s 60324 (84009)	Loss/tok 2.5504 (3.1905)	LR 2.000e-03
0: TRAIN [3][670/1938]	Time 0.067 (0.085)	Data 8.89e-05 (5.39e-04)	Tok/s 75982 (84055)	Loss/tok 3.2084 (3.1905)	LR 2.000e-03
0: TRAIN [3][680/1938]	Time 0.092 (0.084)	Data 8.58e-05 (5.32e-04)	Tok/s 91457 (84004)	Loss/tok 3.1594 (3.1888)	LR 1.000e-03
0: TRAIN [3][690/1938]	Time 0.093 (0.084)	Data 8.82e-05 (5.26e-04)	Tok/s 91761 (83970)	Loss/tok 3.0740 (3.1876)	LR 1.000e-03
0: TRAIN [3][700/1938]	Time 0.119 (0.084)	Data 8.92e-05 (5.20e-04)	Tok/s 95792 (84041)	Loss/tok 3.3828 (3.1875)	LR 1.000e-03
0: TRAIN [3][710/1938]	Time 0.066 (0.084)	Data 9.08e-05 (5.14e-04)	Tok/s 81110 (84103)	Loss/tok 3.1341 (3.1871)	LR 1.000e-03
0: TRAIN [3][720/1938]	Time 0.152 (0.085)	Data 9.13e-05 (5.08e-04)	Tok/s 94675 (84165)	Loss/tok 3.6017 (3.1884)	LR 1.000e-03
0: TRAIN [3][730/1938]	Time 0.151 (0.085)	Data 1.05e-04 (5.02e-04)	Tok/s 97542 (84268)	Loss/tok 3.4645 (3.1920)	LR 1.000e-03
0: TRAIN [3][740/1938]	Time 0.066 (0.085)	Data 1.34e-04 (4.97e-04)	Tok/s 77207 (84212)	Loss/tok 3.0090 (3.1908)	LR 1.000e-03
0: TRAIN [3][750/1938]	Time 0.118 (0.085)	Data 8.27e-05 (4.91e-04)	Tok/s 97958 (84178)	Loss/tok 3.3026 (3.1895)	LR 1.000e-03
0: TRAIN [3][760/1938]	Time 0.043 (0.085)	Data 8.51e-05 (4.86e-04)	Tok/s 61603 (84161)	Loss/tok 2.6446 (3.1881)	LR 1.000e-03
0: TRAIN [3][770/1938]	Time 0.066 (0.085)	Data 8.87e-05 (4.83e-04)	Tok/s 78654 (84155)	Loss/tok 2.8952 (3.1886)	LR 1.000e-03
0: TRAIN [3][780/1938]	Time 0.151 (0.085)	Data 8.89e-05 (4.78e-04)	Tok/s 97221 (84150)	Loss/tok 3.4869 (3.1886)	LR 1.000e-03
0: TRAIN [3][790/1938]	Time 0.066 (0.085)	Data 8.58e-05 (4.73e-04)	Tok/s 79083 (84141)	Loss/tok 2.7099 (3.1887)	LR 1.000e-03
0: TRAIN [3][800/1938]	Time 0.092 (0.084)	Data 8.80e-05 (4.68e-04)	Tok/s 91548 (84085)	Loss/tok 3.1050 (3.1878)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][810/1938]	Time 0.152 (0.085)	Data 8.51e-05 (4.64e-04)	Tok/s 96582 (84124)	Loss/tok 3.5695 (3.1879)	LR 1.000e-03
0: TRAIN [3][820/1938]	Time 0.091 (0.084)	Data 8.20e-05 (4.59e-04)	Tok/s 94056 (84069)	Loss/tok 2.9764 (3.1861)	LR 1.000e-03
0: TRAIN [3][830/1938]	Time 0.117 (0.085)	Data 8.54e-05 (4.54e-04)	Tok/s 97784 (84123)	Loss/tok 3.3473 (3.1869)	LR 1.000e-03
0: TRAIN [3][840/1938]	Time 0.065 (0.084)	Data 8.68e-05 (4.50e-04)	Tok/s 78871 (84124)	Loss/tok 2.8662 (3.1853)	LR 1.000e-03
0: TRAIN [3][850/1938]	Time 0.152 (0.084)	Data 9.01e-05 (4.46e-04)	Tok/s 97064 (84107)	Loss/tok 3.4976 (3.1849)	LR 1.000e-03
0: TRAIN [3][860/1938]	Time 0.117 (0.085)	Data 1.29e-04 (4.42e-04)	Tok/s 101154 (84182)	Loss/tok 3.1601 (3.1859)	LR 1.000e-03
0: TRAIN [3][870/1938]	Time 0.067 (0.085)	Data 9.20e-05 (4.38e-04)	Tok/s 77878 (84182)	Loss/tok 3.0777 (3.1848)	LR 1.000e-03
0: TRAIN [3][880/1938]	Time 0.151 (0.085)	Data 8.42e-05 (4.34e-04)	Tok/s 99978 (84173)	Loss/tok 3.4648 (3.1845)	LR 1.000e-03
0: TRAIN [3][890/1938]	Time 0.067 (0.085)	Data 8.92e-05 (4.30e-04)	Tok/s 77078 (84153)	Loss/tok 2.8944 (3.1838)	LR 1.000e-03
0: TRAIN [3][900/1938]	Time 0.064 (0.084)	Data 1.22e-04 (4.26e-04)	Tok/s 81068 (84131)	Loss/tok 2.9835 (3.1832)	LR 1.000e-03
0: TRAIN [3][910/1938]	Time 0.043 (0.084)	Data 9.08e-05 (4.23e-04)	Tok/s 60475 (84087)	Loss/tok 2.5851 (3.1827)	LR 1.000e-03
0: TRAIN [3][920/1938]	Time 0.065 (0.084)	Data 8.37e-05 (4.19e-04)	Tok/s 79308 (84047)	Loss/tok 2.9944 (3.1816)	LR 1.000e-03
0: TRAIN [3][930/1938]	Time 0.091 (0.084)	Data 8.73e-05 (4.15e-04)	Tok/s 90963 (84021)	Loss/tok 3.0786 (3.1802)	LR 1.000e-03
0: TRAIN [3][940/1938]	Time 0.067 (0.084)	Data 8.82e-05 (4.12e-04)	Tok/s 76016 (84037)	Loss/tok 3.0618 (3.1792)	LR 1.000e-03
0: TRAIN [3][950/1938]	Time 0.091 (0.084)	Data 8.99e-05 (4.09e-04)	Tok/s 91907 (84025)	Loss/tok 3.1138 (3.1785)	LR 1.000e-03
0: TRAIN [3][960/1938]	Time 0.092 (0.084)	Data 8.39e-05 (4.05e-04)	Tok/s 92232 (84043)	Loss/tok 2.9972 (3.1774)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][970/1938]	Time 0.149 (0.084)	Data 9.82e-05 (4.02e-04)	Tok/s 98005 (84093)	Loss/tok 3.5514 (3.1783)	LR 1.000e-03
0: TRAIN [3][980/1938]	Time 0.067 (0.084)	Data 8.56e-05 (3.99e-04)	Tok/s 76992 (84065)	Loss/tok 2.9071 (3.1775)	LR 1.000e-03
0: TRAIN [3][990/1938]	Time 0.065 (0.084)	Data 8.94e-05 (4.00e-04)	Tok/s 80579 (84057)	Loss/tok 2.9486 (3.1772)	LR 1.000e-03
0: TRAIN [3][1000/1938]	Time 0.067 (0.084)	Data 1.35e-04 (3.97e-04)	Tok/s 76458 (84036)	Loss/tok 2.9420 (3.1765)	LR 1.000e-03
0: TRAIN [3][1010/1938]	Time 0.068 (0.084)	Data 9.13e-05 (3.94e-04)	Tok/s 77391 (84032)	Loss/tok 2.9493 (3.1762)	LR 1.000e-03
0: TRAIN [3][1020/1938]	Time 0.091 (0.084)	Data 8.70e-05 (3.91e-04)	Tok/s 91810 (84066)	Loss/tok 3.1096 (3.1756)	LR 1.000e-03
0: TRAIN [3][1030/1938]	Time 0.092 (0.084)	Data 9.56e-05 (3.88e-04)	Tok/s 91696 (84074)	Loss/tok 3.0452 (3.1746)	LR 1.000e-03
0: TRAIN [3][1040/1938]	Time 0.065 (0.084)	Data 8.68e-05 (3.85e-04)	Tok/s 81177 (84043)	Loss/tok 3.0977 (3.1734)	LR 1.000e-03
0: TRAIN [3][1050/1938]	Time 0.092 (0.084)	Data 8.75e-05 (3.82e-04)	Tok/s 89521 (83985)	Loss/tok 3.1462 (3.1721)	LR 1.000e-03
0: TRAIN [3][1060/1938]	Time 0.092 (0.084)	Data 8.82e-05 (3.79e-04)	Tok/s 89264 (83984)	Loss/tok 3.1669 (3.1725)	LR 1.000e-03
0: TRAIN [3][1070/1938]	Time 0.066 (0.084)	Data 8.58e-05 (3.77e-04)	Tok/s 76379 (84019)	Loss/tok 2.9812 (3.1730)	LR 1.000e-03
0: TRAIN [3][1080/1938]	Time 0.043 (0.084)	Data 8.63e-05 (3.74e-04)	Tok/s 59586 (84001)	Loss/tok 2.5259 (3.1727)	LR 1.000e-03
0: TRAIN [3][1090/1938]	Time 0.094 (0.084)	Data 8.61e-05 (3.71e-04)	Tok/s 88347 (84015)	Loss/tok 3.0774 (3.1724)	LR 1.000e-03
0: TRAIN [3][1100/1938]	Time 0.066 (0.084)	Data 8.80e-05 (3.69e-04)	Tok/s 78917 (83964)	Loss/tok 2.9080 (3.1715)	LR 1.000e-03
0: TRAIN [3][1110/1938]	Time 0.066 (0.084)	Data 9.06e-05 (3.66e-04)	Tok/s 79015 (83972)	Loss/tok 2.9487 (3.1705)	LR 1.000e-03
0: TRAIN [3][1120/1938]	Time 0.043 (0.083)	Data 1.23e-04 (3.64e-04)	Tok/s 63136 (83927)	Loss/tok 2.5855 (3.1698)	LR 1.000e-03
0: TRAIN [3][1130/1938]	Time 0.092 (0.084)	Data 8.87e-05 (3.61e-04)	Tok/s 92035 (83964)	Loss/tok 3.1078 (3.1700)	LR 1.000e-03
0: TRAIN [3][1140/1938]	Time 0.093 (0.084)	Data 8.70e-05 (3.59e-04)	Tok/s 88145 (84022)	Loss/tok 3.1439 (3.1701)	LR 1.000e-03
0: TRAIN [3][1150/1938]	Time 0.067 (0.084)	Data 1.22e-04 (3.57e-04)	Tok/s 77629 (83997)	Loss/tok 2.9189 (3.1694)	LR 1.000e-03
0: TRAIN [3][1160/1938]	Time 0.043 (0.084)	Data 8.63e-05 (3.54e-04)	Tok/s 62021 (83993)	Loss/tok 2.5756 (3.1696)	LR 1.000e-03
0: TRAIN [3][1170/1938]	Time 0.090 (0.084)	Data 1.19e-04 (3.52e-04)	Tok/s 93633 (84055)	Loss/tok 3.0877 (3.1703)	LR 1.000e-03
0: TRAIN [3][1180/1938]	Time 0.093 (0.084)	Data 8.99e-05 (3.50e-04)	Tok/s 90442 (84048)	Loss/tok 3.0232 (3.1698)	LR 1.000e-03
0: TRAIN [3][1190/1938]	Time 0.043 (0.084)	Data 8.96e-05 (3.48e-04)	Tok/s 62192 (84001)	Loss/tok 2.5010 (3.1686)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][1200/1938]	Time 0.091 (0.083)	Data 8.96e-05 (3.46e-04)	Tok/s 93405 (83994)	Loss/tok 3.1945 (3.1681)	LR 1.000e-03
0: TRAIN [3][1210/1938]	Time 0.092 (0.083)	Data 9.20e-05 (3.44e-04)	Tok/s 90148 (84025)	Loss/tok 3.1941 (3.1676)	LR 1.000e-03
0: TRAIN [3][1220/1938]	Time 0.067 (0.084)	Data 9.56e-05 (3.42e-04)	Tok/s 77873 (84051)	Loss/tok 2.9784 (3.1672)	LR 1.000e-03
0: TRAIN [3][1230/1938]	Time 0.117 (0.084)	Data 9.11e-05 (3.42e-04)	Tok/s 99946 (84081)	Loss/tok 3.2658 (3.1671)	LR 1.000e-03
0: TRAIN [3][1240/1938]	Time 0.091 (0.083)	Data 8.85e-05 (3.40e-04)	Tok/s 91636 (84070)	Loss/tok 3.0955 (3.1659)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][1250/1938]	Time 0.067 (0.084)	Data 9.18e-05 (3.38e-04)	Tok/s 79513 (84115)	Loss/tok 2.9582 (3.1662)	LR 1.000e-03
0: TRAIN [3][1260/1938]	Time 0.117 (0.084)	Data 9.54e-05 (3.36e-04)	Tok/s 100116 (84130)	Loss/tok 3.1478 (3.1659)	LR 1.000e-03
0: TRAIN [3][1270/1938]	Time 0.068 (0.084)	Data 8.75e-05 (3.34e-04)	Tok/s 77134 (84121)	Loss/tok 3.1575 (3.1653)	LR 1.000e-03
0: TRAIN [3][1280/1938]	Time 0.069 (0.084)	Data 8.61e-05 (3.32e-04)	Tok/s 75283 (84107)	Loss/tok 2.7736 (3.1649)	LR 1.000e-03
0: TRAIN [3][1290/1938]	Time 0.067 (0.084)	Data 8.75e-05 (3.30e-04)	Tok/s 78972 (84123)	Loss/tok 2.8587 (3.1647)	LR 1.000e-03
0: TRAIN [3][1300/1938]	Time 0.119 (0.084)	Data 1.29e-04 (3.28e-04)	Tok/s 96393 (84144)	Loss/tok 3.2445 (3.1647)	LR 1.000e-03
0: TRAIN [3][1310/1938]	Time 0.095 (0.084)	Data 8.77e-05 (3.26e-04)	Tok/s 87524 (84166)	Loss/tok 3.1707 (3.1659)	LR 1.000e-03
0: TRAIN [3][1320/1938]	Time 0.119 (0.084)	Data 8.51e-05 (3.25e-04)	Tok/s 97659 (84202)	Loss/tok 3.3347 (3.1661)	LR 1.000e-03
0: TRAIN [3][1330/1938]	Time 0.066 (0.084)	Data 8.87e-05 (3.23e-04)	Tok/s 76280 (84217)	Loss/tok 2.9858 (3.1675)	LR 1.000e-03
0: TRAIN [3][1340/1938]	Time 0.065 (0.084)	Data 8.46e-05 (3.21e-04)	Tok/s 78620 (84193)	Loss/tok 2.8705 (3.1669)	LR 1.000e-03
0: TRAIN [3][1350/1938]	Time 0.118 (0.084)	Data 9.54e-05 (3.19e-04)	Tok/s 99059 (84205)	Loss/tok 3.4098 (3.1666)	LR 1.000e-03
0: TRAIN [3][1360/1938]	Time 0.117 (0.084)	Data 8.70e-05 (3.18e-04)	Tok/s 98766 (84228)	Loss/tok 3.3869 (3.1668)	LR 1.000e-03
0: TRAIN [3][1370/1938]	Time 0.066 (0.084)	Data 8.54e-05 (3.16e-04)	Tok/s 77020 (84247)	Loss/tok 2.9484 (3.1673)	LR 1.000e-03
0: TRAIN [3][1380/1938]	Time 0.043 (0.084)	Data 8.94e-05 (3.14e-04)	Tok/s 60138 (84194)	Loss/tok 2.5062 (3.1663)	LR 1.000e-03
0: TRAIN [3][1390/1938]	Time 0.067 (0.084)	Data 9.04e-05 (3.13e-04)	Tok/s 77727 (84163)	Loss/tok 2.9337 (3.1654)	LR 1.000e-03
0: TRAIN [3][1400/1938]	Time 0.119 (0.084)	Data 9.13e-05 (3.11e-04)	Tok/s 96873 (84184)	Loss/tok 3.4355 (3.1666)	LR 1.000e-03
0: TRAIN [3][1410/1938]	Time 0.120 (0.084)	Data 8.75e-05 (3.10e-04)	Tok/s 95986 (84187)	Loss/tok 3.3243 (3.1666)	LR 1.000e-03
0: TRAIN [3][1420/1938]	Time 0.066 (0.084)	Data 8.58e-05 (3.08e-04)	Tok/s 79575 (84173)	Loss/tok 3.0552 (3.1659)	LR 1.000e-03
0: TRAIN [3][1430/1938]	Time 0.067 (0.084)	Data 1.07e-04 (3.07e-04)	Tok/s 76893 (84190)	Loss/tok 3.0388 (3.1657)	LR 1.000e-03
0: TRAIN [3][1440/1938]	Time 0.091 (0.084)	Data 8.96e-05 (3.05e-04)	Tok/s 94097 (84187)	Loss/tok 3.1128 (3.1652)	LR 1.000e-03
0: TRAIN [3][1450/1938]	Time 0.092 (0.084)	Data 8.94e-05 (3.04e-04)	Tok/s 89084 (84225)	Loss/tok 3.1744 (3.1654)	LR 1.000e-03
0: TRAIN [3][1460/1938]	Time 0.066 (0.084)	Data 1.22e-04 (3.02e-04)	Tok/s 77948 (84253)	Loss/tok 2.9765 (3.1657)	LR 1.000e-03
0: TRAIN [3][1470/1938]	Time 0.152 (0.084)	Data 9.18e-05 (3.01e-04)	Tok/s 97032 (84272)	Loss/tok 3.4607 (3.1656)	LR 1.000e-03
0: TRAIN [3][1480/1938]	Time 0.043 (0.084)	Data 8.61e-05 (2.99e-04)	Tok/s 62386 (84260)	Loss/tok 2.5270 (3.1651)	LR 1.000e-03
0: TRAIN [3][1490/1938]	Time 0.119 (0.084)	Data 8.80e-05 (2.98e-04)	Tok/s 96688 (84273)	Loss/tok 3.5227 (3.1659)	LR 5.000e-04
0: TRAIN [3][1500/1938]	Time 0.091 (0.084)	Data 8.73e-05 (2.97e-04)	Tok/s 92071 (84272)	Loss/tok 3.0957 (3.1655)	LR 5.000e-04
0: TRAIN [3][1510/1938]	Time 0.066 (0.084)	Data 9.97e-05 (2.95e-04)	Tok/s 77961 (84271)	Loss/tok 3.0136 (3.1648)	LR 5.000e-04
0: TRAIN [3][1520/1938]	Time 0.119 (0.084)	Data 9.75e-05 (2.94e-04)	Tok/s 97228 (84256)	Loss/tok 3.2640 (3.1642)	LR 5.000e-04
0: TRAIN [3][1530/1938]	Time 0.091 (0.084)	Data 8.89e-05 (2.93e-04)	Tok/s 91510 (84288)	Loss/tok 3.0034 (3.1644)	LR 5.000e-04
0: TRAIN [3][1540/1938]	Time 0.065 (0.084)	Data 8.34e-05 (2.91e-04)	Tok/s 75502 (84271)	Loss/tok 2.8943 (3.1636)	LR 5.000e-04
0: TRAIN [3][1550/1938]	Time 0.118 (0.084)	Data 8.56e-05 (2.90e-04)	Tok/s 100704 (84277)	Loss/tok 3.1404 (3.1633)	LR 5.000e-04
0: TRAIN [3][1560/1938]	Time 0.119 (0.084)	Data 1.31e-04 (2.89e-04)	Tok/s 96154 (84307)	Loss/tok 3.2980 (3.1634)	LR 5.000e-04
0: TRAIN [3][1570/1938]	Time 0.066 (0.084)	Data 8.68e-05 (2.87e-04)	Tok/s 79802 (84301)	Loss/tok 3.1930 (3.1633)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][1580/1938]	Time 0.152 (0.084)	Data 9.56e-05 (2.86e-04)	Tok/s 99026 (84305)	Loss/tok 3.4839 (3.1631)	LR 5.000e-04
0: TRAIN [3][1590/1938]	Time 0.067 (0.084)	Data 8.94e-05 (2.85e-04)	Tok/s 77657 (84298)	Loss/tok 2.8952 (3.1627)	LR 5.000e-04
0: TRAIN [3][1600/1938]	Time 0.063 (0.084)	Data 9.97e-05 (2.84e-04)	Tok/s 81361 (84298)	Loss/tok 2.9107 (3.1620)	LR 5.000e-04
0: TRAIN [3][1610/1938]	Time 0.151 (0.084)	Data 8.99e-05 (2.83e-04)	Tok/s 99153 (84329)	Loss/tok 3.4264 (3.1622)	LR 5.000e-04
0: TRAIN [3][1620/1938]	Time 0.065 (0.084)	Data 1.01e-04 (2.81e-04)	Tok/s 78021 (84287)	Loss/tok 2.8162 (3.1613)	LR 5.000e-04
0: TRAIN [3][1630/1938]	Time 0.117 (0.084)	Data 9.25e-05 (2.80e-04)	Tok/s 99602 (84305)	Loss/tok 3.1123 (3.1610)	LR 5.000e-04
0: TRAIN [3][1640/1938]	Time 0.067 (0.084)	Data 9.01e-05 (2.79e-04)	Tok/s 76057 (84286)	Loss/tok 3.1240 (3.1605)	LR 5.000e-04
0: TRAIN [3][1650/1938]	Time 0.068 (0.084)	Data 8.51e-05 (2.78e-04)	Tok/s 75758 (84289)	Loss/tok 2.8222 (3.1604)	LR 5.000e-04
0: TRAIN [3][1660/1938]	Time 0.044 (0.084)	Data 8.77e-05 (2.77e-04)	Tok/s 58849 (84291)	Loss/tok 2.5161 (3.1602)	LR 5.000e-04
0: TRAIN [3][1670/1938]	Time 0.092 (0.084)	Data 9.13e-05 (2.76e-04)	Tok/s 92116 (84329)	Loss/tok 3.1417 (3.1601)	LR 5.000e-04
0: TRAIN [3][1680/1938]	Time 0.151 (0.084)	Data 1.06e-04 (2.75e-04)	Tok/s 98059 (84322)	Loss/tok 3.4413 (3.1597)	LR 5.000e-04
0: TRAIN [3][1690/1938]	Time 0.042 (0.084)	Data 8.44e-05 (2.74e-04)	Tok/s 61356 (84312)	Loss/tok 2.4310 (3.1588)	LR 5.000e-04
0: TRAIN [3][1700/1938]	Time 0.066 (0.084)	Data 8.30e-05 (2.72e-04)	Tok/s 80503 (84310)	Loss/tok 2.8108 (3.1582)	LR 5.000e-04
0: TRAIN [3][1710/1938]	Time 0.119 (0.084)	Data 9.13e-05 (2.71e-04)	Tok/s 99205 (84293)	Loss/tok 3.3450 (3.1577)	LR 5.000e-04
0: TRAIN [3][1720/1938]	Time 0.151 (0.084)	Data 8.99e-05 (2.70e-04)	Tok/s 98425 (84277)	Loss/tok 3.4515 (3.1573)	LR 5.000e-04
0: TRAIN [3][1730/1938]	Time 0.119 (0.084)	Data 8.70e-05 (2.69e-04)	Tok/s 99400 (84293)	Loss/tok 3.3110 (3.1574)	LR 5.000e-04
0: TRAIN [3][1740/1938]	Time 0.120 (0.084)	Data 9.08e-05 (2.68e-04)	Tok/s 96499 (84307)	Loss/tok 3.2033 (3.1573)	LR 5.000e-04
0: TRAIN [3][1750/1938]	Time 0.121 (0.084)	Data 9.01e-05 (2.67e-04)	Tok/s 95800 (84299)	Loss/tok 3.3148 (3.1571)	LR 5.000e-04
0: TRAIN [3][1760/1938]	Time 0.091 (0.084)	Data 8.77e-05 (2.66e-04)	Tok/s 92674 (84253)	Loss/tok 3.0375 (3.1562)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][1770/1938]	Time 0.066 (0.084)	Data 9.08e-05 (2.65e-04)	Tok/s 78093 (84254)	Loss/tok 3.0562 (3.1561)	LR 5.000e-04
0: TRAIN [3][1780/1938]	Time 0.093 (0.084)	Data 8.65e-05 (2.64e-04)	Tok/s 91004 (84247)	Loss/tok 3.0904 (3.1554)	LR 5.000e-04
0: TRAIN [3][1790/1938]	Time 0.065 (0.084)	Data 9.23e-05 (2.63e-04)	Tok/s 81824 (84228)	Loss/tok 2.9952 (3.1546)	LR 5.000e-04
0: TRAIN [3][1800/1938]	Time 0.042 (0.084)	Data 9.49e-05 (2.62e-04)	Tok/s 60516 (84165)	Loss/tok 2.5080 (3.1537)	LR 5.000e-04
0: TRAIN [3][1810/1938]	Time 0.091 (0.084)	Data 1.01e-04 (2.61e-04)	Tok/s 91214 (84197)	Loss/tok 3.0569 (3.1539)	LR 5.000e-04
0: TRAIN [3][1820/1938]	Time 0.066 (0.084)	Data 8.94e-05 (2.61e-04)	Tok/s 77449 (84190)	Loss/tok 2.8786 (3.1536)	LR 5.000e-04
0: TRAIN [3][1830/1938]	Time 0.064 (0.084)	Data 1.34e-04 (2.60e-04)	Tok/s 78526 (84177)	Loss/tok 3.0577 (3.1530)	LR 5.000e-04
0: TRAIN [3][1840/1938]	Time 0.118 (0.084)	Data 8.54e-05 (2.59e-04)	Tok/s 98799 (84183)	Loss/tok 3.1383 (3.1526)	LR 5.000e-04
0: TRAIN [3][1850/1938]	Time 0.065 (0.084)	Data 8.56e-05 (2.58e-04)	Tok/s 80752 (84184)	Loss/tok 2.9523 (3.1521)	LR 5.000e-04
0: TRAIN [3][1860/1938]	Time 0.091 (0.083)	Data 8.39e-05 (2.57e-04)	Tok/s 91532 (84192)	Loss/tok 3.0176 (3.1516)	LR 5.000e-04
0: TRAIN [3][1870/1938]	Time 0.068 (0.084)	Data 9.80e-05 (2.56e-04)	Tok/s 78010 (84190)	Loss/tok 2.9253 (3.1518)	LR 5.000e-04
0: TRAIN [3][1880/1938]	Time 0.094 (0.084)	Data 8.77e-05 (2.55e-04)	Tok/s 88837 (84203)	Loss/tok 2.9886 (3.1515)	LR 5.000e-04
0: TRAIN [3][1890/1938]	Time 0.067 (0.084)	Data 8.68e-05 (2.54e-04)	Tok/s 76511 (84218)	Loss/tok 3.0094 (3.1515)	LR 5.000e-04
0: TRAIN [3][1900/1938]	Time 0.093 (0.084)	Data 8.75e-05 (2.53e-04)	Tok/s 89481 (84257)	Loss/tok 3.0329 (3.1518)	LR 5.000e-04
0: TRAIN [3][1910/1938]	Time 0.120 (0.084)	Data 9.13e-05 (2.52e-04)	Tok/s 96751 (84265)	Loss/tok 3.2592 (3.1517)	LR 5.000e-04
0: TRAIN [3][1920/1938]	Time 0.044 (0.084)	Data 8.56e-05 (2.52e-04)	Tok/s 61282 (84243)	Loss/tok 2.5180 (3.1510)	LR 5.000e-04
0: TRAIN [3][1930/1938]	Time 0.065 (0.084)	Data 9.18e-05 (2.51e-04)	Tok/s 79853 (84257)	Loss/tok 2.8546 (3.1508)	LR 5.000e-04
:::MLL 1560905413.455 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 524}}
:::MLL 1560905413.455 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.438 (0.438)	Decoder iters 99.0 (99.0)	Tok/s 20450 (20450)
0: Running moses detokenizer
0: BLEU(score=24.07946739228596, counts=[37232, 18675, 10605, 6297], totals=[65557, 62554, 59552, 56554], precisions=[56.79332489284135, 29.85420596604534, 17.80796614723267, 11.134490929023588], bp=1.0, sys_len=65557, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1560905414.649 eval_accuracy: {"value": 24.08, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 535}}
:::MLL 1560905414.650 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 3	Training Loss: 3.1484	Test BLEU: 24.08
0: Performance: Epoch: 3	Training: 1348826 Tok/s
0: Finished epoch 3
:::MLL 1560905414.650 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 557}}
0: Closing preprocessed data file
:::MLL 1560905414.651 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 568}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-06-19 12:50:26 AM
RESULT,RNN_TRANSLATOR,,716,nvidia,2019-06-19 12:38:29 AM
